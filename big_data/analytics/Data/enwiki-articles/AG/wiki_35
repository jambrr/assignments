<doc id="42387" url="https://en.wikipedia.org/wiki?curid=42387" title="Whistle">
Whistle

An aerodynamic whistle (or call) is a simple aerophone, an instrument which produces sound from a stream of gas, most commonly air. It may be mouth-operated, or powered by air pressure, steam, or other means. Whistles vary in size from a small slide whistle or nose flute type to a large multi-piped church organ.
Whistles have been around since early humans first carved out a gourd or branch and found they could make sound with it. In prehistoric Egypt, small shells were used as whistles. Many present day wind instruments are inheritors of these early whistles. With the rise of more mechanical power, other forms of whistles have been developed.
One characteristic of a whistle is that it creates a pure, or nearly pure, tone. There are many ways to create pure tones, but we restrict the descriptions here to what are called aerodynamic whistles. Strictly speaking, they are fluid mechanical whistles since they occur in gases, such as air or steam, as well as in liquids, such as water. The only difference between them is the fluid density and the sound speed.
The word aerodynamic whistle is used here since it is in common use. The conversion of flow energy to sound comes from an interaction between a solid material and a fluid stream. The forces in some whistles are sufficient to set the solid material in motion. Classic examples are Aeolian tones that result in galloping power lines, or the Tacoma Narrows Bridge (galloping Gertie). Other examples are circular disks set into vibration.
The whistles described in this article are in a subclass where only the fluid is in motion and there is no significant dependent motion of the interacting solid. Depending on the geometry there are two basic types of whistles, those that generate sound though oscillations of fluid mass flow and those that generate sound through oscillations of the force applied to the surrounding medium.
Early police whistles.
Joseph Hudson set up J Hudson & Co in Birmingham, UK in 1870. With his younger brother James, he designed the 'Acme City' brass whistle. This became the first referee whistle used at association football matches during the 1878–79 Football Association Cup match between Nottingham Forest and Sheffield. Prior to the introduction of the whistle, handkerchiefs were used by the umpires to signal to the players.
In 1883 he began experimenting with pea-whistle designs that could produce an intense sound that could grab attention from over a mile away. His invention was discovered by accident, when he accidentally dropped his violin and it shattered on the floor. Observing how the discordant sound of the breaking strings travelled (trill effect), Hudson had the idea to put a pea in the whistle. Prior to this, whistles were much quieter, and were only thought of as musical instruments or toys for children. After observing the problems that local police were having with effectively communicating with rattles, he realised that his whistle designs could be used as an effective aid to their work.
Hudson demonstrated his whistle to Scotland Yard and was awarded his first contract in 1884. Both Ratchet rattles and whistles were used to call for back-up in areas where neighborhood beats overlapped, and following their success in London, the whistle was adopted by most police in the United Kingdom.
This police whistle monopoly gradually made Hudson the largest whistle manufacturer in the world, supplying police forces and other general services everywhere. His whistle is still used by many forces worldwide. His design, was improved as the 'Acme Thunderer', the first ever pea whistle, which remains the most used whistle in the world; for train guards, dog handlers and police officers.From the 1880s and 90s, J. Hudson & Co began facing greater competition, as other whistle manufacturing companies were established, including W. Dowler & Sons, J. Barrall, R. A. Walton, H. A. Ward and A. De Courcy & Co. In 1987, Ron Foxcroft released the Fox 40 pealess whistle, designed to replace the pea whistle and be more reliable.
Typical whistle sources and uses.
There are numerous sources and uses of aerodynamic whistles. They can be used for hunting, controlling dogs, as toys, for sports events, as musical instruments, for police or military use, as ship or lighthouse fog horns, as steam whistles on trains, for scheduling and emergency use in industrial settings such as mines, in fluidic circuits, in domestic settings such at the tea pot whistle, for reducing foam bubbles, and for human whistling. They also occur as accidental byproducts of fluid flow such as supersonic jets, cavity resonances, whistling telephone wires, and idling circular saws.
Types.
Wilson, et al., in their study of human whistling (see below), pointed out the importance of including the symmetry or asymmetry of the unstable flow in addition to the feedback classes listed below. Because of the close relationship of flow symmetry to the sound field generated, their concept was included here as part of the sound source description (monopole -symmetric and dipole - asymmetric).
The Monopole Whistle.
Whistles that generate sound through fluctuations of mass flow across a boundary are called monopole-like sources. The figure on the right is an example of a small sphere whose volume is oscillating. Any arbitrary fixed boundary drawn around the sphere will show a net mass flow across it. If the sphere is small enough relative to the sound wavelength it is emitting, it can be called a point monopole. For this type of source, the sound is emitted radially so the sound field is the same in every direction and decays with the inverse square of the distance. Real whistles are only approximations to this idealized model. most have boundaries around them such as the Hole Tone described below. Nevertheless, much can be learned about whistles with the useful form of the sound power equation for the monopole. Using the definitions below, it can be expressed as
formula_1
The variables U and L are considered characteristic of the source and their correct choice is important.
The Dipole Whistle.
Whistles that generate sound through fluctuations of momentum or a force exerted on the surrounding medium are called dipole-like sources. The figure on the right is example of a small "rigid" sphere that is moving back and forth in a given direction. If the sphere is small with respect to the wavelength of the sound emitted, it can be called a point dipole. A force must be applied to the sphere in a specific direction to move it. The surrounding medium in the direction of motion is compressed to radiate sound, but the medium at right angles slides past the sphere and is not compressed.
This results in a non-uniform sound field, unlike the monopole whistle. Real whistles are only approximations to this idealized model. Although tuning forks are not whistles, they create sound fields that are very close to the idealized dipole model. Nevertheless, much can be learned about whistles with the useful form of the sound power equation for the dipole. Using the definitions below, it can be expressed as
formula_2
Once again, U and L must be chosen correctly.
Feedback categories.
Aerodynamic whistles rely on the instability of flows and some mechanism downstream to send the disturbance back to the origin to continue the instability. There are several ways that feedback can occur.
Category I.
The sound from a Category I whistle is primarily a "byproduct" of source motion. In every case, there is a back reaction of the medium on the source (resistive and reactive impedance). One example of a weak back reaction is a vibrating iron body in air. The densities are so different the back reaction is often ignored. Back reactions of air on an air source or water on a water source can be different. In many cases, say turbulent jets, the sound created is random and it is convenient to consider the sound to be merely a byproduct of the flow. In this category the back reaction is insufficient to strongly control source motion, so whistles are not in this category.
Category II.
The back reaction of the medium is an "determinant" of source motion. In many important cases, linear thinking (small cause = small effect) is fallacious. Unstable fluid motion or the sound generated by it can feedback to the source and control it, much like audio feedback squeal. The basic requirements for a feedback controlled system are: (1) a source of steady power; (2) an amplification mechanism that can convert the steady power to time varying power; (3) a disturbance which supplies the oscillations to be amplified; (4) a means of generating sound or other oscillatory fluid motion; and (5) a means for feedback of that oscillatory motion as a disturbance to the input of the amplifier. Whistles are in this category. There are several ways to describe the feedback process.
Class 1.
The feedback is essentially incompressible; the speed of sound, although finite, is sufficiently large that it can be considered infinite. This action may be called near field or hydrodynamic feedback. There are a number Class I devices. The feedback that causes a stick in a water stream to vibrate, or a flag to wave, is due to hydrodynamic feedback.
Class II.
The feedback is compressible and depends on the speed of sound. This action may be called intermediate field, quasi-compressible feedback. A well known example is the hole tone (described below) where the feedback distance of a compressible (sound) wave is very small as compared to the wavelength of the sound.
Class III.
The feedback is compressible and depends on the speed of sound. This may be called far field or acoustic feedback. the feedback distance of a compressible wave can be an appreciable fraction of the wavelength of the sound. An example is the flute.
The figure on the right shows a block diagram of these feedback mechanisms. All aerodynamic whistles operate under one of the classes.
Stages.
There are feedback loops associated with many whistle operations and they are non-linear. Because of the nonlinearity, there can be several possible conditions for a given flow speed or geometry. Which of these is dominant is determined by the gain of the unstable flow at a particular frequency and whether the feedback is constructive or destructive.
Early studies have used the term stage to describe the possible states of feedback as shown schematically in the figure on the right. As the flow speed increases (Reynolds number - Re) the frequency slowly climbs (nearly constant Strouhal number -St) but then the frequency jumps up abruptly to a higher stage. As the flow speed is later decreased the frequency slowly decreases but then jumps down abruptly to a lower stage. This pattern is called a "hysteresis loop".
At any particular flow speed it is possible for one of several loops to be dominant, depending on how that speed was achieved. In a number of the whistles described here, Stage I is associated with the development of a single vortex in the distance between initiation of the flow instability and initiation of the feedback signal.
Higher stages are associated with more vortices in that distance, hinting that this distance can be an important characteristic dimension. In several whistles, three stages have been identified (edge tone). By blowing hard in some musical wind instruments Stage I jumps to Stage II; it is called "overblowing".
Flow instability.
Flow instability is the engine for whistles. It converts steady energy to time dependent energy. Conversion of laminar flow to turbulent flow is a well known example. Small disturbances to the laminar flow cause the transition.
An example is shown in the figure on the right with a water jet. The laminar two-dimensional jet amplifies small disturbances at the orifice to generate a "vortex street". For this case, the flow speed, in terms of Reynolds Number, was graphed against the disturbance frequency, in terms of the Strouhal Number for a variety of disturbance amplitudes to reveal the region of instability as shown in the figure on the left. The value of D in the figure represents the ratio of the lateral disturbance displacement to the nozzle width; the disturbances were minute.
The disturbance was temporal in the example, but can also be spatial. Transition to turbulence can occur over a rough surface or over an irregular shape, such as an aircraft spoiler. All whistle mechanisms described here are created by temporal disturbances that are in one of the three classes.
One important source of instability in a fluid is the presence of a velocity gradient or shear layer with an inflection point. Thus, fluid instability can be described as a three dimensional region with flow speed on one axis, disturbance amplitude on the second, and velocity profile on the third.
In a whistle, the instability starts at some point in the three dimensional region and then moves along some path in that region as the local variables change. This makes the comprehensive understanding of whistle instability mechanisms very difficult.
Scaling.
Whistles come in all shapes and sizes, but their operation can be unified through the concepts of dynamic and geometric similarity. Nature knows nothing of the specific measurement systems we use; it cares only about ratios between the various forces, time scales, and the several dimensions. To compare them we need to take into account the established ratios that are relevant to whistle operation.
Similarity is best exposed by determining a speed, U, that is characteristic of the dynamics, and a dimension, L, that is characteristic of the geometry. If these values are used in dimensionless numbers, such as those listed below, much understanding of the phenomenon can be achieved.
Strouhal number.
formula_3
The first number is the ratio of unsteady inertial forces to steady inertial forces. The number was named in honor of Vincenz Strouhal who first deduced the relationship between the vortex shedding frequency around a cylinder and the flow speed. The characteristic variables were the cylinder diameter L1, and the speed of the flow over it U. He found the number to be reasonably constant over given a Reynolds number range.
This number permits relationships to be developed between the different sizes and speeds. Now the Strouhal Number can be derived directly from the dimensionless form of the mass continuity equation. This equation may be referred to as a "fluid mechanical" Strouhal number in comparison with the second version which may be referred to as the "acoustical" Strouhal number.
The first version is used for dynamic similarity of the fluid motion in whistles while the second version is used for dynamic similarity of the acoustical motion in whistles. Many whistles, especially those with Class III feedback, require use of both numbers (see (Monopole-Dipole Whistles). The acoustical Strouhal number is essentially the Helmholtz number with the formula_4 deleted.
Mach number.
formula_5
It is the ratio of the steady speed to the speed of sound. The number was named in honor of Ernst Mach who first studied (among other things) supersonic flow and shock waves.
This number describes the range between flows that can be considered incompressible and flows where compressibility is important. Now the Mach number can be derived directly from the dimensionless form of the momentum equation.
Reynolds number.
formula_6
It is The ratio of the steady inertial forces to the steady viscous forces.
The number was named in honor of Osborne Reynolds, an engineer who did pioneering studies on the transition of laminar to turbulent flow in pipes.
Now the Reynolds number can be derived directly from the dimensionless form of the momentum equation.
Rossby number.
formula_7
It is the ratio of linear velocity to tangential velocity for swirl flows. The frequency is characteristic of the rotation rate of the flow.
The number was named in honor of Carl-Gustav Rossby, a meteorologist who first described the large scale motions of the atmosphere in terms of fluid mechanics. He described the jet stream, and his number was first used to describe the motion associated with the Coriolis force in the atmosphere.
Now the Rossby number can be derived directly from the dimensionless form of the momentum equation in curvilinear coordinates.
Dimensionless force.
formula_8 The ratio of the actual dynamic force to the steady momentum..
Dimensionless volumetric flow rate.
formula_9 The ratio of the dynamic volumetric flow rate to the steady volumetric flow rate.
Monopole-like whistles.
In these whistles, the flow instability is symmetric, often resulting in periodic ring vortices and the sound generation is associated with fluctuations of volumetric (mass) flow rates. The sound field is as close to the directivity of an actual monopole source as local geometry allows.
Hole tone (tea pot whistle, bird call).
The steady flow from a circular orifice can be converted to an oscillatory flow by adding a downstream plate with a circular hole aligned with the orifice. Small perturbations in the flow feedback to the orifice to cause a variable volumetric flow rate through the downstream hole because of the symmetry of the feedback.
The disturbance in the jet is a symmetric vortex ring that moves at some speed slower than the mean jet speed until it encounters the hole and some fluid is forced through it, resulting in a monopole-like sound field in the half space outside. The oscillatory volumetric flow in the hole sends a wave back to the orifice to complete the feedback loop and causing a nearly pure tone.
The figure on the right shows a schematic of the geometry.
To invoke dynamic similarity, the characteristic speed in a study was chosen to be the average speed of the jet at the orifice U (deduced from measured volumetric flow rate) and the characteristic length was chosen to be the orifice diameter δ. Tests were made at five spacing distances h/δ from the orifice. Two scaling laws were used; the Strouhal number was graphed as a function of Reynolds number. The results are shown in the figure on the right.
The frequency of the tone determined by how often a vortex encountered the hole while moving at some speed u less than the initial jet speed. Since the jet slowed as it proceeded toward the hole, the speed of the vortex slowed with it so the frequency and Strouhal number was greater at closer spacing. The Strouhal number data showed clearly the almost linear relation between frequency and initial jet speed. The number would be more constant if the actual jet speed at the hole could have been used as the characteristic speed. In four of the distances tested, there were jumps between Stage I and Stage II. The hysteresis loops are clear indications of the complex nature of the jet instability gain structure.
The uniformity of the measured sound field for this whistle confirmed its monopole-like nature. Measurements of speed dependence of the sound level showed it to be very close to formula_10, further confirming the monopole nature of the source. At these speeds and spacings, the feedback was normally Class II, but reflecting surfaces as far away as three meters and with proper phasing, controlled the tone, converting the feedback to Class III.
At higher Reynolds Numbers, the flow became chaotic resulting in broad-band sound. The hole tone has been rediscovered in the form of the tea pot whistle. They found that above a Reynolds number of 2000, hole tone operation occurred with symmetric vortex evolution and a constant Strouhal Number with Reynolds number. Comparison of their data with data in the figure suggests that the cylindrical enclosure between the two orifices raises the Strouhal number. There was no mention of frequency jumps. They noted that at lower speeds, the cylindrical volume responded as a Helmholtz resonator. Baron Rayleigh was aware of this whistle; it was called the "bird call" then.
There seems to be evidence that events similar to the hole tone occur on aircraft landing gear covers with circular holes. In Australia, there is the "Tenterfield Fox Whistle" and the "Traditional Fox Whistle" that appear to operate as hole tones.
Corrugated pipe whistle.
This whistle has dozens of popular names. Pipes with sinusoidal variations of radius are often created to permit bending. Steady flow through the pipe at low Reynolds numbers results in a fluctuating volumetric flow rate that generates a monopole-like sound field at the pipe exit. Examples of such pipes are shown in figure on the right.
The yellow plastic pipe is actually a child’s toy that sounds when the pipe is whirled around. The metal pipe shown was used in the Concorde cockpit to provide cooling air to the pilots, but its loud tone got it canceled. This whistle is similar in many respects to the hole tone, in particular the teapot whistle. It is subject to frequency jumps and hysteresis loops. There are numerous articles on the internet about this whistle and it has been studied.
The characteristic speed is the mean flow through the pipe U and the characteristic length must be a multiple of the spacing between corrugations, nL, where n is an integer number and L is the distance between corrugations. At low speeds, the unstable interior flow needs to travel several corrugations to establish the feedback loop. As the speed increases, the loop can be established with fewer corrugations. Simple tests were performed on the yellow plastic tube.
The Strouhal number,
formula_11 was used as the scaling factor. The highest frequency (7554 Hz) was found in the “overblown” condition and n was presumed as one corrugation. At the least flow rate, the frequency of 2452 Hz compared favorably to n=3. At intermediate flow rates, several non-harmonically related frequencies occurred simultaneously suggesting that several corrugations were involved in the sound generation. In the smaller metal tube, a predominant tone appeared at 6174 Hz and corresponded to n=2. A unique aspect of this whistle is that the internal flow carries both the unstable vortex downstream and the returning feedback signal upstream.
Pipe tone (Pfeifenton).
The unique feature of this whistle is that the tone sounds only with flow through the orifice from outside; it is an acoustical diode. A cylindrical cavity with a small circular, square-edged hole at one end and totally open at the other is known to generate a tone when air is passed through it. It is subject to frequency jumps and hysteresis loops similar to the hole tone. There appear to be two stages and the feedback is likely Class II if the tube is lshort. The fundamental tone occurs near formula_12, so one characteristic dimension is L, the length of the tube. The characteristic speed U is that of the flow through the hole.
A monopole-like sound field is generated by volumetric flow rate oscillations. Karthik and Anderson have studied this phenomenon and concluded that symmetric vortex shedding on the cavity side is the driving agency.
An example of this device is shown in the figure on the right; it had a 0.125 inch diameter hole, was 1.9 inches long, and 0.8 inches in diameter. The quarter wave resonance was calculated to be 1780 Hz, while the measured fundamental was 1625 Hz with detectable second and third harmonics. End corrections for radiation from the openings is needed to bring the two frequencies in consonance. To determine the end corrections, two additional dimensions are needed: the diameter of the orifice d1 and the diameter of the tube d2.
Hartmann, Galton whistles (stem jet).
The previous whistles occur at low flow speeds, this whistle occurs at very high speeds. When a subsonic jet impinges on a cavity, jet instability becomes part of the feedback loop as with the hole tone. When a supersonic jet impinges on a cavity, shock wave instability becomes part of the feedback loop. The figure on the right is one example of this whistle. A cylindrical cavity with one end open and facing the supersonic circular jet will result in extremely intense sound. The shapes in the figure represent the shock/expansion cells within the jet. A related configuration, called the "stem jet", has a central rod in the jet that extends to support and align the cavity. There are a number of other geometric variations, all of which operate in similar fashion.
These devices have been studied, and reviewed by Raman. Here we look primarily at the Hartmann whistle. The shock cells of the jet interact with the shock in front of the cavity (the flow in the cavity being subsonic). Small symmetric disturbances in the jet stream are amplified as they proceed toward the cavity (similar in some respects to the hole tone) causing the shock in front of the cavity to oscillate. The shock front acts much like a piston source of high energy resulting in a monopole-like sound field. Again the volumetric flow is directional unlike the theoretical monopole.
The sound field may be similar to that created by oscillatory flow from a pipe, except for presence of the supersonic jet structure which can strongly modify the directivity. The original equation of Hartmann is shown below.
formula_13
The diameter of the orifice and cavity is d, the distance between orifice and cavity is h, and the orifice pressure P was given in kilograms per meter squared. At the lower limit of h the second term disappears. In this case, the equation could have been reformatted in terms of the acoustical Strouhal Number as shown in the second equation above. The characteristic speed U at the nozzle is the sound speed formula_10. It is interesting that the number is very close to that found by Strouhal for flow over a cylinder. There are two characteristic length scales. Nozzle diameter d characterizes the sound power while the separation distance h characterizes the frequency.
Comprehensive studies of this phenomenon have shown that the position of the cavity is critical in creating sound. The process has hysteresis loops and the frequencies are related to multiples of the quarter wavelength resonance of the cavity. After reformatting Hartmann’s formula, and using the new formulation above, an equation for sound power can be written.
formula_15
Since the characteristic speed U and sound speed are essentially the same, it can be rewritten as the second equation. This equation has the same structure as the one for the point monopole shown above. Although the amplitude factor A replaces the dimensionless volumetric flow rate in these equations, the speed dependence strongly confirms the monopole-like characteristics of the Hartmann whistle. A cousin to the Hartmann whistle is shown in the figure on the right, the "Galton whistle". Here the cavity is excited by an "annular jet" which oscillates symmetrically around the sharp edges of the cavity. It appears to be a circular version of the edge tone (discussed below) in which the symmetry of the otherwise dipole source of the edge tone is converted to a monopole source.
Since it is highly likely that the oscillations are coherent around the periphery, there should be a fluctuating volumetric flow rate from the cavity with only a small net lateral force. Thus the source is yet another version of a monopole-like geometry; the volumetric flow rate is a cylindrical area between the jet and cavity.
Rijke tube.
There are a number of whistle phenomena in which heat plays a role. The temperature in a sound wave varies, but since it is so small normally it is common to neglect its effects. When amplification can occur a small variation can grow and have important influence on the sound field created. The most well known thermal whistle is the "Rijke Tube". Peter Rijke placed a heated a gauze material inside a vertical tube.
Originally, the gauze was heated with a Bunsen burner; later, a wire grid was heated electrically. The heat transferred to the air in the tube sets it into near half-wave resonance if the gauze is placed below the midpoint of the tube as shown in the figure on the right. There is no theoretically optimum position, as the wave speed upward is the sound speed formula_10 plus u, the convection speed, while the wave speed downward is formula_10 minus u. Without a convection flow, the midpoint and the lower tube end are the best locations for heat transfer. With convection, a compromise position halfway between the two points is normally chosen which depends on the amount of heat added. One characteristic length associated with frequency is tube length L.
Another characteristic length associated with sound power is αL, the position of the gauze. The characteristic speed must be the convection speed u at the heat source. For detailed study of the whistle, see Matveev. Since the first mode resonance is about half-wave, the sound field emitted from the tube is from two in-phase monopole-like sources, one at either end. A gas flame inside a tube can drive resonance; it was called a "singing flame". There is a reverse Rijke Tube when hot air passes through a cold grid.
Sondhauss and Taconis tubes.
The Sondhauss Tube is one of the early thermal tone generators; it was discovered in the glass blowing industry. A bulb with hot air is connected to one end of a tube that is at room temperature. When the cold tube is blown, tube acoustic oscillations occur. It was discussed by Baron Rayleigh in his Theory of Sound. This device is not considered a true whistle since the oscillations decay as the temperatures equalize.
In analyzing this tube, Rayleigh noted that if heat had been added at the point of highest density in the sound wave, and subtracted at the point of lowest density, the vibration would be encouraged. Another thermal effect is called the Taconis oscillation. If a stainless steel tube has one side at room temperature and the other side in contact with liquid helium, spontaneous acoustic oscillations are observed. Again, the Sondhauss tube is not a true whistle.
Human whistle.
The number and variety of whistles created by humans is quite large, yet very little has been done to examine the physics of the process. There are three possible mechanisms: Helmholtz resonance, symmetric hole tone operation (monopole), or asymmetric edge tone operation (dipole).
Wilson and his colleagues have simulated the human whistle by creating a cylinder 2.04 inches in diameter with a rounded orifice at one end that supplied a jet and another rounded orifice at the other end of the same diameter and on the same axis. The geometry was very similar to that of the tea pot whistle. After a number of tests at various speeds, orifice diameters, and orifice thicknesses, they concluded that the whistle was created by a Helmholtz resonance in the cylinder volume. There was enough data for one case in their study to calculate the Strouhal and Reynolds numbers. The results are shown in the figure on the right.
The Strouhal number was essentially constant over the limited speed range, suggesting hole tone operation with Class I or Class II feedback. Their work indicated symmetric unstable vortex flow as would be expected but there was no mention of stages. In the study by Henrywood, it was noted that Helmholtz resonance could occur at low speeds. The flexibility of the mouth suggests that although a hole tone feedback mechanism is highly likely, the possibility of Helmholtz resonances in the mouth cavity and asymmetric edge tone actions with the teeth are considered possible.
Dipole-like whistles.
In these whistles, the flow instability is asymmetric, often resulting in rows of alternate vortices and the sound generation is associated with fluctuations of applied force. The sound field is as close to a dipole source as local geometry allows.
Aeolian tone.
The steady flow over a cylinder (or similar object) will generate vortex shedding and consequent sound. The early Greeks used this phenomenon to develop a harp and the sound was called an Aeolian tone after Aeolus, God of the Wind.
Whistling telephone wires, automobile radio antennae, certain automobile front grilles, and smoke stacks are other examples of this tone. At very low Reynolds numbers, the flow around a cylinder is stable, forming two fixed vortices behind it. As the speed increases, the flow, although laminar, becomes unstable and vortices are shed "alternately".
Hydrodynamic feedback (Class I) influences the formation of new vortices and exerts a fluctuating force on the cylinder. The flow field is shown in the upper figure on the right (created by Gary Koopman). Theodor von Karman identified and analyzed the flow behind objects like a cylinder and since then this special flow has been called the "Karman vortex street". Vincenz Strouhal was the first to scientifically investigate the sound emitted by flow around a rigid cylinder. At low Reynolds numbers the tone was pure and the frequency was proportional to the steady flow speed U and inversely proportional to the cylinder diameter d.
For many applications, the first equation below is often used. A review of the literature produced the figure on the right for the Strouhal number. At low Reynolds numbers the Strouhal number rises as inertial effects begin to dominate and then decays slightly at higher numbers. The second equation below is a best fit for the data between 1000<Re<100000.
formula_18
It is surprising how often oscillatory flow phenomenon have Strouhal Numbers in this range. For shape comparison, the Strouhal number for an ellipse has been measured at 0.218, a cylinder at 0.188, a square at 0.160, and a triangle at 0.214. The characteristic dimension is that of the object lateral to the flow and the characteristic speed is that of the impinging flow.
The second equation suggests that the Strouhal Number is a weak negative function of Reynolds number. This suggests that the dynamic similarity approximation is reasonable. The fluctuating force exerted on the cylinder is a result of the flow circulation around it caused by the alternate vortex separation as suggested in the third figure. The fact that the vortices are not directly behind the cylinder suggests that the force vector has both a lift and drag component resulting in lift and drag dipoles.
An approximate way to relate the sound generated to the flow characteristics is to perturb the standard drag equation with velocity perturbations as shown in the upper equation below.(lift measurements for cylinders are generally not available). The upper equation is the modified drag equation with both drag component u and lift component v and the cross sectional area dL where d is the cylinder diameter and w is the length.
formula_19
Manipulation of the equation yields the lower two equations for the dipole sound power of both lift and drag. Each time a vortex is shed, the drag velocity fluctuation u has the same sign, but the lateral velocity fluctuation v, has opposite signs, since the vortex is shed on alternate sides. As a result, The drag dipole would be expected to be twice the frequency of the lift dipole.
Phillips found the lateral velocity fluctuations were two orders of magnitude greater than the longitudinal, so the lift dipole is 20 dB above the drag dipole. He found the drag dipole did occur at twice the frequency of the lift dipole. At higher speeds, the vortex separation may not be correlated over the entire length of the cylinder resulting in multiple essentially independent dipole sources and lower sound power. The lower figure on the right shows the correlation coefficient as a function of distance along the cylinder and is from the Etkin, et al. study.
Vortex meters.
Is there a use of Aeolian tone knowledge other than making musical instruments? There are several accurate flow meters that are designed to take advantage of the constancy of the Strouhal Number with Reynolds Number to provide a linear relationship between flow rate and measured frequency. They are called "vortex meters".
A particular shaped object is placed within a pipe and a pressure sensor is embedded either in the pipe wall or in the inserted object. Although a number of shapes have been used, there are several that work well. The figure on the right shows data from a meter called the "Deltameter". The inserted shape was that of a trapezoid with the wider end facing upstream. The graph shows a nearly linear relationship with flow speed over nearly a 1000 to 1 range of Reynolds Numbers (12,530 to 1,181,000). Orifice plates as flow meters typically have a 5 to 1 speed range, while turbine meters may have up to 100 to 1 range.
It should be noted that three-dimensional (viscous) effects occur at low Reynolds numbers so dynamic similarity is not achieved there. At higher speeds the Strouhal Number is close to that for the Aeolian tone. The dependence of Strouhal Number on Reynolds Number for this confined geometry is slightly negative as was found also for the Aeolian tone.
Mountain vortex tone.
Photographs taken from space have shown alternate arrangements of clouds around mountains; The figure on the right shows an example. Does this type of event create periodic sound? The NOAA laboratories in Boulder, Colorado, were tasked with detecting the extremely low frequency sound of nuclear tests. They detected one and by triangulation determined it was occurring in the Aleutian chain. As the figure suggests, it was vortex shedding from a volcanic cone. There are numerous satellite photos available on the web showing this phenomena in many places of the world.
These whistles generate enough sound to be detected but at frequencies below 1 Hz they are inaudible. Like the Aeolian tone, the feedback is Class I. Using Strouhal's number, it might be possible to estimate the wind velocities; they appear to be quite high.
Trailing edge tone.
The boundary layer on the airfoil of a glider is laminar and vortex shedding similar to that of a cylinder occurs at the trailing edge. The sound can be a nearly pure tone.
The figure on the left shows a one-third octave band spectrum taken under a glider flyover; the tone is 15 dB above the broad band sound. The aircraft speed U was and the frequency was near 1400 Hz.
Based on a Strouhal Number of 0.20, the characteristic dimension δ was calculated to be near ¼ inch; the boundary layer thickness. A dipole sound field was created at the trailing edge due to the fluctuating force exerted on it.
At higher speeds on powered aircraft, the boundary layer on the airfoil is turbulent and more complex vortex shedding patterns have been observed. Since it is difficult to measure in flight, Hayden made static tests.
The figure on the right shows an example. A boundary layer flow was created on both sides of a thin rigid flat plate which terminated with a square trailing edge. Note the nearly pure tone at 2000 Hz with a Strouhal number of 0.21 protruding above the turbulent sound spectrum. Once again the magic number of Strouhal appears. The characteristic speed was the mean speed of the jet, U and the characteristic dimension was chosen as the trailing edge thickness.t. The better characteristic dimension would have been the boundary layer thickness, but fortunately the two dimensions were almost the same. The measured sound field was clearly dipole-like (modified slightly by the plate presence).
The lower figure on the right shows a number of turbulent sound spectra measured at various speeds. The frequencies were Strouhal number scaled with U and the sound levels were scaled with the dipole sound power rule of formula_10 over a speed range of 3 to 1. The data fit was quite good, confirming dynamic similarity and the dipole model. The slight discrepancy in level and frequency overlap suggests that both the dimensionless force and the Strouhal number had weak dependence on the Reynolds number.
Another characteristic dimension is the airfoil chord. In these tests the jet width was sufficient to keep the vortex shedding coherent across it. On an airfoil there would be a correlation length less than the wingspan resulting in a number independent dipoles arrayed laterally. The sound power would be diminished somewhat. Since the dipole model is based on the time rate of change of the force, reduction of sound power might be accomplished by reducing that rate. One possible means would be for the opposite sides of the surface to gradually sense each other spatially prior to the trailing edge and thus reduce the rate at the edge. This might be done by a section of graduated porous or flexible materials.
Circular saw whistle.
An edge tone occurs when a jet impinges on a fixed surface. A trailing edge tone occurs when an exterior flow passes over a trailing edge. There is a whistle that is a combination of an edge tone and a trailing edge tone and might be called a "wake-edge tone". It occurs in rotating circular saws under idling conditions and may be called the "circular saw whistle". Under load conditions, blade vibration plays a role which is not addressed here.
There have been several studies of the fundamental sound generating mechanisms of this whistle.
A drawing of typical blade construction is shown in the figure on the right. Research has shown that the sound field is dipole with the primary axis perpendicular to the blade plane. The sources are fluctuating forces acting on each cutting blade. Bies determined that the characteristic speed was the blade velocity, formula_21, and the characteristic dimension was the tooth area. Other researchers used blade thickness as the characteristic dimension. Cho and Mote found that the Strouhal number formula_22 was between 0.1 and 0.2 where h was the blade thickness. Poblete, et al., found Strouhal numbers between 0.12 and 0.18. If the edge tone is relevant, perhaps the characteristic dimension should be the gap between the blades.
The researchers deduced that the fluctuating force was proportional to formula_10, but the sound power was found to vary from formula_10 to formula_10. If the measurement bandwidth is broad and the measurement distance is out of the near field, there are two dynamic factors (Strouhal number and dimensionless force), that can cause the exponent to be less than six. Both the Deltameter and hole tone data show the Strouhal number is a weak negative function of Reynolds number, which is squared in the sound power equation. This would result in a reduced speed exponent. This factor is not likely to explain the large reduction in exponent however.
The blade geometry was highly variable in the tests, so it is likely that the negative dependence of the dimensionless force on Reynolds number is the major factor. This whistle has two features that separate it from the other whistles described here. The first is that there are a multiplicity of these dipole sources arrayed around the periphery, that most likely are radiating at the same frequency, but incoherently. The second is that blade motion creates a steady, but rotating, pressure field at each blade. The rotating steady force creates a rotating dipole field which has an influence in the geometric near field. The feedback is Class I (hydrodynamic) and there is no indication that stages other than Stage 1 occur.
Ring tone.
The word "ring" here refers to something akin to that worn on the finger and not the sound from a cellphone. The flow from a circular orifice impinging on a toroidal ring of the same diameter as the orifice will result in a tone; it is called a "ring tone". It is similar to the hole tone described above except that because the plate was replaced by a ring a fundamental change in the resultant sound field occurs. Small disturbances at the ring feed back to the orifice to be amplified by the flow instability (Class I). The unstable flow creates a set of symmetric (ring) vortices that later impinge on the physical ring.
The passage of a vortex by the ring is shown schematically in the figure on the right in three steps. The flow vectors in the figure are merely suggestive of direction.
When two vortices are equidistant from the ring, one being beyond and the other approaching, the net circulation around the ring is zero; the null point for the flow oscillation. Each vortex creates a circular (ring) flow field whose axis varies slightly from the vertical as it passes. The figure suggests that the main component of the force on the physical ring is in the direction of the jet flow. If the vortex is a true ring (all parts are in phase), a dipole sound field directed along the jet axis is created.
The figure also suggests that there is a lateral component of force which can only be interpreted as a weak radial dipole. Experiments have been performed on the ring tone. The lower figure on the right shows the relationship of frequency to Reynolds number. If the Strouhal number were graphed in lieu of the frequency, it would have shown that contours were reasonably constant similar to those for the hole tone. Close examination of the data in the figure showed a slight negative dependence of Strouhal number on Reynolds number.
It appears that this whistle has only two stages. The sound field was measured and clearly indicated a dipole whose axis was aligned with the jet axis. Since there were no reflecting surfaces near the source, the data also indicated that a weaker radial dipole component also existed. Such a field can only exist if there is a time delay at a distant point between each of the force components.
Inaudible whistles.
Most of the whistles described generate nearly pure tones that can be heard. The mountain tones discussed above are examples of tones that are inaudible because they are below the frequency range of humans. There are others whose sound levels are below the audible range of humans. For example, the vortex street behind a stick underwater might radiate at audible frequencies but not sufficiently to be heard by a Scuba diver. There are others that are both below audible frequencies and below audible levels.
An unstable water jet, similar to the one shown in the flow instability section above, was not disturbed deliberately, but was allowed to rise to a free water surface. On contact with the surface, a slight jet asymmetry caused an unsymmetrical raised surface that fed back to the jet origin and began a process that looked like a weak version of the flow instability figure. If the jet was not powered, but warmer than the surrounding fluid, it would rise and when encountering the surface would generate a similar feedback system.
Such a phenomenon was observed, but not photographed, in the Owens Valley of California. Early in the morning with no wind, thin clouds were observed to form above the valley. The distinction was that they were created alternately and moved in opposite directions away from a central location on the valley floor, suggesting the existence of an inaudible free convection whistle. The reason for including this type of whistle is that we tend to think that it is necessary for a forced jet flow to encounter a solid material to create a whistle. Perhaps it would be more correct to generalize the concept to a particular impedance mismatch rather than a solid object. The Hartmann whistle and the jet screech fits into this generalization. The concept also applies to any fluid motion as opposed to a strictly forced flow.
Vortex whistle.
When the swirling flow within a pipe encounters the exit, it can become unstable. An example of the original system is shown in the figure on the left. The instability arises when there is a reversed flow on the axis.
The axis of rotation itself precesses around the pipe axis resulting in a rotating force at the pipe exit and results in a rotating dipole sound field. Studies of this whistle have shown that dynamic similarity based on the pipe diameter d as the characteristic length scale, and inlet mean flow speed U as the characteristic speed was not achieved, as shown in the lower figure on the right. A more correct speed would be that characteristic of the swirl fd, where f is the precession (and sound) frequency, based on the Rossby number. To test the relevance of this new characteristic speed, the flow rate was increased and the frequency and level of the sound was measured. Using the dipole model, the calculated force was found to be nearly proportional to formula_10, confirming the correctness of the new characteristic speed.
Measurements showed that the vortex whistle was created by a rotating asymmetric vortex which created a rotating force vector in the plane of the exit and a rotating dipole sound field. The phenomenon of swirl instability has been shown to occur in other situations. One was the flow separation on the upper side of delta-shaped airfoils of high speed aircraft (Concorde), The angle of attack of the leading edge resulted in a swirl flow that became unstable. Another is the flow within cyclone separators; the swirling flow there occurs in an annular region between two tubes. The flow reverses at the closed end of the outer tube and exits through the inner tube. Under certain conditions, the flow in the reversal region becomes unstable, resulting in a period rotating force on the outer tube.
Periodic vibration of a cyclone separator would indicate vortex instability. Large centrifugal fans sometimes use radial inlet blades that can be rotated to control the flow into the fan; they create a swirling flow. At near shutoff, where the swirl is very high, "rotating blade stall" of the fan blades occurs. Although not researched, it is highly likely that swirl instability is the cause. The feedback is clearly hydrodynamic (Class I) and there is no indication that more than one stage occurs.
Swirl meter.
The method of creating swirl in the vortex whistle was considered the cause for lack of dynamic similarity, so the swirl was created in a pipe with a contraction having swirl blades followed by an expansion to create the required axial backflow. This was the vortex whistle in a pipe. Measurements made with this geometry, are shown in the figure on the right. As can be seen, dynamic similarity was achieved with both air and water. This whistle became a flow meter called the "swirlmeter". Its accuracy rivals that of the vortex shedding meters described above but has a higher pressure drop. The feedback is hydrodynamic (Class I) and only one stage was found.
Edge tone.
When a rectangular jet impinges on a sharp edged object such as a wedge, a feedback loop can be established resulting in a nearly pure tone. The figure on the right shows schematically the circulation of two vortices as they pass the wedge. This simple diagram suggests that there is a force applied to the wedge whose angle varies as the vortices pass.
As found in the Aeolian tone, the vertical component (lift) is large and results in a dipole-like sound field at the wedge (shown in the lower figure) and a much weaker horizontal component (drag) at twice the frequency (not shown). The drag component may contribute as part of the driving force for musical instruments (discussed below). A seminal study by Powell of this phenomenon has exposed many details of the edge tone phenomenon. He showed that this whistle has three stages and the feedback loop was hydrodynamic (Class I). A semi-empirical equation for the frequency, developed by Curle, when converted to Strouhal Number, is
formula_27
This equation, applicable for formula_28, shows the mean speed U of the jet at the orifice as the characteristic speed and the distance h from orifice to the edge as the characteristic dimension. The integer n represents the various vortex modes. It also suggests that dynamic similarity is achieved to a first approximation; one deviation is that the speed at the wedge, which is less than that at the orifice, should be the characteristic speed. A weak negative Reynolds number effect is likely. The orifice width d also has some influence; it is related to vortex size and lateral correlation of the shedding process.
The presence of a dipole sound field and a periodic force proportional to formula_10 was confirmed by Powell. Numerical simulations of the edge tone and extensive references can be found in a NASA report. The lower figure on the right may be called a "wake edge tone". If the preferred frequencies of the trailing edge instability match the preferred frequencies of the free edge tone, a stronger dipole sound should arise. There does not appear to be any research on this configuration.
Shallow cavity tone.
The study of sound generated by flow over cavities at high speed has been well funded by the federal government so a considerable amount of effort has been made. The problem relates to flow over aircraft cavities in flight such as bomb bays or wheel wells. Flow over a cavity in a surface can result in excitation of a feedback loop and nearly pure tones. Unlike the edge tone noted above, the cavity edge is typically square, but also can be an edge as part of a thin structural shell. Cavities can be separated into "shallow" or "deep" ones, the difference being that for deep cavities a Class III (acoustical) feedback path may be controlling. Shallow cavities are addressed here and are those in which the cavity length L is greater than the cavity depth D.
At high speeds U, the flow is turbulent and in some studies the speed can be supersonic and the sound generated can be quite high level. One study has shown that several modes of oscillation (stages) can occur in a shallow cavity; the modes being related to the number of vortices in the distance L. For shorter cavities and lower Mach Numbers, there is a "shear-layer mode", while for longer cavities and higher Mach Numbers there is a "wake mode". The shear-layer mode is characterized well by the feedback process described by Rossiter. The wake mode is characterized instead by a large-scale vortex shedding with a Strouhal number independent of Mach number. There is an empirical equation for these data; it is called "Rossiter’s formula".
Lee and others have shown it in Strouhal number form as
formula_30
The bracketed term includes two feedback loop speeds; the downstream speed is the speed of the vortices u and the upstream speed is that of soundformula_10. The various modes are described by an integer n with an empirical delay constant β (near 0.25). The integer n is closely related to the number of vortices en route to the edge. It is clear from shadowgraphs that the fluctuating force near the downstream edge is the sound source. Since the Mach number of the flow can be appreciable, refraction makes it difficult to determine the major axis of the dipole-like sound field. The preferred frequencies in shallow cavities are different from those for the edge tone.
Police whistle.
The title above is used since it is commonly used to describe whistles similar to those used by police in America and elsewhere. There are a number of whistles that operate in the same manner as the police whistle and there are number of whistles that are used by police elsewhere that do not operate in the same manner as the police whistle. The London Metropolitan police use a linear whistle, more like a small recorder. Police whistles are commonly used by referees and umpires in sporting events.
The cross section of a common whistle is shown in the figure on the right. The cavity is a closed end cylinder (3/4 inch diameter), but with the cylinder axis lateral to the jet axis. The orifice is 1/16 inch wide and the sharp edge is 1/4 inch from the jet orifice. When blown weakly, the sound is mostly broad band with a weak tone. When blown more forcefully, a strong tone is established near 2800 Hz and adjacent bands are at least 20 dB down. If the whistle is blown yet more forcefully, the level of the tone increases and the frequency increases only slightly suggesting Class I hydrodynamic feedback and operation only in Stage I.
There does not appear to be any detailed research on police whistle operation. Considering the edge tone, noted above, one might expect several jumps in frequency, but none occur. This suggests that if multiple vortices exist in the unstable jet, they do not control.
The diagram on the right suggests a plausible explanation of whistle operation. Within the cavity is an off-center vortex. In the upper drawing, the vortex center is near the jet; the nearby cavity flow is slower and the pressure is less than atmospheric so the jet is directed into the cavity. When the jet moves toward the cavity an additional thrust is given to the interior vertical flow which then rotates around and back to the edge. At that point, the cavity flow and the local pressure are sufficient to force the jet to move away from the cavity.
An interior vortex of this type would explain why no frequency jumps occur. Since the excess fluid in the cavity must be discharged, the jet lateral movement must be considerably larger than that found in the edge tone; this is likely the reason for the high level sound. The flow over the edge results in an applied force and a dipole-like sound field. The characteristic speed must be U the jet exit speed. The characteristic dimension must be D, the cavity diameter.
The frequency of the sound is closely related to the rotation rate of the cavity vortex. With a frequency near 2800 Hz the interior rotation rate must be very high. It is likely that the Rossby number formula_32 would be a valuable dynamic similarity number. The Boatswain's pipe is similar to the police whistle except the cavity is spherical creating a more complex vortex.
Levavasseur whistle.
This whistle is essentially the police whistle turned into a torus, magnifying its sound making potential. A cross-section through the middle of the whistle is shown in the figure on the right.
An annular duct carries the fluid that creates the annular jet. The jet impinges on a sharp ended ring with two toroidal cavities on either side. In Levavasseur's patent, a structure is added downstream of the annular opening to act as a coupling horn to direct the sound. The sound generated is very intense. It appears that no scientific study has been done to elucidate the detailed feedback mechanisms of its operation, although it is clear that this whistle has Class I feedback mechanism, similar to the police whistle.
The characteristic speed U is that of the annular jet. The characteristic dimension D is the cavity diameter and it appears that both cavities have similar dimensions. Again, the Rossby number Vformula_32 is likely to be a relevant dynamic number, since the operation of the inner cavity must be similar to that in the police whistle. It is likely that the vortex in the outer cavity is in anti-phase with the inner cavity to amplify jet displacement and thus the sound output.
Screech tone.
Strong tones can occur in both rectangular and circular jets when the pressure ratio is greater than the critical and the flow becomes supersonic on exit, resulting in a sequence of repetitive shock cells. These cells can be seen in the exhaust of rockets or jets operating with an afterburner. As with subsonic jets, these flows can be unstable.
In a rectangular jet, the instability can show as asymmetric cell distortions. The asymmetry sends waves back to the nozzle which sets up a Class III feedback loop and a strong periodic dipole sound field; it is called "screech tone". Powell first described the phenomenon and because of application to military aircraft and potential structural fatigue, much subsequent work has been done. The sound field is sufficiently intense for it to appear on a shadowgraph as shown in the figure on the right (from M.G. Davies) for a rectangular supersonic jet. The dipole nature of the source is clear by the phase reversal on either side of the jet. There is lateral motion of the shock cells that gives the dipole its axis.
Supersonic flows can be quite complex and some tentative explanations are available. As with hole and ring tones, these jets can be sensitive to local sound reflecting surfaces.
The characteristic speed, U, is that in the exit plane, and the characteristic dimension L is the nozzle width, to which the cell dimensions are proportional. Circular supersonic jets also generate screech tones. In this case, however, there can be three "modes" of motion: symmetric (toroidal), asymmetric (sinuous), and helical. These whistles are unlike the others listed above; the sound is generated without interaction with a solid; it is truly an aerodynamic whistle.
Fluidic oscillators.
These devices are whistles that do not radiate sound, but are still aerodynamic whistles.
The upper figure on the right shows the basic arrangement of one version of the device. The circle on the left is the fluid source (air or liquid). A jet is formed that either goes into the upper or lower channel.
The black lines are the feedback paths. If the fluid is in the lower channel, some fluid is fed back to the jet origin via the black tube and pushes the jet to the upper channel.
There has been considerable development of these devices from circuit switches that are immune to electromagnetic pulses to more modern uses.
One uniqueness of this whistle compared to the others described is that the length of the feedback path can be chosen arbitrarily. Although the channels are divided by a wedge shape, edge tone operation is avoided by the Coanda effect. The second figure on the right shows results from one study indicating a constant Strouhal number with Reynolds number. The data had been normalized to a reference value.
In another study one set of their frequency data was recalculated in terms of Strouhal number and it was found to rise slowly and then be constant over a range of flow rates. Kim found a similar result; the Strouhal number increased with Reynolds number and then stayed constant as shown in the lower figure on the right. Another uniqueness of this whistle is that the feedback is sufficiently strong that the jet is bodily diverted instead of depending on flow instability vortex development to control it. The geometry of the device suggests that it is essentially a dipole source that operates in Stage I with Class I (hydrodynamic) feedback.
Monopole-dipole whistles.
There are a number of whistles that possess the characteristics of both monopole and dipole sound sources. In several of the whistles described below, the driving source is dipole (generally an edge tone) and the responding source is a monopole (generally a tube or cavity in proximity to the dipole).
The fundamental difference of these whistles from those described above is that there are now two sets of characteristic variables. For the driving source, the characteristic speed is U, and the characteristic dimension is L1. For the responding source, the characteristic speed is formula_10 and the characteristic dimension is L2, typically the corrected cavity depth or tube length. The non dimensional descriptors for each of these are the "fluid mechanical" Strouhal number and the "acoustical" Strouhal number. The tie between these two numbers is the commonality of the frequency.
Jug whistle.
Blowing over the edge of a jug or bottle can create a nearly pure tone of low frequency. The driving force is the flow over the jug edge so one might expect an edge tone dipole sound field. In this case, The curvature and roundness of the edge makes a strong edge tone unlikely. Any periodicity at the edge is likely submerged in the Class III feedback from the jug volume. The unsteady edge flow sets up a classical Helmholtz resonator response in which the interior geometry and the jug neck determines the resultant frequency. A resonance equation is shown below.
formula_35
It is a transcendental equation where Ac is the cross sectional area of a cylindrical cavity of depth L. Ao is the area of the circular orifice of depth Lo, δe is the exterior end correction, δi is the interior end correction, and kL is the Helmholtz number (acoustical Strouhal Number with formula_4 added). A cylindrical cavity nine inches deep and 4.25 inches in diameter was connected to a circular orifice 1.375 inches in diameter and 1.375 inches deep. The measured frequency was close to 140 Hz. If the cavity acted as a one-quarter wavelength resonator, the frequency would have been 377 Hz; clearly not a longitudinal resonance.
The equation above indicated 146 Hz and the Nielsen equation indicated 138 Hz. Clearly, the whistle was being driven by a cavity resonance. This is an example of a whistle being driven in edge tone fashion but the result is a monopole sound field.
Deep cavity tone.
Flow over a cavity that is considered "deep" can create a whistle similar to that over shallow cavities. "Deep" is generally distinguished from "shallow" by the cavity depth being greater than the width. There are two geometries that have been studied. The first geometry is flow exterior to the cavity such as on an aircraft.
There are two characteristic dimensions (cavity width L associated with vortex development and cavity depth D associated with acoustical response). There are two characteristic speeds (flow speed U associated with vortex development and sound speed formula_10 associated with cavity response). It was found that the feedback was Class III and the Strouhal Numbers ranging from 0.3 to 0.4 were associated with a single vortex pattern (Stage I) across the gap while Strouhal numbers ranging from 0.6 to 0.9 were associated with two vortices across the gap (Stage II).
The second geometry is flow in a duct with a side branch. Selamet and his colleagues have made extensive studies of whistle phenomena in ducts with side branches that are closed at one end. For these studies, The cavity depth was L and D was the side branch diameter. The "fluid mechanical" Strouhal and "acoustical" Strouhal numbers were
formula_38
An arbitrary constant β was used to represent the impedance at the junction of the side branch with the duct. n was an integer representing the stage number. They noted that the Strouhal number remained constant with increase of speed.
Pipe organ.
The pipe organ is another example of a potentially dipole sound source being driven as a monopole source. An air jet is directed at a sharp edge setting up flow oscillations as in the edge tone. The edge is part of a generally cylindrical tube of length L. An example is shown in the figure on the right. The unstable jet drives fluid alternately into the tube and out. The streamlines clearly are distorted from those of the free edge tone. There is a stagnation point opposite the source. The dashed lines, colored in red, are those most strongly modified. The red streamlines in the tube are now augmented by the oscillatory flow in the tube, a superposition of resistive and reactive dipole flow and resistive acoustic flow.
The tube length determines whether the tube acoustic pressure or velocity is the dominant influence on the frequency of the tube. Simple models of organ pipe resonance is based on open-open pipe resonance (formula_39 but corrections must be made to take into account that one end of the pipe radiates into the surrounding medium and the other radiates through a slit with a jet flow. Boelkes and Hoffmann have made measurements of end correction for open-open tubes and derived the relation δ=0.33D. This cannot be exact since the driving end is not open.
The radiation ±impedance at the driving end should move the tube toward aformula_40 condition, further lowering the frequency. Since there are two coupled systems, so there are two characteristics scales. For the pipe component, the characteristic dimension is L and the characteristic speed is formula_10. For the edge tone component, the characteristic dimension is the orifice to edge distance h and the characteristic speed is that of the jet U. It would seem that the maximum oscillatory gain of the system would occur when the preferred pipe frequency matches the preferred edge tone frequency with suitable phase. This relationship expressed in terms of Strouhal Numbers is:
formula_42
If dynamic similarity holds for both resonances, the latter equation suggests how organ pipes are scaled. The apparent simplicity of the equation hides important variable factors such as the effective pipe length formula_43 where δ1 is correction for the open end and δ2 is the correction for the end near the jet. The jet disturbance (vortex) speed from orifice to edge will vary with mean speed U, edge distance h, and slit width d as suggested in the Edge Tone section.
The Strouhal relationship suggests that the jet Mach number and the ratio of effective pipe length to the edge distance are important in a first approximation. Normal pipe operation would be a monopole sound source in Stage I with Class III feedback.
Flutes, recorders and piccolos.
A number of musical instruments, other than the pipe organ, are based on the edge tone phenomenon, the most common of which are the flute, the piccolo (a small version of the flute), and the recorder. The flute can be blown lateral to the instrument or at the end as the other ones are. A native end blown flute is shown in the figure.
They are all subject to frequency jumps when overblown, suggesting the dipole-monopole relationship. The monopole aspects are relatively fixed. The characteristic dimension of the tube, L2, is fixed; the characteristic speed,formula_10, is fixed. The effective length of the tube is fixed since the radiation impedances at each end are fixed. Unlike the pipe organ, however, these instruments have side ports to change the resonance frequency and thus the acoustical Strouhal number.
The dipole aspects are also relatively fixed. The jet orifice dimension and the distance to the edge, h is fixed. Although the jet speed U can vary, the fluid mechanical Strouhal number is relatively constant and normally operates in Stage I. When there is phase coherent gain of the two aspects, they operate as Class III monopole sources. The efficiency of the monopole radiation is considerably greater than that of the dipole so the dipole pattern is noticed, The details of system gain and interaction between these two dynamic systems is yet to be fully uncovered. It is a testimony to the skills of early instrument makers that they were able to achieve the right port sizes and positions for a given note without scientific measurement instruments.
Singing Sand.
For many years, people have heard tones from the motion of sand, mostly on sand dunes. In many cases, the tune was akin to a whistle in that it could be at a single frequency. In most cases, the wind over the peak of a dune sets the sand on the dune into motion, and it is that motion that generates the sound.
In the recent past, a number of researchers have attempted to explain the origin of the sound. One might consider this source an aerodynamic whistle since it is wind caused and one proposal is that the sand moves as an unstable fluidized sand/air mixture.
There are several other theories, but the aerodynamic whistle possibility is not correct since it can occur by simple mechanical motion of the sand. It is likely a stick/slip mechanism which would be a dipole like source rather than a volumetric (monopole) oscillation of the sand particles.

</doc>
<doc id="42388" url="https://en.wikipedia.org/wiki?curid=42388" title="History of Bermuda">
History of Bermuda

Initial discovery.
Bermuda was discovered by Juan de Bermudez in 1505.
The island is shown as "La Bermuda" in Peter Martyr's "Legatio Babylonica" (1511). Bermudez returned again in 1515, with the chronicler Oviedo y Valdés. Oviedo's account of the second visit (published in 1526) records that they made no attempt to land because of weather.
In 1609, Sir George Somers set sail aboard the "Sea Venture," the new flagship of the Virginia Company, leading a fleet of nine vessels, loaded with provisions and settlers for the new English colony of Jamestown, in Virginia. The fleet was caught in a storm, and the Sea Venture was separated and began to flounder. When the reefs to the East of Bermuda were spotted, the ship was deliberately driven on them to prevent its sinking, thereby saving all aboard (150 sailors and settlers, and one dog). The survivors spent ten months on Bermuda. Several were lost-at-sea when the Sea Venture's longboat was rigged with a mast and sent in search of Jamestown. Neither it nor its crew were ever seen again. The remainder built two new ships: the Deliverance, largely from the material stripped from the Sea Venture (which sat high-and-dry on the reef, and was still being cannibalised in 1612 – its guns were used to arm a fort) and the Patience. The latter was made necessary by the food stores the survivors had begun to collect and stockpile in Bermuda, and which could not be accommodated aboard the Deliverance. It was built almost entirely from material sourced on the islands. When the two new vessels were complete, most of the survivors set sail, completing their journey to Jamestown.
They arrived there only to find the colony's population almost annihilated by the Starving Time, which had left only 60 survivors out of the 500 who had preceded them, and most of these survivors were sick or dying. The food the Sea Venture survivors brought with them was woefully insufficient, and the colony seemed unviable. It was decided to abandon it, and to return everyone to England. Loaded aboard the two ships, they were prevented from making this evacuation by the timely arrival of another relief fleet, bearing Governor Lord De La Warre, among others. The Sea Venture survivors had brought pork from the pigs that had been found wild on the island, which had presumably been left by previous visitors. This led the Jamestown colonists to refer to "Bermuda Hogs" as a form of currency. Somers returned to Bermuda with the Patience to obtain more food supplies, but died there from a "surfeit of pork". The Patience, captained by his nephew, Matthew Somers, returned to England, instead of Virginia. Somers left three volunteers – Carter, Chard and Waters – behind on Bermuda (two when the Deliverance and Patience had departed, and the third following the Patience's return) to maintain the claim of the island for the England, leaving the Virginia Company in possession of the island. As a result, Bermuda has been continuously inhabited since the wrecking of the Sea Venture, and claims its origin from that date, and not the official settlement of 1612.
Returning to Somers' hometown of Lyme Regis, in Dorset, his body (which had been pickled in a barrel) was landed via "The Cobb", the notable breakwater which protects town's harbour. His heart, however, was left buried on what would subsequently also be known as "The Somers Isles". After reaching England, the reports of the survivors of the "Sea Venture" aroused great interest about Bermuda. Accounts were published by two survivors, William Strachey and Sylvester Jordain. Two years later, in 1612, the Virginia Company's Royal Charter was officially extended to include the island, and a party of 60 settlers was sent, under the command of Sir Richard Moore, the island's first governor. Joining the three men left behind by the "Deliverance" and the "Patience" (who had taken up residence on Smith's Island), they founded and commenced construction of the town of St. George.
Bermuda struggled throughout the following seven decades to develop a viable economy. The Virginia Company, finding the colony unprofitable, briefly handed its administration to the Crown in 1614. The following year, 1615, King James I granted a charter to a new company, the Somers Isles Company, formed by the same shareholders, which ran the colony until it was dissolved in 1684 (The Virginia Company itself was dissolved after its charter was revoked in 1624). Representative government was introduced to Bermuda in 1620, when its House of Assembly held its first session, and it became a self-governing colony.
Early colony.
Bermuda was divided into nine equally-sized administrative areas. These comprised one public territory (today known as St. George's) and eight "tribes" (today known as "parishes"). These "tribes" were areas of land partitioned off to the "adventurers" (investors) of the Company – Devonshire, Hamilton, Paget, Pembroke, Sandys, Smith's, Southampton and Warwick (thus far, this usage of the word "tribes" is unique to the Bermuda example).
Initially, the colony grew tobacco as its only crop. The Company repeatedly advised more variety, not only because of the risks involved in a single-crop economy, but also because the Bermuda-grown tobacco was of particularly low quality (the Company was frequently forced to burn the supply that arrived back in England). It would take Bermuda some time to move away from this, especially as tobacco was the main form of currency.
Agriculture was not a profitable business for Bermudians in any case. The land area under cultivation was so small (especially by comparison to the plots granted settlers in Virginia), that fields could not be allowed to lie fallow, and farmers attempted to produce three crops each year. Islanders quickly turned to shipbuilding and maritime trades, but the Company, which gained its profits only from the land under cultivation, forbade the construction of any vessels without its license. Its interference in Bermudians livelihood would lead to its dissolution in 1684.
Indentured Servitude and Slavery in 17th Century Bermuda.
The first slaves were brought to Bermuda soon after the colony was established. Despite this, Bermuda's 17th Century agricultural economy did not become dependent on slavery. Unlike the plantation economies that developed in English colonies in the southeast region of North America and in the West Indies, the system of indentured servitude, which lasted in Bermuda until 1684, ensured a large supply of cheap labour. As a result, Bermuda's white Anglo-Saxon population remained the majority into the 18th Century despite a continuous influx of Latin American and African Blacks, Native Americans, Irish and Scots. The first Blacks to come to Bermuda in numbers were free West Indians, who emigrated from territories taken from Spain. They worked under seven years indenture, as did most English settlers, to repay the Company for the cost of their transport. As the size of the Black population grew, however, many attempts were made to reduce it. The terms of indenture for Blacks were successively raised to 99 years. Many of the Black slaves brought to Bermuda arrived as part of the cargoes seized by Bermudian privateers.
Slaves could be obtained by sale or purchase, auction debt, legal seizure or by gift. The price of a slave depended on demand. Throughout the 17th century Black children sold for £8, women from £10 to £20, and able bodied Black and Indian men for around £26. Blacks and Indians never willingly accepted their status as slaves and seized any available opportunity to escape or rebel. It was not easy to escape because of the size of the island and the nearest land being more than away, but still slaves ran off from their masters and hid in the caves along Bermuda's coast. Others sought to plot against their masters. One such plot occurred in 1656 when a dozen Black men, led by William Force, a free Black man plotted to murder their English masters. As the appointed night arrived for the uprising, two of the slaves lost their nerve and reported the conspiracy to authorities. The conspirators were rounded up and tried by court martial. Two were hung and Force was later sent to the Bahamas with most of the island's other free blacks. In 1673 15 Blacks conspired to kill their masters, Again, one of the conspirators lost his nerve and reported the conspiracy. He was granted his freedom, five were branded, had their noses slit, and were whipped before being executed. The other conspirators were branded and whipped. This conspiracy resulted in the passage, in 1674, of more stringent laws effecting a slave's freedom of movement. A slave found off his estate without a ticket from his owner could be beaten with a rod or whip. A second offense would result in an ear being cut off. Offending for a third time resulted in being whipped until the skin was broken and being branded.
The local government attempted to legislate the emigration of free blacks, and during times of war, with food supplies scarce, it was considered patriotic to export horses and slaves. The first two slaves brought into the Island, a Black and a Native American, had been sought for their skills in pearl diving, but Bermuda proved to have no pearls. Slaves were also brought directly from Africa, and in large numbers from North America, especially from New England, where various Algonquian peoples were falling victim to English expansion. Native American slaves were brought in large numbers possibly from as far as Mexico. Native American slaves were reportedly preferred as house servants as they proved less troublesome than the black slaves and Irish laborers, who were constantly fomenting rebellion.
Bermuda had actually tended towards the Royalist side in the English Civil War, but largely escaped the effects of the conflict, and the aftermath of the Parliamentary forces' victory. However, in the 1650s, following Cromwell's adventures in Ireland, and his attempt to force his protectorship on independent Scotland, Irish prisoners of war and civilians, and smaller numbers of Scottish prisoners, were also sent to Bermuda.
The English overlords of Ireland had used forced emigration as a way of pacifying Ireland since the start of the century, but the Cromwellian invasion gave vent to a holocaust in Ireland. Between 1641 and 1652, over 550,000 Irish were killed by the English. 300,000 were forced into indentured servitude, and the Irish population of Ireland fell from 1,466,000 to 616,000. Between 1652 and 1659, another 50,000 Irish men, women and children were sent to the West Indies, Virginia and Bermuda.
After the uncovering of a coup-plot by Irish laborers and Black slaves, however, further emigration of Irish servants was banned. The slave trade would be outlawed in Bermuda in 1807, and all slaves were freed in 1834. At the end of the 17th century, whites composed the majority of Bermuda's population. Blacks and Native Americans were both small minorities. They combined, however, absorbing the Irish and Scots, and no small part of the White English bloodline, to be described as a single demographic group a century later, with the Bermuda's population being divided into White and Black Bermudians. As 10,000 Bermudians had emigrated, prior to American independence, most of them White, this left Blacks with a slight majority. Portuguese immigration, which began with a shipload of Madeiran families in the 1840s has been offset by sustained immigration from the West Indies which began at the end of the 19th century. Today, about 60% of Bermudians are described as being of African descent, although many may have greater European ancestry, and almost all Bermudians would be able to easily find ancestors and relatives of either African or European descent.
As Bermuda's primary industry became maritime, following the 1684 removal of the impediments placed by the Somers Isles Company, most Bermudian slaves worked in shipbuilding and seafaring, or, in the case of the most unfortunate, in raking salt in the Turks Islands.
Bermuda, salt and the Turks Islands.
After the elimination of their indigenous population by Spanish slavers, the "Turks Islands", or "Salt Islands", were not fully colonised until 1681, when salt collectors from Bermuda built the first permanent settlement on Grand Turk Island. The salt collectors were drawn by the shallow waters around the islands that made salt mining a much easier process than in Bermuda. They occupied the Turks only seasonally, for six months a year, however, returning to Bermuda when it was no longer viable to rake salt. Their colonization established the English (subsequently, "British") dominance of the archipelago that has lasted to the present day. The Bermudians destroyed the local habitat in order to develop the salt industry that became the central pillar of Bermuda's economy, felling huge numbers of trees to discourage rainfall that would adversely affect their operation. This deforestation, a foretaste of the deforestation of Bermuda by shipbuilding a century before the cedar blight, has yet to be repaired. Most of the salt mined in the Turks and Caicos Islands was sold through Bermudian merchant houses on the American seaboard, including in New England and Newfoundland where it was used for preserving cod. Bermudian vessels carried salted cod on their returns to Bermuda, establishing it as a traditional part of the Bermudian diet (at least on Sundays).
Bermuda spent much of the 18th century in a protracted legal battle with the Bahamas (which had itself been colonised by Bermudians in 1647) over the Turks Islands. Under British law, no colony could hold colonies of its own. The Turks Islands were not recognised by Britain either as a colony in its own right, or as a part of Bermuda. They were held to be, like rivers in Britain, for the common use. As a result, there was a great deal of political turmoil surrounding the ownership of the Turks (and Caicos).
When the Bermudian sloop "Seaflower" was seized by the Bahamians in 1701, the response of Bermuda Governor Bennett was to issue letters of marque to Bermudian Privateers. In 1706, Spanish and French forces ousted the Bermudians, but were driven out themselves three years later by a Bermudian privateer under the command of Captain Lewis Middleton in what was probably Bermuda's only independent military operation. His ship, the "Rose", attacked a Spanish and a French privateer holding a captive English vessel. Defeating the two enemy vessels, the "Rose" then cleared out the thirty-man garrison left by the Spanish and French.
The struggle with the Bahamas began in earnest in 1766, when the King's representative in the Bahamas, Mr Symmer, on his own authority, wrote a constitution which legislated for and taxed the Bermudians on the Turks. The Secretary of State, Lord Hillsborough, for the Crown, issued orders that the Bermudian activities on the Turks should not be obstructed or restrained in any way. As a result of this order, Symmer's constitution was dissolved. The Bermudians on the Turks appointed commissioners to govern themselves, with the assent of the King's local agent. They drew up regulations for good government, but the Bahamian governor, William Shirley, drew up his own regulations for the Turks and ordered that no one might work at salt raking who had not signed assent to his regulations.
Following this, a raker was arrested and the salt pans were seized and divided by force. The Bahamas government attempted to appoint judicial authorities for the Turks in 1768, but these were refused by the Bermudians. In 1773 the Bahamian government passed an act attempting to tax the salt produced in the Turks, but the Bermudians refused to pay it. In 1774, the Bahamians passed another, similar act, and this they submitted for the Crown's assent. The Crown passed this act on to the Bermudian government which objected to it, and which rejected Bahamian jurisdiction over the Turks. The Crown, as a consequence, refused assent of the Act as applied to include the Turks, and, in the form in which it finally passed, the Bahamas, but not the Turks, were included.
The Bermudians on the Turks continued to be governed under their own regulations, with the assent of the royal agent, until 1780, when a more formal version of those regulations was submitted for the assent of the Crown, which was given. Those regulations, issued as a royal order, stated that all British subjects had the right ("free liberty") to rake and gather salt on the Turks, providing that they conformed to the regulations, which expressly rejected Bahamian jurisdiction over the Turks. Despite this refutation by a higher authority of their right to impinge upon Bermudian activities on the Turks, the Bahamian government continued to harass the Bermudians (unsurprisingly, given the lucrativeness of the Turks salt trade).
Although the salt industry on the Turks had largely been a Bermudian preserve, it had been seen throughout the 17th century as the right of all British subjects to rake there, and small numbers of Bahamians had been involved. In 1783, the French had landed a force on Grand Turk which a British force of 100 men, under then-Captain Horatio Nelson, had been unable to dislodge, but which was soon withdrawn.
Following this, the Bahamians were slow to return to the Turks, while the Bermudians quickly resumed salt production, sending sixty to seventy-five ships to the Turks each year, during the six months that salt could be raked. Nearly a thousand Bermudians spent part of the year on the Turks engaged in salt production, and the industry became more productive.
The Bahamas, meanwhile, was incurring considerable expense in absorbing loyalist refugees from the now-independent American colonies, and returned to the idea of taxing Turks salt for the needed funds. The Bahamian government ordered that all ships bound for the Turk Islands obtain a license at Nassau first. The Bermudians refused to do this. Following this, Bahamian authorities seized the Bermuda sloops "Friendship" and "Fanny" in 1786. Shortly after, three Bermudian vessels were seized at Grand Caicos, with $35,000 worth of goods salvaged from a French ship. French privateers were becoming a menace to Bermudian operations in the area, at the time, but the Bahamians were their primary concern.
The Bahamian government re-introduced a tax on salt from the Turks, annexed them to the Bahamas, and created a seat in the Bahamian parliament to represent them. The Bermudians refused these efforts also, but the continual pressure from the Bahamians had a negative effect on the salt industry. In 1806, the Bermudian customs authorities went some way toward acknowledging the Bahamian annexation when it ceased to allow free exchange between the Turks and Bermuda (this affected many enslaved Bermudians, who, like the free ones, had occupied the Turks only seasonally, returning to their homes in Bermuda after the year's raking had finished).
That same year, French privateers attacked the Turks, burning ships and absconding with a large sloop. The Bahamians refused to help, and the Admiralty in Jamaica claimed the Turks were beyond its jurisdiction. Two hurricanes, the first in August 1813, the second in October 1815, destroyed more than two hundred buildings and significant salt stores; and sank many vessels. By 1815, the United States, the primary client for Turks salt, had been at war with Britain (and hence Bermuda) for three years, and had established other sources of salt.
With the destruction wrought by the storm, and the loss of market, many Bermudians abandoned the Turks, and those remaining were so distraught that they welcomed the visit of the Bahamian governor in 1819. The British government eventually assigned political control to the Bahamas, which the Turks and Caicos remained a part of until the 1840s.
One Bermudian salt raker, Mary Prince, however, was to leave a scathing record of Bermuda's activities there in "The History of Mary Prince", a book which helped to propel the abolitionist cause to the 1834 emancipation of slaves throughout the Empire.
Shipbuilding and the maritime economy.
Due to the islands' isolation, for many years Bermuda remained an outpost of 17th-century British civilization, with an economy based on the use of the islands' Bermuda cedar ("Juniperus bermudiana") trees for shipbuilding, and Bermudians' control of the Turks Islands, and their salt trade. Especially as its control of the Turks became threatened, Bermuda's mariners also diversified their trade to include activities such as whaling and privateering.
Privateering.
Bermudians turned from their failed agricultural economy to the sea after the 1684 dissolution of the Somers Isles Company. With a total landmass of , and lacking any natural resources, other than the Bermuda cedar, the colonists applied themselves fully to the maritime trades, developing the speedy Bermuda sloop, which was well suited both to commerce and to commerce raiding. Bermudian merchant vessels turned to privateering at every opportunity, during the 18th Century, preying on the shipping of Spain, France and other nations during a series of wars. They typically left Bermuda with very large crews. This advantage in manpower was vital in seizing larger vessels, which themselves often lacked enough crewmembers to put up a strong defence. The extra crew men were also useful as prize crews for returning captured vessels. Despite close links to the American colonies (and the material aid provided the continental rebels in the form of a hundred barrels of stolen gunpowder), Bermudian privateers turned as aggressively on American shipping during the American War of Independence. An American naval captain, ordered to take his ship out of Boston Harbour to eliminate a pair of Bermudian privateering vessels, which had been picking off vessels missed by the Royal Navy, returned frustrated, saying "the Bermudians sailed their ships two feet for every one of ours". The only attack on Bermuda during the war was carried out by two sloops captained by a pair of Bermudian-born brothers (they damaged a fort and spiked its guns before retreating). It greatly surprised the Americans to discover that the crews of Bermudian privateers included Black slaves, as, with limited manpower, Bermuda had legislated that a part of all Bermudian crews must be made up of Blacks. In fact, when the Bermudian privateer "Regulator" was captured, virtually all of her crew were found to be Black slaves. Authorities in Boston offered these men their freedom, but all 70 elected to be treated as Prisoners of War. Sent to New York on the sloop "Duxbury", they seized the vessel and sailed it back to Bermuda. The American War of 1812 was to be the encore of Bermudian privateering, which had died out after the 1790s, due partly to the buildup of the naval base in Bermuda, which reduced the Admiralty's reliance on privateers in the western Atlantic, and partly to successful American legal suits, and claims for damages pressed against British privateers, a large portion of which were aimed squarely at the Bermudians. During the course of the American War of 1812, Bermudian privateers were to capture 298 ships (the total captures by "all" British naval and privateering vessels between the Great Lakes and the West Indies was 1,593 vessels).
Bermuda and the American War of Independence.
American independence was to lead to tremendous changes for Bermuda. Prior to the war, with no useful landmass or natural resources, Bermuda was largely ignored and left to its own devices by the London government.
This ensured that the guiding hands on shaping the colony's society and economy were Bermudian ones. Although the British Government retained theoretical control via the appointed Governor, the real power in Bermuda remained with the wealthy Bermudian merchant families who dominated the economy, and filled the benches of the House of Assembly and the Privy Council, with the President of the Privy Council being undoubtedly the Bermudian with the greatest political power.
The same lack of economic opportunities within Bermuda had led islanders to abandon agriculture following the dissolution of the Somers Isles Company in 1684, and turn wholeheartedly towards maritime activities. Bermuda played key roles in settling the New World, especially the southern colonies of what would become the USA. Bermudian merchant families established branches in ports on the American Atlantic Seaboard, and used their social networks, merchant fleet and their control of the salt trade (with resulted from de facto Bermudian control of the Turks Islands) to achieve a leading position in the merchant trade through those ports.
Bermudians diversified their interests widely, wherever they could reach by the sea. Involved in logging in Central America, merchant shipping between the North American colonies and the West Indies, fishing the Grand Banks ('til forbidden to by the Palliser's Act of 1775), whaling, and privateering.
As Bermuda's economy became wholly concerned with the sea, the colony became dependent on food imports from North America.
The primary enabler of the development of Bermuda's web of commerce across the Americas was the growth of the British Empire, and from the beginning its primary trading partners were the British colonies on the North American continent.
When thirteen of these colonies rebelled, entering a war of secession, the strong bonds of blood, commerce, and history meant that most Bermudians sympathised with the rebels. It is entirely probable that, if Bermuda had not been so remote from the continental coastline, and had the Royal Navy not enjoyed near supremacy on the ocean, Bermuda would have been the fourteenth colony to join the rebellion. As this was not possible, Bermudians initially assisted the colonists by selling them Bermuda sloops via neutral ports to use as privateers. The number is unclear, but seems to have been very many, with British authorities reporting up to a thousand, although this number is clearly impossible given other sources state the number of ships built in Bermuda during the entire century numbered a thousand. Some historians state that the Bermudian-built privateers played a decisive role in the Americans achieving independence. With trade between the rebelling colonies and the rest of the Empire banned by both sides, Bermudians were faced with the threat of starvation, as well as the destruction of their trade. The Americans were dependent on Bermuda for salt, which the islanders offered the rebels in exchange for food. The Americans insisted on receiving gunpowder. Benjamin Franklin and Henry Tucker Sr. (a colonel of the Bermuda Militia, and a former President of the Privy Council, whose son, Henry Tucker, was then President of the Privy Council and son-in-law to Governor Breure, and whose other two sons were a colonel in the Virginia Militia and a politician in the rebel administration), orchestrated the theft of a hundred barrels of gunpowder from a magazine in St. George's, which was supplied to the Americans. Following this, the Continental Congress authorised trade with Bermuda (although this trade remained illegal in Bermuda).
As the war progressed, with no hope of joining the rebellion due to power of the Royal Navy (in the letter he had addressed to Bermudians soliciting the theft of the gunpowder, George Washington had written "We would not wish to in volve you in an Opposition, in which from your Situation, we should be unable to support you: – We knew not therefore to what Extent to sollicit your Assistance in availing ourselves of this Supply"), with increasing numbers of American loyalists in Bermuda (such as the privateer Bridger Goodrich), and with their economic opportunities dwindling, Bermudians overcame their sympathies for their erstwhile countrymen and unleashed their privateers (which, by the middle of the 18th Century already outnumbered those of any of the mainland colonies) upon American shipping. The Bermudian effectiveness was such that, when the US sued British privateers for wrongful seizures in British courts, following the war, a sizable part of the damages they were awarded were to have come from Bermudians, like Hezekiah Frith (although, with the local authorities tasked with collecting these damages being in sympathy with the defendants, most of these damages were never paid).
The fallout of the war was that Britain lost all of its continental naval bases between the Maritimes and Spanish Florida, ultimately the West Indies. This launched Bermuda into a new prominence with the London Government, as its location, near the halfway point from Nova Scotia to the Caribbean, and off the US Atlantic Seaboard, allowed the Royal Navy to operate fully in the area, protecting British trade routes, and potentially commanding the American Atlantic coast in the event of war. The value of Bermuda in the hands of, or serving as a base for, enemies of the United States was shown by the roles it played in the American War of 1812 and the American Civil War. The blockade of the Atlantic ports by the Royal Navy throughout the first war (described in the USA as the "Second War of Independence") was orchestrated from Bermuda, and the task force that burned Washington DC in 1814 was launched from the colony. During the latter war, Confederate blockade runners delivered European munitions into Southern harbours from Bermuda, smuggling cotton in the reverse direction.
Consequently, the very features that made Bermuda such a prized base for the Royal Navy (its headquarters in the North Atlantic and West Indies 'til after the Second World War, also meant it was perpetually threatened by US invasion, as the US would have liked to both deny the base to an enemy, and use it as a way to extend its defences hundreds of miles out to sea, which would not happen 'til the Second World War.
As a result of the large regular army garrison established to protect the naval facilities, Bermuda's parliament allowed the Bermudian militia to become defunct after the end of the American war in 1815. More profound changes took place, however. The post American independence buildup of Royal Navy facilities in Bermuda meant the Admiralty placed less reliance on Bermudian privateers in the area. Combined with the effects of the American lawsuits, this meant the activity died out in Bermuda until a brief resurgence during the American War of 1812. With the American continental ports having become foreign territory, the Bermudian merchant shipping trade was seriously injured. During the course of American War of 1812, the Americans had developed other sources for salt, and Bermudians salt trade fell upon hard times. Control of the Turks Islands ultimately passed into the hands of Bermuda's sworn enemy, the Bahamas, in 1819. The shipbuilding industry had caused the deforestation of Bermuda's cedar by the start of the 19th Century. As ships became larger, increasingly were built from metal, and with the advent of steam power, and with the vastly reduced opportunities Bermudians found for commerce due to US independence and the greater control exerted over their economies by developing territories, Bermuda's shipbuilding industry and maritime trades were slowly strangled.
The chief leg of the Bermudian economy became defence infrastructure. Even after tourism began in the later 19th Century, Bermuda remained, in the eyes of London, a base more than a colony, and this led to a change in the political dynamics within Bermuda as its political and economic ties to Britain were strengthened, and its independence on the world stage was diminished. By the end of the 19th Century, except for the presence of the naval and military facilities, Bermuda was thought of by non-Bermudians and Bermudians alike as a quiet, rustic backwater, completely at odds with the role it had played in the development of the English-speaking Atlantic world, a change that had begun with American independence.
Naval and military base.
Following the loss of Britain's ports in thirteen of its former continental colonies, Bermuda was also used as a stop-over point between Canada and Britain's Caribbean possessions, and assumed a new strategic prominence for the Royal Navy. Hamilton, a centrally located port founded in 1790, became the seat of government in 1815. This was partly resultant from the Royal Navy having invested twelve years, following American independence, in charting Bermuda's reefs. It did this in order to locate the deepwater channel by which shipping might reach the islands in, and at the West of, the Great Sound, which it had begun acquiring with a view to building a naval base. However, that channel also gave access to Hamilton Harbour.
With the buildup of the Royal Naval establishment in the first decades of the 19th century, a large number of military fortifications and batteries were constructed, and the numbers of regular infantry, artillery, and support units that composed the British Army garrison were steadily increased. The investment into military infrastructure by the War Office proved unsustainable, and poorly thought-out, with far too few artillery men available to man the hundreds of guns emplaced. Many of the forts were abandoned, or removed from use, soon after construction. Following the Crimean War, the trend was towards reducing military garrisons in colonies like Bermuda, partly for economic reasons, and partly as it became recognised that the Royal Navy's own ships could provide a better defence for the Dockyard, and Bermuda. Still, the important strategic location of Bermuda meant that the withdrawal, which began, at least in intent, in the 1870s, was carried out very slowly over several decades, continuing until after World War I. The last Regular Army units were not withdrawn until the Dockyard itself closed in the 1950s. In the 1860s, however, the major build-up of naval and military infrastructure brought vital money into Bermuda at a time when its traditional maritime industries were giving way under the assault of steel hulls and steam propulsion. The American Civil War, also, briefly, provided a shot-in-the-arm to the local economy. Tourism and agricultural industries would develop in the latter half of the 19th century. However, it was defence infrastructure that formed the central platform of the economy into the 20th century.
Anglo-Boer War.
During the Anglo-Boer War (1899–1902), Bermuda received and housed a total of 5,000 Boer prisoners of war on five of its islands. They were placed related to their views and authorities' assessment of risk. "Bitterenders" (Afrikaans: "Bittereinders"), men who refused to pledge allegiance to the British Crown, were interned on Darrell's Island and closely guarded. Other islands were allowed to be nearly self-governing: Morgan's Island held 884 men, including 27 officers; Tucker's Island held 809 Boer prisoners, Burt's Island had 607, and Port's Island held 35.
In June 1901, "The New York Times" reported an attempted mutiny by 900 Boer prisoners of war en route to Bermuda on the "Armenian", noting it was suppressed. It described the preparation of the camps for the men and said that martial law would hold on Darrell's Island. Several escapes happened soon after their arrival. A young Boer soldier escaped from Darrell's Island soon after arrival, reached the main docks, and stowed away on the steamship "Trinidad", arriving in New York 9 July. He hoped to be allowed to stay in the US. Three prisoners of war escaped on 10 July from Darrell's Island to mainland Bermuda.
Tourism.
Tourism in Bermuda first developed in Victorian times, catering to a wealthy elite seeking to escape North American winters. Many also came hoping to find young aristocrats among the officers of the Garrison and Naval base to whom they might marry their daughters. Local hoteliers were quick to exploit this, and organised many dances and gatherings during the 'season', to which military and naval officers were given a blanket invitation.
Due historically to a third of Bermuda's manpower being at sea at any one time, and to many of those seamen ultimately settling elsewhere, especially as the Bermudian maritime industry began to suffer, the colony was noted for having a high proportion of unmarried women well into the 20th century. Many Bermudian women had traditionally wed naval or military officers. With the arrival of tourism, young local women had to compete with American girls. Most Bermudian women who married officers left Bermuda when their husbands were stationed elsewhere. Enlisted men married Bermudians, and many of those remained in Bermuda when they left the Army.
In the early 20th century, as modern transportation and communication systems developed, Bermuda's tourism industry began to develop and thrive. The island became a popular destination for a broader spectrum of wealthy US, Canadian, and British tourists. In addition, the tariff enacted by the United States against its trading partners in 1930 cut off Bermuda's once-thriving agricultural export trade—primarily fresh vegetables to the US—spurring the island to put more effort into developing the tourism industry,
Imperial Airways and Pan-American World Airways both began flying to Bermuda in the 1930s (by which time the summer had become more important for tourists making briefer visits). It was not until after the Second World War, when the first airport for landplanes was built and the advent of the Jet Age, that tourism fully realised its potential.
World Wars.
Bermuda sent volunteer troops to fight in Europe with the British Army. They suffered severe losses.
During World War II, Bermuda's importance as a military base increased because of its location on the major trans-Atlantic shipping route. The Royal Naval dockyard on Ireland Island played a role similar to that it had during World War I, overseeing the formation of trans-Atlantic convoys composed of hundreds of ships. The military garrison, which included four local territorial units, maintained a guard against potential enemy attacks on the Island.
In 1941, the United States signed a lend-lease agreement with the United Kingdom, giving the British surplus U.S. Navy destroyers in exchange for 99-year lease rights to establish naval and air bases in certain British territories. Although not included in this trade, Winston Churchill granted the US similar 99-year leases "freely and without consideration" in both Bermuda and Newfoundland. (The commonly held belief that the Bermudian bases were part of the trade is not correct.) The advantage for Britain of granting these base rights was that the neutral US effectively took responsibility for the security of these territories, freeing British forces to be deployed to the sharper ends of the War. The terms of the base rights granted for Bermuda provided that the airfield constructed by the US would be used jointly with the Royal Air Force (RAF).
The Bermuda bases consisted of of land, largely reclaimed from the sea. The USAAF airfield, Fort Bell (later, US Air Force Base Kindley Field, and, later still, US Naval Air Station Bermuda) was on St. David's Island, while the Naval Operations Base, a Naval Air Station for maritime patrol flying boats, (which became the Naval Air Station Annex after US Naval air operations relocated to ) was at the western end of the island in the Great Sound. These joined two other air stations already operating on Bermuda, the pre-war civil airport on Darrell's Island, which had been taken over by the RAF, and the Fleet Air Arm's Royal Naval Air Station, HMS Malabar, on Boaz Island.
Recent history.
Bermuda has prospered economically since World War II, developing into a highly successful offshore financial centre. Although tourism remains important to Bermuda's economy, it has for three decades been second to international business in terms of economic importance to the island.
On 10 March 1973, the Governor of the island Sir Richard Sharples was assassinated along with his aide-de-camp. The Governor's dog was also killed. Erskine Burrows was found guilty of this assassination. His hanging, on 2 December 1977 was followed by three days of riots.
Though Bermuda has been classified as a self-governed colony since 1620, internal self-government was bolstered by the establishment of a formal constitution in 1968, and the introduction of universal adult suffrage; debate about independence has ensued, although a 1995 independence referendum was soundly defeated. For many, Bermudian independence would mean little other than the obligation to staff foreign missions and embassies around the world, which would be a heavy obligation for Bermuda's small population, and the loss of British passports (which could severely restrict travel, as few enough countries have even heard of little Bermuda, and could regard travellers with suspicion). Another concern, which raised its head during the 1991 Gulf War, was the loss of the protection provided by the Royal Navy, especially, to the large number of merchant vessels on Bermuda's shipping register. The Bermuda government is unlikely to be able to provide naval protection to oil tankers plying the Persian Gulf, or other potentially dangerous waters. At present, Bermuda is able to take advantage of its status as part of the United Kingdom to attract overseas shipping operators to its register, although it does not contribute to the navy's budget. With independence, it was feared, a large chunk of the money currently flowing into the Bermuda Government's coffers would disappear. The current government is promoting independence – by means of a general election (that is, the government of the day would have the power to decide whether to go independent or not) as opposed to a referendum (a direct vote by the people) – by establishing a committee to investigate (though the committee is notably staffed with party members, and without representation by the opposition party). This stance is being supported by the UN, who have sent delegations to the island claiming that Bermuda is being suppressed by the British.
Effective 1 September 1995, both US military bases were closed; British and Canadian bases on the island closed at about the same time. Unresolved issues concerning the 1995 withdrawal of US forces—primarily related to environmental factors—delayed the formal return of the base lands to the Government of Bermuda. The United States formally returned the base lands in 2002.
The island suffered major damage from Hurricane Fabian in 2003. It was also hit by Hurricane Bertha in July 2008 and Hurricane Gonzalo in October 2014.

</doc>
<doc id="42389" url="https://en.wikipedia.org/wiki?curid=42389" title="History of Bhutan">
History of Bhutan

Bhutan's early history is steeped in mythology and remains obscure. Some of the structures provide evidence that Bhutan as early as 2000 BC. According to a legend it was ruled or controlled by a Cooch-Behar king, Sangaldip, around the 7th century BC, but not much is known prior to the introduction of Tibetan Buddhism in the 9th century, when turmoil in Tibet forced many monks to flee to Bhutan. In the 12th century, the Drukpa Kagyupa school was established and remains the dominant form of Buddhism in Bhutan today. The country's political history is intimately tied to its religious history and relations among the various monastic schools and monasteries.
Bhutan is one of only a few countries which have been independent throughout their history, never conquered, occupied, or governed by an outside power (notwithstanding occasional nominal tributary status). Although there has been speculation that it was under the Kamarupa Kingdom or the Tibetan Empire in the 7th to 9th centuries, firm evidence is lacking. From the time historical records are clear, Bhutan has continuously and successfully defended its sovereignty.
The consolidation of Bhutan occurred in 1616 when Ngawanag Namgyal, a lama from western Tibet known as the Zhabdrung Rinpoche, defeated three Tibetan invasions, subjugated rival religious schools, codified the "Tsa Yig", an intricate and comprehensive system of law, and established himself as ruler over a system of ecclesiastical and civil administrators. After his death, infighting and civil war eroded the power of the Zhabdrung for the next 200 years. In 1885 Ugyen Wangchuck was able to consolidate power, and began cultivating closer ties with the British in India.
In 1907, Ugyen Wangchuck was elected as the hereditary ruler of Bhutan, crowned on December 17, 1907, and installed as the head of state, the Druk Gyalpo (Dragon King). In 1910, King Ugyen and the British signed the Treaty of Punakha which provided that British India would not interfere in the internal affairs of Bhutan if the country accepted external advice in its external relations. When Ugyen Wangchuck died in 1926, his son Jigme Wangchuck became ruler, and when India gained independence in 1947, the new Indian Government recognized Bhutan as an independent country. In 1949 India and Bhutan signed the Treaty of Peace and Friendship, which provided that India would not interfere in Bhutan's internal affairs, but would guide its foreign policy. Succeeded in 1952 by his son Jigme Dorji Wangchuck, Bhutan began to slowly emerge from its isolation and began a program of planned development. The National Assembly of Bhutan, the Royal Bhutanese Army, and the Royal Court of Justice were established, along with a new code of law. Bhutan became a member of the United Nations in 1971.
In 1972, Jigme Singye Wangchuck ascended the throne at age 20. He emphasized modern education, decentralization of governance, the development of hydroelectricity and tourism and improvements in rural developments. He was perhaps best known internationally for his overarching development philosophy of "gross national happiness." It recognizes that there are many dimensions to development and that economic goals alone are not sufficient. Satisfied with Bhutan's transitioning democratization process, he abdicated in December 2006 rather than wait until the promulgation of the new constitution in 2008. His son, Jigme Khesar Namgyel Wangchuck, became King upon his abdication.
Prehistory.
Neolithic tools found in the Bhutan indicate that people have been living in the Himalayan region for at least eleven thousand years ago. The earliest inhabitants of Bhutan and adjoining Himalayan areas of South Asia were the people from Indus Valley Civilization, a people of Dravidian origin whose history predates the onset of Bronze Age in South Asia around 3300BC before the coming of other ethnic groups from Tibet and South China some 2,000 years ago.
Origins and early settlement, 600–1600.
A state of Lhomon (literally, southern darkness) or Monyul (dark land, a reference to the Monpa one of the Tibeto-Burman people of Bhutan), possibly a part of Tibet that was then beyond the pale of Buddhist teachings. Monyul is thought to have existed between AD 100 and AD 600. The names Lhomon Tsendenjong (southern Mon sandalwood country) and Lhomon Khashi (southern Mon country of four approaches), found in ancient Bhutanese and Tibetan chronicles, may also have credence and have been used by some Bhutanese scholars when referring to their homeland. Variations of the Sanskrit words Bhota-ant (end of Bhot) or Bhu-uttan (meaning highlands) have been suggested by historians as origins of the name Bhutan, which came into common foreign use in the late 19th century and is used in Bhutan only in English-language official correspondence. The traditional name of the country since the 17th century has been Drukyul—country of the Drukpa, the Dragon people, or the Land of the Thunder Dragon, a reference to the country's dominant Buddhist sect.
Some scholars believe that during the early historical period the inhabitants were fierce mountain aborigines, the Monpa, who were of neither the Tibetan or Mongol stock that later overran northern Bhutan. The people of Monyul practiced a shamanistic religion, which emphasized worship of nature and the existence of good and evil spirits. During the latter part of this period, historical legends relate that the mighty king of Monyul invaded a southern region known as the Duars, subduing the regions of modern Assam, West Bengal, and Bihar in India.
Arrival of Buddhism.
Buddhism was first introduced to Bhutan in the 7th century. Tibetan king Songtsän Gampo (reigned 627–49), a convert to Buddhism, ordered the construction of two Buddhist temples, at Bumthang in central Bhutan and at Kyichu (near Paro) in the Paro Valley. Buddhism was propagated in earnest in 746 under King Sindhu Rāja ("also" Künjom; Sendha Gyab; Chakhar Gyalpo), an exiled Indian king who had established a government in Bumthang at Chakhar Gutho Palace. 
Buddhism replaced but did not eliminate the Bon religious practices that had also been prevalent in Tibet until the late 6th century. Instead, Buddhism absorbed Bon and its believers. As the country developed in its many fertile valleys, Buddhism matured and became a unifying element. It was Buddhist literature and chronicles that began the recorded history of Bhutan.
In 747, a Buddhist saint, Padmasambhava (known in Bhutan as Guru Rimpoche and sometimes referred to as the Second Buddha), came to Bhutan from India at the invitation of one of the numerous local kings. After reportedly subduing eight classes of demons and converting the king, Guru Rimpoche moved on to Tibet. Upon his return from Tibet, he oversaw the construction of new monasteries in the Paro Valley and set up his headquarters in Bumthang. According to tradition, he founded the Nyingmapa sect—also known as the "old sect" or Red Hat sect—of Mahayana Buddhism, which became for a time the dominant religion of Bhutan. Guru Rimpoche plays a great historical and religious role as the national patron saint who revealed the tantras—manuals describing forms of devotion to natural energy—to Bhutan. Following the guru's sojourn, Indian influence played a temporary role until increasing Tibetan migrations brought new cultural and religious contributions.
There was no central government during this period. Instead, small independent monarchies began to develop by the early 9th century. Each was ruled by a deb (king), some of whom claimed divine origins. The kingdom of Bumthang was the most prominent among these small entities. At the same time, Tibetan Buddhist monks (lam in Dzongkha, Bhutan's official national language) had firmly rooted their religion and culture in Bhutan, and members of joint Tibetan-Mongol military expeditions settled in fertile valleys. By the 11th century, all of Bhutan was occupied by Tibetan-Mongol military forces.
Sectarian rivalry.
By the 10th century, Bhutan's political development was heavily influenced by its religious history. Following a period in which Buddhism was in decline in Tibet in the 11th century, contention among a number of subsects emerged. The Mongol overlords of Tibet and Bhutan patronized a sequence of subsects until their own political decline in the 14th century. By that time, the Gelugpa or Yellow Hat school had, after a period of anarchy in Tibet, become a powerful force resulting in the flight to Bhutan of numerous monks of various minor opposing sects. Among these monks was the founder of the Lhapa subsect of the Kargyupa school, to whom is attributed the introduction of strategically built dzong. Although the Lhapa subsect had been successfully challenged in the 12th century by another Kargyupa subsect—the Drukpa—led by Tibetan monk Phajo Drugom Shigpo, it continued to proselytize until the 17th century. The Drukpa spread throughout Bhutan and eventually became a dominant form of religious practice. Between the 12th century and the 17th century, the two Kargyupa subsects vied with one another from their respective dzong as the older form of Nyingmapa Buddhism was eclipsed.
Theocratic government, 1616–1907.
Consolidation and defeat of Tibetan invasions, 1616–51.
In the 17th century, a theocratic government independent of Tibetan political influence was established, and premodern Bhutan emerged. The theocratic government was founded by an expatriate Drukpa monk, Ngawang Namgyal, who arrived in Bhutan in 1616 seeking freedom from the domination of the Gelugpa subsect led by the Dalai Lama (Ocean Lama) in Lhasa. After a series of victories over rival subsect leaders and Tibetan invaders, Ngawang Namgyal took the title Zhabdrung (At Whose Feet One Submits, or, in many Western sources, Dharma Raja), becoming the temporal and spiritual leader of Bhutan. Considered the first great historical figure of Bhutan, he united the leaders of powerful Bhutanese families in a land called Drukyul. He promulgated a code of law and built a network of impregnable dzong, a system that helped bring local lords under centralized control and strengthened the country against Tibetan invasions. Many dzong were extant in the late 20th century.
During the first war with Tibet, c. 1627, Portuguese Jesuits Estêvão Cacella and João Cabral were the first recorded Europeans to visit Bhutan on their way to Tibet. They met with Ngawang Namgyal, presented him with firearms, gunpowder and a telescope, and offered him their services in the war against Tibet, but the Zhabdrung declined the offer. After a stay of nearly eight months Cacella wrote a long letter from the Chagri Monastery reporting the travel. This is a rare report of the Zhabdrung remaining.
Tibetan armies invaded Bhutan around 1629, in 1631, and again in 1639, hoping to throttle Ngawang Namgyal's popularity before it spread too far. In 1634 Ngawang Namgyal defeated Karma Tenkyong's army in the Battle of Five Lamas. The invasions were thwarted, and the Drukpa subsect developed a strong presence in western and central Bhutan, leaving Ngawang Namgyal supreme. In recognition of the power he accrued, goodwill missions were sent to Bhutan from Cooch Behar in the Duars (present-day northeastern West Bengal), Nepal to the west, and Ladakh in western Tibet. The ruler of Ladakh even gave a number of villages in his kingdom to Ngawang Namgyal.
Bhutan's troubles were not over, however. In 1643, a joint Mongol-Tibetan force sought to destroy Nyingmapa refugees who had fled to Bhutan, Sikkim, and Nepal. The Mongols had seized control of religious and civil power in Tibet in the 1630s and established Gelugpa as the state religion. Bhutanese rivals of Ngawang Namgyal encouraged the Mongol intrusion, but the Mongol force was easily defeated in the humid lowlands of southern Bhutan. Another Tibetan invasion in 1647 also failed.
During Ngawang Namgyal's rule, administration comprised a state monastic body with an elected head, the Je Khenpo (lord abbot), and a theocratic civil government headed by the Druk Desi (regent of Bhutan, also known as Deb Raja in Western sources). The Druk Desi was either a monk or a member of the laity—by the 19th century, usually the latter; he was elected for a three-year term, initially by a monastic council and later by the State Council (Lhengye Tshokdu). The State Council was a central administrative organ that included regional rulers, the Zhabdrung's chamberlains, and the Druk Desi. In time, the Druk Desi came under the political control of the State Council's most powerful faction of regional administrators. The Zhabdrung was the head of state and the ultimate authority in religious and civil matters. The seat of government was at Thimphu, the site of a 13th-century dzong, in the spring, summer, and fall. The winter capital was at Punakha Dzong, a dzong established northeast of Thimphu in 1527. The kingdom was divided into three regions (east, central, and west), each with an appointed ponlop, or governor, holding a seat in a major dzong. Districts were headed by dzongpon, or district officers, who had their headquarters in lesser dzong. The ponlop were combination tax collectors, judges, military commanders, and procurement agents for the central government. Their major revenues came from the trade between Tibet and India and from land taxes.
Ngawang Namgyal's regime was bound by a legal code called the Tsa Yig, which described the spiritual and civil regime and provided laws for government administration and for social and moral conduct. The duties and virtues inherent in the Buddhist dharma (religious law) played a large role in the new legal code, which remained in force until the 1960s.
Administrative integration and conflict with Tibet, 1651–1728.
To keep Bhutan from disintegrating, Ngawang Namgyal's death in 1651 apparently was kept a carefully guarded secret for fifty-four years. Initially, Ngawang Namgyal was said to have entered into a religious retreat, a situation not unprecedented in Bhutan, Sikkim, or Tibet during that time. During the period of Ngawang Namgyal's supposed retreat, appointments of officials were issued in his name, and food was left in front of his locked door.
Ngawang Namgyal's son and stepbrother, in 1651 and 1680, respectively, succeeded him. They started their reigns as minors under the control of religious and civil regents and rarely exercised authority in their own names. For further continuity, the concept of multiple reincarnation of the first Zhabdrung—in the form of either his body, his speech, or his mind—was invoked by the Je Khenpo and the Druk Desi, both of whom wanted to retain the power they had accrued through the dual system of government. The last person recognized as the bodily reincarnation of Ngawang Namgyal died in the mid-18th century, but speech and mind reincarnations, embodied by individuals who acceded to the position of Zhabdrung Rinpoche, were recognized into the early 20th century. The power of the state religion also increased with a new monastic code that remained in effect in the early 1990s. The compulsory admission to monastic life of at least one son from any family having three or more sons was instituted in the late 17th century. In time, however, the State Council became increasingly secular as did the successive Druk Desi, ponlop, and dzongpon, and intense rivalries developed among the ponlop of Tongsa and Paro and the dzongpon of Punakha, Thimphu, and Wangdue Phodrang.
During the first period of succession and further internal consolidation under the Druk Desi government, there was conflict with Tibet and Sikkim. Internal opposition to the central government resulted in overtures by the opponents of the Druk Desi to Tibet and Sikkim. In the 1680s, Bhutan invaded Sikkim in pursuit of a rebellious local lord. In 1700, Bhutan again invaded Sikkim, and in 1714 Tibetan forces, aided by Mongolia, invaded Bhutan but were unable to gain control.
Western outposts.
During the 17th century Bhutan maintained close relations with Ladakh, and assisted Ladakh in its 1684 war with Tibet. Ladakh had earlier granted Bhutan several enclaves near Mount Kailash in western Tibet; these were monasteries of the Drukpa sect and so fell under the authority of the Bhutanese Je Khenpo and the Zhabdrung. These enclaves persisted under Bhutanese control even after the rest of western Tibet came under the control of the Dalai Lama and his Gelugpa sect. Not until 1959 were the Bhutanese enclaves seized by the Chinese. In addition to these outposts in Tibet, Bhutan for a time held monastic fiefs in Ladakh, Zanskar, and Lahul (now part of India), as well as in Lo Manthang and Dolpo (now part of Nepal).
Civil conflict, 1728–72.
Though the invaders were unable to take control, the political system remained unstable. Regional rivalries contributed to the gradual disintegration of Bhutan at the time the first British agents arrived.
In the early 18th century, Bhutan had successfully developed control over the principality of Cooch Behar. The raja of Cooch Behar had sought assistance from Bhutan against the Indian Mughals in 1730, and Bhutanese political influence was not long in following. By the mid-1760s, Thimphu considered Cooch Behar its dependency, stationing a garrison force there and directing its civil administration. When the Druk Desi invaded Sikkim in 1770, Cooch Behari forces joined their Bhutanese counterparts in the offensive. In a succession dispute in Cooch Behar two years later, however, the Druk Desi's nominee for the throne was opposed by a rival who invited British troops, and, in effect, Cooch Behar became a dependency of the British East India Company.
British intrusion, 1772–1906.
Under the Cooch Behari agreement with the British, a British expeditionary force drove the Bhutanese garrison out of Cooch Behar and invaded Bhutan in 1772–73. The Druk Desi petitioned Lhasa for assistance from the Panchen Lama, who was serving as regent for the youthful Dalai Lama. In correspondence with the British governor general of India, however, the Panchen Lama instead punished the Druk Desi and invoked Tibet's claim of suzerainty over Bhutan.
Failing to receive help from Tibet, the Druk Desi signed a Treaty of Peace with the British East India Company on April 25, 1774. Bhutan agreed to return to its pre-1730 boundaries, paid a symbolic tribute of five horses to Britain, and, among other concessions, allowed the British to harvest timber in Bhutan. Subsequent missions to Bhutan were made by the British in 1776, 1777, and 1783, and commerce was opened between British India and Bhutan, and, for a short time, Tibet. In 1784, the British turned over to Bhutanese control Bengal Duars territory, where boundaries were poorly defined. As in its other foreign territories, Bhutan left administration of the Bengal Duars territory to local officials and collected its revenues. Although major trade and political relations failed to develop between Bhutan and Britain, the British had replaced the Tibetans as the major external threat.
Boundary disputes plagued Bhutanese–British relations. To reconcile their differences, Bhutan sent an emissary to Calcutta in 1787, and the British sent missions to Thimphu in 1815 and 1838. The 1815 mission was inconclusive. The 1838 mission offered a treaty providing for extradition of Bhutanese officials responsible for incursions into Assam, free and unrestricted commerce between India and Bhutan, and settlement of Bhutan's debt to the British. In an attempt to protect its independence, Bhutan rejected the British offer. Despite increasing internal disorder, Bhutan had maintained its control over a portion of the Assam Duars more or less since its reduction of Cooch Behar to a dependency in the 1760s. After the British gained control of Lower Assam in 1826, tension between the countries began to rise as Britain exerted its strength. Bhutanese payments of annual tribute to the British for the Assam Duars gradually fell into arrears. British demands for payment led to military incursions into Bhutan in 1834 and 1835, resulting in defeat for Bhutan's forces and a temporary loss of territory.
The British proceeded in 1841 to annex the formerly Bhutanese-controlled Assam Duars, paying a compensation of 10,000 rupees a year to Bhutan. In 1842, Bhutan gave to the British control of some of the troublesome Bengal Duars territory it had administered since 1784.
Charges and countercharges of border incursions and protection of fugitives led to an unsuccessful Bhutanese mission to Calcutta in 1852. Among other demands, the mission sought increased compensation for its former Duars territories; instead the British deducted nearly 3,000 rupees from the annual compensation and demanded an apology for alleged plundering of British-protected lands by members of the mission. Following more incidents and the prospect of an anti-Bhutan rebellion in the Bengal Duars, British troops deployed to the frontier in the mid-1850s. The Sepoy Rebellion in India in 1857-58 and the demise of the British East India Company's rule prevented immediate British action. Bhutanese armed forces raided Sikkim and Cooch Behar in 1862, seizing people, property, and money. The British responded by withholding all compensation payments and demanding release of all captives and return of stolen property. Demands to the Druk Desi went unheeded, as he was alleged to be unaware of his frontier officials' actions against Sikkim and Cooch Behar.
Britain sent a peace mission to Bhutan in early 1864, in the wake of the recent conclusion of a civil war there. The dzongpon of Punakha—who had emerged victorious—had broken with the central government and set up a rival Druk Desi, while the legitimate Druk Desi sought the protection of the ponlop of Paro and was later deposed. The British mission dealt alternately with the rival ponlop of Paro and the ponlop of Tongsa (the latter acting on behalf of the Druk Desi), but Bhutan rejected the peace and friendship treaty it offered. Britain declared war in November 1864. Bhutan had no regular army, and what forces existed were composed of dzong guards armed with matchlocks, bows and arrows, swords, knives, and catapults. Some of these dzong guards, carrying shields and wearing chainmail armor, engaged the well-equipped British forces.
The Duar War (1864–65) lasted only five months and, despite some battlefield victories by Bhutanese forces, resulted in Bhutan's defeat, loss of part of its sovereign territory, and forced cession of formerly occupied territories. Under the terms of the Treaty of Sinchula, signed on November 11, 1865, Bhutan ceded territories in the Assam Duars and Bengal Duars, as well as the eighty-three-square-kilometer territory of Dewangiri in southeastern Bhutan, in return for an annual subsidy of 50,000 rupees. The land that was to become Bhutan House was ceded from Bhutan to British India in 1865 at the conclusion the Duar War and as a condition of the Treaty of Sinchula.
In the 1870s and 1880s, renewed competition among regional rivals—primarily the pro-British ponlop of Tongsa and the anti-British, pro-Tibetan ponlop of Paro—resulted in the ascendancy of Ugyen Wangchuck, the Ponlop of Tongsa. From his power base in central Bhutan, Ugyen Wangchuck had defeated his political enemies and united the country following several civil wars and rebellions in 1882-85. His victory came at a time of crisis for the central government, however. British power was becoming more extensive to the south, and in the west Tibet had violated its border with Sikkim, incurring British disfavor. After 1,000 years of close ties with Tibet, Bhutan faced the threat of British military power and was forced to make serious geopolitical decisions. The British, seeking to offset potential Russian advances in Lhasa, wanted to open trade relations with Tibet. Ugyen Wangchuck, on the advice of his closest adviser Ugyen Dorji, saw the opportunity to assist the British and in 1903-4 volunteered to accompany a British mission to Lhasa as a mediator. For his services in securing the Anglo-Tibetan Convention of 1904, Ugyen Wangchuck was knighted and thereafter continued to accrue greater power in Bhutan. Ugyen Dorji, as well as his descendants, went on to maintain British favor on behalf of the government from Bhutan House in Kalimpong, India.
Establishment of the hereditary monarchy, 1907.
Ugyen Wangchuck's emergence as the national leader coincided with the realization that the dual political system was obsolete and ineffective. He had removed his chief rival, the ponlop of Paro, and installed a supporter and relative, a member of the pro-British Dorji family, in his place. When the last Zhabdrung died in 1903 and a reincarnation had not appeared by 1906, civil administration came under the control of Ugyen Wangchuck. Finally, in 1907, the fifty-fourth and last Druk Desi was forced to retire, and despite recognitions of subsequent reincarnations of Ngawang Namgyal, the Zhabdrung system came to an end.
In November 1907, an assembly of leading Buddhist monks, government officials, and heads of important families was held to end the moribund 300-year-old dual system of government and to establish a new absolute monarchy. Ugyen Wangchuck was elected its first hereditary Druk Gyalpo ("Dragon King") and subsequently reigned from 1907–26. Bhutan's Political Officer John Claude White took photographs of the coronation ceremony. The Dorji family became hereditary holders of the position of Gongzim (Chief Chamberlain), the top government post. The British, wanting political stability on their northern frontier, approved of the entire development.
Britain's earlier entreaties in Lhasa had unexpected repercussions at this time. The Chinese, concerned that Britain would seize Tibet, invaded Tibet in 1910 and asserted political authority. In the face of the Chinese military occupation, the Dalai Lama fled to India. China laid claim not only to Tibet but also to Bhutan, Nepal, and Sikkim. With these events, Bhutanese and British interests coalesced.
On January 8, 1910, Sikkim Political Officer and Tibetologist Sir Charles Alfred Bell engaged Bhutan and signed the Treaty of Punakha. The Treaty of Punakha amended two articles of the 1865 treaty: the British agreed to double their annual stipend to 100,000 rupees and "to exercise no interference in the internal administration of Bhutan." In turn, Bhutan agreed "to be guided by the advice of the British Government in regard to its external relations." The Treaty of Punakha guaranteed Bhutan's defense against China; China, in no position to contest British power, conceded the end of the millennium-long Tibetan-Chinese influence. It also assigned land in Motithang (Thimphu) and a hill station between Chukha and Thimphu to the British, assigning a portion of Kalimpong (Bhutan House) to Bhutan.
Much of Bhutan's modern development has been attributed by Bhutanese historians to the first Druk Gyalpo. Internal reforms included introducing Western-style schools, improving internal communications, encouraging trade and commerce with India, and revitalizing the Buddhist monastic system. Toward the end of his life, Ugyen Wangchuck was concerned about the continuity of the family dynasty, and in 1924 he sought British assurance that the Wangchuck family would retain its preeminent position in Bhutan. His request led to an investigation of the legal status of Bhutan vis-à-vis the suzerainty held over Bhutan by Britain and the ambiguity of Bhutan's relationship to India. Both the suzerainty and the ambiguity were maintained.
Development of centralized government, 1926–52.
Ugyen Wangchuck died in 1926 and was succeeded by his son, Jigme Wangchuck (reigned 1926–52). The second Druk Gyalpo continued his father's centralization and modernization efforts and built more schools, dispensaries, and roads. During Jigme Wangchuck's reign, monasteries and district governments were increasingly brought under royal control. However, Bhutan generally remained isolated from international affairs.
The issue of Bhutan's status vis-à-vis the government of India was reexamined by London in 1932 as part of the issue of the status of India itself. It was decided to leave the decision to join an Indian federation up to Bhutan when the time came. When British rule over India ended in 1947, so too did Britain's association with Bhutan. India succeeded Britain as the de facto protector of the Himalayan kingdom, and Bhutan retained control over its internal government. It was two years, however, before a formal agreement recognized Bhutan's independence.
Following the precedent set by the Treaty of Punakha, on August 8, 1949, Thimphu signed the Treaty of Friendship Between the Government of India and the Government of Bhutan, according to which external affairs, formerly guided by Britain, were to be guided by India. Like Britain, India agreed not to interfere in Bhutan's internal affairs. India also agreed to increase the annual subsidy to 500,000 rupees per year. Important to Bhutan's national pride was the return of Dewangiri. Some historians believe that if India had been at odds with China at this time, as it was to be a decade later, it might not have acceded so easily to Bhutan's request for independent status.
Modernization under Jigme Dorji, 1952–72.
The third Druk Gyalpo, Jigme Dorji Wangchuck, was enthroned in 1952. Earlier he had married the European-educated cousin of the chogyal (king) of Sikkim and with her support made continual efforts to modernize his nation throughout his twenty-year reign. Among his first reforms was the establishment of the National Assembly — the Tshogdu — in 1953. Although the Druk Gyalpo could issue royal decrees and exercise veto power over resolutions passed by the National Assembly, its establishment was a major move toward a constitutional monarchy.
When the Chinese communists took over Tibet in 1951, Bhutan closed its frontier with Tibet and sided with its powerful neighbor to the south. To offset the chance of Chinese encroachment, Bhutan began a modernization program. Land reform was accompanied by the abolition of slavery and serfdom and the separation of the judiciary from the executive branch of government. Mostly funded by India after China's invasion of Tibet in 1959, the modernization program also included the construction of roads linking the Indian plains with central Bhutan. An all-weather road was completed in 1962 between Thimphu and Phuntsholing, the overland gateway town on the southwest border with India. Dzongkha was made the national language during Jigme Dorji's reign. Additionally, development projects included establishing such institutions as a national museum in Paro and a national library, national archives, and national stadium, as well as buildings to house the National Assembly, the High Court (Thrimkhang Gongma), and other government entities in Thimphu. The position of gongzim, held since 1907 by the Dorji family, was upgraded in 1958 to lonchen (prime minister) and was still in the hands of the Dorji. Jigme Dorji Wangchuck's reforms, however, although lessening the authority of the absolute monarchy, also curbed the traditional decentralizatio of political authority among regional leaders and strengthened the role of the central government in economic and social programs.
Modernization efforts moved forward in the 1960s under the direction of the lonchen, Jigme Palden Dorji, the Druk Gyalpo's brother-in-law. In 1962, however, Dorji incurred disfavor with the Royal Bhutan Army over the use of military vehicles and the forced retirement of some fifty officers. Religious elements also were antagonized by Dorji's efforts to reduce the power of the state-supported religious institutions. In April 1964, while the Druk Gyalpo was in Switzerland for medical care, Dorji was assassinated in Phuntsholing by an army corporal. The majority of those arrested and accused of the crime were military personnel and included the army chief of operations, Namgyal Bahadur, the Druk Gyalpo's uncle, who was executed for his part in the plot.
The unstable situation continued under Dorji's successor as acting lonchen, his brother Lhendup Dorji, and for a time under the Druk Gyalpo's brother, Namgyal Wangchuck, as head of the army. According to some sources, a power struggle ensued between pro-Wangchuck loyalists and "modernist" Dorji supporters. The main issue was not an end to or lessening of the power of the monarchy but "full freedom from Indian interference." Other observers believe the 1964 crisis was not so much a policy struggle as competition for influence on the palace between the Dorji family and the Druk Gyalpo's Tibetan consort, Yanki, and her father. Lhendup Dorji had earlier threatened to kill Yanki—his sister's rival—and ordered her arrest when, fearing for her life and that of her 2-year-old son by the Druk Gyalpo, she sought refuge in India during the political crisis. Lhendup also incurred the disapproval of the Druk Gyalpo by seeking to become sole regent of the kingdom after his brother's death, eliminating the Queen and the king's brother. Before returning to Bhutan from Switzerland, Jigme Dorji met with the Indian Secretary General and Foreign Secretary in Calcutta(booty) who offered Indian support, including paratroopers if necessary, to help the Druk Gyalpo restore order in the kingdom. Unable to regain the Druk Gyalpo's trust, Lhendup fled to London, while other supporters in the military and government fled to Nepal and Calcutta. Afterwards, in concurrence of the National Assembly, Lhendup Dorji and other family members were exiled in 1965. However, the exiles continued their attacks on the Druk Gyalpo and India, worsening relations between India and China. The tense political situation continued and in July 1965 there was an assassination attempt on the Druk Gyalpo. The Dorjis were not implicated in the attempt—which was described as a "private matter"—and the would-be assassins were pardoned by the Druk Gyalpo.
In 1966, to increase the efficiency of government administration, Jigme Dorji Wangchuck made Thimphu the year-round capital. In May 1968, the comprehensive Rules and Regulations of the National Assembly revised the legal basis of the power granted to the National Assembly. The Druk Gyalpo decreed that henceforth sovereign power, including the power to remove government ministers and the Druk Gyalpo himself, would reside with the National Assembly. The following November, the Druk Gyalpo renounced his veto power over National Assembly bills and said he would step down if two-thirds of the legislature passed a no-confidence vote. Although he did nothing to undermine the retention of the Wangchuck dynasty, the Druk Gyalpo in 1969 called for a triennial vote of confidence by the National Assembly (later abolished by his successor) to renew the Druk Gyalpo's mandate to rule.
Diplomatic overtures also were made during Jigme Dorji Wangchuck's reign. Although always seeking to be formally neutral and nonaligned in relations with China and India, Bhutan also sought more direct links internationally than had occurred previously under the foreign-policy guidance of India. Consequently, in 1962 Bhutan joined the Colombo Plan for Cooperative, Economic, and Social Development in Asia and the Pacific known as the Colombo Plan, and in 1966 notified India of its desire to become a member of the United Nations (UN). In 1971, after holding observer status for three years, Bhutan was admitted to the UN. In an effort to maintain Bhutan as a stable buffer state, India continued to provide substantial amounts of development aid.
Jigme Dorji Wangchuck ruled until his death in July 1972 and was succeeded by his seventeen-year-old son, Jigme Singye Wangchuck. The close ties of the Wangchuck and Dorji families were reemphasized in the person of the new king, whose mother, Ashi Kesang Dorji (ashi means princess), was the sister of the lonchen, Jigme Palden Dorji. Jigme Singye Wangchuck, who had been educated in India and Britain, had been appointed ponlop of Tongsa in May 1972 and by July that year had become the Druk Gyalpo. With his mother and two elder sisters as advisers, the new Druk Gyalpo was thrust into the affairs of state. He was often seen among the people, in the countryside, at festivals, and, as his reign progressed, meeting with foreign dignitaries in Bhutan and abroad. His formal coronation took place in June 1974, and soon thereafter the strains between the Wangchucks and Dorjis were relieved with the return that year of the exiled members of the latter family. The reconciliation, however, was preceded by reports of a plot to assassinate the new Druk Gyalpo before his coronation could take place and to set fire to the Tashichho Dzong (Fortress of the Glorious Religion, the seat of government in Thimphu). Yanki (who had four children with the Druk Gyalpo, including two sons, between 1962–1972) was the alleged force behind the plot, which was uncovered three months before the coronation; thirty persons were arrested, including high government and police officials. However, Lawrence Sittling, secretary to Jigme Dorji Wangchuck, later reported that the assassination plot was a fabrication by a Chinese diplomat designed to alienate Bhutan from India. But the truth was not any more politically acceptable—those arrested were Tibetan Khampas rebels, trained in India, who were traveling through Bhutan on a mission to Tibet. (Encyclopaedia of Saarc Nations, Syed) Under pressure from China, the Bhutanese government demanded that the four thousand Tibetan refugees living in Bhutan either become Bhutanese citizens or go into exile. Most chose exile. (Syed)
International relations, 1972–present.
When civil war broke out in Pakistan in 1971, Bhutan was the first nation to recognize the new government of Bangladesh, and formal diplomatic relations were established in 1973. An event in 1975 may have served as a major impetus to Bhutan to speed up reform and modernization. In that year, neighboring Sikkim's monarchy, which had endured for more than 300 years, was ousted following a plebiscite in which the Nepalese majority outvoted the Sikkimese minority. Sikkim, long a protectorate of India, became India's twenty-second state.
To further ensure its independence and international position, Bhutan gradually established diplomatic relations with other nations and joined greater numbers of regional and international organizations. Many of the countries with which Bhutan established relations provided development aid. Moderization of daily life brought new problems to Bhutan in the late 1980s. Television broadcast was official introduced in Bhutan in 1999.
Assamese separatists.
Several guerrilla groups seeking to establish an independent Assamese state in northeast India have set up guerrilla bases in the forests of southern Bhutan, from which they launched cross-border attacks on targets in Assam. The largest guerrilla group was the ULFA (United Liberation Front of Asom). Negotiations aimed at removing them peacefully from these bases failed in the spring of 2003. Bhutan was faced with the prospect of having to strengthen its token army force to evict the guerrillas.
Military action against Assamese separatists December 2003.
On 15 December 2003 the Royal Bhutan Army began military operations against guerrilla camps in southern Bhutan, in coordination with Indian armed forces who lined the border to the south to prevent the guerrillas from dispersing back into Assam. News sources indicated that of the 30 camps that were target, 13 were controlled by ULFA, 12 camps by the National Democratic Front of Bodoland (NDFB), and 5 camps controlled by the Kamatapur Liberation Organisation (KLO). By January, government news reports indicated the guerillas had been routed from their bases.
Refugee community.
In 1988, Bhutan was reported to have evicted some number of Nepali-speaking residents (Bhutanese reports say about 5,000 and Refugee reports says over 100,000) from districts in southern Bhutan, creating a large refugee community that was now being detained in seven temporary United Nations refugee camps in Nepal and Sikkim. The actual numbers were difficult to establish, as many of those in the camps were reported to be holding forged identity papers, and impoverished Nepalese citizens and started to migrate to the Nepalese community leaving their refugee camps. The reason for leaving refugee camps was to find a job, and services to those living in camps. Few of them returned to the refugee camps. As a result, the number of people living in the camps decreased exponentially. Although the Bhutanese government claimed that only about 5000 initially left the country, the number of actual migration was more than that.
After years of negotiations between Nepal and Bhutan, in 2000 Bhutan agreed in principle to allow certain classes of the refugees to return to Bhutan. However the situation was at a standstill, after violence was committed on Bhutanese officials by the angered people in the camps. Significant unrest was now reported to be fomenting in the camps, especially as the United Nations terminated a number of educational and welfare programmes in an effort to force Bhutan and Nepal to come to terms.
As the Bhutanese government was unwilling to take them into their country many developed nations offered the refugees to allow them to settle in their own countries which included USA and Australia. As many as 20,000 Bhutanese refugees have been resettled in these countries.
Formalized democracy.
Constitution.
On March 26, 2005, "an auspicious day when the stars and elements converge favourably to create an environment of harmony and success", the king and government distributed a draft of the country's first constitution, requesting that every citizen review it. A new house of parliament, the National Council, is chartered consisting of 20 elected representatives from each of the dzonghags, persons selected by the King. The National Council would be paired with the other already existing house, the National Assembly.
Per the Constitution, the monarchy is given a leadership role in setting the direction for the government as long as the King shall demonstrate his commitment and ability to safeguard the interests of the kingdom and its people.
Jigme Khesar Namgyel Wangchuck.
On December 15, 2006, the fourth Druk Gyalpo, His Majesty Jigme Singye Wangchuck, abdicated all of his powers as King to his son, Prince Jigme Khesar Namgyel Wangchuck, with a specific intention to prepare the young King for the country's transformation to a full-fledged, democratic form of government due to occur in 2008.
The previous King's abdication in favour of his son was originally set to occur in 2008 as well, but there was an apparent concern that the new King should have hands-on experience as the nation's leader before presiding over a transformation in the country's form of government. According to the national newspaper, the Kuensel, the previous King stated to his cabinet that "as long as he himself continued to be King, the Crown Prince would not gain the actual experience of dealing with issues and carrying out the responsibilities of a head of state. With parliamentary democracy to be established in 2008, there was much to be done; so it was necessary that he gained this valuable experience."
The fourth Druk Gyalpo further stated that

</doc>
<doc id="42395" url="https://en.wikipedia.org/wiki?curid=42395" title="Warsaw Ghetto">
Warsaw Ghetto

The Warsaw Ghetto (, called by the German authorities "Jüdischer Wohnbezirk in Warschau", Jewish residential district in Warsaw; ) was the largest of all the Jewish ghettos in Nazi-occupied Europe during World War II. It was established in the Muranów neighborhood of the Polish capital between October and November 16, 1940, part of the territory of the General Government of German-occupied Poland, with over 400,000 Jews from the vicinity residing in an area of . From there, at least 254,000 Ghetto residents were sent to the Treblinka extermination camp over the course of two months in the summer of 1942.
The death toll among the Jewish inhabitants of the Ghetto, between starvation, disease, deportations to extermination camps, "Großaktion Warschau", the Warsaw Ghetto Uprising, and the subsequent razing of the ghetto, is estimated to be at least 300,000.
Creation.
The construction of the ghetto wall started on April 1, 1940.
The Warsaw Ghetto was established by the German Governor-General Hans Frank on October 16, 1940 in an area of Warsaw primarily occupied by Polish Jews. Frank ordered all Jews in Warsaw and its suburbs rounded up and herded into the Ghetto. At this time, the population in the Ghetto was estimated to be 400,000 people, about 30% of the population of Warsaw; however, the area of the Ghetto was only about 2.4% of that of Warsaw.
The Germans closed the Warsaw Ghetto to the outside world on November 15, 1940. The wall was typically high and topped with barbed wire. Escapees could be shot on sight. The borders of the ghetto changed many times during the next years.
The ghetto was divided by Chłodna Street, which due to its importance (as one of Warsaw's major east-west arteries) was excluded from it. The area south of Chłodna was known as the “Small Ghetto”, while the area north of this street was the “Large Ghetto”. Those two parts were connected by Żelazna Street, and a special gate was built at its intersection with Chłodna Street. In January 1942, the gate was closed and a wooden footbridge was built in its place, which after the war became one of the symbols of the Holocaust.
The first commissioner of the Warsaw ghetto was his chief organizer SA-Standartenführer Waldemar Schön. He was succeeded in May 1941 by Heinz Auerswald.
Administration of the Ghetto.
Like all the ghettos in Poland, the Germans ascribed the administration to a Judenrat (a council of the Jews), led by an "" (the eldest). In Warsaw this role was attributed to Adam Czerniaków, who chose a policy of collaboration with the Nazis rather than revolt. Adam Czerniaków confided his harrowing experience in several diaries. He became aware of his own tragic duplicity in July 1942 and committed suicide.
Although his personality has remained less infamous than Mordechai Chaim Rumkowski, the "Ältester" of the Lodz Ghetto, Adam Czerniaków's collaboration with the Nazi policy is the paradigm of the attitude of the majority of the European Jews vis à vis Nazism. The Jewish collaboration authority was supported by a Jewish Ghetto Police. According to Lucy S. Dawidowicz:
Conditions.
During the next year and a half, thousands of Polish Jews as well as some Romani people from smaller cities and the countryside were brought into the Ghetto, while diseases (especially typhus), and starvation kept the inhabitants at about the same number. Average food rations in 1941 for Jews in Warsaw were limited to 184 calories, compared to 699 calories for gentile Poles and 2,613 calories for Germans.
Unemployment was a major problem in the ghetto. Illegal workshops were created to manufacture goods to be sold illegally on the outside and raw goods were smuggled in, often by children. Hundreds of four- to eight-year-old Jewish children went across en masse to the "Aryan side," sometimes several times a day, smuggling food into the ghettos, returning with goods that often weighed more than they did. Smuggling was often the only source of subsistence for Ghetto inhabitants, who would otherwise have died of starvation.
Despite the grave hardships, life in the Warsaw Ghetto was rich with educational and cultural activities, conducted by its underground organizations. Hospitals, public soup kitchens, orphanages, refugee centers and recreation facilities were formed, as well as a school system. Some schools were illegal and operated under the guise of a soup kitchen. There were secret libraries, classes for the children and even a symphony orchestra. Rabbi Alexander Zusia Friedman, secretary-general of Agudath Israel of Poland, was one of the Torah leaders in the Warsaw Ghetto. He organized an underground network of religious schools, including "a Yesodei HaTorah school for boys, a Bais Yaakov school for girls, a school for elementary Jewish instruction, and three institutions for advanced Jewish studies". These schools, operating under the guise of kindergartens, medical centers and soup kitchens, were a place of refuge for thousands of children and teens, and hundreds of teachers. In 1941, when the Germans gave official permission to the local Judenrat to open schools, these schools came out of hiding and began receiving financial support from the official Jewish community.
Over 100,000 of the Ghetto's residents died due to rampant disease or starvation, as well as random killings, even before the Nazis began massive deportations of the inhabitants from the Ghetto's "Umschlagplatz" to the Treblinka extermination camp during the Grossaktion Warschau, part of the countrywide Operation Reinhard. Between "Tisha B'Av" (July 23) and "Yom Kippur" (September 21) of 1942, about 254,000 Ghetto residents (or at least 300,000 by different accounts) were sent to Treblinka and murdered there.
Friedman alerted world Jewry to the start of deportations from the Warsaw Ghetto in a coded message. His telegram read: "Mr. Amos kept his promise from the fifth-third." He was referring to the Book of Amos, chapter 5, verse 3, which reads: "The city that goes out a thousand strong will have a hundred left, and the one that goes out a hundred strong will have ten left to the House of Israel".
Polish resistance officer Jan Karski reported to the Western governments in 1942 on the situation in the Ghetto and on the extermination camps. By the end of 1942, it was clear that the deportations were to their deaths, and many of the remaining Jews decided to fight.
For years, Ghetto residents in the group "Oyneg Shabbos" had discreetly chronicled conditions and hid their photos, writings, and short films in improvised time capsules; their activity increased after learning that transports to "resettlement" actually led to the mass killings. In May 1942, Germans began filming a propaganda movie titled ""Das Ghetto"" which was never completed. Footage is shown in the 2010 documentary called "A Film Unfinished" which concerns the making of ""Das Ghetto"" and correlates scenes from 'Das Ghetto' with descriptions of the filming of these scenes that Czerniakow mentions in his diary.
Warsaw Ghetto Uprising and destruction of the Ghetto.
On January 18, 1943, after almost four months without any deportations, the Germans suddenly entered the Warsaw ghetto intent upon a further deportation. Within hours, some 600 Jews were shot and 5,000 others rounded up. 
The Germans expected no resistance, but preparations to resist had been going on since the previous autumn. The first instances of Jewish armed resistance began that day. The Jewish fighters had some success: the expulsion stopped after four days and the ŻOB and ŻZW resistance organizations took control of the Ghetto, building shelters and fighting posts and operating against Jewish collaborators.
The final battle started on the eve of Passover of April 19, 1943, when a Nazi force consisting of several thousand troops entered the ghetto. After initial setbacks, the Germans under the field command of Jürgen Stroop systematically burned and blew up the ghetto buildings, block by block, rounding up or murdering anybody they could capture. Significant resistance ended on April 28, and the Nazi operation officially ended in mid-May, symbolically culminating with the demolition of the Great Synagogue of Warsaw on May 16. According to the official report, at least 56,065 people were killed on the spot or deported to German Nazi concentration and death camps (Treblinka, Poniatowa, Majdanek, Trawniki).
Remnants of the Ghetto today.
The ghetto was almost entirely leveled during the uprising; however, a number of buildings and streets survived, mostly in the "small ghetto" area, which had been included into the Aryan part of the city in August 1942 and was not involved in the fighting. In 2008 and 2010 Warsaw Ghetto boundary markers were built along the borders of the former Jewish quarter, where from 1940−1943 stood the gates to the ghetto, wooden footbridges over Aryan streets, and the buildings important to the ghetto inmates. The four buildings at 7, 9, 12 and 14 Próżna Street are among the best known original residential buildings that in 1940-41 housed Jewish families in the Warsaw Ghetto. They have largely remained empty since the war. The street is a focus of the annual Warsaw Jewish Festival. In 2011−2013 buildings at number 7 and 9 underwent extensive renovations and have become office space.
The Nożyk Synagogue also survived the war. It was used as a horse stable by the German Wehrmacht. The synagogue has today been restored and is once again used as an active synagogue. The best preserved fragments of the ghetto wall are located 55 Sienna Street, 62 Złota Street, and 11 Waliców Street (the last two being walls of the pre-war buildings). There are two Warsaw Ghetto Heroes' monuments, unveiled in 1946 and 1948, near the place where the German troops entered the ghetto on 19 April 1943. In 1988 a stone monument was built to mark the Umschlagplatz.
There is also a small memorial at ul. Mila 18 to commemorate the site of the Jewish underground headquarters during the Ghetto Uprising. In December 2012, a controversial statue of a kneeling and praying Adolf Hitler was installed in a courtyard of the Ghetto. The artwork by Italian artist, Maurizio Cattelan, entitled "HIM", has received mixed reactions worldwide. Many feel that it is unnecessarily offensive, while others, such as Poland's chief rabbi, Michael Schudrich, feel that is thought-provoking, even "educational".

</doc>
<doc id="42398" url="https://en.wikipedia.org/wiki?curid=42398" title="201">
201

__NOTOC__
Year 201 (CCI) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar. At the time, it was known as the Year of the Consulship of Fabianus and Arrius (or, less frequently, year 954 "Ab urbe condita"). The denomination 201 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="42399" url="https://en.wikipedia.org/wiki?curid=42399" title="History of Bosnia and Herzegovina">
History of Bosnia and Herzegovina

This is a history of Bosnia and Herzegovina.
Prehistory and Roman era.
Bosnia has been inhabited at least since Neolithic times. In the late Bronze Age, the Neolithic population was replaced by more warlike Indo-European tribes known as the Illyrians. Celtic migrations in the 4th and 3rd century BCE displaced many Illyrian tribes from their former lands, but some Celtic and Illyrian tribes mixed. Concrete historical evidence for this period is scarce, but overall it appears that the region was populated by a number of different peoples speaking distinct languages. Conflict between the Illyrians and Romans started in 229 BCE, but Rome wouldn't complete its annexation of the region until 9 CE. In the Roman period, Latin-speaking settlers from all over the Roman empire settled among the Illyrians and Roman soldiers were encouraged to retire in the region.
Christianity had already arrived in the region by the end of the 1st century, and numerous artifacts and objects from the time testify to this. Following events from the years 337 and 395 when the Empire split, Dalmatia and Pannonia were included in the Western Roman Empire. The region was conquered by the Ostrogoths in 455, and further exchanged hands between the Alans and Huns in the years to follow.
Middle Ages.
By the 6th century, Emperor Justinian had re-conquered the area for the Byzantine Empire. The Slavs, a migratory people from southeastern Europe, were allied by the Eurasian Avars in the 6th century, and together they invaded the Eastern Roman Empire in the 6th and 7th centuries, settling in what is now Bosnia and Herzegovina and the surrounding lands. More South Slavs came in a second wave, and according to some scholars were invited by Emperor Heraclius to drive the Avars from Dalmatia.
Modern knowledge of Bosnia in the western Balkans during the Dark Ages is patchy. Upon the looter invasions by the Avars and Slavs from 6th-9th century, bringing Slavic languages, both probably gave way to feudalism only with the might by the Frankish penetrating into the region in the late 9th century (Bosnia probably originated as one such pre-feudal entity). It was also around this time that the Bosnians were Christianized. Bosnia, due to its geographic position and terrain, was probably one of the last areas to go through this process, which presumably originated from the urban centers along the Dalmatian coast.
Banate of Bosnia.
It is only from the 9th century that Frankish and Byzantine sources begin to mention early Slavic polities in the region. In this regard, the earliest widely acknowledged reference to Bosnia dates from the 10th century "De Administrando Imperio" written by Byzantine emperor Constantine Porphyrogenitus, during which period Bosnia is briefly a part of the short-lived Serbian state of Časlav, after whose death in battle in about 960, much of Bosnia finds itself briefly incorporated into the Croatian state of Krešimir II. Shortly thereafter, in 997, Samuel of Bulgaria marches through Bosnia and asserts his over-lordship in parts of it, however, only to be defeated by the Byzantine Empire in 1018 which annexes Bulgaria and asserts its suzerainty in Bosnia. This lasted until later in the century when some parts of Bosnia are briefly incorporated into Croatia and others into Duklja from which the latter Bosnia appears to have seceded in about 1101, upon which Bosnia's bans tried to rule for themselves. However, they all too often found themselves in a tug-of-war between Hungary and the Byzantine Empire. In the year of 1137, Hungary annexes most of Bosnia, then briefly losing her in 1167 to the Byzantine Empire before regaining her in 1180. Thus, prior to 1180 and the reign of Ban Kulin parts of Bosnia were briefly found in Serb or Croat units, but neither neighbor had held the Bosnians long enough to acquire their loyalty or to impose any serious claim to Bosnia.
The first recorded Ban (viceroy) was Ban Borić, vassal to the Hungarian king. However, he was deposed when he backed the loser in a succession crisis over the Hungarian throne. In 1167, Byzantium reconquered Bosnia and eventually emplaced their own vassal as Ban – the native Ban Kulin (r. 1180-1204). However, this vassalage was largely nominal, and Bosnia had for all practical purposes made itself into an independent state under Kulin. Ban Kulin presided over nearly three decades of peace and stability during which he strengthened the country's economy through treaties with Dubrovnik and Venice. His rule also marked the start of a controversy with the Bosnian Church, an indigenous Christian sect considered heretical by both the Roman Catholic and Eastern Orthodox churches. In response to Hungarian attempts to use church politics regarding the issue as a way to reclaim sovereignty over Bosnia, Kulin held a council of local church leaders to renounce the heresy in 1203. Despite this, Hungarian ambitions remained unchanged long after Kulin's death in 1204, waning only after an unsuccessful invasion in 1254, which also fostered the schism of the Bosnian Church.
Kingdom of Bosnia.
Bosnian history from then until the early 14th century was marked by the power struggle between the Šubić and Kotromanić families. This conflict came to an end in 1322, when Stjepan II Kotromanić became ban. By the time of his death in 1353, he had succeeded in annexing territories to the north and west, as well as Zahumlje and parts of Dalmatia. He was succeeded by his nephew Tvrtko who, following a prolonged struggle with nobility and inter-family strife, gained full control of the country in 1367. Under Tvrtko, Bosnia grew in both size and power, finally becoming an independent kingdom in 1377. Following his death in 1391 however, Bosnia fell into a long period of decline. The Ottoman Empire had already started its conquest of Europe and posed a major threat to the Balkans throughout the first half of the 15th century. Finally, after decades of political and social instability, Bosnia officially fell in 1463, while resistance was active and fierce for a few more centuries. Southern regions of Bosnia, nowadays known as "Herzegovina" would follow in 1483, with a Hungarian-backed reinstated "Bosnian Kingdom" being the last to succumb in 1527.
Ottoman Era (1463–1878).
The Ottoman conquest of Bosnia marked a new era in the country's history and introduced tremendous changes in the political and cultural landscape of the region. Although the kingdom had been crushed and its high nobility executed, the Ottomans nonetheless allowed for the preservation of Bosnia's identity by incorporating it as an integral province of the Ottoman Empire with its historical name and territorial integrity - a unique case among subjugated states in the Balkans. Within this sandžak (and eventual vilayet) of Bosnia, the Ottomans introduced a number of key changes in the territory's socio-political administration; including a new landholding system, a reorganization of administrative units, and a complex system of social differentiation by class and religious affiliation.
The four centuries of Ottoman rule also had a drastic impact on Bosnia's population make-up, which changed several times as a result of the empire's conquests, frequent wars with European powers, migrations, and epidemics. A native Slavic-speaking Muslim community emerged and eventually became the largest ethno-religious group (mainly as a result of a gradually rising number of conversions to Islam), while a significant number of Sephardi Jews arrived following their expulsion from Spain in the late 15th century. The Bosnian Christian communities also experienced major changes. The Bosnian Franciscans (and the Catholic population as a whole) were protected by official imperial decree, although on the ground these guarantees were often disregarded and their numbers dwindled. The Orthodox community in Bosnia, initially confined to Herzegovina and southeastern Bosnia, spread throughout the country during this period and went on to experience relative prosperity until the 19th century. Meanwhile, the native schismatic Bosnian Church disappeared altogether.
As the Ottoman Empire thrived and expanded into Central Europe, Bosnia was relieved of the pressures of being a frontier province and experienced a prolonged period of general welfare and prosperity. A number of cities, such as Sarajevo and Mostar, were established and grew into major regional centers of trade and urban culture. Within these cities, various Sultans and governors financed the construction of many important works of Bosnian architecture (such as the Stari most and Gazi Husrev-beg's Mosque). Furthermore, numerous Bosnians played influential roles in the Ottoman Empire's cultural and political history during this time. Bosnian soldiers formed a large component of the Ottoman ranks in the battles of Mohács and Krbava field, two decisive military victories, while numerous other Bosnians rose through the ranks of the Ottoman military bureaucracy to occupy the highest positions of power in the Empire, including admirals, generals, and grand viziers. Many Bosnians also made a lasting impression on Ottoman culture, emerging as mystics, scholars, and celebrated poets in the Turkish, Arabic, and Persian languages.
By the late 17th century, however, the Ottoman Empire's military misfortunes caught up with the country, and the conclusion of the Great Turkish War with the Treaty of Karlowitz in 1699 once again made Bosnia the empire's westernmost province. But they allowed some of the Bosnian tribes to immigrate into the Arabian countries (Palestine, Jordan). The following hundred years were marked by further military failures, numerous revolts within Bosnia, and several outbursts of plague. The Porte's efforts at modernizing the Ottoman state were met with great hostility in Bosnia, where local aristocrats stood to lose much through the proposed reforms. This, combined with frustrations over political concessions to nascent Christian states in the east, culminated in a famous (albeit ultimately unsuccessful) revolt by Husein Gradaščević in 1831. Related rebellions would be extinguished by 1850, but the situation continued to deteriorate. Later, agrarian unrest eventually sparked the Herzegovinian rebellion, a widespread peasant uprising, in 1875. The conflict rapidly spread and came to involve several Balkan states and Great Powers, which eventually forced the Ottomans to cede administration of the country to Austria-Hungary through the Treaty of Berlin in 1878.
Occupation by Austria-Hungary (1878–1918).
Though an Austria-Hungary military force quickly subjugated initial armed resistance upon take-over, tensions remained in certain parts of the country (particularly Herzegovina) and a mass emigration of predominantly Muslim dissidents occurred. However, a state of relative stability was reached soon enough and Austro-Hungarian authorities were able to embark on a number of social and administrative reforms which intended to make Bosnia and Herzegovina into a "model colony". With the aim of establishing the province as a stable political model that would help dissipate rising South Slav nationalism, Habsburg rule did much to codify laws, to introduce new political practices, and generally to provide for modernization.
Although successful economically, Austro-Hungarian policy - which focused on advocating the ideal of a pluralist and multi-confessional Bosnian nation (largely favored by the Muslims) - failed to curb the rising tides of nationalism. The concept of Croat and Serb nationhood had already spread to Bosnia and Herzegovina's Catholics and Orthodox communities from neighboring Croatia and Serbia in the mid 19th century, and was too well-entrenched to allow for the widespread acceptance of a parallel idea of Bosnian nationhood. By the latter half of the 1910s, nationalism was an integral factor of Bosnian politics, with national political parties corresponding to the three groups dominating elections.
The idea of a unified South Slavic state (typically expected to be spearheaded by independent Serbia) became a popular political ideology in the region at this time, including in Bosnia and Herzegovina. The Austro-Hungarian government's decision to formally annex Bosnia-Herzegovina in 1908 (the Bosnian Crisis) added to a sense of urgency among these nationalists. The political tensions caused by all this culminated on 28 June 1914, when Serb nationalist youth Gavrilo Princip assassinated the heir to the Austro-Hungarian throne, Archduke Franz Ferdinand, in Sarajevo; an event that proved to be the spark that set off World War I. Although 10% of the Bosniak population died serving in the armies or being killed by the various warring states, Bosnia and Herzegovina itself managed to escape the conflict relatively unscathed.
Kingdom of Yugoslavia (1918–41).
Following World War I, Bosnia was incorporated into the South Slav kingdom of Serbs, Croats and Slovenes (soon renamed Yugoslavia). 
Political life in Bosnia at this time was marked by two major trends: social and economic unrest over the Agrarian Reform of 1918–19 manifested through mass colonization and property confiscation; also formation of several political parties that frequently changed coalitions and alliances with parties in other Yugoslav regions. The dominant ideological conflict of the Yugoslav state, between Croatian regionalism and Serbian centralization, was approached differently by Bosnia's major ethnic groups and was dependent on the overall political atmosphere. Although the initial split of the country into 33 oblasts erased the presence of traditional geographic entities from the map, the efforts of Bosnian politicians such as Mehmed Spaho ensured that the six oblasts carved up from Bosnia and Herzegovina corresponded to the six sanjaks from Ottoman times and, thus, matched the country's traditional boundary as a whole.
The establishment of the Kingdom of Yugoslavia in 1929, however, brought the redrawing of administrative regions into banates that purposely avoided all historical and ethnic lines, removing any trace of a Bosnian entity. Serbo-Croat tensions over the structuring of the Yugoslav state continued, with the concept of a separate Bosnian division receiving little or no consideration. The famous Cvetković-Maček agreement that created the Croatian banate in 1939 encouraged what was essentially a partition of Bosnia between Croatia and Serbia. However, outside political circumstances forced Yugoslav politicians to shift their attention to the rising threat posed by Adolf Hitler's Nazi Germany. Following a period that saw attempts at appeasement, the joining of the Tripartite Pact, and a coup d'état, Yugoslavia was finally invaded by Germany on 6 April 1941.
World War II (1941–1945).
Once the kingdom of Yugoslavia was conquered by Nazi forces in World War II, all of Bosnia was ceded to the Independent State of Croatia (NDH). The NDH rule over Bosnia led to widespread persecution and genocide. The Jewish population was nearly exterminated. Hundreds of thousands of Serbs died either in Ustaše concentration camps or in widespread mass killings by Ustaše militia. Many Serbs themselves took up arms and joined the Chetniks, a Serb nationalist movement with the aim of establishing an ethnically homogeneous 'Greater Serbian' state.
The Chetniks were responsible for widespread persecution and murder of non-Serbs and communist sympathizers, with the Muslim population of Bosnia, Herzegovina and Sandžak being a primary target. Once captured, Muslim villages were systematically massacred by the Chetniks. The total estimate of Muslims killed by Chetniks is between 80,000 and 100,000, most likely about 86,000 or 6.7 percent of their population (8.1 percent in Bosnia and Herzegovina alone). Several Bosnian Muslim paramilitary units joined the NDH forces to counter their own persecution in the hands of the Serbs in Bosnia. On 12 October 1941 a group of 108 notable Muslim citizens of Sarajevo signed the Resolution of Sarajevo Muslims by which they condemned the persecution of Serbs organized by Ustaše, made distinction between Muslims who participated in such persecutions and the wider Muslim population, presented information about the persecutions of Muslims by Serbs and requested security for all citizens of the country, regardless of their identity. According to the US Holocaust Museum, 320,000-340,000 ethnic Serbs were murdered. According to the Yad Vashem Holocaust Museum and Research Center, "More than 500,000 Serbs were murdered in horribly sadistic ways, 250,000 were expelled, and another 200,000 were forced to convert" during WWII in the Independent State of Croatia (modern day Croatia and Bosnia).
Starting in 1941, Yugoslav communists under the leadership of Josip Broz Tito organized their own multi-ethnic resistance group, the Partisans, who fought against Axis, Ustaše, and Chetnik forces. They too, committed numerous atrocities, mainly against political opponents of all ethnicities. Some Bosnian Muslims joined the SS Handschar division, an SS division of the Nazis that pledged allegiance to both Adolf Hitler and NDH leader, Ante Pavelić. The division was the first SS division which was constituted of non-Germans. On 25 November 1943 the Anti-Fascist Council of National Liberation of Yugoslavia with Tito at its helm held a founding conference in Jajce where Bosnia and Herzegovina was reestablished as a republic within the Yugoslavian federation in its Ottoman borders. Military success eventually prompted the Allies to support the Partisans. On 6 April 1945 Sarajevo was captured by the Partisans. The end of the war resulted in the establishment of the Federal People's Republic of Yugoslavia, with the constitution of 1946 officially making Bosnia and Herzegovina one of six constituent republics in the new state.
Socialist Yugoslavia (1945–1992).
Because of its central geographic position within the Yugoslavian federation, post-war Bosnia was strategically selected as a base for the development of the military defense industry. This contributed to a large concentration of arms and military personnel in Bosnia, a significant factor in the war that followed the breakup of Yugoslavia in the 1990s. However, Bosnia's existence within Yugoslavia was, for the most part, peaceful and prosperous. While it was one of the poorer republics in the early 1950s, Bosnia's economy recovered quickly, as its extensive natural resources were exploited to stimulate industrial development. The Yugoslavian communist doctrine of "brotherhood and unity" particularly suited Bosnia's diverse and multi-ethnic society that, because of such an imposed system of tolerance, thrived culturally and socially.
Though considered a political backwater of the federation for much of the 1950s and 1960s, the 1970s saw the ascension of a strong Bosnian political elite. While working within the communist system, politicians such as Džemal Bijedić, Branko Mikulić and Hamdija Pozderac reinforced and protected the sovereignty of Bosnia and Herzegovina Their efforts proved key during the turbulent period following Tito's death in 1980, and are today considered some of the early steps towards Bosnian independence. However, the republic could not escape the increasingly nationalistic climate of the time unscathed. With the fall of communism and the start of the breakup of Yugoslavia, the old communist doctrine of tolerance began to lose its potency, creating an opportunity for nationalist elements in the society to spread their influence.
Bosnian War (1992–1995).
The first multi-party parliamentary elections held on 18 and 25 November 1990 led to a national assembly dominated by three ethnically-based parties, which had formed a loose coalition to oust the communists from power. Croatia and Slovenia's subsequent declarations of independence and the warfare that ensued placed Bosnia and Herzegovina and its three constituent peoples in an awkward position. A significant split soon developed on the issue of whether to stay with the Yugoslav federation (overwhelmingly favored among Serbs) or seek independence (overwhelmingly favored among Bosniaks and Croats). A declaration of sovereignty on 15 October 1991 was followed by a referendum for independence from Yugoslavia on 29 February and 1 March 1992. The referendum was boycotted by the great majority of Bosnian Serbs, so with a voter turnout of 64%, 98% of which voted in favor of the proposal. Bosnia and Herzegovina became an independent state on 3 March 1992.
While the first casualty of the war is debated, significant Serbian offensives began in March 1992 in Eastern and Northern Bosnia. Following a tense period of escalating tensions the opening shots in the incipient Bosnian conflict were fired when Serb paramilitary forces attacked Bosniak villages around Čapljina on 7 March 1992 and around Bosanski Brod and Goražde on 15 March. These minor attacks were followed by much more serious Serb artillery attacks on Neum on 19 March and on Bosanski Brod on 24 March. The killing of a Bosniak civilian woman on 5 April 1992 by a sniper, while she was demonstrating in Sarajevo against the raising of barricades by Bosnian Serbs, is widely regarded as marking the start of warfare between the three major communities. Open warfare began in Sarajevo on 6 April.
International recognition of Bosnia and Herzegovina meant that the Yugoslav People's Army (JNA) officially withdrew from the republic's territory, although their Bosnian Serb members merely joined the Army of Republika Srpska. Armed and equipped from JNA stockpiles in Bosnia, supported by volunteers, Republika Srpska's offensives in 1992 managed to place much of the country under its control. By 1993, when an armed conflict erupted between the Sarajevo government and the Croat statelet of Herzeg-Bosnia, about 70% of the country was controlled by the Serbs.
In March 1994, the signing of the Washington accords between the Bosniak and ethnic-Croatian leaders led to the creation of a joint Bosniak-Croat Federation of Bosnia and Herzegovina. This, along with international outrage at Serb war crimes and atrocities (most notably the Srebrenica massacre of as many as 8,000 Bosniak males in July 1995) helped turn the tide of war. The signing of the Dayton Agreement in Paris by the presidents of Bosnia and Herzegovina (Alija Izetbegović), Croatia (Franjo Tuđman), and Yugoslavia (Slobodan Milošević) brought a halt to the fighting, roughly establishing the basic structure of the present-day state. The three years of war and bloodshed had left between 90,000 and 110,000 people killed and more than 2 million displaced.
Independent Bosnia and Herzegovina (1995–present).
Since its 1992 independence and the 1995 Constitutional framework of the Dayton Agreement, Bosnia and Herzegovina has followed a path of state-building, while remaining under final international supervision through the figure of the High Representative for Bosnia and Herzegovina. Bosnia and Herzegovina is a confederation of two "Entities" - the Federation of Bosnia and Herzegovina and the Republika Srpska, as well as the district of Brčko. Each of the Entities has its own constitution and extensive legislative powers.
Bosnia and Herzegovina is a potential candidate country for accession into the EU; an EU-BiH Stabilization and Association Agreement was signed in 2008. Its accession to NATO is in the negotiation phase, and a Membership Action Plan was signed in April 2010.
On 15 November 2015 Republika Srpska will start a Judiciary question referendum which will be part of a plan for Republika Srpska independence. There are speculations that this could start new conflicts which could lead to a new armed conflict.

</doc>
<doc id="42400" url="https://en.wikipedia.org/wiki?curid=42400" title="Socialization">
Socialization

Socialization, also spelled socialisation, is a term used by sociologists, social psychologists, anthropologists, political scientists, and educationalists to refer to the lifelong process of inheriting and disseminating norms, customs, values and ideologies, providing an individual with the skills and habits necessary for participating within their own society. Socialization is thus "the means by which social and cultural continuity are attained".
Socialization describes a process which may lead to desirable outcomes—sometimes labeled "moral"—as regards the society where it occurs. Individual views on certain issues, for instance race or economics, are influenced by the society's consensus and usually tend toward what that society finds acceptable or "normal". Many socio-political theories postulate that socialization provides only a partial explanation for human beliefs and behaviors, maintaining that agents are not blank slates predetermined by their environment; scientific research provides evidence that people are shaped by both social influences and genes. Genetic studies have shown that a person's environment interacts with his or her genotype to influence behavioral outcomes.
Theories.
Socialization is the process by which human infants begin to acquire the skills necessary to perform as a functioning member of their society, and is the most influential learning process one can experience. Unlike many other living species, whose behavior is biologically set, humans need social experiences to learn their culture and to survive. Although cultural variability manifests in the actions, customs, and behaviors of whole social groups (societies), the most fundamental expression of culture is found at the individual level. This expression can only occur after an individual has been socialized by his or her parents, family, extended family, and extended social networks. This reflexive process of both learning and teaching is how cultural and social characteristics attain continuity. Many scientists say socialization essentially represents the whole process of learning throughout the life course and is a central influence on the behavior, beliefs, and actions of adults as well as of children.
Klaus Hurrelmann.
From the late 1980s, sociological and psychological theories have been connected with the term socialization. One example of this connection is the theory of Klaus Hurrelmann. In his book ""Social Structure and Personality Development"" (Hurrelmann 1989/2009), he develops "The "Model of Productive Processing of Reality" (PPR)." The core idea is that socialization refers to an individual's personality development. It is the result of the productive processing of interior and exterior realities. Bodily and mental qualities and traits constitute a person's inner reality; the circumstances of the social and physical environment embody the external reality. Reality processing is productive because human beings actively grapple with their lives and attempt to cope with the attendant developmental tasks. The success of such a process depends on the personal and social resources available. Incorporated within all developmental tasks is the necessity to reconcile personal individuation and social integration and so secure the "I-dentity." (Hurrelmann1989/2009: 42)
Lawrence Kohlberg.
Lawrence Kohlberg studied moral reasoning and developed a theory of the stages of moral development (how individuals reason situations as right from wrong). The first stage is the pre-conventional stage, where a person (typically children) experience the world in terms of pain and pleasure, with their moral decisions solely reflecting this experience. Second, the conventional stage (typical for adolescents and adults) is characterized by an acceptance of society's conventions concerning right and wrong, even when there are no consequences for obedience or disobedience. Finally, the post-conventional stage (more rarely achieved) occurs if a person moves beyond society's norms to consider abstract ethical principles when making moral decisions.
Carol Gilligan.
Carol Gilligan compared the moral development of girls and boys in her theory of gender and moral development. She claimed (1982, 1990) that boys have a justice perspective meaning that they rely on formal rules to define right and wrong. Girls, on the other hand, have a care and responsibility perspective where personal relationships are considered when judging a situation. Gilligan also studied the effect of gender on self-esteem. She claimed that society's socialization of females is the reason why girls' self-esteem diminishes as they grow older. Girls struggle to regain their personal strength when moving through adolescence as they have fewer female teachers and most authority figures are men.
Erik H. Erikson.
Erik H. Erikson (1902–1994) explained the challenges throughout the life course. The first stage in the life course is infancy, where babies learn trust and mistrust. The second stage is
toddlerhood where children around the age of two struggle with the challenge of autonomy versus doubt. In stage three, preschool, children struggle to understand the difference between initiative and guilt. Stage four, pre-adolescence, children learn about industriousness and inferiority. In the fifth stage called adolescence, teenagers experience the challenge of gaining identity versus confusion. The sixth stage, young adulthood, is when young people gain insight to life when dealing with the challenge of intimacy and isolation. In stage seven, or middle adulthood, people experience the challenge of trying to make a difference (versus self-absorption). In the final stage, stage eight or old age, people are still learning about the challenge of integrity and despair.
George Herbert Mead.
George Herbert Mead (1863–1931) developed a theory of social behaviorism to explain how social experience develops an individual's self-concept. Mead's central concept is the self: It is composed of self-awareness and self-image. Mead claimed that the self is not there at birth, rather, it is developed with social experience. Since social experience is the exchange of symbols, people tend to find meaning in every action. Seeking meaning leads us to imagine the intention of others. Understanding intention requires imagining the situation from the others' point of view. In effect, others are a mirror in which we can see ourselves. Charles Horton Cooley (1902-1983) coined the term looking glass self, which means self-image based on how we think others see us. According to Mead the key to developing the self is learning to take the role of the other. With limited social experience, infants can only develop a sense of identity through imitation. Gradually children learn to take the roles of several others. The final stage is the generalized other, which refers to widespread cultural norms and values we use as a reference for evaluating others.
Judith R. Harris.
Judith R. Harris's (b. 1938) proposed theory of group socialization (GS theory) states that a child’s adult personality is determined by childhood and adolescent peer groups outside of the home environment and that “parental behaviors have no effect on the psychological characteristics their children will have as adults.” Harris proposes this theory based on behavioral genetics, sociological views of group processes, context-specific learning, and evolutionary theory. While Harris proposed this theory, she attributes the original idea to Eleanor E. Maccoby and John A. Martin both of whom are doctors and wrote the chapter on family socialization found in the fourth edition of The "Handbook of Child Psychology". After extensively reviewing the research conducted on parent-child interactions, Maccoby and Martin (1983) state that their findings suggest that parental behavior and the home environment has either no effect on the social development of children, or the effect varies significantly between children.
Behavioral genetics suggest that up to fifty percent of the variance in adult personality is due to genetic differences. The environment in which a child is raised accounts for only approximately ten percent in the variance of an adult’s personality. As much as twenty percent of the variance is due to measurement error. This suggests that only a very small part of an adult’s personality is influenced by factors parents control (i.e. the home environment). Harris claims that while it’s true that siblings don’t have identical experiences in the home environment (making it difficult to associate a definite figure to the variance of personality due to home environments), the variance found by current methods is so low that researchers should look elsewhere to try to account for the remaining variance.
Harris also states that developing long-term personality characteristics away from the home environment would be evolutionarily beneficial because future success is more likely to depend on interactions with peers than interactions with parents and siblings. Also, because of already existing genetic similarities with parents, developing personalities outside of childhood home environments would further diversify individuals, increasing their evolutionary success.
Language Socialization.
Based on comparative research in different societies, focusing on the role of language in child development, linguistic anthropologists Elinor Ochs and Bambi Schieffelin have developed the theory of language socialization. 
They discovered that the processes of enculturation and socialization do not occur apart from the process of language acquisition, but that children acquire language and culture together in what amounts to an integrated process. Members of all societies socialize children both "to" and "through" the use of language; acquiring competence in a language, the novice is by the same token socialized into the categories and norms of the culture, while the culture, in turn, provides the norms of the use of language.
Stages.
Richard Moreland and John Levine (1982) created a model of group socialization based upon the assumption that individuals and groups change their evaluations and commitments to each other over time. Since these changes happen in all groups, Moreland and Levine speculate that there is a predictable sequence of stages that occur in order for an individual to transition through a group.
Moreland and Levine identify five stages of socialization which mark this transition; investigation, socialization, maintenance, resocialization, and remembrance. During each stage, the individual and the group evaluate each other which leads to an increase or decrease in commitment to socialization. This socialization pushes the individual from prospective, new, full, marginal, and ex member.
Stage 1: Investigation
This stage is marked by a cautious search for information. The individual compares groups in order to determine which one will fulfill their needs ("reconnaissance"), while the group estimates the value of the potential member ("recruitment"). The end of this stage is marked by entry to the group, whereby the group asks the individual to join and they accept the offer.
Stage 2: Socialization
Now that the individual has moved from prospective member to new member, they must accept the group’s culture. At this stage, the individual accepts the group’s norms, values, and perspectives ("assimilation"), and the group adapts to fit the new member’s needs ("accommodation"). The acceptance transition point is then reached and the individual becomes a full member. However, this transition can be delayed if the individual or the group reacts negatively. For example, the individual may react cautiously or misinterpret other members’ reactions if they believe that they will be treated differently as a new comer.
Stage 3: Maintenance 
During this stage, the individual and the group negotiate what contribution is expected of members (role negotiation). While many members remain in this stage until the end of their membership, some individuals are not satisfied with their role in the group or fail to meet the group’s expectations ("divergence").
Stage 4: Resocialization
-If the divergence point is reached, the former full member takes on the role of a marginal member and must be resocialized. There are two possible outcomes of resocialization: differences are resolved and the individual becomes a full member again ("convergence"), or the group expels the individual or the individual decides to leave ("exit").
Stage 5: Remembrance 
In this stage, former members reminisce about their memories of the group, and make sense of their recent departure. If the group reaches a consensus on their reasons for departure, conclusions about the overall experience of the group become part of the group’s "tradition".
Types.
Primary socialization for a child is very important because it sets the ground work for all future socialization. Primary Socialization occurs when a child learns the attitudes, values, and actions appropriate to individuals as members of a particular culture. It is mainly influenced by the immediate family and friends. For example, if a child saw his/her mother expressing a discriminatory opinion about a minority group, then that child may think this behavior is acceptable and could continue to have this opinion about minority groups.
Secondary socialization
Secondary socialization refers to the process of learning what is the appropriate behavior as a member of a smaller group within the larger society. Basically, it is the behavioral patterns reinforced by socializing agents of society. Secondary socialization takes place outside the home. It is where children and adults learn how to act in a way that is appropriate for the situations they are in. Schools require very different behavior from the home, and Children must act according to new rules. New teachers have to act in a way that is different from pupils and learn the new rules from people around them. Secondary Socialization is usually associated with teenagers and adults, and involves smaller changes than those occurring in primary socialization. Such examples of Secondary Socialization are entering a new profession or relocating to a new environment or society.
Anticipatory socialization 
Anticipatory socialization refers to the processes of socialization in which a person "rehearses" for future positions, occupations, and social relationships. For example, a couple might move in together before getting married in order to try out, or anticipate, what living together will be like. Research by Kenneth J. Levine and Cynthia A. Hoffner suggests that parents are the main source of anticipatory socialization in regards to jobs and careers.
Re-socialization refers to the process of discarding former behavior patterns and reflexes, accepting new ones as part of a transition in one's life. This occurs throughout the human life cycle. Re-socialization can be an intense experience, with the individual experiencing a sharp break with his or her past, as well as a need to learn and be exposed to radically different norms and values. One common example involves re-socialization through a total institution, or "a setting in which people are isolated from the rest of society and manipulated by an administrative staff". Re-socialization via total institutions involves a two step process: 1) the staff work to root out a new inmate's individual identity & 2) the staff attempt to create for the inmate a new identity. Other examples of this are the experience of a young man or woman leaving home to join the military, or a religious convert internalizing the beliefs and rituals of a new faith. An extreme example would be the process by which a transsexual learns to function socially in a dramatically altered gender role.
Organizational socialization is the process whereby an employee learns the knowledge and skills necessary to assume his or her organizational role. As newcomers become socialized, they learn about the organization and its history, values, jargon, culture, and procedures. This acquired knowledge about new employees' future work environment affects the way they are able to apply their skills and abilities to their jobs. How actively engaged the employees are in pursuing knowledge affects their socialization process. They also learn about their work group, the specific people they work with on a daily basis, their own role in the organization, the skills needed to do their job, and both formal procedures and informal norms. Socialization functions as a control system in that newcomers learn to internalize and obey organizational values and practices.
Group socialization
Group socialization is the theory that an individual's peer groups, rather than parental figures, influences his or her personality and behavior in adulthood. Adolescents spend more time with peers than with parents. Therefore, peer groups have stronger correlations with personality development than parental figures do. For example, twin brothers, whose genetic makeup are identical, will differ in personality because they have different groups of friends, not necessarily because their parents raised them differently.
Entering high school is a crucial moment in many adolescent's lifespan involving the branching off from the restraints of their parents. When dealing with new life challenges, adolescents take comfort in discussing these issues within their peer groups instead of their parents. Peter Grier, staff writer of the Christian Science Monitor describes this occurrence as,"Call it the benign side of peer pressure. Today's high-schoolers operate in groups that play the role of nag and nanny-in ways that are both beneficial and isolating."
Gender socialization
Henslin (1999:76) contends that "an important part of socialization is the learning of culturally defined gender roles." Gender socialization refers to the learning of behavior and attitudes considered appropriate for a given sex. Boys learn to be boys and girls learn to be girls. This "learning" happens by way of many different agents of socialization. The family is certainly important in reinforcing gender roles, but so are one’s friends, school, work and the mass media. Gender roles are reinforced through "countless subtle and not so subtle ways" (1999:76).
As parents are present in a child's life from the beginning, their influence in a child's early socialization is very important, especially in regards to gender roles. Sociologists have identified four ways in which parents socialize gender roles in their children: Shaping gender related attributes through toys and activities, differing their interaction with children based on the sex of the child, serving as primary gender models, and communicating gender ideals and expectations.
Racial socialization
Racial socialization has been defined as "the developmental processes by which children acquire the behaviors, perceptions, values, and attitudes of an ethnic group, and come to see themselves and others as members of the group". The existing literature conceptualizes racial socialization as having multiple dimensions. Researchers have identified five dimensions that commonly appear in the racial socialization literature: cultural socialization, preparation for bias, promotion of mistrust, egalitarianism, and other. Cultural socialization refers to parenting practices that teach children about their racial history or heritage and is sometimes referred to as pride development. Preparation for bias refers to parenting practices focused on preparing children to be aware of, and cope with, discrimination. Promotion of mistrust refers to the parenting practices of socializing children to be wary of people from other races. Egalitarianism refers to socializing children with the belief that all people are equal and should be treated with a common humanity.
Planned socialization
Planned socialization occurs when other people take actions designed to teach or train others—from infancy on.
Natural Socialization
Natural socialization occurs when infants and youngsters explore, play and discover the social world around them. Natural socialization is easily seen when looking at the young of almost any mammalian species (and some birds). Planned socialization is mostly a human phenomenon; and all through history, people have been making plans for teaching or training others. Both natural and planned socialization can have good and bad features: It is wise to learn the best features of both natural and planned socialization and weave them into our lives.
Positive socialization
Positive socialization is the type of social learning that is based on pleasurable and exciting experiences. We tend to like the people who fill our social learning processes with positive motivation, loving care, and rewarding opportunities.
Negative socialization
Negative socialization occurs when others use punishment, harsh criticisms or anger to try to "teach us a lesson;" and often we come to dislike both negative socialization and the people who impose it on us. There are all types of mixes of positive and negative socialization; and the more positive social learning experiences we have, the happier we tend to be—especially if we learn useful information that helps us cope well with the challenges of life. A high ratio of negative to positive socialization can make a person unhappy, defeated or pessimistic about life.
Social institutions.
In the social sciences, institutions are the structures and mechanisms of social order and cooperation governing the behavior of a set of individuals within a given human collectivity. Institutions are identified with a social purpose and permanence, transcending individual human lives and intentions, and with the making and enforcing of rules governing cooperative human behavior. Types of institution include:
Some sociologists and theorists of culture have recognized the power of mass communication as a socialization device. Denis McQuail recognizes the argument:
Other uses.
To "socialize" may also mean simply to associate or mingle with people socially. In American English, "socialized" has come to refer, usually in a pejorative sense, to the ownership structure of socialism or to the expansion of the welfare state. Traditionally, socialists and Marxists both used the term "socialization of industry" to refer to the reorganization of institutions so that the workers are all owners (cooperatives) and to refer to the implementation of workplace democracy.

</doc>
<doc id="42402" url="https://en.wikipedia.org/wiki?curid=42402" title="Persecution of homosexuals in Nazi Germany and the Holocaust">
Persecution of homosexuals in Nazi Germany and the Holocaust

Upon the rise of Adolf Hitler and the National Socialist German Workers Party (the Nazi Party) in Germany, gay men and, to a lesser extent, lesbians, were two of the numerous groups targeted by the Nazis and were ultimately among Holocaust victims. Beginning in 1933, gay organizations were banned, scholarly books about homosexuality, and sexuality in general, (such as those from the Institut für Sexualwissenschaft, run by Jewish gay rights campaigner Magnus Hirschfeld) were burned, and homosexuals within the Nazi Party itself were murdered. The Gestapo compiled lists of homosexuals, who were compelled to sexually conform to the "German norm."
Between 1933 and 1945, an estimated 100,000 men were arrested as homosexuals, of whom some 50,000 were officially sentenced. Most of these men served time in regular prisons, and an estimated 5,000 to 15,000 of those sentenced were incarcerated in Nazi concentration camps. It is unclear how many of the 5,000 to 15,000 eventually perished in the camps, but leading scholar Rüdiger Lautmann believes that the death rate of homosexuals in concentration camps may have been as high as 60%. Homosexuals in the camps were treated in an unusually cruel manner by their captors.
After the war, the treatment of homosexuals in concentration camps went unacknowledged by most countries, and some men were even re-arrested and imprisoned based on evidence found during the Nazi years. It was not until the 1980s that governments began to acknowledge this episode, and not until 2002 that the German government apologized to the gay community. This period still provokes controversy, however. In 2005, the European Parliament adopted a resolution on the Holocaust which included the persecution of homosexuals.
Purge.
In late February 1933, as the moderating influence of Ernst Röhm weakened, the Nazi Party launched its purge of homosexual (gay, lesbian, and bisexual; then known as homophile) clubs in Berlin, outlawed sex publications, and banned organized gay groups. As a consequence, many fled Germany (e.g., Erika Mann, Richard Plant).
In March 1933, Kurt Hiller, the main organizer of Magnus Hirschfeld's Institute of Sex Research, was sent to a concentration camp.
On May 6, 1933, Nazi Youth of the Deutsche Studentenschaft made an organized attack on the Institute of Sex Research. A few days later on May 10, the Institute's library and archives were publicly hauled out and burned in the streets of the opernplatz. Around 20,000 books and journals, and 5,000 images, were destroyed. Also seized were the Institute's extensive lists of names and addresses of homosexuals. In the midst of the burning, Joseph Goebbels gave a political speech to a crowd of around 40,000 people. 
Hitler initially protected Röhm from other elements of the Nazi Party which held his homosexuality to be a violation of the party's strong anti-gay policy. However, Hitler later changed course when he perceived Röhm to be a potential threat to his power. During the Night of the Long Knives in 1934, a purge of those whom Hitler deemed threats to his power took place, he had Röhm murdered and used Röhm's homosexuality as a justification to suppress outrage within the ranks of the SA. After solidifying his power, Hitler would include gay men among those sent to concentration camps during the Holocaust.
Heinrich Himmler had initially been a supporter of Röhm, arguing that the charges of homosexuality against him were manufactured by Jews. But after the purge, Hitler elevated Himmler's status and he became very active in the suppression of homosexuality. He exclaimed, "We must exterminate these people root and branch... the homosexual must be eliminated."
Shortly after the purge in 1934, a special division of the Gestapo was instituted to compile lists of gay individuals. In 1936, Himmler created the "Reichszentrale zur Bekämpfung der Homosexualität und Abtreibung" (Reich Central Office for the Combating of Homosexuality and Abortion).
Nazi Germany thought of German gay men as against the plan of creating a "master race" and sought to force them into sexual and social conformity. Gay men who would not change or feign a change in their sexual orientation were sent to concentration camps under the "Extermination Through Work" campaign.
More than one million gay Germans were targeted, of whom at least 100,000 were arrested and 50,000 were serving prison terms as "convicted homosexuals". Hundreds of European gay men living under Nazi occupation were castrated under court order.
Some persecuted under these laws would not have identified themselves as gay. Such "anti-homosexual" laws were widespread throughout the western world until the 1960s and 1970s, so many gay men did not feel safe to come forward with their stories until the 1970s when many so-called "sodomy laws" were repealed.
Lesbians were not widely persecuted under Nazi anti-gay laws, as it was considered easier to persuade or force them to comply with accepted heterosexual behavior. However, they were viewed as a threat to state values.
Definition of homosexuality.
The first event that led towards the fight against homosexuality in Nazi Germany was the unification of the German state in 1871 known as the Second Reich. The new state brought forth a new penal code which included paragraph 175. It read, "An unnatural sex act committed between persons of male sex or by humans with animals is punishable by imprisonment; the loss of civil rights might also be imposed." The law was interpreted differently across the nation until the ruling of a court case on April 23, 1880. The "Reichsgericht" (Imperial Court of Justice) ruled that a criminal homosexual act had to involve either anal, oral, or intercrural sex between two men. Anything less than that was deemed harmless play. The German police force found this new interpretation of paragraph 175 extremely difficult to prove in court since it was hard to find witnesses to these acts. This left the attitude towards homosexuality very relaxed during World War I and early in the rise of the Nazi Party.
After the Night of the Long Knives, the Nazis amended paragraph 175 due to what they saw as loopholes in the law. The most significant change to the law was the change from "An unnatural sex act committed between persons of male sex" to "A male who commits a sex offense with another male." This expanded the reach of the law to persecute gay men. Kissing, mutual masturbation and love-letters between men served as a legitimate reason for the police to make an arrest. Unfortunately for homosexuals, the law never states what a sex offence actually is, leaving it open to subjective interpretation. Men who practiced what was known to be harmless amusement with other men were now being arrested under the law.
Homosexuality and the SS.
According to Geoffrey J. Giles the SS, and its leader Heinrich Himmler, were particularly concerned about homosexuality. More than any other Nazi leader, Himmler's writing and speeches denounced homosexuality. However, despite consistently condemning homosexuals and homosexual activity, Himmler was less consistent in his punishment of homosexuals. In Geoffrey Giles' article "The Denial of Homosexuality: Same-Sex Incidents in Himmler's SS", several cases are put forward where members of the Nazi SS are tried for homosexual offences. On a case by case basis, the outcomes vary widely, and Giles gives documented evidence where the judges could be swayed by evidence demonstrating the accused's "aryan-ness" or "manliness", that is, by describing him as coming from true Germanic stock and perhaps fathering children. Reasons for Himmler's leniency in some cases may derive from the difficulty in defining homosexuality, particularly in a society that glorifies the masculine ideal and brotherhood.
Not only was Himmler's persecution of homosexuals based on this masculine ideal, but it was also driven by societal issues. On February 18, 1937 Himmler gave his most detailed speech on the topic in Bad Tölz. Himmler starts his speech off covering the social aspect of the problem. He reminds his listeners of the number of registered members in homosexual associations. He was not convinced that every homosexual was registered in these clubs, but he was also not convinced everyone registered was a homosexual. Himmler estimated the number of homosexuals from one to two million people, or 7 to 10% of men in Germany. He explained "If this remains the case, it means that our nation (Volk) will be destroyed (lit. ‘go kaput’) by this plague." Adding the number of homosexuals to the number of men that died in the previous war, Himmler estimated that this would equal four million men. If these four million men are no longer capable of having sex with a female, then this 'upsets the balance of the sexes in Germany and is leading to catastrophe.' Apparently, Germany was having population issues with the number of killed men during the First World War. Himmler believed "A people of good race which has too few children has a sure ticket for the grave, for insignificance in fifty to one hundred years, for burial in two hundred and fifty years."
Concentration camps.
Estimates vary widely as to the number of gay men imprisoned in concentration camps during the Holocaust, ranging from 5,000 to 15,000, many of whom died. In addition, records as to the specific reasons for internment are non-existent in many areas, making it hard to put an exact number on exactly how many gay men perished in death camps. See "pink triangle".
Gay men suffered unusually cruel treatment in the concentration camps. They faced persecution not only from German soldiers but also from other prisoners, and many gay men were beaten to death. Additionally, gay men in forced labor camps routinely received more grueling and dangerous work assignments than other non-Jewish inmates, under the policy of "Extermination Through Work". SS soldiers also were known to use gay men for target practice, aiming their weapons at the pink triangles their human targets were forced to wear..
The harsh treatment can be attributed to the view of the SS guards toward gay men, as well as to the homophobic attitudes present in German society at large. The marginalization of gay men in Germany was reflected in the camps. Many died from beatings, some of them inflicted by other prisoners. Nazi doctors often used gay men for scientific experiments in an attempt to locate a "gay gene" to "cure" any future Aryan children who were gay.
Experiences such as these can account for the high death rate of gay men in the camps as compared to the other "asocial" groups. A study by Rüdiger Lautmann found that 60% of gay men in concentration camps died, as compared to 41% for political prisoners and 35% for Jehovah's Witnesses. The study also shows that survival rates for gay men were slightly higher for internees from the middle and upper classes and for married bisexual men and those with children.
Post-War.
Homosexual concentration camp prisoners were not acknowledged as victims of Nazi persecution. Reparations and state pensions available to other groups were refused to gay men, who were still classified as criminals — the 1935 version of Paragraph 175 remained in force in West Germany until 1969 when the "Bundestag" voted to return to the pre-1935 version. Paragraph 175 was not repealed until 1994, although both East and West Germany liberalized their criminal laws against adult homosexuality in the late 1960s.
Holocaust survivors who were homosexual could be re-imprisoned for "repeat offences", and were kept on the modern lists of "sex offenders". Under the Allied Military Government of Germany, some homosexuals were forced to serve out their terms of imprisonment, regardless of the time spent in concentration camps.
The Nazis' anti-gay policies and their destruction of the early gay rights movement were generally not considered suitable subject matter for Holocaust historians and educators. It was not until the 1970s and 1980s that there was some mainstream exploration of the theme, with Holocaust survivors writing their memoirs, plays such as "Bent", and more historical research and documentaries being published about the Nazis' homophobia and their destruction of the German gay-rights movement.
Since the 1980s, some European and international cities have erected memorials to remember the thousands of homosexual people who were murdered and persecuted during the Holocaust. Major memorials can be found in Berlin, Amsterdam (Netherlands), Montevideo (Uruguay), San Francisco (United States of America), Tel Aviv (Israel) and Sydney (Australia). In 2002, the German government issued an official apology to the gay community.
In 2005, the European Parliament marked the 60th anniversary of the liberation of the Auschwitz concentration camp with a minute's silence and the passage of a resolution which included the following text:
...27 January 2005, the sixtieth anniversary of the liberation of Nazi Germany's death camp at Auschwitz-Birkenau, where a combined total of up to 1.5 million Jews, Roma, Poles, Russians and prisoners of various other nationalities, and homosexuals, were murdered, is not only a major occasion for European citizens to remember and condemn the enormous horror and tragedy of the Holocaust, but also for addressing the disturbing rise in anti-Semitism, and especially anti-Semitic incidents, in Europe, and for learning anew the wider lessons about the dangers of victimising people on the basis of race, ethnic origin, religion, social classification, politics or sexual orientation...
An account of a gay Holocaust survivor, Pierre Seel, details life for gay men during Nazi control. In his account he states that he participated in his local gay community in the town of Mulhouse. When the Nazis gained power over the town his name was on a list of local gay men ordered to the police station. He obeyed the directive to protect his family from any retaliation. Upon arriving at the police station he notes that he and other gay men were beaten. Some gay men who resisted the SS had their fingernails pulled out. Others had their bowels punctured, causing them to bleed profusely. After his arrest he was sent to the concentration camp at Schirmeck. There, Seel stated that during a morning roll-call, the Nazi commander announced a public execution. A man was brought out, and Seel recognized his face. It was the face of his eighteen-year-old lover from Mulhouse. Seel states that the Nazi guards then stripped the clothes of his lover, placed a metal bucket over his head, and released trained German Shepherd dogs on him, which mauled him to death.
Rudolf Brazda, believed to be the last surviving person who was sent to a Nazi concentration camp because of his homosexuality, died in France in August 2011, aged 98. Brazda was sent to Buchenwald in August 1942 and held there until its liberation by U.S. forces in 1945. Brazda, who settled in France after the war, was later awarded the Legion of Honour.
Early Holocaust and genocide discourse.
Arising from the dominant discourse of the Jewish suffering during the years of Nazi domination, and building on the divergence of differential victimhoods brought to light by studies of the Roma and the mentally ill, who suffered massively under the eugenics programs of the Third Reich, the idea of a "Gay Holocaust" was first explored in the early 1970s. However, extensive research on the topic was impeded by a continuation of Nazi policies on homosexuals in post-war East and West Germany and continued western notions of homophobia.
The word "genocide" was generated from a need for new terminology in order to understand the gravity of the crimes committed by the Nazis. First coined by Raphael Limkin in 1944, the word became politically charged when The Genocide Act was enacted by the United Nations on December 9, 1948, which created an obligation for governments to respond to such atrocities in the future. The debate on the "Gay Holocaust" is therefore a highly loaded debate which would result in an international acknowledgement of state sponsored homophobia as a precursor to genocide should the proponents of the "Gay Holocaust" succeed. However the United Nations definition does not include sexual orientation (or even social and political groups) within its qualifications for the crime. Genocide by the U.N. definition is limited to national, ethnical, racial or religious groups and as this is the only accord to which nations have pledged allegiance, it stands as the dominant understanding of the term. It is, however, what Michel-Rolph Trouillot terms "an age when collective apologies are becoming increasingly common" as well as a time when the established Holocaust discourse has settled and legitimized claims of the Jewish, Roma and mentally ill victims of Nazi persecution so it would seem an appropriate time to at least bring attention to the debate of the Gay Holocaust, even if the issue is not to be settled.
A lack of research means that there is relatively little data on the dispersion of gay men throughout the camps. However, Heinz Heger suggests in his book "The Men with the Pink Triangle" that they were subjected to harsher labor than smaller targeted groups, such as the political prisoners, and furthermore suffered a much higher mortality rate. They also lacked a support network within the camps and were ostracized in the prison community. Homosexuals, like the mentally ill and many Jews and Roma, were also subjected to medical experimentation in the hopes of finding a cure to homosexuality at the camp in Buchenwald.
The conception of Jewish exclusivity in the Holocaust went unchallenged in the early years of study on the subject. It is undeniable that the Jews suffered the greatest death toll, and entire communities were obliterated in Eastern Europe and to a great extent in western countries. The notion of exclusivity however is challenged by the existence of similar forces working against different social and ethnic groups such as homosexuals and the Roma, which resulted in the victimization and systematic destruction of homosexual lives and lifestyles, as well as those of the Roma. An inclusion of social groups in a definition of genocide would further challenge the notion of the Jewish genocide as unique within the context of the Holocaust. This sentiment has been further articulated by Elie Weisel, who argued that "a focus on other victims may detract from the specificity of the Holocaust". Other scholars such as William J. Spurlin have suggested that such positions foster a misrepresentation of history and devalue the suffering of other victims of Nazi atrocities. Simon Wiesenthal argues, for example, that "the Holocaust transcended the confines of Jewish community and that there were other victims." In the mid-1970s new discourses emerged that challenged the exclusivity of the Jewish genocide within the Holocaust, though not without great resistance.
Changes with the civil rights movement.
The civil rights movements of North America in the 1970s saw an emergence of victim claims through revision and appropriation of historical narratives. The shift from the traditionally conservative notion of history as the story of power and those who held it, social historians emerged with narratives of those who suffered and resisted these powers. African Americans created their own narrative, as firmly based on evidence as the discourses already in existence, as part of a social movement towards civil rights based on a history of victimization and racism. Along similar lines, the gay and lesbian movement in the United States also utilized revisionism to write the narrative that had only just garnered an audience willing to validate it.
There were two processes at work in this new discourse, revisionism and appropriation, which Arlene Stein teases out in her article "Whose Memory, Whose Victimhood?", both of which were used at different points in the movement for civil rights. The revisionist project was taken on in a variety of mediums, historical literature being only one of many. The play "Bent" and a limited number of memoirs which recall "The Diary of Anne Frank" coincided with the appropriation of the pink triangle as a symbol of the new movement and a reminder to "never forget." While the focus of these early revisions was not necessarily to determine the Nazi policy on homosexuals as genocidal, they began a current towards legitimizing the victimization of homosexuals under the regime, a topic that had not been addressed until the 1970s.
Historical works eventually focused on the nature and intent of Nazi policy. Heinz Heger, Gunter Grau and Richard Plant all contributed greatly to the early Holocaust discourse which emerged throughout the 1970s and early 1980s. Central to these studies was the notion that statistically speaking, homosexuals suffered greater losses than many of the smaller minorities under Nazi persecution such as the Jehovah’s Witnesses and within the camps experienced harsher treatments and ostracization as well as execution at the hands of firing squads and the gas chambers.
These early revisionist discourses were joined by a popular movement of appropriation, which invoked the global memory of the Holocaust to shed light on social disparities for homosexuals within the United States. Larry Kramer who was one of the founders of ACT UP, an HIV/AIDS activist group that used shock tactics to bring awareness to the disease and attention to the need for funding popularized the AIDS-as-Holocaust discourse. "The slowness of government response at federal and local levels of government, the paucity of funds for research and treatment, particularly in the early days of the epidemic stems, Kramer argued, from deep-seated homophobic impulses and constituted 'intentional genocide'."
While the appropriation of the Holocaust discourse helped to grab the attention needed for an appropriate response to the pandemic it is highly problematic and perhaps counterproductive to the historical discourse of the time. The notion of AIDS-as-Holocaust and the accompanying notion of AIDS-as-genocide greatly oversimplify the meaning and the intention of genocide as a crime. While parallels can be drawn such as specific group experiencing disproportionate mortality resulting from a seeming neglect by the institutions designed to protect them, the central factors of intention and systematic planning are absent and the use of the word dilutes the severity of the act.
The Holocaust frame was used again in the early 1990s, this time in relation to right-wing homophobic campaigns throughout the United States. The conservative response yielded a new discourse working against the "Gay Holocaust" academia, which emphasized the gay and lesbian revisionism as a victimist discourse which sought sympathy and recognition as a pragmatic means of garnering special status and civil rights outside those of the moral majority. Arlene Stein identifies four central elements to the conservative reaction to the Gay Holocaust discourse: she argues that the right is attempting to dispel the notion that gays are victims, pit two traditionally liberal constituencies against one another (gays and Jews), thereby drawing parallels between Jews and Christians and legitimating its own status as an oppressed and morally upright group.
The victimist argument raises a central tenet as to the reasons for which the discourse of a "Gay Holocaust" has experienced so much resistance politically and popularly (in the conscious of the public). Alyson M. Cole addresses the anti-victim discourse that has emerged in western politics since the end of the 1980s. She asserts "anti-victimists transformed discussions of social obligation, compensations and remedial or restorative procedures into criticisms of the alleged propensity of self-anointed victims to engage in objectionable conduct." Though she is clear that the anti-victimist discourse is not limited to right-wing politics, the case of the "Gay Holocaust" situates itself along these political boundaries and the anti-victim discourse is highly relevant to the debate on homosexual claims to genocide under the Third Reich. Cole also identifies a central conflict within the anti-victim discourse, which sheds light on the weakness in the conservative argument against the Gay Holocaust. While anti-victimists shun the victim and target it for ridicule as a pity-seeking subject-person while simultaneously extolling the virtues of what Cole identifies as the true victim, the true victim holds certain personal qualities, which allow for it to be beyond the ridicule given to the victimist. Propriety, responsibility, individuality and innocence are the central attributes of the true victim and in the case of the Gay Holocaust discourse, the claims made for the recognition of genocide or genocidal processes under Nazi Germany allow the claimants to be relegated to the victimist status, making their 'anti-victim' claims bogus.
Post-revisionist framing of the "Gay Holocaust".
In recent years new work has been done on the Gay Holocaust and rather than emphasizing the severity of destruction to communities or the exclusivity of the genocidal process of the Nazi regime, it focuses on the intersections of social constructions such as gender and sexuality within the context of social organization and political domination. Spurlin claims that these all functioned with one another in forming Germany’s social order and final solution to these social problems. Rather than being autonomous policies, "They were part of a much larger strategy of social disenfranchisement and the marking of enemies..." This discourse incorporates numerous disciplines including gender studies, queer studies, Holocaust studies and genocide studies to tease out the axis at which they meet in social control specifically under National Socialism in Germany.

</doc>
<doc id="42404" url="https://en.wikipedia.org/wiki?curid=42404" title="Kibology">
Kibology

Kibology is a parody religion, named after Kibo, the central figure. Practitioners of Kibology are called "Kibologists" or (sometimes more disdainfully) "Kibozos".
James "Kibo" Parry and his friends began Kibology about 1989
at the suggestion of Mark Jason Dominus. In its early Usenet days it was centered in the newsgroups talk.bizarre and alt.slack, until the creation of alt.religion.kibology in late 1991. The faux religion aspect faded in the mid-1990s, and the newsgroup became oriented to the sense of humor of Kibo and his "followers". The newsgroup also follows various internet "mad scientists" and "crackpots" with a mixture of mockery and appreciation for the unintended humor they produce. A similar Internet phenomenon exists surrounding the Church of the SubGenius. Some posters also "wackyparse," which is to say, they comment on misreadings with humorous effect.
In addition to "Leader Kibo," other Kibologists have developed cult followings of their own from their unusual and humorous writing. The most prominent of these include David Pacheco, Lisa Pea (Elisabeth Rea Higgins), Matt McIrvin, Stephen Will Tanner, Stefan Kapusniak, M Otis Beard, Joe Bay, Gardner S Trask III, Dag Ågren, and E Teflon Piano. In 2003, the group formed a band, Interröbang Cartel.

</doc>
<doc id="42405" url="https://en.wikipedia.org/wiki?curid=42405" title="Ichthyology">
Ichthyology

Ichthyology (from Greek: ἰχθύς, "ikhthus", "fish"; and λόγος, "logos", "study"), also known as Fish Science, is the branch of biology devoted to the study of fish. This includes bony fishes (Osteichthyes), cartilaginous fish (Chondrichthyes), and jawless fish (Agnatha). While a large number of species have been discovered and described, approximately 250 new species are officially described by science each year. According to FishBase, 34,300 species of fish had been described by October 2015.
History.
The study of fish dates from the Upper Paleolithic Revolution (with the advent of 'high culture'). The science of ichthyology was developed in several interconnecting epochs, each with various significant advancements.
The study of fish receives its origins from human's desire to feed, clothe, and equip themselves with useful implements. According to Michael Barton, a prominent ichthyologist and professor at Centre College, "the earliest ichthyologists were hunters and gatherers who had learned how to obtain the most useful fish, where to obtain them in abundance, and at what times they might be the most available". Early cultures manifested these insights in abstract and identifiable artistic expressions.
1500 BC–40 AD.
Informal, scientific descriptions of fish are represented within the Judeo-Christian tradition. The kashrut forbade the consumption of fishes without scales or appendages. Theologians and ichthyologists speculate that the apostle Peter and his contemporaries harvested the fish that are today sold in modern industry along the Sea of Galilee, presently known as Lake Kinneret. These fish include cyprinids of the genera "Barbus" and "Mirogrex", cichlids of the genus "Sarotherodon", and "Mugil cephalus" of the family Mugilidae.
335 BC–80 AD.
Aristotle incorporated ichthyology into formal scientific study. Between 335 BC–322 BC, he provided the earliest taxonomic classification of fish, accurately describing 117 species of Mediterranean fish. Furthermore, Aristotle documented anatomical and behavioral differences between fish and marine mammals. After his death, some of his pupils continued his ichthyological research. Theophrastus, for example, composed a treatise on amphibious fish. The Romans, although less devoted to science, wrote extensively about fish. Pliny the Elder, a notable Roman naturalist, compiled the ichthyological works of indigenous Greeks, including verifiable and ambiguous peculiarities such as the sawfish and mermaid respectively. Pliny's documentation was the last significant contribution to ichthyology until the European Renaissance.
European Renaissance.
The writings of three sixteenth century scholars, Hippolito Salviani, Pierre Belon, and Guillaume Rondelet, signify the conception of modern ichthyology. The investigations of these individuals were based upon actual research in comparison to ancient recitations. This property popularized and emphasized these discoveries. Despite their prominence, Rondelet's "De Piscibus Marinum" is regarded as the most influential, identifying 244 species of fish.
16th–17th century.
The incremental alterations in navigation and shipbuilding throughout the Renaissance marked the commencement of a new epoch in ichthyology. The Renaissance culminated with the era of exploration and colonization, and upon the cosmopolitan interest in navigation came the specialization in naturalism. Georg Marcgrave of Saxony composed the "Naturalis Brasilae" in 1648. This document contained a description of 100 species of fish indigenous to the Brazilian coastline. In 1686, John Ray and Francis Willughby collaboratively published "Historia Piscium", a scientific manuscript containing 420 species of fish, 178 of these newly discovered. The fish contained within this informative literature were arranged in a provisional system of classification.
The classification used within the "Historia Piscium" was further developed by Carl Linnaeus, the "father of modern taxonomy". His taxonomic approach became the systematic approach to the study of organisms, including fish. Linnaeus was a professor at the University of Uppsala and an eminent botanist; however, one of his colleagues, Peter Artedi, earned the title "father of ichthyology" through his indispensable advancements. Artedi contributed to Linnaeus's refinement of the principles of taxonomy. Furthermore, he recognized five additional orders of fish: Malacopterygii, Acanthopterygii, Branchiostegi, Chondropterygii, and Plagiuri. Artedi developed standard methods for making counts and measurements of anatomical features that are modernly exploited. Another associate of Linnaeus, Albertus Seba, was a prosperous pharmacist from Amsterdam. Seba assembled a cabinet, or collection, of fish. He invited Artedi to utilize this assortment of fish; unfortunately, in 1735, Artedi fell into an Amsterdam canal and drowned at the age of 30.
Linnaeus posthumously published Artedi's manuscripts as "Ichthyologia, sive Opera Omnia de Piscibus" (1738). His refinement of taxonomy culminated in the development of the binomial nomenclature which is in use by contemporary ichthyologists. Furthermore, he revised the orders introduced by Artedi, placing significance on pelvic fins. Fishes lacking this appendage were placed within the order Apodes; fishes containing abdominal, thoracic, or jugular pelvic fins were termed Abdominales, Thoracici, and Jugulares respectively. However, these alterations were not grounded within evolutionary theory. Therefore, it would take over a century until Charles Darwin would provide the intellectual foundation from which we would be permitted to perceive that the degree of similarity in taxonomic features was a consequence of phylogenetic relationship.
Modern era.
Close to the dawn of the nineteenth century, Marcus Elieser Bloch of Berlin and Georges Cuvier of Paris made attempts to consolidate the knowledge of ichthyology. Cuvier summarized all of the available information in his monumental "Histoire Naturelle des Poissons". This manuscript was published between 1828 and 1849 in a 22 volume series. This documentat describes 4,514 species of fish, 2,311 of these new to science. It remains one of the most ambitious treatises of the modern world. Scientific exploration of the Americas advanced our knowledge of the remarkable diversity of fishes. Charles Alexandre Lesueur was a student of Cuvier. He made a cabinet of fishes dwelling within the Great Lakes and Saint Lawrence River regions.
Adventurous individuals such as John James Audubon and Constantine Samuel Rafinesque figure in the faunal documentation of North America. These persons often traveled with one another. Rafinesque wrote "Ichthyologia Ohiensis" in 1820. In addition, Louis Agassiz of Switzerland established his reputation through the study of freshwater fishes and the first comprehensive treatment of paleoichthyology, Poissons Fossiles. In the 1840s, Agassiz moved to the United States, where he taught at Harvard University until his death in 1873.
Albert Günther published his "Catalogue of the Fishes of the British Museum" between 1859 and 1870, describing over 6,800 species and mentioning another 1,700. Generally considered one of the most influential ichthyologists, David Starr Jordan wrote 650 articles and books on the subject as well as serving as president of Indiana University and Stanford University.
Notable ichthyologists.
Members of this list meet one or more of the following criteria: 1) Author of 50 or more fish taxon names, 2) Author of major reference work in ichthyology, 3) Founder of major journal or museum, 4) Person most notable for other reasons who has also worked in ichthyology.

</doc>
<doc id="42408" url="https://en.wikipedia.org/wiki?curid=42408" title="Single non-transferable vote">
Single non-transferable vote

Single non-transferable vote or SNTV is an electoral system used in multi-member constituency elections.
Voting.
In any election, each voter casts one vote for one candidate in a multi-candidate race for multiple offices. Posts are filled by the candidates with the most votes. Thus, in a three-seat constituency, the three candidates receiving the largest numbers of votes would win office.
SNTV can be used with non-partisan ballots.
Example.
There are three seats to be filled and five candidates: A, B, C, D and E. 
C, D and E are the winning candidates.
This breaks down by party as:
Party Y has more votes than Party Z, but fewer seats because of an inefficient spread of votes across the candidates. If either party had risked trying to win all three seats, then Party X would have a higher chance of winning a seat, in the event of an uneven distribution of votes.
Proportional representation.
SNTV facilitates minority representation.
SNTV can result in proportional representation when political parties have accurate information about their relative levels of electoral support, and nominate candidates in accordance with their respective level of electoral support. If there are "n" candidates to be elected, Candidate A can guarantee being elected by receiving one more than 1/("n"+1) of the votes (the Droop quota), because "n" other candidates cannot all receive more than Candidate A. It can become very difficult for parties to receive representation proportional to their strength, because they are forced to judge their strength prior to deciding how many candidates to field (strategic nomination). If they field too many, their supporters votes might be split across too many candidates, evenly diluting their share to the point where they "all" lose to a less diluted opposing party. If the party fields too few candidates, they might not win seats proportional to their hypothetical true level of support and excess votes would be wasted on their winning candidates. 
The relative risks of strategic nomination are not the same for parties in other positions of electoral success. A large party with a majority of seats would have much more to lose from the split vote effect than to gain from avoiding the wasted vote effect, and so would likely decide to err on the side of fielding fewer candidates. A small party with little representation would be more risk-tolerant and err on the side of too many candidates, potentially gaining seats greater than their proportion of the electorate by winning with narrower margins of victory than the candidates from larger parties.
SNTV electoral systems, like proportional electoral systems generally, typically produce more proportional electoral outcomes as the size of the electoral districts (number of seats in each constituency) increases.
Potential for tactical voting.
The potential for tactical voting in a single non-transferable vote system is large. Receiving only one vote, the rational voter must only vote for a candidate that has a chance of winning, but will not win by too great a margin, thus taking votes away from party colleagues. This also creates opportunities for tactical nominations, with parties nominating candidates similar to their opponents' candidates in order to split the vote. SNTV has been measured through the lens of such concepts as "decision-theoretic analysis". Professor Gary W. Cox, an expert on SNTV, has studied the results of this system’s use in Japan. Cox has an explanation of real-world data finding the, “two systems and semi-proportional are alike in their strategic voting equilibria.” (Cox 608) His research shows that voters use the information offered in campaigns (polls, reporting, fundraising totals, endorsements, etc.), to rationally decide who the most viable candidates are then vote for them. 
SNTV can also result in complicated intra-party dynamics because in a SNTV system, a candidate must not only run against candidates from the other party, but must also run against candidates from their own party. 
Because running on issues may lead to a situation in which a candidate becomes too popular and therefore draws votes away from other allied candidates, SNTV may encourage legislators to join factions which consist of patron-client relationships in which a powerful legislator can apportion votes to his or her supporters.
In addition, parties must ensure that their supporters evenly distribute their votes among the party's candidates. Historically, in Taiwan, the Kuomintang did this by sending members a letter telling them which candidate to vote for. With the Democratic Progressive Party, vote sharing is done informally, as members of a family or small group will coordinate their votes. The New Party had a surprisingly effective system by asking party supporters to vote for the candidate that corresponded to their birthdate. This led to a system of vote allocation which had been adopted by all parties for the 2004 ROC Legislative elections.
Usage.
SNTV is used for legislative elections in Afghanistan, Jordan and for the elections to the upper house of Indonesia and for the senate of Thailand under the 1997 constitution.
Puerto Rico.
In Puerto Rico, where SNTV is known as "at-large representation" ("representación por acumulación" in Spanish), political parties vary the ballot order of their candidates across electoral divisions, in order to ensure each candidate has a roughly equal chance of being elected. Since most voters choose the candidates placed at the top of their party lists on the ballots they receive, at-large candidates from the same party usually obtain approximately equal vote totals.
The two major Puerto Rican political parties, the Popular Democratic Party and the New Progressive Party, usually nominate six candidates for each chamber, while the much smaller Puerto Rican Independence Party runs single-candidate slates for both the Senate and the House of Representatives. The overall distribution of legislative seats is largely determined by the results for the sixteen Senate and forty House district seats, elected by plurality voting.
Japan, South Korea, and Taiwan.
SNTV was once used to elect the parliaments of Japan, South Korea and the Republic of China (Taiwan), but its use has been discontinued for the most part. It is still used in Japan for some seats in the House of Councillors (Sangi-in), prefectural assemblies and municipal assemblies, and in Taiwan for the six aboriginal seats in the Legislative Yuan (national legislature), as well as local assemblies.
In Taiwan, the party structure was further complicated by the fact that while members of the Legislative Yuan were elected by SNTV, executive positions were (and still are) elected by a First Past the Post. This created a party system in which smaller factionalized parties, which SNTV promotes, have formed two large coalitions that resembles the two party system which First Past the Post rewards. Starting with the 2008 legislative elections, the SNTV system was discarded in favor of a mixed "single member district" (SMD) with proportional representation based on national party votes, similar to Japan.
Hong Kong.
Although the electoral system for about half of the seats of the Legislative Council of the territory is nominally a proportional representation system with party lists and Hare quota, in practice political parties would field multiple lists in the same constituency. For example, the Democratic Party fielded three separate lists in the eight-seat New Territories West constituency in the 2008 election, aiming to win three seats (which they ended up with two winners). Split list or split tickets is done in order to win more seats with fewer votes, since the first candidate on each list would require less than the Hare quota to get a seat. Supporters are asked to split their votes among the lists of the same party, usually along geographical location of residence.
In the 2012 election, no candidate list won more than one seat in any of the six PR constituencies which returned a total of 40 seats, rendering the result effectively the same as SNTV.

</doc>
<doc id="42411" url="https://en.wikipedia.org/wiki?curid=42411" title="John Wesley">
John Wesley

John Wesley (; 2 March 1791) was an Anglican minister and theologian who, with his brother Charles and fellow cleric George Whitefield, is credited with the foundation of the evangelical movement known as Methodism. His work and writings also played a leading role in the development of the Holiness movement and Pentecostalism.
Educated at Charterhouse School and Oxford University, Wesley was elected a fellow of Lincoln College, Oxford in 1726 and ordained a priest two years later. Returning to Oxford in 1729 after serving as curate at his father's parish, he led the Holy Club, a society formed for the purpose of study and the pursuit of a devout Christian life; it had been founded by his brother Charles, and counted George Whitefield among its members. After an unsuccessful ministry of two years at Savannah in the Georgia Colony, Wesley returned to London and joined a religious society led by Moravian Christians. On 24 May 1738 he experienced what has come to be called his evangelical conversion, when he felt his "heart strangely warmed". He subsequently departed from the Moravians, beginning his own ministry.
A key step in the development of Wesley's ministry was, like Whitefield, to travel and preach outdoors. In contrast to Whitefield's Calvinism, Wesley embraced the Arminian doctrines that dominated the Church of England at the time. Moving across Great Britain, North America and Ireland, he helped form and organise small Christian groups that developed intensive and personal accountability, discipleship and religious instruction. Most importantly, he appointed itinerant, unordained evangelists to travel and preach as he did and to care for these groups of people. Under Wesley's direction, Methodists became leaders in many social issues of the day, including prison reform and the abolition of slavery.
Although he was not a systematic theologian, Wesley argued for the notion of Christian perfection and against Calvinismand, in particular, against its doctrine of predestination. He held that, in this life, Christians could achieve a state where the love of God "reigned supreme in their hearts", giving them outward holiness. His evangelicalism, firmly grounded in sacramental theology, maintained that means of grace were the manner by which God sanctifies and transforms the believer, encouraging people to experience Jesus Christ personally.
Throughout his life, Wesley remained within the established Anglican church, insisting that the Methodist movement lay well within its tradition. Although sometimes maverick in his interpretation and use of church policy, he became widely respected and, by the end of his life, had been described as "the best loved man in England".
Early life.
John Wesley was born in 1703 in Epworth, 23 miles (37 km) north-west of Lincoln, as the fifteenth child of Samuel Wesley and his wife Susanna Wesley (née Annesley). Samuel Wesley was a graduate of the University of Oxford and a poet who, from 1696, was rector of Epworth. He married Susanna, the twenty-fifth child of Samuel Annesley, a dissenting minister, in 1689. Ultimately, she bore him nineteen children, of which nine lived beyond infancy. She and Samuel Wesley had become members of the Church of England as young adults.
As in many families at the time, Wesley's parents gave their children their early education. Each child, including the girls, was taught to read as soon as they could walk and talk. They were expected to become proficient in Latin and Greek and to have learned major portions of the New Testament by heart. Susanna Wesley examined each child before the midday meal and before evening prayers. Children were not allowed to eat between meals and were interviewed singularly by their mother one evening each week for the purpose of intensive spiritual instruction. In 1714, at age 11, Wesley was sent to the Charterhouse School in London (under the mastership of John King from 1715), where he lived the studious, methodical and, for a while, religious life in which he had been trained at home.
Apart from his disciplined upbringing, a rectory fire which occurred on 9 February 1709, when Wesley was five years old, left an indelible impression. Some time after 11:00 p.m., the rectory roof caught on fire. Sparks falling on the children’s beds and cries of "fire" from the street roused the Wesleys who managed to shepherd all their children out of the house except for John who was left stranded on the second floor. With stairs aflame and the roof about to collapse, Wesley was lifted out of the second floor window by a parishioner standing on another man’s shoulders. Wesley later utilised the phrase, "a brand plucked out of the fire", quoting Zechariah 3:2, to describe the incident. This childhood deliverance subsequently became part of the Wesley legend, attesting to his special destiny and extraordinary work.
Education.
In June 1720, Wesley entered Christ Church, Oxford. In 1724, Wesley graduated as a Bachelor of Arts and decided to pursue a Master of Arts degree. He was ordained a deacon on 25 September 1725, holy orders being a necessary step toward becoming a fellow and tutor at the university.
In the year of his ordination he read Thomas à Kempis and Jeremy Taylor, and began to seek the religious truths which underlay the great revival of the 18th century. The reading of Law's "Christian Perfection" and "A Serious Call to a Devout and Holy Life" gave him, he said, a sublimer view of the law of God; and he resolved to keep it, inwardly and outwardly, as sacredly as possible, believing that in obedience he would find salvation. He pursued a rigidly methodical and abstemious life, studied the Scriptures, and performed his religious duties diligently, depriving himself so that he would have alms to give. He began to seek after holiness of heart and life.
In March 1726, Wesley was unanimously elected a fellow of Lincoln College, Oxford. This carried with it the right to a room at the college and regular salary. While continuing his studies, Wesley taught Greek, lectured on the New Testament and moderated daily disputations at the university. However, a call to ministry intruded upon his academic career. In August 1727, after taking his master's degree, Wesley returned to Epworth. His father had requested his assistance in serving the neighbouring cure of Wroote. Ordained a priest on 22 September 1728, Wesley served as a parish curate for two years. He returned to Oxford in November 1729 at the request of the Rector of Lincoln College and to maintain his status as junior Fellow.
Holy Club.
During Wesley's absence, his younger brother Charles (1707–88) matriculated at Christ Church. Along with two fellow students, he formed a small club for the purpose of study and the pursuit of a devout Christian life. On Wesley's return, he became the leader of the group which increased somewhat in number and greatly in commitment. The group met daily from six until nine for prayer, psalms, and reading of the Greek New Testament. They prayed every waking hour for several minutes and each day for a special virtue. While the church's prescribed attendance was only three times a year, they took communion every Sunday. They fasted on Wednesdays and Fridays until three o'clock as was commonly observed in the ancient church. In 1730, the group began the practice of visiting prisoners in jail. They preached, educated, and relieved jailed debtors whenever possible, and cared for the sick.
Given the low ebb of spirituality in Oxford at that time, it was not surprising that Wesley's group provoked a negative reaction. They were considered to be religious "enthusiasts" which in the context of the time meant religious fanatics. University wits styled them the "Holy Club," a title of derision. Currents of opposition became a furor following the mental breakdown and death of a group member, William Morgan. In response to the charge that "rigorous fasting" had hastened his death, Wesley noted that Morgan had left off fasting a year and a half since. In the same letter, which was widely circulated, Wesley referred to the name "Methodist" which "some of our neighbors are pleased to compliment us." That name was used by an anonymous author in a published pamphlet (1733) describing Wesley and his group, "The Oxford Methodists".
For all of his outward piety, Wesley sought to cultivate his inner holiness or at least his sincerity as evidence of being a true Christian. A list of "General Questions" which he developed in 1730 evolved into an elaborate grid by 1734 in which he recorded his daily activities hour-by-hour, resolutions he had broken or kept, and ranked his hourly "temper of devotion" on a scale of 1 to 9. Wesley also regarded the contempt with which he and his group were held to be a mark of a true Christian. As he put it in a letter to his father, "Till he be thus contemned, no man is in a state of salvation."
Journey to Savannah, Georgia.
On 14 October 1735, Wesley and his brother Charles sailed on "The Simmonds" from Gravesend in Kent for Savannah in the Province of Georgia in the American colonies at the request of James Oglethorpe, who had founded the colony in 1733 on behalf of the Trustees for the Establishment of the Colony of Georgia in America. Oglethorpe wanted Wesley to be the minister of the newly formed Savannah parish, a new town laid out in accordance with the famous Oglethorpe Plan.
It was on the voyage to the colonies that the Wesleys first came into contact with Moravian settlers. Wesley was influenced by their deep faith and spirituality rooted in pietism. At one point in the voyage a storm came up and broke the mast off the ship. While the English panicked, the Moravians calmly sang hymns and prayed. This experience led Wesley to believe that the Moravians possessed an inner strength which he lacked. The deeply personal religion that the Moravian pietists practised heavily influenced Wesley's theology of Methodism.
Wesley arrived in the colony in February 1736. He approached the Georgia mission as a High Churchman, seeing it as an opportunity to revive "primitive Christianity" in a primitive environment. Although his primary goal was to evangelise the Native Americans, a shortage of clergy in the colony largely limited his ministry to European settlers in Savannah. While his ministry has often been judged to have been a failure in comparison to his later success as a leader in the Evangelical Revival, Wesley gathered around him a group of devoted Christians who met in a number of small group religious societies. At the same time, attendance at church services and communion increased over the course of nearly two years in which he served as Savannah's parish priest.
Nonetheless, Wesley's High Church ministry was controversial amongst the colonists and it ended in disappointment after Wesley fell in love with a young woman named Sophia Hopkey. Following her marriage to William Williamson, Wesley believed Sophia's former zeal for practising the Christian faith declined. In strictly applying the rubrics of the Book of Common Prayer, Wesley denied her communion after she failed to signify to him in advance her intention of taking it. As a result, legal proceedings against him ensued in which a clear resolution seemed unlikely. In December 1737, Wesley fled the colony and returned to England.
It has been widely recognised that one of the most significant accomplishments of Wesley's Georgia mission was his publication of a "Collection of Psalms and Hymns". The "Collection" was the first Anglican hymnal published in America, and the first of many hymn-books Wesley published. It included five hymns he translated from German.
Wesley's Aldersgate Experience.
Wesley returned to England depressed and beaten. It was at this point that he turned to the Moravians. Both he and Charles received counsel from the young Moravian missionary Peter Boehler, who was temporarily in England awaiting permission to depart for Georgia himself. John's famous "Aldersgate experience" of 24 May 1738, at a Moravian meeting in Aldersgate Street, London, in which he heard a reading of Martin Luther's preface to the Epistle to the Romans, revolutionised the character and method of his ministry. The previous week he had been highly impressed by the sermon of John Heylyn, whom he was assisting in the service at St Mary-le-Strand. Earlier that day, he had heard the choir at St. Paul's Cathedral singing Psalm 130, where the Psalmist calls to God "Out of the depths."
But it was still a depressed Wesley who attended a service on the evening of 24 May. John Wesley recounted his Aldersgate Experience in his Journal: "In the evening I went very unwillingly to a society in Aldersgate Street, where one was reading Luther’s Preface to the Epistle to the Romans. About a quarter before nine, while he was describing the change which God works in the heart through faith in Christ, I felt my heart strangely warmed. I felt I did trust in Christ, Christ alone for salvation, and an assurance was given me that he had taken away my sins, even mine, and saved me from the law of sin and death." 
Daniel L. Burnett called this event Wesley's "Evangelical Conversion". Wesley himself explained in a 1738 letter to his brother Samuel: "By a Christian, I mean one who so believes in Christ, as that sin hath no more dominion over him: And in the obvious sense of the word, I was not a Christian till May the 24th last past. For till then sin had dominion over me, although I fought with it continually; but surely, then, from that time to this it hath not; – such is the free grace of God in Christ."
A few weeks later, Wesley preached a sermon on the doctrine of personal salvation by faith, which was followed by another, on God's grace "free in all, and free for all." Daniel L. Burnett writes: “The significance of Wesley’s Aldersgate Experience is monumental. It is the pivotal point in his life and the Methodist movement. Without it the names of Wesley and Methodism would likely be nothing more than obscure footnotes in the pages of church history.”
After Aldersgate: Working with the Moravians.
Wesley allied himself with the Moravian society in Fetter Lane. In 1738 he went to Herrnhut, the Moravian headquarters in Germany, to study. On his return to England, Wesley drew up rules for the "bands" into which the Fetter Lane Society was divided and published a collection of hymns for them. He met frequently with this and other religious societies in London but did not preach often in 1738, because most of the parish churches were closed to him.
Wesley's Oxford friend, the evangelist George Whitefield, was also excluded from the churches of Bristol upon his return from America. Going to the neighbouring village of Kingswood, in February 1739, Whitefield preached in the open air to a company of miners. Later he preached in Whitefield's Tabernacle. Wesley hesitated to accept Whitefield's call to copy this bold step. Overcoming his scruples, he preached the first time at Whitefield's invitation sermon in the open air, near Bristol, in April 1739. Wesley wrote,
Wesley was unhappy about the idea of field preaching as he believed Anglican liturgy had much to offer in its practice. Earlier in his life he would have thought that such a method of saving souls was "almost a sin." He recognised the open-air services were successful in reaching men and women who would not enter most churches. From then on he took the opportunities to preach wherever an assembly could be brought together, more than once using his father's tombstone at Epworth as a pulpit. Wesley continued for fifty years – entering churches when he was invited, and taking his stand in the fields, in halls, cottages, and chapels, when the churches would not receive him.
Late in 1739 Wesley broke with the Moravians in London. Wesley had helped them organise the Fetter Lane Society, and those converted by his preaching and that of his brother and Whitefield had become members of their bands. But he believed they fell into heresy by supporting quietism, so he decided to form his own followers into a separate society. "Thus," he wrote, "without any previous plan, began the Methodist Society in England." He soon formed similar societies in Bristol and Kingswood, and wherever Wesley and his friends made converts.
Persecutions and lay preaching.
From 1739 onward, Wesley and the Methodists were persecuted by clergy and magistrates for various reasons. Though Wesley had been ordained an Anglican priest, many other Methodist leaders had not received ordination. And for his own part, Wesley flouted many regulations of the Church of England concerning parish boundaries and who had authority to preach. This was seen as a social threat that disregarded institutions. Clergy attacked them in sermons and in print, and at times mobs attacked them. Wesley and his followers continued to work among the neglected and needy. They were denounced as promulgators of strange doctrines, fomenters of religious disturbances; as blind fanatics, leading people astray, claiming miraculous gifts, attacking the clergy of the Church of England, and trying to re-establish Catholicism.
Wesley felt that the church failed to call sinners to repentance, that many of the clergy were corrupt, and that people were perishing in their sins. He believed he was commissioned by God to bring about revival in the church, and no opposition, persecution, or obstacles could prevail against the divine urgency and authority of this commission. The prejudices of his high-church training, his strict notions of the methods and proprieties of public worship, his views of the apostolic succession and the prerogatives of the priest, even his most cherished convictions, were not allowed to stand in the way.
Unwilling that people should perish in their sins and unable to reach them from church pulpits, following the example set by George Whitefield, Wesley began field preaching. Seeing that he and the few clergy co-operating with him could not do the work that needed to be done, he was led, as early as 1739, to approve local preachers. He evaluated and approved men who were not ordained by the Anglican Church to preach and do pastoral work. This expansion of lay preachers was one of the keys of the growth of Methodism.
Chapels and Organisations.
As his societies needed houses to worship in, Wesley began to provide chapels, first in Bristol at the New Room, then in London and elsewhere. The Foundry was an early chapel utilised by Wesley. The location of the Foundry shown on an 18th-century map, where it rests between Tabernacle Street and Worship Street in the Moorfields area of London. When the Wesleys spotted the building atop Windmill Hill, north of Finsbury Fields, the structure which previously cast brass guns and mortars for the Royal Ordnance had been sitting vacant for 23 years; it has been abandoned because of an explosion on 10 May 1716.
The Bristol chapel (built in 1739) was at first in the hands of trustees. A large debt was contracted, and Wesley's friends urged him to keep it under his own control, so the deed was cancelled and he became sole trustee. Following this precedent, all Methodist chapels were committed in trust to him until by a "deed of declaration", all his interests in them were transferred to a body of preachers called the "Legal Hundred".
When disorder arose among some members of the societies, Wesley adopted giving tickets to members, with their names written by his own hand. These were renewed every three months. Those deemed unworthy did not receive new tickets and dropped out of the society without disturbance. The tickets were regarded as commendatory letters.
When the debt on a chapel became a burden, it was proposed that one in 12 members should collect offerings regularly from the 11 allotted to him. Out of this grew the Methodist class-meeting system in 1742. In order to keep the disorderly out of the societies, Wesley established a probationary system. He undertook to visit each society regularly in what became the quarterly visitation, or conference. As the number of societies increased, Wesley could not keep personal contact, so in 1743 he drew up a set of "General Rules" for the "United Societies". These were the nucleus of the Methodist "Discipline", still the basis.
Over time, a shifting pattern of societies, circuits, quarterly meetings, annual Conferences, classes, bands, and select societies took shape. At the local level, there were numerous societies of different sizes which were grouped into circuits to which traveling preachers were appointed for two-year periods. Circuit officials met quarterly under a senior traveling preacher or "assistant." Conferences with Wesley, traveling preachers and others were convened annually for the purpose of coordinating doctrine and discipline for the entire connection. Classes of a dozen or so society members under a leader met weekly for spiritual fellowship and guidance. In early years, there were "bands" of the spiritually gifted who consciously pursued perfection. Those who were regarded to have achieved it were grouped in select societies or bands. In 1744, there were 77 such members. There also was a category of penitents which consisted of backsliders.
As the number of preachers and preaching-places increased, doctrinal and administrative matters needed to be discussed; so John and Charles Wesley, along with four other clergy and four lay preachers, met for consultation in London in 1744. This was the first Methodist conference; subsequently, the conference (with Wesley as its president) became the ruling body of the Methodist movement. Two years later, to help preachers work more systematically and societies receive services more regularly, Wesley appointed "helpers" to definitive circuits. Each circuit included at least 30 appointments a month. Believing that the preacher's efficiency was promoted by his being changed from one circuit to another every year or two, Wesley established the "itinerancy" and insisted that his preachers submit to its rules.
Ordination of ministers.
As the societies multiplied, they adopted the elements of an ecclesiastical system. The divide between Wesley and the Church of England widened. The question of division from the Church of England was urged by some of his preachers and societies, but most strenuously opposed by his brother Charles. Wesley refused to leave the Church of England, believing that Anglicanism was "with all her blemishes, [...] nearer the Scriptural plans than any other in Europe". In 1745 Wesley wrote that he would make any concession which his conscience permitted, in order to live in peace with the clergy. He could not give up the doctrine of an inward and present salvation by faith itself. He would not stop preaching, nor dissolve the societies, nor end preaching by lay members. As a cleric of the established church he had no plans to go further.
When, in 1746, Wesley read Lord King on the primitive church, he became convinced that the concept of apostolic succession in Anglicanism was a "fable". He wrote that he was "a scriptural "episkopos" as much as many men in England."
Many years later, Edward Stillingfleet's "Irenicon" led him to decide that ordination could be valid when performed by a presbyter rather than a bishop. Nevertheless, many believe that Wesley was consecrated a bishop in 1763 by Erasmus of Arcadia, and that Wesley could not openly announce his episcopal consecration without incurring the penalty of the Præmunire Act.
In 1784, he believed he could not longer wait for the Bishop of London to ordain someone for the American Methodists, who were without the sacraments after the American War of Independence. The Church of England had been disestablished in the United States, where it had been the state church in most of the southern colonies. The Church of England had not yet appointed a United States bishop to what would become the Protestant Episcopal Church in America. Wesley ordained Thomas Coke by the laying on of hands although Coke was already a priest in the Church of England. Wesley appointed him to be superintendent of Methodists in the United States. He also ordained Richard Whatcoat and Thomas Vasey as presbyters. Whatcoat and Vasey sailed to America with Coke. Wesley intended that Coke and Asbury (whom Coke ordained) should ordain others in the newly founded Methodist Episcopal Church in the United States.
His brother Charles grew alarmed and begged Wesley to stop before he had "quite broken down the bridge" and not embitter his [Charles'] last moments on earth, nor "leave an indelible blot on our memory." Wesley replied that he had not separated from the church, nor did he intend to, but he must and would save as many souls as he could while alive, "without being careful about what may possibly be when I die." Although Wesley rejoiced that the Methodists in America were free, he advised his English followers to remain in the established church and he himself died within it.
Doctrine and theology.
The 20th-century Wesley scholar Albert Outler argued in his introduction to the 1964 collection "John Wesley" that Wesley developed his theology by using a method that Outler termed the Wesleyan Quadrilateral. In this method, Wesley believed that the living core of the Christian faith was revealed in Scripture; and the Bible was the sole foundational source of theological or doctrinal development. The centrality of Scripture was so important for Wesley that he called himself "a man of one book" – meaning the Bible – although he was well-read for his day. However, he believed that doctrine had to be in keeping with Christian orthodox tradition. So, tradition was considered the second aspect of the Quadrilateral.
Wesley contended that a part of the theological method would involve experiential faith. In other words, truth would be vivified in personal experience of Christians (overall, not individually), if it were really truth. And every doctrine must be able to be defended rationally. He did not divorce faith from reason. Tradition, experience and reason, however, were subject always to Scripture, Wesley argued, because only there is the Word of God revealed "so far as it is necessary for our salvation."
The doctrines which Wesley emphasised in his sermons and writings are prevenient grace, present personal salvation by faith, the witness of the Spirit, and sanctification. Prevenient grace was the theological underpinning of his belief that all persons were capable of being saved by faith in Christ. Unlike the Calvinists of his day, Wesley did not believe in predestination, that is, that some persons had been elected by God for salvation and others for damnation. He understood that Christian orthodoxy insisted that salvation was only possible by the sovereign grace of God. He expressed his understanding of humanity's relationship to God as utter dependence upon God's grace. God was at work to enable all people to be capable of coming to faith by empowering humans to have actual existential freedom of response to God.
Wesley defined the witness of the Spirit as: "an inward impression on the soul of believers, whereby the Spirit of God directly testifies to their spirit that they are the children of God." He based this doctrine upon certain Biblical passages (see Romans 8:15–16 as an example). This doctrine was closely related to his belief that salvation had to be "personal." In his view, a person must ultimately believe the Good News for himself or herself; no one could be in relation to God for another.
Sanctification he described in 1790 as the "grand "depositum" which God has lodged with the people called `Methodists'." Wesley taught that sanctification was obtainable after justification by faith, between justification and death. He did not contend for "sinless perfection"; rather, he contended that a Christian could be made "perfect in love". (Wesley studied Eastern Orthodoxy and particularly the doctrine of Theosis). This love would mean, first of all, that a believer's motives, rather than being self-centred, would be guided by the deep desire to please God. One would be able to keep from committing what Wesley called, "sin rightly so-called." By this he meant a conscious or intentional breach of God's will or laws. A person could still be able to sin, but intentional or wilful sin could be avoided.
Secondly, to be made perfect in love meant, for Wesley, that a Christian could live with a primary guiding regard for others and their welfare. He based this on Christ's quote that the second great command is "to love your neighbour as you love yourself." In his view, this orientation would cause a person to avoid any number of sins against his neighbour. This love, plus the love for God that could be the central focus of a person's faith, would be what Wesley referred to as "a fulfilment of the law of Christ."
Advocacy of Arminianism.
Wesley entered controversies as he tried to enlarge church practice. The most notable of his controversies was that on Calvinism. His father was of the Arminian school in the church. Wesley came to his own conclusions while in college and expressed himself strongly against the doctrines of Calvinistic election and reprobation. His system of thought has become known as Wesleyan Arminianism, the foundations of which were laid by Wesley and Fletcher.
Whitefield inclined to Calvinism. In his first tour in America, he embraced the views of the New England School of Calvinism. When in 1739 Wesley preached a sermon on "Freedom of Grace", attacking the Calvinistic understanding of predestination as blasphemous, as it represented "God as worse than the devil," Whitefield asked him not to repeat or publish the discourse, as he did not want a dispute. Wesley published his sermon anyway. Whitefield was one of many who responded. The two men separated their practice in 1741. Wesley wrote that those who held to unlimited atonement did not desire separation, but "those who held 'particular redemption' would not hear of any accommodation."
Whitefield, Harris, Cennick, and others, became the founders of Calvinistic Methodism. Whitefield and Wesley, however, were soon back on friendly terms, and their friendship remained unbroken although they travelled different paths. When someone asked Whitefield if he thought he would see Wesley in heaven, Whitefield replied, "I fear not, for he will be so near the eternal throne and we at such a distance, we shall hardly get sight of him."
In 1770, the controversy broke out anew with violence and bitterness, as people's view of God related to their views of men and their possibilities. Augustus Montague Toplady, Rowland, Richard Hill and others were engaged on one side, while Wesley and Fletcher stood on the other. Toplady was editor of "The Gospel Magazine", which had articles covering the controversy.
In 1778, Wesley began the publication of "The Arminian Magazine", not, he said, to convince Calvinists, but to preserve Methodists. He wanted to teach the truth that "God willeth all men to be saved." A "lasting peace" could be secured in no other way.
Support for abolitionism.
Later in his ministry, Wesley was a keen abolitionist, Wesley was a friend of John Newton and William Wilberforce who were also influential in the abolition of slavery in Britain.
Personality and activities.
John Wesley travelled generally on horseback, preaching two or three times each day. Stephen Tomkins writes that he "rode 250,000 miles, gave away 30,000 pounds, ... and preached more than 40,000 sermons... "
He formed societies, opened chapels, examined and commissioned preachers, administered aid charities, prescribed for the sick, helped to pioneer the use of electric shock for the treatment of illness, superintended schools and orphanages, and received at least £20,000 for his publications but used little of it for himself.
Wesley practiced a vegetarian diet and abstained from wine.
After attending a performance in Bristol Cathedral in 1758, Wesley said: "I went to the cathedral to hear Mr. Handel's "Messiah". I doubt if that congregation was ever so serious at a sermon as they were during this performance. In many places, especially several of the choruses, it exceeded my expectation."
He is described as below medium height, well proportioned, strong, with a bright eye, a clear complexion, and a saintly, intellectual face. Wesley married very unhappily at the age of 48 to a widow, Mary Vazeille, described as "a well-to-do widow and mother of four children."
In 1770, at the death of George Whitefield, Wesley wrote a memorial sermon which praised Whitefield's admirable qualities and acknowledged the two men's differences: "There are many doctrines of a less essential nature ... In these we may think and let think; we may 'agree to disagree.' But, meantime, let us hold fast the essentials..." Wesley was the first to put the phrase "agree to disagree" in print.
Wesley died on 2 March 1791, in his 87th year. As he lay dying, his friends gathered around him, Wesley grasped their hands and said repeatedly, "Farewell, farewell." At the end, he said, "The best of all is, God is with us", lifted his arms and raised his feeble voice again, repeating the words, "The best of all is, God is with us." He was entombed at Wesley's Chapel, which he built in City Road, London, in England. The site also is now both a place of worship and a visitor attraction, incorporating the Museum of Methodism and John Wesley's House.
Because of his charitable nature he died poor, leaving as the result of his life's work 135,000 members and 541 itinerant preachers under the name "Methodist". It has been said that "when John Wesley was carried to his grave, he left behind him a good library of books, a well-worn clergyman's gown" and the Methodist Church.
Literary work.
Wesley was a logical thinker and expressed himself clearly, concisely and forcefully in writing. His written sermons are characterised by spiritual earnestness and simplicity. They are doctrinal but not dogmatic. His "Notes on the New Testament" (1755) are enlightening. Both the "Sermons" (about 140) and the "Notes" are doctrinal standards. Wesley was a fluent, powerful and effective preacher. He usually preached spontaneously and briefly, though occasionally at great length.
As an organiser, a religious leader and a statesman, he was eminent. He knew how to lead and control men to achieve his purposes. He used his power, not to provoke rebellion, but to inspire love. His mission was to spread "Scriptural holiness"; his means and plans were such as Providence indicated. The course thus mapped out for him he pursued with a determination from which nothing could distract him.
Wesley's prose "Works" were first collected by himself (32 vols., Bristol, 1771–74, frequently reprinted in editions varying greatly in the number of volumes). His chief prose works are a standard publication in seven octavo volumes of the Methodist Book Concern, New York. The "Poetical Works" of John and Charles, ed. G. Osborn, appeared in 13 vols., London, 1868–72.
In addition to his "Sermons" and "Notes" are his "Journals" (originally published in 20 parts, London, 1740–89; new ed. by N. Curnock containing notes from unpublished diaries, 6 vols., vols. i.-ii., London and New York, 1909–11); "The Doctrine of Original Sin" (Bristol, 1757; in reply to Dr. John Taylor of Norwich); "An Earnest Appeal to Men of Reason and Religion" (originally published in three parts; 2d ed., Bristol, 1743), an elaborate defence of Methodism, describing the evils of the times in society and the church; a "Plain Account of Christian Perfection" (1766).
Wesley adapted the Book of Common Prayer for use by American Methodists. In his Watch Night service, he made use of a pietist prayer now generally known as the Wesley Covenant Prayer, perhaps his most famous contribution to Christian liturgy. He also was a noted hymn-writer, translator and compiler of a hymnal.
Wesley also wrote on divine physics, such as in "Desideratum", subtitled "Electricity made Plain and Useful by a Lover of Mankind and of Common Sense" (1759).
In spite of the proliferation of his literary output, Wesley was challenged for plagiarism for borrowing heavily from an essay by Samuel Johnson, publishing in March 1775. Initially denying the charge, Wesley later recanted and apologised officially.
Legacy.
Wesley continues to be the primary theological influence on Methodists and Methodist-heritage groups the world over; the largest bodies being the United Methodist Church, the Methodist Church of Great Britain and the African Methodist Episcopal Church. Wesleyan teachings also serve as a basis for the holiness movement, which includes denominations like the Wesleyan Church, the Free Methodist Church, the Church of the Nazarene, the Christian and Missionary Alliance, the Church of God (Anderson, IN), and several smaller groups, and from which Pentecostalism and parts of the Charismatic Movement are offshoots. Wesley's call to personal and social holiness continues to challenge Christians who attempt to discern what it means to participate in the Kingdom of God. In addition, he refined Arminianism with a strong evangelical emphasis on the Reformed doctrine of justification by faith.
He is commemorated in the Calendar of Saints of the Evangelical Lutheran Church in America on 2 March with his brother Charles. The Wesley brothers are also commemorated on 3 March in the Calendar of Saints of the Episcopal Church and on 24 May in the Anglican calendar.
Wesley's legacy is preserved in Kingswood School, which he founded in 1748 in order to educate the children of the growing number of Methodist preachers. Also, one of the four form houses at the St Marylebone Church of England School, London, is named after John Wesley.
In 2002, Wesley was listed at number 50 on the BBC's list of the 100 Greatest Britons.
In 1831, Wesleyan University in Middletown, Connecticut, was the first institution of higher education in the United States to be named after Wesley. The now secular institution was founded as an all-male Methodist college. About 20 unrelated colleges and universities in the US were subsequently named after him.
In film.
In 1954, the Radio and Film Commission of the Methodist Church in cooperation with J. Arthur Rank produced the film "John Wesley". The film was a live-action re-telling of the story of the life of John Wesley, with Leonard Sachs as Wesley.
In 2009, a more ambitious feature film, "Wesley", was released by Foundery Pictures, starring Burgess Jenkins as John Wesley, with June Lockhart as Susanna Wesley, R. Keith Harris as Charles Wesley, and the Golden Globe winner Kevin McCarthy as Bishop Ryder. The film was directed by the award-winning film-maker John Jackman.

</doc>
<doc id="42412" url="https://en.wikipedia.org/wiki?curid=42412" title="1062">
1062

__NOTOC__
Year 1062 (MLXII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Europe.
</onlyinclude>

</doc>
<doc id="42415" url="https://en.wikipedia.org/wiki?curid=42415" title="Electronic Power Control">
Electronic Power Control

(Electric) Power Control deals with routing electric power, controlling its quality, and controlling the devices attached to a power line. A number of technologies have evolved for using the power wiring to turn devices attached to the power line on and off, monitoring attached devices, and similar activities. A collective term for one set of technologies is smart buildings. It should be mentioned that unlike within commercial buildings, smart building technologies within peoples homes have been less than an overwhelming success; problems include reliability, cost, questions about how great the need is, and the fact that many smart building tasks such as turning on a lamp at dusk can be performed equally well by simpler, cheaper, and often more reliable, mechanical or electro-mechanical devices. The best known smart building technologies for the home environment are X10 and CEBus and for the commercial Lonworks, DyNet, DSI, DALI and analog systems.

</doc>
<doc id="42416" url="https://en.wikipedia.org/wiki?curid=42416" title="X10 (industry standard)">
X10 (industry standard)

X10 is a protocol for communication among electronic devices used for home automation ("domotics"). It primarily uses power line wiring for signaling and control, where the signals involve brief radio frequency bursts representing digital information. A wireless radio based protocol transport is also defined.
X10 was developed in 1975 by Pico Electronics of Glenrothes, Scotland, in order to allow remote control of home devices and appliances. It was the first general purpose domotic network technology and remains the most widely available.
Although a number of higher bandwidth alternatives exist, X10 remains popular in the home environment with millions of units in use worldwide, and inexpensive availability of new components.
History.
In 1970, a group of engineers started a company in Glenrothes, Scotland called Pico Electronics. The company developed the first single chip calculator. When calculator integrated circuit prices started to fall, Pico refocused on commercial products rather than plain ICs.
In 1974, the Pico engineers jointly developed a LP record turntable, the ADC Accutrac 4000, with Birmingham Sound Reproducers, at the time the largest manufacturer of record changers in the world. It could be programmed to play selected tracks, and could be operated by a remote control using ultrasound signals, which sparked the idea of remote control for lights and appliances. By 1975, the X10 project was conceived, so named because it was the tenth project. In 1978, X10 products started to appear in RadioShack and Sears stores. Together with BSR a partnership was formed, with the name X10 Ltd. At that time the system consisted of a 16 channel command console, a lamp module, and an appliance module. Soon after came the wall switch module and the first X10 timer.
In the 1980s, the CP-290 computer interface was released. Software for the interface runs on the Commodore 64, Apple II, Macintosh, MS-DOS, and MS-Windows.
In 1985, BSR went out of business, and X10 (USA) Inc. was formed. In the early 1990s, the consumer market was divided into two main categories, the ultra-high-end with a budget at US$100,000 and the mass market with budgets at US$2,000 to US$35,000. CEBus (1984) and LonWorks (1991) were attempts to improve reliability and replace X10.
Power line carrier control overview.
Household electrical wiring which powers lights and appliances is used to send digital data between X10 devices. This data is encoded onto a 120 kHz carrier which is transmitted as bursts during the relatively quiet zero crossings of the 50 or 60 Hz AC alternating current waveform. One bit is transmitted at each zero crossing.
The digital data consists of an address and a command sent from a controller to a controlled device. More advanced controllers can also query equally advanced devices to respond with their status. This status may be as simple as "off" or "on", or the current dimming level, or even the temperature or other sensor reading. Devices usually plug into the wall where a lamp, television, or other household appliance plugs in; however some built-in controllers are also available for wall switches and ceiling fixtures.
The relatively high-frequency carrier wave carrying the signal cannot pass through a power transformer or across the phases of a multiphase system. For split phase systems, the signal can be passively coupled from phase-to-phase using a passive capacitor, but for three phase systems or where the capacitor provides insufficient coupling, an active X10 repeater can be used. To allow signals to be coupled across phases and still match each phase's zero crossing point, each bit is transmitted three times in each half cycle, offset by 1/6 cycle.
It may also be desirable to block X10 signals from leaving the local area so, for example, the X10 controls in one house do not interfere with the X10 controls in a neighboring house. In this situation, inductive filters can be used to attenuate the X10 signals coming into or going out of the local area.
Protocol.
Whether using power line or radio communications, packets transmitted using the X10 control protocol consist of a four bit "house code" followed by one or more four bit "unit codes", finally followed by a four bit command. For the convenience of users configuring a system, the four bit house code is selected as a letter from A through P while the four bit unit code is a number 1 through 16.
When the system is installed, each controlled device is configured to respond to one of the 256 possible addresses (16 house codes × 16 unit codes); each device reacts to commands specifically addressed to it, or possibly to several broadcast commands.
The protocol may transmit a message that says "select code A3", followed by "turn on", which commands unit "A3" to turn on its device. Several units can be addressed before giving the command, allowing a command to affect several units simultaneously. For example, "select A3", "select A15", "select A4", and finally, "turn on", causes units A3, A4, and A15 to all turn on.
Note that there is no restriction that prevents using more than one house code within a single house. The "all lights on" command and "all units off" commands will only affect a single house code, so an installation using multiple house codes effectively has the devices divided into separate zones.
One way vs two way.
Inexpensive X10 devices only receive commands and do not acknowledge their status to the rest of the network. Two-way controller devices allow for a more robust network but cost two to four times more and require two-way X10 devices.
Physical layer details.
In the 60 Hz AC current flow, a bit value of one is represented by a 1 millisecond burst of 120 kHz at the zero crossing point (nominally 0°, but within 200 microseconds of the zero crossing point), immediately followed by the absence of a pulse. A zero value is represented by the absence of 120 kHz at the zero crossing point (pulse), immediately followed by the presence of a pulse. All messages are sent twice to reduce false signaling. After allowing for retransmission, line control, etc., data rates are around 20 bit/s, making X10 data transmission so slow that the technology is confined to turning devices on and off or other very simple operations.
In order to provide a predictable start point, every data frame transmitted always begin with a "start code" of 1110. Immediately after the start code, a "house code" (A–P) appears, and after the letter code comes a "function code". Function codes may specify a unit number code (1–16) or a command code, the selection between the two modes being determined by the last bit where 0=unit number and 1=command. One start code, one letter code, and one function code is known as an X10 frame and represent the minimum components of a valid X10 data packet.
Each frame is sent twice in succession to make sure the receivers understand it over any power line noise for purposes of redundancy, reliability, and to accommodate line repeaters.
Whenever the data changes from one address to another address, from an address to a command, or from one command to another command, the data frames must be separated by at least 6 clear zero crossings (or "000000"). The sequence of six zeros resets the device decoder hardware.
Later developments (1997) of hardware are improvements of the native X10 hardware. In Europe (2001) for the 230 VAC 50 Hz market. All improved products use the same X10 protocol and are compatible.
RF protocol.
To allow for wireless keypads, remote switches, motion sensors, et cetera, an RF protocol is also defined. X10 wireless devices send data packets that are nearly identical to the NEC IR protocol used by many IR remotes, and a radio receiver then provides a bridge which translates these radio packets to ordinary X10 power line control packets. The wireless protocol operates at a frequency of 310 MHz in the U.S. and 433.92 MHz in European systems.
The devices available using the radio protocol include:
Hardware support.
Device modules.
Depending on the load that is to be controlled, different modules must be used. For incandescent lamp loads, a "lamp module" or "wall switch" module can be used. These modules switch the power using a TRIAC solid state switch and are also capable of dimming the lamp load. Lamp modules are almost silent in operation, and generally rated to control loads ranging from approximately 40 to 500 watts.
For loads other than incandescent lamps, such as fluorescent lamps, high-intensity discharge lamps, and electrical home appliances, the triac-based electronic switching in the lamp module is unsuitable and an "appliance module" must be used instead. These modules switch the power using an impulse relay. In the U.S., these modules are generally rated to control loads up to 15 amperes (1800 watts at 120V).
Many device modules offer a feature called "local control". If the module is switched off, operating the power switch on the lamp or appliance will cause the module to turn on. In this way, a lamp can still be lit or a coffee pot turned on without the need to use an X10 controller. Wall switch modules may not offer this feature.
Some wall switch modules offer a feature called "local dimming". Ordinarily, the local push button of a wall switch module simply offers on/off control with no possibility of locally dimming the controlled lamp. If local dimming is offered, holding down the push button will cause the lamp to cycle through its brightness range.
Higher end modules have more advanced features such as programmable on levels, customizable fade rates, the ability to transmit commands when used (referred to as 2-way devices), and "scene" support.
There are sensor modules that sense and report temperature, light, infra-red, motion, or contact openings and closures. Device modules include thermostats, audible alarms and controllers for low voltage switches.
Controllers.
X10 controllers range from extremely simple to very sophisticated.
The simplest controllers are arranged to control four X10 devices at four sequential addresses (1–4 or 5–8). The controllers typically contain the following buttons:
More sophisticated controllers can control more units and/or incorporate timers that perform preprogrammed functions at specific times each day. Units are also available that use passive infrared motion detectors or photocells to turn lights on and off based on external conditions.
Finally, very sophisticated units are available that can be fully programmed or, like the X10 Firecracker, use a program running in an external computer. These systems can execute many different timed events, respond to external sensors, and execute, with the press of a single button, an entire "scene", turning lights on, establishing brightness levels, and so on. Control programs are available for computers running Microsoft Windows, Apple's Macintosh, Linux and FreeBSD operating systems.
Burglar alarm systems are also available. These systems contain door/window sensors, as well as motion sensors that use a coded radio frequency (RF) signal to identify when they are tripped or just to routinely check-in and give a heart-beat signal to show that the system is still active. Users can arm and disarm their system via several different remote controls that also use a coded RF signal to ensure security. When an alarm is triggered the console will make an outbound telephone call with a recorded message. The console will also use X10 protocols to flash lights when an alarm has been triggered while the security console sounds an external siren. Using X10 protocols, signals will also be sent to remote sirens for additional security.
Bridges.
There are bridges to translate X10 to other domotic standards (e.g., KNX). ioBridge can be used to translate the X10 protocol to a web service API via the X10 PSC04 Powerline Interface Module. The magDomus home controller from magnocomp allows interconnection and inter-operation between most home automation technologies.
Limitations.
Compatibility.
Solid-state switches used in X10 controls pass a very small leakage current. Compact fluorescent lamps may display nuisance blinking when switched off; CFL manufacturers recommend against controlling lamps with solid-state timers or remote controls.
Some X10 controllers with triac solid-state outputs may not work well or at all with low power devices (below 50 watts) or devices like fluorescent bulbs that do not present resistive loads, due to the small leakage current of the device and any protective filtering around the triac. Use of an appliance module, which has a relay with metallic contacts, rather than a lamp module, may resolve this problem.
Not all devices can be used on a dimmer. Fluorescent lamps are not dimmable with incandescent lamp dimmers; certain models of compact fluorescent lamps are dimmable but cost more. Motorized appliances such as fans, etc. generally will not operate as expected on a dimmer.
Wiring and interfering sources.
One problem with X10 is excessive attenuation of signals between the two live conductors in the 3-wire 120/240 volt system used in typical North American residential construction. Signals from a transmitter on one live conductor may not propagate through the high impedance of the distribution transformer winding to the other live conductor. Often, there's simply no reliable path to allow the X10 signals to propagate from one transformer leg wire to the other; this failure may come and go as large 240 volt devices such as stoves or dryers are turned on and off. (When turned on, such devices provide a low-impedance bridge for the X10 signals between the two leg wires.) This problem can be permanently overcome by installing a capacitor between the leg wires as a path for the X10 signals; manufacturers commonly sell signal couplers that plug into 240 volt sockets that perform this function. More sophisticated installations install an active repeater device between the legs, while others combine signal amplifiers with a coupling device. A repeater is also needed for inter-phase communication in homes with three-phase electric power. In many countries outside North America, entire houses are typically wired from a single 240 volt single-phase wire, so this problem does not occur.
Television receivers or household wireless devices may cause spurious "off" or "on" signals. Noise filtering (as installed on computers as well as many modern appliances) may help keep external noise out of X10 signals, but noise filters not designed for X10 may also attenuate X10 signals traveling on the branch circuit to which the appliance is connected.
Certain types of power supplies used in modern electronic equipment, such as computers, television receivers and satellite receivers, attenuate passing X10 signals by providing a low impedance path to high frequency signals. Typically, the capacitors used on the inputs to these power supplies short the X10 signal from line to neutral, suppressing any hope of X10 control on the circuit near that device. Filters are available that will block the X10 signals from ever reaching such devices; plugging offending devices into such filters can cure mysterious X10 intermittent failures.
Having a backup power supply or standby power supply such as used with computers or other electronic devices can totally kill that leg in a household installation because of the filtering used in the power supply.
Commands getting lost.
X10 signals can only be transmitted one command at a time, first by addressing the device to control, and then sending an operation for that device to perform. If two X10 signals are transmitted at the same time they may collide or interleave, leading to commands that either cannot be decoded or that trigger incorrect operations. The CM15A and RR501 Transceiver can avoid these signal collisions that can sometimes occur with other models.
Relatively slow.
The X10 protocol is slow. It takes roughly three quarters of a second to transmit a device address and a command. While generally not noticeable when using a tabletop controller, it becomes a noticeable problem when using 2-way switches or when utilizing some sort of computerized controller. The apparent delay can be lessened somewhat by using slower device dim rates. With more advanced modules another option is to use group control (lighting scene) extended commands. These allow adjusting several modules at once by a single command.
Limited functionality.
X10 protocol does support more advanced control over the dimming speed, direct dim level setting and group control (scene settings). This is done via extended message set, which is an official part of X10 standard. However support for all extended messages is not mandatory, and many cheaper modules implement only the basic message set. These require adjusting each lighting circuit one after the other, which can be visually unappealing and also very slow.
Interference and lack of encryption.
The standard X10 power line and RF protocols lack support for encryption, and can only address 256 devices. Unfiltered power line signals from close neighbors using the same X10 device addresses may interfere with each other. Interfering RF wireless signals may similarly be received, with it being easy for anyone nearby with an X10 RF remote to wittingly or unwittingly cause mayhem if an RF to power line device is being used on a premises.

</doc>
<doc id="42418" url="https://en.wikipedia.org/wiki?curid=42418" title="Cable">
Cable

An electrical cable is made of two or more wires running side by side and bonded, twisted, or braided together to form a single assembly, the ends of which can be connected to two devices, enabling the transfer of electrical signals from one device to the other. Cables are used for a wide range of purposes, and each must be tailored for that purpose. Cables are used extensively in electronic devices for power and signal circuits. Long-distance communication takes place over undersea cables. Power cables are used for bulk transmission of alternating and direct current power, especially using high-voltage cable. Electrical cables are extensively used in building wiring for lighting, power and control circuits permanently installed in buildings. Since all the circuit conductors required can be installed in a cable at one time, installation labor is saved compared to certain other wiring methods.
The term originally referred to a nautical line of specific length where multiple ropes, each laid clockwise, are then laid together anti-clockwise and shackled to produce a strong thick line, resistant to water absorption, that was used to anchor large ships. In mechanics, cables, otherwise known as wire ropes, are used for lifting, hauling, and towing or conveying force through tension. In electrical engineering cables are used to carry electric currents. An optical cable contains one or more optical fibers in a protective jacket that supports the fibers.
Etymology.
Ropes made of a lot of strands of natural fibers such as, hemp, sisal, manila, and also cotton have been used for 1000 years for hoisting and hauling.{tristanwashere} By the 19th century, deeper mines as well as construction of larger and larger sailing ships increased demand for stronger ropes. In 1830 the Royal Navy defined a cable as three hawser laid (clockwise) ropes, each approximately 120 fathoms in length, laid anti-clockwise, tightly twisted and shackled to a resulting length of approximately 100 fathoms. The tight twists, shortened the overall length of the ropes but both strengthened the ropes and reduced the ability of the rope to absorb water making them ideal for mooring.
Improvements to steelmaking techniques made high-quality steel available at lower cost, and so wire ropes became common in mining and other industrial applications while continuing the practice of anti-cyclical twists to strengthen them even further. By the middle of the 19th century, manufacture of large submarine telegraph cables was done using machines similar to those used for manufacture of mechanical cables. As the move from rope to wire happened, the specific length associated with a cable fell into disuse.
As electricity became even more ubiquitous the practice of using more than bare copper led to groupings of wires and various sheathing and shackling methods that resembled the mechanical cabling so the term was adopted for electrical wiring. In the 19th century and early 20th century, electrical cable was often insulated using cloth, rubber or paper. Plastic materials are generally used today, except for high-reliability power cables. The term has also come to be associated with communications because of its use in electrical communications.
Electrical cables.
Electrical cable is an assembly consisting of one or more conductors with their own insulations and optional screens, individual covering(s), assembly protection and protective covering(s). Electrical cables may be made more flexible by stranding the wires. In this process, smaller individual wires are twisted or braided together to produce larger wires that are more flexible than solid wires of similar size. Bunching small wires before concentric stranding adds the most flexibility. Copper wires in a cable may be bare, or they may be plated with a thin layer of another metal, most often tin but sometimes gold, silver or some other material. Tin, gold, and silver are much less prone to oxidation than copper, which may lengthen wire life, and makes soldering easier. Tinning is also used to provide lubrication between strands. Tinning was used to help removal of rubber insulation. Tight lays during stranding makes the cable extensible (CBA – as in telephone handset cords).
Cables can be securely fastened and organized, such as by using trunking, cable trays, cable ties or cable lacing. Continuous-flex or flexible cables used in moving applications within cable carriers can be secured using strain relief devices or cable ties.
At high frequencies, current tends to run along the surface of the conductor. This is known as the skin effect.
Cables and electromagnetic fields.
Any current-carrying conductor, including a cable, radiates an electromagnetic field. Likewise, any conductor or cable will pick up energy from any existing electromagnetic field around it. These effects are often undesirable, in the first case amounting to unwanted transmission of energy which may adversely affect nearby equipment or other parts of the same piece of equipment; and in the second case, unwanted pickup of noise which may mask the desired signal being carried by the cable, or, if the cable is carrying power supply or control voltages, pollute them to such an extent as to cause equipment malfunction.
The first solution to these problems is to keep cable lengths in buildings short, since pick up and transmission are essentially proportional to the length of the cable. The second solution is to route cables away from trouble. Beyond this, there are particular cable designs that minimize electromagnetic pickup and transmission. Three of the principal design techniques are shielding, coaxial geometry, and twisted-pair geometry.
Shielding makes use of the electrical principle of the Faraday cage. The cable is encased for its entire length in foil or wire mesh. All wires running inside this shielding layer will be to a large extent decoupled from external "electrical" fields, particularly if the shield is connected to a point of constant voltage, such as earth or ground. Simple shielding of this type is not greatly effective against low-frequency "magnetic" fields, however - such as magnetic "hum" from a nearby power transformer. A grounded shield on cables operating at 2.5 kV or more gathers leakage current and capacitive current, protecting people from electric shock and equalizing stress on the cable insulation.
Coaxial design helps to further reduce low-frequency magnetic transmission and pickup. In this design the foil or mesh shield has a circular cross section and the inner conductor is exactly at its center. This causes the voltages induced by a magnetic field between the shield and the core conductor to consist of two nearly equal magnitudes which cancel each other.
A twisted pair has two wires of a cable twisted around each other. This can be demonstrated by putting one end of a pair of wires in a hand drill and turning while maintaining moderate tension on the line. Where the interfering signal has a wavelength that is long compared to the pitch of the twisted pair, alternate lengths of wires develop opposing voltages, tending to cancel the effect of the interference.
Fire protection.
In building construction, electrical cable jacket material is a potential source of fuel for fires. To limit the spread of fire along cable jacketing, one may use cable coating materials or one may use cables with jacketing that is inherently fire retardant. The plastic covering on some metal clad cables may be stripped off at installation to reduce the fuel source for fires. Inorganic coatings and boxes around cables safeguard the adjacent areas from the fire threat associated with unprotected cable jacketing. However, this fire protection also traps heat generated from conductor losses, so the protection must be thin.
To provide fire protection to a cable, the insulation is treated with fire retardant materials, or non-combustible mineral insulation is used (MICC cables
Hybrid cables.
Hybrid optical and electrical cables can be used in wireless outdoor fiber-to-the-antenna (FTTA) applications. In these cables, the optical fibers carry information, and the electrical conductors are used to transmit power. These cables can be placed in several environments to serve antenna mounted on poles, towers or other structures.
According to Telcordia GR-3173, "Generic Requirements for Hybrid Optical and Electrical Cables for Use in Wireless Outdoor Fiber To The Antenna (FTTA) Applications," these hybrid cables are intended to carry optical fibers, twisted pair/quad elements, coaxial cables or current-carrying electrical conductors under a common outer jacket. The power conductors used in these hybrid cables are for directly powering an antenna or for powering tower-mounted electronics exclusively serving an antenna. They have a nominal voltage normally less than 60 VDC or 108/120 VAC. However, other voltages may be present depending on the application and the relevant National Electrical Code (NEC).
Since the voltage levels and power levels used within these hybrid cables vary, for the purposes of applicable codes, the hybrid cable shall be considered a power cable. As noted in GR-3173, from an NESC perspective (i.e., IEEE C2, "National Electrical Safety Code® [HeyStraven®]"), since these cables are not communications cables and are not power limited, they are considered power cables and need to comply with clearance, separation, and other safety rules.
See also.
For transmission see: Power cable, High-voltage cable and HVDC

</doc>
<doc id="42419" url="https://en.wikipedia.org/wiki?curid=42419" title="Harald Bluetooth">
Harald Bluetooth

Harald "Blåtand" Gormsson (, ) (probably born c. 935) was a King of Denmark and Norway.
He was the son of King Gorm the Old and of Thyra Dannebod. He died in 985 or 986 having ruled as King of Denmark from c. 958 and King of Norway for a few years; probably around 970. Some sources say his son Sweyn Forkbeard forcibly deposed him.
The Jelling stones.
Harald had the Jelling stones erected to honour his parents. The "Encyclopædia Britannica" considers the runic inscriptions as the best-known in Denmark. The biography of Harald Bluetooth is summed up by this runic inscription from the Jelling stones:
"King Harald bade these memorials to be made after Gorm, his father, and Thyra, his mother. The Harald who won the whole of Denmark and Norway and turned the Danes to Christianity."
Denmark's conversion to Christianity.
King Harald Bluetooth's conversion to Christianity is a contested bit of history, not least because medieval writers such as Widukind of Corvey and Adam of Bremen give conflicting accounts of how it came about.
Widukind of Corvey, writing during the lives of King Harald and Otto I, claims that Harald was converted by a "cleric by the name of Poppa" who, when asked by Harald to prove his faith in Christ, carried a "great weight" of iron heated by a fire without being burned.
Adam of Bremen, writing 100 years after King Harald's death in "History of the Archbishops of Hamburg-Bremen", finished in 1076, describes Harald being forcibly converted by Otto I, after a defeat in battle. However, Widukind does not mention such an event in his contemporary "Res gestae saxonicae sive annalium libri tres" or "Deeds of the Saxons". Four hundred years later, the "Heimskringla" relates that Harald was converted with Earl Haakon, by Otto II.
A cleric named Poppa, perhaps the same one, also appears in Adam of Bremen's history, but in connection with Eric of Sweden, who had supposedly conquered Denmark (there is no evidence anywhere else that this happened). The story of this otherwise unknown Poppo or Poppa's miracle and baptism of Harald is also depicted on the gilded altar piece in the Church of Tamdrup in Denmark (see image at top of this article). The altar itself dates to about 1200. Adam of Bremen's claim regarding Otto I and Harald appears to have been inspired by an attempt to manufacture a historical reason for the archbishops of Hamburg-Bremen to claim jurisdiction over Denmark (and thus the rest of Scandinavia); in the 1070s, the Danish King was in Rome asking for Denmark to have its own arch-bishop, and Adam's account of Harald's supposed conversion (and baptism of both him and his "little son" Sweyn, with Otto serving as Sweyn's godfather) is followed by the unambiguous claim that "At that time Denmark on this side of the sea, which is called Jutland by the inhabitants, was divided into three dioceses and subjected to the bishopric of Hamburg."
As noted above, Harald's father, Gorm the Old, had died in 958, and had been buried in a mound with many goods, after the pagan practice. The mound itself was from c. 500 BCE, but Harald had it built higher over his father's grave, and added a second mound to the south. Mound-building was a newly revived custom in the 10th century, perceivably as an "appeal to old traditions in the face of Christian customs spreading from Denmark's southern neighbors, the Germans."
After his conversion, around the 960s, Harald had his father's body reburied in the church next to the now empty mound, and erected one of the Jelling stones described above.
Harald undoubtedly professed Christianity at that time and contributed to its growth, but with limited success in Denmark and Norway.
Reign.
During his reign, Harald oversaw the reconstruction of the Jelling runic stones, and numerous other public works. The most famous is fortifying the fortress of Aros (nowadays Aarhus, capital of Jutland) which was situated in a central position in his kingdom in the year 979. Some believe these projects were a way for him to consolidate economic and military control of his country and the main city. Ring forts were built in five strategic locations with Aarhus perfectly in the middle: Trelleborg on Zealand, Borrering in eastern Zealand (the inner construction of this fort is still yet to be established), Nonnebakken on Funen, Fyrkat in Himmerland (northern Jutland) and Aggersborg near Limfjord. All five fortresses had similar designs: ""perfectly circular with gates opening to the four corners of the earth, and a courtyard divided into four areas which held large houses set in a square pattern."" A sixth "Trelleborg" of similar design, located at Borgeby, in Scania, has been dated to about 1000 and may also have been built by King Harald and a second fort named Trelleborg is located near the modern town of Trelleborg in Scania in present-day Sweden, but is of older date and thus pre-dates the reign of Harald Bluetooth.
He constructed the oldest known bridge in southern Scandinavia, the 5 meters wide, 760 meters long Ravninge Bridge at Ravninge meadows.
While quiet prevailed throughout the interior, he turned his energies to foreign enterprises. He came to the help of Richard the Fearless of Normandy in 945 and 963, while his son conquered Samland, and after the assassination of King Harald Greycloak of Norway, managed to force the people of that country into temporary subjugation to himself.
The Norse sagas present Harald in a rather negative light. He was forced twice to submit to the renegade Swedish prince Styrbjörn the Strong of the Jomsvikings- first by giving Styrbjörn a fleet and his daughter Thyra, the second time by giving up himself as hostage, along with yet another fleet. When Styrbjörn brought this fleet to Uppsala to claim the throne of Sweden, Harald broke his oath and fled with his Danes to avoid facing the Swedish army at the Battle of Fýrisvellir.
As a consequence of Harald's army having lost to the Germans at the Danevirke in 974, he no longer had control of Norway, and Germans settled back into the border area between Scandinavia and Germany. They were driven out of Denmark in 983 by an alliance of Obodrite soldiers and troops loyal to Harald, but soon after, Harald was killed fighting off a rebellion led by his son Swein. He is believed to have died in 986, although several accounts claim 985 as his year of death.
From 1835 to 1977 it was wrongly believed, that Harald ordered the death of the Haraldskær Woman, a bog body previously thought to be Gunnhild, Mother of Kings until radiocarbon dating proved otherwise.
The nickname "Bluetooth".
Harald's nickname "Bluetooth" (blátǫnn) first documented appearance is in the "Chronicon Roskildense" from 1140. The usual explanation is that Harald must have had a conspicuous bad tooth that appeared "blue" (i.e. black, as "blue" meant dark).
Another explanation, is that he was called Thegn in England (corrupted to "tan" when the name came back into Old Norse) — in England, Thane meant chief. Since blue meant "dark", his nickname was really "dark chieftain".
A third theory, according to curator at the Royal Jelling Hans Ole Mathiesen, was that Harald went about clothed in blue. The blue color was in fact the most expensive, so by walking in blue Harald underlined his royal dignity.
The Hiddensee treasure.
One of the lesser known archaeological finds in Germany is the Hiddensee treasure which was discovered by chance in the 19th century on the tiny island Hiddensee in the Baltic Sea.
In each of the years 1864 and 1872 the island Hiddensee was hit by severe storm floods. The first storm flood broke Hiddensee in two when the entire centre section of the island was flooded, something which could only be reversed by extensive building measures. After the second storm flood the famous Hiddensee treasure was supposed to have been found.
All items of the Hiddensee treasure, including a brooch (fibula), a neck ring and 14 pendants are of pure gold and weigh approximately 600 grams altogether. It is plausible that the treasure originally belonged to the family of Harald Bluetooth.
A replica of the Hiddensee treasure can be seen today in the Hiddensee Local History Museum. The original is kept in the Stralsund Museum of Cultural History.
The Hiddensee treasure has an insured value amounting to 70M EUR.
The curmsun disc.
In autumn 2014, the curmsun disc, a golden disc bearing the name of Harald Bluetooth, appeared in Sweden. The disc is mainly made of gold and is measuring 4.5 cm in diameter and has a weight of 25.23 grams. On the obverse there is a Latin inscription and on the reverse there is a cross and an octagon. The inscription translates as "Harald Gormsson king of Danes, Scania, Jomsborg, Oldenburg". It is assumed that the disc was a part of a Viking hoard found in 1840, close to the island of Wolin, Poland. The disc was rediscovered in 2014 by an 11-year-old schoolgirl who found it in an old casket and then brought it to school.
The curmsun disc underwent metallurgical analysis at University of Lund in Sweden. The analysis showed that the alloy has a gold content ranging between 83,3-92,8%. According to a hypothesis by Swedish archeologist Sven Rosborn the disc may have been created by a Frankish monk in connection with Harald's death. Danish anthropologist Karen Schousboe presents another hypothesis as she believes that the disc was a wedding gift, created in the 960s. She also sees a connection to 10th-12th century Byzantine talismans.
Mateusz Bogucki, of University of Warsaw, states in an interview with Anders Lundt Hansen in the Danish magazine Weekendavisen that the type of cross and the letters on the Curmsun Disc do not correspond to crosses and letters on coins of the 10th century. Mateusz Bogucki has noticed similarities between the cross on the curmsun disc with crosses on coins from Halle/Schwaben in the 13th century.
Swedish numismatist Lars O. Lagerqvist, former chief of the Swedish Royal Coin Cabinet and specialist in counterfeits, says that nothing on the curmsun disc indicates a counterfeit. He has noticed similarities between the type faces on the curmsun disc and type faces on coins from 10th and 11th century.
Cultural depictions and references.
Bluetooth communication protocol.
"Bluetooth" now commonly refers to the Bluetooth wireless specification design started by Ericsson, Nokia, Intel and Toshiba to enable cable-free connections between computers, mobile phones, PDAs, printers, etc. The Bluetooth communications protocol in these devices is named after the king, because he unified Denmark and Norway much like the technology whose goal was to unify computers and cellular phones.
The Bluetooth logo consists of the younger futhark runes, also known as the Nordic runes for his initials, H ( hagall) and B ( berkanan) (Long-branch runes version). The younger futhark rune names have been documented; "hagalaz" and "berkana" etc. are posited elder futhark rune names in a posited proto-Germanic.
Sid Meier's Civilization V.
Harald Bluetooth, depicted as leader of Denmark in the video game Sid Meier's Civilization V. He was released along with Denmark as DLC, in the May 2011 DLC The Civilization and Scenario Pack.

</doc>
<doc id="42420" url="https://en.wikipedia.org/wiki?curid=42420" title="LAN switching">
LAN switching

LAN switching is a form of packet switching used in local area networks (LAN). Switching technologies are crucial to network design, as they allow traffic to be sent only where it is needed in most cases, using fast, hardware-based methods. LAN switching uses different kinds of network switches. A standard switch is known as a "layer 2 switch" and is commonly found in nearly any LAN. "Layer 3" or "layer 4" switches require advanced technology (see managed switch) and are more expensive, and thus are usually only found in larger LANs or in special network environments.
Layer 2 switching.
Layer 2 switching uses the media access control address (MAC address) from the host's network interface cards (NICs) to decide where to forward frames. Layer 2 switching is hardware-based, which means switches use application-specific integrated circuit (ASICs) to build and maintain filter tables (also known as MAC address tables or CAM tables). One way to think of a layer 2 switch is as a multiport bridge.
Layer 2 switching provides the following
Layer 2 switching is highly efficient because there is no modification to the data packet and the frame encapsulation of the packet changes only when the data packet is passing through dissimilar media (such as from Ethernet to FDDI). Layer 2 switching is used for workgroup connectivity and network segmentation (breaking up collision domains). This allows a flatter network design with more network segments than traditional networks joined by repeater hubs and routers.
Layer 2 switching has helped develop new components in the network infrastructure.
These new technologies allow more data to flow off from local subnets and onto a routed network, where a router's performance can become the bottleneck.
Limitations.
Layer 2 switches have the same limitations as bridge networks. Bridges are good if a network is designed by the 80/20 rule: users spend 80 percent of their time on their local segment.
Bridged networks break up collision domains, but the network remains one large broadcast domain. Similarly, layer 2 switches (bridges) cannot break up broadcast domains, which can cause performance issues and limits the size of your network. Broadcast and multicasts, along with the slow convergence of spanning tree, can cause major problems as the network grows. 
Because of these problems, layer 2 switches cannot completely replace routers in the internetwork.
Layer 3 switching.
Layer 3 switching is solely based on (destination) IP address stored in the header of IP datagram (see layer 4 switching later on this page for the difference). The difference between a layer 3 switch and a router is the way the device is making the routing decision. Traditionally, routers use microprocessors to make forwarding decisions in software, while the switch performs only hardware-based packet switching (by specialized ASIC with the help of content-addressable memory). However, some traditional routers can have advanced hardware functions as well in some of the higher-end models.
The main advantage of layer 3 switches is the potential to lower latency as it can route a packet at the same time it would need to forward it as switch. In extreme, connecting two distinct segments (e.g. VLANs) with a router external to a standard layer 2 switch requires passing the frame from the switch to the router (first L2 hop), making the routing decision inside the router (L3 hop), and then passing the frame back to the switch (second L2 hop). A layer 3 switch accomplishes the same task with a single hop, combining the L3 hop with just one L2 hop.
As layer 3 switches have no different functionally than a traditional router, they could be placed anywhere in the network because they cost-effectively replace expensive advanced routers with similar throughput and latency. The operations that a layer 3 switch does in the same manner as a traditional router are:
The benefits of layer 3 switching include the following:
The switching algorithm is relatively simple and is the same for most of the routed protocols: a host would like to send a packet to a host on another network. Having acquired a router's address by some means, the source host sends the packet directly to that router's physical (MAC) address. The protocol (network layer) address is that of the destination host.
The router examines the packet's destination protocol address and determines whether it knows how to forward the packet or not. If the router does not know how to forward the packet, it typically drops the packet. If it knows how to forward packet, it changes the destination physical address to that of the next hop router and transmits the packet.
The next hop may be the destination or the next router, which executes the same switching process. As the packet moves through the internetwork, its physical address changes, but its protocol address remains same.
IEEE has developed hierarchical terminology that is useful in describing this process. Network devices without the capability to forward packets between subnetworks are called end systems (ESs, singular ES), whereas network devices with these capabilities are called intermediate systems (ISs). ISs are further divided into those that communicate only within their routing domain (Intradomain ES) and those that communicate both within and between routing domains (Interdomains IS). A routing domain is generally considered as portion of an internetwork under common administrative authority and is regulated by a particular set of administrative guidelines. Routing domains are also called autonomous systems.
Layer 4 switching.
Layer 4 switching means hardware-based layer 3 switching technology that can also consider the type of network traffic (for example, distinguish between HTTP, FTP or VoIP). Layer 4 switching provides additional datagram inspection by reading the port numbers found in the Transport layer header to make routing decisions (i.e. ports used by UDP or TCP). These port numbers are found in RFC 1700 and reference the upper-layer protocol, program, or application.
The largest benefit of layer 4 switching is that the network administrator can configure a layer 4 switch to prioritize data traffic by application, which means a QoS can be defined for each user. For example, a number of users can be defined as a Video group and be assigned more priority, or band-width, based on the need for video conferencing.
Layer 4 information has been used to help make routing decisions for quite a while. For example, extended access lists can filter packets based on layer 4 port numbers. Another example is accounting information gathered by open standards using sFlow provided by companies like Arista Networks or proprietary solutions like NetFlow switching in Cisco's higher-end routers.
Multi-layer switching (MLS).
Multi-layer switching combines layer 2, 3 and 4 switching technologies and provides high-speed scalability with low latency. It accomplishes this high combination of high-speed scalability with low latency by using huge filter tables based on the criteria designed by the network administrator.
Multi-layer switching can move traffic at wire speed and also provide layer 3 routing, which can remove the bottleneck from the network routers. This technology is based on the idea of "route once, switch many".
Multi-layer switching can make routing/switching decisions based on the following
There is no performance difference between a layer 3 and a layer 4 switch because the routing/switching is all hardware based (routing decision is done by specialized ASIC with the help of content-addressable memory).

</doc>
<doc id="42423" url="https://en.wikipedia.org/wiki?curid=42423" title="Monetarism">
Monetarism

In monetary economics, monetarism is a school of thought that emphasises the role of governments in controlling the amount of money in circulation. Monetarists believe that variation in the money supply has major influences on national output in the short run and the price level over longer periods, and that objectives of monetary policy are best met by targeting the growth rate of the money supply rather than by engaging in discretionary monetary policy.
Monetarism today is mainly associated with the work of Milton Friedman, who was among the generation of economists to accept Keynesian economics and then criticise Keynes' theory of gluts using fiscal policy (government spending). Friedman and Anna Schwartz wrote an influential book, "A Monetary History of the United States, 1867–1960", and argued "inflation is always and everywhere a monetary phenomenon." Though he opposed the existence of the Federal Reserve, Friedman advocated, given its existence, a central bank policy aimed at keeping the supply and demand for money at equilibrium, as measured by growth in productivity and demand.
Description.
Monetarism is an economic theory that focuses on the macroeconomic effects of the supply of money and central banking. Formulated by Milton Friedman, it argues that excessive expansion of the money supply is inherently inflationary, and that monetary authorities should focus solely on maintaining price stability.
This theory draws its roots from two historically antagonistic schools of thought: the hard money policies that dominated monetary thinking in the late 19th century, and the monetary theories of John Maynard Keynes, who, working in the inter-war period during the failure of the restored gold standard, proposed a demand-driven model for money. While Keynes had focused on the value stability of currency, with the resulting panics based on an insufficient money supply leading to alternate currency and collapse, then Friedman focused on price stability, which is the equilibrium between supply and demand for money.
The result was summarised in a historical analysis of monetary policy, "Monetary History of the United States 1867–1960", which Friedman coauthored with Anna Schwartz. The book attributed inflation to excess money supply generated by a central bank. It attributed deflationary spirals to the reverse effect of a failure of a central bank to support the money supply during a liquidity crunch.
Friedman originally proposed a fixed "monetary rule", called Friedman's k-percent rule, where the money supply would be automatically increased by a fixed percentage per year. Under this rule, there would be no leeway for the central reserve bank as money supply increases could be determined "by a computer", and business could anticipate all money supply changes. With other monetarists he believed that the active manipulation of the money supply or its growth rate is more likely to destabilise than stabilise the economy.
Opposition to the gold standard.
Most monetarists oppose the gold standard. Friedman, for example, viewed a pure gold standard as impractical. For example, whereas one of the benefits of the gold standard is that the intrinsic limitations to the growth of the money supply by the use of gold or silver would prevent inflation, if the growth of population or increase in trade outpaces the money supply, there would be no way to counteract deflation and reduced liquidity (and any attendant recession) except for the mining of more gold or silver under a gold or silver standard.
Rise.
Clark Warburton is credited with making the first solid empirical case for the monetarist interpretation of business fluctuations in a series of papers from 1945.p. 493 Within mainstream economics, the rise of monetarism accelerated from Milton Friedman's 1956 restatement of the quantity theory of money. Friedman argued that the demand for money could be described as depending on a small number of economic variables.
Thus, where the money supply expanded, people would not simply wish to hold the extra money in idle money balances; i.e., if they were in equilibrium before the increase, they were already holding money balances to suit their requirements, and thus after the increase they would have money balances surplus to their requirements. These excess money balances would therefore be spent and hence aggregate demand would rise. Similarly, if the money supply were reduced people would want to replenish their holdings of money by reducing their spending. In this, Friedman challenged a simplification attributed to Keynes suggesting that "money does not matter." Thus the word 'monetarist' was coined.
The rise of the popularity of monetarism also picked up in political circles when Keynesian economics seemed unable to explain or cure the seemingly contradictory problems of rising unemployment and inflation in response to the collapse of the Bretton Woods system in 1972 and the oil shocks of 1973. On the one hand, higher unemployment seemed to call for Keynesian reflation, but on the other hand rising inflation seemed to call for Keynesian disinflation.
In 1979, President Jimmy Carter appointed a Federal Reserve chief Paul Volcker, who made inflation fighting his primary objective, and restricted the money supply (in accordance with the Friedman rule) to tame inflation in the economy. The result was the creation of the desired price stability .
Monetarists not only sought to explain present problems; they also interpreted historical ones. Milton Friedman and Anna Schwartz in their book "A Monetary History of the United States, 1867–1960" argued that the Great Depression of 1930 was caused by a massive contraction of the money supply and not by the lack of investment Keynes had argued. They also maintained that post-war inflation was caused by an over-expansion of the money supply.
They made famous the assertion of monetarism that 'inflation is always and everywhere a monetary phenomenon'. Many Keynesian economists initially believed that the Keynesian vs. monetarist debate was solely about whether fiscal or monetary policy was the more effective tool of demand management. By the mid-1970s, however, the debate had moved on to other issues as monetarists began presenting a fundamental challenge to Keynesianism.
Many monetarists sought to resurrect the pre-Keynesian view that market economies are inherently stable in the absence of major unexpected fluctuations in the money supply. Because of this belief in the stability of free-market economies they asserted that active demand management (e.g. by the means of increasing government spending) is unnecessary and indeed likely to be harmful. The basis of this argument is an equilibrium between "stimulus" fiscal spending and future interest rates. In effect, Friedman's model argues that current fiscal spending creates as much of a drag on the economy by increased interest rates as it creates present consumption: that it has no real effect on total demand, merely that of shifting demand from the investment sector (I) to the consumer sector (C).
When Margaret Thatcher, leader of the Conservative Party in the United Kingdom, won the 1979 general election defeating the incumbent Labour Party led by James Callaghan, Britain had endured several years of severe inflation, which was rarely below 10% and by the time of the election in May 1979 stood at 5.4%. Thatcher implemented monetarism as the weapon in her battle against inflation, and succeeded at reducing it to 4.6% by 1983.
James Callaghan himself had adopted policies echoing monetarism while serving as prime minister from 1976 to 1979, adopting deflationary policies and reducing public spending in response to high inflation and national debt. He initially had some success, as inflation was below 10% by the summer of 1978, although unemployment now stood at 1,500,000. However, by the time of his election defeat barely a year later, inflation had fallen to 5.4%.
Criticism.
According to Alan Blinder and Robert Solow, fiscal policy becomes impotent only when an interest-elasticity of the demand for money is zero. Empirically, such a perfect inelasticity does not occur. However, there are limited policy options when the interest rate is at or near the zero lower bound
Although Milton Friedman believed that wealth effects make deficit spending contractionary, Blinder and Solow believed that in reality fiscal stimulus is effective. To see this, they used a government budget constraint equation which includes interest on government bonds: 
where B is the number of bonds whose face value per unit bond is 1 dollar. T is the tax function. In the long-run stationary state, 
which gives 
Immediately, it turns out that under money financing the fiscal multiplier becomes 
because in this case 
formula_5. Both monetarists and Keynesians agree with the idea that money-financed deficit spending has an expansionary impact on the economy. If deficits are financed by bonds, the long-run fiscal multiplier becomes larger than that by money-creation:
Thus their research shows that, in the long run, bond-financed government spending increases the income level more than money-financed deficit spending does.
Practice.
A realistic theory should be able to explain the deflationary waves of the late 19th century, the Great Depression, and the stagflation period beginning with the uncoupling of exchange rates in 1972. Monetarists argue that there was no inflationary investment boom in the 1920s. Instead, monetarist thinking centers on the contraction of the M1 during the 1931–1933 period, and argues from there that the Federal Reserve could have avoided the Great Depression by moves to provide sufficient liquidity. In essence, they argue that there was an insufficient supply of money.
From their conclusion that incorrect central bank policy is at the root of large swings in inflation and price instability, monetarists argued that the primary motivation for excessive easing of central bank policy is to finance fiscal deficits by the central government. Hence, restraint of government spending is the most important single target to restrain excessive monetary growth.
With the failure of demand-driven fiscal policies to restrain inflation and produce growth in the 1970s, the way was paved for a new policy of fighting inflation through the central bank, which would be the bank's cardinal responsibility. In typical economic theory, this would be accompanied by austerity shock treatment, as is generally recommended by the International Monetary Fund: such a course was taken in the United Kingdom, where government spending was slashed in the late 1970s and early 1980s under the political ascendance of Prime Minister Margaret Thatcher. In the United States, the opposite approach was taken and real government spending increased much faster during President Ronald Reagan's first four years (4.22%/year) than it did under Carter (2.55%/year).
In the ensuing short term, unemployment in both countries remained stubbornly high while central banks raised interest rates to restrain credit. These policies dramatically reduced inflation rates in both countries (the United States' inflation rate fell from almost 14% in 1980 to around 3% in 1983 ), allowing liberalisation of credit and the reduction of interest rates, which led ultimately to the inflationary economic booms of the 1980s. Arguments have been raised, however, that the fall of the inflation rate may be less from control of the money supply and more to do with the unemployment level's effect on demand; some also claim the use of credit to fuel economic expansion is itself an anti-monetarist tool, as it can be argued that an increase in money supply alone constitutes inflation.
Monetarism re-asserted itself in central bank policy in western governments at the end of the 1980s and beginning of the 1990s, with a contraction both in spending and in the money supply, ending the booms experienced in the US and UK.
1990s.
In the late 1980s, Paul Volcker was succeeded by Alan Greenspan. His handling of monetary policy in the run-up to the 1991 recession was criticised from the right as being excessively tight, and costing George H. W. Bush re-election. The incoming Democratic president Bill Clinton reappointed Alan Greenspan, and kept him as a core member of his economic team. Greenspan, while still fundamentally monetarist in orientation, argued that doctrinaire application of theory was insufficiently flexible for central banks to meet emerging situations.
The crucial test of this flexible response by the Federal Reserve was the Asian financial crisis of 1997–1998, which the Federal Reserve met by flooding the world with dollars, and organising a bailout of Long-Term Capital Management. Some have argued that 1997–1998 represented a monetary policy bind, just as the early 1970s had represented a fiscal policy bind, and that while asset inflation had crept into the United States (which demanded that the Fed tighten the money supply), the Federal Reserve needed to ease liquidity in response to the capital flight from Asia. Greenspan himself noted this when he stated that the American stock market showed signs of irrationally high valuations.
In 2000, Alan Greenspan raised interest rates several times. These actions were believed by many to have caused the bursting of the dot-com bubble. In late 2001, as a decisive reaction to the September 11 attacks and the various corporate scandals which undermined the economy, the Greenspan-led Federal Reserve initiated a series of interest rate cuts that brought the Federal Funds rate down to 1% in 2004. His critics, notably Steve Forbes, attributed the rapid rise in commodity prices and gold to Greenspan's loose monetary policy, and by late 2004 the price of gold was higher than its 12-year moving average; these same forces were also blamed for excessive asset inflation and the weakening of the dollar . These policies of Alan Greenspan are blamed by the followers of the Austrian School for creating excessive liquidity, causing lending standards to deteriorate, and resulting in the housing bubble of 2004–2006.
Currently, the American Federal Reserve follows a modified form of monetarism, where broader ranges of intervention are possible in light of temporary instabilities in market dynamics. This form does not yet have a generally accepted name.
In Europe, the European Central Bank follows a more orthodox form of monetarism, with tighter controls over inflation and spending targets as mandated by the Economic and Monetary Union of the European Union under the Maastricht Treaty to support the euro. This more orthodox monetary policy followed credit easing in the late 1980s through 1990s to fund German reunification, which was blamed for the weakening of European currencies in the late 1990s.
Current state.
Since 1990, the classical form of monetarism has been questioned. This is because of events that many economists interpreted as being inexplicable in monetarist terms: the disconnection of the money supply growth from inflation in the 1990s and the failure of pure monetary policy to stimulate the economy in the 2001–2003 period. Greenspan argued that the 1990s decoupling was explained by a virtuous cycle of productivity and investment on one hand, and a certain degree of "irrational exuberance" in the investment sector on the other.
There are also arguments linking monetarism and macroeconomics, and treat monetarism as a special case of Keynesian theory. The central test case over the validity of these theories would be the possibility of a liquidity trap, like that experienced by Japan. Ben Bernanke, Princeton professor and another former chairman of the U.S. Federal Reserve, argued that monetary policy could respond to zero interest rate conditions by direct expansion of the money supply. In his words, "We have the keys to the printing press, and we are not afraid to use them." Progressive economist Paul Krugman has advanced the counterargument that this would have a corresponding devaluationary effect, like the sustained low interest rates of 2001–2004 produced against world currencies.
These disagreements — along with the role of monetary policies in trade liberalisation, international investment and central bank policy — remain lively topics of investigation and argument.
See also.
General:

</doc>
<doc id="42424" url="https://en.wikipedia.org/wiki?curid=42424" title="Treblinka extermination camp">
Treblinka extermination camp

Treblinka () was an extermination camp, built by Nazi Germany in occupied Poland during World War II. It was located near the village of Treblinka north-east of Warsaw in what is now the Masovian Voivodeship. The camp operated between 23 July 1942 and 19 October 1943 as part of Operation Reinhard, the most deadly phase of the Final Solution. During this time, it is estimated that between 700,000 and 900,000 Jews were killed in its gas chambers, along with 2,000 Romani people. More Jews were killed at Treblinka than at any other Nazi extermination camp apart from Auschwitz.
Managed by the German SS and the Eastern European "Trawnikis" (also known as "Hiwi" guards), the camp consisted of two separate units. Treblinka I was a forced-labour camp ("Arbeitslager") whose prisoners worked in the gravel pit or irrigation area and in the forest, where they cut wood to fuel the crematoria. Between 1941 and 1944, more than half of its 20,000 inmates died from summary executions, hunger, disease and mistreatment.
The second camp, Treblinka II, was an extermination camp ("Vernichtungslager"). A small number of men who were not killed immediately upon arrival became its Jewish slave-labour units called "Sonderkommandos," forced to bury the victims' bodies in mass graves. These bodies were exhumed in 1943 and cremated on large open-air pyres along with the bodies of new victims. Gassing operations at Treblinka II ended in October 1943 following a revolt by the "Sonderkommandos" in early August. Several SS Hiwi guards were killed and 200 prisoners escaped from the camp; almost a hundred survived the subsequent chase. The camp was dismantled ahead of the Soviet advance. A farmhouse for a watchman was built on the site and the ground ploughed over in an attempt to hide the evidence of genocide.
In postwar Poland, the government bought most of the land where the camp had stood, and built a large stone memorial there between 1959 and 1962. In 1964 Treblinka was declared a national monument of Jewish martyrology in a ceremony at the site of the former gas chambers. In the same year the first German trials were held regarding war crimes committed at Treblinka by former SS members. After the end of communism in Poland in 1989, the number of visitors coming to Treblinka from abroad increased. An exhibition centre at the camp opened in 2006. It was later expanded and made into a branch of the Siedlce Regional Museum.
Background.
Following the invasion of Poland in 1939 most of the 3.5 million Polish Jews were rounded up and put into newly established ghettos by Nazi Germany. The system was intended to isolate the Jews from the outside world in order to facilitate their exploitation and abuse. The supply of food was inadequate, living conditions were cramped and unsanitary, and the Jews had no way to earn money. Malnutrition and lack of medicine led to soaring mortality rates. The initial victories of the Wehrmacht over the Soviet Union inspired plans for the German colonisation of occupied Poland, including all territory within the General Government. At the Wannsee Conference held near Berlin on 20 January 1942, new plans were outlined for the genocide of the Jews, known as the "Final Solution" to the Jewish Question. The extermination programme was codenamed "Aktion Reinhard" in German, to differentiate it from the "Einsatzgruppen" operations in territories conquered by Nazi Germany, in which half a million Jews had already been killed.
Treblinka was one of three secret extermination camps set up for Operation Reinhard; the other two were Bełżec and Sobibór. All three were equipped with gas chambers disguised as shower rooms, for the "processing" of entire transports of people. The lethal agent was established following a pilot project of mobile killing conducted at Soldau and Chełmno extermination camp that began operating in 1941 and used gas vans. Chełmno (German: "Kulmhof") was a testing ground for the establishment of faster methods of killing and incinerating people. It was not a part of Reinhard, which was marked by the construction of stationary facilities for mass murder. Treblinka was the third extermination camp of Operation Reinhard to be built, following Bełżec and Sobibór, and incorporated lessons learned from their construction. Alongside the Reinhard camps, mass killing facilities using Zyklon B were developed at the Majdanek concentration camp in March 1942 and at Auschwitz II-Birkenau in September.
The Nazi plan to kill Polish Jews from across the General Government during "Aktion Reinhard" was overseen in occupied Poland by "SS-Obergruppenführer" Odilo Globocnik, the deputy of "Reichsführer-SS" Heinrich Himmler in Berlin. The Operation Reinhard camps reported directly to the Reich Main Security Office (German: "Reichssicherheitshauptamt" or RSHA for short), which was also headed by Himmler. The staff of Operation Reinhard, most of whom had been involved in the Action T4 euthanasia programme, used T4 as a framework for the construction of facilities. All of the Jews who were killed in the Reinhard camps came from ghettos.
Location.
The two parallel camps of Treblinka were built northeast of the Polish capital Warsaw. Before World War II, it was the site of a gravel mining enterprise for the production of concrete, connected to most of the major cities in central Poland by the Małkinia–Sokołów Podlaski railway junction and the Treblinka village station. The mine was owned and operated by the Polish industrialist Marian Łopuszyński, who added the new railway track to the existing line. When the German SS took over Treblinka I, the quarry was already equipped with heavy machinery that was ready to use. Treblinka was well-connected but isolated enough, halfway between some of the largest Jewish ghettos in Nazi-occupied Europe, including the ghetto in Warsaw and the ghetto in Białystok, the capital of the newly formed "Bezirk Bialystok". The Warsaw Ghetto had 500,000 Jewish inmates, and the Białystok Ghetto had about 60,000.
Treblinka was divided into two separate camps that were 2 kilometres apart. Two engineering firms, the Schoenbronn Company of Leipzig and the Warsaw branch of Schmidt–Munstermann, oversaw the construction of both camps. Between 1942 and 1943 the extermination centre was further redeveloped with a crawler excavator. New gas chambers made of brick and cement mortar were freshly erected, and mass cremation pyres were also introduced. The perimeter was enlarged to provide a buffer zone, making it impossible to approach the camp from the outside. The number of trains caused panic among the residents of nearby settlements. They would likely have been killed if caught near the railway tracks.
Treblinka I.
Founded officially on 15 November 1941, Treblinka I was a forced-labour camp ("Arbeitslager"), initially for Poles and Jews captured nearby. It replaced an "ad hoc" company set up in June 1941 by "Sturmbannführer" Ernst Gramss. A new barracks and barbed wire fencing tall were erected in late 1941. To obtain the workforce for Treblinka I, civilians were sent to the camp "en masse" for real or imagined offences, and sentenced to hard labour by the Gestapo office in Sokołów, which was headed by Gramms. The average length of a sentence was six months, but many prisoners had their sentences extended indefinitely. Twenty thousand people passed through Treblinka I during its three-year existence. About half of them died there from exhaustion, hunger and disease. Those who survived were released after serving their sentences; these were generally Poles from nearby villages.
At any given time, Treblinka I had a workforce of 1,000–2,000 prisoners, most of whom worked 12- to 14-hour shifts in the large quarry and later also harvested wood from the nearby forest as fuel for the open-air crematoria in Treblinka II. There were German, Czech and French Jews among them, as well as Poles captured in "łapankas", farmers unable to deliver food requisitions, hostages trapped by chance, and people who attempted to harbour Jews outside the Jewish ghettos or who performed restricted actions without permits. Beginning in July 1942, Jews and non-Jews were separated. Women mainly worked in the sorting barracks, where they repaired and cleaned military clothing delivered by freight trains, while most of the men worked at the gravel mine. There were no work uniforms, and inmates who lost their own shoes were forced to go barefoot or scavenge them from dead prisoners. Water was rationed, and punishments were regularly delivered at roll-calls. From December 1943 the inmates were no longer carrying any specific sentences. The camp operated officially until 23 July 1944, when the imminent arrival of Soviet forces led to its abandonment.
During its entire operation, Treblinka I's commandant was "Sturmbannführer" Theodor van Eupen. He ran the camp with several SS men and almost 100 "Hiwi" guards. The quarry, spread over an area of , supplied road construction material for German military use and was part of the strategic road-building programme in the war with the Soviet Union. It was equipped with a mechanical digger for shared use by both Treblinka I and II. Eupen worked closely with the SS and German police commanders in Warsaw during the deportation of Jews in early 1943 and had prisoners brought to him from the Warsaw Ghetto for the necessary replacements. According to Franciszek Ząbecki, the local station master, Eupen often killed prisoners by "taking shots at them, as if they were partridges". A widely feared overseer was "Untersturmführer" Franz Schwarz, who executed prisoners with a pickaxe or hammer.
Treblinka II.
Treblinka II (officially the "SS-Sonderkommando Treblinka") was divided into three parts: Camp 1 was the administrative compound where the guards lived, Camp 2 was the receiving area where incoming transports of prisoners were offloaded, and Camp 3 was the location of the gas chambers. All three parts were built by two groups of German Jews expelled from Berlin and imprisoned at the Warsaw Ghetto (a total of 238 men from 17 to 35 years of age). "Hauptsturmführer" Richard Thomalla, the head of construction, brought in German Jews because they could speak German. Construction began on 10 April 1942, when Bełżec and Sobibór were already in operation. The entire death camp, which was either or in size (sources vary), was surrounded by two rows of barbed-wire fencing tall. This fence was later woven with pine tree branches to obstruct the view of the camp. More Jews were brought in from the surrounding settlements to work on the new railway ramp within the Camp 2 receiving area, which was ready by June 1942.
The first section of Treblinka II (Camp 1) was the "Wohnlager" administrative and residential compound; it had a telephone line. The main road within the camp was paved and named "Seidel Straße" after "Unterscharführer" Kurt Seidel, the SS corporal who supervised its construction. A few side roads were lined with gravel. The main gate for road traffic was erected on the north side. Barracks were built with supplies delivered from Warsaw, Sokołów Podlaski, and Kosów Lacki. There was a kitchen, a bakery, and dining rooms; all were equipped with high-quality items taken from Jewish ghettos. The Germans and Ukrainians each had their own sleeping quarters, positioned at an angle for better control of all entrances. There were also two barracks behind an inner fence for the Jewish work commandos. "SS-Untersturmführer" Kurt Franz set up a small zoo in the centre next to his horse stables, with two foxes, two peacocks and a roe deer (brought in 1943). Smaller rooms were built as laundry, tailors, and cobblers, and for woodworking and medical aid. Closest to the SS quarters were separate barracks for the Polish and Ukrainian serving, cleaning and kitchen women.
The next section of Treblinka II (Camp 2, also called the lower camp or "Auffanglager"), was the receiving area where the railway unloading ramp extended from the Treblinka line into the camp. There was a platform surrounded by barbed-wire fencing. A new building, erected on the platform, was disguised as a railway station complete with a wooden clock and fake rail terminal signs. "SS-Scharführer" Josef Hirtreiter worked on the unloading ramp and was remembered for being especially cruel; he grabbed children by their feet and smashed their heads against wagons. Behind a second fence, about from the track, there were two long barracks used for undressing, with a cashier's booth which collected money and jewellery, ostensibly for safekeeping. Jews who resisted were taken away or beaten to death by the guards. The area where the women and children were shorn of their hair was on the other side of the path from the men. All buildings in the lower camp, including the barber barracks, contained the piled up clothing and belongings of the prisoners. Further to the right, there was a fake infirmary called "Lazaret", with the Red Cross sign on it. It was a small barracks surrounded by barbed wire where the sick, old, wounded and "difficult" prisoners were taken. Directly behind the "Lazaret" building there was an open excavation pit seven metres (23 ft) deep. These prisoners were led to the edge of the pit and shot one at a time by "Blockführer" Willi Mentz, nicknamed "Frankenstein" by the inmates. Mentz single-handedly executed thousands of Jews, aided by his supervisor, August Miete, who was called the "Angel of Death" by the prisoners. The pit was also used to burn identity papers deposited by new arrivals at the undressing area.
The third section of Treblinka II (Camp 3, also called the upper camp) was the main killing zone with gas chambers at its centre. It was completely screened from the railway tracks by an earth bank built with the help of a mechanical digger. This mound was elongated in shape, similar to a retaining wall, and can be seen in a sketch produced during the 1967 trial of Treblinka II commandant Franz Stangl. On the other sides, the zone was camouflaged from new arrivals like the rest of the camp, using tree branches woven into barbed wire fences by the "Tarnungskommando" (the work detail led out to collect them). From the undressing barracks there was a fenced-off path leading through the forested area into the gas chambers. It was cynically called "die Himmelstraße" ("the road to heaven") or "der Schlauch" ("the tube") by the SS. For the first eight months of the camp's operation, the excavator was used to dig burial ditches on both sides of the gas chambers; these ditches were long, wide, and deep. In early 1943, they were replaced with cremation pyres up to long, with rails laid across the pits on concrete blocks. The 300 prisoners who operated the upper camp lived in separate barracks behind the gas chambers.
Killing process.
Unlike other Nazi concentration camps across German-occupied Europe, in which prisoners were used as forced labour for the German war effort, death camps ("Vernichtungslager") like Treblinka, Bełżec, and Sobibór had only one function: to kill those sent there. To prevent incoming victims from realising its nature, Treblinka II was disguised as a transit camp for deportations further east, complete with made-up train schedules, a fake train-station clock with hands painted on it, names of destinations, a fake ticket window, and the sign "Ober Majdan", a code word for Treblinka commonly used to deceive passengers departing from Western Europe. Majdan was a prewar landed estate away from the camp.
Polish Jews.
The mass deportation of Jews from the Warsaw Ghetto began on 22 July 1942 with the first shipment of 6,000 people. The gas chambers started operation the following morning. For the next two months, deportations from Warsaw continued on a daily basis via two shuttle trains (the second one, from 6 August 1942), each carrying about 4,000 to 7,000 people crying for water. No other trains were allowed to stop at the Treblinka station. The first daily trains came in the early morning, often after an overnight wait, and the second, in mid-afternoon. All new arrivals were sent immediately to the undressing area by the "Sonderkommando" squad that managed the arrival platform, and from there to the gas chambers. According to German records, including the official report by SS "Brigadeführer" Jürgen Stroop, 265,000 Jews were transported in freight trains from the Warsaw Ghetto to Treblinka during the period from 22 July to 12 September 1942.
The rail traffic on Polish railway lines was extremely dense. An average of 420 German military trains were passing through every 24 hours on top of internal traffic already in 1941. The Holocaust trains were routinely delayed en route; some transports took many days to arrive. Hundreds of prisoners died from exhaustion, suffocation and thirst while in transit to the camp in the overcrowded wagons. In extreme cases such as the Biała Podlaska transport of 6,000 Jews travelling only a distance, up to 90 per cent of people were already dead when the sealed doors flew open. From September 1942 on, both Polish and foreign Jews were greeted with a brief verbal announcement. An earlier signboard with directions was removed because it was clearly insufficient. The deportees were told that they had arrived at a transit point on the way to Ukraine and needed to shower and have their clothes disinfected before receiving work uniforms and new orders.
Foreign Jews and Romani people.
Treblinka received transports of almost 20,000 foreign Jews between October 1942 and March 1943, including 8,000 from the German Protectorate of Bohemia and Moravia via Theresienstadt, and over 11,000 from Bulgarian-occupied Thrace, Macedonia, and Pirot following an agreement with the Nazi-allied Bulgarian government. They had train tickets and arrived predominantly in passenger carriages with considerable luggage, travel foods and drinks, all of which were taken by the SS to the food storage barracks. The provisions included such items as smoked mutton, speciality breads, wine, cheese, fruit, tea, coffee, and sweets. Unlike Polish Jews arriving in Holocaust trains from nearby ghettos in cities like Warsaw, Radom, and those of "Bezirk Bialystok", the foreign Jews received a warm welcome upon arrival from an SS man (either Otto Stadie or Willy Mätzig), after which they were killed like the others. Treblinka dealt mainly with Polish Jews, Bełżec handled the Jews from Austria and the Sudetenland, and Sobibór was the final destination for Jews from France and the Netherlands. Auschwitz-Birkenau "processed" Jews from almost every other country in Europe. The frequency of arriving transports slowed down in winter.
The decoupled locomotive went back to the Treblinka station or to the layover yard in Małkinia for the next load, while the victims were pulled from the carriages onto the platform by "Kommando Blau", one of the Jewish work details forced to assist the Germans at the camp. They were led through the gate amidst chaos and screaming. They were separated by gender behind the gate; women were pushed into the undressing barracks and barber on the left, and men were sent to the right. All were ordered to tie their shoes together and strip. Some kept their own towels. The Jews who resisted were taken to the "Lazaret", also called the "Red Cross infirmary", and shot behind it. Women had their hair cut off; therefore, it took longer to prepare them for the gas chambers than men. The hair was used in the manufacture of socks for U-boat crews and hair-felt footwear for the "Deutsche Reichsbahn".
Most of those killed at Treblinka were Jews, but about 2,000 Romani people also died there. Like the Jews, the Romani were first rounded up and sent to the ghettos; at a conference on 30 January 1940 it was decided that all 30,000 Romani living in Germany proper were to be deported to former Polish territory. Most of these were sent to Jewish ghettos in the General Government, such as those in Warsaw and Łódź. As with the Jews, most Romani who went to Treblinka died in the gas chambers, although some were shot. The majority of the Jews living in ghettos were sent to Bełżec, Sobibór, or Treblinka to be executed; most of the Romani living in the ghettos were shot on the spot. There were no known Romani escapees or survivors from Treblinka.
Gas chambers.
After undressing, the newly arrived Jews were beaten with whips to drive them towards the gas chambers; hesitant men were treated particularly brutally. Rudolf Höss, the commandant at Auschwitz, contrasted the practice at Treblinka of deceiving the victims about the showers with his own camp's practice of telling them they had to go through a "delousing" process. According to the postwar testimony of some SS officers, men were always gassed first, while women and children waited outside the gas chambers for their turn. During this time, the women and children could hear the sounds of suffering from inside the chambers, and they became aware of what awaited them, which caused panic, distress, and even involuntary defecation. According to Stangl, a train transport of about 3,000 people could be "processed" in three hours. In a 14-hour workday, 12,000 to 15,000 people were killed. After the new gas chambers were built, the duration of the killing process was reduced to an hour and a half.
The gassing area was entirely closed off with tall wooden fencing made of vertical boards. Originally, it consisted of three interconnected barracks long and wide, disguised as showers. They had double walls insulated by earth packed down in between. The interior walls and ceilings were lined with roofing paper. The floors were covered with tin-plated sheet metal, the same material used for the roof. Solid wooden doors were insulated with rubber and bolted from the outside by heavy cross-bars. The victims were gassed with the exhaust fumes from the engine of a Red Army tank captured during Operation Barbarossa; "SS-Scharführer" Erich Fuchs was responsible for installing it. The engine was brought in by the SS at the time of the camp's construction and housed in a room with a generator that supplied the camp with electricity. The tank engine exhaust pipe ran just below the ground and opened into all three gas chambers. The fumes could be seen seeping out. After about 20 minutes the bodies were removed by dozens of "Sonderkommandos", placed onto carts and wheeled away. The system was imperfect and required a lot of effort; trains that arrived later in the day had to wait on layover tracks overnight at Treblinka, Małkinia, or Wólka Okrąglik.
Between August and September 1942, a large new building with a concrete foundation was built from bricks and mortar under the guidance of Action T4 euthanasia expert Erwin Lambert. It contained 8–10 gas chambers, each of which was 8 metres by 4 metres (26 ft by 13 ft), and it had a corridor in the centre. Stangl supervised its construction and brought in building materials from the nearby village of Małkinia by dismantling factory stock. During this time victims continued to arrive daily and were led naked past the building site to the original gas chambers. people every day, a fact which Globocnik once boasted about to Kurt Gerstein, a fellow SS officer from Disinfection Services. The new gas chambers were seldom used to their full capacity; 12,000–15,000 victims remained the daily average.
The killing process at Treblinka differed significantly from the method used at Auschwitz and Majdanek, where the poison gas Zyklon B (hydrogen cyanide) was used. At Treblinka, Sobibór, and Bełżec, the victims died from suffocation and carbon monoxide poisoning. After visiting Treblinka on a guided tour, Auschwitz commandant Rudolf Höss concluded that using exhaust gas was inferior to the cyanide used at his extermination camp. The chambers became silent after 12 minutes and were closed for 20 minutes or less. According to Jankiel Wiernik, who survived the 1943 prisoner uprising and escaped, when the doors of the gas chambers had been opened, the bodies of the dead were standing and kneeling rather than lying down, due to the severe overcrowding. Dead mothers embraced the bodies of their children. Prisoners who worked in the "Sonderkommandos" later testified that the dead frequently let out a last gasp of air when they were extracted from the chambers. Some victims showed signs of life during the disposal of the corpses, but the guards routinely refused to react.
Cremation pits.
The Germans became aware of the political danger associated with the mass burial of corpses in 1943, when the Polish victims of the Soviet Katyn massacre were discovered near occupied Smolensk and reported to Berlin. Those 22,000 officers' bodies were well preserved underground, attesting to the Soviet mass murder. By April, Nazi propaganda began to draw the attention of the international community to this war crime using newsfilm. To prove their claim, the Germans brought in the Katyn Commission (a group of twelve forensic experts from various European countries) to examine the bodies in detail and report its findings; it concluded that the Soviets were responsible. The Germans attempted to use the commission's results to drive a wedge between the Allies. The secret orders to exhume the corpses buried at Treblinka and burn them came directly from the Nazi leadership, possibly from Himmler, who was very concerned about covering up Nazi crimes. The cremations began shortly after his visit to the camp in late February or early March 1943.
Within Treblinka II, there were at least two large cremation pits constructed to incinerate bodies. The pits were used to cremate the new corpses along with the old ones, which had to be dug up as they had been buried during the first six months of the camp's operation. They used rails as grates under the instructions of Herbert Floß, the camp's cremation expert. The bodies were placed on grates over wood, splashed with petrol, and burned. It was a harrowing sight, according to Jankiel Wiernik, with the bellies of pregnant women exploding from boiling amniotic fluid. He wrote that "the heat radiating from the pits was maddening." The bodies burned for five hours, without the ashing of bones. The pyres operated 24 hours a day. Once the system had been perfected, 10,000–12,000 bodies at a time could be incinerated.
The open air burn pits were located east of the new gas chambers and refuelled from 4 a.m. (or after 5 a.m. depending on work-load) to 6 p.m. in roughly 5-hour intervals. The current camp memorial includes a flat grave marker resembling one of them. It is constructed from melted basalt and has a concrete foundation. It is a symbolic grave, as the Nazis spread the actual human ashes, mixed with sand, over 22,000 square metres (237,000 square feet).
Organization of the camp.
The camp was operated by 20–25 German and Austrian members of the SS-Totenkopfverbände and 80–120 "Wachmänner" ("watchmen") guards who had been trained at a special SS facility in the Trawniki concentration camp near Lublin, Poland; all "Wachmänner" guards were trained at Trawniki. The guards were mainly ethnic German "Volksdeutsche" from the east and Ukrainians, with some Russians, Tatars, Moldovans, Latvians, and Central Asians, all of whom had served in the Red Army. They were enlisted by Karl Streibel, the commander of the Trawniki camp, from the prisoner of war (POW) camps for Soviet soldiers. The degree to which their recruitment was voluntary remains disputed; while conditions in the Soviet POW camps were dreadful, some Soviet POWs collaborated with the Germans even before cold, hunger, and disease began devastating the POW camps in mid-September 1941.
The work at Treblinka was carried out under threat of death by Jewish prisoners organised into specialised "Sonderkommando" squads or work details. At the Camp 2 "Auffanglager" receiving area each squad had a different coloured triangle. The triangles made it impossible for new arrivals to try to blend in with members of the work details. The blue unit ("Kommando Blau") managed the rail ramp and unlocked the freight wagons. They met the new arrivals, carried out people who had died en route, removed bundles, and cleaned the wagon floors. The red unit ("Kommando Rot"), which was the largest squad, unpacked and sorted the belongings of victims after they had been "processed". The red unit delivered these belongings to the storage barracks, which were managed by the yellow unit ("Kommando Gelb"), who separated the items by quality, removed the Star of David from all outer garments, and extracted any money sewn into the linings. The yellow unit was followed by the "Desinfektionskommando", who disinfected the belongings, including sacks of hair from "processed" women. The "Goldjuden" unit ("money Jews") collected and counted banknotes and evaluated the gold and jewellery.
A different group of about 300 men, called the "Totenjuden" ("Jews of death"), lived and worked in Camp 3 across from the gas chambers. For the first six months they took the corpses away for burial after gold teeth had been extracted. Once cremation began in early 1943 they took the corpses to the pits, refuelled the pyres, crushed the remaining bones with mallets, and collected the ashes for disposal. Each trainload of "deportees" brought to Treblinka consisted of an average of sixty heavily guarded wagons. They were divided into three sets of twenty at the layover yard. Each set was processed within the first two hours of backing onto the ramp, and was then made ready by the "Sonderkommandos" to be exchanged for the next set of twenty wagons.
Members of all work units were continuously beaten by the guards and often shot or hanged at the gallows. Only the strongest men were selected from new arrivals daily to obtain the necessary replacements. There were other work details which had no contact with the transports: the "Holzfällerkommando" ("woodcutter unit") cut and chopped firewood, and the "Tarnungskommando" ("disguise unit") camouflaged the structures of the camp. Another work detail was responsible for cleaning the common areas. The Camp 1 "Wohnlager" residential compound contained barracks for about 700 "Sonderkommandos" which, when combined with the 300 "Totenjuden" living across from the gas chambers, brought their grand total to roughly one thousand at a time.
Going to work bloodied and bruised would lead to execution. If a prisoner was beaten and sustained black eyes, open wounds and severe swelling, he was called a "clepsydra" (Greek for "water clock", the Polish word klepsydra is a synonym for "obituary") by the other prisoners and most likely shot that evening at roll call or the next day if the bruised cheeks began to swell up. Many "Sonderkommando" prisoners hanged themselves at night. Suicides in the "Totenjuden" barracks occurred at the rate of 15 to 20 per day. The work crews – usually unable to eat or sleep from fear and anxiety – were almost entirely replaced every few days; members of the old work detail were sent to their deaths except for the most resilient.
Treblinka prisoner uprising.
In early 1943, an underground Jewish resistance organisation was formed at Treblinka with the goal of seizing control of the camp and escaping to freedom. The planned revolt was preceded by a long period of secret preparations. The clandestine unit was first organised by a former Jewish captain of the Polish Army, Dr. Julian Chorążycki, who was described by fellow plotter Samuel Rajzman as noble and essential to the action. His organising committee included Zelomir Bloch (leadership), Rudolf Masaryk, Marceli Galewski, Samuel Rajzman, Dr. Irena Lewkowska ("Irka", from the sick bay for the "Hiwis"), Leon Haberman, Hershl (Henry) Sperling from Częstochowa, and several others. Chorążycki (who treated the German patients) killed himself with poison on 19 April 1943 when faced with imminent capture, so that the Germans could not discover the plot by torturing him. The next leader was another former Polish Army officer, Dr. Berek Lajcher, who arrived on 1 May. Born in Częstochowa, he had practised medicine in Wyszków and was expelled by the Nazis to Wegrów in 1939.
The initial date of the revolt was set for 15 June 1943, but it had to be postponed. A fighter smuggled a grenade in one of the early May trains carrying captured rebels from the Warsaw Ghetto Uprising, which had begun on 19 April 1943. When he detonated it in the undressing area, the SS and guards were thrown into a panic. After the explosion, Treblinka received only about 7,000 Jews from the capital for fear of similar incidents; the remaining 42,000 Warsaw Jews were deported to Majdanek, instead. The burning of unearthed corpses continued at full speed until the end of July. The Treblinka II conspirators became increasingly concerned about their future as the amount of work for them began to decline. With fewer transports arriving, they realised "they were next in line for the gas chambers."
Day of the revolt and survivors.
The uprising was launched on the hot summer day of 2 August 1943 (Monday, a regular day of rest from gassing), when a group of Germans and 40 Ukrainians drove off to the River Bug to swim. The conspirators silently unlocked the door to the arsenal near the train tracks, with a key that had been duplicated earlier. They had stolen 20–25 rifles, 20 hand grenades, and several pistols, and delivered them in a cart to the gravel work detail. At 3:45 p.m., 700 Jews launched an insurgency that lasted for 30 minutes. They set buildings ablaze, exploded a tank of petrol, and set fire to the surrounding structures. A group of armed Jews attacked the main gate, and others attempted to climb the fence. Machine-gun fire from about 25 Germans and 60 Ukrainian "Trawnikis" resulted in near-total slaughter. Lajcher was killed along with most of the insurgents. About 200 Jews escaped from the camp. Half of them were killed after a chase in cars and on horses. The Jews did not cut the phone wires, and Stangl called in hundreds of German reinforcements, who arrived from four different towns and set up roadblocks along the way. Partisans of the "Armia Krajowa" (Polish: Home Army) transported some of the surviving escapees across the river and others like Sperling ran 30 kilometres (19 miles) and were then helped and fed by Polish villagers. Of those who broke through, around 70 are known to have survived until the end of the war, including the future authors of published Treblinka memoirs: Richard Glazar, Chil Rajchman, Jankiel Wiernik, and Samuel Willenberg.
Among the Jewish prisoners who escaped after setting fire to the camp, there were two 19-year-olds, Samuel Willenberg and Kalman Taigman, who had both arrived in 1942 and had been forced to work there under pain of death. Taigman died in 2012 and Willenberg in 2016. Taigman stated of his experience, "It was hell, absolutely hell. A normal man cannot imagine how a living person could have lived through it – killers, natural-born killers, who without a trace of remorse just murdered every little thing." Willenberg and Taigman emigrated to Israel after the war and devoted their last years to retelling the story of Treblinka. Escapees Hershl Sperling and Richard Glazar both suffered from survivor guilt syndrome and eventually killed themselves.
After the uprising.
In spite of the revolt, Treblinka II continued to function and remained a top priority for the SS for another year. Stangl met the head of Operation Reinhard, Odilo Globocnik, and inspector Christian Wirth in Lublin, and decided not to draft a report, as no native Germans had died putting down the revolt. Stangl wanted to rebuild the camp, but Globocnik told him it would be closed down shortly and Stangl would be transferred to Trieste to help fight the partisans there. The Nazi high command may have felt that Stangl, Globocnik, Wirth, and other Reinhard personnel knew too much and wanted to dispose of them by sending them to the front. With almost all the Jews from the German ghettos (established in Poland) killed, there would have been little point in rebuilding the facility. Auschwitz had enough capacity to fulfil the Nazis' remaining extermination needs, rendering Treblinka redundant.
The camp's new commandant Kurt Franz, formerly its deputy commandant, took over in August. After the war he testified that gassings had stopped by then. In reality, despite the extensive damage to the camp, the gas chambers were intact, and the killing of Polish Jews continued. Speed was reduced, with only ten wagons rolled onto the ramp at a time, while the others had to wait. The last two rail transports of Jews were brought to the camp for gassing from the Białystok Ghetto on 18 and 19 August 1943. They consisted of 78 wagons (37 the first day and 39 the second), according to a communiqué published by the Office of Information of the "Armia Krajowa", based on observation of Holocaust trains passing through the village of Treblinka. The 39 wagons that came to Treblinka on 19 August 1943 were carrying at least 7,600 survivors of the Białystok Ghetto Uprising.
On 19 October 1943, Operation Reinhard was terminated by a letter from Odilo Globocnik. The following day, a large group of Jewish "Arbeitskommandos" who had worked on dismantling the camp structures over the previous few weeks were loaded onto the train and transported, via Siedlce and Chełm, to Sobibór to be gassed on 20 October 1943. Franz followed Globocnik and Stangl to Trieste in November. Clean-up operations continued over the winter. As part of these operations, Jews from the surviving work detail dismantled the gas chambers brick-by-brick and used them to erect a farmhouse on the site of the camp's former bakery. Globocnik confirmed its purpose as a secret guard post for a Nazi-Ukrainian agent to remain behind the scenes, in a letter he sent to Himmler from Trieste on 5 January 1944. A "Hiwi" guard called Oswald Strebel, a Ukrainian "Volksdeutscher" (ethnic German), was given permission to bring his family from Ukraine for "reasons of surveillance", wrote Globocnik; Strebel had worked as a guard at Treblinka II. He was instructed to tell visitors that he had been farming there for decades, but the local Poles were well aware of the existence of the camp.
Operational command of Treblinka II.
Irmfried Eberl.
"SS-Obersturmführer" Irmfried Eberl was appointed the camp's first commandant on 11 July 1942. He was a psychiatrist from Bernburg Euthanasia Centre and the only physician-in-chief to command an extermination camp during World War II. According to some, his poor organisational skills caused the operation of Treblinka to turn disastrous; others point out that the number of transports that were coming in reflected the Nazi high command's wildly unrealistic expectations of Treblinka's ability to "process" these prisoners. The early gassing machinery frequently broke down due to overuse, forcing the SS to shoot Jews assembled for suffocation. The workers did not have enough time to bury them, and the mass graves were overflowing. According to the testimony of his colleague "Unterscharführer" Hans Hingst, Eberl's ego and thirst for power exceeded his ability: "So many transports arrived that the disembarkation and gassing of the people could no longer be handled." On incoming Holocaust trains to Treblinka, many of the Jews locked inside correctly guessed what was going to happen to them. The odour of decaying corpses could be smelled up to away.
Oskar Berger, a Jewish eyewitness who escaped during the 1943 uprising, told of the camp's state when he arrived there in August 1942:
When Odilo Globocnik made a surprise visit to Treblinka on 26 August 1942 with Christian Wirth and Wirth's adjutant from Bełżec, Josef Oberhauser, Eberl was dismissed on the spot. Among the reasons for dismissal were: incompetently disposing of the tens of thousands of dead bodies, using inefficient methods of killing, and not properly concealing the mass killing. Eberl was transferred to Berlin, closer to operational headquarters in Hitler's Chancellery, where the main architect of the Holocaust, Heinrich Himmler, had just stepped up the pace of the programme. Globocnik assigned Wirth to remain in Treblinka temporarily to help clean up the camp. On 28 August 1942, Globocnik suspended deportations. He chose Franz Stangl, who had previously been the commandant of the Sobibór extermination camp, to assume command of the camp as Eberl's successor. Stangl had a reputation as a competent administrator with a good understanding of the project's objectives, and Globocnik trusted that he would be capable of resuming control.
Franz Stangl.
Stangl arrived at Treblinka in late August 1942. He replaced Eberl on 1 September. Years later, he described what he first saw when he came on the scene, in a 1971 interview with Gitta Sereny:
Stangl reorganised the camp, and the transports of Warsaw and Radom Jews began to arrive again on 3 September 1942. According to Israeli historian Yitzhak Arad, Stangl wanted the camp to look attractive, so he ordered the paths paved in the "Wohnlager" administrative compound. Flowers were planted along "Seidel Straße" as well as near the SS living quarters. He ordered that all arriving prisoners should be greeted by the SS with a verbal announcement translated by the working Jews. The deportees were told that they were at a transit point on the way to Ukraine. Some of their questions were answered by Germans wearing lab coats as tools for deception. At times Stangl carried a whip and wore a white uniform, so he was nicknamed the "White Death" by prisoners. Although he was directly responsible for the camp's operations, according to his own testimony Stangl limited his contact with Jewish prisoners as much as possible. He claimed that he rarely interfered with the cruel acts perpetrated by his subordinate officers at the camp. He became desensitised to the killings, and came to perceive prisoners not as humans but merely as "cargo" that had to be destroyed, he said.
Treblinka song.
According to postwar testimonies, when transports were temporarily halted, then-deputy commandant Kurt Franz wrote lyrics to a song meant to celebrate the Treblinka extermination camp. In reality, prisoner Walter Hirsch wrote them for him. The melody came from something Franz remembered from Buchenwald. The music was upbeat, in the key of D major. The song was taught to the newly arriving Jews assigned to work in the "Sonderkommando". They were forced to memorise it by nightfall of their first day at the camp. Survivor Samuel Willenberg remembered the song beginning: "With firm steps we march ..." Years later, "Unterscharführer" Franz Suchomel recalled the lyrics as follows: "We know only the word of the Commander. / We know only obedience and duty. / We want to keep working, working, / until a bit of luck beckons us some time. Hurray!"
A musical ensemble was formed, under duress, by Artur Gold, a popular Jewish prewar composer from Warsaw. He arranged the theme to the Treblinka song for the prisoner orchestra which he conducted. Gold arrived in Treblinka in 1942 and played music in the SS mess hall at the "Wohnlager" on German orders. He died during the uprising.
Kurt Franz.
After the Treblinka revolt in August 1943 and termination of Operation Reinhard in October 1943, Stangl went with Globocnik to Trieste in northern Italy where SS reinforcements were needed. The third and last Treblinka II commandant was Kurt Franz, nicknamed "Lalka" () by the prisoners because he had "an innocent face". According to survivor Hershl Sperling, as deputy commandant Franz beat prisoners to death for minor infractions or had his dog Barry tear them to pieces. He managed Treblinka II until November 1943. The subsequent clean-up of the Treblinka II perimeter was completed by prisoners of nearby Treblinka I "Arbeitslager" in the following months. Franz's deputy was "Hauptscharführer" Fritz Küttner, who maintained a network of "Sonderkommando" informers and did the hands-on killings.
Kurt Franz maintained a photo album against orders never to take photographs inside Treblinka. He named it "Schöne Zeiten" ("Good Times"). His album is a rare source of images illustrating the mechanised grave digging, brickworks in Małkinia and the Treblinka zoo, among others. Franz was careful not to photograph the gas chambers.
The Treblinka I gravel mine functioned at full capacity under the command of Theodor van Eupen until July 1944, with new forced labourers sent to him by "Kreishauptmann" Ernst Gramss from Sokołów. The mass shootings continued into 1944. With Soviet troops closing in, the last 300 to 700 "Sonderkommandos" disposing of the incriminating evidence were executed by "Trawnikis" in late July 1944, long after the camp's official closure. Strebel, the ethnic German who had been installed in the farmhouse built in place of the camp's original bakery using bricks from the gas chambers, set fire to the building and fled to avoid capture.
Arrival of the Soviets.
In late July 1944, Soviet forces began to approach from the east. The departing Germans who already destroyed most direct evidence of genocidal intent burned surrounding villages to the ground, including 761 buildings in Poniatowo, Prostyń, and Grądy. Many families were killed. The fields of grain that once fed the SS were burned. On 19 August 1944, German forces blew up the church in Prostyń and its bell tower, the last defensive strongpoint against the Red Army in the area. When the Soviets entered Treblinka on 16 August, the extermination zone had been levelled, ploughed over, and planted with lupins. What remained, wrote visiting Soviet war correspondent Vasily Grossman, were small pieces of bone in the soil, human teeth, scraps of paper and fabric, broken dishes, jars, shaving brushes, rusted pots and pans, cups of all sizes, mangled shoes, and lumps of human hair. The road leading to the camp was pitch black. Until mid-1944 human ashes (up to 20 carts every day) had been regularly strewn by the remaining prisoners along the road for in the direction of Treblinka I. When the war ended, destitute and starving locals started walking up the Black Road (as they began to call it) in search of man-made nuggets shaped from melted gold in order to buy bread.
Early attempts at preservation.
The new Soviet-installed government did not preserve evidence of the camp. The scene was not legally protected at the conclusion of World War II. In September 1947, 30 students from the local school, led by their teacher Feliks Szturo and priest Józef Ruciński, collected larger bones and skull fragments into farmers' wicker baskets and buried them in a single mound. The same year the first remembrance committee "Komitet Uczczenia Ofiar Treblinki" (KUOT; Committee for the Remembrance of the Victims of Treblinka) formed in Warsaw, and launched a design competition for the memorial. 
Stalinist officials allocated no funding for the design competition nor for the memorial, and the committee disbanded in 1948; by then many survivors had left the country. In 1949, the town of Sokołów Podlaski protected the camp with a new fence and gate. A work crew with no archaeological experience was sent in to landscape the grounds. In 1958, after the end of Stalinism in Poland, the Warsaw provincial council declared Treblinka to be a place of martyrology. Over the next four years, 127 hectares (318 acres) of land that had formed part of the camp was purchased from 192 farmers in the villages of Prostyń, Grądy, Wólka Okrąglik and Nowa Maliszewa.
Construction of the memorial.
The construction of a monument tall designed by sculptor Franciszek Duszeńko was inaugurated on 21 April 1958 with the laying of the cornerstone at the site of the former gas chambers. The sculpture represents the trend toward large avant-garde forms introduced in the 1960s throughout Europe, with a granite tower cracked down the middle and capped by a mushroom-like block carved with abstract reliefs and Jewish symbols. Treblinka was declared a national monument of martyrology on 10 May 1964 during an official ceremony attended by 30,000 people. The monument was unveiled by Zenon Kliszko, the Marshal of the Sejm of the Republic of Poland, in the presence of survivors of the Treblinka uprising from Israel, France, Czechoslovakia and Poland. The camp custodian's house (built nearby in 1960) was turned into an exhibition space following the collapse of communism in Poland in 1989 and the retirement of the custodian; it opened in 2006. It was later expanded and made into a branch of the Siedlce Regional Museum.
Death count.
There are many estimates of the total number of people killed at Treblinka; most scholarly estimates range from 700,000 to 900,000, meaning that more Jews died at Treblinka than at any other Nazi extermination camp apart from Auschwitz. The Treblinka museum in Poland states that at least 800,000 people died at Treblinka; Yad Vashem, which is Israel's Holocaust museum, puts the number killed at 870,000; and the United States Holocaust Memorial Museum gives a range of 870,000 to 925,000.
First estimates.
The first estimate of the number of people killed at Treblinka came from Vasily Grossman, a Soviet war reporter who visited Treblinka in July 1944 as the Soviet forces marched westward across Poland. He published an article called "The Hell Called Treblinka", which appeared in the November 1944 issue of "Znayma", a monthly Russian literary magazine. In the article he claimed that 3 million people had been killed at Treblinka. He may not have been aware that the short station platform at Treblinka II greatly reduced the number of wagons that could be unloaded at one time, and may have been adhering to the Soviet trend of exaggerating Nazi crimes for propaganda purposes. In 1947 the Polish historian Zdzisław Łukaszkiewicz estimated the death count as 780,000, based on the accepted record of 156 transports with an average of 5,000 prisoners each.
Court exhibits and affidavits.
The Treblinka trials of the 1960s took place in Düsseldorf and produced the two official West German estimates. During the 1965 trial of Kurt Franz, the Court of Assize in Düsseldorf concluded that at least 700,000 people were killed at Treblinka, following a report by Dr. Helmut Krausnick, director of the Institute for Contemporary History in Munich. During Franz Stangl's trial in 1969 the same court reassessed the number to be at least 900,000 after new evidence from Dr. Wolfgang Scheffler.
A chief witness for the prosecution at Düsseldorf in the 1965, 1966, 1968 and 1970 trials was Franciszek Ząbecki, who was employed by the "Deutsche Reichsbahn" as a rail traffic controller at Treblinka village from 22 May 1941. In 1977 he published his book "Old and New Memories", in which he used his own records to estimate that at least 1,200,000 people died at Treblinka. His estimate was based on the maximum capacity of a trainset during the "Grossaktion" Warsaw of 1942 rather than its yearly average. The original German waybills in his possession did not have the number of prisoners listed. Ząbecki, a Polish member of railway staff before the war, was one of the few non-German witnesses to see most transports that came into the camp; he was present at the Treblinka station when the first Holocaust train arrived from Warsaw. Ząbecki was a member of the "Armia Krajowa" (Polish: Home Army), which formed most of the Polish resistance movement in World War II, and kept a daily record of the extermination transports. He also clandestinely photographed the burning Treblinka II perimeter during the uprising in August 1943. Ząbecki witnessed the last set of five enclosed freight wagons carrying "Sonderkommandos" to the Sobibór gas chambers on 20 October 1943. In 2013, his son Piotr Ząbecki wrote an article about him for "Życie Siedleckie" that revised the number to 1,297,000. Ząbecki's daily records of transports to the camp, and demographic information regarding the number of people deported from each ghetto to Treblinka, were the two main sources for estimates of the death toll.
In his 1987 book "Belzec, Sobibor, Treblinka: The Operation Reinhard Death Camps", Israeli historian Yitzhak Arad stated that at least 763,000 people were killed at Treblinka between July 1942 and April 1943. A considerable number of other estimates followed: see table (below).
Höfle Telegram.
A further source of information became available in 2001. The Höfle Telegram was an encrypted message sent to Berlin on 31 December 1942 by Operation Reinhard deputy commander Hermann Höfle, detailing the number of Jews deported by DRB to each Reinhard death camp up to that point. Discovered among declassified documents in Britain, it shows that by the official count of the German Transport Authority 713,555 Jews were sent to Treblinka in 1942. The number of deaths was probably higher, according to the "Armia Krajowa" communiqués. On the basis of the telegram and additional undated German evidence for 1943 listing 67,308 people deported, historian Jacek Andrzej Młynarczyk calculated that by the official DRB count, 780,863 people were brought by "Deutsche Reichsbahn" to Treblinka.
Treblinka trials.
The first official trial for war crimes committed at Treblinka was held in Düsseldorf between 12 October 1964 and 24 August 1965, preceded by the 1951 trial of "SS-Scharführer" Josef Hirtreiter, which was triggered by charges of war crimes unrelated to his service at the camp. The trial was delayed because the United States and the Soviet Union had lost interest in prosecuting German war crimes with the onset of the Cold War. Many of the more than 90,000 Nazi war criminals recorded in German files were serving in positions of prominence under West German chancellor Konrad Adenauer. In 1964 and 1965 eleven former SS camp personnel were brought to trial by West Germany, including commandant Kurt Franz. He was sentenced to life imprisonment, along with Artur Matthes ("Totenlager") and Willi Mentz and August Miete (both from "Lazaret"). Gustav Münzberger (gas chambers) received 12 years, Franz Suchomel (gold and money) 7 years, Otto Stadie (operation) 6 years, Erwin Lambert (gas chambers) 4 years, and Albert Rum ("Totenlager") 3 years. Otto Horn (corpse detail) was acquitted.
The second commandant of Treblinka II, Franz Stangl, escaped with his wife and children from Austria to Brazil in 1951. Stangl found work at a Volkswagen factory in São Paulo. His role in the mass murder of Jews was known to the Austrian authorities, but Austria did not issue a warrant for his arrest until 1961. Stangl was registered under his real name at the Austrian consulate in Brazil. It took another six years before the famous Nazi hunter Simon Wiesenthal tracked him down and triggered his arrest. After his extradition from Brazil to West Germany Stangl was tried for the deaths of around 900,000 people. He admitted to the killings but argued: "My conscience is clear. I was simply doing my duty." Stangl was found guilty on 22 October 1970, and sentenced to life imprisonment. He died of heart failure in prison in Düsseldorf on 28 June 1971.
Material gain.
The theft of cash and valuables, collected from the victims of gassing, was conducted by the higher-ranking SS men on an enormous scale. It was a common practice among the concentration camps' top echelon everywhere; two Majdanek concentration camp commandants, Koch and Florstedt, were tried and executed by the SS for the same offence in April 1945. When the top-ranking officers went home, they would sometimes request a private locomotive from Klinzman and Emmerich at the Treblinka station to transport their personal "gifts" to Małkinia for a connecting train. Then, they would drive out of the camp in cars without any incriminating evidence on their person, and later arrive at Małkinia to transfer the goods.
The overall amount of material gain by Nazi Germany is unknown except for the period between 22 August and 21 September 1942, when there were 243 wagons of goods sent and recorded. Globocnik delivered a written tally to Reinhard headquarters on 15 December 1943 with the SS profit of RM 178,745,960.59, including 2,909.68 kilograms of gold (6,415 lb), 18,733.69 kg of silver (41,300 lb), 1,514 kg of platinum (3,338 lb), and 249,771.50 American dollars, as well as 130 diamond solitaires, 2,511.87 carats of brilliants, 13,458.62 carats of diamonds, and 114 kg of pearls (251 lb). The amount of loot Globocnik stole is unknown; Suchomel claimed in court to have filled a box with one million Reichsmarks for him.
Archaeological studies.
Neither the Jewish religious leaders in Poland nor the authorities allowed archaeological excavations at the camp out of respect for the dead. Approval for a limited archaeological study was issued for the first time in 2010 to a British team from Staffordshire University using non-invasive technology and Lidar remote sensing. The soil resistance was analysed at the site with ground-penetrating radar. Features that appeared to be structural were found, two of which were thought to be the remains of the gas chambers, and the study was allowed to continue.
The archaeological team performing the search discovered three new mass graves. The remains were reinterred out of respect for the victims. At the second dig the findings included yellow tiles stamped with a pierced mullet star resembling a Star of David, and building foundations with a wall. The star was soon identified as the logo of Polish ceramics factory manufacturing floor tiles, founded by Jan Dziewulski and brothers Józef and Władysław Lange (Dziewulski i Lange - D✡L since 1886), nationalised and renamed under communism after the war. As explained by forensic archaeologist Caroline Sturdy Colls, the new evidence was important because the second gas chambers built at Treblinka were housed in the only brick building in the camp; this provides the first physical evidence for their existence. In his memoir describing his stay in the camp, survivor Jankiel Wiernik says that the floor in the gas chambers (which he helped build) was made of similar tiles. The discoveries became a subject of the 2014 documentary by the Smithsonian Channel. More forensic work has been planned.
March of the Living.
Treblinka museum receives most visitors per day during the annual March of the Living educational programme which brings young people from around the world to Poland, to explore the remnants of the Holocaust. The visitors whose primary destination is the march at Auschwitz II-Birkenau, visit Treblinka in the preceding days. In 2009, 300 Israeli students attended the ceremony led by Eli Shaish from the Ministry of Education. In total 4,000 international students visited. In 2013 the number of students who came, ahead of the Auschwitz commemorations, was 3,571. In 2014, 1,500 foreign students visited.

</doc>
<doc id="42431" url="https://en.wikipedia.org/wiki?curid=42431" title="San Juan Mountains">
San Juan Mountains

The San Juan Mountains are a high and rugged mountain range in the Rocky Mountains in southwestern Colorado, and is the largest mountain range in Colorado by area. The area is highly mineralized (the Colorado Mineral Belt) and figured in the gold and silver mining industry of early Colorado. Major towns, all old mining camps, include Creede, Lake City, Silverton, Ouray, and Telluride. Large scale mining has ended in the region, although independent prospectors still work claims throughout the range. The last large scale mines were the Sunnyside Mine near Silverton, which operated until late in the 20th century and the Idarado Mine on Red Mountain Pass that closed down in the 1970s. Famous old San Juan mines include the Camp Bird and Smuggler Union mines, both located between Telluride and Ouray.
The Summitville mine was the scene of a major environmental disaster in the 1990s when the liner of a cyanide-laced tailing pond began leaking heavily. Summitville is in the Summitville caldera, one of many extinct volcanoes making up the San Juan volcanic field. One, La Garita Caldera, is in diameter. Large beds of lava, some extending under the floor of the San Luis Valley, are characteristic of the eastern slope of the San Juans.
Tourism is now a major part of the regional economy, with the narrow gauge railway between Durango and Silverton being an attraction in the summer. Jeeping is popular on the old trails which linked the historic mining camps, including the notorious Black Bear Road. Visiting old ghost towns is popular, as is wilderness trekking and mountain climbing. Many of the old mining camps are now popular sites of summer homes. Though the San Juans are extremely steep and receive a lot of snow, so far only Telluride has made the transition to a major ski resort. Purgatory (now known as Durango Mountain Resort) is a small ski area north of Durango near the Tamarron Resort. There is also skiing on Wolf Creek Pass at the Wolf Creek ski area. Recently Silverton Mountain ski area has begun operation near Silverton.
The Rio Grande drains the east side of the range. The other side of the San Juans, the western slope of the continental divide, is drained by tributaries of the San Juan, Dolores and Gunnison rivers, which all flow into the Colorado River.
The San Juan and Uncompahgre National Forests cover a large portion of the San Juan Mountains.
The San Juan Mountains also have the distinction of being the location of the highest airport with scheduled airline service in the U.S., being Telluride Airport at an elevation of 9,070 feet.
History of the area.
Mining operators in the San Juan mountain area formed the San Juan District Mining Association (SJDMA) in 1903, as a direct result of a Western Federation of Miners proposal to the Telluride Mining Association for the eight-hour day, which had been approved in a referendum by 72 percent of Colorado voters. The new association consolidated the power of thirty-six mining properties in San Miguel, Ouray, and San Juan counties. The SJDMA refused to consider any reduction in hours or increase in wages, helping to provoke a bitter strike.
Acceleration of snowmelt by dust.
Dust blown in from adjoining deserts sometimes accelerates snowmelt in the San Juans. 

</doc>
<doc id="42437" url="https://en.wikipedia.org/wiki?curid=42437" title="Quest Software">
Quest Software

Quest Software, now part of Dell Software a division of Dell Inc., was a software manufacturer headquartered in Aliso Viejo, California. Founded in 1987, Quest developed and supported software used in a variety of industries to simplify IT management. Quest Software was acquired by Dell, announced on July 2, 2012, for $2.36 billion.
The company was known for TOAD, a product used by database professionals, in addition to other products for IT development, management, monitoring and protection. It offered both packaged and custom software applications, as well as associated software infrastructure components, such as databases, application servers, operating systems and hypervisors. The company had a reputation for following IT spending trends and investing in technology areas such as virtualization, cloud automation and backup and recovery.
Quest had a worldwide presence, with more than 60 offices in 23 countries with a customer base of more than 100,000. As of December 2011, Quest had 3,900 full-time employees worldwide and annual revenues of $857 million.
Dell's acquisition of Quest Software became official on September 27, 2012.
Products.
Quest offers products for application management, database management, Microsoft Windows management (including Active Directory, Exchange and SharePoint) and Virtualization Management (including desktop virtualization, server virtualization and cloud automation). Quest's application management products focus in the ERP, Java EE and Microsoft .NET market spaces. Quest was recognized as a leader in Gartner's Magic Quadrant for Application Performance Monitoring in 2011.
Database management tools include support for MySQL, SQL Server, DB2, Sybase, and Oracle. The company's well-known TOAD product is also now available for cloud and NoSQL databases.
In the Windows infrastructure management arena, products cater to Active Directory, Exchange, SharePoint, Microsoft Windows, and System Center users.
The company also offers products to manage virtualized environments, including desktop virtualization, server virtualization and cloud automation tools.
The company divides its products into six solution families:
History.
1987 to 2000.
1987 – Quest Software was founded in Newport Beach, California with a line of high availability and middleware products for HP Multi-Programming Executive (MPE).
1995 – Vinny Smith joined the company, which at the time had 35 employees and $9.5 million in revenue.
1996 – Quest entered the database management market with an Oracle SQL database tuning product;
1997 – Quest expanded beyond North America by opening an office in the United Kingdom.
1998 – Doug Garn joined Quest as the vice president of sales. Quest also added offices in Germany and Australia. Smith became CEO.
1999 – On August 13, 1999, Quest Software went public. The company also entered the application change management market by acquiring Stat.
2000 – Quest expanded further into application management by acquiring Foglight, a monitoring product; and continued the global expansion with new offices in France and the Netherlands. At the end of 2000, the company had 1,400 employees and $167 million in revenue.
2000 to present.
2001 – Quest entered the Microsoft management market by acquiring Fastlane Technologies, and broadened its database offerings beyond Oracle with a new product for the IBM DB2 database.
2002 – Quest opened an office in Japan and expanded their application management offerings to custom web applications written in Java with the acquisition of Sitraka.
2003 – Quest officially entered the Microsoft SQL Server market, and IDC named Quest #1 in distributed database management software.
2004 – The company expanded its Microsoft infrastructure management capabilities by acquiring Aelita Software, and won Microsoft's prestigious Global ISV of the Year award for the first time. Gartner also named Quest #1 in Application Management. 2004 also saw Quest expand operations in Asia with new offices in Singapore, Korea and China.
2005 – Doug Garn became president of Quest. The company acquired Imceda Software that year to add SQL Server backup and recovery capabilities, and Vintela for identity management. At the end of 2005, Quest had 2,750 employees worldwide and revenues of $476 million.
2006 – The company entered the Microsoft SharePoint market. Quest acquired ScriptLogic, which provided a solid entry into the small-to-medium-sized business market. Quest also acquired Charonware s.r.o from the Czech Republic, the makers of CASE Studio2, and folded it into the TOAD Data Modeler product. This year also saw Quest ranked #1 by Gartner for application management in North America, and #1 in Database Development and Management by IDC.
2007 – Quest received Microsoft's Global ISV of the Year award for the second time. The company also begin its focus on virtualization by acquiring Provision Networks, a desktop virtualization management company.
2008 – Doug Garn became CEO and president, and Vinny Smith became executive chairman. Quest purchased Vizioncore as an entry into the server virtualization market.
2009 – Alan Fudge becomes vice president of sales, and Quest acquired PacketTrap for network monitoring.
2010 – The company acquired Voelcker to round out their identity management offerings. Quest continued building out its virtualization business and also entered the private cloud automation market by acquiring Surgient. Quest was also ranked in the leaders' quadrant by Gartner for application performance monitoring. Revenues were $767 million at the end of 2010, and the company counted approximately 3,500 employees.
2011 – Quest acquired BakBone Software, e-DMZ, RemoteScan, Symlabs,ChangeBASE, VKernel and BiTKOO.
2011 – Vinny Smith became CEO and Chairman, and Doug Garn became Vice Chairman.
2012 – On Friday September 28, 2012, Dell announced that it has completed the acquisition of Quest Software

</doc>
<doc id="42440" url="https://en.wikipedia.org/wiki?curid=42440" title="1096">
1096

__NOTOC__
Year 1096 (MXCVI) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="42441" url="https://en.wikipedia.org/wiki?curid=42441" title="Physical modelling synthesis">
Physical modelling synthesis

In sound synthesis, physical modelling synthesis refers to methods in which the waveform of the sound to be generated is computed by using a mathematical model, being a set of equations and algorithms to simulate a physical source of sound, usually a musical instrument. Such a model consists of (possibly simplified) laws of physics that govern the sound production, and will typically have several parameters, some of which are constants that describe the physical materials and dimensions of the instrument, while others are time-dependent functions that describe the player's interaction with it, such as plucking a string, or covering toneholes.
For example, to model the sound of a drum, there would be a formula for how striking the drumhead injects energy into a two dimensional membrane. Thereafter the properties of the membrane (mass density, stiffness, etc.), its coupling with the resonance of the cylindrical body of the drum, and the conditions at its boundaries (a rigid termination to the drum's body) would describe its movement over time and thus its generation of sound.
Similar stages to be modelled can be found in instruments such as a violin, though the energy excitation in this case is provided by the slip-stick behavior of the bow against the string, the width of the bow, the resonance and damping behavior of the strings, the transfer of string vibrations through the bridge, and finally, the resonance of the soundboard in response to those vibrations.
Although physical modelling was not a new concept in acoustics and synthesis, having been implemented using finite difference approximations of the wave equation by Hiller and Ruiz in 1971, it was not until the development of the Karplus-Strong algorithm, the subsequent refinement and generalization of the algorithm into the extremely efficient digital waveguide synthesis by Julius O. Smith III and others, and the increase in DSP power in the late 1980s that commercial implementations became feasible.
Yamaha signed a contract with Stanford University in 1989 to jointly develop digital waveguide synthesis, and as such most patents related to the technology are owned by Stanford or Yamaha.
The first commercially available physical modelling synthesizer made using waveguide synthesis was the Yamaha VL1 in 1994.
While the efficiency of digital waveguide synthesis made physical modelling feasible on common DSP hardware and native processors, the convincing emulation of physical instruments often requires the introduction of non-linear elements, scattering junctions, etc. In these cases, digital waveguides are often combined with FDTD, finite element or wave digital filter methods, increasing the computational demands of the model.
Technologies associated with physical modelling.
"Examples of physical modelling synthesis:"
Hardware synthesizers.
While not purely a hardware synth, the DS-XG sound cards based on the Yamaha YMF-7#4 family of audio chipsets (including 724, 744, 754, and 764), including the Yamaha WaveForce 192 (SW192XG) as well as many from other manufacturers and even some PC motherboards with such an audio chipset, included hardware-assisted software VL physical modelling (like a VL70m or PLG-VL, and compatible with same) along with the Yamaha XG, wave audio, and 3D gaming sound capabilities of the chipset. Unfortunately, only the VxD (Virtual Device Drivers) drivers for pre-NT kernel versions of Windows (3.x, 9#, and ME) support the physical modelling feature. Neither the WDM (Windows Device Model) drivers for Windows 98, 98SE, nor ME, nor any driver for any NT-kernel version of Windows (NT, 2000, XP, Vista, Windows 2003 Server, Windows 7, Windows 2008 Server, nor likely any future OSes) support this, nor can they due to OS limitations. Those OSes do support the other features of the card, though.
In their prime, the DS-XG sound cards were easily the most affordable way of obtaining genuine VL technology for anyone who already had a Windows 3.x, 9#, or ME PC. Such cards could be had brand new for as low as $12 USD (YMF-724 versions). But since they were not fully compatible with the AC-97 and later AC-98 standards, these chipsets faded from the market and have not been manufactured by Yamaha in nearly a decade.
Technics WSA1 and its rackmounted counterpart WSA1R was Technics' first and only try at high-end synthesizers. It featured 64 voices of polyphony with a combination of sample playback (for initial transients) and DSP acoustic modelling. Technics WSA1 was launched in 1995, but the musical community did not have enough confidence in Technics to buy a $5000 hardware synth. Only about 600 keyboards and 300 rack models were ever made, and most were sold at highly discounted prices.
Various Roland synth models (V-Synth, V-Combo, XV-5080, Fantom, etc.), use a technology called COSM ("Composite Object Sound Modeling") which uses physical modeling techniques to more accurately replicate guitars, brass and other instruments. COSM has been superseded by "SuperNatural", which is also based on physical modeling techniques. Introduced first in 2008 as part of the ARX expansion boards for Fantom hardware synthesizers, "SuperNatural" modeling is used in Roland's V-Drums (TD-30, TD-15, TD-11), V-Accordions (FR-7, FR-8) and various synth models (Jupiter 80, Integra 7, FA-08, JD-Xi, etc.)

</doc>
<doc id="42442" url="https://en.wikipedia.org/wiki?curid=42442" title="Terrorism in Yemen">
Terrorism in Yemen

In the War on Terrorism in Yemen, the US government describes Yemen as "an important partner in the global war on terrorism".
Attacks against civilian targets.
On December 30, 2002, a suspected Islamic fundamentalist killed three US workers and wounded one in a hospital in Jibla, using a semi-automatic rifle. The suspect was arrested and identified as Abid Abdulrazzaq Al-Kamil.
Jews in Yemen reportedly fled their homes due to threats from Muslim extremists. A notable incident was the murder of Moshe Ya'ish al-Nahari of Raydah in December 2008.
Al Qaeda members sent letters to 45 Jews living in al-Salem, near Sana'a, on January 19, 2007, accusing them of involvement in an "international Zionist conspiracy". The letters said that if the Jews did not abandon their homes in ten days, they would be abducted and murdered and their homes would be looted. The Jewish community sent a complaint to President Abdullah Salah and are temporarily staying in a hotel near Sana'a. The Yemeni government has promised that their homes will be protected and they may return to them.
On September 17, 2008, Al Qaeda militants attacked the U.S. Embassy in Sana'a. 20 people were killed, including: six militants, six policemen and seven civilians. One American was among those killed.
Attacks on tourists.
On July 2, 2007, a suicide bomber killed eight Spanish tourists and their two Yemeni drivers in Ma'rib.
On January 18, 2008, Al Qaeda militants opened fire on a convoy of tourists in Hadhramaut killing two Belgian tourists and two Yemenis, the tourists' driver and their guide.
Attacks targeted South Korean tourists in March 2009. Four Korean tourists alongside their local Yemeni guide were killed. Two attackers also died.
2010 cargo plane bomb plot.
On October 29, 2010, UK Prime Minister David Cameron said the device in a package sent from Yemen and found on a US-bound cargo plane was designed to go off on the aircraft. But Cameron said investigators could not yet be certain about when the device, intercepted at East Midlands Airport, was supposed to explode. A second device containing explosives was found on a cargo plane in Dubai. The US suspected al-Qaeda involvement.
In Yemen, police arrested but later released a woman suspected of posting the packages. President Ali Abdullah Saleh pledged that his country would continue fighting al-Qaeda "in cooperation with its partners." He added, "But we do not want anyone to interfere in Yemeni affairs by hunting down al-Qaeda," as heavily armed troops patrolled Sanaa. Yemeni authorities also closed down the local offices of the US cargo firms United Parcel Service and FedEx, who had already suspended all shipments out of the country and pledged full co-operation with investigators. US President Barack Obama's national security adviser, John Brennan, has phoned Saleh to offer US help in fighting al-Qaeda, the White House said.
The explosive devices, which triggered security alerts in the US, UK, and Middle East, were apparently both inserted in printer cartridges and placed in packages addressed to synagogues in the Chicago area. Pentaerythritol tetranitrate (PETN) - an explosive favoured by the Yemeni-based militant group, al-Qaeda in the Arabian Peninsula (AQAP) - was discovered in both devices. Obama discussed the apparent terrorist plot with Cameron by phone, expressing his "appreciation for the professionalism of American and British services involved" in disrupting it.
Later, Cameron told reporters at his country residence, Chequers, that it was believed the explosive device intercepted at East Midlands Airport was "designed to go off on the aeroplane". "We cannot be sure about when that was supposed to take place," he added. "There is no early evidence that it was meant to take place over British soil, but of course we cannot rule it out." The prime minister said the authorities had immediately banned packages coming to or through the UK from Yemen, and would be "looking extremely carefully at any further steps we have to take". UK Home Secretary Theresa May said the government did not believe the plotters would have known the location of the device when it was planned to explode. While details of the device found in Britain were not released, photographs emerged on the US media of an ink toner cartridge covered in white powder and connected to a circuit board. The British government's remarks suggest the authorities in both the UK and the US remain uncertain about the precise targets and, indeed, aim of this latest apparent plot. According to Dubai police, the explosives they found were also inside the toner cartridge of a printer, placed in a cardboard box containing English-language books and souvenirs.The cartridge contained PETN and plastic explosives mixed with lead azide, they said. Lead azide is an explosive commonly used in detonators. "The device was prepared in a professional manner and equipped with an electrical circuit linked to a mobile telephone card concealed in the printer," the police said. For US Homeland Security Secretary Janet Napolitano, the plot bore "all the hallmarks of al-Qaeda and in particular [AQAP". Unnamed US officials quoted by the Associated Press said al-Qaeda's explosives expert in Yemen, Ibrahim Hassan al-Asiri, was the likely suspect behind the bomb-making. They said Asiri had helped make the bomb used in the failed Detroit Christmas Day bomb attack and another PETN device used in a failed suicide attack against a top Saudi counter-terrorism official last year. The White House has said Saudi Arabia provided information that helped identify the threat, while the UK's "Daily Telegraph" reported that an MI6 officer responsible for Yemen had received a tip-off.
Military/police counter-terrorism operations.
Following the September 11, 2001 attacks in the United States, President Ali Abdullah Saleh made an effort to eliminate the Islamist militant presence. By November 2002, Yemeni government troops detained 104 suspected al-Qaeda members.
In December 2001, a search by government forces for two Yemeni believed to be senior al Qaeda members hiding near Ma'rib led to a gun battle with tribesmen which ended in the deaths of 34 people, including 18 soldiers. To defuse the situation, ten Ma'rib sheiks were detained as hostages of the state in comfortable rooms in the presidential palace for 35 days, until 43 lesser tribesmen took their place.
In the first months of 2002 the Bush Administration approved sending about 100 Special Operations Forces to Yemen, a power base for Al Qaeda.
In November 2002, six Yemenis suspected of being members of al Qaeda were blown up in their car in the province of Marib by a Hellfire missile attack from an unmanned CIA RQ-1 Predator aircraft. Among the dead was Abu Ali al-Harithi.
In 2004, the Australian Broadcasting Corporation's (ABC-TV) international affairs program "Foreign Correspondent" investigated this targeted killing and the involvement of then US Ambassador as part of a special report titled "The Yemen Option". The report also examined the evolving tactics and countermeasures in dealing with Al Qaeda inspired attacks.
In December 2002, Spanish troops boarded and detained a ship, at the request of the United States, that was transporting Scud missiles from North Korea to Yemen. After two days, when the United States determined it had no right under international law to continue to detain the shipment, they let it continue on to Yemen.
On July 30, 2009, three soldiers were killed in a clash with al-Qaeda militants in Marib province.
A raid on an alleged al-Qaeda camp on December 17, 2009, led to the deaths of 46 civilians, 14 of which were female and 21 were children ABC News reported that US cruise missiles were part of the bombardment of the camps, which targeted Abu Hureira Qasm al-Rimi. A local official and a tribal source claimed that 49 civilians, including 23 women and 17 children, were among those killed in the strike. The same day a clash between security forces and al-Qaeda members in Abhar left four militants dead.
An air raid targeted an al-Qaeda meeting in Wadi Rafadh in Shabwa province on December 24, 2009. Another 34 al-Qaeda militants were killed in the attack. Among the dead were also Saudis and Iranians, according to the security forces. The number of al-Qaeda members arrested in the previous week rose to 29.
US air attacks.
The U.S. first said it used targeted killing in November 2002, with the cooperation and approval of the government of Yemen.
A CIA-controlled Predator drone fired a Hellfire missile at an SUV in the Yemeni desert containing Qaed Salim Sinan al-Harethi, a Yemeni suspected senior al-Qaeda lieutenant believed to have been the mastermind behind the October 2000 USS "Cole" bombing that killed 17 Americans. He was on a list of targets whose capture or death had been called for by US President George W. Bush. In addition to al-Harethi, five other occupants of the SUV were killed, all of whom were suspected al-Qaeda terrorists, and one of whom (Kamal Derwish) was an American.[http://www.historycommons.org/entity.jsp?entity=kamal_derwish]
On December 17, 2009, the village of Al Ma`jalah was hit by a cruise missile, killing 41 people, including 14 women, 21 children, and 14 alleged al-Qaeda members. While the Yemeni government initially took responsibility, photographs of American components and a Wikileaks cable suggest that it was carried out by the United States.
In May 2010 an errant US drone attack targeting al Qaeda terrorists in Wadi Abida, Yemen, killed five people, among them Jaber al-Shabwani, deputy governor of Maarib province who was mediating between the government and the militants. The killing so angered Shabwani's tribesmen that in the subsequent weeks they fought heavily with government security forces, twice attacking a major oil pipeline in Maarib.
On May 5, 2011, a missile fired from a U.S. drone killed Abdullah and Mosaad Mubarak, brothers who may have been militants. The missile was fired on their car and both died instantly. The strike was aimed at killing Anwar al-Awlaki, but al-Awlaki appears to have survived.
On 3 June 2011 American manned jets or drones attacked and killed Abu Ali al-Harithi, a midlevel al-Qaeda operative, and several other militant suspects, including Ammar Abadah Nasser al-Wa'eli, in a strike in southern Yemen. Four civilians were also reportedly killed in the strike. The strike was reportedly coordinated by American special forces and CIA operatives based in Sana. According to the Associated Press, in 2011 the US government began building an airbase near or in Yemen from which the CIA and US military plans to operate drones over Yemen. The "Washington Post" reported that the US previously used a base in Djibouti to operate drones over Yemen. The "Wall Street Journal" reports that a US drone base in the Seychelles could be used to operate drones over Yemen.
According to local residents and unnamed American and Yemeni government officials, on 14 July 2011 US manned aircraft or drones attacked and destroyed a police station in Mudiya in Abyan Province which had been occupied by al Qaeda militants. Yemeni media and government gave conflicting accounts on the number of casualties, estimated at between 6 and 50 killed. The same day and nearby, drone missiles reportedly hit a car belonging to Yemeni al Qaeda leader Fahd al-Quso, but al-Quso survived the attack.
On 1 August 2011, US drones and reportedly Yemeni aircraft attacked three targets with bombs and missiles in South Yemen, killing 15 suspected al Qaeda militants and wounding 17 others. The locations targeted included al-Wahdah, al-Amodiah, and al-Khamilah in Abyan province. One of those killed was reportedly militant leader Naser al-Shadadi.
According to the Yemen Post "At least 35 US drone attacks were reported in Yemen over the last two month."
On 24 August 2011, unidentified aircraft attacked suspected al-Qaeda militants near Zinjibar. The strikes reportedly killed 30 militants and wounded 40 others.
According to Yemeni officials as reported in the "Long War Journal", US airstrikes in southeast Abyan province on 30 August to 1 September 2011 killed 30 AQAP militants. The militants were reportedly engaged in combat with Yemeni military forces.
Two airstrikes by US-operated aircraft on 21 September 2011 reportedly killed four AQAP fighters in Abyan and seven AQAP fighters in Shaqra.
On 30 September 2011, US drone-launched missiles killed four people, including Al Qaeda propagandist Anwar al-Awlaki, in Al Jawf Governorate. The strike also killed Samir Kahn, American-born editor of "Inspire" magazine. The strike marked the first known time that the US had deliberately targeted US citizens in a drone attack.
A reported drone strike in Zinjibar on 5 October 2011 killed five AQAP militants. According to Yemeni government officials, a US airstrike on 14 October 2011 killed seven AQAP militants, including Egyptian-born Ibrahim al-Bana, AQAP's media chief.
A drone strike on 22 December 2011 near Zinjibar reportedly killed Abdulrahman al-Wuhayshi, a relative of Yemeni al-Qaeda leader Nasir al-Wuhayshi. A further eight militants were reported killed in an air strike near Jaar on 17 December 2011.
An airstrike, reportedly performed by US aircraft, on 31 January 2012 near the city of Lawder in Abyan province, killed 11 AQAP militants. The dead reportedly included Abdul Monem al-Fahtani, a participant in the USS Cole bombing.
Drones engaged in three attacks over three days from 9–11 March 2012. The first strike targeted an AQAP hideout near Al Baydah, Baydah province, reportedly killing local AQAP leader Abdulwahhab al-Homaiqani and 16 of his militants. The second strike hit Jaar in Abyan province, reportedly killing 20 AQAP fighters. The third strike, also in Jaar, reportedly killed three AQAP militants and targeted a storage location for weapons AQAP had seized after overruning a Yemeni military base in Al Koud the week before. A fourth drone strike on 14 March 2012 in Al Bydah reportedly killed four AQAP militants in a vehicle.
On April 11, 14 militants were killed in a drone strike in Lauder town, northeast of Zinjibar, Abyan province. A drone strike on 22 April in the Al Samadah area, near the border of Marib and Al Jawf provinces, killed AQAP senior leader Mohammed Saeed al Umda (also known as Ghareeb al Taizi).
On 6 May 2012 a suspected US drone strike killed Fahd Mohammed Ahmed al-Quso and another al Qaida militant in southern Shabwa province.
On 12 December 2013, 17 people were killed in a wedding convoy in the District of Rada' which falls in the Governorate of Al-Bayda'. The U.S. drone mistakenly targeted a wedding convoy after intelligence reports identified the vehicles as carrying suspects of the AQAP organization. Five of the killed had been suspected, but the remainder were civilians.
On 3 March 2014 an airstrike, believed to have been carried out by an American drone, killed three people suspected of being members of AQAP. Mujahid Gaber Saleh al Shabwani, who is one of Yemen's 25 most wanted AQAP operatives, is thought to have been killed in the strike.
On 20 and 21 April 2014, three drone strikes by the US government killed at least two dozen suspected AQAP members and destroyed one of the group's training camps in southern Yemen, according to a statement released by the Yemeni Interior Ministry. In a statement, the group admitted that five civilians had been wounded and three killed during the attack.
On 13 June 2014 a suspected US drone strike targeted a car in the Mafraq al-Saeed area of the Shabwah province, killing five alleged AQAP operatives on board.
It is estimated that a total of 98 US drone attacks have been conducted in Yemen between 2002 and 2015; 41 in 2012, 26 in 2013 and 14 in 2014.

</doc>
<doc id="42445" url="https://en.wikipedia.org/wiki?curid=42445" title="Atomic mass unit">
Atomic mass unit

The unified atomic mass unit (symbol: u) or dalton (symbol: Da) is the standard unit that is used for indicating mass on an atomic or molecular scale (atomic mass). One unified atomic mass unit is approximately the mass of one nucleon (either a single proton or neutron) and is numerically equivalent to 1g/mol. It is defined as one twelfth of the mass of an unbound neutral atom of carbon-12 in its nuclear and electronic ground state, and has a value of . The CIPM has categorised it as a non-SI unit accepted for use with the SI, and whose value in SI units must be obtained experimentally.
The amu without the "unified" prefix is technically an obsolete unit based on oxygen, which was replaced in 1961. However, many sources still use the term "amu" but now define it in the same way as u (i.e., based on carbon-12). In this sense, most uses of the terms "atomic mass units" and "amu" today actually refer to unified atomic mass unit. For standardization a specific atomic nucleus (carbon-12 vs. oxygen-16) had to be chosen because the average mass of a nucleon depends on the count of the nucleons in the atomic nucleus due to mass defect. This is also why the mass of a proton (or neutron) by itself is more than (and not equal to) 1 u.
The atomic mass unit is not the unit of mass in the atomic units system, which is rather the electron rest mass ("m"e).
History.
The relative atomic mass (atomic weight) scale has traditionally been a relative scale, that is without an explicit unit, with the first relative atomic mass basis suggested by John Dalton in 1803 as 1H. Despite the initial mass of 1H being used as the natural unit for relative atomic mass, it was suggested by Wilhelm Ostwald that relative atomic mass would be best expressed in terms of units of 1/16 mass of oxygen. This evaluation was made prior to the discovery of the existence of elemental isotopes, which occurred in 1912.
The discovery of isotopic oxygen in 1929 led to a divergence in relative atomic mass representation, with isotopically weighted oxygen (i.e., naturally occurring oxygen relative atomic mass) given a value of exactly 16 atomic mass units (amu) in chemistry, while pure 16O (oxygen-16) was given the mass value of exactly 16 amu in physics.
The divergence of these values could result in errors in computations, and was unwieldy. The chemistry amu, based on the relative atomic mass (atomic weight) of natural oxygen (including the heavy naturally-occurring isotopes 17O and 18O), was about as massive as the physics amu, based on pure isotopic 16O.
For these and other reasons, the reference standard for both physics and chemistry was changed to carbon-12 in 1961. The choice of carbon-12 was made to minimise further divergence with prior literature. The new and current unit was referred to as the "unified atomic mass unit" u. and given a new symbol, "u," which replaced the now deprecated "amu" that had been connected to the old oxygen-based system. The Dalton (Da) is another name for the unified atomic mass unit.
Despite this change, modern sources often still use the old term "amu" but define it as u ( of the mass of a carbon-12 atom), as mentioned in the article's introduction. Therefore, in general, "amu" likely does not refer to the old oxygen standard unit, unless the source material originates from or before the 1960s.
The unified atomic mass unit u was defined as:
Terminology.
The unified atomic mass unit and the dalton are different names for the same unit of measure. As with other unit names such as watt and newton, "dalton" is not capitalized in English, but its symbol Da is capitalized. With the introduction of the name "dalton", there has been a gradual change towards using that name in preference to the name "unified atomic mass unit":
Relationship to SI.
The definition of the mole, an SI base unit, was accepted by the CGPM in 1971 as:
The definition of the mole also determines the value of the universal constant that relates the number of entities to amount of substance for any sample. This constant is called the Avogadro constant, symbol "N"A or "L", and has the value (entities per mole).
Given that the unified atomic mass unit is one twelfth the mass of one atom of carbon-12, meaning the mass of such an atom is 12 u, it follows that there are "N"A atoms of carbon-12 in 0.012 kg of carbon-12. This can be expressed mathematically as
Masses of proteins are often expressed in daltons. For example, a protein with a molecular weight of has a mass of 64 kDa.

</doc>
<doc id="42446" url="https://en.wikipedia.org/wiki?curid=42446" title="Reason">
Reason

Reason is the capacity for consciously making sense of things, applying logic, establishing and verifying facts, and changing or justifying practices, institutions, and beliefs based on new or existing information. It is closely associated with such characteristically human activities as philosophy, science, language, mathematics, and art and is normally considered to be a definitive characteristic of human nature.
The concept of reason is sometimes referred to as rationality and sometimes as discursive reason, in opposition to intuitive reason.
Reason or "reasoning" is associated with thinking, cognition, and intellect. Reason, like habit or intuition, is one of the ways by which thinking comes from one idea to a related idea. For example, it is the means by which rational beings understand themselves to think about cause and effect, truth and falsehood, and what is good or bad. It is also closely identified with the ability to self-consciously change beliefs, attitudes, traditions, and institutions, and therefore with the capacity for freedom and self-determination.
In contrast to reason as an abstract noun, a reason is a consideration which explains or justifies some event, phenomenon or behaviour. The field of logic studies ways in which human beings reason through argument.
Psychologists and cognitive scientists have attempted to study and explain how people reason, e.g. which cognitive and neural processes are engaged, and how cultural factors affect the inferences that people draw. The field of automated reasoning studies how reasoning may or may not be modeled computationally. Animal psychology considers the question of whether animals other than humans can reason.
Etymology and related words.
In the English language and other modern European languages, "reason", and related words, represent words which have always been used to translate Latin and classical Greek terms in the sense of their philosophical usage.
The earliest major philosophers to publish in English, such as Francis Bacon, Thomas Hobbes, and John Locke also routinely wrote in Latin and French, and compared their terms to Greek, treating the words ""logos"", ""ratio"", ""raison"" and "reason" as inter-changeable. The meaning of the word "reason" in senses such as "human reason" also overlaps to a large extent with "rationality" and the adjective of "reason" in philosophical contexts is normally "rational", rather than "reasoned" or "reasonable". Some philosophers such as Thomas Hobbes, for example, also used the word "ratiocination" as a synonym for "reasoning".
Philosophical history.
The proposal that reason gives humanity a special position in nature has been argued to be a defining characteristic of western philosophy and later western modern science, starting with classical Greece. Philosophy can be described as a way of life based upon reason, and in the other direction reason has been one of the major subjects of philosophical discussion since ancient times. Reason is often said to be reflexive, or "self-correcting," and the critique of reason has been a persistent theme in philosophy. It has been defined in different ways, at different times, by different thinkers.
Classical philosophy.
For many classical philosophers, nature was understood teleologically, meaning that every type of thing had a definitive purpose which fit within a natural order that was itself understood to have aims. Perhaps starting with Pythagoras or Heraclitus, the cosmos is even said to have reason. Reason, by this account, is not just one characteristic that humans happen to have, and that influences happiness amongst other characteristics. Reason was considered of higher stature than other characteristics of human nature, such as sociability, because it is something humans share with nature itself, linking an apparently immortal part of the human mind with the divine order of the cosmos itself. Within the human mind or soul ("psyche"), reason was described by Plato as being the natural monarch which should rule over the other parts, such as spiritedness ("thumos") and the passions. Aristotle, Plato's student, defined human beings as rational animals, emphasizing reason as a characteristic of human nature. He "defined" the highest human happiness or well being ("eudaimonia") as a life which is lived consistently, excellently and completely in accordance with reason.
The conclusions to be drawn from the discussions of Aristotle and Plato on this matter are amongst the most debated in the history of philosophy. But teleological accounts such as Aristotle's were highly influential for those who attempt to explain reason in a way which is consistent with monotheism and the immortality and divinity of the human soul. For example, in the neo-platonist account of Plotinus, the cosmos has one soul, which is the seat of all reason, and the souls of all individual humans are part of this soul. Reason is for Plotinus both the provider of form to material things, and the light which brings individuals souls back into line with their source. Such neo-Platonist accounts of the rational part of the human soul were standard amongst medieval Islamic philosophers, and under this influence, mainly via Averroes, came to be debated seriously in Europe until well into the renaissance, and they remain important in Iranian philosophy.
Subject-centred reason in early modern philosophy.
The early modern era was marked by a number of significant changes in the understanding of reason, starting in Europe. One of the most important of these changes involved a change in the metaphysical understanding of human beings. Scientists and philosophers began to question the teleological understanding of the world. Nature was no longer assumed to be human-like, with its own aims or reason, and human nature was no longer assumed to work according to anything other than the same "laws of nature" which affect inanimate things. This new understanding eventually displaced the previous world view that derived from a spiritual understanding of the universe.
Accordingly, in the 17th century, René Descartes explicitly rejected the traditional notion of humans as "rational animals," suggesting instead that they are nothing more than "thinking things" along the lines of other "things" in nature. Any grounds of knowledge outside that understanding was, therefore, subject to doubt.
In his search for a foundation of all possible knowledge, Descartes deliberately decided to throw into doubt "all" knowledge – "except" that of the mind itself in the process of thinking:
At this time I admit nothing that is not necessarily true. I am therefore precisely nothing but a thinking thing; that is a mind, or intellect, or understanding, or reason – words of whose meanings I was previously ignorant.
This eventually became known as epistemological or "subject-centred" reason, because it is based on the "knowing subject", who perceives the rest of the world and itself as a set of objects to be studied, and successfully mastered by applying the knowledge accumulated through such study. Breaking with tradition and many thinkers after him, Descartes explicitly did not divide the incorporeal soul into parts, such as reason and intellect, describing them as one indivisible incorporeal entity.
A contemporary of Descartes, Thomas Hobbes described reason as a broader version of "addition and subtraction" which is not limited to numbers. This understanding of reason is sometimes termed "calculative" reason. Similar to Descartes, Hobbes asserted that "No discourse whatsoever, can end in absolute knowledge of fact, past, or to come" but that "sense and memory" is absolute knowledge.
In the late 17th century, through the 18th century, John Locke and David Hume developed Descartes' line of thought still further. Hume took it in an especially skeptical direction, proposing that there could be no possibility of deducing relationships of cause and effect, and therefore no knowledge is based on reasoning alone, even if it seems otherwise.
Hume famously remarked that, "We speak not strictly and philosophically when we talk of the combat of passion and of reason. Reason is, and ought only to be the slave of the passions, and can never pretend to any other office than to serve and obey them." Hume also took his definition of reason to unorthodox extremes by arguing, unlike his predecessors, that human reason is not qualitatively different from either simply conceiving individual ideas, or from judgments associating two ideas, and that "reason is nothing but a wonderful and unintelligible instinct in our souls, which carries us along a certain train of ideas, and endows them with particular qualities, according to their particular situations and relations." It followed from this that animals have reason, only much less complex than human reason.
In the 18th century, Immanuel Kant attempted to show that Hume was wrong by demonstrating that a "transcendental" self, or "I", was a necessary condition of all experience. Therefore, suggested Kant, on the basis of such a self, it is in fact possible to reason both about the conditions and limits of human knowledge. And so long as these limits are respected, reason can be the vehicle of morality, justice and understanding.
Substantive and formal reason.
In the formulation of Kant, who wrote some of the most influential modern treatises on the subject, the great achievement of reason is that it is able to exercise a kind of universal law-making. Kant was able therefore to re-formulate the basis of moral-practical, theoretical and aesthetic reasoning, on "universal" laws.
Here practical reasoning is the self-legislating or self-governing formulation of universal norms, and theoretical reasoning the way humans posit universal laws of nature.
Under practical reason, the moral autonomy or freedom of human beings depends on their ability to behave according to laws that are given to them by the proper exercise of that reason. This contrasted with earlier forms of morality, which depended on religious understanding and interpretation, or nature for their substance.
According to Kant, in a free society each individual must be able to pursue their goals however they see fit, so long as their actions conform to principles given by reason. He formulated such a principle, called the "categorical imperative", which would justify an action only if it could be universalized:
Act only according to that maxim whereby you can, at the same time, will that it should become a universal law.
In contrast to Hume then, Kant insists that reason itself (German "Vernunft") has natural ends itself, the solution to the metaphysical problems, especially the discovery of the foundations of morality. Kant claimed that this problem could be solved with his "transcendental logic" which unlike normal logic is not just an instrument, which can be used indifferently, as it was for Aristotle, but a theoretical science in its own right and the basis of all the others.
According to Jürgen Habermas, the "substantive unity" of reason has dissolved in modern times, such that it can no longer answer the question "How should I live?" Instead, the unity of reason has to be strictly formal, or "procedural." He thus described reason as a group of three autonomous spheres (on the model of Kant's three critiques):
For Habermas, these three spheres are the domain of experts, and therefore need to be mediated with the "lifeworld" by philosophers. In drawing such a picture of reason, Habermas hoped to demonstrate that the substantive unity of reason, which in pre-modern societies had been able to answer questions about the good life, could be made up for by the unity of reason's formalizable procedures.
The critique of reason.
Hamann, Herder, Kant, Hegel, Kierkegaard, Nietzsche, Heidegger, Foucault, Rorty, and many other philosophers have contributed to a debate about what reason means, or ought to mean. Some, like Kierkegaard, Nietzsche, and Rorty, are skeptical about subject-centred, universal, or instrumental reason, and even skeptical toward reason as a whole. Others, including Hegel, believe that it has obscured the importance of intersubjectivity, or "spirit" in human life, and attempt to reconstruct a model of what reason should be.
Some thinkers, e.g. Foucault, believe there are other "forms" of reason, neglected but essential to modern life, and to our understanding of what it means to live a life according to reason.
In the last several decades, a number of proposals have been made to "re-orient" this critique of reason, or to recognize the "other voices" or "new departments" of reason:
For example, in opposition to subject-centred reason, Habermas has proposed a model of communicative reason that sees it as an essentially cooperative activity, based on the fact of linguistic intersubjectivity.
Nikolas Kompridis has proposed a widely encompassing view of reason as "that ensemble of practices that contributes to the opening and preserving of openness" in human affairs, and a focus on reason's possibilities for social change.
The philosopher Charles Taylor, influenced by the 20th century German philosopher Martin Heidegger, has proposed that reason ought to include the faculty of disclosure, which is tied to the way we make sense of things in everyday life, as a new "department" of reason.
In the essay "What is Enlightenment?", Michel Foucault proposed a concept of critique based on Kant's distinction between "private" and "public" uses of reason. This distinction, as suggested, has two dimensions:
Reason compared to related concepts.
Reason compared to Logic.
The terms "logic" or "logical" are sometimes used as if they were identical with the term "reason" or with the concept of being "rational", or sometimes logic is seen as the most pure or the defining form of reason. For example in modern economics, rational choice is assumed to equate to logically consistent choice.
Reason and logic can however be thought of as distinct, although logic is one important aspect of reason. Author Douglas Hofstadter, in "Gödel, Escher, Bach", characterizes the distinction in this way. Logic is done inside a system while reason is done outside the system by such methods as skipping steps, working backward, drawing diagrams, looking at examples, or seeing what happens if you change the rules of the system.
Reason is a type of thought, and the word "logic" involves the attempt to describe rules or norms by which reasoning operates, so that orderly reasoning can be taught. The oldest surviving writing to explicitly consider the rules by which reason operates are the works of the Greek philosopher Aristotle, especially "Prior Analysis" and "Posterior Analysis". Although the Ancient Greeks had no separate word for logic as distinct from language and reason, Aristotle's newly coined word "syllogism" ("syllogismos") identified logic clearly for the first time as a distinct field of study. When Aristotle referred to "the logical" ("hē logikē"), he was referring more broadly to rational thought.
Reason compared to cause-and-effect thinking, and symbolic thinking.
As pointed out by philosophers such as Hobbes, Locke and Hume, some animals are also clearly capable of a type of "associative thinking", even to the extent of associating causes and effects. A dog once kicked, can learn how to recognize the warning signs and avoid being kicked in the future, but this does not mean the dog has reason in any strict sense of the word. It also does not mean that humans acting on the basis of experience or habit are using their reason.
Human reason requires more than being able to associate two ideas, even if those two ideas might be described by a reasoning human as a cause and an effect, perceptions of smoke, for example, and memories of fire. For reason to be involved, the association of smoke and the fire would have to be thought through in a way which can be explained, for example as cause and effect. In the explanation of Locke, for example, reason requires the mental use of a third idea in order to make this comparison by use of syllogism.
More generally, reason in the strict sense requires the ability to create and manipulate a system of symbols, as well as indices and icons, according to Charles Sanders Peirce, the symbols having only a nominal, though habitual, connection to either smoke or fire. One example of such a system of artificial symbols and signs is language.
The connection of reason to symbolic thinking has been expressed in different ways by philosophers. Thomas Hobbes described the creation of "Markes, or Notes of remembrance" ("Leviathan" Ch.4) as "speech". He used the word "speech" as an English version of the Greek word "logos" so that speech did not need to be communicated. When communicated, such speech becomes language, and the marks or notes or remembrance are called "Signes" by Hobbes. Going further back, although Aristotle is a source of the idea that only humans have reason ("logos"), he does mention that animals with imagination, for whom sense perceptions can persist, come closest to having something like reasoning and "nous", and even uses the word "logos" in one place to describe the distinctions which animals can perceive in such cases.
Reason, imagination, mimesis, and memory.
Reason and imagination rely on similar mental processes. Imagination is not only found in humans. Aristotle, for example, stated that "phantasia" (imagination: that which can hold images or "phantasmata") and "phronein" (a type of thinking that can judge and understand in some sense) also exist in some animals. According to him, both are related to the primary perceptive ability of animals, which gathers the perceptions of different senses and defines the order of the things that are perceived without distinguishing universals, and without deliberation or "logos". But this is not yet reason, because human imagination is different.
The recent modern writings of Terrence Deacon and Merlin Donald, writing about the origin of language, also connect reason connected to not only language, but also mimesis, More specifically they describe the ability to create language as part of an internal modeling of reality specific to humankind. Other results are consciousness, and imagination or fantasy. In contrast, modern proponents of a genetic pre-disposition to language itself include Noam Chomsky and Steven Pinker, to whom Donald and Deacon can be contrasted.
As reason is symbolic thinking, and peculiarly human, then this implies that humans have a special ability to maintain a clear consciousness of the distinctness of "icons" or images and the real things they represent. Starting with a modern author, Merlin Donald writes
A dog might perceive the "meaning" of a fight that was realistically play-acted by humans, but it could not reconstruct the message or distinguish the representation from its referent (a real fight). [...] Trained apes are able to make this distinction; young children make this distinction early – hence, their effortless distinction between play-acting an event and the event itself
In classical descriptions, an equivalent description of this mental faculty is "eikasia", in the philosophy of Plato. This is the ability to perceive whether a perception is an image of something else, related somehow but not the same, and therefore allows humans to perceive that a dream or memory or a reflection in a mirror is not reality as such. What Klein refers to as "dianoetic eikasia" is the "eikasia" concerned specifically with thinking and mental images, such as those mental symbols, icons, "signes", and marks discussed above as definitive of reason. Explaining reason from this direction: human thinking is special in the way that we often understand visible things as if they were themselves images of our intelligible "objects of thought" as "foundations" ("hypothēses" in Ancient Greek). This thinking ("dianoia") is "...an activity which consists in making the vast and diffuse jungle of the visible world depend on a plurality of more 'precise' "noēta"."
Both Merlin Donald and the Socratic authors such Plato and Aristotle emphasize the importance of "mimesis", often translated as "imitation" or "representation". Donald writes
Imitation is found especially in monkeys and apes [... but ...] Mimesis is fundamentally different from imitation and mimicry in that it involves the invention of intentional representations. [...] Mimesis is not absolutely tied to external communication.
"Mimēsis" is a concept, now popular again in academic discussion, that was particularly prevalent in Plato's works, and within Aristotle, it is discussed mainly in the "Poetics". In Michael Davis's account of the theory of man in this work.
It is the distinctive feature of human action, that whenever we choose what we do, we imagine an action for ourselves as though we were inspecting it from the outside. Intentions are nothing more than imagined actions, internalizings of the external. All action is therefore imitation of action; it is poetic...
Donald like Plato (and Aristotle, especially in "On Memory and Recollection"), emphasizes the peculiarity in humans of voluntary initiation of a search through one's mental world. The ancient Greek "anamnēsis", normally translated as "recollection" was opposed to "mneme" or "memory". Memory, shared with some animals, requires a consciousness not only of what happened in the past, but also "that" something happened in the past, which is in other words a kind of "eikasia" "...but nothing except man is able to recollect." Recollection is a deliberate effort to search for and recapture something once known. Klein writes that, "To become aware of our having forgotten something means to begin recollecting." Donald calls the same thing "autocueing", which he explains as follows: "Mimetic acts are reproducible on the basis of internal, self-generated cues. This permits voluntary recall of mimetic representations, without the aid of external cues – probably the earliest form of representational "thinking"."
In a celebrated paper in modern times, the fantasy author and philologist J.R.R. Tolkien wrote in his essay "On Fairy Stories" that the terms "fantasy" and "enchantment" are connected to not only "...the satisfaction of certain primordial human desires..." but also "...the origin of language and of the mind."
Logical reasoning methods and argumentation.
Looking at logical categorizations of different types of reasoning the traditional main division made in philosophy is between deductive reasoning and inductive reasoning. Formal logic has been described as "the science of deduction". The study of inductive reasoning is generally carried out within the field known as informal logic or critical thinking.
Deductive reasoning.
A subdivision of Philosophy is Logic. Logic is the study of reasoning. Deduction is a form of reasoning in which a conclusion follows necessarily from the stated premises. A deduction is also the conclusion reached by a deductive reasoning process. One classic example of deductive reasoning is that found in syllogisms like the following:
The reasoning in this argument is valid, because there is no way in which the premises, 1 and 2, could be true and the conclusion, 3, be false.
Inductive reasoning.
Induction is a form of inference producing propositions about unobserved objects or types, either specifically or generally, based on previous observation. It is used to ascribe properties or relations to objects or types based on previous observations or experiences, or to formulate general statements or laws based on limited observations of recurring phenomenal patterns.
Inductive reasoning contrasts strongly with deductive reasoning in that, even in the best, or strongest, cases of inductive reasoning, the truth of the premises does not guarantee the truth of the conclusion. Instead, the conclusion of an inductive argument follows with some degree of probability. Relatedly, the conclusion of an inductive argument contains more information than is already contained in the premises. Thus, this method of reasoning is ampliative.
A classic example of inductive reasoning comes from the empiricist David Hume:
Abductive reasoning.
Abductive reasoning, or argument to the best explanation, is a form of inductive reasoning, since the conclusion in an abductive argument does not follow with certainty from its premises and concerns something unobserved. What distinguishes abduction from the other forms of reasoning is an attempt to favour one conclusion above others, by attempting to falsify alternative explanations or by demonstrating the likelihood of the favoured conclusion, given a set of more or less disputable assumptions. For example, when a patient displays certain symptoms, there might be various possible causes, but one of these is preferred above others as being more probable.
Analogical reasoning.
Analogical reasoning is incorrectly reasoning from the particular to the particular. An example follows:
Analogical reasoning can be viewed as a form of inductive reasoning from a single example, but if it is intended as inductive reasoning it is a bad example, because inductive reasoning typically uses a large number of examples to reason from the particular to the general. Analogical reasoning often leads to wrong conclusions. For example
Fallacious reasoning.
Flawed reasoning in arguments is known as fallacious reasoning. Bad reasoning within arguments can be because it commits either a formal fallacy or an informal fallacy.
Formal fallacies occur when there is a problem with the form, or structure, of the argument. The word "formal" refers to this link to the "form" of the argument. An argument that contains a formal fallacy will always be invalid.
An informal fallacy is an error in reasoning that occurs due to a problem with the "content", rather than mere "structure", of the argument.
Traditional problems raised concerning reason.
Philosophy is sometimes described as a life of reason, with normal human reason pursued in a more consistent and dedicated way than usual. Two categories of problem concerning reason have long been discussed by philosophers concerning reason, essentially being reasonings about reasoning itself as a human aim, or philosophizing about philosophizing. The first question is concerning whether we can be confident that reason can achieve knowledge of truth better than other ways of trying to achieve such knowledge. The other question is whether a life of reason, a life that aims to be guided by reason, can be expected to achieve a happy life more so than other ways of life (whether such a life of reason results in knowledge or not).
Reason versus truth, and "first principles".
Since classical times a question has remained constant in philosophical debate (which is sometimes seen as a conflict between movements called Platonism and Aristotelianism) concerning the role of reason in confirming truth. People use logic, deduction, and induction, to reach conclusions they think are true. Conclusions reached in this way are considered more certain than sense perceptions on their own. On the other hand, if such reasoned conclusions are only built originally upon a foundation of sense perceptions, then, our most logical conclusions can never be said to be certain because they are built upon the very same fallible perceptions they seek to better.
This leads to the question of what types of first principles, or starting points of reasoning, are available for someone seeking to come to true conclusions. In Greek, "first principles" are "archai", "starting points", and the faculty used to perceive them is sometimes referred to in Aristotle and Plato as "nous" which was close in meaning to "awareness" or "consciousness".
Empiricism (sometimes associated with Aristotle but more correctly associated with British philosophers such as John Locke and David Hume, as well as their ancient equivalents such as Democritus) asserts that sensory impressions are the only available starting points for reasoning and attempting to attain truth. This approach always leads to the controversial conclusion that absolute knowledge is not attainable. Idealism, (associated with Plato and his school), claims that there is a "higher" reality, from which certain people can directly arrive at truth without needing to rely only upon the senses, and that this higher reality is therefore the primary source of truth.
Philosophers such as Plato, Aristotle, Al-Farabi, Avicenna, Averroes, Maimonides, Aquinas and Hegel are sometimes said to have argued that reason must be fixed and discoverable—perhaps by dialectic, analysis, or study. In the vision of these thinkers, reason is divine or at least has divine attributes. Such an approach allowed religious philosophers such as Thomas Aquinas and Étienne Gilson to try to show that reason and revelation are compatible. According to Hegel, "...the only thought which Philosophy brings with it to the contemplation of History, is the simple conception of reason; that reason is the Sovereign of the World; that the history of the world, therefore, presents us with a rational process."
Since the 17th century rationalists, reason has often been taken to be a subjective faculty, or rather the unaided ability (pure reason) to form concepts. For Descartes, Spinoza and Leibniz, this was associated with mathematics. Kant attempted to show that pure reason could form concepts (time and space) that are the conditions of experience. Kant made his argument in opposition to Hume, who denied that reason had any role to play in experience.
Reason versus emotion or passion.
After Plato and Aristotle, western literature often treated reason as being the faculty that trained the passions and appetites. Stoic philosophy by contrast considered all passions bad. After the critiques of reason in the early Enlightenment the appetites were rarely discussed or conflated with the passions. Some Enlightenment camps took after the Stoics to say Reason should oppose Passion rather than order it, while others like the Romantics considered Passion the ruler over Reason or to the exclusion of Reason, thus the Modern colloquy of "follow your heart".
Reason has been seen as a slave, or judge, of the passions, notably in the work of David Hume, and more recently of Freud. Reasoning which claims that the object of a desire is demanded by logic alone is called "rationalization".
Rousseau first proposed, in his second "Discourse", that reason and political life is not natural and possibly harmful to mankind. He asked what really can be said about what is natural to mankind. What, other than reason and civil society, "best suits his constitution"? Rousseau saw "two principles prior to reason" in human nature. First we hold an intense interest in our own well-being. Secondly we object to the suffering or death of any sentient being, especially one like ourselves. These two passions lead us to desire more than we could achieve. We become dependent upon each other, and on relationships of authority and obedience. This effectively puts the human race into slavery. Rousseau says that he almost dares to assert that nature does not destine men to be healthy. According to Velkley, "Rousseau outlines certain programs of rational self-correction, most notably the political legislation of the "Contrat Social" and the moral education in "". All the same, Rousseau understands such corrections to be only ameliorations of an essentially unsatisfactory condition, that of socially and intellectually corrupted humanity."
This quandary presented by Rousseau led to Kant's new way of justifying reason as freedom to create good and evil. These therefore are not to be blamed on nature or God. In various ways, German Idealism after Kant, and major later figures such Nietzsche, Bergson, Husserl, Scheler, and Heidegger, remain pre-occupied with problems coming from the metaphysical demands or "urges" of "reason". The influence of Rousseau and these later writers is also large upon art and politics. Many writers (such as Nikos Kazantzakis) extol passion and disparage reason. In politics modern nationalism comes from Rousseau's argument that rationalist cosmopolitanism brings man ever further from his natural state.
Another view on reason and emotion was proposed in the 1994 book titled "Descartes' Error" by Antonio Damasio. In it, Damasio presents the "Somatic Marker Hypothesis" which states that emotions guide behavior and decision-making. Damasio argues that these somatic markers (known collectively as "gut feelings") are "intuitive signals" that direct our decision making processes in a certain way that cannot be solved with rationality alone. Damasio further argues that rationality requires emotional input in order to function.
Reason versus faith or tradition.
There are many religious traditions, some of which are explicitly fideist and others of which claim varying degrees of rationalism. Secular critics sometimes accuse all religious adherents of irrationality, since they claim such adherents are guilty of ignoring, suppressing, or forbidding some kinds of reasoning concerning some subjects (such as religious dogmas, moral taboos, etc.). Though the theologies and religions such as classical monotheism typically do not claim to be irrational, there is often a perceived conflict or tension between faith and tradition on the one hand, and reason on the other, as potentially competing sources of wisdom, law and truth.
Religious adherents sometimes respond by arguing that faith and reason can be reconciled, or have different non-overlapping domains, or that critics engage in a similar kind of irrationalism: 
Some commentators have claimed that Western civilization can be almost defined by its serious testing of the limits of tension between "unaided" reason and faith in "revealed" truths—figuratively summarized as Athens and Jerusalem, respectively. Leo Strauss spoke of a "Greater West" that included all areas under the influence of the tension between Greek rationalism and Abrahamic revelation, including the Muslim lands. He was particularly influenced by the great Muslim philosopher Al-Farabi. To consider to what extent Eastern philosophy might have partaken of these important tensions, Strauss thought it best to consider whether dharma or tao may be equivalent to Nature (by which we mean "physis" in Greek). According to Strauss the beginning of philosophy involved the "discovery or invention of nature" and the "pre-philosophical equivalent of nature" was supplied by "such notions as 'custom' or 'ways, which appear to be "really universal in all times and places". The philosophical concept of nature or natures as a way of understanding "archai" (first principles of knowledge) brought about a peculiar tension between reasoning on the one hand, and tradition or faith on the other.
Although there is this special history of debate concerning reason and faith in the Islamic, Christian and Jewish traditions, the pursuit of reason is sometimes argued to be compatible with the other practice of other religions of a different nature, such as Hinduism, because they do not define their tenets in such an absolute way.
Reason in particular fields of study.
Reason in political philosophy and ethics.
Aristotle famously described reason (with language) as a part of human nature, which means that it is best for humans to live "politically" meaning in communities of about the size and type of a small city state ("polis" in Greek). For example...
It is clear, then, that a human being is more of a political ["politikon" = of the "polis"] animal ["zōion"] than is any bee or than any of those animals that live in herds. For nature, as we say, makes nothing in vain, and humans are the only animals who possess reasoned speech ["logos"]. Voice, of course, serves to indicate what is painful and pleasant; that is why it is also found in other animals, because their nature has reached the point where they can perceive what is painful and pleasant and express these to each other. But speech ["logos"] serves to make plain what is advantageous and harmful and so also what is just and unjust. For it is a peculiarity of humans, in contrast to the other animals, to have perception of good and bad, just and unjust, and the like; and the community in these things makes a household or city ["polis"]. [...] By nature, then, the drive for such a community exists in everyone, but the first to set one up is responsible for things of very great goodness. For as humans are the best of all animals when perfected, so they are the worst when divorced from law and right. The reason is that injustice is most difficult to deal with when furnished with weapons, and the weapons a human being has are meant by nature to go along with prudence and virtue, but it is only too possible to turn them to contrary uses. Consequently, if a human being lacks virtue, he is the most unholy and savage thing, and when it comes to sex and food, the worst. But justice is something political do with the "polis", for right is the arrangement of the political community, and right is discrimination of what is just. (Aristotle's Politics 1253a 1.2. Peter Simpson's translation, with Greek terms inserted in square brackets.)
The concept of human nature being fixed in this way, implied, in other words, that we can define what type of community is always best for people. This argument has remained a central argument in all political, ethical and moral thinking since then, and has become especially controversial since firstly Rousseau's Second Discourse, and secondly, the Theory of Evolution. Already in Aristotle there was an awareness that the "polis" had not always existed and had needed to be invented or developed by humans themselves. The household came first, and the first villages and cities were just extensions of that, with the first cities being run as if they were still families with Kings acting like fathers.
Friendship ["philia"] seems to prevail man and woman according to nature ["kata phusin"; for people are by nature ["tēi phusei"] pairing ["sunduastikon"] more than political ["politikon" = of the "polis"], inasmuch as the household ["oikos"] is prior ["proteron" = earlier] and more necessary than the "polis" and making children is more common ["koinoteron"] with the animals. In the other animals, community ["koinōnia"] goes no further than this, but people live together ["sumoikousin"] not only for the sake of making children, but also for the things for life; for from the start the functions ["erga"] are divided, and are different man and woman. Thus they supply each other, putting their own into the common ["eis to koinon". It is for these that both utility ["chrēsimon" and pleasure ["hēdu"] seem to be found in this kind of friendship. (Nicomachean Ethics, VIII.12.1162a. Rough literal translation with Greek terms shown in square brackets.)
Rousseau in his Second Discourse finally took the shocking step of claiming that this traditional account has things in reverse: with reason, language and rationally organized communities all having developed over a long period of time merely as a result of the fact that some habits of cooperation were found to solve certain types of problems, and that once such cooperation became more important, it forced people to develop increasingly complex cooperation—often only to defend themselves from each other.
In other words, according to Rousseau, reason, language and rational community did not arise because of any conscious decision or plan by humans or gods, nor because of any pre-existing human nature. As a result, he claimed, living together in rationally organized communities like modern humans is a development with many negative aspects compared to the original state of man as an ape. If anything is specifically human in this theory, it is the flexibility and adaptability of humans. This view of the animal origins of distinctive human characteristics later received support from Charles Darwin's Theory of Evolution.
The two competing theories concerning the origins of reason are relevant to political and ethical thought because, according to the Aristotelian theory, a best way of living together exists independently of historical circumstances. According to Rousseau, we should even doubt that reason, language and politics are a good thing, as opposed to being simply the best option given the particular course of events that lead to today. Rousseau's theory, that human nature is malleable rather than fixed, is often taken to imply, for example by Karl Marx, a wider range of possible ways of living together than traditionally known.
However, while Rousseau's initial impact encouraged bloody revolutions against traditional politics, including both the French Revolution and the Russian Revolution, his own conclusions about the best forms of community seem to have been remarkably classical, in favor of city-states such as Geneva, and rural living.
Psychology.
Scientific research into reasoning is carried out within the fields of psychology and cognitive science. Psychologists attempt to determine whether or not people are capable of rational thought in a number of different circumstances.
Assessing how well someone engages in reasoning is the project of determining the extent to which the person is rational or acts rationally. It is a key research question in the psychology of reasoning. Rationality is often divided into its respective theoretical and practical counterparts.
Behavioral experiments on human reasoning.
Experimental cognitive psychologists carry out research on reasoning behaviour. Such research may focus, for example, on how people perform on tests of reasoning such as intelligence or IQ tests, or on how well people's reasoning matches ideals set by logic (see, for example, the Wason test). Experiments examine how people make inferences from conditionals e.g., "If A then B" and how they make inferences about alternatives, e.g., "A or else B". They test whether people can make valid deductions about spatial and temporal relations, e.g., "A is to the left of B", or "A happens after B", and about quantified assertions, e.g., "All the A are B". Experiments investigate how people make inferences about factual situations, hypothetical possibilities, probabilities, and counterfactual situations.
Developmental studies of children's reasoning.
Developmental psychologists investigate the development of reasoning from birth to adulthood. Piaget's theory of cognitive development was the first complete theory of reasoning development. Subsequently, several alternative theories were proposed, including the neo-Piagetian theories of cognitive development.
Neuroscience of reasoning.
The biological functioning of the brain is studied by neurophysiologists and neuropsychologists. Research in this area includes research into the structure and function of normally functioning brains, and of damaged or otherwise unusual brains. In addition to carrying out research into reasoning, some psychologists, for example, clinical psychologists and psychotherapists work to alter people's reasoning habits when they are unhelpful.
Computer science.
Automated reasoning.
In artificial intelligence and computer science, scientists study and use automated reasoning for diverse applications including automated theorem proving the formal semantics of programming languages, and formal specification in software engineering.
Meta-reasoning.
Meta-reasoning is reasoning about reasoning. In computer science, a system performs meta-reasoning when it is reasoning about its own operation. This requires a programming language capable of reflection, the ability to observe and modify its own structure and behaviour.
Evolution of reason.
A species could benefit greatly from better abilities to reason about, predict and understand the world. French social and cognitive scientist Dan Sperber, with his colleague Hugo describes the idea that there could have been other forces driving the evolution of reason. Sperber points out that reasoning is very difficult for humans to do effectively, and that it is hard for individuals to doubt their own beliefs. Reasoning is most effective when it is done as a collective - as demonstrated by the success of projects like science. Sperber says this could suggest that there are not just individual, but group selection pressures at play. Any group that managed to find ways of reasoning effectively would reap benefits for all its members, increasing their fitness. This could also help explain why humans, according to Sperber, are not optimized to reason effectively alone. Patricia Cohen, writing for The New York Times, summarizes some of Mercier's thoughts on this "Argumentative Theory" (which states that reason is adapted to persuasion). To Cohen, the idea is that humans debate like lawyers: they often commit to one side of an argument and converse until the truth is discovered.

</doc>
