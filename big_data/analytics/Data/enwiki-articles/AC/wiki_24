<doc id="14059" url="https://en.wikipedia.org/wiki?curid=14059" title="Howard Hughes">
Howard Hughes

Howard Robard Hughes, Jr. (December 24, 1905 – April 5, 1976) was an American business tycoon, entrepreneur, investor, aviator, aerospace engineer, inventor, filmmaker and philanthropist. During his life, he was known as one of the most financially successful individuals in the world. As a maverick film tycoon, Hughes gained prominence in Hollywood from the late 1920s, making big-budget and often controversial films like "The Racket" (1928), "Hell's Angels" (1930), "Scarface" (1932), and "The Outlaw" (1943).
Hughes formed the Hughes Aircraft Company in 1932, hiring numerous engineers and designers. He spent the rest of the 1930s setting multiple world air speed records and building the Hughes H-1 Racer and H-4 Hercules (now better known as the "Spruce Goose"). He also acquired and expanded Trans World Airlines (TWA, subsequently acquired by and merged with American Airlines) and later acquired Air West, renaming it Hughes Airwest. Hughes Airwest was eventually acquired by and merged into Republic Airlines.
Hughes was included in "Flying" Magazine's list of the 51 Heroes of Aviation, ranking at 25. He is remembered for his eccentric behavior and reclusive lifestyle in later life, caused in part by a worsening obsessive–compulsive disorder (OCD) and chronic pain. His legacy is maintained through the Howard Hughes Medical Institute.
Early years.
Hughes' birthplace is recorded as either Humble or Houston, Texas. The date is also uncertain, though Hughes claimed that his birthday was Christmas Eve. A 1941 affidavit birth certificate of Hughes, signed by his aunt Annette Gano Lummis and Estelle Boughton Sharp, states that he was born on December 24, 1905, in Harris County, Texas. However, his baptismal record of October 7, 1906, in the parish register of St. John's Episcopal Church in Keokuk, Iowa, has his birth listed as September 24, 1905, without reference to the place of birth.
His parents were Howard R. Hughes, Sr., a successful inventor and businessman from Missouri of English descent, and Allene Stone Gano. His father had patented the two-cone roller bit, which allowed rotary drilling for petroleum in previously inaccessible places. The senior Hughes made the shrewd and lucrative decision to commercialize the invention by leasing the bits instead of selling them, and founded the Hughes Tool Company in 1909. Hughes's uncle was the famed novelist, screenwriter, and film director Rupert Hughes.
Hughes demonstrated interest in science and technology at a young age. In particular, he had great engineering aptitude, building Houston's first "wireless" radio transmitter at age 11. He went on to be one of the first licensed ham radio operators in Houston, having the assigned callsign W5CY (originally 5CY). At 12, Hughes was photographed in the local newspaper, identified as being the first boy in Houston to have a "motorized" bicycle, which he had built from parts from his father's steam engine. He was an indifferent student, with a liking for mathematics, flying, and mechanics. He took his first flying lesson at 14, and later attended math and aeronautical engineering courses at Caltech.
Allene Hughes died in March 1922 from complications of an ectopic pregnancy. Howard Hughes, Sr., died of a heart attack in 1924. Their deaths apparently inspired Hughes to include the creation of a medical research laboratory in the will that he signed in 1925 at age 19. Howard Sr.'s will had not been updated since Allene's death, and Hughes inherited 75 percent of the family fortune. On his 19th birthday, Hughes was declared an emancipated minor, enabling him to take full control of his life.
Hughes was an excellent and enthusiastic golfer from a young age, often scoring near par figures, and held a handicap of three during his twenties. He played frequently with top players, including Gene Sarazen. Hughes rarely played competitively, and gradually gave up his passion for the sport to pursue other interests.
Hughes withdrew from Rice University shortly after his father's death. On June 1, 1925, he married Ella Botts Rice, daughter of David Rice and Martha Lawson Botts of Houston. They moved to Los Angeles, where he hoped to make a name for himself as a filmmaker.
Business career.
Hughes enjoyed a highly successful business career beyond engineering, aviation, and filmmaking, though many of his career endeavors involved varying entrepreneurial roles. The Summa Corporation was the name adopted for the business interests of Howard Hughes after he sold the tool division of Hughes Tool Company in 1972. The company serves as the principal holding company for Hughes's business ventures and investments. It is primarily involved in aerospace and defense, electronics, mass media, manufacturing, and hospitality industries, but has maintained a strong presence in a wide variety of industries including real estate, petroleum drilling and oilfield services, consulting, entertainment, and mining. Much of his fortune was later used for philanthropic causes, notably towards health care and medical research.
Entertainment.
Hughes entered the entertainment industry after dropping out of Rice University and moving to Los Angeles. His first two films, "Everybody's Acting" (1927) and "Two Arabian Knights" (1928), were financial successes, the latter winning the first Academy Award for Best Director of a comedy picture.
"The Racket" (1928) and "The Front Page" (1931) were also nominated for Academy Awards.
Hughes spent $3.8 million to make the flying film "Hell's Angels" (1930). It earned nearly $8 million, about double the production and advertising costs. "Hell's Angels" received one Academy Award nomination for Best Cinematography.
He produced another hit, "Scarface" (1932), a production delayed by censors' concern over its violence.
"The Outlaw" (1943) was completed in 1941 and featured Jane Russell. It also received considerable attention from industry censors, this time owing to Russell's revealing costumes. Hughes designed a special bra for his leading lady, although Russell decided against wearing it.
RKO.
For a period of time in the 1940s to late 1950s, Hughes Tool Company ventured into the film and media industry where it then owned the RKO companies, including: RKO Pictures; RKO Studios; RKO Theatres, a chain of movie theatres; the RKO Radio Network, a network of radio stations.
In 1948, Hughes gained control of RKO, a struggling major Hollywood studio, by acquiring 25 percent of the outstanding stock from Floyd Odlum's Atlas Corporation. Within weeks of taking control, he dismissed three-quarters of the work force, and production was shut down for six months in 1949 while he undertook the investigation of the politics of all remaining studio employees. Completed pictures would be sent back for re-shooting if he felt that his star (especially female) was not properly presented, or if a film's anti-communist politics were not sufficiently clear. In 1952, an abortive sale to a Chicago-based group with no experience in the industry disrupted studio operations even further.
Hughes sold the RKO theaters in 1953 as settlement of the "United States v. Paramount Pictures, Inc." antitrust case. With the sale of the profitable theaters, the shaky status of the film studio became increasingly apparent. A steady stream of lawsuits from RKO's minority shareholders became an increasing nuisance, charging him with financial misconduct and corporate mismanagement, especially because Hughes wanted to focus on his aircraft-manufacturing and TWA holdings during the Korean War years. Eager to be rid of the distraction, Hughes offered to buy out all other stockholders.
He had gained near-total control of RKO by the end of 1954, at a cost of nearly $24 million, becoming the closest thing to a sole owner of a Hollywood studio seen in three decades. Six months later, Hughes sold the studio to the General Tire and Rubber Company for $25 million. Hughes retained the rights to pictures that he had personally produced, including those made at RKO. He also retained Jane Russell's contract. For Howard Hughes, this was the virtual end of his 25-year involvement in motion pictures; his reputation as a financial wizard emerged unscathed. He reportedly walked away from RKO having made $6.5 million in personal profit.
General Tire was interested mainly in exploiting the value of the RKO library for television programming, though it made some attempts to continue producing films. After a year and a half of mixed success, General Tire shut down film production at RKO for good at the end of January 1957. The studio lots in Hollywood and Culver City were sold to Desilu Productions later that year for $6.15 million.
Real estate.
Beyond extending his business prowess in the manufacturing, aviation, entertainment, and hospitality industries, Hughes was a successful real estate investor. Hughes was deeply involved in the American real estate industry where he amassed vast holdings of undeveloped land both in Las Vegas and in the desert surrounding the city that had gone unused during his lifetime. In 1968, the Hughes Tool Company purchased the North Las Vegas Air Terminal.
Originally known as Summa Corporation, The Howard Hughes Corporation was formed in 1972 when the oil tools business of Hughes Tool Company, then owned by Howard Hughes, Jr., was floated on the New York Stock Exchange under the Hughes Tool name. This forced the remaining businesses of the "original" Hughes Tool to adopt a new corporate name Summa. The name "Summa", Latin for "highest", was adopted without the approval of Hughes himself, who preferred to keep his own name on the business and suggested HRH Properties (for Hughes Resorts and Hotels, and also his own initials).
Initially staying in the Desert Inn, Hughes refused to vacate his room and instead decided to purchase the entire hotel. Hughes extended his financial empire to include Las Vegas real estate, hotels and media outlets, spending an estimated $300 million and using his considerable powers to take-over many of the well known hotels, especially the organized crime connected venues and he quickly became one of the most powerful men in Las Vegas. He was instrumental in changing the image of Las Vegas from its Wild West roots into a more refined cosmopolitan city.
Technology.
Another portion of Hughes's business interests lay in aviation, airlines, and the aerospace and defense industries. Hughes was a lifelong aircraft enthusiast and pilot. At Rogers Airport in Los Angeles, he learned to fly from pioneer aviators, including Moye Stephens. He set many world records and commissioned the construction of custom aircraft for himself while heading Hughes Aircraft at the airport in Glendale, CA. Operating from there, the most technologically important aircraft he commissioned was the Hughes H-1 Racer. On September 13, 1935, Hughes, flying the H-1, set the landplane airspeed record of over his test course near Santa Ana, California (Giuseppe Motta reached 362 mph in 1929 and George Stainforth reached 407.5 mph in 1931, both in seaplanes). This was the last time in history that the world airspeed record was set in an aircraft built by a private individual. A year and a half later, on January 19, 1937, flying the same H-1 Racer fitted with longer wings, Hughes set a new transcontinental airspeed record by flying non-stop from Los Angeles to Newark in 7 hours, 28 minutes and 25 seconds (beating his own previous record of 9 hours, 27 minutes). His average ground speed over the flight was .
The H-1 Racer featured a number of design innovations: it had retractable landing gear (as Boeing Monomail had five years before) and all rivets and joints set flush into the body of the aircraft to reduce drag. The H-1 Racer is thought to have influenced the design of a number of World War II fighters such as the Mitsubishi Zero, the Focke-Wulf Fw 190 and the F8F Bearcat; although that has never been reliably confirmed. The H-1 Racer was donated to the Smithsonian in 1975 and is on display at the National Air and Space Museum.
Round-the-world flight.
On July 14, 1938, Hughes set another record by completing a flight around the world in just 91 hours (3 days, 19 hours, 17 minutes), beating the previous record set in 1933 by Wiley Post in a single engine Lockheed Vega by almost four days. Hughes returned home ahead of photographs of his flight. Taking off from New York City, Hughes continued to Paris, Moscow, Omsk, Yakutsk, Fairbanks, Minneapolis, then returning to New York City. For this flight he flew a Lockheed 14 Super Electra (NX18973, a twin-engine transport with a four-man crew) fitted with the latest radio and navigational equipment. Hughes wanted the flight to be a triumph of American aviation technology, illustrating that safe, long-distance air travel was possible. While he had previously been relatively obscure despite his wealth, being better known for dating Katharine Hepburn, New York City now gave Hughes a ticker-tape parade in the Canyon of Heroes. In 1938, the William P. Hobby Airport in Houston, Texas—known at the time as Houston Municipal Airport—was renamed after Hughes, but the name was changed back after people objected to naming the airport after a living person.
He also had a role in the design and financing of both the Boeing 307 Stratoliner and Lockheed L-049 Constellation.
Hughes received many awards as an aviator, including the Harmon Trophy in 1936 and 1938, the Collier Trophy and the Bibesco Cup of the Fédération Aéronautique Internationale in 1938, the Octave Chanute Award in 1940, and a special Congressional Gold Medal in 1939 "in recognition of the achievements of Howard Hughes in advancing the science of aviation and thus bringing great credit to his country throughout the world". According to his obituary in the "New York Times", Hughes never bothered to come to Washington to pick up the Congressional Gold Medal, which was eventually mailed to him.
Hughes D-2 and XF-11.
The Hughes D-2 was conceived in 1939 as a bomber with five crew members, powered by 42-cylinder Wright R-2160 Tornado engines. In the end it appeared as two-seat fighter-reconnaissance aircraft designated the D-2A, powered by two Pratt & Whitney R-2800-49 engines. The aircraft was constructed using the Duramold process. The prototype was brought to Harper's Dry Lake California in great secrecy in 1943 and first flew on June 20 of that year. Acting on a recommendation of the president's son, Colonel Elliott Roosevelt, who had become friends with Hughes, in September 1943 the USAAF ordered 100 of a reconnaissance development of the D-2, known as the F-11. Hughes then attempted to get the military to pay for the development of the D-2. In November 1944, the hangar containing the D-2A was reportedly hit by lightning and the aircraft was destroyed. The D-2 design was abandoned, but led to the extremely controversial Hughes XF-11. The XF-11 was a large all-metal, two-seat reconnaissance aircraft, powered by two Pratt & Whitney R-4360-31 engines, each driving a set of contra-rotating propellers. Only the two prototypes were completed; the second one with a single propeller per side.
Near-fatal crash of the Sikorsky S-43.
In the spring of 1943 Hughes spent nearly a month in Las Vegas, test flying his Sikorsky S-43 amphibian aircraft, practicing touch-and-go landings on Lake Mead in preparation for flying the H-4 Hercules. The weather conditions at the lake during the day were ideal and he enjoyed Las Vegas at night. On May 17, 1943, Hughes flew the Sikorsky from California carrying two CAA aviation inspectors, two of his employees and actress Ava Gardner. Hughes dropped Gardner off in Las Vegas and proceeded to Lake Mead to conduct qualifying tests in the S-43. The test flight did not go well. The Sikorsky crashed into Lake Mead, killing CAA inspector Ceco Cline and Hughes employee Richard Felt. Hughes suffered a severe gash on the top of his head when he hit the upper control panel and had to be rescued by one of the others on board. Hughes paid divers $100,000 to raise the aircraft and later spent more than $500,000 restoring the aircraft.
Near-fatal crash of the XF-11.
Hughes was involved in a near-fatal aircraft accident on July 7, 1946, while performing the first flight of the prototype U.S. Army Air Forces reconnaissance aircraft, the XF-11, near Hughes airfield at Culver City, California. An oil leak caused one of the contra-rotating propellers to reverse pitch, causing the aircraft to yaw sharply and lose altitude rapidly. Hughes attempted to save the aircraft by landing it at the Los Angeles Country Club golf course, but just seconds before reaching the course, the XF-11 started to drop dramatically and crashed in the Beverly Hills neighborhood surrounding the country club.
When the XF-11 finally came to a halt after destroying three houses, the fuel tanks exploded, setting fire to the aircraft and a nearby home at 808 North Whittier Drive, owned by Lt Col. Charles E. Meyer. Hughes managed to pull himself out of the flaming wreckage but lay beside the aircraft until he was rescued by Marine Master Sgt. William L. Durkin, who happened to be in the area visiting friends. Hughes sustained significant injuries in the crash, including a crushed collar bone, multiple cracked ribs, crushed chest with collapsed left lung, shifting his heart to the right side of the chest cavity, and numerous third-degree burns. An oft-told story said that Hughes sent a check to the Marine weekly for the remainder of his life as a sign of gratitude. However, Durkin's daughter denied that he took any money for the rescue.
Despite his physical injuries, Hughes was proud that his mind was still working. As he lay in his hospital bed, he decided that he did not like the bed's design. He called in plant engineers to design a customized bed, equipped with hot and cold running water, built in six sections, and operated by 30 electric motors, with push-button adjustments. The hospital bed was designed by Hughes specifically to alleviate the pain caused by moving with severe burn injuries. Despite the fact that he never had the chance to use the bed that he designed, Hughes's bed served as a prototype for the modern hospital bed. Hughes's doctors considered his recovery almost miraculous. Hughes, however, believed that neither miracle nor modern medicine contributed to his recovery. Instead he vigorously asserted that the natural life-giving properties of fresh squeezed orange juice were responsible. (He drank only orange juice that he had personally witnessed being freshly squeezed.) 
Many attribute his long-term dependence on opiates to his use of codeine as a painkiller during his convalescence. The trademark mustache he wore afterward was used to hide a scar on his upper lip resulting from the accident.
H-4 Hercules.
The War Production Board (not the military) originally contracted with Henry Kaiser and Hughes to produce the gigantic HK-1 Hercules flying boat for use during World War II to transport troops and equipment across the Atlantic as an alternative to seagoing troop transport ships that were vulnerable to German U-boats. The project was opposed by the military services, thinking it would siphon resources from higher priority programs, but was advocated by Hughes's powerful allies in Washington, D.C. After disputes, Kaiser withdrew from the project and Hughes elected to continue it as the H-4 Hercules. However, the aircraft was not completed until after the end of World War II.
The Hercules was the world's largest flying boat, the largest aircraft made from wood, and, at , had the longest wingspan of any aircraft (the next largest wingspan was about ). (The Hercules is no longer the longest or heaviest aircraft ever built; both of those titles are currently held by the Antonov An-225 "Mriya".)
The Hercules flew only once for one mile (1.6 km), and above the water, with Hughes at the controls, on November 2, 1947.
The Hercules was nicknamed the "Spruce Goose" by critics, but was actually made largely from birch (not spruce), rather than of aluminum, because the contract required that Hughes build the aircraft of "non-strategic materials". It was built in Hughes's Westchester, California facility. In 1947, Howard Hughes was summoned to testify before the Senate War Investigating Committee to explain why the H-4 development had been so troubled, and why the F-11 had resulted in only two prototypes after $22 million spent. General Elliott Roosevelt and numerous other USAAF officers were also called to testify in hearings that transfixed the nation during August and again in November 1947. In a hotly disputed testimony over TWA's route awards and malfeasance in the defense acquisition process, Hughes turned the tables on his main interlocutor, Maine Senator Owen Brewster, and the hearings were widely interpreted as a Hughes victory. After display at the Long Beach, California harbor, the Hercules was moved to McMinnville, Oregon, where it is now part of the Evergreen Aviation Museum.
Hughes Aircraft.
Hughes Aircraft Company, a division of Hughes Tool Company, was originally founded by Hughes in 1932, in a rented corner of a Lockheed Aircraft Corporation hangar in Burbank, California, to build the H-1 racer. During and after World War II, Hughes fashioned his company into a major defense contractor. The Hughes Helicopters division started in 1947 when helicopter manufacturer Kellett sold their latest design to Hughes for production. The company was a major American aerospace and defense contractor manufacturing numerous technology related products that include spacecraft vehicles, military aircraft, radar systems, electro-optical systems, the first working laser, aircraft computer systems, missile systems, ion-propulsion engines (for space travel), commercial satellites, and other electronics systems.
In 1948, Hughes created a new division of the company, the Hughes Aerospace Group. The Hughes Space and Communications Group and the Hughes Space Systems Division were later spun off in 1948 to form their own divisions and ultimately became the Hughes Space and Communications Company in 1961. In 1953, Howard Hughes gave all his stock in the Hughes Aircraft Company to the newly formed Howard Hughes Medical Institute, thereby turning the aerospace and defense contractor into a tax-exempt charitable organization. The Howard Hughes Medical Institute sold Hughes Aircraft in 1985 to General Motors for $5.2 billion. In 1997, General Motors sold Hughes Aircraft to Raytheon and in 2000, sold Hughes Space & Communications to Boeing. A combination of Boeing, GM and Raytheon acquired the Hughes Research Laboratories, where it focused on advanced developments in microelectronics, information & systems sciences, materials, sensors, and photonics; their workspace spans from basic research to product delivery. It has particularly emphasized capabilities in high performance integrated circuits, high power lasers, antennas, networking, and smart materials.
Airlines.
In 1939, at the urging of Jack Frye, president of Trans World Airlines (TWA), Hughes quietly purchased a majority share of TWA stock for nearly $7 million and took control of the airline. Upon assuming ownership, Hughes was prohibited by federal law from building his own aircraft. Seeking an aircraft that would perform better than TWA's fleet of Boeing 307 Stratoliners, Hughes and Frye approached Boeing's competitor, Lockheed. Hughes had a good relationship with Lockheed since they had built the aircraft he used in his record flight around the world in 1938. Lockheed agreed to Hughes and Frye's request that the new aircraft be built in secrecy. The result was the revolutionary Constellation and TWA purchased the first 40 of the new airliners off the production line.
In 1956, Hughes placed an order for 63 Convair 880s for TWA at a cost of $400 million. Although Hughes was extremely wealthy at this time, outside creditors demanded that Hughes relinquish control of TWA in return for providing the money. In 1960, Hughes was ultimately forced out of TWA, although he owned 78% of the company and battled to regain control.
Before Hughes' removal, the TWA jet financing issue precipitated the end of Hughes' relationship with Noah Dietrich. Dietrich claimed Hughes developed a plan by which Hughes Tool Company profits would be inflated to sell the company for a windfall that would pay the bills for the 880s. Dietrich agreed to go to Texas to implement the plan on the condition that Hughes agreed to a capital gains arrangement he had long promised Dietrich. When Hughes balked, Dietrich resigned immediately. "Noah", Dietrich quoted Hughes as replying, "I cannot exist without you!" Dietrich stood firm and eventually had to sue to retrieve personal possessions from his office after Hughes ordered it locked.
In 1966, a U.S. federal court forced Hughes to sell his TWA shares because of concerns over conflict of interest between his ownership of both TWA and Hughes Aircraft. The sale of his TWA shares netted him a profit of $547 million.
In 1970, Hughes went back into the airline business, buying San Francisco-based Air West and renaming it Hughes Airwest. Air West had been formed in 1968 by the merger of Bonanza Air Lines, Pacific Air Lines and West Coast Airlines, all of which operated in the western U.S. By the late 1970s, Hughes Airwest operated an all-jet fleet of Boeing 727-200, Douglas DC-9-10, and McDonnell Douglas DC-9-30 jetliners serving an extensive route network in the western U.S. with flights to Mexico and western Canada as well. By 1980, the airline's route system reached as far east as Houston Hobby Airport and Milwaukee with a total of forty-two (42) destinations being served. Hughes Airwest was then acquired by and merged into Republic Airlines (1979–1986) in late 1980. Republic was subsequently acquired by and merged into Northwest Airlines which in turn was eventually merged into Delta Air Lines.
Personal life.
In 1929 Hughes' wife, Ella, returned to Houston and filed for divorce. Hughes dated many famous women, many of them decades younger, including Billie Dove, Faith Domergue, Bette Davis, Ava Gardner, Olivia de Havilland, Katharine Hepburn, Ginger Rogers and Gene Tierney. He also proposed to Joan Fontaine several times, according to her autobiography "No Bed of Roses". Jean Harlow accompanied him to the premiere of "Hell's Angels", but Noah Dietrich wrote many years later that the relationship was strictly professional, as Hughes apparently personally disliked Harlow. In his 1971 book, "Howard: The Amazing Mr. Hughes", Dietrich said that Hughes genuinely liked and respected Jane Russell, but never sought romantic involvement with her. According to Russell's autobiography, however, Hughes once tried to bed her after a party. Russell (who was married at the time) refused him, and Hughes promised it would never happen again. The two maintained a professional and private friendship for many years. Hughes remained good friends with Tierney who, after his failed attempts to seduce her, was quoted as saying "I don't think Howard could love anything that did not have a motor in it." Later, when Tierney's daughter Daria was born deaf and blind and with a severe learning disability, because of Tierney's being exposed to rubella during her pregnancy, Hughes saw to it that Daria received the best medical care and paid all expenses.
In 1933, Hughes purchased unseen the "Rover", a luxury steam yacht previously owned by British shipping magnate Lord Inchcape. "I have never seen the "Rover" but bought it on the blue prints, photographs and the reports of Lloyd's surveyors. My experience is that the English are the most honest race in the world." Hughes renamed the yacht "Southern Cross" and later sold her to Swedish entrepreneur Axel Wenner-Gren.
On July 11, 1936, Hughes struck and killed a pedestrian named Gabriel S. Meyer with his car, at the corner of 3rd Street and Lorraine in Los Angeles. Hughes was certified as sober at the hospital he was taken to after the accident, but an attending doctor made a note that Hughes had been drinking. A witness to the accident told police that Hughes was driving erratically and too fast, and that Meyer had been standing in the safety zone of a streetcar stop. Hughes was booked on suspicion of negligent homicide and held overnight in jail until his attorney, Neil McCarthy, obtained a writ of habeas corpus for his release pending a coroner's inquest. By the time of the coroner's inquiry, however, the witness had changed his story and claimed that Meyer had moved directly in front of Hughes's car. Nancy Bayly (Watts), who was in the car with Hughes at the time of the accident, corroborates this version. On July 16, 1936, Hughes was held blameless by a coroner's jury at the inquest into Meyer's death. Hughes told reporters outside the inquiry, "I was driving slowly and a man stepped out of the darkness in front of me."
On January 12, 1957, Hughes married actress Jean Peters. The couple met in the 1940s, before Peters became a film actress. They had a highly publicized romance in 1947 and there was talk of marriage, but she said she could not combine it with her career. Some later claimed that Peters was "the only woman ever loved", and he reportedly had his security officers follow her everywhere even when they were not in a relationship. Such reports were confirmed by actor Max Showalter, who became a close friend of Peters while shooting "Niagara" (1953). Showalter told in an interview that because he frequently met with Peters, Hughes' men threatened to ruin his career if he did not leave her alone.
Howard Hughes Medical Institute.
In 1953, Hughes launched the Howard Hughes Medical Institute in Miami, Florida, and currently located in Chevy Chase, Maryland, formed with the express goal of basic biomedical research, including trying to understand, in Hughes' words, the "genesis of life itself", due to his lifelong interest in science and technology. Hughes' first will, which he signed in 1925 at the age of 19, stipulated that a portion of his estate should be used to create a medical institute bearing his name. When a major battle with the IRS loomed ahead, Hughes gave all his stock in the Hughes Aircraft Company to the institute, thereby turning the aerospace and defense contractor into a for-profit entity of a fully tax-exempt charity. Hughes' internist, Verne Mason, who treated Hughes after his 1946 aircraft crash, was chairman of the institute's medical advisory committee. The Howard Hughes Medical Institute's new board of trustees sold Hughes Aircraft in 1985 to General Motors for $5.2 billion, allowing the institute to grow dramatically.
The deal was the topic of a protracted legal battle between Hughes and the Internal Revenue Service, which Hughes ultimately won. After his death in 1976, many thought that the balance of Hughes' estate would go to the institute, although it was ultimately divided among his cousins and other heirs, given the lack of a will to the contrary. The HHMI was the 4th largest private organization as of 2007 and the largest devoted to biological and medical research, with an endowment of $16.3 billion as of June 2007.
Nixon scandal.
Shortly before the 1960 Presidential election, Richard Nixon was alarmed when it was revealed that his brother, Donald, received a $205,000 loan from Hughes. It has long been speculated that Nixon's drive to learn what the Democrats were planning in 1972 was based in part on his belief that the Democrats knew about a later bribe that his friend Bebe Rebozo had received from Hughes after Nixon took office.
In late 1971, Donald Nixon was collecting intelligence for his brother in preparation for the upcoming presidential election. One of Donald's sources was John H. Meier, a former business adviser of Hughes who had also worked with Democratic National Committee Chair Larry O'Brien.
Meier, in collaboration with former Vice President of the United States Hubert Humphrey and others, wanted to feed misinformation to the Nixon campaign. Meier told Donald that he was sure the Democrats would win the election because Larry O’Brien had a great deal of information on Richard Nixon’s illicit dealings with Howard Hughes that had never been released; O’Brien didn’t actually have any such information, but Meier wanted Nixon to think he did. Donald told his brother that O’Brien was in possession of damaging Hughes information that could destroy his campaign. Terry Lenzner, who was the chief investigator for the Senate Watergate Committee, speculates that it was Nixon's desire to know what O'Brien knew about Nixon's dealings with Hughes that may have partially motivated the Watergate break-in.
Glomar Explorer.
In 1972, Hughes was approached by the CIA to help secretly recover Soviet submarine K-129, which had sunk near Hawaii four years earlier. Thus, the special-purpose salvage vessel "Glomar Explorer" was born. Hughes' involvement provided the CIA with a plausible cover story, having to do with civilian marine research at extreme depths and the mining of undersea manganese nodules. In the summer of 1974, "Glomar Explorer" attempted to raise the Soviet vessel.
However, during the recovery a mechanical failure in the ship's grapple caused half of the submarine to break off and fall to the ocean floor. This section is believed to have held many of the most sought-after items, including its code book and nuclear missiles. Two nuclear-tipped torpedoes and some cryptographic machines were recovered, along with the bodies of six Soviet submariners who were subsequently given formal burial at sea in a filmed ceremony. The operation, known as Project Azorian (but incorrectly referred to by the press as Project Jennifer), became public in February 1975 when burglars obtained secret documents from Hughes' headquarters in June 1974. Though he lent his name to the operation, Hughes and his companies had no actual involvement in the project. The "Glomar Explorer" was eventually acquired by Transocean Inc., an offshore oil and gas drilling rig company.
Obsessive–compulsive disorder and physical decline.
As early as the 1930s, Hughes displayed signs of mental illness, primarily obsessive-compulsive disorder. Close friends reported that he was obsessed with the size of peas, one of his favorite foods, and used a special fork to sort them by size.
While directing "The Outlaw", Hughes became fixated on a small flaw in one of Jane Russell's blouses, claiming that the fabric bunched up along a seam and gave the appearance of two nipples on each breast. He reportedly wrote a detailed memorandum to the crew on how to fix the problem. Richard Fleischer, who directed "His Kind of Woman" with Hughes as executive producer, wrote at length in his autobiography about the difficulty of dealing with the tycoon. In his book, "Just Tell Me When to Cry", Fleischer explained that Hughes was fixated on trivial details and was alternately indecisive and obstinate. He also revealed that Hughes's unpredictable mood swings made him wonder if the film would ever be completed.
In 1947, after the U.S. Government rejected his massive H-4 Hercules, Hughes, who had suffered a near-fatal aircraft crash in 1946, told his aides that he wanted to screen some movies at a film studio near his home. He stayed in the studio's darkened screening room for more than four months, never leaving. He ate only chocolate bars and chicken and drank only milk, later urinating in the empty bottles and containers. He was surrounded by dozens of Kleenex boxes that he continuously stacked and re-arranged. He wrote detailed memos to his aides giving them explicit instructions not to look at him nor speak to him unless spoken to. Throughout this period, Hughes sat fixated in his chair, often naked, continually watching movies. When he finally emerged in the spring of 1948, his hygiene was terrible. He had not bathed nor cut his hair and nails for weeks, although this may have been due to allodynia (pain response to stimuli that would normally not cause pain).
After the screening room incident, Hughes moved into a bungalow at the Beverly Hills Hotel where he also rented rooms for his aides, his wife, and numerous girlfriends. He would sit naked in his bedroom with a pink hotel napkin placed over his genitals, watching movies. This may have been because Hughes found the touch of clothing painful due to his allodynia. He may have watched movies to distract him from his pain—a common practice among patients with intractable pain, especially those who do not receive adequate treatment. In one year, Hughes spent an estimated $11 million at the hotel.
Hughes began purchasing all restaurant chains and four star hotels that had been founded within the state of Texas. This included, if for only a short period, many unknown franchises currently out of business. He placed ownership of the restaurants with the Howard Hughes Medical Institute, and all licenses were resold shortly after.
Another time, he became obsessed with the 1968 film "Ice Station Zebra", and had it run on a continuous loop in his home. According to his aides, he watched it 150 times.
Hughes insisted on using tissues to pick up objects to insulate himself from germs. He would also notice dust, stains or other imperfections on people's clothes and demand that they take care of them. Once one of the most visible men in America, Hughes ultimately vanished from public view—though tabloids continued to follow rumors of his behavior and whereabouts. He was reported to be terminally ill, mentally unstable, or even dead.
Injuries from numerous aircraft crashes caused Hughes to spend much of his later life in pain, and he eventually became addicted to codeine, which he injected intramuscularly. Hughes had his hair cut and nails trimmed only once a year, likely due to the pain caused by the RSD/CRPS, which was caused by the plane crashes.
Las Vegas baron and recluse.
The wealthy and aging Howard Hughes, accompanied by his entourage of personal aides, began moving from one hotel to another, always taking up residence in the top floor penthouse. In the last ten years of his life, 1966 to 1976, Hughes lived in hotels in many cities—including Beverly Hills, Boston, Las Vegas, Nassau, Freeport, Vancouver, London, Managua, and Acapulco.
On November 24, 1966 (Thanksgiving Day), Hughes arrived in Las Vegas by railroad car and moved into the Desert Inn. Because he refused to leave the hotel, and to avoid further conflicts with the owners, Hughes bought the Desert Inn in early 1967. The hotel's eighth floor became the nerve center of Hughes' empire and the ninth-floor penthouse became his personal residence. Between 1966 and 1968, he bought several other hotel-casinos—including the Castaways, New Frontier, the Landmark Hotel and Casino, and the Sands. He bought the small Silver Slipper casino just so he could have its trademark neon silver slipper moved. Visible from Hughes' bedroom, it apparently had kept him up at night.
After Hughes left the Desert Inn, hotel employees discovered his drapes had not been opened in the nine years he lived there, and had rotted through. An unusual incident marked an earlier Hughes connection to Las Vegas: during his 1954 engagement at the Last Frontier hotel in Las Vegas, flamboyant entertainer Liberace mistook Howard Hughes for his lighting director, instructing him to instantly bring up a blue light should he start to play "Clair de lune". Hughes nodded in compliance—but the hotel's entertainment director arrived and introduced Hughes to Liberace.
Hughes wanted to change the image of Las Vegas to something more glamorous. As Hughes wrote in a memo to an aide, "I like to think of Las Vegas in terms of a well-dressed man in a dinner jacket and a beautifully jeweled and furred female getting out of an expensive car." Hughes bought several local television stations (including KLAS-TV).
Hughes' considerable business holdings were overseen by a small panel unofficially dubbed "The Mormon Mafia" because of the many Latter-day Saints on the committee, led by Frank William Gay. In addition to supervising day-to-day business operations and Hughes' health, they also went to great pains to satisfy Hughes' every whim. Hughes once became fond of Baskin-Robbins' banana nut ice cream, so his aides sought to secure a bulk shipment for him—only to discover that Baskin-Robbins had discontinued the flavor. They put in a request for the smallest amount the company could provide for a special order, 200 gallons (750 L), and had it shipped from Los Angeles. A few days after the order arrived, Hughes announced he was tired of banana nut and wanted only chocolate marshmallow ice cream. The Desert Inn ended up distributing free banana nut ice cream to casino customers for a year. In a 1996 interview, ex–Howard Hughes communicator Robert Maheu said, "There is a rumor that there is still some banana nut ice cream left in the freezer. It is most likely true."
As an owner of several major Las Vegas businesses, Hughes wielded much political and economic influence in Nevada and elsewhere. During the 1960s and early 1970s, he disapproved of underground nuclear testing at the Nevada Test Site. Hughes was concerned about the risk from residual nuclear radiation, and attempted to halt the tests. When the tests finally went through despite Hughes' efforts, the detonations were powerful enough that the entire hotel where he was staying trembled with the shock waves. In two separate, last-ditch maneuvers, Hughes instructed his representatives to offer million-dollar bribes to both presidents Lyndon B. Johnson and Richard Nixon.
In 1970, Jean Peters filed for divorce. The two had not lived together for many years. Peters requested a lifetime alimony payment of $70,000 a year, adjusted for inflation, and waived all claims to Hughes' estate. Hughes offered her a settlement of over a million dollars, but she declined it. Hughes did not insist on a confidentiality agreement from Peters as a condition of the divorce. Aides reported that Hughes never spoke ill of her. She refused to discuss her life with Hughes and declined several lucrative offers from publishers and biographers. Peters would state only that she had not seen Hughes for several years before their divorce and had only dealt with him by phone.
Hughes was living in the Intercontinental Hotel near Lake Managua in Nicaragua, seeking privacy and security, when a magnitude 6.5 earthquake damaged Managua in December 1972. As a precaution, Hughes moved to the Nicaraguan National Palace and stayed there as a guest of Anastasio Somoza Debayle before leaving for Florida on a private jet the following day. He subsequently moved into the Penthouse at the Xanadu Princess Resort on Grand Bahama Island, which he had recently purchased. He lived almost exclusively in the penthouse of the Xanadu Beach Resort & Marina for the last four years of his life. Hughes had spent a total of $300 million on his many properties in Las Vegas.
Memoir hoax.
In 1972, author Clifford Irving caused a media sensation when he claimed he had co-written an authorized autobiography of Hughes. Hughes was so reclusive that he did not immediately publicly refute Irving's statement, leading many to believe the Irving book was genuine. However, before the book's publication Hughes finally denounced Irving in a teleconference and the entire project was eventually exposed as a hoax. Irving was later convicted of fraud and spent 17 months in prison. In 1974, the Orson Welles film "F for Fake" included a section on the Hughes biography hoax. In 1977, "The Hoax" by Clifford Irving was published in the United Kingdom, telling his story of these events. The 2006 film "The Hoax", starring Richard Gere, is also based on these events.
Death.
Hughes was reported to have died on April 5, 1976, at 1:27 p.m. on board an aircraft owned by Robert Graf and piloted by Jeff Abrams. He was en route from his penthouse at the Acapulco Fairmont Princess Hotel in Mexico to the Methodist Hospital in Houston, Texas. Other accounts indicate that he died in the flight from Freeport, Grand Bahama, to Houston.
After receiving a call, his senior counsel, Frank P. Morse, ordered his staff to get his body on a plane and return him to the United States. It was common that foreign countries would hold a corpse as ransom so that an estate could not be settled. Morse ordered the pilots to announce Hughes' death once they entered U.S. airspace.
His reclusive activities (and possibly his drug use) made him practically unrecognizable. His hair, beard, fingernails, and toenails were long—his tall frame now weighed barely , and the FBI had to use fingerprints to conclusively identify the body. Howard Hughes' alias, John T. Conover, was used when his body arrived at a morgue in Houston on the day of his death. There, his body was received by Dr. Jack Titus.
A subsequent autopsy recorded kidney failure as the cause of death. Hughes was in extremely poor physical condition at the time of his death. He suffered from malnutrition. While his kidneys were damaged, his other internal organs, including his brain, were deemed perfectly healthy. X-rays revealed five broken-off hypodermic needles in the flesh of his arms. To inject codeine into his muscles, Hughes had used glass syringes with metal needles that easily became detached.
Hughes is buried in the Glenwood Cemetery in Houston, Texas, next to his parents.
The red brick house where Hughes lived as a teenager at 3921 Yoakum St., Houston today serves as the headquarters of the Theology Department of the University of St. Thomas.
Estate.
Approximately three weeks after Hughes' death, a handwritten will was found on the desk of an official of The Church of Jesus Christ of Latter-day Saints in Salt Lake City. The so-called "Mormon Will" gave $1.56 billion to various charitable organizations (including $625 million to the Howard Hughes Medical Institute); nearly $470 million to the upper management in Hughes' companies and to his aides; $156 million to first cousin William Lummis; and $156 million split equally between his two ex-wives Ella Rice and Jean Peters.
Hughes left his entire estate, in his last will, and according to his senior counsel (Frank P. Morse) to the Hughes Medical Institute as he had no connection to family and was seriously ill. This is contrary to the many wills that have surfaced after his death. The original will, that included payments to aides, never surfaced and was apparently in a home surrounding the Desert Inn Golf Course, belonging to the mother of an assistant. He had no desire to leave any money to family, aides, churches, including William Gay and Frank Morse. Hughes was not Mormon and had no reason to leave his estate to that church. Frank P. Morse is still the attorney of record for Hughes. Gay has devoted his life to the Mormon Church.
A further $156 million was endowed to a gas-station owner named Melvin Dummar, who told reporters that late one evening in December 1967, he found a disheveled and dirty man lying along U.S. Highway 95, north of Las Vegas. The man asked for a ride to Las Vegas. Dropping him off at the Sands Hotel, Dummar said the man told him he was Hughes. Dummar then claimed that days after Hughes' death, a "mysterious man" appeared at his gas station, leaving an envelope containing the will on his desk. Unsure if the will was genuine, and unsure of what to do, Dummar left the will at the LDS Church office. In June 1978, after a seven-month trial, a Nevada court rejected the Mormon Will as a forgery, and declared that Hughes had died intestate. Jonathan Demme's 1980 film "Melvin and Howard", written by Bo Goldman and starring Jason Robards and Paul Le Mat, was based on Dummar's story.
Hughes' $2.5 billion estate was eventually split in 1983 among 22 cousins, including William Lummis, who serves as a trustee of the Howard Hughes Medical Institute.
The U.S. Supreme Court ruled that Hughes Aircraft was owned by the Howard Hughes Medical Institute, which sold it to General Motors in 1985 for $5.2 billion. The court rejected suits by the states of California and Texas claiming they were owed inheritance tax. In 1984, Hughes' estate paid an undisclosed amount to Terry Moore, who claimed she and Hughes had secretly married on a yacht in international waters off Mexico in 1949 and never divorced. Moore never produced proof of a marriage, but her book, "The Beauty and the Billionaire", became a bestseller.
Popular culture.
Howard Hughes has now emerged as one of the 20th century's most iconic business and aviation figures spawning a wide range of cultural references.

</doc>
<doc id="14062" url="https://en.wikipedia.org/wiki?curid=14062" title="Hook of Holland">
Hook of Holland

The Hook of Holland () is a town in the southwestern corner of Holland, at the mouth of the New Waterway shipping canal into the North Sea. The town is administered by the municipality of Rotterdam as a district of that city. Its district covers an area of 16.7 km2 (of which 13.92 km2 is land). On 1 January 1999 it had an estimated population of 9,400.
Towns near the Hook include Monster, 's-Gravenzande, Naaldwijk and Delft to the northeast, and Maassluis to the southeast. On the other side of the river is the Europort and the Maasvlakte. The wide sandy beach, one section of which is designated for use by naturists, runs for approximately 18 kilometres to Scheveningen and for most of this distance is backed by extensive sand dunes through which there are foot and cycle paths.
On the north side of the New Waterway, to the west of the town, is a pier part of which is accessible to pedestrians and cyclists.
The Berghaven is a small harbour on the New Waterway where the Rotterdam and Europort pilots are based. This small harbour is only for the use of the pilot service, government vessels and the Hook of Holland lifeboat.
During World War II this was one of the most important places for the Germans to hold because of the harbour.
Transport links.
Railways.
There are two railway stations, Hook of Holland Strand, which is at the end of the line and closest to the beach and Hook of Holland Haven, which is close to the town centre, adjacent to the ferry terminal and the small harbour, the Berghaven. The railway line connects the Hook to Rotterdam via Maassluis, Vlaardingen and Schiedam, the trains running every half hour during the day.
Ferry.
The ferry terminal is operated by Stena Line with the passenger terminal and access for passenger cars being next to Hook of Holland Holland Haven Station and the freight entrance being to the east of the town.
A ferry service to eastern England has operated from the Hook since 1893 with only the two World Wars interrupting the service. Currently two routes are being operated: one, a day and night freight and passenger service, to Harwich, Essex and the other, a night, freight-only service to North Killingholme Haven, Lincolnshire. From 1 March 2009 the Harwich service will be departing at 2:30 pm and 10:45 pm and the Killingholme service at 9:15 pm.
A local ferry operated by RET links the Hook with the Maasvlakte part of the Port of Rotterdam.
Motorways.
In just 10 kilometres one has access to the A20 heading east towards Rotterdam and Utrecht. At 17 kilometres, one could access the A4 heading north towards the Hague and Amsterdam.

</doc>
<doc id="14063" url="https://en.wikipedia.org/wiki?curid=14063" title="Hugh Binning">
Hugh Binning

Hugh Binning (1627–1653) was a Scottish philosopher and theologian. Binning was born in Scotland during the reign of Charles I, ordained in the (Presbyterian) Church of Scotland and died during the time of Oliver Cromwell and the Commonwealth of England.
A precocious child, Binning at age 13 was admitted to the study of philosophy at the University of Glasgow. By the age of 19, he was appointed regent and professor of philosophy at the University of Glasgow. Three years later, he was called to be minister and presided at a church in Govan, adjacent to the city of Glasgow; a post he held until his untimely death of consumption at the age of 26. He was a follower of James Dalrymple. In later life he was well known as an evangelical Christian.
Impact of the Commonwealth.
Hugh Binning was born two years after Charles I ascended to the thrones of England, Ireland, and Scotland. At the time, each was an independent country sharing the same monarch. The Acts of Union 1707 integrated Scotland and England to form the Kingdom of Great Britain; the Acts of Union 1800 integrated Ireland to form United Kingdom of Great Britain and Ireland.
The period was dominated by both political and religious strife between the three independent countries. The religious dispute centered on whether religion was to be dictated by the monarch or was to be the choice of the people; whether people have a direct relationship with God or they needed to use an intermediary. The civil disputes centered on the extent of the king's power, a question of the Divine right of kings; specifically whether the King has right to raise taxes and armed forces without the Consent of the governed. These wars ultimately changed the relationship between king and subjects.
In 1638 the General Assembly of the Church of Scotland voted to remove bishops and the Book of Common Prayer that had been introduced by Charles I to impose the Anglican model on the Presbyterian Church of Scotland. Public riots occurred. The result was the Wars of the Three Kingdoms, an interrelated series of conflicts that took place in the three countries sharing the same monarch. The first of the conflicts was in 1639, the First of the Bishops' Wars, a single border skirmish between England and Scotland; also known as "the war the armys did not wanted to fight."
To maintain his English power base Charles I made secret alliances with Catholic Ireland and Presbyterian Scotland to invade Anglican England, promising that each country could establish their own separate state religion. Once these secret entreaties became known to the English Long Parliament, the Congregationalist faction (of which Oliver Cromwell was a primary spokesman) took matters into their own hands and Parliament established an army separate from the King. Then, Charles I was executed in January 1649, which led to the rule of Cromwell and the establishment of the Commonwealth. The conflicts concluded with The English Restoration of the monarchy with the return of Charles II, in 1660.
The Act of Classes was passed by the Parliament of Scotland on 23 January 1649; the act banned Royalists (people supporting the monarchy) from holding political or military office. In exile, Charles II signed the Treaty of Breda (1650) with the Scottish Parliament; among other things, the treaty established Presbyterianism as the national religion. Charles was crowned King of Scots at Scone in January 1651. By September 1651 Scotland was annexed by England, its legislative institutions abolished, Presbyterianism dis-established, and Charles was forced into exile in France.
The Scottish Parliament rescinded the Act of Classes in 1651, which produced a split within Scottish Society. The sides of the conflict were called the Resolutioners (who supported the rescission of the act – supported the monarchy and the Scottish House of Stewart) and the Protesters (who supported Cromwell and the Commonwealth); Binning sided with the Resolutioners.
When Cromwell sent troops to Scotland, he was also attempting to dis-establish Presbyterianism and the Church of Scotland, Binning spoke against Cromwell's act. On Saturday 19 April 1651, Cromwell entered Glasgow and the next day he heard a sermon by three ministers who condemned Cromwell for invading Scotland. That evening, Cromwell summoned those ministers and others, to a debate on the issue. At the debate, Rev Hugh Binning is said to have out-debated Cromwell’s ministers so completely that he silenced Cromwell’s ministers.
Politics.
Hugh Binning political views were based on his theology. Binning was a Covenanter, a movement that began in Scotland at Greyfriars Kirkyard in 1638 with the National Covenant and continued with the 1643 Solemn League and Covenant – in effect a treaty between the English Long Parliament and Scotland for the preservation of the reformed religion in exchange for troops to confront the threat of Irish Catholic troops joining the Royalist army. Binning could also be described as a Resolutioners; both political positions were taken because of their religious implications. However, he saw the evils of the politics of his day was not a “fomenter of factions” writing “A Treatise of Christian Love” as a response.
Theology.
Because of the turmoil time in which Hugh Binning lived, politics and religion were inexorably intertwined. Binning was a Calvinist and follower of John Knox. As a profession, Binning was trained as a Philosopher, and he believed that philosophy was the servant of theology. He thought that both Philosophy and Theology should be taught in parallel. Binning’s writing, which are primarily a collection of his sermons, “forms an important bridge between the 17th century, when philosophy in Scotland was heavily dominated by Calvinism, and the 18th century when figures such as Francis Hutcheson re-asserted a greater degree of independence between the two and allied philosophy with the developing human sciences.”
Religiously, Hugh Binning was, what we would call today, an Evangelical Calvinist. He spoke on the primacy of God’s love as the ground of salvation: 
With regards to the extent of the ‘atonement’, Hugh Binning, like many Scottish theologians of his day, was not a ‘hyper-Calvinist,’ since it was not until the Synod of Dort in 1619 that the Reformed tradition come to accept limited atonement, one of the primary tenants of the Five Points of Calvinism. Binning did not hold that the offer of redemption applied only to the few that are elect but said that “the ultimate ground of faith is in the electing will of God.” In Scotland during the 1600s the questions concerning atonement revolved around the terms in which the offer was experssed.
Binning believed that "forgiveness is based on Christ's death, understood as a satisfaction and as a sacrifice: 'If he had pardoned sin without any satisfaction what rich grace it had
been! But truly, to provide the Lamb and sacrifice himself, to find out
the ransom, and to exact it of his own Son, in our name, is a testimony
of mercy and grace far beyond that. But then, his justice is very conspicuous
in this work.'"
Works.
All of the works of Hugh Binning were published posthumously and were primarily collections of his sermons. Of his speaking style, it was said: "There is originality without any affectation, a rich imagination, without anything fanciful or extravert, the utmost simplicity, without an thing mean or trifling." 
Personal life.
Hugh Binning was the son of John Binning and Margaret M'Kell. Margaret was the daughter of Rev. Matthew M'Kell,
who was a minister in the parish of Bothwell, Scotland, and sister of Hugh M'Kell, a minister in Edinburgh.
Hugh Binning was born on the estate of his father in Dalvennan, Straiton, in the shire of Ayr. The family owned other land in the parishes of Straiton and Colmonell as well as Maybole in Carrick. 
In 1645, James Dalrymple, 1st Viscount of Stair, who was Hugh’s master (primary professor) in the study of philosophy, announced he was retiring from the University of Glasgow. After a national search for a replacement on the faculty, three men were selected to compete for the position. Hugh was one of those selected, but was at a disadvantage because of his extreme youth and because he was not of noble birth. However, he had strong support from the existing faculty, who suggested that the candidates speak extemporaneously on any topic of the candidate’s choice. After hearing Hugh speak, the other candidates withdrew, making Hugh a regent and professor of philosophy, while he was still 18 years old.
On 7 February 1648 (at the age of 21) Hugh was appointed an Advocate before the Court of Sessions (an attorney). In the same year he married Barbara Simpson (sometimes called Mary), daughter of Rev. James Simpson a minister in Ireland. Their son, John, was born in 1650.
Hugh died around September 1653 and was buried in the churchyard of Govan, where Patrick Gillespie, then principal of the University of Glasgow, ordered a monument inscribed in Latin, roughly translated: 
Hugh’s widow, Barbara (sometimes called Mary), then remarried James Gordon, an Anglican priest at Cumber in Ireland. Together they had a daughter, Jean who married Daniel MacKenzie, who was on the winning side of the Battle of Bothwell Bridge serving as an ensign under Lieutenant-Colonel William Ramsay (who became the third Earl of Dalhousie), in the Earl of Mar’s Regiment of Foot.
Hugh’s son, John Binning, married Hanna Keir, who was born in Ireland. The Binning’s were Covenanters, a resistance movement that objected to the return of Charles II (who was received into the Catholic Church on his deathbed). They were on the losing side in the 1679 Battle of Bothwell Bridge. Most of the rebels who were not executed were exiled to the Americas; about 30 Covenanters were exiled to the Carolinas on the Carolina Merchant in 1684. After the battle, John and Hanna were separated. 
In the aftermath of the battle at Bothwell Bridge, Hugh’s widow (now Barbara Gordon) tried to reclaim the family estate at Dalvennan by saying that John and his wife owed his step father a considerable some of money. The legal action was successful and Dalvennan became the possession of John’s half sister Jean, and her husband Daniel MacKenzie. In addition, Jean came into possession Hanna Keir's property in Ireland.
By 1683, Jean was widowed. John Binning was branded a traitor, was sentenced to death and forfeited his property to the Crown. John’s wife (Hanna Keir) was branded as a traitor and forfeited her property in Ireland. In 1685 Jean "donated" the Binning family's home at Dalvennan and other properties, along with the Keir properties to Roderick MacKenzie, who was a Scottish advocate of James II (James VII of Scotland), and the baillie of Carrick. According to an act of the Scottish Parliament, Roderick MacKenzie was also very effective in “suppressing the rebellious, fanatical party in the western and other shires of this realm, and putting the laws to vigorous execution against them”
Since Bothwell Bridge, Hanna had been hiding from the authorities. In 1685 Hanna was in Edinburgh where she was found during a sweep for subversives and imprisoned in the Tolbooth of Edinburgh, a combination city hall and prison. Those arrested with Hanna were exiled to North America, however she developed Dysentery and remained behind. By 1687, near death, Hanna petitioned the Privy Council of Scotland for her release; she was exiled to her family in Ireland, where she died around 1692. 
In 1690 the Scottish Parliament rescinded John's fines and forfeiture, but he was not able to recover his family’s estates, the courts suggesting that John had relinquished his claim to Dalvennan in exchange for forgiveness of debt, rather than forfeiture.
There is little documentation about John after his wife's death. John received a small income from royalties on his father Hugh’s works after parliament extended copyrights on Hugh’s writings to him. However, the income was not significant and John made several petitions to the Scottish parliament for money, the last occurring in 1717. It is thought that John died in Somerset county, in southwestern England. 

</doc>
<doc id="14064" url="https://en.wikipedia.org/wiki?curid=14064" title="Henry Home, Lord Kames">
Henry Home, Lord Kames

Henry Home, Lord Kames (169627 December 1782) was a Scottish advocate, judge, philosopher, writer and agricultural improver. A central figure of the Scottish Enlightenment, a founder member of the Philosophical Society of Edinburgh, and active in the Select Society, his protégés included David Hume, Adam Smith, and James Boswell.
Biography.
Born at Kames House, between Eccles and Birgham, Berwickshire, he was educated at home by a private tutor. He studied law at Edinburgh, was called to the bar in 1724, and became an advocate. He soon acquired reputation by a number of publications on the civil and Scottish law, and was one of the leaders of the Scottish Enlightenment. In 1752, he was "raised to the bench", thus acquiring the title of Lord Kames.
Home was on the panel of judges in the Joseph Knight case which ruled that there could be no slavery in Scotland.
His address in 1775 is shown as New Street on the Canongate.
Writings.
Home wrote much about the importance of property to society. In his "Essay Upon Several Subjects Concerning British Antiquities", written just after the Jacobite rising of 1745 he described how the politics of Scotland were not based on loyalty to Kings or Queens as Jacobites had said but on royal land grants given in return for loyalty.
In "Historical Law Tracts" and later in "Sketches on the History of Man" he described human history as having four distinct stages. The first was as a hunter-gatherer where people avoided each other out of competition. The second stage he described was a herder of domestic animals which required forming larger societies. No laws were needed at these stages except those given by the head of the family or society. Agriculture was the third stage requiring greater cooperation and new relationships to allow for trade or employment (or slavery). He argued that 'the intimate union among a multitude of individuals, occasioned by agriculture' required a new set of rights and obligations in society. This requires laws and law enforcers. A fourth stage moves from villages and farms to seaports and market towns requiring yet more laws and complexity but also much to benefit from. Kames could see these stages within Scotland itself, with the pastoral/agricultural highlands, the agricultural/industrial lowlands and the growing commercial ("polite") towns of Glasgow and Edinburgh.
Home was a polygenist, he believed God had created different races on earth in separate regions. In his book "Sketches on the History of Man" in 1734 Home claimed that the environment, climate, or state of society could not account for racial differences, so that the races must have come from distinct, separate stocks.
The above studies created the genre of the story of civilization and defined the fields of anthropology and sociology and therefore the modern study of history for two hundred years.
In the popular book "Elements of Criticism" (1762) Home interrogated the notion of fixed or arbitrary rules of literary composition, and endeavoured to establish a new theory based on the principles of human nature. The late eighteenth-century tradition of sentimental writing was associated with his notion that 'the genuine rules of criticism are all of them derived from the human heart. Prof Neil Rhodes has argued that Lord Kames played a significant role in the development of English as an academic discipline in the Scottish Universities.
Social milieu.
He enjoyed intelligent conversation and cultivated a large number of intellectual associates, among them John Home, David Hume and James Boswell.[http://www.jamesboswell.info/People/people.php?person=57]. Lord Monboddo was also a frequent debater of Kames, although these two usually had a fiercely competitive and adversarial relationship.

</doc>
<doc id="14065" url="https://en.wikipedia.org/wiki?curid=14065" title="Harwich">
Harwich

Harwich is a town in Essex, England and one of the Haven ports, located on the coast with the North Sea to the east. It is in the Tendring district. Nearby places include Felixstowe to the northeast, Ipswich to the northwest, Colchester to the southwest and Clacton-on-Sea to the south. It is the northernmost coastal town within Essex.
Its position on the estuaries of the Stour and Orwell rivers and its usefulness to mariners as the only safe anchorage between the Thames and the Humber led to a long period of maritime significance, both civil and military. The town became a naval base in 1657 and was heavily fortified, with Harwich Redoubt, Beacon Hill Battery, and Bath Side Battery.
Harwich today is contiguous with Dovercourt and the two, along with Parkeston, are often referred to collectively as Harwich.
History.
The town's name means "military settlement," from Old English "here-wic".
The town received its charter in 1238, although there is evidence of earlier settlement – for example, a record of a chapel in 1177, and some indications of a possible Roman presence.
Because of its strategic position, Harwich was the target for the invasion of Britain by William of Orange on 11 November 1688. However, unfavourable winds forced his fleet to sail into the English Channel instead and eventually land at Torbay. Due to the involvement of the Schomberg family in the invasion, Charles Louis Schomberg was made Marquess of Harwich.
Writer Daniel Defoe devotes a few pages to the town in "A tour thro' the Whole Island of Great Britain". Visiting in 1722, he noted its formidable fort and harbour "of a vast extent". The town, he recounts, was also known for an unusual spring rising on Beacon Hill (a promontory to the north-east of the town), which "petrified" clay, allowing it to be used to pave Harwich's streets and build its walls. The locals also claimed that "the same spring is said to turn wood into iron", but Defoe put this down to the presence of "copperas" in the water. Regarding the atmosphere of the town, he states: "Harwich is a town of hurry and business, not much of gaiety and pleasure; yet the inhabitants seem warm in their nests and some of them are very wealthy".
Royal Naval Dockyard.
A Royal Navy Dockyard was established at Harwich in 1652. It was ideally positioned for readying the fleet in the Anglo-Dutch Wars of the seventeenth century. Thereafter its importance waned; it ceased to operate as a Royal Dockyard in 1713, but was leased to a succession of private operators under whom naval and commercial shipbuilding continued. The Navy maintained a small storage and refitting base there until 1829. One unusual structure surviving from the dockyard is a very rare treadwheel crane of 1667, which was in use until the early twentieth century before being re-sited on Harwich Green in the 1930s. The dockyard bell, dating from 1666, is preserved on the original site, which still operates as a commercial port (known as Navyard since 1964). A wooden board on the dockyard gate lists some 58 Men-of-war built at the Old Naval Yard there from 1660-1827. During the First World War a flotilla, the Harwich Force, was based at the port. During the Second World War parts of Harwich were again requisitioned for naval use, and ships were based at HMS Badger, a shore establishment on the site of what is now Harwich International Port. Badger was decommissioned in 1946, but the Royal Naval Auxiliary Service maintained a headquarters on the site until 1992.
Port.
The Royal Navy is no longer present in Harwich but Harwich International Port at nearby Parkeston continues to offer regular ferry services to the Hook of Holland (Hoek van Holland) in the Netherlands. Many operations of the large container port at Felixstowe and of Trinity House, the lighthouse authority, are managed from Harwich.
The port is famous for the phrase "Harwich for the Continent", seen on road signs and in London & North Eastern Railway (LNER) advertisements.
Lighthouses.
At least three pairs of lighthouses have been built over recent centuries as leading lights, to help guide vessels into Harwich. The earliest pair were wooden structures: the High Light stood on top of the old Town Gate, whilst the Low Light (featured in a painting by Constable) stood on the foreshore. Both were coal-fired.
In 1818 these were replaced by stone structures, designed by John Rennie Senior, which can still be seen today (they no longer function as lighthouses: one houses the town's maritime museum, the other is (in 2015) also being converted into a museum). They were owned by General Rebow of Wivenhoe Park, who was able to charge 1d per ton on all cargo entering the port, for upkeep of the lights. In 1836 Rebow's lease on the lights was purchased by Trinity House, but in 1863 they were declared redundant due to a change the position of the channel used by ships entering and leaving the port, caused by shifting sands.
They were in turn replaced by the pair of cast iron lights at nearby Dovercourt; these too remain in situ, but were decommissioned (again due to shifting of the channel) in 1917.
Architecture.
Despite, or perhaps because of, its small size Harwich is highly regarded in terms of architectural heritage, and the whole of the older part of the town, excluding Navyard Wharf, is a conservation area.
The regular street plan with principal thoroughfares connected by numerous small alleys indicates the town’s medieval origins, although many buildings of this period are hidden behind 18th century facades.
The extant medieval structures are largely private homes. The house featured in the image of Kings Head St to the left is unique in the town and is an example of a sailmaker's house, thought to have been built circa 1600. Notable public buildings include the parish church of St. Nicholas (1821) in a restrained Gothic style, with many original furnishings, including a somewhat altered organ in the west end gallery. There is also the Guildhall of 1769, the only Grade I listed building in Harwich.
The Pier Hotel of 1860 and the building that was the Great Eastern Hotel of 1864 can both been seen on the quayside, both reflecting the town's new importance to travellers following the arrival of the railway line from Colchester in 1854. In 1923, The Great Eastern Hotel was closed by the newly formed LNER, as the Great Eastern Railway had opened a new hotel with the same name at the new passenger port at Parkeston Quay, causing a decline in numbers.
The hotel became the Harwich Town Hall, which included the Magistrates Court and, following changes in local government, was sold and divided into apartments.
Also of interest are the High Lighthouse (1818), the unusual Treadwheel Crane (late 17th century), the Old Custom Houses on West Street, a number of Victorian shopfronts and the Electric Palace Cinema (1911), one of the oldest purpose-built cinemas to survive complete with its ornamental frontage and original projection room still intact and operational.
There is little notable building from the later parts of the 20th century, but major recent additions include the lifeboat station and two new structures for Trinity House. The Trinity House office building, next door to the Old Custom Houses, was completed in 2005. All three additions are influenced by the high-tech style.
Notable inhabitants.
Harwich was the home town of Christopher Jones, the master and quarter-owner of the Mayflower. The famous diarist Samuel Pepys was the Member of Parliament for Harwich. Christopher Newport, captain of the expedition that founded Jamestown, Virginia, also hailed from Harwich. Captain Charles Fryatt lived in Harwich, and his body was brought back from Belgium in 1919 and he was buried at Dovercourt. More recently, the reclusive but well-known award-winning Western Australian writer Randolph Stow made his home in Harwich, in the knowledge that his ancestors lived in the area. Stow's 1984 novel "The Suburbs of Hell" draws on Old Harwich for its setting. Stow was a notable and determined supporter of the project to conserve the historic character of the town.
Harwich has also historically hosted a number of notable visitors, including many members of British and European royalty; in many instances these visits are linked with Harwich's rich maritime past.
Sport.
Harwich is home to Harwich & Parkeston F.C.; Harwich and Dovercourt RFC; Harwich & Dovercourt Sailing Club; Harwich, Dovercourt & Parkeston Swimming Club; Harwich & Dovercourt Rugby Union Football Club; Harwich & Dovercourt Cricket Club; and Harwich Runners who with support from Harwich Swimming Club host the annual Harwich Triathlons.

</doc>
<doc id="14067" url="https://en.wikipedia.org/wiki?curid=14067" title="Hendrick Avercamp">
Hendrick Avercamp

Hendrick Avercamp (January 27, 1585 (bapt.) – May 15, 1634 (buried)) was a Dutch painter. Avercamp was born in Amsterdam, where he studied with the Danish-born portrait painter Pieter Isaacks (1569–1625), and perhaps also with David Vinckboons. In 1608 he moved from Amsterdam to Kampen in the province of Overijssel. Avercamp was mute and was known as "de Stomme van Kampen" (the mute of Kampen).
As one of the first landscape painters of the 17th-century Dutch school, he specialized in painting the Netherlands in winter. Avercamp's paintings are colorful and lively, with carefully crafted images of the people in the landscape. Many of Avercamp's paintings feature people ice skating on frozen lakes.
Avercamp's work enjoyed great popularity and he sold his drawings, many of which were tinted with water-color, as finished pictures to be pasted into the albums of collectors. The Royal Collection has an outstanding collection of his works.
Avercamp died in Kampen and was interred there in the Sint Nicolaaskerk.
Artwork.
Avercamp probably painted in his studio on the basis of sketches he had made in the winter.
Avercamp is famous even abroad for his winter landscapes. The passion for painting skating characters probably came from his childhood: he was practicing this hobby with his parents. The last quarter of the 16th century, during which Avercamp was born, was one of the coldest periods of the Little Ice Age.
The Flemish painting tradition is mainly expressed in Avercamp's early work. This is consistent with the landscapes of Pieter Bruegel the Elder. Avercamp painted landscapes with a high horizon and many figures who are working on something. The paintings are narrative, with many anecdotes. For instance, naughty details are included in the painting "Winter landscape with skaters": a couple making love, buttocks and a peeing male. 
Later in his life drawing the atmosphere was also important in his work. The horizon also gradually dropped down under more and more air.
Avercamp used the painting technique of aerial perspective. The depth is suggested by change of color in the distance. To the front objects are painted, such as trees or a boat. This technique strengthens the impression of depth in the painting.
Avercamp has also painted cattle and seascapes.
Sometimes Avercamp used paper frames, which were a cheap alternative to oil paintings. He first drew with pen and ink. This work was then covered with finishing paint. The contours of the drawing remained. Even with this technique Avercamp could show the pale wintry colors and nuances of the ice .
Avercamp produced about a hundred paintings. His main artwork can be seen in the Rijksmuseum in Amsterdam, the Mauritshuis in The Hague and abroad. The Rijksmuseum presented from November 20, 2009 to February 15, 2010 an exhibition of his work entitled "Little Ice Age".

</doc>
<doc id="14068" url="https://en.wikipedia.org/wiki?curid=14068" title="Hans Baldung">
Hans Baldung

Hans Baldung Grien or Grün ( September 1545) was a German artist in painting and printmaking who was considered the most gifted student of Albrecht Dürer. Throughout his lifetime, Baldung developed a distinctive style, full of color, expression and imagination. His talents were varied, and he produced a great and extensive variety of work including portraits, woodcuts, altarpieces, drawings, tapestries, allegories and mythological motifs.
Early life.
Hans Baldung was born in Swabia, Germany in the year 1484 to a family of intellectuals, academics and professionals. His father was a lawyer and his uncle was a doctor, and many other of his family members maintained professional degrees. In fact, Baldung was the first male in his family not to attend university but was one of the first German artists to come from an academic family. His earliest training as an artist began around 1500 in the Upper Rhineland by an artist from Strasbourg.
Life as a student of Dürer.
Beginning in 1503, Baldung was an apprentice for the most well renowned German artist of the day: Albrecht Dürer. Here, he was given his nickname “Grien.” This name foremost comes from his preference to the color green, because he usually wore green clothes. He got this nickname also to distinguish him from the two other Hans’ in the apprenticeship, Hans Schäufelein and Hans Suess von Kulmbach. He later included it in his monogram, and it has also been suggested that it came from "grienhals", a German word for witch. Hans quickly picked up Dürer's influence and style, and they became good friends. In his later trip to the Netherlands in 1521 Dürer's diary shows that he took with him and sold prints by Baldung. On Dürer's death Baldung was sent a lock of his hair, which suggests a close friendship. Near the end of his apprenticeship, Grien oversaw the production of stained glass, woodcuts and engravings, and therefore developed an affinity for them.
Strasbourg.
In 1509, when Baldung’s apprenticeship was complete, he moved back to Strasbourg and became a citizen there. He became the celebrity of the town, and was commissioned to make lots of art. The following year he married Margarethe Herlin, joined the guild "zur Steltz", opened a workshop, and began signing his works with the HGB monogram that he used for the rest of his career. His style also became much more mannerist. He loved it there.
Witchcraft and Religious Imagery.
In addition to traditional, religious subjects, Baldung was concerned during these years with the profane theme of the imminence of death and with scenes of sorcery and witchcraft. He was responsible for introducing supernatural and erotic themes into German art. He often depicted witches, also a local interest: Strasbourg's humanists studied witchcraft and its bishop was charged with ferreting out witches. His most characteristic paintings are fairly small in scale; a series of puzzling, often erotic allegories and mythological works. The number of Hans Baldung's religious works diminished with the Protestant Reformation's discouragement of idolatry. But earlier, around the same time that he produced Adam and Eve, the artist became interested in themes related to death, the supernatural, witchcraft and sorcery. That Mankind's mortality became a subject for Baldung was not unusual. Baldung’s fascination with witchcraft lasted well into the end of his career.
Work.
Painting.
Throughout his life, Baldung painted numerous portraits, known for their sharp characterizations. While Dürer rigorously details his models, Baldung's style differs by focusing more on the personality of the represented character, an abstract conception of the model's state of mind. Baldung settled eventually in Strasbourg and then to Freiburg im Breisgau, where he executed what is known as his masterpiece. Here in painted an eleven-panel altarpiece for the Freiburg Cathedral, still intact today, depicting scenes from the life of the Virgin, including, The Annunciation, The Visitation, The Nativity, The Flight into Egypt, The Crucifixion, Four Saints and The Donators. These depictions were a large part of the artist’s greater body of work containing several renowned pieces of the Virgin.
The earliest pictures assigned to him by some are altar-pieces with the monogram H. B. interlaced, and the date of 1496, in the monastery chapel of Lichtenthal near Baden-Baden. Another early work is a portrait of the emperor Maximilian, drawn in 1501 on a leaf of a sketch-book now in the print-room at Karlsruhe. "The Martyrdom of St Sebastian and the Epiphany" (now Berlin, 1507), were painted for the market-church of Halle in Saxony.
Baldung's prints, though Düreresque, are very individual in style, and often in subject. They show little direct Italian influence. His paintings are less important than his prints. He worked mainly in woodcut, although he made six engravings, one very fine. He joined in the fashion for chiaroscuro woodcuts, adding a tone block to a woodcut of 1510. Most of his hundreds of woodcuts were commissioned for books, as was usual at the time; his "single-leaf" woodcuts (i.e. prints not for book illustration) are fewer than 100, though no two catalogues agree as to the exact number.
Unconventional as a draughtsman, his treatment of human form is often exaggerated and eccentric (hence his linkage, in the art historical literature, with European Mannerism), whilst his ornamental style—profuse, eclectic, and akin to the self-consciously "German" strain of contemporary limewood sculptors—is equally distinctive. Though Baldung has been commonly called the Correggio of the north, his compositions are a curious medley of glaring and heterogeneous colours, in which pure black is contrasted with pale yellow, dirty grey, impure red and glowing green. Flesh is a mere glaze under which the features are indicated by lines.
His works are notable for their individualistic departure from the Renaissance composure of his model, Dürer, for the wild and fantastic strength that some of them display, and for their remarkable themes. In the field of painting, his "Eve, the Serpent and Death" (National Gallery of Canada) shows his strengths well. There is special force in the "Death and the Maiden" panel of 1517 (Basel), in the "Weather Witches" (Frankfurt), in the monumental panels of "Adam" and "Eve" (Madrid), and in his many powerful portraits. Baldung's most sustained effort is the altarpiece of Freiburg, where the Coronation of the Virgin, and the Twelve Apostles, the Annunciation, Visitation, Nativity and Flight into Egypt, and the Crucifixion, with portraits of donors, are executed with some of that fanciful power that Martin Schongauer bequeathed to the Swabian school.
As a portrait painter he is well known. He drew Charles V, as well as Maximilian; and his bust of Margrave Philip in the Munich Gallery tells us that he was connected with the reigning family of Baden as early as 1514. At a later period he had sittings with Margrave Christopher of Baden, Ottilia his wife, and all their children, and the picture containing these portraits is still in the gallery at Karlsruhe. Like Dürer and Cranach, Baldung supported the Protestant Reformation. He was present at the diet of Augsburg in 1518, and one of his woodcuts represents Luther in quasi-saintly guise, under the protection of (or being inspired by) the Holy Spirit, which hovers over him in the shape of a dove.
References.
Attribution:

</doc>
<doc id="14070" url="https://en.wikipedia.org/wiki?curid=14070" title="Hammered dulcimer">
Hammered dulcimer

The hammered dulcimer is a percussion instrument and stringed instrument with the strings typically stretched over a trapezoidal sounding board. The hammered dulcimer is set before the musician, who may sit cross legged on the floor or on a stool at an instrument set on legs. The player holds a small spoon shaped mallet hammer in each hand to strike the strings ("cf." Appalachian dulcimer). The Graeco-Roman "dulcimer" (sweet song) derives from the Latin "dulcis" (sweet) and the Greek "melos" (song). The dulcimer, in which the strings are beaten with small hammers, originated from the psaltery, in which the strings are plucked. 
Various types of hammered dulcimers are traditionally played in Iraq, India, Iran, Southwest Asia, China, and parts of Southeast Asia, Central Europe (Hungary, Romania, Slovakia, Poland, Czech Republic, Switzerland (particularly Appenzell), Austria and Bavaria), the Balkans, Eastern Europe (Ukraine and Belarus) and Scandinavia. The instrument is also played in the United Kingdom (Wales, East Anglia, Northumbria) and the U.S., where its traditional use in folk music saw a notable revival in the late 20th century.
Strings and tuning.
A dulcimer usually has two bridges, a bass bridge near the right and a treble bridge on the left side. The bass bridge holds up bass strings, which are played to the left of the bridge. The treble strings can be played on either side of the treble bridge. In the usual construction, playing them on the left side gives a note a fifth higher than playing them on the right of the bridge.
The dulcimer comes in various sizes, identified by the number of strings that cross each of the bridges. A 15/14, for example, has 15 strings crossing the treble bridge and 14 crossing the bass bridge, and can span three octaves. The strings of a hammered dulcimer are usually found in pairs, two strings for each note (though some instruments have three or four strings per note). Each set of strings is tuned in unison and is called a course. As with a piano, the purpose of using multiple strings per course is to make the instrument louder, although as the courses are rarely in perfect unison, a chorus effect usually results like a mandolin. A hammered dulcimer, like an autoharp, harp, or piano, requires a tuning wrench for tuning, since the dulcimer's strings are wound around tuning pins with square heads. (Ordinarily, 5 mm "zither pins" are used, similar to, but smaller in diameter than piano tuning pins, which come in various sizes ranging upwards from "1/0" or 7 mm.)
The strings of the hammered dulcimer are often tuned according to a circle of fifths pattern. Typically, the lowest note (often a G or D) is struck at the lower right-hand of the instrument, just to the left of the right-hand (bass) bridge. As a player strikes the courses above in sequence, they ascend following a repeating sequence of two whole steps and a half step. With this tuning, a diatonic scale is broken into two tetrachords, or groups of four notes. For example, on an instrument with D as the lowest note, the D major scale is played starting in the lower-right corner and ascending the bass bridge: D – E – F – G. This is the lower tetrachord of the G major scale. At this point the player returns to the bottom of the instrument and shifts to the treble strings to the right of the treble bridge to play the higher tetrachord: A – B – C – D. The player can continue up the scale on the right side of the treble bridge with E – F – G – A – B, but the next note will be C, not C, so he or she must switch to the left side of the treble bridge (and closer to the player) to continue the D major scale. See the drawing on the left above, in which "DO" would correspond to D (see Movable do solfège).
The shift from the bass bridge to the treble bridge is required because the bass bridge's fourth string G is the start of the lower tetrachord of the G scale. The player could go on up a couple notes (G - A - B), but the next note will be a flatted seventh (C natural in this case), because this note is drawn from the G tetrachord. This D major scale with a flatted seventh is the mixolydian mode in D.
The same thing happens as the player goes up the treble bridge – after getting to La (B in this case), one has to go to the left of the treble bridge. Moving from the left side of the bass bridge to the right side of the treble bridge is analogous to moving from the right side of the treble bridge to the left side of the treble bridge.
The whole pattern can be shifted up by three courses, so that instead of a D-major scale one would have a G-major scale, and so on. This transposes one equally tempered scale to another. Shifting down three courses transposes the D-major scale to A-major, but of course the first Do-Re-Mi would be shifted off the instrument.
This tuning results in most, but not all, notes of the chromatic scale being available. To fill in the gaps, many modern dulcimer builders include extra short bridges at the top and bottom of the soundboard, where extra strings are tuned to some or all of the missing pitches. Such instruments are often called "chromatic dulcimers" as opposed to the more traditional "diatonic dulcimers".
The tetrachord markers found on the bridges of most hammered dulcimers in the English-speaking world were introduced by the American player and maker Sam Rizzetta in the 1960s.
In the Alps there are also chromatic dulcimers with crossed strings, which are in a whole tone distance in every row. This chromatic "Salzburger hackbrett" was developed in the mid 1930s from the diatonic hammered dulcimer by Tobi Reizer and his son along with Franz Peyer and Heinrich Bandzauner. In the postwar period it was one of the instruments taught in state-sponsored music schools.
Hammered dulcimers of non-European descent may have other tuning patterns, and builders of European-style dulcimers sometimes experiment with alternate tuning patterns.
Hammers.
The instrument is referred to as hammered in reference to the small mallets (referred to as "hammers") that players use to strike the strings. Hammers are usually made of wood (most likely hard woods such as maple, cherry, padauk, oak, walnut, or any other hard wood), but can also be made from any material, including metal and plastic. In the Western hemisphere, hammers are usually stiff, but in Asia, flexible hammers are often used. The head of the hammer can be left bare for a sharp attack sound, or can be covered with adhesive tape, leather, or fabric for a softer sound. 
Two-sided hammers are also available. The heads of two sided hammers are usually oval or round. Most of the time, one side is left as bare wood while the other side may be covered in leather or a softer material such as piano felt.
Several traditional players have used hammers that differ substantially from those in common use today. Paul Van Arsdale (b. 1920), a player from upstate New York, uses flexible hammers made from hacksaw blades, with leather-covered wooden blocks attached to the ends (these are modeled after the hammers used by his grandfather, Jesse Martin). The Irish player John Rea (1915–1983) used hammers made of thick steel wire, wound with wool. He made these himself from old bicycle spokes. Billy Bennington (1900–1986), a player from Norfolk in England, used cane hammers bound with wool.
Hammered dulcimers, psalteries, pianos and harpsichords.
The hammered dulcimer was extensively used during the Middle Ages in England, France, Italy, Germany, the Netherlands and Spain. Although it had a distinctive name in each country, it was everywhere regarded as a kind of psalterium. The importance of the method of setting the strings in vibration by means of hammers, and its bearing on the acoustics of the instrument, were recognized only when the invention of the pianoforte had become a matter of history. It was then perceived that the psalterium in which the strings were plucked, and the dulcimer in which they were struck, when provided with keyboards, gave rise to two distinct families of instruments, differing essentially in tone quality, in technique and in capabilities: the evolution of the psalterium stopped at the harpsichord, that of the dulcimer gave us the pianoforte.
Around the world.
Versions of the hammered dulcimer are used throughout the world. In Eastern Europe, a larger descendant of the hammered dulcimer called the cimbalom is played and has been used by a number of classical composers, including Zoltán Kodály, Igor Stravinsky and Pierre Boulez. The khim is the name of both the Thai and the Khmer hammered dulcimer.
The santur or santoor is a type of hammered dulcimer that originated in Mesopotamia and is found in Iran, Iraq and India.

</doc>
<doc id="14071" url="https://en.wikipedia.org/wiki?curid=14071" title="Humanae vitae">
Humanae vitae

Humanae vitae (Latin "Of Human Life") is an encyclical written by Pope Paul VI and issued on 25 July 1968. Subtitled "On the Regulation of Birth", it re-affirms the orthodox teaching of the Catholic Church regarding married love, responsible parenthood, and the continued rejection of most forms of birth control. In formulating his teaching he did not accept (for reasons he gave in the encyclical) the conclusions arrived at by the Pontifical Commission on Birth Control established by his predecessor, Pope John XXIII, and which he himself had expanded.
Mainly because of its prohibition of all forms of "artificial" contraception, the encyclical was politically controversial, but it affirms traditional Church moral teaching on the sanctity of life and the procreative and unitive nature of conjugal relations. While Paul VI continued to teach through the medium of 122 Apostolic Constitutions, 8 Apostolic Exhortations, 121 Apostolic Letters, innumerable homilies, letters and reflections, he saw no need to issue any more encyclicals in the remaining ten years of his pontificate.
Between 1980 and 1984, Pope John Paul II delivered 129 addresses relating to the nature of marital love dubbed Catholic Theology of the Body, which fully vindicates "Humanae vitae". Pope Benedict XVI called this topic "controversial, yet so crucial for humanity's future". "Humanae vitae" became "a sign of contradiction but also of continuity of the Church's doctrine and tradition... What was true yesterday is true also today."
Summary.
Affirmation of traditional teaching.
In this encyclical Paul VI reaffirmed the Catholic Church's orthodox view of marriage and marital relations and a continued condemnation of "artificial" birth control. There were two Papal committees and numerous independent experts looking into the latest advancement of science and medicine on the question of artificial birth control, which were noted by the Pope in his encyclical. The expressed views of Paul VI reflected the teachings of his predecessors, especially Pius XI, Pius XII and John XXIII, all of whom had insisted on the divine obligations of the marital partners in light of their partnership with God the creator.
Doctrinal basis.
Paul VI himself, even as commission members issued their personal views over the years, always reaffirmed the teachings of the Church, repeating them more than once in the first years of his Pontificate.
To Pope Paul VI, as with of all his predecessors, marital relations are much more than a union of two people. They constitute a union of the loving couple with a loving God, in which the two persons create a new person materially, while God completes the creation by adding the soul. For this reason, Paul VI teaches in the first sentence of "Humanae vitae", that the transmission of human life is a most serious role in which married people collaborate freely and responsibly with God the Creator. This is divine partnership, so Paul VI does not allow for arbitrary human decisions, which may limit divine providence. According to Paul VI, marital relations are a source of great joy, but also of difficulties and hardships. The question of human procreation, exceeds in the view of Paul VI specific disciplines such as biology, psychology, demography or sociology. According to Paul VI, married love takes its origin from God, who is love, and from this basic dignity, he defines his position:
The encyclical opens with an assertion of the competency of the magisterium of the Catholic Church to decide questions of morality. It then goes on to observe that circumstances often dictate that married couples should limit the number of children, and that the sexual act between husband and wife is still worthy even if it can be foreseen not to result in procreation. Nevertheless, it is held that the sexual act must retain its intrinsic relationship to the procreation of human life.
Every action specifically intended to prevent procreation is forbidden, except in medically necessary circumstances. Therapeutic means necessary to cure diseases are exempted, even if a foreseeable impediment to procreation should result, but only if infertility is not directly intended. This is held to directly contradict the moral order which was established by God. Abortion, even for therapeutic reasons, is absolutely forbidden, as is sterilization, even if temporary. Therapeutic means which induce infertility are allowed ("e.g.", hysterectomy), if they are not specifically intended to cause infertility (e.g., the uterus is cancerous, so the preservation of life is intended). Natural family planning methods (abstaining from intercourse during certain parts of the menstrual cycle) are allowed, since they take advantage of a faculty provided by nature.
The acceptance of artificial methods of birth control is then claimed to result in several negative consequences, among them a general lowering of moral standards resulting from sex without consequences, and the danger that men may reduce women to being a mere instrument for the satisfaction of own desires; finally, abuse of power by public authorities, and a false sense of autonomy.
Appeal to natural law and conclusion.
Public authorities should oppose laws which undermine natural law; scientists should further study effective methods of natural birth control; doctors should further familiarize themselves with this teaching, in order to be able to give advice to their patients, priests must spell out clearly and completely the Church's teaching on marriage. The encyclical acknowledges that "perhaps not everyone will easily accept this particular teaching", but that "...it comes as no surprise to the church that she, no less than her Divine founder is destined to be a sign of contradiction." Noted is the duty of proclaiming the entire moral law, "both natural and evangelical." The encyclical also points out that the Roman Catholic Church cannot "declare lawful what is in fact unlawful", because she is concerned with "safeguarding the holiness of marriage, in order to guide married life to its full human and Christian perfection." This is to be the priority for his fellow bishops and priests and lay people. The Pope predicts that future progress in social cultural and economic spheres will make marital and family life more joyful, provided God's design for the world is faithfully followed. The encyclical closes with an appeal to observe the natural laws of the Most High God. "These laws must be wisely and lovingly observed."
History.
Origins.
There had been a long-standing general Christian prohibition on contraception and abortion, with such Church Fathers as Clement of Alexandria and Saint Augustine condemning the practices. It was not until the 1930 Lambeth Conference that the Anglican Communion allowed for contraception in limited circumstances. Mainline Protestant denominations have since removed prohibitions against artificial contraception. In a partial reaction, Pope Pius XI wrote the encyclical "Casti connubii" ("On Christian Marriage") in 1930, reaffirming the Catholic Church's belief in various traditional Christian teachings on marriage and sexuality, including the prohibition of artificial birth control even within marriage. "Casti connubii" is against contraception and regarding natural family planning allowed married couples to use their nuptial rights "in the proper manner" when because of either time or defects, new life could not be brought forth.
The commission of John XXIII.
With the appearance of the first oral contraceptives in 1960, dissenters in the Church argued for a reconsideration of the Church positions. In 1963 Pope John XXIII established a commission of six European non-theologians to study questions of birth control and population. The commission that Pope John XXIII formed to study population problems as well as acceptable methods of birth control met once in 1963 and twice in 1964. As Vatican Council II was concluding, Pope Paul VI enlarged it to fifty-eight members, including married couples, laywomen, as well as theologians and bishops. The last document issued by the council (Gaudium et spes) contained a very important section titled "Fostering the Nobility of Marriage" (1965, nos. 47-52), which discusses marriage from the personalist point of view. The "duty of responsible parenthood" was affirmed, but the determination of licit and illicit forms of regulating birth was reserved to Pope Paul VI. After the close of the council a fifth and final meeting of the commission was held, again enlarged to include sixteen bishops as an executive committee, in Rome in the spring of 1966. The commission was only consultative but did make a report to Paul VI approved by a majority of members, proposing that he might use his authority to approve at least some form of contraception for married couples. A minority number of members opposed this report and issued a parallel report to the Pope. After two more years of study and consultation, the pope issued "Humanae vitae", which removed any doubt that hormonal anti-ovulants are contraceptive. He explained why he did not accept the opinion of the majority report of the commission (1968, #6).
The role of Karol Wojtyła.
According to George Weigel's biography of John Paul II, Paul VI named Archbishop Karol Wojtyła (later Pope John Paul II) to the commission. However, the Communist authorities in Poland would not permit him to travel to Rome to take part in person. Wojtyła had earlier defended the church's position from a philosophical standpoint in his 1960 book "Love and Responsibility". Wojtyła's position was strongly considered, and was reflected in the final draft of the encyclical, although much of his language and arguments were not incorporated. Weigel attributes much of the poor reception of the encyclical to the omission of many of Wojtyła's arguments.
Drafting of the Encyclical.
In his role as Theologian of the Pontifical Household Mario Luigi Ciappi advised Pope Paul VI during the drafting of "Humanae vitae". Ciappi, a doctoral graduate of the "Pontificium Athenaeum Internationale Angelicum", the future Pontifical University of Saint Thomas Aquinas, "Angelicum", served as professor of dogmatic theology there and was Dean of the "Angelicum's" Faculty of Theology from 1935 to 1955.
Highlights.
Faithfulness to God's design.
13. Men rightly observe that a conjugal act imposed on one's partner without regard to his or her condition or personal and reasonable wishes in the matter, is no true act of love, and therefore offends the moral order in its particular application to the intimate relationship of husband and wife. If they further reflect, they must also recognize that an act of mutual love which impairs the capacity to transmit life which God the Creator, through specific laws, has built into it, frustrates His design which constitutes the norm of marriage, and contradicts the will of the Author of life. Hence to use this divine gift while depriving it, even if only partially, of its meaning and purpose, is equally repugnant to the nature of man and of woman, and is consequently in opposition to the plan of God and His holy will. But to experience the gift of married love while respecting the laws of conception is to acknowledge that one is not the master of the sources of life but rather the minister of the design established by the Creator. Just as man does not have unlimited dominion over his body in general, so also, and with more particular reason, he has no such dominion over his specifically sexual faculties, for these are concerned by their very nature with the generation of life, of which God is the source. "Human life is sacred—all men must recognize that fact," Our predecessor Pope John XXIII recalled. "From its very inception it reveals the creating hand of God."
Lawful therapeutic means.
15. ...the Church does not consider at all illicit the use of those therapeutic means necessary to cure bodily diseases, even if a foreseeable impediment to procreation should result therefrom — provided such impediment is not directly intended.
Recourse to infertile periods.
16. ...If therefore there are well-grounded reasons for spacing births, arising from the physical or psychological condition of husband or wife, or from external circumstances, the Church teaches that married people may then take advantage of the natural cycles immanent in the reproductive system and engage in marital intercourse only during those times that are infertile, thus controlling birth in a way which does not in the least offend the moral principles which We have just explained.
Concern of the Church.
18. It is to be anticipated that perhaps not everyone will easily accept this particular teaching. There is too much clamorous outcry against the voice of the Church, and this is intensified by modern means of communication. But it comes as no surprise to the Church that she, no less than her divine Founder, is destined to be a "sign of contradiction.") She does not, because of this, evade the duty imposed on her of proclaiming humbly but firmly the entire moral law, both natural and evangelical. Since the Church did not make either of these laws, she cannot be their arbiter—only their guardian and interpreter. It could never be right for her to declare lawful what is in fact unlawful, since that, by its very nature, is always opposed to the true good of man. In preserving intact the whole moral law of marriage, the Church is convinced that she is contributing to the creation of a truly human civilization. She urges man not to betray his personal responsibilities by putting all his faith in technical expedients. In this way she defends the dignity of husband and wife. This course of action shows that the Church, loyal to the example and teaching of the divine Savior, is sincere and unselfish in her regard for men whom she strives to help even now during this earthly pilgrimage "to share God's life as sons of the living God, the Father of all men".
Developing countries.
23. We are fully aware of the difficulties confronting the public authorities in this matter, especially in the developing countries. In fact, We had in mind the justifiable anxieties which weigh upon them when We published Our encyclical letter "Populorum Progressio". But now We join Our voice to that of Our predecessor John XXIII of venerable memory, and We make Our own his words: "No statement of the problem and no solution to it is acceptable which does violence to man's essential dignity; those who propose such solutions base them on an utterly materialistic conception of man himself and his life. The only possible solution to this question is one which envisages the social and economic progress both of individuals and of the whole of human society, and which respects and promotes true human values." No one can, without being grossly unfair, make divine Providence responsible for what clearly seems to be the result of misguided governmental policies, of an insufficient sense of social justice, of a selfish accumulation of material goods, and finally of a culpable failure to undertake those initiatives and responsibilities which would raise the standard of living of peoples and their children.
Response and criticism.
Galileo affair comparison.
Cardinal Leo Joseph Suenens, a moderator of the ecumenical council, questioned, "whether moral theology took sufficient account of scientific progress, which can help determine, what is according to nature. I beg you my brothers let us avoid another Galileo affair. One is enough for the Church." In an interview in "Informations Catholiques Internationales" on 15 May 1969, he criticized the Pope’s decision again as frustrating the collegiality defined by the Council, calling it a non-collegial or even an anti-collegial act. He was supported by Vatican II theologians such as Karl Rahner, Hans Küng, and several bishops, including Christopher Butler, who called it one of the most important contributions to contemporary discussion in the Church.
Open dissent.
The publication of the encyclical marks the first time in the twentieth century that open dissent from the laity about teachings of the Church was voiced widely and publicly. The teaching has been criticized by development organizations and others who claim that it limits the methods available to fight worldwide population growth and struggle against HIV/AIDS. Within two days of the encyclical's release, a group of dissident theologians, led by Rev. Charles Curran, then of The Catholic University of America, issued a statement stating, "spouses may responsibly decide according to their conscience that artificial contraception in some circumstances is permissible and indeed necessary to preserve and foster the value and sacredness of marriage.
Canadian bishops.
Two months later, the controversial "Winnipeg Statement" issued by the Canadian Conference of Catholic Bishops stated that those who cannot accept the teaching should not be considered shut off from the Catholic Church, and that individuals can in good conscience use contraception as long as they have first made an honest attempt to accept the difficult directives of the encyclical.
Dutch Catechism.
The Dutch Catechism of 1966, based on the Dutch bishops' interpretation of the just completed Vatican Council, and the first post-Council comprehensive Catholic catechism, noted the lack of mention of artificial contraception in the Council. "As everyone can ascertain nowadays, there are several methods of regulating births. The Second Vatican Council did not speak of any of these concrete methods… This is a different standpoint than that taken under Pius XI some thirty years which was also maintained by his successor ... we can sense here a clear development in the Church, a development, which is also going on outside the Church."
Poland.
There were significant struggles between the Church and the Communist rulers of Poland, who promoted abortion and birth control. 
Soviet Union.
In the Soviet Union, "Literaturnaja Gazeta", a publication of Soviet intellectuals, included an editorial and statement by Russian physicians against the encyclical.
Ecumenical reactions.
Ecumenical reactions were mixed. Liberal and Moderate Lutherans and the World Council of Churches were disappointed. Eugene Carson Blake criticised the concepts of nature and natural law, which, in his view, still dominated Catholic theology, as outdated. This concern dominated several articles in Catholic and non-Catholic journals at the time. Patriarch Athenagoras I stated his full agreement with Pope Paul VI: “He could not have spoken in any other way.”
Latin America.
In Latin America, much support developed for the Pope and his encyclical. As World Bank President Robert McNamara declared at the 1968 Annual Meeting of the International Monetary Fund and the World Bank Group that countries permitting birth control practices will get preferential access to resources, doctors in La Paz, Bolivia, called it insulting that money should be exchanged for the conscience of a Catholic nation. In Colombia, Cardinal Anibal Muñoz Duque declared, if American conditionality undermines Papal teachings, we prefer not to receive one cent. The Senate of Bolivia passed a resolution, stating that "Humanae vitae" can be discussed in its implications on individual consciences, but, it is of greatest significance, because the papal document defends the rights of developing nations to determine their own population policies. The Jesuit Journal "Sic" dedicated one edition to the encyclical with supportive contributions.
Cardinal Martini.
In the book "Nighttime conversations in Jerusalem. On the risk of faith." well-known liberal Cardinal Carlo Maria Martini accused Paul VI of deliberately concealing the truth, leaving it to theologians and pastors to fix things by adapting precepts to practice:
"I knew Paul VI well. With the encyclical, he wanted to express consideration for human life. He explained his intention to some of his friends by using a comparison: although one must not lie, sometimes it is not possible to do otherwise; it may be necessary to conceal the truth, or it may be unavoidable to tell a lie. It is up to the moralists to explain where sin begins, especially in the cases in which there is a higher duty than the transmission of life." 
Response of Pope Paul VI.
Pope Paul VI was troubled by the encyclical's reception in the West. Acknowledging the controversy, Paul VI in a letter to the Congress of German Catholics (Aug. 30, 1968), stated: "May the lively debate aroused by our encyclical lead to a better knowledge of God’s will." In March 1969, he had a meeting with one of the main critics of "Humanae vitae", Cardinal Leo Joseph Suenens. Paul heard him out and said merely, "Yes, pray for me; because of my weaknesses, the Church is badly governed". On 23 June 1978, weeks before his death, in an address to the College of Cardinals, Paul VI reaffirmed his "Humanae vitae": "following the confirmations of serious science", and which sought to affirm the principle of respect for the laws of nature and of "a conscious and ethically responsible paternity".
Legacy.
Although polls show that many Catholics dissent from church teaching on contraception, there has nevertheless been a resurgence of support for it in certain quarters. Roman Catholic lay writers such as Janet E. Smith, theologian Scott and his wife Kimberly Hahn, Christopher West, Matt Fradd, and Mary Shivanandan have all written extensively in support of the teaching, and on the reasons behind it. At the official level, Catholicism’s commitment to “Humanae Vitae” is more stable than ever. According to John L. Allen, Jr., "In addition, three decades of bishops’ appointments by John Paul II and Benedict XVI, both unambiguously committed to “Humanae Vitae,” mean that senior leaders in Catholicism these days are far less inclined than they were in 1968 to distance themselves from the ban on birth control, or to soft-pedal it. A striking number of Catholic bishops have recently brought out documents of their own defending “Humanae Vitae.” Also, developments in fertility awareness since the 1960s have given rise to natural family planning organizations such as the [http://www.woomb.org Billings Ovulation Method, Couple to Couple League and the Creighton Model FertilityCare System, which actively provide formal instruction on the use and reliability of natural methods of birth control.
Pope John Paul I.
Albino Luciani's views on "Humanae vitae" have been debated. Journalist John L. Allen, Jr. claims that "it's virtually certain that John Paul I would not have reversed Paul VI’s teaching, particularly since he was no doctrinal radical. Moreover, as Patriarch in Venice some had seen a hardening of his stance on social issues as the years went by." According to Allen "...it is reasonable to assume that John Paul I would not have insisted upon the negative judgment in "Humanae Vitae" as aggressively and publicly as John Paul II did, and probably would not have treated it as a quasi-infallible teaching. It would have remained a more 'open' question". Other sources take a different view and note that during his time as Patriarch of Venice that "Luciani was intransigent with his upholding of the teaching of the Church and severe with those, through intellectual pride and disobedience paid no attention to the Church's prohibition of contraception", though while not condoning the sin, he was tolerant of those who sincerely tried and failed to live up to the Church's teaching. The book states that "...if some people think that his compassion and gentleness in this respect implies he was against Humane Vitae one can only infer it was wishful thinking on their part and an attempt to find an ally in favor of artificial contraception."
Pope John Paul II.
After he became pope in 1978, John Paul II continued on the Catholic Theology of the Body of his predecessors with a series of lectures, entitled "Theology of the Body", in which he talked about an "original unity between man and women", purity of heart (on the Sermon on the Mount), marriage and celibacy and reflections on "Humane vitae", focusing largely on responsible parenthood and marital chastity. John Paul II readdressed some of the same issues in his 1993 encyclical "Veritatis splendor". He reaffirmed much of "Humanae vitae", and specifically described the practice of artificial contraception as an act not permitted by Catholic teaching in any circumstances. The same encyclical also clarifies the use of conscience in arriving at moral decisions, including in the use of contraception. However, John Paul also said, “It is not right then to regard the moral conscience of the individual and the magisterium of the Church as two contenders, as two realities in conflict. The authority which the magisterium enjoys by the will of Christ exists so that the moral conscience can attain the truth with security and remain in it.” John Paul quoted "Humanae vitae" as a compassionate encyclical, "Christ has come not to judge the world but to save it, and while he was uncompromisingly stern towards sin, he was patient and rich in mercy towards sinners".
Pope Benedict XVI.
On 12 May 2008, Benedict XVI accepted an invitation to talk to participants in the International Congress organized by the Pontifical Lateran University on the 40th anniversary of "Humanae vitae". He put the encyclical in the broader view of love in a global context, a topic he called "so controversial, yet so crucial for humanity's future." "Humanae vitae" became "a sign of contradiction but also of continuity of the Church's doctrine and tradition... What was true yesterday is true also today." The Church continues to reflect "in an ever new and deeper way on the fundamental principles that concern marriage and procreation." The key message of "Humanae vitae" is love. Benedict states, that the fullness of a person is achieved by a unity of soul and body, but neither spirit nor body alone can love, only the two together. If this unity is broken, if only the body is satisfied, love becomes a commodity.
Pope Francis.
On 1 May 2014, Pope Francis, in an interview given to Italian newspaper "Corriere della Sera", expressed his opinion and praise for "Humanae Vitae": "Everything depends on how 'Humanae Vitae' is interpreted. Paul VI himself, in the end, urged confessors to be very merciful and pay attention to concrete situations. But his genius was prophetic, he had the courage to take a stand against the majority, to defend moral discipline, to exercise a cultural restraint, to oppose present and future neo-Malthusianism. The question is not of changing doctrine, but of digging deep and making sure that pastoral care takes into account situations and what it is possible for persons to do."

</doc>
<doc id="14072" url="https://en.wikipedia.org/wiki?curid=14072" title="History of Wikipedia">
History of Wikipedia

The history of Wikipedia formally began with the launch of Wikipedia on 15 January 2001 by Jimmy Wales and Larry Sanger. Its technological and conceptual underpinnings predate this; the earliest known proposal for an online encyclopedia was made by Rick Gates in 1993, but the concept of a free-as-in-freedom online encyclopedia (as distinct from mere open source or freemium) was proposed by Richard Stallman in December 2000.
Crucially, Stallman's concept specifically included the idea that no central organization should control editing. This "massively multiplayer" characteristic was in stark contrast to contemporary digital encyclopedias such as Microsoft Encarta, "Encyclopædia Britannica", and even Bomis's Nupedia, which was Wikipedia's direct predecessor. In 2001, the license for Nupedia was changed to GFDL, and Wales and Sanger launched Wikipedia using the concept and technology of a wiki pioneered in 1995 by Ward Cunningham. Initially, Wikipedia was intended to complement Nupedia, an online encyclopedia project edited solely by experts, by providing additional draft articles and ideas for it. In practice, Wikipedia quickly overtook Nupedia, becoming a global project in multiple languages and inspiring a wide range of other online reference projects.
According to Alexa Internet, Wikipedia is the world's seventh-most-popular website in terms of overall visitor traffic. Wikipedia's total worldwide monthly readership is approximately 495 million. Worldwide in August 2015, WMF Labs tallied 18 billion page views for the month. According to comScore, Wikipedia receives over 117 million monthly unique visitors from the United States alone.
Historical overview.
Background.
The concept of compiling the world's knowledge in a single location dates to the ancient Libraries of Alexandria and Pergamum, but the modern concept of a general-purpose, widely distributed, printed encyclopedia originated with Denis Diderot and the 18th-century French encyclopedists. The idea of using automated machinery beyond the printing press to build a more useful encyclopedia can be traced to Paul Otlet's 1934 book "Traité de documentation"; Otlet also founded the Mundaneum, an institution dedicated to indexing the world's knowledge, in 1910. This concept of a machine-assisted encyclopedia was further expanded in H. G. Wells' book of essays "World Brain" (1938) and Vannevar Bush's future vision of the microfilm-based Memex in "As We May Think" (1945). Another milestone was Ted Nelson's hypertext design Project Xanadu, which was begun in 1960.
Advances in information technology in the late 20th century led to changes in the form of encyclopedias. While previous encyclopedias, notably the "Encyclopædia Britannica", were book-based, Microsoft's Encarta, published in 1993, was available on CD-ROM and hyperlinked. The development of the World Wide Web led to many attempts to develop internet encyclopedia projects. An early proposal for a web-based encyclopedia was Interpedia in 1993 by Rick Gates; this project died before generating any encyclopedic content. Free software proponent Richard Stallman described the usefulness of a "Free Universal Encyclopedia and Learning Resource" in 1999. His published document "aims to lay out what the free encyclopedia needs to do, what sort of freedoms it needs to give the public, and how we can get started on developing it." On Wednesday 17 January 2001, two days after the founding of Wikipedia, the Free Software Foundation's (FSF) GNUPedia project went online, competing with Nupedia, but today the FSF encourages people "to visit and contribute to ".
Formulation of the concept.
Wikipedia was initially conceived as a feeder project for the Wales-founded Nupedia, an earlier project to produce a free online encyclopedia, volunteered by Bomis, a web-advertising firm owned by Jimmy Wales, Tim Shell and Michael E. Davis. Nupedia was founded upon the use of highly qualified volunteer contributors and an elaborate multi-step peer review process. Despite its mailing list of interested editors, and the presence of a full-time editor-in-chief, Larry Sanger, a graduate philosophy student hired by Wales, the writing of content for Nupedia was extremely slow, with only 12 articles written during the first year.
Wales and Sanger discussed various ways to create content more rapidly. The idea of a wiki-based complement originated from a conversation between Larry M. Sanger and Ben Kovitz. Ben Kovitz was a computer programmer and regular on Ward Cunningham's revolutionary wiki "the WikiWikiWeb". He explained to Sanger what wikis were, at that time a difficult concept to understand, over a dinner on Tuesday 2 January 2001. Wales first stated, in October 2001, that "Larry had the idea to use Wiki software", though he later stated in December 2005 that Jeremy Rosenfeld, a Bomis employee, introduced him to the concept. Sanger thought a wiki would be a good platform to use, and proposed on the Nupedia mailing list that a wiki based upon UseModWiki (then v. 0.90) be set up as a "feeder" project for Nupedia. Under the subject "Let's make a wiki", he wrote:
Wales set one up and put it online on Wednesday 10 January 2001.
Founding of Wikipedia.
There was considerable resistance on the part of Nupedia's editors and reviewers to the idea of associating Nupedia with a wiki-style website. Sanger suggested giving the new project its own name, "Wikipedia", and Wikipedia was soon launched on its own domain, wikipedia.com, on Monday 15 January 2001. The bandwidth and server (located in San Diego) used for these initial projects were donated by Bomis. Many former Bomis employees later contributed content to the encyclopedia: notably Tim Shell, co-founder and later CEO of Bomis, and programmer Jason Richey.
In December 2008, Wales stated that he made Wikipedia's first edit, a test edit with the text "Hello, World!" The oldest article still preserved is the article [//en.wikipedia.org/w/index.php?title=Wikipedia:UuU&oldid=291430 UuU], created on Tuesday 16 January 2001, at 21:08 UTC. The existence of the project was formally announced and an appeal for volunteers to engage in content creation was made to the Nupedia mailing list on 17 January.
The project received many new participants after being mentioned on the Slashdot website in July 2001, having already earned two minor mentions in March 2001. It then received a prominent pointer to a story on the community-edited technology and culture website Kuro5hin on 25 July. Between these relatively rapid influxes of traffic, there had been a steady stream of traffic from other sources, especially Google, which alone sent hundreds of new visitors to the site every day. Its first major mainstream media coverage was in the "New York Times" on Thursday 20 September 2001.
The project gained its 1,000th article around Monday 12 February 2001, and reached 10,000 articles around 7 September. In the first year of its existence, over 20,000 encyclopedia entries were created – a rate of over 1,500 articles per month. On Friday 30 August 2002, the article count reached 40,000.
Wikipedia's earliest edits were long believed lost, since the original UseModWiki software deleted old data after about a month. On Tuesday 14 December 2010, developer Tim Starling found backups on SourceForge containing every change made to Wikipedia from its creation in January 2001 to 17 August 2001.
Namespaces, subdomains, and internationalization.
Early in Wikipedia's development, it began to expand internationally, with the creation of new namespaces, each with a distinct set of usernames. The first subdomain created for a non-English Wikipedia was "deutsche.wikipedia.com" (created on Friday 16 March 2001, 01:38 UTC), followed after a few hours by "Catalan.wikipedia.com" (at 13:07 UTC). The Japanese Wikipedia, started as nihongo.wikipedia.com, was created around that period, and initially used only Romanized Japanese. For about two months Catalan was the one with the most articles in a non-English language, although statistics of that early period are imprecise. The French Wikipedia was created on or around 11 May 2001, in a wave of new language versions that also included Chinese, Dutch, Esperanto, Hebrew, Italian, Portuguese, Russian, Spanish, and Swedish. These languages were soon joined by Arabic and Hungarian. In September 2001, an announcement pledged commitment to the multilingual provision of Wikipedia, notifying users of an upcoming roll-out of Wikipedias for all major languages, the establishment of core standards, and a push for the translation of core pages for the new wikis. At the end of that year, when international statistics first began to be logged, Afrikaans, Norwegian, and Serbian versions were announced.
In January 2002, 90% of all Wikipedia articles were in English. By January 2004, fewer than 50% were English, and this internationalization has continued to increase as the encyclopedia grows. As of 2014, about 85.5% of all Wikipedia articles are contained within non-English Wikipedia versions.
Development of Wikipedia.
In March 2002, following the withdrawal of funding by Bomis during the dot-com bust, Larry Sanger left both Nupedia and Wikipedia. By 2002, Sanger and Wales differed in their views on how best to manage open encyclopedias. Both still supported the open-collaboration concept, but the two disagreed on how to handle disruptive editors, specific roles for experts, and the best way to guide the project to success.
Wales went on to establish self-governance and bottom-up self-direction by editors on Wikipedia. He made it clear that he would not be involved in the community's day-to-day management, but would encourage it to learn to self-manage and find its own best approaches. As of 2007, Wales mostly restricts his own role to occasional input on serious matters, executive activity, advocacy of knowledge, and encouragement of similar reference projects.
Sanger says he is an "inclusionist" and is open to almost anything. He proposed that experts still have a place in the Web 2.0 world. He returned briefly to academia, then joined the Digital Universe Foundation. In 2006, Sanger founded Citizendium, an open encyclopedia that used real names for contributors in an effort to reduce disruptive editing, and hoped to facilitate "gentle expert guidance" to increase the accuracy of its content. Decisions about article content were to be up to the community, but the site was to include a statement about "family-friendly content". He stated early on that he intended to leave Citizendium in a few years, by which time the project and its management would presumably be established.
Organization.
The Wikipedia project has grown rapidly in the course of its life, at several levels. Content has grown organically through the addition of new articles, new wikis have been added in English and non-English languages, and entire new projects replicating these growth methods in other related areas (news, quotations, reference books and so on) have been founded as well. Wikipedia itself has grown, with the creation of the Wikimedia Foundation to act as an umbrella body and the growth of software and policies to address the needs of the editorial community. These are documented below:
Timeline.
2000.
In March 2000, the Nupedia project was started. Its intention was to publish articles written by experts which would be licensed as free content. Nupedia was founded by Jimmy Wales, with Larry Sanger as editor-in-chief, and funded by the web-advertising company Bomis.
2001.
In January 2001, Wikipedia began as a side-project of Nupedia, to allow collaboration on articles prior to entering the peer-review process. The name was suggested by Sanger on 11 January 2001. The "wikipedia.com" and "wikipedia.org" domain names were registered on 12 and 13 January, respectively, with "wikipedia.org" being brought online on the same day. The project formally opened on 15 January (""), with the first international Wikipedias – the French, German, Catalan, Swedish, and Italian editions – being created between March and May. The "neutral point of view" (NPOV) policy was officially formulated at this time, and Wikipedia's first slashdotter wave arrived on 26 July. The first media report about Wikipedia appeared in August 2001 in the newspaper "Wales on Sunday". The September 11 attacks spurred the appearance of breaking news stories on the homepage, as well as information boxes linking related articles.
2002.
2002 saw the end of funding for Wikipedia from Bomis and the departure of Larry Sanger. The forking of the Spanish Wikipedia also took place with the establishment of the "Enciclopedia Libre". The first portable MediaWiki software went live on 25 January. Bots were introduced, Jimmy Wales confirmed that Wikipedia would never run commercial advertising, and the first sister project (Wiktionary) and first formal Manual of Style were launched. A separate board of directors to supervise the project was proposed and initially discussed at Meta-Wikipedia.
2003.
The English Wikipedia passed 100,000 articles in 2003, while the next largest edition, the German Wikipedia, passed 10,000. The Wikimedia Foundation was established, and Wikipedia adopted its jigsaw world logo. Mathematical formulae using TeX were reintroduced to the website. The first Wikipedian social meeting took place in Munich, Germany, in October. The basic principles of Wikipedia's (known colloquially as "ArbCom") were developed, mostly by , and other early Wikipedians.
Wikisource was created as a separate project on November 24, 2003, to host free textual sources.
2004.
The worldwide Wikipedia article pool continued to grow rapidly in 2004, doubling in size in 12 months, from under 500,000 articles in late 2003 to over 1 million in over 100 languages by the end of 2004. The English Wikipedia accounted for just under half of these articles. The website's server farms were moved from California to Florida, and CSS style configuration sheets were introduced, and the first attempt to block Wikipedia occurred, with the website being blocked in China for two weeks in June. The formal election of a board and Arbitration Committee began. The first formal projects were proposed to deliberately balance content and seek out systemic bias arising from Wikipedia's community structure.
"Bourgeois v. Peters", (11th Cir. 2004), a court case decided by the United States Court of Appeals for the Eleventh Circuit was one of the earliest . It stated: "We also reject the notion that the Department of Homeland Security's threat advisory level somehow justifies these searches. Although the threat level was 'elevated' at the time of the protest, 'to date, the threat level has stood at yellow (elevated) for the majority of its time in existence. It has been raised to orange (high) six times."
Wikimedia Commons was created in September 7, 2004 to host media files for Wikipedia in all languages.
2005.
In 2005, Wikipedia became the most popular reference website on the Internet, according to Hitwise, with the English Wikipedia alone exceeding 750,000 articles. Wikipedia's first multilingual and subject portals were established in 2005. A formal fundraiser held in the first quarter of the year raised almost US$100,000 for system upgrades to handle growing demand. China again blocked Wikipedia in October 2005.
The first major Wikipedia scandal, the Seigenthaler incident, occurred in 2005, when a well-known figure was found to have a vandalized biography which had gone unnoticed for months. In the wake of this and other concerns, the first policy and system changes specifically designed to counter this form of abuse were established. These included a new privilege policy update to assist in sock puppetry investigations, a new feature called , a more strict policy on biographies of living people and the tagging of such articles for stricter review. A restriction of new article creation to registered users only was put in place in December 2005.
Wikimania 2005, the first Wikimania conference, was held from 4 to 8 August 2005 at the "Haus der Jugend" in Frankfurt, Germany, attracting about 380 attendees.
2006.
The English Wikipedia gained its one-millionth article, Jordanhill railway station, on 1 March 2006. The first approved Wikipedia article selection was made freely available to download, and "Wikipedia" became registered as a trademark of the Wikimedia Foundation. The congressional aides biography scandals – multiple incidents in which congressional staffers and a campaign manager were caught trying to covertly alter Wikipedia biographies – came to public attention, leading to the resignation of the campaign manager. Nonetheless, Wikipedia was rated as one of the top five global brands of 2006.
Jimmy Wales indicated at Wikimania 2006 that Wikipedia had achieved sufficient volume and called for an emphasis on quality, perhaps best expressed in the call for . A new privilege, "oversight", was created, allowing specific versions of archived pages with unacceptable content to be marked as non-viewable. Semi-protection against anonymous vandalism, introduced in 2005, proved more popular than expected, with over 1,000 pages being semi-protected at any given time in 2006.
2007.
Wikipedia continued to grow rapidly in 2007, possessing over 5 million registered editor accounts by 13 August. The 250 language editions of Wikipedia contained a combined total of 7.5 million articles, totalling 1.74 billion words in approximately 250 languages, by 13 August. The English Wikipedia gained articles at a steady rate of 1,700 a day, with the wikipedia.org domain name ranked the 10th-busiest in the world. Wikipedia continued to garner visibility in the press – the Essjay controversy broke when a prominent member of Wikipedia was found to have lied about his credentials. Citizendium, a competing online encyclopedia, launched publicly. A new trend developed in Wikipedia, with the encyclopedia addressing people whose notability stemmed from being a participant in a news story by adding a redirect from their name to the larger story, rather than creating a distinct biographical article. On 9 September 2007, the English Wikipedia gained its two-millionth article, El Hormiguero. There was some controversy in late 2007 when the Volapük Wikipedia jumped from 797 to over 112,000 articles, briefly becoming the 15th-largest Wikipedia edition, due to automated stub generation by an enthusiast for the Volapük constructed language.
According to the "MIT Technology Review", the number of regularly active editors on the English-language Wikipedia peaked in 2007 at more than 51,000, and has since been declining.
2008.
Various in many areas continued to expand and refine article contents within their scope. In April 2008, the 10-millionth Wikipedia article was created, and by the end of the year the English Wikipedia exceeded 2.5 million articles.
2009.
By late August 2009, the number of articles in all Wikipedia editions had exceeded 14 million. The three-millionth article on the English Wikipedia, Beate Eriksen, was created on 17 August 2009 at 04:05 UTC. On 27 December 2009, the German Wikipedia exceeded one million articles, becoming the second edition after the English Wikipedia to do so. A "TIME" article listed Wikipedia among 2009's best websites.
Wikipedia content became licensed under Creative Commons in 2009.
2010.
On 24 March, the European Wikipedia servers went offline due to an overheating problem. Failover to servers in Florida turned out to be broken, causing DNS resolution for Wikipedia to fail across the world. The problem was resolved quickly, but due to DNS caching effects, some areas were slower to regain access to Wikipedia than others.
On 13 May, the site released a new interface. New features included an updated logo, new navigation tools, and a link wizard. However, the classic interface remained available for those who wished to use it. On 12 December, the English Wikipedia passed the 3.5-million-article mark, while the French Wikipedia's millionth article was created on 21 September. The 1-billionth Wikimedia project edit was performed on 16 April.
2011.
Wikipedia and its users held hundreds of celebrations worldwide to commemorate the site's 10th anniversary on 15 January. The site began efforts to expand its growth in India, holding its first Indian conference in Mumbai in November 2011. The English Wikipedia passed the 3.6-million-article mark on 2 April, and reached 3.8 million articles on 18 November. On 7 November 2011, the German Wikipedia exceeded 100 million page edits, becoming the second language edition to do so after the English edition, which attained 500 million page edits on 24 November 2011. The Dutch Wikipedia exceeded 1 million articles on 17 December 2011, becoming the fourth Wikipedia edition to do so.
Between 4 and 6 October 2011, the Italian Wikipedia became intentionally inaccessible in protest against the Italian Parliament's proposed DDL intercettazioni law, which, if approved, would allow any person to force websites to remove information that is perceived as untrue or offensive, without the need to provide evidence.
Also in October 2011, Wikimedia announced the launch of Wikipedia Zero, an initiative to enable free mobile access to Wikipedia in developing countries through partnerships with mobile operators.
2012.
On 16 January, Wikipedia co-founder Jimmy Wales announced that the English Wikipedia would shut down for 24 hours on 18 January as part of a protest meant to call public attention to the proposed Stop Online Piracy Act and PROTECT IP Act, two anti-piracy laws under debate in the United States Congress. Calling the blackout a "community decision", Wales and other opponents of the laws believed that they would endanger free speech and online innovation. A similar blackout was staged on 10 July by the Russian Wikipedia, in protest against a proposed Russian internet regulation law.
In late March 2012, the announced Wikidata, a universal platform for sharing data between all Wikipedia language editions. The US$1.7-million Wikidata project was partly funded by Google, the Gordon and Betty Moore Foundation, and the Allen Institute for Artificial Intelligence. Wikimedia Deutschland assumed responsibility for the first phase of Wikidata, and initially planned to make the platform available to editors by December 2012. Wikidata's first phase became fully operational in March 2013.
In April 2012, Justin Knapp from Indianapolis, Indiana, became the first single contributor to make over one million edits to Wikipedia. Jimmy Wales congratulated Knapp for his work and presented him with the site's "Special Barnstar" medal and the "Golden Wiki" award for his achievement. Wales also declared that 20 April would be "Justin Knapp Day".
On 13 July 2012, the English Wikipedia gained its 4-millionth article, Izbat al-Burj. In October 2012, historian and Wikipedia editor Richard J. Jensen opined that the English Wikipedia was "nearing completion", noting that the number of regularly active editors had fallen significantly since 2007, despite Wikipedia's rapid growth in article count and readership.
According to Alexa Internet, Wikipedia was the world's sixth-most-popular website as of November 2012. Dow Jones ranked Wikipedia fifth worldwide as of December 2012.
2013.
On 22 January 2013, the Italian Wikipedia became the fifth language edition of Wikipedia to exceed 1 million articles, while the Russian and Spanish Wikipedias gained their millionth articles in May 11 and 16 respectively. On 15 July the Swedish and on 24 September the Polish Wikipedias gained their millionth articles, becoming the eighth and ninth Wikipedia editions to do so.
On 27 January, the main belt asteroid 274301 was officially renamed "Wikipedia" by the Committee for Small Body Nomenclature.
The first phase of the Wikidata database, automatically providing interlanguage links and other data, became available for all language editions in March 2013.
In April 2013, the French secret service was accused of attempting to censor Wikipedia by threatening a Wikipedia volunteer with arrest unless "classified information" about a military radio station was deleted.
In July, the VisualEditor editing system was launched, forming the first stage of an effort to allow articles to be edited with a word processor-like interface instead of using wikimarkup. An editor specifically designed for smartphones and other mobile devices was also launched.
2014.
A print edition of the English Wikipedia, comprising 1,000 volumes and over 1,100,000 pages, was exhibited by German Wikipedia contributors in 2014. The project sought funding through Indiegogo, and was intended to honor the contributions of Wikipedia's editors. On 22 October 2014, the first monument to Wikipedia was unveiled in the Polish town of Slubice.
2015.
In mid-2015, Wikipedia was the world's seventh-most-popular website according to Alexa Internet, down one place from the position it held in November 2012. At the start of 2015, Wikipedia remained the largest general-knowledge encyclopedia online, with a combined total of over 36 million mainspace articles across all 291 language editions. On average, Wikipedia receives a total of 10 billion global pageviews from around 495 million unique visitors every month, including 85 million visitors from the United States alone, where it is the sixth-most-popular site.
"Print Wikipedia" was an art project by Michael Mandiberg that printed out the 7473 volumes of Wikipedia as it existed on April 7, 2015. Each volume has 700 pages.
History by subject area.
Fundraising.
Every year, Wikipedia runs a fundraising campaign to support its operations.
External impact.
Effect of biographical articles.
Because Wikipedia biographies are often updated as soon as new information comes to light, they are often used as a reference source on the lives of notable people. This has led to attempts to manipulate and falsify Wikipedia articles for promotional or defamatory purposes (see Controversies). It has also led to novel uses of the biographical material provided. Some notable people's lives are being affected by their Wikipedia biography.
Early roles of Wales and Sanger.
Sanger played an important role in the early stages of creating Wikipedia. Wales says that Sanger was his subordinate employee. Sanger initially brought the wiki concept to Wales and suggested it be applied to Nupedia and then, after some initial skepticism, Wales agreed to try it. It was Jimmy Wales, along with other people, who came up with the broader idea of an open-source, collaborative encyclopedia that would accept contributions from ordinary people and it was Wales who invested in it. Wales stated in October 2001 that "Larry had the idea to use Wiki software." Sanger coined the portmanteau "Wikipedia" as the project name. In review, Larry Sanger conceived of a wiki-based encyclopedia as a strategic solution to Nupedia's inefficiency problems. In terms of project roles, Sanger spearheaded and pursued the project as its leader in its first year, and did most of the early work in formulating policies (including "Ignore all rules") and "Neutral point of view" and building up the community. Upon departure in March 2002, Sanger emphasized the main issue was purely the cessation of Bomis' funding for his role, which was not viable part-time, and his changing personal priorities; however, by 2004, the two had drifted apart and Sanger became more critical. Two weeks after the launch of Citizendium, Sanger criticized Wikipedia, describing the latter as "broken beyond repair." By 2005 Wales began to dispute Sanger's role in the project, three years after Sanger left.
In 2005, Wales described himself simply as the founder of Wikipedia; however, according to Brian Bergstein of the Associated Press, "Sanger has long been cited as a co-founder." There is evidence that Sanger was called co-founder, along with Wales, as early as 2001, and he is referred to as such in early Wikipedia press releases and Wikipedia articles and in a September 2001 "New York Times" article for which both were interviewed. In 2006, Wales said, "He used to work for me [...] I don't agree with calling him a co-founder, but he likes the title"; nonetheless, before January 2004, Wales did not dispute Sanger's status as co-founder and, indeed, identified himself as "co-founder" as late as August 2002. In Sanger's introductory message to the Nupedia mailing list, he said that "Jimmy Wales contacted me and asked me to apply as editor-in-chief of Nupedia. Apparently, Bomis, Inc. (which owns Nupedia)... who could manage this sort of long-term project, he thought I would be perfect for the job. This is indeed my dream job". Sanger said "He had had the idea for Nupedia since at least last fall".
As of March 2007: Wales emphasized this employer–employee relationship and his ultimate authority, terming himself Wikipedia's sole founder; and Sanger emphasized their statuses as co-founders, referencing earlier versions of Wikipedia pages (2004, 2006), press releases (2002–2004), and media coverage from the time of his involvement routinely terming them in this manner.
Controversies.
In a separate but similar incident, the campaign manager for Cathy Cox, Morton Brilliant, resigned after being found to have added negative information to the Wikipedia entries of political opponents. Following media publicity, the incidents tapered off around August 2006.
Notable forks and derivatives.
There are a number of . No list of sites using the software is maintained,
A significant number of sites use the MediaWiki software and concept, popularized by Wikipedia.
Specialized foreign language forks using the Wikipedia concept include Enciclopedia Libre (Spanish), "Wikiweise" (German), WikiZnanie (Russian), Susning.nu (Swedish), and Baidu Baike (Chinese). Some of these (such as "Enciclopedia Libre") use GFDL or compatible licenses as used by Wikipedia, leading to exchange of material with their respective language Wikipedias.
In 2006, Larry Sanger founded Citizendium, based upon a modified version of MediaWiki. The site cited its aims were 'to improve on the Wikipedia model with "gentle expert oversight", among other things'. (See also Nupedia).
Publication on other media.
The German Wikipedia was the first to be partly published also using other media (rather than online on the internet), including releases on CD in November 2004 and more extended versions on CDs or DVD in April 2005 and December 2006. In December 2005, the publisher Zenodot Verlagsgesellschaft mbH, a sister company of Directmedia, published a 139-page book explaining Wikipedia, its history and policies, which was accompanied by a 7.5 GB DVD containing 300,000 articles and 100,000 images from the German Wikipedia. Originally, Directmedia also announced plans to print the German Wikipedia in its entirety, in 100 volumes of 800 pages each. Publication was due to begin in October 2006, and finish in 2010. In March 2006, however, this project was called off.
In September 2008, Bertelsmann published a 1000 pages volume with a selection of popular German Wikipedia articles. Bertelsmann paid voluntarily 1 Euro per sold copy to Wikimedia Deutschland.
The first CD version containing a selection of articles from the English Wikipedia was published in April 2006 by as the "2006 Wikipedia CD Selection". In April 2007, "Wikipedia Version 0.5", a CD containing around 2000 articles selected from the online encyclopedia was published by the Wikimedia Foundation and Linterweb. The selection of articles included was based on both the quality of the online version and the importance of the topic to be included. This CD version was created as a test-case in preparation for a DVD version including far more articles. The CD version can be purchased online, downloaded as a DVD image file or , or accessed online at the project's website.
A free software project has also been launched to make a static version of Wikipedia available for use on iPods. The "Encyclopodia" project was started around March 2006 and can currently be used on 1st to 4th generation iPods.
Lawsuits.
In limited ways, the Wikimedia Foundation is protected by Section 230 of the Communications Decency Act. In the defamation action "Bauer et al. v. Glatzer et al.", it was held that Wikimedia had no case to answer because of this section. A similar law in France caused a lawsuit to be dismissed in October 2007. In 2013, a German appeals court (the Higher Regional Court of Stuttgart) ruled that Wikipedia is a "service provider" not a "content provider", and as such is immune from liability as long as it takes down content that is accused of being illegal.
External links.
Wikipedia records and archives.
Historical summaries
Size and statistics
Discussion and debate archives
Other
Sister projects

</doc>
<doc id="14073" url="https://en.wikipedia.org/wiki?curid=14073" title="Hydropower">
Hydropower

Hydropower or water power (from the , "water" ) is power derived from the energy of falling water or fast running water, which may be harnessed for useful purposes. Since ancient times, hydropower from many kinds of watermills has been used as a renewable energy source for irrigation and the operation of various mechanical devices, such as gristmills, sawmills, textile mills, trip hammers, dock cranes, domestic lifts, and ore mills. A trompe, which produces compressed air from falling water, is sometimes used to power other machinery at a distance.
In the late 19th century, hydropower became a source for generating electricity. Cragside in Northumberland was the first house powered by hydroelectricity in 1878 and the first commercial hydroelectric power plant was built at Niagara Falls in 1879. In 1881, street lamps in the city of Niagara Falls were powered by hydropower.
Since the early 20th century, the term has been used almost exclusively in conjunction with the modern development of hydroelectric power. International institutions such as the World Bank view hydropower as a means for economic development without adding substantial amounts of carbon to the atmosphere,
but in some cases dams environmental issues.
History.
In India, water wheels and watermills were built; in Imperial Rome, water powered mills produced flour from grain, and were also used for sawing timber and stone; in China, watermills were widely used since the Han dynasty In China and the rest of the Far East, hydraulically operated "pot wheel" pumps raised water into crop or irrigation canals.
The power of a wave of water released from a tank was used for extraction of metal ores in a method known as hushing. The method was first used at the Dolaucothi Gold Mines in Wales from 75 AD onwards, but had been developed in Spain at such mines as Las Médulas. Hushing was also widely used in Britain in the Medieval and later periods to extract lead and tin ores. It later evolved into hydraulic mining when used during the California Gold Rush.
In the Middle Ages, Islamic mechanical engineer Al-Jazari described designs for 50 devices, many of them water powered in his book, "The Book of Knowledge of Ingenious Mechanical Devices", including devices to serve wine, clocks and five devices lift water from rivers or pools, though three are animal-powered and one can be powered by animal or water. These include an endless belt with jugs attached, a cow-powered shadoof and a reciprocating device with hinged valves.
In 1753, French engineer Bernard Forest de Bélidor published "Architecture Hydraulique" which described vertical- and horizontal-axis hydraulic machines. By the late nineteenth century, the electric generator was developed by a team led by project managers and prominent pioneers of renewable energy Jacob S. Gibbs and Brinsley Coleberd and could now be coupled with hydraulics. The growing demand for the Industrial Revolution would drive development as well.
At the beginning of the Industrial Revolution in Britain, water was the main source of power for new inventions such as Richard Arkwright's water frame. Although the use of water power gave way to steam power in many of the larger mills and factories, it was still used during the 18th and 19th centuries for many smaller operations, such as driving the bellows in small blast furnaces (e.g. the Dyfi Furnace) and gristmills, such as those built at Saint Anthony Falls, which uses the 50-foot (15 m) drop in the Mississippi River.
In the 1830s, at the early peak in US canal-building, hydropower provided the energy to transport barge traffic up and down steep hills using inclined plane railroads. As railroads overtook canals for transportation, canal systems were modified and developed into hydropower systems; the history of Lowell, Massachusetts is a classic example of commercial development and industrialization, built upon the availability of water power.
Technological advances had moved the open water wheel into an enclosed turbine or water motor. In 1848 James B. Francis, while working as head engineer of Lowell's Locks and Canals company, improved on these designs to create a turbine with 90% efficiency. He applied scientific principles and testing methods to the problem of turbine design. His mathematical and graphical calculation methods allowed confident design of high efficiency turbines to exactly match a site's specific flow conditions. The Francis reaction turbine is still in wide use today. In the 1870s, deriving from uses in the California mining industry, Lester Allan Pelton developed the high efficiency Pelton wheel impulse turbine, which utilized hydropower from the high head streams characteristic of the mountainous California interior.
Hydraulic power-pipe networks.
Hydraulic power networks used pipes to carrying pressurized water and transmit mechanical power from the source to end users. The power source was normally a head of water, which could also be assisted by a pump. These were extensive in Victorian cities in the United Kingdom. A hydraulic power network was also developed in Geneva, Switzerland. The world famous Jet d'Eau was originally designed as the over-pressure relief valve for the network.
Compressed air hydro.
Where there is a plentiful head of water it can be made to generate compressed air directly without moving parts. In these designs, a falling column of water is purposely mixed with air bubbles generated through turbulence or a venturi pressure reducer at the high level intake. This is allowed to fall down a shaft into a subterranean, high-roofed chamber where the now-compressed air separates from the water and becomes trapped. The height of the falling water column maintains compression of the air in the top of the chamber, while an outlet, submerged below the water level in the chamber allows water to flow back to the surface at a lower level than the intake. A separate outlet in the roof of the chamber supplies the compressed air. A facility on this principle was built on the Montreal River at Ragged Shutes near Cobalt, Ontario in 1910 and supplied 5,000 horsepower to nearby mines.
Hydropower types.
Hydropower is used primarily to generate electricity. Broad categories include:
Calculating the amount of available power.
A hydropower resource can be evaluated by its available power. Power is a function of the hydraulic head and rate of fluid flow. The head is the energy per unit weight (or unit mass) of water. The static head is proportional to the difference in height through which the water falls. Dynamic head is related to the velocity of moving water. Each unit of water can do an amount of work equal to its weight times the head.
The power available from falling water can be calculated from the flow rate and density of water, the height of fall, and the local acceleration due to gravity.
In SI units, the power is:
formula_1
where
To illustrate, power is calculated for a turbine that is 85% efficient, with water at 1000 kg/cubic metre (62.5 pounds/cubic foot) and a flow rate of 80 cubic-meters/second (2800 cubic-feet/second), gravity of 9.81 metres per second squared and with a net head of 145 m (480 ft).
In SI units:
In English units, the density is given in pounds per cubic foot so acceleration due to gravity is inherent in the unit of weight. A conversion factor is required to change from foot lbs/second to kilowatts:
Operators of hydroelectric stations will compare the total electrical energy produced with the theoretical potential energy of the water passing through the turbine to calculate efficiency. Procedures and definitions for calculation of efficiency are given in test codes such as ASME PTC 18 and IEC 60041. Field testing of turbines is used to validate the manufacturer's guaranteed efficiency. Detailed calculation of the efficiency of a hydropower turbine will account for the head lost due to flow friction in the power canal or penstock, rise in tail water level due to flow, the location of the station and effect of varying gravity, the temperature and barometric pressure of the air, the density of the water at ambient temperature, and the altitudes above sea level of the forebay and tailbay. For precise calculations, errors due to rounding and the number of significant digits of constants must be considered.
Some hydropower systems such as water wheels can draw power from the flow of a body of water without necessarily changing its height. In this case, the available power is the kinetic energy of the flowing water. Over-shot water wheels can efficiently capture both types of energy.
The water flow in a stream can vary widely from season to season. Development of a hydropower site requires analysis of flow records, sometimes spanning decades, to assess the reliable annual energy supply. Dams and reservoirs provide a more dependable source of power by smoothing seasonal changes in water flow. However reservoirs have significant environmental impact, as does alteration of naturally occurring stream flow. The design of dams must also account for the worst-case, "probable maximum flood" that can be expected at the site; a spillway is often included to bypass flood flows around the dam. A computer model of the hydraulic basin and rainfall and snowfall records are used to predict the maximum flood.
Hydropower sustainability.
As with other forms of economic activity, hydropower projects can have both a positive and a negative environmental and social impact, because the construction of a dam and power plant, along with the impounding of a reservoir, creates certain social and physical changes.
A number of tools have been developed to assist projects.
Most new hydropower project must undergo an Environmental and Social Impact Assessment. This provides a base line understand of the pre project conditions, estimates potential impacts and puts in place management plans to avoid, mitigate, or compensate for impacts.
The Hydropower Sustainability Assessment Protocol is another tool which can be used to promote and guide more sustainable hydropower projects. It is a methodology used to audit the performance of a hydropower project across more than twenty environmental, social, technical and economic topics. A Protocol assessment provides a rapid sustainability health check. It does not replace an environmental and social impact assessment (ESIA), which takes place over a much longer period of time, usually as a mandatory regulatory requirement.
The World Commission on Dams final report describes a framework for planning water and energy projects that is intended to protect dam-affected people and the environment, and ensure that the benefits from dams are more equitably distributed.
IFC's Environmental and Social Performance Standards define IFC clients' responsibilities for managing their environmental and social risks.
The World Bank’s safeguard policies are used by the Bank to help identify, avoid, and minimize harms to people and the environment caused by investment projects.
The Equator Principles is a risk management framework, adopted by financial institutions, for determining, assessing and managing environmental and social risk in projects.

</doc>
<doc id="14076" url="https://en.wikipedia.org/wiki?curid=14076" title="Horse breed">
Horse breed

A horse breed is a selectively bred population of domesticated horses, often with pedigrees recorded in a breed registry. However, the term is sometimes used in a very broad sense to define landrace animals, or naturally selected horses of a common phenotype located within a limited geographic region. Depending on definition, hundreds of "breeds" exist today, developed for many different uses. Horse breeds are loosely divided into three categories based on general temperament: spirited "hot bloods" with speed and endurance; "cold bloods," such as draft horses and some ponies, suitable for slow, heavy work; and "warmbloods," developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. 
Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits are usually the result of a combination of natural crosses and artificial selection methods aimed at producing horses for specific tasks. Certain breeds are known for certain talents. For example, Standardbreds are known for their speed in harness racing. Some breeds have been developed through centuries of crossings with other breeds, while others, such as Tennessee Walking Horses and Morgans, developed from a single sire from which all current breed members descend. There are more than 300 horse breeds in the world today.
Origin of breeds.
Modern horse breeds developed in response to a need for "form to function", the necessity to develop certain physical characteristics in order to perform a certain type of work. Thus, powerful but refined breeds such as the Andalusian or the Lusitano developed in the Iberian peninsula as riding horses that also had a great aptitude for dressage, while heavy draft horses such as the Clydesdale and the Shire developed out of a need to perform demanding farm work and pull heavy wagons. Ponies of all breeds originally developed mainly from the need for a working animal that could fulfill specific local draft and transportation needs while surviving in harsh environments. However, by the 20th century, many pony breeds had Arabian and other blood added to make a more refined pony suitable for riding. Other horse breeds developed specifically for light agricultural work, heavy and light carriage and road work, various equestrian disciplines, or simply as pets.
Purebreds and registries.
Horses have been selectively bred since their domestication. Today, there are over 300 breeds of horses in the world. However, the concept of purebred bloodstock and a controlled, written breed registry only became of significant importance in modern times. Today, the standards for defining and registration of different breeds vary. Sometimes purebred horses are called Thoroughbreds, which is incorrect; "Thoroughbred" is a specific breed of horse, while a "purebred" is a horse (or any other animal) with a defined pedigree recognized by a breed registry. 
An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful breeding practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines. Though these pedigrees were originally transmitted via an oral tradition, written pedigrees of Arabian horses can be found that date to the 14th century. In the same period of the early Renaissance, the Carthusian monks of southern Spain bred horses and kept meticulous pedigrees of the best bloodstock; the lineage survives to this day in the Andalusian horse. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the Arabian stallions imported to England from the Middle East that became the foundation stallions for the breed.
Some breed registries have a closed stud book, where registration is based on pedigree, and no outside animals can gain admittance. For example, a registered Thoroughbred or Arabian must have two registered parents of the same breed. 
Other breeds have a partially closed stud book but still allow certain infusions from other breeds. For example, the modern Appaloosa must have at least one Appaloosa parent, but may also have a Quarter Horse, Thoroughbred, or Arabian parent so long as the offspring exhibits appropriate color characteristics. The Quarter Horse normally requires both parents to be registered Quarter Horses, but allows "Appendix" registration of horses with one Thoroughbred parent, and the horse may earn its way to full registration by completing certain performance requirements.
Open stud books exist for horse breeds that either have not yet developed a rigorously defined standard phenotype, or for breeds that register animals who conform to an ideal via the process of passing a studbook selection process. Most of the warmblood breeds used in sport horse disciplines, have open stud books to varying degrees. While pedigree is considered, outside bloodlines are admitted to the registry if the horses meet the set standard for the registry. These registries usually require a studbook selection process involving judging of an individual animal's quality, performance, and conformation before registration is finalized. A few "registries," particularly some color breed registries, are very open and will allow membership of all horses that meet limited criteria, such as coat color and species, regardless of pedigree or conformation.
Breed registries also differ as to their acceptance or rejection of breeding technology. For example, all Jockey Club Thoroughbred registries require that a registered Thoroughbred be a product of a natural mating, so called "live cover". A foal born of two Thoroughbred parents, but by means of artificial insemination or embryo transfer, cannot be registered in the Thoroughbred studbook. On the other hand, since the advent of DNA testing to verify parentage, most breed registries now allow artificial insemination (AI), embryo transfer (ET), or both. The high value of stallions has helped with the acceptance of these techniques because they allow a stallion to breed more mares with each "collection," and greatly reduce the risk of injury during mating. Cloning of horses is highly controversial, and at the present time most mainstream breed registries will not accept cloned horses, though several cloned horses and mules have been produced. Such restrictions have led to legal challenges in the United States, sometime based on state law and sometimes based on antitrust laws.
Hybrids.
Horses can crossbreed with other equine species to produce hybrids. These hybrid types are not breeds, but they resemble breeds in that crosses between certain horse breeds and other equine species produce characteristic offspring. The most common hybrid is the mule, a cross between a "jack" (male donkey) and a mare. A related hybrid, the hinny, is a cross between a stallion and a jenny (female donkey). Most other hybrids involve the zebra (see Zebroid). With rare exceptions, most equine hybrids are sterile and cannot reproduce. A notable exception is hybrid crosses between horses and "Equus ferus przewalskii", commonly known as Przewalski's horse.

</doc>
<doc id="14078" url="https://en.wikipedia.org/wiki?curid=14078" title="Halfbakery">
Halfbakery

Halfbakery is a community-based ideas bank used by people who wish to propose and develop (not always serious) half-baked inventions. It has distinguished itself by minimalism, irreverence, and a cast of regulars whose takes on suggested inventions are often funnier than the original submission.
Usage.
Halfbakery can be read by anyone, but only logged-in users can contribute. A user with an account can submit new "ideas" (the inventions) and add "links" and "annotations" to existing ideas. An account is currently
gained by e-mailing the "bakesperson," an e-mail address provided on the site. Logged in users can also vote "for" or "against" a particular idea. Users are able to edit and delete their ideas, links, annotations, votes, and even their whole account. A few selected users can illustrate ideas, and the illustrations are linked to on the idea's page.
The site is run by the bakesperson, jutta, and a small group of volunteer moderators who can contribute ideas themselves and have rights to delete ideas, annotations and links provided by other users. Moderators, however, must adhere to guidelines and are generally forbidden from deleting the links or annotations of other users. Moderators are unable to see who cast votes or alter votes other than their own.
Ideas.
Halfbakery has a variety of mechanisms for creating and discussing ideas.
An idea is initially created with the following components:
Once created, an idea can be discussed by other users. Users can add annotations, include links to other Halfbaked ideas, and vote on the idea. All user comments, links, and votes can be changed or removed by the users who posted them, and comments and links can also be removed by the idea creator.
Writing style.
The style of writing on the Halfbakery is distinctive. Close attention is paid to grammatical, semantical and spelling mistakes, in contrast to other online forums. Writing is seldom overly formal, but too much slang or contraction is frowned upon. Ideas can be either serious or satirical, but ideas written too formally are not well received. Lack of paragraph breaks usually draws criticism concerning the perceived readability. A common occurrence is a user giving a negative vote, promising to retract it once the offending mistakes have been removed.
Jargon.
There is a lot of Halfbakery jargon, due to its communal nature. Here are a few examples:
In-jokes.
The Halfbakery has a number of in-jokes. These mostly take the form of humorous misspellings, prodigious reference to things, or reference to several of the regular users. A few in-jokes are listed here, although the only way to know all of them is to be a regular user of the Halfbakery for some time.

</doc>
<doc id="14082" url="https://en.wikipedia.org/wiki?curid=14082" title="Horse breeding">
Horse breeding

Horse breeding is reproduction in horses, and particularly the human-directed process of selective breeding of animals, particularly purebred horses of a given breed. Planned matings can be used to produce specifically desired characteristics in domesticated horses. Furthermore, modern breeding management and technologies can increase the rate of conception, a healthy pregnancy, and successful foaling.
Terminology.
The male parent of a horse, a stallion, is commonly known as the "sire" and the female parent, the mare, is called the "dam". Both are genetically important, as each parent provides half of the genetic makeup of the ensuing offspring, called a foal. Contrary to popular misuse, "colt" refers to a young male horse only; "filly" is a young female. Though many horse owners may simply breed a family mare to a local stallion in order to produce a companion animal, most professional breeders use selective breeding to produce individuals of a given phenotype, or breed. Alternatively, a breeder could, using individuals of differing phenotypes, create a new breed with specific characteristics.
A horse is "bred" where it is foaled (born). Thus a foal conceived in England but foaled in the United States is regarded as being bred in the US. In some cases, most notably in the Thoroughbred breeding industry, American- and Canadian-bred horses may also be described by the state or province in which they are foaled. Some breeds denote the country, or state, where conception took place as the origin of the foal.
Similarly, the "breeder", is the person who owned or leased the mare at the time of foaling. That individual may not have had anything to do with the mating of the mare. It is important to review each breed registry's rules to determine which applies to any specific foal.
In the horse breeding industry, the term "half-brother" or "half-sister" only describes horses which have the same dam, but different sires. Horses with the same sire but different dams are simply said to be "by the same sire", and no sibling relationship is implied. "Full" (or "own") siblings have both the same dam and the same sire. The terms paternal half-sibling, and maternal half-sibling are also often used. Three-quarter siblings are horses out of the same dam, and are by sires that are either half-brothers (i.e. same dam) or who are by the same sire.
Thoroughbreds and Arabians are also classified through the "distaff" or direct female line, known as their "family" or "tail female" line, tracing back to their taproot foundation bloodstock or the beginning of their respective stud books. The female line of descent always appears at the bottom of a tabulated pedigree and is therefore often known as the "bottom line". In addition, the maternal grandfather of a horse has a special term: wikt:damsire.
"Linebreeding" technically is the duplication of fourth generation or more distant ancestors. However, the term is often used more loosely, describing horses with duplication of ancestors closer than the fourth generation. It also is sometimes used as a euphemism for the practice of inbreeding, a practice that is generally frowned upon by horse breeders, though used by some in an attempt to fix certain traits.
Estrous cycle of the mare.
The estrous cycle (also spelled oestrous) controls when a mare is sexually receptive toward a stallion, and helps to physically prepare the mare for conception. It generally occurs during the spring and summer months, although some mares may be sexually receptive into the late fall, and is controlled by the photoperiod (length of the day), the cycle first triggered when the days begin to lengthen. The estrous cycle lasts about 19–22 days, with the average being 21 days. As the days shorten, the mare returns to a period when she is not sexually receptive, known as anestrus. Anestrus - occurring in the majority of, but not all, mares - prevents the mare from conceiving in the winter months, as that would result in her foaling during the harshest part of the year, a time when it would be most difficult for the foal to survive.
This cycle contains 2 phases:
Depending on breed, on average, 16% of mares have double ovulations, allowing them to twin, though this does not affect the length of time of estrus or diestrus.
Effects on the reproductive system during the estrous cycle.
Changes in hormone levels can have great effects on the physical characteristics of the reproductive organs of the mare, thereby preparing, or preventing, her from conceiving.
Hormones involved in the estrous cycle, during foaling, and after birth.
The cycle is controlled by several hormones which regulate the estrous cycle, the mare's behavior, and the reproductive system of the mare. The cycle begins when the increased day length causes the pineal gland to reduce the levels of melatonin, thereby allowing the hypothalamus to secrete GnRH.
Breeding and gestation.
While horses in the wild mate and foal in mid to late spring, in the case of horses domestically bred for competitive purposes, especially horse racing and various futurities, it is desirable that they be born as close to January 1 in the northern hemisphere or August 1 in the southern hemisphere as possible, so as to be at an advantage in size and maturity when competing against other horses in the same age group. When an early foal is desired, barn managers will put the mare "under lights" by keeping the barn lights on in the winter to simulate a longer day, thus bringing the mare into estrus sooner than she would in nature. Mares signal estrus and ovulation by urination in the presence of a stallion, raising the tail and revealing the vulva. A stallion, approaching with a high head, will usually nicker, nip and nudge the mare, as well as sniff her urine to determine her readiness for mating.
Once fertilized, the oocyte (egg) remains in the oviduct for approximately 5.5 more days, and then descends into the uterus. The initial single cell combination is already dividing and by the time of entry into the uterus, the egg might have already reached the blastocyst stage.
The gestation period lasts for about eleven months, or about 340 days (normal average range 320–370 days). During the early days of pregnancy, the conceptus is mobile, moving about in the uterus until about day 16 when "fixation" occurs. Shortly after fixation, the embryo proper (so called up to about 35 days) will become visible on trans-rectal ultrasound (about day 21) and a heartbeat should be visible by about day 23. After the formation of the endometrial cups and early placentation is initiated (35–40 days of gestation) the terminology changes, and the embryo is referred to as a fetus. True implantation - invasion into the endometrium of any sort - does not occur until about day 35 of pregnancy with the formation of the endometrial cups, and true placentation (formation of the placenta) is not initiated until about day 40-45 and not completed until about 140 days of pregnancy. The fetus sex can be determined by day 70 of the gestation using ultrasound. Halfway through gestation the fetus is the size of between a rabbit and a beagle. The most dramatic fetal development occurs in the last 3 months of pregnancy when 60% of fetal growth occurs.
Colts are carried on average about 4 days longer than fillies.
Care of the pregnant mare.
Domestic mares receive specific care and nutrition to ensure that they and their foals are healthy. Mares are given vaccinations against diseases such as the Rhinopneumonitis (EHV-1) virus (which can cause abortions) as well as vaccines for other conditions that may occur in a given region of the world. Pre-foaling vaccines are recommended 4–6 weeks prior to foaling to maximize the immunoglobulin content of the colostrum in the first milk. Mares are dewormed a few weeks prior to foaling, as the mare is the primary source of parasites for the foal.
Mares can be used for riding or driving during most of their pregnancy. Exercise is healthy, though should be moderated when a mare is heavily in foal. Exercise in excessively high temperatures has been suggested as being detrimental to pregnancy maintenance during the embryonic period; however ambient temperatures encountered during the research were in the region of 100 degrees F and the same results may not be encountered in regions with lower ambient temperatures.
During the first several months of pregnancy, the nutritional requirements do not increase significantly since the rate of growth of the fetus is very slow. However, during this time, the mare may be provided supplemental vitamins and minerals, particularly if forage quality is questionable. During the last 3–4 months of gestation, rapid growth of the fetus increases the mare's nutritional requirements. Energy requirements during these last few months, and during the first few months of lactation are similar to those of a horse in full training. Trace minerals such as copper are extremely important, particularly during the tenth month of pregnancy, for proper skeletal formation. Many feeds designed for pregnant and lactating mares provide the careful balance required of increased protein, increased calories through extra fat as well as vitamins and minerals. Overfeeding the pregnant mare, particularly during early gestation, should be avoided, as excess weight may contribute to difficulties foaling or fetal/foal related problems.
Foaling.
Mares due to foal are usually separated from other horses, both for the benefit of the mare and the safety of the soon-to-be-delivered foal. In addition, separation allows the mare to be monitored more closely by humans for any problems that may occur while giving birth. In the northern hemisphere a special foaling stall that is large and clutter free is frequently used, particularly by major breeding farms. Originally, this was due in part to a need for protection from the harsh winter climate present when mares foal early in the year, but even in moderate climates, such as Florida, foaling stalls are still common because they allow closer monitoring of mares. Smaller breeders often use a small pen with a large shed for foaling, or they may remove a wall between two box stalls in a small barn to make a large stall. In the milder climates seen in much of the southern hemisphere, most mares foal outside, often in a paddock built specifically for foaling, especially on the larger stud farms. Many stud farms worldwide employ technology to alert human managers when the mare is about to foal, including webcams, closed-circuit television, or assorted types of devices that alert a handler via a remote alarm when a mare lies down in a position to foal.
On the other hand, some breeders, particularly those in remote areas or with extremely large numbers of horses, may allow mares to foal out in a field amongst a herd, but may also see higher rates of foal and mare mortality in doing so.
Most mares foal at night or early in the morning, and prefer to give birth alone when possible. Labor is rapid, often no more than 30 minutes, and from the time the feet of the foal appear to full delivery is often only about 15 to 20 minutes. Once the foal is born, the mare will lick the newborn foal to clean it and help blood circulation. In a very short time, the foal will attempt to stand and get milk from its mother. A foal should stand and nurse within the first hour of life.
To create a bond with her foal, the mare licks and nuzzles the foal, enabling her to distinguish the foal from others. Some mares are aggressive when protecting their foals, and may attack other horses or unfamiliar humans that come near their newborns.
After birth, a foal's navel is dipped in antiseptic to prevent infection, it is sometimes given an enema to help clear the meconium from its digestive tract, and the newborn is monitored to ensure that it stands and nurses without difficulty. While most horse births happen without complications, many owners have first aid supplies prepared and a veterinarian on call in case of a birthing emergency. People who supervise foaling should also watch the mare to be sure that she passes the placenta in a timely fashion, and that it is complete with no fragments remaining in the uterus, where retained fetal membranes could cause a serious inflammatory condition (endometritis) and/or infection. If the placenta is not removed from the stall after it is passed, a mare will often eat it, an instinct from the wild, where blood would attract predators.
Foal care.
Foals develop rapidly, and within a few hours a wild foal can travel with the herd. In domestic breeding, the foal and dam are usually separated from the herd for a while, but within a few weeks are typically pastured with the other horses. A foal will begin to eat hay, grass and grain alongside the mare at about 4 weeks old; by 10–12 weeks the foal requires more nutrition than the mare's milk can supply. Foals are typically weaned at 4–8 months of age, although in the wild a foal may nurse for a year.
How breeds develop.
Beyond the appearance and conformation of a specific type of horse, breeders aspire to improve physical performance abilities. This concept, known as matching "form to function," has led to the development of not only different breeds, but also families or bloodlines within breeds that are specialists for excelling at specific tasks.
For example, the Arabian horse of the desert naturally developed speed and endurance to travel long distances and survive in a harsh environment, and domestication by humans added a trainable disposition to the animal's natural abilities. In the meantime, in northern Europe, the locally adapted heavy horse with a thick, warm coat was domesticated and put to work as a farm animal that could pull a plow or wagon. This animal was later adapted through selective breeding to create a strong but rideable animal suitable for the heavily armored knight in warfare.
Then, centuries later, when people in Europe wanted faster horses than could be produced from local horses through simple selective breeding, they imported Arabians and other oriental horses to breed as an outcross to the heavier, local animals. This led to the development of breeds such as the Thoroughbred, a horse taller than the Arabian and faster over the distances of a few miles required of a European race horse or light cavalry horse. Another cross between oriental and European horses produced the Andalusian, a horse developed in Spain that was powerfully built, but extremely nimble and capable of the quick bursts of speed over short distances necessary for certain types of combat as well as for tasks such as bullfighting.
Later, the people who settled the Americas needed a hardy horse that was capable of working with cattle. Thus, Arabians and Thoroughbreds were crossed on Spanish horses, both domesticated animals descended from those brought over by the Conquistadors, and feral horses such as the Mustangs, descended from the Spanish horse, but adapted by natural selection to the ecology and climate of the west. These crosses ultimately produced new breeds such as the American Quarter Horse and the Criollo of Argentina.
In modern times, these breeds themselves have since been selectively bred to further specialize at certain tasks. One example of this is the American Quarter Horse. Once a general-purpose working ranch horse, different bloodlines now specialize in different events. For example, larger, heavier animals with a very steady attitude are bred to give competitors an advantage in events such as team roping, where a horse has to start and stop quickly, but also must calmly hold a full-grown steer at the end of a rope. On the other hand, for an event known as cutting, where the horse must separate a cow from a herd and prevent it from rejoining the group, the best horses are smaller, quick, alert, athletic and highly trainable. They must learn quickly, have conformation that allows quick stops and fast, low turns, and the best competitors have a certain amount of independent mental ability to anticipate and counter the movement of a cow, popularly known as "cow sense."
Another example is the Thoroughbred. While most representatives of this breed are bred for horse racing, there are also specialized bloodlines suitable as show hunters or show jumpers. The hunter must have a tall, smooth build that allows it to trot and canter smoothly and efficiently. Instead of speed, value is placed on appearance and upon giving the equestrian a comfortable ride, with natural jumping ability that shows bascule and good form.
A show jumper, however, is bred less for overall form and more for power over tall fences, along with speed, scope, and agility. This favors a horse with a good galloping stride, powerful hindquarters that can change speed or direction easily, plus a good shoulder angle and length of neck. A jumper has a more powerful build than either the hunter or the racehorse.
History of horse breeding.
The history of horse breeding goes back millennia. Though the precise date is in dispute, humans could have domesticated the horse as far back as approximately 4500 BCE. However, evidence of planned breeding has a more blurry history.
One of the earliest people known to document the breedings of their horses were the Bedouin of the Middle East, the breeders of the Arabian horse. While it is difficult to determine how far back the Bedouin passed on pedigree information via an oral tradition, there were written pedigrees of Arabian horses by CE 1330. The Akhal-Teke of West-Central Asia is another breed with roots in ancient times that was also bred specifically for war and racing. The nomads of the Mongolian steppes bred horses for several thousand years as well.
The types of horse bred varied with culture and with the times. The uses to which a horse was put also determined its qualities, including smooth amblers for riding, fast horses for carrying messengers, heavy horses for plowing and pulling heavy wagons, ponies for hauling cars of ore from mines, packhorses, carriage horses and many others.
Medieval Europe bred large horses specifically for war, called destriers. These horses were the ancestors of the great heavy horses of today, and their size was preferred not simply because of the weight of the armor, but also because a large horse provided more power for the knight’s lance. Weighing almost twice as much as a normal riding horse, the destrier was a powerful weapon in battle.
On the other hand, during this same time, lighter horses were bred in northern Africa and the Middle East, where a faster, more agile horse was preferred. The lighter horse suited the raids and battles of desert people, allowing them to outmaneuver rather than overpower the enemy. When Middle Eastern warriors and European knights collided in warfare, the heavy knights were frequently outmaneuvered. The Europeans, however, responded by crossing their native breeds with "oriental" type horses such as the Arabian, Barb, and Turkoman horse This cross-breeding led both to a nimbler war horse, such as today's Andalusian horse, but also created a type of horse known as a Courser, a predecessor to the Thoroughbred, which was used as a message horse.
During the Renaissance, horses were bred not only for war, but for haute ecole riding, derived from the most athletic movements required of a war horse, and popular among the elite nobility of the time. Breeds such as the Lipizzan were developed from Spanish-bred horses for this purpose, and also became the preferred mounts of cavalry officers, who were derived mostly from the ranks of the nobility. It was during this time that firearms were developed, and so the light cavalry horse, a faster and quicker war horse, was bred for “shoot and run” tactics rather than the shock action as in the Middle Ages.
After Charles II retook the British throne in 1660, horse racing, which had been banned by Cromwell, was revived. The Thoroughbred was developed 40 years later, bred to be the ultimate racehorse, through the lines of three foundation Arabian stallions.
In the 18th century, James Burnett, Lord Monboddo noted the importance of selecting appropriate parentage to achieve desired outcomes of successive generations. Monboddo worked more broadly in the abstract thought of species relationships and evolution of species. The Thoroughbred breeding hub in Lexington, Kentucky was developed in the late 18th century, and became a mainstay in American racehorse breeding.
The 17th and 18th centuries saw more of a need for fine carriage horses in Europe, bringing in the dawn of the warmblood. The warmblood breeds have been exceptionally good at adapting to changing times, and from their carriage horse beginnings they easily transitioned during the 20th century into a sport horse type. Today’s warmblood breeds, although still used for competitive driving, are more often seen competing in show jumping or dressage.
The Thoroughbred continues to dominate the horseracing world, although its lines have been more recently used to improve warmblood breeds and to develop sport horses.
The predecessor of the American Quarter Horse was developed in the 18th century, mainly for quarter racing (racing ¼ of a mile). The breed was later adapted for work in the west, and “cow sense” was particularly bred for as their use for herding cattle increased. However, because there was also a need for animals suitable for sprint racing, the modern Quarter Horse has two distinct types: the sleeker racing type and the stock horse type. The racing type most resembles the finer-boned ancestors of the first racing Quarter Horses, and the type is still used for ¼-mile races. The stock horse type, used in western events, is bred for a shorter stride, docile temperament, and cow sense.
Horses were needed for heavy draft and carriage work until replaced by the automobile, truck, and tractor. After this time, draft and carriage horse numbers dropped significantly, though light riding horses remained popular for recreational pursuits. Draft horses today are used on a few small farms, but today are seen mainly for pulling and plowing competitions rather than farm work. Heavy harness horses are now used as an outcross with lighter breeds, such as the Thoroughbred, to produce the modern warmblood breeds popular in sport horse disciplines, particularly at the Olympic level.
Deciding to breed a horse.
Breeding a horse is an endeavor where the owner, particularly of the mare, will usually need to invest considerable time and money. For this reason, a horse owner needs to consider several factors, including:
There are value judgements involved in considering whether an animal is suitable breeding stock, hotly debated by breeders. Additional personal beliefs may come into play when considering a suitable level of care for the mare and ensuing foal, the potential market or use for the foal, and other tangible and intangible benefits to the owner.
If the breeding endeavor is intended to make a profit, there are additional market factors to consider, which may vary considerably from year to year, from breed to breed, and by region of the world. In many cases, the low end of the market is saturated with horses, and the law of supply and demand thus allows little or no profit to be made from breeding unregistered animals or animals of poor quality, even if registered.
The minimum cost of breeding for a mare owner includes the stud fee, and the cost of proper nutrition, management and veterinary care of the mare throughout gestation, parturition, and care of both mare and foal up to the time of weaning. Veterinary expenses may be higher if specialized reproductive technologies are used or health complications occur.
Making a profit in horse breeding is often difficult. While some owners of only a few horses may keep a foal for purely personal enjoyment, many individuals breed horses in hopes of making some money in the process.
A rule of thumb is that a foal intended for sale should be worth three times the cost of the stud fee if it were sold at the moment of birth. From birth forward, the costs of care and training are added to the value of the foal, with a sale price going up accordingly. If the foal wins awards in some form of competition, that may also enhance the price.
On the other hand, without careful thought, foals bred without a potential market for them may wind up being sold at a loss, and in a worst-case scenario, sold for "salvage" value—a euphemism for sale to slaughter as horsemeat.
Therefore, a mare owner must consider their reasons for breeding, asking hard questions of themselves as to whether their motivations are based on either emotion or profit and how realistic those motivations may be.
Choosing breeding stock.
The stallion should be chosen to complement the mare, with the goal of producing a foal that has the best qualities of both animals, yet avoids having the weaker qualities of either parent. Generally, the stallion should have proven himself in the discipline or sport the mare owner wishes for the "career" of the ensuing foal. Mares should also have a competition record showing that they also have suitable traits, though this does not happen as often.
Some breeders consider the quality of the sire to be more important than the quality of the dam. However, other breeders maintain that the mare is the most important parent. Because stallions can produce far more offspring than mares, a single stallion can have a greater overall impact on a breed. However, the mare may have a greater influence on an individual foal because its physical characteristics influence the developing foal in the womb and the foal also learns habits from its dam when young. Foals may also learn the "language of intimidation and submission" from their dam, and this imprinting may affect the foal's status and rank within the herd. Many times, a mature horse will achieve status in a herd similar to that of its dam; the offspring of dominant mares become dominant themselves.
A purebred horse is usually worth more than a horse of mixed breeding, though this matters more in some disciplines than others. The breed of the horse is sometimes secondary when breeding for a sport horse, but some disciplines may prefer a certain breed or a specific phenotype of horse. Sometimes, purebred bloodlines are an absolute requirement: For example, most racehorses in the world must be recorded with a breed registry in order to race.
Bloodlines are often considered, as some bloodlines are known to cross well with others. If the parents have not yet proven themselves by competition or by producing quality offspring, the bloodlines of the horse are often a good indicator of quality and possible strengths and weaknesses. Some bloodlines are known not only for their athletic ability, but could also carry a conformational or genetic defect, poor temperament, or for a medical problem. Some bloodlines are also fashionable or otherwise marketable, which is an important consideration should the mare owner wish to sell the foal.
Horse breeders also consider conformation, size and temperament. All of these traits are heritable, and will determine if the foal will be a success in its chosen discipline. The offspring, or "get", of a stallion are often excellent indicators of his ability to pass on his characteristics, and the particular traits he actually passes on. Some stallions are fantastic performers but never produce offspring of comparable quality. Others sire fillies of great abilities but not colts. At times, a horse of mediocre ability sires foals of outstanding quality.
Mare owners also look into the question of if the stallion is fertile and has successfully "settled" (i.e. impregnated) mares. A stallion may not be able to breed naturally, or old age may decrease his performance. Mare care boarding fees and semen collection fees can be a major cost.
Costs related to breeding.
Breeding a horse can be an expensive endeavor, whether breeding a backyard competition horse or the next Olympic medalist. Costs may include:
Stud fees are determined by the quality of the stallion, his performance record, the performance record of his get (offspring), as well as the sport and general market that the animal is standing for.
The highest stud fees are generally for racing Thoroughbreds, which may charge from two to three thousand dollars for a breeding to a new or unproven stallion, to several hundred thousand dollars for a breeding to a proven producer of stakes winners. Stallions in other disciplines often have stud fees that begin in the range of $1,000 to $3,000, with top contenders who produce champions in certain disciplines able to command as much as $20,000 for one breeding. The lowest stud fees to breed to a grade horse or an animal of low-quality pedigree may only be $100–$200, but there are trade-offs: the horse will probably be unproven, and likely to produce lower-quality offspring than a horse with a stud fee that is in the typical range for quality breeding stock.
As a stallion's career, either performance or breeding, improves, his stud fee tends to increase in proportion. If one or two offspring are especially successful, winning several stakes races or an Olympic medal, the stud fee will generally greatly increase. Younger, unproven stallions will generally have a lower stud fee earlier on in their careers.
To help decrease the risk of financial loss should the mare die or abort the foal while pregnant, many studs have a live foal guarantee (LFG) - also known as "no foal, free return" or "NFFR" - allowing the owner to have a free breeding to their stallion the next year. However, this is not offered for every breeding.
Covering the mare.
There are two general ways to "cover" or breed the mare:
After the mare is bred or artificially inseminated, she is checked using ultrasound 14–16 days later to see if she “took”, and is pregnant. A second check is usually performed at 28 days. If the mare is not pregnant, she may be bred again during her next cycle.
It is considered safe to breed a mare to a stallion of much larger size. Because of the mare’s type of placenta and its attachment and blood supply, the foal will be limited in its growth within the uterus to the size of the mare's uterus, but will grow to its genetic potential after it is born. Test breedings have been done with draft horse stallions bred to small mares with no increase in the number of difficult births.
Live cover.
When breeding live cover, the mare is usually boarded at the stud. She may be "teased" several times with a stallion that will not breed to her, usually with the stallion being presented to the mare over a barrier. Her reaction to the teaser, whether hostile or passive, is noted. A mare that is in heat will generally tolerate a teaser (although this is not always the case), and may present herself to him, holding her tail to the side. A veterinarian may also determine if the mare is ready to be bred, by ultrasound or palpating daily to determine if ovulation has occurred. Live cover can also be done in liberty on a paddock or on pasture, although due to safety and efficacy concerns, it is not common at professional breeding farms.
When it has been determined that the mare is ready, both the mare and intended stud will be cleaned. The mare will then be presented to the stallion, usually with one handler controlling the mare and one or more handlers in charge of the stallion. Multiple handlers are preferred, as the mare and stallion can be easily separated should there be any trouble.
The Jockey Club, the organization that oversees the Thoroughbred industry in the United States, requires all registered foals to be bred through live cover. Artificial insemination, listed below, is not permitted. Similar rules apply in other countries.
By contrast, the U.S. standardbred industry allows registered foals to be bred by live cover, or by artificial insemination (AI) with fresh or frozen (not dried) semen. No other artificial fertility treatment is allowed. In addition, foals bred via AI of frozen semen may only be registered if the stallion's sperm was collected during his lifetime, and used no later than the calendar year of his death or castration.
Artificial insemination.
Artificial insemination (AI) has several advantages over live cover, and has a very similar conception rate:
A stallion is usually trained to mount a phantom (or dummy) mare, although a live mare may be used, and he is most commonly collected using an artificial vagina (AV) which is heated to simulate the vagina of the mare. The AV has a filter and collection area at one end to capture the semen, which can then be processed in a lab. The semen may be chilled or frozen and shipped to the mare owner or used to breed mares "on-farm". When the mare is in heat, the person inseminating introduces the semen directly into her uterus using a syringe and pipette.
Advanced reproductive techniques.
Often an owner does not want to take a valuable competition mare out of training to carry a foal. This presents a problem, as the mare will usually be quite old by the time she is retired from her competitive career, at which time it is more difficult to impregnate her. Other times, a mare may have physical problems that prevent or discourage breeding. However, there are now several options for breeding these mares. These options also allow a mare to produce multiple foals each breeding season, instead of the usual one. Therefore, mares may have an even greater value for breeding.

</doc>
<doc id="14084" url="https://en.wikipedia.org/wiki?curid=14084" title="Heterosexuality">
Heterosexuality

Heterosexuality is romantic attraction, sexual attraction or sexual behavior between persons of the opposite sex or gender. As a sexual orientation, heterosexuality is "an enduring pattern of emotional, romantic, and/or sexual attractions" to persons of the opposite sex; it "also refers to a person's sense of identity based on those attractions, related behaviors, and membership in a community of others who share those attractions."
Along with bisexuality and homosexuality, heterosexuality is one of the three main categories of sexual orientation within the heterosexual–homosexual continuum. Someone who is heterosexual is commonly referred to as "straight."
The term "heterosexual" or "heterosexuality" is usually applied to humans, but heterosexual behavior is observed in all mammals and in other animals.
Language.
Etymology.
"Hetero-" comes from the Greek word "έτερος" [héteros], meaning "other party" or "another", used in science as a prefix meaning "different"; and the Latin word for sex (that is, characteristic sex or sexual differentiation). The term ""heterosexual"" was first published in 1892 in C.G. Chaddock's translation of Krafft-Ebing's "Psychopathia Sexualis". The noun came into use from the early 1920s, but did not enter common use until the 1960s. The colloquial shortening "hetero" is attested from 1933. The abstract noun "heterosexuality" is first recorded in 1900. The word ""heterosexual"" was first listed in Merriam-Webster's "New International Dictionary" as a medical term for "morbid sexual passion for one of the opposite sex"; however, in 1934 in their "Second Edition Unabridged" it is defined as a "manifestation of sexual passion for one of the opposite sex; normal sexuality". 
The adjective "heterosexual" is used for intimate relationships or sexual relations between male and female.
Terminology.
The current use of the term "heterosexual" has its roots in the broader 19th century tradition of personality taxonomy. It continues to influence the development of the modern concept of sexual orientation, and can be used to describe individuals' sexual orientation, sexual history, or self-identification. Some reject the term "heterosexual" as the word only refers to one's sexual behavior and does not refer to non-sexual romantic feelings. The term ""heterosexual"" is suggested to have come into use as a neologism after, and opposite to the word "homosexual" by Karl Maria Kertbeny in 1868. In LGBT slang, the term "breeder" has been used as a denigrating phrase to deride heterosexuals. Hyponyms of heterosexual include "heteroflexible".
The word can be informally shortened to "hetero". The term ""straight"" originated as a mid-20th century gay slang term for heterosexuals, ultimately coming from the phrase ""to go straight"" (as in "straight and narrow"), or stop engaging in homosexual sex. One of the first uses of the word in this way was in 1941 by author G. W. Henry. Henry's book concerned conversations with homosexual males and used this term in connection with the reference to "ex-gays". It currently simply is a colloquial term for "heterosexual" having, like many words, changed in primary meaning over time. Some object to usage of the term ""straight"" because it implies that non-heteros are crooked.
Symbolism.
Heterosexual symbolism dates back to the earliest artifacts of humanity, with ritual fertility carvings and primitive art. This was later expressed in the symbolism of fertility rites and polytheistic worship, which often included images of human reproductive organs, such as lingam in Hinduism. Modern symbols of heterosexuality in societies derived from European traditions still reference symbols used in these ancient beliefs. One such image is a combination of the symbol for Mars, the Roman god of war, as the definitive male symbol of masculinity, and Venus, the Roman goddess of love and beauty, as the definitive female symbol of femininity. The unicode character for this combined symbol is ⚤ (U+26A4).
Religious aspects.
The Judeo-Christian tradition has several scriptures related to heterosexuality. In Genesis 2:24, there is a commandment stating "Therefore shall a man leave his father and his mother, and shall cleave unto his wife: and they shall be one flesh" () In 1 Corinthians, Christians are advised:
For the most part, religious traditions in the world reserve marriage to heterosexual unions, but there are exceptions including certain Buddhist and Hindu traditions, Unitarian Universalist, Metropolitan Community Church and some Anglican dioceses and some Quaker, United Church of Canada and Reform and Conservative Jewish congregations.
Almost all religions believe that lawful sex between a man and a woman is allowed, but there are a few that believe that it is a sin, such as The Shakers, The Harmony Society, and The Ephrata Cloister. These religions tend to view all sexual relations as sinful, and promote celibacy. Other religions view heterosexual relationships as being inferior to celibacy. Some religions require celibacy for certain roles, such as Catholic priests; however, the Catholic Church also views heterosexual marriage as sacred and necessary.
Demographics.
The demographics of sexual orientation are difficult to establish due to a lack of reliable data. However, the history of human sexuality shows that attitudes and behavior have varied across societies. According to major studies, 89% to 98% of people have had only heterosexual contact within their lifetime; but this percentage falls to 79–84% when either or both same-sex attraction and behavior are reported. In a 2006 study, 80% of respondents anonymously reported heterosexual feelings, although 97–98% identified themselves as heterosexual. A 1992 study reported that 93.9% of males in Britain have always had heterosexual experience, while in France the number was reported at 95.9%.
In the United States, according to a "The Williams Institute" report in April 2011, 96% or approximately 250 million of the adult population are heterosexual.
Polling.
According to a 2008 poll, 85% of Britons have only opposite-sex sexual contact while only 94% of Britons identify themselves as heterosexual. Similarly, a survey by the UK Office for National Statistics (ONS) in 2010 found that 95% of Britons identified as heterosexual, 1.5% of Britons identified themselves as homosexual or bisexual, and the last 3.5% gave more vague answers such as "don't know", "other", or did not respond to the question.
An October 2012 Gallup poll provided unprecedented demographic information about those who identify as heterosexual, arriving at the conclusion that 96.6%, with a margin of error of ±1%, of all U.S. adults identify as heterosexual.
In a 2015 Yougov survey of 1,632 adults of the United Kingdom, 88.7% identified as heterosexual, 5.5% as homosexual and 2.1% as bisexual. Asked to place themselves on the Kinsey scale, 72% of all adults, and 46% of adults aged 18–24 years, picked a score of zero, meaning that they identify as totally heterosexual. 4% of the total sample, and 6% of young adults, picked a score of six, meaning a totally homosexual identity.
In another Yougov survey of 1,000 adults of the United States, 89% of the sample identified as heterosexual, 4% as homosexual (among 2% as homosexual male and 2% as homosexual female) and 4% as bisexual (of either sex).
Academic study.
Biological and environmental.
The relationship between biology and sexual orientation is a subject of research. No simple and singular determinant for sexual orientation has been conclusively demonstrated; various studies point to different, even conflicting positions, but scientists hypothesize that a combination of genetic, hormonal, and social factors determine sexual orientation. Biological theories for explaining the causes of sexual orientation are more popular, and biological factors may involve a complex interplay of genetic factors and the early uterine environment, or biological and social factors. These factors, which may be related to the development of heterosexual or other orientation, include genes, prenatal hormones, and brain structure and their interaction with the environment.
Prenatal hormonal theory.
The neurobiology of the masculinization of the brain is fairly well understood. Estradiol and testosterone, which is catalyzed by the enzyme 5α-reductase into dihydrotestosterone, act upon androgen receptors in the brain to masculinize it. If there are few androgen receptors (people with androgen insensitivity syndrome) or too much androgen (females with congenital adrenal hyperplasia), there can be physical and psychological effects. It has been suggested that both male and female heterosexuality are results of variation in this process. In these studies heterosexuality in females is linked to a lower amount of masculinization than is found in lesbian females, though when dealing with male heterosexuality there are results supporting both higher and lower degrees of masculinization than homosexual males.
Heterosexual behaviors in animals.
Most sexual reproduction in the animal world is facilitated through opposite-sex sexual activity, although there are also animals that reproduce asexually, including protozoa and lower invertebrates.
Reproductive sex does not necessarily require a heterosexual orientation, since orientation refers to a long-term enduring pattern of sexual and emotional attraction leading often to long-term social bonding, while reproductive sex requires only the basic act of intercourse only to fertile the ovum by sperm, often done one time only.
Behavioral studies.
At the beginning of the 20th century, early theoretical discussions in the field of psychoanalysis posited original bisexuality in human psychological development. Quantitative studies by Alfred Kinsey in the 1940s and Dr. Fritz Klein's sexual orientation grid in the 1980s find distributions similar to those postulated by their predecessors.
According to "Sexual Behavior in the Human Male" by Alfred Kinsey and several other modern studies, the majority of humans have had both heterosexual and homosexual experiences or sensations and are bisexual. Kinsey himself, along with current sex therapists, focused on the historicity and fluidity of sexual orientation. Kinsey's studies consistently found sexual orientation to be something that evolves in many directions over a person's lifetime; rarely, but not necessarily, including forming attractions to a new sex. Rarely do individuals radically reorient their sexualities rapidly—and still less do they do so volitionally—but often sexualities expand, shift, and absorb new elements over decades. For example, socially normative "age-appropriate" sexuality requires a shifting object of attraction (especially in the passage through adolescence). Contemporary queer theory, incorporating many ideas from social constructionism, tends to look at sexuality as something that has meaning only within a given historical framework. Sexuality, then, is seen as a participation in a larger social discourse and, though in some sense fluid, not as something strictly determinable by the individual.
Other studies have disputed Kinsey's methodology. "His figures were undermined when it was revealed that he had disproportionately interviewed homosexuals and prisoners (many sex offenders)."
Sexologists have attributed discrepancies in some findings to negative societal attitudes towards a particular sexual orientation. For example, people may state different sexual orientations depending on whether their immediate social environment is public or private. Reluctance to disclose one's actual sexual orientation is often referred to as "being in the closet." Individuals capable of enjoyable sexual relations with both sexes or one sex may feel inclined to restrict themselves to heterosexual or homosexual relations in societies that stigmatize same-sex or opposite-sex relations.
Nature and nurture.
The considerable "nature and nurture" debate exists over whether predominantly biological or psychological factors produce sexual orientation in humans, or whether both significantly factor into sexual orientation. Candidate factors include genes, the exposure of fetuses to certain hormones (or lack thereof) and environmental factors.
Critique of studies.
The studies performed in order to find the origin of sexual orientation have been criticized for being too limited in scope, mostly for focusing only on heterosexuality and homosexuality as two diametrically opposite poles with no orientation in between. It is also asserted that scientific studies focus too much on the search for a biological explanation for sexual orientation, and not enough on the combined effects of both biology and psychology.
In a brief by the Council for Responsible Genetics, it was stated that sexual orientation is not fixed either way, and on the discourse over sexual orientation: "Noticeably missing from this debate is the notion, championed by Kinsey, that human sexual expression is as variable among people as many other complex traits. Yet just like intelligence, sexuality is a complex human feature that modern science is attempting to explain with genetics. Research on brain size, hormone levels, finger length, and other biological traits have yet to yield evidence for this, however. It is important to note that traits such as these result from a combination of gene expression and developmental and other environmental factors. Well-known biologist and social theorist, Anne Fausto-Sterling advocates in her book Sexing the Body, for what scientists term a “systems approach” to be applied to our understanding of sexual preference. Rather than determining that this results from purely biological processes, a trait evolves from developmental processes that include both biological and social elements." According to the American Psychological Association (APA), there are numerous theories about the origins of a person's sexual orientation, but some believe that "sexual orientation is most likely the result of a complex interaction of environmental, cognitive and biological factors," and that genetic factors play a "significant role" in determining a person's sexuality.
Sexual fluidity.
Often, sexual orientation and sexual orientation identity are not distinguished, which can impact accurately assessing sexual identity and whether or not sexual orientation is able to change; sexual orientation identity can change throughout an individual's life, and may or may not align with biological sex, sexual behavior or actual sexual orientation. While the Centre for Addiction and Mental Health and American Psychiatric Association state that sexual orientation is innate, continuous or fixed throughout their lives for some people, but is fluid or changes over time for others, the American Psychological Association distinguishes between sexual orientation (an innate attraction) and sexual orientation identity (which may change at any point in a person's life). 
A 2012 study found that 2% of a sample of 2,560 adult participants reported a change of sexual orientation identity after a 10-year period. For men, a change occurred in 0.78% of those who had identified as heterosexual, 9.52% of homosexuals, and 47% of bisexuals. For women, a change occurred in 1.36% of heterosexuals, 63.6% of lesbians, and 64.7% of bisexuals. The researchers suggested that heterosexuality may be a more stable identity because of its normative status.
A 2-year study by Lisa M. Diamond on a sample of 80 non-heterosexual female adolescents (age 16-23) reported that half of the participants had changed sexual-minority identities more than once, one third of them during the 2-year follow-up. Diamond concluded that "although sexual attractions appear fairly stable, sexual identities and behaviors are more fluid."
In a 2004 study, the female subjects (both gay and straight women) became sexually aroused when they viewed heterosexual as well as lesbian erotic films. Among the male subjects, however, the straight men were turned on only by erotic films with women, the gay ones by those with men. The study's senior researcher said that women's sexual desire is less rigidly directed toward a particular sex, as compared with men's, and it is more changeable over time.
Sexual orientation change efforts.
Sexual orientation change efforts are methods that aim to change sexual orientation, used to try to convert homosexual and bisexual people to heterosexuality. Scientists and mental health professionals generally do not believe that sexual orientation is a choice. There are no studies of adequate scientific rigor that conclude that sexual orientation change efforts work to change a person's sexual orientation. Those efforts have been controversial due to tensions between the values held by some faith-based organizations, on the one hand, and those held by LGBT rights organizations and professional and scientific organizations and other faith-based organizations, on the other. The longstanding consensus of the behavioral and social sciences and the health and mental health professions is that homosexuality "per se" is a normal and positive variation of human sexual orientation, and therefore not a mental disorder. 
No major mental health professional organization has sanctioned efforts to change sexual orientation and virtually all of them have adopted policy statements cautioning the profession and the public about treatments that purport to change sexual orientation. These include the American Psychiatric Association, American Psychological Association, American Counseling Association, National Association of Social Workers in the USA, the Royal College of Psychiatrists, and the Australian Psychological Society. The American Psychological Association states that "sexual orientation is not a choice that can be changed at will", and "sexual orientation identity—not sexual orientation—appears to change via psychotherapy, support groups, and life events." The American Psychiatric Association says "individuals maybe become aware at different points in their lives that they are heterosexual, gay, lesbian, or bisexual". While opposing conversion therapy, they encourage gay affirmative psychotherapy and "encourages mental health professionals to avoid misrepresenting the efficacy of sexual orientation change efforts by promoting or promising change in sexual orientation when providing assistance to individuals distressed by their own or others' sexual orientation and concludes that the benefits reported by participants in sexual orientation change efforts can be gained through approaches that do not attempt to change sexual orientation". The American Psychological Association and the Royal College of Psychiatrists expressed concerns that the positions espoused by NARTH are not supported by the science and create an environment in which prejudice and discrimination can flourish.
Social and historical.
Since the 1960s and 1970s, a large body of research has provided evidence and analysis of the extent to which heterosexuality and homosexuality are socially organized and historically changing. This work challenges the assumption that heterosexuality, homosexuality, and sexualities of all varieties, can be understood as primarily biological and psychological phenomena.
A heterosexual couple, a man and woman in an intimate relationship, form the core of a nuclear family.
Many societies throughout history have insisted that a marriage take place before the couple settle down, but enforcement of this rule or compliance with it has varied considerably. In some jurisdictions, when an unmarried man and woman live together long enough, they are deemed to have established a common-law marriage.
Heteronormativity and heterosexism.
Heteronormativity denotes or relates to a world view that promotes heterosexuality as the normal or preferred sexual orientation for people to have. It can assign strict gender roles to males and females. The term was popularized by Michael Warner in 1991. Many gender and sexuality scholars argue that compulsory heterosexuality, a continual and repeating reassertion of hetersoexual norms, is facet of heterosexism. Compulsory heterosexuality is the idea that female heterosexuality is both assumed and enforced by a patriarchal society. Heterosexuality is then viewed as the natural inclination or obligation by both sexes. Consequently, anyone who differs from the normalcy of heterosexuality is deemed deviant or abhorrent.
Heterosexism is a form of bias or discrimination in favor of opposite-sex sexuality and relationships. It may include an assumption that everyone is heterosexual and may involve a varied level of discrimination against gays, lesbians, bisexuals, heteroflexibles, or transgender individuals.
Straight pride is a slogan that arose in the late 1980s and early 1990s and has been used primarily by social conservative groups as a political stance and strategy. The term is described as a response to gay pride adopted by various LGBT groups in the early 1970s or to the accommodations provided to gay pride initiatives.
A heterosexual ally is a heterosexual person who supports equal civil rights for persons with non-heterosexual orientations. Heterosexual allies may also support LGBT social movements.

</doc>
<doc id="14086" url="https://en.wikipedia.org/wiki?curid=14086" title="Hopewell Centre (Hong Kong)">
Hopewell Centre (Hong Kong)

Hopewell Centre () is a , 64-storey skyscraper at 183 Queen's Road East, in Wan Chai, Hong Kong Island in Hong Kong. The tower is the first circular skyscraper in Hong Kong. It is named after Hong Kong-listed property firm Hopewell Holdings Limited, which constructed the building. Hopewell Holdings Limited's headquarters are in the building and its Chief executive officer, Gordon Wu, has his office on the top floor.
Description.
Construction started in 1977 and was completed in 1980. Upon completion, Hopewell Centre surpassed Jardine House as Hong Kong's tallest building. It was also the second tallest building in Asia at the time. It kept its title in Hong Kong until 1989, when the Bank of China Tower was completed. The building is now the 20th tallest building in Hong Kong.
The building uses a circular floor plan. Although the front entrance is on the 'ground floor', commuters are taken through a set of escalators to the 3rd floor lift lobby. Hopewell Centre stands on the slope of a hill so steep that the building has its back entrance on the 17th floor towards Kennedy Road. There is a circular private swimming pool on the roof of the building.
A revolving restaurant located on the 62nd floor, called "Revolving 66", overlooks other tall buildings below and the harbour. It was originally called Revolving 62, but soon changed its name as locals kept calling it Revolving 66. It completes a 360-degree rotation each hour. Passengers take either office lifts (faster) or the scenic lifts (with a view) to the 56/F, where they transfer to smaller lifts up to the 62/F. The restaurant is now named View 62 by Paco Roncero. 
The building comprises several groups of lifts. Lobbies are on the 3rd and 17th floor, and are connected to Queen's Road East and Kennedy Road respectively. A mini-skylobby is on the 56th floor and serves as a transfer floor for diners heading to the 60/F and 62/F restaurants. The building's white 'bumps' between the windows have built in window-washer guide rails.
This skyscraper was the filming location for R&B group Dru Hill's music video for "How Deep Is Your Love," directed by Brett Ratner, who also directed the movie Rush Hour, whose soundtrack features the song. The circular private swimming pool is well visible in this music video. This swimming pool has also featured in an Australian television advertisement by one of that country's major gaming companies, Tattersall's Limited, promoting a weekly lottery competition.

</doc>
<doc id="14089" url="https://en.wikipedia.org/wiki?curid=14089" title="Harwich, Massachusetts">
Harwich, Massachusetts

Harwich is a New England town on Cape Cod, in Barnstable County in the state of Massachusetts in the United States. At the 2010 census it had a population of 12,243. The town is a popular vacation spot, located near the Cape Cod National Seashore. Harwich's beaches are on the Nantucket Sound side of Cape Cod. Harwich has three active harbors. Saquatucket, Wychmere and Allen Harbors are all in Harwich Port. The town of Harwich includes the villages of Pleasant Lake, West Harwich, East Harwich, Harwich Port, Harwich Center, North Harwich and South Harwich.
History.
Harwich was first settled by Europeans in 1670 as part of Yarmouth. The town was officially incorporated in 1694, and originally included the lands of the current town of Brewster. Early industry involved fishing and farming. The town is considered by some to be the birthplace of the cranberry industry, with the first commercial operation opened in 1846. There are still many bogs in the town, although the economy is now more centered on tourism and as a residential community. The town is also the site of the start/finish line of the "Sail Around the Cape", which rounds the Cape counter-clockwise, returning via the Cape Cod Canal.
Attractions.
Since 1976, the town has hosted the annual Harwich Cranberry Festival, noted for its fireworks display, in September.
In the summer, the town is host to the Harwich Mariners of the Cape Cod Baseball League. The Mariners were the 2008 league champions. The team plays at Whitehouse Field.
The Harwich Antique Center West Harwich is a large group shop that features over 40 dealers. They feature Victorian furniture, primitive items, vingate and collectables, ephemera, coins, jewelry, lamps, military items, postcards and much more. 
The Patriot Square Shopping Center in neighboring South Dennis is convenient for residents of North Harwich and West Harwich. The plaza contains a Stop & Shop supermarket and other stores around it. Supermarkets in Harwich include a Shaw's Star Market on the Harwich Port/West Harwich border and another Stop & Shop in East Harwich.
Geography.
According to the United States Census Bureau, the town has a total area of , of which is land and , or 36.97%, is water. The seven villages of Harwich are West Harwich, North Harwich, East Harwich, South Harwich, Harwich Center, Harwich Port and Pleasant Lake. These are also referred to as the Harwiches.
Harwich is on the southern side of Cape Cod, just west of the southeastern corner. It is bordered by Dennis to the west, Brewster to the north, Orleans to the northeast, Chatham to the east, and Nantucket Sound to the south. Harwich is approximately east of Barnstable, east of the Cape Cod Canal, south of Provincetown, and southeast of Boston.
The town shares the largest lake on the Cape, called Long Pond, with the town of Brewster. Long Pond serves as a private airport for planes with the ability to land on water. The village of Pleasant Lake is at the southwest corner of the lake. Numerous other smaller bodies of water dot the town. Sand Pond, a public beach and swimming area, is located off Great Western Road in North Harwich.
The shore is home to several harbors and rivers, including the Herring River, Allens Harbor, Wychmere Harbor, Saquatucket Harbor, and the Andrews River. The town is also the home to the Hawksnest State Park, as well as a marina and several beaches, including two on Long Pond. There are also many beaches in West Harwich and South Harwich.
Demographics.
As of the census of 2000, there were 12,386 people, 5,471 households, and 3,545 families residing in the town. The population density was 588.6 people per square mile (227.3/km²). There were 9,450 housing units at an average density of 449.1 per square mile (173.4/km²). The racial makeup of the town was 95.41% White, 0.71% Black or African American, 0.19% Native American, 0.22% Asian, 0.05% Pacific Islander, 2.03% from other races, and 1.40% from two or more races. 0.96% of the population were Hispanic or Latino of any race.
There were 5,471 households out of which 21.3% had children under the age of 18 living with them, 53.4% were married couples living together, 9.0% had a female householder with no husband present, and 35.2% were non-families. 29.8% of all households were made up of individuals and 16.9% had someone living alone who was 65 years of age or older. The average household size was 2.20 and the average family size was 2.72.
In the town the population was spread out with 18.3% under the age of 18, 4.2% from 18 to 24, 22.1% from 25 to 44, 25.8% from 45 to 64, and 29.6% who were 65 years of age or older. The median age was 49 years. For every 100 females there were 84.5 males. For every 100 females age 18 and over, there were 79.7 males.
The median income for a household in the town was $41,552, and the median income for a family was $51,070. Males had a median income of $38,948 versus $27,439 for females. The per capita income for the town was $23,063. About 2.9% of families and 15.5% of the population were below the poverty line, including 8.4% of those under age 18 and 8.1% of those age 65 or over.
The town of Harwich contains several smaller census-designated places (CDPs) for which the U.S. Census reports more focused geographic and demographic information. The CDPs in Harwich are Harwich Center, Harwich Port (including South Harwich), East Harwich and Northwest Harwich (including West Harwich, North Harwich, and Pleasant Lake).
Government.
Harwich is represented in the Massachusetts House of Representatives as a part of the Fourth Barnstable district, which includes (with the exception of Brewster) all the towns east and north of Harwich on the Cape. The town is represented in the Massachusetts Senate as a part of the Cape and Islands District, which includes all of Cape Cod, Martha's Vineyard and Nantucket except the towns of Bourne, Falmouth, Sandwich and a portion of Barnstable. The town is patrolled by the Second (Yarmouth) Barracks of Troop D of the Massachusetts State Police.
On the national level, Harwich is a part of Massachusetts's 9th congressional district, and is currently represented by William R. Keating. The state's senior member of the United States Senate is Elizabeth Warren, elected in 2012. The junior senator is Ed Markey, elected in 2013.
Harwich is governed by the open town meeting form of government, led by a town administrator and a board of selectmen.
Public and health services.
There are three libraries in the town. The municipal library, the Brooks Free Library in Harwich Center, is the largest and is a member of the Cape Libraries Automated Materials Sharing (CLAMS) library network. There are two smaller non-municipal libraries – the Chase Library on Route 28 in West Harwich at the Dennis town line, and the Harwich Port Library on Lower Bank Street in Harwich Port.
Harwich is the site of the Long Pond Medical Center, which serves the southeastern Cape region.
Harwich has police and fire departments, with one fire and police station headquarters, and Station 2 in East Harwich.
There are post offices in Harwich Port, South Harwich, West Harwich, and East Harwich.
Education.
Harwich's schools are part of the Monomoy Regional School District. Harwich Elementary School serves students from pre-school through fourth grade, Monomoy Regional Middle School which serves both Harwich and its joining town, Chatham. This middle school serves grades 5–8, and Monomoy Regional High School serves grades 9–12 for both Harwich and Chatham. Monomoy's teams are known as the Sharks. Harwich is known for its excellent boys basketball, girls basketball, girls field hockey, softball and baseball teams.
The Lighthouse Charter School recently moved into where the Harwich Cinema building was located.
Harwich is the site of Cape Cod Regional Technical High School, a grades 9–12 high school which serves most of Cape Cod. The town is also home to Holy Trinity PreSchool, a Catholic pre-school which serves pre-kindergarten in West Harwich.
Transportation.
Roadways.
Two of Massachusetts major routes, U.S. Route 6 and Massachusetts Route 28, cross the town. The town has the southern termini of Routes 39 and 124, and a portion of Route 137 passes through the town. Route 39 leads east through East Harwich to Orleans. Route 28 passes through West Harwich and Harwich Port, connecting the towns of Dennis and Chatham. Route 124 leads from Harwich Center to Brewster, and Route 137 cuts through East Harwich leading from Chatham to Brewster.
Cape Cod Rail Trail.
A portion of the Cape Cod Rail Trail, as well as several other bicycle routes, are in town. There is no rail service in town, but the Cape Cod Rail Trail rotary is located in North Harwich near Main Street.
Air travel.
Other than the occasional sea plane landing on the pond, the nearest airport is in neighboring Chatham; the nearest regional service is at Barnstable Municipal Airport; and the nearest national and international air service is at Logan International Airport in Boston.
CCRTA bus connections.
In recent years parts of Cape Cod have introduced bus service, especially during the summer to help cut down on traffic.

</doc>
<doc id="14090" url="https://en.wikipedia.org/wiki?curid=14090" title="Hull classification symbol">
Hull classification symbol

The United States Navy, United States Coast Guard, and United States National Oceanic and Atmospheric Administration (NOAA) use a hull classification symbol (sometimes called hull code or hull number) to identify their ships by type and by individual ship within a type. The system is analogous to the pennant number system that the Royal Navy and other European and Commonwealth navies use.
History.
United States Navy.
The U.S. Navy began to assign unique Naval Registry Identification Numbers to its ships in the 1890s. The system was a simple one in which each ship received a number which was appended to its ship type, fully spelled out, and added parenthetically after the ship's name when deemed necessary to avoid confusion between ships. Under this system, for example, the battleship "Indiana" was USS "Indiana" (Battleship No. 1), the cruiser "Olympia" was USS "Olympia" (Cruiser No. 6), and so on. Beginning in 1907, some ships also were referred to alternatively by single-letter or three-letter codes—for example, USS "Indiana" (Battleship No. 1) could be referred to as USS "Indiana" (B-1) and USS "Olympia" (Cruiser No. 6) could also be referred to as USS "Olympia" (C-6), while USS "Pennsylvania" (Armored Cruiser No. 4) could be referred to as USS "Pennsylvania" (ACR-4). However, rather than replacing it, these codes coexisted and were used interchangeably with the older system until the modern system was instituted on 17 July 1920.
During World War I, the U.S. Navy acquired large numbers of privately owned and commercial ships and craft for use as patrol vessels, mine warfare vessels, and various types of naval auxiliary ships, some of them with identical names. To keep track of them all, the Navy assigned unique identifying numbers to them. Those deemed appropriate for patrol work received section patrol numbers (SP), while those intended for other purposes received "identification numbers", generally abbreviated "Id. No." or "ID;" some ships and craft changed from an SP to an ID number or vice versa during their careers, without their unique numbers themselves changing, and some ships and craft assigned numbers in anticipation of naval service never were acquired by the Navy. The SP/ID numbering sequence was unified and continuous, with no SP number repeated in the ID series or vice versa so that there could not be, for example, both an "SP-435" and an "Id. No 435". The SP and ID numbers were used parenthetically after each boat's or ship's name to identify it; although this system pre-dated the modern hull classification system and its numbers were not referred to at the time as "hull codes" or "hull numbers," it was used in a similar manner to today's system and can be considered its precursor.
United States Revenue Cutter Service and United States Coast Guard.
The United States Revenue Cutter Service, which merged with the United States Lifesaving Service in January 1915 to form the modern Coast Guard, began following the Navy's lead in the 1890s, with its cutters having parenthetical numbers called Naval Registry Identification Numbers following their names, such as (Cutter No. 1), etc. This persisted until the Navy's modern hull classification system's introduction in 1920, which included Coast Guard ships and craft.
United States Coast and Geodetic Survey.
Like the U.S. Navy, the United States Coast and Geodetic Survey – a uniformed seagoing service of the United States Government and a predecessor of the National Oceanic and Atmospheric Administration (NOAA) – adopted a hull number system for its fleet in the 20th century. Its largest vessels, "Category I" oceanographic survey ships, were classified as "ocean survey ships" and given the designation "OSS". Intermediate-sized "Category II" oceanographic survey ships received the designation "MSS" for "medium survey ship," and smaller "Category III" oceanographic survey ships were given the classification "CSS" for "coastal survey ship." A fourth designation, "ASV" for "auxiliary survey vessel," included even smaller vessels. In each case, a particular ship received a unique designation based on its classification and a unique hull number separated by a space rather than a hyphen; for example, the third Coast and Geodetic Survey ship named "Pioneer" was an ocean survey ship officially known as USC&GS "Pioneer" (OSS 31). The Coast and Geodetic Surveys system persisted after the creation of NOAA in 1970, when NOAA took control of the Surveys fleet, but NOAA later changed to its modern hull classification system.
The modern hull classification system.
United States Navy.
The U.S. Navy instituted its modern hull classification system on 17 July 1920, doing away with section patrol numbers, "identification numbers", and the other numbering systems described above. In the new system, all hull classification symbols are at least two letters; for basic types the symbol is the first letter of the type name, doubled, except for aircraft carriers.
The combination of symbol and hull number identify a modern Navy ship uniquely. A heavily modified or re-purposed ship may receive a new symbol, and either retain the hull number or receive a new one. For example, gun cruiser was converted to a gun/missile cruiser, changing the hull number to CAG-1. Also, the system of symbols has changed a number of times both since it was introduced in 1907 and since the modern system was instituted in 1920, so ships' symbols sometimes change without anything being done to the physical ship.
Hull numbers are assigned by classification. Duplication between, but not within, classifications is permitted. Hence, CV-1 was the aircraft carrier and BB-1 was the battleship .
Ship types and classifications have come and gone over the years, and many of the symbols listed below are not presently in use. The Naval Vessel Register maintains an online database of U.S. Navy ships showing which symbols are presently in use.
After World War II and until 1975, the U.S. Navy defined a "frigate" as a type of surface warship larger than a destroyer and smaller than a cruiser; in other navies, such a ship generally was referred to as a "flotilla leader", or "destroyer leader" —hence the U.S. Navy's use of "DL" for "frigate" prior to 1975—while "frigates" in other navies were smaller than destroyers and more like what the U.S. Navy termed a "destroyer escort", "ocean escort", or "DE". The United States Navy 1975 ship reclassification of cruisers, frigates, and ocean escorts brought U.S. Navy classifications into line with other nations' classifications, and at least cosmetically. i.e., in terms of terminology, eliminated the perceived "cruiser gap" with the Soviet Navy by redesignating the former "frigates" as "cruisers".
Military Sealift Command.
If a U.S. Navy ship's hull classification symbol begins with "T-", it is part of the Military Sealift Command, has a primarily civilian crew, and is a United States Naval Ship (USNS) in non-commissioned service – as opposed to a commissioned United States Ship (USS) with an all-military crew.
United States Coast Guard.
If a ship's hull classification symbol begins with "W", it is a ship of the United States Coast Guard. Until 1965, the Coast Guard used U.S. Navy hull classification codes, prepending a "W" to their beginning. In 1965, it retired some of the less mission-appropriate Navy-based classifications and developed new ones of its own, most notably WHEC for "high endurance cutter" and WMEC for "medium endurance cutter".
National Oceanic and Atmospheric Administration.
The National Oceanic and Atmospheric Administration (NOAA), a component of the United States Department of Commerce and one of the uniformed seagoing services of the United States, also uses a hull classification symbol system, which it also calls "hull numbers," for its fleet.
After NOAA took over the former Coast and Geodetic Survey fleet in 1970 along with research vessels of other government agencies, it adopted a new system of ship classification. In its system, the NOAA fleet is divided into two broad categories, research ships and survey ships. The research ships, which include oceanographic and fisheries research vessels, are given hull numbers beginning with "R", while the survey ships, generally hydrographic survey vessels, receive hull numbers beginning with "S". The letter is followed by a three-digit number; the first digit indicates the NOAA "class" (i.e., size) of the vessel, which NOAA assigns based on the ship's gross tonnage and horsepower, while the next two digits combine with the first digit to create a unique three-digit identifying number for the ship.
Generally, each NOAA hull number is written with a space between the letter and the three-digit number, as in, for example, or .
Unlike the Navy, once an older NOAA ship leaves service, a newer one can be given the same hull number; for example, "S 222" was assigned to , then assigned to NOAAS "Thomas Jefferson" (S 222), which entered NOAA service after "Mount Mitchell" was stricken.
United States Navy hull classification codes.
The U.S. Navy's system of alpha-numeric ship designators, and its associated hull numbers, have been for several decades a unique method of categorizing ships of all types: combatants, auxiliaries and district craft. Though considerably changed in detail and expanded over the years, this system remains essentially the same as when formally implemented in 1920. It is a very useful tool for organizing and keeping track of naval vessels, and also provides the basis for the identification numbers painted on the bows (and frequently the sterns) of most U.S. Navy ships.
The ship designator and hull number system's roots extend back to the late 1880s, when ship type serial numbers were assigned to most of the new-construction warships of the emerging "Steel Navy". During the course of the next thirty years, these same numbers were combined with filing codes used by the Navy's clerks to create an informal version of the system that was put in place in 1920. Limited usage of ship numbers goes back even earlier, most notably to the "Jeffersonian Gunboats" of the early 1800s and the "Tinclad" river gunboats of the Civil War Mississippi Squadron.
It is important to understand that hull number letter prefixes are not acronyms, and should not be carelessly treated as abbreviations of ship type classifications. Thus, "DD" does not stand for anything more than "Destroyer". "SS" simply means "Submarine". And "FF" is the post-1975 type code for "Frigate."
The hull classification codes for ships in active duty in the United States Navy are governed under Secretary of the Navy Instruction 5030.8B (SECNAVINST 5030.8B).
Warships.
Warships are designed to participate in combat operations.
The origin of the 2 letter code derives from the need to distinguish various cruiser subtypes.
Aircraft carrier type.
Aircraft carriers are ships designed primarily for the purpose of conducting combat operations by aircraft which engage in attacks against airborne, surface, sub-surface and shore targets. Contrary to popular belief, the "CV" hull classification symbol does not stand for "Carrier Vessel". The "CV" designation was originally derived from cruisers, since aircraft carriers were seen as an extension of the sea control and denial mission of cruisers. The "V" designation for heavier-than-air craft comes from the French verb "voler" (to fly). Since 1935, "CV" has been a two-letter, unitary hull classification symbol meaning "aircraft carrier". Aircraft carriers are designated in two sequences: the first sequence runs from CV-1 USS "Langley" to the very latest ships, and the second sequence, "CVE" for escort carriers, ran from CVE-1 "Long Island" to CVE-127 "Okinawa" before being discontinued.
Surface combatant type.
Surface combatants are ships which are designed primarily to engage enemy forces on the high seas. The primary surface combatants are battleships, cruisers and destroyers. Battleships are very heavily armed and armored; cruisers moderately so; destroyers and smaller warships, less so. Before 1920, ships were called "<type> no. X", with the type fully pronounced. The types were commonly abbreviated in ship lists to "B-X", "C-X", "D-X" et cetera—for example, before 1920, would have been called "USS "Minnesota", Battleship number 22" orally and "USS "Minnesota", B-22" in writing. After 1920, the ship's name would have been both written and pronounced "USS "Minnesota" (BB-22)". In generally decreasing size, the types are:
Submarine type.
Submarines are all self-propelled submersible types (usually started with SS) regardless of whether employed as combatant, auxiliary, or research and development vehicles which have at least a residual combat capability. While some classes, including all diesel-electric submarines, are retired from USN service, non-U.S. navies continue to employ SS, SSA, SSAN, SSB, SSC, SSG, SSM, and SST types. With the advent of new Air Independent Propulsion/Power (AIP) systems, both SSI and SSP are used to distinguish the types within the USN, but SSP has been declared the preferred term. SSK, retired by the USN, continues to be used colloquially and interchangeably with SS for diesel-electric attack/patrol submarines within the USN, and more formally by the Royal Navy and British firms such as Jane's Information Group.
Patrol combatant type.
Patrol combatants are ships whose mission may extend beyond coastal duties and whose characteristics include adequate endurance and sea keeping, providing a capability for operations exceeding 48 hours on the high seas without support. This notably included Brown Water Navy/Riverine Forces during the Vietnam War. Few of these ships are in service today.
Amphibious warfare type.
Amphibious warfare vessels include all ships having organic capability for amphibious warfare and which have characteristics enabling long duration operations on the high seas. There are two classifications of craft: amphibious warfare ships which are built to cross oceans, and landing craft, which are designed to take troops from ship to shore in an invasion.
Ships
Landing Craft
Expeditionary Support.
Operated by Military Sealift Command, have ship prefix "USNS", hull code begins with "T-".
Combat Logistics Type.
Ships which have the capability to provide underway replenishment to fleet units.
Mine warfare type.
Mine warfare ships are those ships whose primary function is mine warfare on the high seas.
Coastal defense type.
Coastal defense ships are those whose primary function is coastal patrol and interdiction.
Mobile logistics type.
Mobile logistics ships have the capability to provide direct material support to other deployed units operating far from home ports.
Auxiliary type.
An auxiliary ship is designed to operate in any number of roles supporting combatant ships and other naval operations.
Support ships.
Support ships are not designed to participate in combat, and are generally not armed. For ships with civilian crews (owned by and/or operated for Military Sealift Command and the Maritime Administration), the prefix T- is placed at the front of the hull classification.
Support type.
Support ships are designed to operate in the open ocean in a variety of sea states to provide general support to either combatant forces or shore based establishments. They include smaller auxiliaries which, by the nature of their duties, leave inshore waters.
Service type craft.
Service craft are navy-subordinated craft (including non-self-propelled) designed to provide general support to either combatant forces or shore-based establishments. The suffix "N" refers to non-self-propelled variants.
United States Coast Guard craft.
Prior to 1965, U.S. Coast Guard ships used the same designation as naval ships, but preceded by a "W" to indicate Coast Guard subordination.
Temporary designations.
United States Navy Designations (Temporary) are a form of U.S. Navy ship designation, intended for temporary identification use. Such designations usually occur during periods of sudden mobilization, such as that which occurred prior to, and during, World War II or the Korean War, when it was determined that a sudden temporary need arose for a ship for which there was no official Navy designation.
During World War II, for example, a number of commercial vessels were requisitioned, or acquired, by the U.S. Navy to meet the sudden requirements of war. A yacht acquired by the U.S. Navy during the start of World War II might seem desirable to the Navy whose use for the vessel might not be fully developed or explored at the time of acquisition.
On the other hand, a U.S. Navy vessel, such as the yacht in the example above, already in commission or service, might be desired, or found useful, for another need or purpose for which there is no official designation.
Numerous other U.S. Navy vessels were launched with a temporary, or nominal, designation, such as YMS or PC, since it could not be determined, at time of construction, what they should be used for. Many of these were vessels in the 150 to 200 feet length class with powerful engines, whose function could be that of a minesweeper, patrol craft, submarine chaser, seaplane tender, tugboat, or other. Once their destiny, or capability, was found or determined, such vessels were reclassified with their actual designation.
National Oceanic and Atmospheric Administration hull codes.
The letter is paired with a three-digit number. The first digit of the number is determined by the ships "power tonnage," defined as the sum of its shaft horsepower and gross international tonnage, as follows:
The second and third digits are assigned to create a unique three-digit hull number.

</doc>
<doc id="14091" url="https://en.wikipedia.org/wiki?curid=14091" title="Habeas corpus">
Habeas corpus

Habeas corpus (; Medieval Latin translating roughly to "You should have the body") is a recourse in law whereby a person can report an unlawful detention or imprisonment before a court, usually through a prison official.
A writ of "habeas corpus" is known as "the great and efficacious writ in all manner of illegal confinement", being a remedy available to the meanest against the mightiest. It is a summons with the force of a court order; it is addressed to the custodian (a prison official for example) and demands that a prisoner be taken before the court, and that the custodian present proof of authority, allowing the court to determine whether the custodian has lawful authority to detain the prisoner. If the custodian is acting beyond his or her authority, then the prisoner must be released. Any prisoner, or another person acting on his or her behalf, may petition the court, or a judge, for a writ of "habeas corpus". One reason for the writ to be sought by a person other than the prisoner is that the detainee might be held incommunicado. Most civil law jurisdictions provide a similar remedy for those unlawfully detained, but this is not always called "habeas corpus". For example, in some Spanish-speaking nations, the equivalent remedy for unlawful imprisonment is the "amparo de libertad" ('protection of freedom').
"Habeas corpus" has certain limitations. Though a writ of right, it is not a writ of course. It is technically only a procedural remedy; it is a guarantee against any detention that is forbidden by law, but it does not necessarily protect other rights, such as the entitlement to a fair trial. So if an imposition such as internment without trial is permitted by the law, then "habeas corpus" may not be a useful remedy. In some countries, the process has been temporarily or permanently suspended, in all of a government's jurisdictions or only some, because of what might be construed by some government institutions as a series of events of such relevance to the government as to warrant a suspension; in more recent times, such an event has been frequently referred to as a state of emergency.
The right to petition for a writ of "habeas corpus" has nonetheless long been celebrated as the most efficient safeguard of the liberty of the subject. The jurist Albert Venn Dicey wrote that the British Habeas Corpus Acts "declare no principle and define no rights, but they are for practical purposes worth a hundred constitutional articles guaranteeing individual liberty".
The writ of "habeas corpus" is one of what are called the "extraordinary", "common law", or "prerogative writs", which were historically issued by the English courts in the name of the monarch to control inferior courts and public authorities within the kingdom. The most common of the other such prerogative writs are "quo warranto", "prohibito", "mandamus", "procedendo", and "certiorari". The due process for such petitions is not simply civil or criminal, because they incorporate the presumption of non-authority. The official who is the respondent must prove his authority to do or not do something. Failing this, the court must decide for the petitioner, who may be any person, not just an interested party. This differs from a motion in a civil process in which the movant must have standing, and bears the burden of proof.
Etymology.
From Latin "habeas", 2nd person singular present subjunctive active of "habere", "to have", "to hold"; and "corpus", accusative singular of "corpus" "body". In reference to more than one person, "habeas corpora".
Literally, the phrase means "you shall have the body". The complete phrase "habeas corpus ad subjiciendum" means "you shall have the person for the purpose of subjecting him/her to (examination)". These are the opening words of writs in 14th century Anglo-French documents requiring a person to be brought before a court or judge, especially to determine if that person is being legally detained.
Similarly named writs.
The full name of the writ is often used to distinguish it from similar ancient writs, also named "habeas corpus". These include:
Origins in England.
Habeas Corpus originally stems from the Assize of Clarendon, a re-issuance of rights during the reign of Henry II of England.
In the 17th century the foundations for "habeas corpus" were "wrongly thought" to have originated in Magna Carta. This charter declared that:
William Blackstone cites the first recorded usage of "habeas corpus ad subjiciendum" in 1305, during the reign of King Edward I. However, other writs were issued with the same effect as early as the reign of Henry II in the 12th century. Blackstone explained the basis of the writ, saying "The King is at all times entitled to have an account, why the liberty of any of his subjects is restrained, wherever that restraint may be inflicted". The procedure for issuing a writ of "habeas corpus" was first codified by the Habeas Corpus Act 1679, following judicial rulings which had restricted the effectiveness of the writ. A previous law (the Habeas Corpus Act 1640) had been passed forty years earlier to overturn a ruling that the command of the King was a sufficient answer to a petition of "habeas corpus".
Then, as now, the writ of "habeas corpus" was issued by a superior court in the name of the Sovereign, and commanded the addressee (a lower court, sheriff, or private subject) to produce the prisoner before the royal courts of law. A "habeas corpus" petition could be made by the prisoner him or herself or by a third party on his or her behalf and, as a result of the Habeas Corpus Acts, could be made regardless of whether the court was in session, by presenting the petition to a judge. Since the 18th century the writ has also been used in cases of unlawful detention by private individuals, most famously in "Somersett's Case" (1772), where the black slave Somersett was ordered to be freed. In that case these famous words are said to have been uttered "The air of England has long been too pure for a slave, and every man is free who breathes it". During the Seven Years' War and later conflicts, the Writ was used on behalf of soldiers and sailors pressed into military and naval service. The Habeas Corpus Act 1816 introduced some changes and expanded the territoriality of the legislation.
The privilege of "habeas corpus" has been suspended or restricted several times during English history, most recently during the 18th and 19th centuries. Although internment without trial has been authorised by statute since that time, for example during the two World Wars and the Troubles in Northern Ireland, the "habeas corpus" procedure has in modern times always technically remained available to such internees. However, as "habeas corpus" is only a procedural device to examine the lawfulness of a prisoner's detention, so long as the detention is in accordance with an Act of Parliament, the petition for "habeas corpus" is unsuccessful. Since the passage of the Human Rights Act 1998, the courts have been able to declare an Act of Parliament to be incompatible with the European Convention on Human Rights, but such a declaration of incompatibility has no legal effect unless and until it is acted upon by the government.
The wording of the writ of "habeas corpus" implies that the prisoner is brought to the court for the legality of the imprisonment to be examined. However, rather than issuing the writ immediately and waiting for the return of the writ by the custodian, modern practice in England is for the original application to be followed by a hearing with both parties present to decide the legality of the detention, without any writ being issued. If the detention is held to be unlawful, the prisoner can usually then be released or bailed by order of the court without having to be produced before it. It is also possible for individuals held by the state to petition for judicial review, and individuals held by non-state entities to apply for an injunction.
With the development of modern public law, applications for habeas corpus have been to some extent discouraged, in favour of applications for judicial review.
The Writ, however, maintains its vigour, and was held by the UK Supreme Court to be available in respect of a prisoner captured by British forces in Afghanistan, albeit that the Secretary of State made a valid return to the Writ justifying the detention of the claimant.
Other jurisdictions.
Australia.
The writ of "habeas corpus" as a procedural remedy is part of Australia's English law inheritance. In 2005, the Australian parliament passed the Australian Anti-Terrorism Act 2005. Some legal experts questioned the constitutionality of the act, due in part to limitations it placed on "habeas corpus".
Canada.
"Habeas corpus" rights are part of the British legal tradition inherited by Canada. The rights exist in the common law but have been enshrined in the Constitution Act 1982, under Section Ten of the Charter of Rights and Freedoms. This states that "Everyone has the right on arrest or detention ... (c) to have the validity of the detention determined by way of "habeas corpus" and to be released if the detention is not lawful".
Suspension of the writ in Canadian history occurred famously during the October Crisis, during which the War Measures Act was invoked by the Governor General of Canada on the constitutional advice of Prime Minister Pierre Trudeau, who had received a request from the Quebec Cabinet. The Act was also used to justify German, Slavic, and Ukrainian Canadian internment during the First World War, and the internment of German-Canadians, Italian-Canadians and Japanese-Canadians during the Second World War. The writ was suspended for several years following the Battle of Fort Erie (1866) during the Fenian Rising, though the suspension was only ever applied to suspects in the Thomas D'Arcy McGee assassination.
The writ is available where there is no other adequate remedy. However, a superior court always has the discretion to grant the writ even in the face of an alternative remedy (see "May v Ferndale Institution"). Under the Criminal Code the writ is largely unavailable if a statutory right of appeal exists, whether or not this right has been exercised.
France.
A fundamental human right in the "1789 Declaration of the Rights of Man" drafted by Lafayette in cooperation with Thomas Jefferson, the guarantees against arbitrary detention are enshrined in the French Constitution and regulated by the Penal Code. The safeguards are equivalent to those found under the Habeas-Corpus provisions found in Germany, the United States and several Commonwealth countries. The French system of accountability prescribes severe penalties for ministers, police officers and civil and judiciary authorities who either violate or fail to enforce the law.
"Article 7 of Declaration also provides that "No individual may be accused, arrested, or detained except where the law so prescribes, and in accordance with the procedure it has laid down"... The Constitution further states that "No one may be arbitrarily detained. The judicial authority, guardian of individual liberty, ensures the observance of this principle under the condition specified by law." Its article 5 provides that everyone has the right to liberty and sets forth permissible circumstances under which people may be deprived of their liberty and procedural safeguards in case of detention. In particular, it states that "anyone deprived of his liberty by arrest or detention shall be entitled to take proceedings by which the lawfulness of his detention shall be decided speedily by a court and his release ordered if the detention is not lawful".
France and the United States played a synergistic role in the international team, led by Eleanor Roosevelt, which crafted the Universal Declaration of Human Rights. The French judge and Nobel Peace Laureate René Cassin produced the first draft and argued against arbitrary detentions. René Cassin and the French team subsequently championed the Habeas-Corpus provisions enshrined in the European Convention for the Protection of Human Rights and Fundamental Freedoms.
Germany.
Germany has constitutional guarantees against improper detention and have been implemented in statutory law in a manner that can be considered as equivalent to writs of habeas corpus.
Article 104, paragraph 1 of the Basic Law for the Federal Republic of Germany provides that deprivations of liberty may be imposed only on the basis of a specific enabling statute that also must include procedural rules. Article 104, paragraph 2 requires that any arrested individual be brought before a judge by the end of the day following the day of the arrest. For those detained as criminal suspects, article 104, paragraph 3 specifically requires that the judge must grant a hearing to the suspect in order to rule on the detention.
Restrictions on the power of the authorities to arrest and detain individuals also emanate from article 2 paragraph 2 of the Basic Law which guarantees liberty and requires a statutory authorization for any deprivation of liberty. In addition, several other articles of the Basic Law have a bearing on the issue. The most important of these are article 19, which generally requires a statutory basis for any infringements of the fundamental rights guaranteed by the Basic Law while also guaranteeing judicial review; article 20, paragraph 3, which guarantees the rule of law; and article 3 which guarantees equality.
In particular, a constitutional obligation to grant remedies for improper detention is required by article 19, paragraph 4 of the Basic Law, which provides as follows: "Should any person's right be violated by public authority, he may have recourse to the courts. If no other jurisdiction has been established, recourse shall be to the ordinary courts."
India.
The Indian judiciary, in a catena of cases, has effectively resorted to the writ of "habeas corpus" to secure release of a person from illegal detention. For example, in October 2009, the Karnataka High Court heard a habeas corpus petition filed by the parents of a girl who married a Muslim boy from Kannur district and was allegedly confined in a "madrasa" in Malapuram town. Usually, in most other jurisdictions, the writ is directed at police authorities. The extension to non-state authorities has its grounds in two cases: the 1898 Queen's Bench case of "Ex Parte Daisy Hopkins", wherein the Proctor of Cambridge University did detain and arrest Hopkins without his jurisdiction, and Hopkins was released and that of "Somerset v Stewart", in which an African slave whose master had moved to London was freed by action of the writ.
The Indian judiciary has dispensed with the traditional doctrine of "locus standi", so that if a detained person is not in a position to file a petition, it can be moved on his behalf by any other person. The scope of "habeas" relief has expanded in recent times by actions of the Indian judiciary.
The habeas writ was used in the Rajan case, a student victim of torture in local police custody during the nationwide Emergency in India in 1976. On 12 March 2014, Subrata Roy's counsel approached the Chief Justice moving a habeas corpus petition. It was also filed by the Panthers Party to protest the imprisonment of Anna Hazare, a social activist.
Ireland.
In the Republic of Ireland, access to a similar remedy to "habeas corpus" is guaranteed by Article 40.4 of the 1937 constitution. This guarantees "personal liberty" to each individual and outlines a detailed procedure. It does not mention the Latin term but includes the English phrase "produce the body". The constitution provides that this procedure is not binding on the Defence Forces during a state of war or armed rebellion.
The term 'habeas corpus' as used in the Rules of the Superior Courts does not refer to the constitutional procedure outlined below, but to provisions still operable of the Habeas Courpus Acts- The State (Ahern) v Cotter IR 188.
The expression 'order of Habeas Corpus' does not include an order made pursuant to Article 40, section 4 of the Constitution. Order 84 r 1(2) RSC.
Article 40.4.2° states that a prisoner, or anyone acting on his behalf, may make a complaint to the High Court (or to any High Court judge) of unlawful detention. The court must then investigate the matter "forthwith" and may order that the defendant bring the prisoner before the court and give reasons for his detention. The court must immediately release the detainee unless it is satisfied that he is being held lawfully. The full text of the provision is as follows:
The state inherited "habeas corpus" as part of the common law when it seceded from the United Kingdom in 1922, but the remedy was also guaranteed by Article 6 of the Constitution of the Irish Free State in force from 1922 to 1937. A similar provision was included when the current constitution was adopted in 1937. Since that date, "habeas corpus" has been restricted by two constitutional amendments, the Second Amendment in 1941 and the Sixteenth Amendment in 1996.
Before the Second Amendment, an individual detained had the constitutional right to apply to any High Court judge for a writ of "habeas corpus" and to as many High Court judges as he wished. Since the Second Amendment, a prisoner has had only the right to apply to one judge, and, once a writ has been issued, the President of the High Court has authority to choose the judge or panel of three judges who will decide the case. The amendment also added a requirement that if the High Court believes someone's detention to be invalid due to the unconstitutionality of a law, it must refer the matter to the Irish Supreme Court and may only release the individual on bail in the interim.
In 1965, the Supreme Court ruled in the "O'Callaghan" case that the provisions of the constitution meant that an individual charged with a crime could be refused bail only if she was likely to flee or to interfere with witnesses or evidence. Since the Sixteenth Amendment, it has been possible for a court to take into account whether a person has committed serious crimes while on bail in the past.
Italy.
In Italy the principle of "habeas corpus" is enshrined in Article 13 of the Constitution, which states:
"Personal liberty is inviolable.
No one may be detained, inspected, or searched nor otherwise subjected to any restriction of personal liberty except by order of the Judiciary stating a reason and only in such cases and in such manner as provided by the law.
In exceptional circumstances and under such conditions of necessity and urgency as shall conclusively be defined by the law, the police may take provisional measures that shall be referred within 48 hours to the Judiciary for validation and which, in default of such validation in the following 48 hours, shall be revoked and considered null and void.
Any act of physical and moral violence against a person subjected to restriction of personal liberty shall be punished.
The law shall establish the maximum duration of preventive detention."
Malaysia.
In Malaysia, the remedy of "habeas corpus" is guaranteed by the federal constitution, although not by name. Article 5(2) of the Constitution of Malaysia provides that "Where complaint is made to a High Court or any judge thereof that a person is being unlawfully detained the court shall inquire into the complaint and, unless satisfied that the detention is lawful, shall order him to be produced before the court and release him".
As there are several statutes, for example, the Internal Security Act 1960, that still permit detention without trial, the procedure is usually effective in such cases only if it can be shown that there was a procedural error in the way that the detention was ordered.
New Zealand.
In New Zealand, "habeas corpus" may be invoked against the government or private individuals. In 2006, a child was allegedly kidnapped by his maternal grandfather after a custody dispute. The father began "habeas corpus" proceedings against the mother, the grandfather, the grandmother, the great grandmother, and another person alleged to have assisted in the kidnap of the child. The mother did not present the child to the court and so was imprisoned for contempt of court. She was released when the grandfather came forward with the child in late January 2007.
Pakistan.
Issuance of a writ is an exercise of an extraordinary jurisdiction of the superior courts in Pakistan. A writ of habeas corpus may be issued by any High Court of a province in Pakistan. Article 199 of the 1973 Constitution of the Islamic Republic of Pakistan, specifically provides for the issuance of a writ of habeas corpus, empowering the courts to exercise this prerogative. Subject to the Article 199 of the Constitution, "A High Court may, if it is satisfied that no other adequate remedy is provided by law, on the application of any person, make an order that a person in custody within the territorial jurisdiction of the Court be brought before it so that the Court may satisfy itself that he is not being held in custody without a lawful authority or in an unlawful manner". The hallmark of extraordinary constitutional jurisdiction is to keep various functionaries of State within the ambit of their authority. Once a High Court has assumed jurisdiction to adjudicate the matter before it, justiciability of the issue raised before it is beyond question. The Supreme Court of Pakistan has stated clearly that the use of words "in an unlawful manner" implies that the court may examine, if a statute has allowed such detention, whether it was a colorable exercise of the power of authority. Thus, the court can examine the malafides of the action taken.
The Philippines.
In the Bill of Rights of the Philippine constitution, "habeas corpus" is guaranteed in terms almost identically to those used in the U.S. Constitution. in Article 3, Section 15 of the Constitution of the Philippines states that "The privilege of the writ of "habeas corpus" shall not be suspended except in cases of invasion or rebellion when the public safety requires it".
In 1971, after the Plaza Miranda bombing, the Marcos administration, under Ferdinand Marcos, suspended "habeas corpus" in an effort to stifle the oncoming insurgency, having blamed the Filipino Communist Party for the events of August 21. Many considered this to be a prelude to Martial Law. After widespread protests, however, the Marcos administration decided to reintroduce the writ. In December 2009, "habeas corpus" was suspended in Maguindanao as the province was placed under martial law. This occurred in response to the Maguindanao massacre.
Scotland.
The Parliament of Scotland passes a law to have the same effect as "habeas corpus" in the 18th century. This now known as the Criminal Procedure Act 1701 c.6. It was originally called "the Act for preventing wrongful imprisonment and against undue delays in trials". It is still in force although certain parts have been repealed.
Spain.
In 1526, the "Fuero Nuevo" established a form of "habeas corpus" in the territory of the "Señorío de Vizcaya". The present Constitution of Spain states that "A "habeas corpus" procedure shall be provided for by law to ensure the immediate handing over to the judicial authorities of any person illegally arrested". The statute which regulates the procedure is the Law of Habeas Corpus of 24 May 1984 which provides that a person imprisoned may, on her or his own or through a third person, allege that she or he is imprisoned unlawfully and request to appear before a judge. The request must specify the grounds on which the detention is considered to be unlawful which can be, for example, that the custodian holding the prisoner does not have the legal authority, that the prisoner's constitutional rights have been violated, or that he has been subjected to mistreatment. The judge may then request additional information if needed and may issue a "habeas corpus" order at which point the custodian has 24 hours to bring the prisoner before the judge.
United States.
The United States inherited "habeas corpus" from the English common law. In England, the writ was issued in the name of the monarch. When the original thirteen American colonies declared independence, and became a republic based on popular sovereignty, any person, in the name of the people, acquired authority to initiate such writs. The U.S. Constitution specifically includes the "habeas" procedure in the Suspension Clause (Clause 2), located in Article One, Section 9. This states that "The privilege of the writ of "habeas corpus" shall not be suspended, unless when in cases of rebellion or invasion the public safety may require it". Section 9 is under Article 1 which states, "legislative Powers herein granted shall be vested in the Congress of the United States ..."
The writ of "habeas corpus ad subjiciendum" is a civil, not criminal, "ex parte" proceeding in which a court inquires as to the legitimacy of a prisoner's custody. Typically, "habeas corpus" proceedings are to determine whether the court that imposed sentence on the defendant had jurisdiction and authority to do so, or whether the defendant's sentence has expired. "Habeas corpus" is also used as a legal avenue to challenge other types of custody such as pretrial detention or detention by the United States Bureau of Immigration and Customs Enforcement pursuant to a deportation proceeding.
Presidents Abraham Lincoln and Ulysses Grant suspended "habeas corpus" during the Civil War and Reconstruction for some places or types of cases. Following the September 11 attacks, President George W. Bush attempted to place Guantanamo Bay detainees outside of the jurisdiction of "habeas corpus", but the Supreme Court of the United States overturned this action in "Boumediene v. Bush".
Equivalent remedies.
Poland.
In 1430, King Władysław II Jagiełło of Poland granted the Privilege of Jedlnia, which proclaimed, "Neminem captivabimus nisi iure victum" ("We will not imprison anyone except if convicted by law"). This revolutionary innovation in civil libertarianism gave Polish citizens due process-style rights that did not exist in any other European country for another 250 years. Originally, the Privilege of Jedlnia was restricted to the nobility (the szlachta), but it was extended to cover townsmen in the 1791 Constitution. Importantly, social classifications in the Polish–Lithuanian Commonwealth were not as rigid as in other European countries; townspeople and Jews were sometimes ennobled. The Privilege of Jedlnia provided broader coverage than many subsequently enacted habeas corpus laws because Poland's nobility constituted an unusually large percentage of the country's total population, which was Europe's largest. As a result, by the 16th century, it was protecting the liberty of between 500 thousand and a million Poles.
Roman-Dutch law.
In South Africa and other countries whose legal systems are based on Roman-Dutch law, the "interdictum de homine libero exhibendo" is the equivalent of the writ of "habeas corpus". In South Africa, it has been entrenched in the Bill of Rights, which provides in section 35(2)(d) that every detained person has the right to challenge the lawfulness of the detention in person before a court and, if the detention is unlawful, to be released.
World "habeas corpus".
In the 1950s, American lawyer Luis Kutner began advocating an international writ of "habeas corpus" to protect individual human rights. In 1952, he filed a petition for a "United Nations Writ of Habeas Corpus" on behalf of William N. Oatis, an American journalist jailed the previous year by the Communist government of Czechoslovakia. Alleging that Czechoslovakia had violated Oatis's rights under the United Nations Charter and the Universal Declaration of Human Rights and that the United Nations General Assembly had "inherent power" to fashion remedies for human rights violations, the petition was filed with the United Nations Commission on Human Rights. The Commission forwarded the petition to Czechoslovakia, but no other United Nations action was taken. Oatis was released in 1953. Kutner went on to publish numerous articles and books advocating the creation of an "International Court of Habeas Corpus".
International Human Rights Standards.
Article 3 of the Universal Declaration of Human Rights provides that "everyone has the right to life, liberty and security of person". Article 5 of the European Convention on Human Rights goes further and calls for persons detained to have the right to challenge their detention, providing at article 5.4:
"Everyone who is deprived of his liberty by arrest or detention shall be entitled to take proceedings by which the lawfulness of his detention shall be decided speedily by a court and his release ordered if the detention is not lawful."

</doc>
<doc id="14092" url="https://en.wikipedia.org/wiki?curid=14092" title="Henry the Navigator">
Henry the Navigator

Infante Henrique of Portugal, Duke of Viseu (4 March 1394 – 13 November 1460), better known as Henry the Navigator "()" was an important figure in 15th-century Portuguese politics and in the early days of the Portuguese Empire. Through his administrative direction, he is regarded as the main initiator of what would be known as the Age of Discoveries. Henry was the third child of the Portuguese king John I and responsible for the early development of Portuguese exploration and maritime trade with other continents through the systematic exploration of Western Africa, the islands of the Atlantic Ocean, and the search for new routes.
King John I was the founder of the House of Aviz. Henry encouraged his father to conquer Ceuta (1415), the Muslim port on the North African coast across the Straits of Gibraltar from the Iberian Peninsula. He learned of the opportunities from the Saharan trade routes that terminated there, and became fascinated with Africa in general; he was most intrigued by the Christian legend of Prester John and the expansion of Portuguese trade. Henry is regarded as the patron of Portuguese exploration.
Life.
Henry was the third surviving son of King John I and his wife Philippa, sister of King Henry IV of England. He was baptized in Porto, and may have been born there, probably when the royal couple was living in the city's old mint, now called Casa do Infante (Prince's House), or in the region nearby. Another possibility is that he was born at the Monastery of Leça do Bailio, in Leça da Palmeira, during the same residential passage of the royal couple in the city of Porto.
Henry was 21 when he and his father and brothers captured the Moorish port of Ceuta in northern Morocco. Ceuta had long been a base for Barbary pirates who raided the Portuguese coast, depopulating villages by capturing their inhabitants to be sold in the African slave market. Following this success, Henry started to explore the coast of Africa, most of which was unknown to Europeans. His objectives included finding the source of the West African gold trade and the legendary Christian kingdom of Prester John, and stopping the pirate attacks on the Portuguese coast. At that time the ships of the Mediterranean were too slow and too heavy to make these voyages. Under his direction, a new and much lighter ship was developed, the caravel, which could sail further and faster, and, above all, was highly maneuverable and could sail much nearer the wind, or "into the wind". This made the caravel largely independent of the prevailing winds. With the caravel, Portuguese mariner explored the shallow waters and rivers as well as the open ocean with wide autonomy. In 1419, Henry's father appointed him governor of the province of the Algarve.
Resources and income.
On 25 May 1420, Henry gained appointment as the Grand Master of the Military Order of Christ, the Portuguese successor to the Knights Templar, which had its headquarters at Tomar, in central Portugal. Henry held this position for the remainder of his life, and the Order was an important source of funds for Henry's ambitious plans, especially his persistent attempts to conquer the Canary Islands, which the Portuguese had claimed to have discovered before the year 1346.
In 1425, his second brother the Infante Peter, Duke of Coimbra, made a tour of Europe. While largely a diplomatic mission, among his goals was to seek out geographic material for his brother Henry. Peter returned from Venice with a current world map drafted by a Venetian cartographer.
In 1431 he donated houses for the "Estudo Geral" to reunite all the sciences — grammar, logic, rhetoric, arithmetic, music, and astronomy — into what would later become the University of Lisbon. For other subjects like medicine or philosophy, he ordered that each room should be decorated according to each subject that was being taught.
Henry also had other resources. When John I died in 1433, Henry's eldest brother Edward became king. He granted Henry all profits from trading within the areas he discovered as well as the sole right to authorize expeditions beyond Cape Bojador. Henry also held a monopoly on tuna fishing in the Algarve. When Edward died eight years later, Henry supported his brother Peter for the regency during the minority of Edward's son Afonso V, and in return received a confirmation of this levy.
Henry functioned as a primary organizer of the disastrous expedition to Tangier in 1437. Henry's younger brother Ferdinand was given as a hostage to guarantee that the Portuguese would fulfill the terms of the peace agreement that had been made with Çala Ben Çala. The "Portuguese Cortes" refused to approve the return of Ceuta in exchange for the Infante Ferdinand who remained in captivity until his death six years later.
Prince Regent Peter had an important role and responsibility in the Portuguese maritime expansion in the Atlantic Ocean and Africa during his administration. Henry promoted the colonization of the Azores during Peter's regency (1439–1448).
For most of the latter part of his life, Henry concentrated on his maritime activities, or on Portuguese court politics.
Vila do Infante and Portuguese exploration.
According to João de Barros, in the Algarve he repopulated a village that he called Terçanabal (from "terça nabal" or "tercena nabal"). This village was situated in a strategic position for his maritime enterprises and was later called Vila do Infante ("Estate or Town of the Prince").
It is traditionally suggested that Henry gathered at his villa on the Sagres peninsula a school of navigators and map-makers. However modern historians hold this to be a misconception. He did employ some cartographers to chart the coast of Mauritania after the voyages he sent there, but there was no center of navigation science or observatory in the modern sense of the word, nor was there an organized navigational center.
Referring to Sagres, sixteenth-century Portuguese mathematician and cosmographer Pedro Nunes remarked, "from it our sailors went out well taught and provided with instruments and rules which all map makers and navigators should know."
The view that Henry's court rapidly grew into the technological base for exploration, with a naval arsenal and an observatory, etc., although repeated in popular culture, has never been established. Henry did possess geographical curiosity, and employed cartographers. Jehuda Cresques, a noted cartographer, has been said to have accepted an invitation to come to Portugal to make maps for the infante. This last incident probably accounts for the legend of the School of Sagres, which is now discredited.
The first contacts with the African slave market were made by expeditions to ransom Portuguese subjects enslaved by pirate attacks on Portuguese ships or villages. As Sir Peter Russell remarks in his biography, "In Henryspeak, conversion and enslavement were interchangeable terms."
Henry's explorers.
Henry sponsored voyages, collecting a 20% tax ("o quinto") on the profits made by naval expeditions, which was the usual practice in the Iberian states of that time. The nearby port of Lagos provided a convenient harbor from which these expeditions left. The voyages were made in very small ships, mostly the caravel, a light and maneuverable vessel. The caravel used the lateen sail, the prevailing rig in Christian Mediterranean navigation since late antiquity. Most of the voyages sent out by Henry consisted of one or two ships that navigated by following the coast, stopping at night to tie up along some shore.
During Prince Henry's time and after, the Portuguese navigators discovered and perfected the North Atlantic "Volta do Mar" (the "turn of the sea" or "return from the sea"). This was a major step in the history of navigation, when an understanding of oceanic wind patterns was crucial to Atlantic navigation, from Africa and the open ocean to Europe, and enabling the main route between the New World and Europe in the North Atlantic, in future voyages of discovery. Understanding the Atlantic gyre and the "volta do mar" enabled Portuguese mariners who sailed south and southwest towards the Canary Islands and West Africa to beat upwind to the Strait of Gibraltar and home. To do this, they first had to sail far to the west — that is, away from continental Portugal, and seemingly in the wrong direction. They could then turn northeast, to the area around the Azores islands, and finally east to Europe. This route would catch usable following winds. Christopher Columbus used it on his transatlantic voyages.
Madeira.
The first explorations followed not long after the capture of Ceuta in 1415. Henry was interested in locating the source of the caravans that brought gold to the city. During the reign of his father, John I, João Gonçalves Zarco and Tristão Vaz Teixeira were sent to explore along the African coast. Zarco, a knight in service to Prince Henry, had commanded the caravels guarding the coast of Algarve from the incursions of the Moors. He had also been at Ceuta.
In 1418, Zarco and Teixeira were blown off-course by a storm while making the "volta do mar" westward swing to return to Portugal. They found shelter at an island they named Porto Santo. Henry directed that Porto Santo be colonized. The move to claim the Madeiran islands was probably a response to Castile's efforts to claim the Canary Islands. In 1420, settlers then moved to the nearby island of Madeira.
The Azores.
A chart drawn by the Catalan cartographer, Gabriel de Vallseca of Mallorca, has been interpreted to indicate that the Azores were first discovered by Diogo de Silves in 1427. In 1431, Gonçalo Velho was dispatched with orders to determine the location of "islands" first identified by de Silves. Velho apparently got a far as the Formigas, in the eastern archipelago, before having to return to Sagres, probably due to bad weather.
By this time the Portuguese navigators had also reached the Sargasso Sea (western North Atlantic region), naming it after the Sargassum seaweed growing there ("sargaço" / "sargasso" in Portuguese).
West African coast.
Until Henry's time, Cape Bojador remained the most southerly point known to Europeans on the desert coast of Africa. Superstitious searers held that beyond the cape lay sea monsters and the edge of the world. In 1434, Gil Eanes, the commander of one of Henry's expeditions, became the first European known to pass Cape Bojador.
Using the new ship type, the expeditions then pushed onwards. Nuno Tristão and Antão Gonçalves reached Cape Blanco in 1441. The Portuguese sighted the Bay of Arguin in 1443 and built an important fort there around the year 1448. Dinis Dias soon came across the Senegal River and rounded the peninsula of Cap-Vert in 1444. By this stage the explorers had passed the southern boundary of the desert, and from then on Henry had one of his wishes fulfilled: the Portuguese had circumvented the Muslim land-based trade routes across the western Sahara Desert, and slaves and gold began arriving in Portugal. By 1452, the influx of gold permitted the minting of Portugal's first gold "cruzado" coins. A cruzado was equal to 400 reis at the time. From 1444 to 1446, as many as forty vessels sailed from Lagos on Henry's behalf, and the first private mercantile expeditions began.
Alvise Cadamosto explored the Atlantic coast of Africa and discovered several islands of the Cape Verde archipelago between 1455 and 1456. In his first voyage, which started on 22 March 1455, he visited the Madeira Islands and the Canary Islands. On the second voyage, in 1456, Cadamosto became the first European to reach the Cape Verde Islands. António Noli later claimed the credit. By 1462, the Portuguese had explored the coast of Africa as far as present-day Sierra Leone. Twenty-eight years later, Bartolomeu Dias proved that Africa could be circumnavigated when he reached the southern tip of the continent, now known as the "Cape of Good Hope." In 1498, Vasco da Gama became the first European sailor to reach India by sea.
Origin of the 'Navigator' nickname.
No one used the nickname 'Navigator' to refer to prince Henry during his lifetime or in the following three centuries. The term was coined by two nineteenth-century German historians: Heinrich Schaefer and Gustav de Veer. Later on it was made popular by two British authors who included it in the titles of their biographies of the prince: Henry Major in 1868 and Raymond Beazley in 1895. In Portuguese, even in modern times, it is uncommon to call him by this epithet; the preferred use is "Infante D. Henrique".

</doc>
<doc id="14094" url="https://en.wikipedia.org/wiki?curid=14094" title="Human cloning">
Human cloning

Human cloning is the creation of a genetically identical copy of a human. The term is generally used to refer to artificial human cloning, which is the reproduction of human cells and tissue. It does not refer to the natural conception and delivery of identical twins. The possibility of human cloning has raised controversies. These ethical concerns have prompted several nations to pass laws regarding human cloning and its legality.
Two commonly discussed types of theoretical human cloning are: "therapeutic cloning" and "reproductive cloning". Therapeutic cloning would involve cloning cells from a human for use in medicine and transplants, and is an active area of research, but is not in medical practice anywhere in the world, . Two common methods of therapeutic cloning that are being researched are somatic-cell nuclear transfer and, more recently, pluripotent stem cell induction. Reproductive cloning would involve making an entire cloned human, instead of just specific cells or tissues.
History.
Although the possibility of cloning humans had been the subject of speculation for much of the 20th century, scientists and policy makers began to take the prospect seriously in the mid-1960s.
Nobel Prize-winning geneticist Joshua Lederberg advocated cloning and genetic engineering in an article in The American Naturalist in 1966 and again, the following year, in The Washington Post. He sparked a debate with conservative bioethicist Leon Kass, who wrote at the time that "the programmed reproduction of man will, in fact, dehumanize him." Another Nobel Laureate, James D. Watson, publicized the potential and the perils of cloning in his Atlantic Monthly essay, "Moving Toward the Clonal Man", in 1971.
With the cloning of a sheep known as Dolly in 1996 by somatic cell nuclear transfer (SCNT), the idea of human cloning became a hot debate topic. Many nations outlawed it, while a few scientists promised to make a clone within the next few years. The first hybrid human clone was created in November 1998, by Advanced Cell Technology. It was created using SCNT - a nucleus was taken from a man's leg cell and inserted into a cow's egg from which the nucleus had been removed, and the hybrid cell was cultured, and developed into an embryo. The embryo was destroyed after 12 days.
In 2004 and 2005, Hwang Woo-suk, a professor at Seoul National University, published two separate articles in the journal "Science" claiming to have successfully harvested pluripotent, embryonic stem cells from a cloned human blastocyst using somatic-cell nuclear transfer techniques. Hwang claimed to have created eleven different patent-specific stem cell lines. This would have been the first major breakthrough in human cloning. However, in 2006 "Science" retracted both of his articles on clear evidence that much of his data from the experiments was fabricated.
In January 2008, Dr. Andrew French and Samuel Wood of the biotechnology company Stemagen announced that they successfully created the first five mature human embryos using SCNT. In this case, each embryo was created by taking a nucleus from a skin cell (donated by Wood and a colleague) and inserting it into a human egg from which the nucleus had been removed. The embryos were developed only to the blastocyst stage, at which point they were studied in processes that destroyed them. Members of the lab said that their next set of experiments would aim to generate embryonic stem cell lines; these are the "holy grail" that would be useful for therapeutic or reproductive cloning.
In 2011, scientists at the New York Stem Cell Foundation announced that they had succeeded in generating embyronic stem cell lines, but their process involved leaving the oocyte's nucleus in place, resulting in triploid cells, which would not be useful for cloning.
In 2013, a group of scientists led by Shoukhrat Mitalipov published the first report of embryonic stem cells created using SCNT. In this experiment, the researchers developed a protocol for using SCNT in human cells, which differs slightly from the one used in other organisms. Four embryonic stem cell lines from human fetal somatic cells were derived from those blastocysts. All four lines were derived using oocytes from the same donor, ensuring that all mitochondrial DNA inherited was identical. A year later, a team led by Robert Lanza at Advanced Cell Technology reported that they had replicated Mitalipov's results and further demonstrated the effectiveness by cloning adult cells using SCNT.
Methods.
Somatic cell nuclear transfer (SCNT).
In somatic cell nuclear transfer ("SCNT"), the nucleus of a somatic cell is taken from a donor and transplanted into a host egg cell, which had its own genetic material removed previously, making it an enucleated egg. After the donor somatic cell genetic material is transferred into the host oocyte with a micropipette, the somatic cell genetic material is fused with the egg using an electric current. Once the two cells have fused, the new cell can be permitted to grow in a surrogate or artificially. This is the process that was used to successfully clone Dolly the sheep (see section on History in this article).
Induced pluripotent stem cells (iPSCs).
Creating induced pluripotent stem cells ("iPSCs") is a long and inefficient process. Pluripotency refers to a stem cell that has the potential to differentiate into any of the three germ layers: endoderm (interior stomach lining, gastrointestinal tract, the lungs), mesoderm (muscle, bone, blood, urogenital), or ectoderm (epidermal tissues and nervous system). A specific set of genes, often called "reprogramming factors", are introduced into a specific adult cell type. These factors send signals in the mature cell that cause the cell to become a pluripotent stem cell. This process is highly studied and new techniques are being discovered frequently on how to better this induction process.
Depending on the method used, reprogramming of adult cells into iPSCs for implantation could have severe limitations in humans. If a virus is used as a reprogramming factor for the cell, cancer-causing genes called oncogenes may be activated. These cells would appear as rapidly dividing cancer cells that do not respond to the body's natural cell signaling process. However, in 2008 scientists discovered a technique that could remove the presence of these oncogenes after pluripotency induction, thereby increasing the potential use of iPSC in humans.
Comparing SCNT to Reprogramming.
Both the processes of SCNT and iPSCs have benefits and deficiencies. Historically, reprogramming methods were better studied than SCNT derived embryonic stem cells (ESCs). However, more recent studies have put more emphasis on developing new procedures for SCNT-ESCs. The major advantage of SCNT over iPSCs at this time is the speed with which cells can be produced. iPSCs derivation takes several months while SCNT would take a much shorter time, which could be important for medical applications. New studies are working to improve the process of iPSC in terms of both speed and efficiency with the discovery of new reprogramming factors in oocytes. Another advantage SCNT could have over iPSCs is its potential to treat mitochondrial disease, as it utilizes a donor oocyte. No other advantages are known at this time in using stem cells derived from one method over stem cells derived from the other.
Uses, actual and potential.
Work on cloning techniques has advanced our basic understanding of developmental biology in humans. Observing human pluripotent stem cells grown in culture provides great insight into human embryo development, which otherwise cannot be seen. Scientists are now able to better define steps of early human development. Studying signal transduction along with genetic manipulation within the early human embryo has the potential to provide answers to many developmental diseases and defects. Many human-specific signaling pathways have been discovered by studying human embryonic stem cells. Studying developmental pathways in humans has given developmental biologists more evidence toward the hypothesis that developmental pathways are conserved throughout species.
iPSCs and cells created by SCNT are useful for research into the causes of disease, and as model systems used in drug discovery.
Cells produced with SCNT, or iPSCs could eventually be used in stem cell therapy, or to create organs to be used in transplantation, known as regenerative medicine. Stem cell therapy is the use of stem cells to treat or prevent a disease or condition. Bone marrow transplantation is a widely used form of stem cell therapy. No other forms of stem cell therapy are in clinical use at this time. Research is underway to potentially use stem cell therapy to treat heart disease, diabetes, and spinal cord injuries. Regenerative medicine is not in clinical practice, but is heavily researched for its potential uses. This type of medicine would allow for autologous transplantation, thus removing the risk of organ transplant rejection by the recipient. For instance, a person with liver disease could potentially have a new liver grown using their same genetic material and transplanted to remove the damaged liver. In current research, human pluripotent stem cells have been promised as a reliable source for generating human neurons, showing the potential for regenerative medicine in brain and neural injuries.
Ethical implications.
In bioethics, the ethics of cloning refers to a variety of ethical positions regarding the practice and possibilities of cloning, especially human cloning. While many of these views are religious in origin, the questions raised by cloning are faced by secular perspectives as well. Human therapeutic and reproductive cloning are not commercially used; animals are currently cloned in laboratories and in livestock production.
Advocates support development of therapeutic cloning in order to generate tissues and whole organs to treat patients who otherwise cannot obtain transplants, to avoid the need for immunosuppressive drugs, and to stave off the effects of aging. Advocates for reproductive cloning believe that parents who cannot otherwise procreate should have access to the technology.
Opposition to therapeutic cloning mainly centers around the status of embyronic stem cells, which has connections with the abortion debate.
Some opponents of reproductive cloning have concerns that technology is not yet developed enough to be safe - for example, the position of the American Association for the Advancement of Science , while others emphasize that reproductive cloning could be prone to abuse (leading to the generation of humans from whom organs and tissues would be harvested), and have concerns about how cloned individuals could integrate with families and with society at large.
Religious groups are divided, with some opposing the technology as usurping God's (in monotheistic traditions) place and, to the extent embryos are used, destroying a human life; others support therapeutic cloning's potential life-saving benefits.
Current law.
In 2015 it was reported that about 70 countries had banned human cloning.
Australia.
Australia has prohibited human cloning, though , a bill legalizing therapeutic cloning and the creation of human embryos for stem cell research passed the House of Representatives. Within certain regulatory limits, and subject to the effect of state legislation, therapeutic cloning is now legal in some parts of Australia.
Canada.
Canadian law prohibits the following: cloning humans, cloning stem cells, growing human embryos for research purposes, and buying or selling of embryos, sperm, eggs or other human reproductive material. It also bans making changes to human DNA that would pass from one generation to the next, including use of animal DNA in humans. Surrogate mothers are legally allowed, as is donation of sperm or eggs for reproductive purposes. Human embryos and stem cells are also permitted to be donated for research.
There have been consistent calls in Canada to ban human reproductive cloning since the 1993 Report of the Royal Commission on New Reproductive Technologies. Polls have indicated that an overwhelming majority of Canadians oppose human reproductive cloning, though the regulation of human cloning continues to be a significant national and international policy issue. The notion of "human dignity" is commonly used to justify cloning laws. The basis for this justification is that reproductive human cloning necessarily infringes notions of human dignity.
Colombia.
Human cloning is prohibited in Article 133 of the Colombian Penal Code.
European Union.
The European Convention on Human Rights and Biomedicine prohibits human cloning in one of its additional protocols, but this protocol has been ratified only by Greece, Spain and Portugal. The Charter of Fundamental Rights of the European Union explicitly prohibits reproductive human cloning. The charter is legally binding for the institutions of the European Union under the Treaty of Lisbon and for member states of the Union implementing EU law.
India.
India does not have specific law regarding cloning but has guidelines prohibiting whole human cloning or reproductive cloning. India allows therapeutic cloning and the use of embryonic stem cells for research proposes.
Romania.
Cloning a human is prohibited in Romania.
Serbia.
Human cloning is explicitly prohibited in Article 24, "Right to Life" of the 2006 Constitution of Serbia.
South Africa.
In terms of section 39A of the Human Tissue Act 65 of 1983, genetic manipulation of gametes or zygotes outside the human body is absolutely prohibited. A zygote is the cell resulting from the fusion of two gametes; thus the fertilised ovum. Section 39A thus prohibits human cloning.
United Kingdom.
On January 14, 2001 the British government passed The Human Fertilisation and Embryology (Research Purposes) Regulations 2001 to amend the Human Fertilisation and Embryology Act 1990 by extending allowable reasons for embryo research to permit research around stem cells and cell nuclear replacement, thus allowing therapeutic cloning. However, on November 15, 2001, a pro-life group won a High Court legal challenge, which struck down the regulation and effectively left all forms of cloning unregulated in the UK. Their hope was that Parliament would fill this gap by passing prohibitive legislation. Parliament was quick to pass the Human Reproductive Cloning Act 2001 which explicitly prohibited reproductive cloning. The remaining gap with regard to therapeutic cloning was closed when the appeals courts reversed the previous decision of the High Court.
The first license was granted on August 11, 2004 to researchers at the University of Newcastle to allow them to investigate treatments for diabetes, Parkinson's disease and Alzheimer's disease. The Human Fertilisation and Embryology Act 2008, a major review of fertility legislation, repealed the 2001 Cloning Act by making amendments of similar effect to the 1990 Act. The 2008 Act also allows experiments on hybrid human-animal embryos.
United Nations.
On December 13, 2001, the United Nations General Assembly began elaborating an international convention against the reproductive cloning of humans. A broad coalition of States, including Spain, Italy, the Philippines, the United States, Costa Rica and the Holy See sought to extend the debate to ban all forms of human cloning, noting that, in their view, therapeutic human cloning violates human dignity. Costa Rica proposed the adoption of an international convention to ban all forms of human cloning. Unable to reach a consensus on a binding convention, in March 2005 a non-binding United Nations Declaration on Human Cloning, calling for the ban of all forms of human cloning contrary to human dignity, was adopted.
United States.
In 1998, 2001, 2004, 2005, and 2007, the United States House of Representatives voted whether to ban all human cloning, both reproductive and therapeutic. Each time, divisions in the Senate over therapeutic cloning prevented either competing proposal (a ban on both forms or reproductive cloning only) from passing. On March 10, 2010 a bill (HR 4808) was introduced with a section banning federal funding for human cloning. Such a law, if passed, would not prevent research from occurring in private institutions (such as universities) that have both private and federal funding. There are currently no federal laws in the United States which ban cloning completely, and any such laws would raise difficult constitutional questions similar to the issues raised by abortion. Fifteen American states (Arkansas, California, Connecticut, Iowa, Indiana, Massachusetts, Maryland, Michigan, North Dakota, New Jersey, Rhode Island, South Dakota, Florida, Georgia, and Virginia) ban reproductive cloning and three states (Arizona, Maryland, and Missouri) prohibit use of public funds for such activities.
In popular culture.
Science fiction has used cloning, most commonly and specifically human cloning, due to the fact that it brings up controversial questions of identity. In Aldous Huxley’s "Brave New World" (1932), human cloning is a major plot device that not only drives the story but also makes the reader think critically about what identity means; this concept was re-examined fifty years later in C. J. Cherryh’s novels "Forty Thousand in Gehenna" (1983) and "Cyteen" (1988). Kazuo Ishiguro's 2005 novel "Never Let Me Go" centers on human clones and considers the ethics of the practice.
The reduction in the value of the individual human life in a resource-optimized clone-based society is examined in the 1967 novel "Logan's Run", and the later movie.
A recurring sub-theme of cloning fiction is the use of clones as a supply of organs for transplantation. The 2005 Kazuo Ishiguro novel "Never Let Me Go" and the 2010 film adaption are set in an alternate history in which cloned humans are created for the sole purpose of providing organ donations to naturally born humans, despite the fact that they are fully sentient and self-aware. The 2005 film "The Island" revolves around a similar plot, with the exception that the clones are unaware of the reason for their existence. In the futuristic novel "The House of the Scorpion", clones are used to grow organs for their wealthy "owners", and the main character was a complete clone.
The use of human cloning for military purposes has also been explored in several works. Star Wars portrays human cloning in "Clone Wars", ' and ', in the form of the Grand Army of the Republic, an army of cloned soldiers.
Orphan Black, a sci-fi/drama television series explores the ethical issues, and biological advantages/disadvantages of human cloning through a fictional scientific study on the behavioral adaptation of clones in society.

</doc>
<doc id="14097" url="https://en.wikipedia.org/wiki?curid=14097" title="History of Asia">
History of Asia

The history of Asia can be seen as the collective history of several distinct peripheral coastal regions such as, East Asia, South Asia, and the Middle East linked by the interior mass of the Eurasian steppe.
The coastal periphery was the home to some of the world's earliest known civilizations, with each of the three regions developing early civilizations around fertile river valleys. These valleys were fertile because the soil there was rich and could bear many root crops. The civilizations in Mesopotamia, the Indus Valley, and China shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Other notions such as that of writing likely developed individually in each area. Cities, states and then empires developed in these lowlands.
The steppe region had long been inhabited by mounted nomads, and from the central steppes they could reach all areas of the Asian continent. The northern part of the continent, covering much of Siberia was also inaccessible to the steppe nomads due to the dense forests and the tundra. These areas in Siberia were very sparsely populated.
The centre and periphery were kept separate by mountains and deserts. The Caucasus, Himalaya, Karakum Desert, and Gobi Desert formed barriers that the steppe horsemen could only cross with difficulty. While technologically and culturally the city dwellers were more advanced, they could do little militarily to defend against the mounted hordes of the steppe. However, the lowlands did not have enough open grasslands to support a large horsebound force. Thus the nomads who conquered states in the Middle East were soon forced to adapt to the local societies.
Asia's history would feature major developments seen in other parts of the world, as well as events that would affect those other regions. These include the trade of the Silk Road, which spread cultures, languages, religion, and disease throughout Afro-Eurasian trade. Another major advancement was the innovation of gunpowder in medieval China, which led to advanced warfare through the use of guns.
Prehistory.
A temple area in southeastern Turkey at Göbekli Tepe dated to 10,000 BCE has been seen as the beginning of the "Neolithic 1" culture. This site was developed by nomadic hunter-gatherers since there is no permanent housing in the vicinity. This temple site is the oldest known man-made place of worship. By 8500–8000 BCE farming communities began to spread to Anatolia, Northern Africa and north Mesopotamia.
A report by archaeologist Rakesh Tewari on Lahuradewa, India shows new C14 datings that range between 9000 and 8000 BCE associated with rice, making Lahuradewa the earliest Neolithic site in entire South Asia.
The prehistoric Beifudi site near Yixian in Hebei Province, China, contains relics of a culture contemporaneous with the Cishan and Xinglongwa cultures of about 8000–7000 BCE, neolithic cultures east of the Taihang Mountains, filling in an archaeological gap between the two Northern Chinese cultures. The total excavated area is more than 1,200 square meters and the collection of neolithic findings at the site consists of two phases.
Around 5500 BCE the Halafian culture appeared in the Levant, Lebanon, Palestine, Syria, Anatolia and northern Mesopotamia, based upon dryland agriculture.
In southern Mesopotamia were the alluvial plains of Sumer and Elam. Since there was little rainfall, irrigation systems were necessary. The Ubaid culture flourished from 5500 BCE.
Bronze Age.
The Chalcolithic period (or Copper Age) began about 4500 BCE, then the Bronze Age began about 3500 BCE, replacing the Neolithic cultures.
The Indus Valley Civilization (IVC) was a Bronze Age civilization (3300–1300 BCE; mature period 2600–1900 BCE) which was centered mostly in the western part of the Indian Subcontinent; it is considered that an early form of Hinduism was performed during this civilization. Some of the great cities of this civilization include Harappa and Mohenjo-daro, which had a high level of town planning and arts. The cause of the destruction of these regions around 1700 BCE is debatable, although evidence suggests it was caused by natural disasters (especially flooding) and Indo-European invaders. These invaders are commonly referred to as the Aryan and their dominance created the Vedic period, which lasted from roughly 1500 to 500 BCE. During this period, the Sanskrit language developed and the Vedas were written, epic hymns that told tales of Aryan gods and wars. This was the basis for the Aryan religion, which would eventually sophisticate and develop into Hindusim, a religion based on the caste system of class (which consisted of the four varnas), the brahman priesthood, and the developing semi-monotheism.
China and Vietnam were also centres of metalworking. Dating back to the Neolithic Age, the first bronze drums, called the Dong Son drums have been uncovered in and around the Red River Delta regions of Vietnam and Southern China. These relate to the prehistoric Dong Son Culture of Vietnam.
Song Da bronze drum's surface, Dong Son culture, Vietnam
In Ban Chiang, Thailand (Southeast Asia), bronze artifacts have been discovered dating to 2100 BCE.
In Nyaunggan, Burma bronze tools have been excavated along with ceramics and stone artifacts. Dating is still currently broad (3500–500 BCE).
Iron Age.
The Iron Age saw the widespread use of iron tools, weaponry, and armor throughout the major civilizations of Asia.
Middle East.
The Achaemenid dynasty of the Persian Empire, founded by Cyrus the Great, ruled an area from Greece and Turkey to the Indus River and Central Asia during the 6th to 4th centuries BCE. Persian politics included a tolerance for other cultures, a highly centralized government, and significant infrastructure developments. Later, in Darius the Great's rule, the territories were integrated, a bureaucracy was developed, nobility were assigned military positions, tax collection was carefully organized, and spies were used to ensure the loyalty of regional officials. The primary religion of Persia at this time was Zoroastrianism, developed by the philosopher Zoroaster. It introduced an early form of monotheism to the area. The religion banned animal sacrifice and the use of intoxicants in rituals; and introduced the concept of spiritual salvation through personal moral action, an end time, and both general and Particular judgment with a heaven or hell. These concepts would heavily influence later emperors and the masses. More importantly, Zoroastrianism would be an important precursor for the Abrahamic religions such as Christianity, Islam, or Judaism. The Persian Empire was successful in establishing peace and stability throughout the Middle East and were a major influence in art, politics (affecting Hellenistic leaders), and religion.
Alexander the Great conquered this dynasty in the 4th century BCE, creating the brief Hellenistic period. He was unable to establish stability and after his death, Persia broke into small, weak dynasties including the Seleucid Empire, followed by the Parthian Empire. By the end of the Classical age, Persia had been reconsolidated into the Sassanid Empire, also known as the second Persian Empire.
The Roman Empire would later control parts of Western Asia. The Seleucid, Parthian and Sassanid dynasties of Persia dominated Western Asia for centuries.
India.
The Maurya and Gupta empires are called the Golden Age of India and were marked by extensive inventions and discoveries in science, technology, art, religion, and philosophy that crystallized the elements of what is generally known as Indian culture. The religions of Hinduism and Buddhism, which began in Indian sub-continent, were an important influence on South, East and Southeast Asia.
By 600 BCE, India had been divided into sixteen regional states that would occasionally feud amongst themselves. In 327 BCE, Alexander the Great came to India with a vision of conquering the whole world. He crossed northwestern India and created the province Bactria but could not move further because his army was afraid of the foot soldiers of India. Shortly prior, the soldier Chandragupta Maurya began to take control of the Ganges river and soon established the Maurya Empire. The Maurya Empire (Sanskrit: मौर्य राजवंश, Maurya Rājavanśha) was the geographically extensive and powerful empire in ancient India, ruled by the Mauryan dynasty from 321 to 185 BCE. It was one of the world's largest empires in its time, stretching to the Himalayas in the north, what is now Assam in the east, probably beyond modern Pakistan in the west, and annexing Balochistan and much of what is now Afghanistan, at its greatest extent. India was united for the first time in the Maurya empire. The government established by Chandragupta was led by an autocratic king, who primarily relied on the military to assert his power. It also applied the use of a bureaucracy and even sponsored a postal service. Chandragupta's grandson, Ashoka, greatly extended the empire by conquering most of modern-day India (save for the southern tip). He eventually converted to Buddhism, though, and began a peaceful life where he promoted the religion as well as humane methods throughout India. The Maurya Empire would disintegrate soon after Ashoka's death and was conquered by the Kushan invaders from the northwest, establishing the Kushan Empire. Their conversion to Buddhism caused the religion to be associated with foreigners and therefore a decline in its popularity occurred.
The Kushan Empire would fall apart by 220 CE, creating more political turmoil in India. Then in 320, the Gupta Empire (Sanskrit: गुप्त राजवंश, Gupta Rājavanśha) was established and covered much of the Indian Subcontinent. Founded by Maharaja Sri-Gupta, the dynasty was the model of a classical civilization. Gupta kings united the area primarily through negotiation of local leaders and families as well as strategical intermarriage. Their rule covered less land than the Maurya Empire, but established the greatest stability. In 535, the empire ended when India was overrun by the Huns.
Classical China.
Zhou Dynasty.
Since 1029 BCE, the Zhou Dynasty ( ), had existed in China and it would continue to until 258 BCE. The Zhou dynasty had been using a feudal system by giving power to local nobility and relying on their loyalty in order to control its large territory. As a result, the Chinese government at this time tended to be very decentralized and weak, and there was often little the emperor could do to resolve national issues. Nonetheless, the government was able to retain its position with the creation of the Mandate of Heaven, which could establish an emperor as divinely chosen to rule. The Zhou additionally discouraged the human sacrifice of the preceding eras and unified the Chinese language. Finally, the Zhou government encouraged settlers to move into the Yangtze River valley, thus creating the Chinese Middle Kingdom.
But by 500 BCE, its political stability began to decline due to repeated nomadic incursions and internal conflict derived from the fighting princes and families. This was lessened by the many philosophical movements, starting with the life of Confucius. His philosophical writings (called Confucianism) concerning the respect of elders and of the state would later be popularly used in the Han Dynasty. Additionally, Laozi's concepts of Taoism, including yin and yang and the innate duality and balance of nature and the universe, became popular throughout this period. Nevertheless, the Zhou Dynasty eventually disintegrated as the local nobles began to gain more power and their conflict devolved into the Warring States period, from 402 to 201 BCE.
Qin Dynasty.
One leader eventually came on top, Qin Shi Huang (, "Shǐ Huángdì"), who overthrew the last Zhou emperor and established the Qin dynasty. The Qin Dynasty (Chinese: 秦朝; pinyin: Qín Cháo) was the first ruling dynasty of Imperial China, lasting from 221 to 207 BCE. The new Emperor abolished the feudal system and directly appointed a bureaucracy that would rely on him for power. Huang's imperial forces crushed any regional resistance, and they furthered the Chinese empire by expanding down to the South China Sea and northern Vietnam. Greater organization brought a uniform tax system, a national census, regulated road building (and cart width), standard measurements, standard coinage, and an official written and spoken language. Further reforms included new irrigation projects, the encouragement of silk manufacturing, and (most famously) the beginning of the construction of the Great Wall of China—designed to keep out the nomadic raiders who'd constantly badger the Chinese people. However, Shi Huang was infamous for his tyranny, forcing laborers to build the Wall, ordering heavy taxes, and severely punishing all who opposed him. He oppressed Confucians and promoted Legalism, the idea that people were inherently evil, and that a strong, forceful government was needed to control them. Legalism was infused with realistic, logical views and rejected the pleasures of educated conversation as frivolous. All of this made Shi Huang extremely unpopular with the people. As the Qin began to weaken, various factions began to fight for control of China.
Han Dynasty.
The Han Dynasty (simplified Chinese: 汉朝; traditional Chinese: 漢朝; pinyin: Hàn Cháo ;206 BCE – 220 CE) was the second imperial dynasty of China, preceded by the Qin Dynasty and succeeded by the Three Kingdoms (220–265 CE). Spanning over four centuries, the period of the Han Dynasty is considered a golden age in Chinese history. One of the Han Dynasty's greatest emperors, Emperor Wu of Han, established a peace throughout China comparable to the Pax Romana seen in the Mediterranean a hundred years later. To this day, China's majority ethnic group refers to itself as the "Han people". The Han Dynasty was established when two peasants succeeded in rising up against Shi Huang's significantly weaker successor-son. The new Han government retained the centralization and bureaucracy of the Qin, but greatly reduced the repression seen before. They expanded their territory into Korea, Vietnam, and Central Asia, creating an even larger empire than the Qin.
The Han developed contacts with the Persian Empire in the Middle East and the Romans, through the Silk Road, with which they were able to trade many commodities—primarily silk. Many ancient civilizations were influenced by the Silk Road, which connected China, India, the Middle East and Europe. Han emperors like Wu also promoted Confucianism as the national "religion" (although it is debated by theologians as to whether it is defined as such or as a philosophy). Shrines devoted to Confucius were built and Confucian philosophy was taught to all scholars who entered the Chinese bureaucracy. The bureaucracy was further improved with the introduction of an examination system that selected scholars of high merit. These bureaucrats were often upper-class people educated in special schools, but whose power was often checked by the lower-class brought into the bureaucracy through their skill. The Chinese imperial bureaucracy was very effective and highly respected by all in the realm and would last over 2,000 years. The Han government was highly organized and it commanded the military, judicial law (which used a system of courts and strict laws), agricultural production, the economy, and the general lives of its people. The government also promoted intellectual philosophy, scientific research, and detailed historical records.
However, despite all of this impressive stability, central power began to lose control by the turn of the Common Era. As the Han Dynasty declined, many factors continued to pummel it into submission until China was left in a state of chaos. By 100 CE, philosophical activity slowed, and corruption ran rampant in the bureaucracy. Local landlords began to take control as the scholars neglected their duties, and this resulted in heavy taxation of the peasantry. Taoists began to gain significant ground and protested the decline. They started to proclaim magical powers and promised to save China with them; the Taoist Yellow Turban Rebellion in 184 (led by rebels in yellow scarves) failed but was able to weaken the government. The aforementioned Huns combined with diseases killed up to half of the population and officially ended the Han Dynasty by 220. The ensuing period of chaos was so terrible it lasted for three centuries, where many weak regional rulers and dynasties failed to establish order in China. This period of chaos and attempts at order is commonly known as that of the Six Dynasties. The first part of this included the Three Kingdoms which started in 220 and describes the brief and weak successor "dynasties" that followed the Han. In 265, the Jin dynasty of China was started and this soon split into two different empires in control of northwestern and southeastern China. In 420, the conquest and abdication of those two dynasties resulted in the first of the Southern and Northern Dynasties. The Northern and Southern Dynasties passed through until finally, by 557, the Northern Zhou Dynasty ruled the north and the Chen Dynasty ruled the south.
Medieval history.
During this period, the Eastern world empires continued to expand through trade, migration and conquests of neighboring areas. Gunpowder was widely used as early as the 11th century and they were using moveable type printing five hundred years before Gutenberg created his press. Buddhism, Taoism, Confucianism were the dominant philosophies of the Far East during the Middle Ages. Marco Polo was not the first Westerner to travel to the Orient and return with amazing stories of this different culture, but his accounts published in the late 13th and early 14th centuries were the first to be widely read throughout Europe.
Islamic Middle East.
The Islamic Caliphate and other Islamic states took over the Middle East, Caucasus and Central Asia during the Muslim conquests of the 7th century, and later expanded into the Indian subcontinent and Malay archipelago.
At the beginning of the Medieval Age in 500, the Middle East was separated into small, weak states; the two most prominent were the Sassanid Empire in Persia (modern-day Iran), and the Byzantine Empire in Turkey. In the Arabian peninsula (now Saudi Arabia), the nomadic Bedouin tribes dominated the desert, where they worshipped idols and remained in small clans tied together by kinship. Urbanization and agriculture was very limited, save for a few regions near the coast. Mecca and Medina were two of these cites that were important hubs for trade between Africa and Eurasia. This commerce was central to city-life, where most inhabitants were merchants.
Early Islamic Empire.
From 613 to 630, Muhammad spread the faith of Islam in the Arabian desert, culminating in his victory at Mecca. He then unified the tribes into an Islamic Empire, ruled by a religious and political leader, the caliph. They would proceed to conquer the Sassanids, and modern-day Syria, Palestine, Egypt, and Libya. An Arabic navy was created that soon dominated the Mediterranean, crippled the Byzantine Empire, and put it under siege for centuries to come. Issues in deciding the caliphs to succeed Muhammad led to the Ridda wars and eventually the Sunni-Shia split, two different sects of Islam; the Sunni eventually became dominant and established the Umayyad Caliphate.
The Umayyad were centered at their capital, Damascus in what's now Syria. With the Umayyad came more conquest, giving them rule over central Asia, most of northern Africa, and from there, the Iberian Peninsula (modern-day Spain and Portugal). Little conversion occurred at this time due to the disrespect non-Arab Muslims, or mawali (Arabic: موالي), received from the Umayyad. Christians and Jews were treated with more respect as dhimmi (Arabic: ذمي), specifically the Ahl al-Kitab (Arabic: أهل الكتاب) or "people of the book," referring to the Holy Bible which they all shared. During the Umayyad age, women's position also improved from that of pre-Islamic Arabia; Muhammad's teachings banned adultery, encouraged marriage and kindness to wives and daughters, and proclaimed equality of women and men "in the eyes of God."
Abbasid Empire.
The Umayyad Empire began to decline in the early 8th century when its leaders became more and more detached from their people, especially the warriors who had fought for their conquest. A new political group, the Abbasids, joined the upset warriors, Shia, and mawali, and overthrew the Umayyad in 750 during the Battle of the Zab. The remaining Umayyads fled to Iberia, and established the independent, Muslim Caliphate of Córdoba. The establishment of the Abbasid Caliphate started with moving the capital to Baghdad in Persia (now Iraq) in 762 and with this came the application of certain Persian political institutions. This included the creation of an absolute monarchy, which ruled without question, as well as an improved bureaucracy, led by the wazir who took most of the political and administrative responsibilities the caliph previously had. The Abbasid also experienced a boom in trade, specifically that at sea, sending dhows that continued expansion, first by sending merchants and missionaries to India and Southeast Asia. Eventually conflict would arise due to a piracy issue in India, and the Abbasid would begin to conquer the western area of India which they traded with. The first expedition was led by Turkish general Qutb-ud-din Aybak and established the Mamluk Sultanate in 1206, ruled by the sultan (Arabic: سلطان) which means "authority."
However, the Abbasid government soon fell to the same vices as the Umayyad. Different factions in the royal court would fight for power, especially various groups of the Turkic peoples. The caliph began to rely on advisors from wealthy families, which would sometimes render him a mere puppet. This happened when the Persian Buyid dynasty was established in 934. The Shia government lasted only a little over a century. They were quickly overpowered by the Turkish people who would create the Seljuq dynasty by 1051, reestablishing the Sunni government. Nevertheless, succession issues and the squabbling factions would continue through the First Crusade, launched by Christian western Europeans in 1095, which was largely ignored by the more powerful Muslim princes despite its success at capturing Jerusalem. The next eight Crusades would succeed to varying degrees, and the Christians would lose considerable ground when the Muslims were united under Saladin in the late 12th century. By 1291, after the final crusade and the fall of Acre, the Christians had lost all of the territory they originally gained.
The increasingly divided regions of the Abbasid caliphate would face new challenges in the early 13th century, during the invasion of the central Asian nomadic peoples, the Mongols; led by the infamous Genghis Khan, the Mongols raided much of the eastern empire. In 1258, Genghis Khan's grandson Hulegu Khan would finish his grandfather's work with the sacking of Baghdad and the death of the caliph. The Mongols eventually retreated, but the chaos that ensued throughout the empire deposed the Seljuq Turks. In 1401, the weak and limping caliphate was further plagued by the Turko-Mongol, Timur, and his ferocious raids. By then, another group of Turks had arisen as well, the Ottomans. Based in Anatolia, by 1566 they would conquer the Mesopotamia region, the Balkans, Greece, Byzantium, most of Egypt, most of north Africa, and parts of Arabia, unifying them under the Ottoman Empire. The rule of the Ottoman sultans marked the end of the Postclassical Era in the Middle East, and of the caliphate.
India.
The Indian early medieval age, 600 to 1200, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. No ruler of this period was able to create an empire and consistently control lands much beyond his core region. During this time, pastoral peoples whose land had been cleared to make way for the growing agricultural economy were accommodated within caste society, as were new non-traditional ruling classes.
The Muslim conquest in the Indian subcontinent mainly took place from the 12th century onwards, though earlier Muslim conquests made limited inroads into the region, beginning during the period of the ascendancy of the Rajput Kingdoms in North India, although Sindh and Multan were captured in 8th century.
Medieval China.
Postclassical China saw the rise and fall of the Sui, Tang, Song, and Yuan dynasties and therefore improvements in its bureaucracy, the spread of Buddhism, and the advent of Neo-Confucianism. The Middle Ages were an unsurpassed era for Chinese ceramics and painting. Medieval architectural masterpieces the Great South Gate in Todaiji, Japan, and the Tien-ning Temple in Peking, China are some of the surviving constructs from this era.
Sui Dynasty.
A new powerful dynasty began to rise in the 580s, amongst the divided factions of China. This was started when an aristocrat named Yang Jian married his daughter into the Northern Zhou Dynasty. He proclaimed himself Emperor Wen of Sui and appeased the nomadic military by abandoning the Confucian scholar-gentry. Emperor Wen soon led the conquest of the southern Chen Dynasty and united China once more under the Sui Dynasty. The emperor lowered taxes and constructed granaries that he used to prevent famine and control the market. Later Wen's son would murder him for the throne and declare himself Emperor Yang of Sui. Emperor Yang revived the Confucian scholars and the bureaucracy, much to anger of the aristocrats and nomadic military leaders. Yang became an excessive leader who overused China's resources for personal luxury and perpetuated exhaustive attempts to reconquer Korea. His military failures and neglect of the empire forced his own ministers to assassinate him in 618, ending the Sui Dynasty.
Tang dynasty.
Fortunately, one of Yang's most respectable advisors, Li Yuan, was able to claim the throne quickly, preventing a chaotic collapse. He proclaimed himself Emperor Gaozu, and established the Tang dynasty in 623. The Tang saw expansion of China through conquest to Tibet in the west, Vietnam in the south, and Manchuria in the north. Tang emperors also improved the education of scholars in the Chinese bureaucracy. A Ministry of Rites was established and the examination system was improved to better qualify scholars for their jobs. In addition, Buddhism became popular in China with two different strains between the peasantry and the elite, the Pure Land and Zen strains, respectively. Greatly supporting the spread of Buddhism was Empress Wu, who additionally claimed an unofficial "Zhou Dynasty" and displayed China's tolerance of a woman ruler, which was rare at the time. However, Buddhism would also experience some backlash, especially from Confucianists and Taoists. This would usually involve criticism about how it was costing the state money, since the government was unable to tax Buddhist monasteries, and additionally sent many grants and gifts to them.
The Tang dynasty began to decline under the rule of Emperor Xuanzong, who began to neglect the economy and military and caused unrest amongst the court officials due to the excessive influence of his concubine, Yang Guifei, and her family. This eventually sparked a revolt in 755. Although the revolt failed, subduing it required involvement with the unruly nomadic tribes outside of China and distributing more power to local leaders—leaving the government and economy in a degraded state. The Tang dynasty officially ended in 907 and various factions led by the aforementioned nomadic tribes and local leaders would fight for control of China in the Five Dynasties and Ten Kingdoms period.
Song dynasty.
By 960, most of China had been reunited under the Song dynasty, although it lost territories in the north and could not defeat one of the nomadic tribes there—the Liao dynasty of the highly sinicized Khitan people. From then on, the Song would have to pay tribute to avoid invasion and thus set the precedent for other nomadic kingdoms to oppress them. The Song also saw the revival of Confucianism in the form of Neo-Confucianism. This had the effect of putting the Confucian scholars at a higher status than aristocrats or Buddhists and also intensified the reduction of power in women. The infamous practice of foot binding developed in this period as a result. Eventually the Liao dynasty in the north was overthrown by the Jin dynasty of the Manchu-related Jurchen people. The new Jin kingdom invaded northern China, leaving the Song to flee farther south and creating the Southern Song dynasty in 1126. There, cultural life flourished.
Yuan Dynasty.
By 1227, the Mongols had conquered the Western Xia kingdom northwest of China. Soon the Mongols incurred upon the Jin empire of the Jurchens. Chinese cities were soon besieged by the Mongol hordes that showed little mercy for those who resisted and the Southern Song Chinese were quickly losing territory. In 1271 the current great khan, Kublai Khan, claimed himself Emperor of China and officially established the Yuan Dynasty. By 1290, all of China was under control of the Mongols, marking the first time they were ever completely conquered by a foreign invader; the new capital was established at Khanbaliq (modern-day Beijing). Kublai Khan segregated Mongol culture from Chinese culture by discouraging interactions between the two peoples, separating living spaces and places of worship, and reserving top administrative positions to Mongols, thus preventing Confucian scholars to continue the bureaucratic system. Nevertheless, Kublai remained fascinated with Chinese thinking, surrounding himself with Chinese Buddhist, Taoist, or Confucian advisors.
Mongol women displayed a contrasting independent nature compared to the Chinese women who continued to be suppressed. Mongol women often rode out on hunts or even to war. Kublai's wife, Chabi, was a perfect example of this; Chabi advised her husband on several political and diplomatic matters; she convinced him that the Chinese were to be respected and well-treated in order to make them easier to rule. However this was not enough to affect Chinese women's position, and the increasingly Neo-Confucian successors of Kublai further repressed Chinese and even Mongol women.
The Black Death, which would later ravage Western Europe, had its beginnings in Asia, where it wiped out large populations in China in 1331.
Japan.
During this time period Japan went under the process of sinicization, or the impression of Chinese cultural and political ideas. Japan sinicized mostly because the emperor and other leaders at the time were largely impressed by China's bureaucracy. The major influences China had on this region were the spread of Confucianism, the spread of Buddhism, and the establishment of a bureaucracy (although it was vulnerable to favoritism towards the wealthy). In Japan, these later medieval centuries saw a return to the traditional Shinto faith and the continuing popularity of Zen Buddhism.
Medieval Japan is marked by the beginning of the Asuka period. During this time, Yamato dynasty is established, along with the beginning of recorded Japanese history and a capital in the southern Nara region. In 600, the Japanese send their first diplomatic mission to China, catalyzing the process of adoption of Chinese culture. The Yamato establish their power with a Chinese-based bureaucracy and encourage the spread of Buddhism, discovered through China. The latter was achieved particularly through the construction of Buddhist temples in cities and the countryside.
Mongol Empire.
The Mongol Empire conquered a large part of Asia in the 13th century, an area extending from China to Europe. Medieval Asia was the kingdom of the Khans. Never before had any person controlled as much land as Genghis Khan. He built his power unifying separate Mongol tribes before expanding his kingdom south and west. He and his grandson, Kublai Khan, controlled lands in China, Burma, Central Asia, Russia, Iran, the Middle East, and Eastern Europe. Estimates are that the Mongol armies reduced the population of China by nearly a third. Genghis Khan was a pagan who tolerated nearly every religion, and their culture often suffered the harshest treatment from Mongol armies. The Khan armies pushed as far west as Jerusalem before being defeated in 1260.
Early modern period.
The Russian Empire began to expand into Asia from the 17th century, and would eventually take control of all of Siberia and most of Central Asia by the end of the 19th century. The Ottoman Empire controlled Anatolia, the Middle East, North Africa and the Balkans from the 16th century onwards. In the 17th century, the Manchu conquered China and established the Qing Dynasty. In the 16th century, the Mughal Empire controlled much of India and initiated the second golden age for India. China was the largest economy in the world for much of the time, followed by India until the 18th century.
Ming China.
By 1368, Zhu Yuanzhang had claimed himself Hongwu Emperor and established the Ming Dynasty of China. Immediately, the new emperor and his followers drove the Mongols and their culture out of China and beyond the Great Wall. The new emperor was somewhat suspicious of the scholars that dominated China's bureaucracy, for he had been born a peasant and was uneducated. Nevertheless, Confucian scholars were necessary to China's bureaucracy and were reestablished as well as reforms that would improve the exam systems and make them more important in entering the bureaucracy than ever before. The exams became more rigorous, cut down harshly on cheating, and those who excelled were more highly appraised. Finally, Hongwu also directed more power towards the role of emperor so as to end the corrupt influences of the bureaucrats.
Society and economy.
The Hongwu emperor, perhaps for his sympathy of the common-folk, had built many irrigation systems and other public projects that provided help for the peasant farmers. They were also allowed to cultivate and claim unoccupied land without having to pay any taxes and labor demands were lowered. However, none of this was able to stop the rising landlord class that gained many privileges from the government and slowly gained control of the peasantry. Moneylenders foreclosed on peasant debt in exchange for mortgages and bought up farmer land, forcing them to become the landlords' tenants or to wander elsewhere for work. Also during this time, Neo-Confucianism intensified even more than the previous two dynasties (the Song and Yuan). Focus on the superiority of elders over youth, men over women, and teachers over students resulted in minor discrimination of the "inferior" classes. The fine arts grew in the Ming era, with improved techniques in brush painting that depicted scenes of court, city or country life; people such as scholars or travelers; or the beauty of mountains, lakes, or marshes. The Chinese novel fully developed in this era, with such classics written such as "Water Margin", "Journey to the West", and "Jin Ping Mei".
Economics grew rapidly in the Ming Dynasty as well. The introduction of American crops such as maize, sweet potatoes, and peanuts allowed for cultivation of crops in infertile land and helped prevent famine. The population boom that began in the Song dynasty accelerated until China's population went from 80 or 90 million to 150 million in three centuries, culminating in 1600. This paralleled the market economy that was growing both internally and externally. Silk, tea, ceramics, and lacquer-ware were produced by artisans that traded them in Asia and to Europeans. Westerners began to trade (with some Chinese-assigned limits), primarily in the port-towns of Macau and Canton. Although merchants benefited greatly from this, land remained the primary symbol of wealth in China and traders' riches were often put into acquiring more land. Therefore, little of these riches were used in private enterprises that could've allowed for China to develop the market economy that often accompanied the highly-successful Western countries.
Foreign interests.
In the interest of national glory, the Chinese began sending impressive junk ships across the South China Sea and the Indian Ocean. From 1403 to 1433, the Yongle Emperor commissioned expeditions led by the admiral Zheng He, a Muslim eunuch from China. Chinese junks carrying hundreds of soldiers, goods, and animals for zoos, traveled to Southeast Asia, Persia, southern Arabia, and east Africa to show off Chinese power. Their prowess exceeded that of current Europeans at the time, and had these expeditions not ended, the world economy may be different from today. In 1433, the Chinese government decided that the cost of a navy was an unnecessary expense. The Chinese navy was slowly dismantled and focus on interior reform and military defense began. It was China's longstanding priority that they protect themselves from nomads and they have accordingly returned to it. The growing limits on the Chinese navy would leave them vulnerable to foreign invasion by sea later on.
As was inevitable, Westerners arrived on the Chinese east coast, primarily Jesuit missionaries which reached the mainland in 1582. They attempted to convert the Chinese people to Christianity by first converting the top of the social hierarchy and allowing the lower classes to subsequently convert. To further gain support, many Jesuits adopted Chinese dress, customs, and language. Some Chinese scholars were interested in certain Western teachings and especially in Western technology. By the 1580s, Jesuit scholars like Matteo Ricci and Adam Schall amazed the Chinese elite with technological advances such as European clocks, improved calendars and cannons, and the accurate prediction of eclipses. Although some the scholar-gentry converted, many were suspicious of the Westerners whom they called "barbarians" and even resented them for the embarrassment they received at the hand of Western correction. Nevertheless, a small group of Jesuit scholars remained at the court to impress the emperor and his advisors.
Decline.
Near the end of the 1500s, the extremely centralized government that gave so much power to the emperor had begun to fail as more incompetent rulers took the mantle. Along with these weak rulers came increasingly corrupt officials who took advantage of the decline. Once more the public projects fell into disrepair due to neglect by the bureaucracy and resulted in floods, drought, and famine that rocked the peasantry. The famine soon became so terrible that some peasants resorted to selling their children to slavery to save them from starvation, or to eating bark, the feces of geese, or other people. Many landlords abused the situation by building large estates where desperate farmers would work and be exploited. In turn, many of these farmers resorted to flight, banditry, and open rebellion.
All of this corresponded with the usual dynastic decline of China seen before, as well as the growing foreign threats. In the mid-16th century, Japanese and ethnic Chinese pirates began to raid the southern coast, and neither the bureaucracy nor the military were able to stop them. The threat of the northern Manchu people also grew. The Manchu were an already large state north of China, when in the early 17th century a local leader named Nurhaci suddenly united them under the Eight Banners—armies that the opposing families were organized into. The Manchus adopted many Chinese customs, specifically taking after their bureaucracy. Nevertheless, the Manchus still remained a Chinese vassal. In 1644 Chinese administration became so weak, the 16th and last emperor, the Chongzhen Emperor, did not respond to the severity of an ensuing rebellion by local dissenters until the enemy had invaded the Forbidden City (his personal estate). He soon hanged himself in the imperial gardens. For a brief amount of time, the Shun Dynasty was claimed, until a loyalist Ming official called support from the Manchus to put down the new dynasty. The Shun Dynasty ended within a year and the Manchu were now within the Great Wall. Taking advantage of the situation, the Manchus marched on the Chinese capital of Beijing. Within two decades all of China belonged to the Manchu and the Qing Dynasty was established.
Late modern period.
Qing China.
By 1644, the northern Manchu people had conquered China and established a foreign dynasty—the Qing Dynasty—once more. The Manchu Qing emperors, especially Confucian scholar Kangxi, remained largely conservative—retaining the bureaucracy and the scholars within it, as well as the Confucian ideals present in Chinese society. However, changes in the economy and new attempts at resolving certain issues occurred too. These included increased trade with Western countries that brought large amounts of silver into the Chinese economy in exchange for tea, porcelain, and silk textiles. This allowed for a new merchant-class, the compradors, to develop. In addition, repairs were done on existing dikes, canals, roadways, and irrigation works. This, combined with the lowering of taxes and government-assigned labor, was supposed to calm peasant unrest. However, the Qing failed to control the growing landlord class which had begun to exploit the peasantry and abuse their position.
By the late 18th century, both internal and external issues began to arise in Qing China's politics, society, and economy. The exam system with which scholars were assigned into the bureaucracy became increasingly corrupt; bribes and other forms of cheating allowed for inexperienced and inept scholars to enter the bureaucracy and this eventually caused rampant neglect of the peasantry, military, and the previously mentioned infrastructure projects. Poverty and banditry steadily rose, especially in rural areas, and mass migrations looking for work throughout China occurred. The perpetually conservative government refused to make reforms that could resolve these issues.
Opium War.
China saw its status reduced by what it perceived as parasitic trade with Westerners. Originally, European traders were at a disadvantage because the Chinese cared little for their goods, while European demand for Chinese commodities such as tea and porcelain only grew. In order to tip the trade imbalance in their favor, British merchants began to sell Indian opium to the Chinese. Not only did this sap Chinese bullion reserves, it also led to widespread drug addiction amongst the bureaucracy and society in general. A ban was placed on opium as early as 1729 by the Yongzheng Emperor, but little was done to enforce it. By the early 19th century, under the new Daoguang Emperor, the government began serious efforts to eradicate opium from Chinese society. Leading this endeavour were respected scholar-officials including Imperial Commissioner Lin Zexu.
After Lin destroyed more than 20,000 chests of opium in the summer of 1839, Europeans demanded compensation for what they saw as unwarranted Chinese interference in their affairs. When it was not paid, the British declared war later the same year, starting what became known as the First Opium War. The outdated Chinese junks were no match for the advanced British gunboats, and soon the Yangzi River region came under threat of British bombardment and invasion. The emperor had no choice but to sue for peace, resulting in the exile of Lin and the Treaty of Nanking, which ceded the British control of Hong Kong and opened up trade and diplomacy with other European countries, including Germany, France, and the USA.
Contemporary history.
The European powers had control of other parts of Asia by the early 20th century, such as British India, French Indochina, Spanish East Indies, and Portuguese Macau and Goa. The Great Game between Russia and Britain was the struggle for power in the Central Asian region in the nineteenth century. The Trans-Siberian Railway, crossing Asia by train, was complete by 1916. Parts of Asia remained free from European control, although not influence, such as Persia, Thailand and most of China. In the twentieth century, Imperial Japan expanded into China and Southeast Asia during the Second World War. After the war, many Asian countries became independent from European powers. During the Cold War, the northern parts of Asia were communist controlled with the Soviet Union and People's Republic of China, while western allies formed pacts such as CENTO and SEATO. Conflicts such as the Korean War, Vietnam War and Soviet invasion of Afghanistan were fought between communists and anti-communists. In the decades after the Second World War, a massive restructuring plan drove Japan to become the world's second-largest economy, a phenomenon known as the Japanese post-war economic miracle. The Arab-Israeli conflict has dominated much of the recent history of the Middle East. After the Soviet Union's collapse in 1991, there were many new independent nations in Central Asia.
China.
Prior to World War II, China faced a civil war between Mao Zedong's Communist party and Chiang Kai-shek's nationalist party; the nationalists appeared to be in the lead. However, once the Japanese invaded in 1937, the two parties were forced to form a temporary cease-fire in order to defend China. The nationalists faced many military failures that caused them to lose territory and subsequently, respect from the Chinese masses. In contrast, the communists' use of guerilla warfare (led by Lin Biao) proved effective against the Japanese's conventional methods and put the Communist Party on top by 1945. They also gained popularity for the reforms they were already applying in controlled areas, including land redistribution, education reforms, and widespread health care. For the next four years, the nationalists would be forced to retreat to the small island east of China, known as Taiwan (formerly known as Formosa), where they remain today. In mainland China, the People's Republic of China was established by the Communist Party, with Mao Zedong as its state chairman.
The communist government in China was defined by the party cadres. These hard-line officers controlled the People's Liberation Army, which itself controlled large amounts of the bureaucracy. This system was further controlled by the Central Committee, which additionally supported the state chairman who was considered the head of the government. The People's Republic's foreign policies included the repressing of secession attempts in Mongolia and Tibet and supporting of North Korea and North Vietnam in the Korean War and Vietnam War, respectively. Additionally, by 1960 China began to cut off its connections with the Soviet Union due to border disputes and an increasing Chinese sense of superiority, especially the personal feeling of Mao over the Russian premier, Nikita Khrushchev.
Today China, India, South Korea, Japan and Russia play important roles in world economics and politics. China today is the world's second largest economy and the second fastest growing economy. Indian economy is the seventh-largest in the world by nominal GDP and the third-largest by purchasing power parity and is the fastest growing economy.

</doc>
<doc id="14098" url="https://en.wikipedia.org/wiki?curid=14098" title="History of the Americas">
History of the Americas

The prehistory of the Americas (North, South, and Central America, and the Caribbean) begins with people migrating to these areas from Asia during the height of an Ice Age. These groups are generally believed to have been isolated from peoples of the "Old World" until the coming of Europeans in the 10th century from Norway and with the Voyages of Christopher Columbus in 1492.
The ancestors of today's American Indigenous peoples were the Paleo-Indians; they were hunter-gatherers who migrated into North America. The most popular theory asserts that migrants came to the Americas via the Bering Land Bridge, Beringia, the land mass now covered by the cold ocean waters in the Bering Strait. Small lithic stage peoples followed megafauna like bison, mammoth (now extinct), and caribou, thus gaining the modern nickname "big-game hunters." Groups of people may also have traveled into North America on shelf or sheet ice along the northern Pacific coast.
Cultural traits brought by the first immigrants later evolved and spawned such cultures as Iroquois on North America and Pirahã of South America. These cultures later developed into civilizations. In many cases, these cultures expanded at a later date than their Old World counterparts. Cultures that may be considered advanced or civilized include: Norte Chico, Cahokia, Zapotec, Toltec, Olmec, Maya, Aztec, Purepecha, Chimor, Mixtec, Moche, Mississippian, Puebloan, Totonac, Teotihuacan, Huastec people, Purépecha, Izapa, Mazatec, Muisca, and the Inca.
After the voyages of Christopher Columbus in 1492, Spanish, Portuguese and later English, French and Dutch colonial expeditions arrived in the New World, conquering and settling the discovered lands, which led to a transformation of the cultural and physical landscape in the Americas. Spain colonized most of the American continent from present-day Southwestern United States, Florida and the Caribbean to the southern tip of South America. Portugal settled in what is mostly present-day Brazil while England established colonies in the Eastern coast of the United States, as well as the North Pacific coast and most of Canada. France settled in Quebec and other parts of Eastern Canada and claimed an area in what is today Central United States. The Netherlands settled some Caribbean islands and parts of Northern South America.
European colonization of the Americas led to the rise of new cultures, civilizations and eventually states, which resulted from the fusion of Native American and European traditions, peoples and institutions. The transformation of American cultures through colonization is evident in architecture, religion, gastronomy, the arts and particularly languages, the most widespread being Spanish (376 million speakers), English (348 million) and Portuguese (201 million). The colonial period lasted approximately three centuries, from the early 16th to the early 19th centuries, when Brazil and the larger Hispanic American nations declared independence. The United States obtained independence from England much earlier, in 1776, while Canada formed a federal dominion in 1867. Others remained attached to their European parent state until the end of the 19th century, such as Cuba and Puerto Rico which were linked to Spain until 1898. Smaller territories such as Guyana obtained independence in the mid-20th century, while certain Caribbean islands remain part of a European power to this day.
Pre-colonization.
Migration into the continents.
The specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000 – 17,000 years ago, when sea levels were significantly lowered due to the Quaternary glaciation. These people are believed to have followed herds of now-extinct pleistocene megafauna along "ice-free corridors" that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific Northwest coast to South America. Evidence of the latter would since have been covered by a sea level rise of a hundred meters following the last ice age.
Archaeologists contend that the Paleo-Indian migration out of Beringia (eastern Alaska), ranges from 40,000 to around 16,500 years ago. This time range is a hot source of debate. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 16,000 – 13,000 years before present.
The American Journal of Human Genetics released an article in 2007 stating "Here we show, by using 86 complete mitochondrial genomes, that all Indigenous American haplogroups, including Haplogroup X (mtDNA), were part of a single founding population." Amerindian groups in the Bering Strait region exhibit perhaps the strongest DNA or mitochondrial DNA relations to Siberian peoples. The genetic diversity of Amerindian indigenous groups increase with distance from the assumed entry point into the Americas. Certain genetic diversity patterns from West to East suggest, particularly in South America, that migration proceeded first down the west coast, and then proceeded eastward. Geneticists have variously estimated that peoples of Asia and the Americas were part of the same population from 42,000 to 21,000 years ago.
New studies shed light on the founding population of indigenous Americans, suggesting that their ancestry traced to both east Asian and western Eurasians who migrated to North America directly from Siberia. A 2013 study in the journal Nature reported that DNA found in the 24,000-year-old remains of a young Boy in Mal’ta Siberia suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have "had a more north-easterly distribution 24,000 years ago than commonly thought" Professor Kelly Graf said that "Our findings are significant at two levels. First, it shows that Upper Paleolithic Siberians came from a cosmopolitan population of early modern humans that spread out of Africa to Europe and Central and South Asia. Second, Paleoindian skeletons with phenotypic traits atypical of modern-day Native Americans can be explained as having a direct historical connection to Upper Paleolithic Siberia." A route through Beringia is seen as more likely than the Solutrean hypothesis.
On October 3, 2014, the Oregon cave, where the oldest DNA evidence of human habitation in North America was found, was added to the National Register of Historic Places. The DNA, radiocarbon dated to 14,300 years ago, was found in fossilized human coprolites uncovered in the Paisley Five Mile Point Caves in south-central Oregon.
Lithic stage (before 8000 BCE).
The Lithic stage or "Paleo-Indian period", is the earliest classification term referring to the first stage of human habitation in the Americas, covering the Late Pleistocene era. The time period derives its name from the appearance of "Lithic flaked" stone tools. Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest well known human activity in the Americas. Lithic reduction stone tools are used by archaeologists and anthropologists to classify cultural periods.
Archaic stage (8000 BCE – 1000 BCE).
Several thousand years after the first migrations, the first complex civilizations arose as hunter-gatherers settled into semi-agricultural communities. Identifiable sedentary settlements began to emerge in the so-called Middle Archaic period around 6000 BCE. Particular archaeological cultures can be identified and easily classified throughout the Archaic period.
In the late Archaic, on the north-central coastal region of Peru, a complex civilization arose which has been termed the Norte Chico civilization, also known as Caral-Supe. It is the oldest known civilization in the Americas and one of the six sites where civilization originated independently and indigenously in the ancient world, flourishing between the 30th and 18th centuries BC. It pre-dated the Mesoamerican Olmec civilization by nearly two millennia. It was contemporaneous with the Egypt following the unification of its kingdom under Menes and the emergence of the first Egyptian hieroglyphics.
Monumental architecture, including earthwork platform mounds and sunken plazas have been identified as part of the civilization. Archaeological evidence points to the use of textile technology and the worship of common god symbols. Government, possibly in the form of theocracy, is assumed to have been required to manage the region. However, numerous questions remain about its organization. In archaeological nomenclature, the culture was pre-ceramic culture of the pre-Columbian Late Archaic period. It appears to have lacked ceramics and art.
Ongoing scholarly debate persists over the extent to which the flourishing of Norte Chico resulted from its abundant maritime food resources, and the relationship that these resources would suggest between coastal and inland sites.
The role of seafood in the Norte Chico diet has been a subject of scholarly debate. In 1973, examining the Aspero region of Norte Chico, Michael E. Moseley contended that a maritime subsistence (seafood) economy had been the basis of society and its early flourishing. This theory, later termed "maritime foundation of Andean Civilization" was at odds with the general scholarly consensus that civilization arose as a result of intensive grain-based agriculture, as had been the case in the emergence of civilizations in northeast Africa (Egypt) and southwest Asia (Mesopotamia).
While earlier research pointed to edible domestic plants such as squash, beans, lucuma, guava, pacay, and camote at Caral, publications by Haas and colleagues have added avocado, achira, and corn (Zea Mays) to the list of foods consumed in the region. In 2013, Haas and colleagues reported that maize was a primary component of the diet throughout the period of 3000 to 1800 BC.
Cotton was another widespread crop in Norte Chico, essential to the production of fishing nets and textiles. Jonathan Haas noted a mutual dependency, whereby "The prehistoric residents of the Norte Chico needed the fish resources for their protein and the fishermen needed the cotton to make the nets to catch the fish."
In the 2005 book "", journalist Jonathan Mann surveyed the literature at the time, reporting a date "sometime before 3200 BC, and possibly before 3500 BC" as the beginning date for the formation of Norte Chico. He notes that the earliest date securely associated with a city is 3500 BC, at Huaricanga in the (inland) Fortaleza area.
The Norte Chico civilization began to decline around 1800 BC as more powerful centers appeared to the south and north along its coast, and to the east within the Andes Mountains.
Mesoamerica, the Woodland Period, and Mississippian Culture (2000 BCE – 500 CE).
After the decline of the Norte Chico civilization, several large, centralized civilizations developed in the Western Hemisphere: Chavin, Nazca, Moche, Huari, Quitus, Cañaris, Chimu, Pachacamac, Tiahuanaco, Aymara and Inca in the Central Andes (Ecuador, Peru and Bolivia); Muisca in Colombia ; Taínos in Dominican Republic (Hispaniola, Española) and part of Caribbean; and the Olmecs, Maya, Toltecs, Mixtecs, Zapotecs, Aztecs and Purepecha in southern North America (Mexico, Guatemala).
The Olmec civilization was the first Mesoamerican civilization, beginning around 1600-1400 BC and ending around 400 BC. Mesoamerica is considered one of the six sites around the globe in which civilization developed independently and indigenously. This civilization is considered the mother culture of the Mesoamerican civilizations. The Mesoamerican calendar, numeral system, writing, and much of the Mesoamerican pantheon seem to have begun with the Olmec.
Some elements of agriculture seem to have been practiced in Mesoamerica quite early. The domestication of maize is thought to have begun around 7,500 to 12,000 years ago. The earliest record of lowland maize cultivation dates to around 5100 BC. Agriculture continued to be mixed with a hunting-gathering-fishing lifestyle until quite late compared to other regions, but by 2700 BC, Mesoamericans were relying on maize, and living mostly in villages. Temple mounds and classes started to appear. By 1300/ 1200 BC, small centres coalesced into the Olmec civilization, which seems to have been a set of city-states, united in religious and commercial concerns. The Olmec cities had ceremonial complexes with earth/clay pyramids, palaces, stone monuments, aqueducts and walled plazas. The first of these centers was at San Lorenzo (until 900 bc). La Venta was the last great Olmec centre. Olmec artisans sculpted jade and clay figurines of Jaguars and humans. Their iconic giant heads - believed to be of Olmec rulers - stood in every major city.
The Olmec civilization ended in 400 BC, with the defacing and destruction of San Lorenzo and La Venta, two of the major cities. It nevertheless spawned many other states, most notably the Mayan civilization, whose first cities began appearing around 700/ 600 BC. Olmec influences continued to appear in many later Mesoamerican civilizations.
Cities of the Aztecs, Mayas, and Incas were as large and organized as the largest in the Old World, with an estimated population of 200,000 to 350,000 in Tenochtitlan, the capital of the Aztec empire. The market established in the city was said to have been the largest ever seen by the conquistadors when they arrived. The capital of the Cahokians, Cahokia - located near modern East St. Louis, Illinois may have reached a population of over 20,000. At its peak, between the 12th and 13th centuries, Cahokia may have been the most populous city in North America. Monk's Mound, the major ceremonial center of Cahokia, remains the largest earthen construction of the prehistoric New World.
These civilizations developed agriculture as well, breeding maize (corn) from having ears 2–5 cm in length to perhaps 10–15 cm in length. Potatoes, tomatoes, pumpkins, beans, avocados, and chocolate are now the most popular of the pre-Columbian agricultural products. The civilizations did not develop extensive livestock as there were few suitable species, although alpacas and llamas were domesticated for use as beasts of burden and sources of wool and meat in the Andes. By the 15th century, maize was being farmed in the Mississippi River Valley after introduction from Mexico. The course of further agricultural development was greatly altered by the arrival of Europeans.
Classic stage (800 CE – 1533 CE).
Oasisamerica.
The Pueblo people of what is now the Southwestern United States and northern Mexico, living conditions were that of large stone apartment like adobe structures. They live in Arizona, New Mexico, Utah, Colorado, and possibly surrounding areas.
Aridoamerica.
Chichimeca was the name that the Mexica (Aztecs) generically applied to a wide range of semi-nomadic peoples who inhabited the north of modern-day Mexico, and carried the same sense as the European term "barbarian". The name was adopted with a pejorative tone by the Spaniards when referring especially to the semi-nomadic hunter-gatherer peoples of northern Mexico.
Mesoamerica.
The Zapotec emerged around 1500 years BCE. Their writing system influenced the later Olmec. They left behind the great city Monte Alban.
The Olmec civilization emerged around 1200 BCE in Mesoamerica and ended around 400 BCE. Olmec art and concepts influenced surrounding cultures after their downfall. This civilization was thought to be the first in America to develop a writing system. After the Olmecs abandoned their cities for unknown reasons, the Maya, Zapotec and Teotihuacan arose.
The Purepecha civilization emerged around 1000 CE in Mesoamerica . They flourished from 1100 CE to 1530 CE. They continue to live on in the state of Michoacán. Fierce warriors, they were never conquered and in their glory years, successfully sealed off huge areas from Aztec domination.
Maya history spans 3,000 years. The Classic Maya may have collapsed due to changing climate in the end of the 10th century.
The Toltec were a nomadic people, dating from the 10th - 12th century, whose language was also spoken by the Aztecs.
Teotihuacan (4th century BCE - 7/8th century CE) was both a city, and an empire of the same name, which, at its zenith between 150 and the 5th century, covered most of Mesoamerica.
The Aztec having started to build their empire around 14th century found their civilization abruptly ended by the Spanish conquistadors. They lived in Mesoamerica, and surrounding lands. Their capital city Tenochtitlan was one of the largest cities of all time.
South America.
The oldest known civilization of the Americas was established in the Norte Chico region of modern Peru. Complex society emerged in the group of coastal valleys, between 3000 and 1800 BCE. The Quipu, a distinctive recording device among Andean civilizations, apparently dates from the era of Norte Chico's prominence.
The Chavín established a trade network and developed agriculture by as early as (or late compared to the Old World) 900 BCE according to some estimates and archaeological finds. Artifacts were found at a site called Chavín in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned from 900 BCE to 300 BCE.
Holding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533.
Known as "Tahuantinsuyu", or "the land of the four regions", in Quechua, the Inca culture was highly distinct and developed. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful trepanation of the skull in Inca civilization.
European colonization.
Around 1000, the Vikings established a short-lived settlement in Newfoundland, now known as L'Anse aux Meadows. Speculations exist about other Old World discoveries of the New World, but none of these are generally or completely accepted by most scholars. 
Spain sponsored a major exploration led by Christopher Columbus in 1492; it quickly led to extensive European colonization of the Americas. The Europeans brought Old World diseases which are thought to have caused catastrophic epidemics and a huge decrease of the native population. Columbus came at a time in which many technical developments in sailing techniques and communication made it possible to report his voyages easily and to spread word of them throughout western Europe. It was also a time of growing religious, imperial and economic rivalries that led to a competition for the establishment of colonies.
Colonial period.
15th to 19th century colonies in the New World:
Decolonization.
The formation of sovereign states in the New World begins with the United States Declaration of Independence of 1776. The American Revolutionary War lasted until 1783.
The Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Simón Bolívar and José de San Martín, among others, led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of the continent politically allied, they rapidly became independent of one another as well, and several further wars were fought, such as the Paraguayan War and the War of the Pacific. (See Latin American integration.) In the Portuguese colony Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese king Dom João VI, proclaimed the country's independence in 1822 and became Brazil's first Emperor. This was peacefully accepted by the crown in Portugal, upon compensation.
Effects of slavery.
Slavery has had a significant role in the economic development the New World after the colonization of the Americas by the Europeans. The cotton, tobacco, and sugar cane harvested by slaves became important exports for the United States and the Caribbean countries.
20th century.
North America.
As a part of the British Empire Canada immediately entered World War I. Canada bore the brunt of several major battles during the early stages of the war including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on July 1, 1916, the First day on the Somme.
The United States stayed apart from the conflict until 1917, joining the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe. Mexico was not part of the war as the country was embroiled in the Mexican Revolution at the time.
The 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street Crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada. From 1936 to 1949, this was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.
Once again, Canada found herself at war before her neighbours, however even Canadian contributions were slight before the Japanese attack on Pearl Harbor. The entry of the United States into the war helped to tip the balance in favour of the allies. Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to declare war to the Axis nations and entered the conflict. The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs. The United States especially emerged as a "superpower".
The early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. In Canada Quebec was transformed by the Quiet Revolution and the emergence of Quebec nationalism. Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as the ""El Milagro Mexicano"" (Mexican miracle). The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.
The Civil Rights Movement ended Jim Crow and empowered black voters in the 1960s, which allowed blacks to move into high government offices. However, the dominant New Deal coalition collapsed in the mid 1960s in disputes over race and the Vietnam War, and the conservative movement began its rise to power, as the once dominant liberalism weakened and collapsed.. Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. Eventually in 1982 at the end of his tenure, Canada received a new constitution.
Canada's Brian Mulroney not only ran on a similar platform but also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989. Mexican presidents Miguel de la Madrid, in the early 1980s and Carlos Salinas de Gortari in the late 1980s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and regulation to stimulate the economy.
The end of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On January 1, 1994, Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years. The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which also involved Canada. Canada did not support the United States' later move to invade Iraq, however.
In the U.S. the Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s (decade), which burst in 2008 and became the Late-2000s recession with prolonged unemployment.
Central America.
Despite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856–1857 the region successfully established a military coalition to repel an invasion by United States adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).
In 1907, a Central American Court of Justice was created. On December 13, 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market ("CACM"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 "Football War" between El Salvador and Honduras. A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.
South America.
In the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These dictatorships detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the United States Cold War doctrine of "National Security" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see Túpac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is widespread now. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued.
International indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century. In recent years, South American governments have drifted to the left, with socialist leaders being elected in Chile, Bolivia, Brazil, Venezuela, and a leftist president in Argentina and Uruguay. Despite the move to the left, South America is still largely capitalist. With the founding of the Union of South American Nations, South America has started down the road of economic integration, with plans for political integration in the European Union style.
Notes on Historiography.
The historiography of the Americas has been changing over time. Prior to the 1950s, the study of history prioritized European civilization and its emergence as the beginning of all global civilization. Many scholars (such as Grafton Elliot Smith) suggested that cultural diffusion from a single originating culture was the only way to explain new technologies—a theory called hyperdiffusionism. History prior to 1950 prioritized written documents as the record of a culture. Given the relative abundance of European (and later, Chinese) documents, this emphasis "writing as history" placed little emphasis on pre-literate cultures and cultures with unconventional means of documentation (e. g., quipus). However, since then, there has been a broader use of archaeology, anthropology, and genetics as a means of describing the history of civilization prior to the emergence of writing, plus an appreciation for non-written records. It has been also recognized that independent development of technologies can occur in different locations. As such, there has been a greater recognition of the development of indigenous American cultural development.

</doc>
