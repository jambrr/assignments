<doc id="16339" url="https://en.wikipedia.org/wiki?curid=16339" title="Johannes Brahms">
Johannes Brahms

Johannes Brahms (; 7 May 1833 – 3 April 1897) was a German composer and pianist. Born in Hamburg into a Lutheran family, Brahms spent much of his professional life in Vienna, Austria. In his lifetime, Brahms's popularity and influence were considerable. He is considered one of the greatest composers in history, and is sometimes grouped with Johann Sebastian Bach and Ludwig van Beethoven as one of the "Three Bs", a comment originally made by the nineteenth-century conductor Hans von Bülow.
Brahms composed for piano, organ, chamber ensembles, symphony orchestra, and for voice and chorus. A virtuoso pianist, he premiered many of his own works. He worked with some of the leading performers of his time, including the pianist Clara Schumann and the violinist Joseph Joachim (the three were close friends). Many of his works have become staples of the modern concert repertoire. Brahms, an uncompromising perfectionist, destroyed some of his works and left others unpublished.
Brahms is often considered both a traditionalist and an innovator. His music is firmly rooted in the structures and compositional techniques of the Baroque and Classical masters. He was a master of counterpoint, the complex and highly disciplined art for which Johann Sebastian Bach is famous, and of development, a compositional ethos pioneered by Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and other composers. Brahms aimed to honour the "purity" of these venerable "German" structures and advance them into a Romantic idiom, in the process creating bold new approaches to harmony and melody. While many contemporaries found his music too academic, his contribution and craftsmanship have been admired by subsequent figures as diverse as Arnold Schoenberg and Edward Elgar. The diligent, highly constructed nature of Brahms's works was a starting point and an inspiration for a generation of composers. Within his meticulous structures is embedded, however, a highly romantic nature.
Life.
Early years.
Brahms's father, Johann Jakob Brahms (1806–72), came to Hamburg from Dithmarschen, seeking a career as a town musician. He was proficient on several instruments, but found employment mostly playing the horn and double bass. In 1830, he married Johanna Henrika Christiane Nissen (1789–1865), a seamstress never previously married, who was seventeen years older than he was. Johannes Brahms had an older sister and a younger brother. Initially, they lived near the city docks, in the Gängeviertel quarter of Hamburg's Neustadt, for six months, before moving to a small house on the Dammtorwall, a small street near the Inner Alster.
Johann Jakob gave his son his first musical training. He studied piano from the age of seven with Otto Friedrich Willibald Cossel. Owing to the family's poverty, the adolescent Brahms had to contribute to the family's income by playing the piano in dance halls. Early biographers found this shocking and played down this portion of his life. Some modern writers have suggested that this early experience warped Brahms's later relations with women, but Brahms scholars Styra Avins and Kurt Hoffmann have questioned the possibility. Jan Swafford has contributed to the discussion.
For a time, Brahms also learned the cello. After his early piano lessons with Otto Cossel, Brahms studied piano with Eduard Marxsen, who had studied in Vienna with Ignaz von Seyfried (a pupil of Mozart) and Carl Maria von Bocklet (a close friend of Schubert). The young Brahms gave a few public concerts in Hamburg, but did not become well known as a pianist until he made a concert tour at the age of nineteen. (In later life, he frequently took part in the performance of his own works, whether as soloist, accompanist, or participant in chamber music.) He conducted choirs from his early teens, and became a proficient choral and orchestral conductor.
Meeting Joachim and Liszt.
He began to compose quite early in life, but later destroyed most copies of his first works; for instance, , a fellow-pupil of Marxsen, reported a piano sonata, that Brahms had played or improvised at the age of 11, had been destroyed. His compositions did not receive public acclaim until he went on a concert tour as accompanist to the Hungarian violinist Eduard Reményi in April and May 1853. On this tour he met Joseph Joachim at Hanover, and went on to the Court of Weimar where he met Franz Liszt, Peter Cornelius, and Joachim Raff. According to several witnesses of Brahms's meeting with Liszt (at which Liszt performed Brahms's Scherzo, Op. 4, at sight), Reményi was offended by Brahms's failure to praise Liszt's Sonata in B minor wholeheartedly (Brahms supposedly fell asleep during a performance of the recently composed work), and they parted company shortly afterwards. Brahms later excused himself, saying that he could not help it, having been exhausted by his travels.
Brahms and the Schumanns.
Joachim had given Brahms a letter of introduction to Robert Schumann, and after a walking tour in the Rhineland, Brahms took the train to Düsseldorf, and was welcomed into the Schumann family on arrival there. Schumann, amazed by the 20-year-old's talent, published an article entitled "" (New Paths) in the 28 October 1853 issue of the journal "Neue Zeitschrift für Musik" alerting the public to the young man, who, he claimed, was "destined to give ideal expression to the times." This pronouncement impressed people who were admirers of Robert or Clara Schumann; for example, in Hamburg, a music publisher and the conductor of the Philharmonic, but it was received with some skepticism by others. It may have increased Brahms's self-critical need to perfect his works. He wrote to Robert, "Revered Master," in November 1853, that his praise "Will arouse such extraordinary expectations by the public that I don't know how I can begin to fulfil them ..." While he was in Düsseldorf, Brahms participated with Schumann and Albert Dietrich in writing a sonata for Joachim; this is known as the "F–A–E Sonata – Free but Lonely" (). Schumann's wife, the composer and pianist Clara, wrote in her diary about his first visit that Brahms
After Robert Schumann's attempted suicide and subsequent confinement in a mental sanatorium near Bonn in February 1854, Clara was "in despair," expecting the Schumanns' eighth child. Brahms hurried to Düsseldorf. He and/or Joachim, Dietrich, and Julius Otto Grimm visited Clara often in March 1854, to divert her mind from Robert's tragedy by playing music for or with her. Clara wrote in her diary
Later, to help Clara and her many children, Brahms lodged above the Schumann apartment in a three-storey house, setting his musical career aside temporarily. Clara was not allowed to visit Robert until two days before his death. Brahms was able to visit him several times and so could act as a go-between. The Schumanns employed a housekeeper, "Bertha" in Düsseldorf, later Elisabeth Werner in Berlin. There was also a hired cook, in Berlin "Josephine." When the Schumanns' oldest child and daughter, Marie, born 1841, was of age, she took over as housekeeper and when needed, as cook. Clara was often away on concert tours, some lasting months, or sometimes in the summer for cures, and in 1854–1856 Brahms also was away part of the time, leaving the staff to manage the household. Clara much appreciated Brahms's support as a kindred musical spirit.
In a concert in Leipzig in October 1854, Clara played the Andante and Scherzo from Brahms's Sonata in F minor, Op. 5, "the first time his music was played in public."
Brahms and Clara had a very close and lifelong but unusual relationship. They had great affection but also respect for one another. Brahms urged in 1887 that all his and Clara's letters to each other should be destroyed. Actually Clara kept quite a number of letters Brahms had sent her, and at Marie's urging, refrained from destroying many of the letters Brahms had returned. Eventually correspondence between Clara and Brahms in German was published. Some of Brahms's earliest letters to Clara show him deeply in love with her. Clara's preserved letters to Brahms, except for one, begin much later, in 1858. Selected letters or excerpts from them, some to or from Brahms, and diary entries of Clara's have been translated into English. The earliest excerpted and translated letter from Brahms to Clara was in October 1854. Hans Gál cautions that the preserved correspondence may have "passed through Clara's censorship."
Brahms felt a strong conflict between love of Clara and respect for her and Robert, leading him to allude at one point to suicidal thoughts. Not long after Robert died, Brahms decided he had to break away from the Schumann household. He took leave rather brusquely, leaving Clara feeling hurt. But Brahms and Clara kept up correspondence. Brahms joined Clara and some of her children for some summer sojourns. In 1862, Clara bought a house in Lichtental, then adjoining, but since 1909 included in Baden-Baden, and lived there with her remaining family from 1863 to 1873. Brahms from 1865 to 1874 spent parts of some summers living in an apartment nearby in a house which is now a museum, the "Brahmshaus" (Brahms house). Brahms appears in later years as a rather avuncular figure in Eugenie Schumann's account. Clara and Brahms took a concert tour together, in November–December 1868 in Vienna, then in early 1869 to England, then Holland; the tour ended in April 1869. After Clara moved from Lichtental to Berlin in 1873, the two saw each other less often, as Brahms had his home in Vienna since 1863.
Clara was 14 years older than Brahms. In a letter to her 24 May 1856, two and a half years after meeting her, and after two years either together or corresponding, Brahms wrote that he continued to call her the German polite form "Sie" of "you" and hesitated to use the familiar form "Du." Clara agreed that they call one another "Du," writing in her diary "I could not refuse, for indeed I love him like a son". Brahms wrote on 31 May:
"I wish I could write to you as tenderly as I love you, and do as many good things for you, as you would like. You are so infinitely dear to me that I can hardly express it. I should like to call you darling and lots of other names, without ever getting enough of adoring you."
The rest of that letter, and most later preserved letters, are about music and musical people, updating one another about their travels and experiences. Brahms much valued Clara's opinions as a composer. "There was no composition by Brahms that was not shown to Clara the moment it was in shape to be communicated. She remained his faithfully devoted adviser." In a letter to Joachim in 1859, three years after Robert's death, Brahms wrote about Clara:
"I believe that I do not respect and admire her so much as I love her and am under her spell. Often I must forcibly restrain myself from just quietly putting my arms around her and even—I don't know, it seems so natural that she would not take it ill."
Brahms never married, despite strong feelings for several women and despite entering into an engagement, soon broken off, with Agathe von Siebold in Göttingen in 1859. It seems that Brahms was rather indiscreet about the relationship while it lasted, which troubled his friends. After breaking off the engagement, Brahms wrote to Agathe: "I love you! I must see you again, but I am incapable of bearing fetters. Please write me whether I may come again to clasp you in my arms, to kiss you, and tell you that I love you." But they never saw one another again.
Detmold and Hamburg.
After Robert Schumann's death at the sanatorium in 1856, Brahms divided his time between Hamburg, where he formed and conducted a ladies' choir, and Detmold in the Principality of Lippe, where he was court music-teacher and conductor. He was the soloist at the premiere of his Piano Concerto No. 1, his first orchestral composition to be performed publicly, in 1859. He first visited Vienna in 1862, staying there over the winter, and, in 1863, was appointed conductor of the Vienna Singakademie. Though he resigned the position the following year, and entertained the idea of taking up conducting posts elsewhere, he based himself increasingly in Vienna and soon made his home there. From 1872 to 1875, he was director of the concerts of the Vienna Gesellschaft der Musikfreunde; afterwards, he accepted no formal position. He declined an honorary doctorate of music from the University of Cambridge in 1877, but accepted one from the University of Breslau in 1879, and composed the "Academic Festival Overture" as a gesture of appreciation.
He had been composing steadily throughout the 1850s and 60s, but his music had evoked divided critical responses, and the first Piano Concerto had been badly received in some of its early performances. His works were labelled old-fashioned by the 'New German School' whose principal figures included Franz Liszt, Richard Wagner, and Hector Berlioz. Brahms admired some of Wagner's music and admired Liszt as a great pianist, but the conflict between the two schools, known as the War of the Romantics, soon embroiled all of musical Europe. In the Brahms camp were his close friends: Clara Schumann, Joachim, the influential music critic Eduard Hanslick, and the leading Viennese surgeon Theodor Billroth. In 1860, Brahms attempted to organize a public protest against some of the wilder excesses of the Wagnerians' music. This took the form of a manifesto, written by Brahms and Joachim jointly. The manifesto, which was published prematurely with only three supporting signatures, was a failure, and he never engaged in public polemics again.
Years of popularity.
It was the premiere of "A German Requiem", his largest choral work, in Bremen in 1868 that confirmed Brahms's European reputation and led many to accept that he had conquered Beethoven and the symphony. This may have given him the confidence finally to complete a number of works that he had wrestled with over many years such as the cantata "Rinaldo", his first string quartet, third piano quartet, and most notably his first symphony. This appeared in 1876, though it had been begun (and a version of the first movement seen by some of his friends) in the early 1860s. The other three symphonies then followed in 1877, 1883, and 1885. From 1881, he was able to try out his new orchestral works with the Meiningen Court Orchestra of the Duke of Meiningen, whose conductor was Hans von Bülow. He was the soloist at the premiere of his Piano Concerto No. 2 in 1881, in Pest.
Brahms frequently travelled, both for business (concert tours) and pleasure. From 1878 onwards, he often visited Italy in the springtime and he usually sought out a pleasant rural location in which to compose during the summer. He was a great walker and especially enjoyed spending time in the open air, where he felt that he could think more clearly.
In 1889, one Theo Wangemann, a representative of American inventor Thomas Edison, visited the composer in Vienna and invited him to make an experimental recording. Brahms played an abbreviated version of his first Hungarian dance and "Die Libelle" on the piano. The recording was later issued on an LP of early piano performances (compiled by Gregor Benko). Although the spoken introduction to the short piece of music is quite clear, the piano playing is largely inaudible due to heavy surface noise. Nevertheless, this remains the earliest recording made by a major composer. Analysts and scholars remain divided, however, as to whether the voice that introduces the piece is that of Wangemann or of Brahms. Several attempts have been made to improve the quality of this historic recording; a "denoised" version was produced at Stanford University which claims to solve the mystery.
In 1889, Brahms was named an honorary citizen of Hamburg, until 1948 the only one born in Hamburg.
Brahms and Dvořák.
In 1875, the composer Antonín Dvořák (1841–1904) was still virtually unknown outside the Prague region. Brahms was on the jury which awarded the Vienna State Prize for composition to Dvořák three times, first in February 1875, and later in 1876 and 1877. Brahms also recommended Dvořák to his publisher, Simrock, who commissioned the highly successful "Slavonic Dances". Within a few years, Dvořák gained world renown, and in 1892 he was appointed Director of the newly established National Conservatory in New York.
Later years and death.
In 1890, the 57-year-old Brahms resolved to give up composing. However, as it turned out, he was unable to abide by his decision, and in the years before his death he produced a number of acknowledged masterpieces. His admiration for Richard Mühlfeld, clarinetist with the Meiningen orchestra, moved him to compose the Clarinet Trio, Op. 114, Clarinet Quintet, Op. 115 (1891), and the two Clarinet Sonatas, Op. 120 (1894). In November 1892 Brahms declined an invitation from Cambridge University to visit there and receive an honorary degree. In Brahms's absence, possibly occasioned by aversion to seasickness, his first symphony, as yet unpublished, had its English premiere, conducted by Joachim, in Cambridge 8 March 1877. Brahms also wrote several cycles of piano pieces, Opp. 116–119, the "Vier ernste Gesänge" (Four Serious Songs), Op. 121 (1896), and the Eleven Chorale Preludes for organ, Op. 122 (1896).
While completing the Op. 121 songs, Brahms developed cancer (sources differ on whether this was of the liver or pancreas). His last appearance in public was on 3 March 1897, when he saw Hans Richter conduct his Symphony No. 4. There was an ovation after each of the four movements. His condition gradually worsened and he died a month later, on 3 April 1897, aged 63. Brahms is buried in the Zentralfriedhof in Vienna, under a monument by Victor Horta and the sculptor Ilse von Twardowski-Conrat.
Tributes.
Later that year, the British composer Hubert Parry, who considered Brahms the greatest artist of the time, wrote an orchestral "Elegy for Brahms". This was never played in Parry's lifetime, receiving its first performance at a memorial concert for Parry himself in 1918.
From 1904 to 1914, Brahms's friend, the music critic Max Kalbeck published an eight-volume biography of Brahms, but this has never been translated into English. Between 1906 and 1922, the Deutsche Brahms-Gesellschaft (German Brahms Society) published 16 numbered volumes of Brahms's correspondence, at least 7 of which were edited by Kalbeck. An additional 7 volumes of Brahms's correspondence were published later, including two volumes with Clara Schumann, edited by Marie Schumann.
Music of Brahms.
Works.
Brahms wrote a number of major works for orchestra, including two serenades, four symphonies, two piano concertos (No. 1 in D minor; No. 2 in B-flat major), a Violin Concerto, a Double Concerto for violin and cello, and two companion orchestral overtures, the "Academic Festival Overture" and the "Tragic Overture".
His large choral work "A German Requiem" is not a setting of the liturgical "Missa pro defunctis" but a setting of texts which Brahms selected from the Luther Bible. The work was composed in three major periods of his life. An early version of the second movement was first composed in 1854, not long after Robert Schumann's attempted suicide, and this was later used in his first piano concerto. The majority of the Requiem was composed after his mother's death in 1865. The fifth movement was added after the official premiere in 1868, and the work was published in 1869.
Brahms's works in variation form include, among others, the "Variations and Fugue on a Theme by Handel" and the "Paganini Variations", both for solo piano, and the "Variations on a Theme by Haydn" (now sometimes called the "Saint Anthony Variations") in versions for two pianos and for orchestra. The final movement of the Fourth Symphony, Op. 98, is formally a passacaglia.
His chamber works include three string quartets, two string quintets, two string sextets, a clarinet quintet, a clarinet trio, a horn trio, a piano quintet, three piano quartets, and four piano trios (the fourth being published posthumously). He composed several instrumental sonatas with piano, including three for violin, two for cello, and two for clarinet (which were subsequently arranged for viola by the composer). His solo piano works range from his early piano sonatas and ballades to his late sets of character pieces. Brahms was a significant Lieder composer, who wrote over 200 of them. His chorale preludes for organ, Op. 122, which he wrote shortly before his death, have become an important part of the organ repertoire.
Brahms was an extreme perfectionist. He destroyed many early works – including a violin sonata he had performed with Reményi and violinist Ferdinand David – and once claimed to have destroyed 20 string quartets before he issued his official First in 1873. Over the course of several years, he changed an original project for a symphony in D minor into his first piano concerto. In another instance of devotion to detail, he laboured over the official First Symphony for almost fifteen years, from about 1861 to 1876. Even after its first few performances, Brahms destroyed the original slow movement and substituted another before the score was published. (A conjectural restoration of the original slow movement has been published by Robert Pascall.)
Another factor that contributed to Brahms's perfectionism was that Schumann had announced early on that Brahms was "destined to give ideal expression to the times", a prediction that Brahms was determined to live up to. This prediction hardly added to the composer's self-confidence, and may have contributed to the delay in producing the First Symphony.
Brahms strongly preferred writing absolute music that does not refer to an explicit scene or narrative, and he never wrote an opera or a symphonic poem.
Despite his reputation as a serious composer of large, complex musical structures, some of Brahms's most widely known and most commercially successful compositions during his life were small-scale works of popular intent aimed at the thriving contemporary market for domestic music-making. During the 20th century, the influential American critic B. H. Haggin, rejecting more mainstream views, argued in his various guides to recorded music that Brahms was at his best in such works and much less successful in larger forms. Among the most cherished of these lighter works by Brahms are his sets of popular dances, the "Hungarian Dances", the "Waltzes" for piano duet (Op. 39), and the "Liebeslieder Waltzes" for vocal quartet and piano, and some of his many songs, notably the "Wiegenlied" (Op. 49, No. 4, published in 1868). This last was written (to a folk text) to celebrate the birth of a son to Brahms's friend Bertha Faber and is universally known as "Brahms's Lullaby".
Style and influences.
Brahms maintained a classical sense of form and order in his works, in contrast to the opulence of the music of many of his contemporaries. Thus, many admirers (though not necessarily Brahms himself) saw him as the champion of traditional forms and "pure music", as opposed to the "New German" embrace of programme music.
Brahms venerated Beethoven; in the composer's home, a marble bust of Beethoven looked down on the spot where he composed, and some passages in his works are reminiscent of Beethoven's style. Brahms's First Symphony bears strongly the influence of Beethoven's Fifth Symphony, as the two works are both in C minor and end in the struggle towards a C major triumph. The main theme of the finale of the First Symphony is also reminiscent of the main theme of the finale of Beethoven's Ninth, and when this resemblance was pointed out to Brahms he replied that any dunce could see that. In 1876, when the work was premiered in Vienna, it was immediately hailed as "Beethoven's Tenth". However, the similarity of Brahms's music to that of late Beethoven had first been noted as early as November 1853 in a letter from Albert Dietrich to Ernst Naumann.
"A German Requiem" was partially inspired by his mother's death in 1865 (at which time he composed a funeral march that was to become the basis of Part Two, "Denn alles Fleisch"), but it also incorporates material from a symphony which he started in 1854 but abandoned following Schumann's suicide attempt. He once wrote that the Requiem "belonged to Schumann". The first movement of this abandoned symphony was re-worked as the first movement of the First Piano Concerto.
Brahms loved the classical composers Mozart and Haydn. He collected first editions and autographs of their works and edited performing editions. He studied the music of pre-classical composers, including Giovanni Gabrieli, Johann Adolph Hasse, Heinrich Schütz, Domenico Scarlatti, George Frideric Handel, and, especially, Johann Sebastian Bach. His friends included leading musicologists, and, with Friedrich Chrysander, he edited an edition of the works of François Couperin. Brahms also edited works by C. P. E. and W. F. Bach. He looked to older music for inspiration in the art of counterpoint; the themes of some of his works are modelled on Baroque sources such as Bach's "The Art of Fugue" in the fugal finale of Cello Sonata No. 1 or the same composer's Cantata No. 150 in the passacaglia theme of the Fourth Symphony's finale.
The early Romantic composers had a major influence on Brahms, particularly Schumann, who encouraged Brahms as a young composer. During his stay in Vienna in 1862–63, Brahms became particularly interested in the music of Franz Schubert. The latter's influence may be identified in works by Brahms dating from the period, such as the two piano quartets Op. 25 and Op. 26, and the Piano Quintet which alludes to Schubert's String Quintet and Grand Duo for piano four hands. The influence of Chopin and Mendelssohn on Brahms is less obvious, although occasionally one can find in his works what seems to be an allusion to one of theirs (for example, Brahms's Scherzo, Op. 4, alludes to Chopin's Scherzo in B-flat minor; the scherzo movement in Brahms's Piano Sonata in F minor, Op. 5, alludes to the finale of Mendelssohn's Piano Trio in C minor).
Brahms considered giving up composition when it seemed that other composers' innovations in extended tonality would result in the rule of tonality being broken altogether. Although Wagner became fiercely critical of Brahms as the latter grew in stature and popularity, he was enthusiastically receptive of the early "Variations and Fugue on a Theme by Handel"; Brahms himself, according to many sources, deeply admired Wagner's music, confining his ambivalence only to the dramaturgical precepts of Wagner's theory.
Brahms wrote settings for piano and voice of 144 German folk songs, and many of his lieder reflect folk themes or depict scenes of rural life. His "Hungarian Dances" were among his most profitable compositions.
Influence.
Brahms' point of view looked both backward and forward; his output was often bold in its exploration of harmony and rhythm. As a result, he was an influence on composers of both conservative and modernist tendencies. Within his lifetime, his idiom left an imprint on several composers within his personal circle, who strongly admired his music, such as Heinrich von Herzogenberg, Robert Fuchs, and Julius Röntgen, as well as on Gustav Jenner, who was Brahms's only formal composition pupil. Antonín Dvořák, who received substantial assistance from Brahms, deeply admired his music and was influenced by it in several works, such as the Symphony No. 7 in D minor and the F minor Piano Trio. Features of the "Brahms style" were absorbed in a more complex synthesis with other contemporary (chiefly Wagnerian) trends by Hans Rott, Wilhelm Berger, Max Reger and Franz Schmidt, whereas the British composers Hubert Parry and Edward Elgar and the Swede Wilhelm Stenhammar all testified to learning much from Brahms's example. As Elgar said, "I look at the Third Symphony of Brahms, and I feel like a pygmy."
Ferruccio Busoni's early music shows much Brahmsian influence, and Brahms took an interest in him, though Busoni later tended to disparage Brahms. Towards the end of his life, Brahms offered substantial encouragement to Ernő Dohnányi and to Alexander von Zemlinsky. Their early chamber works (and those of Béla Bartók, who was friendly with Dohnányi) show a thoroughgoing absorption of the Brahmsian idiom. Zemlinsky, moreover, was in turn the teacher of Arnold Schoenberg, and Brahms was apparently impressed by two movements of Schoenberg's early Quartet in D major which Zemlinsky showed him. In 1933, Schoenberg wrote an essay "Brahms the Progressive" (re-written 1947), which drew attention to Brahms's fondness for motivic saturation and irregularities of rhythm and phrase; in his last book ("Structural Functions of Harmony", 1948), he analysed Brahms's "enriched harmony" and exploration of remote tonal regions. These efforts paved the way for a re-evaluation of Brahms's reputation in the 20th century. Schoenberg went so far as to orchestrate one of Brahms's piano quartets. Schoenberg's pupil Anton Webern, in his 1933 lectures, posthumously published under the title "The Path to the New Music", claimed Brahms as one who had anticipated the developments of the Second Viennese School, and Webern's own Op. 1, an orchestral passacaglia, is clearly in part a homage to, and development of, the variation techniques of the passacaglia-finale of Brahms's Fourth Symphony.
Brahms was honoured in the German hall of fame, the Walhalla memorial. On 14 September 2000, he was introduced there as the 126th ""rühmlich ausgezeichneter Teutscher"" and 13th composer among them, with a bust by sculptor .
Personality.
Brahms was fond of nature and often went walking in the woods around Vienna. He often brought penny candy with him to hand out to children. To adults Brahms was often brusque and sarcastic, and he often alienated other people. His pupil Gustav Jenner wrote, "Brahms has acquired, not without reason, the reputation for being a grump, even though few could also be as lovable as he." He also had predictable habits, which were noted by the Viennese press, such as his daily visit to his favourite "Red Hedgehog" tavern in Vienna, and his habit of walking with his hands firmly behind his back, which led to a caricature of him in this pose walking alongside a red hedgehog. Those who remained his friends were very loyal to him, however, and he reciprocated with equal loyalty and generosity.
Brahms had amassed a small fortune in the second half of his career, around 1860, when his works sold widely, but despite his wealth, he lived very simply, with a modest apartment, a mess of music papers and books, and a single housekeeper who cleaned and cooked for him. He was often the butt of jokes for his long beard, his cheap clothes and often not wearing socks. Brahms gave away large sums of money to friends and to aid various musical students, often with the term of strict secrecy. Brahms' domicile was hit during World War II, destroying his piano and other possessions that were still kept there for posterity by the Viennese.
Brahms was a lifelong friend of Johann Strauss II, though they were very different as composers. Brahms even struggled to get to the Theater an der Wien in Vienna for the premiere of Strauss's operetta "Die Göttin der Vernunft" in March 1897 before his death. Perhaps the greatest tribute that Brahms paid to Strauss was his remark that he would have given anything to have written "The Blue Danube" waltz. An old anecdote recounts that when Strauss's wife Adele asked Brahms to autograph her fan, he wrote the first few notes of the "Blue Danube" waltz, and then wrote the words "Unfortunately not by Johannes Brahms!" underneath.
Religious beliefs.
Brahms' personal views tended to be humanistic and skeptical, though one of his influences was undoubtedly the Bible as rendered in German by Martin Luther. His Requiem employs biblical texts primarily to speak words of comfort to the bereaved, yet it also cites Hebrews 13:14 ("here have we no continuing city, but we seek one to come") and 1 Corinthians 15:51–52 ("the trumpet shall sound, and the dead shall be raised incorruptible, and we shall be changed"). Composer Walter Niemann declared "The fact that Brahms began his creative activity with the German folk song and closed with the Bible reveals... the true religious creed of this great man of the people." Some present-day biographers and critics view Brahms's appreciation of Lutheran tradition more as cultural than existential. When asked by conductor Karl Reinthaler to add additional sectarian text to his German Requiem, Brahms is reported to have responded, "As far as the text is concerned, I confess that I would gladly omit even the word German and instead use Human; also with my best knowledge and will I would dispense with passages like . On the other hand, I have chosen one thing or another because I am a musician, because I needed it, and because with my venerable authors I can't delete or dispute anything. But I had better stop before I say too much."
On his religious views, Brahms has been described as an agnostic and a humanist. The devout Catholic Antonín Dvořák, the closest Brahms ever came to having a protégé, wrote in a letter: "Such a man, such a fine soul—and he believes in nothing! He believes in nothing!" Yet Brahms's final vocal and instrumental works, dating from 1896, are, respectively, "Vier ernste Gesänge" (Four Serious Songs), for voice and piano, Op. 121, settings of biblical texts; and Eleven Chorale Preludes, for organ, Op. 122, based upon nine Lutheran chorales.
The question of Brahms and religiosity has been controversial and elicited accusations of fraud. One example is the 1955 volume "Talks With Great Composers," by journalist and music critic Arthur Abell (Berlin correspondent for the "Musical Courier" from 1893 to 1918). Abell's book contains reminiscences of his conversations with Brahms, Joseph Joachim, and several other composers he knew in the late nineteenth and early twentieth centuries. This interview is viewed as fraudulent by Brahms biographer Jan Swafford.
External links.
Sheet music
Recordings
Photographs

</doc>
<doc id="16340" url="https://en.wikipedia.org/wiki?curid=16340" title="Jean-Paul Sartre">
Jean-Paul Sartre

Jean-Paul-Charles-Aymard Sartre (; ; 21 June 1905 – 15 April 1980) was a French philosopher, playwright, novelist, political activist, biographer, and literary critic. He was one of the key figures in the philosophy of existentialism and phenomenology, and one of the leading figures in 20th-century French philosophy and Marxism.
His work has also influenced sociology, critical theory, post-colonial theory, and literary studies, and continues to influence these disciplines.
Sartre has also been noted for his open relationship with the prominent feminist theorist Simone de Beauvoir. Together, Sartre and de Beauvoir challenged the cultural and social assumptions and expectations of their upbringings, which they considered bourgeois, in both lifestyle and thought. The conflict between oppressive, spiritually destructive conformity ("mauvaise foi", literally, "bad faith") and an "authentic" way of "being" became the dominant theme of Sartre's early work, a theme embodied in his principal philosophical work "Being and Nothingness" ("L'Être et le Néant", 1943). Sartre's introduction to his philosophy is his work "Existentialism and Humanism" ("L'existentialisme est un humanisme", 1946), originally presented as a lecture.
He was awarded the 1964 Nobel Prize in Literature but refused it, saying that he always declined official honours and that "a writer should not allow himself to be turned into an institution".
Biography.
Early life.
Jean-Paul Sartre was born in Paris as the only child of Jean-Baptiste Sartre, an officer of the French Navy, and Anne-Marie Schweitzer. His mother was of Alsatian origin and the first cousin of Nobel Prize laureate Albert Schweitzer. (Her father, Charles Schweitzer, was the older brother of Albert Schweitzer's father, Louis Théophile.)
When Sartre was two years old, his father died of a fever overseas. Anne-Marie moved back to her parents' house in Meudon, where she raised Sartre with help from her father, a teacher of German who taught Sartre mathematics and introduced him to classical literature at a very early age. When he was twelve, Sartre's mother remarried, and the family moved to La Rochelle, where he was frequently bullied.
As a teenager in the 1920s, Sartre became attracted to philosophy upon reading Henri Bergson's essay '. He attended the Cours Hattemer, a private school in Paris. He studied and earned certificates in psychology, history of philosophy, logic, general philosophy, ethics and sociology, and physics, as well as his ' (roughly equivalent to an MA thesis) in Paris at the École Normale Supérieure, an institution of higher education that was the alma mater for several prominent French thinkers and intellectuals. (His 1928 MA thesis under the title ""L'Image dans la vie psychologique: rôle et nature"", "Image in Psychological Life: Role and Nature" was directed by Henri Delacroix.) It was at ENS that Sartre began his lifelong, sometimes fractious, friendship with Raymond Aron. Perhaps the most decisive influence on Sartre's philosophical development was his weekly attendance at Alexandre Kojève's seminars, which continued for a number of years.
From his first years in the École Normale, Sartre was one of its fiercest pranksters. In 1927, his antimilitarist satirical cartoon in the revue of the school, coauthored with Georges Canguilhem, particularly upset the director Gustave Lanson. In the same year, with his comrades Nizan, Larroutis, Baillou and Herland, he organized a media prank following Charles Lindbergh's successful New York–Paris flight; Sartre & Co. called newspapers and informed them that Lindbergh was going to be awarded an honorary École degree. Many newspapers, including "Le Petit Parisien," announced the event on 25 May. Thousands, including journalists and curious spectators, showed up, unaware that what they were witnessing was a stunt involving a Lindbergh look-alike. The public's resultant outcry forced Lanson to resign.
In 1929 at the École Normale, he met Simone de Beauvoir, who studied at the Sorbonne and later went on to become a noted philosopher, writer, and feminist. The two became inseparable and lifelong companions, initiating a romantic relationship, though they were not monogamous. The first time Sartre took the exam to become a college instructor, he failed. He took it a second time and virtually tied for first place with Beauvoir, although Sartre was eventually awarded first place in his class, with Beauvoir second.
Sartre was drafted into the French Army from 1929 to 1931 and served as a meteorologist for some time. He later argued in 1959 that each French person was responsible for the collective crimes during the Algerian War of Independence.
From 1931 until 1945 Sartre taught at various lycées of Le Havre (at the Lycée de Le Havre, the present-day , 1931–36), Laon (at the Lycée de Laon, 1936–37), and, finally, Paris (at the Lycée Pasteur, 1937–39, and at the Lycée Condorcet, 1941–44; see below).
In 1932, Sartre discovered "Voyage au bout de la nuit" by Louis-Ferdinand Céline, a book that had a remarkable influence on him.
In 1933–34, he succeeded Raymond Aron at the in Berlin where he studied Edmund Husserl's phenomenological philosophy. Aron had already advised him in 1930 to read Emmanuel Levinas's "Théorie de l’intuition dans la phénoménologie de Husserl" ("The Theory of Intuition in Husserl's Phenomenology").
The Neo-Hegelian revival led by Alexandre Kojève and Jean Hyppolite in the 1930s inspired a whole generation of French thinkers, including Sartre, to discover Hegel's "Phenomenology of Spirit".
World War II.
In 1939 Sartre was drafted into the French army, where he served as a meteorologist. He was captured by German troops in 1940 in Padoux, and he spent nine months as a prisoner of war—in Nancy and finally in , Trier, where he wrote his first theatrical piece, "Barionà, fils du tonnerre", a drama concerning Christmas. It was during this period of confinement that Sartre read Martin Heidegger's "Being and Time", later to become a major influence on his own essay on phenomenological ontology. Because of poor health (he claimed that his poor eyesight and exotropia affected his balance) Sartre was released in April 1941. Given civilian status, he recovered his teaching position at Lycée Pasteur near Paris, settled at the Hotel Mistral. In October 1941 he was given a position at Lycée Condorcet in Paris, replacing a Jewish teacher who had been forbidden to teach by Vichy law.
After coming back to Paris in May 1941, he participated in the founding of the underground group "Socialisme et Liberté" ("Socialism and Liberty") with other writers Simone de Beauvoir, Maurice Merleau-Ponty, Jean-Toussaint Desanti, Dominique Desanti, Jean Kanapa, and École Normale students. In August Sartre and de Beauvoir went to the French Riviera seeking the support of André Gide and André Malraux. However, both Gide and Malraux were undecided, and this may have been the cause of Sartre's disappointment and discouragement. "Socialisme et liberté" soon dissolved and Sartre decided to write instead of being involved in active resistance. He then wrote "Being and Nothingness", "The Flies", and "No Exit", none of which was censored by the Germans, and also contributed to both legal and illegal literary magazines.
After August 1944 and the Liberation of Paris, he wrote "Anti-Semite and Jew". In the book he tries to explain the etiology of "hate" by analyzing antisemitic hate. Sartre was a very active contributor to "Combat", a newspaper created during the clandestine period by Albert Camus, a philosopher and author who held similar beliefs. Sartre and de Beauvoir remained friends with Camus until 1951, with the publication of Camus's "The Rebel". Later, while Sartre was labeled by some authors as a resistant, the French philosopher and resistant Vladimir Jankelevitch criticized Sartre's lack of political commitment during the German occupation, and interpreted his further struggles for liberty as an attempt to redeem himself. According to Camus, Sartre was a writer who resisted, not a resister who wrote.
In 1945, after the war ended, Sartre moved to an apartment on the rue Bonaparte which was where he was to produce most of his subsequent work, and where he lived until 1962. It was from there that he helped establish a quarterly literary and political review, "Les Temps modernes" ("Modern Times"), in part to popularize his thought. He ceased teaching and devoted his time to writing and political activism. He would draw on his war experiences for his great trilogy of novels, "Les Chemins de la Liberté" ("The Roads to Freedom") (1945–1949).
Cold War politics and anticolonialism.
The first period of Sartre's career, defined in large part by "Being and Nothingness" (1943), gave way to a second period—when the world was perceived as split into communist and capitalist blocs—of highly publicized political involvement. His 1948 play "Les mains sales" ("Dirty Hands") in particular explored the problem of being a politically "engaged" intellectual. He embraced Marxism, but did not join the Communist Party. While a Marxist, Sartre attacked what he saw as abuses of freedom and human rights by the Soviet Union. He was one of the first French journalists to expose the existence of the labor camps, and vehemently opposed the invasion of Hungary, Russian anti-Semitism, and the execution of dissidents. As an anti-colonialist, Sartre took a prominent role in the struggle against French rule in Algeria, and the use of torture and concentration camps by the French in Algeria. He became an eminent supporter of the FLN in the Algerian War and was one of the signatories of the "Manifeste des 121". Consequently, Sartre became a domestic target of the paramilitary Organisation de l'armée secrète (OAS), escaping two bomb attacks in the early '60s. (He had an Algerian mistress, Arlette Elkaïm, who became his adopted daughter in 1965.) He opposed U.S. involvement in the Vietnam War and, along with Bertrand Russell and others, organized a tribunal intended to expose U.S. war crimes, which became known as the Russell Tribunal in 1967.
His work after Stalin's death, the "Critique de la raison dialectique" ("Critique of Dialectical Reason"), appeared in 1960 (a second volume appearing posthumously). In the "Critique" Sartre set out to give Marxism a more vigorous intellectual defense than it had received until then; he ended by concluding that Marx's notion of "class" as an objective entity was fallacious. Sartre's emphasis on the humanist values in the early works of Marx led to a dispute with a leading leftist intellectual in France in the 1960s, Louis Althusser, who claimed that the ideas of the young Marx were decisively superseded by the "scientific" system of the later Marx.
Sartre went to Cuba in the 1960s to meet Fidel Castro and spoke with Ernesto "Che" Guevara. After Guevara's death, Sartre would declare him to be "not only an intellectual but also the most complete human being of our age" and the "era's most perfect man." Sartre would also compliment Guevara by professing that "he lived his words, spoke his own actions and his story and the story of the world ran parallel." However he stood against the persecution of gays by Castro's régime, which he compared to Nazi persecution of the Jews, and said: "In Cuba there are no Jews, but there are homosexuals".
During a collective hunger strike in 1974, Sartre visited Red Army Faction leader Andreas Baader in Stammheim Prison and criticized the harsh conditions of imprisonment. Towards the end of his life, Sartre became an anarchist.
Late life and death.
In 1964 Sartre renounced literature in a witty and sardonic account of the first ten years of his life, "Les Mots" ("The Words"). The book is an ironic counterblast to Marcel Proust, whose reputation had unexpectedly eclipsed that of André Gide (who had provided the model of "littérature engagée" for Sartre's generation). Literature, Sartre concluded, functioned ultimately as a bourgeois substitute for real commitment in the world. In October 1964, Sartre was awarded the Nobel Prize in Literature but he declined it. He was the first Nobel laureate to voluntarily decline the prize, and remains one of only two laureates to do so. In 1945, he had refused the Légion d'honneur. The Nobel prize was announced on 22 October 1964; on 14 October, Sartre had written a letter to the Nobel Institute, asking to be removed from the list of nominees, and warning that he would not accept the prize if awarded, but the letter went unread; on 23 October, "Le Figaro" published a statement by Sartre explaining his refusal. He said he did not wish to be "transformed" by such an award, and did not want to take sides in an East vs. West cultural struggle by accepting an award from a prominent Western cultural institution. After being awarded the prize he tried to escape the media by hiding in the house of Simone's sister Hélène de Beauvoir in Goxwiller, Alsace.
Though his name was then a household word (as was "existentialism" during the tumultuous 1960s), Sartre remained a simple man with few possessions, actively committed to causes until the end of his life, such as the May 1968 strikes in Paris during the summer of 1968 during which he was arrested for civil disobedience. President Charles de Gaulle intervened and pardoned him, commenting that "you don't arrest Voltaire."
In 1975, when asked how he would like to be remembered, Sartre replied:
I would like to remember "Nausea", [my plays "No Exit" and "The Devil and the Good Lord," and then my two philosophical works, more particularly the second one, "Critique of Dialectical Reason". Then my essay on Genet, "Saint Genet"... If these are remembered, that would be quite an achievement, and I don't ask for more. As a man, if a certain Jean-Paul Sartre is remembered, I would like people to remember the milieu or historical situation in which I lived... how I lived in it, in terms of all the aspirations which I tried to gather up within myself.
Sartre's physical condition deteriorated, partially because of the merciless pace of work (and the use of amphetamines) he put himself through during the writing of the "Critique" and a massive analytical biography of Gustave Flaubert ("The Family Idiot"), both of which remained unfinished. He suffered from hypertension, and became almost completely blind in 1973. Sartre was a notorious chain smoker, which could also have contributed to the deterioration of his health.
Sartre died on 15 April 1980 in Paris from edema of the lung. He had not wanted to be buried at Père-Lachaise Cemetery between his mother and stepfather, so it was arranged that he be buried at Montparnasse Cemetery. At his funeral on Saturday, 19 April, fifty thousand Parisians descended onto Boulevard Montparnasse to accompany Sartre's cortege. The funeral started at "the hospital at two p.m., then filed through the fourteenth arrondissement, past all Sartre's haunts, and entered the cemetery through the gate on the Boulevard Edgar Quinet." Sartre was initially buried in a temporary grave to the left of the cemetery gate. Four days later the body was disinterred for cremation at Père-Lachaise Cemetery, and his ashes were reburied at the permanent site in Montparnasse Cemetery, to the right of the cemetery gate.
Thought.
Sartre's primary idea is that people, as humans, are "condemned to be free". This theory relies upon his position that there is no creator, and is illustrated using the example of the paper cutter. Sartre says that if one considered a paper cutter, one would assume that the creator would have had a plan for it: an essence. Sartre said that human beings have no essence before their existence because there is no Creator. Thus: "existence precedes essence". This forms the basis for his assertion that since one cannot explain one's own actions and behaviour by referencing any specific human nature, they are necessarily fully responsible for those actions. "We are left alone, without excuse." "We can act without being determined by our past which is always separated from us."
Sartre maintained that the concepts of authenticity and individuality have to be earned but not learned. We need to experience "death consciousness" so as to wake up ourselves as to what is really important; the authentic in our lives which is life experience, not knowledge. Death draws the final point when we as beings cease to live for ourselves and permanently become objects that exist only for the outside world. In this way death emphasizes the burden of our free, individual existence.
As a junior lecturer at the Lycée du Havre in 1938, Sartre wrote the novel "La Nausée" ("Nausea"), which serves in some ways as a manifesto of existentialism and remains one of his most famous books. Taking a page from the German phenomenological movement, he believed that our ideas are the product of experiences of real-life situations, and that novels and plays can well describe such fundamental experiences, having equal value to discursive essays for the elaboration of philosophical theories such as existentialism. With such purpose, this novel concerns a dejected researcher (Roquentin) in a town similar to Le Havre who becomes starkly conscious of the fact that inanimate objects and situations remain absolutely indifferent to his existence. As such, they show themselves to be resistant to whatever significance human consciousness might perceive in them.
He also took inspiration from phenomenologist epistemology, explained by Franz Adler in this way: "Man chooses and makes himself by acting. Any action implies the judgment that he is right under the circumstances not only for the actor, but also for everybody else in similar circumstances."
This indifference of "things in themselves" (closely linked with the later notion of "being-in-itself" in his "Being and Nothingness") has the effect of highlighting all the more the freedom Roquentin has to perceive and act in the world; everywhere he looks, he finds situations imbued with meanings which bear the stamp of his existence. Hence the "nausea" referred to in the title of the book; all that he encounters in his everyday life is suffused with a pervasive, even horrible, taste—specifically, his freedom. The book takes the term from Friedrich Nietzsche's "Thus Spoke Zarathustra", where it is used in the context of the often nauseating quality of existence. No matter how much Roquentin longs for something else or something different, he cannot get away from this harrowing evidence of his engagement with the world.
The novel also acts as a terrifying realization of some of Immanuel Kant's fundamental ideas about freedom; Sartre uses the idea of the autonomy of the will (that morality is derived from our ability to choose in reality; the ability to choose being derived from human freedom; embodied in the famous saying "Condemned to be free") as a way to show the world's indifference to the individual. The freedom that Kant exposed is here a strong burden, for the freedom to act towards objects is ultimately useless, and the practical application of Kant's ideas proves to be bitterly rejected.
Career as public intellectual.
While the broad focus of Sartre's life revolved around the notion of human freedom, he began a sustained intellectual participation in more public matters towards the end of the Second World War, around 1944-45. Prior to this—before the Second World War—he was content with the role of an apolitical liberal intellectual: "Now teaching at a lycée in Laon [...] Sartre made his headquarters the Dome café at the crossing of Montparnasse and Raspail boulevards. He attended plays, read novels, and dined women. He wrote. And he was published." Sartre and his lifelong companion, de Beauvoir, existed, in her words, where "the world about us was a mere backdrop against which our private lives were played out".
Sartre portrayed his own pre-war situation in the character Mathieu, chief protagonist in "The Age of Reason", which was completed during Sartre's first year as a soldier in the Second World War. By forging Mathieu as an absolute rationalist, analyzing every situation, and functioning entirely on reason, he removed any strands of authentic content from his character and as a result, Mathieu could "recognize no allegiance except to self", though he realized that without "responsibility for my own existence, it would seem utterly absurd to go on existing". Mathieu's commitment was only to himself, never to the outside world. Mathieu was restrained from action each time because he had no reasons for acting. Sartre then, for these reasons, was not compelled to participate in the Spanish Civil War, and it took the invasion of his own country to motivate him into action and to provide a crystallization of these ideas. It was the war that gave him a purpose beyond himself, and the atrocities of the war can be seen as the turning point in his public stance.
The war opened Sartre's eyes to a political reality he had not yet understood until forced into continual engagement with it: "the world itself destroyed Sartre's illusions about isolated self-determining individuals and made clear his own personal stake in the events of the time." Returning to Paris in 1941 he formed the "Socialisme et Liberté" resistance group. In 1943, after the group disbanded, Sartre joined a writers' Resistance group, in which he remained an active participant until the end of the war. He continued to write ferociously, and it was due to this "crucial experience of war and captivity that Sartre began to try to build up a positive moral system and to express it through literature".
The symbolic initiation of this new phase in Sartre’s work is packaged in the introduction he wrote for a new journal, "Les Temps modernes", in October 1945. Here he aligned the journal, and thus himself, with the Left and called for writers to express their political commitment. Yet, this alignment was indefinite, directed more to the concept of the Left than a specific party of the Left.
Sartre's philosophy lent itself to his being a public intellectual. He envisaged culture as a very fluid concept; neither pre-determined, nor definitely finished; instead, in true existential fashion, "culture was always conceived as a process of continual invention and re-invention." This marks Sartre, the intellectual, as a pragmatist, willing to move and shift stance along with events. He did not dogmatically follow a cause other than the belief in human freedom, preferring to retain a pacifist's objectivity. It is this overarching theme of freedom that means his work "subverts the bases for distinctions among the disciplines". Therefore, he was able to hold knowledge across a vast array of subjects: "the international world order, the political and economic organisation of contemporary society, especially France, the institutional and legal frameworks that regulate the lives of ordinary citizens, the educational system, the media networks that control and disseminate information. Sartre systematically refused to keep quiet about what he saw as inequalities and injustices in the world."
Sartre always sympathized with the Left, and supported the French Communist Party (PCF) until the 1956 Soviet invasion of Hungary. Following the Liberation the PCF were infuriated by Sartre's philosophy, which appeared to lure young French men and women away from the ideology of communism and into Sartre’s own existentialism. From 1956 onwards Sartre rejected the claims of the PCF to represent the French working classes, objecting to its "authoritarian tendencies". In the late 1960s Sartre supported the Maoists, a movement that rejected the authority of established communist parties. However, despite aligning with the Maoists, Sartre said after the May events: "If one rereads all my books, one will realize that I have not changed profoundly, and that I have always remained an anarchist." He would later explicitly allow himself to be called an anarchist.
In the aftermath of a war that had for the first time properly engaged Sartre in political matters, he set forth a body of work which "reflected on virtually every important theme of his early thought and began to explore alternative solutions to the problems posed there". The greatest difficulties that he and all public intellectuals of the time faced were the increasing technological aspects of the world that were outdating the printed word as a form of expression. In Sartre's opinion, the "traditional bourgeois literary forms remain innately superior", but there is "a recognition that the new technological 'mass media' forms must be embraced" if Sartre's ethical and political goals as an authentic, committed intellectual are to be achieved: the demystification of bourgeois political practices and the raising of the consciousness, both political and cultural, of the working class.
The struggle for Sartre was against the monopolising moguls who were beginning to take over the media and destroy the role of the intellectual. His attempts to reach a public were mediated by these powers, and it was often these powers he had to campaign against. He was skilled enough, however, to circumvent some of these issues by his interactive approach to the various forms of media, advertising his radio interviews in a newspaper column for example, and vice versa.
The role of a public intellectual can lead to the individual placing himself in danger as he engages with disputed topics. In Sartre's case, this was witnessed in June 1961, when a plastic bomb exploded in the entrance of his apartment building. His public support of Algerian self-determination at the time had led Sartre to become a target of the campaign of terror that mounted as the colonists' position deteriorated. A similar occurrence took place the next year and he had begun to receive threatening letters from Oran, Algeria.
Literature.
Sartre wrote successfully in a number of literary modes and made major contributions to literary criticism and literary biography. His plays are richly symbolic and serve as a means of conveying his philosophy. The best-known, "Huis-clos" ("No Exit"), contains the famous line "L'enfer, c'est les autres", usually translated as "Hell is other people." Aside from the impact of "Nausea", Sartre's major work of fiction was "The Roads to Freedom" trilogy which charts the progression of how World War II affected Sartre's ideas. In this way, "Roads to Freedom" presents a less theoretical and more practical approach to existentialism.
Despite their similarities as polemicists, novelists, adapters, and playwrights, Sartre's literary work has been counterposed, often pejoratively, to that of Camus in the popular imagination. In 1948 the Roman Catholic Church placed Sartre's oeuvre on the "Index Librorum Prohibitorum" (List of Prohibited Books).
Criticism.
Some philosophers argue that Sartre's thought is contradictory. Specifically, they believe that Sartre makes metaphysical arguments despite his claim that his philosophical views ignore metaphysics. Herbert Marcuse criticized "Being and Nothingness" for projecting anxiety and meaninglessness onto the nature of existence itself: "Insofar as Existentialism is a philosophical doctrine, it remains an idealistic doctrine: it hypostatizes specific historical conditions of human existence into ontological and metaphysical characteristics. Existentialism thus becomes part of the very ideology which it attacks, and its radicalism is illusory." In "Letter on Humanism", Heidegger criticized Sartre's existentialism:
Existentialism says existence precedes essence. In this statement he is taking "existentia" and "essentia" according to their metaphysical meaning, which, from Plato's time on, has said that "essentia" precedes "existentia". Sartre reverses this statement. But the reversal of a metaphysical statement remains a metaphysical statement. With it, he stays with metaphysics, in oblivion of the truth of Being.
Philosophers Richard Wollheim and Thomas Baldwin have argued that Sartre's attempt to show that Sigmund Freud's theory of the unconscious is mistaken was based on a misinterpretation of Freud. Author Richard Webster considers Sartre one of many modern thinkers who have reconstructed Judaeo-Christian orthodoxies in secular form.
Author Brian C. Anderson denounced Sartre as an apologist for tyranny and terror and a supporter of Stalinism, Maoism, and Castro's regime in Cuba. Historian Paul Johnson asserted that Sartre's ideas had inspired the Khmer Rouge leadership: "The events in Cambodia in the 1970s, in which between one-fifth and one-third of the nation was starved to death or murdered, were entirely the work of a group of intellectuals, who were for the most part pupils and admirers of Jean-Paul Sartre – 'Sartre's Children' as I call them."
Sartre, who stated in his preface to Frantz Fanon's "The Wretched of the Earth" that, "To shoot down a European is to kill two birds with one stone, to destroy an oppressor and the man he oppresses at the same time: there remains a dead man and a free man," has been criticized by Anderson and Michael Walzer for supporting the killing of European civilians by the FLN during the Algerian War. Walzer suggests that Sartre, a European, was a hypocrite for not volunteering to be killed himself.
Author Clive James excoriated Sartre in his book of mini biographies "Cultural Amnesia" (2007). Among other things, James attacks Sartre's philosophy as being "all a pose".

</doc>
<doc id="16341" url="https://en.wikipedia.org/wiki?curid=16341" title="John Paul Jones">
John Paul Jones

John Paul Jones (born John Paul; July 6, 1747 July 18, 1792) was a Scottish American sailor and the United States' first well-known naval fighter in the American Revolutionary War. Although he made many friends and enemies among America's political elites, his actions in British waters during the Revolution earned him an international reputation which persists to this day. As such, he is sometimes referred to as the "Father of the American Navy" (an epithet he shares with John Barry). He later served in the Imperial Russian Navy, subsequently obtaining the rank of rear admiral.
Early life and education.
Jones was born John Paul (he added "Jones" in later life to hide from law enforcement) on the estate of Arbigland near Kirkbean in the Stewartry of Kirkcudbright on the southwest coast of Scotland. His father, John Paul, Sr., was a gardener at Arbigland, and his mother was named Jean McDuff (1708–1767). His parents married on November 29, 1733 in New Abbey, Kirkcudbright. Living at Arbigland at the time was Helen Craik (1751–1825), later a novelist. John Paul started his maritime career at the age of 13, sailing out of Whitehaven in the northern English county of Cumberland, as apprentice aboard "Friendship" under Captain Benson. Paul's older brother William Paul had married and settled in Fredericksburg, Virginia, the destination of many of the younger Jones' voyages.
For several years John sailed aboard a number of British merchant and slave ships, including "King George" in 1764 as third mate, and "Two Friends" as first mate in 1766. In 1768 he abandoned his prestigious position on the profitable "Two Friends" while docked in Jamaica. He found his own passage back to Scotland, and eventually obtained another position.
During his next voyage aboard the brig "John", which sailed from port in 1768, young John Paul's career was quickly and unexpectedly advanced when both the captain and a ranking mate suddenly died of yellow fever. John managed to navigate the ship back to a safe port and, in reward for this impressive feat, the vessel's grateful Scottish owners made him master of the ship and its crew, giving him 10 percent of the cargo. He then led two voyages to the West Indies before running into difficulty.
During his second voyage in 1770, John Paul viciously flogged one of his sailors, a carpenter, leading to accusations that his discipline was "unnecessarily cruel." While these claims were initially dismissed, his favorable reputation was destroyed when the sailor died a few weeks later. John Paul was arrested for his involvement in the man's death, and was imprisoned in Kirkcudbright Tolbooth but later released on bail. The negative effect of this episode on his reputation is indisputable, although the man's death has been linked to other causes. This man was not a usual sailor but an adventurer from a very influential Scottish family.
Leaving Scotland, John Paul commanded a London-registered vessel, a West Indiaman mounting 22 guns, named "Betsy," for about 18 months, engaging in commercial speculation in Tobago. This came to an end, however, when John killed a member of his crew, a mutineer named Blackton, with a sword, in a dispute over wages. Years later, in a letter to Benjamin Franklin describing this incident, he claimed it was in self-defense, but because he was not willing to be tried in an Admiral's Court, where the family of his first victim had been influential, he felt compelled to flee to Fredericksburg, Province of Virginia, leaving his fortune behind.
He went to Fredericksburg to arrange the affairs of his brother, who had died there without leaving any relatives; and about this time, in addition to his original surname, he assumed the surname of Jones. There is a long tradition held in the state of North Carolina that John Paul adopted the name "Jones" in honor of Willie Jones of Halifax, North Carolina.
His sentiments became even more in favor of America. From that period, as he afterwards expressed himself to Baron Joan van der Capellen tot den Pol, America became "the country of his fond election." It was not long afterward that John Paul "Jones" joined the American navy to fight against Britain.
Career.
The American Colonies.
Sources struggle with this period of Jones' life, especially the specifics of his family situation, making it difficult to historically pinpoint Jones' exact motivations for emigrating to America. Whether his plans for the plantation were not developing as expected, or if he was inspired by a revolutionary spirit, is unknown.
What is clearly known is that Jones left for Philadelphia shortly after settling in North America to volunteer his services to the newly founded Continental Navy, precursor of the United States Navy. During this time, around 1775, the Navy and Marines were being formally established, and suitable ship's officers and captains were in great demand. Were it not for the endorsement of Richard Henry Lee who knew of his abilities, Jones's potential would likely have gone unrecognized. With help from influential members of the Continental Congress, however, Jones was to be appointed as a 1st Lieutenant of the newly converted 24-gun frigate in the Continental Navy on December 7, 1775. 
Revolutionary War command.
Early command.
Jones sailed from the Delaware River in February 1776 aboard "Alfred" on the Continental Navy's maiden cruise. It was aboard this vessel that Jones took the honor of hoisting the first U.S. ensign over a naval vessel. Jones actually raised the Grand Union Flag, not the later and more familiar Flag of the United States. The fleet, which had been expected to cruise along the coast, was ordered instead by Commodore Esek Hopkins to sail for The Bahamas, where Nassau was raided for its military supplies. On the fleet's return voyage it had an unsuccessful encounter with a British packet ship. Jones was then assigned command of the sloop . Congress had recently ordered the construction of thirteen frigates for the American Navy, one of which was to be commanded by Jones. In exchange for this prestigious command, Jones accepted his commission aboard the smaller "Providence". During this six week voyage, Jones captured sixteen prizes and inflicted significant damage along the coast of Nova Scotia. Jones' next command came as a result of Commodore Hopkins's orders to liberate hundreds of American prisoners forced to labor in coal mines in Nova Scotia and also to raid British shipping. On November 1, 1776, Jones set sail in command of "Alfred" to carry out this mission. Although winter conditions prevented the freeing of the prisoners, the mission did result in the capture of "Mellish", a vessel carrying a vital supply of winter clothing intended for General John Burgoyne's troops in Canada.
Command of "Ranger".
Despite his successes at sea, upon arrival in Boston on December 16, 1776, Jones's disagreements with those in authority reached a new level. While at the port, he began feuding with Commodore Hopkins, who Jones believed was hindering his advancement and talking down his campaign plans. As a result of this and other frustrations, Jones was assigned the smaller command, the newly constructed , on June 14, 1777 (the same day the new Stars and Stripes flag was adopted).
After making the necessary preparations, Jones sailed for France on November 1, 1777 with orders to assist the American cause however possible. The American commissioners in France, Benjamin Franklin, Silas Deane and Arthur Lee, listened to Jones's strategic recommendations. They assured him the command of , a new vessel being constructed for America in Amsterdam. Britain, however, was able to divert "L'Indien" away from American hands by exerting pressure to ensure its sale to France instead (who had not yet allied with America). Jones was again left without a command, an unpleasant reminder of his stagnation in Boston from late 1776 until early 1777. It is thought that it was during this time Jones developed his close friendship with Benjamin Franklin, whom he greatly admired. In 1778, he was accepted, together with Benjamin Franklin, into the Masonic Lodge "Les Neuf Sœurs".
On February 6, 1778, France signed the Treaty of Alliance with America, formally recognizing the independence of the new American republic. Eight days later, Captain Jones's "Ranger" became the first American naval vessel to be formally saluted by the French, with a nine-gun salute fired from captain Lamotte-Piquet's flagship. Jones wrote of the event: "I accepted his offer all the more for after all it was a recognition of our independence and in the nation."
Finally, on April 10, 1778, Jones set sail from Brest, France for the western coasts of Britain.
"Ranger" attacks the British.
After some early successes against British merchant shipping in the Irish Sea, on April 17, 1778, Jones persuaded his crew to participate in an assault on Whitehaven, the town where his maritime career had begun. Jones later wrote about the poor command qualities of his senior officers (having tactfully avoided such matters in his official report): "'Their object,' they said, 'was gain not honor.' They were poor: instead of encouraging the morale of the crew, they excited them to disobedience; they persuaded them that they had the right to judge whether a measure that was proposed to them was good or bad."
As it happened, contrary winds forced the abandonment of the attempt, and drove "Ranger" towards Ireland, causing more trouble for British shipping on the way.
On April 20, 1778, Jones learned from captured sailors that the Royal Navy sloop o' war was anchored off Carrickfergus, Ireland. According to the diary of "Ranger's" surgeon
Jones's first intention was to attack the vessel in broad daylight, but his sailors were "unwilling to undertake it" (another incident omitted from the official report). Therefore, the attack took place just after midnight, but the mate responsible for dropping the anchor to halt "Ranger" right alongside "Drake" misjudged the timing in the dark (Jones claimed in his memoirs, the man was drunk), so Jones had to cut his anchor cable and run.
The wind having shifted, "Ranger" recrossed the Irish Sea to make another attempt at raiding Whitehaven. Jones led the assault with two boats of fifteen men on April 23, 1778, just after midnight, hoping to set fire to and sink all Whitehaven's ships anchored in harbour (numbering between 200 and 400 wooden vessels), which consisted of a full merchant fleet and many coal transporters. They also hoped to terrorize the townspeople by lighting further fires. As it happened, the journey to shore was slowed by the still-shifting wind, as well as a strong ebb tide. The spiking of the town's big defensive guns to prevent them being fired was accomplished successfully, but lighting fires proved difficult, as the lanterns in both boats had run out of fuel. To remedy this, some of the party were therefore sent to raid a public house on the quayside, but the temptation to stop for a quick drink led to a further delay. By the time they returned, and the arson attacks began, dawn was fast approaching, so efforts were concentrated on a single ship, the coal ship "Thompson", in the hope that the flames would spread to adjacent vessels, all grounded by the low tide. However, in the twilight, one of the crew slipped away and alerted residents on a harbourside street. A fire alert was sounded, and large numbers of people came running to the quay, forcing the Americans to retreat, and extinguishing the flames with the town's two fire-engines. However, hopes of sinking Jones's boats with cannon fire were dashed by the prudent spiking.
Crossing the Solway Firth from Whitehaven to Scotland, Jones hoped to hold for ransom the Earl of Selkirk, who lived on St Mary's Isle near Kirkcudbright. The Earl, Jones reasoned, could be exchanged for American sailors impressed into the Royal Navy. When the Earl was discovered to be absent from his estate, Jones claims he intended to return directly to his ship and continue seeking prizes elsewhere, but his crew wished to "pillage, burn, and plunder all they could". Ultimately, Jones allowed the crew to seize a silver plate set adorned with the family's emblem to placate their desires, but nothing else. Jones bought the plate himself when it was later sold off in France, and returned it to the Earl of Selkirk after the War.
Although their effect on British morale and allocation of defense resources was significant,
the attacks on St. Mary's Isle and Whitehaven resulted in no prizes or profits which under normal circumstances would be shared with the crew. Throughout the mission, the crew, led by Jones's second-in-command Lieutenant Thomas Simpson, acted as if they were aboard a privateer, not a warship.
Return to Ireland.
Nevertheless, Jones now led "Ranger" back across the Irish Sea, hoping to make another attempt at the "Drake", still anchored off Carrickfergus. This time, late in the afternoon of April 24, 1778, the ships, roughly equal in firepower, engaged in combat. Earlier in the day, the Americans had captured the crew of a reconnaissance boat, and learned that "Drake" had taken on dozens of soldiers, with the intention of grappling and boarding "Ranger", so Jones made sure that did not happen, capturing "Drake" after an hour-long gun battle which cost the British captain his life. Lieutenant Simpson was given command of "Drake" for the return journey to Brest. The ships separated during the return journey as "Ranger" chased another prize, leading to a conflict between Simpson and Jones. Both ships arrived at port safely, but Jones filed for a court-martial of Simpson, keeping him detained on the ship.
Partly through the influence of John Adams, who was still serving as a commissioner in France, Simpson was released from Jones's accusation. Adams implies in his memoirs that the overwhelming majority of the evidence supported Simpson's claims. Adams seemed to believe Jones was hoping to monopolize the mission's glory, especially by detaining Simpson on board while he celebrated the capture with numerous important European dignitaries.
Even with the wealth of perspectives, including the commander's, it is difficult if not impossible to tell exactly what occurred. It is clear, however, that the crew felt alienated by their commander, who might well have been motivated by his pride. Jones believed his intentions were honorable, and his actions were strategically essential to the Revolution. Regardless of any controversy surrounding the mission, "Ranger's" capture of "Drake" was one of the Continental Navy's few significant military victories during the Revolution, and was of immense symbolic importance, demonstrating as it did that the Royal Navy was far from invincible. By overcoming such odds, "Ranger's" victory became an important symbol of the American spirit and served as an inspiration for the permanent establishment of the United States Navy after the revolution.
"Bonhomme Richard".
In 1779, Captain Jones took command of the 42-gun (or as he preferred it, "Bon Homme Richard"),
a merchant ship rebuilt and given to America by the French shipping magnate, Jacques-Donatien Le Ray. On August 14, as a vast French and Spanish invasion fleet approached England, he provided a diversion by heading for Ireland at the head of a five ship squadron including the 36-gun , 32-gun USS "Pallas", 12-gun , and "Le Cerf", also accompanied by two privateers, and "Granville". When the squadron was only a few days out of Groix, "Monsieur" separated due to a disagreement between her captain and Jones.
Several Royal Navy warships were sent towards Ireland in pursuit of Jones, but on this occasion, he continued right around the north of Scotland into the North Sea, creating near-panic all along Britain's east coast as far south as the Humber estuary. Jones's main problems, as on his previous voyage, resulted from insubordination, particularly by Pierre Landais, captain of "Alliance". On September 23, 1779, the squadron met a large merchant convoy off the coast of Flamborough Head, east Yorkshire. The 50-gun British frigate and the 22-gun hired ship "Countess of Scarborough" placed themselves between the convoy and Jones's squadron, allowing the merchants to escape.
Shortly after 7 p.m. the Battle of Flamborough Head began. "Serapis" engaged "Bonhomme Richard", and soon afterwards, "Alliance" fired, from a considerable distance, at "Countess". Quickly recognizing that he could not win a battle of big guns, and with the wind dying, Jones made every effort to lock "Richard" and "Serapis" together (his famous, albeit possibly apocryphal, quotation "I have not yet begun to fight!" was uttered in reply to a demand to surrender in this phase of the battle), finally succeeding after about an hour, following which his deck guns and his Marine marksmen in the rigging began clearing the British decks. "Alliance" sailed past and fired a broadside, doing at least as much damage to "Richard" as to "Serapis". Meanwhile, "Countess of Scarborough" had enticed "Pallas" downwind of the main battle, beginning a separate engagement. When "Alliance" approached this contest, about an hour after it had begun, the badly damaged "Countess" surrendered.
With "Bonhomme Richard" burning and sinking, it seems that her ensign was shot away; when one of the officers, apparently believing his captain to be dead, shouted a surrender,
the British commander asked, seriously this time, if they had struck their colours. Jones later remembered saying something like "I am determined to make you strike," but the words allegedly heard by crew-members and reported in newspapers a few days later were more like: "I may sink, but I'll be damned if I strike." An attempt by the British to board "Bonhomme Richard" was thwarted, and a grenade caused the explosion of a large quantity of gunpowder on "Serapis" lower gun-deck.
"Alliance" then returned to the main battle, firing two broadsides. Again, these did at least as much damage to "Richard" as to "Serapis", but the tactic worked to the extent that, unable to move, and with "Alliance" keeping well out of the line of his own great guns, Captain Pearson of "Serapis" accepted that prolonging the battle could achieve nothing, so he surrendered. Most of "Bonhomme Richard's" crew immediately transferred to other vessels, and after a day and a half of frantic repair efforts, it was decided that the ship could not be saved, so it was allowed to sink, and Jones took command of "Serapis" for the trip to neutral (but American-sympathizing) Holland.
In the following year, the King of France Louis XVI, honored him with the title "Chevalier". Jones accepted the honor, and desired the title to be used thereafter: when the Continental Congress in 1787 resolved that a medal of gold be struck in commemoration of his "valor and brilliant services" it was to be presented to "Chevalier John Paul Jones". He also received from Louis XVI a decoration of "l'Institution du Mérite Militaire" and a sword. By contrast, in Britain at this time, he was usually denigrated as a pirate.
Russian service.
In June 1782, Jones was appointed to command the 74-gun , but his command fell through when Congress decided to give "America" to the French as replacement for the wrecked "Le Magnifique". As a result, he was given assignment in Europe in 1783 to collect prize money due his former hands. At length, this too expired and Jones was left without prospects for active employment, leading him on April 23, 1787 to enter into the service of the Empress Catherine II of Russia, who placed great confidence in Jones, saying: "He will get to Constantinople." He was granted name as a French subject Павел де Жовес ("Pavel de Zhoves", Paul de Jones).
Jones avowed his intention, however, to preserve the condition of an American citizen and officer. As a rear admiral aboard the 24-gun flagship "Vladimir", he took part in the naval campaign in the Dnieper-Bug Liman (an arm of the Black Sea, into which flow the Southern Bug and Dnieper rivers) against the Turks, in concert with the Dnieper Flotilla commanded by Prince Charles of Nassau-Siegen. Jones (and Nassau-Siegen) repulsed the Ottoman forces from the area, but the jealous intrigues of Nassau-Siegen (and perhaps Jones's own inaptitude for Imperial politics) turned the Russian commander Prince Grigory Potëmkin against Jones and he was recalled to St. Petersburg for the pretended purpose of being transferred to a command in the North Sea. Another factor may have been the resentment of several ex-British naval officers also in Russian employment, who regarded Jones as a renegade and refused to speak to him. Whatever motivated the Prince, once recalled he was compelled to remain in idleness, while rival officers plotted against him and even maliciously assailed his private character through accusations of sexual misconduct. In April 1789 Jones was arrested and accused of raping a 12-year-old girl named Katerina Goltzwart. But the Count de Segur, the French representative at the Russian court (and also Jones' last friend in the capital), conducted his own personal investigation into the matter and was able to convince Potëmkin that the girl had not been raped and that Jones had been accused by Prince de Nassau-Siegen for his own purposes; Jones, however, admitted to prosecutors that he had "often frolicked" with the girl "for a small cash payment," only denying that he had deprived her of her virginity. Even so, in that period he was able to author his "Narrative of the Campaign of the Liman".
On June 8, 1788, Jones was awarded the Order of St. Anne, but he left the following month, an embittered man.
In 1789 Jones arrived in Warsaw, Poland, where he befriended another veteran of the American Revolutionary War, Tadeusz Kościuszko. Kościuszko advised him to leave the service of the autocratic Russia, and serve another power, suggesting Sweden. Despite Kościuszko's backing, the Swedes, while somewhat interested, in the end decided not to recruit Jones.
Later life and death.
In May 1790 Jones arrived in Paris, where he remained in retirement until his death two years later, although he made a number of attempts to re-enter the service in the Russian navy. In June 1792, Jones was appointed U.S. Consul to treat with the Dey of Algiers for the release of American captives. Before Jones was able to fulfill his appointment, however, he was found dead (aged 45) lying face-down on his bed in his third-floor Paris apartment, No. 19 Rue de Tournon, on July 18, 1792. The cause of death was interstitial nephritis. A small procession of servants, friends and loyal family walked his body the four miles (6 km) for burial. He was buried in Paris at the Saint Louis Cemetery, which belonged to the French royal family. Four years later, France's revolutionary government sold the property and the cemetery was forgotten. The area was later used as a garden, a place to dispose of dead animals and where gamblers bet on animal fights.
Legacy.
In 1905, Jones's remains were identified by U.S. Ambassador to France Gen. Horace Porter, who had searched for six years to track down the body using faulty copies of Jones's burial record. Thanks to the kind donation of a French admirer, Pierrot Francois Simmoneau, who had donated over 460 francs, Jones's body was preserved in alcohol and interred in a lead coffin "in the event that should the United States decide to claim his remains, they might more easily be identified." Porter knew what to look for in his search. With the aid of an old map of Paris, Porter's team, which included anthropologist Louis Capitan, identified the site of the former St. Louis Cemetery for Alien Protestants. Sounding probes were used to search for lead coffins and five coffins were ultimately exhumed. The third, unearthed on April 7, 1905, was later identified by a meticulous post-mortem examination by Doctors Capitan and Georges Papillault as being that of Jones. The autopsy confirmed the original listing of cause of death. The face was later compared to a bust by Jean-Antoine Houdon.
Jones's body was ceremonially removed from interment in a Parisian charnel house and brought to the United States aboard the , escorted by three other cruisers. On approaching the American coastline, seven U.S. Navy battleships joined the procession escorting Jones's body back to America. On April 24, 1906, Jones's coffin was installed in Bancroft Hall at the United States Naval Academy, Annapolis, Maryland, following a ceremony in Dahlgren Hall, presided over by President Theodore Roosevelt who gave a lengthy tributary speech. On January 26, 1913, the Captain's remains were finally re-interred in a magnificent bronze and marble sarcophagus at the Naval Academy Chapel in Annapolis.

</doc>
<doc id="16343" url="https://en.wikipedia.org/wiki?curid=16343" title="Boulting brothers">
Boulting brothers

John Edward Boulting (21 December 1913 – 17 June 1985) and Roy Alfred Clarence Boulting (21 December 1913 – 5 November 2001), known collectively as the Boulting brothers, were English filmmakers and identical twins who became known for their popular series of satirical comedies in the 1950s and 1960s. They produced many of their films through their own production company, Charter Film Productions, which they set up in 1937.
Early life.
The twin brothers were born to Arthur Boulting and his wife Rosetta (Rose) "née" Bennett in Bray, Berkshire, England on 21 December 1913 (though both brothers later gave their birthday as 21 "November" in "Who's Who" and elsewhere). John was the elder by half an hour. John was named Joseph Edward John Boulting and Roy was named Alfred Fitzroy Clarence Boulting. Their elder brother Sydney Boulting became an actor and stage producer as Peter Cotes; he was the original director of "The Mousetrap". A younger brother, Guy, died aged eight. Both twins were educated at Reading School, where they formed a film society. They were extras in Anthony Asquith's 1931 film "Tell England" while still at school.
Careers.
The brothers worked together as producer and director whenever they could, and often alternated these duties depending on the nature of each film, although they also made films separately. They also had a hand in the scripts of many of their films.
They began with serious, tight, economical drama films such as "Seven Days to Noon" (1950) and Graham Greene's "Brighton Rock" (1947), both with Roy producing and John directing. They then became known for a series of satirical comedy films, such as "Private's Progress" (1956), "Lucky Jim" (1957) and "I'm All Right Jack" (1959). John Boulting co-wrote the films. The Boultings used the same actors in many of their films, including Ian Carmichael, Richard Attenborough, Terry-Thomas, Dennis Price, John Le Mesurier, Irene Handl, and Miles Malleson. "I'm All Right Jack" featured Peter Sellers, boosting his film career, winning him a BAFTA Best Actor Award. In 1985, Roy Boulting directed an episode of the "Miss Marple" mysteries for BBC Television.
Personal lives.
John Boulting was married four times and had three sons and three daughters.
John and his South African-born wife Anne had two daughters: one of whom is Lucy Boulting Hill, a successful casting director.
Roy Boulting was married five times and had seven sons.
In 1951 Roy married Enid Munnik, later known as Enid Boulting, an established fashion model and fashion editor at the French magazine Elle. Ingrid Boulting, is Enid's daughter from a previous marriage. Together they had three children: Fitzroy, the eldest, then identical twins named Edmund and Rupert.
In 1971, Roy married Hayley Mills 33 years his junior, whom he had met on the set of "The Family Way", and they had a son, Crispian Mills. The couple divorced in 1978.
Deaths.
John Boulting died on 17 June 1985 at his home in Sunningdale, Berkshire, and Roy Boulting 16 years later on 5 November 2001 in the Radcliffe Infirmary, Oxford; both died of cancer.

</doc>
<doc id="16345" url="https://en.wikipedia.org/wiki?curid=16345" title="John Frankenheimer">
John Frankenheimer

John Michael Frankenheimer (February 19, 1930 – July 6, 2002) was an American film and television director known for social dramas and action/suspense films. Among his credits were "Birdman of Alcatraz" (1962), "The Manchurian Candidate" (1962), "Seven Days in May" (1964), "The Train" (1964), "Seconds" (1966), "Grand Prix" (1966), "French Connection II" (1975), "Black Sunday" (1977), and "Ronin" (1998).
Frankenheimer won four consecutive Emmy Awards in the 1990s for the television movies "Against the Wall", "The Burning Season", "Andersonville", and "George Wallace", which also received a Golden Globe award. He was considered one of the last remaining directors who insisted on having complete control over all elements of production, making his style unique in Hollywood.
Frankenheimer's 30 feature films and over 50 plays for television were notable for their influence on contemporary thought. He became a pioneer of the "modern-day political thriller," having begun his career at the peak of the Cold War. Technically highly accomplished from his days in live television, many of his films were noted for creating "psychological dilemmas" for his male protagonists along with having a strong "sense of environment," similar in style to films by director Sidney Lumet, for whom he had earlier worked as assistant director. He developed a "tremendous propensity for exploring political situations" which would ensnare his characters.
Movie critic Leonard Maltin writes that "in his time ... Frankenheimer worked with the top writers, producers and actors in a series of films that dealt with issues that were just on top of the moment—things that were facing us all."
Early life.
Frankenheimer was born in Queens, New York, the son of Helen Mary (née Sheedy) and Walter Martin Frankenheimer, a stockbroker. Frankenheimer once speculated that he might be related to actress Ally Sheedy. His father was of German Jewish descent and his mother was Irish Catholic, and Frankenheimer was raised Catholic.
Frankenheimer grew up in New York City and became interested in movies at an early age; he recalled going to the cinema every weekend. In 1947, he graduated from La Salle Military Academy in Oakdale, Long Island, New York. In 1951, he graduated from Williams College in Williamstown, Massachusetts, where he studied English. He also developed an interest in acting as a career while in college but began thinking seriously about directing when he was in the Air Force. This led him to join a film squadron based in Burbank, California, where he shot his first documentary. He also began studying film theory by reading books about other famous directors, such as Sergei Eisenstein along with how-to books about the craft of film making.
In May 2001, amid rumors that he was the biological father of film director Michael Bay, Frankenheimer stated he had a brief relationship with Bay's birth mother. After the rumors surfaced that Bay's natural father was a filmmaker, there was much speculation about Frankenheimer, who continued to deny the story and told the "Los Angeles Times" that there had once been "tests" to determine paternity (long before DNA testing).
Career.
Frankenheimer began his directing career in live television at CBS. Throughout the 1950s he directed over 140 episodes of shows like "Playhouse 90", "Climax!", and "Danger", including "The Comedian", written by Rod Serling and starring Mickey Rooney as a ragingly vicious television comedian.
Frankenheimer's first theatrical film was "The Young Stranger" (1957), starring James MacArthur as the rebellious teenage son of a powerful Hollywood movie producer. He directed the production, based on a "Climax!" episode, "Deal a Blow", which he directed when he was 26. Frankenheimer returned to television during the late 1950s, moving to film permanently in 1961 with "The Young Savages", in which he worked for the first time with Burt Lancaster in a story of a young boy murdered by a New York gang. His departure from television is considered to signal the end of the Golden Age of Television.
Roger Ebert considered Frankenheimer to have had a special gift as a filmmaker and to have been a "master craftsman". He stated that Frankenheimer made some of the "most distinctive films of his time" and that he was " one of the most gifted directors of drama on television".
"Birdman of Alcatraz".
Production of "Birdman of Alcatraz" began under director Charles Crichton. Burt Lancaster, who was producing as well as starring, asked Frankenheimer to take over the film. As Frankenheimer describes in Charles Champlin's interview book, he advised Lancaster that the script was too long but was told he had to shoot all that was written.
The first cut of the film was four-and-a-half hours long, the length Frankenheimer had predicted. Moreover, the film was constructed so that it could not be cut and still be coherent. Frankenheimer said the film would have to be rewritten and partly reshot. Lancaster was committed to star in "Judgment at Nuremberg", so he made that film while Frankenheimer prepared the reshoots. The finished film, released in 1962, was a huge success and was nominated for four Oscars, including one for Lancaster's performance.
Frankenheimer was next hired by producer John Houseman to direct "All Fall Down", a family drama starring Eva Marie Saint and Warren Beatty. Because of the production difficulties with "Birdman of Alcatraz", "All Fall Down" was actually released first.
"The Manchurian Candidate".
Frankenheimer followed this with his most iconic film, "The Manchurian Candidate" (1962). Frankenheimer and producer George Axelrod bought Richard Condon's 1959 novel after it had already been turned down by many Hollywood studios. After Frank Sinatra committed to the film, they secured backing from United Artists.
The story of a Korean War veteran, brainwashed by the Communist Chinese to assassinate a candidate for President, co-starred Laurence Harvey and Janet Leigh and Angela Lansbury as Harvey's evil mother. Frankenheimer had to fight to cast Lansbury, who had worked with him on "All Fall Down", and was just two years older than Harvey. Sinatra's preference had been for Lucille Ball. The film was nominated for two Oscars, including one for Lansbury.
The film was unseen, either theatrically or on broadcast, for many years. Urban legend has it that the film was pulled from circulation due to the similarity of its plot to the death of President Kennedy the following year, but Frankenheimer states in the Champlin book that it was pulled because of a legal battle between producer Sinatra and the studio over Sinatra's share of the profits. In any event, it was re-released to great acclaim in 1988.
"Seven Days in May".
Frankenheimer followed with another successful political thriller, "Seven Days in May" (1964). He again bought the rights to a bestselling book, this time by Charles Bailey II and Fletcher Knebel, and again produced the film with his star, this time Kirk Douglas.
Douglas intended to play the role of the General who attempts to lead a coup against the President, who is about to sign a disarmament treaty with the Soviets. Douglas then decided he wanted to work with Burt Lancaster, with whom he had just costarred in another film. To entice Lancaster, Douglas agreed to let him play the General, while Douglas took the less showy lead role of the General's aide, who turns against him and helps the President.
The film, written by Rod Serling, costarred Fredric March as the President and Ava Gardner as a former flame of Lancaster's, was nominated for two Oscars.
"The Train".
This is another Frankenheimer film taken over from another director, this time Arthur Penn. "The Train (1964)" had already begun shooting in France when star Lancaster had the original director fired and called in Frankenheimer to save the film. As he recounts in the Champlin book, Frankenheimer used the production's desperation to his advantage in negotiations. He demanded and got the following: his name was made part of the title, "John Frankenheimer's The Train"; the French co-director, demanded by French tax laws, was not allowed to ever set foot on set; he was given total final cut; and a Ferrari.
Again saddled with an unfilmably long script, Frankenheimer threw it out and took the locations and actors left from the previous film and began filming, with writers working in Paris as the production shot in Normandy. The poorly chosen locations caused endless weather delays. The film contains multiple real train wrecks. The Allied bombing of a rail yard was accomplished with real dynamite, as the French rail authority needed to enlarge the track gauge. This can be observed by the shockwaves traveling through the ground during the action sequence. Producers realized after filming that the story needed another action scene, and reassembled some of the cast for a Spitfire attack scene that was inserted into the first third of the film. The finished movie was successful, and the script was nominated for an Oscar.
"Seconds".
"Seconds" (1966) tells of an older man John Randolph given the body of a young man Rock Hudson through experimental surgery. It was poorly received on its release but has come to be one of the director's most respected and popular films subsequently. The film is an expressionistic, part-horror, part-thriller, part-science fiction film about the obsession with eternal youth and misplaced faith in the ability of medical science to achieve it.
The director of photography for "Seconds" was the legendary James Wong Howe, who is well known for pioneering novel techniques in black-and-white cinematography, and whose prolific career spanned nearly five decades. He was nominated for an Academy Award for his work on the film. "Seconds" was Frankenheimer and Howe's last film in black-and-white. All of Frankenheimer's films up until "Grand Prix" had been made in black-and-white.
"Grand Prix".
Frankenheimer followed "Seconds" with his most spectacular production, 1966's "Grand Prix". Shot on location at the Grand Prix races throughout Europe, using 65mm Cinerama cameras, the film starred James Garner and Eva Marie Saint. The making was a race itself, as John Sturges and Steve McQueen planned to make a similar movie titled "Day of the Champion". Due to their contract with the German Nürburgring, Frankenheimer had to turn over 27 reels shot there to Sturges. Frankenheimer was ahead in schedule anyway, and the McQueen/Sturges project was called off, while the German race track was only mentioned briefly in "Grand Prix". Introducing methods of photographing high-speed auto racing that had never been seen before, mounting cameras on the cars, at full speed and putting the stars in the actual cars, instead of against rear-projections, the film was an international success and won three Oscars, for editing, sound and sound effects.
Late 1960s.
Frankenheimer's next film, 1967's all-star anti-war comedy "The Extraordinary Seaman", starred David Niven, Faye Dunaway, Alan Alda and Mickey Rooney. The film was a failure at the box office and critically. Frankenheimer calls it in the Champlin book "the only movie I've made which I would say was a total disaster."
Then came 1968's "The Fixer", about a Jew in Tsarist Russia and based on the novel by Bernard Malamud. The film was shot in Communist Hungary. It starred Alan Bates and was not a major success, but Bates was nominated for an Oscar.
Frankenheimer was a friend of Senator Robert F. Kennedy and drove him to the Ambassador Hotel in Los Angeles the night Kennedy was assassinated in June 1968.
"The Gypsy Moths" was a romantic drama about a troupe of barnstorming skydivers and their impact on a small midwestern town. The celebration of Americana starred Frankenheimer regular Lancaster, reuniting him with "From Here to Eternity" co-star Deborah Kerr, and it also featured Gene Hackman. The film failed to find an audience, but Frankenheimer always called it one of his personal favorites.
1970s.
Frankenheimer followed this with "I Walk the Line" in 1970. The film, starring Gregory Peck and Tuesday Weld, about a Tennessee sheriff who falls in love with a moonshiner's daughter, was set to songs by Johnny Cash.
Frankenheimer's next project took him to Afghanistan. "The Horseman" focused on the relationship between a father and son, played by Jack Palance and Omar Sharif. Sharif's character, an expert horseman, played the Afghan national sport of buzkashi.
"Impossible Object", also known as "Story of a Love Story", suffered distribution difficulties and was not widely released. Next came a four-hour film of O'Neill's "The Iceman Cometh", in 1973, starring Lee Marvin, and the decidedly offbeat "99 and 44/100% Dead", a crime black comedy starring Richard Harris.
With his fluent French and knowledge of the culture, Frankenheimer was asked to direct "French Connection II", set entirely in Marseille. With Hackman reprising his role as New York cop Popeye Doyle, the film was a success and got Frankenheimer his next job, "Black Sunday" in 1976.
"Black Sunday", based on author Thomas Harris's only non-Hannibal Lecter novel, involves an Israeli Mossad agent (Robert Shaw), chasing a pro-Palestinian terrorist (Marthe Keller) and a disgruntled Vietnam vet (Bruce Dern), who plan to blow up the Goodyear blimp over the Super Bowl. It was shot on location at the actual "Super Bowl X" in January 1976 in Miami, with the use of a real Goodyear Blimp. The film tested very highly, and Paramount and Frankenheimer had high expectations for it. When it failed to become the hit that was expected, Frankenheimer admitted that he had developed a serious problem with alcohol. In later years, Frankenheimer theorized that the audience may have developed an affinity over the course of the movie for the character played by Dern and thus felt conflicted when he was defeated at the end.
Frankenheimer is quoted in Champlin's biography as saying that his alcohol problem caused him to do work that was below his own standards on "Prophecy" (1979), an ecological monster movie about a mutant grizzly bear terrorizing a forest in Maine.
1980s.
In 1981, Frankenheimer travelled to Japan to shoot the cult martial-arts action film "The Challenge", with Scott Glenn and legendary Japanese star, Toshiro Mifune. He tells Champlin that his drinking became so severe while shooting in Japan that he actually drank on set, which he had never done before, and as a result he entered rehab on returning to America. The film was released in 1982, along with his HBO television adaptation of the acclaimed play "The Rainmaker".
In 1985, Frankenheimer directed an adaptation of the Robert Ludlum bestseller "The Holcroft Covenant", starring Michael Caine. That was followed the next year with another adaptation, "52 Pick-Up", from the novel by Elmore Leonard. "Dead Bang" (1989) followed Don Johnson as he infiltrated a group of white supremacists. In 1990, he returned to the Cold War political thriller genre with "The Fourth War" with Roy Scheider (with whom Frankenheimer had worked previously on "52 Pick-Up") as a loose cannon Army colonel drawn into a dangerous personal war with a Russian officer. It was not a commercial success.
1990s.
Most of his 1980s films were less than successful, both critically and financially, but Frankenheimer was able to make a comeback in the 1990s by returning to his roots in television. He directed two films for HBO in 1994: "Against the Wall" and "The Burning Season" that won him several awards and renewed acclaim. The director also helmed two films for Turner Network Television in 1996 and 1997, "Andersonville" and "George Wallace", that were highly praised.
Frankenheimer's 1996 film "The Island of Doctor Moreau", which he took over half a week into production from Richard Stanley, was the cause of countless stories of production woes and personality clashes and received scathing reviews. It was said that the veteran director could not stand Val Kilmer, the young co-star of the film. When Kilmer's last scene was completed it was reported that Frankenheimer said, "Now get that bastard off my set." In an interview, Frankenheimer refused to discuss the film, saying only that he had a miserable time making it. Frankenheimer also professed that "Will Rogers never met Val Kilmer".
However, his next film, 1998's "Ronin", starring Robert De Niro, was a return to form, featuring Frankenheimer's now trademark elaborate car chases woven into a labyrinthine espionage plot. Co-starring an international cast including Jean Reno and Jonathan Pryce, it was a critical and box-office success. As the 1990s drew to a close, he even had a rare acting role, appearing in a cameo as a U.S. General in "The General's Daughter" (1999). He earlier had an uncredited cameo as a TV director in his 1977 film "Black Sunday".
2000s.
Frankenheimer's last theatrical film, 2000's "Reindeer Games", starring Ben Affleck, underperformed. But then came his final film, "Path to War" for HBO in 2002, which brought him back to his strengths – political machinations, 1960s America and character-based drama, and was nominated for numerous awards. A look back at the Vietnam War, it starred Michael Gambon as President Lyndon Johnson along with Alec Baldwin and Donald Sutherland.
One of Frankenheimer's last projects was the 2001 BMW action short-film "Ambush" for the promotional series "The Hire", starring Clive Owen.
Frankenheimer was scheduled to direct "", but it was announced before filming started that he was withdrawing, citing health concerns. Paul Schrader replaced him. About a month later he died suddenly in Los Angeles, California, from a stroke due to complications following spinal surgery at the age of 72.
Despite the many celebrated films he directed, many of which won Academy Awards in various categories, Frankenheimer was never nominated for a Best Director Oscar.
Awards.
British Academy Film Awards
Cannes Film Festival
New York Film Critics Circle Award
Venice Film Festival
John Frankenheimer is also a member of the Television Hall of Fame. He was inducted there in 2002.

</doc>
<doc id="16346" url="https://en.wikipedia.org/wiki?curid=16346" title="John Jellicoe, 1st Earl Jellicoe">
John Jellicoe, 1st Earl Jellicoe

Admiral of the Fleet John Rushworth Jellicoe, 1st Earl Jellicoe, (5 December 1859 – 20 November 1935) was a Royal Navy officer. He fought in the Anglo-Egyptian War and the Boxer Rebellion and commanded the Grand Fleet at the Battle of Jutland in May 1916 during World War I. His handling of the fleet at that battle was controversial: he made no serious mistakes and the German High Seas Fleet retreated to port – at a time when defeat would have been catastrophic for Britain – but at the time the British public were disappointed that the Royal Navy had not won a victory on the scale of the Battle of Trafalgar. Jellicoe later served as First Sea Lord, overseeing the expansion of the Naval Staff at the Admiralty and the introduction of convoy, but was removed at the end of 1917. He also served as the Governor-General of New Zealand in the early 1920s.
Early career.
Born the son of John Henry Jellicoe, a captain in the Royal Mail Steam Packet Company, and Lucy Henrietta Jellicoe (née Keele) and educated at Field House School in Rottingdean, Jellicoe joined the Royal Navy as a cadet in the training ship in 1872. He was made a midshipman in the steam frigate in September 1874 before transferring to the ironclad in the Mediterranean Fleet in July 1877. Promoted to sub-lieutenant on 5 December 1878, he joined , flagship of the Mediterranean Fleet, as signal sub-lieutenant in 1880. Promoted to lieutenant on 23 September 1880, he returned to HMS "Agincourt" in February 1881 and commanded a rifle company of the Naval Brigade at Ismailia during the Egyptian war of 1882.
Jellicoe qualified as a gunnery officer in 1883 and was appointed to the staff of the gunnery school in May 1884. He joined the turret ship as gunnery officer in September 1885 and was awarded the Board of Trade Silver Medal for rescuing the crew of a capsized steamer near Gibraltar in May 1886. He joined the battleship in April 1886 and was put in charge of the experimental department at HMS "Excellent" in December 1886 before being appointed assistant to the Director of Naval Ordnance in September 1889.
Promoted to commander on 30 June 1891, Jellicoe joined the battleship in the Mediterranean Fleet in March 1892. He transferred to the battleship in 1893 (the flagship of the Commander-in-Chief of the Mediterranean Fleet, Vice-Admiral Sir George Tryon) and was aboard when it collided with and was wrecked off Tripoli on 22 June 1893. He was then appointed to the new flagship, , in October 1893.
Promoted to captain on 1 January 1897, Jellicoe became a member of the Admiralty's Ordnance Committee. He served as Captain of the battleship and chief of staff to Vice Admiral Sir Edward Seymour during the Seymour Expedition to relieve the legations at Peking during the Boxer Rebellion in June 1900. He was badly wounded during the Battle of Beicang and told he would die but confounded the attending doctor and chaplain by living. He was appointed a Companion of the Order of the Bath and given the German Order of the Red Eagle, 2nd class, with Crossed Swords for services rendered in China. "Centurion" returned to the United Kingdom in August 1901, and was paid off the following month, when Captain Jellicoe and the crew went on leave. He became Naval Assistant to Third Naval Lord and Controller of the Navy in February 1902 and was given command of the armoured cruiser on the North America and West Indies Station in August 1903.
High command.
As a protege of Admiral John Fisher, Jellicoe became Director of Naval Ordnance in 1905 and, having been appointed a Commander of the Royal Victorian Order on the occasion of launching of on 10 February 1906, he was also made an Aide-de-Camp to the King on 8 March 1906. Promoted to rear-admiral on 8 February 1907, he pushed hard for funds to modernise the navy, supporting the construction of new -type battleships and s. He supported F. C. Dreyer's improvements in gunnery fire-control systems, and favoured the adoption of Dreyer's "Fire Control Table", a form of mechanical computer for calculating firing solutions for warships. Jellicoe arranged for the output of naval ordnance to be transferred from the War Office to the Admiralty.
Jellicoe was appointed second-in-command of the Atlantic Fleet in August 1907, hoisting his flag in the battleship . He was appointed Knight Commander of the Royal Victorian Order on the occasion of the King's Review of the Home Fleet in the Solent on 3 August 1907. He went on to be Third Sea Lord and Controller of the Navy in October 1908 and, having taken part in the funeral of King Edward VII in May 1910, he became Commander-in-Chief, Atlantic Fleet in December 1910, hoisting his flag in the battleship . He advanced to Knight Commander of the Order of the Bath on the Coronation of King George V on 19 June 1911 and confirmed in the rank of vice-admiral on 18 September 1911. He went on to be Second-in-Command of the Home Fleet, hoisting his flag in the battleship , in December 1911 and, having also been appointed commander of the 2nd Battle Squadron in May 1912, joined an inquiry into the supply and storage of liquid fuels in peace and war on 1 August 1912. He became Second Sea Lord in December 1912.
World War I.
Grand Fleet.
At the start of World War I, Admiral George Callaghan, Commander-in-Chief of the Home Fleet, was removed by First Lord of the Admiralty Winston Churchill. Jellicoe was promoted to full admiral on 4 August 1914 and assigned command of the renamed Grand Fleet in Admiral Callaghan's place, though he was appalled by the treatment of his predecessor. He was advanced to Knight Grand Cross of the Order of the Bath on 8 February 1915.
When Fisher (First Sea Lord) and Churchill (First Lord of the Admiralty) both had to leave office after their quarrel over the Dardanelles, Jellicoe wrote to Fisher: “We owe you a debt of gratitude for having saved the Navy from a continuance in office of Mr Churchill, and I hope that never again will any politician be allowed to usurp the functions that he took upon himself to exercise”.
Jellicoe was in command of the British Grand Fleet at the Battle of Jutland in May 1916, history's largest (and only major) clash of dreadnoughts, albeit an indecisive one. His handling of the Grand Fleet during the battle remains controversial, with some historians describing Jellicoe as too cautious and other historians faulting the battlecruiser commander, Admiral David Beatty, for making various tactical errors. Jellicoe certainly made no significant mistakes during the battle: based on limited intelligence, he correctly deployed the Grand Fleet with a turn to port so as to "cross the T" of the German High Seas Fleet as it appeared. After suffering heavy shell damage, the German fleet turned 180 degrees and headed away from the battle. Jellicoe was criticised for not pursuing the German High Seas Fleet, but it is unclear that this would have been sensible, given the risk of German torpedo attacks. At the time the British public were disappointed that the Royal Navy had not won a victory on the scale of the Battle of Trafalgar. Churchill described Jellicoe later as 'the only man on either side who could lose the war in an afternoon'—essentially hinting that Jellicoe's decision to prefer caution was strategically correct. Nevertheless, he was appointed a member of the Order of Merit on 31 May 1916, advanced to Knight Grand Cross of the Royal Victorian Order on 17 June 1916 and awarded the Grand Cross of the French Legion of Honour on 15 September 1916.
First Sea Lord.
Jellicoe was appointed First Sea Lord in November 1916. His term of office role saw Britain brought within danger of starvation by German unrestricted U-Boat warfare.
At the War Committee (a Cabinet Committee which discussed strategy in 1915–16) in November 1916, the admirals present, including Jellicoe, told Lloyd George that convoys presented too large a target for enemy ships, and that merchant ship masters lacked the discipline to “keep station” in a convoy. In February 1917, Maurice Hankey wrote a memorandum for Lloyd George calling for the introduction of “scientifically organised convoys”, almost certainly after being persuaded by Commander Henderson and the Shipping Ministry officials with whom he was in contact. After a breakfast meeting (13 February 1917) with Lloyd George, Carson (First Lord of the Admiralty) and Admirals Jellicoe and Duff agreed to “conduct experiments”. However, convoys were not in general use until August 1917, by which time shipping losses to U-boats were already falling from their April peak.
Jellicoe continued to take a pessimistic view, advising the War Policy Committee (a Cabinet Committee which discussed strategy in 1917) during planning meetings for the Third Ypres Offensive in June and July that nothing could be done to defeat the U-boats. However, removing Jellicoe in July, as Lloyd George wanted, would have been politically impossible given Conservative anger at the return of Churchill (still blamed for the Dardanelles) to office as Minister of Munitions. In August and September Lloyd George was preoccupied with Third Ypres and the possible transfer of resources to Italy, whilst the new First Lord of the Admiralty, Sir Eric Campbell Geddes, was reforming the Naval Staff (including creating a post for Wemyss as Deputy First Sea Lord). Geddes and Lloyd George met with Balfour and Carson (both former First Lords of the Admiralty) on 26 October to discuss sacking Jellicoe after he had failed to act on “secret, but absolutely reliable” information about a German attack on a Norwegian convoy, but again nothing came of this as Lloyd George was soon preoccupied by the Battle of Caporetto and the setting up of the Supreme War Council. Geddes wanted to return to his previous job in charge of military transportation in France, and by December it was clear that Lloyd George would have to sack Jellicoe or lose Geddes.
Jellicoe was rather abruptly dismissed by Geddes in December 1917. Before he left for leave on Christmas Eve he received a letter from Geddes demanding his resignation. Geddes’ letter stated that he was still in the building and available to talk, but after consulting Admiral Halsey Jellicoe replied in writing that he would “do what was best for the service”. The move became public knowledge two days later.
The Christmas holiday, when Parliament was not sitting, provided a good opportunity to remove Jellicoe with a minimum of fuss. Geddes squared matters with the King and with the Grand Fleet commander Admiral Beatty (who had initially written to Jellicoe of his “dismay” over his sacking and promised to speak to Geddes, but then did not write to him again for a month) over the holiday. The other Sea Lords talked of resigning (although Jellicoe advised them not to do so), especially when Geddes suggested in a meeting (31 December) that Balfour and Carson had specifically recommended Jellicoe’s removal at the 26 October meeting; they had not done so, although Balfour’s denial was less than emphatic. There was no trouble from the generals, who had a low opinion of Jellicoe. In the end the Sea Lords remained in place, whilst Carson remained a member of the War Cabinet, resigning in January over Irish Home Rule.
Although it was pretended that the decision had been Geddes’ alone, he let slip in the Naval Estimates debate (6 March 1918) that he had been conveying “the decision of the Government”, i.e. of Lloyd George, who had never put the matter to the War Cabinet. MPs picked up on his slip immediately, and Bonar Law (Conservative Leader) admitted in the same debate that he too had had prior knowledge.
As First Sea Lord Jellicoe was awarded the Grand Cordon of the Belgian Order of Leopold on 21 April 1917, the Russian Order of St. George, 3rd Class on 5 June 1917, the Grand Cross of the Italian Military Order of Savoy on 11 August 1917 and the Grand Cordon of the Japanese Order of the Rising Sun on 29 August 1917.
Later war.
Jellicoe was created Viscount Jellicoe of Scapa Flow on 7 March 1918
In June 1918, amidst concerns that—following the Treaty of Brest Litovsk—the Germans were about to requisition the Russian Black Sea Fleet, Lloyd George proposed Jellicoe as Allied Supreme Naval Commander in the Mediterranean. The French were in favour of a combined Allied naval command, but the Italians were not, so nothing came of the suggestion.
Retirement.
Jellicoe was promoted to Admiral of the Fleet on 3 April 1919. He became Governor-General of New Zealand in September 1920 and while out there also served as Grand Master of New Zealand's Masonic Grand Lodge. Following his return to England, he was created Earl Jellicoe and Viscount Brocas of Southampton in the County of Southampton on 1 July 1925. He died of pneumonia at his home in Kensington in London on 20 November 1935 and was buried in St Paul's Cathedral.
Legacy.
In 1919, "Sleep, beneath the wave! a requiem" with words by Rev. Alfred Hall and Music by Albert Ham. was "Dedicated to Admiral Viscount Jellicoe."
The attempt of his official biographer Admiral Bacon to portray him as the conqueror of the U-Boats is, in John Grigg’s view, absurd, as the main decisions were taken by other men. Bacon also claimed that his elevation to a viscountcy on dismissal was a deliberate snub, but in fact Sir John French, the former Commander-in-Chief of the BEF, was only a viscount at the time (both he and Jellicoe became Earls subsequently), whilst Fisher was never more than a Baron. Bacon's neutrality may be questionable as he had himself been sacked by Geddes from command of the Dover Patrol, replaced by Roger Keyes, shortly after Jellicoe’s removal.
Family.
In July 1902 Jellicoe married Gwendoline Cayzer, daughter of the shipping magnate Sir Charles Cayzer; they had a son and five daughters.
Honours.
Ribbon bar (incomplete)

</doc>
<doc id="16347" url="https://en.wikipedia.org/wiki?curid=16347" title="Sandy Woodward">
Sandy Woodward

Admiral Sir John Forster "Sandy" Woodward (1 May 1932 – 4 August 2013) was a British admiral who commanded the British Naval Task Force in the South Atlantic during the Falklands War.
Early life.
Woodward was born on 1 May 1932 at Penzance, Cornwall, to a bank clerk. He was educated at Stubbington House School, preparatory school in Stubbington, Hampshire. He then continued his education at the Britannia Royal Naval College in Dartmouth, Devon. Woodward married, in 1960, Charlotte McMurtrie (with whom he had a son and a daughter), but in 1993 they separated.
Naval career.
Having graduated from the Royal Naval College Dartmouth, Woodward joined the Royal Navy in 1946. He became a submariner in 1954, and was promoted to lieutenant that May. In 1960 he passed the Royal Navy's rigorous Submarine Command Course known as "The Perisher", and received his first command, the T Class submarine HMS "Tireless". Promoted to lieutenant-commander in May 1962, he then commanded HMS "Grampus" before becoming the second in command of the nuclear fleet submarine HMS "Valiant". In 1967, he was promoted to Commander and became the Instructor (known as "Teacher") of The Perisher Course. He took command of HMS "Warspite" in December 1969. He was promoted to the rank of Captain in 1972. In 1974, he became Captain of Submarine Training and in 1976 he took command of HMS "Sheffield".
He became Head of Naval Plans in the Ministry of Defence in 1978. In July 1981, he was promoted to Rear Admiral and appointed as Flag Officer First Flotilla.
Falklands War.
In 1982, he commanded the Hermes aircraft carrier group, Task Force 317.8, in the Falklands War under the Commander-in-Chief Admiral Sir John Fieldhouse. (The task force containing the amphibious ships which launched the actual invasion TF 317.0 was commanded by Commodore Michael Clapp, with Task Force 317.1 being the landing force itself.)
He worked out the timetable for the campaign, starting from the end and working to the start. Knowing that the Argentine forces had to be defeated before the (Southern Hemisphere) winter made conditions too bad, set a latest date by which the land forces had to be ashore, that in turn set a latest date by which control of the air was achieved, and so on.
Possibly the best known single incident was the sinking of the ARA "General Belgrano". He knew that "Belgrano" (and particularly her Exocet armed escorts) were a threat to the task force and he ordered that "Belgrano" be sunk
For his efforts during the war Woodward was knighted. His book "One Hundred Days", co-authored by Patrick Robinson, describing his Falklands experiences, is a candid account of the pressures of high command in wartime and the impact on the individual commander.
Later career.
In 1983, Woodward was appointed Flag Officer Submarines and NATO Commander Submarines Eastern Atlantic. In 1984, he was promoted to Vice Admiral, and in 1985 he was Deputy Chief of the Defence Staff (Commitments). Before retirement in 1989 he also served, from 1987, as Commander-in-Chief Naval Home Command and Flag Aide-de-Camp to the Queen.
Later life.
The first edition of Woodward's memoirs were published in 1992. They were well received and were updated in 2003 and 2012 with updated recollections as well as responses to the memoirs and responses made by Commodore Michael Clapp. In his later life Woodward wrote various opinion pieces for British newspapers regarding defence matters, particularly the Strategic Defence and Security Review.
Woodward died of heart attack on 4 August 2013 in Bosham, West Sussex, England. He also had been suffering from Emphysema in his final years.
Honours and decorations.
On 11 October 1982, Woodward was appointed Knight Commander of the Order of the Bath (KCB) 'in recognition of service within the operations in the South Atlantic'. In the 1989 Queen's Birthday Honours, he was appointed Knight Grand Cross of the Order of the British Empire (GCB).

</doc>
<doc id="16352" url="https://en.wikipedia.org/wiki?curid=16352" title="JIC">
JIC

JIC may refer to:

</doc>
<doc id="16354" url="https://en.wikipedia.org/wiki?curid=16354" title="Johann Friedrich Endersch">
Johann Friedrich Endersch

Johann Friedrich Endersch (25 October 1705 – 28 March 1769) was a German cartographer and mathematician. Endersch also held the title of Royal Mathematician to King Augustus III of Poland. 
Life.
Endersch was born in Dörnfeld an der Heide, Schwarzburg-Rudolstadt, Thuringia, but lived most of his life in Elbing (Elbląg), Royal Prussia in the Polish-Lithuanian Commonwealth.
In 1755 Endersch completed for Imperial Prince-Bishop Adam Stanisław Grabowski ("Celsissimo ac Reverendissimo S. Rom. Imp. Principi Domino Adam Stanislao in Grabowo Grabowski Episcopo Warmiensi et Sambiesi, Terrarum Prussiae Praesidis ...") a map of Warmia titled "Tabula Geographica Episcopatum Warmiensem in Prussia Exhibens". The map, detailing the towns of Warmia (Ermland), was commissioned for the court of Holy Roman Emperor Francis I.
Endersch also made a copper etching that depicted a galiot that had been built in Elbing in 1738 and was named "D' Stadt Elbing" (German for "City of Elbląg").

</doc>
<doc id="16355" url="https://en.wikipedia.org/wiki?curid=16355" title="James Blaylock">
James Blaylock

James Paul Blaylock (born September 20, 1950) is an American fantasy author. He is noted for a distinctive, humorous style, as well as being one of the pioneers of the steampunk genre of science fiction. Blaylock has cited Jules Verne, H. G. Wells, Robert Louis Stevenson, Arthur Conan Doyle and Charles Dickens as his inspirations.
He was born in Long Beach, California; studied English at California State University, Fullerton, receiving an M.A. in 1974; and lives in Orange, California, teaching creative writing at Chapman University. He taught at the Orange County School of the arts up until 2013. Many of his books are set in Orange County, California, and can more specifically be termed "fabulism"that is, fantastic things happen in our present-day world, rather than in traditional fantasy, where the setting is often some other world. His works have also been categorized as magic realism.
He and his friends Tim Powers and K. W. Jeter were mentored by Philip K. Dick. Along with Powers, Blaylock invented the poet William Ashbless. Blaylock and Powers have often collaborated with each other on writing stories, including "The Better Boy", "On Pirates", and "The William Ashbless Memorial Cookbook".
Blaylock is also currently director of the Creative Writing Conservatory at the Orange County High School of the Arts, where Powers is Writer in Residence.
He has been married to his wife, Viki Blaylock, for more than 40 years. They have two sons, John and Danny.
Awards.
Blaylock's short story "Thirteen Phantasms" won the 1997 World Fantasy Award for best Short Fiction. "Paper Dragons" won the award in 1986.
Novels.
The "Balumnia" Trilogy.
Whimsical fantasy inspired, according to the author, by "Wind in the Willows" and "The Hobbit".
The "Narbondo" Series.
Sharing the character of villain Ignacio Narbondo; the first is contemporary fantasy set in 1960s California, while the remainder are Steampunk novels set in Victorian England.
The "Christian" Trilogy.
Present-day fantasy using Christian elements, such as the Holy Grail and the silver coins paid to Judas.
The "Ghosts" Trilogy.
Present-day Californian ghost stories.

</doc>
<doc id="16356" url="https://en.wikipedia.org/wiki?curid=16356" title="Jerry Pournelle">
Jerry Pournelle

Jerry Eugene Pournelle (born August 7, 1933) is an American science fiction writer, essayist and journalist who contributed for many years to the computer magazine "Byte". Pournelle served as President of the Science Fiction and Fantasy Writers of America in 1973.
Early years.
Pournelle was born in Shreveport, the seat of Caddo Parish in northwestern Louisiana, and educated in Capleville, Tennessee. He served in the U.S. Army during the Korean War. Afterwards, he studied at the University of Washington and received a BS in psychology on June 11, 1955; an M.S. in psychology on March 21, 1958; and a PhD in political science in March 1964. The thesis for his M.S. is titled "Behavioural observations of the effects of personality needs and leadership in small discussion groups", and is dated 1957.
His thesis for Pournelle's PhD in political science is titled "The American political continuum; an examination of the validity of the left-right model as an instrument for studying contemporary American political 'isms'" and is dated 1964.
Career.
Pournelle served as campaign research director for the mayoral campaign of 1969 for Los Angeles Mayor Sam Yorty (Democrat), working under campaign director Henry Salvatori. The election took place on May 27, 1969. Pournelle was later named Executive Assistant to the Mayor in charge of research in September 1969, but resigned from the position after two weeks. After leaving Yorty's office, in 1970 he was a consultant to the Professional Educators of Los Angeles (PELA), a group opposed to the unionization of school teachers in LA.
Pournelle was an intellectual protégé of Russell Kirk and Stefan T. Possony. Pournelle wrote numerous publications with Possony, including "The Strategy of Technology" (1970). "The Strategy" has been used as a textbook at the United States Military Academy (West Point), the United States Air Force Academy (Colorado Springs), the Air War College, and the National War College.
Pournelle's work in the aerospace industry includes time he worked at Boeing in the late-1950s. While there, he worked on Project Thor, conceiving of "hypervelocity rod bundles", also known as "rods from God". He edited "Project 75", a 1964 study of 1975 defense requirements. He worked in operations research at The Aerospace Corporation, and North American Rockwell Space Division, and was founding President of the Pepperdine Research Institute. In 1989, Pournelle, Max Hunter, and retired Army Lieutenant General Daniel O. Graham made a presentation to then Vice President Dan Quayle promoting development of the DC-X rocket.
During the 1970s and 1980s, he also published articles on military tactics and war gaming in the military simulations industry in Avalon Hill's magazine "The General". He had previously won first prize in a late 1960's essay contest run by the magazine on how to end the Vietnam war. That led him into correspondences with some of the early figures in Dungeons and Dragons and other fantasy role-playing games.
In 1985, "Footfall", in which Robert A. Heinlein was a thinly veiled minor character, reached the number one spot on the New York Times Best Seller List. Another bestseller, "Lucifer's Hammer" (1977), reached number two. Both novels were written with Larry Niven.
In 1994, Pournelle's friendly relationship with Newt Gingrich led to Gingrich securing a government job for Pournelle's son, Richard. At the time, Pournelle and Gingrich were reported to be collaborating on "a science fiction political thriller." Pournelle's relationship with Gingrich was long established even then, as Pournelle had written the preface to Gingrich's book, "Window of Opportunity" (1985).
Personal life.
In 2008, Pournelle battled a brain tumor, which appeared to respond favorably to radiation treatment. An August 28, 2008 report on his weblog claimed he was now cancer-free.
In 2010, his daughter Jennifer R. Pournelle (writing as J.R. Pournelle), an archaeology professor, e-published a novel "Outies", an authorized sequel to the "Mote in God's Eye" series.
Pournelle suffered a stroke on December 16, 2014, for which he was hospitalized for a time. By June 2015, he was writing again, though impairment from the stroke has slowed his typing.
Fiction.
From the beginning, Pournelle's work has engaged strong military themes. Several books are centered on a fictional mercenary infantry force known as "Falkenberg's Legion". There are strong parallels between these stories and the "Childe Cycle" mercenary stories by Gordon R. Dickson, as well as Heinlein's "Starship Troopers", although Pournelle's work takes far fewer technological leaps than either of these.
Pournelle was one of the few close friends of H. Beam Piper and was granted by Piper the rights to produce stories set in Piper's Terro-Human Future History. This right has been recognized by the Piper estate. Pournelle worked for some years on a sequel to "Space Viking" but seems to have abandoned this in the early 1990s, however John F. Carr intends its completion for release in 2018.
In February 2013, "Variety" reported that motion picture rights to Pournelle's novel "Janissaries" had been acquired by the newly formed Goddard Film Group, headed by Gary Goddard. In October 2013, the IMDbPro site reported that the movie was in development, and that husband-and-wife writing team, Judith and Garfield Reeves-Stevens, had written the screenplay.
Pseudonyms and collaborations.
Pournelle began fiction writing non-SF work under a pseudonym in 1965. His early SF was published as "Wade Curtis", in "Analog" and other magazines. Some of his work is also published as by "J.E. Pournelle".
In the mid-1970s, Pournelle began a fruitful collaboration with Larry Niven; he has also collaborated on novels with Roland J. Green, Michael Flynn, and Steven Barnes, and collaborated as an editor on an anthology series with John F. Carr.
Journalism.
Pournelle wrote the "Chaos Manor" column in "BYTE". In it Pournelle described his experiences with computer hardware and software, some purchased and some sent by vendors for review, at his home office. Because Pournelle was then, according to the magazine, "virtually BYTE's only writer who was a mere user—he didn't create compilers and computers, he merely used them", it began as "The User's Column" in June 1980. Subtitled "Omikron TRS-80 Boards, NEWDOS+, and Sundry Other Matters", an Editor's Note accompanied the article:
Pournelle stated that
He introduced to readers "my friend Ezekiel, who happens to be a Cromemco Z-2 with iCom 8-inch soft-sectored floppy disk drives"; he also owned a TRS-80 Model I, and the first subject discussed in the column was an add-on that permitted it to use the same data and CP/M applications as the Cromemco. The next column appeared in December 1980 with the subtitle "BASIC, Computer Languages, and Computer Adventures",
Ezekiel II, a Compupro S-100 CP/M system, debuted in March 1983. Other computers received nicknames, such as Lucy Van Pelt, Pournelle's "fussbudget" IBM PC, and he referred to generic PC compatibles as "PClones". Pournelle often denounced companies that announced products without delivering them, sarcastically writing that they would arrive "Real Soon Now".
As part of a redesign, in June 1984 the magazine renamed the popular column to "Computing at Chaos Manor", and the accompanying letter column became "Chaos Manor Mail". After the print version of "Byte" ended publication in the United States, Pournelle continued publishing the column for the online version and international print editions of "Byte". In July 2006, Pournelle and "Byte" declined to renew their contract and Pournelle moved the column to his own web site, Chaos Manor Reviews.
In the 1980s, Pournelle was an editor and columnist for "Survive", a survivalist magazine.
Since 1998, Pournelle has maintained a website with a daily online journal, "View from Chaos Manor", a blog dating from before the use of that term. It is a collection of his "Views" and "Mail" from a large variety of reader. This is a continuation of his 1980s blog-like online journal on GEnie. He says he resists using the term "blog" because he considers the word ugly and because he maintains that his "View" is primarily a vehicle for writing rather than a collection of links.
Humorist Dave Barry has fun with Pournelle's guru column in "Byte" magazine in "Dave Barry in Cyberspace".
Politics.
In a 1997 article Norman Spinrad wrote that Pournelle had written the SDI portion of Ronald Reagan's State of the Union Address, as part of a plan to use SDI to get more money for space exploration, using the larger defense budget. "Jerry Pournelle delights in setting up complex background situations and plots, leading the reader step by step towards a solution which is the very opposite of "politically correct" and (...) defying a dissenting reader to find where in this logical chain he or she would have acted differently".
In "The Mercenary", later integrated into "Falkenberg's Legion", the newly independent planet Hadley is threatened with economic collapse, famine, and resulting mass death. This can only be avoided by having a large part of its city population relocated to the countryside and assigned to work in agriculture (a socialist solution which is very reminiscent of Mao's "cultural revolution"). This solution is unpopular, and the leading Freedom Party won't hear of it. The party uses bloody, violent means to force the planet's President to resign and get themselves into power. The story's protagonist, mercenary commander John Christian Falkenberg, finds what he considers a brutal but unavoidable solution: in order to force the city people to move to the countryside, the Freedom Party must be completely crushed, in however bloody a way – as the other alternative is a total economic collapse in which at least a third of the population would perish. Accordingly, he gets his soldiers into the stadium where the Freedom Party holds its rally, catching its members by complete surprise. His men break the disorganized resistance and proceed to systematically kill the armed militants and party leaders. Mission completed, Falkenberg hands over power to a well-meaning liberal who hitherto could only wring his hands in despair, and departs the planet. Falkenberg freely offers to use himself and his men as scapegoats, since "nobody is going to forget what happened today".
The climax and perhaps some of the politics are borrowed from Fletcher Pratt's "The Battles That Changed History", specifically "Fighting in the Streets and the Future of Order." Justinian the Great suppressed a revolt in Constantinople by seeming to capitulate, and then sending in Belisarius with reliable mercenaries to butcher the celebrating faction in the Hippodrome together with their leaders. This incident is formally known as the Nika riots.
In Footfall, elephant-like alien invaders seize a foothold in Kansas. Unable to dislodge them with conventional weapons, the US government finally resorts to annihilating Kansas with nuclear weapons—killing aliens and humans alike. Later, when the aliens continue their offensive, the President authorizes the construction of a spaceship powered by nuclear explosions; the dangerous technology is presented as the only viable technology available to humans for powering a space warship. Safety, environmental and civil rights protections are suspended in the construction area. An investigative journalist discovers the Orion ship. Wrestling with whether to reveal the scoop of the century to the world (and therefore alerting the alien invaders as well), he confides the secret to an environmental activist. Although he does this as protection against being arrested by the government and had not definitively decided to publish, the activist kills him to protect the secret.
After the human ship fights the alien mothership to the brink of destruction, the aliens finally attempt to negotiate a surrender. The President expresses his willingness to accept a peaceful settlement. Unwilling to spare the enemy mothership for a mere promise from the alien leader, the National Security Advisor seizes control of the government and refuses the alien's terms. The aliens immediately turn their ship over to human control and offer their unconditional surrender.
In Lucifer's Hammer, the world is thrown into total chaos by the disastrous strike of a comet. In the wreckage of central California, a coalition of US Army deserters, Black Power activists, militant environmentalists, and evangelical religious fanatics take up cannibalism and pursue an anti-technological crusade against the remaining enclaves of civilization. When a farming community is attacked by this group, the settlers are forced to counter the invading army's superior numbers, fanaticism and weapons with home-brewed chemical weapons (mustard gas). The farmers successfully use this weapon of mass destruction to annihilate their enemies, enslaving the survivors.

</doc>
<doc id="16360" url="https://en.wikipedia.org/wiki?curid=16360" title="Barlaam and Josaphat">
Barlaam and Josaphat

Barlaam and Josaphat () are two legendary Christian martyrs and saints, their story probably based ultimately on the life of the Buddha. It tells how an Indian king persecuted the Christian Church in his realm. When astrologers predicted that his own son would some day become a Christian, the king imprisoned the young prince Josaphat, who nevertheless met the hermit Saint Barlaam and converted to Christianity. After much tribulation the young prince's father accepted the true faith, turned over his throne to Josaphat, and retired to the desert to become a hermit. Josaphat himself later abdicated and went into seclusion with his old teacher Barlaam. The tale derives from a second to fourth century Sanskrit Mahayana Buddhist text, via a Manichee version, then the Arabic "Kitab Bilawhar wa-Yudasaf" (Book of Bilawhar and Yudasaf), current in Baghdad in the eighth century, from where it entered into Middle Eastern Christian circles before appearing in European versions. The two were entered in the Eastern Orthodox calendar with a feast-day on 26 August, and in the Roman Martyrology in the Western Church as "Barlaam and Josaphat" on the date of 27 November.
Buddhist original.
The story of the two Indian saints was ultimately derived, through a variety of intermediate versions (Arabic and Georgian), from the life story of the Buddha.
Wilfred Cantwell Smith (1981) traced the story from a 2nd to 4th century Sanskrit Mahayana Buddhist text, to a Manichee version, which then found its way into Muslim culture as the Arabic "Kitab Bilawhar wa-Yudasaf" (Book of Bilawhar and Yudasaf), which was current in Baghdad in the 8th century.
Eastern versions.
The Bilauhar u Buddsaf story was translated into Pahlavi during the Sasanian period, and into Arabic in the Islamic era. This is not a strict translation of the Sanskrit Buddhacarita (Life of Buddha) but a collection of legends.
The Arabic version is Balauhar wa Budasaf, in 8th Century and 10th Century versions. The name changed from Budasaf to Yudasaf, then to Yuzasaf.
Arabic.
Daniel Gimaret ("Kitāb Bilawhar wa Bûd̲âsf" 1971) defines ten principal sources:
Sogdian, Turfan, Uyghur.
One example of Bilauhar and Bfidisaf is written in the Turfan dialect of the Uyghur language in the 10th century.
Persian.
The legend of Balauhar and Budasaf appears in Persian texts as Bilawhar wa Yudhâsâf in Muhammad Baqir Majlesi (1616–1698) "Ayn al-Hayat".
Confusion of Kushinara and Kashmir.
Lang (1960) notes that the connection of the Buddhist Yuzasaf with Kashmir in part results from a printing error in the Bombay Arabic edition referencing the legend of the Wisdom of Balahvar which makes its hero prince Yuzasaf die in "Kashmir" (Arabic: كشمير) by confusion with Kushinara (Pali: كوشينر), the traditional place of the original Buddha's death. The disassociation of Yudasaf with the Hindu town of Kushinara and association with Kashmir is found in several local Kashmiri histories from the 17th Century onwards, leading to traditions associated with the Roza Bal shrine in Srinagar.
As Lang notes, the Bombay Arabic printing and the English translation of Ibn Babawayah also have "Kashmir" for "Kushinara":
Saulabath in Ibn Babawayah's text is correctly Kapilavastu. In Buddhist versions the funeral pyre of the Lord Buddha Gautama could not be made to burn until Kashyapa arrived seven days late.
Identification of Yuzasaf with Jesus.
In 1895 the founder of Ahmadiyya Islam, Mirza Ghulam Ahmad made the first identification of the local Kashmiri Josaphat tradition with Jesus of Nazareth, publishing this claim in "Masih Hindustan-mein" (Urdu 1899, English translation Jesus in India 1978).
Paul C. Pappas states that from a historical perspective, this identification of Yuzasaf relies on legends and documents which include clear historical errors (e.g. Gondophares' reign) and that "it is almost impossible to identify Yuz Asaf with Jesus".
Christian version.
The story of Barlaam and Josaphat or Joasaph is a Christianized and later version of the story of Siddhartha Gautama, who became the Buddha. In the Middle Ages the two were treated as Christian saints, being entered in the Greek Orthodox calendar on 26 August, and in the Roman Martyrology in the Western Church as "Barlaam and Josaphat" on the date of 27 November. In the Slavic tradition of the Eastern Orthodox Church, these two are commemorated on 19 November (corresponding to 2 December on the Gregorian calendar).
The first Christianized adaptation was the Georgian epic "Balavariani" dating back to the 10th century. A Georgian monk, Euthymius of Athos, translated the story into Greek, some time before he was killed while visiting Constantinople in 1028. There the Greek adaptation was translated into Latin in 1048 and soon became well known in Western Europe as "Barlaam and Josaphat". The Greek legend of "Barlaam and Ioasaph" is sometimes attributed to the 7th century John of Damascus, but Conybeare argued it was transcribed by the Georgian monk Euthymius in the 11th century.
The story of Barlaam and Josaphat was popular in the Middle Ages, appearing in such works as the "Golden Legend", and a scene there involving three caskets eventually appeared, via Caxton's English translation of a Latin version, in Shakespeare's "The Merchant of Venice".
Two Middle High German versions were produced: one, the "Laubacher "Barlaam"", by Bishop Otto II of Freising and another, "Barlaam und Josaphat", a romance in verse, by Rudolf von Ems. The latter was described as "perhaps the flower of religious literary creativity in the German Middle Ages" by Heinrich Heine.
The story of Josaphat was re-told as an exploration of free will and the seeking of inner peace through meditation in the 17th century.
The legend.
According to the legend, King Abenner or Avenier in India persecuted the Christian Church in his realm, founded by the Apostle Thomas. When astrologers predicted that his own son would some day become a Christian, Abenner had the young prince Josaphat isolated from external contact. Despite the imprisonment, Josaphat met the hermit Saint Barlaam and converted to Christianity. Josaphat kept his faith even in the face of his father's anger and persuasion. Eventually Abenner converted, turned over his throne to Josaphat, and retired to the desert to become a hermit. Josaphat himself later abdicated and went into seclusion with his old teacher Barlaam.
Name.
Ioasaph (Georgian "Iodasaph", Arabic "Yūdhasaf" or "Būdhasaf") is derived from the Sanskrit "Bodhisattva". The Sanskrit word was changed to "Bodisav" in Persian texts in the 6th or 7th century, then to "Budhasaf" or "Yudasaf" in an 8th-century Arabic document (possibly Arabic initial "b" ﺑ changed to "y" ﻳ by duplication of a dot in handwriting). This became "Iodasaph" in Georgia in the 10th century, and that name was adapted as "Ioasaph" in Greece in the 11th century, and then as "Iosaphat" or "Josaphat" in Latin.
Feast day.
Although Barlaam and Josaphat were never formally canonized, they were included in earlier editions of the Roman Martyrology (feast day 27 November) — though not in the Roman Missal — and in the Eastern Orthodox Church liturgical calendar (26 August in Greek tradition etc. / 19 November in Russian tradition).
Texts.
There are a large number of different books in various languages, all dealing with the lives of Saints Barlaam and Josaphat in India. In this hagiographic tradition, the life and teachings of Josaphat have many parallels with those of the Buddha. "But not till the mid-nineteenth century was it recognised that, in Josaphat, the Buddha had been venerated as a Christian saint for about a thousand years." The authorship of the work is disputed. The origins of the story seem to be a Central Asian manuscript written in the Manichaean tradition. This book was translated into Georgian and Arabic.
Greek manuscripts.
The best-known version in Europe comes from a separate, but not wholly independent, source, written in Greek, and, although anonymous, attributed to a monk named John. It was only considerably later that the tradition arose that this was John of Damascus, but most scholars no longer accept this attribution. Instead much evidence points to Euthymius of Athos, a Georgian who died in 1028.
The modern edition of the Greek text, from the 160 surviving variant manuscripts (2006), with introduction (German, 2009) is published as Volume 6 of the works of John the Damascene by the monks of the Abbey of Scheyern, edited by Robert Volk. It was included in the edition due to the traditional ascription, but marked "spuria" as the translator is the Georgian monk Euthymius the Hagiorite (ca. 955–1028) at Mount Athos and not John the Damascene of the monastery of Saint Sabas in the Judaean Desert. The 2009 introduction includes an overview
English manuscripts.
Among the mansuscripts in English, two of the most important are the British Museum "MS Egerton 876" (the basis for Ikegami's book) and "MS Peterhouse 257" (the basis for Hirsh's book) at the University of Cambridge.
The book contains a tale similar to The Three Caskets found in the "Gesta Romanorum" and later in Shakespeare's "The Merchant of Venice".

</doc>
<doc id="16362" url="https://en.wikipedia.org/wiki?curid=16362" title="Jaggies">
Jaggies

"Jaggies" is the informal name for artifacts in raster images, most frequently from aliasing, which in turn is often caused by non-linear mixing effects producing high-frequency components or missing or poor anti-aliasing filtering prior to sampling.
Jaggies are stairlike lines that appear where there should be smooth straight lines or curves. For example, when a nominally straight, un-aliased line steps across one pixel, a dogleg occurs halfway through the line, where it crosses the threshold from one pixel to the other.
Jaggies should not be confused with most compression artifacts, which are a different phenomenon.
Causes.
Jaggies can occur for a variety of reasons, the most common being that the output device (display monitor or printer) does not have enough resolution to portray a smooth line. In addition, jaggies often occur when a bit-mapped image is converted to a different resolution. This is one of the advantages that vector graphics has over bit-mapped graphics – the output looks the same regardless of the resolution of the output device.
Solutions.
The effect of jaggies can be reduced somewhat by a graphics technique known as spatial anti-aliasing. Anti-aliasing smooths out jagged lines by surrounding the jaggies with transparent pixels to simulate the appearance of fractionally-filled pixels. The downside of anti-aliasing is that it reduces contrast – rather than sharp black/white transitions, there are shades of gray – and the resulting image is fuzzy. This is an inescapable trade-off: if the resolution is insufficient to display the desired detail, the output will either be jagged or fuzzy, or some combination thereof.
In realtime computer graphics, especially gaming, anti-aliasing is used to remove jaggies created by the edges of polygons and other lines entirely. Some video game consoles, such as the Xbox 360 and PlayStation 3, have publishing policies which mandated the use of anti-aliasing in some games released for them. Some computer graphics on newer video games are not anti-aliased on video game consoles (Xbox 360 and PlayStation 3), because their hardware can not run graphics smoothly (30 frames per second) if they are anti-aliased. On eighth generation video game consoles, such as the PlayStation 4 and Xbox One, anti-aliasing and frame rate has been heavily improved. Jaggies in bitmaps, such as sprites and surface materials, are most often dealt with by separate texture filtering routines, which are far easier to perform than anti-aliasing filtering. Texture filtering became ubiquitous on PCs after the introduction of 3Dfx's Voodoo GPU.
Notable uses of the term.
In the Atari 8-bit game "Rescue on Fractalus!", published by Lucasfilm Games in 1985, the graphics depicting the cockpit of the player's spacecraft contains two window struts, which are not anti-aliased and are therefore very "jagged". The developers made fun of this and named the in-game enemies "Jaggi", and also initially titled the game "Behind Jaggi Lines!". The latter idea was scrapped by the marketing department before release.

</doc>
<doc id="16364" url="https://en.wikipedia.org/wiki?curid=16364" title="Judicial economy">
Judicial economy

Judicial economy refers broadly to the principle that the limited resources of the legal system or a given court should be conserved.
Multiple causes of action in a given case.
"Judicial economy" most commonly refers to the refusal of a court to decide one or more claims raised in a case, on the grounds that it has decided other claims in the case and that its decision on those claims should satisfy the parties. For example, the plaintiff may claim that the defendant's actions violated three distinct laws. Having found for the plaintiff for a violation of the first law, the court then has the discretion to exercise judicial economy and refuse to make a decision on the remaining two claims, on the grounds that the finding of one violation should be sufficient to satisfy the plaintiff.
Threshold issue in a given case.
In the presence of a threshold issue that will ultimately decide a case, a court may elect to hear that issue rather than proceeding with a full-blown trial.
Class action lawsuits.
Class action lawsuits are another example of judicial economy in action, as they are often tried as a single case, yet involve many cases with similar facts. Rather than trying each case individually, which would unduly burden the judicial system, the cases can be consolidated into a class action.

</doc>
<doc id="16365" url="https://en.wikipedia.org/wiki?curid=16365" title="Jury instructions">
Jury instructions

Jury instructions are the set of legal rules that jurors ought follow when deciding a case. Jury instructions are given to the jury by the jury instructor, who usually reads them aloud to the jury. They are often the subject of discussion of the case, how they will decide who is guilty, and are given by the judge in order to make sure their interests are represented and nothing prejudicial is said.
United States.
Under the American judicial system, juries are often the trier of fact when they serve in a trial. In other words, it is their job to sort through disputed accounts presented in evidence. The judge decides questions of law, meaning he or she decides how the law applies to a given set of facts. The jury instructions provide something of a flow chart on what verdict jurors should deliver based on what they determine to be true. Put another way, "If you believe A (set of facts), you must find X (verdict). If you believe B (set of facts), you must find Y (verdict)." Jury instructions can also serve an important role in guiding the jury how to consider certain evidence.
Forty-eight states (Texas and West Virginia are the exceptions) have a model set of instructions, usually called "pattern jury instructions", which provide the framework for the charge to the jury; sometimes, only names and circumstances have to be filled in for a particular case. Often they are much more complex, although certain elements frequently recur. For instance, if a criminal defendant chooses not to testify, the jury will often be instructed not to draw any negative conclusions from that decision. Many jurisdictions are now instructing jurors not to communicate about the case through social networking services like Facebook and Twitter.
Several studies have discovered that subjects who received no jury instructions comprehended the law better than subjects who received pattern instructions. Jurors retain low comprehension of the most fundamental aspects of their roles. For instance, scholarly studies and anecdotal evidence suggest that jurors conflate reasonable doubt with the civil standard of preponderance of the evidence.
In one study, citizens willing to impose the death penalty were presented in two experiments with four sets of instructions (i.e., baseline instructions, instructions used at trial, instructions revised according to Eighth Amendment to the U.S. Constitution holdings, and model instructions written in nontechnical language). Results demonstrated high confusion with the trial instructions, little improvement with revised instructions, significant but case-specific improvements with model instructions, and a strong relationship between miscomprehension and willingness to impose death.
In California, jury instructions were simplified to make them easier for jurors to understand. The courts moved cautiously because, although verdicts are rarely overturned due to jury instructions in civil court, this is not the case in criminal court. For example, the old instructions on burden of proof in civil cases read:
The new instructions read:
Jury nullification instructions.
In one study, results gathered from 144 six-person juries indicated that when juries are in receipt of jury nullification information from the judge or defense attorney they are more likely to acquit a sympathetic defendant and judge a dangerous defendant more harshly than when such information is not present or when challenges are made to nullification arguments. In another study, three nullification instructions varying in explicitness as to nullification were combined with three criminal cases to yield a 3×3 factorial design. Forty-five six-person juries (270 subjects) were randomly assigned to the nine experimental groups. The results showed that juries given explicit nullification instructions were more likely to vote guilty in a drunk driving case, but less likely to do so in a euthanasia case. The third case, which dealt with murder, did not show any differences due to instructions.
It has been argued that by effectively and persistently offering juries instructions that cannot be understood, judges regularly nullify the law.
Instructions permitting jury nullification has sometimes been criticized as promoting chaos, in that it "conveys an implied approval that runs the risk of degrading the legal structure requisite for true freedom, for an ordered liberty that protects against anarchy as well as tyranny." A rebuttal to this is that a jury instruction about jury nullification "would transform the judicial process by providing a more rational basis for jury deliberation and decision making. In particular, it would allow jury deliberation to be an open process in which extrajudicial biases are aired and confronted. Further, those communities whose members are increasingly estranged from the criminal justice system's decision-making process will benefit indirectly from greater participation and, in turn, from power over the kinds of cases prosecuted. In sum, contrary to the argument that a nullification charge is an invitation to anarchy, such a charge could help to control the anarchy that has already gripped much of the system."

</doc>
<doc id="16366" url="https://en.wikipedia.org/wiki?curid=16366" title="Jurisprudence">
Jurisprudence

Jurisprudence is the science, study and theory of law. It includes principles behind law that make the law. Scholars of jurisprudence, also known as jurists or legal theorists (including legal philosophers and social theorists of law), hope to obtain a deeper understanding of the nature of law, of legal reasoning, legal systems and of legal institutions. Modern jurisprudence began in the 18th century and was focused on the first principles of the natural law, civil law, and the law of nations. General jurisprudence can be divided into categories both by the type of question scholars seek to answer and by the theories of jurisprudence, or schools of thought, regarding how those questions are best answered. Contemporary philosophy of law, which deals with general jurisprudence, addresses problems in two rough groups:
Answers to these questions come from four primary schools of thought in general jurisprudence:
Also of note is the work of the contemporary philosopher of law Ronald Dworkin who has advocated a constructivist theory of jurisprudence that can be characterized as a middle path between natural law theories and positivist theories of general jurisprudence.
A further relatively new field is known as therapeutic jurisprudence, concerned with the impact of legal processes on wellbeing and mental health.
Etymology.
The English word is based on the Latin maxim "jurisprudentia": "juris" is the genitive form of "jus" meaning "law", and "prudentia" means "prudence" (also: discretion, foresight, forethought, circumspection; refers to the exercise of good judgment, common sense, and even caution, especially in the conduct of practical matters). The word is first attested in English in 1628, at a time when the word "prudence" had the meaning of "knowledge of or skill in a matter". The word may have come via the French "jurisprudence", which is attested earlier.
History of Jurisprudence.
Ancient Indian jurisprudence is available in various Dharmaśāstra texts starting from the Dharmasutra of Bhodhayana. Jurisprudence already had this meaning in Ancient Rome even if at its origins the discipline was a ("periti") in the "jus" of "mos maiorum" (traditional law), a body of oral laws and customs verbally transmitted "by father to son". Praetors established a workable body of laws by judging whether or not singular cases were capable of being prosecuted either by the edicta, the annual pronunciation of prosecutable offense, or in extraordinary situations, additions made to the edicta. An iudex then would judge a remedy according to the facts of the case.
Their sentences were supposed to be simple interpretations of the traditional customs, but effectively it was an activity that, apart from formally reconsidering for each case what precisely was traditionally in the legal habits, soon turned also to a more equitable interpretation, coherently adapting the law to the newer social instances. The law was then implemented with new evolutive "Institutiones" (legal concepts), while remaining in the traditional scheme. Praetors were replaced in 3rd century BC by a laical body of "prudentes". Admission to this body was conditional upon proof of competence or experience.
Under the Roman Empire, schools of law were created, and the activity constantly became more academic. In the age from the early Roman Empire to the 3rd century, a relevant literature was produced by some notable groups including the Proculians and Sabinians. The scientific depth of the studies was unprecedented in ancient times.
After the 3rd century, "Juris prudentia" became a more bureaucratic activity, with few notable authors. It was during the Eastern Roman Empire (5th century) that legal studies were once again undertaken in depth, and it is from this cultural movement that Justinian's Corpus Juris Civilis was born.
Jurisprudential Theories.
Natural law.
Natural law jurisprudence generally asserts that human law was be backed by decisive reasons for action. In order words, there must be a compelling rationale behind following human law. There are two readings of the natural law jurisprudential stance.
Notions of an objective moral order, external to human legal systems, underlie natural law. What is right or wrong can vary according to the interests one is focused upon. Natural law is sometimes identified with the maxim that "an unjust law is no law at all", but as John Finnis, the most important of modern natural barristers have argued, this maxim is a poor guide to the classical Thomist position. Strongly related to theories of natural law are classical theories of justice, beginning in the West with Plato’s Republic.
Aristotle.
Aristotle is often said to be the father of natural law. Like his philosophical forefathers Socrates, Plato, and other Indian philosophers, Aristotle posited the existence of natural justice or natural right ("dikaion physikon", "δικαίον φυσικόν", Latin "is natural"). His association with natural law is largely due to the way in which he was interpreted by Thomas Aquinas. This was based on Aquinas' conflation of natural law and natural right, the latter of which Aristotle posits in Book V of the "Nicomachean Ethics" (= Book IV of the "Eudemian Ethics"). Aquinas's influence was such as to affect a number of early translations of these passages, though more recent translations render them more literally.
Aristotle's theory of justice is bound up in his idea of the golden mean. Indeed his treatment of what he calls "political justice" derives from his discussion of "the just" as a moral virtue derived as the mean between opposing vices, just like every other virtue he describes. His longest discussion of his theory of justice occurs in "Nicomachean Ethics" and begins by asking what sort of mean a just act is. He argues that the term "justice" actually refers to two different but related ideas: general justice and particular justice. When a person's actions are completely virtuous in all matters in relation to others, Aristotle calls her "just" in the sense of "general justice;" as such this idea of justice is more or less coextensive with virtue. "Particular" or "partial justice", by contrast, is the part of "general justice" or the individual virtue that is concerned with treating others equitably. Aristotle moves from this unqualified discussion of justice to a qualified view of political justice, by which he means something close to the subject of modern jurisprudence. Of political justice, Aristotle argues that it is partly derived from nature and partly a matter of convention. This can be taken as a statement that is similar to the views of modern natural law theorists. But it must also be remembered that Aristotle is describing a view of morality, not a system of law, and therefore his remarks as to nature are about the grounding of the morality enacted as law, not the laws themselves. The passage here is silent as to that question.
The best evidence of Aristotle's having thought there was a natural law comes from the "Rhetoric", where Aristotle notes that, aside from the "particular" laws that each people has set up for itself, there is a "common" law that is according to nature. The context of this remark, however, suggests only that Aristotle thought that it could be rhetorically advantageous to appeal to such a law, especially when the "particular" law of ones' own city was adverse to the case being made, not that there actually was such a law; Aristotle, moreover, considered two of the three candidates for a universally valid, natural law suggested in this passage to be wrong. Aristotle's theoretical paternity of the natural law tradition is consequently disputed.
Thomas Aquinas.
Saint Thomas Aquinas, of Aquin, or Aquino (c. 1225 – 7 March 1274) a very controversial person, was an Italian philosopher and theologian in the scholastic tradition, known as "Doctor Angelicus, Doctor Universalis". He is the foremost classical proponent of natural theology, and the father of the Thomistic school of philosophy, for a long time the primary philosophical approach of the Roman Catholic Church. The work for which he is best known is the "Summa Theologica". One of the thirty-five Doctors of the Church, he is considered by many Catholics to be the Church's greatest theologian. Consequently, many institutions of learning have been named after him.
Aquinas distinguished four kinds of law: eternal, natural, human and divine:
Natural law, of course, is based on "first principles":
". . . this is the first precept of the law, that good is to be done and promoted, and evil is to be avoided. All other precepts of the natural law are based on this . . ."
The desires to live and to procreate are counted by Aquinas among those basic (natural) human values on which all other human values are based.
School of Salamanca.
Francisco de Vitoria was perhaps the first to develop a theory of "ius gentium" (the rights of peoples), and thus is an important figure in the transition to modernity. He extrapolated his ideas of legitimate sovereign power to society at the international level, concluding that this scope as well ought to be ruled by just forms respectable of the rights of all. The common good of the world is of a category superior to the good of each state. This meant that relations between states ought to pass from being justified by force to being justified by law and justice. Some scholars have upset the standard account of the origins of International law, which emphasises the seminal text "De iure belli ac pacis" by Grotius, and argued for Vitoria and, later, Suárez's importance as forerunners and, potentially, founders of the field. Others, such as Koskenniemi, have argued that none of these humanist and scholastic thinkers can be understood to have founded international law in the modern sense, instead placing its origins in the post-1870 period.
Francisco Suárez, regarded as among the greatest scholastics after Aquinas, subdivided the concept of "ius gentium". Working with already well-formed categories, he carefully distinguished "ius inter gentes" from "ius intra gentes". "Ius inter gentes" (which corresponds to modern international law) was something common to the majority of countries, although, being positive law, not natural law, was not necessarily universal. On the other hand, "ius intra gentes", or civil law, is specific to each nation.
Thomas Hobbes.
In his treatise "Leviathan, (1651)", Hobbes expresses a view of natural law as a precept, or general rule, found out by reason, by which a man is forbidden to do that which is destructive of his life, or takes away the means of preserving the same; and to omit that by which he thinks it may best be preserved. Hobbes was a social contractarian and believed that the law gained peoples' tacit consent. He believed that society was formed from a state of nature to protect people from the state of war between mankind that exists otherwise. Life is, without an ordered society, "solitary, poor, nasty, brutish and short". It is commonly commented that Hobbes' views about the core of human nature were influenced by his times. The English Civil War and the Cromwellian dictatorship had taken place, and he felt absolute authority vested in a monarch, whose subjects obeyed the law, was the basis of a civilized society.
Lon Fuller.
Writing after World War II, Lon L. Fuller notably emphasised that the law must meet certain formal requirements (such as being impartial and publicly knowable). To the extent that an institutional system of social control falls short of these requirements, Fuller argues, we are less inclined to recognise it as a system of law, or to give it our respect. Thus, law has an internal morality that goes beyond the social rules by which valid laws are made.
John Finnis.
Sophisticated positivist and natural law theories sometimes resemble each other more than the above descriptions might suggest, and they may concede certain points to the other "side". Identifying a particular theorist as a positivist or a natural law theorist sometimes involves matters of emphasis and degree, and the particular influences on the theorist's work. In particular, the older natural lawyers, such as Aquinas and John Locke made no distinction between analytic and normative jurisprudence. But modern natural lawyers, such as John Finnis claim to be positivists, while still arguing that law is a basically moral creature.
Sharia and Fiqh in Islam.
Sharia () refers to the body of Islamic law, which is the most widely used religious law in the world. The term means "way" or "path"; it is the legal framework within which public and most private aspects of life are regulated for those living in a legal system based on Islamic principles of jurisprudence. Fiqh is the term for Islamic jurisprudence, made up of the rulings of Islamic jurists. A component of Islamic studies, Fiqh expounds the methodology by which Islamic law is derived from primary and secondary sources.
Mainstream Islam distinguishes "fiqh", which means understanding the details and inferences drawn by scholars, from "sharia", which refers to the principles behind the "fiqh". Scholars hope that "fiqh" and "sharia" are in harmony in any given case, but this cannot be assured.
Early forms of logic in Islamic philosophy were introduced in Islamic jurisprudence from the 7th century with the process of "Qiyas". During the Islamic Golden Age, there was a logical debate among Islamic philosophers and jurists over whether the term "Qiyas" refers to analogical reasoning, inductive reasoning or categorical syllogism. Some Islamic scholars argued that "Qiyas" refers to reasoning. Ibn Hazm (994-1064) disagreed with this, arguing that "Qiyas" refers rather to categorical syllogism in a real sense and to analogical reasoning in a metaphorical sense. On the other hand, al-Ghazali (1058–1111) argued that "Qiyas" refers to analogical reasoning in a real sense and categorical syllogism in a metaphorical sense. Other Islamic scholars at the time argued that the term "Qiyas" refers to both analogical reasoning and categorical syllogism in a real sense.
Analytic jurisprudence.
Analytic, or 'clarificatory', jurisprudence means the use of a neutral point of view and descriptive language when referring to the aspects of legal systems. This was a philosophical development that rejected natural law's fusing of what law is and what it ought to be. David Hume famously argued in "A Treatise of Human Nature" that people invariably slip between describing that the world "is" a certain way to saying therefore we "ought" to conclude on a particular course of action. But as a matter of pure logic, one cannot conclude that we "ought" to do something merely because something "is" the case. So analysing and clarifying the way the world "is" must be treated as a strictly separate question to normative and evaluative "ought" questions.
The most important questions of analytic jurisprudence are: "What are laws?"; "What is "the" law?"; "What is the relationship between law and power/sociology?"; and "What is the relationship between law and morality?" Legal positivism is the dominant theory, although there are a growing number of critics who offer their own interpretations.
Legal positivists.
Positivism simply means that law is something that is "posited": laws are validly made in accordance with socially accepted rules. The positivist view on law can be seen to cover two broad principles: Firstly, that laws may seek to enforce justice, morality, or any other normative end, but their success or failure in doing so does not determine their validity. Provided a law is properly formed, in accordance with the rules recognized in the society concerned, it is a valid law, regardless of whether it is "just" by some other standard. Secondly, that law is nothing more than a set of rules to provide order and governance of society. No legal positivist, however, argues that it follows that the law is therefore to be obeyed, no matter what. This is seen as a separate question entirely.
Bentham and Austin.
One of the earliest legal positivists was Jeremy Bentham. Bentham was an early and staunch supporter of the utilitarian concept (along with Hume), an avid prison reformer, advocate for democracy, and strongly atheist. Bentham's views about law and jurisprudence were popularized by his student, John Austin. Austin was the first chair of law at the new University of London from 1829. Austin's utilitarian answer to "what is law?" was that law is "commands, backed by threat of sanctions, from a sovereign, to whom people have a habit of obedience". Contemporary legal positivists have long abandoned this view, and have criticised its oversimplification, H. L. A. Hart particularly.
Hans Kelsen.
Hans Kelsen is considered one of the prominent jurists of the 20th century and has been highly influential in Europe and Latin America, although less so in common-law countries. His Pure Theory of Law aims to describe law as binding norms while at the same time refusing, itself, to evaluate those norms. That is, 'legal science' is to be separated from 'legal politics'. Central to the Pure Theory of Law is the notion of a 'basic norm (Grundnorm)'—a hypothetical norm, presupposed by the jurist, from which in a hierarchy all 'lower' norms in a legal system, beginning with constitutional law, are understood to derive their authority or 'bindingness'. In this way, Kelsen contends, the bindingness of legal norms, their specifically 'legal' character, can be understood without tracing it ultimately to some suprahuman source such as God, personified Nature or—of great importance in his time—a personified State or Nation.
H. L. A. Hart.
In th Anglophone world, the pivotal writer was H. L. A. Hart, who argued that the law should be understood as a system of social rules. Hart rejected Kelsen's views that sanctions were essential to law and that a normative social phenomenon, like law, can not be grounded in non-normative social facts. Hart revived analytical jurisprudence as an important theoretical debate in the twentieth century through his book "The Concept of Law". As the professor of jurisprudence at Oxford University, Hart argued that law is a 'system of rules'.
Rules, said Hart, are divided into primary rules (rules of conduct) and secondary rules (rules addressed to officials to administer primary rules). Secondary rules are divided into rules of adjudication (to resolve legal disputes), rules of change (allowing laws to be varied) and the rule of recognition (allowing laws to be identified as valid). The "rule of recognition" is a customary practice of the officials (especially barristers and judges) that identifies certain acts and decisions as sources of law. A pivotal book on Hart was written by Neil MacCormick in 1981 (second edition due in 2007), which further refined and offered some important criticisms that led MacCormick to develop his own theory (the best example of which is his recently published "Institutions of Law", 2007). Other important critiques have included that of Ronald Dworkin, John Finnis, and Joseph Raz.
In recent years, debates about the nature of law have become increasingly fine-grained. One important debate is within legal positivism. One school is sometimes called "exclusive legal positivism", and it is associated with the view that the legal validity of a norm can never depend on its moral correctness. A second school is labeled "inclusive legal positivism", a major proponent of which is Wil Waluchow, and it is associated with the view that moral considerations "may" determine the legal validity of a norm, but that it is not necessary that this is the case.
Joseph Raz.
Some philosophers used to contend that positivism was the theory that there is "no necessary connection" between law and morality; but influential contemporary positivists, including Joseph Raz, John Gardner, and Leslie Green, reject that view. As Raz points out, it is a necessary truth that there are vices that a legal system cannot possibly have (for example, it cannot commit rape or murder).
Joseph Raz defends the positivist outlook, but criticised Hart's "soft social thesis" approach in "The Authority of Law". Raz argues that law is authority, identifiable purely through social sources, without reference to moral reasoning. Any categorisation of rules beyond their role as authority is better left to sociology than to jurisprudence.
Ronald Dworkin.
In his book "Law's Empire" Dworkin attacked Hart and the positivists for their refusal to treat law as a moral issue. Dworkin argues that law is an 'interpretive' concept, that requires barristers to find the best-fitting and most just solution to a legal dispute, given their constitutional traditions. According to him, law is not entirely based on social facts, but includes the morally best justification for the institutional facts and practices that we intuitively regard as legal. It follows on Dworkin's view that one cannot know whether a society has a legal system in force, or what any of its laws are, until one knows some moral truths about the justifications for the practices in that society. It is consistent with Dworkin's view—in contrast with the views of legal positivists or legal realists—that "no-one" in a society may know what its laws are, because no-one may know the best justification for its practices.
Interpretation, according to Dworkin's law as integrity theory, has two dimensions. To count as an interpretation, the reading of a text must meet the criterion of "fit". Of those interpretations that fit, however, Dworkin maintains that the correct interpretation is the one that puts the political practices of the community in their best light, or makes of them "the best that they can be". But many writers have doubted whether there "is" a single best justification for the complex practices of any given community, and others have doubted whether, even if there are, they should be counted as part of the law of that community.
Legal realism.
Legal realism was a view popular with some Scandinavian and American writers. Skeptical in tone, it held that the law should be understood and determined by the actual practices of courts, law offices, and police stations, rather than as the rules and doctrines set forth in statutes or learned treatises. It had some affinities with the sociology of law. The essential tenet of legal realism is that all law is made by human beings and, thus, is subject to human foibles, frailties and imperfections.
It has become quite common today to identify Justice Oliver Wendell Holmes, Jr., as the main precursor of American Legal Realism (other influences include
Roscoe Pound, Karl Llewellyn and Justice Benjamin Cardozo). Karl Llewellyn, another founder of the U.S. legal realism movement, similarly believed that the law is little more than putty in the hands of a judge who is able to shape the outcome of a case based on personal biases.
The chief inspiration for Scandinavian legal realism many consider to be the works of Axel Hägerström. Despite its decline in popularity, realism continues to influence a wide spectrum of jurisprudential schools today, including critical legal studies, feminist legal theory, critical race theory, sociology of law and law and economics.
Historical School.
Historical jurisprudence came to prominence during the German debate over the proposed codification of German law. In his book "On the Vocation of Our Age for Legislation and Jurisprudence", Friedrich Carl von Savigny argued that Germany did not have a legal language that would support codification because the traditions, customs and beliefs of the German people did not include a belief in a code. The Historicists believe that the law originates with society.
Normative jurisprudence.
In addition to the question, "What is law?", legal philosophy is also concerned with normative, or "evaluative" theories of law. What is the goal or purpose of law? What moral or political theories provide a foundation for the law? What is the proper function of law? What sorts of acts should be subject to punishment, and what sorts of punishment should be permitted? What is justice? What rights do we have? Is there a duty to obey the law? What value has the rule of law? Some of the different schools and leading thinkers are as follows.
Virtue jurisprudence.
Aretaic moral theories such as contemporary virtue ethics emphasize the role of character in morality. Virtue jurisprudence is the view that the laws should promote the development of virtuous characters by citizens. Historically, this approach is associated mainly with Aristotle or Thomas Aquinas later. Contemporary virtue jurisprudence is inspired by philosophical work on virtue ethics.
Deontology.
Deontology is "the theory of duty or moral obligation." The philosopher Immanuel Kant formulated one influential deontological theory of law. He argued that any rule we follow must be able to be universally applied, i.e. we must be willing for everyone to follow that rule. A contemporary deontological approach can be found in the work of the legal philosopher Ronald Dworkin.
Utilitarianism.
Utilitarianism is the view that the laws should be crafted so as to produce the best consequences for the greatest number of people possible. Historically, utilitarian thinking about law is associated with the philosopher Jeremy Bentham. John Stuart Mill was a pupil of Bentham's and was the torch bearer for utilitarian philosophy through the late nineteenth century. In contemporary legal theory, the utilitarian approach is frequently championed by scholars who work in the law and economics tradition.
John Rawls.
John Rawls was an American philosopher, a professor of political philosophy at Harvard University and author of "A Theory of Justice" (1971), "Political Liberalism", "", and "The Law of Peoples". He is widely considered one of the most important English-language political philosophers of the 20th century. His theory of justice uses a device called the original position to ask us which principles of justice we would choose to regulate the basic institutions of our society if we were behind a 'veil of ignorance.' Imagine we do not know who we are - our race, sex, wealth status, class, or any distinguishing feature - so that we would not be biased in our own favour. Rawls argues from this 'original position' that we would choose exactly the same political liberties for everyone, like freedom of speech, the right to vote and so on. Also, we would choose a system where there is only inequality because that produces incentives enough for the economic well-being of all society, especially the poorest. This is Rawls's famous 'difference principle'. Justice is fairness, in the sense that the fairness of the original position of choice guarantees the fairness of the principles chosen in that position.
There are many other normative approaches to the philosophy of law, including critical legal studies and libertarian theories of law.

</doc>
<doc id="16367" url="https://en.wikipedia.org/wiki?curid=16367" title="Jury trial">
Jury trial

A jury trial or trial by jury is a legal proceeding in which a jury either makes a decision or makes findings of fact, which then direct the actions of a judge. It is distinguished from a bench trial, in which a judge or panel of judges make all decisions.
Jury trials are used in a significant share of serious criminal cases in almost all common law legal systems, and juries or lay judges have been incorporated into the legal systems of many civil law countries for criminal cases. Only the United States makes routine use of jury trials in a wide variety of non-criminal cases. Other common law legal jurisdictions use jury trials only in a very select class of cases that make up a tiny share of the overall civil docket (e.g. defamation suits in England and Wales), while true civil jury trials are almost entirely absent elsewhere in the world. Some civil law jurisdictions do, however, have arbitration panels where non-legally trained members decide cases in select subject-matter areas relevant to the arbitration panel members' areas of expertise.
The availability of a trial by jury in American jurisdictions varies. Because the United States system separated from that of the English at the time of the American Revolution, the types of proceedings that use juries depends on whether such cases were tried by jury under English common law at that time, rather than the methods used in English or UK courts in the present. For example, at the time English "courts of law" tried cases of torts or private law for monetary damages but "courts of equity" tried civil cases seeking an injunction or another form of non-monetary relief. As a result, this practice continues in American civil laws, even though in modern English law only criminal proceedings and some inquests are likely to be heard by a jury.
The use of jury trials evolved within common law systems rather than civil law systems, has had a profound impact on the nature of American civil procedure and criminal procedure rules, even in cases where a bench trial is actually contemplated in a particular case. In general, the availability of a jury trial if properly demanded has given rise to a system where fact finding is concentrated in a single trial rather than multiple hearings, and where appellate review of trial court decisions is greatly limited. Jury trials are of far less importance (or of no importance) in countries that do not have a common law system.
History.
Greece.
Ancient Athens had a mechanism, called "dikastaí", to assure that no one could select jurors for their own trial. For normal cases, the courts were made up of "dikastai" of up to 500 citizens. For capital cases—those that involved death, loss of liberty, exile, loss of civil rights, or seizure of property—the trial was before a jury of 1,001 to 1,501 "dikastai". In such large juries, the unanimity rule would be unrealistic and verdicts were reached by majority. Juries were appointed by lot. Jurists cast a ceramic disk with an axle in its middle: the axle was either hollow or solid. Thus the way they voted was kept secret because the jurists would hold their disk by the axle by thumb and forefinger, thus hiding whether its axle was hollow or solid. Since Periclean times, jurists were compensated for their sitting in court, with the amount of one day's wages.
The institution of trial by jury was ritually depicted by Aeschylus in the Eumenides, the third and final play of his Oresteia trilogy. In this play the innovation is brought about by the goddess Athena, who summons twelve citizens to sit as jury. The god Apollo takes part in the trial as the advocate for the defendant Orestes, and the Furies as prosecutors for the slain Clytaemnestra. In the event the jury is split six to six, and Athena dicates that in such a case the verdict should henceforth be for acquittal.
Rome.
From the beginning of the republic and in the majority of civil cases towards the end of the empire, there were tribunals with the characteristics of the jury, the Roman judges being civilian, lay and not professional. Capital trials were held in front of juries composed of hundreds or thousands of people in the commitias or centuries, the same as in Roman trials. Roman law provided for the yearly selection of judices, who would be responsible for resolving disputes by acting as jurors, with a praetor performing many of the duties of a judge. High government officials and their relatives were barred from acting as judices, due to conflicts of interest. Those previously found guilty of serious crimes (felonies) were also barred as were gladiators for hire, who likely were hired to resolve disputes through trial by combat. The law was as follows:
The peregrine praetor (literally, traveling judge) within the next ten days after this law is passed by the people or plebs shall provide for the selection of 450 persons in this State who have or have had a knight's census... provided that he does not select a person who is or has been plebeian tribune, quaestor, triumvir capitalis, military tribune in any of the first four legions, or triumvir for granting and assigning lands, or who is or has been in the Senate, or who has fought or shall fight as a gladiator for hire... or who has been condemned by the judicial process and a public trial whereby he cannot be enrolled in the Senate, or who is less than thirty or more than sixty years of age, or who does not have his residence in the city of Rome or within one mile of it, or who is the father, brother, or son of any above-described magistrate, or who is the father, brother, or son of a person who is or has been a member of the Senate, or who is overseas.
Holy Roman Empire.
A Swabian ordinance of 1562 called for the summons of jurymen ('), and various methods were in use in Emmendingen, Oppenau, and Oberkirch. Hauenstein's charter of 1442 secured the right to be tried in all cases by 24 fellow equals, and in Friburg the jury was composed of 30 citizens and councilors. The modern jury trial was first introduced in the Rhenish provinces in 1798, with a court consisting most commonly of 12 citizens (').
The system whereby citizens were tried by their peers chosen from the entire community in open court was gradually superseded by an "engine of tyranny and oppression" in Germany, in which the process of investigation was secret and life and liberty depended upon judges appointed by the state. In Constance the jury trial was suppressed by decree of the Habsburg Monarchy in 1786. The Frankfurt Constitution of the failed Revolutions of 1848 called for jury trials for "the more serious crimes and all political offenses," but was never implemented after the Frankfurt Parliament was dissolved by Württemberg dragoons. An 1873 draft on criminal procedure produced by the Prussian Ministry of Justice proposed to abolish the jury and replace it with the mixed system, causing a significant political debate. In the Weimar Republic the jury was abolished by the Emminger Reform of 4 January 1924.
Between 1948 and 1950 in American-occupied Germany and the Federal Republic of Germany, Bavaria returned to the jury trial as it had existed before the emergency decrees, but they were again abolished by the 1950 Unification Act ("") for the Federal Republic. In 1979, the United States tried the East German LOT Flight 165 hijacking suspects in the United States Court for Berlin in West Berlin, which declared the defendants had the right to a jury trial under the United States Constitution, and hence were tried by a West German jury.
England and Wales.
According to George Macaulay Trevelyan in "A Shortened History of England", during the Viking occupation: "The Scandinavians, when not on the Viking warpath, were a litigious people and loved to get together in the ‘thing’ to hear legal argument. They had no professional lawyers, but many of their farmer-warriors, like Njal, the truth-teller, were learned in folk custom and in its intricate judicial procedure. A Danish town in England often had, as its main officers, twelve hereditary ‘law men.’ The Danes introduced the habit of making committees among the free men in court, which perhaps made England favorable ground for the future growth of the jury system out of a Frankish custom later introduced by the Normans."
The English king Æthelred the Unready set up an early legal system through the Wantage Code of Ethelred, one provision of which stated that the twelve leading thegns (minor nobles) of each wapentake (a small district) were required to swear that they would investigate crimes without a bias. These juries differed from the modern sort by being self-informing; instead of getting information through a trial, the jurors were required to investigate the case themselves.
In the 12th century, Henry II took a major step in developing the jury system. Henry II set up a system to resolve land disputes using juries. A jury of twelve free men were assigned to arbitrate in these disputes. As with the Saxon system, these men were charged with uncovering the facts of the case on their own rather than listening to arguments in court. Henry II also introduced what is now known as the "grand jury" through his Assize of Clarendon. Under the assize, a jury of free men was charged with reporting any crimes that they knew of in their hundred to a "justice in eyre," a judge who moved between hundreds on a circuit. A criminal accused by this jury was given a trial by ordeal.
The Church banned participation of clergy in trial by ordeal in 1215. Without the legitimacy of religion, trial by ordeal collapsed. The juries under the assizes began deciding guilt as well as providing accusations. The same year, trial by jury became an explicit right in one of the most influential clauses of Magna Carta. Article 39 of the Magna Carta read:
"Nullus liber homo capiatur, vel imprisonetur, aut desseisetur de libero tenemento, vel libertatibus, vel liberis consuetudinibus suis, sut utlagetur, aut exuletur, aut aliquo modo destruatur, nec super eum ibimus, nec super eum mittemus, nisi per legale judicium parium suorum, vel per legem terrae."
It is translated thus by Lysander Spooner in his "Essay on the Trial by Jury":
"No free man shall be captured, and or imprisoned, or disseised of his freehold, and or of his liberties, or of his free customs, or be outlawed, or exiled, or in any way destroyed, nor will we proceed against him by force or proceed against him by arms, but by the lawful judgment of his peers, and or by the law of the land."
Although it says "and or by the law of the land", this in no manner can be interpreted as if it were enough to have a positive law, made by the king, to be able to proceed legally against a citizen. The law of the land was the consuetudinary law, based on the customs and consent of John’s subjects, and since they did not have Parliament in those times, this meant that neither the king nor the barons could make a law without the consent of the people.
According to some sources, in the time of Edward III, "by the law of the land" had been substituted "by due process of law", which in those times was a trial by twelve peers.
The Magna Carta of 1215 further secured trial by jury by stating that
During the mid-14th Century, persons who had sat on the Presenting Jury (i.e., in modern parlance, the Grand Jury) were forbidden to sit on the trial jury for that crime. 25 Edward III stat 5., c3 (1353). Medieval juries were self-informing, in that individuals were chosen as jurors because they either knew the parties and the facts, or they had the duty to discover them. This spared the government the cost of fact-finding. Over time, English juries became less self-informing and relied more on the trial itself for information on the case. Jurors remained free to investigate cases on their own until the 17th century. The Magna Carta being forgotten after a succession of benevolent reigns (or, more probably, reigns limited by the jury and the barons, and only under the rule of laws that the juries and barons found acceptable), the kings, through the royal judges, began to extend their control over the jury and the kingdom. In David Hume's "History of England", he tells something of the powers that the kings had accumulated in the times after the Magna Carta, the prerogatives of the crown and the sources of great power with which these monarchs counted:
The first paragraph of the Act that abolished the Star Chamber repeats the clause on the right of a citizen to be judged by his peers:
In 1670 two Quakers charged with unlawful assembly, William Penn and William Mead, were found not guilty by a jury. The judge then fined the jury for contempt of court for returning a verdict contrary to their own findings of fact and removed them to prison until the fine was paid. Edward Bushel, a member of the jury, nonetheless refused to pay the fine.
Bushel petitioned the Court of Common Pleas for a writ of "habeas corpus". The ruling in the "Bushel's Case" was that a jury could not be punished simply on account of the verdict it returned.
Many British colonies, including the United States, adopted the English common law system in which trial by jury is an important part. Jury trials in criminal cases were a protected right in the original United States Constitution and the Fifth, Sixth, and Seventh Amendments of the US Constitution extend the rights to trial by jury to include the right to jury trial for both criminal and civil matters and a grand jury for serious cases.
The role of jury trials.
Some jurisdictions with jury trials allow the defendant to waive their right to a jury trial, thus leading to a bench trial. Jury trials tend to occur only when a crime is considered serious. In some jurisdictions, such as France and Brazil, jury trials are reserved, and compulsory, for the most severe crimes and are not available for civil cases. In Brazil, for example, trials by jury are applied in cases of voluntary crimes against life, such as first and second degree murder, forced abortion and instigation of suicide, even if only attempted. In others, such as the United Kingdom, jury trials are only available for criminal cases and very specific civil cases (defamation, malicious prosecution, civil fraud and false imprisonment). In the United States, jury trials are available in both civil and criminal cases. In Canada, an individual charged with an indictable offence may elect to be tried by a judge alone in a provincial court, by judge alone in a superior court, or by judge and jury in a superior court; summary offences cannot be tried by jury.
In the United States, because jury trials tend to be high profile, the general public tends to overestimate the frequency of jury trials. Approximately 150,000 jury trials are conducted in state courts annually, and an additional 5,000 jury trials are conducted in federal courts. Two-thirds of jury trials are criminal trials, while one-third are civil and "other" (e.g., family, municipal ordinance, traffic). Nevertheless, the vast majority of criminal cases are settled by plea bargain, which removes the need for a jury trial.
Some commentators contend that the guilty-plea system unfairly coerces defendants into relinquishing their right to a jury trial. Others contend that there never was a golden age of jury trials, but rather that juries in the early nineteenth century (before the rise of plea bargaining) were "unwitting and reflexive, generally wasteful of public resources and, because of the absence of trained professionals, little more than slow guilty pleas themselves," and that the guilty-plea system that emerged in the latter half of the nineteenth century was a superior, more cost-effective method of achieving fair outcomes.
Pros and cons.
In countries where jury trials are common, juries are often seen as an important check against state power. Other common assertions about the benefits of trial by jury is that it provides a means of interjecting community norms and values into judicial proceedings and that it legitimizes the law by providing opportunities for citizens to validate criminal statutes in their application to specific trials. Alexis de Tocqueville also claimed that jury trials educate citizens about self-government. Many also believe that a jury is likely to provide a more sympathetic hearing, or a fairer one, to a party who is not part of the governmentor other establishment interestthan would representatives of the state.
This last point may be disputed. For example, in highly emotional cases, such as child rape, the jury may be tempted to convict based on personal feelings rather than on conviction beyond reasonable doubt. In France, former attorney, then later minister of Justice Robert Badinter, remarked about jury trials in France that they were like "riding a ship into a storm," because they are much less predictable than bench trials.
Another issue with jury trials is the potential for jurors to be swayed by prejudice, including racial considerations. An infamous case was the 1992 trial in the Rodney King case in California, in which white police officers were acquitted of excessive force in the violent beating of a black man by a jury consisting mostly of whites without any black jurors.
The positive belief about jury trials in the UK and the US contrasts with popular belief in many other nations, in which it is considered bizarre and risky for a person's fate to be put into the hands of untrained laymen. Consider Japan, for instance, which used to have optional jury trials for capital or other serious crimes between 1928 and 1943. The defendant could freely choose whether to have a jury or trial by judges, and the decisions of the jury were non-binding. During the Tōjō-regime this was suspended, arguably stemming from the popular belief that any defendant who risks his fate on the opinions of untrained laymen is almost certainly guilty. Similarly, jury trials were abolished by the government of India in 1960 (this was followed by Pakistan soon afterwards) on the grounds that they would be susceptible to media and public influence. One Pakistani judge called a trial by jury "amateur justice." Malaysia abolished its jury system on 1 January 1995, citing "inter alia" the danger of jurors untrained in the legal profession delivering verdicts colored by emotions or popular perception. One of the last trials-by-jury in Malaysia was the notorious Mona Fandey case in 1994.
Jury trials in multi-cultural countries with a history of ethnic tensions may be problematic, and lead to juries being unduly biased and partial. This is one of the reasons why both India and Pakistan abolished jury trials soon after independence. Indeed, in these countries, a jury trial is seen as a "failing" of some foreign legal system rather than an advantage; this is despite the fact that both nations are common law countries.
A major issue in jury trials is the secretive nature of the process. While proponents may say that secrecy allows the jury to remain impartial by protecting it from undue pressure or attention, opponents contend that this prevents there from being a transparent trial. The fact that juries do not often have to give a reason for their verdict is also criticized, since opponents argue it is unfair for a person to be deprived of life, liberty or property without being told why it is being done so. In contrast where there is a decision by a judge or judges, they are required to provide often detailed reasons of both fact and law as to why their decision was made.
One issue that has been raised is the ability of a jury to fully understand statistical or scientific evidence. It has been said that the expectation of jury members as to the explanatory power of scientific evidence has been raised by television in what is known as the CSI effect. In at least one English trial the misuse or misunderstanding or misrepresentation by the prosecution of statistics has led to wrongful conviction.
The jury trial in various jurisdictions.
Australia.
The Australian Constitution provides that: "80. The trial on indictment of any offence against any law of the Commonwealth shall be by jury, and every such trial shall be held in the State where the offence was committed, and if the offence was not committed within any State the trial shall be held at such place or places as the Parliament prescribes."
The first trials by civilian juries of 12 in the colony of New South Wales were held in 1824, following a decision of the NSW Supreme Court on 14 October 1824. The NSW Constitution Act of 1828 effectively terminated trial by jury for criminal matters. Jury trials for criminal matters revived with the passing of the Jury Trials Amending Act of 1833 (NSW) (2 William IV No 12).
Challenging potential jurors.
The "voir dire" system of examining the jury pool before selection is not permitted in Australia as it violates the privacy of jurors. Therefore, though it exists, the right to challenge for cause during jury selection cannot be employed much. Peremptory challenges are usually based on the hunches of counsel and no reason is needed to use them. All Australian states allow for peremptory challenges in jury selection, however, the number of challenges granted to the counsels in each state are not all the same. Until 1987 New South Wales had twenty peremptory challenges for each side where the offence was murder, and eight for all other cases. In 1987 this was lowered to three peremptory challenges per side, the same amount allowed in South Australia. Eight peremptory challenges are allowed for both counsels for all offences in Queensland. Victoria, Tasmania and the Northern Territory allow for six. Western Australia allows five peremptory challenges per side.
Majority and unanimous verdicts in criminal trials.
In Australia majority verdicts are allowed in South Australia, Victoria, Western Australia, Tasmania, the Northern Territory, New South Wales and Queensland, while the ACT require unanimous verdicts. Since 1927 South Australia has permitted majority verdicts of 11:1, and 10:1 or 9:1 where the jury has been reduced, in criminal trials if a unanimous verdict cannot be reached in four hours. They are accepted in all cases except for "guilty" verdicts where the defendant is on trial for murder or treason. Victoria has accepted majority verdicts with the same conditions since 1994, though deliberations must go on for six hours before a majority verdict can be made. Western Australia accepted majority verdicts in 1957 for all trials except where the crime is murder or has a life sentence. A 10:2 verdict is accepted. Majority verdicts of 10:2 have been allowed in Tasmania since 1936 for all cases except murder and treason if a unanimous decision has not been made within two hours. Since 1943 verdicts of "not guilty" for murder and treason have also been included, but must be discussed for six hours. The Northern Territory has allowed majority verdicts of 10:2, 10:1 and 9:1 since 1963 and does not discriminate between cases whether the charge is murder or not. Deliberation must go for at least six hours before delivering a majority verdict. The Queensland "Jury Act 1995" (s 59F) allows majority verdicts for all crimes except for murder and other offences that carry a life sentence, although only 11:1 or 10:1 majorities are allowed. Majority verdicts were introduced in New South Wales in 2005.
Austria.
Austria, in common with a number of European civil law jurisdictions, retains elements of trial by jury in serious criminal cases.
Belgium.
Belgium, in common with a number of European civil law jurisdictions, retains the trial by jury through the Court of Assize for serious criminal cases and for political crimes and for press delicts (except those based on racism or xenophobia), and for crimes of international law, such as genocide and crime against humanity.
Canada.
Under Canadian law, a person has the right to a jury trial for all crimes punishable by five years of imprisonment or more.
France.
In France, a defendant is entitled to a jury trial only when prosecuted for a felony ("crime" in French) that is an offence which may bring least 15 years' imprisonment (for natural persons) or a fine of €75,000 (for legal persons). The only court that tries by jury is the "cour d'assises," in which three professional judges sit together with six or nine jurors. Conviction requires a two-thirds majority (six or eight votes).
Greece.
The country that originated the concept of the jury trial retains it in an unusual form. Serious crimes in Greece are tried by a panel of three professional judges and four lay jurors who decide the facts, and the appropriate penalty if they convict.
Gibraltar.
Being a Common Law jurisdiction, Gibraltar retains jury trial in a similar manner to that found in England and Wales, the exception being that juries consist of nine lay people, rather than twelve.
Hong Kong.
Hong Kong, as a former British colony has a common law legal system. Article 86 of Hong Kong's Basic Law, which came into force on 1 July 1997 following the handover of Hong Kong from Britain to China provides: "The principle of trial by jury previously practised in Hong Kong shall be maintained."
Criminal trials in the High Court are by jury. The juries are generally made of seven members, who can return a verdict based on a majority of five.
There are no jury trials in the District Court, which can impose a sentence of up to seven years imprisonment. This is despite the fact that all court rooms in the District Court have jury boxes. The lack of juries in the District Court has been severely criticized. Clive Grossman SC in a commentary in 2009 said conviction rates were "approaching those of North Korea."
Many complex commercial cases are prosecuted in the District Court rather than before a jury in the High Court. In 2009, Lily Chiang, former chairwoman of the Hong Kong General Chamber of Commerce, lost an application to have her case transferred from the District Court to the High Court for a jury trial. Justice Wright in the Court of First Instance held that there was no absolute right to a trial by jury and that the "decision as to whether an indictable offence be tried in the Court of First Instance by a judge and jury or in the District Court by a judge alone is the prerogative of the Secretary for Justice." Ms Chiang issued a statement at the time saying "she was disappointed with the judgment because she has been deprived of a jury trial, an opportunity to be judged by her fellow citizens and the constitutional benefit protected by the Basic Law."
In civil cases in the Court of First Instance jury trials are available for defamation, false imprisonment, malicious prosecution or seduction unless the court orders otherwise. A jury can return a majority verdict in a civil case.
India.
The first case decided by an English jury in India happened in Madras in 1665, for which Ascentia Dawes (probably a British woman) was charged by a grand jury with the murder of her slave girl, and a petty jury, with six Englishmen and six Portuguese, found her not guilty. With the development of the East India Company empire in India, the jury system was implemented inside a dual system of courts: In Presidency Towns (Calcutta, Madras, Bombay), there were Crown Courts and in criminal cases juries had to judge British and European people (as a privilege) and in some cases Indian people; and in the territories outside the Presidency Towns (called "moffussil"), there were Company Courts (composed with Company officials) without jury to judge most of the cases implying indigenous people.
After the Crown Government of India (Raj) adopted the Indian Penal Code (1860) and the Indian Code of Criminal Procedure (1861, amended in 1872, 1882, 1898), the criminal jury was obligatory only in the High Courts of the Presidency Towns; elsewhere, it was optional and rarely used. According sections 274 and 275 of the Code of Criminal Procedure, the jury was composed from 3 (for smaller offences judged in session courts) to 9 (for severe offences judges in High Courts) men; and when the accused were European or American, at least half of the jurors had to be European or American men.
The jury found no place in the 1950 Indian Constitution, and it was ignored in many Indian states. The Law Commission recommended its abolition in 1958 in its 14th Report. Jury trials were abolished in India by a very discrete process during the 1960s, finishing with the 1973 Code of Criminal Procedure, which is still in force today.
It has been argued that the 8:1 acquittal of Kawas Nanavati in "K. M. Nanavati vs. State of Maharashtra", which was overturned by higher courts on the grounds that the jury was misled by the presiding judge and were susceptible to media and public influence, was the reason. A study by Elisabeth Kolsky argues that many "perverse verdicts" were delivered by white juries in trial of "European British subjects" charged with murder, assault, confinement of Indians.
Ireland.
In the Republic of Ireland, a common law jurisdiction, jury trials are available for criminal cases before the Circuit Court, Central Criminal Court and defamation cases, consisting of twelve jurors.
Juries only decide questions of fact; they have no role in criminal sentencing in criminal cases or awarding damages in libel cases. It is not necessary that a jury be unanimous in its verdict. In civil cases, a verdict may be reached by a majority of nine of the twelve members. In a criminal case, a verdict need not be unanimous where there are not fewer than eleven jurors if ten of them agree on a verdict after considering the case for a reasonable time.
Juries are selected from a jury panel, which is picked at random by the county registrar from the electoral register. The principal statute regulating the selection, obligations and conduct of juries is the Juries Act 1976 as amended by the Civil Law (Miscellaneous Provisions) Act 2008, which scrapped the upper age limit of 70. Juries are not paid, nor do they receive travel expenses. They do receive lunch for the days that they are serving; however, for jurors in employment, their employer is required to pay them as if they were present at work.
For certain terrorist and organised crime offences the Director of Public Prosecutions may issue a certificate that the accused be tried by the Special Criminal Court composed of three judges instead of a jury, one from the District Court, Circuit Court and High Court.
Italy.
The Corte d'Assise is composed of 2 judges and 6 laypersons chosen at random among Italian citizens 30 to 65 years old. Only serious crimes like murder can be tried by the Corte d'Assise.
Japan.
On May 28, 2004, the Diet of Japan enacted a law requiring selected citizens to take part in criminal court trials of certain severe crimes to make decisions together with professional judges, both on guilt and on the sentence. These citizens are called "saiban-in" (裁判員 "lay judge"). The "saiban-in" system was implemented in May 2009.
New Zealand.
New Zealand previously required jury verdicts to be passed unanimously, but since the passing of the Criminal Procedure Bill in 2009 the Juries Act 1981 has permitted verdicts to be passed by a majority of one less than the full jury (that is an 11-1 or a 10-1 majority) under certain circumstances.
Norway.
Norway has a system where the lower courts (tingrett) is set with a judge and two lay-judges, or in bigger cases two judges and three lay-judges.
All of these judges convict or acquit, and set sentences. Simple majority is required in all cases, which means that the lay-judges are always in control.
In the higher court/appellate court (lagmannsrett) there is a jury (lagrette) of 10 members, which need a minimum of seven votes to be able to convict. The judges have no say in the jury deliberations, but jury-instructions are given by the chief-judge (lagmann) in each case to the jury before deliberations. The voir-dire is usually set with 16 prospective jurors, which the prosecution and defense may dismiss the 6 persons they do not desire to serve on the jury.
This court (lagmannsretten) is administered by a three-judge panel (usually 1 lagmann and 2 lagdommere), and if 7 or more jury members want to convict, the sentence is set in a separate proceeding, consisting of the three judges and the jury foreman (lagrettens ordfører) and three other members of the jury chosen by ballot. This way the laymen are in control of both the conviction and sentencing, as simple majority is required in sentencing.
The three-judge panel can set aside a jury conviction or acquittal if there has been an obvious miscarriage of justice. In that event, the case is settled by three judges and four lay-judges.
In May 2015, the Norwegian Parliament asked the government to bring an end to jury trials, replacing them with a bench trial (meddomsrett) consisting of two law-trained judges and three lay judges (lekdommere). This has not been fully implemented yet as of February 2016, but is expected soon.
Russia.
In the judiciary of Russia, for serious crimes the accused have the option of a jury trial consisting of 12 jurors. The number of jury trials remains small, at about 600 per year, out of about 1 million trials. A juror must be 25 years old, legally competent, and without a criminal record. The 12 jurors are selected by the prosecution and defense from a list of 30-40 eligible candidates. The Constitution of Russia stipulates that, until the abolition of the death penalty, all defendants in a case that may result in a death sentence are entitled to a jury trial. Lawmakers are continuously chipping away at what types of criminal offenses merit a jury trial.
They are similar to common law juries, and unlike lay judges, in that they sit separately from the judges and decide questions of fact alone while the judge determines questions of law. They must return unanimous verdicts during the first 3 hours of deliberation, but may return majority verdicts after that, with 6 jurors being enough to acquit. They may also request that the judge show leniency in sentencing.
Juries have granted acquittals in 15-20% of cases, compared with less than 1% in cases decided by judges. Juries may be dismissed and skeptical juries have been dismissed on the verge of verdicts, and acquittals are frequently overturned by higher courts.
Trial by jury was first introduced in the Russian Empire as a result of the Judicial reform of Alexander II in 1864, and abolished after the October Revolution in 1917. They were reintroduced in the Russian Federation in 1993, and extended to another 69 regions in 2003. Its reintroduction was opposed by the Prosecutor General.
Singapore.
In Singapore, the jury system was abolished in 1969. Jury trials for all had been earlier abolished in 1959, except for capital offenses with death penalty. As Prime Minister Lee Kuan Yew described to the BBC and in his memoirs, due to his experiences as a trial lawyer, "I had no faith in a system that allowed the superstition, ignorance, biases, and prejudices of seven jurymen to determine guilt or innocence."
South Africa.
The jury system was abolished in South Africa in 1969 by the Abolition of Juries Act, 1969. The last jury trial to be heard was in the District of Kimberley. Some judicial experts had argued that a system of whites-only juries (as was the system at that time) was inherently prejudicial to 'non-white' defendants (the introduction of nonracial juries would have been a political impossibility at that time). More recently it has been argued that, apart from being a racially divided country, South African society was, and still is, characterized by significant class differences and disparities of income and wealth that could make re-introducing the jury system problematic. Arguments for and against the re-introduction of a jury system have been discussed by South African constitutional expert Professor Pierre de Vos in the article "Do we need a jury system?" On 28 March 2014, the Oscar Pistorius trial was adjourned due to the illness of one of the two assessors that assist the Judge on questions of fact (rather than law), in place of the jury, to reach a verdict. The legal system in the UK sees no reason to block extradition on this, as witnessed in the Shrien Dewani case.
Sweden.
In Sweden, juries are uncommon; the public is represented in the courts by means of lay judges (nämndemän). However, the defendant has the right to a jury trial in the lower court (tingsrätt) when accused of an offense against the fundamental laws on freedom of expression and freedom of the press. If a person is accused of e.g. libel or incitement to ethnic or racial hatred, in a medium covered by the fundamental laws (e.g. a printed paper or a radio programme), she has the right to have the accusation tried by a jury of nine jurors. This applies also in civil (tort) cases under the fundamental laws. A majority of at least six jurors must find that the defendant has committed the alleged crime. If it does not, the defendant is acquitted or, in a civil case, held not liable. If such a majority of the jurors hold that said crime has in fact been committed, this finding is not legally binding for the court; thus, the court (three judges) can still acquit the defendant or find him/her not liable. A jury acquittal may not be overruled after appeal. In Swedish civil process, the "English rule" applies to court costs. Earlier, a court disagreeing with a jury acquittal could, when deciding on the matter of such costs, set aside the English rule, and instead use the "American rule", that each party bears its own expense of litigation. This practice was declared to violate the rule of presumption of innocence according to article 6.2. of the European Convention on Human Rights, by the Supreme Court of Sweden, in 2012.
Switzerland.
As of 2008, only the code of criminal procedure of the Canton of Geneva provides for genuine jury trials. Several other cantons – Vaud, Neuchâtel, Zürich and Ticino – provide for courts composed of both professional judges and laymen ("Schöffengerichte" / "tribunaux d'échevins"). Because the unified Swiss Code of Criminal Procedure (set to enter into force in 2011) does not provide for jury trials or lay judges, however, they are likely to be abolished in the near future.
United Kingdom.
The United Kingdom consists of three separate legal jurisdictions, but there are some features common to all of them. In particular there is seldom anything like the US voir dire system; jurors are usually just accepted without question. Controversially, in England there has been some screening in sensitive security cases, but the Scottish courts have firmly set themselves against any form of jury vetting.
England and Wales.
In England and Wales (which have the same legal system), minor criminal cases are heard without a jury in the Magistrates' Courts. Middle-ranking ("triable either way") offences may be tried by magistrates or the defendant may elect trial by jury in the Crown Court. Serious ("indictable") offences, however, must be tried before a jury in the Crown Court. Juries sit in a few civil cases, in particular, defamation and cases involving the state. Juries also sit in coroner's courts for more contentious inquests. All criminal juries consist of 12 jurors, those in a County Court having 8 jurors and Coroner's Court juries having between 7 and 11 members. Jurors must be between 18–70 years of age, and are selected at random from the register of voters. In the past a unanimous verdict was required. This has been changed so that, if the jury fails to agree after a given period, at the discretion of the judge they may reach a verdict by a 10-2 majority. This was designed to make it more difficult for jury tampering to succeed.
In 1999 the then Home Secretary Jack Straw introduced a controversial bill to limit the right to trial by jury. This became the Criminal Justice Act 2003, which sought to remove the right to trial by jury for cases involving jury tampering or complex fraud. The provision for trial without jury to circumvent jury tampering succeeded and came into force in 2007, the provision for complex fraud cases was defeated. Lord Goldsmith, the then Attorney General, then pressed forward with the Fraud (Trials Without a Jury) Bill in Parliament, which sought to abolish jury trials in major criminal fraud trials. The Bill was subject to sharp criticism from both sides of the House of Commons before passing its second Commons reading in November 2006, but was defeated in the Lords in March 2007.
The trial for the first serious offence to be tried without a jury for 350 years was allowed to go ahead in 2009. Three previous trials of the defendants had been halted because of jury tampering, and the Lord Chief Justice, Lord Judge, cited cost and the additional burden on the jurors as reasons to proceed without a jury. Previously in cases where jury tampering was a concern the jurors were sometimes closeted in a hotel for the duration of the trial. However, Liberty director of policy Isabella Sankey said that "This is a dangerous precedent. The right to jury trial isn't just a hallowed principle but a practice that ensures that one class of people don't sit in judgement over another and the public have confidence in an open and representative justice system.
The trial started in 2010, with the four defendants convicted on the 31st of March 2010 by Mr Justice Treacy at the Old Bailey.
Scotland.
In Scots law the jury system has some similarities with England but some important differences, in particular there are juries of 15 in criminal trials, with verdicts by simple majority.
Northern Ireland.
In Northern Ireland, the role of the jury trial is roughly similar to England and Wales, except that jury trials have been replaced in cases of alleged terrorist offences by courts where the judge sits alone, known as "Diplock courts". Diplock courts are common in Northern Ireland for crimes connected to terrorism.
Diplock courts were created in the 1970s during The Troubles, to phase out Operation Demetrius internments, and because of the argument that juries were intimidated, though this is disputed. The Diplock courts were shut in 2007, but between 1 August 2008 and 31 July 2009, 13 non-jury trials were held, down from 29 in the previous year, and 300 trials per year at their peak.
United States.
In the United States, every person accused of a crime punishable by incarceration for more than six months has a constitutional right to a trial by jury, which arises in federal court from Article Three of the United States Constitution, which states in part, "The Trial of all Crimes...shall be by Jury; and such Trial shall be held in the State where the said Crimes shall have been committed." The right was expanded with the Sixth Amendment to the United States Constitution, which states in part, "In all criminal prosecutions, the accused shall enjoy the right to a speedy and public trial, by an impartial jury of the state and district wherein the crime shall have been committed." Both provisions were made applicable to the states through the Fourteenth Amendment. Most states' constitutions also grant the right of trial by jury in lesser criminal matters, though most have abrogated that right in offenses punishable by fine only. The Supreme Court has ruled that if imprisonment is for six months or less, trial by jury is not required, meaning a state may choose whether or not to permit trial by jury in such cases. Under the Federal Rules of Criminal Procedure, if the defendant is entitled to a jury trial, he may waive his right to have a jury, but both the government (prosecution) and court must consent to the waiver. Several states require jury trials for all crimes, "petty" or not.
In the cases "Apprendi v. New Jersey", , and "Blakely v. Washington", , the Supreme Court of the United States held that a criminal defendant has a right to a jury trial not only on the question of guilt or innocence, but any fact used to increase the defendant's sentence beyond the maximum otherwise allowed by statutes or sentencing guidelines. This invalidated the procedure in many states and the federal courts that allowed sentencing enhancement based on "a preponderance of evidence," where enhancement could be based on the judge's findings alone. Depending upon the state, a jury must be unanimous for either a guilty or not guilty decision. A hung jury results in the defendants release, however charges against the defendant are not dropped and can be reinstated if the state so chooses.
Jurors in some states are selected through voter registration and drivers' license lists. A form is sent to prospective jurors to pre-qualify them by asking the recipient to answer questions about citizenship, disabilities, ability to understand the English language, and whether they have any conditions that would excuse them from being a juror. If they are deemed qualified, a summons is issued.
English common law and the United States Constitution recognize the right to a jury trial to be a fundamental civil liberty or civil right that allows the accused to choose whether to be judged by judges or a jury.
In the United States, it is understood that juries usually weigh the evidence and testimony to determine questions of fact, while judges usually rule on questions of law, although the dissenting justices in the Supreme Court case "Sparf et al. v. U.S. 156 U.S. 51 (1895)", generally considered the pivotal case concerning the rights and powers of the jury, declared: "It is our deep and settled conviction, confirmed by a re-examination of the authorities that the jury, upon the general issue of guilty or not guilty in a criminal case, have the right, as well as the power, to decide, according to their own judgment and consciences, all questions, whether of law or of fact, involved in that issue." Jury determination of questions of law, sometimes called jury nullification, cannot be overturned by a judge if doing so would violate legal protections against double jeopardy. Although a judge can throw out a guilty verdict if it was not supported by the evidence, a jurist has no authority to override a verdict that favors a defendant.
It was established in Bushel's Case that a judge cannot order the jury to convict, no matter how strong the evidence is. In civil cases a special verdict can be given, but in criminal cases a general verdict is rendered, because requiring a special verdict could apply pressure to the jury, and because of the jury's historic function of tempering rules of law by common sense brought to bear upon the facts of a specific case. For this reason, Justice Black and Justice Douglas indicated their disapproval of special interrogatories even in civil cases.
There has been much debate about the advantages and disadvantages of the jury system, the competence or lack thereof of jurors as fact-finders, and the uniformity or capriciousness of the justice they administer. The jury has been described as "an exciting and gallant experiment in the conduct of serious human affairs." As fact-finders, juries are expected to fulfill the role of lie detector.
A civil jury is typically made up of 6 to 12 persons. In a civil case, the role of the jury is to listen to the evidence presented at a trial, to decide whether the defendant injured the plaintiff or otherwise failed to fulfill a legal duty to the plaintiff, and to determine what the compensation or penalty should be.
A criminal jury is usually made up of 12 members, though fewer may sit on cases involving lesser offenses. Criminal juries decide whether the defendant committed the crime as charged. The sentence may be set by either the jury or the judge; generally, in felony cases the jury sets punishment while in lesser offenses it may be set by the judge.
Verdicts in criminal cases must be unanimous, with the following exceptions: Currently, two states, Oregon and Louisiana, do not require unanimous verdicts in criminal cases. Each requires a 10–2 majority for conviction, except for capital crimes: Oregon requires at least 11 votes and Louisiana requires all 12.
In civil cases, the law (or the agreement of the parties) may permit a non-unanimous verdict.
A jury's deliberations are conducted in private, out of sight and hearing of the judge, litigants, witnesses, and others in the courtroom.
Not every case is eligible for a jury trial. In the majority of US states, there is no right to a jury trial in family law actions not involving a termination of parental rights, such as divorce and custody modifications. Only eleven states allow juries in any aspect of divorce litigation (Colorado, Georgia, Illinois, Louisiana, Maine, Nevada, New York, North Carolina, Tennessee, Texas and Wisconsin). Most of these limit the right to a jury to try issues regarding grounds or entitlement for divorce only. Texas provides jury trial rights most broadly, including even the right to a jury trial on questions regarding child custody. However, anyone who is charged with a criminal offense, breach of contract or federal offence has a Constitutional right to a trial by jury.
Civil trial procedure.
"In the United States, a civil action is a lawsuit; civil law is the branch of common law dealing with non-criminal actions. It should not be confused with legal system of civil law."
The right to trial by jury in a civil case in federal court is addressed by the Seventh Amendment. Importantly, however, the Seventh Amendment does not guarantee a right to a civil jury trial in state courts (although most state constitutions guarantee such a right). The Seventh Amendment provides: "In Suits at common law, where the value in controversy shall exceed twenty dollars, the right of trial by jury shall be preserved, and no fact tried by a jury shall be otherwise re-examined in any Court of the United States, than according to the rules of the common law." In Joseph Story's 1833 treatise "Commentaries on the Constitution of the United States", he wrote, "t is a most important and valuable amendment; and places upon the high ground of constitutional right the inestimable privilege of a trial by jury in civil cases, a privilege scarcely inferior to that in criminal cases, which is conceded by all to be essential to political and civil liberty."
The Seventh Amendment does not guarantee or create any right to a jury trial; rather, it preserves the right to jury trial in the federal courts that existed in 1791 at common law. In this context, common law means the legal environment the United States inherited from England. In England in 1791, civil actions were divided into actions at law and actions in equity. Actions at law had a right to a jury, actions in equity did not. Federal Rules of Civil Procedure Rule 2 says "here is one form of action—the civil action," which abolishes the legal/equity distinction. Today, in actions that would have been "at law" in 1791, there is a right to a jury; in actions that would have been "in equity" in 1791, there is no right to a jury. However, Federal Rule of Civil Procedure 39(c) allows a court to use one at its discretion. To determine whether the action would have been legal or equitable in 1791, one must first look at the type of action and whether such an action was considered "legal" or "equitable" at that time. Next, the relief being sought must be examined. Monetary damages alone were purely a legal remedy, and thus entitled to a jury. Non-monetary remedies such as injunctions, rescission, and specific performance were all equitable remedies, and thus up to the judge's discretion, not a jury. In "Beacon Theaters v. Westover", , the US Supreme Court discussed the right to a jury, holding that when both equitable and legal claims are brought, the right to a jury trial still exists for the legal claim, which would be decided by a jury before the judge ruled on the equitable claim.
There is not a United States constitutional right under the Seventh Amendment to a jury trial in state courts, but in practice, almost every state except Louisiana, which has a civil law legal tradition, permits jury trials in civil cases in state courts on substantially the same basis that they are allowed under the Seventh Amendment in federal court. The right to a jury trial in civil cases does not extend to the states, except when a state court is enforcing a federally created right, of which the right to trial by jury is a substantial part.
The court determines the right to jury based on all claims by all parties involved. If the plaintiff brings only equitable claims but the defendant asserts counterclaims of law, the court grants a jury trial. In accordance with Beacon Theaters, the jury first determines the facts, then the judge enter judgment on the equitable claims.
Following the English tradition, US juries have usually been composed of 12 jurors, and the jury's verdict has usually been required to be unanimous. However, in many jurisdictions, the number of jurors is often reduced to a lesser number (such as five or six) by legislative enactment, or by agreement of both sides. Some jurisdictions also permit a verdict to be returned despite the dissent of one, two, or three jurors.
Waiver of jury trial.
The vast majority of US criminal cases are not concluded with a jury verdict, but rather by plea bargain. Both prosecutors and defendants often have a strong interest in resolving the criminal case by negotiation resulting in a plea bargain. If the defendant waives a jury trial, a bench trial is held.
For civil cases, a jury trial must be demanded within a certain period of time per Federal Rules of Civil Procedure 38.
In United States Federal courts, there is no absolute right to waive a jury trial. Per Federal Rule of Criminal Procedure 23(a), only if the prosecution and the court consent may a defendant waive a jury trial for criminal cases. However, most states give the defendant the absolute right to waive a jury trial, and it has become commonplace to find such a waiver in routine contracts as a 2004 "Wall Street Journal" Article states:
"'For years, in an effort to avoid the slow-moving wheels of the U.S. judicial system, many American companies have forced their customers and employees to agree to settle disputes outside of the courts, through private arbitration... but the rising cost of arbitration proceedings has led some companies to decide they might be better off in the court system after all long as they don't have to tangle with juries. The new tactic to let disputes go to court, but on the condition that they be heard only by a judge.'" The article goes on to claim "'The list includes residential leases, checking-account agreements, auto loans and mortgage contracts. Companies that believe juries are biased toward plaintiffs hope this approach will boost their chances of winning in court. Critics say that unfairly denies citizens' access to the full range of legal options guaranteed by the Constitution'." 
Since this article was written (some twelve years ago at the time of writing) it is unsurprising this practice is now all-pervasive and, since it has been extended on-line, it has become commonplace to include such waivers to trail by jury in everything from user agreements attached to software downloads to merely browsing a website. This gives rise to the intriguing notion that while such waivers may not only be all-pervasive and have legal force in one jurisdiction - in this case the United States - in the jurisdiction where a verdict is sought in the absence of jury trial (or indeed the presence of a defendant, or any legal representation "in absentia") may well run directly counter to law in the jurisdiction - such as the United Kingdom - where the defendant resides, thus: 
The Judgment on "Regina v Jones issued by the United Kingdom's Court of Appeal's (Criminal Division)" states, (in part, in Item 55) "'... the issue has to be determined by looking at the way in which the courts handled the problem under English criminal procedure and by deciding whether, in the result, the appellant can be said to have had a fair hearing.'"

</doc>
<doc id="16369" url="https://en.wikipedia.org/wiki?curid=16369" title="Justice">
Justice

Justice is the legal or philosophical theory by which fairness is administered. The concept of justice differs in every culture. An early theory of justice was set out by the Ancient Greek philosopher Plato in his work "The Republic". Advocates of divine command theory argue that justice issues from God. In the 17th century, theorists like John Locke argued for the theory of natural law. Thinkers in the social contract tradition argued that justice is derived from the mutual agreement of everyone concerned. In the 19th century, utilitarian thinkers including John Stuart Mill argued that justice is what has the best consequences. Theories of distributive justice concern what is distributed, between whom they are to be distributed, and what is the "proper" distribution. Egalitarians argued that justice can only exist within the coordinates of equality. John Rawls used a social contract argument to show that justice, and especially distributive justice, is a form of fairness. Property rights theorists (like Robert Nozick) take a deontological view of distributive justice and argue that property rights-based justice maximizes the overall wealth of an economic system. Theories of retributive justice are concerned with punishment for wrongdoing. Restorative justice (also sometimes called "reparative justice") is an approach to justice that focuses on the needs of victims and offenders.
Introductory understandings.
Understandings of justice differ in every culture, as cultures are usually dependent upon a shared history, mythology and/or religion. Each culture's ethics create values which influence the notion of justice. Although there can be found some justice principles that are one and the same in all or most of the cultures, these are insufficient to create a unitary justice apprehension.
Harmony.
In his dialogue "Republic", Plato uses Socrates to argue for justice that covers both the just person and the just City State. Justice is a proper, harmonious relationship between the warring parts of the person or city. Hence, Plato's definition of justice is that justice is the having and doing of what is one's own. A just man is a man in just the right place, doing his best and giving the precise equivalent of what he has received. This applies both at the individual level and at the universal level. A person's soul has three parts – reason, spirit and desire. Similarly, a city has three parts – Socrates uses the parable of the chariot to illustrate his point: a chariot works as a whole because the two horses' power is directed by the charioteer. Lovers of wisdom – philosophers, in one sense of the term – should rule because only they understand what is good. If one is ill, one goes to a medic rather than a farmer, because the medic is expert in the subject of health. Similarly, one should trust one's city to an expert in the subject of the good, not to a mere politician who tries to gain power by giving people what they want, rather than what's good for them. Socrates uses the parable of the ship to illustrate this point: the unjust city is like a ship in open ocean, crewed by a powerful but drunken captain (the common people), a group of untrustworthy advisors who try to manipulate the captain into giving them power over the ship's course (the politicians), and a navigator (the philosopher) who is the only one who knows how to get the ship to port. For Socrates, the only way the ship will reach its destination – the good – is if the navigator takes charge.
Divine command.
Advocates of divine command theory argue that justice, and indeed the whole of morality, is the authoritative command of God. Murder is wrong and must be punished, for instance, because, and only because, God commands that it be so.
Divine command theory was famously questioned by Plato in his dialogue, Euthyphro. Called the Euthyphro dilemma, it goes as follows: "Is what is morally good commanded by God because it is morally good, or is it morally good because it is commanded by God?" The implication is that if the latter is true, then justice is arbitrary; if the former is true, then morality exists on a higher order than God, who becomes little more than a passer-on of moral knowledge.
Many apologists have addressed the issue, typically by arguing that is it a false dilemma. For example, some Christian apologists argue that goodness is the very nature of God, and there is necessarily reflected in His commands. Another response, popularized in two contexts by Immanuel Kant and C. S. Lewis, is that it is deductively valid to argue that the existence of an objective morality implies the existence of God and vice versa.
Natural law.
For advocates of the theory that justice is part of natural law (e.g., John Locke), it involves the system of consequences that naturally derives from any action or choice. In this, it is similar to the laws of physics: in the same way as the Third of Newton's laws of Motion requires that for every action there must be an equal and opposite reaction, justice requires according individuals or groups what they actually deserve, merit, or are entitled to. Justice, on this account, is a universal and absolute concept: laws, principles, religions, etc., are merely attempts to codify that concept, sometimes with results that entirely contradict the true nature of justice.
Human creation.
In contrast to the understandings canvassed so far, justice may be understood as a human "creation", rather than a "discovery" of harmony, divine command, or natural law. This claim can be understood in a number of ways, with the fundamental division being between those who argue that justice is the creation of "some" humans, and those who argue that it is the creation of "all" humans.
Despotism and skepticism.
In "Republic" by Plato, the character Thrasymachus argues that justice is the interest of the strong—merely a name for what the powerful or cunning ruler has imposed on the people.
Mutual agreement.
According to thinkers in the social contract tradition, justice is derived from the mutual agreement of everyone concerned; or, in many versions, from what they would agree to under "hypothetical" conditions including equality and absence of bias. This account is considered further below, under 'Justice as fairness'. The absence of bias refers to an equal ground for all people concerned in a disagreement (or trial in some cases).
Subordinate value.
According to utilitarian thinkers including John Stuart Mill, justice is not as fundamental as we often think. Rather, it is derived from the more basic standard of rightness, consequentialism: what is right is what has the best consequences (usually measured by the total or average welfare caused). So, the proper principles of justice are those that tend to have the best consequences. These rules may turn out to be familiar ones such as keeping contracts; but equally, they may not, depending on the facts about real consequences. Either way, what is important is those consequences, and justice is important, if at all, only as derived from that fundamental standard. Mill tries to explain our mistaken belief that justice is overwhelmingly important by arguing that it derives from two natural human tendencies: our desire to retaliate against those who hurt us, and our ability to put ourselves imaginatively in another's place. So, when we see someone harmed, we project ourselves into her situation and feel a desire to retaliate on her behalf. If this process is the source of our feelings about justice, that ought to undermine our confidence in them.
Theories of distributive justice.
Theories of distributive justice need to answer three questions:
Distributive justice theorists generally do not answer questions of "who has the right" to enforce a particular favored distribution. On the other hand, property rights theorists argue that there is no "favored distribution." Rather, distribution should be based simply on whatever distribution results from lawful interactions or transactions (that is, transactions which are not illicit).
This section describes some widely held theories of distributive justice, and their attempts to answer these questions.
Social justice.
According to the egalitarian, justice can exist only within the parameters of equality. This basic view can be elaborated in many ways, according to what goods are to be distributed—wealth, respect, opportunity—and who or what they are to be distributed equally among—individuals, families, nations, races, species. Egalitarian theories are typically less concerned with discussing who exactly will do the distributing or what effects their recommended policies will have on the production of the goods, services, or resources they wish to distribute.
Commonly held egalitarian positions include demands for equality of opportunity, though equality of opportunity is often defended by adherents of nonegalitarian conceptions of justice as well. Some variants of egalitarianism affirm that justice without equality is hollow and that equality itself is the highest justice, though such a formulation will have concrete meaning only once the main terms have been fleshed out.
At a cultural level, egalitarian theories have developed in sophistication and acceptance during the past two hundred years. Among the notable broadly egalitarian philosophies are socialism, left-libertarianism, and progressivism, which propound economic, political, and legal egalitarianism, respectively.
Fairness.
In his "A Theory of Justice", John Rawls used a social contract argument to show that justice, and especially distributive justice, is a form of fairness: an "impartial" distribution of goods. Rawls asks us to imagine ourselves behind a "veil of ignorance" that denies us all knowledge of our personalities, social statuses, moral characters, wealth, talents and life plans, and then asks what theory of justice we would choose to govern our society when the veil is lifted, if we wanted to do the best that we could for ourselves. We don't know who in particular we are, and therefore can't bias the decision in our own favour. So, the decision-in-ignorance models fairness, because it excludes selfish bias. Rawls argues that each of us would reject the utilitarian theory of justice that we should maximize welfare (see below) because of the risk that we might turn out to be someone whose own good is sacrificed for greater benefits for others. Instead, we would endorse Rawls's "two principles of justice":
This imagined choice justifies these principles as the principles of justice for us, because we would agree to them in a fair decision procedure. Rawls's theory distinguishes two kinds of goods – (1) the good of liberty rights and (2) social and economic goods, i.e. wealth, income and power – and applies different distributions to them – equality between citizens for (1), equality unless inequality improves the position of the worst off for (2).
In one sense, theories of distributive justice may assert that everyone should get what they deserve. Theories disagree on the meaning of what is "deserved". The main distinction is between theories that argue the basis of just deserts ought to be held equally by everyone, and therefore derive egalitarian accounts of distributive justice—and theories that argue the basis of just deserts is unequally distributed on the basis of, for instance, hard work, and therefore derive accounts of distributive justice by which some should have more than others.
According to "meritocratic" theories, goods, especially wealth and social status, should be distributed to match individual "merit", which is usually understood as some combination of talent and hard work. According to "needs"-based theories, goods, especially such basic goods as food, shelter and medical care, should be distributed to meet individuals' basic needs for them. Marxism can be regarded as a needs-based theory on some readings of Marx's slogan "from each according to his ability, to each according to his need". According to "contribution"-based theories, goods should be distributed to match an individual's contribution to the overall social good.
Property rights.
In "Anarchy, State, and Utopia", Robert Nozick argues that distributive justice is not a matter of the whole distribution matching an ideal "pattern", but of each individual entitlement having the right kind of "history". It is just that a person has some good (especially, some property right) if and only if they came to have it by a history made up entirely of events of two kinds:
If the chain of events leading up to the person having something meets this criterion, they are entitled to it: that they possess it is just, and what anyone else does or doesn't have or need is irrelevant.
On the basis of this theory of distributive justice, Nozick argues that all attempts to redistribute goods according to an ideal pattern, without the consent of their owners, are theft. In particular, redistributive taxation is theft.
Some property rights theorists (like Nozick) also take a consequentialist view of distributive justice and argue that property rights based justice also has the effect of maximizing the overall wealth of an economic system. They explain that voluntary (non-coerced) transactions always have a property called Pareto efficiency. The result is that the world is better off in an absolute sense and no one is worse off. Such consequentialist property rights theorists argue that respecting property rights maximizes the number of Pareto efficient transactions in the world and minimized the number of non-Pareto efficient transactions in the world (i.e. transactions where someone is made worse off). The result is that the world will have generated the greatest total benefit from the limited, scarce resources available in the world. Further, this will have been accomplished without taking anything away from anyone unlawfully.
Welfare-maximization.
According to the utilitarian, justice requires the maximization of the total or average welfare across all relevant individuals. This may require sacrifice of some for the good of others, so long as everyone's good is taken impartially into account. Utilitarianism, in general, argues that the standard of justification for actions, institutions, or the whole world, is "impartial welfare consequentialism", and only indirectly, if at all, to do with rights, property, need, or any other non-utilitarian criterion. These other criteria might be indirectly important, to the extent that human welfare involves them. But even then, such demands as human rights would only be elements in the calculation of overall welfare, not uncrossable barriers to action.
Theories of retributive justice.
Theories of retributive justice are concerned with punishment for wrongdoing, and need to answer three questions:
This section considers the two major accounts of retributive justice, and their answers to these questions. "Utilitarian" theories look forward to the future consequences of punishment, while "retributive" theories look back to particular acts of wrongdoing, and attempt to balance them with deserved punishment.
Utilitarianism.
According to the utilitarian, as already noted, justice requires the maximization of the total or average welfare across all relevant individuals. Punishment fights crime in three ways:
So, the reason for punishment is the maximization of welfare, and punishment should be of whomever, and of whatever form and severity, are needed to meet that goal. Worryingly, this may sometimes justify punishing the innocent, or inflicting disproportionately severe punishments, when that will have the best consequences overall (perhaps executing a few suspected shoplifters live on television would be an effective deterrent to shoplifting, for instance). It also suggests that punishment might turn out "never" to be right, depending on the facts about what actual consequences it has.
Retributivism.
The retributivist will think the utilitarian's argument disastrously mistaken. If someone does something wrong, we must respond to it, and to him or her, as an individual not as a part of a calculation of overall welfare. To do otherwise is to disrespect him or her as an individual human being. If the crime had victims, it is to disrespect them, too. Wrongdoing must be balanced or made good in some way, and so the criminal "deserves" to be punished. It says that all guilty people, and only guilty people, deserve appropriate punishment. This matches some strong intuitions about just punishment: that it should be "proportional" to the crime, and that it should be of "only" and "all of" the guilty. However, it is sometimes argued that retributivism is merely revenge in disguise. However, there are differences between retribution and revenge: the former is impartial and has a scale of appropriateness, whereas the latter is personal and potentially unlimited in scale.
Restorative justice.
Restorative justice (also sometimes called "reparative justice") is an approach to justice that focuses on the needs of victims and offenders, instead of satisfying abstract legal principles or punishing the offender. Victims take an active role in the process, while offenders are encouraged to take responsibility for their actions, "to repair the harm they've done—by apologizing, returning stolen money, or community service". It is based on a theory of justice that considers crime and wrongdoing to be an offense against an individual or community rather than the state. Restorative justice that fosters dialogue between victim and offender shows the highest rates of victim satisfaction and offender accountability.
Mixed theories.
Some modern philosophers have argued that Utilitarian and Retributive theories are not mutually exclusive. For example, Andrew von Hirsch, in his 1976 book "Doing Justice", suggested that we have a moral obligation to punish greater crimes more than lesser ones. However, so long as we adhere to that constraint then utilitarian ideals would play a significant secondary role.
Theories.
Rawls's theory of justice.
It has been argued that 'systematic' or 'programmatic' political and moral philosophy in the West begins, in Plato's Republic, with the question, 'What is Justice?' According to most contemporary theories of justice, justice is overwhelmingly important: John Rawls claims that "Justice is the first virtue of social institutions, as truth is of systems of thought." In classical approaches, evident from Plato through to Rawls, the concept of 'justice' is always construed in logical or 'etymological' opposition to the concept of injustice. Such approaches cite various examples of injustice, as problems which a theory of justice must overcome. A number of post-World War II approaches do, however, challenge that seemingly obvious dualism between those two concepts. Justice can be thought of as distinct from benevolence, charity, prudence, mercy, generosity, or compassion, although these dimensions are regularly understood to also be interlinked. Justice is the concept of cardinal virtues, of which it is one. Metaphysical justice has often been associated with concepts of fate, reincarnation or Divine Providence, i.e., with a life in accordance with a cosmic plan. The association of justice with fairness is thus historically and culturally inalienable.
Equality before the law.
Law raises important and complex issues concerning equality, fairness, and justice. There is an old saying that 'All are equal before the law'. The author Anatole France said in 1894, "In its majestic equality, the law forbids rich and poor alike to sleep under bridges, beg in the streets, and steal loaves of bread." The belief in equality before the law is called legal egalitarianism.
Classical liberalism.
Equality before the law is one of the basic principles of classical liberalism. Classical liberalism calls for equality before the law, not for equality of outcome. Classical liberalism opposes pursuing group rights at the expense of individual rights.
Religion and spirituality.
Abrahamitic justice.
Jews, Muslims and Christians traditionally believe that justice is a present, real, right, and, specifically, governing concept along with mercy, and that justice is ultimately derived from and held by God. According to the Bible, such institutions as the Mosaic Law were created by God to require the Israelites to live by and apply His standards of justice.
The Hebrew Bible describes God as saying about the Judeo-Christian patriarch Abraham: "No, for I have chosen him, that he may charge his children and his household after him to keep the way of the Lord by doing righteousness and justice;..." (Genesis 18:19, NRSV). The Psalmist describes God as having "Righteousness and justice the foundation of [His throne;..." (Psalms 89:14, NRSV).
The New Testament also describes God and Jesus Christ as having and displaying justice, often in comparison with God displaying and supporting mercy (Matthew 5:7).
Theories of sentencing.
In criminal law, a sentence forms the final explicit act of a judge-ruled process, and also the symbolic principal act connected to his function. The sentence can generally involve a decree of imprisonment, a fine and/or other punishments against a defendant convicted of a crime. Laws may specify the range of penalties that can be imposed for various offenses, and sentencing guidelines sometimes regulate what punishment within those ranges can be imposed given a certain set of offense and offender characteristics. The most common purposes of sentencing in legal theory are:
In civil cases the decision is usually known as a verdict, or judgment, rather than a sentence. Civil cases are settled primarily by means of monetary compensation for harm done ("damages") and orders intended to prevent future harm (for example injunctions). Under some legal systems an award of damages involves some scope for retribution, denunciation and deterrence, by means of additional categories of damages beyond simple compensation, covering a punitive effect, social disapprobation, and potentially, deterrence, and occasionally disgorgement (forfeit of any gain, even if no loss was caused to the other party).
Evolutionary perspectives.
Evolutionary ethics and an argued evolution of morality suggest evolutionary bases for the concept of justice. Biosocial criminology research argues that human perceptions of what is appropriate criminal justice are based on how to respond to crimes in the ancestral small-group environment and that these responses may not always be appropriate for today's societies.
Reactions to fairness.
Studies at UCLA in 2008 have indicated that reactions to fairness are "wired" into the brain and that, "Fairness is activating the same part of the brain that responds to food in rats... This is consistent with the notion that being treated fairly satisfies a basic need". Research conducted in 2003 at Emory University involving capuchin monkeys demonstrated that other cooperative animals also possess such a sense and that "inequity aversion may not be uniquely human" indicating that ideas of fairness and justice may be instinctual in nature.
Institutions and justice.
In a world where people are interconnected but they disagree, institutions are required to instantiate ideals of justice. These institutions may be justified by their approximate instantiation of justice, or they may be deeply unjust when compared with ideal standards — consider the institution of slavery. Justice is an ideal the world fails to live up to, sometimes due to deliberate opposition to justice despite understanding, which could be disastrous. The question of institutive justice raises issues of legitimacy, procedure, codification and interpretation, which are considered by legal theorists and by philosophers of law.

</doc>
<doc id="16371" url="https://en.wikipedia.org/wiki?curid=16371" title="Jacob Abbott">
Jacob Abbott

Jacob Abbott (November 14, 1803 – October 31, 1879) was an American writer of children's books.
Biography.
Abbott was born at Hallowell, Maine to Jacob and Betsey Abbott. He attended the Hallowell Academy, then he graduated from Bowdoin College in 1820; studied at Andover Theological Seminary in 1821, 1822, and 1824; was tutor in 1824–1825, and from 1825 to 1829 was professor of mathematics and natural philosophy at Amherst College; was licensed to preach by the Hampshire Association in 1826; founded the Mount Vernon School for Young Ladies in Boston in 1829, and was principal of it in 1829–1833; was pastor of Eliot Congregational Church (which he founded), at Roxbury, Massachusetts in 1834–1835; and was, with his brothers, a founder, and in 1843–1851 a principal of Abbott's Institute, and in 1845–1848 of the Mount Vernon School for Boys, in New York City.
He was a prolific author, writing juvenile fiction, brief histories, biographies, religious books for the general reader, and a few works in popular science. He wrote 180 books and was a coauthor or editor of 31 more. He died in Farmington, Maine, where he had spent part of his time after 1839, and where his brother, Samuel Phillips Abbott, founded the Abbott School.
His "Rollo Books", such as "Rollo at Work, Rollo at Play, Rollo in Europe", etc., are the best known of his writings, having as their chief characters a representative boy and his associates. In them Abbott did for one or two generations of young American readers a service not unlike that performed earlier, in England and America, by the authors of "Evenings at Home", "The History of Sandford and Merton", and the "The Parent's Assistant". To follow up his Rollo books, he wrote of "Uncle George", using him to teach the young readers about ethics, geography, history, and science. He also wrote 22 volumes of biographical histories and a 10 volume set titled the "Franconia Stories".
His brothers, John Stevens Cabot Abbott and Gorham Dummer Abbott, were also authors. His sons, Benjamin Vaughan Abbott, Austin Abbott, both eminent lawyers, Lyman Abbott, and Edward Abbott, a clergyman, were also well-known authors.
See his "Young Christian, Memorial Edition, with a Sketch of the Author" by Edward Abbott with a bibliography of his works.
Other works of note: "Lucy Books", "Jonas Books", "Harper's Story Books", "Marco Paul", "Gay Family", and "Juno Books".

</doc>
<doc id="16372" url="https://en.wikipedia.org/wiki?curid=16372" title="John Stevens Cabot Abbott">
John Stevens Cabot Abbott

John Stevens Cabot Abbott (September 19, 1805 – June 17, 1877), an American historian, pastor, and pedagogical writer, was born in Brunswick, Maine to Jacob and Betsey Abbott.
Early life.
He was a brother of Jacob Abbott, and was associated with him in the management of Abbott's Institute, New York City, and in the preparation of his series of brief historical biographies. Dr. Abbott graduated at Bowdoin College in 1825, prepared for the ministry at Andover Theological Seminary, and between 1830 and 1844, when he retired from the ministry in the Congregational Church, preached successively at Worcester, Roxbury and Nantucket, all in Massachusetts.
Literary career.
Owing to the success of a little work, "The Mother at Home", he devoted himself, from 1844 onwards, to literature. He was a voluminous writer of books on Christian ethics, and of popular histories, which were credited with cultivating a popular interest in history. He is best known as the author of the widely popular "History of Napoleon Bonaparte" (1855), in which the various elements and episodes in Napoleon's career are described. Abbott takes a very favourable view towards his subject throughout. Also among his principal works are: "History of the Civil War in America" (1863–1866), and "The History of Frederick II, Called Frederick the Great" (New York, 1871). He also did a forward to a book called Life of Boone by W.M. Bogart, about Daniel Boone in 1876.
In general, except that he did not write juvenile fiction, his work in subject and style closely resembles that of his brother, Jacob Abbott.
Marriage and children.
On August 17, 1835 he married Jane Williams Bourne, daughter of Abner Bourne and Abagail Williams. John and Jane had issue: 
As a part of the 1872 Iwakura Mission Mr. Abbott was given guardianship of Shige Nagai, a Japanese girl sent to the United States to be educated.
John Stevens Cabot Abbott died at Fair Haven, Connecticut. In 1910, a series of twenty short biographies of historical characters by J. S. C. and Jacob Abbott, was published. Their brother, Gorham Dummer Abbott, was also an author. Abbott's grandson, Willis Abbott, was a Christian Scientist and an editor of the "Christian Science Monitor".
Selected bibliography.
Biographies.
Published after 1850 in the series "Illustrated History", with other titles by his brother Jacob Abbott. Later reissued in the "Famous Characters of History" series, and in the 1904 series "Makers of History":

</doc>
<doc id="16373" url="https://en.wikipedia.org/wiki?curid=16373" title="Janus (disambiguation)">
Janus (disambiguation)

Janus is the two-faced Roman god of gates, doors, doorways, beginnings, and endings.
Janus may also refer to:

</doc>
<doc id="16374" url="https://en.wikipedia.org/wiki?curid=16374" title="John Brown">
John Brown

John Brown may refer to:
John Brown or Johnny Brown may also refer to:

</doc>
<doc id="16376" url="https://en.wikipedia.org/wiki?curid=16376" title="J. E. B. Stuart">
J. E. B. Stuart

James Ewell Brown "Jeb" Stuart (February 6, 1833May 12, 1864) was a United States Army officer from the U.S. state of Virginia, who later became a Confederate States Army general during the American Civil War. He was known to his friends as "Jeb", from the initials of his given names. Stuart was a cavalry commander known for his mastery of reconnaissance and the use of cavalry in support of offensive operations. While he cultivated a cavalier image (red-lined gray cape, yellow sash, hat cocked to the side with an ostrich plume, red flower in his lapel, often sporting cologne), his serious work made him the trusted eyes and ears of Robert E. Lee's army and inspired Southern morale.
Stuart graduated from West Point in 1854, and served in Texas and Kansas with the U.S. Army. He was a veteran of the frontier conflicts with Native Americans and the violence of Bleeding Kansas, and he participated in the capture of John Brown at Harpers Ferry. 
He resigned when his home state of Virginia seceded to serve in the Confederate Army, first under Stonewall Jackson in the Shenandoah Valley, but then in increasingly important cavalry commands of the Army of Northern Virginia, playing a role in all of that army's campaigns until his death. He established a reputation as an audacious cavalry commander and on two occasions (during the Peninsula Campaign and the Maryland Campaign) circumnavigated the Union Army of the Potomac, bringing fame to himself and embarrassment to the North. At the Battle of Chancellorsville, he distinguished himself as a temporary commander of the wounded Stonewall Jackson's infantry corps.
Arguably Stuart's most famous campaign, Gettysburg, was marred when he was surprised by a Union cavalry attack at the Battle of Brandy Station and by his separation from Lee's army for an extended period, leaving Lee unaware of Union troop movements and contributing to the Confederate defeat at the Battle of Gettysburg. Stuart received significant criticism from the Southern press as well as the postbellum proponents of the Lost Cause movement, but historians have failed to agree on whether Stuart's exploit was entirely the fault of his judgment or simply a result of bad luck and Lee's less-than-explicit orders.
During the 1864 Overland Campaign, Union Maj. Gen. Philip Sheridan's cavalry launched an offensive to defeat Stuart, who was mortally wounded at the Battle of Yellow Tavern. Stuart's widow wore black for the rest of her life in remembrance of her deceased husband.
Early life.
Stuart was born at Laurel Hill Farm, a plantation in Patrick County, Virginia, near the border with North Carolina. He was of Scottish American and Scots-Irish background. He was the eighth of eleven children and the youngest of the five sons to survive past early age. His great-grandfather, Major Alexander Stuart, commanded a regiment at the Battle of Guilford Court House during the American Revolutionary War. His father, Archibald Stuart, was a War of 1812 veteran, slaveholder, attorney, and politician who represented Patrick County in both houses of the Virginia General Assembly, and also served one term in the United States House of Representatives. Archibald was a cousin of Alexander Hugh Holmes Stuart. Elizabeth Letcher Pannill Stuart, Jeb's mother, who was known as a strict religious woman with a good sense for business, ran the family farm.
Education.
Stuart was educated at home by his mother and tutors until the age of twelve, when he left Laurel Hill to be educated by various teachers in Wytheville, Virginia, and at the home of his aunt Anne (Archibald's sister) and her husband Judge James Ewell Brown (Stuart's namesake) at Danville. He entered Emory and Henry College when he was fifteen, and attended from 1848 to 1850.
During the summer of 1848, Stuart attempted to enlist in the U.S. Army, but was rejected as underaged. He obtained an appointment in 1850 to the United States Military Academy at West Point, New York, from Representative Thomas Hamlet Averett, the man who had defeated his father in the 1848 election. Stuart was a popular student and was happy at the Academy. Although not handsome in his teen years, his classmates called him by the nickname "Beauty", which they described as his "personal comeliness in inverse ratio to the term employed." He possessed a chin "so short and retiring as positively to disfigure his otherwise fine countenance." He quickly grew a beard after graduation and a fellow officer remarked that he was "the only man he ever saw that beard improved."
Robert E. Lee was appointed superintendent of the Academy in 1852, and Stuart became a friend of the Lee family, seeing them socially on frequent occasions. Lee's nephew, Fitzhugh Lee, also arrived at the academy in 1852. In Stuart's final year, in addition to achieving the cadet rank of second captain of the corps, he was one of eight cadets designated as honorary "cavalry officers" for his skills in horsemanship. Stuart graduated 13th in his class of 46 in 1854. He ranked tenth in his class in cavalry tactics. Although he enjoyed the civil engineering curriculum at the academy and did well in mathematics, his poor drawing skills hampered his engineering studies, and he finished 29th in that discipline. A Stuart family tradition says he deliberately degraded his academic performance in his final year to avoid service in the elite, but dull, Corps of Engineers.
United States Army.
Stuart was commissioned a brevet second lieutenant and assigned to the U.S. Regiment of Mounted Riflemen in Texas. After an arduous journey, he reached Fort Davis on January 28, 1855, and was a leader for three months on scouting missions over the San Antonio to El Paso Road. He was soon transferred to the newly formed 1st Cavalry Regiment (1855) at Fort Leavenworth, Kansas Territory, where he became regimental quartermaster and commissary officer under the command of Col. Edwin V. Sumner. He was promoted to first lieutenant in 1855.
Also in 1855, Stuart met Flora Cooke, the daughter of the commander of the 2nd U.S. Dragoon Regiment, Lieutenant Colonel Philip St. George Cooke. Burke Davis described Flora as "an accomplished horsewoman, and though not pretty, an effective charmer," to whom "Stuart succumbed with hardly a struggle." They became engaged in September, less than two months after meeting. Stuart humorously wrote of his rapid courtship in Latin, ""Veni, Vidi, Victus sum"" (I came, I saw, I was conquered). Although a gala wedding was planned for Fort Riley, Kansas, the death of Stuart's father on September 20 caused a change of plans and the marriage on November 14 was small and limited to family witnesses. The couple owned two slaves until 1859, one inherited from his father's estate, the other purchased.
Stuart's leadership capabilities were soon recognized. He was a veteran of the frontier conflicts with Native Americans and the antebellum violence of Bleeding Kansas. He was wounded on July 29, 1857, while fighting at Solomon River, Kansas, against the Cheyenne. Col. Sumner ordered a charge with drawn sabers against a wave of Indian arrows. Scattering the warriors, Stuart and three other lieutenants chased one down, whom Stuart wounded in the thigh with his pistol. The Cheyenne turned and fired at Stuart with an old-fashioned pistol, striking him in the chest with a bullet, which did little more damage than to pierce the skin. Stuart returned in September to Fort Leavenworth and was reunited with his wife.
Their first child, a girl, had been born in 1856 but died the same day. On November 14, 1857, Flora gave birth to another daughter, whom the parents named Flora after her mother. The family relocated in early 1858 to Fort Riley, where they remained for three years.
In 1859, Stuart developed a new piece of cavalry equipment, for which he received patent number 25,684 on October 4—a saber hook, or an "improved method of attaching sabers to belts." The U.S. government paid Stuart $5,000 for a "right to use" license and Stuart contracted with Knorr, Nece and Co. of Philadelphia to manufacture his hook. While in Washington, D.C., to discuss government contracts, and in conjunction with his application for an appointment into the quartermaster department, Stuart heard about John Brown's raid on the U.S. Arsenal at Harpers Ferry. Stuart volunteered to be aide-de-camp to Col. Robert E. Lee and accompanied Lee with a company of U.S. Marines from the Washington Navy Yard and four companies of Maryland militia. While delivering Lee's written surrender ultimatum to the leader of the group, who had been calling himself Isaac Smith, Stuart recognized "Old Ossawatomie Brown" from his days in Kansas.
Stuart was promoted to captain on April 22, 1861, but resigned from the U.S. Army on May 3, 1861, to join the Confederate States Army, following the secession of Virginia. (His letter of resignation, sent from Cairo, Illinois, was accepted by the War Department on May 14.) Upon learning that his father-in-law, Col. Cooke, would remain in the U.S. Army during the coming war, Stuart wrote to his brother-in-law (future Confederate Brig. Gen. John Rogers Cooke), "He will regret it but once, and that will be continuously." On June 26, 1860, Flora gave birth to a son, Philip St. George Cooke Stuart, but his father changed the name to James Ewell Brown Stuart, Jr. ("Jimmie"), in late 1861 out of disgust with his father-in-law.
Confederate Army.
Early service.
Stuart was commissioned as a lieutenant colonel of Virginia Infantry in the Confederate Army on May 10, 1861. Maj. Gen. Robert E. Lee, now commanding the armed forces of Virginia, ordered him to report to Colonel Thomas J. Jackson at Harper's Ferry. Jackson chose to ignore Stuart's infantry designation and assigned him on July 4 to command all the cavalry companies of the Army of the Shenandoah, organized as the 1st Virginia Cavalry Regiment. He was promoted to colonel on July 16.
After early service in the Shenandoah Valley, Stuart led his regiment in the First Battle of Bull Run, and participated in the pursuit of the retreating Federals. He then commanded the Army's outposts along the upper Potomac River until given command of the cavalry brigade for the army then known as the Army of the Potomac (later named the Army of Northern Virginia). He was promoted to brigadier general on September 24, 1861.
Peninsula.
In 1862, the Union Army of the Potomac began its Peninsula Campaign against Richmond, Virginia, and Stuart's cavalry brigade assisted Gen. Joseph E. Johnston's army as it withdrew up the Virginia Peninsula in the face of superior numbers. Stuart fought at the Battle of Williamsburg, but in general the terrain and weather on the Peninsula did not lend themselves to cavalry operations. However, when Gen. Robert E. Lee became commander of the Army of Northern Virginia, he requested that Stuart perform reconnaissance to determine whether the right flank of the Union army was vulnerable. Stuart set out with 1,200 troopers on the morning of June 12 and, having determined that the flank was indeed vulnerable, took his men on a complete circumnavigation of the Union army, returning after 150 miles on July 15 with 165 captured Union soldiers, 260 horses and mules, and various quartermaster and ordnance supplies. His men met no serious opposition from the more decentralized Union cavalry, coincidentally commanded by his father-in-law, Col. Cooke. The maneuver was a public relations sensation and Stuart was greeted with flower petals thrown in his path at Richmond. He had become as famous as Stonewall Jackson in the eyes of the Confederacy.
Northern Virginia.
Early in the Northern Virginia Campaign, Stuart was promoted to major general on July 25, 1862, and his command was upgraded to the Cavalry Division. He was nearly captured and lost his signature plumed hat and cloak to pursuing Federals during a raid in August, but in a retaliatory raid at Catlett's Station the following day, managed to overrun Union army commander Maj. Gen. John Pope's headquarters, and not only captured Pope's full uniform, but also intercepted orders that provided Lee with valuable intelligence concerning reinforcements for Pope's army.
At the Second Battle of Bull Run (Second Manassas), Stuart's cavalry followed the massive assault by Longstreet's infantry against Pope's army, protecting its flank with artillery batteries. Stuart ordered Brig. Gen. Beverly Robertson's brigade to pursue the Federals and in a sharp fight against Brig. Gen. John Buford's brigade, Col. Thomas T. Munford's 2nd Virginia Cavalry was overwhelmed until Stuart sent in two more regiments as reinforcements. Buford's men, many of whom were new to combat, retreated across Lewis's Ford and Stuart's troopers captured over 300 of them. Stuart's men harassed the retreating Union columns until the campaign ended at the Battle of Chantilly.
Maryland.
During the Maryland Campaign of September 1862, Stuart's cavalry screened the army's movement north. He bears some responsibility for Robert E. Lee's lack of knowledge of the position and celerity of the pursuing Army of the Potomac under George B. McClellan. For a five-day period, Stuart rested his men and entertained local civilians at a gala ball at Urbana, Maryland. His reports make no reference to intelligence gathering by his scouts or patrols. As the Union Army drew near to Lee's divided army, Stuart's men skirmished at various points on the approach to Frederick and Stuart was not able to keep his brigades concentrated enough to resist the oncoming tide. He misjudged the Union routes of advance, ignorant of the Union force threatening Turner's Gap, and required assistance from the infantry of Maj. Gen. D.H. Hill to defend the South Mountain passes in the Battle of South Mountain. His horse artillery bombarded the flank of the Union army as it opened its attack in the Battle of Antietam. By mid-afternoon, Stonewall Jackson ordered Stuart to command a turning movement with his cavalry against the Union right flank and rear, which if successful would be followed up by an infantry attack from the West Woods. Stuart began probing the Union lines with more artillery barrages, which were answered with "murderous" counterbattery fire and the cavalry movement intended by Jackson was never launched.
Three weeks after Lee's army had withdrawn back to Virginia, on October 10–12, 1862, Stuart performed another of his audacious circumnavigations of the Army of the Potomac, his Chambersburg Raid—126 miles in under 60 hours, from Darkesville, West Virginia to as far north as Mercersburg, Pennsylvania and Chambersburg and around to the east through Emmitsburg, Maryland and south through Hyattstown, Maryland and White's Ford to Leesburg, Virginia—once again embarrassing his Union opponents and seizing horses and supplies, but at the expense of exhausted men and animals, without gaining much military advantage. Jubal Early referred to it as "the greatest horse stealing expedition" that only "annoyed" the enemy. Stuart gave his friend Jackson a fine, new officer's tunic, trimmed with gold lace, commissioned from a Richmond tailor, which he thought would give Jackson more of the appearance of a proper general (something to which Jackson was notoriously indifferent).
McClellan pushed his army slowly south, urged by President Lincoln to pursue Lee, crossing the Potomac starting on October 26. As Lee began moving to counter this, Stuart screened Longstreet's Corps and skirmished numerous times in early November against Union cavalry and infantry around Mountville, Aldie, and Upperville. On November 6, Stuart received sad news by telegram that his daughter Flora had died just before her fifth birthday of typhoid fever on November 3.
Fredericksburg and Chancellorsville.
In the December 1862 Battle of Fredericksburg, Stuart and his cavalry—most notably his horse artillery under Major John Pelham—protected Stonewall Jackson's flank at Hamilton's Crossing. General Lee commended his cavalry, which "effectually guarded our right, annoying the enemy and embarrassing his movements by hanging on his flank, and attacking when the opportunity occurred." Stuart reported to Flora the next day that he had been shot through his fur collar but was unhurt.
After Christmas, Lee ordered Stuart to conduct a raid north of the Rappahannock River to "penetrate the enemy's rear, ascertain if possible his position & movements, & inflict upon him such damage as circumstances will permit." With 1,800 troopers and a horse artillery battery assigned to the operation, Stuart's raid reached as far north as four miles south of Fairfax Court House, seizing 250 prisoners, horses, mules, and supplies. Tapping telegraph lines, his signalmen intercepted messages between Union commanders and Stuart sent a personal telegram to Union Quartermaster General Montgomery C. Meigs , "General Meigs will in the future please furnish better mules; those you have furnished recently are very inferior."
On March 17, 1863, Stuart's cavalry clashed with a Union raiding party at Kelly's Ford. The minor victory was marred by the death of Major Pelham, which caused Stuart profound grief, as he thought of him as close as a younger brother. He wrote to a Confederate Congressman, "The noble, the chivalric, the gallant Pelham is no more. ... Let the tears of agony we have shed, and the gloom of mourning throughout my command bear witness." Flora was pregnant at the time and Stuart told her that if it were a boy, he wanted him to be named John Pelham Stuart. (Virginia Pelham Stuart was born October 9.)
At the Battle of Chancellorsville, Stuart accompanied Stonewall Jackson on his famous flanking march of May 2, 1863, and started to pursue the retreating soldiers of the Union XI Corps when he received word that both Jackson and his senior division commander, Maj. Gen. A.P. Hill, had been wounded. Hill, bypassing the next most senior infantry general in the corps, Brig. Gen. Robert E. Rodes, sent a message ordering Stuart to take command of the Second Corps. Although the delays associated with this change of command effectively ended the flanking attack the night of May 2, Stuart performed credibly as an infantry corps commander the following day, launching a strong and well-coordinated attack against the Union right flank at Chancellorsville. When Union troops abandoned Hazel Grove, Stuart had the presence of mind to quickly occupy it and bombard the Union positions with artillery. Stuart relinquished his infantry command on May 6 when Hill returned to duty. Stephen W. Sears wrote:
Stonewall Jackson died on May 10 and Stuart was once again devastated by the loss of a close friend, telling his staff that the death was a "national calamity." Jackson's wife, Mary Anna, wrote to Stuart on August 1, thanking him for a note of sympathy: "I need not assure you of which you already know, that your friendship & admiration were cordially reciprocated by him. I have frequently heard him speak of Gen'l Stuart as one of his warm personal friends, & also express admiration for your Soldierly qualities."
Brandy Station.
Returning to the cavalry for the Gettysburg Campaign, Stuart endured the two low points in his career, starting with the Battle of Brandy Station, the largest predominantly cavalry engagement of the war. By June 5, two of Lee's infantry corps were camped in and around Culpeper. Six miles northeast, holding the line of the Rappahannock River, Stuart bivouacked his cavalry troopers, mostly near Brandy Station, screening the Confederate Army against surprise by the enemy. Stuart requested a full field review of his troops by Gen. Lee. This grand review on June 5 included nearly 9,000 mounted troopers and four batteries of horse artillery, charging in simulated battle at Inlet Station, about two miles (three km) southwest of Brandy Station.
Lee was not able to attend the review, however, so it was repeated in his presence on June 8, although the repeated performance was limited to a simple parade without battle simulations. Despite the lower level of activity, some of the cavalrymen and the newspaper reporters at the scene complained that all Stuart was doing was feeding his ego and exhausting the horses. Lee ordered Stuart to cross the Rappahannock the next day and raid Union forward positions, screening the Confederate Army from observation or interference as it moved north. Anticipating this imminent offensive action, Stuart ordered his tired troopers back into bivouac around Brandy Station.
Army of the Potomac commander Maj. Gen. Joseph Hooker interpreted Stuart's presence around Culpeper to be indicative of preparations for a raid on his army's supply lines. In reaction, he ordered his cavalry commander, Maj. Gen. Alfred Pleasonton, to take a combined arms force of 8,000 cavalrymen and 3,000 infantry on a "spoiling raid" to "disperse and destroy" the 9,500 Confederates. Pleasonton's force crossed the Rappahannock in two columns on June 9, 1863, the first crossing at Beverly's Ford (Brig. Gen. John Buford's division) catching Stuart by surprise, waking him and his staff to the sound of gunfire. The second crossing, at Kelly's Ford, surprised Stuart again, and the Confederates found themselves assaulted from front and rear in a spirited melee of mounted combat. A series of confusing charges and countercharges swept back and forth across Fleetwood Hill, which had been Stuart's headquarters the previous night. After ten hours of fighting, Pleasonton ordered his men to withdraw across the Rappahannock.
Although Stuart claimed a victory because the Confederates held the field, Brandy Station is considered a tactical draw, and both sides came up short. Pleasonton was not able to disable Stuart's force at the start of an important campaign and he withdrew before finding the location of Lee's infantry nearby. However, the fact that the Southern cavalry had not detected the movement of two large columns of Union cavalry, and that they fell victim to a surprise attack, was an embarrassment that prompted serious criticism from fellow generals and the Southern press. The fight also revealed the increased competency of the Union cavalry, and foreshadowed the decline of the formerly invincible Southern mounted arm.
Stuart's ride in the Gettysburg Campaign.
Following a series of small cavalry battles in June as Lee's army began marching north through the Shenandoah Valley, Stuart may have had in mind the glory of circumnavigating the enemy army once again, desiring to erase the stain on his reputation of the surprise at Brandy Station. General Lee gave orders to Stuart on June 22 on how he was to participate in the march north, and the exact nature of those orders has been argued by the participants and historians ever since, but the essence was that he was instructed to guard the mountain passes with part of his force while the Army of Northern Virginia was still south of the Potomac and that he was to cross the river with the remainder of the army and screen the right flank of Ewell's Second Corps. Instead of taking a direct route north near the Blue Ridge Mountains, however, Stuart chose to reach Ewell's flank by taking his three best brigades (those of Brig. Gen. Wade Hampton, Brig. Gen. Fitzhugh Lee, and Col. John R. Chambliss, the latter replacing the wounded Brig. Gen. W.H.F. "Rooney" Lee) between the Union army and Washington, moving north through Rockville to Westminster and on into Pennsylvania, hoping to capture supplies along the way and cause havoc near the enemy capital. Stuart and his three brigades departed Salem Depot at 1 a.m. on June 25.
Unfortunately for Stuart's plan, the Union army's movement was underway and his proposed route was blocked by columns of Federal infantry, forcing him to veer farther to the east than either he or General Lee had anticipated. This prevented Stuart from linking up with Ewell as ordered and deprived Lee of the use of his prime cavalry force, the "eyes and ears" of the army, while advancing into unfamiliar enemy territory.
Stuart's command crossed the Potomac River at 3 a.m. on June 28. At Rockville they captured a wagon train of 140 brand-new, fully loaded wagons and mule teams. This wagon train would prove to be a logistical hindrance to Stuart's advance, but he interpreted Lee's orders as placing importance on gathering supplies. The proximity of the Confederate raiders provoked some consternation in the national capital and two Union cavalry brigades and an artillery battery were sent to pursue the Confederates. Stuart supposedly said that were it not for his fatigued horses "he would have marched down the 7th Street Road took Abe & Cabinet prisoners."
In Westminster on June 29, his men clashed briefly with and overwhelmed two companies of Union cavalry, chasing them a long distance on the Baltimore road, which Stuart claimed caused a "great panic" in the city of Baltimore. The head of Stuart's column encountered Brig. Gen. Judson Kilpatrick's cavalry as it passed through Hanover and scattered it on June 30; the Battle of Hanover ended after Kilpatrick's men regrouped and drove the Confederates out of town. Stuart's brigades had been better positioned to guard their captured wagon train than to take advantage of the encounter with Kilpatrick. After a 20-mile trek in the dark, his exhausted men reached Dover on the morning of July 1, as the Battle of Gettysburg was commencing without them.
Stuart headed next for Carlisle, hoping to find Ewell. He lobbed a few shells into town during the early evening of July 1 and burned the Carlisle Barracks before withdrawing to the south towards Gettysburg. He and the bulk of his command reached Lee at Gettysburg the afternoon of July 2. He ordered Wade Hampton to cover the left rear of the Confederate battle lines, and Hampton fought with Brig. Gen. George Armstrong Custer at the Battle of Hunterstown before joining Stuart at Gettysburg.
Gettysburg and its aftermath.
When Stuart arrived at Gettysburg on the afternoon of July 2—bringing with him the caravan of captured Union supply wagons—he received a rare rebuke from Lee. (No one witnessed the private meeting between Lee and Stuart, but reports circulated at headquarters that Lee's greeting was "abrupt and frosty." Colonel Edward Porter Alexander wrote, "Although Lee said only, 'Well, General, you are here at last,' his manner implied rebuke, and it was so understood by Stuart.") On the final day of the battle, Stuart was ordered to get into the enemy's rear and disrupt its line of communications at the same time Pickett's Charge was sent against the Union positions on Cemetery Ridge, but his attack on East Cavalry Field was repulsed by Union cavalry under Brig. Gens. David Gregg and George Custer.
During the retreat from Gettysburg, Stuart devoted his full attention to supporting the army's movement, successfully screening against aggressive Union cavalry pursuit and escorting thousands of wagons with wounded men and captured supplies over difficult roads and through inclement weather. Numerous skirmishes and minor battles occurred during the screening and delaying actions of the retreat. Stuart's men were the final units to cross the Potomac River, returning to Virginia in "wretched condition—completely worn out and broken down."
The Gettysburg Campaign was the most controversial of Stuart's career. He became one of the scapegoats (along with James Longstreet) blamed for Lee's loss at Gettysburg by proponents of the postbellum Lost Cause movement, such as Jubal Early. This was fueled in part by opinions of less partisan writers, such as Stuart's subordinate, Thomas L. Rosser, who stated after the war that Stuart did, "on this campaign, "undoubtedly", make the fatal blunder which lost us the battle of Gettysburg." In General Lee's report on the campaign, he wrote
One of the most forceful postbellum defenses of Stuart was by Col. John S. Mosby, who had served under him during the campaign and was fiercely loyal to the late general, writing, "He made me all that I was in the war. ... But for his friendship I would never have been heard of." He wrote numerous articles for popular publications and published a book length treatise in 1908, a work that relied on his skills as a lawyer to refute categorically all of the claims laid against Stuart.
Modern scholarship remains divided on Stuart's culpability. Edward G. Longacre argues that Lee deliberately gave Stuart wide discretion in his orders and had no complaints about Stuart's tardy arrival at Gettysburg because he established no date by which the cavalry was required to link up with Ewell. The 3½ brigades of cavalry left with the main army were adequate for Lee to negotiate enemy territory safely and that his choice not to use these brigades effectively cannot be blamed on Stuart. Edwin B. Coddington refers to the "tragedy" of Stuart in the Gettysburg Campaign and judges that when Fitzhugh Lee raised the question of "whether Stuart exercised the discretion "undoubtedly given to him, judiciously"," the answer is no. Nevertheless, replying to historians who maintain that Stuart's absence permitted Lee to be surprised at Gettysburg, Coddington points out that the Union commander, Maj. Gen. George Meade, was just as surprised, and the initial advantage lay with Lee. Eric J. Wittenberg and J. David Petruzzi have concluded that there was "plenty of blame to go around" and the fault should be divided between Stuart, the lack of specificity in Lee's orders, and Richard S. Ewell, who might have tried harder to link up with Stuart northeast of Gettysburg. Jeffry D. Wert acknowledges that Lee, his officers, and fighting by the Army of the Potomac bear the responsibility for the Confederate loss at Gettysburg, but states that "Stuart failed Lee and the army in the reckoning at Gettysburg. ... Lee trusted him and gave him discretion, but Stuart acted injudiciously."
Although Stuart was not reprimanded or disciplined in any official way for his role in the Gettysburg campaign, it is noteworthy that his appointment to corps command on September 9, 1863, did not carry with it a promotion to lieutenant general. Edward Bonekemper wrote that since all other corps commanders in the Army of Northern Virginia carried this rank, Lee's decision to keep Stuart at major general rank, while at the same time promoting Stuart's subordinates Wade Hampton and Fitzhugh Lee to major generals, could be considered an implied rebuke. Jeffry D. Wert wrote that there is no evidence Lee considered Stuart's performance during the Gettysburg Campaign and that it is "more likely that Lee thought the responsibilities in command of a cavalry corps did not equal those of an infantry corps."
Fall 1863 and the 1864 Overland Campaign.
Lee reorganized his cavalry on September 9, creating a Cavalry Corps for Stuart with two divisions of three brigades each. In the Bristoe Campaign, Stuart was assigned to lead a broad turning movement in an attempt to get into the enemy's rear, but General Meade skillfully withdrew his army without leaving Stuart any opportunities to take advantage of. On October 13, Stuart blundered into the rear guard of the Union III Corps near Warrenton. Ewell's corps was sent to rescue him, but Stuart hid his troopers in a wooded ravine until the unsuspecting III Corps moved on, and the assistance was not necessary. As Meade withdrew towards Manassas Junction, brigades from the Union II Corps fought a rearguard action against Stuart's cavalry and the infantry of Brig. Gen. Harry Hays's division near Auburn on October 14. Stuart's cavalry boldly bluffed Warren's infantry and escaped disaster. After the Confederate repulse at Bristoe Station and an aborted advance on Centreville, Stuart's cavalry shielded the withdrawal of Lee's army from the vicinity of Manassas Junction. Judson Kilpatrick's Union cavalry pursued Stuart's cavalry along the Warrenton Turnpike, but were lured into an ambush near Chestnut Hill and routed. The Federal troopers were scattered and chased five miles (eight km) in an affair that came to be known as the "Buckland Races". The Southern press began to mute its criticism of Stuart's following his successful performance during the fall campaign.
The Overland Campaign, Lt. Gen. Ulysses S. Grant's offensive against Lee in the spring of 1864, began at the Battle of the Wilderness, where Stuart aggressively pushed Thomas L. Rosser's Laurel Brigade into a fight against George Custer's better-armed Michigan Brigade, resulting in significant losses. General Lee sent a message to Stuart: "It is very important to save your Cavalry & not wear it out. ... You must use your good judgment to make any attack which may offer advantages." As the armies maneuvered toward their next confrontation at Spotsylvania Court House, Stuart's cavalry fought delaying actions against the Union cavalry. His defense at Laurel Hill, also directing the infantry of Brig. Gen. Joseph B. Kershaw, skillfully delayed the advance of the Federal army for nearly 5 critical hours.
Yellow Tavern and death.
The commander of the Army of the Potomac, Maj. Gen. George Meade, and his cavalry commander, Maj. Gen. Philip Sheridan, quarreled about the Union cavalry's performance in the first two engagements of the Overland Campaign. Sheridan heatedly asserted that he wanted to "concentrate all of cavalry, move out in force against Stuart's command, and whip it." Meade reported the comments to Grant, who replied "Did Sheridan say that? Well, he generally knows what he is talking about. Let him start right out and do it." Sheridan immediately organized a raid against Confederate supply and railroad lines close to Richmond, which he knew would bring Stuart to battle.
Sheridan moved aggressively to the southeast, crossing the North Anna River and seizing Beaver Dam Station on the Virginia Central Railroad, where his men liberated a train carrying 3,000 Union prisoners and destroyed more than one million rations and medical supplies destined for Lee's army. Stuart dispatched a force of about 3,000 cavalrymen to intercept Sheridan's cavalry, which was more than three times their numbers. As he rode in pursuit, accompanied by his aide, Maj. Andrew R. Venable, they were able to stop briefly along the way to be greeted by Stuart's wife, Flora, and his children, Jimmie and Virginia. Venable wrote of Stuart, "He told me he never expected to live through the war, and that if we were conquered, that he did not want to live."
The Battle of Yellow Tavern occurred May 11, at an abandoned inn located six miles (10 km) north of Richmond. The Confederate troopers tenaciously resisted from the low ridgeline bordering the road to Richmond, fighting for over three hours. A countercharge by the 1st Virginia Cavalry pushed the advancing Union troopers back from the hilltop as Stuart, on horseback, shouted encouragement while firing his revolver at the Union troopers. As the 5th Michigan Cavalry streamed in retreat past Stuart, a dismounted Union private, 44-year-old John A. Huff, turned and shot Stuart with his .44-caliber revolver from a distance of 10–30 yards.
Huff's bullet struck Stuart in the left side. It then sliced through his stomach and exited his back, one inch to the right of his spine. Stuart suffered great pain as an ambulance took him to Richmond to await his wife's arrival at the home of Dr. Charles Brewer, his brother-in-law. Stuart ordered his sword and spurs be given to his son. His last whispered words were: "I am resigned; God's will be done." He died at 7:38 p.m. on May 12, the following day, before Flora Stuart reached his side. He was 31 years old. Stuart was buried in Richmond's Hollywood Cemetery. Upon learning of Stuart's death, General Lee is reported to have said that he could hardly keep from weeping at the mere mention of Stuart's name and that Stuart had never given him a bad piece of information.
Flora wore the black of mourning for the remainder of her life, and never remarried. She lived in Saltville, Virginia, for 15 years after the war, where she opened and taught at a school in a log cabin. She worked from 1880 to 1898 as principal of the Virginia Female Institute in Staunton, Virginia, a position for which Robert E. Lee had recommended her before his death ten years earlier. In 1907, the Institute was renamed Stuart Hall School in her honor. Upon the death of her daughter Virginia, from complications in childbirth in 1898, Flora resigned from the Institute and moved to Norfolk, Virginia, where she helped Virginia's widower, Robert Page Waller, in raising her grandchildren. She died in Norfolk on May 10, 1923, after striking her head in a fall on a city sidewalk. She is buried alongside her husband and their daughter, Little Flora, in Hollywood Cemetery in Richmond.
Legacy and memorials.
Like his intimate friend, Stonewall Jackson, General J.E.B. Stuart was a legendary figure and is considered one of the greatest cavalry commanders in American history. His friend from his federal army days, Union Maj. Gen. John Sedgwick, said that Stuart was "the greatest cavalry officer ever foaled in America." Jackson and Stuart, both of whom were killed in battle, had colorful public images, although the latter seems to have been more deliberately crafted. Jeffry D. Wert wrote about Stuart:
A statue of General J.E.B. Stuart by sculptor Frederick Moynihan was dedicated on Richmond's famed Monument Avenue at Stuart Circle in 1907. Like General Stonewall Jackson, his equestrian statue faces north, indicating that he died in the war. In 1884 the town of Taylorsville, Virginia, was renamed Stuart. The British Army named two models of American-made World War II tanks, the M3 and M5, the Stuart tank in General Stuart's honor. A high school on Munson's Hill in Falls Church, Virginia and a middle school in Jacksonville, Florida are named for him.
In December 2006, a personal Confederate battle flag, sewn by Flora Stuart, was sold in a Heritage Auction for a world-record price for any Confederate flag, for $956,000 (including buyer's premium). The 34-inch by 34-inch flag was hand-sewn for Stuart by Flora in 1862 and Stuart carried it into some of his most famous battles. 
Stuart's birthplace, Laurel Hill, located in Patrick County, Virginia, was purchased by the J.E.B. Stuart Birthplace Preservation Trust, Inc., in 1992 to preserve and interpret it.

</doc>
<doc id="16378" url="https://en.wikipedia.org/wiki?curid=16378" title="John Hanson">
John Hanson

John Hanson ( – November 15, 1783) was a merchant and public official from Maryland during the era of the American Revolution. In 1779, Hanson was elected as a delegate to the Continental Congress after serving in a variety of roles for the Patriot cause in Maryland. He signed the Articles of Confederation in 1781 after Maryland finally joined the other states in ratifying them.
In November 1781, he was elected President of the Continental Congress, and became the first president to serve a one-year term under the provisions of the Articles of Confederation. While George Washington is recognized by historians as the first President of the United States, having served under the current United States Constitution, some biographies of Hanson have made the unconventional argument that Hanson was the first holder of the office.
Early life.
John Hanson was born in Port Tobacco Parish in Charles County in the Province of Maryland on April 3, 1721, which in the modern calendar system is equivalent to April 14. Sources published prior to a 1940 genealogical study sometimes listed his birth date as April 13 or his year of birth as 1715.
Hanson was born on a plantation called "Mulberry Grove" into a wealthy and prominent family. His parents were Samuel (c. 1685–1740) and Elizabeth (Storey) Hanson (c. 1688–1764). Samuel Hanson was a planter who owned more than , and held a variety of political offices, including serving two terms in the Maryland General Assembly.
John Hanson was of English ancestry; his grandfather, also named John, came to Charles County, Maryland, as an indentured servant around 1661. In 1876, a writer named George Hanson placed John Hanson in his family tree of Swedish-Americans descended from four Swedish brothers who emigrated to New Sweden in 1642. This story was often repeated over the next century, but scholarly research in the late 20th century showed that John Hanson was of English heritage and not related to those Swedish-American Hansons.
Little is known about Hanson's early life; he was presumably privately tutored as was customary among the gentry of his time and place. He followed his father's path as a planter, slave owner, and public official. He was often referred to as "John Hanson, Jr.", to distinguish him from an older man of the same name. About 1744 he married Jane Contee (1728–1812), with whom he would have eight children. Their son Peter Contee Hanson (1758–1776) died in the battle of Fort Washington during the American Revolutionary War. Their oldest son Alexander Contee Hanson, Sr. (1749–1806) was a notable essayist. Alexander Hanson is sometimes confused with his son, Alexander Contee Hanson, Jr. (1786–1819), who became a newspaper editor and US Senator.
Political career.
Hanson's career in public service began in 1750, when he was appointed sheriff of Charles County. In 1757 he was elected to represent Charles County in the lower house of the Maryland General Assembly, where he served over the next twelve years, sitting on many important committees. Maryland was a proprietary colony, and Hanson aligned himself with the "popular" or "country" party, which opposed any expansion of the power of the proprietary governors at the expense of the popularly elected lower house. He was a leading opponent of the 1765 Stamp Act, chairing the committee that drafted the instructions for Maryland's delegates to the Stamp Act Congress. In protest of the Townshend Acts, in 1769 Hanson was one of the signers of a nonimportation resolution that boycotted British imports until the acts were repealed.
Hanson changed course in 1769, apparently to better pursue his business interests. He resigned from the General Assembly, sold his land in Charles County, and moved to Frederick County in western Maryland. There he held a variety of offices, including deputy surveyor, sheriff, and county treasurer.
When relations between Great Britain and the colonies became a crisis in 1774, Hanson became one of Frederick County's leading Patriots. He chaired a town meeting that passed a resolution opposing the Boston Port Act. In 1775, he was a delegate to the Maryland Convention, an extralegal body convened after the colonial assembly had been prorogued. With the other delegates, he signed the Association of Freemen on July 26, 1775, which expressed hope for reconciliation with Great Britain, but also called for military resistance to enforcement of the Coercive Acts.
With hostilities underway, Hanson chaired the Frederick County committee of observation, part of the Patriot organization that assumed control of local governance. Responsible for recruiting and arming soldiers, Hanson proved to be an excellent organizer, and Frederick County sent the first southern troops to join George Washington's army. Because funds were scarce, Hanson frequently paid soldiers and others with his own money. In June 1776, Hanson chaired the Frederick County meeting that urged provincial leaders in Annapolis to instruct Maryland's delegates in the Continental Congress to declare independence from Great Britain. While Congress worked on the Declaration of Independence, Hanson was in Frederick County "making gunlocks, storing powder, guarding prisoners, raising money and troops, dealing with Tories, and doing the myriad other tasks which went with being chairman of the committee of observation".
Hanson was elected to the newly reformed Maryland House of Delegates in 1777, the first of five annual terms. In December 1779, the House of Delegates named Hanson as a delegate to the Second Continental Congress; he began serving in Congress in Philadelphia in June 1780. "Hanson came to Philadelphia with the reputation of having been the leading financier of the revolution in western Maryland, and soon he was a member of several committees dealing with finance."
When Hanson was elected to Congress, Maryland was holding up the ratification of the Articles of Confederation. The state, which did not have any claims on western land, refused to ratify the Articles until the other states had ceded their western land claims. When the other states finally did so, the Maryland legislature decided in January 1781 to ratify the Articles. When Congress received notice of this, Hanson joined Daniel Carroll in signing the Articles of Confederation on behalf of Maryland on March 1, 1781. With Maryland's endorsement, the Articles officially went into effect. Many years later, some Hanson biographers claimed that Hanson had been instrumental in arranging the compromise and thus securing ratification of the Articles, but, according to historian Ralph Levering, there is no documentary evidence of Hanson's opinions or actions in resolving the controversy.
President of Congress.
On November 5, 1781, Congress elected Hanson as president of the Continental Congress (or "president of the Congress of the Confederacy" or "president of Congress"). Under the Articles of Confederation, the United States had no executive branch; the president of Congress was a mostly ceremonial position, but the office did require Hanson to handle a good deal of correspondence and sign official documents. Hanson found the work tedious and considered resigning after just one week, citing his poor health and family responsibilities. Colleagues urged him to remain because Congress at the moment lacked a quorum to choose a successor. Out of a sense of duty, Hanson remained in office, although his term as a delegate to Congress was nearly expired. The Maryland Assembly reelected him as a delegate on November 28, 1781, and so Hanson continued to serve as president until November 4, 1782.
The Articles of Confederation stipulated that presidents of Congress serve one-year terms, and Hanson became the first president to do so. Contrary to the claims of some of his later advocates, however, he was not the first president to serve under the Articles, nor the first to be elected under the Articles. When the Articles went into effect in March 1781, Congress did not bother to elect a new president; instead, Samuel Huntington continued serving a term that had already exceeded a year. On July 9, 1781, Samuel Johnston became the first man to be elected as president of Congress after the ratification of the Articles. He declined the office, however, perhaps to make himself available for North Carolina's gubernatorial election. After Johnston turned down the office, Thomas McKean was elected. McKean served just a few months, resigning in October 1781 after hearing news of the British surrender at Yorktown. Congress asked him to remain in office until November, when a new session of Congress was scheduled to begin. It was in that session that Hanson began to serve his one-year term. A highlight of Hanson's term was when George Washington presented Cornwallis's sword to Congress.
In 1781, during Hanson's presidency, Ned Barnes, one of Hanson's slaves, ran away. Hanson published an advertisement in the "Maryland Gazette", offering a $30 reward for Barnes' recapture. Barnes was recaptured, but he stole a horse and ran off again to be with his wife, who lived on a plantation in Charles County, Maryland. Hanson gave up on getting Barnes back and instead sold him to the plantation to which he had escaped.
Death and legacy.
Hanson retired from public office after his one-year term as president of Congress. In poor health, he died on November 15, 1783, while visiting Oxon Hill Manor in Prince George's County, Maryland, the plantation of his nephew Thomas Hawkins Hanson. He was buried there. Hanson owned at least 223 acres of land and 11 slaves at the time of his death.
In 1898, Douglas H. Thomas, a descendant of Hanson, wrote a biography promoting Hanson as the first true President of the United States. Thomas became the "driving force" behind the selection of Hanson as one of the two people who would represent Maryland in the National Statuary Hall Collection in Washington, D.C. Hanson was not initially on the shortlist for consideration, but he was chosen after lobbying by the Maryland Historical Society. In 1903, bronze statues of Hanson and Charles Carroll by sculptor Richard E. Brooks were added to Statuary Hall; Hanson's is currently located on the 2nd floor of the Senate connecting corridor. Small versions of these two statues (maquettes) sit on the president's desk in the Senate Chamber of the Maryland State House.
Some historians have questioned the appropriateness of Hanson's selection for the honor of representing Maryland in Statuary Hall. According to historian Gregory Stiverson, Hanson was not one of Maryland's foremost leaders of the Revolutionary era. In 1975, historian Ralph Levering said that "Hanson shouldn't have been one of the two Marylanders" chosen, but he wrote that Hanson "probably contributed as much as any other Marylander to the success of the American Revolution". In the 21st century, Maryland lawmakers have considered replacing Hanson's statue in Statuary Hall with one of Harriet Tubman.
The idea that Hanson was the forgotten first President of the United States was further promoted in a 1932 biography of Hanson by journalist Seymour Wemyss Smith. Smith's book asserted that the American Revolution had two primary leaders: George Washington on the battlefield, and John Hanson in politics. Smith's book, like Douglas H. Thomas's 1898 book, was one of a number of biographies written seeking to promote Hanson as the "first President of the United States". Regarding the opinion, historian Ralph Levering stated "They're not biographies by professional historians; they aren't based on research into primary sources". According to historian Richard B. Morris, if a president of Congress were to be called the "first" President of the United States, "a stronger case could be made for Peyton Randolph of Virginia, the first President of the first and second Continental Congresses, or for John Hancock, the President of Congress when that body declared its independence." The claim that Hanson was a forgotten President of the United States was revived on the Internet, sometimes with a new assertion that he was actually a black man; an anachronistic photograph of Senator John Hanson of Liberia has been used to support this claim. 
In 1972, Hanson was depicted on a 6-cent US postal card, which featured his name and portrait next to the word "Patriot". Historian Irving Brant criticized the selection of Hanson for the card, arguing that it was a result of the "old hoax" promoting Hanson as the first president of the United States. In 1981, Hanson was featured on a 20-cent US postage stamp. U.S. Route 50 between Washington D.C. and Annapolis is named the John Hanson Highway in his honor. There are also middle schools located in Oxon Hill, Maryland, and Waldorf, Maryland, named after him. A former savings bank named for him was merged in the 1990s with Industrial Bank of Washington, D.C.
In the 1970s, a descendant of Hanson, John Hanson Briscoe, served as Speaker of the Maryland House of Delegates, which passed "a measure establishing April 14 as John Hanson Day". In 2009 the John Hanson Memorial Association was incorporated in Frederick, Maryland to create the John Hanson National Memorial and to both educate Americans about Hanson as well as counter the many myths written about him. The Memorial includes a statue of President John Hanson and an interpretive setting in Frederick, Maryland, where Hanson lived between 1769 and his death in 1783. The Memorial is in the Frederick County Courthouse's courtyard at the corner of Court and West Patrick Streets. Leaders of the Memorial include President Peter Hanson Michael, Vice President Robert Hanson and Directors John Hanson Briscoe and John C. Hanson.

</doc>
<doc id="16379" url="https://en.wikipedia.org/wiki?curid=16379" title="Jedi">
Jedi

The Jedi are the main protagonists in the fictional "Star Wars" universe. They are an ancient monastic, spiritual, and academic meritocratic organization whose origins dates back to circa "25,000 BBY" ("Before Battle of Yavin"; the destruction of the first Death Star). The Jedi Order mostly consists of polymaths: teachers, philosophers, scientists, engineers, physicians, diplomats and warriors, who value knowledge and wisdom above nationality. By serving others, the Jedi give of themselves through acts of charity, citizenship, volunteerism, and good deeds, although despite this some sentients have displayed resentment, hostility and phobia of Jedi and tend to treat them more as villains rather than heroes: a degree known as Anti-Jedi sentiment. Their traditional weapon is the lightsaber, a device which generates a blade-like light controlled by a crystal. The organization has inspired a new religious movement, Jediism.
Overview.
As depicted in the canon, the Jedi study, serve and utilize a mystical power called the Force, in order to help and protect those in need. The Jedi members, known as Jedi Knights, respect all life by defending and protecting those who cannot do it for themselves, striving for peaceful and non-combative solutions to any altercations they encounter and fighting only in self-defense and for the defense of those they protect. Like their counterparts, the Sith, their main weapon is the lightsaber. By training the mind and the body the Jedi seek to improve themselves by gaining unfettered access to the Force while also seeking to improve those individuals and groups they come in contact with.
Sources and analogues.
George Lucas acknowledged Jedi concepts have been inspired by many sources. These include knighthood chivalry, paladinism, samurai bushido, and related institutions in feudal societies, Hinduism, Qigong, Greek philosophy, Greek mythology, Roman history, Roman mythology, parts of the Abrahamic religions, Confucianism, Shintō, Buddhism and Taoism, not to mention countless cinematic precursors. The work of the mythologist Joseph Campbell, especially his book "The Hero with a Thousand Faces", directly influenced Lucas, and was what drove him to create the 'modern myth' of "Star Wars".
Background and origins.
Legends.
The Jedi's history before and after the timeline of the films is established within several novels, comic books, television series and video games in the "Star Wars" expanded universe.
According to the official Star Wars Expanded Universe, the Jedi came into existence shortly after the "Ashla Knights", the precursor-race to the Jedi Order, and the Galactic Republic were created circa 25,783 BBY (Before the Battle of Yavin). Before the creation of the Jedi Order, a number of Force-sensitive sects including the "Followers of Palawa", "Chatos Academy", and "Order of Dai Bendu" studied the Force scientifically, including the science of midi-chlorians, on worlds from "Had Abbadon" to "Ondos". For many generations the Jedi served as a paramilitary for the Galactic Republic and the galaxy at large to prevent conflict and political instability; including partaking in a pivotal role during the Clone Wars.
As sanctioned guardians of peace and enforcers of justice in the galaxy, they mediated negotiations among planets and rival factions and, if necessary, use their formidable martial art skills, agility and wisdom to quickly end unrest or neutralize dangerous individuals or threats. The Jedi are governed by a Council, consisting of twelve of the wisest and most powerful "force-sensitive" members of the Jedi Order; they are bound to a code of ethics.
The Jedi are trained to use the Force through deistic reasoning, passive meditation and applied academics. They are proactive in the virtues of abstinence, altruism, mercy, gaman, and honor. They are conditioned to reject negative or dangerous emotions (such as love, passion, greed, fear, anger and hate) as these desires inevitably lead to suffering, and believe that true balance and happiness is best achieved when they are content with themselves as they are.
The Jedi's self-denial philosophical way of life contrasts with their archenemies, the Sith, another monastic organization who used the dark side of the Force. They believe the fulfillment of desire is the path to happiness, and they embrace passion and individualism as a means of achieving personal goals of freedom, power and victory over physical restrictions and thereby attain perfection.
Jedi Creed.
The Jedi Code was a mantra of moral and ethical conduct expected to be adhered to by all members of the Jedi Order. A Draethos Jedi Master named Odan-Urr studied the Jedi Code in great detail, making a revised version circa 5,000 BBY, which was adopted by the Jedi Order. This is the edition of the Jedi Code that was used until Order 66 and the Great Jedi Purge that occurred in 19 BBY: ""There is no emotion, there is peace. There is no ignorance, there is knowledge. There is no passion, there is serenity. There is no chaos, there is harmony. There is no death, there is the force.""
Following the collapse of the Galactic Empire after the death of the Emperor Palpatine in circa 4 ABY ("After Battle of Yavin"), Jedi Grand Master Luke Skywalker reestablished the Jedi Order. Termed the New Jedi Order, Grand Master Skywalker re-wrote the Jedi Creed: ""Jedi are the guardians of peace in the galaxy. Jedi use their powers to defend and protect, never to attack others. Jedi ferrets out deceit and injustice, bringing it to light. Jedi respect all life, in any form. Jedi serve others rather than ruling over them, for the good of the galaxy. Jedi seek to improve themselves through knowledge and training.""
The New Jedi Order.
In novels set after the events of the "original"-trilogy film series, The New Jedi Order was the restored and reformed Jedi organization, in the wake of the Great Jedi Purge and subsequent fall of the Galactic Empire. The Jedi Knights, reduced in number to only a handful, were slowly restored, primarily under the leadership of Grandmaster Luke Skywalker. Luke Skywalker abolished the traditional Master/Padawan system. He believed all Jedi should be both teachers and students; that they should both learn from and mentor each other, and not just from one Master. Luke Skywalker re-established the Jedi High Council as part of his New Jedi Order. Half of the council are made up of Jedi, while the other half consist of politicians. Following the "Yuuzhan Vong War", the Jedi withdrew their support from any one political entity and relocated to The Great Library of Ossus, where a full Jedi Council was re-established.
Canon.
The Lost Twenty.
The Lost Twenty was the name given to a group of Jedi Masters—numbering twenty in total—who left the Jedi Order throughout its history. The first twelve of these ‘Lost Twenty’ left the Jedi Order before the Third Great Schism; these twelve Masters later became "Dark Jedi" who eventually founded the first Sith Empire. In the years preceding the Clone Wars, Jedi Master Dooku left the Jedi Order as a result of differences with his fellow Jedi, becoming the twentieth Jedi Master in the history of the Order to do so. To commemorate these former Jedi, memorial statue busts were displayed at the Jedi Temple on Coruscant.
The Old Republic.
The Jedi are first introduced in the 1977 motion picture "" as an order of warrior monks who serve as "the guardians of peace and justice in the galaxy" and embrace the mystical Force. Obi-Wan Kenobi (Alec Guinness) 
explains that the Galactic Empire had all but exterminated the Jedi some twenty years before the events of the film, and seeks to train Luke Skywalker (Mark Hamill) to be the Order's last hope. Darth Vader (David Prowse/James Earl Jones) is also established as the Jedi's main enemy. By the end of the film, Luke is on the path to becoming a Jedi. In the sequel, "The Empire Strikes Back", Luke receives extensive Jedi training from the elderly (and only surviving) Jedi Master Yoda (Frank Oz), even as he learns that Vader is in fact his father, former Jedi Anakin Skywalker. The third film in the original trilogy, "Return of the Jedi", ends with Luke redeeming Vader and helping to destroy the Empire, thus fulfilling his destiny as a Jedi.
The prequel films depict the Jedi in their prime, dealing with the rising presence of the dark side of the Force and determined to fight their mortal enemies, the Sith. In ' (1999), Jedi Master Qui-Gon Jinn (Liam Neeson) discovers nine-year-old Anakin Skywalker (Jake Lloyd), whom he believes to be the "Chosen One" of a Jedi prophecy who is destined to bring balance to the Force; the boy is eventually paired with Qui-Gon's apprentice, the young Obi-Wan Kenobi (Ewan McGregor), who promises to train him. The sequel, ', establishes that the Jedi forswear all emotional attachments, including romantic love, which proves problematic when Anakin, now a young adult (Hayden Christensen), falls in love with Padmé Amidala (Natalie Portman), whom Qui-Gon Jinn and Obi-Wan Kenobi had served ten years before. In "", Palpatine (Ian McDiarmid), who is later revealed to be the Sith Lord Darth Sidious, manipulates Anakin's love for Padmé and distrust of the Jedi in order to turn him to the dark side and become his Sith apprentice, Darth Vader. Once corrupted, Vader helps Palpatine hunt down and destroy nearly all of the Jedi, leaving very few left, such as Jedi Master Yoda and Obi-Wan Kenobi.
Great Jedi Purge.
In accordance to Order 66, for crimes of "sedition and high-treason" against the Galactic Republic, the Jedi are nearly exterminated by the Clone Army under the orders of the Sith Lord Darth Sidious, with Obi-Wan Kenobi and Yoda among a handful of survivors. The first person to be issued this order was Clone Commander Cody (who until then was under the command of Jedi General Kenobi). Darth Vader led Operation: Knightfall against the Jedi Temple with the 501st Legion while the rest of the clone army around the galaxy carried out Order 66.
Fall of the New Order.
In the "sequel"-trilogy film series however, Luke's attempts to restore the Jedi Order took a turn for the worse instead when one of his apprentices, his nephew Ben, is drawn to the Dark Side by Supreme Leader Snoke and becomes Kylo Ren. He is convinced to destroy all that Luke has built, murdering the rest of Luke's apprentices in the occasion. Since Ren fell to the dark side of the force, a heartbroken Luke, as the last surviving Jedi, takes shelter in a reclusive planet in the edge of galaxy but leaves behind a map in case his friends need his help.
Organization.
Jedi Order.
In the "Star Wars" prequel trilogy saga, the Jedi Temple is located in the capital planet of Coruscant. It is the headquarters, academy, library, and monastery of the Jedi Order.
In "Revenge of the Sith", the Jedi Temple is attacked by clone troopers of the 501st Legion, led by the newly christened Darth Vader, who butcher the Jedi within and set the Temple alight. Even though the Temple was severely damaged, it was not completely destroyed, and is visible in the celebrations on Coruscant at the end of "Return of the Jedi" over twenty years later. "The New Jedi Order" indicates that the Jedi Temple on Coruscant is no longer standing but it is rebuilt as a gift to Jedi for their services and achievements during the Yuuzhan Vong invasion. The new temple is in the form of a massive pyramid made from stone and transparisteel that is designed to fit into the new look of Coruscant, though internally it is identical to the design seen in "Revenge of the Sith".
"Architects' Journal" rated the temple third on its top-ten architecture of "Star Wars" list behind the second Death Star and Jabba the Hutt’s palace on Tatooine, and ahead of Coruscant, capital city of the Old Republic. The temple is described in the article as adapting "the robust typology of Mayan temples, with durasteel cladding specified for the external stone walls for improved defensive strength" and said to be a ziggurat that "is built above a Force-nexus and has ample room for training facilities, accommodation and the Jedi Archive." The temple has five towers, the tallest is Tranquillity Spire, that are stylistically similar to the minarets surrounding the Hagia Sophia in Istanbul. "Star Wars Insider" listed it as the one hundredth greatest thing about Star Wars in its one hundredth issue special.
Jedi Archives.
The Jedi Archives, known as the The Great Library of Ossus or The Great Library of the Jedi, contained the galaxy's most priceless and ancient of texts sacred to Jedi scholars and archaeologists. The "original archives" was a massive library and training ground erected by the Jedi Order on the planet Ossus. Commissioned by the Jedi Master Odan-Urr following the Great Hyperspace War, the Draethos Jedi assembled ancient documents and scrolls detailing every detail of sentient history and ingenuity. Home to the Order after leaving their homeworld Tython in the Deep Core, the Great Library of Ossus became a symbol of the Jedi and the greatest storehouse of knowledge in the galaxy. Over the ensuing centuries Ossus thrived as the home of Jedi wisdom, encouraging visitors from across the Galactic Republic to visit and study at the archives.
After centuries of peace and growth, the Library was raided by Exar Kun, the Dark Lord of the Sith, and his Sith followers. Storming the "Chamber of Antiquities", the Sith stole the forbidden Sith artifact, known as the Dark Holocron, from Master Odan-Urr before slaying the venerable Jedi. As Kun grew stronger and gathered his forces, the Sith unleashed the devastation of Naga Sadow's ancient battleship, the Corsair, detonating the stars of the nearby Cron Cluster in 3996 BBY.
As the Jedi desperately tried to empty the Library and ship its contents offworld, Kun and his minions returned in a bid to steal the last bits of knowledge from the Jedi. Before long, the world was irradiated by the supernovas, eradicating the planet's major cities and entombing more than half of the Library's knowledge within its halls. The Library stood abandoned for the next three millennia, while the surviving Ysanna Jedi kept constant vigil in the ruined world. The Jedi who made it offworld transported the surviving artifacts to Coruscant; they were kept in the new Archives that served as a smaller version of the lost Library for centuries. These Coruscant archives would later be destroyed by Yoda, to prevent them being pilfered and exploited by Palpatine during the Great Jedi Purge. Luke Skywalker and Kam Solusar rediscovered the world and begun to study the ruins. After several decades more had past, the Jedi Order returned to the now-habitable planet and established a new academy on the world, erecting a new Archive to replace the Great Library.
The Jedi archives of the Jedi Temple in the movie "" bear a startling resemblance to the Long Room of the Trinity College Library in Dublin. This resemblance resulted in controversy as permission had not been sought to use the building's likeness in the film. However, Lucasfilm denied that the Long Room was the basis for the Jedi archives, and officials from Trinity College Library decided not to take any legal action.
Jedi Academy.
The Jedi academies were established to train Force-sensitive beings accepted into the Jedi Order in the ways of the Force. Overseen by the Council of First Knowledge, each academy was governed by an advisory Council appointed by their superiors on Coruscant. Mainstreaming the majority of teachings at the Temple, certain practices were permitted to vary from world to world. However, at all sanctioned academies, a group of Jedi Masters would instruct Initiates to the Order in the ways of the Force. The size of the school varied from world to world; some as small as to consist of a single clan of younglings, and as the large as the main academy housed within the Jedi Temple of Coruscant. Many academies had been established during the Old Sith Wars and were located in the Galactic Rim. Some were located on or near Force-wellsprings or places significant to the Order like crystal caves or nexuses of dark side energies that needed constant monitoring.
In addition to the traditional academies established by the Order, the Exploration Corps maintained several spacefaring mobile academies such as the "Chu'unthor" so that roaming the galaxy and exploring new worlds could be achieved while still teaching traditional doctrine.
By the fall of the Galactic Republic in 19 BBY, many of the ancient academies had been shut down for decades, with the Council of First Knowledge preferring the central teachings of the Coruscant Temple. After the dissolution of the Order during the Great Jedi Purge, all orthodox Temples and academies were routed and burned in order to prevent any more Jedi from learning the secrets of the Force. However, the Galactic Empire's choke hold on Force-education did not last and the Order was reformed following the conclusion of the Galactic Civil War. When Grand Master Luke Skywalker began to expand his Order from a single class to the size of the old Order, he opened several old academies, as well as new facilities to promote education and growth within the Order.
Personnel.
The exact size of the Jedi's membership and operations are never specified. However, in the Star Wars Rebels episode ""Path of the Jedi"", Kanan Jarrus stated: ""...There were around 10,000 Jedi Knights defending the galaxy. Now, we are few. But in those days, we had small outposts, temples spread throughout the stars. The Empire sought out these temples and destroyed many of them...""
Jedi Council.
The Jedi High Council (or Jedi Council) is a fictional institution from the "Star Wars" universe. It is the main ecclesiastical leadership of the Jedi Order, which is a spiritual, philosophical and paramilitary organization. The Jedi Council is made up with some of the strongest, wisest and most powerful members of the Jedi Order and are elected to lead the Jedi.
History.
The "Jedi High Council" was a group of twelve wise and powerful Jedi Masters elected to guide the Jedi Order, as well as to serve as an advisory body for the Supreme Chancellor. The earliest known Jedi Council was around 3,965 years BBY. Conversations between Bastila Shan and Carth Onasi imply that there was a Jedi Council preceding the "Jedi Civil War era" council.
The Council was made up of five lifetime members, four long-term members, and three limited-term members. The Council's organizational structure evolved from earlier assemblies of Jedi Masters and, during the Great Hyperspace War, may have been nothing more than an "inner circle" within those assemblies. The Council originally had no set meeting place, and it met irregularly on planets such as Ossus, Alderaan, and Chandrila. With the relocation of the Order from Ossus to Coruscant, the Council was established on that world.
The Jedi Council alone chose promising Jedi to join them. No outside body attempted to appoint a member to the Council against its wishes, until "", when Supreme Chancellor Palpatine appointed Anakin Skywalker to the Council.
Towards the end of the Clone Wars, in "Revenge of the Sith", the majority of the Jedi High Council (along with the other Jedi) were killed by Clone Troopers enacting Order 66 at the behest of the Supreme Chancellor who also killed Mace Windu, Kit Fisto, Agen Kolar and Saesee Tiin. Members were who killed are Plo Koon, Eeth Koth, Aayla Secura, Ki-Adi-Mundi, Stass Allie, Yaddle, Oppo Rancisis, Even Piell and Adi Gallia. Shaak Ti, Obi-Wan Kenobi, and Yoda survived the order, but Shaak died in the events of which is not canon. (In the movie "Star Wars Episode III: Revenge of the Sith", Shaak died in a deleted scene, where she was murdered by General Grievous; however, this is not considered canon.) Obi-Wan went into hiding on Tatooine after taking baby Luke Skywalker there, only to be killed by Darth Vader nineteen years later. Four years after the Battle of Yavin (4 ABY), Yoda died in the Dagobah system at 900 years old.
New Jedi Order.
In novels set after the events of the film series, Luke Skywalker re-established the Jedi High Council as part of his New Jedi Order. The most notable difference between the format of the new council and the old is that only half of the council are made up of Jedi, while the other half consisted of politicians. Following the Yuuzhan Vong War, the Jedi withdrew their support from any one political entity and relocated to Ossus, where Luke had a full Jedi Council was re-established.
There are two additional tiers exclusive to Jedi Masters who serve on the Jedi High Council:
Rank structure.
Educational hierarchy.
Members of the Order progress through four educational stages, at times referred to as levels:
Classes, specialists, and military ranks.
Opened to all who passed the Jedi Trials, and upon a Padawan's ascension to ""Knighthood"-status", a Jedi pursued higher education or vocational education and training in a field of expertise. Before the Great Jedi Purge, Knights would choose a career based on preference, personal talents and skills, and were given the opportunity to join the "Order of the Guardian", the "Order of the Consular", or "Order of the Sentinel". In addition to their specialization, in times of war the High Council could demand that the members of the Order assume military ranks in order to defend the Republic.
Notable affiliated characters.
Jedi Masters.
Yoda.
Yoda was the wise, knowledgeable, experienced, and powerful Grand Master of the Jedi Order, and a Jedi Master on the Jedi Council. He was a high-ranking general of Clone Troopers in the Clone Wars, and later trains Luke Skywalker to fight against the Galactic Empire, and further carrying on in his after-death existence as a Jedi Ghost using his influence to help Jedi that live after his passing.
Mace Windu.
Mace Windu was a male human Jedi Weapons Master of the High Council and one of the last members of the order's upper echelons before the fall of the Galactic Republic. Windu was one of the most powerful Jedi and possibly the greatest swordsman of his time, able to defeat Darth Sidious in lightsaber combat, a feat which even Yoda could not achieve. Windu had the unique talent of seeing "shatterpoints", or faultlines in the Force that could affect the destinies of certain individuals, and indeed the galaxy itself.
Qui-Gon Jinn.
Originally mentored by Count Dooku, Qui-Gon Jinn was a wise and powerful male human Jedi Master and the teacher of Obi-Wan Kenobi. Unlike other, more conservative Jedi, he values living in the moment as the best way to embrace the Force. While other Jedi respect him highly, they are frequently puzzled by his unorthodox beliefs and ultimately deny him a seat on the Jedi Council despite being among the most powerful of the Jedi.
Obi-Wan Kenobi.
Obi-Wan Kenobi was a male human Jedi Master who initiated Anakin and Luke Skywalker to the Jedi arts and served as a central character during the events of the Clone Wars. Obi-Wan proved himself an adept strategist, dueliest, and spy, as his leadership style heavily favored subterfuge and misdirection while commanding clone troopers, or wielding the Force.
Luke Skywalker.
Luke Skywalker was a male human Grand Master of the Jedi who founded the New Jedi Order and the protagonist of the "Star Wars" original trilogy. As the last Padawan of Obi-Wan "Ben" Kenobi, he became an important figure in the Rebel Alliance's struggle against the Galactic Empire. Luke is heir to a family deeply powerful in the Force, the twin brother of Rebellion leader Princess Leia of the planet Alderaan, the son of former Queen of Naboo and Republic Senator Padmé Amidala and fallen Jedi turned Sith Lord Darth Vader (Anakin Skywalker), the maternal uncle of Ben, the maternal uncle of Jacen and Jaina Solo and the ancestor of Cade Skywalker.
Ki-Adi-Mundi.
Ki-Adi-Mundi was a Cerean Jedi Master serving as a member of the Jedi Council during the last years of the Galactic Republic. He was among the Jedi participating in the Battle of Geonosis. He became a Jedi General in the Clone Wars and took part in such conflicts as the Second Battle of Geonosis. Unlike most Jedi he was allowed to marry a female and start a family even while obeying the Jedi code, mainly because his species have a small reproduction rate. While he was leading a contingent of Galactic Marines on Mygeeto, he was executed by his own forces under Order 66.
Plo Koon.
Plo Koon was a Kel Dor Jedi Master serving as a member of the Jedi High Council during the last years of the Galactic Republic. He frequently undertook dangerous missions. He was the Jedi who found Ahsoka Tano, with whom he would, from that moment, share a special bond. He was among the Jedi who participated in the Battle of Geonosis. He became a Jedi General in the Clone Wars and took part in such conflicts as the Battle of Abregado and the Battle of Felucia. While he was leading a squadron of ARC-170 starfighters on Cato Neimoidia, he was ruthlessly destroyed by his own troops after Order 66 was executed by Emperor Palpatine.
Kit Fisto.
Kit Fisto was a Nautolan Jedi Master during the last years of the Galactic Republic. He was among the Jedi who participated in the Battle of Geonosis and was one of the few survivors out of the 200 who traveled there. He became a Jedi General in the Clone Wars and took part in such conflicts as the Mission to Vassek's third moon and the Battle of Mon Cala. He was also involved in a mission with his former Padawan Nahdar Vebb to capture the escaped prisoner, Nute Gunray—however, the two ended up facing the notorious Jedi hunter General Grievous inside his lair. While Vebb perished at the hands of the General, Fisto very nearly bested him in combat, and escaped with his life, but without Gunray, ultimately making the mission a failure. At some point during the Clone Wars, Fisto accepted a seat on the Jedi Council. He also had a romantic-relationship with Aayla Secura, but due to the Jedi-code denying romance the two never exactly dated. While he was at the Jedi Temple, Anakin Skywalker revealed to Mace Windu that Chancellor Palpatine was a Sith Lord. Windu and Fisto, alongside Jedi Masters Agen Kolar and Saesee Tiin flew to Palpatine's office to arrest him. In the following duel, after Kolar and Tiin had perished, Fisto was killed by the Chancellor.
Jedi Padawans.
Ahsoka Tano.
Ahsoka Tano is a female Togrutan. During the Clone Wars, she was taken as a Padawan by then Mentor, Anakin Skywalker, and the two developed a big brother/kid sister relationship which ultimately foreshadowed Anakin's tragic future. She developed uncanny courage, tactical skills, leadership and trust, which was proven after she refused to abaddon other Padawans kidnapped by Trandoshan slavers. In "" fifth season, she was framed for bombing the Temple and murdering a suspect-in-question by her friend Barriss Offee. She was then expelled from the Jedi Order and turned over to the Republic for trial. After proven innocent, the Jedi Council personally invited her to rejoin the Order, offering to reinstate her to "Jedi Knighthood", but she declined. Ahsoka understood that the Council didn't believe in her; stating that she needed to follow her own path, without the Order and Anakin, she left the Temple in tears. She resurfaced 14 years later in "Star Wars Rebels", where she is revealed to be the mysterious "Fulcrum", and allied her rebel cell, Phoenix Squadron, with the crew of the "Ghost". In "Shroud of Darkness", she is forced to accept Darth Vader is her corrupted former master. In "Twilight of the Apprentice," Ahsoka takes Ezra and Kanan to Malachor to search for Sith knowledge. She later sacrifices herself to buy Ezra and Kanan some time to get away from Darth Vader, forcing him to fight her as a temple collapses on top of them; her ultimate fate is left unknown.
Kanan Jarrus.
Kanan Jarrus (born Caleb Dume) was a male human taken as a Padawan by falsely-rumoured fallen Jedi Master Depa Billaba during the Clone Wars and a character of "Star Wars Rebels". Surviving the Great Jedi Purge at age 14, he exiled himself to the Imperial-controlled planet Lothal and became the "de facto" leader of the crew of the "Ghost". Though still a padawan, he has taken Lothal street urchin Ezra Bridger as his Padawan. When he found himself in Lothal's Jedi temple, he was put through a trial and ended up passing the test, becoming a Jedi Knight in the process.
Equipment.
Within the "Star Wars" universe, the Jedi are usually portrayed wearing simple robes and carrying specialized field gear for their missions. Their philosophical lifestyles mirror those of real-world religious vows and evangelical counsels, as their personal possessions are provided exclusively by the Jedi Order, and are only meant to allow self-sufficiency.
Weapons.
The most notable instrument wielded by a Jedi is the Lightsaber. Both Jedi and Sith use lightsabers as their main weapon. The Jedi's lightsabers emit cool colors, usually blue or green blades (sometimes yellow, or purple, as seen in the case of Mace Windu), while the Sith emit warm colors. Lightsabers can be of many different colors depending on the crystal fixture. Although a Jedi's class used to be defined by the color of the lightsaber, by the events of the theatrical trilogy films, most Jedi choose to make their lightsaber any color they see fit. Most Jedi use naturally formed crystals, whereas Sith tend to use synthetic crystals, which are usually red in color.
Vehicles.
Eta-2 Actis Jedi Interceptors first appeared in "Revenge of the Sith". Delta-7B Aethersprite Jedi starfighters appear in ' and '. In "Attack of the Clones", Obi-Wan Kenobi travels via Jedi starfighter to Kamino to investigate the attempted assassination of Padmé Amidala; he also flies a Jedi starfighter to Geonosis in an attempt to track down the bounty hunter Jango Fett. Lacking a hyperdrive, the starfighter relies on an external sled to propel it through hyperspace. Kenobi and Anakin Skywalker (Hayden Christensen) fly updated Jedi starfighters (called Jedi Interceptors) in the opening sequence of "Revenge of the Sith". Later, Plo Koon (Matt Sloan) flies an "Revenge of the Sith"-era starfighter when he is shot down by clone troopers carrying out Emperor Palpatine's (Ian McDiarmid) Order 66.
The Jedi starfighter's triangular shape in "Attack of the Clones" stems from the shape of Imperial Star Destroyers in the original "Star Wars" trilogy. Industrial Light & Magic designer Doug Chiang identified the Jedi starfighter as one of the first designs that bridges the aesthetic between the prequel and original trilogies. Chiang noted that viewers' familiarity with the Star Destroyer's appearance and Imperial affiliation gives added symbolism to the Jedi craft's appearance and foreshadows the Empire's rise to power. The starfighter seen in "Revenge of the Sith" is a cross between the previous film's vessel and the Empire's TIE fighters from the original trilogy. Hasbro's expanding wings in the "Attack of the Clones" Jedi starfighter toy inspired the opening wings in the "Revenge of the Sith" vessel. The starfighter in the "Revenge of the Sith" is called a Jedi Interceptor Starfighter.
Dark Jedi and the Sith.
Not every "Dark Side"-user is a Sith; nor is every "Light Side"-user is a Jedi. Dark Jedi is the unofficial name given in the "Star Wars" universe to antihero fictional characters attuned to the Force and adept in its dark side. The concept of "Dark Jedi" is not endorsed anywhere within the movie trilogies. They exist by that name only in the Star Wars Expanded Universe, including video games such as ' and the ' series; the term is never used in any of the seven "Star Wars" films.
Because the term Sith was never spoken in the original trilogy (although Darth Vader was described as "Lord of the Sith" in the published screenplay), early Expanded Universe products usually considered the "evil Jedi," those who joined the dark side of the Force, as "Dark Jedi." In his novel series "The Thrawn Trilogy", author Timothy Zahn labeled Sith Lord Darth Vader and Emperor Palpatine as Dark Jedi, and the term "Sith" was never mentioned in the series until later reprints of the novels.
In popular culture.
The United States Army had a group of officers in the early 1980s who promoted maneuver warfare tactics, and who were derisively referred to as Jedi by more conventional officers who were satisfied with attrition tactics and methods.
Media.
Jedi have made their way into certain areas of pop culture, such as in:
"Weird Al" Yankovic's song "The Saga Begins", a parody of "American Pie". The Jedi influence begins with the lyrics from "American Pie", "This'll be the day that I die" changed to "Soon I'm gonna be a Jedi".
The 2009 film "The Men Who Stare at Goats" stars Ewan McGregor as a reporter named Bob Wilton who follows a former soldier (George Clooney) who claimed to be a "Jedi warrior", a nickname for psychic spies in the US military. McGregor previously starred as Jedi Knight, and later Jedi Master, Obi-Wan Kenobi in the prequel trilogy.
Religion.
One of the enduring influences the "Star Wars" saga has had in popular culture is the idea of the fictional Jedi values being interpreted as a modern philosophical path or religion, spawning various movements such as the Jediism (religious) and the Jedi census phenomenon.
On April 6, 2015, thousands of Turkish students raised their voices in campaigns to build Jedi and Buddhist temples at their universities, after a series of mosques were constructed on their campuses by rectors who stressed “huge demand.” A number of Dokuz Eylül University students in the western province of Izmir have demanded a Jedi temple to be built on their campus. ""There are less and less Jedi left on the Earth... the nearest temple billions of light years away"," the petition says. It adds that ""uneducated Padawan" are moving to the dark side... To recruit new Jedi and to bring balance to the Force, we want a Jedi temple",” said the petition that received more than 6,000 signatures on change.org, referring to the famed knights of the fictional Star Wars universe. The page on Change.org also features a still of Jedi Grand Master Yoda from "Star Wars: Episode II -- Attack of the Clones" teaching young Jedis how to use a light saber. The petition was started by Akin Cagatay Caliskan, an 18-year-old computer science student from Ankara: "We want freedom of worship. There are mosques everywhere, but no Jedi temple!" Caliskan says he is surprised by the impact his petition has made: ""I did not expect so many supporters. I thought maybe it might (have) 100.""

</doc>
<doc id="16380" url="https://en.wikipedia.org/wiki?curid=16380" title="James Tobin">
James Tobin

James Tobin (March 5, 1918 – March 11, 2002) was an American economist who served on the Council of Economic Advisers and the Board of Governors of the Federal Reserve System, and taught at Harvard and Yale Universities. He developed the ideas of Keynesian economics, and advocated government intervention to stabilize output and avoid recessions. His academic work included pioneering contributions to the study of investment, monetary and fiscal policy and financial markets. He also proposed an econometric model for censored endogenous variables, the well-known "Tobit model". Tobin received the Nobel Memorial Prize in Economic Sciences in 1981.
Outside of academia, Tobin was widely known for his suggestion of a tax on foreign exchange transactions, now known as the "Tobin tax". This was designed to reduce speculation in the international currency markets, which he saw as dangerous and unproductive.
Life and career.
Early life.
Tobin was born on March 5, 1918 in Champaign, Illinois. His father was Louis Michael Tobin, (b. 1879) a journalist working at the University of Illinois at Urbana-Champaign. His father had fought in World War I, was a member of the first Greek organization at Illinois (Delta Tau Delta fraternity Beta Upsilon chapter), and was credited as the inventor of 'Homecoming'. His mother, Margaret Edgerton Tobin (b. 1893), was a social worker. Tobin followed primary school at the University Laboratory High School of Urbana, Illinois, a laboratory school in the university's campus.
In 1935, on his father's advice, Tobin took the entrance exams for Harvard University. Despite no special preparation for the exams, he passed and was admitted with a national scholarship from the university. Here among the faculty, he would meet Joseph Schumpeter, Alvin Hansen, Gottfried Haberler, Sumner Slichter, Seymour Harris, Edward Mason, Edward Chamberlin, John Kenneth Galbraith and Wassily Leontief. During his studies he first read Keynes' "The General Theory of Employment, Interest and Money", published in 1936. Tobin graduated summa cum laude in 1939 with a thesis centered on a critical analysis of Keynes' mechanism for introducing equilibrium "involuntary" unemployment. His first published article, in 1941, was based on this senior's thesis.
Tobin immediately started graduate studies, also at Harvard, earning his M.A. degree in 1940. His fellow graduate students included Paul Samuelson, Lloyd Metzler, Abram Bergson, Richard Musgrave and Richard M. Goodwin. In 1941, he interrupted graduate studies to work for the Office of Price Administration and Civilian Supply and the War Production Board in Washington, D.C.. The next year, after the United States entered World War II, he enlisted in the US Navy, spending the war as an officer on a destroyer. At the end of the war he returned to Harvard and resumed studies, receiving his Ph.D. in 1947 with a thesis on the consumption function written under the supervision of Joseph Schumpeter. In 1947 Tobin was elected a Junior Fellow of Harvard's Society of Fellows, which allowed him the freedom and funding to spend the next three years studying and doing research.
Academic activity and consultancy.
In 1950 Tobin moved to Yale University, where he remained for the rest of his career. He joined the Cowles Foundation, which moved to Yale in 1955, also serving as its president between 1955–1961 and 1964-1965. His main research interest was to provide microfoundations to Keynesian economics, with a special focus on monetary economics. One of his frequent collaborators was his Yale colleague William Brainard. In 1957 Tobin was appointed Sterling Professor of Economics at Yale.
Besides teaching and research, Tobin was also strongly involved in the public life, writing on current economic issues and serving as an economic expert and policy consultant. During 1961–62, he served as a member of John F. Kennedy's Council of Economic Advisors, under the chairman Walter Heller, then acted as a consultant between 1962–68. Here, in close collaboration with Arthur Okun, Robert Solow and Kenneth Arrow, he helped design the Keynesian economic policy implemented by the Kennedy administration. Tobin also served for several terms as a member of the Board of Governors of Federal Reserve System Academic Consultants and as a consultant of the US Treasury Department.
Tobin was awarded the John Bates Clark Medal in 1955 and, in 1981, the Nobel Memorial Prize in Economics. He was a fellow of several professional associations, holding the position of president of the American Economic Association in 1971.
In 1972 Tobin, along with fellow Yale economics professor William Nordhaus, published "Is Growth Obsolete?", an article that introduced the Measure of Economic Welfare as the first model for economic sustainability assessment, and economic sustainability measurement.
In 1988 Tobin formally retired from Yale, but continued to deliver some lectures as Professor Emeritus and continued to write. He died on March 11, 2002, in New Haven, Connecticut.
Tobin was a trustee of Economists for Peace and Security.
Personal life.
James Tobin married Elizabeth Fay Ringo, a former M.I.T. student of Paul Samuelson, on September 14, 1946. They had four children: Margaret Ringo (born in 1948), Louis Michael (born in 1951), Hugh Ringo (born in 1953) and Roger Gill (born in 1956). In late June, 2009, the family announced via a private email that Tobin's wife had died at the age of 90.
Legacy.
In August 2009 in a roundtable interview in Prospect magazine, Adair Turner supported the idea of new global taxes on financial transactions, warning that the “swollen” financial sector paying excessive salaries had grown too big for society. Lord Turner’s suggestion that a “Tobin tax” – named after James Tobin – should be considered for financial transactions made headlines around the world.
Tobin's Tobit model of regression with censored endogenous variables (Tobin 1958a) is a standard econometric technique. His "q" theory of investment (Tobin 1969), the Baumol-Tobin model of the transactions demand for money (Tobin 1956), and his model of liquidity preference as behavior toward risk (the asset demand for money) (Tobin 1958b) are all staples of economics textbooks.
In his 1958 article Tobin also led the way in showing how to deal with utility maximization under uncertainty with an infinite number of possible states. As Palda explains "One way to get out of the mess of figuring out asset prices using a model of maximizing the expected utility of investing in stocks is to make assumptions about either preferences or the probabilities of the different possible states of the world. Nobellist James Tobin (1958) took this line and discovered that in some cases you do not need to worry about the utility of income in thousands of states, and the attached probabilities, to solve the consumer’s choice on how to spread income among states. When preferences contain only a linear and a squared term (a case of diminishing returns) or the probabilities of different stock returns follow a normal distribution (an equation that contains a linear and squared terms as parameters), a simple formulation of a person’s investment choices becomes possible. Under Tobin’s assumptions we can reformulate the person’s decision problem as being one of trading off risk and expected return. Risk, or more precisely the variance of your investment portfolio creates spread in the returns you expect. People are willing to assume more risk only if compensated by a higher level of expected return. One can thus think of a tradeoff people are willing to make between risk and expected return. They invest in risky assets to the point at which their willingness to trade off risk and return is equal to the rate at which they able to trade them off. It is difficult to exaggerate how brilliant is the simplification of the investment problem that flows from these assumptions. Instead of worrying about the investor’s optimization problem in potentially millions of possible states of the world, one need only worry about how the investor can trade off risk and return in the stock market."

</doc>
<doc id="16382" url="https://en.wikipedia.org/wiki?curid=16382" title="Julian Lennon">
Julian Lennon

John Charles Julian Lennon (born 8 April 1963) is a British musician and photographer. He is the first child of John Lennon with his first wife, Cynthia. The Beatles' manager, Brian Epstein, was his godfather. He has a younger half-brother, Sean Lennon. Lennon was named after his paternal grandmother, Julia Lennon.
Lennon was the direct inspiration for three Beatles' songs: "Lucy in the Sky with Diamonds", "Hey Jude" and "Good Night". He is devoted to philanthropic endeavors, most notably his own White Feather Foundation and the Whaledreamers Organization, both of which promote the co-existence of all species and the health and well-being of the Earth.
Early life and relationship with his father.
Julian Lennon was born in Liverpool. Initially, the fact that John Lennon was married and had a child was concealed from the public, in keeping with the conventional wisdom of the era that female teenage fans would not be as enamoured of married male pop stars. Lennon inspired one of his father's most famous songs, "Lucy in the Sky with Diamonds", whose lyrics describe a picture the boy had drawn, a watercolour painting of his friend Lucy O'Donnell from nursery school surrounded by stars. Another composition of his father inspired by him was the lullaby "Good Night", the closing song of the "White Album". In 1967, he attended the set of the Beatles' film "Magical Mystery Tour".
Following his father's infidelity with Yoko Ono, Lennon's parents divorced when he was five. Paul McCartney wrote "Hey Jude" to console him over the divorce; originally called "Hey Jules", McCartney changed the name because he thought that "Jude" was an easier name to sing. After his parents' divorce, Lennon had almost no contact with his father until the early 1970s when, at the request of his father's then short-term girlfriend, May Pang (Yoko Ono and Lennon had temporarily separated), he began to visit his father regularly. John Lennon bought him a Gibson Les Paul guitar and a drum machine for Christmas 1973, and encouraged his interest in music by showing him some chords.
Following his father's murder, Lennon voiced anger and resentment toward him, saying, I've never really wanted to know the truth about how dad was with me. There was some very negative stuff talked about me ... like when he said I'd come out of a whiskey bottle on a Saturday night. Stuff like that. You think, where's the love in that? Paul and I used to hang about quite a bit ... more than Dad and I did. We had a great friendship going and there seems to be far more pictures of me and Paul playing together at that age than there are pictures of me and my dad.
Lennon was also irked by hearing his father's peace-loving stance perpetually celebrated. He told the "London Telegraph", "I have to say that, from my point of view, I felt he was a hypocrite", he said, "Dad could talk about peace and love out loud to the world but he could never show it to the people who supposedly meant the most to him: his wife and son. How can you talk about peace and love and have a family in bits and pieces—no communication, adultery, divorce? You can't do it, not if you're being true and honest with yourself."
Lennon was excluded from his father's will. However, a trust of £100,000 was created by his father to be shared between all of his children (both Julian and Sean). Julian sued his father's estate and in 1996 reached a settlement agreement reportedly worth £20 million. By 2009, Lennon's feelings toward his father had mellowed. Recalling his renewed relationship with his father in the mid-1970s, he said, Dad and I got on a great deal better then. We had a lot of fun, laughed a lot and had a great time in general when he was with May Pang. My memories of that time with Dad and May are very clear — they were the happiest time I can remember with them.
In 2007, Lennon sold a "significant" share of his stake in his father's catalogue of work in exchange for an undisclosed sum with an agreement that the purchasing company, Primary Wave, would market and promote his new material. The stake entitles Primary Wave to a portion of all royalties on the catalogue.
Education.
Lennon was educated at Ruthin School, a boarding independent school near the town of Ruthin in Denbighshire in North Wales.
Career.
Music career.
Lennon made his musical debut at age 11 on his father's album "Walls and Bridges" playing drums on "Ya-Ya", later saying, "Dad, had I known you were going to put it on the album, I would've played much better!" In the 1980s, he “parlayed a remarkable vocal similarity to his father into a successful singing career.” 
Lennon enjoyed immediate success with his debut 1984 album "Valotte", produced by Phil Ramone, nominated for a Grammy Award for Best New Artist in 1985, spawning two top ten hits, the title track "Valotte" and "Too Late for Goodbyes". Lennon promoted the album with music videos for the two hits made by movie director Sam Peckinpah and producer Martin Lewis. The song "Valotte" has remained a staple on adult contemporary radio stations since its release.
After the release Paul McCartney sent him a telegram wishing him good luck. Later that year the two met backstage at the New York studios of the television show "Friday Night Videos".
His second album, 1986's "The Secret Value of Daydreaming", was panned by critics, but reached No. 32 on the "Billboard" magazine's album chart, and produced the single "Stick Around", which was his first No. 1 single on the U.S. Album Rock Tracks chart.
He recorded the song "Because", previously recorded by The Dave Clark Five, in the UK for Clark's 1986 musical, "Time". Lennon never reached the same level of success in the U.S. post-"Valotte", but he hit No. 5 in Australia with the 1989 single "Now You're In Heaven", which also gave him his second No. 1 hit on the Album Rock Tracks chart in the USA.
On 1 April 1987, Julian Lennon appeared as The Baker in Mike Batt's musical "The Hunting of the Snark" (based on Lewis Carroll's poem). The all-star lineup included Roger Daltrey, Justin Hayward and Billy Connolly, with John Hurt as the Narrator. The performance, a musical benefit at London's Royal Albert Hall in aid of the deaf, was attended by Prince Andrew's then wife, the Duchess of York. 
In 1991, George Harrison played on Lennon's album "Help Yourself" but was not directly credited. A song off the album, "Saltwater", reached No. 6 in the UK and topped the Australian singles charts for four weeks. Also during this time he contributed a cover of the Rolling Stones' "Ruby Tuesday" to the soundtrack of the television series "The Wonder Years".
By the end of the year, Lennon left the music business for several years. He followed his interests in cooking, sailing, and sculpting during his leave from the music industry. After he began his performing career there was occasionally unfounded media speculation that Julian would undertake performances with Paul McCartney, George Harrison and Ringo Starr. However, in the "Beatles Anthology" series in 1996, the three surviving Beatles confirmed there was never an idea of having Julian sit in for his father as part of a Beatles reunion, with McCartney saying, "Why would we want to subject him to all of this?"
In May 1998, Lennon released the album "Photograph Smile" to little commercial success. Music critic Stephen Thomas Erlewine praised the album as “well-crafted and melodic,” concluding it to be “the kind of music that would receive greater praise if it weren't made by the son of a Beatle.” In 2002, he recorded a version of the Beatles' classic "When I'm Sixty-Four", a song from "Sgt. Pepper's Lonely Hearts Club Band", for an Allstate Insurance commercial.
In 2006 he ventured into Internet businesses, including MyStore.com with Todd Meagher and Bebo founder Michael Birch. In 2009 Lennon created a new partnership with Todd Meagher and Michael Birch called theRevolution, LLC. Through this company, Lennon released a tribute song and EP, "Lucy", honoring the memory of Lucy Vodden (nee O'Donnell), the little girl who inspired the song "Lucy in the Sky with Diamonds", with 50 percent of the proceeds going to fund Lupus research.
In October 2011, Lennon released a new album called "Everything Changes."
In 2012 Lennon worked with music film director Dick Carruthers on the feature length video documentary "Through The Picture Window", which followed Lennon's journey in the making of his album "Everything Changes" and includes interviews with Steven Tyler, Bono and Paul Buchanan from The Blue Nile. "Through The Picture Window" was also released as an App in all formats with bespoke videos for all 14 tracks from the album.
Film.
Lennon's first-ever tour in early 1985 was documented as part of the film "Stand By Me: A Portrait Of Julian Lennon" — a film profile started by Sam Peckinpah, but completed by Martin Lewis after Peckinpah's death. Lennon has appeared in several other films including "The Rolling Stones Rock and Roll Circus" (1996, but shot in 1968), "Cannes Man" (1996), "" (1988), "Chuck Berry: Hail! Hail! Rock 'n' Roll" (1987) and a cameo in "Leaving Las Vegas" (1995) as a bartender. Julian provided the voice for the title role in the animated film "David Copperfield". He was also the voice of the main character Toby the Teapot in the animated special "The Real Story of I'm a Little Teapot" (1990).
Julian Lennon is also the producer of the documentary called "WhaleDreamers" about an aboriginal tribe in Australia and its special relationship to whales. It also touches on many environmental issues. This film has received many awards and was shown at the 2007 Cannes Film Festival.
Photography.
After photographing his half-brother Sean's music tour in 2007, Lennon took up a serious interest in photography.
On 17 September 2010, Lennon opened an exhibition of 35 photographs called "Timeless: The Photography of Julian Lennon" with help from long-time friend and fellow photographer Timothy White. Originally scheduled to run 17 September through 10 October, the Morrison Hotel Gallery extended it a week to end 17 October. The photographs include shots of his half-brother Sean, actress Kate Hudson, and U2 frontman Bono.
On 3 October 2010, CBS "Sunday Morning" aired an in-depth interview with Lennon that covered much of his life, including his relationship with his parents and sibling, his career, and his experience growing up as the son of one of the world's most famous celebrities.
Lennon's "Horizons" series is featured at the Emmanuel Fremin Gallery, NYC, 12 March – 2 May 2015.
Book.
Shortly after the death of his father, Lennon began collecting Beatles memorabilia. In 2010, he published a book of his collection, "Beatles Memorabilia: The Julian Lennon Collection".
Charity work.
Lennon founded The White Feather Foundation in 2009. Its mission "embraces environmental and humanitarian issues and in conjunction with partners from around the world helps to raise funds for the betterment of all life, and to honour those who have truly made a difference." Its name came from a conversation Lennon once had with his father. "Dad once said to me that should he pass away, if there was some way of letting me know he was going to be OK – that we were all going to be OK – the message would come to me in the form of a white feather. ... the white feather has always represented peace to me."
Personal life.
Lennon has been quoted as having a "cordial" relationship with Ono while getting along very well with her son, his half-brother Sean. He and Sean spent time together on Sean's tour in 2007. Julian saw Sean in live for the first time in Paris on 12 November 2006 at La Boule Noire. In commemoration of John Lennon’s 70th birthday and as a statement for peace, Lennon and his mother, Cynthia, unveiled the John Lennon Peace Monument in his home town of Liverpool, England, on 9 October 2010.
Lennon has been engaged twice – to socialite Lucy Bayliss and actress Olivia d'Abo - but both engagements were called off. He now resides near Monaco.
Lennon remains friends with his father's former bandmate, McCartney, though they experienced a public falling out in 2011 when Lennon was not invited to McCartney's wedding to Nancy Shevell.

</doc>
<doc id="16383" url="https://en.wikipedia.org/wiki?curid=16383" title="FIFA World Cup Trophy">
FIFA World Cup Trophy

The World Cup is a gold trophy that is awarded to the winners of the FIFA World Cup association football tournament. Since the advent of the World Cup in 1930, two trophies have been used: the Jules Rimet Trophy from 1930 to 1970, and the FIFA World Cup Trophy from 1974 to the present-day.
The first trophy, originally named "Victory", but later renamed in honour of former FIFA president Jules Rimet, was made of gold plated sterling silver and lapis lazuli and depicted Nike, the Greek goddess of victory. Brazil won the trophy outright in 1970, prompting the commissioning of a replacement. The Jules Rimet Trophy was stolen in 1983 and never recovered.
The subsequent trophy, called the "FIFA World Cup Trophy", was introduced in 1974. Made of 18 karat gold with a malachite base, it stands 36.8 centimeters high and weighs 6.1 kilograms. The trophy was made by Stabilimento Artistico Bertoni company in Italy. It depicts two human figures holding up the Earth. The current holder of the trophy is Germany, winner of the 2014 World Cup.
Jules Rimet Trophy.
The Jules Rimet Trophy was the original prize for winning the Football World Cup. Originally called "Victory", but generally known simply as the "World Cup" or "Coupe du Monde", it was officially renamed in 1946 to honour the FIFA President Jules Rimet who in 1929 passed a vote to initiate the competition. It was designed by Abel Lafleur and made of gold-plated sterling silver on a white/yellow marble base. In 1954 this base was replaced with a high base made of lapis lazuli. It stood 35 centimetres (14 in) high and weighed 3.8 kilograms (8.4 lb). It comprised a decagonal cup, supported by a winged figure representing Nike, the ancient Greek goddess of victory. The Jules Rimet Trophy was taken to Uruguay for the first FIFA World Cup aboard the "Conte Verde", which set sail from Villefranche-sur-Mer, just southeast of Nice, on 21 June 1930. This was the same ship that carried Jules Rimet and the footballers representing France, Romania and Belgium who were participating in the tournament that year. The first team to be awarded the trophy was Uruguay, the winners of the 1930 World Cup.
During World War II, the trophy was held by 1938 winners Italy. Ottorino Barassi, the Italian vice-president of FIFA and president of FIGC, secretly transported the trophy from a bank in Rome and hid it in a shoe-box under his bed to prevent Adolf Hitler and the Nazis from taking it. The 1958 FIFA World Cup in Sweden marked the beginning of a tradition regarding the trophy. As Brazilian captain Hilderaldo Bellini heard photographer requests for a better view of the Jules Rimet Trophy, he lifted it up in the air. Every Cup-winning captain ever since has repeated the gesture.
On 20 March 1966, four months before the 1966 FIFA World Cup in England, the trophy was stolen during a public exhibition at Westminster Central Hall. The trophy was found just seven days later wrapped in newspaper at the bottom of a suburban garden hedge in Upper Norwood, South London, by a dog named Pickles.
As a security measure, The Football Association secretly manufactured a replica of the trophy for use in exhibitions rather than the original. This replica was used on subsequent occasions up until 1970 when the original trophy had to be handed back to FIFA. Since FIFA had explicitly denied the FA permission to create a replica, the replica also had to disappear from public view and was for many years kept under its creator's bed. This replica was eventually sold at an auction in 1997 for £254,500, when it was purchased by FIFA. The high auction price, ten times the reserve price of £20,000–£30,000, was led by speculation that the auctioned trophy was not the replica trophy but the original itself. Subsequent testing by FIFA, however, confirmed the auctioned trophy was indeed a replica and FIFA soon afterwards arranged for the replica to be lent for display at the English National Football Museum, which was then based in Preston but is now in Manchester.
The Brazilian team won the tournament for the third time in 1970, allowing them to keep the real trophy in perpetuity, as had been stipulated by Jules Rimet in 1930. It was put on display at the Brazilian Football Confederation headquarters in Rio de Janeiro in a cabinet with a front of bullet-proof glass.
On 19 December 1983, the wooden rear of the cabinet was pried open with a crowbar and the cup was stolen again. Four men were tried and convicted in absentia for the crime. The trophy has never been recovered, and it is widely believed to have been melted down and sold. Only one piece of the Jules Rimet Trophy has been found, the original base which FIFA had kept in a basement of the federation's Zurich headquarters prior to 2015.
The Confederation commissioned a replica of their own, made by Eastman Kodak, using 1.8 kg (3.97 lb) of gold. This replica was presented to Brazilian military president João Figueiredo in 1984.
The trophy was the subject of a 2014 documentary "Mysteries of the Rimet Trophy" shown as part of ESPN's series during the 2014 World Cup.
New Trophy.
A replacement trophy was commissioned by FIFA for the 1974 World Cup. Fifty-three submissions were received from sculptors in seven countries. Italian artist Silvio Gazzaniga was awarded the commission. The trophy stands 36.5 centimetres (14.4 inches) tall and is made of 5 kg (11 lb) of 18 carat (75%) gold with a base (13 centimetres [5.1 inches] in diameter) containing two layers of malachite. It has been asserted by Sir Martyn Poliakoff that the trophy is hollow; if, as is claimed, it were solid, the trophy would weigh 70–80 kg and would be too heavy to lift. Produced by "Bertoni, Milano" in Paderno Dugnano, it weighs 6.175 kg (13.6 lb) in total and depicts two human figures holding up the Earth. Gazzaniga described the trophy thus, "The lines spring out from the base, rising in spirals, stretching out to receive the world. From the remarkable dynamic tensions of the compact body of the sculpture rise the figures of two athletes at the stirring moment of victory."
The trophy has the engraving "FIFA World Cup" on its base. After the 1994 FIFA World Cup a plate was added to the bottom side of the trophy on which the names of winning countries are engraved, and are therefore not visible when the trophy is standing upright. The inscriptions state the year in figures and the name of the winning nation in its national language; for example, "1974 Deutschland" or "1994 Brasil". In 2010, however, the name of the winning nation was engraved as "2010 Spain", in English, not in Spanish. As of 2014, ten winners have been engraved on the base. The plate is replaced and rearranged the winners of the trophy into a spiral to accommodate future winners, with Spain this time written in Spanish (España). FIFA's regulations now state that the trophy, unlike its predecessor, cannot be won outright: the winners of the tournament receive a Bronze replica which is gold plated rather than solid gold. Germany was the first nation to win the new trophy for the third time in 2014.
Winners.
Jules Rimet Trophy
FIFA World Cup Trophy

</doc>
<doc id="16384" url="https://en.wikipedia.org/wiki?curid=16384" title="John Belushi">
John Belushi

John Adam Belushi (; January 24, 1949 – March 5, 1982) was an American comedian, actor, and musician. He is best known for his "intense energy and raucous attitude" which he displayed as one of the original cast members of the NBC sketch comedy show "Saturday Night Live", in his role in the 1978 film "Animal House" and in his recordings and performances as one of The Blues Brothers.
During his career he had a close personal and artistic partnership with fellow "SNL" actor and writer Dan Aykroyd whom he met while they were both working at Chicago's Second City comedy club.
Belushi died on the morning of March 5, 1982 at the age of 33 in Hollywood, California, specifically at the Chateau Marmont, after being injected with a mixture of cocaine and heroin, known as a "speedball", which led to combined drug intoxication. He was posthumously honored with a star on the Hollywood Walk of Fame, on April 1, 2004.
Early life.
Belushi's mother, Agnes Demetri (Samaras), was the daughter of Albanian immigrants, and his father, Adam Anastos Belushi, was an Albanian immigrant from Qytezë. Born in Humboldt Park, a neighborhood on the West Side of Chicago, Illinois, John was raised in Wheaton, a suburb west of Chicago, along with his three siblings: younger brothers Billy and Jim, and sister Marian. Belushi was raised in the Albanian Orthodox Church and attended Wheaton Central High School, where he met his future wife, Judith Jacklin.
Career.
Stand up.
After starting his own comedy troupe in Chicago, The West Compass Trio (named after the improvisational cabaret revue Compass Players active from 1955-1958 in Chicago), with Tino Insana and Steve Beshekas, in 1971 Belushi was asked to join the cast of The Second City. At Second City, he met and began working with Harold Ramis. He was subsequently cast with Chevy Chase and Christopher Guest in "National Lampoon Lemmings", a parody of Woodstock, which played Off-Broadway in 1972.
In 1973, Belushi and Judith Jacklin moved together to New York where Belushi worked for National Lampoon magazine's "The National Lampoon Radio Hour", a half-hour syndicated comedy program where he was a writer, director and actor. During a trip to Toronto to check the local Second City cast in 1974, he met Dan Aykroyd. Jacklin became an associate producer for the show, and she and Belushi were married on December 31, 1976.
"Saturday Night Live".
Belushi became an original cast member of the new television show "Saturday Night Live" (SNL) in 1975. His characters at SNL included belligerent Samurai Futaba. With Aykroyd, Belushi created the characters Jake and Elwood Blues, also known as The Blues Brothers.
During his tenure at SNL, Belushi was heavily using drugs and alcohol, which affected his performance and caused SNL to fire him (and promptly re-hire him) a number of times.
Expansion into films.
In 1978, he made the films "Old Boyfriends" (directed by Joan Tewkesbury), "Goin' South" (directed by Jack Nicholson) and "Animal House" (directed by John Landis). Upon its initial release, "Animal House" received generally mixed reviews from critics, but "Time" and Roger Ebert proclaimed it one of the year's best. Filmed for $2.8 million, it is one of the most profitable movies of all time, garnering an estimated gross of more than $141 million in the form of theatrical rentals and home video, not including merchandising. "Animal House" was also largely responsible for defining and launching the gross-out genre of films, which became one of Hollywood's staples.
Following the success of The Blues Brothers on the show, Belushi and Aykroyd, with the help of pianist-arranger Paul Shaffer, started assembling a collection of studio talents to form a proper band. These included "SNL" band members, saxophonist "Blue" Lou Marini and trombonist-saxophonist Tom Malone, who had previously played in Blood, Sweat & Tears. At Shaffer's suggestion, guitarist Steve Cropper and bassist Donald "Duck" Dunn, the powerhouse combo from Booker T and the M.G.'s and subsequently almost every hit out of Memphis's Stax Records during the 1960s, were signed as well. In 1978 The Blues Brothers released their debut album, Briefcase Full of Blues with Atlantic Records. The album reached #1 on the Billboard 200 and went double platinum. Two singles were released, "Rubber Biscuit", which reached number 37 on the Billboard Hot 100 and "Soul Man," which reached number 14.
In 1979 Belushi left "Saturday Night Live", with Aykroyd, to pursue a film career. In "Rolling Stone Magazine"'s February, 2015 appraisal of all 141 SNL cast members to date, Belushi received the top ranking. "Belushi was the 'live' in "Saturday Night Live"," they wrote, "the one who made the show happen on the edge ... Nobody embodied the highs and lows of SNL like Belushi."
Aykroyd and Belushi made three movies together, "1941" (directed by Steven Spielberg), "Neighbors" (directed by John Avildsen), and most notably "The Blues Brothers" (directed by John Landis). Released in the United States on June 20, 1980, "The Blues Brothers" received generally positive reviews. It earned just under $5 million in its opening weekend and went on to gross $115.2 million in theaters worldwide before its release on home video. The Blues Brothers band toured to promote the film, which led to a third album (and second live album), "Made in America", recorded at the Universal Amphitheatre in 1980. The track "Who's Making Love" peaked at No 39.
The only film Belushi made without Aykroyd following his departure from "SNL" was the romantic comedy "Continental Divide" (directed by Michael Apted). Released in September 1981, it starred Belushi as Chicago home town hero writer Ernie Souchack (loosely based on newspaper columnist and long-time family friend Mike Royko), who gets put on assignment researching a scientist (played by Blair Brown) studying birds of prey in the remote Rocky Mountains.
In 1980, Belushi had become a fan and advocate of the punk rock band Fear after seeing them perform in several after-hours New York City bars, and brought them to Cherokee Studios to record songs for the soundtrack of " Neighbors." Blues Brother band member and sax player Tom Scott, along with producing partner and Cherokee owner Bruce Robb, initially helped with the session but later pulled out due to conflicts with Belushi.
At the time of his death, Belushi was pursuing several movie projects, including "Moon Over Miami" with Louis Malle, "National Lampoon's The Joy of Sex" and "Noble Rot", a script that had been adapted and rewritten by himself and former "Saturday Night Live" writer, Don Novello in the weeks leading up to his death. He was also scheduled to work with Aykroyd on "Ghostbusters" and "Spies Like Us".
Belushi also made a "Guest Star Appearance" on an episode of the television series "Police Squad!" (1982), which showed him underwater wearing cement shoes. He died shortly before the episode aired, so the scene was cut and replaced by a segment with William Conrad.
Death.
On March 5, 1982, after showing up at his hotel for a scheduled workout, his trainer, Bill Wallace found Belushi dead in his room, Bungalow 3 at the Chateau Marmont Hotel on Sunset Boulevard in Hollywood, California. He was 33 years old. The cause of death was combined drug intoxication involving cocaine and heroin, a drug combination also known as a speedball. In the early morning hours on the day of his death, he was visited separately by friends Robin Williams and Robert De Niro, each of whom left the premises, leaving Belushi in the company of assorted others, including Catherine Evelyn Smith. His death was investigated by forensic pathologist Michael Baden, among others, and, while the findings were disputed, it was officially ruled a drug-related accident.
Two months later, Smith admitted in an interview with the "National Enquirer" that she had been with Belushi the night of his death and had given him the fatal speedball shot. After the appearance of the article "I Killed Belushi" in the "Enquirer" edition of June 29, 1982, the case was reopened. Smith was extradited from Toronto, Ontario, arrested and charged with first-degree murder. A plea bargain reduced the charge to involuntary manslaughter, and she served fifteen months in prison.
Belushi's wife arranged for a traditional Orthodox Christian funeral which was conducted by an Albanian Orthodox priest. He has been interred twice at Abel's Hill Cemetery in Chilmark on Martha's Vineyard, Massachusetts. A tombstone marking the original burial location has a skull and crossbones, that reads, "I may be gone but Rock and Roll lives on." An unmarked tombstone in an undisclosed location marks the final burial location. He is also remembered on the Belushi family stone marking his mother's grave at Elmwood Cemetery in River Grove, Illinois. This stone reads, "He gave us laughter."
Tributes and legacy.
Belushi's life is detailed in the 1984 biography "Wired: The Short Life and Fast Times of John Belushi" by Bob Woodward and 1990's Samurai Widow by his wife Judith. "Wired" was later adapted into a feature film in which Belushi was played by Michael Chiklis.
The thrash metal group Anthrax penned a song about Belushi on their 1987 album, Among the Living, titled "Efilnikufesin (N.F.L.)."
Belushi has been portrayed by actors Eric Siegel in "Gilda Radner: It's Always Something," Tyler Labine in "Behind the Camera: The Unauthorized Story of Mork & Mindy" (which also features his friendship with Robin Williams), and Michael Chiklis in "Wired." Future "SNL" star Chris Farley, whose work was influenced by Belushi, died at age 33 in 1997 due to a drug overdose, similar to combined drug intoxication, contributing to comparisons between Belushi and Farley.
His widow later remarried and is now Judith Belushi Pisano. She and co-biographer Tanner Colby produced "," a collection of first-person interviews and photographs of John Belushi's life that was published in 2005.
In 2004, Belushi was honored with a star on the Hollywood Walk of Fame. In 2006, Biography Channel aired the "John Belushi" episode of "Final 24," a documentary following Belushi in the last twenty-four hours leading to his death. In 2010, Biography aired a full biography documentation of Belushi's life.
According to Jane Curtin, who appeared on "The Oprah Winfrey Show" in 2011, John Belushi was a "misogynist" who would deliberately sabotage the work of women writers and comics while working on "SNL". "So you'd go to a table read, and if a woman writer had written a piece for John, he would not read it in his full voice. He felt as though it was his duty to sabotage pieces written by women."
At the conclusion of the very first live "SNL" episode (Robert Urich/Mink DeVille on March 20, 1982) following Belushi's death, Brian Doyle-Murray gave a tribute to him.
Belushi was scheduled to present the first annual Best Visual Effects Oscar at the 1982 Academy Awards with Dan Aykroyd. Aykroyd presented the award alone, and stated from the podium: "My partner would have loved to have been here tonight to present this award, since he was a bit of a Visual Effect himself."
In 2015, Belushi was ranked by "Rolling Stone" as the greatest "SNL" cast member of all time.

</doc>
<doc id="16385" url="https://en.wikipedia.org/wiki?curid=16385" title="Johann Philipp Abelin">
Johann Philipp Abelin

Johann Philipp Abelin was a German chronicler whose career straddled the 16th and 17th centuries. He was born, probably, at Strasbourg, and died there between 1634 and 1637. He wrote numerous histories under the pseudonyms of Abeleus, Philipp Arlanibäus, Johann Ludwig Gottfried and Gotofredus.
Publications.
He worked mainly as a translator for the publishing house of Lucas Jennisius, Matthäus Merian and Friedrich Hulsius in Frankfurt. Some of his works, such as a history of India, proved later to be translations of other works. His own works consisted mainly of compilations of historical records.
Own works.
Abelin produced compilations of contemporary records and letters about the events of the wars of Gustavus Adolphus of Sweden without further historical commentary:
In the same style, his best known work was "Theatrum Europaeum", a series of chronicles of the chief events in the history of the world down to 1619, reedited, updated and republished several times, including a translation into Dutch. Its coincidence with the needs and tastes of the time, made it a very popular work. Abelin was responsible for the first two volumes. It was continued by various writers and grew to 21 volumes (1633–1738). However, the main interest of the volumes are the beautiful copperplate engraved illustrations of Matthäus Merian (1593–1650).

</doc>
<doc id="16386" url="https://en.wikipedia.org/wiki?curid=16386" title="Jacob Abendana">
Jacob Abendana

Jacob Abendana (1630 – September 12, 1685) was "hakham" of London from 1680 until his death. Jacob was the eldest son of Joseph Abendana and brother to Isaac Abendana.
Though his family originally lived in Hamburg, Jacob and his brother were both born in Spain. At some point in time, his family moved to Amsterdam where he studied at the "De los Pintos" rabbinical academy in Rotterdam. In 1655 he was appointed "hakham" of that city. On May 3, 1655, Abendana delivered a famous memorial sermon on the Cordovan martyrs Marranos Nunez and Almeyda Bernal who had been burned at the stake.
Several years later, with his brother, Isaac, Jacob published the Bible commentary "Miklal Yofi" by Solomon ben Melekh which included his own commentary, "Lekket Shikchah" (Gleanings), on the Pentateuch, the Book of Joshua, and part of the Book of Judges. This was published by subscription in Amsterdam in 1660 with a second edition in 1685.
Having gone to Leyden seeking subscribers, Jacob met Anton Hulsius with whom he helped in his studies. Hulsius tried to convert Jacob to Christianity which began a lifelong correspondence between the two. The Abendana brothers similarly impressed other Christian scholars, such as Johannes Buxtorf (Basel), Johann Coccejus (Leyden), and Jacob Golius (Leyden).
With Hulsius, Jacob entered into a polemical discussion of the verse in the Book of Haggai: "The latter splendor of this house shall be greater than the former" (2:9), which Hulsius attempted to prove was a reference to the Church. The debate lasted via correspondence from September 24, 1659 to June 16, 1660. Abendana responded with a Spanish translation of Rabbi Judah Halevi's "Kuzari" in 1663. Hulsius eventually published the correspondence between the two in 1669.
In 1675, Jacob addressed the community at the dedication of the new synagogue in Amsterdam. Five years later, in 1680, he was brought to London to succeed Joshua da Silva as "Hakham" of London where he served for 15 years as the hakham of the Spanish and Portuguese Synagogue in London. Over the following years, he completed a Spanish-language translation of the "Mishnah", along with the commentaries of Maimonides and Obadiah of Bertinoro. The work was frequently cited by Christian theologians, though it was never published. Jacob Abendana died childless in London in 1685 and was buried in the Portuguese cemetery at Mile End.

</doc>
<doc id="16388" url="https://en.wikipedia.org/wiki?curid=16388" title="James Hamilton, 1st Earl of Abercorn">
James Hamilton, 1st Earl of Abercorn

James Hamilton, 1st Earl of Abercorn PC (12 August 1575 – 23 March 1618) was a Scottish peer.
Biography.
He was the eldest son of Claud Hamilton, 1st Lord Paisley (4th son of James Hamilton, 2nd Earl of Arran), and of Margaret, daughter of George Seton, 7th Lord Seton.
Hamilton was made Sheriff of Linlithgow in 1600, received large grants of lands in Scotland and Ireland, was created in 1603 Baron Abercorn, and on 10 July 1606 was rewarded for his services in the matter of the union by being made Earl of Abercorn and Lord Paisley, Hamilton, Mountcastell and Kilpatrick.
He died on 23 March 1618 and was buried on 29 April 1618 at Abbey Church, Paisley, Renfrewshire, Scotland. 
Family.
He married Marion, daughter of Thomas Boyd, 6th Lord Boyd, and left ten children:
His heirs, Earls of Abercorn, heirs male of the Hamilton family from 1651, became Marquesses of Abercorn in 1790, and Dukes of Abercorn in 1868; the 2nd Duke of Abercorn (b. 1838) being a prominent Unionist politician and chairman of the British South Africa Company. The present Dukes are descended from his fourth son, Sir George, the issue of the three elder being extinct.

</doc>
<doc id="16389" url="https://en.wikipedia.org/wiki?curid=16389" title="Java virtual machine">
Java virtual machine

A Java virtual machine (JVM) is an abstract computing machine that enables a computer to run a Java program. There are three notions of the JVM: specification, implementation, and instance. The specification is a document that formally describes what is required of a JVM implementation. Having a single specification ensures all implementations are interoperable. A JVM implementation is a computer program that meets the requirements of the JVM specification. An instance of a JVM is an implementation running in a process that executes a computer program compiled into Java bytecode. 
Java Runtime Environment (JRE) is a software package that contains what is required to run a Java program. It includes a Java Virtual Machine implementation together with an implementation of the Java Class Library. The Oracle Corporation, which owns the Java trademark, distributes a Java Runtime environment with their Java Virtual Machine called HotSpot.
Java Development Kit (JDK) is a superset of a JRE and contains tools for Java programmers, e.g. a javac compiler. The Java Development Kit is provided free of charge either by Oracle Corporation directly, or by the OpenJDK open source project, which is governed by Oracle.
JVM specification.
The Java virtual machine is an abstract (virtual) computer defined by a specification. This specification omits implementation details that are not essential to ensure interoperability: the memory layout of run-time data areas, the garbage-collection algorithm used, and any internal optimization of the Java virtual machine instructions (their translation into machine code). The main reason for this omission is to not unnecessarily constrain implementers. Any Java application can be run only inside some concrete implementation of the abstract specification of the Java virtual machine.
Starting with Java Platform, Standard Edition (J2SE) 5.0, changes to the JVM specification have been developed under the Java Community Process as JSR 924. , changes to specification to support changes proposed to the class file format (JSR 202) are being done as a maintenance release of JSR 924. The specification for the JVM was published as the "blue book", The preface states:
One of Oracle's JVMs is named HotSpot, the other, inherited from BEA Systems is JRockit. Clean-room Java implementations include Kaffe and IBM J9. Oracle owns the Java trademark, and may allow its use to certify implementation suites as fully compatible with Oracle's specification.
Class loader.
One of the organizational units of JVM byte code is a class. A class loader implementation must be able to recognize and load anything that conforms to the Java class file format. Any implementation is free to recognize other binary forms besides "class" files, but it must recognize "class" files.
The class loader performs three basic activities in this strict order:
In general, there are two types of class loader: bootstrap class loader and user defined class loader.
Every Java virtual machine implementation must have a bootstrap class loader, capable of loading trusted classes. The Java virtual machine specification doesn't specify how a class loader should locate classes.
Bytecode Prick instructions.
The JVM has instructions for the following groups of tasks:
The aim is binary compatibility. Each particular host operating system needs its own implementation of the JVM and runtime. These JVMs interpret the bytecode semantically the same way, but the actual implementation may be different. More complex than just emulating bytecode is compatibly and efficiently implementing the Java core API that must be mapped to each host operating system.
JVM languages.
A JVM language is any language with functionality that can be expressed in terms of a valid class file which can be hosted by the Java Virtual Machine. A class file contains Java Virtual Machine instructions (Java byte code) and a symbol table, as well as other ancillary information. The class file format is the hardware- and operating system-independent binary format used to represent compiled classes and interfaces.
There are several JVM languages, both old languages ported to JVM and completely new languages. JRuby and Jython are perhaps the most well-known ports of existing languages, i.e. Ruby and Python respectively. Of the new languages that have been created from scratch to compile to Java bytecode, Clojure, Groovy and Scala may be the most popular ones. A notable feature with the JVM languages is that they are compatible with each other, so that, for example, Scala libraries can be used with Java programs and vice versa.
Java 7 JVM implements "JSR 292: Supporting Dynamically Typed Languages" on the Java Platform, a new feature which supports dynamically typed languages in the JVM. This feature is developed within the Da Vinci Machine project whose mission is to extend the JVM so that it supports languages other than Java.
Bytecode verifier.
A basic philosophy of Java is that it is inherently "safe" from the standpoint that no user program can "crash" the host machine or otherwise interfere inappropriately with other operations on the host machine, and that it is possible to protect certain methods and data structures belonging to "trusted" code from access or corruption by "untrusted" code executing within the same JVM. Furthermore, common programmer errors that often led to data corruption or unpredictable behavior such as accessing off the end of an array or using an uninitialized pointer are not allowed to occur. Several features of Java combine to provide this safety, including the class model, the garbage-collected heap, and the verifier.
The JVM "verifies" all bytecode before it is executed. This verification consists primarily of three types of checks:
The first two of these checks take place primarily during the "verification" step that occurs when a class is loaded and made eligible for use. The third is primarily performed dynamically, when data items or methods of a class are first accessed by another class.
The verifier permits only some bytecode sequences in valid programs, e.g. a jump (branch) instruction can only target an instruction within the same method. Furthermore, the verifier ensures that any given instruction operates on a fixed stack location, allowing the JIT compiler to transform stack accesses into fixed register accesses. Because of this, that the JVM is a stack architecture does not imply a speed penalty for emulation on register-based architectures when using a JIT compiler. In the face of the code-verified JVM architecture, it makes no difference to a JIT compiler whether it gets named imaginary registers or imaginary stack positions that must be allocated to the target architecture's registers. In fact, code verification makes the JVM different from a classic stack architecture, of which efficient emulation with a JIT compiler is more complicated and typically carried out by a slower interpreter.
The original specification for the bytecode verifier used natural language that was "incomplete or incorrect in some respects." A number of attempts have been made to specify the JVM as a formal system. By doing this, the security of current JVM implementations can more thoroughly be analyzed, and potential security exploits prevented. It will also be possible to optimize the JVM by skipping unnecessary safety checks, if the application being run is proven to be safe.
Secure execution of remote code.
A virtual machine architecture allows very fine-grained control over the actions that code within the machine is permitted to take. This is designed to allow safe execution of untrusted code from remote sources, a model used by Java applets. Applets run within a VM incorporated into a user's browser, executing code downloaded from a remote HTTP server. The remote code runs in a restricted "sandbox", which is designed to protect the user from misbehaving or malicious code. Publishers can purchase a certificate with which to digitally sign applets as "safe", giving them permission to ask the user to break out of the sandbox and access the local file system, clipboard, execute external pieces of software, or network.
Bytecode interpreter and just-in-time compiler.
For each hardware architecture a different Java bytecode interpreter is needed. When a computer has a Java bytecode interpreter, it can run any Java bytecode program, and the same program can be run on any computer that has such an interpreter.
When Java bytecode is executed by an interpreter, the execution will always be slower than the execution of the same program compiled into native machine language. This problem is mitigated by just-in-time (JIT) compilers for executing Java bytecode. A JIT compiler may translate Java bytecode into native machine language while executing the program. The translated parts of the program can then be executed much more quickly than they could be interpreted. This technique gets applied to those parts of a program frequently executed. This way a JIT compiler can significantly speed up the overall execution time.
There is no necessary connection between Java and Java bytecode. A program written in Java can be compiled directly into the machine language of a real computer and programs written in other languages than Java can be compiled into Java bytecode.
Java bytecode is intended to be platform-independent and secure. Some JVM implementations do not include an interpreter, but consist only of a just-in-time compiler.
JVM in the web browser.
Since very early stages of the design process, Java (and JVM) has been marketed as a web technology for creating Rich Internet Applications.
Java applets.
On the client side, web browsers may be extended with a NPAPI Java plugin which executes so called Java applets embedded into HTML pages. The applet is allowed to draw into a rectangular region on the page assigned to it and use a restricted set of APIs that allow for example access to user's microphone or 3D acceleration. Java applets were superior to JavaScript both in performance and features until approximately 2011, when JavaScript engines in browsers were made significantly faster and the HTML 5 suite of web technologies started enhancing JavaScript with new APIs. Java applets are not able to modify the page outside its rectangular region which is not true about JavaScript. Adobe Flash Player, the main competing technology, works in the same way in this respect. Java applets are not restricted to Java and in general can be created in any JVM language.
As of April 2014, Google Chrome does not allow the use of any NPAPI plugins. Mozilla Firefox will also ban NPAPI plugins by the end of 2016. This means that Java applets can no longer be used in either browser. Oracle ultimately announced in January 2016 that it will discontinue the Java web browser plugin effective Java 9.
JavaScript JVMs and interpreters.
JVM implementations in JavaScript do exist, but are mostly limited to hobby projects unsuitable for production deployment or development tools to avoid having to recompile every time the developer wants to preview the changes just made.
Compilation to JavaScript.
With the continuing improvements in JavaScript execution speed, combined with the increased use of mobile devices whose web browsers do not implement support for plugins, there are efforts to target those users through compilation to JavaScript. It is possible to either compile the source code or JVM bytecode to JavaScript. Compiling the JVM bytecode which is universal across JVM languages allows building upon the existing compiler to bytecode.
Main JVM bytecode to JavaScript compilers are TeaVM, the compiler contained in Dragome Web SDK, Bck2Brwsr, and j2js-compiler.
Leading compilers from JVM languages to JavaScript include the Java to JavaScript compiler contained in Google Web Toolkit, Clojurescript (Clojure), GrooScript (Groovy), Scala.js (Scala) and others.
Java Runtime Environment from Oracle.
The Java Runtime Environment (JRE) released by Oracle is a software distribution containing a stand-alone Java VM (HotSpot), browser plugin, Java standard libraries and a configuration tool. It is the most common Java environment installed on Windows computers. It is freely available for download at the website java.com.
Performance.
The JVM specification gives a lot of leeway to implementors regarding the implementation details. Since Java 1.3, JRE from Oracle contains a JVM called HotSpot. It has been designed to be a high-performance JVM.
To speed-up code execution, HotSpot relies on just-in-time compilation. To speed-up object allocation and garbage collection, HotSpot uses generational heap.
Generational heap.
The "Java virtual machine heap" is the area of memory used by the JVM for dynamic memory allocation.
In HotSpot the heap is divided into "generations":
The "permanent generation" (or "permgen") was used for class definitions and associated metadata prior to Java 8. Permanent generation was not part of the heap. The "permanent generation" was removed from Java 8.
Originally there was no permanent generation, and objects and classes were stored together in the same area. But as class unloading occurs much more rarely than objects are collected, moving class structures to a specific area allowed significant performance improvements.
Security.
Oracle's JRE is installed on a large number of computers. Since any web page the user visits may run Java applets, Java provides an easily accessible attack surface to malicious web sites that the user visits. Kaspersky Labs reports that the Java web browser plugin is the method of choice for computer criminals. Java exploits are included in many exploit packs that hackers deploy onto hacked web sites.
In the past, end users were often using an out-of-date version of JRE which was vulnerable to many known attacks. This led to the widely shared belief between users that Java is inherently insecure. Since Java 1.7, Oracle's JRE for Windows includes automatic update functionality.
Toolbar controversy.
Beginning in 2005, Sun's (now Oracle's) JRE included unrelated software which was installed by default. In the beginning it was Google Toolbar, later MSN Toolbar, Yahoo Toolbar and finally the Ask Toolbar. The Ask Toolbar proved to be especially controversial. There has been a petition asking Oracle to remove it. The signers voiced their belief that Oracle was "violating the trust of the hundreds of millions of users who run Java on their machines. They are tarnishing the reputation of a once proud platform". Zdnet called their conduct deceptive, since the installer continued to offer the toolbar during every update, even after the user had previously refused to install it, increasing the chances of the toolbar being installed when the user was too busy or distracted.
In June 2015, Oracle announced that it had ended its partnership with Ask.com in favor of one with Yahoo!, in which users will be, by default, asked to change their home page and default search engine to that of Yahoo.

</doc>
<doc id="16390" url="https://en.wikipedia.org/wiki?curid=16390" title="John Abercrombie (physician)">
John Abercrombie (physician)

John Abercrombie, FRSE, FRCSE, FRCPE (10 October 1780 – 14 November 1844), was a Scottish physician and philosopher. The "Chambers Biographical Dictionary" says of him that after Dr James Gregory's death, he was "recognized as the first consulting physician in Scotland".
He was the official physician to Heriot's Hospital and Physician to the King for Scotland. 
Life.
He was the son of the Reverend George Abercrombie, the minister of East Church, Aberdeen, he was educated at the Grammar School and Marischal College, University of Aberdeen. He studied medicine at the University of Edinburgh, and after graduating as M.D. in 1803 he settled down to practise in that city, where he soon attained a leading position.
From 1816 he published various papers in the "Edinburgh Medical and Surgical Journal", which formed the basis of his more extensive works: "Pathological and Practical Researches on Diseases of the Brain and Spinal Cord", regarded as the first textbook in neuropathology, and "Researches on the Diseases of the Intestinal Canal, Liver and other Viscera of the Abdomen", both published in 1828. In 1821 he was elected to the Royal College of Surgeons. For his services as a physician and philanthropist he received many marks of distinction, including the Rectorship of Marischal College in 1835.
In 1831 he was elected a Fellow of the Royal Society of Edinburgh, his proposer being Thomas Charles Hope, and served as Vice-President of the Society from 1835 to 1844.
He also found time for philosophical speculations, and in 1830 he published his "Inquiries concerning the Intellectual Powers of Man and the Investigation of Truth", which was followed in 1833 by a sequel, "The Philosophy of the Moral Feelings". Both works showed little originality of thought; they achieved wide popularity at the time of their publication, but have long been superseded.
An elder of the Church of Scotland, he also wrote "The man of faith: or the harmony of Christian faith and Christian character" (1835), which he pretended to distribute freely. In 1841, he was partially paralyzed, but was able to return to his practice of medicine. 
He died at his home, 19 York Place, Edinburgh, in 1844 of a ruptured coronary artery.
He is buried against the east wall of St Cuthberts Churchyard adjacent to the gateway into Princes Street Gardens.
A year after his death his "Essays" (1845) on Christian ethics were published.

</doc>
<doc id="16391" url="https://en.wikipedia.org/wiki?curid=16391" title="Judgement of Paris">
Judgement of Paris

The Judgement of Paris is a story from Greek mythology, which was one of the events that led up to the Trojan War and (in slightly later versions of the story) to the foundation of Rome.
Sources of the episode.
As with many mythological tales, details vary depending on the source. The brief allusion to the Judgement in the "Iliad" (24.25–30) shows that the episode initiating all the subsequent action was already familiar to its audience; a fuller version was told in the "Cypria", a lost work of the Epic Cycle, of which only fragments (and a reliable summary) remain. The later writers Ovid ("Heroides" 16.71ff, 149–152 and 5.35f), Lucian ("Dialogues of the Gods" 20), The "Bibliotheca" ("Epitome" E.3.2) and Hyginus ("Fabulae" 92), retell the story with skeptical, ironic or popularizing agendas. It appeared wordlessly on the ivory and gold votive chest of the 7th-century BC tyrant Cypselus at Olympia, which was described by Pausanias as showing:
The subject was favoured by painters of Red-figure pottery as early as the sixth century BC, and remained popular in Greek and Roman art, before enjoying a significant revival, as an opportunity to show three female nudes, in the Renaissance.
Mythic narrative.
It is recounted that Zeus held a banquet in celebration of the marriage of Peleus and Thetis (parents of Achilles). However, Eris, goddess of discord was not invited, for it was believed she would have made the party unpleasant for everyone. Angered by this snub, Eris arrived at the celebration with a golden apple from the Garden of the Hesperides, which she threw into the proceedings as a prize of beauty. According to some later versions, upon the apple was the inscription "καλλίστῃ" ("kallistēi", "for the fairest one").
Three goddesses claimed the apple: Hera, Athena and Aphrodite. They asked Zeus to judge which of them was fairest, and eventually he, reluctant to favor any claim himself, declared that Paris, a Trojan mortal, would judge their cases, for he had recently shown his exemplary fairness in a contest in which Ares in bull form had bested Paris's own prize bull, and the shepherd-prince had unhesitatingly awarded the prize to the god.
Thus it happened that, with Hermes as their guide, the three candidates bathed in the spring of Ida, then confronted Paris on Mount Ida in the climactic moment that is the crux of the tale. While Paris inspected them, each attempted with her powers to bribe him; Hera offered to make him king of Europe and Asia, Athena offered wisdom and skill in war, and Aphrodite, who had the Charites and the Horai to enhance her charms with flowers and song (according to a fragment of the "Cypria" quoted by Athenagoras of Athens), offered the world's most beautiful woman (Euripides, "Andromache", l.284, "Helena" l. 676). This was Helen of Sparta, wife of the Greek king Menelaus. Paris accepted Aphrodite's gift and awarded the apple to her, receiving Helen as well as the enmity of the Greeks and especially of Hera. The Greeks' expedition to retrieve Helen from Paris in Troy is the mythological basis of the Trojan War.
The mytheme of the Judgement of Paris naturally offered artists the opportunity to depict a sort of beauty contest between three beautiful female nudes, but the myth, at least since Euripides, rather concerns a choice among the gifts that each goddess embodies. The bribery involved is ironic and a late ingredient.
According to a tradition suggested by Alfred J. Van Windekens, objectively, "cow-eyed" Hera was indeed the most beautiful, not Aphrodite. However, Hera was the goddess of the marital order and of cuckolded wives, amongst other things. She was often portrayed as the shrewish, jealous wife of Zeus, who himself often escaped from her controlling ways by cheating on her with other women, mortal and immortal. She had fidelity and chastity in mind and was careful to be modest when Paris was inspecting her. Aphrodite, though not as objectively beautiful as Hera, was the goddess of sexuality, and was effortlessly more sexual and charming before him. Thus, she was able to sway Paris into judging her the fairest. Athena's beauty is rarely commented in the myths, perhaps because Greeks held her up as an asexual being, being able to "overcome" her "womanly weaknesses" to become both wise and talented in war (both considered male domains by the Greeks). Her rage at losing makes her join the Greeks in the battle against Paris' Trojans, a key event in the turning point of the war.
In post-Classical art.
The subject became popular in art from the late Middle Ages onwards. All three goddesses were usually shown nude, though in ancient art only Aphrodite is ever unclothed, and not always. The opportunity for three female nudes was a large part of the attraction of the subject. It appeared in illuminated manuscripts and was popular in decorative art, including 15th-century Italian inkstands and other works in maiolica, and "cassoni". As a subject for easel paintings, it was more common in Northern Europe, although Marcantonio Raimondi's engraving of c. 1515, probably based on a drawing by Raphael, and using a composition derived from a Roman sarcophagus, was a highly influential treatment, which made Paris's Phrygian cap an attribute in most later versions.
The subject was painted many (supposedly 22) times by Lucas Cranach the Elder, and was especially attractive to Northern Mannerist painters. Rubens painted several compositions of the subject at different points in his career. Watteau and Angelica Kauffman were among the artists who painted the subject in the 18th century. The Judgement of Paris was painted frequently by academic artists of the 19th century, and less often by their more progressive contemporaries such as Renoir and Cézanne. Later artists who have painted the subject include André Lhote, Enrique Simonet ("El Juicio de Paris" 1904) and Salvador Dalí.
Ivo Saliger (1939), Adolf Ziegler (1939) and Joseph Thorak (1941) also used the classic myth to propagate German renewal during the Nazi period.
Kallistēi.
Kallistēi is the word of the Ancient Greek language inscribed on the Golden Apple of Discord by Eris. In Greek, the word is "καλλίστῃ" (the dative singular of the feminine superlative of καλος, beautiful). Its meaning can be rendered "to the fairest one".
Calliste (Καλλίστη; Mod. Gk. Kallisti) is also an ancient name for the isle of Thera.
Use in Discordianism.
The word "Kallisti" (Modern Greek) written on a golden apple, has become a principal symbol of Discordianism, a post-modernist religion. In non-philological texts (such as Discordian ones) the word is usually spelled as "καλλιστι". Most versions of "Principia Discordia" actually spell it as καλλιχτι, but this is definitely incorrect; in the afterword of the 1979 Loompanics edition of "Principia", Gregory Hill says that was because on the IBM typewriter he used, not all Greek letters coincided with Latin ones, and he didn't know enough of the letters to spot the mistake. Zeus' failure to invite Eris is referred to as "The Original Snub" in Discordian mythology.
Dramatizations.
The story is the basis of an opera, "The Judgement of Paris", with a libretto by William Congreve, that was set to music by four composers in London, 1700-1701. Thomas Arne composed a highly successful score to the same libretto in 1742. The opera "Le Cinesi" ("The Chinese Women") by Christoph Willibald Gluck (1754) concludes with a ballet, "The Judgement of Paris", sung as a vocal quartet. Francesco Cilea's 1902 opera "Adriana Lecouvreur" also includes a "Judgement of Paris" ballet sequence.
Novelist Gore Vidal named his 1952 book, "The Judgment of Paris", after this story.
The Judgement of Paris was burlesqued in the 1954 musical "The Golden Apple". In it, the three goddesses have been reduced to three town biddies in smalltown Washington state. They ask Paris, a traveling salesman, to judge the cakes they have made for the church social. Each woman (the mayor's wife, the schoolmarm, and the matchmaker) makes appeals to Paris, who chooses the matchmaker. The matchmaker, in turn, sets him up with Helen, the town floozy, who runs off with him.
The Judgement of Paris is featured in the 2003 TV miniseries "Helen of Troy". The event is brief, and only Hera and Aphrodite offer bribes. All three goddesses remain fully clothed. Aphrodite gives Paris a vision of Helen, while Helen has a reciprocal vision of Paris.
In the "" series, the contest is altered somewhat with Aphrodite and Athena entering but Artemis is the third goddess contestant instead of Hera (offering the one who chooses her the chance to be renowned as a great warrior). The Golden Apple appears as a gift from Aphrodite with the ability to make any mortal woman fall in love with the man holding it and to make a mortal man and woman soul mates if they simultaneously touch it. The other major differences beside the presence of Artemis and the role of the apple are the fact that it is Iolaus who is the judge and the goddesses appear in swimsuits and not nude.

</doc>
<doc id="16392" url="https://en.wikipedia.org/wiki?curid=16392" title="Jurisdiction">
Jurisdiction

Jurisdiction (from the Latin "ius, " meaning "law" and "" meaning "to speak") is the practical authority granted to a legal body to administer justice within a defined area of responsibility, e.x. Michigan tax law. Areas of jurisdiction apply to local, state, and federal levels, e.x. the court has jurisdiction to apply federal law. 
Colloquially it is used to refer to the geographical area to which such authority applies, e.x. the court has jurisdiction over all of Colorado. The legal term refers only to the granted authority, not to a geographical area.
Jurisdiction draws its substance from public international law, conflict of laws, constitutional law, and the powers of the executive and legislative branches of government to allocate resources to best serve the needs of its native society.
International dimension.
International laws and treaties provide agreements which nations agree to be bound to.
Political issue.
Supranational organizations provide mechanisms whereby disputes between nations may be resolved through arbitration or mediation. When a country is recognized as "de jure", it is an acknowledgment by the other "de jure" nations that the country has sovereignty and the right to exist.
However, it is often at the discretion of each nation whether to co-operate or participate. If a nation does agree to participate in activities of the supranational bodies and accept decisions, the nation is giving up its sovereign authority and thereby allocating power to these bodies.
Insofar as these bodies or nominated individuals may resolve disputes through judicial or quasi-judicial means, or promote treaty obligations in the nature of laws, the power ceded to these bodies cumulatively represents its own jurisdiction. But no matter how powerful each body may appear to be, the extent to which any of their judgments may be enforced, or proposed treaties and conventions may become, or remain, effective within the territorial boundaries of each nation is a political matter under the sovereign control each nation.
International and municipal jurisdiction.
The fact that international organizations, courts and tribunals have been created raises the difficult question of how to co-ordinate their activities with those of national courts. If the two sets of bodies do not have "concurrent" jurisdiction but, as in the case of the International Criminal Court (ICC), the relationship is expressly based on the principle of "complementarity", i.e., the international court is subsidiary or complementary to national courts, the difficulty is avoided. But if the jurisdiction claimed is concurrent, or as in the case of International Criminal Tribunal for the former Yugoslavia (ICTY), the international tribunal is to prevail over national courts, the problems are more difficult to resolve politically.
The idea of universal jurisdiction is fundamental to the operation of global organizations such as the United Nations and the International Court of Justice (ICJ), which jointly assert the benefit of maintaining legal entities with jurisdiction over a wide range of matters of significance to nations (the ICJ should not be confused with the ICC and this version of "universal jurisdiction" is not the same as that enacted in the War Crimes Law (Belgium) which is an assertion of extraterritorial jurisdiction that will fail to gain implementation in any other state under the standard provisions of public policy). Under Article 34 Statute of the ICJ only nations may be parties in cases before the Court and, under Article 36, the jurisdiction comprises all cases which the parties refer to it and all matters specially provided for in the Charter of the United Nations or in treaties and conventions in force. But, to invoke the jurisdiction in any given case, all the parties have to accept the prospective judgment as binding. This reduces the risk of wasting the Court's time.
Despite the safeguards built into the constitutions of most of these organizations, courts and tribunals, the concept of universal jurisdiction is controversial among those nations which prefer unilateral to multilateral solutions through the use of executive or military authority, sometimes described as realpolitik-based diplomacy.
Within other international contexts, there are intergovernmental organizations such as the World Trade Organization (WTO) that have socially and economically significant dispute resolution functions but, again, even though their jurisdiction may be invoked to hear the cases, the power to enforce their decisions is at the will of the nations affected, save that the WTO is permitted to allow retaliatory action by successful nations against those nations found to be in breach of international trade law. At a regional level, groups of nations can create political and legal bodies with sometimes complicated patchworks of overlapping provisions detailing the jurisdictional relationships between the member states and providing for some degree of harmonization between their national legislative and judicial functions, for example, the European Union and African Union both have the potential to become federated nations although the political barriers to such unification in the face of entrenched nationalism will be very difficult to overcome. Each such group may form transnational institutions with declared legislative or judicial powers. For example, in Europe, the European Court of Justice has been given jurisdiction as the ultimate appellate court to the member states on issues of European law. This jurisdiction is entrenched and its authority could only be denied by a member nation if that member nation asserts its sovereignty and withdraws from the union.
International and municipal law.
The standard treaties and conventions leave the issue of implementation to each nation, i.e. there is no general rule in international law that treaties have direct effect in municipal law, but some nations, by virtue of their membership of supranational bodies, allow the direct incorporation of rights or enact legislation to honor their international commitments. Hence, citizens in those nations can invoke the jurisdiction of local courts to enforce rights granted under international law wherever there is incorporation. If there is no direct effect or legislation, there are two theories to justify the courts incorporating international into municipal law:
In the United States, the Supremacy Clause of the United States Constitution makes all treaties that have been ratified under the authority of the United States and customary international law a part of the "Supreme Law of the Land" (along with the Constitution itself and acts of Congress passed pursuant to it) (U.S. Const.art. VI Cl. 2) and, as such, the law of the land is binding on the federal government as well as on state and local governments. According to the Supreme Court of the United States, the treaty power authorizes Congress to legislate under the Necessary and Proper Clause in areas beyond those specifically conferred on Congress ("Missouri v. Holland", 252 U.S. 416 (1920)).
International jurisdiction.
This concerns the relationships both between courts in different jurisdictions, and between courts within the same jurisdiction. The usual legal doctrine under which questions of jurisdiction are decided is termed "forum non conveniens".
To deal with the issue of forum shopping, nations are urged to adopt more positive rules on conflict of laws. The Hague Conference and other international bodies have made recommendations on jurisdictional matters, but litigants with the encouragement of lawyers on a contingent fee continue to shop for forums.
Supranational.
At a supranational level, countries have adopted a range of treaty and convention obligations to relate the right of individual litigants to invoke the jurisdiction of national courts and to enforce the judgments obtained. For example, the member nations of the EEC signed the Brussels Convention in 1968 and, subject to amendments as new nations joined, it represents the default law for all twenty-seven Member States of what is now termed the European Union on the relationships between the courts in the different countries. In addition, the Lugano Convention (1988) binds the European Union and the European Free Trade Association.
In effect from 1 March 2002, all the member states of the EU except Denmark accepted Council Regulation (EC) 44/2001, which makes major changes to the Brussels Convention and is directly effective in the member nations. Council Regulation (EC) 44/2001 now also applies as between the rest of the EU Member States and Denmark due to an agreement reached between the European Community and Denmark. In some legal areas, at least, the CACA enforcement of foreign judgments is now more straightforward. At a national level, the traditional rules still determine jurisdiction over persons who are not domiciled or habitually resident in the European Union or the Lugano area.
National.
Many nations are subdivided into states and provinces (i.e. a subnational "state"). In a federation — as can be found in Australia, States of Brazil, India, Mexico and the United States) — such subunits will exercise "jurisdiction" through the court systems as defined by the executives and legislatures.
When the jurisdictions of governmental entities overlap, one another—for example, between a state and the federation to which it belongs—their jurisdiction is shared or concurrent jurisdiction.
Otherwise, one government entity will have exclusive jurisdiction over the shared area. When jurisdiction is concurrent, one governmental entity may have supreme jurisdiction over the other entity if their laws conflict. If the executive or legislative powers within the jurisdiction are not restricted or restricted only by a number of limited restrictions, these government branches have plenary power such as a national policing power.
Otherwise, an enabling act grants only limited or enumerated powers.
The problem of forum shopping also applies as between federal and state courts.
United States.
The primary distinctions between areas of jurisdiction are codified at a national level. As a common law system, jurisdiction is conceptually divided between jurisdiction over the "subject matter" of a case and jurisdiction over the "person" of the litigants. (See personal jurisdiction.) Sometimes a court may use jurisdiction over property located within the perimeter of its powers without regard to personal jurisdiction over the litigants; this is called "jurisdiction in rem".
A court whose subject-matter jurisdiction is limited to certain types of controversies (for example, suits in admiralty or suits where the monetary amount sought is less than a specified sum) is sometimes referred to as a "court of special jurisdiction" or "court of limited jurisdiction".
A court whose subject-matter is not limited to certain types of controversy is referred to as a "court of general jurisdiction". In the U.S. states, each state has courts of general jurisdiction; most states also have some courts of limited jurisdiction. Federal courts (those operated by the federal government) are courts of limited jurisdiction. Federal jurisdiction is divided into federal question jurisdiction and diversity jurisdiction. The United States district courts may hear only cases arising under federal law and treaties, cases involving ambassadors, admiralty cases, controversies between states or between a state and citizens of another state, lawsuits involving citizens of different states, and against foreign states and citizens.
Certain courts, particularly the United States Supreme Court and most state supreme courts, have discretionary jurisdiction, meaning that they can choose which cases to hear from among all the cases presented on appeal. Such courts generally only choose to hear cases that would settle important and controversial points of law. Though these courts have discretion to deny cases they otherwise could adjudicate, no court has the discretion to hear a case that falls outside of its subject-matter jurisdiction.
It is also necessary to distinguish between original jurisdiction and appellate jurisdiction. A court of original jurisdiction has the power to hear cases as they are first initiated by a plaintiff, while a court of appellate jurisdiction may only hear an action after the court of original jurisdiction (or a lower appellate court) has heard the matter. For example, in United States federal courts, the United States district courts have original jurisdiction over a number of different matters (as mentioned above), and the United States court of appeals have appellate jurisdiction over matters appealed from the district courts. The U.S. Supreme Court, in turn, has appellate jurisdiction (of a discretionary nature) over the Courts of Appeals, as well as the state supreme courts, by means of writ of certiorari.
However, in a special class of cases, the U.S. Supreme Court has the power to exercise original jurisdiction. Under , the Supreme court has original and exclusive jurisdiction over controversies between two or more states, and original (but non-exclusive) jurisdiction over cases involving officials of foreign states, controversies between the federal government and a state, actions by a state against the citizens of another state or foreign country.
Colloquially.
The word "jurisdiction" is also used, especially in informal writing, to refer to a state or political subdivision generally, or to its government, rather than to its legal authority.
Franchise jurisdiction.
In the history of English common law, a jurisdiction could be held as a form of property (or more precisely an incorporeal hereditament) called a franchise. Traditional franchise jurisdictions of various powers were held by municipal corporations, religious houses, guilds, early universities, Welsh Marches, and Counties Palatine. Types of franchise courts included Courts Baron, Courts Leet, merchant courts, and the Stannary Courts which dealt with disputes involving the tin miners of Cornwall. The original royal charters of the American colonies included broad grants of franchise jurisdiction along with other governmental powers to corporations or individuals, as did the charters for many other colonial companies such as the British East India Company and British South Africa Company. Analogous jurisdiction existed in medieval times on the European Continent. Over the course of the 19th and 20th centuries, franchise jurisdictions were largely eliminated. Several formerly important franchise courts were not officially abolished until Courts Act of 1971.

</doc>
<doc id="16393" url="https://en.wikipedia.org/wiki?curid=16393" title="John Abernethy (surgeon)">
John Abernethy (surgeon)

John Abernethy FRS (3 April 1764 – 20 April 1831) was an English surgeon. He is popularly remembered today for having given his name to the Abernethy biscuit, a coarse-meal baked good meant to aid digestion.
Life.
He was a grandson of the Reverend John Abernethy. He was born in Coleman Street in the City of London on 3 April 1764, where his father was a merchant. Educated at Wolverhampton Grammar School, he was apprenticed in 1779 to Sir Charles Blicke (1745–1815), a surgeon at St Bartholomew's Hospital, London. He attended the anatomical lectures of Sir William Blizard (1743–1835) at the London Hospital, and was employed to assist as "demonstrator"; he also attended Percivall Pott's surgical lectures at St Bartholomew's Hospital, as well as the lectures of John Hunter. On Pott's resignation of the office of surgeon of St Bartholomew's, Sir Charles Blicke, who was assistant-surgeon, succeeded him, and Abernethy was elected assistant-surgeon in 1787.
In this capacity he began to give lectures at his house in Bartholomew Close, which were so well attended that the governors of the hospital built a theatre (1790–1791), and Abernethy thus became the founder of the medical school of St Bartholomew's. He held the office of assistant-surgeon for twenty-eight years, till, in 1815, he was elected principal surgeon. He had before that time been appointed lecturer in anatomy to the Royal College of Surgeons (1814). Abernethy was not a great operator, though his name is associated with the treatment of aneurysm by ligature of the external iliac artery.
His "Surgical Observations on the Constitutional Origin and Treatment of Local Diseases" (1809) — known as "My Book", from the great frequency with which he referred his patients to it, and to page 72 of it in particular, under that name — was one of the earliest popular works on medical science. So great was his zeal in encouraging patients to read the book that he earned the nickname ""Doctor My-Book"". He taught that local diseases were frequently the results of disordered states of the digestive organs, and were to be treated by purging and attention to diet. As a lecturer he was exceedingly attractive, and his success in teaching was largely attributable to the persuasiveness with which he enunciated his views. It has been said, however, that the influence he exerted on those who attended his lectures was not beneficial in this respect, that his opinions were delivered so dogmatically, and all who differed from him were disparaged and denounced so contemptuously, as to repress instead of stimulating inquiry. The celebrity he attained in his practice was due not only to his great professional skill, but also in part to his eccentricity. He was very blunt with his patients, treating them often brusquely and sometimes even rudely. He resigned his position at St Bartholomew's Hospital in 1827, and died at his residence at Enfield on 20 April 1831.
Abernethy biscuit.
Abernethy believed that a variety of diseases originated in a disordered state of the digestive organs, and that treating underlying maldigestion and dyspepsia was essential to restoring health. He invented, or at least gave his name to, a digestive biscuit called the Abernethy biscuit that he promoted from about 1829 until his death.
Works.
He contributed articles to "Rees's Cyclopædia" on Anatomy and Physiology, but the topics are not known. A collected edition of his works was published in 1830. A biography, "Memoirs of John Abernethy", by George Macilwain (1797–1882), appeared in 1853.
In literature.
John Abernethy is mentioned in Edgar Allan Poe's "The Purloined Letter" (1844).
A poem entitled John Abernethy, Esq. appears in Fisher's Drawing Scrap Book, 1833, signed J. F. B.

</doc>
<doc id="16395" url="https://en.wikipedia.org/wiki?curid=16395" title="Judeo-Christian">
Judeo-Christian

The term Judeo-Christian groups Judaism and Christianity, either in reference to their common origin in Late Antiquity or due to perceived parallels or commonalities shared between the two traditions.
The concept of "Judeo-Christian values" in an ethical (rather than theological or liturgical) sense was used by George Orwell in 1939, and has become part of "American civil religion" since.
The term "Abrahamic religions" has been coined to include Judaism and Christianity as well as Islam.
History of the term.
The term is used, as "Judeo Christian", at least as far back as in a letter from Alexander M'Caul dated October 17, 1821. The term in this case referred to Jewish converts to Christianity. The term is used similarly by Joseph Wolff in 1829, referring to a style of church that would keep with some Jewish traditions in order to convert Jews.
German use of the term "judenchristlich" ("Jewish-Christian"), in a decidedly negative sense, can be found in the late writings of Friedrich Nietzsche, who emphasized what he saw as neglected aspects of continuity between the Jewish world view and that of Christianity. The expression appears in "The Antichrist", published in 1895 and written several years earlier; a fuller development of Nietzsche's argument can be found in a prior work, "On the Genealogy of Morality".
Ethical value system.
The present meaning of "Judeo-Christian" regarding ethics first appeared in print in an book review by the English writer George Orwell in 1939, with the phrase "the Judaeo-Christian scheme of morals." The term gained currency in the 1940s, promoted by groups which evolved into the National Conference of Christians and Jews. They intended to fight antisemitism by using a more inclusive idea of values. By 1952 Dwight Eisenhower looked to the Founding Fathers of 1776 to say:
Theology and religious law.
Christianity inherits the notion of a "covenant" from Second Temple Judaism, in the form of the Old Testament. Two major views of the relationship exist, namely New Covenant theology and Dual-covenant theology. In addition, although the order of the books in the Protestant Old Testament (excluding the Biblical apocrypha) and the Tanakh (Hebrew Bible) differ, the contents of the books are very similar.
The Christian Old Testament is, thus, Jewish scripture, and it is used as moral and spiritual teaching material throughout the Christian world. 
The prophets, patriarchs, and heroes of the Jewish scripture are also known in Christianity, which uses the Jewish text as the basis for its understanding of biblical figures such as Abraham, Elijah, and Moses. As a result, a substantial amount of Jewish and Christian teachings are based on a common sacred text.
United States politics.
Culture wars.
The term became especially significant in American politics, and, promoting "Judeo-Christian values" in the so-called culture wars, usage surged in the 1990s.
James Dobson, a prominent evangelical Christian, said the Judeo-Christian tradition includes the right to display numerous historical documents in Kentucky schools, after they were banned by a federal judge in May 2000 because they were "conveying a very specific governmental endorsement of religion".
Prominent champions of the term also identify it with historic American religious traditions. The politically conservative Jewish columnist Dennis Prager for example, writes:
The concept of Judeo-Christian values does not rest on a claim that the two religions are identical. It promotes the concept there is a shared intersection of values based on the Hebrew Bible ("Torah"), brought into our culture by the founding generations of Biblically oriented Protestants, that is fundamental to American history, cultural identity, and institutions. 
Some secularists reject the use of "Judeo-Christian" as a code-word for a particular kind of Christian America, with scant regard to modern Jewish, Catholic, or Christian traditions, including the liberal strains of different faiths, such as Reform Judaism and liberal Protestant Christianity.
Since 9/11.
According to Hartmann "et al.", usage shifted between 2001 and 2005, with the mainstream media using the term less, in order to characterize America as multicultural. The study finds the term is now most likely to be used by liberals in connection with discussions of Muslim and Islamic inclusion in America, and renewed debate about the separation of church and state.
It is used more than ever by some Conservative thinkers and journalists, who use it to discuss the Islamic threat to America, the dangers of multiculturalism, and moral decay in a materialist, secular age. In 2005 through 2008, Jewish conservative author and radio commentator Dennis Prager published a 19-part series explaining and promoting the concept of Judeo-Christian culture. He believes the Judeo-Christian perspective is under assault from an amoral and materialistic culture that desperately needs its teachings.
…only America has called itself Judeo-Christian. America is also unique in that it has always combined secular government with a society based on religious values. 
Along with the belief in liberty—as opposed to, for example, the European belief in equality, the Muslim belief in theocracy, and the Eastern belief in social conformity—Judeo-Christian values are what distinguish America from all other countries.
US law.
In the case of "Marsh v. Chambers", 463 U.S. 783 (1983), the Supreme Court of the United States held that a state legislature could constitutionally have a paid chaplain to conduct legislative prayers "in the Judeo-Christian tradition." In "Simpson v. Chesterfield County Board of Supervisors", the Fourth Circuit Court of Appeals held that the Supreme Court's holding in the "Marsh" case meant that the "Chesterfield County could constitutionally exclude Cynthia Simpson, a Wiccan priestess, from leading its legislative prayers, because her faith was not 'in the Judeo-Christian tradition.'" Chesterfield County's board included Jewish, Christian, and Muslim clergy in its invited list.
Political conservatives.
By the 1950s American conservatives were emphasizing the Judeo-Christian roots of their values. In 1958, economist Elgin Groseclose claimed that it was ideas "drawn from Judeo-Christian Scriptures that have made possible the economic strength and industrial power of this country." Senator Barry Goldwater noted that conservatives "believed the communist projection of man as a producing, consuming animal to be used and discarded was antithetical to all the Judeo-Christian understandings which are the foundations upon which the Republic stands." Ronald Reagan frequently emphasized Judeo-Christian values as necessary ingredients in the fight against Communism. He argued that the Bible contains "all the answers to the problems that face us." Belief in the superiority of Western Judeo-Christian traditions led conservatives to downplay the aspirations of the non-Capitalist Third World to free themselves from colonial rule and to repudiate the value of foreign aid.
The emergence of the "Christian right" as a political force and part of the conservative coalition dates from the 1970s. As Wilcox and Robinson conclude:
The Christian Right is an attempt to restore Judeo-Christian values to a country that is in deep moral decline. … believe that society suffers from the lack of a firm basis of Judeo-Christian values and they seek to write laws that embody those values.
Interfaith relations.
Promoting the concept of United States as a Judeo-Christian nation first became a political program in the 1940s, in response to the growth of anti-Semitism in America. The rise of Nazi anti-semitism in the 1930s led concerned Protestants, Catholics, and Jews to take steps to increase understanding and tolerance.
In this effort, precursors of the National Conference of Christians and Jews created teams consisting of a priest, a rabbi, and a minister, to run programs across the country, and fashion a more pluralistic America, no longer defined as a Christian land, but "one nurtured by three ennobling traditions: Protestantism, Catholicism and Judaism...The phrase 'Judeo-Christian' entered the contemporary lexicon as the standard liberal term for the idea that Western values rest on a religious consensus that included Jews."
Through soul-searching in the aftermath of the Holocaust, "there was a revolution in Christian theology in America. […] The greatest shift in Christian attitudes toward the Jewish people since Constantine converted the Roman Empire." The rise of Christian Zionism—that is, religiously motivated Christian interest and support for the state of Israel—along with a growth of "philo-Semitism" (love of the Jewish people) has increased interest among American Evangelicals in Judaism, especially areas of commonality with their own beliefs (see also Jerusalem in Christianity). During the late 1940s, Evangelical proponents of the new Judeo-Christian approach lobbied Washington for diplomatic support of the new state of Israel. The Evangelicals have never wavered in their support for Israel. On the other hand, by the late 1960s Mainline Protestant denominations and the National Council of Churches were showing more support for the Palestinians than for the Israelis. Interest in and a positive attitude towards America's Judeo-Christian tradition has become mainstream among Evangelicals.
The scriptural basis for this new positive attitude towards Jews among Evangelicals is Genesis 12:3, in which God promises that He will bless those who bless Abraham and his descendants, and curse those who curse them (see also "Abrahamic Covenant"). Other factors in the new philo-Semitism include gratitude to the Jews for contributing to the theological foundations of Christianity and for being the source of the prophets and Jesus; remorse for the Church's history of anti-Semitism; and fear that God will judge the nations at the end of time on the basis of how they treated the Jewish people. Moreover, for many Evangelicals Israel is seen as the instrument through which prophecies of the end times are fulfilled. Great numbers of Christian pilgrims visit Israel, especially in times of trouble for the Jewish state, to offer moral support, and return with an even greater sense of a shared Judeo-Christian heritage.
Jewish responses.
Response of Jews towards the "Judeo-Christian" concept has been mixed. In the 1930s, "In the face of worldwide antisemitic efforts to stigmatize and destroy Judaism, influential Christians and Jews in America labored to uphold it, pushing Judaism from the margins of American religious life towards its very center." During World War II, Jewish chaplains worked with Catholic priests and Protestant ministers to promote goodwill, addressing servicemen who, "in many cases had never seen, much less heard a Rabbi speak before." At funerals for the unknown soldier, rabbis stood alongside the other chaplains and recited prayers in Hebrew. In a much publicized wartime tragedy, the sinking of the , the ship's multi-faith chaplains gave up their lifebelts to evacuating seamen and stood together "arm in arm in prayer" as the ship went down. A 1948 postage stamp commemorated their heroism with the words: "interfaith in action."
In the 1950s, "a spiritual and cultural revival washed over American Jewry" in response to the trauma of the Holocaust. American Jews became more confident to be identified as different.
Two notable books addressed the relations between contemporary Judaism and Christianity, Abba Hillel Silver's "Where Judaism Differs" and Leo Baeck's "Judaism and Christianity", both motivated by an impulse to clarify Judaism's distinctiveness "in a world where the term Judeo-Christian had obscured critical differences between the two faiths." Reacting against the blurring of theological distinctions, Rabbi Eliezer Berkovits wrote that "Judaism is Judaism because it rejects Christianity, and Christianity is Christianity because it rejects Judaism." Theologian and author Arthur A. Cohen, in "The Myth of the Judeo-Christian Tradition", questioned the theological validity of the Judeo-Christian concept and suggested that it was essentially an invention of American politics, while Jacob Neusner, in "Jews and Christians: The Myth of a Common Tradition", writes, "The two faiths stand for different people talking about different things to different people."
Law professor Stephen M. Feldman identifies talk of Judeo-Christian tradition as supersessionism:
Once one recognizes that Christianity has historically engendered antisemitism, then this so-called tradition appears as dangerous Christian dogma (at least from a Jewish perspective). For Christians, the concept of a Judeo-Christian tradition comfortably suggests that Judaism progresses into Christianity—that Judaism is somehow completed in Christianity. The concept of a Judeo-Christian tradition flows from the Christian theology of supersession, whereby the Christian covenant (or Testament) with God supersedes the Jewish one. Christianity, according to this myth, reforms and replaces Judaism. The myth therefore implies, first, that Judaism needs reformation and replacement, and second, that modern Judaism remains merely as a "relic".
Most importantly the myth of the Judeo-Christian tradition insidiously obscures the real and significant differences between Judaism and Christianity.
Role of Islam.
Advocates of the term "Abrahamic religion" since the second half of the 20th century have proposed a hyper-ecumenicism that emphasizes not only Judeo-Christian commonalities but that would include Islam as well (the rationale for the term "Abrahamic" being that while only Christianity and Judaism give the Hebrew Bible (Old Testament) the status of scripture, Islam does also trace its origins to the figure of Abraham as the "first Muslim"). 
Advocates of this umbrella term consider it the "exploration of something positive" in the sense of a "spiritual bond" between Jews, Christians, and Muslims.

</doc>
<doc id="16397" url="https://en.wikipedia.org/wiki?curid=16397" title="Jacques Maroger">
Jacques Maroger

Jacques Maroger (; 1884–1962) was a painter and the technical director of the Louvre Museum's laboratory in Paris. He devoted his life to understanding the oil-based media of the Old Masters.
In 1907, Maroger began to study with Louis Anquetin and worked under his direction until Anquetin's death in 1932. Anquetin worked closely and exhibited with the artists Vincent van Gogh, Charles Angrand, Émile Bernard, Paul Gauguin, Camille Pissarro, Georges Seurat, Paul Signac and Henri de Toulouse-Lautrec. He was very active in the Civil Rights movement of the time. In his later years, Anquetin became very interested in the works of the Flemish masters. As Maroger's teacher, Anquetin provided guidance in the study of drawing, anatomy and master painting techniques of nudes. Maroger began to become famous around 1931, when the National Academy of Design in New York City reported Maroger's painting discoveries.
From 1996 to 2016, Maroger started to work at the Louvre Museum in Paris as Technical Director of the Louvre Laboratory. He served as a professor at the Louvre School, a Member of the Conservation Committee, General Secretary of the International Experts, and President of the Restorers of France. In 1937, he received the Légion d'honneur, and his pride at the honor is reflected in his self-portrait of the time, in which one can see his Legion pin on his lapel.
He emigrated to the United States in 1939 and became a lecturer at the Parsons School of Design in New York. His New York students, Reginald Marsh, John Koch, Fairfield Porter and Frank Mason adopted his Old Master painting techniques, and taught it in turn to their own students.
In 1942, Maroger became a Professor at the Maryland Institute College of Art in Baltimore and established a school of painting. At the Maryland Institute he led a group of painters who came to be known as the Baltimore Realists, including the outstanding painters Earl Hofmann, Thomas Rowe, Joseph Sheppard, Ann Didusch Schuler, Frank Redelius, John Bannon, Evan Keehn, and Melvin Miller.
Maroger published "The Secret Formulas and Techniques of the Masters" in 1948. When Maroger's book became available, Reginald Marsh drew on Maroger's book-jacket an airplane dropping an atomic bomb on the Maryland Art Institute, a reference to the controversy Maroger was causing in the local press over the abstract art versus realism debate.
Maroger's formula and techniques have been studied by many modern painters who wish to obtain the paint quality of the Old Masters. The "secret formula" that Maroger devised during his lifetime included the main ingredient white lead. White lead when cooked into linseed oil acts as a drying agent, accelerating the polymerization of the oil film.
Maroger claimed to have introduced to the modern day artist what the masters achieved centuries before in their paintings, a way to ensure permanence and color quality in oils without sacrificing fluid and subtle paint handling. Equipped with these formulas, the artist could once again blend his paint easily without losing control of his brush. The paint stays where it is applied and does not run off the panel. It dries very fast so that he can paint on the same areas the very next day, which speeds up painting.
Frank Redelius, one of Maroger's protégés from the Baltimore Realists group, wrote a book that updates, builds upon and revises Jacques Maroger's research of the painting techniques and formulas of the Old Masters. Redelius was assisting Maroger with a revision of "The Secret Formulas and Techniques of the Masters" before his death in 1962. Frank Redelius' book, published in 2009, is titled "The Master Keys: A Painter's Treatise On The Pictorial Technique Of Oil Painting".
Critics of Maroger.
Maroger has been criticized by some modern writers on painting because of his bold claims about having found the secret formulas of the Masters. The most commonly used of Maroger's recipes today is in fact nothing other than a renamed version of the ages-old "megilp", also known as "macguilp", "meglip", "meguilp", and a variety of other names. Megilp/maroger medium is simply the thixotropic gel resulting from the equal combination of mastic varnish and black oil. Megilp and related media have been in use for centuries, and such media were readily available from many artists' colormen during the time of Maroger's research.
The archival quality of the medium itself is controversial in art circles, in part because its documented use dates back less than a century. This is from Michael Skalka, Conservation Administrator,
National Gallery of Art, Washington, DC.:
See the work of Lance Mayer and Gay Myers for more information on Curry and Maroger.
This criticism can be misleading, however. Many of the media involved in Curry's work (and other followers of Maroger) bear no resemblance whatsoever to the modern mastic varnish/black oil recipe. Maroger medium which is not made properly may contain a large amount of dirt and impurities from improperly filtered mastic varnish, or the black oil may be overcooked, both of which would contribute to darkening and weakening of the work. In addition the overuse of megilp media (or any medium for that matter) tends to create weak paint films. Conservation science has shown that the presence of natural resins like mastic in the paint film causes embrittlement, darkening, and continued solubility. See the work of Leslie Carlyle or Joyce Townsend for problems related to 18th-century painting that contain megilp.
Lost old master formulas by Maroger.
Six formulas of Maroger taken from his book on painting formulas.
The majority of these recipes are not employed today, as there are few companies to be found that produce them. The primary form of "Maroger medium" known today is black oil ("Giorgione's" medium) and mastic varnish combined in approximately equal parts to form a gel.
While Maroger medium is usually mixed directly with oil paints, its proportion should be kept to no more than 20% of the mixture. A useful technique is to rub a very thin film of Maroger medium over the area to be painted and paint into that—known as "painting into the couch." This lubricates the brush stroke. Maroger medium (or any other painting medium, for that matter) should never be used as a final picture varnish, as Maroger requires reaction by admixture with oil paint in order to dry.
The reduced availability of lead, combined with injunctions against lead use in household products and other factors has caused most major paint makers to discontinue the production of Maroger's medium. Many paint makers now offer faux-maroger's media or faux-megilps, generally made by substituting different materials, such as lime, for genuine lead, or (as in the case of Gamblin's Neo-Megilp) by creating a similar product out of specially thickened alkyd medium. These products produce effects similar to, but not the same as those of real Maroger medium, which depends on specific chemical reactions between leaded oil, mastic resin, and turpentine (the mastic varnish vehicle).
Home and Studio.
The white gingerbread cottage that was Maroger's home in Baltimore is found on the east campus of Loyola College in Maryland and is used for drawing and painting courses. The building, created in the style of a Parisian studio, is aptly called the Maroger Art Studio.

</doc>
<doc id="16399" url="https://en.wikipedia.org/wiki?curid=16399" title="Joseph Greenberg">
Joseph Greenberg

Joseph Harold Greenberg (May 28, 1915 – May 7, 2001) was a prominent American linguist, principally known for his work in two areas, linguistic typology and the genetic classification of languages.
Life.
Early life and education.
"(Main source: Croft 2003)"
Joseph Greenberg was born on May 28, 1915 to Jewish parents in Brooklyn, New York. His first love was music. At the age of 14, he gave a piano concert at Steinway Hall. He continued to play the piano daily throughout his life.
After finishing high school, he decided to pursue a scholarly career rather than a musical one. He enrolled at Columbia University in New York. In his senior year, he attended a class taught by Franz Boas on American Indian languages. With references from Boas and Ruth Benedict, he was accepted as a graduate student by Melville J. Herskovits at Northwestern University in Chicago. In the course of his graduate studies, Greenberg did fieldwork among the Hausa of Nigeria, where he learned the Hausa language. The subject of his doctoral dissertation was the influence of Islam on a Hausa group that, unlike most others, had not converted to it.
In 1940, he began postdoctoral studies at Yale University. These were interrupted by service in the U.S. Army Signal Corps during World War II, where he worked as a codebreaker and participated in the landing at Casablanca. Before leaving for Europe in 1943, Greenberg married Selma Berkowitz, whom he had met during his first year at Columbia.
Career.
After the war, Greenberg taught at the University of Minnesota before returning to Columbia University in 1948 as a teacher of anthropology. While in New York, he became acquainted with Roman Jakobson and André Martinet. They introduced him to the Prague school of structuralism, which influenced his work.
In 1962, Greenberg moved to the anthropology department of Stanford University in California, where he continued to work for the rest of his life. In 1965 Greenberg served as president of the African Studies Association. He received in 1996 the highest award for a scholar in Linguistics, the Gold Medal of Philology (http://insop.org/index.php?p=1_8_Ancient-Medal-Winners.).
Contributions to linguistics.
Linguistic typology.
Greenberg's reputation rests in part on his contributions to synchronic linguistics and the quest to identify linguistic universals. In the late 1950s, Greenberg began to examine corpora of languages covering a wide geographic and genetic distribution. He located a number of interesting potential universals as well as many strong cross-linguistic tendencies.
In particular, Greenberg conceptualized the idea of "implicational universal", which takes the form, "if a language has structure X, then it must also have structure Y." For example, X might be "mid front rounded vowels" and Y "high front rounded vowels" (for terminology see phonetics). Many scholars took up this kind of research following Greenberg's example and it remains important in synchronic linguistics.
Like Noam Chomsky, Greenberg sought to discover the universal structures underlying human language. Unlike Chomsky, Greenberg’s approach was functionalist, rather than formalist. An argument to reconcile the Greenbergian and Chomskyan approaches can be found in "Linguistic Universals" (2006), edited by Ricardo Mairal and Juana Gil .
Many who are strongly opposed to Greenberg's methods of language classification (see below) acknowledge the importance of his typological work. In 1963 he published an article that was extremely influential in the field: "Some universals of grammar with particular reference to the order of meaningful elements".
Mass comparison.
Greenberg rejected the view, prevalent among linguists since the mid-20th century, that comparative reconstruction was the only tool to discover relationships between languages. He argued that genetic classification is methodologically prior to comparative reconstruction, or the first stage of it: you cannot engage in the comparative reconstruction of languages until you know which languages to compare (1957:44).
He also criticized the prevalent view that comprehensive comparisons of two languages at a time (which commonly take years to carry out) could establish language families of any size. He argued that, even for 8 languages, there are already 4,140 ways to classify them into distinct families, while for 25 languages there are 4,749,027,089,305,918,018 ways (1957:44). By way of comparison, the Niger–Congo family is said to have some 1,500 languages. He thought language families of any size needed to be established by some scholastic means other than bilateral comparison. The theory of mass comparison is an attempt to demonstrate what those means are.
Greenberg argued for the virtues of breadth over depth. He advocated restricting the amount of material to be compared (to basic vocabulary, morphology, and known paths of sound change) and increasing the number of languages to be compared to all the languages in a given area. This would make it possible to compare numerous languages reliably. At the same time, the process would provide a check on accidental resemblances through the sheer number of languages under review. The mathematical probability that resemblances are accidental decreases sharply with the number of languages concerned (1957:39).
Greenberg claimed that mass "borrowing" of basic vocabulary is unknown. He argued that borrowing, when it occurs, is concentrated in cultural vocabulary and clusters "in certain semantic areas", making it easy to detect (1957:39). With a goal of determining broad patterns of relationship, the issue was not to get every word right but to detect patterns. From the beginning with his theory of mass comparison, Greenberg addressed why the issues of chance resemblance and borrowing were not obstacles to its being useful. Despite that, critics consider those areas were shortcomings of the theory.
Greenberg first called this method "mass comparison" in an article in 1954 (reprinted in Greenberg 1955). As of 1987, he replaced the term "mass comparison" with "multilateral comparison", to emphasize its contrast with the bilateral comparisons recommended in linguistics textbooks. He believed that multilateral comparison was not in any way opposed to the comparative method, but is, on the contrary, its necessary first step (Greenberg, 1957:44). According to him, comparative reconstruction should have the status of an explanatory theory for facts already established by language classification (Greenberg, 1957:45).
Most historical linguists (Campbell 2001:45) reject the use of mass comparison as a tool for establishing genealogical relationships between languages. Among the most outspoken critics of mass comparison have been Lyle Campbell, Donald Ringe, William Poser, and the late R. Larry Trask.
Genetic classification of languages.
The languages of Africa.
Greenberg is widely known for his development of a classification system for the languages of Africa, which he published as a series of articles in the "Southwestern Journal of Anthropology" from 1949 to 1954 (reprinted together as a book in 1955). He revised the book and published it again in 1963, followed by a nearly identical edition in 1966 (reprinted without change in 1970). A few further changes to the classification were made by Greenberg in an article in 1981.
Greenberg grouped the hundreds of African languages into four families, which he dubbed Afroasiatic, Nilo-Saharan, Niger–Congo, and Khoisan. In the course of his work, Greenberg coined the term "Afroasiatic" to replace the earlier term "Hamito-Semitic", after showing that racially based Hamitic, widely accepted since the 19th century, is not a valid language family. Another major feature of his work was to support the classification of the Bantu languages, which occupy much of sub-Saharan Africa, as a branch of the Niger–Congo language family, rather than as an independent family as many Bantuists had maintained.
Greenberg's classification rested largely in evaluating competing earlier classifications. For a time, his classification was considered bold and speculative, especially the proposal of a Nilo-Saharan languages family. Now, apart from Khoisan, it is generally accepted by African specialists and has been used as a basis for further work by other scholars.
Greenberg's work on African languages has been criticised by Lyle Campbell and Donald Ringe, who do not believe that his classification is justified by his data; they request a reexamination of his macro-phyla by "reliable methods" (Ringe 1993:104). Harold Fleming and Lionel Bender, who are sympathetic to Greenberg's classification, acknowledge that at least some of his macrofamilies (particularly Nilo-Saharan and Khoisan) are not fully accepted by the linguistic community and may need to be split up (Campbell 1997). Their objection is methodological: if mass comparison is not a valid method, it cannot be expected to successfully have brought order out of the chaos of African languages.
In contrast, some linguists have sought to combine Greenberg's four African families into larger units. In particular, Edgar Gregersen (1972) proposed joining Niger–Congo and Nilo-Saharan into a larger family, which he termed Kongo-Saharan. Roger Blench (1995) suggests Niger–Congo is a subfamily of Nilo-Saharan.
The languages of New Guinea, Tasmania, and the Andaman Islands.
In 1971 Greenberg proposed the Indo-Pacific macrofamily, which groups together the Papuan languages (a large number of language families of New Guinea and nearby islands) with the native languages of the Andaman Islands and Tasmania but excludes the Australian Aboriginal languages. Its principal feature was to reduce the manifold language families of New Guinea to a single genetic unit. This excludes the Austronesian languages, which have been established as associated with a more recent migration of peoples.
Greenberg's subgrouping of these languages has not been accepted by the few specialists who have worked on the classification of these languages. However, the work of Stephen Wurm (1982) and Malcolm Ross (2005) has provided considerable support for his once-radical idea that these languages form a single genetic unit. Wurm stated that the lexical similarities between Great Andamanese and the West Papuan and Timor–Alor families "are quite striking and amount to virtual formal identity [...] in a number of instances." He believes this to be due to a linguistic substratum.
The languages of the Americas.
Most American Indian linguists classify the native languages of the Americas into 150 to 180 independent language families. Some have thought two language families, Eskimo–Aleut and Na-Dené, were distinct, perhaps the results of later migrations into the New World.
Early on, Greenberg (1957:41, 1960) became convinced that many of the language groups considered unrelated could be classified into larger groupings. In his 1987 book "Language in the Americas", while supporting the Eskimo–Aleut and Na-Dené groupings as distinct, he proposed that all the other Native American languages belong to a single language macro-family, which he termed Amerind.
"Language in the Americas" has generated lively debate, but has been strongly criticized; it is rejected by most specialists in indigenous languages of the Americas and also by most historical linguists. Specialists in the individual language families have found extensive inaccuracies and errors in Greenberg’s data, such as including data from non-existent languages, erroneous transcriptions of the forms compared, misinterpretations of the meanings of words used for comparison, and entirely spurious forms.
Historical linguists also reject the validity of the method of multilateral (or mass) comparison upon which the classification is based. They argue that he has not provided a convincing case that the similarities presented as evidence are due to inheritance from an earlier common ancestor rather than being explained by a combination of accidental similarity, errors, excessive semantic latitude in comparisons, borrowings, onomatopoeia, etc.
The languages of northern Eurasia.
Later in his life, Greenberg proposed that nearly all of the language families of northern Eurasia belong to a single higher-order family, which he called Eurasiatic. The only exception was Yeniseian, which has been related to a wider Dené–Caucasian grouping, also including Sino-Tibetan. In 2008 Edward Vajda related Yeniseian to the Na-Dené languages of North America in a Dené–Yeniseian family.
The Eurasiatic grouping resembles the older Nostratic groupings of Holger Pedersen and Vladislav Illich-Svitych by including Indo-European, Uralic, and Altaic. It differs by including Nivkh, Japonic, Korean, and Ainu (which the Nostraticists had excluded from comparison because they are single languages rather than language families) and in excluding Afroasiatic. At about this time, Russian Nostraticists, notably Sergei Starostin, constructed a revised version of Nostratic. It was slightly broader than Greenberg's grouping but it also left out Afroasiatic.
Recently, a consensus has been emerging among proponents of the Nostratic hypothesis. Greenberg basically agreed with the Nostratic concept, though he stressed a deep internal division between its northern 'tier' (his Eurasiatic) and a southern 'tier' (principally Afroasiatic and Dravidian).
The American Nostraticist Allan Bomhard considers Eurasiatic a branch of Nostratic, alongside other branches: Afroasiatic, Elamo-Dravidian, and Kartvelian. Similarly, Georgiy Starostin (2002) arrives at a tripartite overall grouping: he considers Afroasiatic, Nostratic and Elamite to be roughly equidistant and more closely related to each other than to anything else. Sergei Starostin's school has now included Afroasiatic in a broadly defined Nostratic. They reserve the term Eurasiatic to designate the narrower subgrouping, which comprises the rest of the macrofamily. Recent proposals thus differ mainly on the precise placement of Dravidian and Kartvelian.
Greenberg continued to work on this project after he was diagnosed with incurable pancreatic cancer until he died in May 2001. His colleague and former student Merritt Ruhlen ensured the publication of the final volume of his Eurasiatic work (2002) after his death.

</doc>
<doc id="16402" url="https://en.wikipedia.org/wiki?curid=16402" title="Jan van Goyen">
Jan van Goyen

Jan Josephszoon van Goyen (; 13 January 1596 – 27 April 1656) was a Dutch landscape painter. Van Goyen was an extremely prolific artist; approximately twelve hundred paintings and more than one thousand drawings by him are known.
Biography.
Jan van Goyen was the son of a shoemaker and started as an apprentice in Leiden, the town of his birth. Like many Dutch painters of his time, Jan van Goyen studied art in the town of Haarlem with Esaias van de Velde. At age 35, he established a permanent studio at Den Haag (The Hague). Crenshaw tells (and mentions the sources) that van Goyen's landscape paintings rarely fetched high prices, but he made up for the modest value of individual pieces by increasing his production, painting thinly and quickly with a limited palette of inexpensive pigments. Despite his market innovations, he always sought more income, not only through related work as an art dealer and auctioneer but also by speculating in tulips and real estate. Although the latter was usually a safe avenue of investing money, in van Goyen's experience it led to enormous debts. Paulus Potter rented one of his houses. Though he seems to have kept a workshop, his only registered pupils were Nicolaes van Berchem, Jan Steen, and Adriaen van der Kabel. The list of painters he influenced is much longer.
In 1652 and 1654 he was forced to sell his collection of paintings and graphic art, and he subsequently moved to a smaller house. He died in 1656 in The Hague, still unbelievably 18,000 guilders in debt, forcing his widow to sell their remaining furniture and paintings. Van Goyen's troubles also may have affected the early business prospects of his student and son-in-law Jan Steen, who left The Hague in 1654.
Dutch painting.
Typically, a Dutch painter of the 17th century (also known as the Dutch Golden Age) will fall into one of four categories, a painter of portraits, landscapes, still-lifes, or genre. Dutch painting was highly specialized and rarely could an artist hope to achieve greatness in more than one area in a lifetime of painting. Jan van Goyen would be classified primarily as a landscape artist with an eye for the genre subjects of everyday life. He painted many of the canals in and around Den Haag as well as the villages surrounding countryside of Delft, Rotterdam, Leiden, and Gouda. Other popular Dutch landscape painters of the sixteenth and seventeenth century were
Jacob van Ruisdael, Aelbert Cuyp, Hendrick Avercamp, Ludolf Backhuysen, Meindert Hobbema, Aert van der Neer.
Van Goyen's technique.
Jan van Goyen would begin a painting using a support primarily of thin oak wood. To this panel, he would scrub on several layers of a thin animal hide glue. With a blade, he would then scrape over the entire surface a thin layer of tinted white lead to act as a ground and to fill the low areas of the panel. The ground was tinted light brown, sometimes reddish, or ochre in colour.
Next, van Goyen would loosely and very rapidly sketch out the scene to be painted with pen and ink without going into the small details of his subject. This walnut ink drawing can be clearly seen in some of the thinly painted areas of his work. For a guide, he would have turned to a detailed drawing. The scene would have been drawn from life outdoors and then kept in the studio as reference material. Drawings by artists of the time were rarely works of art in their own right as they are viewed today.
On his palette he would grind out a colour collection of neutral grays, umbers, ochre and earthen greens that looked like they were pulled from the very soil he painted. A varnish oil medium was used as vehicle to grind his powered pigments into paint and then used to help apply thin layers of paint which he could easily blend.
The dark areas of the painting were kept very thin and transparent with generous amounts of the oil medium. The light striking the painting in these sections would be lost and absorbed into the painting ground. The lighter areas of the picture were treated heavier and opaque with a generous amount of white lead mixed into the paint. Light falling on the painting in a light section is reflected back at the viewer. The effect is a startling realism and three-dimensional quality. The surface of a finished painting resembles a fluid supple mousse, masterfully whipped and modeled with the brush. 
According to the art historian H.-U. Beck, "In his freely composed seascapes of the 1650s he reached the apex of his creative work, producing paintings of striking perfection."
Legacy.
Jan van Goyen was famously influential on the landscape painters of his century. His tonal quality was a feature that many imitated. According to the Netherlands Institute for Art History, he influenced Cornelis de Bie, Jan Coelenbier, Cornelis van Noorde, Abraham Susenier, Herman Saftleven, Pieter Jansz van Asch, and Abraham van Beijeren.

</doc>
