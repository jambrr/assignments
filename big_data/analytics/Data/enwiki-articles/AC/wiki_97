<doc id="18376" url="https://en.wikipedia.org/wiki?curid=18376" title="Loran-C">
Loran-C

Loran-C was a hyperbolic radio navigation system which allowed a receiver to determine its position by listening to low frequency radio signals transmitted by fixed land-based radio beacons. Loran-C combined two different techniques to provide a signal that was both long-range and highly accurate, traits that had formerly been at odds. The downside was the expense of the equipment needed to interpret the signals, which meant that Loran-C was used primarily by militaries after it was first introduced in 1957.
By the 1970s the electronics needed to implement Loran-C had been dramatically reduced due to the introduction of solid state radio electronics, and especially the use of early microcontrollers to interpret the signal. Low-cost and easy-to-use Loran-C units became common from the late 1970s, especially in the early 1980s, leading to the earlier LORAN system being turned off in favour of installing more Loran-C stations around the world. Loran-C became one of the most common and widely used navigation systems for large areas of North America, Europe, Japan and the entire Atlantic and Pacific areas. The Soviet Union operated a nearly identical system, CHAYKA.
The introduction of civilian satellite navigation in the 1990s led to a very rapid drop-off in Loran-C use. Discussions about the future of Loran-C began in the 1990s, and several turn-off dates were announced and then cancelled. In 2010 the US and Canadian systems were shut down, along with shared Loran-C/CHAYKA stations with Russia. Several other chains remained active, and some had been upgraded for continued use. At the end of 2015, navigation chains in most of Europe were turned off. In December 2015 in the US, there was also renewed discussion of funding an eLoran system and NIST was offering to fund development of a microchip sized eLoran receiver for distribution of timing signals.
History.
Loran-A.
The original LORAN was proposed by Alfred Lee Loomis at a meeting of the Microwave Committee. The US Army Air Corps was interested in the concept for aircraft navigation, and after some discussion they returned a requirement for a system offering accuracy of about at a range of , and a maximum range as great as for high-flying aircraft. The Microwave Committee, by this time organized into what would become the Radiation Laboratory, took up development as Project 3. During the initial meetings a member of the UK liaison team, Taffy Bowen, mentioned that he was aware the British were also working on a similar concept, but had no information on its performance.
The development team, led by Loomis, made rapid progress on the transmitter design and tested several systems during 1940 before settling on a 3 MHz design. Extensive signal-strength measurements were made by mounting a conventional radio receiver in a station wagon and driving around the eastern states. However, the custom receiver design and its associated cathode ray tube displays proved to be a bigger problem. In spite of several efforts to design around the problem, instability in the display prevented accurate timing measurements.
By this time the team had become much more familiar with the British Gee system, and were aware of their work on "strobes", a time base generator that produced well-positioned "pips" on the display that could be used for accurate measurement. They met with the Gee team in 1941, and immediately adopted this solution. They also found that Project 3 and Gee called for almost identical systems, with similar performance, range and accuracy. But Gee had already completed basic development and was entering into initial production, making Project 3 superfluous.
In response, the Project 3 team told the Army Air Force to adopt Gee, and realigned their own efforts to provide long-range navigation on the oceans. This led to US Navy interest, and a series of experiments quickly demonstrated that systems using the basic Gee concept but operating at a much lower frequency around 2 MHz would offer reasonable accuracy on the order of a few miles over distances on the order of , at least at night when signals of this frequency range were able to skip off the ionosphere. Rapid development followed, and a system covering the western Atlantic was operational in 1943. Additional stations followed, covering the European side, and then a massive expansion in the Pacific. By the end of the war there were 72 operational LORAN stations, and as many as 75,000 receivers.
In 1958 the operation of the LORAN system was handed over to the US Coast Guard, which renamed the system "Loran-A", the lower-case name being introduced at that time.
LF LORAN.
There are two ways to implement the timing measurements needed for a hyperbolic navigation system, pulse timing systems like Gee and LORAN, and phase-timing systems like the Decca Navigator System. The former requires sharp pulses of signal, and their accuracy is generally limited to how rapidly the pulses can be turned on and off, which is, in turn, a function of the carrier frequency. The second requires constant signals ("continuous wave") and is easy to use even at low frequencies, but is subject to an ambiguity in location that has to be determined using some other navigation method.
Numerous efforts were made to provide some sort of secondary low-accuracy system that could be used with a phase-comparison system like Decca in order to resolve the ambiguity. Among the many methods were a directional broadcast systems known as the POPI, and a variety of systems combining pulse-timing for low-accuracy navigation and then using phase-comparison for fine adjustment. Decca themselves had set aside one frequency, "9f", for testing this concept, but did not have the chance to do so until much later. Similar concepts were also used in the experimental Navarho system in the US.
It was known from the start of the LORAN project that the same CRT displays that showed the LORAN pulses would also, when suitably magnified, show the individual waves of the intermediate frequency. This meant that pulse-matching could be used to get a rough fix, and then the operator could gain additional timing accuracy by lining up the individual waves within the pulse, like Decca. This could either be used to greatly increase the accuracy of LORAN, or alternately, offer similar accuracy using much lower carrier frequencies, and thus greatly extend range. This would require the transmitter stations to be synchronized both in time and phase, but much of this problem had been solved by Decca engineers.
The long-range option was of considerable interest to the Coast Guard, who set up an experimental system known as LF LORAN in 1945. This operated at much lower frequencies than the original LORAN, 180 kHz, and required very long balloon-borne antennas. Testing was carried out throughout the year, including several long-distance flights as far as Brazil. The experimental system was then sent to Canada where it was used during Operation Muskox in the Arctic. Accuracy was found to be at , a significant advance over LORAN. With the ending of Muskox it was decided to keep the system running under what became known as "Operation Musk Calf", run by a group consisting of the US Air Force, Royal Canadian Air Force, Royal Canadian Navy and Royal Corps of Signals. The system ran until September 1947.
This led to another major test series, this time by the newly formed USAF, known as Operation Beetle. Beetle was located in the far north, on the Canada-Alaska border, and used new guy-stayed steel towers, replacing the earlier system's balloon-lofted cable antennas. The system became operational in 1948 and ran for two years until February 1950. Unfortunately the stations proved poorly sited, as the radio transmission over the permafrost was much shorter than expected and synchronization of the signals between the stations using groundwaves proved impossible. The tests also showed that the system was extremely difficult to use in practice; it was easy for the operator to select the wrong sections of the waveforms on their display, leading to significant real-world inaccuracy.
CYCLAN and Whyn.
In 1946 the Rome Air Development Center sent out contracts for longer-ranged and more-accurate navigation systems that would be used for long-range bombing navigation. As the US Army Air Force was moving towards smaller crews, only three in the Boeing B-47 Stratojet for instance, a high degree of automation was desired. Two contracts were accepted; Sperry Gyroscope proposed the CYCLAN system (CYCLe matching LorAN) which was broadly similar to LF LORAN but with additional automation, and Sylvania proposed Whyn using continuous wave navigation like Decca, but with additional coding using frequency modulation. In spite of great efforts, Whyn could never be made to work, and was abandoned.
CYCLAN operated by sending the same LF LORAN-like signals on two frequencies, LF LORAN's 180 kHz and again on 200 kHz. The associated equipment would look for a rising amplitude that indicated the start of the signal pulse, and then use sampling gates to extract the carrier phase. Using two receivers solved the problem of mis-aligning the pulses, because the phases would only align properly between the two copies of the signal when the same pulses were being compared. None of this was trivial; using the era's tube-based electronics, the experimental CYCLAN system filled much of a semi-trailer.
CYCLAN proved highly successful, so much so that it became increasingly clear that the problems that led the engineers to use two frequencies were simply not as bad as expected. It appeared that a system using a single frequency would work just as well, given the right electronics. This was especially good news, as the 200 kHz frequency was interfering with existing broadcasts, and had to be moved to 160 kHz during testing.
Through this period the issue of radio spectrum use was becoming a major concern, and had led to international efforts to decide on a frequency band suitable for long-range navigation. This process eventually settled on the band from 90 to 100 kHz. CYCLAN appeared to suggest that accuracy at even lower frequencies was not a problem, and the only real concern was the expense of the equipment involved.
Cytac.
The success of the CYCLAN system led to a further contract with Sperry in 1952 for a new system with the twin goals of working in the 100 kHz range while being equally accurate, less complex and less expensive. These goals would normally be contradictory, but the CYCLAN system gave all involved the confidence that these could be met The resulting system was known as Cytac.
To solve the complexity problem, a new circuit was developed to properly time the sampling of the signal. This consisted of a circuit to extract the envelope of the pulse, another to extract the derivative of the envelope, and finally another that subtracted the derivative from the envelope. The result of this final operation would go negative during a very specific and stable part of the rising edge of the pulse, and this zero-crossing was used to trigger a very short-time sampling gate. This system replaced the complex system of clocks used in CYCLAN. By simply measuring the time between the zero-crossings of the master and slave, pulse-timing was extracted.
The output of the envelope sampler was also sent to a phase-shifter that adjusted the output of a local clock that locked to the master carrier using a phase-locked loop. Gating on the slave signal was then compared to this master signal, and a varying voltage was produced depending on the difference in phase. This voltage represented the fine-positioning measurement. The system was generally successful in testing through 1953, but there were concerns raised about the signal power at long ranges, and the possibility of jamming. This led to further modifications of the basic signal. The first was to broadcast a series of pulses instead of just one, broadcasting more energy during a given time and improving the ability of the receivers to tune in a useful signal. By adding a fixed 45° phase shift to every pulse, simple continuous-wave jamming signals could be identified and rejected.
The Cytac system underwent an enormous series of tests across the United States and offshore. Given the potential accuracy of the system, even minor changes to the groundwave synchronization were found to cause errors that could be eliminated - issues such as the number of rivers the signal crossed caused predictable delays that could be measured and then factored into navigation solutions. This led to a series of "correction contours" that could be added to the received signal to adjust for these concerns, and these were printed on the Cytac charts. Using prominent features on dams as target points, a series of tests demonstrated that the uncorrected signals provided accuracy on the order of 100 yards, while adding the correction contour adjustments reduced this to the order of ten yards.
Loran-B and -C.
It was at this moment that the US Air Force (having taken over these efforts while moving from the USAAF) dropped their interest in the project. Although the reasons are not well recorded, it appears the idea of a fully automated bombing system using radio aids was no longer considered possible. The AAF had been involved in missions covering about 1000 km (the distance from London to Berlin) and the Cytac system would work well at these ranges. But as the mission changed to trans-polar missions of 5,000 km or more, even Cytac did not offer the range and accuracy needed. They turned their attention to the use of inertial platforms and Doppler radar systems, cancelling work on Cytac as well as a competing system known as Navarho.
Around this period the Navy began work on a similar system using combined pulse and phase comparison, but based on the existing LORAN frequency of 200 kHz. By this time the Navy had handed operational control of the LORAN system to the Coast Guard, and it was assumed the same arrangement would be true for any new system as well. Thus the Coast Guard was given the choice of naming the systems, and decided to rename the existing system "Loran-A", and the new system Loran-B.
With Cytac fully developed and its test system on the US east coast mothballed, the Navy also decided to re-commission the Cytac system for tests in the long-range role. An extensive series of tests across the Atlantic were carried out by the USCGC "Androscoggin" (WHEC-68) starting in April 1956. Meanwhile, Loran-B proved to have serious problems keeping their transmitters in phase, and that work was abandoned. Minor changes were made to the Cytac systems to further simplify it, including a reduction in the pulse-chain spacing from 1200 to 1000 µs, the pulse rate changed to 20 pps to match the existing Loran-A system, and the phase-shifting between pulses to an alternating 0, 180 degree shift instead of 45 degrees at every pulse within the chain.
The result was Loran-C. Testing with the new system was equally intensive, and overwater flights around Bermuda demonstrated that 50% of fixes lay within a circle. This was a dramatic improvement over the original Loran-A, meeting the accuracy of the Gee system but at much greater range. The first chain were set up using the original experimental Cytac system, along with a second in the Mediterranean in 1957. Further chains covering the North Atlantic and large areas of the Pacific followed. At the time global charts were printed with shaded sections representing the area where a accurate fix could be obtained under most operational conditions.
Improving systems.
Loran-C had originally been designed to be highly automated, allowing the system to be operated more rapidly than the original LORAN's multi-minute measurement. It was also operated in "chains" of linked stations, allowing a fix to be made by simultaneously comparing two slaves to a single master. The downside of this approach was that the required electronic equipment, built using 1950s-era tube technology, was very large. Looking for companies with knowledge of seaborne, multi-channel phase-comparison electronics led, ironically, to Decca, who built the AN/SPN-31, the first widely used Loran-C receiver. The AN/SPN-31 weighed over and had 52 controls.
Airborne units followed, and an adapted AN/SPN-31 was tested in an Avro Vulcan in 1963. By the mid-1960s, units with some transistorization were becoming more common, and a chain was set up in Viet Nam to support the US war efforts there. A number of commercial airline operators experimented with the system as well, using it for navigation on the great circle route between North America and Europe. However, inertial platforms ultimately became more common in this role.
In 1969, Decca sued the Navy for patent infringement, producing ample documentation of their work on the basic concept as early as 1944, along with the "missing" 9f frequency at 98 kHz that had been set aside for experiments using this system. Decca won the initial suit, but the judgement was overturned on appeal when the Navy claimed "wartime expediency".
Loran-D and -F.
When Loran-C became widespread, the USAF once again became interested in using it as a guidance system. They proposed a new system layered on top of Loran-C, using it as the coarse guidance signal in much the same way that pulses were the coarse guidance and phase-comparison used for fine. To provide an extra-fine guidance signal, Loran-D interleaved another train of eight pulses immediately after the signals from one of the existing Loran-C stations, folding the two signals together. This technique became known as "Supernumary Interpulse Modulation" (SIM). These were broadcast from low-power portable transmitters, offering relatively short-range service of high accuracy.
Loran-D was used only experimentally during war-games in the 1960s from a transmitter set in the UK. The system was also used in a limited fashion during the Vietnam War, combined with the Pave Spot laser designator system, a combination known as Pave Nail. Using the AN/ARN-92 mobile transmitters, accuracy on the order of , which the Spot system improved to about . The SIM concept became a system for sending additional data.
At about the same time, Motorola proposed a new system using pseudo-random pulse-chains. This mechanism ensures that no two chains within a given period (on the order of many seconds) will have the same pattern, making it easy to determine if the signal is a groundwave from a recent transmission or a multi-hop signal from a previous one. The system, Multi-User Tactical Navigation Systems (MUTNS) was used briefly but it was found that Loran-D met the same requirements but had the added advantage of being a standard Loran-C signal as well. Although MUTNS was unrelated to the Loran systems, it was sometimes referred to as Loran-F.
Decline.
In spite of its many advantages, the high cost of implementing a Loran-C receiver made it uneconomical for many users. Additionally, as military users upgraded from Loran-A to Loran-C, large numbers of surplus Loran-A receivers were dumped on the market. This made Loran-A popular in spite of being less accurate and fairly difficult to operate. By the early 1970s the introduction of integrated circuits combining a complete radio receiver began to greatly reduce the complexity of Loran-A measurements, and fully automated units the size of a stereo receiver became common. For those users requiring higher accuracy, Decca had considerable success with their Decca Navigator system, and produced units that combined both features.
The same rapid development of microelectronics that made Loran-A so easy to operate worked equally well on the Loran-C signals, and the obvious desire to have a long-range system that could also provide enough accuracy for lake and harbour navigation led to the "opening" of the Loran-C system to public use in 1974. Civilian receivers quickly followed, and dual-system A/C receivers were also common for a time. The switch from A to C was extremely rapid, due largely to rapidly falling prices which led to many user's first receiver being Loran-C. By the late 1970s the Coast Guard decided to turn off Loran-A, in favour of adding additional Loran-C stations to cover gaps is its coverage. The original Loran-A network was shut down in 1979 and 1980, with a few units used in the Pacific for some time.
One of the reasons for Loran-C's opening to the public was the move from Loran to new forms of navigation, including INS, Transit and OMEGA, meant that the security of Loran was no longer as stringent as it was as a primary form of navigation. As these newer systems gave way to GPS through the 1980s and 90s, this process repeated itself, but this time the military was able to separate GPS's signals in such a way that it could provide both secure military and insecure civilian signals at the same time. GPS was more difficult to receive and decode, but by the 1990s the required electronics were already as small and inexpensive as Loran-C, leading to rapid adoption that has become largely universal.
Although Loran-C was largely redundant by 2000, it has not universally disappeared due to a number of concerns. One is that the GPS system can be jammed through a variety of means; although the same is true of Loran-C, the transmitters are close-at-hand and can be adjusted if need be. More importantly, there are effects that might cause the GPS system to become unusable over wide areas, notably space weather events and potential EMP events. Loran, located entirely under the atmosphere, offers more resilience to these sorts of issues. There has been considerable debate about the relative merits of keeping the Loran-C system operational as a result of considerations like these.
In November 2009, the USCG announced that LORAN-C is not needed by the U.S. for maritime navigation. This decision left the fate of LORAN and eLORAN in the U.S. to the Secretary of the Department of Homeland Security. Per a subsequent announcement, the U.S. Coast Guard, in accordance with the DHS Appropriations Act, terminated the transmission of all U.S. LORAN-C signals on 8 February 2010. On 1 August 2010 the U.S. transmission of the Russian American signal was terminated, and on 3 August 2010 all Canadian signals were shut down by the USCG and the CCG.
The European Union had decided that the potential security advantages of Loran are worthy not only of keeping the system operational, but upgrading it and adding new stations. This is part of the wider Eurofix system which combines GPS, Galileo and nine Loran stations into a single integrated system.
However, in 2014, Norway and France both announced that all of their remaining transmitters, which make up a significant part of the Eurofix system, will be shut down on 31 December 2015. The two remaining transmitters in Europe (Anthorn, UK and Sylt, Germany) will no longer be able to sustain a positioning and navigation Loran service, with the result that the UK announced its trial eLoran service would be discontinued from the same date.
Principle.
The navigational method provided by LORAN is based on measuring the time difference between the receipt of signals from a pair of radio transmitters. A given constant time difference between the signals from the two stations can be represented by a hyperbolic line of position (LOP).
If the positions of the two synchronized stations are known, then the position of the receiver can be determined as being somewhere on a particular hyperbolic curve where the time difference between the received signals is constant. In ideal conditions, this is proportionally equivalent to the difference of the distances from the receiver to each of the two stations.
So a LORAN receiver which only receives two LORAN stations cannot fully fix its position—it only narrows it down to being somewhere on a curved line. Therefore, the receiver must receive and calculate the time difference between a second pair of stations. This allows to be calculated a second hyperbolic line on which the receiver is located. Where these two lines cross is the location of the receiver.
In practice, one of the stations in the second pair also may be—and frequently is—in the first pair. This means signals must be received from at least three LORAN transmitters to pinpoint the receiver's location. By determining the intersection of the two hyperbolic curves identified by this method, a geographic fix can be determined.
LORAN method.
In the case of LORAN, one station remains constant in each application of the principle, the "primary", being paired up separately with two other "secondary" stations. Given two secondary stations, the time difference (TD) between the primary and first secondary identifies one curve, and the time difference between the primary and second secondary identifies another curve, the intersections of which will determine a geographic point in relation to the position of the three stations. These curves are referred to as "TD lines".
In practice, LORAN is implemented in integrated regional arrays, or "chains", consisting of one "primary" station and at least two (but often more) "secondary" stations, with a uniform "group repetition interval" (GRI) defined in microseconds. The amount of time before transmitting the next set of pulses is defined by the distance between the start of transmission of primary to the next start of transmission of primary signal.
The secondary stations receive this pulse signal from the primary, then wait a preset number of milliseconds, known as the "secondary coding delay", to transmit a response signal. In a given chain, each secondary's coding delay is different, allowing for separate identification of each secondary's signal. (In practice, however, modern LORAN receivers do not rely on this for secondary identification.)
LORAN chains (GRIs).
Every LORAN chain in the world uses a unique Group Repetition Interval, the number of which, when multiplied by ten, gives how many microseconds pass between pulses from a given station in the chain. (In practice, the delays in many, but not all, chains are multiples of 100 microseconds.) LORAN chains are often referred to by this designation (e.g., GRI 9960, the designation for the LORAN chain serving the Northeast United States).
Due to the nature of hyperbolic curves, a particular combination of a primary and two secondary stations can possibly result in a "grid" where the grid lines intersect at shallow angles. For ideal positional accuracy, it is desirable to operate on a navigational grid where the grid lines are closer to right angles (orthogonal) to each other. As the receiver travels through a chain, a certain selection of secondaries whose TD lines initially formed a near-orthogonal grid can become a grid that is significantly skewed. As a result, the selection of one or both secondaries should be changed so that the TD lines of the new combination are closer to right angles. To allow this, nearly all chains provide at least three, and as many as five, secondaries.
LORAN charts.
Where available, common marine nautical charts include visible representations of TD lines at regular intervals over water areas. The TD lines representing a given primary-secondary pairing are printed with distinct colors, and note the specific time difference indicated by each line. On a nautical chart, the denotation for each Line of Position from a receiver, relative to axis and color, can be found at the bottom of the chart. The color on official charts for stations and the timed-lines of position follow no specific conformance for the purpose of the International Hydrographic Organization (IHO). However, local chart producers may color these in a specific conformance to their standard. Always consult the chart notes, administrations Chart1 reference, and information given on the chart for the most accurate information regarding surveys, datum, and reliability.
There are three major factors when considering signal delay and propagation in relation to LORAN-C:
The chart notes should indicate whether ASF corrections have been made (Canadian Hydrographic Service (CHS) charts, for example, include them). Otherwise, the appropriate correction factors must be obtained before use.
Due to interference and propagation issues suffered from land features and artificial structures such as tall buildings, the accuracy of the LORAN signal can be degraded considerably in inland areas (see Limitations). As a result, nautical charts will not show TD lines in those areas, to prevent reliance on LORAN-C for navigation.
Traditional LORAN receivers display the time difference between each pairing of the primary and one of the two selected secondary stations, which is then used to find the appropriate TD line on the chart. Modern LORAN receivers display latitude and longitude coordinates instead of time differences, and, with the advent of time difference comparison and electronics, provide improved accuracy and better position fixing, allowing the observer to plot their position on a nautical chart more easily. When using such coordinates, the datum used by the receiver (usually WGS84) must match that of the chart, or manual conversion calculations must be performed before the coordinates can be used.
Timing and synchronization.
Each LORAN station is equipped with a suite of specialized equipment to generate the precisely timed signals used to modulate / drive the transmitting equipment. Up to three commercial cesium atomic clocks are used to generate 5 MHz and pulse per second (or 1 Hz) signals that are used by timing equipment to generate the various GRI-dependent drive signals for the transmitting equipment.
While each U.S.-operated LORAN station is supposed to be synchronized to within 100 ns of UTC, the actual accuracy achieved as of 1994 was within 500 ns.
Transmitters and antennas.
LORAN-C transmitters operate at peak powers of 100–4,000 kilowatts, comparable to longwave broadcasting stations. Most use 190–220 metre tall mast radiators, insulated from ground. The masts are inductively lengthened and fed by a loading coil (see: electrical lengthening). A well known-example of a station using such an antenna is Rantum. Free-standing tower radiators in this height range are also used. Carolina Beach uses a free-standing antenna tower. Some LORAN-C transmitters with output powers of 1,000 kW and higher used supertall 412 metre mast radiators (see below). Other high power LORAN-C stations, like George, used four T-antennas mounted on four guyed masts arranged in a square.
All LORAN-C antennas are designed to radiate an omnidirectional pattern. Unlike longwave broadcasting stations, LORAN-C stations cannot use backup antennas because the exact position of the antenna is a part of the navigation calculation. The slightly different physical location of a backup antenna would produce Lines of Position different from those of the primary antenna.
Limitations.
LORAN suffers from electronic effects of weather and the ionospheric effects of sunrise and sunset. The most accurate signal is the groundwave that follows the Earth's surface, ideally over seawater. At night the indirect skywave, bent back to the surface by the ionosphere, is a problem as multiple signals may arrive via different paths (multipath interference). The ionosphere's reaction to sunrise and sunset accounts for the particular disturbance during those periods. Magnetic storms have serious effects as with any radio based system.
LORAN uses ground based transmitters that only cover certain regions. Coverage is quite good in North America, Europe, and the Pacific Rim.
The absolute accuracy of LORAN-C varies from . Repeatable accuracy is much greater, typically from .
LORAN-A and other systems.
LORAN-A was a less accurate system operating in the upper mediumwave frequency band prior to deployment of the more accurate LORAN-C system. For LORAN-A the transmission frequencies 1750 kHz, 1850 kHz, 1900 kHz and 1950 kHz were used, shared with the 1800–2000 kHz amateur 160-meter band. LORAN-A continued in operation partly due to the economy of the receivers and widespread use in civilian recreational and commercial navigation. LORAN-B was a phase comparison variation of LORAN-A while LORAN-D was a short-range tactical system designed for USAF bombers. The unofficial "LORAN-F" was a drone control system. None of these went much beyond the experimental stage. An external link to them is listed below.
LORAN-A was used in the Vietnam War for navigation by large United States aircraft (C-124, C-130, C-97, C-123, HU-16, etc.). A common airborne receiver of that era was the R-65/APN-9 which combined the receiver and cathode ray tube (CRT) indicator into a single relatively lightweight unit replacing the two larger, separate receiver and indicator units which composed the predecessor APN-4 system. The APN-9 and APN-4 systems found wide post–World War II use on fishing vessels in the U.S. They were cheap, accurate and plentiful. The main drawback for use on boats was their need for aircraft power, 115 VAC at 400 Hz. This was solved initially by the use of rotary converters, typically 28 VDC input and 115 VAC output at 400 Hz. The inverters were large, noisy and required significant power. In the 1960s, several firms such as Topaz and Linear Systems marketed solid state inverters specifically designed for these surplus LORAN-A sets. The availability of solid state inverters that used 12 VDC input opened up the surplus LORAN-A sets for use on much smaller vessels which typically did not have the 24-28 VDC systems found on larger vessels. The solid state inverters were very power efficient and widely replaced the more trouble prone rotary inverters.
LORAN-A saved many lives by allowing offshore boats in distress to give accurate position reports. It also guided many boats whose owners could not afford radar safely into fog bound harbors or around treacherous offshore reefs. The low price of surplus LORAN-A receivers (often under $150) meant that owners of many small fishing vessels could afford this equipment, thus greatly enhancing safety. Surplus LORAN-A equipment, which was common on commercial fishing boats, was rarely seen on yachts. The unrefined cosmetic appearance of the surplus equipment was probably a deciding factor.
Pan American World Airways used APN 9s in early Boeing 707 operations. World War II surplus APN-9 looked out of place in the modern 707 cockpit, but was needed. There is an R65A APN-9 set displayed in the museum at San Francisco International Airport, painted gold. It was a retirement gift to an ex Pan Am captain.
An elusive final variant of the APN 9 set was the APN 9A. A USAF technical manual (with photographs and schematics) shows that it had the same case as the APN-9 but a radically different front panel and internal circuitry on the non-RF portions. The APN-9A had vacuum tube flipflop digital divider circuits so that TDs (time delays) between the primary and secondary signal could be selected on front panel rotary decade switches. The older APN-9 set required the user to perform a visual count of crystal oscillator timing marker pips on the CRT and add them up to get a TD. The APN 9A did not make it into widespread military use, if it was used at all, but it did exist and represented a big advance in military LORAN-A receiver technology.
In the 1970s one US company, SRD Labs in Campbell, California, made modern LORAN-A sets including one that was completely automatic with a digital TD readout on the CRT, and autotracking so that TDs were continuously updated. Other SRD models required the user to manually align the primary and secondary signals on the CRT and then a phase locked loop would keep them lined up and provide updated TD readouts thereafter. These SRD LORAN-A sets would track only one pair of stations, providing just one LOP (line of position). For a continuously updated position (two TDs giving intersecting LOPs) rather than just a single LOP, two sets were necessary.
LORAN-A was terminated in the United States on 31 December 1980 and the restrictions on amateur radio use of the 160-meter band were lifted.
Long after LORAN-A broadcasts were terminated, commercial fishermen still referred to old LORAN-A TDs, e.g., "I am on the 4100 line in 35 fathoms", referring to a position outside Bodega Bay. Many LORAN-C sets incorporated LORAN A TD converters so that a LORAN-C set could be used to navigate to a LORAN-A TD defined line or position.
LORAN Data Channel (LDC).
LORAN Data Channel (LDC) is a project underway between the FAA and USCG to send low bit rate data using the LORAN system. Messages to be sent include station identification, absolute time, and position correction messages. In 2001, data similar to Wide Area Augmentation System (WAAS) GPS correction messages were sent as part of a test of the Alaskan LORAN chain. As of November 2005, test messages using LDC were being broadcast from several U.S. LORAN stations.
In recent years, LORAN-C has been used in Europe to send differential GPS and other messages, employing a similar method of transmission known as EUROFIX.
A system called SPS (Saudi Positioning System), similar to EUROFIX, is in use in Saudi Arabia. GPS differential corrections and GPS integrity information are added to the LORAN signal. A combined GPS/LORAN receiver is used, and if a GPS fix is not available it automatically switches over to LORAN.
The future of LORAN.
As LORAN systems are government maintained and operated, their continued existence is subject to public policy. With the evolution of other electronic navigation systems, such as satellite navigation systems, funding for existing systems is not always assured.
Critics, who have called for the elimination of the system, state that the LORAN system has too few users, lacks cost-effectiveness, and that GNSS signals are superior to LORAN. Supporters of continued and improved LORAN operation note that LORAN uses a strong signal, which is difficult to jam, and that LORAN is an independent, dissimilar, and complementary system to other forms of electronic navigation, which helps ensure availability of navigation signals.
On 26 February 2009, the U.S. Office of Management and Budget released the first blueprint for the Financial Year 2010 budget. This document identified the LORAN-C system as “outdated” and supported its termination at an estimated savings of $36 million in 2010 and $190 million over five years.
On 21 April 2009 the U.S. Senate Committee on Commerce, Science and Transportation and the Committee on Homeland Security and Governmental Affairs released inputs to the FY 2010 Concurrent Budget Resolution with backing for the continued support for the LORAN system, acknowledging the investment already made in infrastructure upgrades and recognizing the studies performed and multi-departmental conclusion that eLORAN is the best backup to GPS.
Senator Jay Rockefeller, Chairman of the Committee on Commerce, Science and Transportation, wrote that the committee recognized the priority in "Maintaining LORAN-C while transitioning to eLORAN" as means of enhancing the homeland security, marine safety and environmental protection missions of the Coast Guard.
Senator Collins, the ranking member on the Committee on Homeland Security and Governmental Affairs wrote that the President's budget overview proposal to terminate the LORAN-C system is inconsistent with the recent investments, recognized studies and the mission of the U.S. Coast Guard. The committee also recognizes the $160 million investment already made toward upgrading the LORAN-C system to support the full deployment of eLORAN.
Further, the Committees also recognize the many studies which evaluated GPS backup systems and concluded both the need to back up GPS and identified eLORAN as the best and most viable backup. "This proposal is inconsistent with the recently released (January 2009) Federal Radionavigation Plan (FRP), which was jointly prepared by DHS and the Departments of Defense (DOD) and Transportation (DOT). The FRP proposed the eLORAN program to serve as a Position, Navigation and Timing (PNT) backup to GPS (Global Positioning System)."
On 7 May 2009, President Barack Obama proposed cutting funding (approx. $35 million/year) for LORAN, citing its redundancy alongside GPS. In regard to the pending Congressional bill, H.R. 2892, it was subsequently announced that "he Administration supports the Committee's aim to achieve an orderly termination through a phased decommissioning beginning in January 2010, and the requirement that certifications be provided to document that the LORAN-C termination will not impair maritime safety or the development of possible GPS backup capabilities or needs."
Also on 7 May 2009, the U.S. General Accounting Office (GAO), the investigative arm of Congress, released a report citing the very real potential for the GPS system to degrade or fail in light of program delays which have resulted in scheduled GPS satellite launches slipping by up to three years.
On 12 May 2009 the March 2007 Independent Assessment Team (IAT) report on LORAN was released to the public. In its report the ITA stated that it "unanimously recommends that the U.S. government complete the eLORAN upgrade and commit to eLORAN as the national backup to GPS for 20 years." The release of the report followed an extensive Freedom Of Information Act (FOIA) battle waged by industry representatives against the federal government. Originally completed 20 March 2007 and presented to the co-sponsoring Department of Transportation and Department of Homeland Security (DHS) Executive Committees, the report carefully considered existing navigation systems, including GPS. The unanimous recommendation for keeping the LORAN system and upgrading to eLORAN was based on the team's conclusion that LORAN is operational, deployed and sufficiently accurate to supplement GPS. The team also concluded that the cost to decommission the LORAN system would exceed the cost of deploying eLORAN, thus negating any stated savings as offered by the Obama administration and revealing the vulnerability of the U.S. to GPS disruption.
In November 2009, the U.S. Coast Guard announced that the LORAN-C stations under its control would be closed down for budgetary reasons after 4 January 2010 provided the Secretary of the Department of Homeland Security certified that LORAN is not needed as a backup for GPS.
On 7 January 2010, Homeland Security published a notice of the permanent discontinuation of LORAN-C operation. Effective 2000 UTC 8 February 2010, the United States Coast Guard terminated all operation and broadcast of LORAN-C signals in the USA. The U.S. Coast Guard transmission of the Russian American CHAYKA signal was terminated on 1 August 2010. The transmission of Canadian LORAN-C signals was terminated on 3 August 2010.
eLORAN.
With the perceived vulnerability of GNSS systems, and their own propagation and reception limitations, renewed interest in LORAN applications and development has appeared. Enhanced LORAN, also known as eLORAN or E-LORAN, comprises an advancement in receiver design and transmission characteristics which increase the accuracy and usefulness of traditional LORAN. With reported accuracy as good as ± 8 meters, the system becomes competitive with unenhanced GPS. eLORAN also includes additional pulses which can transmit auxiliary data such as DGPS corrections. eLORAN receivers now use "all in view" reception, incorporating signals from all stations in range, not solely those from a single GRI, incorporating time signals and other data from up to 40 stations. These enhancements in LORAN make it adequate as a substitute for scenarios where GPS is unavailable or degraded.
United Kingdom eLORAN implementation.
On 31 May 2007, the UK Department for Transport (DfT), via the General Lighthouse Authorities (GLA), awarded a 15-year contract to provide a state-of-the-art enhanced LORAN (eLORAN) service to improve the safety of mariners in the UK and Western Europe. The service contract will operate in two phases, with development work and further focus for European agreement on eLORAN service provision from 2007 through 2010, and full operation of the eLORAN service from 2010 through 2022. The first eLORAN transmitter is situated at Anthorn radio station Cumbria, UK, and operated by Babcock Comms, which is part of the Babcock Group PLC.
eLORAN: The UK government has granted approval for seven differential eLoran ship-positioning technology stations to be built along the south and east coasts of the UK to help counter the threat of jamming of global positioning systems. They are set to reach initial operational capability by summer 2014. The General Lighthouse Authorities (GLAs) of the UK and Ireland announced October 31 the initial operational capability of UK maritime eLoran. Seven differential reference stations now provide additional position, navigation, and timing (PNT) information via low-frequency pulses to ships fitted with eLoran receivers. The service will help ensure they can navigate safely in the event of GPS failure in one of the busiest shipping regions in the world, with expected annual traffic of 200,000 vessels by 2020.
Despite these plans, in light of the decision by France and Norway to cease Loran transmissions on 31 December 2015, the UK announced at the start of that month that its eLoran service would be discontinued on the same day.
List of LORAN-C transmitters.
A list of LORAN-C transmitters. Stations with an antenna tower taller than 300 metres (984 feet) are shown in bold.

</doc>
<doc id="18377" url="https://en.wikipedia.org/wiki?curid=18377" title="Lunatic">
Lunatic

Lunatic is an informal term referring to a person who is considered mentally ill, dangerous, foolish or unpredictable, conditions once attributed to lunacy. The term may be considered insulting in serious contexts in modern times, but is now more likely to be used in friendly jest. The word derives from "lunaticus" meaning "of the moon" or "moonstruck". The term was once commonly used in law.
History.
The term "lunatic" derives from the Latin word "lunaticus", which originally referred mainly to epilepsy and madness, as diseases thought to be caused by the moon. By the fourth and fifth centuries astrologers were commonly using the term to refer to neurological and psychiatric diseases. Philosophers such as Aristotle and Pliny the Elder argued that the full moon induced insane individuals with bipolar disorder by providing light during nights which would otherwise have been dark, and affecting susceptible individuals through the well-known route of sleep deprivation. Until at least 1700 it was also a common belief that the moon influenced fevers, rheumatism, episodes of epilepsy and other diseases.
Use of the term "lunatic" in legislation.
In the British jurisdiction of England and Wales the Lunacy Acts 1890–1922 referred to "lunatics", but the Mental Treatment Act 1930 changed the legal term to "person of unsound mind", an expression which was replaced under the Mental Health Act 1959 by "mental illness". "Person of unsound mind" was the term used in 1950 in the English version of the European Convention on Human Rights as one of the types of person who could be deprived of liberty by a judicial process. The 1930 Act also replaced the term "asylum" with "mental hospital". Criminal lunatics became Broadmoor patients in 1948 under the National Health Service (Scotland) Act 1947.
On December 5, 2012 the US House of Representatives passed legislation approved earlier by the US Senate removing the word "lunatic" from all federal laws in the United States. President Obama signed this legislation into law on December 28, 2012.
"Of unsound mind" or "non compos mentis" are alternatives to "lunatic", which was the most conspicuous term used for insanity in the law in the late 19th century.
Lunar distance.
The term "lunatic" was also used by supporters of John Harrison and his marine chronometer method of determining longitude, to refer to proponents of the Method of Lunar Distances, advanced by Astronomer Royal Nevil Maskelyne. 
Later, members of the Lunar Society of Birmingham called themselves "lunaticks". In an age with little street lighting, the society met on or near the night of the full moon.

</doc>
<doc id="18379" url="https://en.wikipedia.org/wiki?curid=18379" title="Linear timecode">
Linear timecode

Linear (or Longitudinal) Timecode (LTC) is an encoding of SMPTE timecode data in an audio signal, as defined in SMPTE 12M specification. The audio signal is commonly recorded on a VTR track or other storage media. The bits are encoded using the biphase mark code (also known as "FM"): a 0 bit has a single transition at the start of the bit period. A 1 bit has two transitions, at the beginning and middle of the period. This encoding is self-clocking. Each frame is terminated by a 'sync word' which has a special predefined sync relationship with any video or film content.
A special bit in the linear timecode frame, the "biphase mark correction" bit, ensures that there are an even number of AC transitions in each timecode frame.
The sound of linear timecode is a jarring and distinctive noise and has been used as a sound-effects shorthand to imply "telemetry" or "computers".
Generation and Distribution.
In broadcast video situations, the LTC generator should be tied-in to house black burst, as should all devices using timecode, to ensure correct color framing and correct synchronization of all digital clocks. When synchronizing multiple clock-dependent digital devices together with video, such as digital audio recorders, the devices must be connected to a common word clock signal that is derived from the house black burst signal. This can be accomplished by using a generator that generates both black burst and video-resolved word clock, or by synchronizing the master digital device to video, and synchronizing all subsequent devices to the word clock output of the master digital device (and to LTC).
Made up of 80 bits per frame, where there may be 24, 25 or 30 frames per second, LTC timecode varies from 960 Hz (binary zeros at 24 frames/s) to 2400 Hz (binary ones at 30 frames/s), and thus is comfortably in the audio frequency range. LTC can exist as either a balanced or unbalanced signal, and can be treated as an audio signal in regards to distribution. Like audio, LTC can be distributed by standard audio wiring, connectors, distribution amplifiers, and patchbays, and can be ground-isolated with audio transformers. It can also be distributed via 75 ohm video cable and video distribution amplifiers, although the voltage attenuation caused by using a 75 ohm system may cause the signal to drop to a level that can not be read by some equipment.
Care has to be taken with analog audio to avoid audible 'breakthrough' (aka "crosstalk") from the LTC track to the audio tracks.
LTC care:
Longitudinal SMPTE timecode should be played back at a middle-level when recorded on an audio track, as both low and high levels will introduce distortion.
Longitudinal timecode data format.
The basic format is an 80-bit code that gives the time of day to the second, and the frame number within the second. Values are stored in binary-coded decimal, least significant bit first.
There are thirty-two bits of user data, usually used for a reel number and date.

</doc>
<doc id="18381" url="https://en.wikipedia.org/wiki?curid=18381" title="John William Strutt, 3rd Baron Rayleigh">
John William Strutt, 3rd Baron Rayleigh

John William Strutt, 3rd Baron Rayleigh (; 12 November 1842 – 30 June 1919) was an English physicist who, with William Ramsay, discovered argon, an achievement for which he earned the Nobel Prize for Physics in 1904. He also discovered the phenomenon now called Rayleigh scattering, which can be used to explain why the sky is blue, and predicted the existence of the surface waves now known as Rayleigh waves. Rayleigh's textbook," The Theory of Sound", is still referred to by acoustic engineers today.
Biography.
John William Strutt, of Terling Place Essex, suffered from frailty and poor health in his early years. He attended Harrow School, before going on to the University of Cambridge in 1861 where he studied mathematics at Trinity College, Cambridge. He obtained a Bachelor of Arts degree (Senior Wrangler and 1st Smith's prize) in 1865, and a Master of Arts in 1868. He was subsequently elected to a Fellowship of Trinity. He held the post until his marriage to Evelyn Balfour, daughter of James Maitland Balfour, in 1871. He had three sons with her. In 1873, on the death of his father, John Strutt, 2nd Baron Rayleigh, he inherited the Barony of Rayleigh.
He was the second Cavendish Professor of Physics at the University of Cambridge (following James Clerk Maxwell), from 1879 to 1884. He first described dynamic soaring by seabirds in 1883, in the British journal "Nature". From 1887 to 1905 he was Professor of Natural Philosophy at the Royal Institution.
Around the year 1900 Lord Rayleigh developed the "duplex" (combination of two) theory of human sound localization using two binaural cues, interaural phase difference (IPD) and interaural level difference (ILD) (based on analysis of a spherical head with no external pinnae). The theory posits that we use two primary cues for sound lateralization, using the difference in the phases of sinusoidal components of the sound and the difference in amplitude (level) between the two ears.
In 1919, Rayleigh served as President of the Society for Psychical Research.
The rayl unit of acoustic impedance is named after him.
As an advocate that simplicity and theory be part of the scientific method, Lord Rayleigh argued for the principle of similitude.
Lord Rayleigh was elected Fellow of the Royal Society on 12 June 1873, and served as president of the Royal Society from 1905 to 1908. From time to time Lord Rayleigh participated in the House of Lords; however, he spoke up only if politics attempted to become involved in science. He died on 30 June 1919, in Witham, Essex. He was succeeded, as the 4th Lord Rayleigh, by his son Robert John Strutt, another well-known physicist.
Religious views.
Lord Rayleigh was an Anglican. Though he did not write about the relationship of science and religion, he retained a personal interest in spiritual matters. When his scientific papers were to be published in a collection by the Cambridge University Press, Strutt wanted to include a religious quotation from The Bible, but he was discouraged from doing so, as he later reported:
Still, he kept his wish and the quotation was printed in the five-volume collection of scientific papers.
In a letter to a family member, he also wrote about his rejection of materialism and spoke of Jesus Christ as a moral teacher:
Honours and awards.
Craters on Mars and the Moon are named in his honour as well as a type of surface wave known as a Rayleigh wave. The asteroid 22740 Rayleigh was named in his honour on 1 June 2007. The rayl, a unit of acoustic impedance, is named for him.

</doc>
<doc id="18382" url="https://en.wikipedia.org/wiki?curid=18382" title="Lunisolar calendar">
Lunisolar calendar

A lunisolar calendar is a calendar in many cultures whose date indicates both the moon phase and the time of the solar year. If the solar year is defined as a tropical year, then a lunisolar calendar will give an indication of the season; if it is taken as a sidereal year, then the calendar will predict the constellation near which the full moon may occur. Usually there is an additional requirement that the year have a whole number of months. In this case ordinary years consist of twelve months but every second or third year is an embolismic year, which adds a thirteenth intercalary, embolismic, or leap month.
Examples.
The Hebrew, Buddhist, Hindu, Kurdish and Bengali calendars, as well as the traditional Chinese, Tibetan, Japanese, Vietnamese, Mongolian and Korean calendars (in the East Asian cultural sphere), plus the ancient Hellenic, Coligny, and Babylonian calendars are all lunisolar. Also, some of the ancient pre-Islamic calendars in South Arabia followed a lunisolar system. The Chinese, Coligny and
Hebrew lunisolar calendars track more or less the tropical year whereas the Buddhist and Hindu lunisolar calendars track the sidereal year. Therefore, the first three give an idea of the seasons whereas the last two give an idea of the position among the constellations of the full moon. The Tibetan calendar was influenced by both the Chinese and Hindu calendars. The Germanic peoples also used a lunisolar calendar before their conversion to Christianity.
The Islamic calendar is lunar, but not a lunisolar calendar because its date is not related to the sun. The civil versions of the Julian and Gregorian calendars are solar, because their dates do not indicate the moon phase — however, both the Gregorian and Julian calendars include undated lunar calendars that allow them to calculate the Christian celebration of Easter, so both are lunisolar calendars in that respect.
Determining leap months.
To determine when an embolismic month needs to be inserted, some calendars rely on direct observations of the state of vegetation, while others compare the ecliptic longitude of the sun and the phase of the moon. The Hawaiians observe the movement of specific stars and insert months accordingly.
On the other hand, in arithmetical lunisolar calendars, an integral number of months is fitted into some integral number of years by a fixed rule. To construct such a calendar (in principle), the average length of the tropical year is divided by the average length of the synodic month, which gives the number of average synodic months in a tropical year as:
Continued fractions of this decimal value (2, 1, 2, 1, 1, 17, ...) give optimal approximations for this value. So in the list below, after the number of synodic months listed in the numerator, approximately an integer number of tropical years as listed in the denominator have been completed:
Note however that in none of the arithmetic calendars is the average year length exactly equal to a true tropical year. Different calendars have different average year lengths and different average month lengths, so the discrepancy between the calendar months and moon is not equal to the values given above.
The 8-year cycle (99 synodic months, including 99−8×12 = 3 embolismic months) was the octaeteris used in the ancient Athenian calendar. The 8-year cycle was also used in early third-century Easter calculations (or old "Computus") in Rome .
The 19-year cycle (235 synodic months, including 235−(19×12) = 7 embolismic months) is the classic Metonic cycle, which is used in most arithmetical lunisolar calendars. It is a combination of the 8- and 11-year period, and whenever the error of the 19-year approximation accumulates to of a mean month, a cycle can be truncated to 11 years (skipping 8 years including 3 embolismic months), after which 19-year cycles can resume. Meton's cycle had an integer number of days, although "Metonic cycle" often means its use without an integer number of days. It was adapted to a mean year of 365.25 days by means of the 4×19 year Callippic cycle (used in the Easter calculations of the Julian calendar).
Rome used an 84-year cycle for Easter calculations from the third century until 457. The native British Christians continued its use as late as 768, when Bishop Elfodd of Bangor finally persuaded them to adopt the improved calendars introduced by St Augustine's mission. The 84-year cycle is equivalent to a Callippic 4×19-year cycle (including 4×7 embolismic months) plus an 8-year cycle (including 3 embolismic months) and so has a total of 1039 months (including 31 embolismic months). This gives an average of 12.3690476... months per year. One cycle was 30681 days, which is about 1.28 days short of 1039 synodic months, 0.66 days more than 84 tropical years, and 0.53 days short of 84 sidereal years.
The next approximation (arising from continued fractions) after the Metonic cycle (such as a 334-year cycle) is very sensitive to the values one adopts for the lunation (synodic month) and the year, especially the year. There are different possible definitions of the year so other approximations may be more accurate for specific purposes. For example, a 353-year cycle including 130 embolismic months for a total of 4366 months (12.36827195...) is more accurate for a northern hemisphere spring equinox year, whereas a 611-year cycle including 225 embolismic months for a total of 7557 months (12.36824877...) has good accuracy for a northern hemisphere summer solstice year, and a 160-year cycle including 59 embolismic months for a total of 1979 months (12.36875) has good accuracy for a sidereal year (approx 12.3687462856 synodic months).
Calculating a leap month.
A rough idea of the frequency of the intercalary or leap month in all lunisolar calendars can be obtained by the following calculation, using approximate lengths of months and years in days:
A representative sequence of common and leap years is ccLccLcLccLccLccLcL, which is the classic nineteen-year Metonic cycle. The Buddhist and Hebrew calendars restrict the leap month to a single month of the year; the number of common months between leap months is, therefore, usually 36, but occasionally only 24 months. Because the Chinese and Hindu lunisolar calendars allow the leap month to occur after or before (respectively) any month but use the true motion of the sun, their leap months do not usually occur within a couple of months of perihelion, when the apparent speed of the sun along the ecliptic is fastest (now about 3 January). This increases the usual number of common months between leap months to roughly 34 months when a doublet of common years occurs, while reducing the number to about 29 months when only a common singleton occurs.
Lunisolar calendars with uncounted time.
An alternative way of dealing with the fact that a solar year does not contain an integer number of months is by including uncounted time in the year that does not belong to any month. Some Coast Salish peoples used a calendar of this kind. For instance, the Chehalis began their count of lunar months from the arrival of spawning chinook salmon (in Gregorian calendar October), and counted 10 months, leaving an uncounted period until the next chinook salmon run.
Gregorian lunisolar calendar.
The Gregorian calendar has a lunisolar calendar, which is used to determine the date of Easter. The rules are in the Computus.

</doc>
<doc id="18383" url="https://en.wikipedia.org/wiki?curid=18383" title="Leonids">
Leonids

The Leonids ( ) are a prolific meteor shower associated with the comet Tempel–Tuttle. The Leonids get their name from the location of their radiant in the constellation Leo: the meteors appear to radiate from that point in the sky. Their proper Greek name should be Leon"t"ids (Λεοντίδαι, "Leontídai"), but the word was initially constructed as a Greek/Latin hybrid and it has been used since. They peak in the month of November.
Earth moves through the meteoroid stream of particles left from the passages of a comet. The stream comprises solid particles, known as meteoroids, ejected by the comet as its frozen gases evaporate under the heat of the Sun when it is close enough – typically closer than Jupiter's orbit. The Leonids are a fast moving stream which encounter the path of Earth and impact at 72 km/s. Larger Leonids which are about 10 mm across have a mass of half a gram and are known for generating bright (apparent magnitude -1.5) meteors. An annual Leonid shower may deposit 12 or 13 tons of particles across the entire planet.
The meteoroids left by the comet are organized in trails in orbits similar to though different from that of the comet. They are differentially disturbed by the planets, in particular Jupiter and to a lesser extent by radiation pressure from the sun, the Poynting–Robertson effect, and the Yarkovsky effect. These trails of meteoroids cause meteor showers when Earth encounters them. Old trails are spatially not dense and compose the meteor shower with a few meteors per minute. In the case of the Leonids, that tends to peak around November 18, but some are spread through several days on either side and the specific peak changes every year. Conversely, young trails are spatially very dense and the cause of meteor outbursts when the Earth enters one. Meteor storms (large outbursts) exceed 1000 meteors per hour, to be compared to the sporadic background (5 to 8 meteors per hour) and the shower background (several per hour).
History.
1800s.
The Leonids are famous because their meteor showers, or storms, can be among the most spectacular. Because of the storm of 1833 and the recent developments in scientific thought of the time (see for example the identification of Halley's Comet) the Leonids have had a major effect on the development of the scientific study of meteors which had previously been thought to be atmospheric phenomena. Though it has been suggested the meteor shower-storm has been noted in ancient times it was the meteor storm of 1833 that broke into people's modern day awareness - it was of truly superlative strength. One estimate is over one hundred thousand meteors an hour, but another, done as the storm abated, estimated in excess of 240,000 meteors during the nine hours of the storm over the entire region of North America east of the Rocky Mountains.
It was marked by several nations of Native Americans: the Cheyenne established a peace treaty and the Lakota calendar was reset. Abolitionists like Harriet Tubman and Frederick Douglass as well as slave-owners took note and others. The New York Evening Post carried a series of articles on the event including reports from Canada to Jamaica, it made news in several states beyond New York and though it appeared in North America was talked about in Europe. The journalism of the event tended to rise above the partisan debates of the time and reviewed facts as they could be sought out. Abraham Lincoln commented on it years later. Near Independence, Missouri, in Clay County, a refugee Mormon community watched the meteor shower on the banks of the Missouri River after having been driven from their homes by local settlers. The founder and first leader of Mormonism, Joseph Smith, afterwards noted in his journal that this event was a literal fulfillment of the word of God and a sure sign that the coming of Christ is close at hand. Though it was noted in the midwest and eastern areas it was also noted in the far west.
Denison Olmsted explained the event most accurately. After spending the last weeks of 1833 collecting information, he presented his findings in January 1834 to the "American Journal of Science and Arts", published in January–April 1834, and January 1836. He noted the shower was of short duration and was not seen in Europe, and that the meteors radiated from a point in the constellation of Leo and he speculated the meteors had originated from a cloud of particles in space. Accounts of the 1866 repeat of the Leonids counted hundreds per minute/a few thousand per hr in Europe. The Leonids were again seen in 1867, when moonlight reduced the rates to 1000 per hour. Another strong appearance of the Leonids in 1868 reached an intensity of 1000 per hour in dark skies. It was in 1866–67 that information on Comet Tempel-Tuttle was gathered pointing it out as the source of the meteor shower. When the storms failed to return in 1899, it was generally thought that the dust had moved on and storms were a thing of the past.
1900s.
Then, in 1966, a spectacular storm was seen over the Americas. Historical notes were gathered thus noting the Leonids back to 900AD. Radar studies showed the 1966 storm included a relatively high percentage of smaller particles while 1965's lower activity had a much higher proportion of larger particles. In 1981 Donald K. Yeomans of the Jet Propulsion Laboratory reviewed the history of meteor showers for the Leonids and the history of the dynamic orbit of Comet Tempel-Tuttle. A graph from it was adapted and re-published in Sky and Telescope. It showed relative positions of the Earth and Tempel-Tuttle and marks where Earth encountered dense dust. This showed that the meteoroids are mostly behind and outside the path of the comet, but paths of the Earth through the cloud of particles resulting in powerful storms were very near paths of nearly no activity. But overall the 1998 Leonids were in a favorable position so interest was rising.
Leading up to the 1998 return, an airborne observing campaign was organized to mobilize modern observing techniques by Peter Jenniskens at NASA Ames Research Center. There were also efforts to observe impacts of meteoroids, as an example of transient lunar phenomenon, on the Moon in 1999. A particular reason to observe the Moon is that our vantage from a location on Earth sees only meteors coming into the atmosphere relatively close to us while impacts on the Moon would be visible from across the Moon in a single view. A sodium tail of the Moon tripled just after the 1998 Leonid shower which was composed of larger meteoroids (which in the case of the Earth was witnessed as fireballs.) However, in 1999 the sodium tail of the Moon did not change from the Leonid impacts.
Research by Kondrat'eva, Reznikov and colleagues at Kazan University had shown how meteor storms could be accurately predicted, but for some years the worldwide meteor community remained largely unaware of these results. The work of David J. Asher, Armagh Observatory and Robert H. McNaught, Siding Spring Observatory and independently by Esko Lyytinen in 1999, following on from the Kazan research, is considered by most meteor experts as the breakthrough in modern analysis of meteor storms. Whereas previously it was hazardous to guess if there would be a storm or little activity, the predictions of Asher and McNaught timed bursts in activity down to ten minutes by narrowing down the clouds of particles to individual streams from each passage of the comet, and their trajectories amended by subsequent passage near planets. However, whether a specific meteoroid trail will be primarily composed of small or large particles, and thus the relative brightness of the meteors, was not understood. But McNaught did extend the work to examine the placement of the Moon with trails and saw a large chance of a storm impacting in 1999 from a trail while there were less direct impacts from trails in 2000 and 2001 (successive contact with trails through 2006 showed no hits.)
2000s.
Viewing campaigns resulted in spectacular footage from the 1999, 2001 and 2002, storms producing up to 3,000 Leonid meteors per hour. Predictions for the Moon's Leonid impacts also noted that in 2000 the side of the Moon facing the stream was away from the Earth but that impacts should be in number enough to raise a cloud of particles kicked off the Moon by impacts would cause a detectable increase in the sodium tail of the Moon. Research using the explanation of meteor trails/streams have explained the storms of the past. The 1833 storm was not due to the recent passage of the comet, but from a direct impact with the previous 1800 dust trail. The meteoroids from the 1733 passage of Comet Tempel-Tuttle resulted in the 1866 storm and the 1966 storm was from the 1899 passage of the comet. The double spikes in Leonid activity in 2001 and in 2002 were due to the passage of the comet's dust ejected in 1767 and 1866. This ground breaking work was soon applied to other meteor showers – for example the 2004 June Bootids. Peter Jenniskens has published predictions for the next 50 years. However, a close encounter with Jupiter is expected to perturb the comet's path, and many streams, making storms of historic magnitude unlikely for many decades. Recent work tries to take into account the roles of differences in parent bodies and the specifics of their orbits, ejection velocities off the solid mass of the core of a comet, radiation pressure from the sun, the Poynting–Robertson effect, and the Yarkovsky effect on the particles of different sizes and rates of rotation to explain differences between meteor showers in terms of being predominantly fireballs or small meteors.
Predictions until the end of the 21st century have been published by Mikhail Maslov.

</doc>
<doc id="18384" url="https://en.wikipedia.org/wiki?curid=18384" title="Labarum">
Labarum

The labarum () was a "vexillum" (military standard) that displayed the "Chi-Rho" symbol ☧, a christogram formed from the first two Greek letters of the word "Christ" (, or Χριστός) — "Chi" (χ) and "Rho" (ρ). It was first used by the Roman emperor Constantine I. Since the vexillum consisted of a flag suspended from the crossbar of a cross, it was ideally suited to symbolize the crucifixion of Christ.
Ancient sources draw an unambiguous distinction between the two terms "labarum" and "Chi-Rho", even though later usage sometimes regards the two as synonyms. The name labarum was applied both to the original standard used by Constantine the Great and to the many standards produced in imitation of it in the Late Antique world, and subsequently.
Etymology.
Beyond its derivation from Latin "labarum", the etymology of the word is unclear. Some derive it from Latin /labāre/ 'to totter, to waver' (in the sense of the "waving" of a flag in the breeze) or "laureum ("laurel standard"). According to the Real Academia Española, the related lábaro is also derived from Latin "labărum" but offers no further derivation from within Latin, as does the Oxford English Dictionary. An origin as a loan into Latin from a Celtic language or Basque has also been postulated. There is a traditional Basque symbol called the lauburu; though the name is only attested from the 19th century onwards the motif occurs in engravings dating as early as the 2nd century AD.
Vision of Constantine.
On the evening of October 27, 312 AD, with his army preparing for the Battle of the Milvian Bridge, the emperor Constantine I claimed to have had a vision which led him to believe he was fighting under the protection of the Christian God.
Lactantius states that, in the night before the battle, Constantine was commanded in a dream to "delineate the heavenly sign on the shields of his soldiers". He obeyed and marked the shields with a sign denoting Christ. Lactantius describes that sign as a "staurogram", or a Latin cross with its upper end rounded in a P-like fashion, rather than the better known Chi-Rho sign described by Eusebius of Caesarea. Thus, it had both the form of a cross and the monogram of Christ's name from the formed letters "Χ" and "Ρ", the first letters of Christ's name in Greek.
From Eusebius, two accounts of a battle survive. The first, shorter one in the "Ecclesiastical History" leaves no doubt that God helped Constantine but does not mention any vision. In his later "Life of Constantine", Eusebius gives a detailed account of a vision and stresses that he had heard the story from the emperor himself. According to this version, Constantine with his army was marching somewhere (Eusebius does not specify the actual location of the event, but it clearly is not in the camp at Rome) when he looked up to the sun and saw a cross of light above it, and with it the Greek words "Ἐν Τούτῳ Νίκα". The traditionally employed Latin translation of the Greek is "in hoc signo vinces"— literally "In this sign, you will conquer." However, a direct translation from the original Greek text of Eusebius into English gives the phrase "By this, conquer!"
At first he was unsure of the meaning of the apparition, but the following night he had a dream in which Christ explained to him that he should use the sign against his enemies. Eusebius then continues to describe the labarum, the military standard used by Constantine in his later wars against Licinius, showing the Chi-Rho sign.
Those two accounts can hardly be reconciled with each other, though they have been merged in popular notion into Constantine seeing the Chi-Rho sign on the evening before the battle. Both authors agree that the sign was not readily understandable as denoting Christ, which corresponds with the fact that there is no certain evidence of the use of the letters chi and rho as a Christian sign before Constantine. Its first appearance is on a Constantinian silver coin from c. 317, which proves that Constantine did use the sign at that time, though not very prominently. He made extensive use of the Chi-Rho and the labarum only later in the conflict with Licinius.
The vision has been interpreted in a solar context (e.g. as a solar halo phenomenon), which would have been reshaped to fit with the Christian beliefs of the later Constantine.
An alternate explanation of the intersecting celestial symbol has been advanced by George Latura, which claims that Plato's visible god in "Timaeus" is in fact the intersection of the Milky Way and the Zodiacal Light, a rare apparition important to pagan beliefs that Christian bishops reinvented as a Christian symbol.
Eusebius' description of the labarum.
"A Description of the Standard of the Cross, which the Romans now call the Labarum."
"Now it was made in the following manner. A long spear, overlaid with gold, formed the figure of the cross by means of a transverse bar laid over it. On the top of the whole was fixed a wreath of gold and precious stones; and within this, the symbol of the Saviour’s name, two letters indicating the name of Christ by means of its initial characters, the letter P being intersected by X in its centre: and these letters the emperor was in the habit of wearing on his helmet at a later period. From the cross-bar of the spear was suspended a cloth, a royal piece, covered with a profuse embroidery of most brilliant precious stones; and which, being also richly interlaced with gold, presented an indescribable degree of beauty to the beholder. This banner was of a square form, and the upright staff, whose lower section was of great length, of the pious emperor and his children on its upper part, beneath the trophy of the cross, and immediately above the embroidered banner."
"The emperor constantly made use of this sign of salvation as a safeguard against every adverse and hostile power, and commanded that others similar to it should be carried at the head of all his armies."
Iconographic career under Constantine.
Among a number of standards depicted on the Arch of Constantine, which was erected, largely with fragments from older monuments, just three years after the battle, the labarum does not appear. A grand opportunity for just the kind of political propaganda that the Arch otherwise was expressly built to present was missed. That is if Eusebius' oath-confirmed account of Constantine's sudden, vision-induced, conversion can be trusted. Many historians have argued that in the early years after the battle the emperor had not yet decided to give clear public support to Christianity, whether from a lack of personal faith or because of fear of religious friction. The arch's inscription does say that the Emperor had saved the "res publica" INSTINCTV DIVINITATIS MENTIS MAGNITVDINE ("by greatness of mind and by instinct impulse of divinity"). As with his predecessors, sun symbolism – interpreted as representing "Sol Invictus" (the Unconquered Sun) or Helios, Apollo or Mithras – is inscribed on his coinage, but in 325 and thereafter the coinage ceases to be explicitly pagan, and Sol Invictus disappears. In his "Historia Ecclesiae" Eusebius further reports that, after his victorious entry into Rome, Constantine had a statue of himself erected, "holding the sign of the Savior cross in his right hand." There are no other reports to confirm such a monument.
Whether Constantine was the first Christian emperor supporting a peaceful transition to Christianity during his rule, or an undecided pagan believer until middle age, strongly influenced in his political-religious decisions by his Christian mother St. Helena, is still in dispute among historians.
As for the labarum itself, there is little evidence for its use before 317. In the course of Constantine's second war against Licinius in 324, the latter developed a superstitious dread of Constantine's standard. During the attack of Constantine's troops at the Battle of Adrianople the guard of the labarum standard were directed to move it to any part of the field where his soldiers seemed to be faltering. The appearance of this talismanic object appeared to embolden Constantine's troops and dismay those of Licinius. At the final battle of the war, the Battle of Chrysopolis, Licinius, though prominently displaying the images of Rome's pagan pantheon on his own battle line, forbade his troops from actively attacking the labarum, or even looking at it directly.
Constantine felt that both Licinius and Arius were agents of Satan, and associated them with the serpent described in the Book of Revelation (). Constantine represented Licinius as a snake on his coins. 
Eusebius stated that in addition to the singular labarum of Constantine, other similar standards (labara) were issued to the Roman army. This is confirmed by the two labara depicted being held by a soldier on a coin of Vetranio (illustrated) dating from 350.
Later usage.
A later Byzantine manuscript indicates that a jewelled labarum standard believed to have been that of Constantine was preserved for centuries, as an object of great veneration, in the imperial treasury at Constantinople. The labarum, with minor variations in its form, was widely used by the Christian Roman emperors who followed Constantine I.
A miniature version of the labarum became part of the imperial regalia of Byzantine rulers, who were often depicted carrying it in their right hands.
The term "labarum" is used generally for any ecclesiastical banner, such as those carried in religious processions.
"The Holy Lavaro" were a set of early national Greek flags, blessed by the Greek Orthodox Church. Under these banners the Greeks united throughout the Greek Revolution (1821), a war of liberation waged against the Ottoman Empire.
Labarum also gives its name (Labaro) to a suburb of Rome adjacent to Prima Porta, one of the sites where the 'Vision of Constantine' is placed by tradition.
The Labarum is also used, within the North American higher education system, as the symbol for the National Fraternity of Alpha Chi Rho
Modern secular interpretations of Constantine's vision.
There are modern astronomical and astrological theories that lend credence to Eusebius's account. In 1948, Fritz Heiland, of the Zeiss planetarium at Jena, published his astronomical interpretation of Constantine's vision, that the fall of the year 312 was attended by an unusual spectacle: the syzygy or close alignment of three bright planets, Mars, Saturn and Jupiter, in the evening sky above the southwest horizon, positioned along a line within about 20 degrees of each other on the border of Capricorn and Sagittarius.
The Swedish geologist Jens Ormo and co-authors suggest that the account may have had its origins in Constantine's witnessing the daylight effects of a meteorite's descent through Earth's atmosphere, of which the impact he believes resulted in the Sirente crater situated in Sirente-Velino Regional Park in Abruzzo, Italy.

</doc>
<doc id="18385" url="https://en.wikipedia.org/wiki?curid=18385" title="Lactantius">
Lactantius

Lucius Caecilius Firmianus Lactantius was an early Christian author (c. 250 – c. 325) who became an advisor to the first Christian Roman emperor, Constantine I, guiding his religious policy as it developed, and a tutor to his son.
Biography.
Lactantius, a Latin-speaking native of North Africa, was not born into a Christian family. He was a pupil of Arnobius who taught at Sicca Veneria, an important town in the kingdom of Numidia. In his early life, he taught rhetoric in his native town, which may have been Cirta in Numidia, where an inscription mentions a certain "L. Caecilius Firmianus".
Lactantius had a successful public career at first. At the request of the Roman Emperor Diocletian, he became an official professor of rhetoric in Nicomedia; the voyage from Africa is described in his poem "Hodoeporicum". There, he associated in the imperial circle with the administrator and polemicist Sossianus Hierocles and the pagan philosopher Porphyry; he first met Constantine, and Galerius, whom he cast as villain in the persecutions. Having converted to Christianity, he resigned his post before Diocletian's purging of Christians from his immediate staff and before the publication of Diocletian's first "Edict against the Christians" (February 24, 303). 
As a Latin "rhetor" in a Greek city, he subsequently lived in poverty according to Saint Jerome and eked out a living by writing until Constantine I became his patron. The persecution forced him to leave Nicomedia and from the outbreak of hostilities until perhaps 311 or 313 he had to live elsewhere. The Emperor Constantine appointed the elderly Lactantius Latin tutor to his son Crispus. Lactantius followed Crispus to Trier in 317, when Crispus was made Caesar (lesser co-emperor) and sent to the city. Crispus was put to death in 326, but when Lactantius died and under what circumstances are unknown.
Writing.
Like so many of the early Christian authors, Lactantius depended on classical models. The early humanists called him the "Christian Cicero" ("Cicero Christianus"). A translator of the "Divine Institutes" wrote: "Lactantius has always held a very high place among the Christian Fathers, not only on account of the subject-matter of his writings, but also on account of the varied erudition, the sweetness of expression, and the grace and elegance of style, by which they are characterized."
He wrote apologetic works explaining Christianity in terms that would be palatable to educated people who still practiced the traditional religions of the Empire. He defended Christian beliefs against the criticisms of Hellenistic philosophers. His "Divinae Institutiones" ("Divine Institutes") were an early example of a systematic presentation of Christian thought.
He was considered somewhat heretical after his death, but Renaissance humanists took a renewed interest in him, more for his elaborately rhetorical Latin style than for his theology. His works were copied in manuscript several times in the 15th century and were first printed in 1465 by the Germans Arnold Pannartz and Konrad Sweynheim at the Abbey of Subiaco. This edition was the first book printed in Italy to have a date of printing, as well as the first use of a Greek alphabet font anywhere, which was apparently produced in the course of printing, as the early pages leave Greek text blank. It was probably the fourth book ever printed in Italy. A copy of this edition was sold at auction in 2000 for more than $1 million.
Prophetic exegesis.
Like many writers in the first few centuries of the early church, Lactantius took a premillennialist view, holding that the second coming of Christ will precede a millennium or a thousand-year reign of Christ on earth. According to Charles E. Hill, "With Lactantius in the early fourth century we see a determined attempt to revive a more “genuine” form of chiliasm." Lactantius quoted the Sibyls extensively. Book VII of "The Divine Institutes" indicates a familiarity with Jewish, Christian, Egyptian and Iranian apocalyptic material.
None of the fathers thus far had been more verbose on the subject of the millennial kingdom than Lactantius or more particular in describing the times and events preceding and following. He held to the literalist interpretation of the millennium, that the millennium originates with the second advent of Christ and marks the destruction of the wicked, the binding of the devil and the raising of the righteous dead.
He depicted Jesus reigning with the resurrected righteous on this earth during the seventh thousand years prior to the general judgment. In the end, the devil, having been bound during the thousand years, is loosed; the enslaved nations rebel against the righteous, who hide underground until the hosts, attacking the Holy City, are overwhelmed by fire and brimstone and mutual slaughter and buried altogether by an earthquake: rather unnecessarily, it would seem, since the wicked are thereupon raised again to be sent into eternal punishment. Next, God renews the earth, after the punishment of the wicked, and the Lord alone is thenceforth worshiped in the renovated earth.
Lactantius confidently stated that the beginning of the end would be the fall, or breakup, of the Roman Empire. However with the conversion of Constantine, and the improved lot of Christians this view fell out of favor. "Many Christians felt that any expectation of the downfall of the empire was as disloyal to God as it was to Rome." 
Attempts to determine the time of the End were viewed as in contradiction to Acts 1:7: "It is not for you to know the times or seasons that the Father has established by his own authority," and Mark 13:32: ""But of that day or hour, no one knows, neither the angels in heaven, nor the Son, but only the Father."

</doc>
<doc id="18386" url="https://en.wikipedia.org/wiki?curid=18386" title="Laconia">
Laconia

Laconia (), also known as Lacedaemonia, is a region in the southeastern part of the Peloponnese peninsula. Its administrative capital is Sparta. The word "laconic" is derived from the name of the region by analogy—to speak in a concise way, as the Spartans were reputed by the Athenians to do.
Geography.
Laconia is bordered by Messenia to the west and Arcadia to the north and is surrounded by the Myrtoan Sea to the east and by the Laconian Gulf and the Mediterranean Sea to the south. It encompasses Cape Malea and Cape Tainaron and a large part of the Mani Peninsula. The islands of Kythira and Antikythera lie to the south, but they administratively belong to the Attica regional unit of islands. The island, Elafonisos, situated between the Laconian mainland and Kythira, is part of Laconia.
The Evrotas is the longest river in the prefecture. The valley of the Evrotas is predominantly an agricultural region that contains many citrus groves, olive groves, and pasture lands. It is the location of the largest orange production in the Peloponnese and probably in all of Greece. Lakonia, a brand of orange juice, is based in Amykles.
The main mountain ranges are the Taygetus (2,407 m) in the west and the Parnon (1,961 m) in the northeast. Taygetus, known as Pentadaktylos ("five-fingers") throughout the Middle Ages, is west of Sparta and the Evrotas valley. It is the highest mountain in Laconia and the Peloponnese and is mostly covered with pine trees. Two roads join the Messenia and Laconia prefectures: one is a tortuous mountain pass through Taygetus and the other bypasses the mountain via the Mani district to the south.
The stalactite cave, Dirou, a major tourist attraction, is located south of Areopolis in the southwest of Laconia.
Climate.
Laconia has a Mediterranean climate with warm winters and hot summers. Snow is rare on the coast throughout the winter but is very common in the mountains.
History.
Ancient history.
In ancient Greece, this was the principal region of the Spartan state. Throughout classical antiquity, the Spartan sphere of influence expanded to Messenia, whose inhabitants (the Helots) were permanently enslaved. Significant archaeological recovery exists at the Vaphio-tomb site in Laconia. Found here are advanced Bronze Age art as well as evidence of cultural associations with the contemporaneous Minoan culture on Crete. Laconia was at war with the Kingdom of Macedonia and saw several battles; at the end of the Mycenaean period, population of Laconia sharply declined. From the early-2nd century BC until 395, it was a part of the Roman Empire.
Medieval history.
In the medieval period, Laconia formed part of the Byzantine Empire. Following the Fourth Crusade, it was gradually conquered by the Frankish Principality of Achaea. In the 1260s, however, the Byzantines recovered Mystras and other fortresses in the region and managed to evict the Franks from Laconia, which became the nucleus of a new Byzantine province. By the mid-14th century, this evolved into the Despotate of Morea, held by the last Greek ruling dynasty, the Palaiologoi. With the fall of the Despotate to the Ottomans in 1460, Laconia was conquered as well.
Modern history.
With the exception of a 30-year interval of Venetian rule, Laconia remained under Ottoman control until the outbreak of the Greek War of Independence of 1821. Following independence, Sparta was selected as the capital of the modern prefecture, and its economy and agriculture expanded. With the incorporation of the British-ruled Ionian Islands into Greece in 1864, Elafonissos became part of the prefecture. After World War II and the Greek Civil War, its population began to somewhat decline, as people moved from the villages toward the larger cities of Greece and abroad.
In 1992, a devastating fire ruined the finest olive crops in the northern part of the prefecture, and affected the area of Sellasia along with Oinountas and its surrounding areas. Firefighters, helicopters and planes battled for days to put out the horrific fire.
The Mani portion along with Gytheio became famous in Greece for filming episodes of "Vendetta", broadcast on Mega Channel throughout Greece and abroad on Mega Cosmos.
In early 2006, flooding ruined olive and citrus crops as well as properties and villages along the Evrotas river. In the summer 2006, a terrible fire devastated a part of the Mani Peninsula, ruining forests, crops, and numerous villages.
Municipalities of Laconia.
The regional unit, Laconia, is subdivided into five municipalities. These are (number as in the map in the infobox):
Prefecture.
As a part of the 2011 Kallikratis government reform, regional unit Laconia was created out of the former prefecture Laconia (). The prefecture had the same territory as the present regional unit. At the same time, the municipalities were reorganised, according to the table below.
Provinces.
"Note:" Provinces no longer hold any legal status in Greece.
Population.
The main cities and towns of Laconia are (ranked by 2011 census population):

</doc>
<doc id="18388" url="https://en.wikipedia.org/wiki?curid=18388" title="Laocoön">
Laocoön

Laocoön (; , ), the son of Acoetes, is a figure in Greek and Roman mythology and the Epic Cycle. He was a Trojan priest who was attacked, with his two sons, by giant serpents sent by the gods. Though not mentioned by Homer, the story of Laocoön had been the subject of a tragedy, now lost, by Sophocles and was mentioned by other Greek writers, though the events around the attack by the serpents vary considerably. The most famous account of these is now in Virgil's "Aeneid" where Laocoön was a priest of Poseidon (or Neptune for the Romans), who was killed with both his sons after attempting to expose the ruse of the Trojan Horse by striking it with a spear.
Virgil gives Laocoön the famous line "Equō nē crēdite, Teucrī / Quidquid id est, timeō Danaōs et dōna ferentēs", or "Do not trust the Horse, Trojans / Whatever it is, I fear the Greeks even bearing gifts." This line is the source of the saying: "Beware of Greeks bearing gifts."
In Sophocles, however, he was a priest of Apollo, who should have been celibate but had married. The serpents killed only the two sons, leaving Laocoön himself alive to suffer. In other versions he was killed for having committed an impiety by making love with his wife in the presence of a cult image in a sanctuary, or simply making a sacrifice in the temple with his wife present. In this second group of versions, the snakes were sent by Poseidon and in the first by Poseidon and Athena, or Apollo, and the deaths were interpreted by the Trojans as proof that the horse was a sacred object. The two versions have rather different morals: Laocoön was either punished for doing wrong, or for being right.
Death.
The most detailed description of Laocoön's grisly fate was provided by Quintus Smyrnaeus in "Posthomerica", a later, literary version of events following the "Iliad". According to Quintus, Laocoön begged the Trojans to set fire to the horse to ensure it was not a trick. Athena, angry with him and the Trojans, shook the ground around Laocoön's feet and painfully blinded him. The Trojans, watching this unfold, assumed Laocoön was punished for the Trojans' mutilating and doubting Sinon, the undercover Greek soldier sent to convince the Trojans to let him and the horse inside their city walls. Thus, the Trojans wheeled the great wooden Horse in. Laocoön did not give up trying to convince the Trojans to burn the horse, and Athena makes him pay even further. She sends two giant sea serpents to strangle and kill him and his two sons. In another version of the story, it was said that Poseidon sent the sea serpents to strangle and kill Laocoön and his two sons.
According to Apollodorus, it was Apollo who sent the two sea serpents. Laocoön had insulted Apollo by sleeping with his wife in front of the "divine image".
Virgil used the story in the "Aeneid." According to Virgil, Laocoön advised the Trojans to not receive the horse from the Greeks. They disregarded Laocoön's advice and were taken in by the deceitful testimony of Sinon. The enraged Laocoön threw his spear at the Horse in response. Minerva then sent sea-serpents to strangle Laocoön and his two sons, Antiphantes and Thymbraeus, for his actions. "Laocoön, ostensibly sacrificing a bull to Neptune on behalf of the city (lines 201ff.), becomes himself the tragic victim, as the simile (lines 223–24) makes clear. In some sense, his death must be symbolic of the city as a whole," S. V. Tracy notes. According to the Hellenistic poet Euphorion of Chalcis, Laocoön is in fact punished for procreating upon holy ground sacred to Poseidon; only unlucky timing caused the Trojans to misinterpret his death as punishment for striking the Horse, which they bring into the city with disastrous consequences. The episode furnished the subject of Sophocles' lost tragedy, "Laocoön".
In "Aeneid", Virgil describes the circumstances of Laocoön's death:
The death of Laocoön was famously depicted in a much-admired marble "Laocoön and his Sons", attributed by Pliny the Elder to the Rhodian sculptors Agesander, Athenodoros, and Polydorus, which stands in the Vatican Museums, Rome. Copies have been executed by various artists, notably Baccio Bandinelli. These show the complete sculpture (with conjectural reconstructions of the missing pieces) and can be seen in Rhodes, at the Palace of the Grand Master of the Knights of Rhodes, Rome, the Uffizi Gallery in Florence and in front of the Archaeological Museum, Odessa, Ukraine, amongst others.
The marble Laocoön provided the central image for Lessing's "Laocoön", 1766, an aesthetic polemic directed against Winckelmann and the comte de Caylus. Daniel Albright reengages the role of the figure of Laocoön in aesthetic thought in his book "Untwisting the Serpent: Modernism in Literature, Music, and Other Arts". El Greco painting
In addition to other literary references, John Barth employs a bust of Laocoön in his novella, "The End of the Road". The R.E.M. song "Laughing" references Laocoön, rendering him female ("Laocoön and her two sons"). The marble's pose is parodied in the comic book "Asterix and the Laurel Wreath". American author Joyce Carol Oates also references Laocoön in her 1989 novel "American Appetites". In Stave V of "A Christmas Carol", by Charles Dickens (1843), Scrooge awakes on Christmas morning, "making a perfect Laocoon of himself with his stockings". Barbara Tuchman's "The March of Folly" begins with an extensive analysis of the Laocoön story. The American feminist poet and author Marge Piercy includes a poem titled, "Laocoön is the name of the figure", in her collection "Stone, Paper, Knife" (1983), relating love lost and beginning.
In Hector Berlioz opera "Les Troyens", the death of Laocoon is a pivotal moment of the first act after Aeneas entrance, sung by eight singers and a double choir ("ottetto et double chœur"). It begins with the verse "Châtiment effroyable" ("frightful punishment").
References.
Classical sources.
Compiled by Tracy, 1987:452 note 3, which also mentions a fragmentary line possibly by Nicander.

</doc>
<doc id="18389" url="https://en.wikipedia.org/wiki?curid=18389" title="Limburg an der Lahn">
Limburg an der Lahn

Limburg an der Lahn (officially abbreviated "Limburg a. d. Lahn") is the district seat of Limburg-Weilburg in Hesse, Germany.
Geography.
Location.
Limburg lies in western Hesse between the Taunus and the Westerwald on the river Lahn.
The town lies roughly centrally in a basin within the Rhenish Slate Mountains which is surrounded by the low ranges of the Taunus and Westerwald and called the Limburg Basin ("Limburger Becken"). Owing to the favourable soil and climate, the Limburg Basin stands as one of Hesse's richest agricultural regions and moreover, with its convenient Lahn crossing, it has been of great importance to transport since the Middle Ages. Within the basin, the Lahn's otherwise rather narrow lower valley broadens out noticeably, making Limburg's mean elevation only 117 m above sea level.
Neighbouring communities.
Limburg forms, together with the town of Diez, a middle centre (in terms of Central place theory) but partially functions as an upper centre to western Middle Hesse.
Limburg's residential neighbourhoods reach beyond the town limits; the neighbouring centres of Elz and Diez run seamlessly together.
Surrounding towns and communities are the community of Elz and the town of Hadamar in the north, the community of Beselich in the northeast, the town of Runkel in the east, the communities of Villmar and Brechen in the southeast, the community of Hünfelden in the south (all in Limburg-Weilburg), the community of Holzheim in the southwest, and the town of Diez and the communities of Aull and Gückingen in the west (all in the Rhein-Lahn-Kreis in Rhineland-Palatinate).
The nearest major cities are Wetzlar and Gießen to the north east, Wiesbaden and Frankfurt to the south and Koblenz to the west.
Constituent communities.
The town consists of eight formerly autonomous "Ortsteile" or villages, listed here by population.
Likewise often called a constituent community is Blumenrod, although this is actually only a big residential neighbourhood in the main town’s south end. Its landmark is the "Domäne Blumenrod", a former manor house that has been restored and remodelled by the Limburg Free Evangelical community.
Limburg’s biggest outlying centre is Lindenholzhausen (3,329 residents as of June 2006); the second biggest is Linter.
Etymology.
The derivation of the name “Limburg” is not quite clear and may well hearken back to a castle built here ("Burg" means "castle" in German). In 910 the town was first mentioned as "Lintpurc". Two of the popular theories are:
History.
About 800, the first castle buildings arose on the Limburg crags. This was probably designed for the protection of a ford over the river Lahn. In the decades that followed, the town developed under the castle's protection. Limburg is first mentioned in documents in 910 under the name of "Lintpurc" when Louis the Child granted Konrad Kurzbold an estate in the community on which he was to build a church. Konrad Kurzbold laid the foundation stone for Saint George's Monastery Church, where he was also buried. The community soon increased in importance with the monastery's founding and profited from the lively goods trade on the "Via Publica".
In 1150, a wooden bridge was built across the Lahn. The long-distance road from Cologne to Frankfurt am Main subsequently ran through Limburg. In the early 13th century, Limburg Castle was built in its current form. Shortly afterwards, the town passed into the ownership of the Lords of Ysenburg. In 1214, the community was granted town rights. Remains of the fortification wall from the years 1130, 1230 and 1340 with a maxiumum length of roughly one thousand metres indicate to this day the blossoming town's quick development in the Middle Ages. There is proof of a mint in Limburg in 1180.
One line of the Lords of Ysenburg resided from 1258 to 1406 at Limburg Castle and took their name from their seat, Limburg. From this line came the House of Limburg-Stirum and also Imagina of Isenburg-Limburg, German King Adolf's wife.
The ruling class among the mediaeval townsfolk were rich merchant families whose houses stood right near the castle tower and were surrounded by the first town wall once it was built. The area of today's Rossmarkt ("Horse Market"), in which many simple craftsmen lived, was only brought within the fortifications once the second town wall was built. The inhabitants there, however, unlike the merchant élite, were accorded no entitlement to a voice in town affairs and were not allowed to send representatives to the town council. Nevertheless, they had to bear the main financial burden of running the town. Only in 1458 were they allowed to send two representatives to town council.
Saint George's Cathedral ("Sankt-Georgs-Dom") built on the old monastery church's site, and also called "Georgsdom", was consecrated in 1235. On 14 May 1289, a devastating fire wiped out great parts of the inner town, although these were subsequently rebuilt. One of the houses built at that time was the Römer 2-4-6, which is today one of Germany's oldest half-timbered houses. In 1337, Limburg's Jews were expelled from the town. Only in 1341 were they once again able to settle in the town, by royal decree. In 1344 a half share of the town was pledged to the Electorate of Trier, and in 1420, the town passed wholly into the ownership of Trier. This event, along with another town fire in 1342, the Black Death in 1349, 1356 and 1365, but above all the rise of the Territorial Princes, led to a gradual decline. In 1315 and 1346, the old stone Lahn Bridge was built (presumably in two sections).
Against the background of the German Peasants' War, unrest also arose among the townsfolk in 1525. After the Elector of Trier had demanded that the townsmen turn a Lutheran preacher out of the town, a board made up of townsmen who were ineligible for council functions handed the council a 30-point comprehensive list of demands on 24 May. It dealt mainly with financial participation and equality in taxation, trade and building issues with the merchant class. In the days that followed, these demands were reduced in negotiations between the council and the board to 16 points, which were likely also taken up with the Elector afterwards. On 5 August, however, Archbishop Richard ordered the council to overturn all concessions to the townsmen. Furthermore, a ban on assembly was decreed, and the ineligible townsmen were stripped of their right to send two representatives to council.
In 1806, Limburg came into the possession of the newly founded Duchy of Nassau. In 1818 the town wall was torn down. In 1827 the town was raised to a Catholic episcopal seat. In 1866 the Duchy and with it Limburg passed to Prussia in the wake of the Austro-Prussian War. As of 1862, Limburg became a railway hub and from 1886 a district seat. In 1892, the Pallottines settled in town, but only the men; the women came in 1895.
During World War I there was a major prisoner of war camp at Limburg an der Lahn. Many Irish members of the British Army were interned there until the end of the war and at one stage they were visited by the Irish republican leader Roger Casement in an attempt to win recruits for the forthcoming Irish rebellion.
From 1919 to 1923, Limburg was the "capital" of a short-lived state called Free State Bottleneck (or "Freistaat Flaschenhals" in German) because it was the nearest unoccupied town to the Weimar Republic.
Politics.
Town council.
The municipal election held on 27 March 2011 yielded the following results:
Mayor.
The town's mayor is currently Martin Richard (CDU).
"Patenschaft".
In 1956, a "Patenschaft" – roughly, a sponsorship – was undertaken for Sudeten Germans driven out of the town of Mährisch Neustadt in the Sternberg district.
Economy and infrastructure.
Transport.
Limburg is a traditional transport hub. Already in the Middle Ages, the "Via Publica" crossed the navigable Lahn here. Today the A 3 (Emmerich–Oberhausen–Cologne–Frankfurt–Nuremberg–Passau) and "Bundesstraße" 8, which both follow the "Via Publica's" alignment as closely as possible, run through the town. "Bundesstraße" 49 links Limburg to Koblenz towards the west and Wetzlar and Gießen towards the east. The section between Limburg and Wetzlar is currently being widened to four lanes. This section as far as Obertiefenbach is also known as "Die lange Meil" ("The Long Mile"). "Bundesstraße" 54 links Limburg on the one hand with Siegen to the north and on the other by way of Diez with Wiesbaden, which may likewise be reached over "Bundesstraße" 417 ("Hühnerstraße").
As early as 1248, a wooden bridge spanned the Lahn, but was replaced after the flooding in 1306 by a stone bridge, the "Alte Lahnbrücke". Other road bridges are the "Lahntalbrücke Limburg" (1964) on the A 3, the "Lahnbrücke" near Staffel and the "Neue Lahnbrücke" from 1968, over which run the "Bundesstraßen" before they cross under the inner town through the "Schiedetunnel", a bypass tunnel.
Once the "Lahntalbahn" had been built, Limburg was joined to the railway network in 1862. Limburg railway station developed into a transport hub. Eschhofen station is also in Limburg. Other railway lines are the "Unterwesterwaldbahn", the "Oberwesterwaldbahn" and the Main-Lahn Railway. At Niedernhausen station on the "Main-Lahn Railway", transfer to the "Ländchesbahn" to Wiesbaden is possible. With the exception of the upper section of the "Lahntalbahn" and express lines to Koblenz and Frankfurt, which are still served by Deutsche Bahn, all railway lines are run by Vectus Verkehrsgesellschaft mbH, based in Limburg.
Once the InterCityExpress Cologne-Frankfurt high-speed rail line had been built, Limburg acquired an ICE station. It is the only railway station in Germany at which exclusively ICE trains stop. The high-speed rail line crosses the Lahn over the "Lahntalbrücke" and then dives into the "Limburger Tunnel".
The nearest airport is Frankfurt Airport, 63 km away on the A 3. Travel time there on the ICE is roughly 20 minutes. Cologne Bonn Airport is 110 km away and can be reached on the ICE in 44 minutes.
The Lahn between Lahnstein and Wetzlar is a "Bundeswasserstraße" ("Federal waterway"). Since the "Lahntalbahn's" expansion, however, the waterway's importance has been declining. It is used mainly by tourists with small motorboats, canoes and rowboats. Limburg is the landing site of the tourboat "Wappen von Limburg".
Public institutions.
Education.
Limburg has four schools which lead to, among other qualifications, the Abitur:
Professional training schools:
Hauptschulen and Realschulen:
Libraries:
St. Vincenz Hospital.
The hospital perched on the Schafsberg overlooking the town has at its disposal 433 beds and 15 specialist departments.
Sport and leisure.
In Limburg there are various sport clubs; some are even represented in "Bundesligen", and even at the world level.
Youth meeting place in Limburg.
The Evangelical Church offers with its "Jugendfreizeitstätte Limburg" (JFS for short, meaning "Youth Leisure Place") a meeting place for youth with many events. With table football, Internet café and many events, this institution is not only church-based, with two staff and a "Zivildienstleistender" supporting the visitors not only with their problems.
Limburg Mothers' Centre.
The "Mütterzentrum Limburg" is a family meeting place for those with or without children on Hospitalstraße. The club is supported by the town of Limburg and the "Bundesland" of Hesse and offers among other things a parents' service that looks after children, a broad array of course offerings for children and adults, a miniature kindergarten and a café.
Culture and sightseeing.
Theatre.
The cabaret troupe "Thing", founded more than 25 years ago, moved after a short time from its initial home in the outlying centre of Staffel to the Josef-Kohlmaier-Halle, a civic event hall, where its stage can now be found in the hall's club rooms. The troupe is run by an independent acting club. On the programme are chanson, cabaret, literature and jazz as well as folk, Rock and performances by singer-songwriters. It makes a point of furthering young artists. Each month, three or four events are staged.
The dedication of "Thing" was recognized on 6 December 2003 when the "Kulturpreis Mittelhessen" ("Middle Hesse Culture Prize") was awarded to it.
Limburg Cathedral has a famous boys' choir, the "Limburger Domsingknaben", although they are actually based at the "Musical Boarding School" in Hadamar just outside Limburg.
Museums.
In Limburg there are several museums. The most important are:
Buildings.
Only a few towns, like Limburg, have been able to keep a full set of nearly unscathed mediaeval buildings. The formerly walled town core between St. George’s Cathedral, Grabenstraße (a street marking the old town moat) and the 600-year-old Lahn Bridge thus stands today as a whole under monumental protection.
The "Altstadt" ("Old Town") boasts a fine cathedral and is full of narrow streets with timber-frame houses, dating mainly from the 17th and 18th centuries. That's why it is located on the German Timber-Frame Road.

</doc>
<doc id="18390" url="https://en.wikipedia.org/wiki?curid=18390" title="Lavrentiy Beria">
Lavrentiy Beria

Lavrentiy Pavlovich Beria () (; ; 29 March 1899 – 23 December 1953) was a Soviet politician of Georgian ethnicity, Marshal of the Soviet Union and state security administrator, chief of the Soviet security and secret police apparatus (NKVD) under Joseph Stalin during World War II, and Deputy Premier in the postwar years (1946–53).
Beria was the longest-lived and most influential of Stalin's secret police chiefs, wielding his most substantial influence during and after World War II. He simultaneously administered vast sections of the Soviet state and served as "de facto" Marshal of the Soviet Union in command of the NKVD field units responsible for anti-Nazi partisan operations on the Eastern Front during World War II, as well as for acting as barrier troops and the apprehension of thousands of "turncoats, deserters, cowards and suspected malingerers." Beria administered the vast expansion of the Gulag labor camps and was primarily responsible for overseeing the secret defense institutions known as sharashkas, critical to the war effort. He also played the decisive role in coordinating the Soviet partisans, developing an impressive intelligence and sabotage network behind German lines. He attended the Yalta Conference with Stalin, who introduced him to U.S. President Franklin D. Roosevelt as "our Himmler". After the war, he organized the communist takeover of the state institutions of Central and Eastern Europe. Beria's uncompromising ruthlessness in his duties and skill at producing results culminated in his success in overseeing the Soviet atomic bomb project. Stalin gave it absolute priority and the project was completed in under five years in no small part due to Soviet espionage against the West organized by Beria's NKVD.
Upon Stalin's death in March 1953, Beria was promoted to First Deputy Premier, where he carried out a campaign of liberalization. He was briefly a part of the ruling ""troika"" with Georgy Malenkov and Vyacheslav Molotov. Beria's overconfidence in his position after Stalin's death led him to misjudge other Politburo members. During the coup d'état led by Nikita Khrushchev and assisted by the military forces of Marshal Georgy Zhukov, Beria was arrested on charges of treason during a meeting in which the full Politburo condemned him. The compliance of the NKVD was ensured by Zhukov's troops, and after interrogation Beria was taken to the basement of the Lubyanka and shot by General Pavel Batitsky.
Early life and rise to power.
Beria was born in Merkheuli, near Sukhumi, in the Sukhumi district of Kutaisi Governorate (now Gulripshi District, Georgia, then part of the Russian Empire). He was from Mingrelian subethnic group of Georgians and grew up in a Georgian Orthodox family. Beria's mother, Marta Jaqeli (1868–1955), was a deeply religious, church-going woman (she spent much time in church and died in a church building); she was previously married and widowed before marrying Beria's father, Pavel Khukhaevich Beria (1872–1922), a landowner from Abkhazia. He also had a brother (name unknown), and a sister named Anna, who was born deaf-mute. In his autobiography, Lavrentiy Beria mentioned only his sister and his niece, implying that his brother (or any other siblings for that matter) either was dead or had no relationship with Beria after he left Merkheuli. Beria attended a technical school in Sukhumi, and joined the Bolsheviks in March 1917 while a student in the Baku Polytechnicum (subsequently known as the Azerbaijan State Oil Academy). As a student, Beria distinguished himself in mathematics and the sciences. The Polytechnicum's curriculum concentrated on the petroleum industry.
Beria also worked for the anti-Bolshevik Mussavatists in Baku. After the city's capture by the Red Army (28 April 1920), Beria was saved from execution only because there was likely little arrangement time and Sergei Kirov had possibly intervened. While in prison, he formed a connection with Nina Gegechkori (1905–10 June 1991), his cellmate's niece, and they eloped on a train. She was 17, a trained scientist from an aristocratic family.
In 1919, at the age of twenty, Beria started his career in state security when the security service of the Azerbaijan Democratic Republic hired him while still a student at the Polytechnicum. In 1920 or 1921 (accounts vary), Beria joined the Cheka – the original Bolshevik secret police. At that time, a Bolshevik revolt took place in the Menshevik-controlled Democratic Republic of Georgia, and the Red Army subsequently invaded. The Cheka became heavily involved in the conflict, which resulted in the defeat of the Mensheviks and the formation of the Georgian SSR. By 1922, Beria was deputy head of the Georgian branch of Cheka's successor, the OGPU.
In 1924 he led the repression of a Georgian nationalist uprising, after which up to 10,000 people were executed. For this display of "Bolshevik ruthlessness," Beria was appointed head of the "secret-political division" of the Transcaucasian OGPU and was awarded the Order of the Red Banner.
In 1926 Beria became head of the Georgian OGPU; Sergo Ordzhonikidze, head of the Transcaucasian party, introduced him to fellow-Georgian Joseph Stalin. As a result, Beria became an ally in Stalin's rise to power. During his years at the helm of the Georgian OGPU, Beria effectively destroyed the intelligence networks that Turkey and Iran had developed in the Soviet Caucasus, while successfully penetrating the governments of these countries with his agents. He also took over Stalin's holiday security.
Beria was appointed Secretary of the Communist Party in Georgia in 1931, and for the whole Transcaucasian region in 1932. He became a member of the Central Committee of the Communist Party of the Soviet Union in 1934. During this time, he began to attack fellow members of the Georgian Communist Party, particularly Gaioz Devdariani, who served as Minister of Education of the Georgian SSR. Beria ordered the executions of Devdariani's brothers George and Shalva, who held important positions in the Cheka and the Communist Party respectively.
By 1935 Beria had become one of Stalin's most trusted subordinates. He cemented his place in Stalin's entourage with a lengthy oration titled, "On the History of the Bolshevik Organisations in Transcaucasia" (later published as a book), which emphasized Stalin's role. When Stalin's purge of the Communist Party and government began in 1934 after the assassination of Leningrad party boss Sergei Kirov (1 December 1934), Beria ran the purges in Transcaucasia. He used the opportunity to settle many old scores in the politically turbulent Transcaucasian republics.
In June 1937 he said in a speech, "Let our enemies know that anyone who attempts to raise a hand against the will of our people, against the will of the party of Lenin and Stalin, will be mercilessly crushed and destroyed."
Head of the NKVD.
In August 1938, Stalin brought Beria to Moscow as deputy head of the People's Commissariat for Internal Affairs (NKVD), the ministry which oversaw the state security and police forces. Under Nikolai Yezhov, the NKVD carried out the Great Purge: the imprisonment or execution of millions of people throughout the Soviet Union as alleged "enemies of the people." By 1938, however, the oppression had become so extensive that it was damaging the infrastructure, economy and even the armed forces of the Soviet state, prompting Stalin to wind the purge down. Stalin had thoughts to appoint Lazar Kaganovich as head of the NKVD, but chose Beria probably because he was a professional secret policeman. In September, Beria was appointed head of the Main Administration of State Security (GUGB) of the NKVD, and in November he succeeded Yezhov as NKVD head (Yezhov was executed in 1940). The NKVD was purged next, with half its personnel replaced by Beria loyalists, many of them from the Caucasus.
Although Beria's name is closely identified with the Great Purge because of his activities while deputy head of the NKVD, his leadership of the organisation marked an easing of the repression begun under Yezhov. Over 100,000 people were released from the labour camps. The government officially admitted that there had been some injustice and "excesses" during the purges, which were blamed entirely on Yezhov. The liberalisation was only relative: arrests and executions continued, and in 1940, as war approached, the pace of the purges again accelerated. During this period, Beria supervised deportations of people identified as political enemies from Poland and the Baltic states after Soviet occupation of those regions.
In March 1939, Beria became a candidate member of the Communist Party's Politburo. Although he did not become a full member until 1946, he was already one of the senior leaders of the Soviet state. In 1941 Beria was made a Commissar General of State Security, the highest quasi-military rank within the Soviet police system of that time, effectively comparable to a Marshal of the Soviet Union.
On 5 March 1940, after the Gestapo–NKVD Third Conference was held in Zakopane, Beria sent a note (no. 794/B) to Stalin in which he stated that the Polish prisoners of war kept at camps and prisons in western Belarus and Ukraine were enemies of the Soviet Union, and recommended their execution. Most of them were military officers, but there were also intelligentsia, doctors, and priests and others for a total of over 22,000. With Stalin's approval, Beria's NKVD executed them in what became known as the Katyn massacre.
From October 1940 to February 1942, the NKVD under Beria carried out a new purge of the Red Army and related industries. In February 1941, Beria became Deputy Chairman of the Council of People's Commissars, and in June, following Nazi Germany's invasion of the Soviet Union, he became a member of the State Defense Committee (GKO). During World War II, he took on major domestic responsibilities and mobilized the millions of people imprisoned in NKVD Gulag camps into wartime production. He took control of the manufacture of armaments, and (with Georgy Malenkov) aircraft and aircraft engines. This was the beginning of Beria's alliance with Malenkov, which later became of central importance.
In 1944, as the Germans were driven from Soviet soil, Beria was in charge of dealing with the various ethnic minorities accused of anti-sovietism and/or collaboration with the invaders, including the Chechens, the Ingush, the Crimean Tatars, the Pontic Greeks and the Volga Germans. All these groups were deported to Soviet Central Asia (see "Population transfer in the Soviet Union.")
In December 1944, Beria's NKVD was assigned to supervise the Soviet atomic bomb project ("Task No. 1"), which built and tested a bomb by 29 August 1949. In this capacity, he ran the successful Soviet espionage campaign against the atomic weapons program of the United States, which obtained much of the technology required. His most important contribution was to provide the necessary workforce for this project, which was extremely labour-intensive. At least 330,000 people, including 10,000 technicians, were involved. The Gulag system provided tens of thousands of people for work in uranium mines and for the construction and operation of uranium processing plants. They also constructed test facilities, such as those at Semipalatinsk and in the Novaya Zemlya archipelago. The NKVD also ensured the necessary security for the project. Amazingly, the physicist Pyotr Kapitsa refused to work with Beria even after he gave him a hunting rifle as a gift. It is notable that Stalin backed Kapitsa in this quarrel.
In July 1945, as Soviet police ranks were converted to a military uniform system, Beria's rank was officially converted to that of Marshal of the Soviet Union. Although he had never held a traditional military command, Beria made a significant contribution to the victory of the Soviet Union in World War II through his organization of wartime production and his use of partisans. Stalin personally never thought much of it, and neither commented publicly on his performance nor awarded him recognition (i.e. Order of Victory) as he did for most other Soviet Marshals.
Postwar politics.
With Stalin nearing 70, the post-war years were dominated by a concealed struggle for succession among his supporters. At the end of the war, the most likely successor seemed to be Andrei Zhdanov, party leader in Leningrad during the war, who was in charge of all cultural matters by 1946. After 1946 Beria formed an alliance with Malenkov to counter Zhdanov's rise.
In January 1946, Beria resigned as chief of the NKVD while retaining general control over national security matters as Deputy Prime Minister and Curator of the Organs of State Security under Stalin, but the new chief, Sergei Kruglov, was not a Beria man. Also, by the summer of 1946, Beria's man Vsevolod Nikolayevich Merkulov was replaced as head of the Ministry for State Security (MGB) by Viktor Abakumov. Abakumov was the head of SMERSH from 1943 to 1946; his relationship with Beria was marked by close collaboration (since Abakumov owed his rise to Beria's support and esteem), but also by rivalry. Stalin had begun to encourage Abakumov to form his own network inside the MGB to counter Beria's dominance of the power ministries. Kruglov and Abakumov moved expeditiously to replace Beria's men in the security apparatus leadership with new people. Very soon Deputy Minister Stepan Mamulov of the Soviet Ministry of Internal Affairs was the only close Beria ally left outside foreign intelligence, on which Beria kept a grip. In the following months, Abakumov started carrying out important operations without consulting Beria, often working in tandem with Zhdanov, and sometimes on Stalin's direct orders. Some observers argue that these operations were aimed – initially tangentially, but with time more directly – at Beria.
One of the first such moves was the Jewish Anti-Fascist Committee affair that commenced in October 1946 and eventually led to the murder of Solomon Mikhoels and the arrest of many other members. This affair damaged Beria; not only had he championed the creation of the committee in 1942, but his own entourage included a substantial number of Jews.
After Zhdanov died suddenly in August 1948, Beria and Malenkov consolidated their power by a purge of Zhdanov's associates known as the "Leningrad Affair." Among the executed were Zhdanov's deputy, Aleksei Kuznetsov; the economic chief, Nikolai Voznesensky; the Party head in Leningrad, Pyotr Popkov; and the Prime Minister of the Russian Republic, Mikhail Rodionov. It was only after Zhdanov's death that Nikita Khrushchev began to be considered as a possible alternative to the Beria-Malenkov axis.
During the postwar years, Beria supervised the successful establishment of Communist regimes in the countries of Eastern Europe, usually by coup d'etat, and hand-picked the leaders. Starting in 1948, Abakumov initiated several investigations against these leaders, which culminated with the arrest in November 1951 of Rudolf Slánský, Bedřich Geminder, and others in Czechoslovakia. These men were generally accused of Zionism and cosmopolitanism, but, more specifically, of providing weapons to Israel. Beria was deeply disturbed by these charges, as large amounts of Czech arms had been sold to Israel on his direct orders. Beria wanted an alliance with Israel to advance the communist cause in the Middle East, while later Soviet leaders chose instead to form a powerful alliance with countries in the Arab World. Altogether, 14 Czechoslovak Communist leaders, 11 of them Jewish, were tried, convicted, and executed (see Slánský trial). Similar investigations in Poland and other Soviet satellite countries occurred at the same time.
Around that time, Abakumov was replaced by Semyon Ignatyev, who further intensified the anti-Semitic campaign. On 13 January 1953, the biggest anti-semitic affair in the Soviet Union was initiated with an article in "Pravda" that began what came to be known as the Doctors' plot, in which a number of the country's prominent Jewish physicians were accused of poisoning top Soviet leaders and arrested. Concurrently, an anti-semitic propaganda campaign, euphemistically termed the "struggle against rootless cosmopolitanism," occurred in the Soviet press. Initially, 37 men were arrested, but the number quickly grew into hundreds. Scores of Soviet Jews were dismissed from their jobs, arrested, sent to the Gulag, or executed. It is alleged that at this time on Stalin's orders the MGB started to prepare to deport all Soviet Jews to the Russian Far East or even massacre them. The issue of how much Stalin (and Beria) were involved in the Doctor's Plot is still disputed (see discussion in Doctors' plot article). Some historians claim that no such deportation was planned, or that the planned deportations were in an early planning stage when abandoned. Days after Stalin's death on 5 March, Beria freed all the arrested doctors, announced that the entire matter was fabricated, and arrested the MGB functionaries directly involved.
In other international issues, Beria (along with Mikoyan) correctly foresaw the victory of Mao Zedong in the Chinese Civil War and greatly helped the communist success by letting the Communist Party of China use Soviet-occupied Manchuria as a staging area and arranging huge weapons shipments to the People's Liberation Army, mainly from the recently captured equipment of the Japanese Kwantung Army.
Stalin's death.
Khrushchev wrote in his memoirs that Beria had, immediately after Stalin's stroke, gone about "spewing hatred against and mocking him." When Stalin showed signs of consciousness, Beria dropped to his knees and kissed his hand. When Stalin fell unconscious again, Beria immediately stood and spat.
Stalin's aide Vasili Lozgachev reported that Beria and Malenkov were the first members of the Politburo to investigate Stalin's condition after his stroke. They arrived at Stalin's dacha at Kuntsevo at 3am on March 2 after being called by Khrushchev and Bulganin. The latter did not want to risk Stalin's wrath by checking themselves. Lozgachev tried in futility to explain to Beria that the then-unconscious Stalin (still in his soiled clothing) was "sick and needed medical attention." Beria angrily dismissed his claims as panic-mongering and quickly left, ordering him, "Don't bother us, don't cause a panic and don't disturb Comrade Stalin!" Calling a doctor was deferred for a full 12 hours after Stalin was rendered paralyzed, incontinent, and unable to speak. This decision is noted as "extraordinary" by Sebag-Montefiore, but also consistent with the standard Stalinist policy of deferring all decision-making (no matter how necessary or obvious) without official orders from higher authority. Beria's decision to avoid immediately calling a doctor was silently supported (or at least not opposed) by the rest of the Politburo, which was rudderless without Stalin's micromanagement and paralyzed by a legitimate fear he would suddenly recover and wreak violent reprisal on anyone who had dared to act without his orders. Stalin's suspicion of doctors in the wake of the Doctors' Plot was well known. At the time of his stroke, his private physician was already being tortured in the basement of the Lubyanka for suggesting the leader required more bed rest.
After Stalin's stroke, Beria claimed to have killed him. This aborted a final purge of Old Bolsheviks Anastas Mikoyan and Vyacheslav Molotov for which Stalin had been laying the groundwork in the year prior to his death. Shortly after Stalin's death, Beria announced triumphantly to the Politburo that he had "done in" and "saved [us all", according to Molotov's memoirs. Notably, Beria never explicitly stated whether he had "initiated" Stalin's stroke or had merely delayed his treatment in the hope he would die (as argued by Sebag-Montefiore and consistent with evidence). Support for the assertion that Stalin was poisoned with warfarin by Beria's associates has been presented from several sources, including Edvard Radzinsky in his biography "Stalin" and a recent study by Miguel A. Faria in the journal Surgical Neurology International. Warfarin (4-Hydroxycoumarins) is cited as the likely agent; it would have produced the symptoms reported, and administering it into Stalin's food or drink was well within the operational abilities of Beria's NKVD. Sebag-Montefiore does not dispute the possibility of an assassination by poison masterminded by Beria, whose hatred for Stalin was palpable by this point, but also notes that Beria never made mention of poison or confessed to using it, even during his later interrogations, and was never alone with Stalin during the period prior to his stroke (he always went with Malenkov to defer suspicion).
After Stalin's death from pulmonary edema brought on by the stroke, Beria's ambitions sprang into full force. In the uneasy silence following the cessation of Stalin's last agonies, Beria was the first to dart forward to kiss his lifeless form (a move likened by Sebag-Montefiore to "wrenching a dead King's ring off his finger"). While the rest of Stalin's inner circle (even Molotov, saved from certain liquidation) stood sobbing unashamedly over the body, Beria reportedly appeared "radiant", "regenerated", and "glistening with ill-concealed relish." When Beria left the room, he broke the somber atmosphere by shouting loudly for his driver, his voice echoing with what Stalin's daughter Svetlana Alliluyeva called "the ring of triumph unconcealed." Alliluyeva noticed how the Politburo seemed openly frightened of Beria and unnerved by his bold display of ambition. "He's off to take power," Mikoyan recalled muttering to Khrushchev. That prompted a "frantic" dash for their own limousines to intercept him at the Kremlin.
Downfall.
After Stalin's death, Beria was appointed First Deputy Premier and reappointed head of the MVD, which he merged with the MGB. His close ally Malenkov was the new Prime Minister and initially the most powerful man in the post-Stalin leadership. Beria was second most powerful, and given Malenkov's personal weakness, was poised to become the power behind the throne and ultimately leader himself. Khrushchev became Party Secretary. Voroshilov became Chairman of the Presidium of the Supreme Soviet (i.e., the head of state).
Given his record, it is not surprising that the other Party leaders were suspicious of Beria's motives. Khrushchev opposed the alliance between Beria and Malenkov, but he was initially unable to challenge them. His opportunity came in June 1953 when a spontaneous uprising against the East German Communist regime broke out in East Berlin.
Based on Beria's own statements, other leaders suspected that in the wake of the uprising, he might be willing to trade the reunification of Germany and the end of the Cold War for massive aid from the United States, as had been received in World War II. The cost of the war still weighed heavily on the Soviet economy. Beria craved the vast financial resources that another (more sustained) relationship with the United States could provide. For example, Beria gave Estonia, Latvia and Lithuania serious prospects of national autonomy, possibly similarly to other Soviet satellite states in Europe.
The East German uprising convinced Molotov, Malenkov, and Nikolai Bulganin that Beria's policies were dangerous and destabilizing to Soviet power. Within days of the events in Germany, Khrushchev persuaded the other leaders to support a Party "coup" against Beria; Beria's principal ally Malenkov abandoned him.
Arrest, trial and execution.
On 26 June 1953, Beria was arrested and held in an undisclosed location near Moscow. Accounts of Beria's fall vary considerably. By the most likely account, Khrushchev prepared an elaborate ambush, convening a meeting of the Presidium on 26 June, where he suddenly launched a scathing attack on Beria, accusing him of being a traitor and spy in the pay of British intelligence. Beria was taken completely by surprise. He asked, "What's going on, Nikita Sergeyevich? Why are you picking fleas in my trousers?" Molotov and others quickly spoke against Beria one after the other, followed by a motion by Khrushchev for his instant dismissal. When Beria finally realized what was happening and plaintively appealed to Malenkov to speak for him, his old friend and crony silently hung his head and refused to meet his gaze. Malenkov pressed a button on his desk as the pre-arranged signal to Marshal Georgy Zhukov and a group of armed officers in a nearby room. They burst in and arrested Beria.
Beria was taken first to the Moscow guardhouse and then to the bunker of the headquarters of Moscow Military District. Defence Minister Nikolai Bulganin ordered the Kantemirovskaya Tank Division and Tamanskaya Motor Rifle Division to move into Moscow to prevent security forces loyal to Beria from rescuing him. Many of Beria's subordinates, proteges and associates were also arrested, among them Vsevolod Merkulov, Bogdan Kobulov, Sergey Goglidze, Vladimir Dekanozov, Pavel Meshik, and Lev Vlodzimirskiy. "Pravda" did not announce Beria's arrest until 10 July, crediting it to Malenkov and referring to Beria's "criminal activities against the Party and the State."
Beria and the others were tried by a special session ("Spetsialnoye Sudebnoye Prisutstvie") of the Supreme Court of the Soviet Union on 23 December 1953 with no defense counsel and no right of appeal. Marshal Ivan Konev was the chairman of the court.
Beria was found guilty of:
Beria and all the other defendants were sentenced to death on 23 December 1953. When the death sentence was passed, Beria pleaded on his knees for mercy before collapsing to the floor and wailing and crying energetically, but to no avail. The other six defendants were executed by firing squad on the same day the trial ended. Beria was executed separately. He was shot through the forehead by General Pavel Batitsky who had to stuff a rag into Beria's mouth to silence his bawling (his final moments bore great similarity to those of his own predecessor, NKVD Chief Nikolai Yezhov, who begged for his life before his execution in 1940). The body of Lavrentiy Pavlovich Beria was subsequently cremated. The remains were buried in a forest near Moscow.
Sexual predator.
At Beria's trial after his June 1953 arrest, a significant number of rapes by him were brought to light. Beria raped large numbers of young women, who were picked off the streets by his bodyguards and brought to his mansion, as well as of using threats and intimidation to extract sexual favors from the wives of Soviet officials.
According to open Soviet archives, he had committed "dozens" of sexual assaults during the years he was NKVD chief. Simon Sebag-Montefiore, a biographer of Stalin, concluded the information "reveals a sexual predator who used his power to indulge himself in obsessive depravity." The records contained the official testimony from Colonel R.S. Sarkisov and Colonel V. Nadaraia, two of Beria's most senior NKVD bodyguards. They stated that on warm nights during the war years, Beria was often driven slowly through the streets of Moscow in his armored Packard limousine. He would point out young women to be detained and escorted to his mansion where wine and a feast awaited them. After dining, Beria would take the women into his soundproofed office and rape them. Beria's bodyguards reported that their orders included handing each victim a flower bouquet as she left Beria's house, the implication being that to accept made it consensual; refusal would mean arrest. In one incident his chief bodyguard, Sarkisov, reported that a woman who had been brought to Beria rejected his advances and ran out of his office; Sarkisov mistakenly handed her the flowers anyway prompting the enraged Beria to declare "Now it's not a bouquet, it's a wreath! May it rot on your grave!" The woman was arrested by the NKVD the next day.
Women also submitted to Beria's sexual advances in exchange for the promise of freeing their relatives from the Gulag. In one case, Beria picked up Tatiana Okunevskaya – a well-known Soviet actress – under the pretence of bringing her to perform for the Politburo. Instead he took her to his dacha where he offered to free her father and grandmother from NKVD prison if she submitted. He then raped her telling her "scream or not, it doesn't matter." Yet Beria already knew her relatives had been executed months earlier. Okunevskaya was arrested shortly afterwards and sentenced to solitary confinement in the Gulag, which she survived.
Beria's sexually predatory nature was well-known to the Politburo, and though Stalin took an indulgent viewpoint (considering Beria's wartime importance), he said, "I don't trust Beria." In one instance when Stalin learned his daughter was alone with Beria at his house, he telephoned her and told her to leave immediately. When Beria complimented Alexander Poskrebyshev's daughter on her beauty, Poskrebyshev quickly pulled her aside and instructed her, "Don't ever accept a lift from Beria." After taking an interest in Marshal Kliment Voroshilov's daughter-in-law during a party at their summer dacha, Beria shadowed their car closely all the way back to the Kremlin, terrifying Voroshilov's wife.
Prior to and during the war, Beria directed Sarkisov to keep a running list of the names and phone numbers of his sexual encounters. Eventually he ordered Sarkisov to destroy the list because it was a security risk, but the colonel retained a secret handwritten copy. When Beria's fall from power began, Sarkisov passed the list to Viktor Abakumov, the former wartime head of SMERSH. He was now chief of the MGB - the successor to the NKVD - who was already aggressively building a case against Beria. Stalin, who was also seeking to undermine Beria, was thrilled by the detailed records kept by Sarkisov, demanding: "Send me everything this asshole writes down!" Sarkisov reported that Beria's sexual appetite had led to him contracting syphilis during the war for which he was secretly treated without the knowledge of Stalin or the Politburo (a fact Beria later admitted during his interrogation). Although the Russian government acknowledged Sarkisov's handwritten list of Beria's victims on 17 January 2003, the victims' names will not be released until 2028.
Evidence suggests that Beria not only abducted and raped women but some were also murdered. His villa in Moscow is now the Tunisian Embassy (at ). In the mid 1990s, routine work in the grounds turned up the bone remains of several young girls buried in the gardens. According to Martin Sixsmith, in a BBC documentary, "Beria spent his nights having teenagers abducted from the streets and brought here for him to rape. Those who resisted were strangled and buried in his wife's rose garden."
Sarkisov and Nadaria's testimony has been partially corroborated by Edward Ellis Smith, an American who served in the U.S. embassy in Moscow after the war. According to Knight, "Smith noted that Beria's escapades were common knowledge among embassy personnel because his house was on the same street as residence for Americans, and those who lived there saw girls brought to Beria's house late at night in a limousine."
Honours and awards.
Beria's awards were rescinded after his execution.
In popular culture.
Theater.
Beria is the central character in "Good Night, Uncle Joe" by Canadian playwright David Elendune. The play is a fictionalized account of the events leading up to Stalin's death.
Film and TV.
Georgian film director Tengiz Abuladze based the character of dictator Varlam Aravidze on Beria in his 1984 film "Repentance". Although banned in the Soviet Union for its semi-allegorical critique of Stalinism, it premiered at the 1987 Cannes Film Festival, winning the FIPRESCI Prize, Grand Prize of the Jury, and the Prize of the Ecumenical Jury.
British actor Bob Hoskins played Beria in the 1991 film "Inner Circle". He was portrayed by Roshan Seth in the 1992 film "Stalin" and, with an Irish accent, by David Suchet in "Red Monarch". Beria also appears in the third episode ("Superbomb") of the 4 part, 2007 BBC docudrama series "Nuclear Secrets", played by Boris Isarov. In the 2008 BBC documentary series "", Beria was portrayed by Polish actor Krzysztof Dracz.
Literature.
In the 1964 science fiction novel by Arkady and Boris Strugatsky, "Hard to Be a God", Beria is personified in the character Don Reba who serves as the king's minister of defense.
Beria is a significant character in the opening chapters of the 1998 novel "Archangel" by British novelist Robert Harris.
In 2012, his alleged personal diary from 1938 to 1953 was published in Russia.
In the 1973 novel "The Beria Papers" by journalist Alan Williams, Beria is depicted as a child rapist.

</doc>
<doc id="18391" url="https://en.wikipedia.org/wiki?curid=18391" title="Lyonel Feininger">
Lyonel Feininger

Lyonel Charles Feininger (July 17, 1871January 13, 1956) was a German-American painter, and a leading exponent of Expressionism. He also worked as a caricaturist and comic strip artist. He was born and grew up in New York City, traveling to Germany at 16 to study and perfect his art. He started his career as a cartoonist in 1894 and met with much success in this area. He was also a commercial caricaturist for 20 years for magazines and newspapers in the USA and Germany. At the age of 36, he started to work as a fine artist. He also produced a large body of photographic works between 1928 and the mid 1950s, but he kept these primarily within his circle of friends. He was also a pianist and composer, with several piano compositions and fugues for organ extant.
Life and work.
Lyonel Feininger was born to German-American violinist and composer Karl Feininger and American singer Elizabeth Feininger. He was born and grew up in New York City, but traveled to Germany at the age of 16 in 1887 to study. In 1888, he moved to Berlin and studied at the Königliche Akademie Berlin under Ernst Hancke. He continued his studies at art schools in Berlin with Karl Schlabitz, and in Paris with sculptor Filippo Colarossi. He started as a caricaturist for several magazines including "Harper's Round Table", "Harper's Young People", "Humoristische Blätter", "Lustige Blätter", "Das Narrenschiff", "Berliner Tageblatt" and "Ulk".
In 1900, he met Clara Fürst, daughter of the painter Gustav Fürst. He married her in 1901, and they had two daughters. In 1905, he separated from his wife after meeting Julia Berg. He married Berg in 1908 and had several children with her.
The artist was represented with drawings at the exhibitions of the annual Berlin Secession in the years 1901 through 1903.
Feininger's career as cartoonist started in 1894. He was working for several German, French and American magazines. In February 1906, when a quarter of Chicago's population was of German descent, James Keeley, editor of The "Chicago Tribune" traveled to Germany to procure the services of the most popular humor artists. He recruited Feininger to illustrate two comic strips "The Kin-der-Kids" and "Wee Willie Winkie's World" for the "Chicago Tribune". The strips were noted for their fey humor and graphic experimentation. He also worked as a commercial caricaturist for 20 years for various newspapers and magazines in both the USA and Germany. Later, Art Spiegelman wrote in "The New York Times Book Review," that Feininger's comics have “achieved a breathtaking formal grace unsurpassed in the history of the medium.”
Feininger started working as a fine artist at the age of 36. He was a member of the "Berliner Sezession" in 1909, and he was associated with German expressionist groups: Die Brücke, the Novembergruppe, Gruppe 1919, the Blaue Reiter circle and Die Blaue Vier (The Blue Four). His first solo exhibit was at Sturm Gallery in Berlin, 1917. When Walter Gropius founded the Bauhaus in Germany in 1919, Feininger was his first faculty appointment, and became the master artist in charge of the printmaking workshop.
From 1909 until 1921, Feininger spent summer vacations on the island of Usedom to recover and to get new inspiration. He continued to create paintings and drawings of Benz for the rest of his life, even after returning to live in the United States. A tour of the sites appearing in the works of Feininger follows a path with markers in the ground to guide visitors.
He designed the cover for the Bauhaus 1919 manifesto: an expressionist woodcut 'cathedral'. He taught at the Bauhaus for several years. Among the students who attended his workshops were Ludwig Hirschfeld Mack (German/Australian (1893–1965), Hans Friedrich Grohs (German 1892 - 1981), and Margarete Koehler-Bittkow (German/American, 1898–1964).
When the Nazi Party came to power in 1933, the situation became unbearable for Feininger and his wife. The Nazi Party declared his work to be "degenerate." They moved to America after his work was exhibited in the 'degenerate art' ("Entartete Kunst") in 1936, but before the 1937 exhibition in Munich. He taught at Mills College before returning to New York. He was elected to the American Academy of Arts and Letters in 1955.
In addition to drawing, Feininger created art with painted toy figures being photographed in front of drawn backgrounds.
Feininger produced a large body of photographic works between 1928 and the mid-1950s. He kept his photographic work within his circle of friends, and it was not shared with the public in his lifetime. He gave some prints away to his colleagues Walter Gropius and Alfred H. Barr, Jr..
Feininger also had intermittent activity as a pianist and composer, with several piano compositions and fugues for organ extant.
His sons, Andreas Feininger and T. Lux Feininger, both became noted artists, the former as a photographer and the latter as a photographer and painter. T. Lux Feininger died July 7, 2011 at the age of 101.
Major retrospective.
A major retrospective exhibition of Lyonel Feininger's work, initially at the Whitney Museum of American Art during June 30-October 16, 2011, was subsequently due to run at the Montreal Museum of Fine Arts during January 20–May 13, 2012. The exhibition is described as "the first in Feininger’s native country in more than forty-five years, and the first ever to include the full breadth of his art" and as "accompanied by a richly illustrated monograph with a feature essay that provides a broad overview of Feininger’s career..." Many critics have argued that the artist's work was at its most mature around 1910 in works in which the power of Feininger as illustrator balance his abstract side; however, we have to consider the possibility that Feininger used cubism as a more artistically succinct tool to establish his version of the concept known as the objective correlative.
Art market.
At a 2001 Christie's auction in London, Feininger's painting "The Green Bridge" (1909) was sold for £2.42 million.

</doc>
<doc id="18393" url="https://en.wikipedia.org/wiki?curid=18393" title="Life">
Life

Life is a characteristic distinguishing physical entities having biological processes, such as signaling and self-sustaining processes, from those that do not, either because such functions have ceased, or because they never had such functions and are classified as inanimate. Various forms of life exist such as plants, animals, fungi, protists, archaea, and bacteria. The criteria can at times be ambiguous and may or may not define viruses, viroids or potential artificial life as living. Biology is the primary science concerned with the study of life, although many other sciences are involved.
What defines life is very controversial. The current definition is that organisms maintain homeostasis, are composed of cells, undergo metabolism, can grow, adapt to their environment, respond to stimuli, and reproduce. However, many other biological definitions have been proposed, and there are also some borderline cases, such as viruses. Biophysicists have also proposed some definitions, many being based on chemical systems. There are also some living systems theories, such as the gaia hypothesis, the idea that the Earth is alive; the former first developed by James Grier Miller. Another one is that life is the property of ecological systems, and yet another is the complex systems biology, a branch or subfield of mathematical biology. Some other systemic definitions includes the theory involving the darwinian dynamic, and the operator theory. However, throughout history, there have been many other theories and definitions about life such as materialism, the belief that everything is made out of matter and that life is merely a complex form of it; hylomorphism, the belief that all things are a combination of matter and form, and the form of a living thing is its soul; spontaneous generation, the belief that life repeatedly emerge from non-life; and vitalism, a discredited scientific hypothesis that living organisms possess a "life force" or "vital spark". Abiogenesis is the natural process of life arising from non-living matter, such as simple organic compounds. Life on Earth arose 3.8-4.1 billion years ago. It is widely accepted that current life on Earth descended from a RNA world, but RNA based life may not have been the first. The mechanism by which life began on Earth is unknown, although many hypotheses have been formulated, most based on the Miller–Urey experiment.
Since appearing, life on Earth has changed its environment on a geologic time scale. To survive in most ecosystems, life can survive and thrive in a wide range of conditions. Some organisms, called extremophiles, can assume forms that enable them to thrive outside the ranges where life is commonly found. Properties common to all organisms are the need for certain core chemical elements needed for biochemical functioning. Aristotle was the first person to classify organisms. Later, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Fungi was later classified as its on kingdom. Eventually new groups of life were revealed, such as cells and microorganisms, and even non-cellular reproducing agents, such as viruses and viroids. Cells are the smallest units of life, often called the "building blocks of life". There are two kind of cells, prokaryotes and eukaryotes. Cells consist of cytoplasm enclosed within a membrane, which contains many biomolecules such as proteins and nucleic acids. Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells.
Though only confirmed on Earth, many believe in the existence of extraterrestrial life. Artificial life is a computer simulation of any aspect of life, which is used to examine systems related to life. Death is the permanent termination all biological functions that sustain an organism, and as such, the end of its life. Extinction is the process by which a group of taxa, normally a species, dies out. Fossils are the preserved remains or traces of organisms.
Definitions.
It is a challenge for scientists and philosophers to define life. This is partially because life is a process, not a substance. Any definition must be general enough to both encompass all known life and any unknown life that may be different from life on Earth.
Biology.
Since there is no unequivocal definition of life, the current definition in biology is descriptive. Life is considered a characteristic of something that exhibits all or most of the following traits:
These complex processes, called physiological functions, have underlying physical and chemical bases, as well as signaling and control mechanisms that are essential to maintaining life.
Alternative definitions.
At a higher level, living beings are thermodynamic systems with an organized molecular structure that can reproduce itself and evolve as survival dictates. Hence, life is a self-sustained chemical system capable of undergoing Darwinian evolution.
Others take a systemic viewpoint that does not necessarily depend on molecular chemistry. One systemic definition of life is that living things are self-organizing and autopoietic (self-producing). Variations of this definition include Stuart Kauffman's definition as an autonomous agent or a multi-agent system capable of reproducing itself or themselves, and of completing at least one thermodynamic work cycle.
Viruses.
Viruses are most often considered replicators rather than forms of life. They have been described as "organisms at the edge of life," since they possess genes, evolve by natural selection, and replicate by creating multiple copies of themselves through self-assembly. However, viruses do not metabolize and they require a host cell to make new products. Virus self-assembly within host cells has implications for the study of the origin of life, as it may support the hypothesis that life could have started as self-assembling organic molecules.
Biophysics.
To reflect the minimum phenomena required, other biological definitions of life have been proposed, with many of these being based upon chemical systems. Biophysicists have commented that living things function on negative entropy. In other words, living processes can be viewed as a delay of the spontaneous diffusion or dispersion of the internal energy of biological molecules towards more potential microstates. In more detail, according to physicists such as John Bernal, Erwin Schrödinger, Eugene Wigner, and John Avery, life is a member of the class of phenomena that are open or continuous systems able to decrease their internal entropy at the expense of substances or free energy taken in from the environment and subsequently rejected in a degraded form.
Living systems theories.
Gaia hypothesis.
The idea that the Earth is alive is found in philosophy and religion, but the first scientific discussion of it was by the Scottish scientist James Hutton. In 1785, he stated that the Earth was a superorganism and that its proper study should be physiology. Hutton is considered the father of geology, but his idea of a living Earth was forgotten in the intense reductionism of the 19th century. The Gaia hypothesis, proposed in the 1960s by scientist James Lovelock, suggests that life on Earth functions as a single organism that defines and maintains environmental conditions necessary for its survival.
Nonfractionability.
The first attempt at a general living systems theory for explaining the nature of life was in 1978, by American biologist James Grier Miller. Such a general theory, arising out of the ecological and biological sciences, attempts to map general principles for how all living systems work. Instead of examining phenomena by attempting to break things down into component parts, a general living systems theory explores phenomena in terms of dynamic patterns of the relationships of organisms with their environment. Robert Rosen (1991) built on this by defining a system component as "a unit of organization; a part with a function, i.e., a definite relation between part and whole." From this and other starting concepts, he developed a "relational theory of systems" that attempts to explain the special properties of life. Specifically, he identified the "nonfractionability of components in an organism" as the fundamental difference between living systems and "biological machines."
Life as a property of ecosystems.
A systems view of life treats environmental fluxes and biological fluxes together as a "reciprocity of influence", and a reciprocal relation with environment is arguably as important for understanding life as it is for understanding ecosystems. As Harold J. Morowitz (1992) explains it, life is a property of an ecological system rather than a single organism or species. He argues that an ecosystemic definition of life is preferable to a strictly biochemical or physical one. Robert Ulanowicz (2009) highlights mutualism as the key to understand the systemic, order-generating behavior of life and ecosystems.
Complex systems biology.
Complex systems biology (CSB) is a field of science that studies the emergence of complexity in functional organisms from the viewpoint of dynamic systems theory. The latter is often called also systems biology and aims to understand the most fundamental aspects of life. A closely related approach to CSB and systems biology, called relational biology, is concerned mainly with understanding life processes in terms of the most important relations, and categories of such relations among the essential functional components of organisms; for multicellular organisms, this has been defined as "categorical biology", or a model representation of organisms as a category theory of biological relations, and also an algebraic topology of the functional organization of living organisms in terms of their dynamic, complex networks of metabolic, genetic, epigenetic processes and signaling pathways.
Darwinian dynamic.
It has also been argued that the evolution of order in living systems and certain physical systems obey a common fundamental principle termed the Darwinian dynamic. The Darwinian dynamic was formulated by first considering how macroscopic order is generated in a simple non-biological system far from thermodynamic equilibrium, and then extending consideration to short, replicating RNA molecules. The underlying order generating process for both types of system was concluded to be basically similar.
Operator theory.
Another systemic definition, called the Operator theory, proposes that 'life is a general term for the presence of the typical closures found in organisms; the typical closures are a membrane and an autocatalytic set in the cell', and also proposes that an organism is 'any system with an organisation that complies with an operator type that is at least as complex as the cell. Life can also be modeled as a network of inferior negative feedbacks of regulatory mechanisms subordinated to a superior positive feedback formed by the potential of expansion and reproduction.
History.
Materialism.
Some of the earliest theories of life were materialist, holding that all that exists is matter, and that life is merely a complex form or arrangement of matter. Empedocles (430 BC) argued that everything in the universe is made up of a combination of four eternal "elements" or "roots of all": earth, water, air, and fire. All change is explained by the arrangement and rearrangement of these four elements. The various forms of life are caused by an appropriate mixture of elements.
Democritus (460 BC) thought that the essential characteristic of life is having a soul ("psyche"). Like other ancient writers, he was attempting to explain what makes something a "living" thing. His explanation was that fiery atoms make a soul in exactly the same way atoms and void account for any other thing. He elaborates on fire because of the apparent connection between life and heat, and because fire moves.
The mechanistic materialism that originated in ancient Greece was revived and revised by the French philosopher René Descartes, who held that animals and humans were assemblages of parts that together functioned as a machine. In the 19th century, the advances in cell theory in biological science encouraged this view. The evolutionary theory of Charles Darwin (1859) is a mechanistic explanation for the origin of species by means of natural selection.
Hylomorphism.
Hylomorphism is a theory, originating with Aristotle (322 BC), that all things are a combination of matter and form. Biology was one of his main interests, and there is extensive biological material in his extant writings. In this view, all things in the material universe have both matter and form, and the form of a living thing is its soul (Greek "psyche", Latin "anima"). There are three kinds of souls: the "vegetative soul" of plants, which causes them to grow and decay and nourish themselves, but does not cause motion and sensation; the "animal soul", which causes animals to move and feel; and the "rational soul", which is the source of consciousness and reasoning, which (Aristotle believed) is found only in man. Each higher soul has all the attributes of the lower one. Aristotle believed that while matter can exist without form, form cannot exist without matter, and therefore the soul cannot exist without the body.
This account is consistent with teleological explanations of life, which account for phenomena in terms of purpose or goal-directedness. Thus, the whiteness of the polar bear's coat is explained by its purpose of camouflage. The direction of causality (from the future to the past) is in contradiction with the scientific evidence for natural selection, which explains the consequence in terms of a prior cause. Biological features are explained not by looking at future optimal results, but by looking at the past evolutionary history of a species, which led to the natural selection of the features in question.
Spontaneous generation.
Spontaneous generation was the belief on the ordinary formation of living organisms without descent from similar organisms. Typically, the idea was that certain forms such as fleas could arise from inanimate matter such as dust or the supposed seasonal generation of mice and insects from mud or garbage. 
The theory of spontaneous generation was proposed by Aristotle, who compiled and expanded the work of prior natural philosophers and the various ancient explanations of the appearance of organisms; it held sway for two millennia. It was decisively dispelled by the experiments of Louis Pasteur in 1859, who expanded upon the investigations of predecessors (such as Francesco Redi. Disproof of the traditional ideas of spontaneous generation is no longer controversial among biologists.
Vitalism.
Vitalism is the belief that the life-principle is non-material. This originated with Georg Ernst Stahl (17th century), and remained popular until the middle of the 19th century. It appealed to philosophers such as Henri Bergson, Friedrich Nietzsche, Wilhelm Dilthey, anatomists like Marie François Xavier Bichat, and chemists like Justus von Liebig. Vitalism included the idea that there was a fundamental difference between organic and inorganic material, and the belief that organic material can only be derived from living things. This was disproved in 1828, when Friedrich Wöhler prepared urea from inorganic materials. This Wöhler synthesis is considered the starting point of modern organic chemistry. It is of historical significance because for the first time an organic compound was produced in inorganic reactions.
During the 1850s, Hermann von Helmholtz, anticipated by Julius Robert von Mayer, demonstrated that no energy is lost in muscle movement, suggesting that there were no "vital forces" necessary to move a muscle. These results led to the abandonment of scientific interest in vitalistic theories, although the belief lingered on in pseudoscientific theories such as homeopathy, which interprets diseases and sickness as caused by disturbances in a hypothetical vital force or life force.
Origin.
The age of the Earth is about 4.54 billion years. Evidence suggests that life on Earth has existed for at least 3.5 billion years, with the oldest physical traces of life dating back 3.7 billion years; however, some theories, such as the Late Heavy Bombardment theory, suggest that life on Earth may have started even earlier, as early as 4.1-4.4 billion years ago, but the chemistry leading to life may have begun shortly after the Big Bang, 13.8 billion years ago, during an epoch when the universe was only 10–17 million years old. All known life forms share fundamental molecular mechanisms, reflecting their common descent; based on these observations, hypotheses on the origin of life attempt to find a mechanism explaining the formation of a universal common ancestor, from simple organic molecules via pre-cellular life to protocells and metabolism. Models have been divided into "genes-first" and "metabolism-first" categories, but a recent trend is the emergence of hybrid models that combine both categories.
There is no current scientific consensus as to how life originated. However, most accepted scientific models build on the Miller–Urey experiment, and the work of Sidney Fox, which shows that conditions on the primitive Earth favored chemical reactions that synthesize amino acids and other organic compounds from inorganic precursors, and phospholipids spontaneously forming lipid bilayers, the basic structure of a cell membrane.
Living organisms synthesize proteins, which are polymers of amino acids using instructions encoded by deoxyribonucleic acid (DNA). Protein synthesis entails intermediary ribonucleic acid (RNA) polymers. One possibility for how life began is that genes originated first, followed by proteins; the alternative being that proteins came first and then genes.
However, since genes and proteins are both required to produce the other, the problem of considering which came first is like that of the chicken or the egg. Most scientists have adopted the hypothesis that because of this, it is unlikely that genes and proteins arose independently.
Therefore, a possibility, first suggested by Francis Crick, is that the first life was based on RNA, which has the DNA-like properties of information storage and the catalytic properties of some proteins. This is called the RNA world hypothesis, and it is supported by the observation that many of the most critical components of cells (those that evolve the slowest) are composed mostly or entirely of RNA. Also, many critical cofactors (ATP, Acetyl-CoA, NADH, etc.) are either nucleotides or substances clearly related to them. The catalytic properties of RNA had not yet been demonstrated when the hypothesis was first proposed, but they were confirmed by Thomas Cech in 1986.
One issue with the RNA world hypothesis is that synthesis of RNA from simple inorganic precursors is more difficult than for other organic molecules. One reason for this is that RNA precursors are very stable and react with each other very slowly under ambient conditions, and it has also been proposed that living organisms consisted of other molecules before RNA. However, the successful synthesis of certain RNA molecules under the conditions that existed prior to life on Earth has been achieved by adding alternative precursors in a specified order with the precursor phosphate present throughout the reaction. This study makes the RNA world hypothesis more plausible.
Geological findings in 2013 showed that reactive phosphorus species (like phosphite) were in abundance in the ocean before 3.5 Ga, and that Schreibersite easily reacts with aqueous glycerol to generate phosphite and glycerol 3-phosphate. It is hypothesized that Schreibersite-containing meteorites from the Late Heavy Bombardment could have provided early reduced phosphorus, which could react with prebiotic organic molecules to form phosphorylated biomolecules, like RNA.
In 2009, experiments demonstrated Darwinian evolution of a two-component system of RNA enzymes (ribozymes) "in vitro". The work was performed in the laboratory of Gerald Joyce, who stated, "This is the first example, outside of biology, of evolutionary adaptation in a molecular genetic system."
Prebiotic compounds may have extraterrestrial origin. NASA findings in 2011, based on studies with meteorites found on Earth, suggest DNA and RNA components (adenine, guanine and related organic molecules) may be formed in outer space.
In March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.
According to the panspermia hypothesis, microscopic life—distributed by meteoroids, asteroids and other small Solar System bodies—may exist throughout the universe.
Environmental conditions.
The diversity of life on Earth is a result of the dynamic interplay between genetic opportunity, metabolic capability, environmental challenges, and symbiosis. For most of its existence, Earth's habitable environment has been dominated by microorganisms and subjected to their metabolism and evolution. As a consequence of these microbial activities, the physical-chemical environment on Earth has been changing on a geologic time scale, thereby affecting the path of evolution of subsequent life. For example, the release of molecular oxygen by cyanobacteria as a by-product of photosynthesis induced global changes in the Earth's environment. Since oxygen was toxic to most life on Earth at the time, this posed novel evolutionary challenges, and ultimately resulted in the formation of Earth's major animal and plant species. This interplay between organisms and their environment is an inherent feature of living systems.
Range of tolerance.
The inert components of an ecosystem are the physical and chemical factors necessary for life — energy (sunlight or chemical energy), water, temperature, atmosphere, gravity, nutrients, and ultraviolet solar radiation protection. In most ecosystems, the conditions vary during the day and from one season to the next. To live in most ecosystems, then, organisms must be able to survive a range of conditions, called the "range of tolerance". Outside that are the "zones of physiological stress", where the survival and reproduction are possible but not optimal. Beyond these zones are the "zones of intolerance", where survival and reproduction of that organism is unlikely or impossible. Organisms that have a wide range of tolerance are more widely distributed than organisms with a narrow range of tolerance.
Extremophiles.
To survive, selected microorganisms can assume forms that enable them to withstand freezing, complete desiccation, starvation, high levels of radiation exposure, and other physical or chemical challenges. These microorganisms may survive exposure to such conditions for weeks, months, years, or even centuries. Extremophiles are microbial life forms that thrive outside the ranges where life is commonly found. They excel at exploiting uncommon sources of energy. While all organisms are composed of nearly identical molecules, evolution has enabled such microbes to cope with this wide range of physical and chemical conditions. Characterization of the structure and metabolic diversity of microbial communities in such extreme environments is ongoing.
Microbial life forms thrive even in the Mariana Trench, the deepest spot on the Earth. Microbes also thrive inside rocks up to 1900 feet below the sea floor under 8500 feet of ocean.
Investigation of the tenacity and versatility of life on Earth, as well as an understanding of the molecular systems that some organisms utilize to survive such extremes, is important for the search for life beyond Earth. For example, lichen could survive for a month in a simulated Martian environment.
Chemical elements.
All life forms require certain core chemical elements needed for biochemical functioning. These include carbon, hydrogen, nitrogen, oxygen, phosphorus, and sulfur—the elemental macronutrients for all organisms—often represented by the acronym CHNOPS. Together these make up nucleic acids, proteins and lipids, the bulk of living matter. Five of these six elements comprise the chemical components of DNA, the exception being sulfur. The latter is a component of the amino acids cysteine and methionine. The most biologically abundant of these elements is carbon, which has the desirable attribute of forming multiple, stable covalent bonds. This allows carbon-based (organic) molecules to form an immense variety of chemical arrangements. Alternative hypothetical types of biochemistry have been proposed that eliminate one or more of these elements, swap out an element for one not on the list, or change required chiralities or other chemical properties.
Classification.
Life is usually classified by eight levels of taxa—domains, kingdoms, phyla, class, order, family, genus, and species. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented.
The first known attempt to classify organisms was conducted by the Greek philosopher Aristotle (384–322 BC), who classified all living organisms known at that time as either a plant or an animal, based mainly on their ability to move. He also distinguished animals with blood from animals without blood (or at least without red blood), which can be compared with the concepts of vertebrates and invertebrates respectively, and divided the blooded animals into five groups: viviparous quadrupeds (mammals), oviparous quadrupeds (reptiles and amphibians), birds, fishes and whales. The bloodless animals were also divided into five groups: cephalopods, crustaceans, insects (which included the spiders, scorpions, and centipedes, in addition to what we define as insects today), shelled animals (such as most molluscs and echinoderms) and "zoophytes". Though Aristotle's work in zoology was not without errors, it was the grandest biological synthesis of the time and remained the ultimate authority for many centuries after his death.
The exploration of the American continent revealed large numbers of new plants and animals that needed descriptions and classification. In the latter part of the 16th century and the beginning of the 17th, careful study of animals commenced and was gradually extended until it formed a sufficient body of knowledge to serve as an anatomical basis for classification. In the late 1740s, Carl Linnaeus introduced his system of binomial nomenclature for the classification of species. Linnaeus attempted to improve the composition and reduce the length of the previously used many-worded names by abolishing unnecessary rhetoric, introducing new descriptive terms and precisely defining their meaning.
The fungi were originally treated as plants. For a short period Linnaeus had classified them in the taxon Vermes in Animalia, but later placed them back in Plantae. Copeland classified the Fungi in his Protoctista, thus partially avoiding the problem but acknowledging their special status. The problem was eventually solved by Whittaker, when he gave them their own kingdom in his five-kingdom system. Evolutionary history shows that the fungi are more closely related to animals than to plants.
As new discoveries enabled detailed study of cells and microorganisms, new groups of life were revealed, and the fields of cell biology and microbiology were created. These new organisms were originally described separately in protozoa as animals and protophyta/thallophyta as plants, but were united by Haeckel in the kingdom Protista; later, the prokaryotes were split off in the kingdom Monera, which would eventually be divided into two separate groups, the Bacteria and the Archaea. This led to the six-kingdom system and eventually to the current three-domain system, which is based on evolutionary relationships. However, the classification of eukaryotes, especially of protists, is still controversial.
As microbiology, molecular biology and virology developed, non-cellular reproducing agents were discovered, such as viruses and viroids. Whether these are considered alive has been a matter of debate; viruses lack characteristics of life such as cell membranes, metabolism and the ability to grow or respond to their environments. Viruses can still be classed into "species" based on their biology and genetics, but many aspects of such a classification remain controversial.
In the 1960s a trend called cladistics emerged, arranging taxa based on clades in an evolutionary or phylogenetic tree.
Cells.
Cells are the basic unit of structure in every living thing, and all cells arise from pre-existing cells by division. Cell theory was formulated by Henri Dutrochet, Theodor Schwann, Rudolf Virchow and others during the early nineteenth century, and subsequently became widely accepted. The activity of an organism depends on the total activity of its cells, with energy flow occurring within and between them. Cells contain hereditary information that is carried forward as a genetic code during cell division.
There are two primary types of cells. Prokaryotes lack a nucleus and other membrane-bound organelles, although they have circular DNA and ribosomes. Bacteria and Archaea are two domains of prokaryotes. The other primary type of cells are the eukaryotes, which have distinct nuclei bound by a nuclear membrane and membrane-bound organelles, including mitochondria, chloroplasts, lysosomes, rough and smooth endoplasmic reticulum, and vacuoles. In addition, they possess organized chromosomes that store genetic material. All species of large complex organisms are eukaryotes, including animals, plants and fungi, though most species of eukaryote are protist microorganisms. The conventional model is that eukaryotes evolved from prokaryotes, with the main organelles of the eukaryotes forming through endosymbiosis between bacteria and the progenitor eukaryotic cell.
The molecular mechanisms of cell biology are based on proteins. Most of these are synthesized by the ribosomes through an enzyme-catalyzed process called protein biosynthesis. A sequence of amino acids is assembled and joined together based upon gene expression of the cell's nucleic acid. In eukaryotic cells, these proteins may then be transported and processed through the Golgi apparatus in preparation for dispatch to their destination.
Cells reproduce through a process of cell division in which the parent cell divides into two or more daughter cells. For prokaryotes, cell division occurs through a process of fission in which the DNA is replicated, then the two copies are attached to parts of the cell membrane. In eukaryotes, a more complex process of mitosis is followed. However, the end result is the same; the resulting cell copies are identical to each other and to the original cell (except for mutations), and both are capable of further division following an interphase period.
Multicellular organisms may have first evolved through the formation of colonies like cells. These cells can form group organisms through cell adhesion. The individual members of a colony are capable of surviving on their own, whereas the members of a true multi-cellular organism have developed specializations, making them dependent on the remainder of the organism for survival. Such organisms are formed clonally or from a single germ cell that is capable of forming the various specialized cells that form the adult organism. This specialization allows multicellular organisms to exploit resources more efficiently than single cells. In January 2016, scientists reported that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.
Cells have evolved methods to perceive and respond to their microenvironment, thereby enhancing their adaptability. Cell signaling coordinates cellular activities, and hence governs the basic functions of multicellular organisms. Signaling between cells can occur through direct cell contact using juxtacrine signalling, or indirectly through the exchange of agents as in the endocrine system. In more complex organisms, coordination of activities can occur through a dedicated nervous system.
Extraterrestrial life.
Though life is confirmed only on the Earth, many think that extraterrestrial life is not only plausible, but probable or inevitable. Other planets and moons in the Solar System and other planetary systems are being examined for evidence of having once supported simple life, and projects such as SETI are trying to detect radio transmissions from possible alien civilizations. Other locations within the Solar System that may host microbial life include the subsurface of Mars, the upper atmosphere of Venus, and subsurface oceans on some of the moons of the giant planets. 
Beyond the Solar System, the region around another main-sequence star that could support Earth-like life on an Earth-like planet is known as the habitable zone. The inner and outer radii of this zone vary with the luminosity of the star, as does the time interval during which the zone survives. Stars more massive than the Sun have a larger habitable zone, but remain on the main sequence for a shorter time interval. Small red dwarfs have the opposite problem, with a smaller habitable zone that is subject to higher levels of magnetic activity and the effects of tidal locking from close orbits. Hence, stars in the intermediate mass range such as the Sun may have a greater likelihood for Earth-like life to develop. The location of the star within a galaxy may also have an impact on the likelihood of life forming. Stars in regions with a greater abundance of heavier elements that can form planets, in combination with a low rate of potentially habitat-damaging supernova events, are predicted to have a higher probability of hosting planets with complex life. The variables of the Drake equation are used to discuss the conditions in planetary systems where civilization is most likely to exist. This suggest that life could also form on other planets.
Artificial life.
Artificial life is a field of study that examines systems related to life, its processes, and its evolution through simulations using computer models, robotics, and biochemistry. The study of artificial life imitates traditional biology by recreating some aspects of biological phenomena. Scientists study the logic of living systems by creating artificial environments—seeking to understand the complex information processing that defines such systems. While life is, by definition, alive, artificial life is generally referred to as data confined to a digital environment and existence.
Synthetic biology is a new area of biotechnology that combines science and biological engineering. The common goal is the design and construction of new biological functions and systems not found in nature. Synthetic biology includes the broad redefinition and expansion of biotechnology, with the ultimate goals of being able to design and build engineered biological systems that process information, manipulate chemicals, fabricate materials and structures, produce energy, provide food, and maintain and enhance human health and the environment.
Death.
Death is the permanent termination of all vital functions or life processes in an organism or cell. It can occur as a result of an accident, medical conditions, biological interaction, malnutrition, poisoning, senescence, or suicide. After death, the remains of an organism re-enter the biogeochemical cycle. Organisms may be consumed by a predator or a scavenger and leftover organic material may then be further decomposed by detritivores, organisms that recycle detritus, returning it to the environment for reuse in the food chain.
One of the challenges in defining death is in distinguishing it from life. Death would seem to refer to either the moment life ends, or when the state that follows life begins. However, determining when death has occurred requires drawing precise conceptual boundaries between life and death. This is problematic, however, because there is little consensus over how to define life. The nature of death has for millennia been a central concern of the world's religious traditions and of philosophical inquiry. Many religions maintain faith in either a kind of afterlife or reincarnation for the soul, or resurrection of the body at a later date.
Extinction.
Extinction is the process by which a group of taxa or species dies out, reducing biodiversity. The moment of extinction is generally considered the death of the last individual of that species. Because a species' potential range may be very large, determining this moment is difficult, and is usually done retrospectively after a period of apparent absence. Species become extinct when they are no longer able to survive in changing habitat or against superior competition. In Earth's history, over 99% of all the species that have ever lived have gone extinct; however, mass extinctions may have accelerated evolution by providing opportunities for new groups of organisms to diversify.
Fossils.
Fossils are the preserved remains or traces of animals, plants, and other organisms from the remote past. The totality of fossils, both discovered and undiscovered, and their placement in fossil-containing rock formations and sedimentary layers (strata) is known as the "fossil record". A preserved specimen is called a fossil if it is older than the arbitrary date of 10,000 years ago. Hence, fossils range in age from the youngest at the start of the Holocene Epoch to the oldest from the Archaean Eon, up to 3.4 billion years old.

</doc>
<doc id="18398" url="https://en.wikipedia.org/wiki?curid=18398" title="La Espero">
La Espero

"La Espero" ("The Hope") is a poem written by Polish-Jewish oculist and doctor L. L. Zamenhof (1859–1917), the initiator of the Esperanto language. The song is often used as the anthem of Esperanto, and is now usually sung to a triumphal march composed by Félicien Menu de Ménil in 1909 (although there is an earlier, less martial tune created in 1891 by Claes Adelsköld, as well as a number of others less well-known). It is sometimes referred to as the hymn of the Esperanto movement.
Some Esperantists object to the use of terms like "hymn" or "anthem" for "La Espero", arguing that these terms have religious and nationalist overtones respectively. 

</doc>
<doc id="18400" url="https://en.wikipedia.org/wiki?curid=18400" title="Loonie">
Loonie

The Canadian one dollar coin, commonly called the loonie, is a gold-coloured one-dollar coin introduced in 1987. It bears images of a common loon, a bird which is common and well known in Canada, on the reverse, and of Queen Elizabeth II on the obverse. It is produced by the Royal Canadian Mint at its facility in Winnipeg.
The coin's outline is an 11-sided curve of constant width. Its diameter of 26.5 mm and its 11-sidedness matched that of the already-circulating Susan B. Anthony dollar in the United States, and its thickness of 1.95 mm was a close match to the latter's 2.0 mm. Its gold colour differed from the silver-coloured Anthony dollar; however, the succeeding Sacagawea and Presidential dollars matched the loonie's overall hue. Other coins using a curve of constant width include the 7-sided British twenty pence and fifty pence coins (the latter of which has similar size and value to the loonie, but is silver in colour).
The coin has become the symbol of the Canadian dollar: media often discuss the rate at which the "loonie" is trading against other currencies. The nickname "loonie" ( in French) became so widely recognized that in 2006 the Royal Canadian Mint secured the rights to it. When the Canadian two-dollar coin was introduced in 1996, it was in turn nicknamed the "toonie" (a portmanteau of "two" and "loonie").
Background.
Canada first minted a silver dollar coin in 1935 to celebrate the 25th anniversary of George V's reign as king. The voyageur dollar, so named because it featured an Indian and a French voyageur paddling a canoe on the reverse, was minted in silver until 1967, after which it was composed primarily of nickel. The coins did not see wide circulation, mainly due to their size and weight; the nickel version weighed and was in diameter, and was itself smaller than the silver version.
By 1982, the Royal Canadian Mint had begun work on a new composition for the dollar coin that it hoped would lead to increased circulation. At the same time, vending machine operators and transit systems were lobbying the Government of Canada to replace the dollar banknotes with wider circulating coins. A Commons committee recommended in 1985 that the dollar bill be eliminated despite a lack of evidence that Canadians would support the move. The government argued that it would save between $175 and $250 million over 20 years by switching from bills that had a lifespan of less than a year to coins that would last two decades.
Introduction.
The government announced on March 25, 1986, that the new dollar coin would be launched the following year as a replacement for the dollar bill, which would be phased out. It was expected to cost $31.8 million to produce the first 300 million coins, but through seigniorage (the difference between the cost of production and the coin's value), expected to make up to $40 million a year on the coins. From the proceeds, a total of $60 million over five years was dedicated toward funding the 1988 Winter Olympics in Calgary.
The failure of the Susan B. Anthony dollar coin in the United States had been considered and it was believed Americans refused to support the coin due to its similarity to their quarter coin and its lack of esthetic appeal. In announcing the new Canadian dollar coin, the government stated it would be the same overall size as the Susan B. Anthony coin – slightly larger than a quarter – to allow for compatibility with American manufactured vending machines, but would be eleven-sided and gold-coloured.
It was planned that the coin would continue using the voyageur theme of its predecessor, but the master dies that had been struck in Ottawa were lost in transit en route to the Mint's facility at Winnipeg. A Commons committee struck to investigate the loss discovered that the Mint had no documented procedures for transport of master dies and that it had shipped them via a local courier in a bid to save $43.50. It was also found to be the third time that the Mint had lost master dies within five years. An internal review by the Royal Canadian Mint argued that while a policy existed to ship the obverse and reverse dies separately, the new coin dies were packaged separately but were part of the same shipment. The Mint also disagreed with the Royal Canadian Mounted Police's contention that the dies were simply lost in transit, believing instead that they were stolen. The dies were never recovered.
Fearing the possibility of counterfeiting, the government approved a new design for the reverse, replacing the voyageur with a Robert-Ralph Carmichael design of a common loon floating in water. The coin was immediately nicknamed the "loonie" across English Canada, and became known as a "huard", French for "loon", in Quebec. The loonie entered circulation on June 30, 1987, as 40 million coins were introduced into major cities across the country. Over 800 million loonies had been struck by the coin's 20th anniversary.
Two years after the loonie's introduction, the Bank of Canada ceased production of the dollar banknote. The final dollar bills were printed on June 30, 1989. Initial support for the coin was mixed, but withdrawing the banknote forced acceptance of the coin.
The loonie has subsequently gained iconic status within Canada, and is now regarded as a national symbol. The term "loonie" has since become synonymous with the Canadian dollar itself. The town of Echo Bay, Ontario, home of Robert-Ralph Carmichael, erected a large loonie monument in his honour in 1992 along the highway, similar to Sudbury's 'Big Nickel'.
Lucky loonie.
Officials for the 2002 Salt Lake Winter Olympics invited the National Hockey League's ice making consultant, Dan Craig, to oversee the city's E Center arena, where the ice hockey tournament was being held. Craig invited a couple of members from the ice crew in his hometown of Edmonton to assist. One of them, Trent Evans, secretly placed a loonie at centre ice. He originally placed a dime, but added the loonie after the smaller coin quickly vanished as the ice surface was built up. He placed the coins after realizing there was no target at centre ice for referees to aim for when dropping the puck for a faceoff. A thin yellow dot was painted on the ice surface over the coins, though the loonie was barely visible to those who knew to look for it.
Keeping the coin a secret, Evans told only a few people of its placement and swore them to secrecy. Among those told were the players of the men's and women's teams. Both Canadian teams went on to win gold medals. Several members of the women's team kissed the spot where the coin was buried following their victory. After the men won their final, the coin was dug up and given to Wayne Gretzky, the team's executive-director, who revealed the existence of the "lucky loonie" at a post-game press conference.
The lucky loonie quickly became a piece of Canadian lore. The original lucky loonie was donated to the Hockey Hall of Fame, and Canadians have subsequently hidden loonies at several international competitions. Loonies were buried in the foundations of facilities built for the 2010 Winter Olympics in Vancouver.
Capitalizing on the tradition, the Royal Canadian Mint has released a commemorative edition "lucky loonie" for each Olympic Games since 2004.
Composition.
The weight of the coin was originally specified as 108 grains, equivalent to 6.998 grams.
When introduced, loonie coins were made of Aureate, a bronze-electroplated nickel combination. Beginning in 2007, some loonie blanks also began to be produced with a cyanide-free brass plating process. In the spring of 2012, the composition switched to multi-ply brass-plated steel. As a result, the weight dropped from 7.00 to 6.27 grams. This has resulted in the 2012 loonie not being accepted in some vending machines. The Toronto Parking Authority estimates that at about $345 per machine, it will cost about $1 million to upgrade almost 3,000 machines to accept the new coins. The Mint states that multi-ply plated steel technology, already used in Canada's smaller coinage, produces an electromagnetic signature that is harder to counterfeit than that for regular alloy coins; also, using steel provides cost savings and avoids fluctuations in price or supply of nickel.
On April 10, 2012, the Royal Canadian Mint announced design changes to the loonie and toonie, which include new security features.
Commemorative editions.
The design has been changed several times for commemorative editions:

</doc>
<doc id="18401" url="https://en.wikipedia.org/wiki?curid=18401" title="Laminar flow">
Laminar flow

In fluid dynamics, laminar flow (or streamline flow) occurs when a fluid flows in parallel layers, with no disruption between the layers. At low velocities, the fluid tends to flow without lateral mixing, and adjacent layers slide past one another like playing cards. There are no cross-currents perpendicular to the direction of flow, nor eddies or swirls of fluids. In laminar flow, the motion of the particles of the fluid is very orderly with all particles moving in straight lines parallel to the pipe walls.
Laminar flow is a flow regime characterized by high momentum diffusion and low momentum convection.
When a fluid is flowing through a closed channel such as a pipe or between two flat plates, either of two types of flow may occur depending on the velocity and viscosity of the fluid: laminar flow or turbulent flow. Laminar flow tends to occur at lower velocities, below a threshold at which it becomes turbulent. Turbulent flow is a less orderly flow regime that is characterised by eddies or small packets of fluid particles which result in lateral mixing. In non-scientific terms, laminar flow is "smooth" while turbulent flow is "rough".
Relationship with the Reynolds number.
The type of flow occurring in a fluid in a channel is important in fluid dynamics problems and subsequently affects heat and mass transfer in fluid systems. The dimensionless Reynolds number is an important parameter in the equations that describe whether fully developed flow conditions lead to laminar or turbulent flow. The Reynolds number is the ratio of the inertial force to the shearing force of the fluid—how fast the fluid is moving relative to how viscous the fluid is, irrespective of the scale of the fluid system. Laminar flow generally occurs when the fluid is moving slowly or the fluid is very viscous. As the Reynolds number increases, such as by increasing the flow rate of the fluid, the flow will transition from laminar to turbulent flow at a specific range of Reynolds numbers, the laminar-turbulent transition range depending on small disturbance levels in the fluid or imperfections in the flow system. If the Reynolds number is very small, much less than 1, then the fluid will exhibit Stokes or creeping flow, where the viscous forces of the fluid dominate the inertial forces.
The specific calculation of the Reynolds number, and the values where laminar flow occurs, will depend on the geometry of the flow system and flow pattern. The common example is flow through a pipe, where the Reynolds number is defined as:

</doc>
<doc id="18402" url="https://en.wikipedia.org/wiki?curid=18402" title="Luanda">
Luanda

Luanda, formerly named São Paulo da Assunção de Loanda, is the capital and largest city in Angola, and the country's most populous and important city, primary port and major industrial, cultural and urban centre. Located on Angola's coast with the Atlantic Ocean, Luanda is both Angola's chief seaport and its administrative centre. It has a metropolitan population of over 6 million. It is also the capital city of Luanda Province, and the world's third most populous Portuguese-speaking city, behind only São Paulo and Rio de Janeiro, both in Brazil, and the most populous Portuguese-speaking capital city in the world, ahead of Brasília, Maputo and Lisbon.
The city is currently undergoing a major reconstruction, with many large developments taking place that will alter the cityscape significantly.
History.
Portuguese rule.
Portuguese explorer Paulo Dias de Novais founded Luanda on 25 January 1576 as "São Paulo da Assumpção de Loanda", with one hundred families of settlers and four hundred soldiers. In 1618, the Portuguese built the fortress called Fortaleza São Pedro da Barra, and they subsequently built two more: Fortaleza de São Miguel (1634) and Forte de São Francisco do Penedo (1765-6). Of these, the Fortaleza de São Miguel is the best preserved.
Luanda was Portugal's bridgehead from 1627, except during the Dutch rule of Luanda, from 1640 to 1648, as Fort Aardenburgh. The city served as the centre of slave trade to Brazil from circa 1550 to 1836. The slave trade was conducted mostly with the Portuguese colony of Brazil; Brazilian ships were the most numerous in the port of Luanda. This slave trade also involved local merchants and warriors who profited from the trade. During this period, no large scale territorial conquest was intended by the Portuguese; only a few minor settlements were established in the immediate hinterland of Luanda, some on the last stretch of the Kwanza River.
In the 17th century, the Imbangala became the main rivals of the Mbundu in supplying slaves to the Luanda market. In the 1750s, between 5,000 and 10,000 slaves were annually sold. By this time, Angola, a Portuguese colony, was in fact like a colony of Brazil, paradoxically another Portuguese colony. A strong degree of Brazilian influence was noted in Luanda until the Independence of Brazil in 1822. In the 19th century, still under Portuguese rule, Luanda experienced a major economic revolution. The slave trade was abolished in 1836, and in 1844, Angola's ports were opened to foreign shipping. By 1850, Luanda was one of the greatest and most developed Portuguese cities in the vast Portuguese Empire outside Continental Portugal, full of trading companies, exporting (together with Benguela) palm and peanut oil, wax, copal, timber, ivory, cotton, coffee, and cocoa, among many other products. Maize, tobacco, dried meat, and cassava flour are also produced locally. The Angolan bourgeoisie was born by this time.
In 1889, Governor Brito Capelo opened the gates of an aqueduct which supplied the city with water, a formerly scarce resource, laying the foundation for major growth. Like most of Portuguese Angola, the cosmopolitan city of Luanda was not affected by the Portuguese Colonial War (1961–1974); economic growth and development in the entire region reached record highs during this period. In 1972, a report called Luanda the "Paris of Africa". Throughout Portugal's Estado Novo period, Luanda grew from a town of 61,208 with 14.6% of those inhabitants being white in 1940, to a wealthy cosmopolitan major city of 475,328 in 1970 with 124,814 Europeans (26.3%) and around 50,000 mixed race inhabitants. Luanda has also become one of the world's most expensive cities.
Independence from Portugal.
By the time of Angolan independence in 1975, Luanda was a modern city. The majority of its population was African, but it was dominated by a strong minority of white Portuguese origin. After the Carnation Revolution in Lisbon on April 25, 1974, with the advent of independence and the start of the Angolan Civil War (1975–2002), most of the white Portuguese Luandans left as refugees, principally for Portugal, with many travelling overland to South Africa. There was an immediate crisis, however, as the local African population lacked the skills and knowledge needed to run the city and maintain its well-developed infrastructure. The large numbers of skilled technicians among the force of Cuban soldiers sent in to support the Popular Movement for the Liberation of Angola (MPLA) government in the Angolan Civil War were able to make a valuable contribution to restoring and maintaining basic services in the city. In the following years, however, slums called "musseques" — which had existed for decades — began to grow out of proportion and stretched several kilometres beyond Luanda's former city limits as a result of the decades-long civil war, and because of the rise of deep social inequalities due to large-scale migration of civil war refugees from other Angolan regions. For decades, Luanda's facilities were not adequately expanded to handle this massive increase in the city's population. After 2002, with the end of the civil war and high economic growth rates fuelled by the wealth provided by the increasing oil and diamond production, major reconstruction started.
Geography.
Human geography.
Luanda is divided into two parts, the "Baixa de Luanda" (lower Luanda, the old city) and the "Cidade Alta" (upper city or the new part). The "Baixa de Luanda" is situated next to the port, and has narrow streets and old colonial buildings. However, massive new constructions have by now covered large areas beyond these traditional limits, and a number of previously independent nuclei — like Viana — were incorporated into the city.
Subdivisions.
Since 2011, Luanda Province is divided into 7 municipalities:
A completely new satellite city, called Luanda Sul has been built. In Camama, Zango and Kilamba Kiaxi, more high-rise developments are to be built. The capital Luanda is growing constantly - and in addition, increasingly beyond the official city limits and even provincial boundaries.
Luanda is the seat of a Roman Catholic archbishop. It is also the location of most of Angola's educational institutions, including the private Catholic University of Angola and the public University of Agostinho Neto. It is also the home of the colonial Governor's Palace and the Estádio da Cidadela (the "Citadel Stadium"), Angola's main stadium, with a total seating capacity of 60,000.
Luanda Sul.
Luanda Sul is a satellite city of Luanda. A small stream flows in southern Luanda Sul, starting near the Quatro de Fevereiro Airport then crossing near Vila de Gamek and emptying into the Atlantic Ocean. Vila de Gamek is in northeastern Luanda Sul, near the Quatro de Fevereiro Airport.
Lis-Luanda International School is in viana. Avenue Pedro de C. Vandunem-Loy straddles the northern border of Luanda Sul. Rua da Samba is in western Luanda Sul, near the Atlantic coast.
Climate.
Luanda has a mild semi-arid climate (Köppen climate classification: "BSh"). The climate is warm to hot but surprisingly dry, owing to the cool Benguela Current, which prevents moisture from easily condensing into rain. Frequent fog prevents temperatures from falling at night even during the completely dry months from June to October. Luanda has an annual rainfall of , but the variability is among the highest in the world, with a co-efficient of variation above 40 percent. Observed records since 1858 range from in 1958 to in 1916. The short rainy season in March and April depends on a northerly counter current bringing moisture to the city: it has been shown clearly that weakness in the Benguela current can increase rainfall about sixfold compared with years when that current is strong.
Demographics.
The inhabitants of Luanda are primarily members of African ethnic groups, mainly Ambundu, Ovimbundu, and Bakongo. The official and the most widely used language is Portuguese, although several Bantu languages are also used, chiefly Kimbundu, Umbundu, and Kikongo. There is a sizable minority population of European origin, especially Portuguese (about 260,000), as well as Brazilians and other Latin Americans. Over the last decades, a significant Chinese community has formed, as has a much smaller Vietnamese community. There is a sprinkling of immigrants from other African countries as well, including a small expatriate South African community. Many people of Luanda are of mixed race — European/Portuguese and native African. In recent years, mainly since the mid-2000s, immigration from Portugal has increased due to Portugal's recession and poor economic situation.
The population of Luanda has grown dramatically in recent years, due in large part to war-time migration to the city, which is safe compared to the rest of the country. Luanda, however, in 2006 saw an increase in violent crime, particularly in the shanty towns that surround the colonial urban core.
Economy.
Around one-third of Angolans live in Luanda, 53% of whom live in poverty. Living conditions in Luanda are poor for most of the people, with essential services such as safe drinking water and electricity still in short supply, and severe shortcomings in traffic conditions. On the other hand, luxury constructions for the benefit of the wealthy minority are booming. Luanda is one of the world's most expensive cities for resident foreigners.
New import tariffs imposed in March 2014 made Luanda even more expensive. As an example, a half-litre tub of vanilla ice-cream at the supermarket was reported to cost US$31. The higher import tariffs applied to hundreds of items, from garlic to cars. The stated aim was to try to diversify the heavily oil-dependent economy and nurture farming and industry, sectors which have remained weak. These tariffs have caused much hardship in a country where the average salary was US$260 in 2010, the latest year for which data was available. However, the average salary in the booming oil industry was over 20 times higher at US$5,400.
Manufacturing includes processed foods, beverages, textiles, cement and other building materials, plastic products, metalware, cigarettes, and shoes/clothes. Petroleum (found in nearby off-shore deposits) is refined in the city, although this facility was repeatedly damaged during the Angolan Civil War of 1975–2002. Luanda has an excellent natural harbour; the chief exports are coffee, cotton, sugar, diamonds, iron, and salt. The city also has a thriving building industry, an effect of the nationwide economic boom experienced since 2002, when political stability returned with the end of the civil war. Economic growth is largely supported by oil extraction activities, although massive diversification is taking place. Large investment (domestic and international), along with strong economic growth, has dramatically increased construction of all economic sectors in the city of Luanda. In 2007, the first modern shopping mall in Angola was established in the city at Belas Shopping mall.
Transport.
Luanda is the starting point of the Luanda railway that goes due east to Malanje. The civil war left the railway non-functional, but the railway has been restored up to Dondo and Malanje.
The main airport of Luanda is Quatro de Fevereiro Airport, which is the largest in the country. Currently, a new international airport, Angola International Airport is under construction southeast of the city, a few kilometres from Viana, which was expected to be opened in 2011. However, as the Angolan government did not continue to make the payments due to the Chinese enterprise in charge of the construction, the firm suspended its work in 2010.
The port of Luanda serves as the largest port of Angola, and connects Angola to the rest of the world. Major expansion of this port is also taking place. In 2014, a new port is being developed at Dande, about 30 km to the north.
Luanda's roads are in a poor state of repair, but are currently undergoing a massive reconstruction process by the government in order to relieve traffic congestion in the city. Major road repairs can be found taking place in nearly every neighbourhood, including a major 6-lane highway connected Luanda to Viana.
Public transit is provided by the suburban services of the Luanda Railway, by the public company TCUL, and by a large fleet of privately owned collective taxis as white-blue painted minibuses called "Candongueiro". https://commons.wikimedia.org/wiki/File%3ACandongueiros.jpg
Candongueiros are mostly Toyota Hiace vans, that are built to carry 12 people. But the candongueiros usually carry at least 15 people. They charge from 100 to 200 kwanzas per trip. They usually do not follow many traffic rules. Don´t stop at signs, drive over pavements and aisles. Their stop points, known as "paragens" are often the places where traffic is worse because they often double park. https://upload.wikimedia.org/wikipedia/commons/5/56/Traffic_in_Luanda.JPG
There is also a private bus company TURA working routes in Luanda.
Renewal and enlargement.
The central government supposedly allocates funds to all regions of the country, but the capital region receives the bulk of these funds. Since the end of the Angolan Civil War (1975–2002), stability has been widespread in the country, and major reconstruction has been going on since 2002 in those parts of the country that were damaged during the civil war. Luanda has been of major concern because its population had multiplied and had far outgrown the capacity of the city, especially because much of its infrastructure (water, electricity, roads etc.) had become obsolete and degraded.
Reconstruction in Luanda has been felt in nearly all aspects of society. Major road rehabilitation, including road widening, application of asphalt, and re-routing efforts are all currently being done throughout Luanda. The Brazilian construction firm Odebrecht have been constructing two six-lane highways. One highway will provide speedy access to Cacuaco, Viana, Samba, and the Kilamba Kiaxi district of Luanda to the new airport of Luanda. The other highway will connect the city centre of Luanda to Viana, and was expected to be completed by the end of 2008. Both ventures are, however, still under way in 2011(2015 now!).
Major social housing is also being constructed to house those who reside in slums, which dominate the landscape of Luanda. A large Chinese firm has been given a contract to construct the majority of replacement housing in Luanda. The Angolan minister of health recently stated poverty in Angola will be overcome by an increase in jobs and the housing of every citizen.
Education.
Universities:
International schools in Luanda:
Sports.
In 2013 Luanda together with Namibe city, hosted the 2013 FIRS Men's Roller Hockey World Cup, the first time that a World Cup of roller hockey was held in Africa.
International relations.
Twin towns – Sister cities.
Luanda is twinned with:

</doc>
<doc id="18403" url="https://en.wikipedia.org/wiki?curid=18403" title="Logical positivism">
Logical positivism

Logical positivism and logical empiricism, which together formed neopositivism, was a movement in Western philosophy that embraced verificationism, an approach that sought to legitimize philosophical discourse on a basis shared with the best examples of empirical sciences. In this theory of knowledge, only statements verifiable either logically or empirically would be "cognitively meaningful". Efforts to convert philosophy to this new "scientific philosophy" were intended to prevent confusion rooted in unclear language and unverifiable claims. The Berlin Circle and the Vienna Circle propounded logical positivism starting in the late 1920s.
Interpreting Ludwig Wittgenstein's early philosophy of language, logical positivists identified a verifiability principle or criterion of cognitive meaningfulness. From Bertrand Russell's logicism they sought reduction of mathematics to logic as well as Russell's logical atomism, Ernst Mach's phenomenalism—whereby the mind knows only actual or potential sensory experience, which is the content of all sciences, whether physics or psychology—and Percy Bridgman's musings that others proclaimed as operationalism. Thereby, only the "verifiable" was scientific and "cognitively meaningful", whereas the unverifiable was unscientific, cognitively meaningless "pseudostatements"—metaphysic, emotive, or such—not candidate to further review by philosophers, newly tasked to organize knowledge, not develop new knowledge.
Logical positivism is commonly portrayed as taking the extreme position that scientific language should never refer to anything unobservable—even the seemingly core notions of causality, mechanism, and principles—but that is an exaggeration. Talk of such unobservables would be metaphorical—direct observations viewed in the abstract—or at worst metaphysical or emotional. "Theoretical laws" would be reduced to "empirical laws", while "theoretical terms" would garner meaning from "observational terms" via "correspondence rules". Mathematics of physics would reduce to symbolic logic via logicism, while rational reconstruction would convert ordinary language into standardized equivalents, all networked and united by a logical syntax. A scientific theory would be stated with its method of verification, whereby a logical calculus or empirical operation could verify its falsity or truth.
In the late 1930s, logical positivists fled Germany and Austria for Britain and United States. By then, many had replaced Mach's phenomenalism with Neurath's physicalism, and Carnap had sought to replace "verification" with simply "confirmation". With World War II's close in 1945, logical positivism became milder, "logical empiricism", led largely by Carl Hempel, in America, who expounded the covering law model of scientific explanation. The logical positivist movement became a major underpinning of analytic philosophy, and dominated Anglosphere philosophy, including philosophy of science, while influencing sciences, into the 1960s. Yet the movement failed to resolve its central problems, and its doctrines were increasingly criticized, most trenchantly by W V O Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, and Carl Hempel.
Roots.
Language.
"Tractatus Logico-Philosophicus", by the young Ludwig Wittgenstein, introduced the view of philosophy as "critique of language", offering the possibility of a theoretically principled distinction of intelligible versus nonsensical discourse. "Tractatus" adhered to a correspondence theory of truth (versus a coherence theory of truth). Wittgenstein's influence also shows in some versions of the verifiability principle. In tractarian doctrine, truths of logic are tautologies, a view widely accepted by logical positivists who were also influenced by Wittgenstein's interpretation of probability although, according to Neurath, some logical positivists found "Tractatus" to contain much metaphysics.
Logicism.
Gottlob Frege began the program of reducing mathematics to logic, continued it with Bertrand Russell, but lost interest in this logicism, and Russell continued it with Alfred North Whitehead in their monumental "Principia Mathematica", inspiring some of the more mathematical logical posivists, such as Hans Hahn and Rudolf Carnap. (Carnap's early anti-metaphysical works employed Russell's theory of types.) Carnap envisioned a universal language that could reconstruct mathematics and thereby encode physics. Yet Kurt Gödel's incompleteness theorem showed this impossible except in trivial cases, and Alfred Tarski's undefinability theorem shattered all hopes of reducing mathematics to logic. Thus, a universal language failed to stem from Carnap's 1934 work "Logische Syntax der Sprache" ("Logical Syntax of Language"). Still, some logical positivists, including Carl Hempel, continued support of logicism.
Empiricism.
In Germany, Hegelian metaphysics was a dominant movement, and Hegelian successors such as F H Bradley explained reality by postulating metaphysical entities lacking empirical basis, drawing reaction in the form of positivism. Starting in the late 19th century, there was "back to Kant" movement. Ernst Mach's positivism and phenomenalism were a major influence.
Origins.
Vienna.
The Vienna Circle, gathering around University of Vienna and Café Central, was led principally by Moritz Schlick. Schlick had held a neo-Kantian position, but later converted, via Carnap's 1928 book "Der logische Aufbau der Welt"—that is, "The Logical Structure of the World"—which became Vienna Circle's "bible", "Aufbau". A 1929 pamphlet written by Otto Neurath, Hans Hahn, and Rudolf Carnap summarized the Vienna Circle's positions. Another member of Vienna Circle to later prove very influential was Carl Hempel. A friendly but tenacious critic of the Circle was Karl Popper, whom Neurath nicknamed the "Official Opposition".
Carnap and other Vienna Circle members, including Hahn and Neurath, saw need for a weaker criterion of meaningfulness than verifiability. A radical "left" wing—led by Neurath and Carnap—began the program of "liberalization of empiricism", and they also emphasized fallibilism and pragmatics, which latter Carnap even suggested as empiricism's basis. A conservative "right" wing—led by Schlick and Waismann—rejected both the liberalization of empiricism and the epistemological nonfoundationalism of a move from phenomenalism to physicalism. As Neurath and somewhat Carnap posed science toward social reform, the split in Vienna Circle also reflected political views.
Berlin.
The Berlin Circle was led principally by Hans Reichenbach.
Rivals.
Both Moritz Schlick and Rudolf Carnap had been influenced by and sought to define logical positivism versus the neo-Kantianism of Ernst Cassirer—the then leading figure of Marburg school, so called—and against Edmund Husserl's phenomenology. Logical positivists especially opposed Martin Heidegger's obscure metaphysics, the epitome of what logical positivism rejected. In the early 1930s, Carnap debated Heidegger over "metaphysical pseudosentences". Despite its revolutionary aims, logical positivism was but one view among many vying within Europe, and logical positivists initially spoke their language.
Export.
As the movement's first emissary to the New World, Moritz Schlick visited Stanford University in 1929, yet otherwise remained in Vienna and was murdered at the University, reportedly by a deranged student, in 1936. That year, a British attendee at some Vienna Circle meetings since 1933, A J Ayer saw his "Language, Truth and Logic", written in English, import logical positivism to the Anglosphere. By then, Nazi political party's 1933 rise to power in Germany had triggered flight of intellectuals. In exile in England, Otto Neurath died in 1945. Rudolf Carnap, Hans Reichenbach, and Carl Hempel—Carnap's protégé who had studied in Berlin with Reichenbach—settled permanently in America. Upon Germany's annexation of Austria in 1939, remaining logical positivists, many of whom were also Jewish, were targeted and continued flight. Logical positivism thus became dominant in the Anglosphere.
Principles.
Analytic/synthetic gap.
Concerning reality, the necessary is a state true in all possible worlds—mere logical validity—whereas the contingent hinges on the way the particular world is. Concerning knowledge, the "a priori" is knowable before or without, whereas the "a posteriori" is knowable only after or through, relevant experience. Concerning statements, the "analytic" is true via terms' arrangement and meanings, thus a tautology—true by logical necessity but uninformative about the world—whereas the "synthetic" adds reference to a state of facts, a contingency.
In 1739, Hume cast a fork aggressively dividing "relations of ideas" from "matters of fact and real existence", such that all truths are of one type or the other. By Hume's fork, truths by relations among ideas (abstract) all align on one side (analytic, necessary, "a priori"), whereas truths by states of actualities (concrete) always align on the other side (synthetic, contingent, "a posteriori"). At any treatises containing neither, Hume orders, "Commit it then to the flames, for it can contain nothing but sophistry and illusion".
Thus awakened from "dogmatic slumber", Kant quested to answer Hume's challenge—but by explaining how metaphysics is possible. Eventually, in his 1781 work, Kant crossed the tines of Hume's fork to identify another range of truths by necessity—synthetic "a priori", statements claiming states of facts but known true before experience—by arriving at transcendental idealism, attributing the mind a constructive role in phenomena by arranging sense data into the very experience "space", "time", and "substance". Thus, Kant saved Newton's law of universal gravitation from Hume's problem of induction by finding uniformity of nature to be "a priori" knowledge. Logical positivists rejected Kant's synthethic "a priori", and staked Hume's fork, whereby a statement is either analytic and "a priori" (thus necessary and verifiable logically) or synthetic and "a posteriori" (thus contingent and verifiable empirically).
Observation/theory gap.
Early, most logical positivists proposed that all knowledge is based on logical inference from simple "protocol sentences" grounded in observable facts. In the 1936 and 1937 papers "Testability and meaning", individual terms replace sentences as the units of meaning. Further, theoretical terms no longer need to acquire meaning by explicit definition from observational terms: the connection may be indirect, through a system of implicit definitions. (Carnap also provides an important, pioneering discussion of disposition predicates.)
Cognitive meaningfulness.
Verification.
The logical positivists' initial stance was that a statement is "cognitively meaningful" only if some finite procedure conclusively determines its truth. By this verifiability principle, only statements verifiable either by their analyticity or by empiricism were "cognitively meaningful". Metaphysics, ontology, as well as much of ethics failed this criterion, and so were found "cognitively meaningless". Moritz Schlick, however, did not view ethical or aesthetic statements as cognitively meaningless. "Cognitive meaningfulness" was variously defined: having a truth value; corresponding to a possible state of affairs; naming a proposition; intelligible or understandable as are scientific statements.
Ethics and aesthetics were subjective preferences, while theology and other metaphysics contained "pseudostatements", neither true nor false. This meaningfulness was cognitive, although other types of meaningfulness—for instance, emotive, expressive, or figurative—occurred in metaphysical discourse, dismissed from further review. Thus, logical positivism indirectly asserted Hume's law, the principle that "is" statements cannot justify "ought" statements, but are separated by an unbridgeable gap. A J Ayer's 1936 book asserted an extreme variant—the boo/hooray doctrine—whereby all evaluative judgments are but emotional reactions.
Confirmation.
In an important pair of papers in 1936 and 1937, "Testability and meaning", Carnap replaced "verification" with "confirmation", on the view that although universal laws cannot be verified they can be confirmed. Later, Carnap employed abundant logical and mathematical methods in researching inductive logic while seeking to provide and account of probability as "degree of confirmation", but was never able to formulate a model. In Carnap's inductive logic, every universal law's degree of confirmation is always zero. In any event, the precise formulation of what came to be called the "criterion of cognitive significance" took three decades (Hempel 1950, Carnap 1956, Carnap 1961).
Carl Hempel became a major critic within the logical positivism movement. Hempel elucidated the paradox of confirmation.
Weak verification.
The second edition of A J Ayer's book arrived in 1946, and discerned "strong" versus "weak" forms of verification. Ayer concluded, "A proposition is said to be verifiable, in the strong sense of the term, if, and only if, its truth could be conclusively established by experience", but is verifiable in the weak sense "if it is possible for experience to render it probable". And yet, "no proposition, other than a tautology, can possibly be anything more than a probable hypothesis". Thus, all are open to weak verification.
Philosophy of science.
Upon the global defeat of Nazism, and the removal from philosophy of rivals for radical reform—Marburg neo-Kantianism, Husserlian phenomenology, Heidegger's "existential hermeneutics"—and while hosted in the climate of American pragmatism and commonsense empiricism, the neopositivists shed much of their earlier, revolutionary zeal. No longer crusading to revise traditional philosophy into a new "scientific philosophy", they became respectable members of a new philosophy subdiscipline, "philosophy of science". Receiving support from Ernest Nagel, logical empiricists were especially influential in the social sciences.
Explanation.
Comtean positivism had viewed science as "description", whereas the logical positivists posed science as "explanation", perhaps to better realize the envisioned unity of science by covering not only fundamental science—that is, fundamental physics—but the special sciences, too, for instance biology, anthropology, psychology, sociology, and economics. The most widely accepted concept of scientific explanation, held even by neopositivist critic Karl Popper, was the deductive-nomological model (DN model). Yet DN model received its greatest explication by Carl Hempel, first in his 1942 article "The function of general laws in history", and more explicitly with Paul Oppenheim in their 1948 article "Studies in the logic of explanation".
In DN model, the stated phenomenon to be explained is the "explanandum"—which can be an event, law, or theory—whereas premises stated to explain it are the "explanans". Explanans must be true or highly confirmed, contain at least one law, and entail the explanandum. Thus, given initial conditions "C1, C2 . . . Cn" plus general laws "L1, L2 . . . Ln", event "E" is a deductive consequence and scientifically explained. In DN model, a law is an unrestricted generalization by conditional proposition—"If A, then B"—and has empirical content testable. (Differing from a merely true regularity—for instance, "George always carries only $1 bills in his wallet"—a law suggests what "must" be true, and is consequent of a scientific theory's axiomatic structure.)
By the Humean empiricist view that humans observe sequence of events, not cause and effect—as causality and causal mechanisms are unobservable—DN model neglects causality beyond mere constant conjunction, first event "A" and then always event "B". Hempel's explication of DN model held natural laws—empirically confirmed regularities—as satisfactory and, if formulated realistically, approximating causal explanation. In later articles, Hempel defended DN model and proposed a probabilistic explanation, inductive-statistical model (IS model). DN model and IS model together form "covering law model", as named by a critic, William Dray. (Derivation of statistical laws from other statistical laws goes to deductive-statistical model (DS model).) Georg Henrik von Wright, another critic, named it "subsumption theory", fitting the ambition of theory reduction.
Unity of science.
Logical positivists were generally committed to "Unified Science", and sought a common language or, in Neurath's phrase, a "universal slang" whereby which all scientific propositions could be expressed. The adequacy of proposals or fragments of proposals for such a language was often asserted on the basis of various "reductions" or "explications" of the terms of one special science to the terms of another, putatively more fundamental. Sometimes these reductions consisted of set-theoretic manipulations of a few logically primitive concepts (as in Carnap's "Logical Structure of the World" (1928)). Sometimes, these reductions consisted of allegedly analytic or "a priori" deductive relationships (as in Carnap's "Testability and meaning"). A number of publications over a period of thirty years would attempt to elucidate this concept.
Theory reduction.
As in Comptean positivism's envisioned unity of science, neopositivists aimed to network all special sciences through the covering law model of scientific explanation. And ultimately, by supplying boundary conditions and supplying bridge laws within the covering law model, all the special sciences' laws would reduce to fundamental physics, the fundamental science.
Critics.
After the Second World War's close in 1945, key tenets of logical positivism, including its atomistic philosophy of science, the verifiability principle, and the fact/value gap, drew escalated criticism. It was clear that empirical claims cannot be verified to be universally true. Thus, as initially stated, the verifiability criterion made universal statements meaningless, and even made statements beyond empiricism for technological but not conceptual reasons meaningless, which would pose significant problems for science. These problems were recognized within the movement, which hosted attempted solutions—Carnap's move to "confirmation", Ayer's acceptance of "weak verification"—but the program drew sustained criticism from a number of directions by the 1950s. Even philosophers disagreeing among themselves on which direction general epistemology ought to take, as well as on philosophy of science, agreed that the logical empiricist program was untenable, and it became viewed as self-contradictory. The verifiability criterion of meaning was itself unverified. Notable critics were Nelson Goodman, Willard Van Orman Quine, Norwood Hanson, Karl Popper, Thomas Kuhn, J L Austin, Peter Strawson, Hilary Putnam, Ludwig von Mises, and Richard Rorty.
Quine.
Although quite empiricist, American logician Willard Van Orman Quine published the 1951 paper "Two Dogmas of Empiricism", which challenged conventional empiricist presumptions. Quine attacked the analytic/synthetic division, which the verificationist program had been hinged upon in order to entail, by consequence of Hume's fork, both necessity and apriocity. Quine's ontological relativity explained that every term in any statement has its meaning contingent on a vast network of knowledge and belief, the speaker's conception of the entire world. Quine later proposed naturalized epistemology.
Hanson.
In 1958, Norwood Hanson's "Patterns of Discovery" undermined the division of observation versus theory, as one can predict, collect, prioritize, and assess data only via some horizon of expectation set by a theory. Thus, any dataset—the direct observations, the scientific facts—is laden with theory.
Popper.
An early, tenacious critic was Karl Popper whose 1934 book "Logik der Forschung", arriving in English in 1959 as "The Logic of Scientific Discovery", directly answered verificationism. Popper heeded the problem of induction as rendering empirical verification logically impossible. And the deductive fallacy of affirming the consequent reveals any phenomenon's capacity to host over one logically possible explanation. Accepting scientific method as hypotheticodeduction, whose inference form is denying the consequent, Popper finds scientific method unable to proceed without falsifiable predictions. Popper thus identifies falsifiability to demarcate not "meaningful" from "meaningless" but simply "scientific" from "unscientific"—a label not in itself unfavorable.
Popper finds virtue in metaphysics, required to develop new scientific theories. And an unfalsifiable—thus unscientific, perhaps metaphysical—concept in one era can later, through evolving knowledge or technology, become falsifiable, thus scientific. Popper also found science's quest for truth to rest on values. Popper disparages the "pseudoscientific", which occurs when an unscientific theory is proclaimed true and coupled with seemingly scientific method by "testing" the unfalsifiable theory—whose predictions are confirmed by necessity—or when a scientific theory's falsifiable predictions are strongly falsified but the theory is persistently protected by "immunizing stratagems", such as the appendage of "ad hoc" clauses saving the theory or the recourse to increasingly speculative hypotheses shielding the theory.
Popper's "scientific" epistemology is falsificationism, which finds that no number, degree, and variety of empirical successes can either verify or confirm scientific theory. Falsificationism finds science's aim as "corroboration" of scientific theory, which strives for scientific realism but accepts the maximal status of strongly corroborated verisimilitude ("truthlikeness"). Explicitly denying the positivist view that all knowledge is scientific, Popper developed the "general" epistemology critical rationalism, which finds human knowledge to evolve by "conjectures and refutations". Popper thus acknowledged the value of the positivist movement, driving evolution of human understanding, but claimed that he had "killed positivism".
Kuhn.
With his landmark, "The Structure of Scientific Revolutions", Thomas Kuhn critically destabilized the verificationist program, which was presumed to call for foundationalism. (Actually, even in the 1930s, Otto Neurath had argued for nonfoundationalism via coherentism by likening science to a boat that scientists must rebuild at sea.) Although Kuhn's thesis itself was attacked even by opponents of neopositivism, in the 1970 postscript to "Structure", Kuhn asserted, at least, that there was no algorithm to science—and, on that, even most of Kuhn's critics agreed.
Powerful and persuasive, Kuhn's book, unlike the vocabulary and symbols of logic's formal language, was written in natural language open to the layperson. Ironically, Kuhn's book was first published in a volume of "Encyclopedia of Unified Science"—a project begun by logical positivists—and some sense unified science, indeed, but by bringing it into the realm of historical and social assessment, rather than fitting it to the model of physics. Kuhn's ideas were rapidly adopted by scholars in disciplines well outside natural sciences, and, as logical empiricists were extremely influential in the social sciences, ushered academia into postpositivism or postempiricism.
Putnam.
The "received view" operates on the "correspondence rule" that states, "The observational terms are taken as referring to specified phenomena or phenomenal properties, and the only interpretation given to the theoretical terms is their explicit definition provided by the correspondence rules". According to Hilary Putnam, a former student of Reichenbach and of Carnap, the dichotomy of observational terms versus theoretical terms introduced a problem within scientific discussion that was nonexistent until this dichotomy was stated by logical positivists. Putnam's four objections:
Putnam also alleged that positivism was actually a form of metaphysical idealism by its rejecting scientific theory's ability to garner knowledge about nature's unobservable aspects. With his "no miracles" argument, posed in 1974, Putnam asserted scientific realism, the stance that science achieves true—or approximately true—knowledge of the world as it exists independently of humans' sensory experience. In this, Putnam opposed not only the positivism but other instrumentalism—whereby scientific theory as but a human tool to predict human observations—filling the void left by positivism's decline.
Retrospect.
By the late 1960s, logical positivism had clearly run its course. Interviewed in the late 1970s, A J Ayer supposed that "the most important" defect "was that nearly all of it was false". Although logical positivism tends to be recalled as a pillar of scientism, Carl Hempel was key in establishing the philosophy subdiscipline philosophy of science where Thomas Kuhn and Karl Popper brought in the era postpositivism. John Passmore found logical positivism to be "dead, or as dead as a philosophical movement ever becomes".
Logical positivism's fall reopened debate over the metaphysical merit of scientific theory, whether it can offer knowledge of the world beyond human experience (scientific realism) versus whether it is but a human tool to predict human experience (instrumentalism). Meanwhile, it became popular among philosophers to rehash the faults and failures of logical positivism without investigation of it. Thereby, logical positivism has been generally misrepresented, sometimes severely. Arguing for their own views, often framed versus logical positivism, many philosophers have reduced logical positivism to simplisms and stereotypes, especially the notion of logical positivism as a type of foundationalism. In any event, the movement helped anchor analytic philosophy in the Anglosphere, and returned Britain to empiricism. Without the logical positivists, who have been tremendously influential outside philosophy, especially in psychology and social sciences, intellectual life of the 20th century would be unrecognizable.
References.
Bechtel, William, "Philosophy of Science: An Overview for Cognitive Science" (Hillsdale NJ: Lawrence Erlbaum Assoc, 1988).
Friedman, Michael, "Reconsidering Logical Positivism" (New York: Cambridge University Press, 1999).
Novick, Peter, "That Noble Dream: The 'Objectivity Question' and the American Historical Profession" (Cambridge UK: Cambridge University Press, 1988).
Stahl, William A & Robert A Campbell, Yvonne Petry, Gary Diver, "Webs of Reality: Social Perspectives on Science and Religion" (Piscataway NJ: Rutgers University Press, 2002).
Suppe, Frederick, ed, "The Structure of Scientific Theories", 2nd edn (Urbana IL: University of Illinois Press, 1977).

</doc>
<doc id="18404" url="https://en.wikipedia.org/wiki?curid=18404" title="Lorentz transformation">
Lorentz transformation

In physics, the Lorentz transformation (or transformations) are coordinate transformations between two coordinate frames that move at constant velocity relative to each other.
Frames of reference can be divided into two groups, inertial (relative motion with constant velocity) and non-inertial (accelerating in curved paths, rotational motion with constant angular velocity, etc.). The term "Lorentz transformations" only refers to transformations between "inertial" frames, usually in the context of special relativity.
In each reference frame, an observer can use a local coordinate system (most exclusively Cartesian coordinates in this context) to measure lengths, and a clock to measure time intervals. An observer is a real or imaginary entity that can take measurements, say humans, or any other living organism—or even robots and computers. An event is something that happens at a point in space at an instant of time, or more formally a point in spacetime. The transformations connect the space and time coordinates of an event as measured by an observer in each frame.
They supersede the Galilean transformation of Newtonian physics, which assumes an absolute space and time (see Galilean relativity). The Galilean transformation is a good approximation only at relative speeds much smaller than the speed of light. Lorentz transformations have a number of unintuitive features that do not appear in Galilean transformations. For example, they reflect the fact that observers moving at different velocities may measure different distances, elapsed times, and even different orderings of events, but always such that the speed of light is the same in all inertial reference frames. The invariance of light speed is one of the postulates of special relativity.
Historically, the transformations were the result of attempts by Lorentz and others to explain how the speed of light was observed to be independent of the reference frame, and to understand the symmetries of the laws of electromagnetism. The Lorentz transformation is in accordance with special relativity, but was derived before special relativity. The transformations are named after the Dutch physicist Hendrik Lorentz.
The Lorentz transformation is a linear transformation. It may include a rotation of space; a rotation-free Lorentz transformation is called a Lorentz boost. In Minkowski space, the mathematical model of spacetime in special relativity, the Lorentz transformations preserve the spacetime interval between any two events. This property is the defining property of a Lorentz transformation. They describe only the transformations in which the spacetime event at the origin is left fixed. They can be considered as a hyperbolic rotation of Minkowski space. The more general set of transformations that also includes translations is known as the Poincaré group.
History.
Many physicists—including Woldemar Voigt, George FitzGerald, Joseph Larmor, and Hendrik Lorentz himself—had been discussing the physics implied by these equations since 1887. Early in 1889, Oliver Heaviside had shown from Maxwell's equations that the electric field surrounding a spherical distribution of charge should cease to have spherical symmetry once the charge is in motion relative to the ether. FitzGerald then conjectured that Heaviside’s distortion result might be applied to a theory of intermolecular forces. Some months later, FitzGerald published the conjecture that bodies in motion are being contracted, in order to explain the baffling outcome of the 1887 ether-wind experiment of Michelson and Morley. In 1892, Lorentz independently presented the same idea in a more detailed manner, which was subsequently called FitzGerald–Lorentz contraction hypothesis. Their explanation was widely known before 1905.
Lorentz (1892–1904) and Larmor (1897–1900), who believed the luminiferous ether hypothesis, also looked for the transformation under which Maxwell's equations are invariant when transformed from the ether to a moving frame. They extended the FitzGerald–Lorentz contraction hypothesis and found out that the time coordinate has to be modified as well ("local time"). Henri Poincaré gave a physical interpretation to local time (to first order in v/c) as the consequence of clock synchronization, under the assumption that the speed of light is constant in moving frames. Larmor is credited to have been the first to understand the crucial time dilation property inherent in his equations.
In 1905, Poincaré was the first to recognize that the transformation has the properties of a mathematical group,
and named it after Lorentz.
Later in the same year Albert Einstein published what is now called special relativity, by deriving the Lorentz transformation under the assumptions of the principle of relativity and the constancy of the speed of light in any inertial reference frame, and by abandoning the mechanical aether.
Derivation.
An "event" is something that happens at a certain point in spacetime, or more generally, the point in spacetime itself. In any inertial frame an event is specified by a time coordinate "t" and a set of Cartesian coordinates to specify position in space in that frame. Subscripts label individual events.
From Einstein's second postulate of relativity follows immediately
in all inertial frames for events connected by "light signals". The quantity on the left is called the "spacetime interval" between events and . The interval between "any two" events, not necessarily separated by light signals, is in fact invariant, i.e., independent of the state of relative motion of observers in different inertial frames, as is shown here (where one can also find several more explicit derivations than presently given) using homogeneity and isotropy of space. The transformation sought after thus must possess the property that
where are the spacetime coordinates used to define events in one frame, and are the coordinates in another frame. Now one observes that a "linear" solution to the simpler problem
solves the general problem too. Finding the solution to the simpler problem is just a matter of look-up in the theory of classical groups that preserve bilinear forms of various signature. The Lorentz transformation is thus an element of the group O(3, 1) or, for those that prefer the other metric signature, .
Generalities.
The relations between the primed and unprimed spacetime coordinates are the Lorentz transformations, each coordinate in one frame is a linear function of all the coordinates in the other frame, and the inverse functions are the inverse transformation. Depending on how the frames move relative to each other, and how they are oriented in space relative to each other, other parameters that describe direction, speed, and orientation enter the transformation equations.
Transformations describing relative motion with constant (uniform) velocity and without rotation of the space coordinate axes are called a "boosts", and the relative velocity between the frames is the parameter of the transformation. The other basic type of Lorentz transformations is rotations in the spatial coordinates only, these are also inertial frames since there is no relative motion, the frames are simply tilted (and not continuously rotating), and in this case quantities defining the rotation are the parameters of the transformation (e.g., axis–angle representation, or Euler angles, etc.). A combination of a rotation and boost is a "homogenous transformation", which transforms the origin back to the origin.
The full Lorentz group also contains special transformations that are neither rotations nor boosts, but rather reflections in a plane through the origin. Two of these can be singled out; spatial inversion in which the spatial coordinates of all events are reversed in sign and temporal inversion in which the time coordinate for each event gets its sign reversed.
Boosts should not be conflated with mere displacements in spacetime; in this case, the coordinate systems are simply shifted and there is no relative motion. However, these also count as symmetries forced by special relativity since they leave the spacetime interval invariant. A combination of a rotation with a boost, followed by a shift in spacetime, is an "inhomogenous Lorentz transformation", an element of the Poincaré group, which is also called the inhomogeneous Lorentz group.
Physical formulation of Lorentz boosts.
Coordinate transformation.
A "stationary" observer in frame defines events with coordinates . Another frame moves with velocity relative to , and an observer in this "moving" frame defines events using the coordinates .
The coordinate axes in each frame are parallel (the and axes are parallel, the and axes are parallel, and the and axes are parallel), remain mutually perpendicular, and relative motion is along the coincident axes. At , the origins of both coordinate systems are the same, . In other words, the times and positions are coincident at this event. If all these hold, then the coordinate systems are said to be in standard configuration, or synchronized.
If an observer in records an event , then an observer in records the "same" event with coordinates
where is the relative velocity between frames in the -direction, is the speed of light, and
(lowercase gamma) is the Lorentz factor.
Here, is the "parameter" of the transformation, for a given boost it is a constant number, but can take a continuous range of values. In the setup used here, positive relative velocity is motion along the positive directions of the axes, zero relative velocity is no relative motion, while negative relative velocity is relative motion along the negative directions of the axes. The magnitude of relative velocity cannot equal or exceed , so only subluminal speeds are allowed. The corresponding range of is .
The transformations are not defined if is outside these limits. At the speed of light () is infinite, and faster than light () is a complex number, each of which make the transformations unphysical. The space and time coordinates are measurable quantities and numerically must be real numbers, not complex.
As an active transformation, an observer in F′ notices the coordinates of the event to be "boosted" in the negative directions of the axes, because of the in the transformations. This has the equivalent effect of the "coordinate system" F′ boosted in the positive directions of the axes, while the event does not change and is simply represented in another coordinate system, a passive transformation.
The inverse relations ( in terms of ) can be found by algebraically solving the original set of equations. A more efficient way is to use physical principles. Here is the "stationary" frame while is the "moving" frame. According to the principle of relativity, there is no privileged frame of reference, so the transformations from to must take exactly the same form as the transformations from to . The only difference is moves with velocity relative to (i.e., the relative velocity has the same magnitude but is oppositely directed). Thus if an observer in notes an event , then an observer in notes the "same" event with coordinates
and the value of remains unchanged. This "trick" of simply reversing the direction of relative velocity while preserving its magnitude, and exchanging primed and unprimed variables, always applies to finding the inverse transformation of every boost in any direction.
Sometimes it is more convenient to use (lowercase beta) instead of , so that
which shows clearer the symmetry in the transformation. From the allowed ranges of and the definition of , it follows . The use of and is standard throughout the literature.
The Lorentz transformations can also be derived in a way that resembles circular rotations in 3d space using the hyperbolic functions. For the boost in the direction, the results are
where (lowercase zeta) is a parameter called "rapidity" (many other symbols are used, including ). Given the strong resemblance to rotations of spatial coordinates in 3d space in the Cartesian xy, yz, and zx planes, a Lorentz boost can be thought of as a hyperbolic rotation of spacetime coordinates in the xt, yt, and zt Cartesian-time planes of 4d Minkowski space. The parameter is the hyperbolic angle of rotation, analogous to the ordinary angle for circular rotations. This transformation can be illustrated with a Minkowski diagram.
The hyperbolic functions arise from the "difference" between the squares of the time and spatial coordinates in the spacetime interval, rather than a sum. The geometric significance of the hyperbolic functions can be visualized by taking or in the transformations. Squaring and subtracting the results, one can derive hyperbolic curves of constant coordinate values but varying , which parametrizes the curves according to the identity
Conversely the and axes can be constructed for varying coordinates but constant . The definition
provides the link between a constant value of rapidity, and the slope of the axis in spacetime. A consequence these two hyperbolic formulae is an identity that matches the Lorentz factor
Comparing the Lorentz transformations in terms of the relative velocity and rapidity, or using the above formulae, the connections between , , and are
Taking the inverse hyperbolic tangent gives the rapidity
Since , it follows . From the relation between and , positive rapidity is motion along the positive directions of the axes, zero rapidity is no relative motion, while negative rapidity is relative motion along the negative directions of the axes.
The inverse transformations are obtained by exchanging primed and unprimed quantities to switch the coordinate frames, and negating rapidity since this is equivalent to negating the relative velocity. Therefore,
The inverse transformations can be similarly visualized by considering the cases when and .
So far the Lorentz transformations have been applied to "one event". If there are two events, there is a spatial separation and time interval between them. It follows from the linearity of the Lorentz transformations that two values of space and time coordinates can be chosen, the Lorentz transformations can be applied to each, then subtracted to get the Lorentz transformations of the differences;
with inverse relations
where (capital Delta) indicates a difference of quantities, e.g., for two values of coordinates, and so on.
These transformations on "differences" rather than spatial points or instants of time are useful for a number of reasons:
Physical implications.
A critical requirement of the Lorentz transformations is the invariance of the speed of light, a fact used in their derivation, and contained in the transformations themselves. If in the equation for a pulse of light along the direction is , then in the Lorentz transformations give , and vice versa, for any .
For relative speeds much less than the speed of light, the Lorentz transformations reduce to the Galilean transformation
in accordance with the correspondence principle. It is sometimes said that nonrelativistic physics is a physics of "instantaneous action at a distance".
Three unintuitive, but correct, predictions of the transformations are:
Vector transformations.
The use of vectors allows positions and velocities to be expressed in arbitrary directions compactly. A single boost in any direction depends on the full relative velocity vector with a magnitude that cannot equal or exceed , so that .
Only time and the coordinates parallel to the direction of relative motion change, while those coordinates perpendicular do not. With this in mind, split the spatial position vector as measured in , and as measured in , each into components perpendicular (⊥) and parallel ( ‖ ) to ,
then the transformations are
where · is the dot product. The Lorentz factor retains its definition for a boost in any direction, since it depends only on the magnitude of the relative velocity. The definition with magnitude is also used by some authors.
Introducing a unit vector in the direction of relative motion, the relative velocity is with magnitude and direction , and vector projection and rejection give respectively
Accumulating the results gives the full transformations,
The projection and rejection also applies to . For the inverse transformations, exchange and to switch observed coordinates, and negate the relative velocity (or simply the unit vector since the magnitude is always positive) to obtain
The unit vector has the advantage of simplifying equations for a single boost, allows either or to be reinstated when convenient, and the rapidity parametrization is immediately obtained by replacing and . It is not convenient for multiple boosts.
The vectorial relation between relative velocity and rapidity is
and the "rapidity vector" can be defined as
each of which serves as a useful abbreviation in some contexts. The magnitude of is the absolute value of the rapidity scalar confined to , which agrees with the range .
Transformation of velocities.
Defining the coordinate velocities and Lorentz factor by
taking the differentials in the coordinates and time of the vector transformations, then dividing equations, leads to
The velocities and are the velocity of some massive object. They can also be for a third inertial frame (say F′′), in which case they must be "constant". Denote either entity by X. Then X moves with velocity relative to F, or equivalently with velocity relative to F′, in turn F′ moves with velocity relative to F. The inverse transformations can be obtained in a similar way, or as with position coordinates exchange and , and change to .
The transformation of velocity is useful in stellar aberration, the Fizeau experiment, and the relativistic Doppler effect.
The Lorentz transformations of acceleration can be similarly obtained by taking differentials in the velocity vectors, and dividing these by the time differential.
Transformation of coordinate derivatives.
Numerous equations in physics are partial differential equations involving space and time coordinates. Since the space and time coordinates change under Lorentz transformations, the derivatives must also. Using the chain rule one finds the transformation of the coordinate time and space derivatives to be
with inverses
These are not quite the same as the transformations of coordinates. It turns out many physical quantities transform either like the coordinates, or like the derivatives.
Transformation of other quantities.
In general, given four quantities and and their Lorentz-boosted counterparts and , a relation of the form
implies the quantities transform under Lorentz transformations similar to the transformation of spacetime coordinates;
The decomposition of (and ) into components perpendicular and parallel to is exactly the same as for the position vector, as is the process of obtaining the inverse transformations (exchange and to switch observed quantities, and reverse the direction of relative motion by );
The quantities collectively make up a "four vector", where is the "timelike component", and the "spacelike component". Examples of and are the following:
For a given object (e.g. particle, fluid, field, material), if or correspond to properties specific to the object like its charge density, mass density, spin, etc., its properties can be fixed in the rest frame of that object. Then the Lorentz transformations give the corresponding properties in a frame moving relative to the object with constant velocity. This breaks some notions taken for granted in non-relativistic physics. For example, the energy of an object is a scalar in non-relativistic mechanics, but not in relativistic mechanics because energy changes under Lorentz transformations; its value is different for various inertial frames. In the rest frame of an object, it has a rest energy and zero momentum. In a boosted frame its energy is different and it appears to have a momentum. Similarly, in non-relativistic quantum mechanics the spin of a particle is a constant vector, but in relativistic quantum mechanics spin depends on relative motion. In the rest frame of the particle, the spin pseudovector can be fixed to be its ordinary non-relativistic spin with a zero timelike quantity , however a boosted observer will perceive a nonzero timelike component and an altered spin.
Not all quantities are invariant in the form as shown above, for example orbital angular momentum does not have a timelike quantity, and neither does the electric field nor the magnetic field . The definition of angular momentum is , and in a boosted frame the altered angular momentum is . Applying this definition using the transformations of coordinates and momentum leads to the transformation of angular momentum. It turns out transforms with another vector quantity related to boosts, see relativistic angular momentum for details. For the case of the and fields, the transformations cannot be obtained as directly using vector algebra. A method of deriving the EM field transformations in an efficient way which also illustrates the unit of the electromagnetic field uses tensor algebra, given below.
Mathematical formulation.
Throughout, italic non-bold capital letters are 4×4 matrices, while non-italic bold letters are 3×3 matrices.
Boost matrix.
The separate algebraic equations are often used in practical calculations, but for theoretical purposes it is useful to arrange the coordinates in column vectors and the quantities defining the transformation into a transformation matrix thus
and all the separate equations compress into one matrix equation;
The boost matrix B is a symmetric matrix, it equals its transpose. In the inverse transformations the transformation matrix is the matrix inverse of the original transformation. Instead of explicitly calculating the inverse matrix by brute force, the simple change suffices, and the inverse transformation is
The boosts along the Cartesian directions can be readily obtained, for example the unit vector in the x direction has components and . Looking at the patterns in the boost matrices along the Cartesian directions, the general boost matrix can be systematically rewritten by introducing
Collecting these into a vector of matrices , the matrix and its square allow the compact expression
or in the rapidity parametrization
which resembles Rodrigues' rotation formula for spatial rotations.
The matrices make one or more successive transformations easier to handle, rather than rotely iterating the transformations to obtain the result of more than one transformation. For two boosts along the same direction, the result is another boost, and rapidity provides a natural way to handle this. If a frame is boosted with rapidity relative to frame in direction , and another frame is boosted with rapidity relative to along the same direction, the separate boosts are
then is the rapidity of the overall boost of relative to in the same direction as ,
Moreover, the relative velocities are related to the rapidities by
and the hyperbolic identity
coincides with the resultant relative velocity of the two relative velocities along the same direction. Thus rapidities add if the boosts are collinear as they are here, while the relative velocities do not. The relative velocities can be in the same or opposite directions, but must be collinear.
For two or more consecutive boosts that are not collinear but in different directions, the result is still a Lorentz transformation, but not a single boost. Also, Lorentz boosts along different directions do not commute, changing their order changes the resultant transformation. The non-commutativity of Lorentz boosts is another unintuitive feature of special relativity that is unlike Galilean relativity. In Newtonian mechanics, any pair of Galilean boosts can be performed in either order, and both results are the same Galilean transformation.
The most general proper Lorentz transformation also contains a rotation of the three axes, because the composition of two boosts is not a pure boost but is a boost followed or preceded by a rotation. The rotation is the Wigner rotation, and gives rise to the Thomas precession. The boost is given by a symmetric matrix, but the general Lorentz transformation matrix need not be symmetric. Explicit formulae for the composite transformation matrices are given in the linked article.
Rotation matrix.
A rotation on the spatial coordinates only, leaving the time coordinate alone, leaves the spacetime interval invariant. Therefore, ordinary spatial rotations are also Lorentz transformations. The 4d matrix is simply
where is a 3d rotation matrix. For the purposes of this article the axis-angle representation will be used here, and the "axis-angle vector" is a useful definition; the angle multiplied by a unit vector parallel to the axis. The inverse of corresponds to rotations using the same axis and angle, but in the opposite sense. The rotation matrix is orthogonal, so the transpose equals the inverse,
Looking at the patterns in the rotation matrices about the Cartesian axes, it is useful to introduce the matrices
Collecting these into a vector , these matrices allow the 4d rotation matrix to be expressed in the Rodrigues quadratic,
In this article, the "right-handed" convention for the spatial coordinates is used (see orientation (vector space)), so that rotations are positive in the anticlockwise sense according to the right-hand rule, and negative in the clockwise sense. This matrix rotates any 3d vector about the axis through angle anticlockwise (an active transformation), which has the equivalent effect of rotating the coordinate frame clockwise about the same axis through the same angle (a passive transformation).
Introduction to the Lorentz group.
It is a result of special relativity that the quantity
is an invariant, where is the Minkowski metric as a square matrix
and the coordinates change under a Lorentz transformation
where Λ is a constant square matrix. Boosts and rotations themselves are Lorentz transformations since each operation leaves the spacetime interval invariant, and the composition of any two is also a Lorentz transformation. Specifically, two pure rotations (without boosts) is a rotation, but two pure boosts (without rotations) is generally a boost followed or preceded by a rotation.
The set of all Lorentz transformations Λ is denoted formula_57. This set together with matrix multiplication forms a group, in this context known as the "Lorentz group". Also, the above expression is a quadratic form of signature (3,1) on spacetime, and the group of transformations which leaves this quadratic form invariant is the indefinite orthogonal group O(3,1), a Lie group. In other words, the Lorentz group is O(3,1). As presented in this article, any Lie groups mentioned are matrix Lie groups. In this context the operation of composition amounts to matrix multiplication.
For the specific cases that Λ is a boost, rotation, or both, there is an additional detail; the determinant of any boost or rotation matrix is +1. The group of Lorentz transformations consisting only of boosts and rotations is called the "restricted Lorentz group", and is the special indefinite orthogonal group SO(3,1).
However, Λ is not limited to boosts and rotations. Other Lorentz transformations may have a determinant of opposite sign and other properties, for example any boosts and/or rotation, combined with parity inversion and/or time reversal, will also leave the above quadratic form invariant. The other transformations are outlined later.
In fact, the above transformation does not include all the symmetries in spacetime. For the spacetime interval to be invariant, it can be shown that it is necessary and sufficient for the coordinate transformation to be of the form
where "C" is a constant column containing translations in time and space. If "C" ≠ 0, this is an inhomogenous Lorentz transformation or Poincaré transformation. If "C" = 0, this is a homogeneous Lorentz transformation.
The set of Poincaré transformations also satisfy the properties of a group and is called the Poincaré group or inhomogeneous Lorentz group. The extra translations mean the Poincaré group is not O(3,1), details are given in the linked article. Under the Erlangen program, Minkowski space can be viewed as the geometry defined by the Poincaré group, which combines Lorentz transformations with translations. This is the full symmetry of special relativity.
Generators and parameters of the homogeneous Lorentz group.
The axis-angle vector and rapidity vector are altogether six continuous variables which make up the group parameters (in this particular representation), and and are the corresponding six generators of the group.
Physically, the generators of the Lorentz group are operators that correspond to important symmetries in spacetime: are the "rotation generators" which correspond to angular momentum, and are the "boost generators" which correspond to the motion of the system in spacetime.
Lorentz generators can be added together, or multiplied by real numbers, to get more Lorentz generators. For example,
is a generator. Therefore, the set of all Lorentz generators
together with the operations of ordinary matrix addition and multiplication of a matrix by a number, forms a vector space over the real numbers. The generators form a basis set of "V", and the components of the axis-angle and rapidity vectors, , are the coordinates of a Lorentz generator with respect to this basis.
Three of the commutation relations of the Lorentz generators are
where the bracket is a binary operation known as the "commutator", and the other relations can be found by taking cyclic permutations of x, y, z components (i.e. change x to y, y to z, and z to x, repeat).
These commutation relations, and the vector space of generators, fulfill the definition of the Lie algebra . In summary, a Lie algebra is defined as a vector space "V" over a field of numbers, and with a binary operation [ , ] (called a Lie bracket in this context) on the elements of the vector space, satisfying the axioms of bilinearity, alternatization, and the Jacobi identity. Here the operation [ , ] is the commutator which satisfies all of these axioms, the vector space is the set of Lorentz generators "V" as given previously, and the field is the set of real numbers.
The exponential map (Lie theory) from the Lie algebra to the Lie group,
provides a one-to-one correspondence between small enough neighborhoods of the origin of the Lie algebra and neighborhoods of the identity element of the Lie group. It the case of the Lorentz group, the exponential map is just the matrix exponential. Globally, the exponential map is not one-to-one, but in the case of the Lorentz group, it is surjective (onto). Hence any group element can be expressed as an exponential of an element of the Lie algebra.
To see the exponential mapping heuristically, consider the infinitesimal Lorentz boost in the x direction for simplicity (the generalization to any direction follows an almost identical procedure). The infinitesimal transformation a small boost away from the identity, obtained by the Taylor expansion of the boost matrix to first order about ,
where the higher order terms not shown are negligible because is small. The derivative of the matrix is the matrix of the entries differentiated with respect to the same variable (see matrix calculus), and it is understood the derivatives are found first then evaluated at , which turn out to give
The derivative of any smooth curve with in the group depending on some group parameter with respect to that group parameter, evaluated at , serves as a definition of a corresponding group generator , and this reflects an infinitesimal transformation away from the identity. The smooth curve can always be taken as an exponential as the exponential will always map smoothly back into the group via for all ; this curve will yield again when differentiated at . In other words, linking terminology used in mathematics and physics: A group generator is any element of the Lie algebra. A group parameter is a component of a coordinate vector representing an arbitrary element of the Lie algebra with respect to some basis. A basis, then, is a set of generators being a basis of the Lie algebra in the usual vector space sense.
In the limit of an infinite number of infinitely small steps, the finite boost transformation in the form of a matrix exponential is obtained
where the limit definition of the exponential has been used (see also characterizations of the exponential function).
Almost identical results appear for the other Cartesian directions, and the general boost matrix is
similarly the general rotation matrix is
and the general Lorentz transformation is
This is in general a product of a rotation and a boost, but the decomposition of a general Lorentz transformation into such factors is nontrivial. In particular,
because the generators do not commute. For a description of how to find the factors of a general Lorentz transformation in terms of a boost and a rotation "in principle" (this usually does not yield an intelligible expression in terms of generators and ), see Wigner rotation. If, on the other hand, "the decomposition is given" in terms of the generators, and one wants to find the product in terms of the generators, then the Baker–Campbell–Hausdorff formula applies.
Generators and parameters of the inhomogeneous Lorentz group.
For inhomogenous Lorentz transformations, the additional generators are the components of the four-momentum: energy is the generator of time translation, and the 3d momentum components are the generators of spatial translations in those directions. The extra parameters corresponding to these generators are displacements in space and time. The commutation relations are enlarged to include the momenta with the boost and rotation generators.
Classification of the homogeneous Lorentz group.
From the invariance of the spacetime interval it follows immediately
and this matrix equation contains the general conditions on the Lorentz transformation to ensure invariance of the spacetime interval. Taking the determinant of the equation using the product rule gives immediately
Writing the Minkowski metric as a block matrix, and the Lorentz transformation in the most general form,
carrying out the block matrix multiplications obtains general conditions on to ensure relativistic invariance. Not much information can be directly extracted from all the conditions, however one of the results 
is useful; always so it follows that
The negative inequality may be unexpected, because multiplies the time coordinate and this has an effect on time symmetry. If the positive equality holds, then is the Lorentz factor.
The determinant and inequality provide four ways to classify Lorentz transformations (herein LTs for brevity). However, any particular LT has only one determinant sign "and" only one inequality. There are four sets which include every possible pair given by the intersections ("n"-shaped symbol meaning "and") of these classifying sets. In set notation the four sets and their intersections are:
where "+" and "−" indicate the determinant sign, while "↑" for ≥ and "↓" for ≤ denote the inequalities. 
The full Lorentz group splits into the union ("u"-shaped symbol meaning "or") of four disjoint sets
A subgroup of a group must be closed under the same operation of the group (here matrix multiplication). In other words, for two Lorentz transformations and from a particular set, the composite Lorentz transformations and must return to the same set and came from. This will not always be the case; it can be shown that the composition of "any" two Lorentz transformations always has the positive determinant and positive inequality, a proper orthochronous transformation.
The orthochronous, proper, proper orthochronous sets of LTs are all subgroups. Another subgroup is the union of proper orthochronous and improper antichronous sets, formula_78. Rotations and boosts are elements of the proper orthochronous Lorentz group. 
The other sets involving the improper and/or antichronous properties do not form subgroups, because the composite transformation always has a positive determinant or inequality, whereas the original separate transformations will have negative determinants and/or inequalities. However, the elements of these sets can be expressed in terms of proper orthochronous transformations with appropriate parity inversion and/or time reversal . These are in matrix form
so if is proper orthochronous, then is improper antichronous, is improper orthochronous, and is proper antichronous.
Tensor formulation.
Contravariant vectors.
Writing the general matrix transformation of coordinates as the matrix equation
allows the transformation of other physical quantities that cannot be expressed as four-vectors, e.g., tensors or spinors of any order in 4d spacetime, to be defined. In the corresponding tensor index notation, the above matrix expression is
where upper and lower indices label covariant and contravariant components respectively, and the summation convention is applied. It is a standard convention to use Greek indices that take the value 0 for time components, and 1, 2, 3 for space components, while Latin indices simply take the values 1, 2, 3, for spatial components. Note that the first index (reading left to right) corresponds in the matrix notation to a "row index". The second index corresponds to the column index.
The transformation matrix is universal for all four-vectors, not just 4-dimensional spacetime coordinates. If is any four-vector, then in tensor index notation
Alternatively, one writes
in which the primed indices denote the indices of A in the primed frame. This notation cuts risk of exhausting the Greek alphabet roughly in half.
For a general -component object one may write
where is the appropriate representation of the Lorentz group, an matrix for every . In this case, the indices should "not" be thought of as spacetime indices (sometimes called Lorentz indices), and they run from to . E.g. if is a bispinor, then the indices are called "Dirac indices".
Covariant vectors.
There are also vector quantities with covariant indices. They are generally obtained from their corresponding objects with contravariant indices by the operation of "lowering an index", e.g.
where is the metric tensor. (The linked article also provides more information about what the operation of raising and lowering indices really is mathematically.) The inverse of this transformation is given by
where, when viewed as matrices, is the inverse of . As it happens, . This is referred to as "raising an index". To transform a covariant vector , first raise its index, then transform it according to the same rule as for contravariant -vectors, then finally lower the index;
But
i. e. it is the -component of the "inverse" Lorentz transformation. One defines (as a matter of notation),
and may in this notation write
Now for a subtlety. The implied summation on the right hand side of
is running over "a row index" of the matrix representing . Thus, in terms of matrices, this transformation should be thought of as the "inverse transpose" of acting on the column vector . That is, in pure matrix notation,
This means exactly that covariant vectors (thought of as column matrices) transform according to the dual representation of the standard representation of the Lorentz group. This notion generalizes to general representations, simply replace with .
Tensors.
If and are linear operators on vector spaces and , then a linear operator may be defined on the tensor product of and , denoted according to
From this it is immediately clear that if and are a four-vectors in , then transforms as
The second step uses the bilinearity of the tensor product and the last step defines a 2-tensor on component form, or rather, it just renames the tensor .
These observations generalize in an obvious way to more factors, and using the fact that a general tensor on a vector space can be written as a sum of a coefficient (component!) times tensor products of basis vectors and basis convectors, one arrives at the transformation law for any tensor quantity . It is given by
where is defined above. This form can generally be reduced to the form for general -component objects given above with a single matrix () operating on column vectors. This latter form is sometimes preferred, e. g. for the electromagnetic field tensor.
Transformation of the electromagnetic field.
Lorentz transformations can also be used to illustrate that the magnetic field and electric field are simply different aspects of the same force — the electromagnetic force, as a consequence of relative motion between electric charges and observers. The fact that the electromagnetic field shows relativistic effects becomes clear by carrying out a simple thought experiment.
The electric and magnetic fields transform differently from space and time, but exactly the same way as relativistic angular momentum and the boost vector.
The electromagnetic field strength tensor is given by
in SI units. In relativity, the Gaussian system of units is often preferred over SI units, even in texts whose main choice of units is SI units, because in it the electric field and the magnetic induction have the same units making the appearance of the electromagnetic field tensor more natural. Consider a Lorentz boost in the -direction. It is given by
where the field tensor is displayed side by side for easiest possible reference in the manipulations below.
The general transformation law becomes
For the magnetic field one obtains
For the electric field results
Here, is used. These results can be summarized by
and are independent of the metric signature. For SI units, substitute . refer to this last form as the view as opposed to the "geometric view" represented by the tensor expression
and make a strong point of the ease with which results that are difficult to achieve using the view can be obtained and understood. Only objects that have well defined Lorentz transformation properties (in fact under "any" smooth coordinate transformation) are geometric objects. In the geometric view, the electromagnetic field is a six-dimensional geometric object in "spacetime" as opposed to two interdependent, but separate, 3-vector fields in "space" and "time". The fields (alone) and (alone) do not have well defined Lorentz transformation properties. The mathematical underpinnings are equations and that immediately yield . One should note that the primed and unprimed tensors refer to the "same event in spacetime". Thus the complete equation with spacetime dependence is
Length contraction has an effect on charge density and current density , and time dilation has an effect on the rate of flow of charge (current), so charge and current distributions must transform in a related way under a boost. It turns out they transform exactly like the space-time and energy-momentum four-vectors,
or, in the simpler geometric view,
One says that charge density transforms as the time component of a four-vector. It is a rotational scalar. The current density is a 3-vector.
The Maxwell equations are invariant under Lorentz transformations.
Spinors.
Equation hold unmodified for any representation of the Lorentz group, including the bispinor representation. In one simply replaces all occurrences of by the bispinor representation ,
The above equation could, for instance, be the transformation of a state in Fock space describing two free electrons.
Transformation of general fields.
A general "noninteracting" multi-particle state (Fock space state) in quantum field theory transforms according to the rule
D_{\sigma_1'\sigma_1}^{(j_1)}(W(\Lambda, p_1))D_{\sigma_2'\sigma_2}^{(j_2)}(W(\Lambda, p_2))\cdots
where is the Wigner rotation and is the representation of .

</doc>
<doc id="18406" url="https://en.wikipedia.org/wiki?curid=18406" title="Luminiferous aether">
Luminiferous aether

In the late 19th century, luminiferous aether, aether or ether, meaning light-bearing aether, was the postulated medium for the propagation of light. It was invoked to explain the ability of the apparently wave-based light to propagate through empty space, something that waves should not be able to do. The assumption of a spatial plenum of luminiferous aether, rather than a spatial vacuum, provided the theoretical medium that was required by wave theories of light.
The concept was the topic of considerable debate throughout its history, as it required the existence of an invisible and infinite material with no interaction with physical objects. As the nature of light was explored, especially in the 19th century, the physical qualities required of the aether became increasingly contradictory. By the late 1800s, the existence of the aether was being questioned, although there was no physical theory to replace it.
The negative outcome of the Michelson–Morley experiment suggested that the aether was non-existent. This led to considerable theoretical work to explain the propagation of light without an aether. A major breakthrough was the theory of relativity, which could explain why the experiment failed to see aether, but was more broadly interpreted to suggest that it wasn't needed. The Michelson-Morley experiment, along with the blackbody radiator and photoelectric effect, was a key experiment in the development of modern physics, which includes both relativity and quantum theory, the latter of which explains the wave-like nature of light.
The history of light and aether.
Particles vs. waves.
To Robert Boyle in the 17th century, shortly before Isaac Newton, the aether was a probable hypothesis and consisted of subtle particles, one sort of which explained the absence of vacuum and the mechanical interactions between bodies, and the other sort of which explained phenomena such as magnetism (and possibly gravity) that were inexplicable on the basis of the purely mechanical interactions of macroscopic bodies:
Isaac Newton contended that light was made up of numerous small particles. This could explain such features as light's ability to travel in straight lines and reflect off surfaces. This theory was known to have its problems: although it explained reflection well, its explanation of refraction and diffraction was less satisfactory. In order to explain refraction, Newton's "Opticks" (1704) postulated an "Aethereal Medium" transmitting vibrations "faster" than light, by which light, when overtaken, is put into "Fits of easy Reflexion and easy Transmission", which caused refraction and diffraction. Newton believed that these vibrations were related to heat radiation:
Is not the Heat of the warm Room convey'd through the vacuum by the Vibrations of a much subtiler Medium than Air, which after the Air was drawn out remained in the Vacuum? And is not this Medium the same with that Medium by which Light is refracted and reflected, and by whose Vibrations Light communicates Heat to Bodies, and is put into Fits of easy Reflexion and easy Transmission?
The modern understanding is that heat radiation "is", like light, electromagnetic radiation. However, Newton considered them to be two different phenomena. He believed heat vibrations to be excited "when a Ray of Light falls upon the Surface of any pellucid Body". He wrote, "I do not know what this Aether is", but that if it consists of particles then they must be "exceedingly smaller than those of Air, or even than those of Light: The exceeding smallness of its Particles may contribute to the greatness of the force by which those Particles may recede from one another, and thereby make that Medium exceedingly more rare and elastic than Air, and by consequence exceedingly less able to resist the motions of Projectiles, and exceedingly more able to press upon gross Bodies, by endeavoring to expand itself."
Christiaan Huygens, prior to Newton, had hypothesized that light was a wave propagating through an aether, but Newton rejected this idea. The main reason for his rejection stemmed from the fact that both men could apparently only envision light to be a longitudinal wave, like sound and other mechanical waves in fluids. However, longitudinal waves by necessity have only one form for a given propagation direction, rather than two polarizations as in a transverse wave, and thus they were unable to explain the phenomenon of birefringence, where two polarizations of light are refracted differently by a crystal. Instead, Newton preferred to imagine non-spherical particles, or "corpuscles", of light with different "sides" that give rise to birefringence. A further reason why Newton rejected light as waves in a medium was because such a medium would have to extend everywhere in space, and would thereby "disturb and retard the Motions of those great Bodies" (the planets and comets) and thus "as it medium is of no use, and hinders the Operation of Nature, and makes her languish, so there is no evidence for its Existence, and therefore it ought to be rejected".
Bradley suggests particles.
In 1720 James Bradley carried out a series of experiments attempting to measure stellar parallax by taking measurements of stars at different times of the year. As the Earth moves around the sun, the apparent angle to a given distant spot changes, and by measuring those angles the distance to the star can be calculated based on the known orbital circumference of the Earth around the sun. He failed to detect any parallax, thereby placing a lower limit on the distance to stars.
During these experiments he also discovered a similar effect; the apparent positions of the stars did change over the year, but not as expected. Instead of the apparent angle being maximized when the Earth was at either end of its orbit with respect to the star, the angle was maximized when the Earth was at its fastest sideways velocity with respect to the star. This interesting effect is now known as stellar aberration.
Bradley explained this effect in the context of Newton's corpuscular theory of light, by showing that the aberration angle was given by simple vector addition of the Earth's orbital velocity and the velocity of the corpuscles of light, just as vertically falling raindrops strike a moving object at an angle. Knowing the Earth's velocity and the aberration angle, this enabled him to estimate the speed of light.
To explain stellar aberration in the context of an aether-based theory of light was regarded as more problematic. As the aberration relied on relative velocities, and the measured velocity was dependent on the motion of the Earth, the aether had to be remaining stationary with respect to the star as the Earth moved through it. This meant that the Earth could travel through the aether, a physical medium, with no apparent effect—precisely the problem that led Newton to reject a wave model in the first place.
Waves theory triumphs.
However, a century later, Young and Fresnel revived the wave theory of light when they pointed out that light could be a transverse wave rather than a longitudinal wave — the polarization of a transverse wave (like Newton's "sides" of light) could explain birefringence, and in the wake of a series of experiments on diffraction the particle model of Newton was finally abandoned. Physicists assumed, moreover, that like mechanical waves, light waves required a medium for propagation, and thus required Huygens's idea of an aether "gas" permeating all space.
However, a transverse wave apparently required the propagating medium to behave as a solid, as opposed to a gas or fluid. The idea of a solid that did not interact with other matter seemed a bit odd , and Augustin-Louis Cauchy suggested that perhaps there was some sort of "dragging", or "entrainment", but this made the aberration measurements difficult to understand. He also suggested that the "absence" of longitudinal waves suggested that the aether had negative compressibility. George Green pointed out that such a fluid would be unstable. George Gabriel Stokes became a champion of the entrainment interpretation, developing a model in which the aether might be (by analogy with pine pitch) rigid at very high frequencies and fluid at lower speeds. Thus the Earth could move through it fairly freely, but it would be rigid enough to support light.
Electromagnetism.
In 1856 Wilhelm Eduard Weber and Rudolf Kohlrausch performed an experiment to measure the numerical value of the ratio of the electromagnetic unit of charge to the electrostatic unit of charge. The result came out to be equal to the product of the speed of light and the square root of two. The following year, Gustav Kirchhoff wrote a paper in which he showed that the speed of a signal along an electric wire was equal to the speed of light. These are the first recorded historical links between the speed of light and electromagnetic phenomena.
James Clerk Maxwell began working on Faraday's lines of force. In his 1861 paper "On Physical Lines of Force" he modelled these magnetic lines of force using a sea of molecular vortices that he considered to be partly made of aether and partly made of ordinary matter. He derived expressions for the dielectric constant and the magnetic permeability in terms of the transverse elasticity and the density of this elastic medium. He then equated the ratio of the dielectric constant to the magnetic permeability with a suitably adapted version of Weber and Kohlrausch's result of 1856, and he substituted this result into Newton's equation for the speed of sound. On obtaining a value that was close to the speed of light as measured by Fizeau, Maxwell concluded that light consists in undulations of the same medium that is the cause of electric and magnetic phenomena.
Maxwell had however expressed some uncertainties surrounding the precise nature of his molecular vortices and so he began to embark on a purely dynamical approach to the problem. He wrote another famous paper in 1864 under the title of A Dynamical Theory of the Electromagnetic Field in which the details of the luminiferous medium were less explicit. Although Maxwell did not explicitly mention the sea of molecular vortices, his derivation of Ampère's circuital law was carried over from the 1861 paper and he used a dynamical approach involving rotational motion within the electromagnetic field which he likened to the action of flywheels. Using this approach to justify the electromotive force equation (the precursor of the Lorentz force equation), he derived a wave equation from a set of eight equations which appeared in the paper and which included the electromotive force equation and Ampère's circuital law. Maxwell once again used the experimental results of Weber and Kohlrausch to show that this wave equation represented an electromagnetic wave that propagates at the speed of light, hence supporting the view that light is a form of electromagnetic radiation.
The apparent need for a propagation medium for such Hertzian waves can be seen by the fact that they consist of perpendicular electric (E) and magnetic (B or H) waves. The E waves consist of undulating dipolar electric fields, and all such dipoles appeared to require separated and opposite electric charges. Electric charge is an inextricable property of matter, so it appeared that some form of matter was required to provide the alternating current that would seem to have to exist at any point along the propagation path of the wave. Propagation of waves in a true vacuum would imply the existence of electric fields without associated electric charge, or of electric charge without associated matter. Albeit compatible with Maxwell's equations, electromagnetic induction of electric fields could not be demonstrated in vacuum, because all methods of detecting electric fields required electrically charged matter.
In addition, Maxwell's equations required that all electromagnetic waves in vacuum propagate at a fixed speed, "c". As this can only occur in one reference frame in Newtonian physics (see Galilean-Newtonian relativity), the aether was hypothesized as the absolute and unique frame of reference in which Maxwell's equations hold. That is, the aether must be "still" universally, otherwise "c" would vary along with any variations that might occur in its supportive medium. Maxwell himself proposed several mechanical models of aether based on wheels and gears, and George Francis FitzGerald even constructed a working model of one of them. These models had to agree with the fact that the electromagnetic waves are transverse but never longitudinal.
Problems.
By this point the mechanical qualities of the aether had become more and more magical: it had to be a fluid in order to fill space, but one that was millions of times more rigid than steel in order to support the high frequencies of light waves. It also had to be massless and without viscosity, otherwise it would visibly affect the orbits of planets. Additionally it appeared it had to be completely transparent, non-dispersive, incompressible, and continuous at a very small scale. Maxwell wrote in "Encyclopædia Britannica":
Aethers were invented for the planets to swim in, to constitute electric atmospheres and magnetic effluvia, to convey sensations from one part of our bodies to another, and so on, until all space had been filled three or four times over with aethers... The only aether which has survived is that which was invented by Huygens to explain the propagation of light.
Contemporary scientists were aware of the problems, but aether theory was so entrenched in physical law by this point that it was simply assumed to exist. In 1908 Oliver Lodge gave a speech on behalf of Lord Rayleigh to the Royal Institution on this topic, in which he outlined its physical properties, and then attempted to offer reasons why they were not impossible. Nevertheless, he was also aware of the criticisms, and quoted Lord Salisbury as saying that "aether is little more than a nominative case of the verb "to undulate"". Others criticized it as an "English invention", although Rayleigh jokingly stated it was actually an invention of the Royal Institution.
By the early 20th Century, aether theory was in trouble. A series of increasingly complex experiments had been carried out in the late 19th century to try to detect the motion of the Earth through the aether, and had failed to do so. A range of proposed aether-dragging theories could explain the null result but these were more complex, and tended to use arbitrary-looking coefficients and physical assumptions. Lorentz and FitzGerald offered within the framework of Lorentz ether theory a more elegant solution to how the motion of an absolute aether could be undetectable (length contraction), but if their equations were correct, the new special theory of relativity (1905) could generate the same mathematics without referring to an aether at all. Aether fell to Occam's Razor.
Relative motion between the Earth and aether.
Aether drag.
The two most important models, which were aimed to describe the relative motion of the Earth and aether, were Augustin-Jean Fresnel's (1818) model of the (nearly) stationary aether including a partial aether drag determined by Fresnel's dragging coefficient,
and George Gabriel Stokes' (1844)
model of complete aether drag. The latter theory was not considered as correct, since it was not compatible with the aberration of light, and the auxiliary hypotheses developed to explain this problem were not convincing. Also, subsequent experiments as the Sagnac effect (1913) also showed that this model is untenable. However, the most important experiment supporting Fresnel's theory was Fizeau's 1851 experimental confirmation of Fresnel's 1818 prediction that a medium with refractive index "n" moving with a velocity "v" would increase the speed of light travelling through the medium in the same direction as "v" from "c"/"n" to:
That is, movement adds only a fraction of the medium's velocity to the light (predicted by Fresnel in order to make Snell's law work in all frames of reference, consistent with stellar aberration). This was initially interpreted to mean that the medium drags the aether along, with a "portion" of the medium's velocity, but that understanding became very problematic after Wilhelm Veltmann demonstrated that the index "n" in Fresnel's formula depended upon the wavelength of light, so that the aether could not be moving at a wavelength-independent speed. This implied that there must be a separate aether for each of the infinitely many frequencies.
Negative aether-drift experiments.
The key difficulty with Fresnel's aether hypothesis arose from the juxtaposition of the two well-established theories of Newtonian dynamics and Maxwell's electromagnetism. Under a Galilean transformation the equations of Newtonian dynamics are invariant, whereas those of electromagnetism are not. Basically this means that while physics should remain the same in non-accelerated experiments, light would not follow the same rules because it is travelling in the universal "aether frame". Some effect caused by this difference should be detectable.
A simple example concerns the model on which aether was originally built: sound. The speed of propagation for mechanical waves, the speed of sound, is defined by the mechanical properties of the medium. Sound travels 4.3 times faster in water than in air. This explains why a person hearing an explosion underwater and quickly surfacing can hear it again as the slower travelling sound arrives through the air. Similarly, a traveller on an airliner can still carry on a conversation with another traveller because the sound of words is travelling along with the air inside the aircraft. This effect is basic to all Newtonian dynamics, which says that everything from sound to the trajectory of a thrown baseball should all remain the same in the aircraft flying (at least at a constant speed) as if still sitting on the ground. This is the basis of the Galilean transformation, and the concept of frame of reference.
But the same was not supposed to be true for light, since Maxwell's mathematics demanded a single universal speed for the propagation of light, based, not on local conditions, but on two measured properties, the permittivity and permeability of free space, that were assumed to be the same throughout the universe. If these numbers did change, there should be noticeable effects in the sky; stars in different directions would have different colours, for instance .
Thus at any point there should be one special coordinate system, "at rest relative to the aether". Maxwell noted in the late 1870s that detecting motion relative to this aether should be easy enough—light travelling along with the motion of the Earth would have a different speed than light travelling backward, as they would both be moving against the unmoving aether. Even if the aether had an overall universal flow, changes in position during the day/night cycle, or over the span of seasons, should allow the drift to be detected.
First order experiments.
Although the aether is almost stationary according to Fresnel, his theory predicts a positive outcome of aether drift experiments only to "second" order in formula_2, because Fresnel's dragging coefficient would cause a negative outcome of all optical experiments capable of measuring effects to "first" order in formula_2. This was confirmed by the following first-order experiments, which all gave negative results. The following list is based on the description of Wilhelm Wien (1898), with changes and additional experiments according to the descriptions of Edmund Taylor Whittaker (1910) and Jakob Laub (1910):
Besides those optical experiments, also electrodynamic first-order experiments were conducted, which should have led to positive results according to Fresnel. However, Hendrik Antoon Lorentz (1895) modified Fresnel's theory and showed that those experiments can be explained by a stationary aether as well:
Second order experiments.
While the "first"-order experiments could be explained by a modified stationary aether, more precise "second"-order experiments were expected to give positive results, however, no such results could be found.
The famous Michelson–Morley experiment compared the source light with itself after being sent in different directions, looking for changes in phase in a manner that could be measured with extremely high accuracy. The publication of their result in 1887, the null result, was the first clear demonstration that something was seriously wrong with the aether concept of that time (after Michelson's first experiment in 1881 that wasn't fully conclusive). In this case the MM experiment yielded a shift of the fringing pattern of about 0.01 of a fringe, corresponding to a small velocity. However, it was incompatible with the expected aether wind effect due to the Earth's (seasonally varying) velocity which would have required a shift of 0.4 of a fringe, and the error was small enough that the value may have indeed been zero. Therefore, the null hypothesis, the hypothesis that there was no aether wind, could not be rejected. More modern experiments have since reduced the possible value to a number very close to zero, about 10−17.
A series of experiments using similar but increasingly sophisticated apparatuses all returned the null result as well. Conceptually different experiments that also attempted to detect the motion of the aether were the Trouton–Noble experiment (1903), whose objective was to detect torsion effects caused by electrostatic fields, and the experiments of Rayleigh and Brace (1902, 1904), to detect double refraction in various media. However, all of them obtained a null result, like Michelson–Morley (MM) previously did.
These "aether-wind" experiments led to a flurry of efforts to "save" aether by assigning to it ever more complex properties, while only few scientists, like Emil Cohn or Alfred Bucherer, considered the possibility of the abandonment of the aether concept. Of particular interest was the possibility of "aether entrainment" or "aether drag", which would lower the magnitude of the measurement, perhaps enough to explain the results of the Michelson-Morley experiment. However, as noted earlier, aether dragging already had problems of its own, notably aberration. In addition, the interference experiments of Lodge (1893, 1897) and Ludwig Zehnder (1895), aimed to show whether the aether is dragged by various, rotating masses, showed no aether drag. A more precise measurement was made in the Hammar experiment (1935), which ran a complete MM experiment with one of the "legs" placed between two massive lead blocks. If the aether was dragged by mass then this experiment would have been able to detect the drag caused by the lead, but again the null result was achieved. The theory was again modified, this time to suggest that the entrainment only worked for very large masses or those masses with large magnetic fields. This too was shown to be incorrect by the Michelson–Gale–Pearson experiment, which detected the Sagnac effect due to Earth's rotation (s. Aether drag hypothesis).
Another, completely different attempt to save "absolute" aether was made in the Lorentz–FitzGerald contraction hypothesis, which posited that "everything" was affected by travel through the aether. In this theory the reason the Michelson–Morley experiment "failed" was that the apparatus contracted in length in the direction of travel. That is, the light was being affected in the "natural" manner by its travel though the aether as predicted, but so was the apparatus itself, cancelling out any difference when measured. FitzGerald had inferred this hypothesis from a paper by Oliver Heaviside. Without referral to an aether, this physical interpretation of relativistic effects was shared by Kennedy and Thorndike in 1932 as they concluded that the interferometer's arm contracts and also the frequency of its light source "very nearly" varies in the way required by relativity.
Similarly the Sagnac effect, observed by G. Sagnac in 1913, was immediately seen to be fully consistent with special relativity. In fact, the Michelson-Gale-Pearson experiment in 1925 was proposed specifically as a test to confirm the relativity theory, although it was also recognized that such tests, which merely measure absolute rotation, are also consistent with non-relativistic theories.
During the 1920s, the experiments pioneered by Michelson were repeated by Dayton Miller, who publicly proclaimed positive results on several occasions, although not large enough to be consistent with any known aether theory. In any case, other researchers were unable to duplicate Miller's claimed results, and in subsequent years the experimental accuracy of such measurements has been raised by many orders of magnitude, and no trace of any violations of Lorentz invariance has been seen. (A later re-analysis of Miller's results concluded that he had underestimated the variations due to temperature.)
Since the Miller experiment and its unclear results there have been many more experiments to detect the aether. Many of the experimenters have claimed positive results. These results have not gained much attention from mainstream science, since they are in contradiction to a large quantity of high-precision measurements, all of them confirming special relativity.
Lorentz aether theory.
Between 1892 and 1904, Hendrik Lorentz created an electron/aether theory, in which he introduced a strict separation between matter (electrons) and aether. In his model the aether is completely motionless, and it won't be set in motion in the neighborhood of ponderable matter. Contrary to other electron models before, the electromagnetic field of the aether appears as a mediator between the electrons, and changes in this field can propagate not faster than the speed of light. A fundamental concept of Lorentz's theory in 1895 was the "theorem of corresponding states" for terms of order v/c. This theorem states that a moving observer (relative to the aether) makes the same observations as a resting observers, after a suitable change of variables. Lorentz noticed that it was necessary to change the space-time variables when changing frames and introduced concepts like physical length contraction (1892) to explain the Michelson–Morley experiment, and the mathematical concept of local time (1895) to explain the aberration of light and the Fizeau experiment. That resulted in the formulation of the so-called Lorentz transformation by Joseph Larmor (1897, 1900) and Lorentz (1899, 1904), whereby it was noted by Larmor that the complete formulation of local time is accompanied by some sort of time dilation of moving electrons in the aether. As Lorentz later noted (1921, 1928), he considered the time indicated by clocks resting in the aether as "true" time, while local time was seen by him as a heuristic working hypothesis and a mathematical artifice. Therefore, Lorentz's theorem is seen by modern authors as being a mathematical transformation from a "real" system resting in the aether into a "fictitious" system in motion.
The work of Lorentz was mathematically perfected by Henri Poincaré who formulated on many occasions the Principle of Relativity and tried to harmonize it with electrodynamics. He declared simultaneity only a convenient convention which depends on the speed of light, whereby the constancy of the speed of light would be a useful postulate for making the laws of nature as simple as possible. In 1900 and 1904 he physically interpreted Lorentz's local time as the result of clock synchronization by light signals. And finally in June and July 1905 he declared the relativity principle a general law of nature, including gravitation. He corrected some mistakes of Lorentz and proved the Lorentz covariance of the electromagnetic equations. However, he used the notion of an aether as a perfectly undetectable medium and distinguished between apparent and real time, so most historians of science argue that he failed to invent special relativity.
End of aether?
Special Relativity.
Aether theory was dealt another blow when the Galilean transformation and Newtonian dynamics were both modified by Albert Einstein's special theory of relativity, giving the mathematics of Lorentzian electrodynamics a new, "non-aether" context. Unlike most major shifts in scientific thought, special relativity was adopted by the scientific community remarkably quickly, consistent with Einstein's later comment that the laws of physics described by the Special Theory were "ripe for discovery" in 1905. Max Planck's early advocacy of the special theory, along with the elegant formulation given to it by Hermann Minkowski, contributed much to the rapid acceptance of special relativity among working scientists.
Einstein based his theory on Lorentz's earlier work. Instead of suggesting that the mechanical properties of objects changed with their constant-velocity motion through an undetectable aether, Einstein proposed to deduce the characteristics that any successful theory must possess in order to be consistent with the most basic and firmly established principles, independent of the existence of a hypothetical aether. He found that the Lorentz transformation must transcend its connection with Maxwell's equations, and must represent the fundamental relations between the space and time coordinates of inertial frames of reference. In this way he demonstrated that the laws of physics remained invariant as they had with the Galilean transformation, but that light was now invariant as well.
With the development of the special relativity, the need to account for a single universal frame of reference had disappeared — and acceptance of the 19th century theory of a luminiferous aether disappeared with it. For Einstein, the Lorentz transformation implied a conceptual change: that the concept of position in space or time was not absolute, but could differ depending on the observer's location and velocity.
Moreover, in another paper published the same month in 1905, Einstein made several observations on a then-thorny problem, the photoelectric effect. In this work he demonstrated that light can be considered as particles that have a "wave-like nature". Particles obviously do not need a medium to travel, and thus, neither did light. This was the first step that would lead to the full development of quantum mechanics, in which the wave-like nature "and" the particle-like nature of light are both considered to be descriptions of the same thing. A summary of Einstein's thinking about the aether hypothesis, relativity and light quanta may be found in his 1909 (originally German) lecture "The Development of Our Views on the Composition and Essence of Radiation".
Lorentz on his side continued to use the aether concept. In his lectures of around 1911 he pointed out that what "the theory of relativity has to say ... can be carried out independently of what one thinks of the aether and the time". He commented that "whether there is an aether or not, electromagnetic fields certainly exist, and so also does the energy of the electrical oscillations" so that, "if we do not like the name of "aether", we must use another word as a peg to hang all these things upon." He concluded that "one cannot deny the bearer of these concepts a certain substantiality".
Other models.
In later years there have been a few individuals who advocated a neo-Lorentzian approach to physics, which is Lorentzian in the sense of positing an absolute true state of rest that is undetectable and which plays no role in the predictions of the theory. (No violations of Lorentz covariance have ever been detected, despite strenuous efforts.) Hence these theories resemble the 19th century aether theories in name only. For example, the founder of quantum field theory, Paul Dirac, stated in 1951 in an article in Nature, titled "Is there an Aether?" that "we are rather forced to have an aether". However, Dirac never formulated a complete theory, and so his speculations found no acceptance by the scientific community.
Einstein's views on the aether.
In 1916, after Einstein completed his foundational work on general relativity, Lorentz wrote a letter to him in which he speculated that within general relativity the aether was re-introduced. In his response Einstein wrote that one can actually speak about a "new aether", but one may not speak of motion in relation to that aether. This was further elaborated by Einstein in some semi-popular articles (1918, 1920, 1924, 1930).
In 1918 Einstein publicly alluded to that new definition for the first time. Then, in the early 1920s, in a lecture which he was invited to give at Lorentz's university in Leiden, Einstein sought to reconcile the theory of relativity with his mentor's cherished concept of the aether. In this lecture Einstein stressed that special relativity took away the last mechanical property of Lorentz's aether: immobility. However, he continued that special relativity does not necessarily rule out the aether, because the latter can be used to give physical reality to acceleration and rotation. This concept was fully elaborated within general relativity, in which physical properties (which are partially determined by matter) are attributed to space, but no substance or state of motion can be attributed to that "aether" (aether = curved space-time).
In another paper of 1924, named "Concerning the Aether", Einstein argued that Newton's absolute space, in which acceleration is absolute, is the "Aether of Mechanics". And within the electromagnetic theory of Maxwell and Lorentz one can speak of the "Aether of Electrodynamics", in which the aether possesses an absolute state of motion. As regards special relativity, also in this theory acceleration is absolute as in Newton's mechanics. However, the difference from the electromagnetic aether of Maxwell and Lorentz lies in the fact, that ""because it was no longer possible to speak, in any absolute sense, of simultaneous states at different locations in the aether, the aether became, as it were, four dimensional, since there was no objective way of ordering its states by time alone."". Now the "aether of special relativity" is still "absolute", because matter is affected by the properties of the aether, but the aether is not affected by the presence of matter. This asymmetry was solved within general relativity. Einstein explained that the "aether of general relativity" is not absolute, because matter is influenced by the aether, just as matter influences the structure of the aether.
So the only similarity of this relativistic aether concept with the classical aether models lies in the presence of physical properties in space. Therefore, as historians such as John Stachel argue, Einstein's views on the "new aether" are not in conflict with his abandonment of the aether in 1905. For, as Einstein himself pointed out, no "substance" and no state of motion can be attributed to that new aether. In addition, Einstein's use of the word "aether" found little support in the scientific community, and played no role in the continuing development of modern physics.

</doc>
<doc id="18408" url="https://en.wikipedia.org/wiki?curid=18408" title="LAME">
LAME

LAME is a converter that converts audio to the MP3 file format. It encodes audio in the MP3 format, hence it is also considered a codec. LAME is required by some programs (e.g., Audacity) to create MP3 files. LAME is required as an add-on for many Open Source audio programs because the MP3 format is encumbered by software patents. Having these patented procedures (which require licensing in some countries) moved to a separate program allows Open Source programmers to avoid having to worry about those copyright issues.
History.
The name LAME is a recursive acronym for "LAME Ain't an MP3 Encoder". Around mid-1998, Mike Cheng created LAME 1.0 as a set of modifications against the "8Hz-MP3" encoder source code. After some quality concerns raised by others, he decided to start again from scratch based on the "dist10" MPEG reference software sources. His goal was only to speed up the dist10 sources, and leave its quality untouched. That branch (a patch against the reference sources) became Lame 2.0. The project quickly became a team project. Mike Cheng eventually left leadership and started working on tooLAME (an MP2 encoder).
Mark Taylor then started pursuing increased quality in addition to better speed, and released version 3.0 featuring gpsycho, a new psychoacoustic model he developed.
A few key improvements, in chronological order:
Patents and legal issues.
Like all MP3 encoders, LAME implements some technology covered by patents owned by the Fraunhofer Society and other entities. The developers of LAME do not themselves license the technology described by these patents. Distributing compiled binaries of LAME, its libraries, or programs that derive from LAME in countries that recognize those patents may be patent infringing.
The LAME developers state that, since their code is only released in source code form, it should only be considered as an educational description of an MP3 encoder, and thus does not infringe any patent by itself when released as source code only. At the same time, they advise users to obtain a patent license for any relevant technologies that LAME may implement before including a compiled version of the encoder in a product. Some software is released using this strategy: companies use the LAME library, but obtain patent licenses.
In November 2005, there were reports that the Extended Copy Protection rootkit included on some Sony Compact Discs included portions of the LAME library without complying with the terms of the LGPL.

</doc>
<doc id="18414" url="https://en.wikipedia.org/wiki?curid=18414" title="Leszek Miller">
Leszek Miller

Leszek Cezary Miller (born 3 July 1946) is a Polish left-wing politician who served as Prime Minister of Poland from 2001 to 2004. He is the current leader of the Democratic Left Alliance.
Childhood and youth.
Born in Żyrardów, Miller comes from a poor, working-class family: His father was a tailor and his mother a needlewoman. His parents broke up when Leszek was six months old. His father, Florian Miller, a Pole of assimilated German ethnicity, left the family and Leszek has never maintained any contact with him. His mother brought him up in a religious spirit – following her wish, he was even, for some time, an altar boy in their church.
Due to hard life conditions, after graduation from vocational school, 17-year-old Leszek got a job in the Textile Linen Plant in Żyrardów, while continuing his education in the evenings at the Vocational Secondary School of Electric Power Engineering. He soon completed his military service on the ORP Bielik submarine.
In 1969, Miller married Aleksandra, three years his junior, in church. The Millers have a son, Leszek, and a granddaughter, Monika.
Career in the People’s Republic of Poland.
Leszek Miller started his political career as an activist of the Socialist Youth Union, where he held the position of Chairman of the Plant Board, soon becoming a member of the Town Committee. After the military service, in 1969, he joined the Polish United Workers' Party (PZPR), People's Poland's communist party.
Many people were pressured to join PZPR in order to advance in their careers or to pursue higher education. Leszek Miller used his affiliation with the Communist party to effectively advance in his studies and professional goals.
In 1973-1974, Leszek Miller was the Secretary of the PZPR Plant Committee. With granted Party’s recommendation, he started political sciences studies at the Party’s Higher School of Political Sciences (Wyższa Szkoła Nauk Społecznych), graduating in 1977. After graduation, Leszek Miller worked at the PZPR Central Committee, supervising the Group, and later on the Department of Youth, Physical Education and Tourism.
In July 1986, Leszek Miller was elected the 1st Secretary of the PZPR Provincial Committee in Skierniewice. In December 1988, he returned to Warsaw, due to his promotion to the position of the Secretary of the PZPR Central Committee.
As a representative of the government side, he took part in the session of the historic “Round Table”, where, together with Andrzej Celiński, he co-chaired the sub-team for youth issues (the only one that closed the session without signing the agreement). In 1989, he became member of the PZPR Political Bureau.
The Third Republic of Poland.
After the PZPR was dissolved, Leszek Miller became a co-founder of the Social Democracy of the Polish Republic (till March 1993, he was Secretary General, then Deputy Chairman and, from December 1997, the Chairman of that party). In December 1999, at the Founding Congress of the Democratic Left Alliance (SLD), he was elected its Chairman, holding the function continuously till February 2004. In 1997-2001 he was the Chairman of the SLD’s caucus.
In 1989, he ran unsuccessfully for Senate as a representative of the Skierniewice Province. In subsequent elections (1991), Leszek Miller was a leader on the election list of the Social Democracy of the Polish Republic in Łódź and, following a considerable success in elections, he won a seat in the Sejm, becoming Chairman of the Parliamentary Group of the Social Democracy of the Polish Republic. In three subsequent elections to the Sejm, he ran all the time from Łódź, each time gaining more and more votes (from 50 thousand in 1991 up to 146 thousand in 2001); he held a seat in Parliament till 2005.
Through all that time he remained one of the leading politicians on the left wing. In early 90’s, together with Mieczysław Rakowski, he was suspected in the case of the, so-called, “Moscow loan”. After revealing that affair in 1991, Włodzimierz Cimoszewicz called Miller to abstain from taking an MP’s oath due to accusations laid against him. When Leszek Miller got cleared of the charges, Prime Minister Cimoszewicz appointed him later as the Minister in Charge of the Office of the Council of Ministers and in 1997 the Minister of Internal Affairs and Administration in his government. In turn, Cimoszewicz became the Minister of Foreign Affairs in Leszek Miller’s cabinet.
In 1993-1996, Miller was the Minister of Labour and Social Policy in the governments of Waldemar Pawlak and Józef Oleksy respectively. In 1996, he was nominated as Senior Minister in charge of the Office of the Council of Ministers. He then got the nickname “The Chancellor”.
Leszek Miller played an important role in concluding the case of Colonel Ryszard Kukliński, for which he was severely criticised within his political circle. A similar disapproval was expressed after Miller’s support for the Concordat and the candidature of Prof. Leszek Balcerowicz to the position of President of the National Bank of Poland.
During the period of the Solidarity Electoral Action’s government, Leszek Miller was in charge of the parliamentary opposition, leading the political fight with the governing party. He was also consolidating the majority of significant left-wing groups around his person. In 1999, he succeeded in establishing one uniform political party – the Democratic Left Alliance – which turned out to be very successful in following elections.
Prime minister.
Following the victory of the Left (41% vs. 12% of the subsequent party) in the Parliamentary Election in 2001, on 19 October 2001, President Aleksander Kwaśniewski appointed Miller Prime Minister and obliged to nominate the government. The new government won the parliamentary vote of confidence on 26 October 2001 (306:140 votes with one abstention). The 16-person cabinet of Prime Minister Miller has been the smallest government of the Polish Republic so far.
Leszek Miller’s government faced a difficult economic situation in Poland, including an unemployment rate above 18%, a high level of public debt, and economic stagnation. At the end of Miller’s term, economic growth exceeded 6%; still, it was too slow to reduce the unemployment rate. During his term, the unpopular program of cuts in public expenses was implemented, together with a hardly successful reform of health care financing. The reforms of the tax system and of the Social Insurance Institution were continued, and the attempt to settle the mass-media market failed. Taxes were significantly lowered – to 19% for companies and for persons running business activity – and the act of freedom in business activity was voted through. A radical, structural reform of secret services was implemented (the State Security Office was dissolved and replaced by the Internal Security Agency and the Intelligence Agency).
Simultaneously, institutional and legal adjustments were continued, resulting from the accession to the European Union. The Accession conditions were negotiated, being the main strategic goal of Miller’s cabinet. On 13 December 2002, at the summit in Copenhagen (Denmark), Prime Minister Leszek Miller completed the negotiations with the European Union. On 16 April 2003 in Athens, Miller, together with Cimoszewicz, signed the Accession Treaty, bringing Poland into the European Union. Miller’s government, in collaboration with various political and social forces, organized the accession referendum with a successful outcome. On 7 and 8 June 2003, 77.45% of the referendum participants voted in favor of Poland’s accession to the European Union. The referendum turn-out reached 58.85%.
Leszek Miller’s government, together with President Kwaśniewski, made a decision (March 2003) to join the international coalition and deploy Polish troops to Iraq, targeting at overthrowing Saddam Hussein’s government. Miller was also a co-signatory of “the letter of 8”, signed by eight European prime ministers, supporting the US position on Iraq. Already in 2002 Miller gave permission to the U.S. government to run a secret CIA prison at Stare Kiejkuty military training center, three hours North of Warsaw. Years later he is facing accusations of acting anti-constitutionally by having tolerated the imprisonment and torture of prisoners.
On 4 December 2003, Leszek Miller suffered injuries in a helicopter crash near Warsaw.
At the end of its term of office, Leszek Miller’s government had the lowest public support of any government since 1989. It was mainly caused by the continuing high unemployment rate, corruption scandals, with Rywingate on top, and by the attempt of fulfilling the plan of reducing social spending (the Hausner’s plan). In result of criticism in his own party, the Democratic Left Alliance, in February 2004, Leszek Miller resigned from chairing the party. Miller was criticized for an excessively liberal approach and for stressing the role of free market mechanisms in economy. He was reproached for his acceptance of a flat tax, which ran counter to the left-wing doctrine. He was also identified with the “chieftain-like style” of leadership. On 26 March 2004, following the decision of the Speaker of the Parliament, Marek Borowski, to found a new dissenting party, the Social Democracy of Poland, Leszek Miller decided to resign from the position of Prime Minister on 2 May 2004, a day after Poland’s accession to the EU. On May 1, 2004, together with President Kwaśniewski, he was in Dublin, taking part in the Grand Ceremony of accession of 10 states, including Poland, to the European Union.
Later career.
In 2005, despite the support of the Łódź Branch of the Democratic Left Alliance, Leszek Miller was not registered on the election list to the Parliament. At the same time, he was offered to run for Senate but refused. Retirement of the old activists was presented in media as “inflow of new blood into the Democratic Left Alliance”. After the election, Leszek Miller became active in journalism, writing mainly for the “Wprost” weekly on liberal economic concepts and current political issues. In the first half of 2005, he stayed at the Woodrow Wilson International Center for Scholars in Washington, D.C., implementing a research project: “Status of the new Poland in the Eastern Europe’s space”.
In September 2007 the former Polish prime minister Leszek Miller become affiliated with Samoobrona, when he decided to run for the Sejm from their lists.

</doc>
<doc id="18420" url="https://en.wikipedia.org/wiki?curid=18420" title="Basis (linear algebra)">
Basis (linear algebra)

A set of vectors in a vector space "V" is called a basis, or a set of basis vectors, if the vectors are linearly independent and every vector in the vector space is a linear combination of this set. In more general terms, a basis is a linearly independent spanning set.
Given a basis of a vector space "V", every element of "V" can be expressed uniquely as a linear combination of basis vectors, whose coefficients are referred to as vector coordinates or components. A vector space can have several distinct sets of basis vectors; however each such set has the same number of elements, with this number being the dimension of the vector space.
Definition.
A basis "B" of a vector space "V" over a field "F" is a linearly independent subset of "V" that spans "V".
In more detail, suppose that "B" = { "v"1, …, "v""n" } is a finite subset of a vector space "V" over a field F (such as the real or complex numbers R or C). Then "B" is a basis if it satisfies the following conditions:
The numbers "a"i are called the coordinates of the vector "x" with respect to the basis "B", and by the first property they are uniquely determined.
A vector space that has a finite basis is called finite-dimensional. To deal with infinite-dimensional spaces, we must generalize the above definition to include infinite basis sets. We therefore say that a set (finite or infinite) "B" ⊂ "V" is a basis, if
The sums in the above definition are all finite because without additional structure the axioms of a vector space do not permit us to meaningfully speak about an infinite sum of vectors. Settings that permit infinite linear combinations allow alternative definitions of the basis concept: see "Related notions" below.
It is often convenient to list the basis vectors in a specific "order", for example, when considering the transformation matrix of a linear map with respect to a basis. We then speak of an ordered basis, which we define to be a sequence (rather than a set) of linearly independent vectors that span "V": see "Ordered bases and coordinates" below.
Properties.
Again, "B" denotes a subset of a vector space "V". Then, "B" is a basis if and only if any of the following equivalent conditions are met:
Every vector space has a basis. The proof of this requires the axiom of choice. All bases of a vector space have the same cardinality (number of elements), called the dimension of the vector space. This result is known as the dimension theorem, and requires the ultrafilter lemma, a strictly weaker form of the axiom of choice.
Also many vector sets can be attributed a standard basis which comprises both spanning and linearly independent vectors.
Standard bases for example:
In R"n", where "e""i" is the "i"th column of the identity matrix.
In P2, where P2 is the set of all polynomials of degree at most 2, is the standard basis.
In M22, where M22 is the set of all 2×2 matrices. and M"m","n" is the 2×2 matrix with a 1 in the "m","n" position and zeros everywhere else.
Change of basis.
Given a vector space "V" over a field "F" and suppose that and are two bases for "V". By definition, if "ξ" is a vector in "V" then for a unique choice of scalars in "F" called the "coordinates of ξ relative to the ordered basis" The vector in "F""n" is called the "coordinate tuple of ξ" (relative to this basis). The unique linear map with for is called the "coordinate isomorphism" for "V" and the basis Thus if and only if .
A set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. The change of basis of any object of the space is related to this matrix. For example, coordinate tuples change with its inverse.
Extending to a basis.
Let "S" be a subset of a vector space "V". To extend "S" to a basis means to find a basis "B" that contains "S" as a subset. This can be done if and only if "S" is linearly independent. Almost always, there is more than one such "B", except in rather special circumstances (i.e. "S" is already a basis, or "S" is empty and "V" has two elements).
A similar question is when does a subset "S" contain a basis. This occurs if and only if "S" spans "V". In this case, "S" will usually contain several different bases.
Example of alternative proofs.
Often, a mathematical result can be proven in more than one way.
Here, using three different proofs, we show that the vectors (1,1) and (−1,2) form a basis for R2.
From the definition of "basis".
We have to prove that these two vectors are linearly independent and that they generate R2.
Part I: If two vectors v,w are linearly independent, then formula_1 (a and b scalars) implies formula_2
To prove that they are linearly independent, suppose that there are numbers a,b such that:
(i.e., they are linearly dependent). Then:<br>
Subtracting the first equation from the second, we obtain:<br>
Adding this equation to the first equation then:<br>
Hence we have linear independence.
Part II: To prove that these two vectors generate R2, we have to let (a,b) be an arbitrary element of R2, and show that there exist numbers r,s ∈ R such that:<br>
Then we have to solve the equations:<br>
Subtracting the first equation from the second, we get:<br>
By the dimension theorem.
Since (−1,2) is clearly not a multiple of (1,1) and since (1,1) is not the zero vector, these two vectors are linearly independent. Since the dimension of R2 is 2, the two vectors already form a basis of R2 without needing any extension.
By the invertible matrix theorem.
Simply compute the determinant
Since the above matrix has a nonzero determinant, its columns form a basis of R2. See: invertible matrix.
Ordered bases and coordinates.
A basis is just a linearly independent "set" of vectors with or without a given ordering. For many purposes it is convenient to work with an ordered basis. For example, when working with a coordinate representation of a vector it is customary to speak of the "first" or "second" coordinate, which makes sense only if an ordering is specified for the basis. For finite-dimensional vector spaces one typically indexes a basis {"v""i"} by the first "n" integers. An ordered basis is also called a frame.
Suppose "V" is an "n"-dimensional vector space over a field F. A choice of an ordered basis for "V" is equivalent to a choice of a linear isomorphism "φ" from the coordinate space F"n" to "V".
"Proof". The proof makes use of the fact that the standard basis of F"n" is an ordered basis.
Suppose first that
is a linear isomorphism. Define an ordered basis {"v""i"} for "V" by
where {e"i"} is the standard basis for F"n".
Conversely, given an ordered basis, consider the map defined by
where "x" = "x"1e1 + "x"2e2 + ... + "x""n"e"n" is an element of F"n". It is not hard to check that "φ" is a linear isomorphism.
These two constructions are clearly inverse to each other. Thus ordered bases for "V" are in 1-1 correspondence with linear isomorphisms F"n" → "V".
The inverse of the linear isomorphism "φ" determined by an ordered basis {"v""i"} equips "V" with "coordinates": if, for a vector "v" ∈ "V", "φ"−1("v") = ("a"1, "a"2...,"a""n") ∈ F"n", then the components "a""j" = "a""j"("v") are the coordinates of "v" in the sense that "v" = "a"1("v") "v"1 + "a"2("v") "v"2 + ... + "a""n"("v") "v""n".
The maps sending a vector "v" to the components "a""j"("v") are linear maps from "V" to F, because of "φ"−1 is linear. Hence they are linear functionals. They form a basis for the dual space of "V", called the dual basis.
Related notions.
Analysis.
In the context of infinite-dimensional vector spaces over the real or complex numbers, the term Hamel basis (named after Georg Hamel) or algebraic basis can be used to refer to a basis as defined in this article. This is to make a distinction with other notions of "basis" that exist when infinite-dimensional vector spaces are endowed with extra structure. The most important alternatives are orthogonal bases on Hilbert spaces, Schauder bases and Markushevich bases on normed linear spaces. The term is also commonly used to mean a basis for the real numbers R as a vector space over the field Q of rational numbers. (In this case, the dimension of R over Q is uncountable, specifically the continuum, the cardinal number 2ℵ0.)
The common feature of the other notions is that they permit the taking of infinite linear combinations of the basis vectors in order to generate the space. This, of course, requires that infinite sums are meaningfully defined on these spaces, as is the case for topological vector spaces – a large class of vector spaces including e.g. Hilbert spaces, Banach spaces or Fréchet spaces.
The preference of other types of bases for infinite-dimensional spaces is justified by the fact that the Hamel basis becomes "too big" in Banach spaces: If "X" is an infinite-dimensional normed vector space which is complete (i.e. "X" is a Banach space), then any Hamel basis of "X" is necessarily uncountable. This is a consequence of the Baire category theorem. The completeness as well as infinite dimension are crucial assumptions in the previous claim. Indeed, finite-dimensional spaces have by definition finite bases and there are infinite-dimensional ("non-complete") normed spaces which have countable Hamel bases. Consider formula_17, the space of the sequences formula_18 of real numbers which have only finitely many non-zero elements, with the norm formula_19 Its standard basis, consisting of the sequences having only one non-zero element, which is equal to 1, is a countable Hamel basis.
Example.
In the study of Fourier series, one learns that the functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are an "orthogonal basis" of the (real or complex) vector space of all (real or complex valued) functions on the interval 2π that are square-integrable on this interval, i.e., functions "f" satisfying
The functions {1} ∪ { sin("nx"), cos("nx") : "n" = 1, 2, 3, ... } are linearly independent, and every function "f" that is square-integrable on 2π is an "infinite linear combination" of them, in the sense that
for suitable (real or complex) coefficients "a""k", "b""k". But many square-integrable functions cannot be represented as "finite" linear combinations of these basis functions, which therefore "do not" comprise a Hamel basis. Every Hamel basis of this space is much bigger than this merely countably infinite set of functions. Hamel bases of spaces of this kind are typically not useful, whereas orthonormal bases of these spaces are essential in Fourier analysis.
Geometry.
The geometric notions of an affine space, projective space, convex set, and cone have related notions of "basis. An affine basis for an "n"-dimensional affine space is formula_22 points in general linear position). A "' is formula_23 points in general position, in a projective space of dimension "n"). A ' of a polytope is the set of the vertices of its convex hull. A ' consists of one point by edge of a polygonal cone. See also a Hilbert basis (linear programming).
Proof that every vector space has a basis.
Let V be any vector space over some field F. Every vector space must contain at least one element: the zero vector 0.
Note that if V = {0}, then the empty set is a basis for V. Now we consider the case where V contains at least one nonzero element, say v.
Define the set X as all linear independent subsets of V. Note that since V contains the nonzero element v, the singleton subset L = {v} of V is necessarily linearly independent.
Hence the set X contains at least the subset L = {v}, and so X is nonempty.
We let X be partially ordered by inclusion: If L1 and L2 belong to X, we say that L1 ≤ L2 when L1 ⊂ L2. It is easy to check that (X, ≤) satisfies the definition of a partially ordered set.
We now note that if Y is a subset of X that is totally ordered by ≤, then the union LY of all the elements of Y (which are themselves certain subsets of V) is an upper bound for Y. To show this, it is necessary to verify both that a) LY belong to X, and that b) every element L of Y satisfies L ≤ LY. Both a) and b) are easy to check.
Now we apply Zorn's lemma, which asserts that because X is nonempty, and every totally ordered subset of the partially ordered set (X, ≤) has an upper bound, it follows that X has a maximal element. (In other words, there exists some element Lmax of X satisfying the condition that whenever Lmax ≤ L for some element L of X, then L = Lmax.)
Finally we claim that Lmax is a basis for V. Since Lmax belong to X, we already know that Lmax is a linearly independent subset of V.
Now suppose Lmax does not span V. Then there exists some vector w of V that cannot be expressed as a linearly combination of elements of Lmax (with coefficients in the field F). Note that such a vector w cannot be an element of Lmax.
Now consider the subset Lw of V defined by Lw = Lmax ∪ {w}. It is easy to see that a) Lmax ≤ Lw (since Lmax is a subset of Lw), and that b) Lmax ≠ Lw (because Lw contains the vector w that is not contained in Lmax).
But the combination of a) and b) above contradict the fact that Lmax is a maximal element of X, which we have already proved. This contradiction shows that the assumption that Lmax does not span V was not true.
Hence Lmax does span V. Since we also know that Lmax is linearly independent over the field F, this verifies that Lmax is a basis for V. Which proves that the arbitrary vector space V has a basis.
Note: This proof relies on Zorn's lemma, which is logically equivalent to the Axiom of Choice. It turns out that, conversely, the assumption that every vector space has a basis can be used to prove the Axiom of Choice. Thus the two assertions are logically equivalent.

</doc>
<doc id="18422" url="https://en.wikipedia.org/wiki?curid=18422" title="Linear algebra">
Linear algebra

Linear algebra is the branch of mathematics concerning vector spaces and linear mappings between such spaces. It includes the study of lines, planes, and subspaces, but is also concerned with properties common to all vector spaces.
The set of points with coordinates that satisfy a linear equation forms a hyperplane in an "n"-dimensional space. The conditions under which a set of "n" hyperplanes intersect in a single point is an important focus of study in linear algebra. Such an investigation is initially motivated by a system of linear equations containing several unknowns. Such equations are naturally represented using the formalism of matrices and vectors.
Linear algebra is central to both pure and applied mathematics. For instance, abstract algebra arises by relaxing the axioms of a vector space, leading to a number of generalizations. Functional analysis studies the infinite-dimensional version of the theory of vector spaces. Combined with calculus, linear algebra facilitates the solution of linear systems of differential equations.
Techniques from linear algebra are also used in analytic geometry, engineering, physics, natural sciences, computer science, computer animation, and the social sciences (particularly in economics). Because linear algebra is such a well-developed theory, nonlinear mathematical models are sometimes approximated by linear models.
History.
The study of linear algebra first emerged from the study of determinants, which were used to solve systems of linear equations. Determinants were used by Leibniz in 1693, and subsequently, Gabriel Cramer devised Cramer's Rule for solving linear systems in 1750. Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination, which was initially listed as an advancement in geodesy.
The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his “Theory of Extension” which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants".
In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra". The first modern and more precise definition of a vector space was introduced by Peano in 1888; by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra. The use of matrices in quantum mechanics, special relativity, and statistics helped spread the subject of linear algebra beyond pure mathematics. The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations.
The origin of many of these ideas is discussed in the articles on determinants and Gaussian elimination.
Educational history.
Linear algebra first appeared in graduate textbooks in the 1940s and in undergraduate textbooks in the 1950s. Following work by the School Mathematics Study Group, U.S. high schools asked 12th grade students to do "matrix algebra, formerly reserved for college" in the 1960s. In France during the 1960s, educators attempted to teach linear algebra through affine dimensional vector spaces in the first year of secondary school. This was met with a backlash in the 1980s that removed linear algebra from the curriculum. In 1993, the U.S.-based Linear Algebra Curriculum Study Group recommended that undergraduate linear algebra courses be given an application-based "matrix orientation" as opposed to a theoretical orientation.
Scope of study.
Vector spaces.
The main structures of linear algebra are vector spaces. A vector space over a field "F" is a set "V" together with two binary operations. Elements of "V" are called "vectors" and elements of "F" are called "scalars". The first operation, "vector addition", takes any two vectors "v" and "w" and outputs a third vector . The second operation, "scalar multiplication", takes any scalar "a" and any vector "v" and outputs a new . The operations of addition and multiplication in a vector space must satisfy the following axioms. In the list below, let "u", "v" and "w" be arbitrary vectors in "V", and "a" and "b" scalars in "F".
The first four axioms are those of "V" being an abelian group under vector addition. Vector spaces may be diverse in nature, for example, containing functions, polynomials or matrices. Linear algebra is concerned with properties common to all vector spaces.
Linear transformations.
Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure. Given two vector spaces "V" and "W" over a field F, a linear transformation (also called linear map, linear mapping or linear operator) is a map
that is compatible with addition and scalar multiplication:
for any vectors "u","v" ∈ "V" and a scalar "a" ∈ F.
Additionally for any vectors "u", "v" ∈ "V" and scalars "a", "b" ∈ F:
When a bijective linear mapping exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), we say that the two spaces are isomorphic. Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view. One essential question in linear algebra is whether a mapping is an isomorphism or not, and this question can be answered by checking if the determinant is nonzero. If a mapping is not an isomorphism, linear algebra is interested in finding its range (or image) and the set of elements that get mapped to zero, called the kernel of the mapping.
Linear transformations have geometric significance. For example, 2 × 2 real matrices denote standard planar mappings that preserve the origin.
Subspaces, span, and basis.
Again, in analogue with theories of other algebraic objects, linear algebra is interested in subsets of vector spaces that are themselves vector spaces; these subsets are called linear subspaces. For example, both the range and kernel of a linear mapping are subspaces, and are thus often called the range space and the nullspace; these are important examples of subspaces. Another important way of forming a subspace is to take a linear combination of a set of vectors "v"1, "v"2, ..., "vk":
where "a"1, "a"2, ..., "a""k" are scalars. The set of all linear combinations of vectors "v"1, "v"2, ..., "vk" is called their span, which forms a subspace.
A linear combination of any system of vectors with all zero coefficients is the zero vector of "V". If this is the only way to express the zero vector as a linear combination of "v"1, "v"2, ..., "vk" then these vectors are linearly independent. Given a set of vectors that span a space, if any vector "w" is a linear combination of other vectors (and so the set is not linearly independent), then the span would remain the same if we remove "w" from the set. Thus, a set of linearly dependent vectors is redundant in the sense that there will be a linearly independent subset which will span the same subspace. Therefore, we are mostly interested in a linearly independent set of vectors that spans a vector space "V", which we call a basis of "V". Any set of vectors that spans "V" contains a basis, and any linearly independent set of vectors in "V" can be extended to a basis. It turns out that if we accept the axiom of choice, every vector space has a basis; nevertheless, this basis may be unnatural, and indeed, may not even be constructible. For instance, there exists a basis for the real numbers, considered as a vector space over the rationals, but no explicit basis has been constructed.
Any two bases of a vector space "V" have the same cardinality, which is called the dimension of "V". The dimension of a vector space is well-defined by the dimension theorem for vector spaces. If a basis of "V" has finite number of elements, "V" is called a finite-dimensional vector space. If "V" is finite-dimensional and "U" is a subspace of "V", then dim "U" ≤ dim "V". If "U"1 and "U"2 are subspaces of "V", then
One often restricts consideration to finite-dimensional vector spaces. A fundamental theorem of linear algebra states that all vector spaces of the same dimension are isomorphic, giving an easy way of characterizing isomorphism.
Matrix theory.
A particular basis {"v"1, "v"2, ..., "vn"} of "V" allows one to construct a coordinate system in "V": the vector with coordinates ("a"1, "a"2, ..., "an") is the linear combination
The condition that "v"1, "v"2, ..., "vn" span "V" guarantees that each vector "v" can be assigned coordinates, whereas the linear independence of "v"1, "v"2, ..., "vn" assures that these coordinates are unique (i.e. there is only one linear combination of the basis vectors that is equal to "v"). In this way, once a basis of a vector space "V" over F has been chosen, "V" may be identified with the coordinate "n"-space F"n". Under this identification, addition and scalar multiplication of vectors in "V" correspond to addition and scalar multiplication of their coordinate vectors in F"n". Furthermore, if "V" and "W" are an "n"-dimensional and "m"-dimensional vector space over F, and a basis of "V" and a basis of "W" have been fixed, then any linear transformation "T": "V" → "W" may be encoded by an "m" × "n" matrix "A" with entries in the field F, called the matrix of "T" with respect to these bases. Two matrices that encode the same linear transformation in different bases are called similar. Matrix theory replaces the study of linear transformations, which were defined axiomatically, by the study of matrices, which are concrete objects. This major technique distinguishes linear algebra from theories of other algebraic structures, which usually cannot be parameterized so concretely.
There is an important distinction between the coordinate "n"-space R"n" and a general finite-dimensional vector space "V". While R"n" has a standard basis {"e"1, "e"2, ..., "en"}, a vector space "V" typically does not come equipped with such a basis and many different bases exist (although they all consist of the same number of elements equal to the dimension of "V").
One major application of the matrix theory is calculation of determinants, a central concept in linear algebra. While determinants could be defined in a basis-free manner, they are usually introduced via a specific representation of the mapping; the value of the determinant does not depend on the specific basis. It turns out that a mapping has an inverse if and only if the determinant has an inverse (every non-zero real or complex number has an inverse). If the determinant is zero, then the nullspace is nontrivial. Determinants have other applications, including a systematic way of seeing if a set of vectors is linearly independent (we write the vectors as the columns of a matrix, and if the determinant of that matrix is zero, the vectors are linearly dependent). Determinants could also be used to solve systems of linear equations (see Cramer's rule), but in real applications, Gaussian elimination is a faster method.
Eigenvalues and eigenvectors.
In general, the action of a linear transformation may be quite complex. Attention to low-dimensional examples gives an indication of the variety of their types. One strategy for a general n-dimensional transformation "T" is to find "characteristic lines" that are invariant sets under "T". If "v" is a non-zero vector such that "Tv" is a scalar multiple of "v", then the line through 0 and "v" is an invariant set under "T" and "v" is called a characteristic vector or eigenvector. The scalar λ such that "Tv" = λ"v" is called a characteristic value or eigenvalue of "T".
To find an eigenvector or an eigenvalue, we note that
where I is the identity matrix. For there to be nontrivial solutions to that equation, det("T" − λ I) = 0. The determinant is a polynomial, and so the eigenvalues are not guaranteed to exist if the field is R. Thus, we often work with an algebraically closed field such as the complex numbers when dealing with eigenvectors and eigenvalues so that an eigenvalue will always exist. It would be particularly nice if given a transformation "T" taking a vector space "V" into itself we can find a basis for "V" consisting of eigenvectors. If such a basis exists, we can easily compute the action of the transformation on any vector: if "v"1, "v"2, ..., "vn" are linearly independent eigenvectors of a mapping of "n"-dimensional spaces "T" with (not necessarily distinct) eigenvalues λ1, λ2, ..., λ"n", and if "v" = "a"1"v"1 + ... + "an vn", then,
Such a transformation is called a diagonalizable matrix since in the eigenbasis, the transformation is represented by a diagonal matrix. Because operations like matrix multiplication, matrix inversion, and determinant calculation are simple on diagonal matrices, computations involving matrices are much simpler if we can bring the matrix to a diagonal form. Not all matrices are diagonalizable (even over an algebraically closed field).
Inner-product spaces.
Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product. The inner product is an example of a bilinear form, and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an "inner product" is a map
that satisfies the following three axioms for all vectors "u", "v", "w" in "V" and all scalars "a" in "F":
Note that in R, it is symmetric.
We can define the length of a vector "v" in "V" by
and we can prove the Cauchy–Schwarz inequality:
In particular, the quantity
and so we can call this quantity the cosine of the angle between the two vectors.
Two vectors are orthogonal if formula_17. An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly nice to deal with, since if "v" = "a"1 "v"1 + ... + "an vn", then formula_18.
The inner product facilitates the construction of many useful concepts. For instance, given a transform "T", we can define its Hermitian conjugate "T*" as the linear transform satisfying
If "T" satisfies "TT*" = "T*T", we call "T" normal. It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span "V".
Applications.
Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.
Solution of linear systems.
Linear algebra provides the formal setting for the linear combination of equations used in the Gaussian method. Suppose the goal is to find and describe the solution(s), if any, of the following system of linear equations:
The Gaussian-elimination algorithm is as follows: eliminate "x" from all equations below "L"1, and then eliminate "y" from all equations below "L"2. This will put the system into triangular form. Then, using back-substitution, each unknown can be solved for.
In the example, "x" is eliminated from "L"2 by adding (3/2)"L"1 to "L"2. "x" is then eliminated from "L"3 by adding "L"1 to "L"3. Formally:
The result is:
Now "y" is eliminated from "L"3 by adding −4"L"2 to "L"3:
The result is:
This result is a system of linear equations in triangular form, and so the first part of the algorithm is complete.
The last part, back-substitution, consists of solving for the known in reverse order. It can thus be seen that
Then, "z" can be substituted into "L"2, which can then be solved to obtain
Next, "z" and "y" can be substituted into "L"1, which can be solved to obtain
The system is solved.
We can, in general, write any system of linear equations as a matrix equation:
The solution of this system is characterized as follows: first, we find a particular solution "x"0 of this equation using Gaussian elimination. Then, we compute the solutions of "Ax" = 0; that is, we find the null space "N" of "A". The solution set of this equation is given by formula_30. If the number of variables is equal to the number of equations, then we can characterize when the system has a unique solution: since "N" is trivial if and only if det "A" ≠ 0, the equation has a unique solution if and only if det "A" ≠ 0.
Least-squares best fit line.
The least squares method is used to determine the best fit line for a set of data. This line will minimize the sum of the squares of the residuals.
Fourier series expansion.
Fourier series are a representation of a function "f": [−π, π] → R as a trigonometric series:
This series expansion is extremely useful in solving partial differential equations. In this article, we will not be concerned with convergence issues; it is nice to note that all Lipschitz-continuous functions have a converging Fourier series expansion, and nice enough discontinuous functions have a Fourier series that converges to the function value at most points.
The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product
The functions "gn"("x") = sin("nx") for "n" > 0 and "hn"("x") = cos("nx") for "n" ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient "ak", we take the inner product with "hk":
and by orthonormality, formula_34; that is,
Quantum mechanics.
Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics, the physical state of a particle is represented by a vector, and observables (such as momentum, energy, and angular momentum) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L2 (the functions φ: R3 → C such that formula_36 is finite), and it evolves according to the Schrödinger equation. Energy is represented as the operator formula_37, where "V" is the potential energy. "H" is also known as the Hamiltonian operator. The eigenvalues of "H" represents the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of "H". The component of "H" in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
Geometric introduction.
Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two dimensional plane "E". When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.
Point coordinates in the plane "E" are ordered pairs of real numbers, ("x","y"), and a line is defined as the set of points ("x","y") that satisfy the linear equation
where "a", "b" and "c" are not all zero.
Then,
or
where x = ("x", "y", 1) is the 3 × 1 set of homogeneous coordinates associated with the point ("x", "y").
Homogeneous coordinates identify the plane "E" with the "z" = 1 plane in three dimensional space. The x−y coordinates in "E" are obtained from homogeneous coordinates y = ("y"1, "y"2, "y"3) by dividing by the third component (if it is nonzero) to obtain y = ("y"1/"y"3, "y"2/"y"3, 1).
The linear equation, λ, has the important property, that if x1 and x2 are homogeneous coordinates of points on the line, then the point "α"x1 + "β"x2 is also on the line, for any real "α" and "β".
Now consider the equations of the two lines "λ"1 and "λ"2,
which forms a system of linear equations. The intersection of these two lines is defined by x = ("x", "y", 1) that satisfy the matrix equation,
or using homogeneous coordinates,
The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates,
the solutions are multiples of the following solution:
if the rows of B are linearly independent (i.e., "λ"1 and "λ"2 represent distinct lines).
Divide through by x3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns. Notice that this yields a point in the "z" = 1 plane only when the 2 × 2 submatrix associated with "x"3 has a non-zero determinant.
It is interesting to consider the case of three lines, λ1, λ2 and λ3, which yield the matrix equation,
which in homogeneous form yields,
Clearly, this equation has the solution x = (0,0,0), which is not a point on the "z" = 1 plane "E". For a solution to exist in the plane "E", the coefficient matrix "C" must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.
Introduction to linear transformations.
Another way to approach linear algebra is to consider linear functions on the two dimensional real plane "E"=R2. Here R denotes the set of real numbers. Let x=(x, y) be an arbitrary vector in "E" and consider the linear function λ: "E"→R, given by
or
This transformation has the important property that if Ay=d, then
This shows that the sum of vectors in "E" map to the sum of their images in R. This is the defining characteristic of a linear map, or linear transformation. For this case, where the image space is a real number the map is called a linear functional.
Consider the linear functional a little more carefully. Let i=(1,0) and j =(0,1) be the natural basis vectors on "E", so that x=xi+yj. It is now possible to see that
Thus, the columns of the matrix A are the image of the basis vectors of "E" in R.
This is true for any pair of vectors used to define coordinates in "E". Suppose we select a non-orthogonal non-unit vector basis v and w to define coordinates of vectors in "E". This means a vector x has coordinates (α,β), such that x=αv+βw. Then, we have the linear functional
where Av=d and Aw=e are the images of the basis vectors v and w. This is written in matrix form as
Coordinates relative to a basis.
This leads to the question of how to determine the coordinates of a vector x relative to a general basis v and w in "E". Assume that we know the coordinates of the vectors, x, v and w in the natural basis i=(1,0) and j =(0,1). Our goal is two find the real numbers α, β, so that x=αv+βw, that is
To solve this equation for α, β, we compute the linear coordinate functionals σ and τ for the basis v, w, which are given by,
The functionals σ and τ compute the components of x along the basis vectors v and w, respectively, that is,
which can be written in matrix form as
These coordinate functionals have the properties,
These equations can be assembled into the single matrix equation,
Thus, the matrix formed by the coordinate linear functionals is the inverse of the matrix formed by the basis vectors.
Inverse image.
The set of points in the plane "E" that map to the same image in R under the linear functional λ define a line in "E". This line is the image of the inverse map, λ−1: R→"E". This inverse image is the set of the points x=(x, y) that solve the equation,
Notice that a linear functional operates on known values for x=(x, y) to compute a value "c" in R, while the inverse image seeks the values for x=(x, y) that yield a specific value "c".
In order to solve the equation, we first recognize that only one of the two unknowns (x,y) can be determined, so we select y to be determined, and rearrange the equation
Solve for y and obtain the inverse image as the set of points,
For convenience the free parameter x has been relabeled t.
The vector p defines the intersection of the line with the y-axis, known as the y-intercept. The vector h satisfies the homogeneous equation,
Notice that if h is a solution to this homogeneous equation, then "t" h is also a solution.
The set of points of a linear functional that map to zero define the "kernel" of the linear functional. The line can be considered to be the set of points h in the kernel translated by the vector p.
Generalizations and related topics.
Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.
In multilinear algebra, one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space, the vector space "V"∗ consisting of linear maps where "F" is the field of scalars. Multilinear maps can be described via tensor products of elements of "V"∗.
If, in addition to vector addition and scalar multiplication, there is a bilinear vector product , the vector space is called an algebra; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).
Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L"p" spaces.
Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.
Algebraic geometry considers the solutions of systems of polynomial equations.
There are several related topics in the field of Computer Programming that utilizes much of the techniques and theorems Linear Algebra encompasses and refers to.

</doc>
<doc id="18423" url="https://en.wikipedia.org/wiki?curid=18423" title="Labia majora">
Labia majora

The labia majora (singular: "labium majus") are two prominent longitudinal cutaneous folds that extend downward and backward from the mons pubis to the perineum. Together with the labia minora they form the labia of the vulva.
The labia majora is homologous to the male scrotum.
Embryology.
Embryologically, they develop from labioscrotal folds.
Anatomy.
The labia majora constitute the lateral boundaries of the pudendal cleft, which contains the labia minora, interlabial sulci, clitoral hood, clitoral glans, frenulum clitoridis, the Hart's Line, and the vulval vestibule, which contains the external openings of the urethra and the vagina. Each labium majus has two surfaces, an outer, pigmented and covered with strong, pubic hair; and an inner, smooth and beset with large sebaceous follicles. The labia majora are covered with squamous epithelium. Between the two there is a considerable quantity of areolar tissue, fat, and a tissue resembling the dartos tunic of the scrotum, besides vessels, nerves, and glands. The labia majora are thicker in front, and form the anterior labial commissure where they meet below the mons pubis. Posteriorly, they are not really joined, but appear to become lost in the neighboring integument, ending close to, and nearly parallel to, each other. Together with the connecting skin between them, they form another commissure the posterior labial commissure which is also the posterior boundary of the pudendum. The interval between the posterior commissure and the anus, from 2.5 to 3 cm. in length, constitutes the perineum. The anterior region of the perineum is known as the urogenital triangle which separates it from the anal region. Between the labia majora and the inner thighs are the labiocrural folds. Between the labia majora and labia minora are the interlabial sulci. Labia majora atrophy after menopause.
Use in grafting.
The fat pad of the labia majora can be used as a graft, often as a so-called "Martius labial fat pad graft", and can be used, for example, in urethrolysis.

</doc>
<doc id="18424" url="https://en.wikipedia.org/wiki?curid=18424" title="Labia minora">
Labia minora

The labia minora (singular: "labium minus"), also known as the inner labia, inner lips, vaginal lips or nymphae, are two flaps of skin on either side of the human vaginal opening in the vulva, situated between the labia majora (outer labia, or outer lips). Inner lips vary widely in size, color, and shape from individual to individual. 
Structure.
The inner lips extend from the clitoris obliquely downward, laterally, and backward on either side of the vulval vestibule, ending between the bottom of the vulval vestibule and the outer lips. The posterior ends (bottom) of the inner lips are usually joined across the middle line by a fold of skin, named the frenulum of labia minora or fourchette.
On the front, each lip forks dividing into two portions surrounding around the clitoris. The upper part of each lip passes above the clitoris to meet the upper part of the other lip—which will often be a little larger or smaller—forming a fold which overhangs the glans clitoridis; this fold is named the clitoral hood. The lower part passes beneath the glans clitoridis and becomes united to its under surface, forming, with the inner lip of the opposite side, the frenulum clitoridis.
Histology.
On the opposed surfaces of the labia minora are numerous sebaceous glands not associated with hair follicles. They are lined by stratified squamous epithelium on those surfaces.
Variation.
From 2003 to 2004, researchers from the Department of Gynaecology, Elizabeth Garret Anderson Hospital in London, measured the labia and other genital structures of 50 women from the age of 18 to 50, with a mean age of 35.6. The results were:
In female anatomy "macronymphia" is the term used for an abnormally large labia minora commonly found as a racial characteristic in certain ethnic groups such as Khoisans.

</doc>
<doc id="18425" url="https://en.wikipedia.org/wiki?curid=18425" title="Leopold von Sacher-Masoch">
Leopold von Sacher-Masoch

Leopold Ritter von Sacher-Masoch (27 January 1836 – 9 March 1895) was an Austrian writer and journalist, who gained renown for his romantic stories of Galician life. The term masochism is derived from his name.
During his lifetime, Sacher-Masoch was well known as a man of letters, a utopian thinker who espoused socialist and humanist ideals in his fiction and non-fiction. Most of his works remain untranslated into English. The novel "Venus in Furs" was until recently his only book commonly available in English, but an English translation by William Holmes of "Die Gottesmutter" was released in 2015 as "The Mother of God".
Biography.
Early life.
Von Sacher-Masoch was born in the city of Lemberg (now Lviv, Ukraine), the capital of the Kingdom of Galicia and Lodomeria, at the time a province of the Austrian Empire, into the Roman Catholic family of an Austrian civil servant, Leopold Johann Nepomuk Ritter von Sacher, and Charlotte von Masoch, a Ukrainian noblewoman. He later combined his surname with his wife's 'von Masoch', at the request of her family (she was the last of the line). Von Sacher served as a Commissioner of the Imperial Police Forces in Lemberg, and he was recognised with a new title of nobility as Sacher-Masoch awarded by the Austrian Emperor.
Galician storyteller.
Leopold studied law, history and mathematics at Graz University, and after graduating moved back to Lemberg where he became a professor. His early, non-fictional publications dealt mostly with Austrian history. At the same time, Masoch turned to the folklore and culture of his homeland, Galicia. Soon he abandoned lecturing and became a free man of letters. Within a decade his short stories and novels prevailed over his historical non-fiction works, though historical themes continued to imbue his fiction.
Panslavist ideas were prevalent in Masoch's literary work, and he found a particular interest in depicting picturesque types among the various ethnicities that inhabited Galicia. From the 1860s to the 1880s he published a number of volumes of "Jewish Short Stories", "Polish Short Stories", "Galician Short Stories", "German Court Stories" and "Russian Court Stories". His works were published in translation in Ukrainian, Polish, Russian and French.
"The Legacy of Cain".
In 1869, Sacher-Masoch conceived a grandiose series of short stories under the collective title "Legacy of Cain" that would represent the author's aesthetic "Weltanschauung". The cycle opened with the manifesto "The Wanderer" that brought out misogynist themes that became peculiar to Masoch's writings. Of the six planned volumes, only the first two were ever completed. By the middle of the 1880s, Masoch abandoned the "Legacy of Cain". Nevertheless, the published volumes of the series included Masoch's best-known stories, and of them, "Venus in Furs" (1869) is the most famous today. The short novel expressed Sacher-Masoch's fantasies and fetishes (especially for dominant women wearing fur). He did his best to live out his fantasies with his mistresses and wives.
Philosemitism.
Sacher-Masoch edited the Leipzig-based monthly literary magazine "Auf der Höhe. Internationale Review" ("At the Pinnacle. International Review"), which was published from October, 1881 to September, 1885. This was a progressive magazine aimed at tolerance and integration for Jews in Saxony, as well as for the emancipation of women with articles on women's education and suffrage.
In his later years, he worked against local antisemitism through an association for adult education called the "Oberhessischer Verein für Volksbildung" (OVV), founded in 1893 with his second wife, Hulda Meister.
Private life.
On 9 December 1869, Sacher-Masoch and his mistress Baroness Fanny Pistor signed a contract making him her slave for a period of six months, with the stipulation that the Baroness wear furs as often as possible, especially when she was in a cruel mood. Sacher-Masoch took the alias of "Gregor", a stereotypical male servant's name, and assumed a disguise as the servant of the Baroness. The two traveled by train to Italy. As in "Venus in Furs", he traveled in the third-class compartment, while she had a seat in first-class, arriving in Venice (Florence, in the novel), where they were not known, and would not arouse suspicion.
Sacher-Masoch pressured his first wife – Aurora von Rümelin, whom he married in 1873 – to live out the experience of the book, against her preferences. Sacher-Masoch found his family life to be unexciting, and eventually got a divorce and married his assistant.
Later years.
In 1875 Masoch wrote "The Ideals of Our Time", an attempt to give a portrait of German society during its Gründerzeit period.
In his late fifties, his mental health began to deteriorate, and he spent the last years of his life under psychiatric care. According to official reports, he died in Lindheim, Altenstadt, Hesse, in 1895. It is also claimed that he died in an asylum in Mannheim in 1905.
Sacher-Masoch is the great-great-uncle to the British singer and actress Marianne Faithfull on the side of her mother, the Viennese Baroness Eva Erisso.
Masochism.
The term "masochism" was coined in 1886 by the Austrian psychiatrist Richard Freiherr von Krafft-Ebing (1840–1902) in his book "Psychopathia Sexualis":
Sacher-Masoch was not pleased with Krafft-Ebing's assertions. Nevertheless, details of Masoch's private life were obscure until Aurora von Rümelin's memoirs, "Meine Lebensbeichte" (1906), were published in Berlin under the pseudonym Wanda v. Dunajew. The following year, a French translation, "Confession de Ma Vie" (1907) by "Wanda von Sacher-Masoch", was printed in Paris by Mercure de France. An English translation of the French edition was published as "The Confessions of Wanda von Sacher-Masoch" (1991) by RE/Search Publications.

</doc>
<doc id="18426" url="https://en.wikipedia.org/wiki?curid=18426" title="Lithography">
Lithography

Lithography () is a method of printing originally based on the immiscibility of oil and water. The printing is from a stone (lithographic limestone) or a metal plate with a smooth surface. It was invented in 1796 by German author and actor Alois Senefelder as a cheap method of publishing theatrical works. Lithography can be used to print text or artwork onto paper or other suitable material.
Lithography originally used an image drawn with oil, fat, or wax onto the surface of a smooth, level lithographic limestone plate. The stone was treated with a mixture of acid and gum arabic, "etching" the portions of the stone that were not protected by the grease-based image. When the stone was subsequently moistened, these etched areas retained water; an oil-based ink could then be applied and would be repelled by the water, sticking only to the original drawing. The ink would finally be transferred to a blank paper sheet, producing a printed page. This traditional technique is still used in some fine art printmaking applications.
In modern lithography, the image is made of a polymer coating applied to a flexible aluminum plate. The image can be printed directly from the plate (the orientation of the image is reversed), or it can be offset, by transferring the image onto a flexible sheet (rubber) for printing and publication.
As a printing technology, lithography is different from intaglio printing (gravure), wherein a plate is either engraved, etched, or stippled to score cavities to contain the printing ink; and woodblock printing or letterpress printing, wherein ink is applied to the raised surfaces of letters or images. Today, most types of high-volume books and magazines, especially when illustrated in colour, are printed with offset lithography, which has become the most common form of printing technology since the 1960s.
The related term "photolithography" refers to when photographic images are used in lithographic printing, whether these images are printed directly from a stone or from a metal plate, as in offset printing. In fact, "photolithography" is used synonymously with "offset printing". The technique as well as the term were introduced in Europe in the 1850s. Beginning in the 1960s, photolithography has played an important role in the fabrication and mass production of integrated circuits in the microelectronics industry.
The principle of lithography.
Lithography uses simple chemical processes to create an image. For instance, the positive part of an image is a water-repelling ("hydrophobic") substance, while the negative image would be water-retaining ("hydrophilic"). Thus, when the plate is introduced to a compatible printing ink and water mixture, the ink will adhere to the positive image and the water will clean the negative image. This allows a flat print plate to be used, enabling much longer and more detailed print runs than the older physical methods of printing (e.g., intaglio printing, letterpress printing).
Lithography was invented by Alois Senefelder in the Kingdom of Bavaria in 1796. In the early days of lithography, a smooth piece of limestone was used (hence the name "lithography": "lithos" (λιθος) is the ancient Greek word for stone). After the oil-based image was put on the surface, a solution of gum arabic in water was applied, the gum sticking only to the non-oily surface. During printing, water adhered to the gum arabic surfaces and was repelled by the oily parts, while the oily ink used for printing did the opposite.
Lithography on limestone.
Lithography works because of the mutual repulsion of oil and water. The image is drawn on the surface of the print plate with a fat or oil-based medium (hydrophobic) such as a wax crayon, which may be pigmented to make the drawing visible. A wide range of oil-based media is available, but the durability of the image on the stone depends on the lipid content of the material being used, and its ability to withstand water and acid. After the drawing of the image, an aqueous solution of gum arabic, weakly acidified with nitric acid is applied to the stone. The function of this solution is to create a hydrophilic layer of calcium nitrate salt, , and gum arabic on all non-image surfaces. The gum solution penetrates into the pores of the stone, completely surrounding the original image with a hydrophilic layer that will not accept the printing ink. Using lithographic turpentine, the printer then removes any excess of the greasy drawing material, but a hydrophobic molecular film of it remains tightly bonded to the surface of the stone, rejecting the gum arabic and water, but ready to accept the oily ink.
Senefelder had experimented during the early 19th century with multicolor lithography; in his 1819 book, he predicted that the process would eventually be perfected and used to reproduce paintings. Multi-color printing was introduced by a new process developed by Godefroy Engelmann (France) in 1837 known as chromolithography. A separate stone was used for each color, and a print went through the press separately for each stone. The main challenge was to keep the images aligned ("in register"). This method lent itself to images consisting of large areas of flat color, and resulted in the characteristic poster designs of this period.
"Lithography, or printing from soft stone, largely took the place of engraving in the production of English commercial maps after about 1852. It was a quick, cheap process and had been used to print British army maps during the Peninsula War. Most of the commercial maps of the second half of the 19th century were lithographed and unattractive, though accurate enough."
Modern lithographic process.
High-volume lithography is used presently to produce posters, maps, books, newspapers, and packaging—just about any smooth, mass-produced item with print and graphics on it. Most books, indeed all types of high-volume text, are now printed using offset lithography.
For offset lithography, which depends on photographic processes, flexible aluminum, polyester, mylar or paper printing plates are used instead of stone tablets. Modern printing plates have a brushed or roughened texture and are covered with a photosensitive emulsion. A photographic negative of the desired image is placed in contact with the emulsion and the plate is exposed to ultraviolet light. After development, the emulsion shows a reverse of the negative image, which is thus a duplicate of the original (positive) image. The image on the plate emulsion can also be created by direct laser imaging in a CTP (Computer-To-Plate) device known as a platesetter. The positive image is the emulsion that remains after imaging. Non-image portions of the emulsion have traditionally been removed by a chemical process, though in recent times plates have come available that do not require such processing.
The plate is affixed to a cylinder on a printing press. Dampening rollers apply water, which covers the blank portions of the plate but is repelled by the emulsion of the image area. Hydrophobic ink, which is repelled by the water and only adheres to the emulsion of the image area, is then applied by the inking rollers.
If this image were transferred directly to paper, it would create a mirror-type image and the paper would become too wet. Instead, the plate rolls against a cylinder covered with a rubber "blanket", which squeezes away the water, picks up the ink and transfers it to the paper with uniform pressure. The paper passes between the blanket cylinder and a counter-pressure or impression cylinder and the image is transferred to the paper. Because the image is first transferred, or "offset" to the rubber blanket cylinder, this reproduction method is known as "offset lithography" or "offset printing".
Many innovations and technical refinements have been made in printing processes and presses over the years, including the development of presses with multiple units (each containing one printing plate) that can print multi-color images in one pass on both sides of the sheet, and presses that accommodate continuous rolls ("webs") of paper, known as web presses. Another innovation was the continuous dampening system first introduced by Dahlgren instead of the old method which is still used on older presses (conventional dampening), which are rollers covered with molleton (cloth) that absorbs the water. This increased control of the water flow to the plate and allowed for better ink and water balance. Current dampening systems include a "delta effect or vario," which slows the roller in contact with the plate, thus creating a sweeping movement over the ink image to clean impurities known as "hickies".
The process of lithography printing is illustrated by this simplified diagram. This press is also called an ink pyramid because the ink is transferred through several layers of rollers with different purposes. Fast lithographic 'web' printing presses are commonly used in newspaper production.
The advent of desktop publishing made it possible for type and images to be modified easily on personal computers for eventual printing by desktop or commercial presses. The development of digital imagesetters enabled print shops to produce negatives for platemaking directly from digital input, skipping the intermediate step of photographing an actual page layout. The development of the digital platesetter during the late 20th century eliminated film negatives altogether by exposing printing plates directly from digital input, a process known as computer to plate printing.
Microlithography and nanolithography.
Microlithography and nanolithography refer specifically to lithographic patterning methods capable of structuring material on a fine scale. Typically, features smaller than 10 micrometers are considered microlithographic, and features smaller than 100 nanometers are considered nanolithographic. Photolithography is one of these methods, often applied to semiconductor manufacturing of microchips. Photolithography is also commonly used for fabricating Microelectromechanical systems (MEMS) devices. Photolithography generally uses a pre-fabricated photomask or reticle as a master from which the final pattern is derived.
Although photolithographic technology is the most commercially advanced form of nanolithography, other techniques are also used. Some, for example electron beam lithography, are capable of much greater patterning resolution (sometimes as small as a few nanometers). Electron beam lithography is also important commercially, primarily for its use in the manufacture of photomasks. Electron beam lithography as it is usually practiced is a form of maskless lithography, in that a mask is not required to generate the final pattern. Instead, the final pattern is created directly from a digital representation on a computer, by controlling an electron beam as it scans across a resist-coated substrate. Electron beam lithography has the disadvantage of being much slower than photolithography.
In addition to these commercially well-established techniques, a large number of promising microlithographic and nanolithographic technologies exist or are being developed, including nanoimprint lithography, interference lithography, X-ray lithography, extreme ultraviolet lithography, magnetolithography and scanning probe lithography. Some of these new techniques have been used successfully for small-scale commercial and important research applications.
Surface-charge lithography, in fact Plasma desorption mass spectrometry can be directly patterned on polar dielectric crystals via pyroelectric effect, 
Diffraction lithography.
Lithography as an artistic medium.
During the first years of the 19th century, lithography had only a limited effect on printmaking, mainly because technical difficulties remained to be overcome. Germany was the main center of production in this period. Godefroy Engelmann, who moved his press from Mulhouse to Paris in 1816, largely succeeded in resolving the technical problems, and during the 1820s lithography was adopted by artists such as Delacroix and Géricault. London also became a center, and some of Géricault's prints were in fact produced there. Goya in Bordeaux produced his last series of prints by lithography—"The Bulls of Bordeaux" of 1828. By the mid-century the initial enthusiasm had somewhat diminished in both countries, although the use of lithography was increasingly favored for commercial applications, which included the prints of Daumier, published in newspapers. Rodolphe Bresdin and Jean-François Millet also continued to practice the medium in France, and Adolf Menzel in Germany. In 1862 the publisher Cadart tried to initiate a portfolio of lithographs by various artists, which was not successful but included several prints by Manet. The revival began during the 1870s, especially in France with artists such as Odilon Redon, Henri Fantin-Latour and Degas producing much of their work in this manner. The need for strictly limited editions to maintain the price had now been realized, and the medium became more accepted.
In the 1890s, color lithography gained success in part by the emergence of Jules Chéret, known as the "father of the modern poster", whose work went on to inspired a new generation of poster designers and painters, most notably Toulouse-Lautrec, and former student of Chéret, Georges de Feure. By 1900 the medium in both color and monotone was an accepted part of printmaking.
During the 20th century, a group of artists, including Braque, Calder, Chagall, Dufy, Léger, Matisse, Miró, and Picasso, rediscovered the largely undeveloped artform of lithography thanks to the Mourlot Studios, also known as "Atelier Mourlot", a Parisian printshop founded in 1852 by the Mourlot family. The Atelier Mourlot originally specialized in the printing of wallpaper; but it was transformed when the founder's grandson, Fernand Mourlot, invited a number of 20th-century artists to explore the complexities of fine art printing. Mourlot encouraged the painters to work directly on lithographic stones in order to create original artworks that could then be executed under the direction of master printers in small editions. The combination of modern artist and master printer resulted in lithographs that were used as posters to promote the artists' work.
Grant Wood, George Bellows, Alphonse Mucha, Max Kahn, Pablo Picasso, Eleanor Coen, Jasper Johns, David Hockney, Susan Dorothea White and Robert Rauschenberg are a few of the artists who have produced most of their prints in the medium. M. C. Escher is considered a master of lithography, and many of his prints were created using this process. More than other printmaking techniques, printmakers in lithography still largely depend on access to good printers, and the development of the medium has been greatly influenced by when and where these have been established.
As a special form of lithography, the serilith process is sometimes used. Seriliths are mixed media original prints created in a process in which an artist uses the lithograph and serigraph processes. The separations for both processes are hand-drawn by the artist. The serilith technique is used primarily to create fine art limited print editions.

</doc>
<doc id="18430" url="https://en.wikipedia.org/wiki?curid=18430" title="Library management">
Library management

Library management is a sub-discipline of institutional management that focuses on specific issues faced by libraries and library management professionals. Library management encompasses normal management tasks as well as intellectual freedom, anti-censorship, and fundraising tasks. Issues faced in library management frequently overlap those faced in management of non-profit organizations.
Basic functions.
Basic tasks in library management include:
Common library construct.
Most physical libraries that store solid media, such as books, articles, film, and other artifacts, adhere to some derivative of the Dewey Decimal System as their method for tagging, storing, and retrieving artifacts based on unique identifiers. The use of such systems have caused librarians to develop and leverage common constructs that act as tools for both librarians, and users of libraries. These constructs include:
Planning and maintaining library facilities.
An important aspect of library management is planning and maintaining library facilities. Planning the construction of new libraries or remodeling those that exist is integral as user needs are often changing. To supplement their operating budget, managers often secure funding through gifts and fundraising. Many facilities are also including cafes, Friends of the Library, and exhibit spaces to help generate additional revenue. These venues must be taken into account when planning for building expansions. 
The site for new construction must be located, the building must be designed, constructed, and then evaluated. Once established, it is important that the building management keep up on regular maintenance. This can also be completed by delegating tasks to maintenance personal or hiring an outside company through bids.
Associations.
The Library Leadership and Management Association (LLAMA), a division of the American Library Association, provides leaders with webinar, conferences, awards and grants, "Library Leadership & Management" (online quarterly magazine), and books. LLAMA membership includes a free subscription to great leadership "Library Leadership & Management" and discounts on conferences and publications.
Publications.
The "Journal of Library Administration" began in 1980 and is currently published by Routledge, 8 times per year. It is a peer-reviewed academic journal that discusses issues pertaining to library management.

</doc>
<doc id="18432" url="https://en.wikipedia.org/wiki?curid=18432" title="English longbow">
English longbow

The English longbow, also called the Welsh longbow, is a powerful type of medieval longbow (a tall bow for archery) about long used by the English and Welsh for hunting and as a weapon in medieval warfare. English use of longbows was effective against the French during the Hundred Years' War, particularly at the start of the war in the battles of Sluys (1340), Crécy (1346), and Poitiers (1356), and perhaps most famously at the Battle of Agincourt (1415). They were less successful after this, with longbowmen having their lines broken at the Battle of Verneuil (1424), and being completely routed at the Battle of Patay (1429) when they were charged before they had set up their defensive position.
The earliest longbow known from England, found at Ashcott Heath, Somerset, is dated to 2665 BC, but no longbows survive from the period when the longbow was dominant (c. 1250–1450 AD), probably because bows became weaker, broke and were replaced, rather than being handed down through generations. More than 130 bows survive from the Renaissance period, however. More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that sank at Portsmouth in 1545.
Description.
Length.
A longbow must be long enough to allow its user to draw the string to a point on the face or body, and the length therefore varies with the user. In continental Europe it was generally seen as any bow longer than . The Society of Antiquaries says it is of in length. Richard Bartelot, of the Royal Artillery Institution, said that the bow was of yew, long, with a arrow. Gaston Phoebus, in 1388, wrote that a longbow should be "of yew or boxwood, seventy inches [] between the points of attachment for the cord". Historian Jim Bradbury said they were an average of about 5 feet and 8 inches. All but the last estimate were made before the excavation of the "Mary Rose", where bows were found ranging in length from with an average length of .
Draw weights.
Estimates for the draw of these bows varies considerably. Before the recovery of the "Mary Rose", Count M. Mildmay Stayner, Recorder of the British Long Bow Society, estimated the bows of the Medieval period drew , maximum, and Mr. W.F. Paterson, Chairman of the Society of Archer-Antiquaries, believed the weapon had a supreme draw weight of only . Other sources suggest significantly higher draw weights. The original draw forces of examples from the "Mary Rose" are estimated by Robert Hardy at at a draw length; the full range of draw weights was between . The draw length was used because that is the length allowed by the arrows commonly found on the "Mary Rose".
A modern longbow's draw is typically or less, and by modern convention measured at . Historically, hunting bows usually had draw weights of , which is enough for all but the very largest game and which most reasonably fit adults can manage with practice. Today, there are few modern longbowmen capable of using bows accurately.
A record of how boys and men trained to use the bows with high draw weights survives from the reign of Henry VII.
What Latimer meant when he describes laying his body into the bow was described thus:
Construction and materials.
The bowstave.
The preferred material to make the longbow was yew, although ash, elm and other woods were also used. Giraldus Cambrensis, Gerald of Wales, speaking of the bows used by the Welsh men of Gwent, says: "They are made neither of horn, ash nor yew, but of elm; ugly unfinished-looking weapons, but astonishingly stiff, large and strong, and equally capable of use for long or short shooting". The traditional construction of a longbow consists of drying the yew wood for 1 to 2 years, then slowly working the wood into shape, with the entire process taking up to four years. (This can be done far more quickly by working the wood down when wet, as a thinner piece of wood will dry much faster.) The bow stave is shaped into a D-section. The outer "back" of sapwood, approximately flat, follows the natural growth rings; modern bowyers often thin the sapwood, while in the "Mary Rose" bows the back of the bow was the natural surface of the wood, only the bark being removed. The inner side ("belly") of the bow stave consists of rounded heartwood. The heartwood resists compression and the outer sapwood performs better in tension. This combination in a single piece of wood (a self bow) forms a natural "laminate", somewhat similar in effect to the construction of a composite bow. Longbows will last a long time if protected with a water-resistant coating, traditionally of "wax, resin and fine tallow".
The trade of yew wood to England for longbows was such that it depleted the stocks of yew over a huge area. The first documented import of yew bowstaves to England was in 1294. In 1350 there was a serious shortage, and Henry IV of England ordered his royal bowyer to enter private land and cut yew and other woods. In 1470 compulsory practice was renewed, and hazel, ash, and laburnum were specifically allowed for practice bows. Supplies still proved insufficient, until by the Statute of Westminster in 1472, every ship coming to an English port had to bring four bowstaves for every tun. Richard III of England increased this to ten for every tun. This stimulated a vast network of extraction and supply, which formed part of royal monopolies in southern Germany and Austria. In 1483, the price of bowstaves rose from two to eight pounds per hundred, and in 1510 the Venetians obtained sixteen pounds per hundred. In 1507 the Holy Roman Emperor asked the Duke of Bavaria to stop cutting yew, but the trade was profitable, and in 1532 the royal monopoly was granted for the usual quantity "if there are that many". In 1562, the Bavarian government sent a long plea to the Holy Roman Emperor asking him to stop the cutting of yew, and outlining the damage done to the forests by its selective extraction, which broke the canopy and allowed wind to destroy neighbouring trees. In 1568, despite a request from Saxony, no royal monopoly was granted because there was no yew to cut, and the next year Bavaria and Austria similarly failed to produce enough yew to justify a royal monopoly.
Forestry records in this area in the 17th century do not mention yew, and it seems that no mature trees were to be had. The English tried to obtain supplies from the Baltic, but at this period bows were being replaced by guns in any case.
The string.
Bow strings were, and still are, made of hemp, flax or silk, and attached to the wood via horn "nocks" that fit onto the end of the bow. Modern synthetic materials (often Dacron) are now commonly used for strings.
The arrow.
A wide variety of arrows were shot from the English longbow. Variations in length, fletchings and heads are all recorded. Perhaps the greatest diversity lies in hunting arrows, with varieties like broad-arrow, wolf-arrow, dog-arrow, Welsh arrow and Scottish arrow being recorded. War arrows were ordered in the thousands for medieval armies and navies, supplied in sheaves normally of 24 arrows. For example, between 1341 and 1359 the English crown is known to have obtained 51,350 sheaves (1,232,400 arrows).
Only one significant group of arrows, from the Mary Rose, has survived. Over 3500 arrows were found, mainly made of poplar but also of ash, beech and hazel. Analysis of the intact specimens shows their length to vary from , with an average length of . Because of the preservation conditions of the Mary Rose no arrowheads survived. However, many heads have survived in other places, which has allowed typologies of arrow heads to be produced, the most modern being the Jessop typology. The most common arrowheads in military use were the short bodkin (Jessop M10) and a small barbed arrow (Jessop M4).
Use and performance.
Training.
Longbows were very difficult to master because the force required to deliver an arrow through the improving armour of medieval Europe was very high by modern standards. Although the draw weight of a typical English longbow is disputed, it was at least and possibly more than , with some estimates as high as . Considerable practice was required to produce the swift and effective combat shooting required. Skeletons of longbow archers are recognisably adapted, with enlarged left arms and often bone spurs on left wrists, left shoulders and right fingers.
It was the difficulty in using the longbow that led various monarchs of England to issue instructions encouraging their ownership and practice, including the Assize of Arms of 1252 and King Edward III's declaration of 1363: "Whereas the people of our realm, rich and poor alike, were accustomed formerly in their games to practise archery – whence by God's help, it is well known that high honour and profit came to our realm, and no small advantage to ourselves in our warlike enterprises... that every man in the same country, if he be able-bodied, shall, upon holidays, make use, in his games, of bows and arrows... and so learn and practise archery." If the people practised archery, it would be that much easier for the King to recruit the proficient longbowmen he needed for his wars. Along with the improving ability of gunfire to penetrate plate armour, it was the long training needed by longbowmen that eventually led to their being replaced by musketmen.
Range.
The range of the medieval weapon is not accurately known, with much depending on both the power of the bow and the type of arrow. It has been suggested that a flight arrow of a professional archer of Edward III's time would reach but the longest mark shot at on the London practice ground of Finsbury Fields in the 16th century was . In 1542, Henry VIII set a minimum practice range for adults using flight arrows of ; ranges below this had to be shot with heavy arrows. Modern experiments broadly concur with these historical ranges. A 667 N (150 lbf) "Mary Rose" replica longbow was able to shoot a arrow and a a distance of . In 2012, Joe Gibbs shot a livery arrow with a 170 lbf yew bow. The effective combat range of longbowmen was generally lower than what could be achieved on the practice range as sustained shooting was tiring and the rigors of campaigning would sap soldiers' strength. Writing 30 years after the Mary Rose sank, Barnabe Rich estimated that if 1000 English archers were mustered then after one week only 100 of them would be able to shoot farther than 200 paces, while 200 would not be able to shoot farther than 180 paces.
Armour penetration.
Modern testing.
In an early modern test by Saxton Pope, a direct hit from a steel bodkin point penetrated Damascus mail armour.
A 2006 test was made by Matheus Bane using a draw (at 28") bow, shooting at 10 yards; according to Bane's calculations, this would be approximately equivalent to a bow at 250 yards. Measured against a replica of the thinnest contemporary "Jack coat" armour, a 905 grain needle bodkin and a 935 grain curved broadhead penetrated over . ("Jack coat" armour could be up to twice as thick as the coat tested; in Bane's opinion such a thick coat would have stopped bodkin arrows but not the cutting force of broadhead arrows.) Against "high quality riveted maille", the needle bodkin and curved broadhead penetrated 2.8". Against a coat of plates, the needle bodkin achieved 0.3" penetration. The curved broadhead did not penetrate but caused 0.3" of deformation of the metal. Results against plate armour of "minimum thickness" (1.2mm) were similar to the coat of plates, in that the needle bodkin penetrated to a shallow depth, the other arrows not at all. In Bane's view, the plate armour would have kept out all the arrows if thicker or worn with more padding.
Other modern tests described by Bane include those by Williams (which concluded that longbows could "not" penetrate mail, but in Bane's view did not use a realistic arrow tip), Robert Hardy's tests (which achieved broadly similar results to Bane), and a "Primitive Archer" test which demonstrated that a longbow could penetrate a plate armour breastplate. However, the "Primitive Archer" test used a longbow at very short range, generating 160 joules (vs. 73 for Bane and 80 for Williams), so probably not representative of battles of the time.
Tests conducted by Mark Stretton circa 2006 focussed on heavier war shafts (as opposed to lighter hunting or distance-shooting 'flights') mated to a variety of heads indicate that the adoption of the heavy bodkin head - similar in form to contemporaneous crossbow warheads - was not merely fashionable imitation: Stretton's findings (based on experimentation using a variety of bows, arrows and heads based on historical examples but the results interpreted in the light of modern knowledge of the effects of blunt force trauma, via the good offices of Cranfield university) show the quarrel-like armour piercing shaft from a yew 'self bow' (with a draw weight of 144lbs at 32 inches) while travelling at 134 feet per second achieved 90% of the range of lighter broad heads while being 45% heavier and thus delivering more kinetic energy.
When translated these figures (102 grams moving at 47.23 metres per second) yield 113.76 joules, comfortably surpassing the 80 joule threshold at which a strike to a vital area is hazardous. (In fact all of the test arrows, fired from test bows, surpassed this potentially mortal limit). In tests Stretton addressed not merely depth of penetration against representative targets but strike angle and discovered that the short, heavy quarrel-form bodkin could penetrate a replica brigandine at up to 40° from perpendicular, and further, when fired at such a target mounted on a travelling rig at 20 miles per hour and thus appropriate to a war horse at the charge, the "added forward momentum" of the target added a full inch of penetration.
If not sufficient to kill a man in plate armour outright - so long as he is protected by thick and substantial energy-absorbing intermediate layers - would have a severe, possibly fatal, blunt trauma effect. (As Stretton acutely observes, if the purpose of the war bow and war shaft was to neutralise opponents as combatants precisely the same logic holds as on a modern battlefield: a wounded man demoralises his fellows and absorbs resources that might otherwise contribute to the battle and, as is still the case today, survivors tend to be those best protected and thus more profitable when taken alive and held hostage or for ransom.)
While obviously arguable Stretton's tests conducted with other arrowhead forms, notably the crescent and long-bodkin, seem indicate that these were far more versatile than some catalogues assume, being far from single-purpose projectiles: the long (circa eight inch) bodkin point mounted on a slender 'flight' arrowshaft could be most effectively against the majority of targets (excepting only plate armour) yet is so well-adapted for use with an incendiary sachet there is no benefit to creating a single-purpose special arrowhead and shaft; similarly the crescent form, while historically used for hunting (the Roman Emperor Commodus was alleged to have used them to decapitate running ostriches and managed to decapitate them) that is demonstrably less destructive than broadheads when directed at small game can "also" demonstrably part a stressed line or split sailcloth (with the proviso the shooter is as expert in the appropriate application of bow, arrow and arrowhead as in matters of range and marksmanship).
In 2011, Mike Loades conducted an experiment in which short bodkin arrows were shot at a range of by bows of . The target was covered in a riveted mail over a fabric armour of deerskin over 24 linen layers. While most arrows went through the mail layer, none fully penetrated the textile armour. The experimenters, however, concluded that a long bodkin arrow would have penetrated through this armour combination. Even so, Loades cautions that this experiment did not reflect normal combat ranges and used powerful bows, so may not be typical of battlefield performance.
Other research has also concluded that later medieval armour, such as that of the Italian city state mercenary companies, was effective at stopping contemporary arrows.
Contemporary accounts.
Gerald of Wales commented on the power of the Welsh longbow in the 12th century:
Against massed men in armour, massed longbows were murderously effective on many battlefields.
Strickland and Hardy suggest that "even at a range of 240 yards heavy war arrows shot from bows of poundages in the mid- to upper range possessed by the Mary Rose bows would have been capable of killing or severely wounding men equipped with armour of wrought iron. Higher-quality armour of steel would have given considerably greater protection, which accords well with the experience of Oxford's men against the elite French vanguard at Poitiers in 1356, and des Ursin's statement that the French knights of the first ranks at Agincourt, which included some of the most important (and thus best-equipped) nobles, remained comparatively unhurt by the English arrows".
Archery was described by contemporaries as ineffective against plate armour in the Battle of Neville's Cross (1346), the siege of Bergerac (1345), and the Battle of Poitiers (1356); such armour became available to European knights of fairly modest means by the late 14th century, though never to all soldiers in any army. Longbowmen were however effective at Poitiers, and this success stimulated changes in armour manufacture partly intended to make armoured men less vulnerable to archery. Nevertheless, at the battle of Agincourt in 1415 and for some decades thereafter, English longbowmen continued to be an effective battleﬁeld force.
Summary.
Modern tests and contemporary accounts agree therefore that well-made plate armour could protect against longbows. However this did not necessarily make the longbow ineffective; thousands of longbowmen were deployed in the English victory at Agincourt against plate armoured French knights in 1415. Clifford Rogers has argued that while longbows might not have been able to penetrate steel breastplates at Agincourt they could still penetrate the thinner armour on the limbs. Most of the French knights advanced on foot but, exhausted by walking across wet muddy terrain in heavy armour enduring a "terrifying hail of arrow shot", they were overwhelmed in the melee.
Less heavily armoured soldiers were more vulnerable than knights. For example, enemy crossbowmen were forced to retreat at Crecy when deployed without their protecting pavises. Horses were generally less well protected than the knights themselves; shooting the French knights' horses from the side (where they were less well armoured) is described by contemporary accounts of the Battle of Poitiers, and at Agincourt John Keegan has argued that the main effect of the longbow would have been in injuring the horses of the mounted French knights.
Shooting rate.
A typical military longbow archer would be provided with between 60 and 72 arrows at the time of battle. Most archers would not shoot arrows at maximum rate, as it would exhaust even the most experienced man. "With the heaviest bows modern war bow archer does not like to try for more than six a minute." Not only do the arms and shoulder muscles tire from the exertion, but the fingers holding the bowstring become strained; therefore, actual rates of shooting in combat would vary considerably. Ranged volleys at the beginning of the battle would differ markedly from the closer, aimed shots as the battle progressed and the enemy neared. On the battlefield English archers stored their arrows stabbed upright into the ground at their feet, reducing the time it took to nock, draw and loose.
Arrows were not unlimited, so archers and their commanders took every effort to ration their use to the situation at hand. Nonetheless, resupply during battle was available. Young boys were often employed to run additional arrows to longbow archers while in their positions on the battlefield. "The longbow was the machine gun of the Middle Ages: accurate, deadly, possessed of a long range and rapid rate of fire, the flight of its missiles was likened to a storm".
In tests against a moving target simulating a charging knight it took some approximately seven seconds to draw, aim and loose an armour-piercing heavy arrow using a replica war bow, that in the seven seconds between the first and second shots the target advanced 70 yards and that the second shot occurred at such close range that, if it was a realistic contest, running away was the only option. This rate of shooting was nearly twice what a Tudor English author expects from a "ready shooter" with the musket. The advantage of early firearms lay in the lower training requirements, the opportunity to take cover while shooting, flatter trajectory, and greater penetration.
Treating arrow wounds.
The only way to remove an arrow cleanly was to tie a piece of cloth soaked in water to the end of it and push it through the victim's wound and out the other side — this was extremely painful. Specialised tools have existed since ancient times: Diocles (successor of Hippocates) devised the graphiscos, a form of cannula with hooks, and the duck-billed forceps (allegedly invented by Heras of Cappadocia) employed during the medieval period to extract arrows from places where bone prevented the arrow being pushed through.
Henry, Prince of Wales, later Henry V, was wounded in the face by an arrow at the Battle of Shrewsbury (1403). The royal physician John Bradmore had such a tool made, which consisted of a pair of smooth tongs. Once carefully inserted into the socket of the arrowhead, the tongs screwed apart till they gripped its walls and allowed the head to be extracted from the wound. Prior to the extraction, the hole made by the arrow shaft had been widened by inserting larger and larger dowels of elder pith wrapped in linen down the entry wound. The dowels were soaked in honey, now known to have antiseptic properties. The wound was then dressed with a poultice of barley and honey mixed in turpentine (pre-dating Ambroise Paré but whose therapeutic use of turpentine was inspired by Roman medical texts that may have been familiar to Bradmore). After 20 days the wound was free of infection.
History.
Etymology.
The first recorded use of the term 'longbow', as distinct from simply 'bow', occurs in a Paston Letter of the 15th century.
Origins.
The origins of the English longbow are disputed. While it is hard to assess the significance of military archery in pre-Norman Conquest Anglo-Saxon warfare, it is clear that archery played a prominent role under the Normans, as the story of the Battle of Hastings shows. Their Anglo-Norman descendants also made use of military archery, as exemplified by their victory at the Battle of the Standard in 1138. During the Anglo-Norman invasions of Wales, Welsh bowmen took a heavy toll of the invaders and Welsh archers would feature in English armies from this point on. However, historians dispute whether this archery used a different kind of bow to the later English Longbow. Traditionally it has been argued that prior to the beginning of the 14th century, the weapon was a self bow between four and five feet in length, known since the 19th century as the shortbow. This weapon, drawn to the chest rather than the ear, was much weaker. However, in 1985, Jim Bradbury reclassified this weapon as the "ordinary wooden bow", reserving the term shortbow for short composite bows and arguing that longbows were a developed form of this ordinary bow. Strickland and Hardy in 2005 took this argument further, suggesting that the shortbow was a myth and all early English bows were a form of longbow. In 2011, Clifford Rogers forcefully restated the traditional case based upon a variety of evidence, including a large scale iconographic survey. In 2012, Richard Wadge added to the debate with an extensive survey of record, iconographic and archaeological evidence, concluding that longbows co-existed with shorter self-wood bows in England in the period between the Norman conquest and the reign of Edward III, but that powerful longbows shooting heavy arrows were a rarity until the later 13th century. Whether or not there was a technological revolution at the end of the 13th century therefore remains in dispute. What is agreed, however, is that the English longbow as an effective weapon system evolved in the late 13th and early 14th centuries.
Fourteenth and fifteenth century.
The longbow decided many medieval battles fought by the English and Welsh, the most significant of which were the Battle of Crécy (1346) and the Battle of Agincourt (1415), during the Hundred Years' War and followed earlier successes, notably at the Battle of Falkirk (1298) and the Battle of Halidon Hill (1333) during the Wars of Scottish Independence.
The longbow was also used against the English by their Welsh neighbours. The Welsh used the longbow mostly in a different manner than the English. In many early period English campaigns, the Welsh used the longbow in ambushes, often at point blank range that allowed their missiles to penetrate armour and generally do a lot of damage.
Although longbows were much faster and more accurate than the black-powder weapons which replaced them, longbowmen always took a long time to train because of the years of practice necessary before a war longbow could be used effectively (examples of longbows from the "Mary Rose" typically had draws greater than ). In an era in which warfare was usually seasonal, and non-noble soldiers spent part of the year working at farms, the year-round training required for the effective use of the longbow was a challenge. A standing army was an expensive proposition to a medieval ruler. Mainland European armies seldom trained a significant longbow corps. Due to their specialized training, English longbowmen were sought as mercenaries in other European countries, most notably in the Italian city-states and in Spain.
The White Company, comprising men-at-arms and longbowmen and commanded by Sir John Hawkwood, is the best known English Free Company of the 14th century. The powerful Hungarian king, Louis the Great, is an example of someone who used longbowmen in his Italian campaigns.
Sixteenth century and later.
Longbows remained in use until around the 16th century, when advances in firearms made gunpowder weapons a significant factor in warfare and such units as arquebusiers and grenadiers began appearing. Despite this, the English Crown made numerous efforts to continue to promote archery practice by banning other sports and fining people for not possessing bows. Indeed, just before the English Civil War, a pamphlet by William Neade entitled "The Double-Armed Man" advocated that soldiers be trained in both the longbow and pike; albeit this advice was followed only by a few town militias. The last recorded use of bows in an English battle seems to have been a skirmish at Bridgnorth, in October 1642, during the Civil War, when an impromptu town militia proved effective against un-armoured musketeers. Longbowmen remained a feature of the Royalist Army, but were not used by the Roundheads.
Longbows have been in continuous production and use for sport and for hunting to the present day, but since 1642 they have been a minority interest, and very few have had the high draw weights of the medieval weapons. Other differences include the use of a stiffened non-bending centre section, rather than a continuous bend.
Serious military interest in the longbow faded after the seventeenth century but occasionally schemes to resurrect its military use were proposed. Benjamin Franklin was a proponent in the 1770s; the Honourable Artillery Company had an archer company between 1784 and 1794; and a man named Richard Mason wrote a book proposing the arming of militia with pike and longbow in 1798. Donald Featherstone also records a Lt. Col. Richard Lee of 44th Foot advocated the military use of the longbow in 1792. There is a record of the use of the longbow in action as late as WWII, when Jack Churchill is credited with a longbow kill in France in 1940. The weapon was certainly considered for use by Commandos during the war but it is not known whether it was used in action.
Tactics.
Battle formations.
The idea that there was a standard formation for English longbow armies was argued by Alfred Byrne in his influential work on the battles of the Hundred Years' War, "The Crecy War". This view was challenged by Jim Bradbury in his book "The Medieval Archer" and more modern works are more ready to accept a variety of formations.
In summary, however, the usual English deployment in the 14th and 15th centuries was as follows:
In the 16th century, these formations evolved in line with new technologies and techniques from the continent. Formations with a central core of pikes and bills were flanked by companies of "shot" made up of a mixture of archers and arquebusiers, sometimes with a skirmish screen of archers and arquebusiers in front.
Surviving bows and arrows.
More than 3,500 arrows and 137 whole longbows were recovered from the "Mary Rose", a ship of Henry VIII's navy that capsized and sank at Portsmouth in 1545. It is an important source for the history of the longbow, as the bows, archery implements and the skeletons of archers have been preserved. The bows range in length from with an average length of . The majority of the arrows were made of poplar, others were made of beech, ash and hazel. Draw lengths of the arrows varied between with the majority having a draw length of . The head would add 5–15 cm depending on type, though some 2–4.5 cm must be allowed for the insertion of the shaft into the socket.
The longbows on the "Mary Rose" were in excellent finished condition. There were enough bows to test some to destruction which resulted in draw forces of 450 N (100 lbf) on average. However, analysis of the wood indicated that they had degraded significantly in the seawater and mud, which had weakened their draw forces. Replicas were made and when tested had draw forces of from 445 N to 823 N (100 to 185 lbf).
In 1980, before the finds from the "Mary Rose", Robert E. Kaiser published a paper stating that there were five known surviving longbows:
Social importance.
The importance of the longbow in English culture can be seen in the legends of Robin Hood, which increasingly depicted him as a master archer, and also in the "Song of the Bow", a poem from "The White Company" by Sir Arthur Conan Doyle.
During the reign of Henry III the Assize of Arms of 1252 required that all "citizens, burgesses, free tenants, villeins and others from 15 to 60 years of age" should be armed. The poorest of them were expected to have a halberd and a knife, and a bow if they owned land worth more than £2. This made it easier for the King to raise an army, but also meant that the bow was a weapon commonly used by rebels during the Peasants' Revolt. From the time that the yeoman class of England became proficient with the longbow, the nobility in England had to be careful not to push them into open rebellion.
It has been conjectured that yew trees were commonly planted in English churchyards to have readily available longbow wood.

</doc>
<doc id="18433" url="https://en.wikipedia.org/wiki?curid=18433" title="Lee Marvin">
Lee Marvin

Lee Marvin (February 19, 1924 – August 29, 1987) was an American film and television actor. Known for his distinctive voice, white hair and stature, Marvin initially appeared in supporting roles, mostly villains, soldiers and other hardboiled characters. From 1957 to 1960, he starred as Detective Lieutenant Frank Ballinger in the NBC crime series, "M Squad".
In 1966, he won several awards, including an Academy Award for Best Actor, and Best Actor BAFTA and the Best Actor Golden Globe, for his dual roles in "Cat Ballou".
Early life.
Marvin was born in New York City. He was son of Lamont Waltman Marvin, an advertising executive and the head of the New York and New England Apple Institute, and his wife Courtenay Washington (née Davidge), a fashion writer and beauty consultant. As with his older brother, Robert, he was named in honor of Confederate General Robert E. Lee, who was his first cousin, four times removed. His father was a direct descendant of Matthew Marvin, Sr., who emigrated from Great Bentley, Essex, England in 1635 and helped found Hartford, Connecticut.
Marvin studied violin when he was young. As a teenager, Marvin "spent weekends and spare time hunting deer, puma, wild turkey and bobwhite in the wilds of the then-uncharted Everglades."
He attended Manumit School, a Christian socialist boarding school in Pawling, New York, during the late 1930s, and later attended St. Leo College Preparatory School in St. Leo, Florida after being expelled from several other schools for bad behavior.
Military service.
World War II.
Marvin left school at 18 to enlist in the United States Marine Corps Reserve on August 12, 1942. He served with the 4th Marine Division in the Pacific Theater during World War II. While serving as a member of "I" Company, 3rd Battalion, 24th Marines, 4th Marine Division, he was wounded in action on June 18, 1944, during the assault on Mount Tapochau in the Battle of Saipan, during which most of his company were casualties. He was shot by machine gun fire, which severed his sciatic nerve, and then was hit again in the foot by a sniper. After over a year of medical treatment in Naval Hospitals, Marvin was given a medical discharge with the rank of private first class (he had been a corporal years earlier) in 1945 at Philadelphia.
Contrary to rumors, Marvin did not serve on Iwo Jima, receive a Navy Cross, or serve with actor, producer, and former Marine Bob Keeshan (later best known as Captain Kangaroo) during World War II.
Marvin's military awards include: the Purple Heart Medal, the Presidential Unit Citation, the American Campaign Medal, the Asiatic-Pacific Campaign Medal, and the World War II Victory Medal.
Acting career.
After the war, while working as a plumber's assistant at a local community theatre in Upstate New York, Marvin was asked to replace an actor who had fallen ill during rehearsals. He then began an amateur off-Broadway acting career in New York City and eventually made it to Broadway with a small role in the original production of Billy Budd.
In 1950, Marvin moved to Hollywood. He found work in supporting roles, and from the beginning was cast in various war films. As a decorated combat veteran, Marvin was a natural in war dramas, where he frequently assisted the director and other actors in realistically portraying infantry movement, arranging costumes, and the use of firearms. His debut was in "You're in the Navy Now" (1951), and in 1952 he appeared in several films, including Don Siegel's "Duel at Silver Creek", "Hangman's Knot", and the war drama "Eight Iron Men". He played Gloria Grahame's vicious boyfriend in Fritz Lang's "The Big Heat" (1953). Marvin had a small but memorable role in "The Wild One" (1953) opposite Marlon Brando (Marvin's gang in the film was called "The Beetles"), followed by "Seminole" (1953) and "Gun Fury" (1953). He also had a notable small role as smart-aleck sailor Meatball in "The Caine Mutiny". He had a substantially more important part as Hector, the small-town hood in "Bad Day at Black Rock" (1955) with Spencer Tracy. Also in 1955, he played a conflicted, brutal bank-robber in "Violent Saturday". A latter-day critic wrote of the character, "Marvin brings a multi-faceted complexity to the role and gives a great example of the early promise that launched his long and successful career."
During the mid-1950s, Marvin gradually began playing more important roles. He starred in "Attack", (1956) and had a supporting role in the Western "Seven Men from Now" (1956). He also starred in "The Missouri Traveler" (1958) but it took over 100 episodes as Chicago cop Frank Ballinger in the successful 1957–1960 television series "M Squad" to actually give him name recognition. One critic described the show as "a hyped-up, violent "Dragnet"... with a hard-as-nails Marvin" playing a tough police lieutenant. Marvin received the role after guest-starring in a memorable "Dragnet" episode as a serial killer. 
In the 1960s, Marvin was given prominent supporting roles in such films as "The Comancheros" (1961), John Ford's "The Man Who Shot Liberty Valance" (1962), and "Donovan's Reef" (1963), all starring John Wayne, with Marvin's roles getting larger with each film. As the vicious Liberty Valance, Marvin played his first title role and held his own with two of the screen's biggest stars (Wayne and James Stewart).
For director Don Siegel, Marvin appeared in "The Killers" (1964) playing an efficient professional assassin alongside Clu Gulager. "The Killers" was also the first film in which Marvin received top billing.
Television series guest appearances he has been in include "Wagon Train", "The Twilight Zone" both a couple of episodes, "Bonanza" and a couple of "Bob Hope" Television Specials.
Playing alongside Vivien Leigh and Simone Signoret, Marvin won the 1966 National Board of Review Award for male actors for his role in "Ship of Fools" (1965).
Marvin won the 1965 Academy Award for Best Actor for his comic role in the offbeat Western "Cat Ballou" starring Jane Fonda. He also won the 1965 Silver Bear for Best Actor at the 15th Berlin International Film Festival.
Marvin next performed in the hit Western "The Professionals" (1966), in which he played the leader of a small band of skilled mercenaries (Burt Lancaster, Robert Ryan, and Woody Strode) rescuing a kidnap victim (Claudia Cardinale) shortly after the Mexican Revolution. He followed that film with the hugely successful World War II epic "The Dirty Dozen" (1967) in which top-billed Marvin again portrayed an intrepid commander of a colorful group (future stars John Cassavetes, Charles Bronson, Telly Savalas, Jim Brown, and Donald Sutherland) performing an almost impossible mission. In the wake of these two films and after having received an Oscar, Marvin was a huge star, given enormous control over his next film "Point Blank".
In "Point Blank", an influential film for director John Boorman, he portrayed a hard-nosed criminal bent on revenge. Marvin, who had selected Boorman himself for the director's slot, had a central role in the film's development, plot line, and staging. In 1968, Marvin also appeared in another Boorman film, the critically acclaimed but commercially unsuccessful World War II character study "Hell in the Pacific", also starring famed Japanese actor Toshiro Mifune. Marvin was originally cast as Pike Bishop (later played by William Holden) in "The Wild Bunch" (1969), but fell out with director Sam Peckinpah and pulled out in order to star in the Western musical "Paint Your Wagon" (1969), in which he was top-billed over a singing Clint Eastwood. Despite his limited singing ability, he had a hit song with "Wand'rin' Star". By this time he was getting paid a million dollars per film, $200,000 less than top star Paul Newman was making at the time; yet he was ambivalent about the film business, even with its financial rewards:
Marvin had a much greater variety of roles in the 1970s and 1980s, with fewer 'bad-guy' roles than in earlier years. His 1970s films included "Monte Walsh" (1970) with Jeanne Moreau, the violent "Prime Cut" (1972) with Gene Hackman, "Pocket Money" (1972) with Paul Newman, "Emperor of the North Pole" (1973) opposite Ernest Borgnine, as Hickey in "The Iceman Cometh" (1973) with Fredric March and Robert Ryan, "The Spikes Gang" (1974) with Noah Beery, Jr., "The Klansman" (1974) with Richard Burton, "Shout at the Devil" (1976) with Roger Moore, "The Great Scout and Cathouse Thursday" (1976) with Oliver Reed, and "Avalanche Express" (1978) with Robert Shaw. Marvin was offered the role of Quint in "Jaws" (1975) but declined, stating "What would I tell my fishing friends who'd see me come off a hero against a dummy shark?".
Marvin's last big role was in Samuel Fuller's "The Big Red One" (1980), a war film based on Fuller's own war experiences. His remaining films were "Death Hunt" (1981) with Charles Bronson, "Gorky Park" (1983), "Dog Day" (1984), and "" (1985; a sequel with Marvin, Ernest Borgnine, and Richard Jaeckel picking up where they'd left off despite being 18 years older); his final appearance was in "The Delta Force" (1986) with Chuck Norris.
Personal life.
During the 1970s, Marvin resided off and on in Woodstock, caring for his dying father, and as a keen fisherman he used to make regular trips to Australia to engage in fishing for marlin at Cairns and Great White Shark at Port Fairy. In 1975 Marvin and his second wife Pamela moved to Tucson, Arizona, where he lived until his death.
Marvin was a Democrat who opposed the Vietnam War. He publicly endorsed John F. Kennedy in the 1960 presidential election.
Marriages and children.
A father of four, Marvin was married twice. His first marriage to Betty Ebeling began in February 1951 and ended in divorce on January 5, 1967; during this time his hobbies included sport fishing off the Baja California coast and duck hunting along the Mexican border near Mexicali. He and Ebeling had a son, Christopher (1952–2013), and three daughters: Courtenay (b. 1954), Cynthia (b. 1956) and Claudia (1958-2012).
Marvin was married to Pamela Feeley from October 18, 1970 until his death.
Community property case.
In 1971, Marvin was sued by Michelle Triola, his live-in girlfriend from 1965 to 1970, who legally changed her surname to "Marvin". Although the couple never married, she sought financial compensation similar to that available to spouses under California's alimony and community property laws. Triola claimed Marvin made her pregnant three times and paid for two abortions, while one pregnancy ended in miscarriage. She claimed the second abortion left her unable to bear children. The result was the landmark "palimony" case, "Marvin v. Marvin", 18 Cal. 3d 660 (1976).
In 1979, Marvin was ordered to pay $104,000 to Triola for "rehabilitation purposes" but the court denied her community property claim for one-half of the $3.6 million which Marvin had earned during their six years of cohabitation – distinguishing non-marital relationship contracts from marriage, with community property rights only attaching to the latter by operation of law. Rights equivalent to community property only apply in non-marital relationship contracts when the parties expressly, whether orally or in writing, contract for such rights to operate between them. In August 1981, the California Court of Appeal found that no such contract existed between them and nullified the award she had received. Michelle Triola died of lung cancer on October 30, 2009.
This case was used as fodder for a mock debate skit on "Saturday Night Live" called "Point Counterpoint", and on "The Tonight Show Starring Johnny Carson" as a skit with Carson as Adam, and Betty White as Eve.
Death.
In December 1986, Marvin was hospitalized for more than two weeks because of condition related to Valley Fever. He went into respiratory distress and was administered steroids to help his breathing. He had major intestinal ruptures as a result, and underwent a colonostomy. Marvin died of a heart attack on August 29, 1987. He is interred at Arlington National Cemetery where his headstone reads "Lee Marvin, PFC, US Marine Corps, World War II".
Television appearances.
Marvin's appearances on television included
"Suspense" (1 episode, 1950), 
"Rebound", 
"M Squad", 
"Climax!", 
"Biff Baker, U.S.A.", 
"Dragnet", 
"The Tonight Show Starring Johnny Carson",
"The Ford Show Starring Tennessee Ernie Ford", 
"General Electric Theater", 
"The Americans", 
"The Investigators", 
"The Barbara Stanwyck Show", 
"Route 66", 
"The Untouchables", 
"Checkmate", 
"The Dick Powell Show", 
"Combat!", 
"The Twilight Zone", 
"Kraft Suspense Theatre", 
"Dr. Kildare", 
"Wagon Train", 
"Bonanza", 
"The Virginian"
and "The Muppet Show".

</doc>
<doc id="18434" url="https://en.wikipedia.org/wiki?curid=18434" title="Lead Belly">
Lead Belly

Huddie William Ledbetter (January 20, 1888 – December 6, 1949) was an American folk and blues musician notable for his strong vocals, virtuosity on the twelve-string guitar, and the folk standards he introduced. He is best known as Lead Belly. Though many releases list him as "Leadbelly", he himself wrote it as "Lead Belly", which is also the spelling on his tombstone and the spelling used by the Lead Belly Foundation.
Lead Belly usually played a twelve-string guitar, but he also played the piano, mandolin, harmonica, violin, and "windjammer" (diatonic accordion). In some of his recordings he sang while clapping his hands or stomping his foot.
Lead Belly's songs covered a wide range, including gospel music; blues about women, liquor, prison life, and racism; and folk songs about cowboys, prison, work, sailors, cattle herding, and dancing. He also wrote songs about people in the news, such as Franklin D. Roosevelt, Adolf Hitler, Jean Harlow, the Scottsboro Boys and Howard Hughes.
Lead Belly was inducted into the Rock and Roll Hall of Fame in 1988 and the Louisiana Music Hall of Fame in 2008.
Biography.
Early life.
Lead Belly was born Huddie William Ledbetter on the Jeter Plantation near Mooringsport, Louisiana, in either January 1888 or 1889. The 1900 United States Census lists "Hudy Ledbetter" as 12 years old, born January 1888, and the 1910 and 1930 censuses also give his age as corresponding to a birth in 1888. The 1940 census lists his age as 51, with information supplied by wife, Martha. However, in April 1942, when Ledbetter filled out his World War II draft registration, he gave his birth date as January 23, 1889, and his birthplace as Freeport, Louisiana. His grave marker bears the date given on his draft registration.
Ledbetter was the younger of two children born to Wesley Ledbetter and Sallie Brown. The pronunciation of his name is purported to be "HYEW-dee" or "HUGH-dee." He can be heard pronouncing his name as "HYEW-dee" on the track "Boll Weevil," from the Smithsonian Folkways album "Lead Belly Sings for Children." His parents had cohabited for several years, but they legally married on February 26, 1888. When Huddie was five years old, the family settled in Bowie County, Texas.
By 1903, Huddie was already a "musicianer", a singer and guitarist of some note. He performed for nearby Shreveport audiences in St. Paul's Bottoms, a notorious red-light district there. He began to develop his own style of music after exposure to various musical influences on Shreveport's Fannin Street, a row of saloons, brothels, and dance halls in the Bottoms, now referred to as Ledbetter Heights.
The 1910 census of Harrison County, Texas, shows "Hudy" Ledbetter living next door to his parents with his first wife, Aletha "Lethe" Henderson. Aletha is registered as age 19 and married one year. Others say she was 15 when they married in 1898. It was in Texas that Ledbetter received his first instrument, an accordion from his uncle Terrell. By his early twenties, having fathered at least two children, Ledbetter left home to make his living as a guitarist and occasional laborer.
Influenced by the sinking of the "Titanic" in April 1912, Ledbetter wrote the song "The Titanic", his first composed on the twelve-string guitar, which later became his signature instrument. Initially played when performing with Blind Lemon Jefferson (1897–1929) in and around Dallas, Texas, the song is about champion African-American boxer Jack Johnson's being denied passage on the "Titanic". Johnson had in fact been denied passage on a ship for being Black, but it was not the "Titanic". Still, the song includes the lyric "Jack Johnson tried to get on board. The Captain, he says, 'I ain't haulin' no coal!' Fare thee, "Titanic"! Fare thee well!" Ledbetter later noted he had to leave out this passage when playing in front of white audiences.
Prison years.
Ledbetter's volatile temper sometimes led him into trouble with the law. In 1915, he was convicted of carrying a pistol and sentenced to time on the Harrison County chain gang. He escaped, finding work in nearby Bowie County under the assumed name of Walter Boyd. In January 1918 he was imprisoned at the Imperial Farm (now Central Unit) in Sugar Land, Texas, after killing one of his own relatives, Will Stafford, in a fight over a woman. While there he may have first heard the traditional prison song "Midnight Special". In 1925 he was pardoned and released after writing a song to Governor Pat Morris Neff seeking his freedom, having served the minimum seven years of a 7- to 35-year sentence. Combined with his good behavior (which included entertaining the guards and fellow prisoners), his appeal to Neff's strong religious beliefs proved sufficient. It was a testament to his persuasive powers, as Neff had run for governor on a pledge not to issue pardons (the only recourse for prisoners, since in most Southern prisons there was no provision for parole). According to Charles K. Wolfe and Kip Lornell, in their book "The Life and Legend of Leadbelly" (1999), Neff had regularly brought guests to the prison on Sunday picnics to hear Ledbetter perform.
In 1930 Ledbetter was sentenced to Louisiana's Angola Prison Farm after a summary trial for attempted homicide for stabbing a white man in a fight. He was "discovered" there three years later during a visit by folklorists John Lomax and his son Alan Lomax.
Deeply impressed by Ledbetter's vibrant tenor and extensive repertoire, the Lomaxes recorded him in 1933 on portable aluminum disc recording equipment for the Library of Congress. They returned with new and better equipment in July 1934, recording hundreds of his songs. On August 1, Ledbetter was released after having again served nearly all of his minimum sentence, following a petition the Lomaxes had taken to Louisiana Governor Oscar K. Allen at his urgent request. It was on the other side of a recording of his signature song, "Goodnight Irene."
A prison official later wrote to John Lomax denying that Ledbetter's singing had anything to do with his release from Angola (state prison records confirm he was eligible for early release due to good behavior). However, both Ledbetter and the Lomaxes believed that the record they had taken to the governor had hastened his release from prison.
The nickname "Lead Belly".
There are several conflicting stories about how Ledbetter acquired the nickname "Lead Belly", but he probably acquired it while in prison. Some claim his fellow inmates called him "Lead Belly" as a play on his family name and his physical toughness. It is recounted that during his second prison term, another inmate stabbed him in the neck (leaving him with a fearsome scar he subsequently covered with a bandana); Ledbetter nearly killed his attacker with his own knife. Others say he earned the name after being wounded in the stomach with buckshot. Another theory is that the name refers to his ability to drink moonshine, the homemade liquor which Southern farmers, black and white, made to supplement their incomes. Blues singer Big Bill Broonzy thought it came from a supposed tendency to lay about as if "with a stomach weighted down by lead" in the shade when the chain gang was supposed to be working. Or it may be simply a corruption of his last name pronounced with a Southern accent. Whatever its origin, he adopted the nickname as a pseudonym while performing.
Life after prison.
When Lead Belly was released from prison, the United States was deep in the Great Depression, and jobs were very scarce. In September 1934, in need of regular work in order to avoid cancellation of his release from prison, Lead Belly asked John Lomax to take him on as a driver. For three months he assisted the 67-year-old in his folk song collecting around the South. (Alan Lomax was ill and did not accompany his father on this trip.)
In December Lead Belly participated in a "smoker" (group sing) at a Modern Language Association meeting at Bryn Mawr College in Pennsylvania, where the senior Lomax had a prior lecturing engagement. He was written up in the press as a convict who had sung his way out of prison. On New Year's Day, 1935, the pair arrived in New York City, where Lomax was scheduled to meet with his publisher, Macmillan, about a new collection of folk songs. The newspapers were eager to write about the "singing convict," and "Time" magazine made one of its first "March of Time" newsreels about him. Lead Belly attained fame (although not fortune).
The following week, he began recording for the American Record Corporation, but these recordings achieved little commercial success. He recorded over 40 sides for ARC (intended to be released on their Banner, Melotone, Oriole, Perfect, and Romeo labels and their short-lived Paramount series), but only five sides were actually issued. Part of the reason for the poor sales may have been that ARC released only his blues songs rather than the folk songs for which he would later become better known. Lead Belly continued to struggle financially. Like many performers, what income he made during his career would come from touring, not from record sales.
In February 1935, he married his girlfriend, Martha Promise, who came north from Louisiana to join him.
The month of February was spent recording his repertoire and those of other African Americans and interviews about his life with Alan Lomax for their forthcoming book, "Negro Folk Songs As Sung by Lead Belly" (1936). Concert appearances were slow to materialize. In March 1935, Lead Belly accompanied John Lomax on a previously scheduled two-week lecture tour of colleges and universities in the Northeast, culminating at Harvard.
At the end of the month, John Lomax decided he could no longer work with Lead Belly and gave him and Martha money to go back to Louisiana by bus. He gave Martha the money her husband had earned during three months of performing, but in installments, on the pretext Lead Belly would spend it all on drinking if given a lump sum. From Louisiana, Lead Belly successfully sued Lomax for both the full amount and release from his management contract. The quarrel was bitter, with hard feelings on both sides. Curiously, in the midst of the legal wrangling, Lead Belly wrote to Lomax proposing they team up again, but it was not to be. Further, the book about Lead Belly published by the Lomaxes in the fall of the following year proved a commercial failure.
In January 1936, Lead Belly returned to New York on his own, without John Lomax, in an attempted comeback. He performed twice a day at Harlem's Apollo Theater during the Easter season in a live dramatic recreation of the "March of Time" newsreel (itself a recreation) about his prison encounter with John Lomax, where he had worn stripes, though by this time he was no longer associated with Lomax.
"Life" magazine ran a three-page article titled "Lead Belly: Bad Nigger Makes Good Minstrel" in its issue of April 19, 1937. It included a full-page, color (rare in those days) picture of him sitting on grain sacks playing his guitar and singing. Also included was a striking picture of Martha Promise (identified in the article as his manager); photos showing Lead Belly's hands playing the guitar (with the caption "these hands once killed a man"); Texas Governor Pat M. Neff; and the "ramshackle" Texas State Penitentiary. The article attributes both of his pardons to his singing of his petitions to the governors, who were so moved that they pardoned him. The text of the article ends with "he... may well be on the brink of a new and prosperous period."
Lead Belly failed to stir the enthusiasm of Harlem audiences. Instead, he attained success playing at concerts and benefits for an audience of leftist folk music aficionados. He developed his own style of singing and explaining his repertoire in the context of Southern black culture, having learned from his participation in Lomax's college lectures. He was especially successful with his repertoire of children's game songs (as a younger man in Louisiana he had sung regularly at children's birthday parties in the black community). He was written about as a heroic figure by the black novelist Richard Wright, then a member of the Communist Party, in the columns of the "Daily Worker," of which Wright was the Harlem editor. The two men became personal friends, though some say Lead Belly himself was apolitical and if anything was a supporter of Wendell Willkie, the centrist Republican candidate for President, for whom he wrote a campaign song. However, he also wrote the song "Bourgeois Blues", which has radical or left-wing lyrics.
In 1939, Lead Belly was back in jail for assault after stabbing a man in a fight in Manhattan. Alan Lomax, then 24, took him under his wing and helped raise money for his legal expenses, dropping out of graduate school to do so. After his release (in 1940–41), Lead Belly appeared as a regular on Alan Lomax and Nicholas Ray's groundbreaking CBS radio show "Back Where I Come From", broadcast nationwide. He also appeared in nightclubs with Josh White, becoming a fixture in New York City's surging folk music scene and befriending the likes of Sonny Terry, Brownie McGhee, Woody Guthrie, and a young Pete Seeger, all fellow performers on "Back Where I Come From". During the first half of the decade, he recorded for RCA, the Library of Congress, and Moe Asch (future founder of Folkways Records) and in 1944 went to California, where he recorded strong sessions for Capitol Records. He lodged with a studio guitar player on Merrywood Drive in Laurel Canyon.
Lead Belly was the first American country blues musician to achieve success in Europe.
In 1949, Lead Belly had a regular radio broadcast on station WNYC in New York, on Henrietta Yurchenco's show on Sunday nights. Later in the year he began his first European tour with a trip to France, but fell ill before its completion and was diagnosed with amyotrophic lateral sclerosis (ALS), or Lou Gehrig's disease (a motor neuron disease). His final concert was at the University of Texas at Austin in a tribute to his former mentor, John Lomax, who had died the previous year. Martha also performed at that concert, singing spirituals with her husband.
Lead Belly died later that year in New York City and was buried in the Shiloh Baptist Church cemetery, in Mooringsport, Louisiana, west of Blanchard, in Caddo Parish. He is honored with a statue across from the Caddo Parish Courthouse, in Shreveport.
Technique.
Lead Belly styled himself "King of the Twelve-String Guitar," and despite his use of other instruments like the accordion, the most enduring image of Lead Belly as a performer is wielding his unusually large Stella twelve-string. This guitar had a slightly longer scale length than a standard guitar, slotted tuners, ladder bracing, and a trapeze-style tailpiece to resist bridge lifting.
Lead Belly played with finger picks much of the time, using a thumb pick to provide a walking bass line and occasionally to strum. This technique, combined with low tunings and heavy strings, gives many of his recordings a piano-like sound. Lead Belly's tuning is debated, but it seems to be a down-tuned variant of standard tuning; it is likely that he tuned his guitar strings relative to one another, so that the actual notes shifted as the strings wore. Lead Belly's playing style was popularized by Pete Seeger, who adopted the twelve-string guitar in the 1950s and released an instructional LP and book using Lead Belly as an exemplar of technique.
In some of the recordings in which Lead Belly accompanied himself, he would make an unusual type of grunt between his verses, best described as "Haah!" "Looky Looky Yonder," "Take This Hammer," "Linin' Track" and "Julie Ann Johnson" feature this unusual vocalization. In "Take This Hammer," Lead Belly explained, "Every time the men say, 'Haah,' the hammer falls. The hammer rings, and we swing, and we sing." The "haah" sound can be heard in work chants sung by Southern railroad section workers, "gandy dancers," in which it was used to coordinate work crews as they laid and maintained tracks.
Legacy.
Lead Belly's work has been widely covered by subsequent musical acts, including Bob Dylan, Brian Wilson, Delaney Davidson, Tom Russell, Lonnie Donegan, Bryan Ferry ("Goodnight, Irene"), the Beach Boys ("Cotton Fields"), Creedence Clearwater Revival ("Midnight Special", "Cotton Fields"), Elvis Presley, ABBA, Pete Seeger, the Weavers, Harry Belafonte, Frank Sinatra, Nat King Cole, the Animals, Jay Farrar, Johnny Cash, Tom Petty, Dr. John, Ry Cooder, Davy Graham, Maria Muldaur, Rory Block, Grateful Dead, Gene Autry, Odetta, Billy Childish (who named his son Huddie), Mungo Jerry, Paul King, Van Morrison, Michelle Shocked, Tom Waits ("Goodnight, Irene"), Scott H. Biram, Ron Sexsmith, British Sea Power, Rod Stewart, Ernest Tubb, Nick Cave and the Bad Seeds, Ram Jam, Spiderbait ("Black Betty"), Blind Willies ("In the Pines"), the White Stripes ("Boll Weevil"), the Fall, Hole, Smog, Old Crow Medicine Show, Spiderbait, Meat Loaf, Ministry, Raffi, Rasputina, Rory Gallagher ("Out on the Western Plains"), the Sensational Alex Harvey Band, Deer Tick, Hugh Laurie, X, Bill Frisell, Koerner, Ray & Glover, Red Hot Chili Peppers, Nirvana, Meat Puppets, Mark Lanegan, WZRD ("Where Did You Sleep Last Night"), Keith Richards, and Phil Lee ("I Got Stripes"), among many others.
Modern rock audiences likely owe their familiarity with Lead Belly to Nirvana's performance of "Where Did You Sleep Last Night" on a televised concert later released as "MTV Unplugged in New York". Singer-guitarist Kurt Cobain refers to his attempt to convince David Geffen to purchase Lead Belly's guitar for him in an interval before the song is played (connecting the song with Lead Belly in a way that is more tangible than the liner notes where Lead Belly appears on other albums). In his notebooks, Cobain listed Lead Belly's "Last Session Vol. 1" as one of the 50 albums most influential in the formation of Nirvana's sound.
Discography.
The Library of Congress recordings.
The Library of Congress recordings, made by John and Alan Lomax from 1934 to 1943, were released in a six-volume series by Rounder Records:
Folkways recordings.
The Folkways recordings, done for Moses Asch from 1941 to 1947, were released in a three-volume series by Smithsonian Folkways:
Smithsonian Folkways has released several other collections of his recordings:

</doc>
<doc id="18435" url="https://en.wikipedia.org/wiki?curid=18435" title="Lower Saxony">
Lower Saxony

Lower Saxony ( , ) is a German state ("Bundesland") situated in northwestern Germany and is second in area, with , and fourth in population (8 million) among the sixteen "Länder" of Germany. In rural areas Northern Low Saxon, a dialect of Low German, and Saterland Frisian, a variety of Frisian, are still spoken, but the number of speakers is declining.
Lower Saxony borders on (from north and clockwise) the North Sea, the states of Schleswig-Holstein, Hamburg, Mecklenburg-Vorpommern, Brandenburg, Saxony-Anhalt, Thuringia, Hesse and North Rhine-Westphalia, and the Netherlands. Furthermore, the state of Bremen forms two enclaves within Lower Saxony, one being the city of Bremen, the other, its seaport city of Bremerhaven. In fact, Lower Saxony borders more neighbours than any other single "Bundesland." The state's principal cities include the state capital Hanover, Braunschweig (Brunswick), Lüneburg, Osnabrück, Oldenburg, Hildesheim, Wolfenbüttel, Wolfsburg and Göttingen.
The northwestern area of Lower Saxony, which lies on the coast of the North Sea, is called East Frisia and the seven East Frisian Islands offshore are popular with tourists. In the extreme west of Lower Saxony is the Emsland, a traditionally poor and sparsely populated area, once dominated by inaccessible swamps. The northern half of Lower Saxony, also known as the North German Plains, is almost invariably flat except for the gentle hills around the Bremen geestland. Towards the south and southwest lie the northern parts of the German Central Uplands: the Weser Uplands and the Harz mountains. Between these two lie the Lower Saxon Hills, a range of low ridges. Thus, Lower Saxony is the only "Bundesland" that encompasses both maritime and mountainous areas.
Lower Saxony's major cities and economic centres are mainly situated in its central and southern parts, namely Hanover, Braunschweig, Osnabrück, Wolfsburg, Salzgitter, Hildesheim and Göttingen. Oldenburg, near the northwestern coastline, is another economic centre. The region in the northeast is called the Lüneburg Heath ("Lüneburger Heide"), the largest heathland area of Germany and in medieval times wealthy due to salt mining and salt trade, as well as to a lesser degree the exploitation of its peat bogs up until about the 1960s. To the north, the Elbe river separates Lower Saxony from Hamburg, Schleswig-Holstein, Mecklenburg-Western Pomerania and Brandenburg. The banks just south of the Elbe are known as "Altes Land (Old Country)". Due to its gentle local climate and fertile soil it is the state's largest area of fruit farming, its chief produce being apples.
Most of the state's territory was part of the historic Kingdom of Hanover; the state of Lower Saxony has adopted the coat of arms and other symbols of the former kingdom. It was created by the merger of the State of Hanover with several smaller states in 1946.
Geography.
Location.
Lower Saxony has a natural boundary in the north in the North Sea and the lower and middle reaches of the River Elbe, although parts of the city of Hamburg lie south of the Elbe. The state and city of Bremen is an enclave entirely surrounded by Lower Saxony. The Bremen/Oldenburg Metropolitan Region is a cooperative body for the enclave area. To the southeast the state border runs through the Harz, low mountains that are part of the German Central Uplands. The northeast and west of the state – which form roughly three-quarters of its land area – belong to the North German Plain, while the south is in the Lower Saxon Hills, including the Weser Uplands, Leine Uplands, Schaumburg Land, Brunswick Land, Untereichsfeld, Elm and Lappwald. In northeast Lower Saxony is Lüneburg Heath. The heath is dominated by the poor sandy soils of the geest, whilst in the central east and southeast in the loess "börde" zone there are productive soils with high natural fertility. Under these conditions—with loam and sand-containing soils—the land is well-developed agriculturally. In the west lie the County of Bentheim, Osnabrück Land, Emsland, Oldenburg Land, Ammerland, Oldenburg Münsterland and – on the coast – East Frisia.
The state is dominated by several large rivers running northwards through the state: the Ems, Weser, Aller and Elbe.
The highest mountain in Lower Saxony is the Wurmberg (971 m) in the Harz. For other significant elevations see: List of mountains and hills in Lower Saxony. Most of the mountains and hills are found in the southeastern part of the state. The lowest point in the state, at about 2.5 metres below sea level, is a depression near Freepsum in East Frisia.
The state's economy, population and infrastructure are centred on the cities and towns of Hanover, Stadthagen, Celle, Braunschweig, Wolfsburg, Hildesheim and Salzgitter. Together with Göttingen in southern Lower Saxony, they form the core of the Hannover–Braunschweig–Göttingen–Wolfsburg Metropolitan Region.
Regions.
General.
Lower Saxony has clear regional divisions that manifest themselves both geographically as well as historically and culturally. In the regions that used to be independent, especially the heartlands of the former states of Brunswick, Hanover, Oldenburg and Schaumburg-Lippe, there is a marked local regional awareness. By contrast, the areas surrounding the Hanseatic cities of Bremen and Hamburg are much more oriented towards those centres.
List of regions.
Sometimes there are overlaps and transition areas between the various regions of Lower Saxony. Several of the regions listed here are part of other, larger regions, that are also included in the list.
Just under 20% of the land area of Lower Saxony is designated as nature parks, i.e.: Dümmer, Elbhöhen-Wendland, Elm-Lappwald, Harz, Lüneburger Heide, Münden, Terra.vita, Solling-Vogler, Lake Steinhude, Südheide, Weser Uplands, Wildeshausen Geest, Bourtanger Moor-Bargerveen.
Climate.
Lower Saxony falls climatically into the north temperate zone of central Europe that is affected by prevailing Westerlies and is located in a transition zone between the maritime climate of Western Europe and the continental climate of Eastern Europe. This transition is clearly noticeable within the state: whilst the northwest experiences an Atlantic (North Sea coastal) to Sub-Atlantic climate, with comparatively low variations in temperature during the course of the year and a surplus water budget, the climate towards the southeast is increasingly affected by the Continent. This is clearly shown by greater temperature variations between the summer and winter halves of the year and in lower and more variable amounts of precipitation across the year. This sub-continental effect is most sharply seen in the Wendland, in the Weser Uplands (Hamelin to Göttingen) and in the area of Helmstedt. The highest levels of precipitation are experienced in the Harz because the Lower Saxon part forms the windward side of this mountain range against which orographic rain falls. The average annual temperature is 8 °C (7.5 °C in the Altes Land and 8.5 °C in the district of Cloppenburg).
Neighbouring states.
Fellow "Länder" bordering on Lower Saxony are Bremen, Hamburg, Schleswig-Holstein, Mecklenburg-Vorpommern, Brandenburg, Saxony-Anhalt, Thuringia, Hesse and North Rhine-Westphalia. No other German state has so many neighbours.
Lower Saxony also has a border with the Dutch provinces of Overijssel, Drenthe and Groningen as well as part of the German North Sea coast.
Administration.
Lower Saxony is divided into 38 districts ("Landkreise" or simply "Kreise"):
Furthermore, there are eight urban districts and two cities with special status:
¹ "following the "Göttingen Law" of 1 January 1964, the town of Göttingen is incorporated into the rural district ("Landkreis") of Göttingen, but is treated as an urban district unless other rules apply. On 1 November 2016 the districts of Osterode and Göttingen will merge under the name Göttingen, not influencing the cities special status."
² "following the "Law on the region of Hanover", Hanover merged with the district of Hanover to form the Hanover Region, which is treated mostly as a rural district, but Hanover is treated as an urban district since 1 November 2001 unless other rules apply."
History.
Regional history prior to foundation of Lower Saxony.
The name of Saxony derives from that of the Germanic tribe of the Saxons. Before the late medieval period, there was a single Duchy of Saxony. The term "Lower Saxony" was used 2016 after the dissolution of the stem duchy the late 13th century to disambiguate the parts of the former duchy ruled by the House of Welf from the Electorate of Saxony on one hand, and from the Duchy of Westphalia on the other.
Period to the Congress of Vienna (1814/1815).
The name and coat of arms of the present state go back to the Germanic tribe of Saxons. During the Migration Period some of the Saxon peoples left their homelands in Holstein about the 3rd century and pushed southwards over the Elbe, where they expanded into the sparsely populated regions in the rest of the lowlands, in the present-day Northwest Germany and the northeastern part of what is now the Netherlands. From about the 7th century the Saxons had occupied a settlement area that roughly corresponds to the present state of Lower Saxony, of Westphalia and a number of areas to the east, for example, in what is now west and north Saxony-Anhalt. The land of the Saxons was divided into about 60 "Gaue". The Frisians had not moved into this region; for centuries they preserved their independence in the most northwesterly region of the present-day Lower Saxon territory. The original language of the folk in the area of Old Saxony was West Low Saxon, one of the varieties of language in the Low German dialect group.
The establishment of permanent boundaries between what later became Lower Saxony and Westphalia began in the 12th century. In 1260, in a treaty between the Archbishopric of Cologne and the Duchy of Brunswick-Lüneburg the lands claimed by the two territories were separated from each other. The border ran along the Weser to a point north of Nienburg. The northern part of the Weser-Ems region was placed under the rule of Brunswick-Lüneburg.
The word "Niedersachsen" was first used before 1300 in a Dutch rhyming chronicle ("Reimchronik"). From the 14th century it referred to the Duchy of Saxe-Lauenburg (as opposed to Saxe-Wittenberg). On the creation of the imperial circles in 1500, a Lower Saxon Circle was distinguished from a Lower Rhenish–Westphalian Circle. The latter included the following territories that, in whole or in part, belong today to the state of Lower Saxony: the Bishopric of Osnabrück, the Bishopric of Münster, the County of Bentheim, the County of Hoya, the Principality of East Frisia, the Principality of Verden, the County of Diepholz, the County of Oldenburg, the County of Schaumburg and the County of Spiegelberg. At the same time a distinction was made with the eastern part of the old Saxon lands from the central German principalities later called Upper Saxony for dynastic reasons. (see also → Electorate of Saxony, History of Saxony).
The close historical links between the domains of the Lower Saxon Circle now in modern Lower Saxony survived for centuries especially from a dynastic point of view. The majority of historic territories whose land now lies within Lower Saxony were sub-principalities of the medieval, Welf estates of the Duchy of Brunswick-Lüneburg. All the Welf princes called themselves dukes "of Brunswick and Lüneburg" despite often ruling parts of a duchy that was forever being divided and reunited as various Welf lines multiplied or died out.
To the end of the Second World War.
Over the course of time two great principalities survived east of the Weser: the Kingdom of Hanover and the Duchy of Brunswick (after 1866 Hanover became a Prussian province; after 1919 Brunswick became a free state). Historically a close tie exists between the royal house of Hanover (Electorate of Brunswick-Lüneburg) to the United Kingdom of Great Britain and Northern Ireland as a result of their personal union in the 18th century.
West of the River Hunte a "de-Westphalianising process" began in 1815: After the Congress of Vienna the territories of the later administrative regions ("Regierungsbezirke") of Osnabrück and Aurich transferred to the Kingdom of Hanover. Until 1946, the Grand Duchy of Oldenburg and the Principality of Schaumburg-Lippe retained their stately authority. Nevertheless, the entire Weser-Ems region (including the city of Bremen) were grouped in 1920 into a Lower Saxon Constituency Association ("Wahlkreisverband IX (Niedersachsen)"). This indicates that at that time the western administrations of the Prussian Province of Hanover and the state of Oldenburg were perceived as being "Lower Saxon".
The forerunners of today's state of Lower Saxony were lands that were geographically and, to some extent, institutionally interrelated from very early on. The County of Schaumburg (not to be confused with the Principality of Schaumburg-Lippe) around the towns of Rinteln and Hessisch Oldendorf did indeed belong to the Prussian province of Hesse-Nassau until 1932, a province that also included large parts of the present state of Hesse, including the cities of Kassel, Wiesbaden and Frankfurt am Main; but in 1932, however, the County of Schaumburg became part of the Prussian Province of Hanover. Also before 1945, namely 1937, the city of Cuxhaven has been fully integrated into the Prussian Province of Hanover by the Greater Hamburg Act, so that in 1946, when the state of Lower Saxony was founded, only four states needed to be merged. With the exception of Bremen and the areas that were ceded to the Soviet Occupation Zone in 1945, all those areas allocated to the new state of Lower Saxony in 1946, had already been merged into the "Constituency Association of Lower Saxony" in 1920.
In a lecture on 14 September 2007, Dietmar von Reeken described the emergence of a "Lower Saxony consciousness" in the 19th century, the geographical basis of which was used to invent a territorial construct: the resulting local heritage societies ("Heimatvereine") and their associated magazines routinely used the terms "Lower Saxony" or "Lower Saxon" in their names. At the end of the 1920s in the context of discussions about a reform of the Reich, and promoted by the expanding local heritage movement ("Heimatbewegung"), a twenty-five year conflict started between "Lower Saxony" and "Westphalia". The supporters of this dispute were administrative officials and politicians, but regionally focussed scientists of various disciplines were supposed to have fuelled the arguments. In the 1930s, a real Lower Saxony did not yet exist, but there was a plethora of institutions that would have called themselves "Lower Saxon". The motives and arguments in the disputes between "Lower Saxony" and "Westphalia" were very similar on both sides: economic interests, political aims, cultural interests and historical aspects.
Post World War II.
After the Second World War most of Northwest Germany lay within the British Zone of Occupation. On 23 August 1946, the British Military Government issued Ordinance No. 46 ""Concerning the dissolution of the provinces of the former state of Prussia in the British Zone and their reconstitution as independent states"", which initially established the State of Hanover on the territory of the former Prussian Province of Hanover. Its minister president, Hinrich Wilhelm Kopf, had already suggested in June 1945 the formation of a state of Lower Saxony, that was to include the largest possible region in the middle of the British Zone. In addition to the regions that actually became Lower Saxony subsequently, Kopf asked, in a memorandum dated April 1946, for the inclusion of the former Prussian district of Minden-Ravensberg (i.e. the Westphalian city of Bielefeld as well as the Westphalian districts of Minden, Lübbecke, Bielefeld, Herford and Halle), the district of Tecklenburg and the state of Lippe. Kopf's plan was ultimately based on a draft for the reform of the German Empire from the late 1920s by Georg Schnath and Kurt Brüning. The strong Welf connotations of this draft, according to Thomas Vogtherr, did not simplify the development of a Lower Saxon identity after 1946.
An alternative model, proposed by politicians in Oldenburg and Brunswick, envisaged the foundation of the independent state of "Weser-Ems", that would be formed from the state of Oldenburg, the Hanseatic City of Bremen and the administrative regions of Aurich and Osnabrück. Several representatives of the state of Oldenburg even demanded the inclusion of the Hanoverian districts of Diepholz, Syke, Osterholz-Scharmbeck and Wesermünde in the proposed state of "Weser-Ems". Likewise an enlarged State of Brunswick was proposed in the southeast to include the "Regierungsbezirk" of Hildesheim and the district of Gifhorn. Had this plan come to fruition, the territory of the present Lower Saxony would have consisted of three states of roughly equal size.
The district council of Vechta protested on 12 June 1946 against being incorporated into the metropolitan area of Hanover ("Großraum Hannover"). If the State of Oldenburg was to be dissolved, Vechta District would much rather be included in the Westphalian region. Particularly in the districts where there was a political Catholicism the notion was widespread, that Oldenburg Münsterland and the "Regierungsbezirk" of Osnabrück should be part of a newly formed State of Westphalia.
Since the foundation of the states of North Rhine-Westphalia and Hanover on 23 August 1946 the northern and eastern border of North Rhine-Westphalia has largely been identical with that of the Prussian Province of Westphalia. Only the Free State of Lippe was not incorporated into North Rhine-Westphalia until January 1947. With that the majority of the regions left of the Upper Weser became North Rhine-Westphalian.
In the end, at the meeting of the Zone Advisory Board on 20 September 1946, Kopf's proposal with regard to the division of the British occupation zone into three large states proved to be capable of gaining a majority. Because this division of their occupation zone into relatively large states also met the interests of the British, on 8 November 1946 Regulation No. 55 of the British military government was issued, by which the State of Lower Saxony with its capital Hanover were founded, backdated to 1 November 1946. The state was formed by a merger of the Free States of Brunswick, of Oldenburg and of Schaumburg-Lippe with the previously formed State of Hanover. But there were exceptions:
The demands of Dutch politicians that the Netherlands should be given the German regions east of the Dutch-German border as war reparations, were roundly rejected at the London Conference of 26 March 1949. In fact only about 1.3 km² of West Lower Saxony was transferred to the Netherlands, in 1949.
"→ see main article Dutch annexation of German territory after World War II"
History of Lower Saxony as a state.
The first Lower Saxon parliament or "Landtag" met on 9 December 1946. It was not elected; rather it was established by the British Occupation Administration (a so-called "appointed parliament"). That same day the parliament elected the Social Democrat, Hinrich Wilhelm Kopf, the former Hanoverian president ("Regierungspräsident") as their first minister president. Kopf led a five-party coalition, whose basic task was to rebuild a state afflicted by the war's rigours. Kopf's cabinet had to organise an improvement of food supplies and the reconstruction of the cities and towns destroyed by Allied air raids during the war years. Hinrich Wilhelm Kopf remained – interrupted by the time in office of Heinrich Hellwege (1955–1959) – as the head of government in Lower Saxony until 1961.
The greatest problem facing the first state government in the immediate post-war years was the challenge of integrating hundreds of thousands of refugees from Germany's former territories in the east (such as Silesia and East Prussia), which had been annexed by Poland and the Soviet Union. Lower Saxony was at the western end of the direct escape route from East Prussia and had the longest border with the Soviet Zone. On 3 October 1950 Lower Saxony took over the sponsorship of the very large number of refugees from Silesia. In 1950 there was still a shortage of 730,000 homes according to official figures.
During the period when Germany was divided, the Lower Saxon border crossing at Helmstedt found itself on the main transport artery to West Berlin and, from 1945 to 1990 was the busiest European border crossing point.
Of economic significance for the state was the "Volkswagen" concern, that restarted the production of civilian vehicles in 1945, initially under British management, and in 1949 transferred into the ownership of the newly founded country of West Germany and state of Lower Saxony. Overall, Lower Saxony, with its large tracts of rural countryside and few urban centres, was one of the industrially weaker regions of the federal republic for a long time. In 1960, 20% of the working population worked on the land. In the rest of the federal territory the figure was just 14%. Even in economically prosperous times the jobless totals in Lower Saxony are constantly higher than the federal average.
In 1961 Georg Diederichs took office as the minister president of Lower Saxony as the successor to Hinrich Wilhelm Kopf. He was replaced in 1970 by Alfred Kubel. The arguments about the Gorleben Nuclear Waste Repository, that began during the time in office of minister president Ernst Albrecht (1976–1990), have played an important role in state and federal politics since the end of the 1970s.
In 1990 Gerhard Schröder entered the office of minister president. On 1 June 1993 the new Lower Saxon constitution entered force, replacing the "Provisional Lower Saxon Constitution" of 1951. It enables referenda and plebiscites and establishes environmental protection as a fundamental state principle.
The former Hanoverian Amt Neuhaus with its parishes of Dellien, Haar, Kaarßen, Neuhaus (Elbe), Stapel, Sückau, Sumte and Tripkau as well as the villages of Neu Bleckede, Neu Wendischthun and Stiepelse in the parish of Teldau and the historic Hanoverian region in the forest district of Bohldamm in the parish of Garlitz transferred with effect from 30 June 1993 from Mecklenburg-Vorpommern to Lower Saxony (Lüneburg district). From these parishes the new municipality of Amt Neuhaus was created on 1 October 1993.
In 1998 Gerhard Glogowski succeeded Gerhard Schröder who became Federal Chancellor. Because he had been linked with various scandals in his home city of Brunswick, he resigned in 1999 and was replaced by Sigmar Gabriel.
From 2003 to his election as Federal President in 2010 Christian Wulff was minister president in Lower Saxony. The Osnabrücker headed a CDU-led coalition with the FDP as does his successor, David McAllister. After the elections on 20 January 2013 McAllister was deselected.
Administrative subdivisions.
Between 1946 and 2004, the state's districts and independent towns were grouped into eight regions, with different status for the two regions ("Verwaltungsbezirke") comprising the formerly free states of Brunswick and Oldenburg. In 1978 the regions were merged into four governorates ("Regierungsbezirke"): Since 2004 the Bezirksregierungen (regional governments) have been broken up again.
1946–1978:
1978–2004:
On 1 January 2005 the four administrative regions or governorates ("Regierungsbezirke"), into which Lower Saxony had been hitherto divided, were dissolved. These were the governorates of Braunschweig, Hanover, Lüneburg and Weser-Ems.
Demographics.
There are about 500,000 non-German citizens in Lower Saxony. The following table illustrates the largest minority groups in Lower Saxony (2013).
Economy.
Agriculture has always been a very important economic factor in Lower Saxony. Wheat, potatoes, rye, and oats as well as beef, pork and poultry are some of the state's present-day agricultural products. The north and northwest of Lower Saxony are mainly made up of coarse sandy soil that makes crop farming difficult and therefore grassland and cattle farming are more prevalent in those areas. Towards the south and southeast, extensive loess layers in the soil left behind by the last ice age allow high-yield crop farming. One of the principal crops there is sugar beet.
Mining has been an important source of income in Lower Saxony for centuries. Silver ore became a foundation of notable economic prosperity in the Harz Mountains as early as the 12th century, while iron mining in the Salzgitter area and salt mining in various areas of the state became another important economic backbone. Although overall yields are comparatively low, Lower Saxony is also an important supplier of crude oil in the European Union. Mineral products still mined today include iron and lignite.
Radioactive waste is frequently transported in the area to the city of Salzgitter, for the deep geological repository Schacht Konrad and between Schacht Asse II in the Wolfenbüttel district and Lindwedel and Höfer.
Manufacturing is another large part of the regional economy. Despite decades of gradual downsizing and restructuring, the car maker Volkswagen with its five production plants within the state's borders still remains the single biggest private-sector employer, its world headquarters based in Wolfsburg. Due to the Volkswagen Law, which has recently been ruled illegal by the European Union's high court, the state of Lower Saxony is still the second largest shareholder, owning 20.3% of the company. Thanks to the importance of car manufacturing in Lower Saxony, a thriving supply industry is centred around its regional focal points. Other mainstays of the Lower Saxon industrial sector include aviation, shipbuilding (e.g. Meyer Werft), biotechnology, and steel.
The service sector has gained importance following the demise of manufacturing in the 1970s and 1980s. Important branches today are the tourism industry with TUI AG in Hanover, one of Europe's largest travel companies, as well as trade and telecommunication.
Politics.
Since 1948, politics in the state has been dominated by the rightist Christian Democratic Union (CDU) and the leftist Social Democratic Party. Lower Saxony was one of the origins of the German environmentalist movement in reaction to the state government's support for underground nuclear waste disposal. This led to the formation of the German Green Party in 1980.
The former Minister-President, Christian Wulff, led a coalition of his CDU with the Free Democratic Party between 2003 and 2010. In the 2008 election, the ruling CDU held on to its position as the leading party in the state, despite losing votes and seats. The CDU's coalition with the Free Democratic Party retained its majority although it was cut from 29 to 10. The election also saw the entry into the state parliament for the first time of the leftist The Left party. On 1 July 2010 David McAllister was elected Minister-President.
After the state election on 20 January 2013, Stephan Weil of the Social Democrats was elected as the new Minister-President. He governs in coalition with the Greens.
Constitution.
The state of Lower Saxony was formed after World War II by merging the former states of Hanover, Oldenburg, Brunswick and Schaumburg-Lippe. Hanover, a former kingdom, is by far the largest of these contributors by area and population and has been a province of Prussia since 1866. The city of Hanover is the largest and capital city of Lower Saxony.
The constitution states that Lower Saxony be a free, republican, democratic, social and environmentally sustainable state inside the Federal Republic of Germany; universal human rights, peace and justice are preassigned guidelines of society, and the human rights and civil liberties proclaimed by the constitution of the Federal Republic are genuine constituents of the constitution of Lower Saxony. Each citizen is entitled to education and there is universal compulsory school attendance.
All government authority is to be sanctioned by the will of the people, which expresses itself via elections and plebiscites. The legislative assembly is a unicameral parliament elected for terms of five years. The composition of the parliament obeys to the principle of proportional representation of the participating political parties, but it is also ensured that each constituency delegates one directly elected representative. If a party wins more constituency delegates than their statewide share among the parties would determine, it can keep all these constituency delegates.
The governor of the state (prime minister) and his ministers are elected by the parliament. As there is a system of five political parties in Germany and so also in Lower Saxony, it is usually the case that two or more parties negotiate for a common political agenda and a commonly determined composition of government where the party with the biggest share of the electorate fills the seat of the governor.
The states of the Federal Republic of Germany, and so Lower Saxony, have legislative responsibility and power mainly reduced to the policy fields of the school system, higher education, culture and media and police, whereas the more important policy fields like economic and social policies, foreign policy etc. are a prerogative of the federal government. Hence the probably most important function of the federal states is their representation in the Federal Council (Bundesrat), where their approval on many crucial federal policy fields, including the tax system, is required for laws to become enacted.
Minister-President of Lower Saxony.
The Minister-President heads the state government, acting as a head of state (even if the federated states have the status of a state, they don't established the office of a head of state but merged the functions with the head of the executive branch) as well as the government leader. He is elected by the Landtag of Lower Saxony.
Religion.
As of 2009 the Evangelical Church in Germany was the faith of 49.7% of the population. It is organised in the five Landeskirchen named Evangelical Lutheran State Church in Brunswick (comprising the former Free State of Brunswick), Evangelical Lutheran Church of Hanover (comprising the former Province of Hanover), Evangelical Lutheran Church in Oldenburg (comprising the former Free State of Oldenburg), Evangelical Lutheran Church of Schaumburg-Lippe (comprising the former Free State of Schaumburg-Lippe), and Evangelical Reformed Church (covering all the state).
The Catholic Church was the faith of 17.5% of the population in 2009. It is organised in the three dioceses of Osnabrück (western part of the state), Münster (comprising the former Free State of Oldenburg) and Hildesheim (northern and eastern part of the state). 32.8% of the Low Saxons were irreligious or adhere to other religions. Islam is a minority faith.
Coat of arms.
The coat of arms shows a white horse (Saxon Steed) on red ground, which is an old symbol of the Saxon people. Legend has it that the horse was a symbol of the Saxon leader Widukind. But this one should have been black. The colour has been changed by Christian baptism of Widukind into white. White and red are the other colours (despite to Gold and black) of the Holy Roman Empire symbolizing Christ as the Saviour, who is still shown with a white flag with a red cross.

</doc>
<doc id="18439" url="https://en.wikipedia.org/wiki?curid=18439" title="LTJ Bukem">
LTJ Bukem

LTJ Bukem is the stage name used by the drum and bass musician, producer and DJ Danny Williamson (born 1967). He and his record label Good Looking are most associated with the jazzy, atmospheric side of drum and bass music.
Life and career.
He was trained as a classical pianist and discovered jazz fusion in his teenage years, having a jazz funk band at one stage. However, by the late 1980s he decided to become a DJ, and gained fame in the rave scene of the early 1990s. As a producer, he released a series of drum and bass tracks such as "Logical Progression" (1991), "Demon's Theme" (1992), "Atlantis" and "Music" (1993). His most notable release was the track "Horizons" (1995) which attained considerable popularity.
He then dipped in visibility as a producer, with his work running the London club night "Speed" and his record label Good Looking Records coming to the fore. A series of compilations entitled "Logical Progression" highlighted a jazz and ambient influenced side of drum and bass. The style became widely known as intelligent drum and bass, although Bukem himself was opposed to the moniker, unhappy with the implication that other styles of jungle were not intelligent. Bukem also explored the downtempo end of electronic lounge music, with sister label Cookin' and the "Earth" series of compilations. Some of the artists who rose to fame under Good Looking in this period include Blame, Seba, Big Bud, Blu Mar Ten, DJ Dream (Aslan Davis), Future Engineers, Tayla, Aquarius (an alias of Photek), Peshay, Source Direct and Artemis.
On 16 July 1995 he did an Essential Mix alongside MC Conrad. In 1997 he remixed the James Bond theme for David Arnold's concept album of James Bond music. In 2000 he finally released a debut solo album, the double-CD "Journey Inwards". The album heavily emphasised his jazz fusion influences. 2001 saw a remix of Herbie Hancock.
He ran the Speed clubnight in London with fellow drum and bass DJ Fabio.
He DJs extensively around the world, often under the 'Progression Sessions' or 'Bukem in Session' banners, with MC Conrad.
Style and influences.
Viewed as an innovator in the drum and bass style, Bukem is known for developing an accessible alternative to that hardcore genre's speedy, assaultive energies. His style pays homage to the Detroit-based sound of early techno, but Bukem also incorporates still earlier influences, particularly the mellow, melodic sonorities of 1970s era jazz fusion as exemplified by Lonnie Liston Smith and Roy Ayers. Early in his career, Bukem was identified for his response to the "almost paranoid hyperkinesis" of breakbeat-based house music, and specifically for his reservations regarding the overbearing force of the hardcore mentality.
Bukem's music from the early 1990s onward represents his efforts to map out an alternative future for drum and bass by incorporating softer-edged influences culled from London's 1980s rare groove and acid jazz scenes. Music on "Logical Progression" reveals these influences, as does his approach on 1993's "Music / Enchanted", which features string arrangements and sounds from nature. His use of keyboards, live vocals and slow- motion breaks on these and future releases earned Bukem's music the tag intelligent drum and bass. While this designation caused controversy within the drum and bass community, it also influenced the popularisation of hardcore music in the UK during the mid-1990s.
Discography.
Remixes

</doc>
