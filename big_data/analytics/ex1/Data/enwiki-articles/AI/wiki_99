<doc id="65811" url="https://en.wikipedia.org/wiki?curid=65811" title="Thebes, Egypt">
Thebes, Egypt

Thebes (, "Thēbai"), known to the ancient Egyptians as Waset, was an ancient Egyptian city located east of the Nile about south of the Mediterranean. Its ruins lie within the modern Egyptian city of Luxor. Thebes was the main city of the fourth Upper Egyptian nome (Sceptre nome). It was close to Nubia and the eastern desert, with their valuable mineral resources and trade routes. It was a cult center and the wealthiest city of ancient Egypt at its heyday. The site of Thebes includes areas on both the eastern bank of the Nile, where the temples of Karnak and Luxor stand and the city proper situated; and the western bank, where a necropolis of large private and royal cemeteries and funerary complexes can be found.
Toponymy.
The Ancient Egyptians originally knew Thebes as Waset ("wꜣs.t"), the "City of the Was". A "was" was the scepter of the pharaohs, a long staff with an animal's head and a forked base.
Thebes is the latinized form of the Greek "Thebai", the hellenized form of the Demotic Egyptian . This was the local name not for the city itself but for the Karnak temple complex on the northern east bank of the city. (' in formal Egyptian.) As early as Homer's "Iliad", the Greeks distinguished the Egyptian Thebes as , (, "Thēbai hekatómpyloi") as opposed to the "Thebes of the Seven Gates" (, "Thēbai heptapyloi") in Boeotia, Greece.
From the end of the New Kingdom, Thebes was known in Egyptian as , the "City of Amun". Amun was the chief of the Theban Triad of gods whose other members were Mut and Khonsu. This name appears in the Bible as the () of the Book of Nahum and probably also as the "No" () mentioned in Ezekiel and Jeremiah. In the "interpretatio graeca", Amun was seen as a form of Zeus. The name was therefore translated into Greek as Diospolis, the "City of Zeus". To distinguish it from the numerous other cities by this name, it was known as the (, "megálē Dióspolis"; ). The Greek names came into wider use after the conquest of Egypt by Alexander the Great, when the country came to be ruled by the Macedonian Ptolemaic dynasty.
Characteristics.
Geography.
Thebes was located along the banks of the Nile River in the middle part of Upper Egypt about 800 km from the Delta. It was built largely on the alluvial plains of the Nile Valley which follows a great bend of the Nile. As a natural consequence, the city was laid in a northeast-southwest axis parallel to the contemporary river channel. Thebes had an area of 93 km2 (36 sq mi) which included parts of the Theban Hills in the west that culminates at the sacred 420-meter (1,378-foot) al-Qurn. In the east lies the mountainous Eastern Desert with its wadis draining into the valley. Significant of these wadis is Wadi Hammamat near Thebes. It was used as an overland trade route going to the Red Sea coast.
In the fourth Upper Egyptian nome, Thebes was found to have neighboring towns such as Per-Hathor, Madu, Djerty, Iuny, Sumenu and Imiotru.
Demographics.
According to George Modelski, Thebes had about 40,000 inhabitants in 2000 BC (compared to 60,000 in Memphis, the largest city of the world at the time). By 1800 BC, the population of Memphis was down to about 30,000, making Thebes the largest city in Egypt at the time. Historian Ian Morris estimated that by 1500 BC, Thebes may have grown to be the largest city in the world, with a population of about 75,000, a position which it held until about 900 BC, when it was surpassed by Nimrud (among others).
Economy.
The archaeological remains of Thebes offer a striking testimony to Egyptian civilization at its height. The Greek poet Homer extolled the wealth of Thebes in the Iliad, Book 9 (c. 8th Century BC): "... in Egyptian Thebes the heaps of precious ingots gleam, the hundred-gated Thebes."
Culture.
More than sixty annual festivals were celebrated in Thebes. The major festivals among these according to the Edfu Geographical Text were: the Beautiful Feast of Opet, the Khoiak (Festival), Festival
of I Shemu, and Festival of II Shemu. Another popular festivity was the halloween-like Beautiful Festival of the Valley.
History.
Old Kingdom.
Thebes was inhabited from around 3200 BC. It was the eponymous capital of Waset, the fourth Upper Egyptian nome. At this time it was still a small trading post while Memphis served as the royal residence of Old Kingdom pharaohs. Although no buildings survive in Thebes older than the portions of the Karnak temple complex, which may date from the Middle Kingdom, the lower part of a statue of Pharaoh Nyuserre of the 5th Dynasty has been found in Karnak. Another statue which was dedicated by the 12th Dynasty king Senusret may have been usurped and re-used, since the statue bears a cartouche of Nyuserre on its belt. Since seven rulers of the 4th to 6th Dynasties appear on the Karnak king list, perhaps at the least there was a temple in the Theban area which dated to the Old Kingdom.
First Intermediate Period.
By 2160 BC a new line of pharaohs (the Ninth and
Tenth Dynasties) consolidated Lower Egypt and northern parts of Upper Egypt from their
capital in Herakleopolis Magna. A rival line (the
Eleventh Dynasty) based at Thebes ruled the remaining part of Upper Egypt.
The Theban rulers were apparently descendants of the prince of Thebes, Intef the Elder. His probable grandson Intef I was the first of the family to claim in life a partial pharaonic titulary, though his power did not extend much further than the general Theban region.
Middle Kingdom.
Finally by c. 2050 BC, Intef III's son Mentuhotep II (meaning "Montu is satisfied"), took the Herakleopolitans by force and reunited Egypt once again under one ruler, thereby starting the period now known as the "Middle Kingdom". Mentuhotep II ruled for 51 years and built the first mortuary temple at Deir el-Bahri, which most likely served as the inspiration for the later and larger temple built next to it by Hatshepsut in the 18th Dynasty. After these events, the 11th Dynasty was short-lived, as less than 20 years had elapsed between the death of Mentuhotep II and that of Mentuhotep IV, in mysterious circumstances.
During the 12th Dynasty, Amenemhat I moved the seat of power North to Itjtawy. Thebes continued to thrive as a religious center as the local god Amun was increasingly becoming prominent throughout Egypt. The oldest remains of a temple dedicated to Amun date to the reign of Senusret I. Thebes was already, in the Middle Kingdom a town of considerable size. Excavations around the Karnak temple show that the Middle Kingdom town had the layout with a grid pattern. The city was at least one kilometer long and 50 hectares in area. Remains of two palatial buildings were also detected.
Starting in the later part of the 12th Dynasty, a group of Canaanite people began settling in the eastern Nile Delta. They eventually founded the 14th Dynasty at Avaris in c. 1805 BC or c. 1710 BC. By doing so, the Asiatics established hegemony over the majority of the Delta region, subtracting these territories from the influence of the 13th Dynasty that had meanwhile succeeded the 12th.
Hyksos Period.
A second wave of Asiatics called Hyksos (from "Heqa-khasut", "rulers of foreign lands" as Egyptians called their leaders) immigrated into Egypt and overran the Canaanite center of power at Avaris, starting the 15th Dynasty there. The Hyksos kings gained the upper hand over Lower Egypt early into the Second Intermediate Period (1657-1549 BC). When the Hyksos took Memphis during or shortly after Merneferre Ay's reign (c. 1700 BC), the rulers of the 13th Dynasty fled south to Thebes, which was restored as capital.
Theban princes (now known as the 16th Dynasty) firmly stood over their immediate region as the Hyksos advanced from the Delta southwards to Middle Egypt. The Thebans resisted the Hyksos' further advance by forming an agreement for a peaceful concurrent rule between them. The Hyksos were able sail upstream past Thebes and some Nile cataracts to trade with the Nubians and the Thebans brought their herds to the Delta without adversaries. The status quo continued until Hyksos ruler Apophis (15th Dynasty) insulted Seqenenre Tao (17th Dynasty) of Thebes. Soon the armies of Thebes marched on the Hyksos-ruled lands. Tao died in battle and his son Kamose took charge of the campaign. After Kamose's death, his brother Ahmose I continued till he captured Avaris, the Hyksos capital. Ahmose I drove the Hyksos out of Egypt and the Levant and reclaimed the lands formerly ruled by them.
New Kingdom and height of Thebes.
Ahmose I founded a new age for a unified Egypt with Thebes as its capital. The city remained as capital during most of the 18th Dynasty (New Kingdom). It also became the center for a newly established professional civil service, where there was a greater demand for scribes and the literate as the royal archives began to fill with accounts and reports. At the city the favored few of 
Nubia were reeducated with Egyptian culture, to serve as administrators of the colony. 
With Egypt stabilized again, religion and religious centers flourished and none more so than Thebes. For instance, Amenhotep III, poured much of his vast wealth from foreign tribute into the temples of Amun. The Theban god Amun became a principal state deity and every building project sought to outdo the last in proclaiming the glory of Amun and the pharaohs themselves. Thutmose I (reigned 1506-1493 BC) began the first great expansion of the venerated Karnak temple. After this, colossal enlargements of the temple became the norm throughout the New Kingdom.
Queen Hatshepsut (reigned 1479-1458 BC) made the Theban economy flourish by renewing trade networks, primarily the Red Sea trade between Thebes' Red Sea port of Al-Qusayr, Elat and the land of Punt. Her successor Thutmose III brought to Thebes a great deal of his war booty that originted as far as Mittani. The 18th Dynasty reached its peak during his son Amenhotep III's reign (1388–1350 BC). Aside from embellishing the temples of Amun, Amenhotep increased construction in Thebes to unprecedented levels. On the west bank, he built the enormous mortuary temple and the equally massive Malkata palace-city which fronted a 364-hectare artificial lake. In the city proper he built the Luxor temple and the Avenue of the Sphinxes leading to Karnak.
For a brief period in the reign of Amenhotep III's son Akhenaten (1351–1334 BC), Thebes fell on evil times; the city was abandoned by the court, and the worship of Amun was proscribed. The capital was moved to the new city of Amarna (Akhetaten), midway between Thebes and Memphis. After his death, his son Tutankhamun made a return to Memphis, but renewed interest in building projects at Thebes which produced even more glorious temples and shrines.
With the 19th Dynasty the seat of government moved to the Delta. Thebes maintained its revenues and prestige through the reigns of Seti I (1290–1279 BC) and Ramesses II (1279–1213 BC), who still resided for part of every year in Thebes. Ramesses II carried out extensive building projects in the city which include enormous statues and obelisks, the third enclosure wall of Karnak temple, additions to the Luxor temple, and the Ramesseum, his grand mortuary temple. The constructions were bankrolled by the large-scale granaries (built around the Ramesseum) which concentrated the taxes collected from Upper Egypt; and by the gold mining expeditions to Nubia and the Eastern Desert. Under Ramesses' long 66-year reign, Egypt and Thebes reached an overwhelming state of prosperity which equaled or even surpassed the earlier peak.
The city continued to be well-kept in the early 20th Dynasty. The Great Harris Papyrus states that Ramesses III (reigned 1187–56) donated 86,486 slaves and vast estates to the temples of Amun. Ramesses III received tributes from all subject peoples including the Sea Peoples and Meshwesh Libyans. Egypt as a whole, however, was experiencing financial problems, that even reached Thebes' village of Deir el-Medina. In the 25th year of his reign, workers in Deir el-Medina began striking for pay and there arose a general unrest of all social classes. Consequently, an unsuccessful harem revolt led to the deaths of many, including Theban officials and women.
Under the later Ramessids, Thebes began to decline; the government fell, it seems, into grave economic difficulties. During the reign of Ramesses IX (1129–1111 BC), about 1114 BC, a series of investigations into the plundering of royal tombs in the necropolis of western Thebes uncovered proof of corruption in high places, following an accusation made by the mayor of the east bank against his colleague on the west. The plundered royal mummies were moved from place to place and at last deposited by the priests of Amun in a tomb-shaft in Deir el-Bahri and in the tomb of Amenhotep II. (The finding of these two hiding places in 1881 and 1898, respectively, was one of the great events of modern archaeological discovery.) Such maladministration in Thebes led to unrest.
Third Intermediate Period.
Control of local affairs tended to come more and more into the hands of the High Priests of Amun, that during the Third Intermediate Period, the High Priest of Amun exerted absolute power over the South, a counterbalance to the 21st and 22nd Dynasty kings who ruled from the Delta. Intermarriage and adoption strengthened the ties between them, daughters of the Tanite kings being installed as God’s Wife of Amun at Thebes, where they wielded greater power. Theban political influence receded only in the Late Period.
By around 750 BC, the Kushites (Nubians) were growing their influence over Thebes and Upper Egypt. Kush, the former colony of Egypt became an empire in itself. In 721 BC, King Shabaka of the Kushites defeated the combined forces of Osorkon IV (22nd Dynasty), Peftjauawybast (23rd Dynasty) Bakenranef (24th Dynasty) and reunified Egypt yet again. His reign saw a significant amount of building work undertaken throughout Egypt, especially at the city of Thebes, which he made the capital of his kingdom. In Karnak he erected a pink granite statue of himself wearing the Pschent (the double crown of Egypt). Taharqa accomplished many notable projects at Thebes (i.e. the Kiosk in Karnak) and Nubia before the Assyrians started to wage war against Egypt.
Late Period.
In 667 BC, attacked by the Assyrian king Ashurbanipal's army, Taharqa abandoned Lower Egypt and fled to Thebes. After his death three years later his nephew (or cousin) Tantamani seized Thebes, invaded Lower Egypt and laid siege to Memphis, but abandoned his attempts to conquer the country in 663 BC and retreated southwards. The Assyrians pursued him and took Thebes, whose name was added to a long list of cities plundered and destroyed by the Assyrians, as Ashurbanipal wrote:
""This city, the whole of it, I conquered it with the help of Ashur and Ishtar. Silver, gold, precious stones, all the wealth of the palace, rich cloth, precious linen, great horses, supervising men and women, two obelisks of splendid electrum, weighing 2,500 talents, the doors of temples I tore from their bases and carried them off to Assyria. With this weighty booty I left Thebes. Against Egypt and Kush I have lifted my spear and shown my power. With full hands I have returned to Nineveh, in good health.""
Thebes never regained its former political significance, but it remained an important religious centre. Assyrians installed Psamtik I (664-610 BC), who ascended to Thebes in 656 BC and brought about the adoption of his own daughter, Nitocris I, as heiress to God's Wife of Amun there. In 525 BC, Persian Cambyses II invaded Egypt and became pharaoh, subordinating the kingdom as a satrapy to the greater Achaemenid Empire.
Graeco-Roman Period.
The good relationship of the Thebans with the central power in the North ended when the native Egyptian pharaohs were finally replaced by Greeks, led by Alexander the Great. He visited Thebes during a celebration of the Opet Festival. In spite of his welcoming visit, Thebes became a center for dissent. Towards the end of the third century BC Hugronaphor (Horwennefer), possibly of Nubian origin, led a revolt against the Ptolemies in Upper Egypt. His successor Ankhmakis, held large parts of Upper Egypt until 185 BC. This revolt was supported by the Theban priesthood. After the suppression of the revolt in 185 BC, Ptolemy V, in need of the support of the priesthood, forgave them.
Half a century later the Thebans rose again, elevating a certain Harsiese to the throne in 132 BC. Harsiese, having helped himself to the funds of the royal bank at Thebes, fled the following year. In 91 BC, another revolt broke out. In the following years Thebes was subdued and the city turned into rubble.
During the Roman occupation (30 BC-349 AD), the remaining communities clustered around the pylon of the Luxor temple. Thebes became part of the Roman province of "Thebais", which later split into "Thebais Superior", centered at the city and "Thebais Inferior", centered at Ptolemais Hermiou. A Roman legion was headquartered in Luxor temple at the time of Roman campaigns in Nubia. Building did not come to an abrupt stop, but the city continued to decline. In the first century AD, Strabo described Thebes as having been relegated to a mere village.
Major sites.
Eastern Thebes:
Western Thebes:
Cultural heritage.
In 1979, the ruins of ancient Thebes were classified by UNESCO as a World Cultural Heritage site. The two great temples—Luxor Temple and Karnak—and the Valley of the Kings and the Valley of the Queens are among the great achievements of ancient Egypt.
External links.
<br>

</doc>
<doc id="65821" url="https://en.wikipedia.org/wiki?curid=65821" title="Pastiche">
Pastiche

A pastiche is a work of visual art, literature, theatre, or music that imitates the style or character of the work of one or more other artists. Unlike parody, pastiche celebrates, rather than mocks, the work it imitates.
The word "pastiche" is a French cognate of the Italian noun "pasticcio", which is a pâté or pie-filling mixed from diverse ingredients. Metaphorically, "pastiche" and "pasticcio" describe works that are either composed by several authors, or that incorporate stylistic elements of other artists' work. Pastiche is an example of eclecticism in art.
Allusion is not pastiche. A literary allusion may refer to another work, but it does not reiterate it. Moreover, allusion requires the audience to share in the author's cultural knowledge. Both allusion and pastiche are mechanisms of intertextuality.
By art.
Literature.
In literature usage, the term denotes a literary technique employing a generally light-hearted tongue-in-cheek imitation of another's style; although jocular, it is usually respectful.
For example, many stories featuring Sherlock Holmes, originally created by Arthur Conan Doyle, have been written as pastiches since the author's time. Ellery Queen and Nero Wolfe are other popular subjects of mystery parodies and pastiches.
A similar example of pastiche is the posthumous continuations of the Robert E. Howard stories, written by other writers without Howard's authorization. This includes the Conan stories of L. Sprague de Camp and Lin Carter.
David Lodge's novel "The British Museum Is Falling Down" (1965) is a pastiche of works by Joyce, Kafka, and Virginia Woolf. In 1991, Alexandra Ripley wrote the novel "Scarlett", a pastiche of "Gone with the Wind", in an unsuccessful attempt to have it recognized as a canonical sequel.
Music.
For instance, Charles Rosen has characterized Mozart's various works in imitation of Baroque style as pastiche, and Edvard Grieg's Holberg Suite was written as a conscious homage to the music of an earlier age. Some of Pyotr Ilyich Tchaikovsky's works, such as his Variations on a Rococo Theme and Serenade for Strings, employ a poised "Classical" form reminiscent of 18th-century composers such as Mozart (the composer whose work was his favorite). Perhaps one of the best examples of pastiche in modern music is the that of George Rochberg, who used the technique in his String Quartet No. 3 of 1972 and Music for the Magic Theater. Rochberg turned to pastiche from serialism after the death of his son in 1963.
"Bohemian Rhapsody" by Queen is unusual as it is a pastiche in both senses of the word, as there are many distinct styles imitated in the song, all 'hodge-podged' together to create one piece of music. A similar earlier example is "Happiness is a Warm Gun" by The Beatles. It's possible to find musical "pastiches" in the whole work of american composer Frank Zappa.
A "pastiche Mass" is a Mass where the constituent movements are from different Mass settings. Most often this convention has been chosen for concert performances, particularly by early music ensembles. Masses are composed of movements: Kyrie, Gloria, Credo, Sanctus, Agnus Dei; for example, the "Missa Solemnis" by Beethoven and the "Messe de Nostre Dame" by Guillaume de Machaut. In a pastiche Mass, the performers may choose a Kyrie from one composer, and a Gloria from another, or, choose a Kyrie from one setting of an individual composer, and a Gloria from another.
Musical theater.
In musical theater pastiche is often an indispensable tool for evoking the sounds of a particular era for which a show is set. For his 1971 musical Follies, a show about a reunion of performers from a musical revue set between the World Wars, Stephen Sondheim wrote over a dozen songs in the style of Broadway songwriters of the 1920s and 1930s. Sondheim imitates not only the music of composers such as Cole Porter, Irving Berlin, Jerome Kern, and George Gershwin but also the lyrics of writers such as Ira Gershwin, Dorothy Fields, Otto Harbach, and Oscar Hammerstein II. For example, Sondheim notes that the torch song "Losing My Mind" sung in the show contains “near-stenciled rhythms and harmonies” from the Gershwins’ "The Man I Love" and lyrics written in the style of Dorothy Fields. Examples of musical pastiche can also be found in other Sondheim shows including Gypsy, Saturday Night, and Anyone Can Whistle.
Film.
Pastiche can also be a cinematic device wherein the creator of the film pays homage to another filmmaker's style and use of cinematography, including camera angles, lighting, and mise en scène. A film's writer may also offer a pastiche based on the works of other writers (this is especially evident in historical films and documentaries but can be found in non-fiction drama, comedy and horror films as well). A major filmmaker, Quentin Tarantino often uses various plots, characteristics and themes from many lesser-known films to create his films. He has even openly stated that he "steals from everyone".
In cinema, the influence of George Lucas' Star Wars films (spawning their own pastiches – see the 1983 3D film "") can be regarded as a function of postmodernity.
Architecture.
In urban planning, "pastiche" is used to describe developments as imitations of the building styles created by major architects: the implication is that the work is unoriginal and of little merit, and the term is generally attributed without reference to its urban context. Many post-war European developments can in this way be described as pastiches of the work of architects and planners such as Le Corbusier or Ebenezer Howard. The term itself is not pejorative, however Alain de Botton describes pastiche as "an unconvincing reproduction of the styles of the past".

</doc>
<doc id="65823" url="https://en.wikipedia.org/wiki?curid=65823" title="Snake Jailbird">
Snake Jailbird

Snake Jailbird (usually referred to as simply Snake) is a recurring fictional character in the animated television series "The Simpsons", who is voiced by Hank Azaria. Snake's first appearance was in the episode "The War of the Simpsons". His catchphrase is "Bye!", which he usually says when he's in trouble. His real name is Chester Turley, but he is also known under the alias Albert Knickerbocker Aloysius Snake. He was named the 19th (out of 25) of IGN's Top 25 Simpsons Peripheral Characters.
Role in "The Simpsons".
Snake is Springfield's resident recidivist felon, always getting arrested but rarely staying in jail. He speaks with a Valley Boy accent. He is partial to fast cars and fast women, and has a knack for reckless abandon. He was formerly an idealistic Indiana Jones-type archaeologist until Moe Szyslak stole the Mayan gold coins he had discovered and was going to donate to the museum, and he decided to take his revenge out on society (convenience stores).
He also attended Middlebury College and repaid his student loans after robbing Moe Szyslak's tavern. He once ran a telemarketing scam, but could not stick to it because he did not like bothering people at home. He also wrote the books, "The Ten Habits of Highly Successful Criminals" and "A Criminal's Guide to Hiding in Mexico". When wearing a prison uniform, his prison number is 7F20, which is the production code of his debut episode.
In "Pygmoelian", it was revealed that Snake had a son. "Luca$" and "The Seemingly Never-Ending Story" both give the son's name as Jeremy. The episode reveals that Snake has custody issues with his son. He is a loving father to Jeremy; unfortunately he is raising his son to be much like himself. In "Pygmoelian", he helps his son steal Lisa's bicycle, and beams with pride when his son exclaims, "Daddy, I'm stealing!" In addition to his son, Snake has a brother (or other male relative) who looks and talks very similarly, as seen in "The Springfield Connection" where the two run a Three-card Monte game. His girlfriend Gloria (who is awkwardly a cop) briefly dated Mr. Burns, but then came back to him. Gloria was voiced by Julia Louis-Dreyfus on several occasions. (In one instance, she visits Snake in prison, where he tells her to kill someone who's been editing his Wikipedia page.)
In "Luca$", he becomes friends with Bart, and Bart tries to protect him from the electric chair.
Other appearances.
He is a major character in ', and a supporting character in "The Simpsons Game". He also appears in '. He appears in "The Simpsons Movie", as part of the local mob headed by Fat Tony.
Character.
Creation.
Snake first appeared in the season two episode "The War of the Simpsons". His first name was first mentioned by Sideshow Bob in "Black Widower". In the script, the writers had simply mentioned a character named Snake and it had been the directors who had assigned that existing character design to the name. Snake's voice is based on Hank Azaria's old college roommate.
Name.
In the episode "The Seemingly Never-Ending Story", Lisa tells a story in which Snake refers to himself as Professor Jailbird. In "Luca$", his name is said to be Albert Knickerbocker Aloysius Snake.

</doc>
<doc id="65827" url="https://en.wikipedia.org/wiki?curid=65827" title="Silicone">
Silicone

Silicones are polymers that include any inert, synthetic compound made up of repeating units of siloxane, which is a chain of alternating silicon atoms and oxygen atoms, frequently combined with carbon and/or hydrogen. They are typically heat-resistant and rubber-like, and are used in sealants, adhesives, lubricants, medicine, cooking utensils, and thermal and electrical insulation. Some common forms include silicone oil, silicone grease, silicone rubber, silicone resin, and silicone caulk.
Chemistry.
More precisely called polymerized siloxanes or polysiloxanes, silicones consist of an inorganic silicon-oxygen backbone chain (⋯-Si-O-Si-O-Si-O-⋯) with organic side groups attached to the silicon atoms. These silicon atoms are tetravalent. So, silicones are polymers constructed from inorganic-organic monomers. Silicones have in general the chemical formula [R2SiO]"n", where R is an organic group such as methyl, ethyl, or phenyl. 
In some cases, organic side groups can be used to link two or more of these -Si-O- backbones together. By varying the -Si-O- chain lengths, side groups, and crosslinking, silicones can be synthesized with a wide variety of properties and compositions. They can vary in consistency from liquid to gel to rubber to hard plastic. The most common siloxane is linear polydimethylsiloxane (PDMS), a silicone oil. The second largest group of silicone materials is based on silicone resins, which are formed by branched and cage-like oligosiloxanes.
Terminology and history.
F. S. Kipping and Matt Saunders coined the word "silicone" in 1901 to describe polydiphenylsiloxane by analogy of its formula, Ph2SiO (Ph stands for phenyl, C6H5), with the formula of the ketone benzophenone, Ph2CO (his term was originally "silicoketone"). Kipping was well aware that polydiphenylsiloxane is polymeric whereas benzophenone is monomeric and noted that Ph2SiO and Ph2CO had very different chemistry. The discovery of the structural differences between Kippings' molecules and the ketones means that "silicone" is no longer the correct term (though it remains in common usage) and that the term "siloxanes" is correct according to the nomenclature of modern chemistry.
Silicone is sometimes mistakenly referred to as silicon. The chemical element silicon is a crystalline metalloid widely used in computers and other electronic equipment. Although silicones contain silicon atoms, they also include carbon, hydrogen, oxygen, and perhaps other kinds of atoms as well, and have very different physical and chemical properties to elemental silicon.
A true "silicone group" with a double bond between oxygen and silicon does not commonly exist in nature; chemists find that the silicon atom usually forms single bonds with each of two oxygen atoms, rather than a double bond to a single atom. Polysiloxanes are among the many substances commonly known as "silicones".
Molecules containing silicon-oxygen double bonds do exist and are called silanones but they are very reactive. Despite this, silanones are important as intermediates in gas-phase processes such as chemical vapor deposition in microelectronics production, and in the formation of ceramics by combustion.
Synthesis.
Most common are materials based on polydimethylsiloxane, which is derived by hydrolysis of dimethyldichlorosilane. This dichloride reacts with water as follows:
The polymerization typically produces linear chains capped with Si-Cl or Si-OH (silanol) groups. Under different conditions the polymer is a cyclic, not a chain.
For consumer applications such as caulks silyl acetates are used instead of silyl chlorides. The hydrolysis of the acetates produce the less dangerous acetic acid (the acid found in vinegar) as the reaction product of a much slower curing process. This chemistry is used in many consumer applications, such as silicone caulk and adhesives.
Branches or cross-links in the polymer chain can be introduced by using organosilicon precursors with fewer methyl groups, such as methyltrichlorosilane and methyltrimethoxysilane. Ideally, each molecule of such a compound becomes a branch point. This process can be used to produce hard silicone resins. Similarly, precursors with three methyl groups can be used to limit molecular weight, since each such molecule has only one reactive site and so forms the end of a siloxane chain.
Combustion.
When silicone is burned in air or oxygen, it forms solid silica (silicon dioxide) as a white powder, char, and various gases. The readily dispersed powder is sometimes called silica fume.
Properties.
Silicones exhibit many useful characteristics, including:
Uses.
Silicones are used in many products. Ullmann's Encyclopedia of Industrial Chemistry lists the following major categories of application: Electrical (e.g., insulation), electronics (e.g., coatings), household (e.g., sealants for cooking apparatus), automobile (e.g., gaskets), aeroplane (e.g., seals), office machines (e.g., keyboard pads), medicine/dentistry (e.g., teeth impression molds), textiles/paper (e.g., coatings). For these applications, an estimated 400,000 tons of silicones were produced in 1991. Specific examples, both large and small are presented below.
Automotive.
In the automotive field, silicone grease is typically used as a lubricant for brake components since it is stable at high temperatures, is not water-soluble, and is far less likely than other lubricants to foul. It is also used as DOT 5 brake fluid.
Automotive spark plug wires are insulated by multiple layers of silicone to prevent sparks from jumping to adjacent wires, causing misfires. Silicone tubing is sometimes used in automotive intake systems (especially for engines with forced induction).
Sheet silicone is used to manufacture gaskets used in automotive engines, transmissions, and other applications.
Automotive body manufacturing plants and paint shops avoid silicones, as they may cause "fish eyes", small, circular craters in the finish.
Additionally, silicone compounds such as silicone rubber are used as coatings and sealants for airbags; the high strength of silicone rubber makes it an optimal adhesive/sealant for high impact airbags. Recent technological advancements allow convenient use of silicone in combination with thermoplastics to provide improvements in scratch and mar resistance and lowered coefficient of friction.
Coatings.
Silicone films can be applied to such silica-based substrates as glass to form a covalently bonded hydrophobic coating.
Many fabrics can be coated or impregnated with silicone to form a strong, waterproof composite such as silnylon.
Defoaming.
Silicones are used as active compound in defoamers due to their low water solubility and good spreading properties.
Dry cleaning.
Liquid silicone can be used as a dry cleaning solvent, providing an alternative to the traditional chlorine-containing perchloroethylene (perc) solvent. Use of silicones in dry cleaning reduces the environmental impact of a typically high-polluting industry.
Electronics.
Electronic components are sometimes encased in silicone to increase stability against mechanical and electrical shock, radiation and vibration, a process called "potting".
Silicones are used where durability and high performance are demanded of components under hard conditions, such as in space (satellite technology). They are selected over polyurethane or epoxy encapsulation when a wide operating temperature range is required (−65 to 315 °C). Silicones also have the advantage of little exothermic heat rise during cure, low toxicity, good electrical properties and high purity.
The use of silicones in electronics is not without problems, however. Silicones are relatively expensive and can be attacked by solvents. Silicone easily migrates as either a liquid or vapor onto other components.
Silicone contamination of electrical switch contacts can lead to failures by causing an increase in contact resistance, often late in the life of the contact, well after any testing is completed. Use of silicone-based spray products in electronic devices during maintenance or repairs can cause later failures.
Firestops.
Silicone foam has been used in North American buildings in an attempt to firestop openings within fire-resistance-rated wall and floor assemblies to prevent the spread of flames and smoke from one room to another. When properly installed, silicone-foam firestops can be fabricated for building code compliance. Advantages include flexibility and high dielectric strength. Disadvantages include combustibility (hard to extinguish) and significant smoke development.
Silicone-foam firestops have been the subject of controversy and press attention due to smoke development from pyrolysis of combustible components within the foam, hydrogen gas escape, shrinkage, and cracking. These problems have led to reportable events among licensees (operators of nuclear power plants) of the Nuclear Regulatory Commission (NRC).
Silicone firestops are also used in aircraft.
Lubricants.
Silicone greases are used for many purposes, such as bicycle chains, airsoft gun parts, and a wide range of other mechanisms. Typically, a dry-set lubricant is delivered with a solvent carrier to penetrate the mechanism. The solvent then evaporates, leaving a clear film that lubricates but does not attract dirt and grit as much as an oil-based or other traditional "wet" lubricant.
Silicone personal lubricants are also available for use in medical procedures or sexual activity. See below.
Medicine.
Silicone is used in microfluidics, seals, gaskets, shrouds, and other applications requiring high biocompatibility. Additionally, the gel form is used in bandages and dressings, breast implants, testicle implants, pectoral implants, contact lenses, and a variety of other medical uses.
Scar treatment sheets are often made of medical grade silicone due to its durability and biocompatibility. Polydimethylsiloxane is often used for this purpose, since its specific crosslinking results in a flexible and soft silicone with high durability and tack.
Polydimethylsiloxane (PDMS) has been used as the hydrophobic block of amphiphilic synthetic block copolymers used to form the vesicle membrane of polymersomes.
Moldmaking.
Two-part silicone systems are used to create rubber molds used to cast resins, foams, rubber, and low-temperature alloys. A silicone mold generally requires little or no mold-release or surface preparation, as most materials do not adhere to silicone. For experimental uses, ordinary one-part silicone can be used to make molds or to mold into shapes. If needed, common vegetable cooking oils or petroleum jelly can be used on mating surfaces as a mold-release agent.
Cooking molds used as bakeware do not require coating with cooking oil, allowing the baked food to be more easily removed from the mold after cooking.
Ophthalmology.
Silicone has many applications like silicone oil used to replace vitreous following vitrectomy, silicone intraocular lenses following cataract extraction, silicone tubes to keep nasolacrimal passage open following dacrycystorhinostomy, canalicular stents for canalicular stenosis, punctal plugs for punctal occlusion in dry eyes, silicone rubber and bands as an external temponade in tractional retinal detachment, and anteriorly located break in rhegmatogenous retinal detachment.
Personal care.
Silicones are ingredients in many hair conditioners, shampoos, and hair gel products. Some silicones, notably the amine functionalized amodimethicones, are excellent conditioners, providing improved compatibility, feel, and softness, and lessening frizz. The phenyltrimethicones, in another silicone family, are used in reflection-enhancing and color-correcting hair products, where they increase shine and glossiness (and possibly effect subtle color changes). Phenyltrimethicones, unlike the conditioning amodimethicones, have refractive indices (typically 1.46) close to that of human hair (1.54). However, if included in the same formulation, amodimethicone and phenyltrimethicone interact and dilute each other, making it difficult to achieve both high shine and excellent conditioning in the same product.
Silicone rubber is commonly used in baby bottle nipples (teats) for its cleanliness, aesthetic appearance, and low extractable content.
Silicones are used in shaving products and personal lubricants.
Plumbing and building construction.
The strength and reliability of silicone rubber is widely acknowledged in the construction industry. One-part silicone sealants and caulks are in common use to seal gaps, joints and crevices in buildings. One-part silicones cure by absorbing atmospheric moisture, which simplifies installation. In plumbing, silicone grease is typically applied to O-rings in brass taps and valves, preventing lime from sticking to the metal.
Toys and hobbies.
Silly Putty and similar materials are composed of silicones dimethyl siloxane, polydimethylsiloxane, and decamethyl cyclopentasiloxane, with other ingredients. This substance is noted for its unusual characteristic that it bounces, but breaks when given a sharp blow; it can also flow like a liquid and will form a puddle given enough time.
Silicone "rubber bands" are a long-lasting popular replacement refill for real rubber bands in the new (2013) fad "rubber band loom" toys at two to four times the price (in 2014). However, they only come in one size, small. Silicone bands also come in bracelet sizes that can be custom embossed with a name or message. Large silicone bands are also sold as utility tie-downs and such.
Formerol is a silicone rubber (marketed as Sugru) used as an arts-and-crafts material as its plasticity allows it to be moulded by hand like modeling clay. It hardens at room temperature and it is adhesive to various substances including glass and aluminum.
In making aquariums, manufacturers now commonly use 100% silicone sealant to join glass plates. Glass joints made with silicone sealant can withstand great pressure, making obsolete the original aquarium construction method of angle-iron and putty.
This same silicone is used to make hinges in aquarium lids or for minor repairs. However, not all commercial silicones are safe for aquarium manufacture, nor is silicone used for the manufacture of acrylic aquariums as silicones do not have long-term adhesion to plastics.
Sex toys and lubricants.
Silicone is a material of choice for soft sex toys, due to its durability, cleanability, non-degradation by petroleum-based lubricants, and lack of phthalates, chemicals suspected of having carcinogenic and mutagenic effects on the skin and mucous membranes.
Production and marketing.
The global demand for silicones approached US$12.5 billion in 2008, approximately 4% up from the previous year. It continues similar growth in the following years to reach $13.5 billion by 2010. The annual growth is expected to be boosted by broader applications, introduction of novel products and increasing awareness of using more environmentally friendly materials.
The leading global manufacturers of silicone base materials belong to three regional organizations: the European Silicone Center(CES) in Brussels, Belgium; the Environment Health and Safety Council (SEHSC) in Herndon, Virginia, USA; and the Silicone Industry Association of Japan (SIAJ) in Tokyo, Japan. Dow Corning Silicones, Evonik Industries, Momentive Performance Materials, Milliken and Company (SiVance Specialty Silicones), Shin-Etsu Silicones, Wacker Chemie, Bluestar Silicones, JNC Corporation, Wacker Asahikasei Silicone, and Dow Corning Toray represent the collective membership of these organizations. A fourth organization, the Global Silicone Council (GSC) acts as an umbrella structure over the regional organizations. All four are non-profit, having no commercial role; their primary missions are to promote the safety of silicones from a health, safety, and environmental perspective. As the European chemical industry is preparing to implement the Registration, Evaluation and Authorisation of Chemicals (REACH) legislation, CES is leading the formation of a consortium of silicones, silanes, and siloxanes producers and importers to facilitate data and cost sharing.
Safety and environmental considerations.
No "marked harmful effects on organisms in the environment" have been noted for silicones. Because they are widely used, they are pervasive. They biodegrade readily, a process that is accelerated by a variety of catalysts, including clays.
Formaldehyde.
At temperatures of approximately 149 °C (300 °F) in oxygen-containing environments, the PDMS in silicone begin to generate formaldehyde, and by 200 °C (392 °F) silicones begin exponential increases of formaldehyde production, based on the thermal stability of function group used (with thermal stability as phenyl < vinyl < methyl < trifluoropropyl) While various silicones (< 3 µg CH2O/(g·hr) for a high consistency silicone rubber to 48 µg CH2O/(g·hr)) were found to be superior to mineral oil and plastics (~400 µg CH2O/(g·hr)) at about 200 °C (392 °F), by 250 °C (482 °F) copious amounts of formaldehyde have been found to be produced for all silicones (1200 µg CH2O/(g·hr) to 4600 µg CH2O/(g·hr)).
Pathology.
Leak of silicone in biological tissues produces an inflammatory reaction, as silicone-induced granuloma (siliconoma).

</doc>
<doc id="65828" url="https://en.wikipedia.org/wiki?curid=65828" title="Smithsonian Institution">
Smithsonian Institution

The Smithsonian Institution ( ), established in 1846 "for the increase and diffusion of knowledge," is a group of museums and research centers administered by the Government of the United States. Originally organized as the "United States National Museum," that name ceased to exist as an administrative entity in 1967.
Termed "the nation's attic"
for its eclectic holdings of 138 million items, the Institution's nineteen museums, nine research centers, and zoo include historical and architectural landmarks, mostly located in the District of Columbia. Additional facilities are located in Arizona, Maryland, Massachusetts, New York City, Virginia, and Panama. A further 170 museums are Smithsonian Affiliates.
The Institution's thirty million annual visitors are admitted without charge.
Funding comes from the Institution's endowment, private and corporate contributions, membership dues, government support, as well as retail, concession and licensing revenues. Institution publications include "Smithsonian" and "Air & Space" magazines.
Founding.
British scientist James Smithson (d. 1829) left most of his wealth to his nephew Henry James Hungerford. When Hungerford died childless in 1835, the estate passed "to the United States of America, to found at Washington, under the name of the Smithsonian Institution, an Establishment for the increase & diffusion of knowledge among men", in accordance with Smithson's will. Congress officially accepted the legacy bequeathed to the nation, and pledged the faith of the United States to the charitable trust on July 1, 1836. The American diplomat Richard Rush was dispatched to England by President Andrew Jackson to collect the bequest. Rush returned in August 1838 with 105 sacks containing 104,960 gold sovereigns (about $500,000 at the time, which is ).
Once the money was in hand, eight years of Congressional haggling ensued over how to interpret Smithson's rather vague mandate "for the increase and diffusion of knowledge." Unfortunately the money was invested by the US Treasury in bonds issued by the state of Arkansas which soon defaulted. After heated debate, Massachusetts Representative (and ex-President) John Quincy Adams persuaded Congress to restore the lost funds with interest and, despite designs on the money for other purposes, convinced his colleagues to preserve it for an institution of science and learning. Finally, on August 10, 1846, President James K. Polk signed the legislation that established the Smithsonian Institution as a trust instrumentality of the United States, to be administered by a Board of Regents and a Secretary of the Smithsonian.
Development.
Though the Smithsonian's first Secretary, Joseph Henry, wanted the Institution to be a center for scientific research, it also became the depository for various Washington and U.S. government collections. The United States Exploring Expedition by the U.S. Navy circumnavigated the globe between 1838 and 1842. The voyage amassed thousands of animal specimens, an herbarium of 50,000 plant specimens, and diverse shells and minerals, tropical birds, jars of seawater, and ethnographic artifacts from the South Pacific Ocean. These specimens and artifacts became part of the Smithsonian collections, as did those collected by several military and civilian surveys of the American West, including the Mexican Boundary Survey and Pacific Railroad Surveys, which assembled many Native American artifacts and natural history specimens.
In 1846, the regents developed a plan for weather observation, and in 1847, money was appropriated for meteorological research. The Institution became a magnet for young scientists from 1857 to 1866, who formed a group called the Megatherium Club.
The Smithsonian played a critical role as the U.S. partner institution in early bilateral scientific exchanges with the Academy of Sciences of Cuba.
When the Detroit philanthropist Charles Lang Freer donated his private collection to the Smithsonian and funds to build the museum to hold it (which was named the Freer Gallery), it was among the Smithsonian's first major donations from a private individual. The gallery opened in 1923.
In 2011, the Smithsonian undertook its first-ever capital fundraising campaign. The $1.5 billion effort raised $1 billion at the three-year mark. Smithsonian officials made the campaign public in October 2014 in an effort to raise the remaining $500 million. More than 60,000 individuals and organizations donated money to the campaign by the time it went public. This included 192 gifts of at least $1 million. Members of the boards of directors of various Smithsonian museums donated $372 million. The Smithsonian said that funds raised will go toward completion of the National Museum of African American History and Culture building, and renovations of the National Air and Space Museum, National Museum of American History, and the Renwick Gallery. A smaller amount of funds will go to educational initiatives and digitization of collections.
Museums.
Nineteen museums and galleries, as well as the National Zoological Park, comprise the Smithsonian museums. Eleven are on the National Mall, the strip of land that runs between the Lincoln Memorial and the United States Capitol. Other museums are located elsewhere in Washington, D.C., with two more in New York City and one in Chantilly, Virginia.
The Smithsonian has close ties with 168 other museums in 39 states, Panama and Puerto Rico. These museums are known as Smithsonian Affiliated museums. Collections of artifacts are given to these museums in the form of long-term loans. The Smithsonian also has a large number of traveling exhibitions. In 2008, 58 of these traveling exhibitions went to 510 venues across the country.
The Smithsonian Institution announced in January 2015 that it is in talks to build its first permanent overseas exhibition space within London's Queen Elizabeth II Olympic Park.
Collections.
Smithsonian collections include 138 million artworks, artifacts and specimens. The National Museum of Natural History houses 127 million of these specimens and artifacts. The Collections Search Center has 9 million digital records available online. The Smithsonian Institution Libraries hold
2 million library volumes. Smithsonian Archives hold 156,830 cubic feet of archival material.
The Smithsonian Institution has different categories of collection displays that you can visit at the museum such as its Historical collections. In 1912 The First Lady Helen Herron Taft had donated her gown to the museum for the First Ladies' Gown display. The museum also has on display for its visitors treasures such as the Star-Spangled Banner, and the Stove pipe hat that was worn by President Lincoln. They also have the Ruby slippers worn by Judy Garland in "The Wizard Of Oz." The institution also has the original Teddy Bear that was named after President Theodore Roosevelt.
Research centers and programs.
The following is a list of Smithsonian research centers, with their affiliated museum in parentheses:
Also of note is the Smithsonian Museum Support Center (MSC), located in Silver Hill, Maryland (Suitland), which is the principal off-site conservation and collections facility for multiple Smithsonian museums, primarily the National Museum of Natural History. The MSC was dedicated in May 1983. The MSC covers of land, with over of space, making it one of the largest set of structures in the Smithsonian. It has over of cabinets, and more than 31 million objects.
Publications.
The Institution publishes "Smithsonian" magazine monthly and "Air & Space" magazine bimonthly. 
"Smithsonian" was the result of Secretary of the Smithsonian S. Dillon Ripley asking the retired editor of "Life" magazine Edward K. Thompson to produce a magazine "about things in which the Smithsonian Institution is interested, might be interested or ought to be interested." Another Secretary of the Smithsonian, Walter Boyne, founded "Air & Space."
Administration.
The Smithsonian Institution was established as a trust instrumentality by act of Congress. More than two-thirds of the Smithsonian's workforce of some 6,300 persons are employees of the federal government. The Smithsonian Office of Protection Services oversees security at the Smithsonian facilities and enforces laws and regulations for National Capital Parks together with the United States Park Police.
The President's 2011 budget proposed just under $800 million in support for the Smithsonian, slightly increased from previous years. Institution exhibits are free of charge, though in 2010 the Deficit Commission recommended admission fees.
As approved by Congress on August 10, 1846, the legislation that created the Smithsonian Institution called for the creation of a Board of Regents to govern and administer the organization. This 17-member board meets at least four times a year and includes as "ex officio" members the Chief Justice of the United States and the Vice President of the United States. The nominal head of the Institution is the Chancellor, an office which has traditionally been held by the Chief Justice. In September 2007, the Board created the position of Chair of the Board of Regents, a position currently held by John W. McCarter of Illinois.
Other members of the Board of Regents are three members of the U.S. House of Representatives appointed by the Speaker of the House; three members of the Senate, appointed by the President "pro tempore" of the Senate; and nine citizen members, nominated by the Board and approved by the Congress in a joint resolution signed by the President of the United States. Regents who are senators or representatives serve for the duration of their elected terms, while citizen Regents serve a maximum of two six-year terms. Regents are compensated on a part-time basis.
The chief executive officer (CEO) of the Smithsonian is the Secretary, who is appointed by the Board of Regents. The Secretary also serves as secretary to the Board of Regents, but is not a voting member of that body. The Secretary of the Smithsonian has the privilege of the floor at the United States Senate. There have been 12 Secretaries. On September 18, 2013, Secretary G. Wayne Clough announced he would retire in October 2014. The Smithsonian Board of Regents said it has asked regent John McCarter, Jr. to lead a search committee. The search committee will consist of other regents and representatives from Smithsonian museums and centers.
On March 10, 2014, the Smithsonian Board of Directors selected Dr. David Skorton, a physician and president of Cornell University as the 13th Secretary of the Smithsonian. Skorton took the reins of the institution on 1 July 2015.
Controversies.
"Enola Gay" display.
In 1995, controversy arose over the exhibit at the National Air and Space Museum associated with display of the "Enola Gay", the Superfortress used by the United States to execute the first atomic bombing in World War II. The American Legion and Air Force Association believed the exhibit put forward only one side of the debate over the atomic bombings of Hiroshima and Nagasaki, and that it emphasized the effect on the victims without the overall context of the war. The Smithsonian changed the exhibit, displaying the aircraft only with associated technical data and without discussion of its historic role in the war.
Censorship of "Seasons of Life and Land".
In 2003, a National Museum of Natural History exhibit, Subhankar Banerjee's "Seasons of Life and Land", featuring photographs of the Arctic National Wildlife Refuge, was censored and moved to the basement by Smithsonian officials because they feared that its subject matter was too politically controversial.
In November 2007, "The Washington Post" reported internal criticism has been raised regarding the institution's handling of the exhibit on the Arctic. According to documents and e-mails, the exhibit and its associated presentation were edited at high levels to add "scientific uncertainty" regarding the nature and impact of global warming on the Arctic. Acting Secretary of the Smithsonian Cristián Samper was interviewed by the "Post", and claimed the exhibit was edited because it contained conclusions that went beyond what could be proven by contemporary climatology. The Smithsonian is now a participant in the U.S. Global Change Research Program.
Copyright restrictions.
The Smithsonian Institution provides access to its image collections for educational, scholarly and nonprofit uses. Commercial uses are generally restricted unless permission is obtained. Smithsonian images fall into different copyright categories; some are protected by copyright, many are subject to license agreements or other contractual conditions, and some fall into the public domain, such as those prepared by Smithsonian employees as part of their official duties. The Smithsonian's terms of use for its digital content, including images, are set forth on the Smithsonian Web site.
In April 2006, the institution entered into an agreement of "first refusal" rights for its vast silent and public domain film archives with Showtime Networks, mainly for use on the Smithsonian Channel, a network created from this deal. Critics contend this agreement effectively gives Showtime control over the film archives, as it requires filmmakers to obtain permission from the network to use extensive amounts of film footage from the Smithsonian archives.
The Smithsonian contends independent producers continue to have unchanged access to the institution and its collections as they had prior to the agreement. The process to gain access to film at the Smithsonian remains the same. Since January 2006, independent producers have made more than 500 requests to film in the museums and collections, and/or to use archival footage and photos.

</doc>
<doc id="65832" url="https://en.wikipedia.org/wiki?curid=65832" title="Soviet submarine K-222">
Soviet submarine K-222

Soviet submarine "K-162" was the world's fastest submarine. The first submarine constructed with a titanium hull, she was the only vessel of the Soviet Union's Project 661 "Anchar" nuclear-powered attack submarine design. The boat is best known in the West by its NATO reporting name Papa class. "K-162" was renamed K-222 in 1978.
Design.
This project was intentionally forced to be highly innovative, as it was forbidden to reuse the past technical solutions. While enforcing innovation, this also slowed the development. Project 661 began in 1959, with design task assigned to OKB-16, one of the two predecessors (the other being SKB-143) of the famous Malachite Central Design Bureau, which would eventually become one of the three Soviet/Russian submarine design centers, along with Rubin Design Bureau and Lazurit Central Design Bureau.
"K-222" was designed as an extremely fast attack submarine, and was the first submarine built with a titanium hull. She was armed with 10 SS-N-7 Starbright (П-70 «Аметист») missiles in individual tubes forward of the sail, between the inner and outer hulls, which were both of titanium alloy. Similar in design to the , "K-222" was designed to intercept and attack aircraft carrier groups. Like the Charlie class and the later , her cruise missiles could only be reloaded in port, making her one of the Soviet Navy's "one shot" boats.
"K-222" had two light-water reactors, designed to be as compact as possible. Unusually, there were no diesel generators, using the powerful battery as the emergency power source.
She is regarded as a predecessor to the and s, and may have tested technologies which were later used in those classes.
History.
"K-222" was laid down on 28 December 1963 and commissioned on 31 December 1969, at Severodvinsk. She was assigned to the Soviet Red Banner Northern Fleet for the duration of her career. She was the world's fastest submarine, reaching a record submerged speed of on trials. Her unofficial maximum speed, reached 30 March 1971, is . However, "K-222"s high speed came at the price of high costs during construction, and both excessive noise and significant damage to external hull features when used.
Though officially named, within the Soviet Navy the boat was commonly referred to as the "Golden Fish", in reference to her cost of development and construction.
On 30 September 1980, one of "K-222"s nuclear reactors was damaged during maintenance in the shipyard. By 1988, she was placed in reserve at the Belomorsk Naval Base in Severodvinsk. Beginning on 5 March 2010 the boat was dismantled at Sevmash, the only facility capable of handling the titanium hull. In an unusual move, the scrapping was performed with the reactors and nuclear fuel still on board, as no provisions had been made in the design for the reactor's removal. The scrapping also began before the European Bank for Reconstruction and Development (EBRD) chose an international consultant for the fuel's unloading.
"K-222" in popular culture.
The sole Papa-class submarine is briefly featured as participating in Soviet anti-convoy operations in the Atlantic in "Red Storm Rising" by Tom Clancy, causing both surprise and frustration to the escorts' commanders.

</doc>
<doc id="65834" url="https://en.wikipedia.org/wiki?curid=65834" title="Schindler's List">
Schindler's List

"Schindler's List" is a 1993 American epic historical period drama film, directed and co-produced by Steven Spielberg and scripted by Steven Zaillian. It is based on the novel "Schindler's Ark" by Thomas Keneally, an Australian novelist. The film is based on the life of Oskar Schindler, an ethnic German businessman who saved the lives of more than a thousand mostly Polish-Jewish refugees during the Holocaust by employing them in his factories. It stars Liam Neeson as Schindler, Ralph Fiennes as "Schutzstaffel" (SS) officer Amon Göth, and Ben Kingsley as Schindler's Jewish accountant Itzhak Stern.
Ideas for a film about the "Schindlerjuden" (Schindler Jews) were proposed as early as 1963. Poldek Pfefferberg, one of the "Schindlerjuden", made it his life's mission to tell the story of Schindler. Spielberg became interested in the story when executive Sid Sheinberg sent him a book review of "Schindler's Ark". Universal Studios bought the rights to the novel, but Spielberg, unsure if he was ready to make a film about the Holocaust, tried to pass the project to several other directors before finally deciding to direct the film himself.
Principal photography took place in Kraków, Poland, over the course of 72 days in 1993. Spielberg shot the film in black and white and approached it as a documentary. Cinematographer Janusz Kamiński wanted to give the film a sense of timelessness. John Williams composed the score, and violinist Itzhak Perlman performs the film's main theme.
"Schindler's List" premiered on November 30, 1993, in Washington, D.C. and it was released on December 15, 1993, in the United States. Often listed among the greatest films ever made, it was also a box office success, earning $321.2 million worldwide on a $22 million budget. It was the recipient of seven Academy Awards (out of twelve nominations), including Best Picture, Best Director, Best Adapted Screenplay, and Best Original Score, as well as numerous other awards (including seven BAFTAs and three Golden Globes). In 2007, the American Film Institute ranked the film 8th on its list of the 100 best American films of all time. The Library of Congress selected it for preservation in the National Film Registry in 2004.
Plot.
In Kraków during World War II, the Germans had forced local Polish Jews into the overcrowded Kraków Ghetto. Oskar Schindler, an ethnic German, arrives in the city hoping to make his fortune. A member of the Nazi Party, Schindler lavishes bribes on Wehrmacht (German armed forces) and SS officials and acquires a factory to produce enamelware. To help him run the business, Schindler enlists the aid of Itzhak Stern, a local Jewish official who has contacts with black marketeers and the Jewish business community. Stern helps Schindler arrange loans to finance the factory. Schindler maintains friendly relations with the Nazis and enjoys wealth and status as "Herr Direktor", and Stern handles administration. Schindler hires Jewish workers because they cost less, while Stern ensures that as many people as possible are deemed essential to the German war effort, which saves them from being transported to concentration camps or killed.
SS-"Untersturmführer" (second lieutenant) Amon Göth arrives in Kraków to oversee construction of Płaszów concentration camp. When the camp is completed, he orders the ghetto liquidated. Many people are shot and killed in the process of emptying the ghetto. Schindler witnesses the massacre and is profoundly affected. He particularly notices a tiny girl in a red coat – one of the few splashes of color in the black-and-white film – as she hides from the Nazis, and later sees her body (identifiable by the red coat) among those on a wagonload being taken away to be burned. Schindler is careful to maintain his friendship with Göth and, through bribery and lavish gifts, continues to enjoy SS support. Göth brutally mistreats his maid and randomly shoots people from the balcony of his villa, and the prisoners are in constant daily fear for their lives. As time passes, Schindler's focus shifts from making money to trying to save as many lives as possible. He bribes Göth into allowing him to build a sub-camp for his workers so that he can better protect them.
As the Germans begin to lose the war, Göth is ordered to ship the remaining Jews at Płaszów to Auschwitz concentration camp. Schindler asks Göth to allow him to move his workers to a new munitions factory he plans to build in his home town of Zwittau-Brinnlitz. Göth agrees, but charges a huge bribe. Schindler and Stern create "Schindler's List" – a list of people to be transferred to Brinnlitz and thus saved from transport to Auschwitz.
The train carrying women and children is accidentally redirected to Auschwitz-Birkenau; Schindler bribes Rudolf Höss, the commandant of Auschwitz with a bag of diamonds to win their release. At the new factory, Schindler forbids the SS guards to enter the production rooms and encourages the Jews to observe the Jewish Sabbath. To keep his workers alive, he spends much of his fortune bribing Nazi officials and buying shell casings from other companies; his factory does not produce any usable armaments during its seven months of operation. Schindler runs out of money in 1945, just as Germany surrenders, ending the war in Europe.
As a Nazi Party member and war profiteer, Schindler must flee the advancing Red Army to avoid capture. The SS guards have been ordered to kill the Jews, but Schindler persuades them not to so they can "return to their families as men, not murderers." He bids farewell to his workers and prepares to head west, hoping to surrender to the Americans. The workers give Schindler a signed statement attesting to his role saving Jewish lives, together with a ring engraved with a Talmudic quotation: "Whoever saves one life saves the world entire." Schindler is touched but is also deeply ashamed, as he feels he should have done even more. As the "Schindlerjuden" (Schindler Jews) wake up the next morning, a Soviet soldier announces that they have been liberated. The Jews leave the factory and walk to a nearby town.
Following scenes depicting Göth's execution after the war and a summary of Schindler's later life, the black-and-white frame changes to a color shot of actual "Schindlerjuden" at Schindler's grave in Jerusalem. Accompanied by the actors who portrayed them, the "Schindlerjuden" place stones on the grave. In the final scene, Neeson places a pair of roses on the grave.
Production.
Development.
Pfefferberg, one of the "Schindlerjuden", made it his life's mission to tell the story of his savior. Pfefferberg attempted to produce a biopic of Oskar Schindler with MGM in 1963, with Howard Koch writing, but the deal fell through. In 1982, Thomas Keneally published his historical novel "Schindler's Ark", which he wrote after a chance meeting with Pfefferberg in Los Angeles in 1980. MCA president Sid Sheinberg sent director Steven Spielberg a "New York Times" review of the book. Spielberg, astounded by Schindler's story, jokingly asked if it was true. "I was drawn to it because of the paradoxical nature of the character," he said. "What would drive a man like this to suddenly take everything he had earned and put it all in the service of saving these lives?" Spielberg expressed enough interest for Universal Pictures to buy the rights to the novel. At their first meeting in spring 1983, he told Pfefferberg he would start filming in ten years. In the end credits of the film, Pfefferberg is credited as a consultant under the name Leopold Page.
Spielberg was unsure if he was mature enough to make a film about the Holocaust, and the project remained "on guilty conscience". Spielberg tried to pass the project to director Roman Polanski, who turned it down. Polanski's mother was killed at Auschwitz, and he had lived in and survived the Kraków Ghetto. Polanski eventually directed his own Holocaust drama "The Pianist" (2002). Spielberg also offered the film to Sydney Pollack and Martin Scorsese, who was attached to direct "Schindler's List" in 1988. However, Spielberg was unsure of letting Scorsese direct the film, as "I'd given away a chance to do something for my children and family about the Holocaust." Spielberg offered him the chance to direct the 1991 remake of "Cape Fear" instead. Billy Wilder expressed an interest in directing the film as a memorial to his family, most of whom died in the Holocaust.
Spielberg finally decided to take on the project when he noticed that Holocaust deniers were being given serious consideration by the media. With the rise of neo-Nazism after the fall of the Berlin Wall, he worried that people were too accepting of intolerance, as they were in the 1930s. Sid Sheinberg greenlit the film on condition that Spielberg made "Jurassic Park" first. Spielberg later said, "He knew that once I had directed "Schindler" I wouldn't be able to do "Jurassic Park"." The picture was assigned a small budget of $22 million, as Holocaust films are not usually profitable. Spielberg forewent a salary for the film, calling it "blood money", and believed the film would flop.
In 1983, Keneally was hired to adapt his book, and he turned in a 220-page script. His adaptation focused on Schindler's numerous relationships, and Keneally admitted he did not compress the story enough. Spielberg hired Kurt Luedtke, who had adapted the screenplay of "Out of Africa", to write the next draft. Luedtke gave up almost four years later, as he found Schindler's change of heart too unbelievable. During his time as director, Scorsese hired Steven Zaillian to write a script. When he was handed back the project, Spielberg found Zaillian's 115-page draft too short, and asked him to extend it to 195 pages. Spielberg wanted more focus on the Jews in the story, and he wanted Schindler's transition to be gradual and ambiguous, not a sudden breakthrough or epiphany. He extended the ghetto liquidation sequence, as he "felt very strongly that the sequence had to be almost unwatchable."
Casting.
Neeson auditioned as Schindler early on, and was cast in December 1992, after Spielberg saw him perform in "Anna Christie" on Broadway. Warren Beatty participated in a script reading, but Spielberg was concerned that he could not disguise his accent and that he would bring "movie star baggage". Kevin Costner and Mel Gibson expressed interest in portraying Schindler, but Spielberg preferred to cast the relatively unknown Neeson, so the actor's star quality would not overpower the character. Neeson felt Schindler enjoyed outsmarting the Nazis, who regarded him as a bit of a buffoon. "They don't quite take him seriously, and he used that to full effect." To help him prepare for the role, Spielberg showed Neeson film clips of Time Warner CEO Steve Ross, who had a charisma that Spielberg compared to Schindler's. He also located a tape of Schindler speaking, which Neeson studied to learn the correct intonations and pitch.
Fiennes was cast as Amon Göth after Spielberg viewed his performances in "" and "Emily Brontë's Wuthering Heights". Spielberg said of Fiennes' audition that "I saw sexual evil. It is all about subtlety: there were moments of kindness that would move across his eyes and then instantly run cold." Fiennes put on to play the role. He watched historic newsreels and talked to Holocaust survivors who knew Göth. In portraying him, Fiennes said "I got close to his pain. Inside him is a fractured, miserable human being. I feel split about him, sorry for him. He's like some dirty, battered doll I was given and that I came to feel peculiarly attached to." Doctors Samuel J. Leistedt and Paul Linkowski of the Université libre de Bruxelles describe Göth's character in the film as a classic psychopath. Fiennes looked so much like Göth in costume that when Mila Pfefferberg (a survivor of the events) met him, she trembled with fear.
The character of Itzhak Stern (played by Ben Kingsley) is a composite of accountant Stern, factory manager Abraham Bankier, and Göth's personal secretary, Mietek Pemper. The character serves as Schindler's alter ego and conscience. Kingsley is best known for his Academy Award winning performance as Gandhi in the 1982 biographical film.
Overall, there are 126 speaking parts in the film. Thousands of extras were hired during filming. Spielberg cast Israeli and Polish actors specially chosen for their Eastern European appearance. Many of the German actors were reluctant to don the SS uniform, but some of them later thanked Spielberg for the cathartic experience of performing in the movie. Halfway through the shoot, Spielberg conceived the epilogue, where 128 survivors pay their respects at Schindler's grave in Jerusalem. The producers scrambled to find the "Schindlerjuden" and fly them in to film the scene.
Filming.
Principal photography began on March 1, 1993 in Kraków, Poland, with a planned schedule of 75 days. The crew shot at or near the actual locations, though the Płaszów camp had to be reconstructed in a nearby abandoned quarry, as modern high rise apartments were visible from the site of the original camp. Interior shots of the enamelware factory in Kraków were filmed at a similar facility in Olkusz, while exterior shots and the scenes on the factory stairs were filmed at the actual factory. The crew was forbidden to do extensive shooting or construct sets on the grounds at Auschwitz, so they shot at a replica constructed just outside the entrance. There were some antisemitic incidents. A woman who encountered Fiennes in his Nazi uniform told him that "the Germans were charming people. They didn't kill anybody who didn't deserve it". Antisemitic symbols were scrawled on billboards near shooting locations, while Kingsley nearly entered a brawl with an elderly German-speaking businessman who insulted Israeli actor Michael Schneider. Nonetheless, Spielberg stated that at Passover, "all the German actors showed up. They put on yarmulkes and opened up Haggadas, and the Israeli actors moved right next to them and began explaining it to them. And this family of actors sat around and race and culture were just left behind."
Shooting "Schindler's List" was deeply emotional for Spielberg, the subject matter forcing him to confront elements of his childhood, such as the antisemitism he faced. He was surprised that he did not cry while visiting Auschwitz; instead he found himself filled with outrage. He was one of many crew members who could not force themselves to watch during shooting of the scene where aging Jews are forced to run naked while being selected by Nazi doctors to go to Auschwitz. Spielberg commented that he felt more like a reporter than a film maker – he would set up scenes and then watch events unfold, almost as though he were witnessing them rather than creating a movie. Several actresses broke down when filming the shower scene, including one who was born in a concentration camp. Spielberg, his wife Kate Capshaw, and their five children rented a house in suburban Kraków for the duration of filming. He later thanked his wife "for rescuing me ninety-two days in a row ... when things just got too unbearable". Robin Williams called Spielberg to cheer him up, given the profound lack of humor on the set. Spielberg spent several hours each evening editing "Jurassic Park", which was scheduled to premiere in June 1993.
Spielberg occasionally used German and Polish language in scenes to recreate the feeling of being present in the past. He initially considered making the film entirely in those languages, but decided "there's too much safety in reading. It would have been an excuse to take their eyes off the screen and watch something else."
Cinematography.
Influenced by the 1985 documentary film "Shoah", Spielberg decided not to plan the film with storyboards, and to shoot it like a documentary. Forty percent of the film was shot with handheld cameras, and the modest budget meant the film was shot quickly over seventy-two days. Spielberg felt that this gave the film "a spontaneity, an edge, and it also serves the subject." He filmed without using Steadicams, elevated shots, or zoom lenses, "everything that for me might be considered a safety net." This matured Spielberg, who felt that in the past he had always been paying tribute to directors such as Cecil B. DeMille or David Lean.
The decision to shoot the film mainly in black and white contributed to the documentary style of cinematography, which cinematographer Janusz Kamiński compared to German Expressionism and Italian neorealism. Kamiński said that he wanted to give the impression of timelessness to the film, so the audience would "not have a sense of when it was made." Spielberg decided to use black and white to match the feel of actual documentary footage of the era. Universal chairman Tom Pollock asked him to shoot the film on a color negative, to allow color VHS copies of the film to later be sold, but Spielberg did not want to accidentally "beautify events."
Music.
John Williams, who frequently collaborates with Spielberg, composed the score for "Schindler's List". The composer was amazed by the film, and felt it would be too challenging. He said to Spielberg, "You need a better composer than I am for this film." Spielberg responded, "I know. But they're all dead!" Itzhak Perlman performs the theme on the violin.
Regarding "Schindler's List", Perlman said:
In the scene where the ghetto is being liquidated by the Nazis, the folk song "Oyfn Pripetshik" ("On the Cooking Stove") () is sung by a children's choir. The song was often sung by Spielberg's grandmother, Becky, to her grandchildren. The clarinet solos heard in the film were recorded by Klezmer virtuoso Giora Feidman. Williams won an Academy Award for Best Original Score for "Schindler's List", his fifth win. Selections from the score were released on a soundtrack album.
Themes and symbolism.
The film explores the theme of good versus evil, using as its main protagonist a "good German", a popular characterization in American cinema. While Göth is characterized as an almost completely dark and evil person, Schindler gradually evolves from Nazi supporter to rescuer and hero. Thus a second theme of redemption is introduced as Schindler, a disreputable schemer on the edges of respectability, becomes a father figure responsible for saving the lives of more than a thousand people.
The girl in red.
While the film is shot primarily in black and white, a red coat is used to distinguish a little girl in the scene depicting the liquidation of the Kraków ghetto. Later in the film, Schindler sees her dead body, recognizable only by the red coat she is still wearing. Spielberg said the scene was intended to symbolise how members of the highest levels of government in the United States knew the Holocaust was occurring, yet did nothing to stop it. "It was as obvious as a little girl wearing a red coat, walking down the street, and yet nothing was done to bomb the German rail lines. Nothing was being done to slow down ... the annihilation of European Jewry," he said. "So that was my message in letting that scene be in color." Andy Patrizio of IGN notes that the point at which Schindler sees the girl's dead body is the point at which he changes, no longer seeing "the ash and soot of burning corpses piling up on his car as just an annoyance." Professor André H. Caron of the Université de Montréal wonders if the red symbolises "innocence, hope or the red blood of the Jewish people being sacrificed in the horror of the Holocaust."
The girl was portrayed by Oliwia Dąbrowska, three years old at the time of filming. Spielberg asked Dąbrowska not to watch the film until she was eighteen, but she watched it when she was eleven, and was "horrified." Upon seeing the film again as an adult, she was proud of the role she played. Although it was unintentional, the character is similar to Roma Ligocka, who was known in the Kraków Ghetto for her red coat. Ligocka, unlike her fictional counterpart, survived the Holocaust. After the film was released, she wrote and published her own story, "The Girl in the Red Coat: A Memoir" (2002, in translation). According to a 2014 interview of family members, the girl in red was inspired by Kraków resident Genya Gitel Chil.
Candles.
The opening scene features a family observing the Shabbat. Spielberg said that "to start the film with the candles being lit ... would be a rich bookend, to start the film with a normal Shabbat service before the juggernaut against the Jews begins." When the color fades out in the film's opening moments, it gives way to a world in which smoke comes to symbolize bodies being burnt at Auschwitz. Only at the end, when Schindler allows his workers to hold Shabbat services, do the images of candle fire regain their warmth. For Spielberg, they represent "just a glint of color, and a glimmer of hope." Sara Horowitz, director of the Koschitzky Centre for Jewish Studies at York University, sees the candles as a symbol for the Jews of Europe, killed and then burned in the crematoria. The two scenes bracket the Nazi era, marking its beginning and end. She points out that normally the woman of the house lights the Sabbath candles and intones the "Kiddush". In the film it is men who perform these rituals, demonstrating not only the subservient role of women, but also the subservient position of Jewish men in relation to Aryan men, especially Göth and Schindler.
Other symbolism.
To Spielberg, the black and white presentation of the film came to represent the Holocaust itself: "The Holocaust was life without light. For me the symbol of life is color. That's why a film about the Holocaust has to be in black-and-white." Robert Gellately notes the film in its entirety can be seen as a metaphor for the Holocaust, with early sporadic violence increasing into a crescendo of death and destruction. He also notes a parallel between the situation of the Jews in the film and the debate in Nazi Germany between making use of the Jews for slave labor or exterminating them outright. Water is seen as giving deliverance by Alan Mintz, Holocaust Studies professor at the Jewish Theological Seminary of America in New York. He notes its presence in the scene where Schindler arranges for a Holocaust train loaded with victims awaiting transport to be hosed down, and the scene in Auschwitz, where the women are given an actual shower instead of receiving the expected gassing.
Release.
The film opened on December 15, 1993. By the time it closed in theaters on September 29, 1994, it had grossed $96.1 million ($ in dollars) in the United States and over $321.2 million worldwide. In Germany, where it was shown in 500 theaters, the film was viewed by over 100,000 people in its first week alone and was eventually seen by six million people. The film was popular in Germany and a success worldwide.
"Schindler's List" made its U.S. network television premiere on NBC on February 23, 1997. Shown without commercials, it ranked #3 for the week with a 20.9/31 rating/share, highest Nielsen rating for any film since NBC's broadcast of "Jurassic Park" in May 1995. The film aired on public television in Israel on Holocaust Memorial Day in 1998.
The DVD was released on March 9, 2004 in widescreen and fullscreen editions, on a double-sided disc with the feature film beginning on side A and continuing on side B. Special features include a documentary introduced by Spielberg. Also released for both formats was a limited edition gift set, which included the widescreen version of the film, Keneally's novel, the film's soundtrack on CD, a senitype, and a photo booklet titled "Schindler's List: Images of the Steven Spielberg Film", all housed in a plexiglass case. The laserdisc gift set was a limited edition that included the soundtrack, the original novel, and an exclusive photo booklet. As part of its 20th anniversary, the movie was released on Blu-ray Disc on March 5, 2013.
Following the success of the film, Spielberg founded the Survivors of the Shoah Visual History Foundation, a nonprofit organization with the goal of providing an archive for the filmed testimony of as many survivors of the Holocaust as possible, to save their stories. He continues to finance that work. Spielberg used proceeds from the film to finance several related documentaries, including "Anne Frank Remembered" (1995), "The Lost Children of Berlin" (1996), and "The Last Days" (1998).
Reception.
Critical response.
"Schindler's List" received acclaim from both film critics and audiences. On Rotten Tomatoes, the film received an approval rating of 96%, based on 81 reviews. The site's consensus reads ""Schindler's List" blends the abject horror of the Holocaust with Steven Spielberg's signature tender humanism to create the director's dramatic masterpiece." Americans such as talk show host Oprah Winfrey and President Bill Clinton urged their countrymen to see it. World leaders in many countries saw the film, and some met personally with Spielberg.
Stephen Schiff of "The New Yorker" called it the best historical drama about the Holocaust, a movie that "will take its place in cultural history and remain there." Roger Ebert of the "Chicago Sun-Times" described it as Spielberg's best, "brilliantly acted, written, directed, and seen." Ebert named it one of his ten favorite films of 1993. Terrence Rafferty, also with "The New Yorker", admired the film's "narrative boldness, visual audacity, and emotional directness." He noted the performances of Neeson, Fiennes, Kingsley, and Davidtz as warranting special praise, and calls the scene in the shower at Auschwitz "the most terrifying sequence ever filmed." In his 2013 movie guide, Leonard Maltin awarded the film a four-star rating, calling it a "staggering adaptation of Thomas Keneally's best-seller," saying "this looks and feels like nothing Hollywood has ever made before". He also described it as "Spielberg's most intense and personal film to date". James Verniere of the "Boston Herald" noted the film's restraint and lack of sensationalism, and called it a "major addition to the body of work about the Holocaust." In his review for the "New York Review of Books", British critic John Gross said his misgivings that the story would be overly sentimentalized "were altogether misplaced. Spielberg shows a firm moral and emotional grasp of his material. The film is an outstanding achievement." Mintz notes that even the film's harshest critics admire the "visual brilliance" of the fifteen-minute segment depicting the liquidation of the Kraków ghetto. He describes the sequence as "realistic" and "stunning". He points out that the film has done much to increase Holocaust remembrance and awareness as the remaining survivors pass away, severing the last living links with the catastrophe. The film's release in Germany led to widespread discussion about why most Germans did not do more to help.
Criticism of the film also appeared, mostly from academia rather than the popular press. Horowitz points out that much of the Jewish activity seen in the ghetto consists of financial transactions such as lending money, trading on the black market, or hiding wealth, thus perpetuating a stereotypical view of Jewish life. Horowitz notes that while the depiction of women in the film accurately reflects Nazi ideology, the low status of women and the link between violence and sexuality is not explored further. History professor Omer Bartov of Brown University notes that the physically large and strongly drawn characters of Schindler and Göth overshadow the Jewish victims, who are depicted as small, scurrying, and frightened – a mere backdrop to the struggle of good versus evil.
Horowitz points out that the film's dichotomy of absolute good versus absolute evil glosses over the fact that the vast majority of Holocaust perpetrators were ordinary people; the movie does not explore how the average German rationalized their knowledge of or participation in the Holocaust. Author Jason Epstein commented that the movie gives the impression that if people were smart enough or lucky enough, they could survive the Holocaust; this was not actually the case. Spielberg responded to criticism that Schindler's breakdown as he says farewell is too maudlin and even out of character by pointing out that the scene is needed to drive home the sense of loss and to allow the viewer an opportunity to mourn alongside the characters on the screen.
Assessment by other filmmakers.
"Schindler's List" was very well received by many of Spielberg's peers. Filmmaker Billy Wilder wrote a long letter of appreciation to Spielberg in which he proclaimed, "They couldn't have gotten a better man. This movie is absolutely perfection." Polanski, who turned down the chance to direct the film, later commented, "I certainly wouldn't have done as good a job as Spielberg because I couldn't have been as objective as he was." He cited "Schindler's List" as an influence on his 1995 film "Death and the Maiden". The success of "Schindler's List" led filmmaker Stanley Kubrick to abandon his own Holocaust project, "Aryan Papers", which would have been about a Jewish boy and his aunt who survive the war by sneaking through Poland while pretending to be Catholic. When scriptwriter Frederic Raphael suggested that "Schindler's List" was a good representation of the Holocaust, Kubrick commented, "Think that's about the Holocaust? That was about success, wasn't it? The Holocaust is about 6 million people who get killed. "Schindler's List" is about 600 who don't."
Filmmaker Jean-Luc Godard accused Spielberg of using the film to make a profit out of a tragedy while Schindler's wife, Emilie Schindler, lived in poverty in Argentina. Keneally disputed claims that she was never paid for her contributions, "not least because I had recently sent Emilie a check myself." He also confirmed with Spielberg's office that payment had been sent from there. Filmmaker Michael Haneke criticized the sequence in which Schindler's women are accidentally sent off to Auschwitz and herded into showers: "There's a scene in that film when we don't know if there's gas or water coming out in the showers in the camp. You can only do something like that with a naive audience like in the United States. It's not an appropriate use of the form. Spielberg meant well – but it was dumb."
The film was criticized by filmmaker and lecturer Claude Lanzmann, director of the nine-hour Holocaust film "Shoah", who called "Schindler's List" a "kitschy melodrama" and a "deformation" of historical truth. "Fiction is a transgression, I am deeply convinced that there is a ban on depiction the Holocaust", he said. Lanzmann also criticized Spielberg for viewing the Holocaust through the eyes of a German, saying "it is the world in reverse." He complained, "I sincerely thought that there was a time before "Shoah", and a time after "Shoah", and that after "Shoah" certain things could no longer be done. Spielberg did them anyway." Spielberg accused him of wanting to be "the only voice in the definitive account of the Holocaust. It amazed me that there could be any hurt feelings in an effort to reflect the truth."
Reaction of the Jewish community.
At a 1994 "Village Voice" symposium about the film, historian Annette Insdorf described how her mother, a survivor of three concentration camps, felt gratitude that the Holocaust story was finally being told in a major film that would be widely viewed. Hungarian Jewish author Imre Kertész, a Holocaust survivor, feels it is impossible for life in a Nazi concentration camp to be accurately portrayed by anyone who did not experience it first-hand. While commending Spielberg for bringing the story to a wide audience, he found the film's final scene at the graveyard neglected the terrible after-effects of the experience on the survivors and implied that they came through emotionally unscathed. Rabbi Uri D. Herscher found the film an "appealing" and "uplifting" demonstration of humanitarianism. Norbert Friedman noted that, like many Holocaust survivors, he reacted with a feeling of solidarity towards Spielberg of a sort normally reserved for other survivors. Albert L. Lewis, Spielberg's childhood rabbi and teacher, described the movie as "Steven's gift to his mother, to his people, and in a sense to himself. Now he is a full human being."
Accolades.
"Schindler's List" featured on a number of "best of" lists, including the "Time" magazine's Top Hundred as selected by critics Richard Corliss and Richard Schickel, "Time Out" magazine's 100 Greatest Films Centenary Poll conducted in 1995, and Leonard Maltin's "100 Must See Movies of the Century". The Vatican named "Schindler's List" among the most important 45 films ever made. A Channel 4 poll named "Schindler's List" the ninth greatest film of all time, and it ranked fourth in their 2005 war films poll. The film was named the best of 1993 by critics such as James Berardinelli, Roger Ebert, and Gene Siskel. Deeming the film "culturally significant", the Library of Congress selected it for preservation in the National Film Registry in 2004.
Spielberg won the Directors Guild of America Award for Outstanding Directing – Feature Film for his work, and shared the Producers Guild of America Award for Best Theatrical Motion Picture with co-producers Branko Lustig and Gerald R. Molen. Steven Zaillian won a Writers Guild of America Award for the screenplay. The film won National Society of Film Critics awards for Best Film, Best Director, Best Supporting Actor, and Best Cinematography. New York Film Critics Circle awards were won for Best Film, Best Supporting Actor, and Best Cinematography. Los Angeles Film Critics Association awards were won for Best Film, Best Cinematography (tied with The Piano), and Best Production Design. The film also won many other awards and nominations worldwide.
Controversies.
For the 1997 American television showing, the film was broadcast virtually unedited. The telecast was the first to receive a TV-M (now TV-MA) rating under the TV Parental Guidelines that had been established earlier that year. Senator Tom Coburn, then an Oklahoma congressman, said that in airing the film, NBC had brought television "to an all-time low, with full-frontal nudity, violence and profanity", adding that it was an insult to "decent-minded individuals everywhere". Under fire from both Republicans and Democrats, Coburn apologized, saying: "My intentions were good, but I've obviously made an error in judgment in how I've gone about saying what I wanted to say." He clarified his opinion, stating that the film ought to have been aired later at night when there would not be "large numbers of children watching without parental supervision".
Controversy arose in Germany for the film's television premiere on ProSieben. Heavy protests ensued when the station intended to televise it with two commercial breaks. As a compromise, the broadcast included one break, consisting of a short news update and several commercials.
In the Philippines, chief censor Henrietta Mendez ordered cuts of three scenes depicting sexual intercourse and female nudity before the movie could be shown in theaters. Spielberg refused, and pulled the film from screening in Philippine cinemas, which prompted the Senate to demand the abolition of the censorship board. President Fidel V. Ramos himself intervened, ruling that the movie could be shown uncut to anyone over the age of 15.
According to Slovak filmmaker Juraj Herz, the scene in which a group of women confuse an actual shower with a gas chamber is taken directly, shot by shot, from his film "Zastihla mě noc" ("Night Caught Up with Me", 1986). Herz wanted to sue, but was unable to fund the case.
The song "Yerushalayim Shel Zahav" ("Jerusalem of Gold") is featured in the film's soundtrack and plays near the end of the film. This caused some controversy in Israel, as the song (which was written in 1967 by Naomi Shemer) is widely considered an informal anthem of the Israeli victory in the Six-Day War. In Israeli prints of the film the song was replaced with "Halikha LeKesariya" ("A Walk to Caesarea") by Hannah Szenes, a World War II resistance fighter.
Impact on Krakow.
Due to the increased interest in Kraków created by the film, the city bought Oskar Schindler's Enamel Factory in 2007 to create a permanent exhibition about the German occupation of the city from 1939 to 1945. The museum opened in June 2010.

</doc>
<doc id="65835" url="https://en.wikipedia.org/wiki?curid=65835" title="Please Please Me">
Please Please Me

Please Please Me is the debut studio album by the English rock band the Beatles. Parlophone rush-released the album on 22 March 1963 in the United Kingdom to capitalise on the success of the singles "Please Please Me" (No. 1 on most lists though only No. 2 on "Record Retailer") and "Love Me Do" (No. 17).
Of the album's 14 songs, eight were written by Lennon–McCartney (originally credited "McCartney–Lennon"), early evidence of what "Rolling Stone" later called "invention of the idea of the self-contained rock band, writing their own hits and playing their own instruments." In 2012, "Please Please Me" was voted 39th on "Rolling Stone" magazine's list of the "500 Greatest Albums of All Time."
Recording.
The norm for British 12" vinyl pop albums in 1963 was to have seven songs on each side (American albums usually had five or six songs per side) leaving producer George Martin needing only ten more tracks if he were to include the four sides ("Love Me Do" / "P.S. I Love You" and "Please Please Me" / "Ask Me Why") of the group's first two singles: “I asked them what they had which we could record quickly, and the answer was their stage act” Martin said. He had at first contemplated recording the album live at the Cavern Club in front of the group's home audience and visited the Liverpool club on 9 December 1962 to consider the technicalities (or, as more recent scholarship indicates, on 12 December 1962.) But when time constraints intervened, he decided to book them at EMI Studios in Abbey Road and record them live there instead. Martin said, "It was a straightforward performance of their stage repertoire—a broadcast, more or less."
Initially only a morning and afternoon session were booked; the evening session was added later. Therefore, at 10:00 am on Monday, 11 February 1963, the Beatles began working their way through their live set song by song, the number of takes varying on each, and finished at 10:45 pm—less than 13 hours later— capturing in essence an authentic representation of the band's Cavern Club-era sound. The day ended with a cover of "Twist and Shout", which had to be recorded last because John Lennon had a particularly bad cold and Martin feared the throat-shredding vocal would ruin Lennon's voice for the day. This performance, caught on the first take, prompted Martin to say: "I don't know how they do it. We've been recording all day but the longer we go on the better they get." Mark Lewisohn would later write: "There can scarcely have been 585 more productive minutes in the history of recorded music" Paul McCartney double tracked his vocal on "A Taste of Honey" and Lennon added harmonica onto "There's A Place" during these sessions. Martin overdubbed piano on "Misery" and celesta on "Baby It's You" on 20 February during which the Beatles were not present.
The song "Hold Me Tight" was recorded during these sessions, but was "surplus to requirements" and not included on the album. "Hold Me Tight" was recorded again on 12 September 1963 for "With the Beatles".
The whole day's session cost around £400 (equivalent to £ as of ). Martin said: "There wasn't a lot of money at Parlophone. I was working to an annual budget of £55,000." This budget had to cover all of the artists on Martin's roster. Individually, under a contract with the Musicians' Union, each Beatle collected a £7 10s (£7.50) session fee for each three-hour session.
Before deciding on the title "Please Please Me", Martin considered calling the album "Off the Beatle Track", a title he would later use for his own orchestral album of Beatles songs. The album was recorded on a two-track BTR tape machine with most of the instruments on one track and the vocals on the other, allowing Martin to better balance the two in the final mono mix. A stereo mix was also made with one track on the left channel and the other on the right, as well as an added layer of reverb to better blend the two tracks together. The two tracks generally divided the instrumental track from the vocals, with the exception of "Boys", in which the close proximity of Ringo's drums to his vocal microphone placed the drums (but not the other instruments) on the vocal channel. Two tracks, "Love Me Do" and "P.S. I Love You", were not mixed for stereo and appear in mono on both versions of the album.
Release.
"Please Please Me" was released as a mono LP album on the Parlophone label in the UK on 22 March 1963, and has remained on UK catalogue continuously since 1963. The stereo version was released on 26 April, over a month after the mono version.
Release formats:
In the United States, most of the songs on "Please Please Me" were first issued on Vee-Jay Records' "Introducing... The Beatles" in 1964, and subsequently on Capitol Records' "The Early Beatles" in 1965. "Please Please Me" was not released in the US until the Beatles' catalogue was standardised for CD.
In Canada, the majority of the album's songs were included upon the Canadian-exclusive release "Twist and Shout", which featured "From Me to You" and "She Loves You" in place of "I Saw Her Standing There" and "Misery".
In New Zealand, the album first appeared only in mono on the black Parlophone label. The following year (1964) EMI (NZ) changed from black to a blue Parlophone label and the album was again available only in mono. Due to constant demand, it was finally made available in stereo, first through the World Record Club on their "Young World" label in both mono and stereo, and finally on the blue Parlophone label.
The album was released on CD on 26 February 1987 in mono, as were their three subsequent albums, "With the Beatles", "A Hard Day's Night" and "Beatles for Sale". It was not released on vinyl or tape in the US until five months later when it was issued for the first time in the US on LP and cassette on 21 July 1987.
"Please Please Me" was remastered and re-released on CD in stereo, along with all the other original UK studio albums, on 9 September 2009. The 2009 remasters replaced the 1987 remasters. A remastered mono CD was also available as part of the "The Beatles in Mono" box set.
Sleeve notes.
As consistent with all early 1960s albums made in the UK, the rear of the album sleeve has sleeve notes. The Beatles' press officer Tony Barrow wrote extensive sleeve notes, which included a brief mention of their early 1960s rivals The Shadows.
Album cover.
George Martin was an honorary fellow of the Zoological Society of London, which owns the London Zoo. Martin thought that it might be good publicity for the zoo to have the Beatles pose outside the insect house for the cover photography of the album. However, the society turned down Martin's offer, and instead, Angus McBean was asked to take the distinctive colour photograph of the group looking down over the stairwell inside EMI's London headquarters in Manchester Square. Martin was to write later: "We rang up the legendary theatre photographer Angus McBean, and bingo, he came round and did it there and then. It was done in an almighty rush, like the music. Thereafter, though, the Beatles' own creativity came bursting to the fore." In 1969, the Beatles asked McBean to recreate this shot. Although the 1969 photograph was originally intended for the then-planned "Get Back" album, it was not used when that project saw eventual release in 1970 as "Let It Be". Instead, the 1969 photograph, along with an unused photograph from the 1963 photo shoot, was used in 1973 for the Beatles' retrospective albums "1962–1966" and "1967–1970". Another unused photograph from the 1963 photo shoot was used for "The Beatles (No. 1)" (also released in 1963).
Reception.
"Please Please Me" hit the top of the UK album charts in May 1963 and remained there for 30 weeks before being replaced by "With the Beatles". This was surprising because the UK album charts at the time tended to be dominated by film soundtracks and easy listening vocalists.
In a 1987 review upon its CD reissue, "Rolling Stone" magazine's Steve Pond recommended "Please Please Me" "for the Beatles' unfettered joy at making music". In 2012, "Please Please Me" was voted 39th on "Rolling Stone" magazine's list of the "500 Greatest Albums of All Time". It was ranked first among the Beatles' early albums, and sixth of all of the Beatles' albums, with "Sgt. Pepper's Lonely Hearts Club Band", "Revolver", "Rubber Soul", "The Beatles (The White Album)" and "Abbey Road" ranked higher.
"Rolling Stone" also placed two songs from the album on its list of The 500 Greatest Songs of All Time: No. 140, "I Saw Her Standing There", and No. 186, "Please Please Me". According to Stephen Thomas Erlewine of AllMusic, "Decades after its release, the album still sounds fresh," the covers are "impressive" and the originals "astonishing."
Track listing.
Track listing per Calkin.
Personnel.
According to Mark Lewisohn:
50th anniversary.
In 2013 the album's 50th Anniversary was celebrated by modern artists re-recording the album in just one day, the same time it took The Beatles to record it 50 years earlier. The Stereophonics recorded a cover of the album's opening track, "I Saw Her Standing There." It and the other recordings were broadcast on BBC Radio 2, and a documentary about the re-recording of The Beatles' debut album was broadcast on BBC Television.

</doc>
<doc id="65836" url="https://en.wikipedia.org/wiki?curid=65836" title="Republicanism in Australia">
Republicanism in Australia

Republicanism in Australia is a movement to change Australia's system of government from a constitutional monarchy to a republic. Republicanism was first espoused in Australia before Federation in 1901. After a period of decline after Federation, the movement again became prominent at the end of the 20th century after successive legal and socio-cultural changes loosened Australia's ties with the United Kingdom.
Politically, republicanism is officially supported by the Labor Party and the Greens, and is also supported by some Liberal Party members of the Australian parliament including leader and prime minister Malcolm Turnbull. In a referendum in 1999, Australian voters rejected a proposal to establish a republic with a parliamentary appointed head of state.
History.
Before federation.
In his journal "The Currency Lad", first published in Sydney in 1832, pastoralist Horatio Wills was the first person to openly espouse Australian republicanism. Born to a convict father, Wills was devoted to the emancipist cause and called for Australia to be an independent nation like the United States. His son Tom Wills was a founder of Australian rules football.
Some leaders and participants of the revolt at the Eureka Stockade in 1854 held republican views and the incident has been used to encourage republicanism in subsequent years, the Eureka Flag appearing in connection with some republican groups. The Australian Republican Association (ARA) was founded in response, advocating the abolition of governors and their titles; the revision of the penal code; payment of members of parliament; nationalisation of land; and an independent federal Australian republic outside of the British Empire. At the same time, a movement emerged in favour of a "White Australia" policy; however British authorities in Whitehall were opposed to segregational laws. To circumvent Westminster, those in favour of the discriminatory policies backed the proposed secession from the Empire as a republic. One attendee of the ARA meetings was the Australian-born poet Henry Lawson, who wrote his first poem, entitled "A Song of the Republic", in "The Republican" journal.
When the Republican League disrupted the Sydney centenary in 1888 on Anniversary Day, one visiting British statesman said "Thank God there is an English fleet in harbour".
Federation and decline.
At the Australian Federation Convention which produced in Sydney in 1891 the first draft that was to become the Australian constitution, a former Premier of New South Wales, George Dibbs, described as the "inevitable destiny of the people of this great country" the establishment of "the Republic of Australia".
However, the fervour of republicanism tailed off in the 1890s as the labour movement became concerned with the federation of Australia. The republican movement dwindled further during and after World War I. Emotionally, patriotic support for the war effort went hand in hand with a renewal of loyalty to the monarchy. "The Bulletin" abandoned republicanism and became a conservative, Empire loyalist paper. The Returned and Services League formed in 1916 and became an important bastion of monarchist sentiment.
The conservative parties were fervently monarchist and, although the Labor Party campaigned for greater Australian independence within the Empire and generally supported the appointment of Australians as governor-general, it did not question the monarchy itself. Under the Labor government of John Curtin, a member of the Royal Family, Prince Henry, Duke of Gloucester, was appointed governor-general during World War II. The royal tour of Queen Elizabeth II in 1954 saw a reported 7 million Australians (out of a total population of 9 million) out to see her.
Whitlam era.
Reflecting re-orientation of trade flows in the 1950s and 1960s, Britain had decided in the late 1960s to re-orientate its trade and economic policy from the Commonwealth of Nations to the European Economic Community, and the 1960s and early 1970s saw a corresponding further reduction in the economic relationship between Britain and the major realms, including Australia. In 1967, the pound sterling was devalued, but Australia did not follow suit, instead moving to a peg between the Australian dollar and the pound sterling at a different rate. In 1971, Australia switched its peg altogether, to the United States dollar, and in June 1972 Britain responded to this and other changes resulting from Britain's shrinking economic presence in the world by shrinking the sterling area, effectively ending the former monetary union.
The election of a Labor majority in 1972 marked the end of a period where Australians saw themselves principally as part of the Commonwealth of Nations (formerly the British Empire), with the Whitlam government implementing a number of reforms that strengthened Australia's independent nationhood.
The Whitlam government ended in 1975 with a constitutional crisis in which Governor-General John Kerr dismissed the ministry and appointed Opposition Leader Malcolm Fraser as prime minister, an act in which the monarch herself was not consulted and, when approached after the event, pointedly refused to intervene, noting that she lacked authority to do so under the Australian constitution. The incident, though, raised questions about the value of maintaining a supposedly "symbolic" office that still possessed many key political powers and what an Australian president with the same reserve powers would do in a similar situation.
Australia Act and other changes.
Moves elsewhere in the Commonwealth of Nations continued to dilute the ties between its members, including between Britain and Australia. In 1982, Britain changed its citizenship laws to remove Commonwealth citizens from the definition of "British subject", which until then was a status shared by all Commonwealth citizens. Like its late acceptance of the Statute of Westminster, Australia was once again one of the last Commonwealth countries to adopt the change in its own laws: the category "British subjects" was abolished in Australian law only in 1987.
In 1986, the Australia Act was enacted, thereafter eliminating the remaining, mainly theoretical, ties between the legislature and judiciary of the United Kingdom and the Australian states. It was later determined by the High Court in Sue v Hill that this legislation established Britain and Australia as independent nations sharing the same person as their relevant sovereign.
At broadly the same time, references to the monarchy were being removed from various institutions. For example, in 1993, the Oath of Citizenship, which included an assertion of allegiance to the Australian monarch, was replaced by a pledge to be loyal to "Australia and its people". Further, the state of Queensland deleted all references to the monarchy from its legislation, with new laws being enacted by its parliament and "binding on the State of Queensland," not the Crown. (Other states, and the Commonwealth, later made similar changes.) 
Barristers in New South Wales (from 1993), Queensland (from 1994), ACT (from 1995), Victoria (from 2000), Western Australia (from 2001), Tasmania (from 2005), Northern Territory (from 2007), Commonwealth (from March 2007) and South Australia (from 2008) were no longer appointed Queen's Counsel (QC), but as Senior Counsel (SC). Many monarchists condemned these changes as moves to a "republic by stealth". However beginning with Queensland in 2013 and then followed by Victoria and the Commonwealth in 2014, the title of Queen's Counsel (QC) has again been conferred. Currently South Australia and New South Wales are discussing the reintroduction. [http://www.australasianlawyer.com.au/news/nsw-bar-to-bring-back-qcs-200397.aspx [http://www.abc.net.au/news/2015-09-04/queens-counsel-senior-counsel-debate-john-rau/6748520]
Nevertheless, all Australian senators and members of the House of Representatives continued to swear "to be faithful and bear true allegiance to Her Majesty" before taking their seats in parliament; as a part of the constitution, any changes to this oath could only be approved by a referendum.
Keating government proposals.
The Australian Labor Party (ALP) first made republicanism its official policy in 1991, with then Prime Minister Bob Hawke describing a republic as "inevitable". Following the ALP decision, the Australian Republican Movement, the leading republican advocacy group, was born. Hawke's successor, Paul Keating, pursued the republican agenda much more actively than Hawke and established the Republic Advisory Committee to produce an options paper on issues relating to the possible transition to a republic to take effect on the centenary of federation: 1 January 2001. The committee produced its report in April 1993 and in it argued that "a republic is achievable without threatening Australia's cherished democratic institutions."
In response to the report, Keating promised a referendum on the establishment of a republic, replacing the governor-general with a president, and removing references to the Australian sovereign. The president was to be nominated by the prime minister and appointed by a two-thirds majority in a joint sitting of the Senate and House of Representatives. The referendum was to be held either in 1998 or 1999. However, Keating's party lost the 1996 federal election in a land slide and he was replaced by John Howard, a monarchist, as prime minister.
1998 Constitutional Convention.
With the change in government in 1996, Prime Minister John Howard proceeded with an alternative policy of holding a constitutional convention. This was held over two weeks in February 1998 at Old Parliament House. Half of the 152 delegates were elected and half were appointed by the federal and state governments. Convention delegates were asked whether or not Australia should become a republic and which model for a republic is preferred. At the opening of the convention, Howard stated that if the convention could not decide on a model to be put to a referendum, then plebiscites would be held on the model preferred by the Australian public.
At the convention, a republic gained majority support (89 votes to 52 with 11 abstentions), but the question of what model for a republic should be put to the people at a referendum produced deep divisions among republicans. Four republican models were debated: two involving direct election of the head of state; one involving appointment on the advice of the prime minister (the McGarvie Model); and one involving appointment by a two-thirds majority of parliament (the bi-partisan appointment model).
The latter was eventually successful at the convention, even though it only obtained a majority because of 22 abstentions in the final vote (57 against delegates voted against the model and 73 voted for, three votes short of an actual majority of delegates). A number of those who abstained were republicans who supported direct election (such as Ted Mack, Phil Cleary, Clem Jones, and Andrew Gunter), thereby allowing the bi-partisan model to succeed. They reasoned that the model would be defeated at a referendum and a second referendum called with direct election as the model.
The convention also made recommendations about a preamble to the constitution and a proposed preamble was also put to referendum.
According to critics, the two-week timeline and quasi-democratic composition of the convention is evidence of an attempt by John Howard to frustrate the republican cause, a claim John Howard adamantly rejects.
1999 Republican referendum.
The republic referendum was held on 6 November 1999, after a national advertising campaign and the distribution of 12.9 million 'Yes/No' case pamphlets. It comprised two questions: The first asked whether Australia should become a republic in which
the governor-general and monarch would be replaced by one office, the President of the Commonwealth of Australia, the occupant elected by a two-thirds vote of the Australian parliament for a fixed term. The second question, generally deemed to be far less important politically, asked whether Australia should alter the constitution to insert a preamble. Neither of the amendments passed, with 55% of all electors and all states voting 'no' to the proposed amendment; it was not carried in any state. The preamble referendum question was also defeated, with a Yes vote of only 39 per cent.
Many opinions were put forward for the defeat, some relating to perceived difficulties with the parliamentary appointment model, others relating to the lack of public engagement or that most Australians were simply happy to keep the status quo. Some republicans voted no because they did not agree with provisions such as the president being instantly dismissible by the prime minister.
Following the referendum.
On 26 June 2003, the Senate referred an inquiry into an Australian republic to the Senate Legal and Constitutional References Committee. During 2004, the committee reviewed 730 submissions and conducted hearings in all state capitals. The committee tabled its report, called "Road to a Republic", on 31 August 2004.
The report examined the contest between minimalist and direct-election models and gave attention to hybrid models such as the electoral college model, the constitutional council model, and models having both an elected president and a governor-general.
The bi-partisan recommendations of committee supported educational initiatives and holding a series of plebiscites to allow the public to choose which model they preferred, prior to a final draft and referendum, along the lines of plebiscites proposed by John Howard at the 1998 constitutional convention.
Issues related to republicanism were raised by the March 2006 tour of Australia by Queen Elizabeth II. Then John Howard, still serving as prime minister, was questioned by British journalists about the future of the monarchy in Australia and there was debate about playing Australia's royal anthem, "God Save the Queen", during the opening of the that year's Commonwealth Games, at which the monarch was present.
Current status.
In the lead-up to the 2010 federal election, then Prime Minister Julia Gillard stated "I believe that this nation should be a republic. I also believe that this nation has got a deep affection for Queen Elizabeth." It was her view that it would be appropriate for Australia to become a republic only once Queen Elizabeth II's reign ends. On the process for becoming a republic, Gillard said "What I would like to see as the prime minister is that we work our way through to an agreement on a republic." In 2010, then opposition leader Tony Abbott, who previously served as Executive Director of Australians for Constitutional Monarchy, supported the status quo. He opined: "While there may very well be further episodes of republicanism in this country, I am far from certain that, at least in our lifetimes, there's likely to be any significant change."
In early 2015, leading up to Australia Day, opposition leader Bill Shorten called for a new push for Australia to become a republic. Former ARM chair Malcolm Turnbull said upon his appointment as prime minister in September of the same year he would not pursue "his dream" of Australia becoming a republic until after the end of the Queen's reign, instead focusing his efforts toward the economy.
Arguments for change.
Independence and head of state.
A central argument made by Australian republicans is that, as Australia is an independent country, it is inappropriate for Australia to share the person of its monarch with the United Kingdom. Republicans argue that the Australian monarch is not Australian and, as such, a person who is a national of another country cannot adequately represent Australia, either to itself or to the rest of the world. Former Chief Justice Gerard Brennan stated that "so long as we retain the existing system our head of state is determined for us essentially by the parliament at Westminster". As Australian Republican Movement member Frank Cassidy put it in a speech on the issue: "In short, we want a resident for President."
Multiculturalism and sectarianism.
Republicans associate the monarchy with British identity and subsequently argue that Australia has changed demographically and culturally, from being "British to our bootstraps", as prime minister Robert Menzies once put it, to being less British, albeit maintaining an "English Core". For some Australians not of British ancestry, they argue, the idea of one person being both monarch of Australia and of the United Kingdom is an anomaly. It is also claimed that there are some Aborigines and some Australians of Irish origin who see the Australian Crown as a symbol of British imperialism.
However, monarchists argue that immigrants who left unstable republics and have arrived in Australia since 1945 welcomed the social and political stability that they found in Australia under a constitutional monarchy. Further, some Aborigines, such as former Senator Neville Bonner, said a republican president would not "care one jot more for my people".
It has also been claimed monarchism and republicanism in Australia delineate historical and persistent sectarian tensions with, broadly speaking, Catholics more likely to be republicans and Protestants more likely to be monarchists. This developed out of a historical cleavage in 19th- and 20th-century Australia, in which republicans were predominantly of Irish Catholic background and loyalists were predominantly of British Protestant background. Whilst mass immigration since the Second World War has diluted this conflict, the Catholic–Protestant divide has been cited as a dynamic in the republic debate, particularly in relation to the referendum campaign in 1999. Nonetheless, others have stated that Catholic–Protestant tensions—at least in the sense of an Irish–British conflict—are at least forty years dead.
It has also been claimed, however, that the Catholic–Protestant divide is intermingled with class issues. Republicanism in Australia has traditionally been supported most strongly by members of the urban working class with Irish Catholic backgrounds, whereas monarchism is a core value associated with urban and rural inhabitants of British Protestant heritage and the middle class, to the extent that there were calls in 1999 for 300,000 exceptionally enfranchised British subjects who were not Australian citizens to be barred from voting on the grounds that they would vote as a loyalist bloc in a tight referendum.
Social values and contemporary Australia.
From some perspectives, it has been argued that several characteristics of the monarchy are in conflict with modern Australian values. The hereditary nature of the monarchy is said to conflict with egalitarianism and dislike of inherited privilege. The laws of succession were, before amendment to them in 2015, held by some to be sexist and the links between the monarchy and the Church of England inconsistent with Australia's secular character. Under the Act of Settlement, the monarch is prohibited from being a Catholic. As it is constitutional, this Australian law overrides anti-discrimination laws, which prohibit arrangements under which becoming a Catholic invalidates any legal rights.
Proposals for change.
A typical proposal for an Australian republic provides for the Queen and governor-general to be replaced by a president or an executive federal council. There is much debate on the appointment or election process that would be used and what role such an office would have.
Methods for selecting a president.
An alternative minimalist approach to change provides for removing the sovereign and retaining the governor-general. The most notable model of this type is the McGarvie Model, while Copernican models replace the monarch with a directly-elected figurehead. These Copernican models allow for regular and periodic elections for the office of head of state while limiting the reserve powers to the appointed governor-general only. A popularly elected head of state would have the same powers as the monarch, but he or she could not dismiss the prime minister. If this were to happen, it would be a first, as all other former Commonwealth realms have created presidencies upon becoming republics.
Alternatively it has been proposed to abolish the roles of the governor-general and the monarchy and have their functions exercised by other constitutional officers such as the Speaker.
Australians for Constitutional Monarchy and the Australian Monarchist League argue that no model is better than the present system and argue that the risk and difficulty of changing the constitution is best demonstrated by inability of republicans to back a definitive design.
Process models.
From its foundation until the 1999 referendum, the Australian Republican Movement (ARM) supported the bi-partisan appointment model, which would result in a President elected by the Parliament of Australia, with the powers currently held by the Governor-General. It is argued that the requirement of a two-thirds majority in a vote of both houses of parliament would result in a bi-partisan appointment, preventing a party politician from becoming president.
The ARM now supports a non-binding plebiscite to decide the model, followed by a binding referendum to amend the Constitution, reflecting the model chosen. Opponents of holding non-binding plebiscites include monarchist David Flint, who described this process as "inviting a vote of no confidence in one of the most successful constitutions in the world," and minimalist republican Greg Craven, who states "a multi-option plebiscite inevitably will produce a direct election model, precisely for the reason that such a process favours models with shallow surface appeal and multiple flaws. Equally inevitably, such a model would be doomed at referendum."
Public opinion.
Polls and surveys generate different responses depending on the wording of the questions, mostly in regards the type of republic, and often appear contradictory. In May 2008, a Morgan poll found 45% believe Australia should become a republic with an elected president, while 42% support Australia remaining a monarchy and 13% are undecided.
The Australian Electoral Survey that is conducted following all elections by the Australian National University has found that support for a republic has remained reasonably static since 1987 at around 60%, if the type of republic is not part of the question. The Electoral Survey also shows that support or opposition is relatively weak: 31% strongly support a republic while only 10% strongly oppose.
An opinion poll held in November 2008 that separated the questions found support for a republic at 50% with 28% opposed. Asked how the president should be chosen if there were to be a republic, 80 percent said elected by the people, against 12 percent who favoured appointment by parliament. In October 2009 another poll by UMR found 59% support for a republic and 33% opposition. 73% supported direct election, versus 18% support for parliamentary appointment.
On 29 August 2010 the "The Sydney Morning Herald" published a poll produced by Neilson, asking multiple questions on the future of the monarchy:
But when asked which of the following statements best described their view:
A survey of 1,000 readers of "The Sun-Herald" and "The Sydney Morning Herald", published in "The Sydney Morning Herald" on 21 November 2010, found 68% of respondents were in favour of Australia becoming a republic, while 25% said it should not. More than half the respondents, 56%, said Australia should become a republic as soon as possible while 31% said it should happen after the Queen dies.
However, an opinion poll conducted in 2011 saw a sharp decline in the support for an Australian republic. The polling conducted by the Morgan Poll in May 2011 showed the support for the monarchy was now 55% (up 17% since 1999), whereas the support for a republic was at 34% (down 20%). The turnaround in support for a republic has been called the "strange death of Australian republicanism".
A poll taken in the wake of the Queen's Diamond Jubilee found that support for the monarchy is at a twenty-five-year high. 58% of respondents supported the monarchy whereas 35% supported a republic.
The Australian Broadcasting Corporation's "Vote Compass" during the Australian federal election, 2013 found that 40.4% of respondents disagreed with the statement ""Australia should end the monarchy and become a republic"" 40.4% disagreed (26.3% strongly disagreed), whilst 38.1% agreed (23.1% strongly agreed) and 21.5% were neutral. Support for a republic was highest among those with a left-leaning political ideology. Younger people had the highest rate for those neutral towards the statement (27.8%) with their support for strongly agreed the lowest of all age groups at 17.1%. Support for a republic was highest in the Australian Capital Territory and Victoria and lowest in Queensland and Western Australia. More men than women said they support a republic.
In early 2014, a ReachTEL poll of 2,146 Australian conducted just after Australia Day showed only 39.4% supported a republic with 41.6% opposed. Lowest support was in the 65+ year cohort followed by the 18-34 year cohort. Geoff Gallop, the then chairman of the Australian Republican Movement, said higher support for a republic among Generation X and baby boomer voters could be explained by them having participated in the 1999 referendum and remembering the 1975 constitutional crisis. 
In April 2014, a poll found that "support for an Australian republic has slumped to its lowest level in more than three decades"; namely, on the eve of the visit to Australia by the Duke and Duchess of Cambridge, and Prince George of Cambridge, 42% of those polled agreed with the statement that "Australia should become a republic", whereas 51% opposed.
ARM commissioned a poll to be conducted by Essential Research from November 5 to 8 in 2015, asking "When Prince Charles becomes King of Australia, will you support or oppose replacing the British monarch with an Australian citizen as Australia's head of state?" Of the 1008 participants, 51% said they would prefer an Australian head of state to "King Charles", 27% opposed and 22% were undecided. However it should be noted that the poll focused on the popularity of a future potential king rather than a republic and no similar poll referencing Prince William was commissioned for comparison. 
The Australian has polled the same question "Are you personally in favour or against Australia becoming a republic?" multiple times since 1999 though it hadn't polled it since 2011. After Australia Day 2016 they found 51% support. This level of support was similar to levels found between 1999 and 2003 by the same newspaper. Total against was 37% which was an increase over the rates polled in all previous polls other than 2011. Uncommitted at 12% was the lowest ever polled. However support was again lowest in the 18-34 year cohort. [http://resources.news.com.au/files/2016/02/05/1227737/627996-160206republic.pdf]
Several days before Australia Day in 2016, ARM posted a petition on the popular change.org website calling for an Australian Head of State. The poll quickly garnered over 13,000 signatures before petering out within a fortnight. To give context, other petitions on change.org such as stopping a backpacker tax started after Australia Day had reached double this figure and was still attracting signatures two months later.
Party political positions.
Liberal-National Coalition.
The Liberal Party is a conservative and classical liberal party. The former generally favours the status quo, the latter favours republicanism. Proponents of republicanism in the Liberal Party include: its current leader and former leader of the Australian Republican Movement Malcolm Turnbull, as well as Joe Hockey and Peter Costello. Supporters of the status quo include former leader and former ACM Leader, Tony Abbott, former opposition leaders Alexander Downer and Brendan Nelson, Cory Bernardi and Sophie Mirabella.
The National Party has few republicans, its former leader, Tim Fischer being the leading example. A conservative party with a rural base, its core constituency has always been strongly monarchist. As such, it remains against change as official policy.
Under former Prime Minister John Howard, a monarchist, the government initiated a process to settle the republican debate, involving a constitutional convention and a referendum. Howard, who supports the status quo, says the matter was resolved by the failure of the referendum.
Australian Labor Party.
Labor has supported constitutional change to become a republic since 1991 and has incorporated republicanism into its platform. Labor is currently the only party that proposes a series of plebiscites to restart the republican process. Along with this, Labor spokesperson (and former federal attorney general) Nicola Roxon has previously said that reform will "always fail if we seek to inflict a certain option on the public without their involvement. This time round, the people must shape the debate".
The Greens.
The Australian Greens party is a strong proponent for an Australian republic, and this is reflected in the Greens 'Constitutional Reform & Democracy' policy. In the Senate, the Greens proposed legislation to hold a plebiscite on the republic at the 2010 federal election.
The Democrats.
The Australian Democrats, formerly Australia's third party, strongly supported a move towards a republic through a system of an elected Head of State through popular voting, but the party was deregistered in 2015 due to a decline in membership.

</doc>
<doc id="65837" url="https://en.wikipedia.org/wiki?curid=65837" title="The Art of War">
The Art of War

The Art of War is an ancient Chinese military treatise dating from the 5th century BC. Attributed to the ancient Chinese military strategist Sun Tzu"Master Sun", the text is composed of 13 chapters, each of which is devoted to one aspect of warfare. It is commonly thought of as a definitive work on military strategy and tactics. It was placed at the head of China's Seven Military Classics upon the collection's creation in 1080 by Emperor Shenzong of Song, and has long been the most influential strategy text in East Asia. It has had an influence on Eastern and Western military thinking, business tactics, legal strategy and beyond.
The book was first translated into French in 1772 by the Jesuit Jean Joseph Marie Amiot and a partial translation into English was attempted by British officer Everard Ferguson Calthrop in 1905. The first annotated English translation was completed and published by Lionel Giles in 1910. Leaders as diverse as Mao Zedong, General Vo Nguyen Giap, General Douglas MacArthur and leaders of Imperial Japan have drawn inspiration from the work.
History.
Text and commentaries.
Sima Qian's 1st century BC work "Records of the Grand Historian" ("Shiji"), the first of China's 24 dynastic histories, records an early Chinese tradition stating that a text on military matters was written by one Sun Wuusually known as "Master Sun" ("Sunzi" or "Sun-tzu" from the State of Qi, and that this text had been read and studied by King Helu of Wu (r. 514495). This text was traditionally identified with the received "Master Sun's Art of War". The conventional view, which is still widely held in China, was that Sun Wu was a military theorist from the end of the Spring and Autumn period (776471) who fled his home state of Qi to the southeastern kingdom of Wu, where he is said to have impressed the king with his ability to train even dainty palace ladies in warfare and to have made Wu's armies powerful enough to challenge their western rivals in the state of Chu.
The prominent strategist, poet, and warlord Cao Cao in the early 3rd century AD authored the earliest known commentary to the "Art of War". Cao's preface makes clear that he edited the text and removed certain passages, but the extent of his changes were unclear historically. "The Art of War" appears throughout the bibliographical catalogs of the Chinese dynastic histories, but listings of its divisions and size varied widely. In the early 20th century, the Chinese writer and reformer Liang Qichao theorized that the text was actually written in the 4th century BC by Sunzi's purported descendant Sun Bin, as a number of historical sources mention a military treatise he wrote.
Authorship.
Beginning around the 12th century, some scholars began to doubt the historical existence of Sunzi, primarily on the grounds that he is not mentioned in the historical classic "Zuozhuan", which mentions most of the notable figures from the Spring and Autumn period. The name "Sun Wu" () does not appear in any text prior to the "Records of the Grand Historian", and has been suspected to be a made-up descriptive cognomen meaning "the fugitive warrior": the surname "Sun" is glossed as the related term "fugitive" ("xùn" ), while "Wu" is the ancient Chinese virtue of "martial, valiant" ("wǔ" ), which corresponds to Sunzi's role as the hero's "doppelgänger" in the story of Wu Zixu. Unlike Sun Wu, Sun Bin appears to have been an actual person who was a genuine authority on military matters, and may have been the inspiration for the creation of the historical figure "Sunzi" through a form of euhemerism.
Yinqueshan tomb discovery.
In 1972, the Yinqueshan Han slips were discovered in two Han dynasty (206AD 220) tombs near the city of Linyi in Shandong Province. Among the many bamboo slip writings contained in the tombswhich were sealed around 134 and 118, respectivelywere two separate texts: one attributed to "Sunzi", corresponding to the received text, and another attributed to Sun Bin, which explains and expands upon the earlier "The Art of War" by Sunzi. The Sun Bin text's material overlaps with much of the "Sunzi" text, and the two may be "a single, continuously developing intellectual tradition united under the Sun name". This discovery showed that much of the historical confusion was due to the fact that there were two texts that could have been referred to as "Master Sun's Art of War", not one. The content of the earlier text is about one-third of the chapters of the modern "The Art of War", and their text matches very closely. It is now generally accepted that the earlier "The Art of War" was completed sometime between 500 and 450.
The 13 chapters.
"The Art of War" is divided into 13 chapters (or "piān"); the collection is referred to as being one "zhuàn" ("whole" or alternatively "chronicle").
Quotations.
Chinese.
Verses from the book occur in modern daily Chinese idioms and phrases, such as the last verse of Chapter 3:
So it is said that if you know your enemies and know yourself, you will not be put at risk even if you have a hundred battles.If you only know yourself, but not your opponent, you may win or may lose.If you know neither yourself nor your enemy, you will always endanger yourself.
This has been more tersely interpreted and condensed into the Chinese modern proverb:
知己知彼，百戰不殆。 ("Zhī jǐ zhī bǐ, bǎi zhàn bù dài.")
If you know both yourself and your enemy, you can win numerous (literally, "a hundred") battles without jeopardy.
English.
Common examples can also be found in English use, such as verse 18 in Chapter 1:
All warfare is based on deception. Hence, when we are able to attack, we must seem unable; when using our forces, we must appear inactive; when we are near, we must make the enemy believe we are far away; when far away, we must make him believe we are near.
This has been abbreviated to its most basic form and condensed into the English modern proverb:
All warfare is based on deception.
Military and intelligence applications.
In many East Asian countries, "The Art of War" was part of the syllabus for potential candidates of military service examinations. Various translations are available.
During the Sengoku era in Japan, a daimyo named Takeda Shingen (1521–1573) is said to have become almost invincible in all battles without relying on guns, because he studied "The Art of War". The book even gave him the inspiration for his famous battle standard "Fūrinkazan" (Wind, Forest, Fire and Mountain), meaning fast as the wind, silent as a forest, ferocious as fire and immovable as a mountain.
The translator Samuel B. Griffith offers a chapter on "Sun Tzu and Mao Tse-Tung" where "The Art of War" is cited as influencing Mao's "On Guerrilla Warfare", "On the Protracted War" and "Strategic Problems of China's Revolutionary War", and includes Mao's quote: "We must not belittle the saying in the book of Sun Wu Tzu, the great military expert of ancient China, 'Know your enemy and know yourself and you can fight a thousand battles without disaster."
During the Vietnam War, some Vietcong officers studied "The Art of War" and reportedly could recite entire passages from memory.
General Vo Nguyen Giap successfully implemented tactics described in "The Art of War" during the Battle of Dien Bien Phu ending major French involvement in Indochina and leading to the accords which partitioned Vietnam into North and South. General Vo, later the main PVA military commander in the Vietnam War, was an avid student and practitioner of Sun Tzu’s ideas.
Finnish Field Marshal Mannerheim and general Aksel Airo were avid readers of "Art of War". They both read it in French; Airo kept the French translation of the book on his bedside table in his quarters.
The Department of the Army in the United States, through its Command and General Staff College, lists "The Art of War" as one example of a book that may be kept at a military unit's library.
"The Art of War" is listed on the Marine Corps Professional Reading Program (formerly known as the Commandant's Reading List). It is recommended reading for all United States Military Intelligence personnel and is required reading for all CIA officers.
According to some authors, the strategy of deception from "The Art of War" was studied and widely used by the KGB: ""I will force the enemy to take our strength for weakness, and our weakness for strength, and thus will turn his strength into weakness"". The book is widely cited by KGB officers in charge of disinformation operations in Vladimir Volkoff's novel "Le Montage.
Application outside the military.
"The Art of War" has been applied to many fields well outside of the military. Much of the text is about how to fight wars without actually having to do battle: it gives tips on how to outsmart one's opponent so that physical battle is not necessary. As such, it has found application as a training guide for many competitive endeavors that do not involve actual combat.
There are business books applying its lessons to office politics and corporate strategy. Many Japanese companies make the book required reading for their key executives. The book is also popular among Western business management, who have turned to it for inspiration and advice on how to succeed in competitive business situations. It has also been applied to the field of education.
"The Art of War" has been the subject of law books and legal articles on the trial process, including negotiation tactics and trial strategy.
"The Art of War" has also been applied in the world of sports. NFL coach Bill Belichick is known to have read the book and used its lessons to gain insights in preparing for games. Australian cricket as well as Brazilian association football coaches Luiz Felipe Scolari and Carlos Alberto Parreira are known to have embraced the text. Scolari made the Brazilian World Cup squad of 2002 study the ancient work during their successful campaign.
"The Art of War" is often quoted while developing tactics and/or strategy in Electronic Sports. Particularly, one of the fundamental books about e-sports, "Play To Win" by MIT graduate David Sirlin, is actually just an analysis about possible applications of the ideas from "The Art of War" in modern Electronic Sports.
"The Art of War" was released in 2014 as an ebook companion alongside the Art of War DLC for Europa Universalis IV, a PC strategy game by Paradox Development Studios, with a forward by Thomas Johansson.
Notable translations.
The book was translated into Manchu as Wylie: Tchauhai paita be gisurengge, Möllendorff: Coohai baita de gisurengge, Discourse on the art of War.
Another Manchu translation was made by Aisin Gioro Qiying.

</doc>
<doc id="65840" url="https://en.wikipedia.org/wiki?curid=65840" title="International Air Transport Association">
International Air Transport Association

The International Air Transport Association (IATA ) is a trade association of the world’s airlines. Consisting of around 260 airlines, primarily major carriers, representing 117 countries, the IATA's member airlines account for carrying approximately 83% of total Available Seat Kilometers air traffic. IATA supports airline activity and helps formulate industry policy and standards. It is headquartered in Montreal, Canada with Executive Offices in Geneva, Switzerland.
History.
IATA was formed in April 1945 in Havana, Cuba. It is the successor to the International Air Traffic Association, which was formed in 1919 at The Hague, Netherlands. At its founding, IATA consisted of 57 airlines from 31 countries. Much of IATA’s early work was technical and it provided input to the newly created International Civil Aviation Organization (ICAO), which was reflected in the annexes of the Chicago Convention, the international treaty that still governs the conduct of international air transport today.
The Chicago Convention couldn’t resolve the issue of who flies where, however, and this has resulted in the thousands of bilateral air transport agreements in existence today. The benchmark standard for the early bilaterals was the 1946 United States-United Kingdom Bermuda Agreement.
IATA was also charged by the governments with setting a coherent fare structure that avoided cut-throat competition but also looked after the interests of the consumer. The first Traffic Conference was held in 1947 in Rio de Janeiro and reached unanimous agreement on some 400 resolutions.
Aviation grew rapidly over the following decades and IATA’s work duly expanded. It transformed its trade association activities to take account of the new dynamics in aviation, which was seeing increasing demand from the leisure sector. Price flexibility became increasingly important and the United States led the way into deregulation in 1978.
IATA has cemented its position as the voice of the aviation industry in recent years, launching a number of important programs and lobbying governments in the wake of successive crises. Despite its factual influence, the IATA is a trade group with no legislative powers.
Priorities.
Safety.
Safety is the number one priority for IATA. The main instrument for safety is the IATA Operational Safety Audit (IOSA) and its successor, Enhanced IOSA. IOSA has also been mandated at the state level by several countries. In 2012, aviation posted its safest year ever. The global Western-built jet accident rate (measured in hull losses per million flights of Western-built jets) was 0.20, the equivalent of one accident every 5 million flights. Future improvements will be founded on data sharing with a database fed by a multitude of sources and housed by the Global Safety Information Center. In June 2014 the IATA set up a special panel to study measures to track aircraft in flight in real time. The move was in response to the disappearance without trace of Malaysia Airlines Flight 370 on 8 March 2014.
Security.
Security has become increasingly important following the September 11 attacks in 2001. Following a series of uncoordinated rules by different countries, the industry has developed a Checkpoint of the Future, which is based on risk assessment and passenger differentiation.
Simplifying the Business.
Simplifying the Business was launched in 2004. This initiative has introduced a number of crucial concepts to passenger travel, including the electronic ticket and the bar coded boarding pass. Many other innovations are being established as part of the Fast Travel initiative, including a range of self-service baggage options.
A new program that has drawn plenty of interest is New Distribution Capability. This will replace the old EDIFACT messaging standard that is still the basis of the global distribution system /travel agent channel and replace it with an XML standard. This will enable the same choices to be offered to high street travel shoppers as are offered to those who book directly through airline websites. A filing with the US Department of Transportation brought over 400 comments.
Environment.
IATA members and all industry stakeholders have agreed to three sequential environmental goals:
At the 69th IATA annual general meeting in Cape Town, South Africa, members overwhelmingly endorsed a resolution on “Implementation of the Aviation Carbon-Neutral Growth (CNG2020) Strategy.”
The resolution provides governments with a set of principles on how governments could:
IATA member airlines agreed that a single mandatory carbon offsetting scheme would be the simplest and most effective option for an MBM.
Services.
IATA provides consulting and training services in many areas crucial to aviation.
Travel Agent accreditation is available for travel professionals. Full accreditation allows agents to sell tickets on behalf of all IATA member airlines.
Cargo Agent accreditation is a similar program.
IATA also runs the Billing and Settlement Plan, which is a $300 billion-plus financial system that looks after airline money.
And it provides a number of business intelligence publications and services.
Training covers all aspects of aviation and ranges from beginner courses through to senior management courses.
Strategic partners.
IATA's Strategic Partners are aviation solution providers who, through their work with various IATA work groups, help build and maintain relationships with key industry stakeholders and work with IATA in serving the air transport industry. 
Publications - standards.
A number of standards are defined under the umbrella of IATA. One of the most important is the transport of dangerous goods (HAZMAT).

</doc>
<doc id="65845" url="https://en.wikipedia.org/wiki?curid=65845" title="Hypothyroidism">
Hypothyroidism

Hypothyroidism, also called underactive thyroid or low thyroid, is a common disorder of the endocrine system in which the thyroid gland does not produce enough thyroid hormone. It can cause a number of symptoms, such as poor ability to tolerate cold, a feeling of tiredness, constipation, depression, and weight gain. Occasionally there may be swelling of the front part of the neck due to goiter. Untreated hypothyroidism during pregnancy can lead to delays in growth and intellectual development in the baby, which is called cretinism.
Worldwide, too little iodine in the diet is the most common cause of hypothyroidism. In countries with enough iodine in the diet, the most common cause of hypothyroidism is the autoimmune condition Hashimoto's thyroiditis. Less common causes include: previous treatment with radioactive iodine, injury to the hypothalamus or the anterior pituitary gland, certain medications, a lack of a functioning thyroid at birth, or previous thyroid surgery. The diagnosis of hypothyroidism, when suspected, can be confirmed with blood tests measuring thyroid-stimulating hormone (TSH) and thyroxine levels.
Prevention at the population level has been with the universal salt iodization. Hypothyroidism can be treated with levothyroxine. --> The dose is adjusted according to symptoms and normalization of the thyroxine and TSH levels. --> Thyroid medication is safe in pregnancy. --> While a certain amount of dietary iodine is important, excessive amounts can worsen certain types of hypothyroidism.
Worldwide about one billion people are estimated to be iodine deficient; however, it is unknown how often this results in hypothyroidism. In Western countries, hypothyroidism occurs in 0.3–0.4% of people. Subclinical hypothyroidism, a milder form of hypothyroidism characterized by normal thyroxine levels and an elevated TSH level, is thought to occur in 4.3–8.5% of people. Hypothyroidism is more common in women than men. People over the age of 60 are more commonly affected. Dogs are also known to develop hypothyroidism and in rare cases cats and horses can also have the disorder. The word hypothyroidism is from Greek "hypo-" meaning "reduced", "thyreos" for "shield", and "eidos" for "form."
Signs and symptoms.
People with hypothyroidism often have no or only mild symptoms. Numerous symptoms and signs are associated with hypothyroidism, and can be related to the underlying cause, or a direct effect of having not enough thyroid hormones. Hashimoto's thyroiditis may present with the mass effect of a goiter (enlarged thyroid gland).
Delayed relaxation after testing the ankle jerk reflex is a characteristic sign in hypothyroidism and is associated with the severity of the hormone deficit.
Myxedema coma.
Myxedema coma is a rare but life-threatening state of extreme hypothyroidism. --> It may occur in those who are known to have hypothyroidism when they develop another illness, but it can be the first presentation of hypothyroidism. --> The illness is characterized by very low body temperature without shivering, confusion, a slow heart rate and reduced breathing effort. --> There may be physical signs suggestive of hypothyroidism, such as skin changes or enlargement of the tongue.
Pregnancy.
Even mild or subclinical hypothyroidism has been associated with impaired fertility and an increased risk of miscarriage. Hypothyroidism in early pregnancy, even with limited or no symptoms, may increase the risk of pre-eclampsia, offspring with lower intelligence, and the risk of infant death around the time of birth. Women are affected by hypothyroidism in 0.3–0.5% of pregnancies. Subclinical hypothyroidism during pregnancy has also been associated with gestational diabetes and birth of the baby before 37 weeks of pregnancy.
Children.
Newborn children with hypothyroidism may have normal birth weight and height (although the head may be larger than expected and the posterior fontanelle may be open). Some may have drowsiness, decreased muscle tone, a hoarse-sounding cry, feeding difficulties, constipation, an enlarged tongue, umbilical hernia, dry skin, a decreased body temperature and jaundice. A goiter is rare, although it may develop later in children who have a thyroid gland that does not produce functioning thyroid hormone. A goiter may also develop in children growing up in areas with iodine deficiency. Normal growth and development may be delayed, and not treating infants may lead to an intellectual impairment (IQ 6–15 points lower in severe cases). --> Other problems include the following: large scale and fine motor skills and coordination, reduced muscle tone, squinting, decreased attention span, and delayed speaking. Tooth eruption may be delayed.
In older children and adolescents, the symptoms of hypothyroidism may include fatigue, cold intolerance, sleepiness, muscle weakness, constipation, a delay in growth, overweight for height, pallor, coarse and thick skin, increased body hair, irregular menstrual cycles in girls, and delayed puberty. Signs may include delayed relaxation of the ankle reflex and a slow heart beat. A goiter may be present with a completely enlarged thyroid gland; sometimes only part of the thyroid is enlarged and it can be knobby in character.
Causes.
Hypothyroidism is caused by inadequate function of the gland itself (primary hypothyroidism) or by not enough stimulation by thyroid-stimulating hormone (central hypothyroidism). Primary hypothyroidism is about a thousandfold more common than central hypothyroidism.
Iodine deficiency is the most common cause of primary hypothyroidism and endemic goiter worldwide. In areas of the world with sufficient dietary iodine, hypothyroidism is most commonly caused by the autoimmune disease Hashimoto's thyroiditis (chronic autoimmune thyroiditis). Hashimoto's may be associated with a goiter. It is characterized by infiltration of the thyroid gland with T lymphocytes and autoantibodies against specific thyroid antigens such as thyroid peroxidase, thyroglobulin and the TSH receptor.
After women give birth, about 5% develop postpartum thyroiditis which can occur up to nine months afterwards. This is characterized by a short period of hyperthyroidism followed by a period of hypothyroidism; 20–40% remain permanently hypothyroid.
Autoimmune thyroiditis is associated with other immune-mediated diseases such as diabetes mellitus type 1, pernicious anemia, myasthenia gravis, celiac disease, rheumatoid arthritis and systemic lupus erythematosus. It may occur as part of autoimmune polyendocrine syndrome (type 1 and type 2).
Pathophysiology.
Thyroid hormone is required for the normal functioning of numerous tissues in the body. In health, the thyroid gland predominantly secretes thyroxine (T4), which is converted into triiodothyronine (T3) in other organs by the selenium-dependent enzyme iodothyronine deiodinase. Triiodothyronine binds to the thyroid hormone receptor in the nucleus of cells, where it stimulates the turning on of particular genes and the production of specific proteins. Additionally, the hormone binds to integrin αvβ3 on the cell membrane, thereby stimulating the sodium–hydrogen antiporter and processes such as formation of blood vessels and cell growth. In blood, almost all thyroid hormone (99.97%) is bound to plasma proteins such as thyroxine-binding globulin; only the free unbound thyroid hormone is biologically active.
The thyroid gland is the only source of thyroid hormone in the body; the process requires iodine and the amino acid tyrosine. Iodine in the bloodstream is taken up by the gland and incorporated into thyroglobulin molecules. The process is controlled by the thyroid-stimulating hormone (TSH, thyrotropin), which is secreted by the pituitary. Not enough iodine, or not enough TSH, can result in decreased production of thyroid hormones.
The hypothalamic–pituitary–thyroid axis plays a key role in maintaining thyroid hormone levels within normal limits. Production of TSH by the anterior pituitary gland is stimulated in turn by thyrotropin-releasing hormone (TRH), released from the hypothalamus. Production of TSH and TRH is decreased by thyroxine by a negative feedback process. Not enough TRH, which is uncommon, can lead to not enough TSH and thereby to not enough thyroid hormone production.
Pregnancy leads to marked changes in thyroid hormone physiology. The gland is increased in size by 10%, thyroxine production is increased by 50%, and iodine requirements are increased. Many women have normal thyroid function but have immunological evidence of thyroid autoimmunity (as evidenced by autoantibodies) or are iodine deficient, and develop evidence of hypothyroidism before or after giving birth.
Diagnosis.
Laboratory testing of thyroid stimulating hormone levels in the blood is considered the best initial test for hypothyroidism; a second TSH level is often obtained several weeks later for confirmation. Levels may be abnormal in the context of other illnesses, and TSH testing in hospitalized people is discouraged unless thyroid dysfunction is strongly suspected. An elevated TSH level indicates that the thyroid gland is not producing enough thyroid hormone, and free T4 levels are then often obtained. Measuring T3 is discouraged in the assessment for hypothyroidism. There are a number of symptom rating scales for hypothyroidism; they provide a degree of objectivity but have limited use for diagnosis.
Many cases of hypothyroidism are associated with mild elevations in creatine kinase and liver enzymes in the blood. They typically return to normal when hypothyroidism has been fully treated. Levels of cholesterol, low-density lipoprotein and lipoprotein (a) can be elevated; the impact of subclinical hypothyroidism on lipid parameters is less well-defined.
Very severe hypothyroidism and myxedema coma is characteristically associated with low sodium levels in the blood together with elevations in antidiuretic hormone, as well as acute worsening of kidney function due to a number of causes.
A diagnosis of hypothyroidism without any lumps or masses felt within the thyroid gland does not require thyroid imaging; however, if the thyroid feels abnormal, diagnostic imaging is then recommended. The presence of antibodies against thyroid peroxidase (TPO) makes it more likely that thyroid nodules are caused by autoimmune thyroiditis, but if there is any doubt, a needle biopsy may be required.
If the TSH level is normal or low and serum free T4 levels are low, this is suggestive of central hypothyroidism (not enough TSH or TRH secretion by the pituitary gland or hypothalamus). There may be other features of hypopituitarism, such as menstrual cycle abnormalities and adrenal insufficiency. There might also be evidence of a pituitary mass such as headaches and vision changes. Central hypothyroidism should be investigated further to determine the underlying cause.
Overt.
In overt primary hypothyroidism, TSH levels are high and T4 and T3 levels are low. Overt hypothyroidism may also be diagnosed in those who have a TSH on multiple occasions of greater than 5mIU/L, appropriate symptoms, and only a borderline low T4. It may also be diagnosed in those with a TSH of greater than 10mIU/L.
Subclinical.
Subclinical hypothyroidism is a milder form of hypothyroidism characterized by an elevated serum TSH level, but with a normal serum free thyroxine level. This milder form of hypothyroidism is most commonly caused by Hashimoto's thyroiditis. In adults it is diagnosed when TSH levels are greater than 5 mIU/L and less than 10mIU/L. The presentation of subclinical hypothyroidism is variable and classic signs and symptoms of hypothyroidism may not be observed. Of people with subclinical hypothyroidism, a proportion will develop overt hypothyroidism each year. In those with detectable antibodies against thyroid peroxidase (TPO), this occurs in 4.3%, while in those with no detectable antibodies, this occurs in 2.6%. Those with subclinical hypothyroidism and detectable anti-TPO antibodies who do not require treatment should have repeat thyroid function tests more frequently (e.g. yearly) compared with those who do not have antibodies.
Pregnancy.
During pregnancy, the thyroid gland must produce 50% more thyroid hormone to provide enough thyroid hormone for the developing fetus and the expectant mother. In pregnancy, free thyroxine levels may be lower than anticipated due to increased binding to thyroid binding globulin and decreased binding to albumin. They should either be corrected for the stage of pregnancy, or total thyroxine levels should be used instead for diagnosis. TSH values may also be lower than normal (particularly in the first trimester) and the normal range should be adjusted for the stage of pregnancy.
In pregnancy, subclinical hypothyroidism is defined as a TSH between 2.5 and 10 mIU/l with a normal thyroxine level, while those with TSH above 10 mIU/l are considered to be overtly hypothyroid even if the thyroxine level is normal. Antibodies against TPO may be important in making decisions about treatment, and should therefore be determined in women with abnormal thyroid function tests.
Determination of TPO antibodies may be considered as part of the assessment of recurrent miscarriage, as subtle thyroid dysfunction can be associated with pregnancy loss, but this recommendation is not universal, and presence of thyroid antibodies may not predict future outcome.
Prevention.
Hypothyroidism may be prevented in a population by adding iodine to commonly used foods. This public health measure has eliminated endemic childhood hypothyroidism in countries where it was once common. In addition to promoting the consumption of iodine-rich foods such as dairy and fish, many countries with moderate iodine deficiency have implemented universal salt iodization (USI). Encouraged by the World Health Organization, 130 countries now have USI, and 70% of the world's population are receiving iodized salt. In some countries, iodized salt is added to bread. Despite this, iodine deficiency has reappeared in some Western countries as a result of attempts to reduce salt intake.
Pregnant and breastfeeding women, who require 66% more daily iodine requirement than non-pregnant women, may still not be getting enough iodine. The World Health Organization recommends a daily intake of 250 µg for pregnant and breastfeeding women. As many women will not achieve this from dietary sources alone, the American Thyroid Association recommends a 150 µg daily supplement by mouth.
Screening.
Screening for hypothyroidism is performed in the newborn period in many countries, generally using TSH. This has led to the early identification of many cases and thus the prevention of developmental delay. It is the most widely used newborn screening test worldwide. While TSH-based screening will identify the most common causes, the addition of T4 testing is required to pick up the rarer central causes of neonatal hypothyroidism. If T4 determination is included in the screening done at birth, this will identify cases of congenital hypothyroidism of central origin in 1:16,000 to 1:160,000 children. Considering that these children usually have other pituitary hormone deficiencies, early identification of these cases may prevent complications.
In adults, widespread screening of the general population is a matter of debate. Some organizations (such as the United States Preventive Services Task Force) state that evidence is insufficient to support routine screening, while others (such as the American Thyroid Association) recommend either intermittent testing above a certain age in both sexes or only in women. Targeted screening may be appropriate in a number of situations where hypothyroidism is common: other autoimmune diseases, a strong family history of thyroid disease, those who have received radioiodine or other radiation therapy to the neck, those who have previously undergone thyroid surgery, those with an abnormal thyroid examination, those with psychiatric disorders, people taking amiodarone or lithium, and those with a number of health conditions (such as certain heart and skin conditions). Yearly thyroid function tests are recommended in people with Down syndrome, as they are at higher risk of thyroid disease.
Management.
Hormone replacement.
Most people with hypothyroidism symptoms and confirmed thyroxine deficiency are treated with a synthetic long-acting form of thyroxine, known as levothyroxine (L-thyroxine). In young and otherwise healthy people with overt hypothyroidism, a full replacement dose (adjusted by weight) can be started immediately; in the elderly and people with heart disease a lower starting dose is recommended to prevent over supplementation and risk of complications. Lower doses may be sufficient in those with subclinical hypothyroidism, while people with central hypothyroidism may require a higher than average dose.
Blood free thyroxine and TSH levels are monitored to help determine whether the dose is adequate. This is done 4–8 weeks after the start of treatment or a change in levothyroxine dose. Once the adequate replacement dose has been established, the tests can be repeated after 6 and then 12 months, unless there is a change in symptoms. In people with central/secondary hypothyroidism, TSH is not a reliable marker of hormone replacement and decisions are based mainly on the free T4 level. Levothyroxine is best taken 30–60 minutes before breakfast, or four hours after food, as certain substances such as food and calcium can inhibit the absorption of levothyroxine. There is no direct way of increasing thyroid hormone secretion by the thyroid gland.
Liothyronine.
Adding liothyronine (synthetic T3) to levothyroxine has been suggested as a measure to provide better symptom control, but this has not been confirmed by studies. In 2007, the British Thyroid Association stated that combined T4 and T3 therapy carried a higher rate of side effects and no benefit over T4 alone. Similarly, American guidelines discourage combination therapy due to a lack of evidence, although they acknowledge that some people feel better when receiving combination treatment. Treatment with liothyronine alone has not received enough study to make a recommendation as to its use; due to its shorter half-life it needs to be taken more often.
People with hypothyroidism who do not feel well despite optimal levothyroxine dosing may request adjunctive treatment with liothyronine. A 2012 guideline from the European Thyroid Association recommends that support should be offered with regards to the chronic nature of the disease, and that other causes for the symptoms should be excluded. Addition of liothyronine should be regarded as experimental, initially only for a trial period of 3 months, and in a set ratio to the current dose of levothyroxine. The guideline explicitly aims to enhance the safety of this approach and to counter its indiscriminate use.
Desiccated animal thyroid.
Desiccated thyroid extract is an animal-based thyroid gland extract, most commonly from pigs. It is a combination therapy, containing forms of T4 and T3. It also contains calcitonin (a hormone produced in the thyroid gland involved in the regulation of calcium levels), T1 and T2; these are not present in synthetic hormone medication. This extract was once a mainstream hypothyroidism treatment, but its use today is unsupported by evidence; British Thyroid Association and American professional guidelines discourage its use.
Subclinical hypothyroidism.
There is little evidence whether there is a benefit from treating subclinical hypothyroidism, and whether this offsets the risks of overtreatment. Untreated subclinical hypothyroidism may be associated with a modest increase in the risk of coronary artery disease. A 2007 review found no benefit of thyroid hormone replacement except for "some parameters of lipid profiles and left ventricular function". There is no association between subclinical hypothyroidism and an increased risk of bone fractures.
Since 2008, consensus American and British opinion has been that in general people with TSH under 10 mIU/l do not require treatment. American guidelines recommend that treatment be considered if the TSH is elevated but below 10 mIU/l in people with symptoms of hypothyroidism, detectable antibodies against thyroid peroxidase, a history of heart disease or are at an increased risk for heart disease.
Myxedema coma.
Myxedema coma or severe decompensated hypothyroidism usually requires admission to the intensive care, close observation and treatment of abnormalities in breathing, temperature control, blood pressure, and sodium levels. Mechanical ventilation may be required, as well as fluid replacement, vasopressor agents, careful rewarming, and corticosteroids (for possible adrenal insufficiency which can occur together with hypothyroidism). Careful correction of low sodium levels may be achieved with hypertonic saline solutions or vasopressin receptor antagonists. For rapid treatment of the hypothyroidism, levothyroxine or liothyronine may be administered intravenously, particularly if the level of consciousness is too low to be able to safely swallow medication.
Pregnancy.
In women with known hypothyroidism who become pregnant, it is recommended that serum TSH levels are closely monitored. Levothyroxine should be used to keep TSH levels within the normal range for that trimester. The first trimester normal range is below 2.5 mIU/L and the second and third trimesters normal range is below 3.0 mIU/L. Treatment should be guided by total (rather than free) thyroxine or by the free T4 index. Similarly to TSH, the thyroxine results should be interpreted according to the appropriate reference range for that stage of pregnancy. The levothyroxine dose often needs to be increased after pregnancy is confirmed, although this is based on limited evidence and some recommend that it is not always required; decisions may need to based on TSH levels.
Women with anti-TPO antibodies who are trying to become pregnant (naturally or by assisted means) may require thyroid hormone supplementation even if the TSH level is normal. This is particularly true if they have had previous miscarriages or have been hypothyroid in the past. Supplementary levothyroxine may reduce the risk of preterm birth and possibly miscarriage. The recommendation is stronger in pregnant women with subclinical hypothyroidism (defined as TSH 2.5–10 mIU/l) who are anti-TPO positive, in view of the risk of overt hypothyroidism. If a decision is made not to treat, close monitoring of the thyroid function (every 4 weeks in the first 20 weeks of pregnancy) is recommended. If anti-TPO is not positive, treatment for subclinical hypothyroidism is not currently recommended. It has been suggested that many of the aforementioned recommendations could lead to unnecessary treatment, in the sense that the TSH cutoff levels may be too restrictive in some ethnic groups; there may be little benefit from treatment of subclinical hypothyroidism in certain cases.
Epidemiology.
Worldwide about one billion people are estimated to be iodine deficient; however, it is unknown how often this results in hypothyroidism. In large population-based studies in Western countries with sufficient dietary iodine, 0.3–0.4% of the population have overt hypothyroidism. A larger proportion, 4.3–8.5%, have subclinical hypothyroidism. Of people with subclinical hypothyroidism, 80% have a TSH level below the 10 mIU/l mark regarded as the threshold for treatment. Children with subclinical hypothyroidism often return to normal thyroid function, and a small proportion develops overt hypothyroidism (as predicted by evolving antibody and TSH levels, the presence of celiac disease, and the presence of a goiter).
Women are more likely to develop hypothyroidism than men. In population-based studies, women were seven times more likely than men to have TSH levels above 10 mU/l. 2–4% of people with subclinical hypothyroidism will progress to overt hypothyroidism each year. The risk is higher in those with antibodies against thyroid peroxidase. Subclinical hypothyroidism is estimated to affect approximately 2% of children; in the adults subclinical hypothyroidism is more common in the elderly, and in Caucasians. There is a much higher rate of thyroid disorders, the most common of which is hypothyroidism, in individuals with Down syndrome and Turner syndrome.
Very severe hypothyroidism and myxedema coma is rare, with it estimated to occur in 0.22 per million people a year. The majority of cases occur in women over 60 years of age, although it may happen in all age groups.
Most hypothyroidism is primary in nature. Central/secondary hypothyroidism affects 1:20,000 to 1:80,000 of the population, or about one out of every thousand people with hypothyroidism.
History.
In 1811, Bernard Courtois discovered iodine was present in seaweed, and iodine intake was linked with goiter size in 1820 by Jean-Francois Coindet. Gaspard Adolphe Chatin proposed in 1852 that endemic goiter was the result of not enough iodine intake, and Eugen Baumann demonstrated iodine in thyroid tissue in 1896.
The first cases of myxedema were recognized in the mid-19th century (1870s), but its connection to the thyroid was not discovered until the 1880s, when myxedema was observed in people following the removal of the thyroid gland (thyroidectomy). The link was further confirmed in the late 19th century when people and animals who had had their thyroid removed showed improvement in symptoms with transplantation of animal thyroid tissue. The severity of myxedema, and its associated risk of mortality and complications, created interest in discovering effective treatments for hypothyroidism. Transplantation of thyroid tissue demonstrated some efficacy, but recurrences of hypothyroidism was relatively common, and sometimes required multiple repeat transplantations of thyroid tissue.
In 1891, the English physician George Redmayne Murray introduced subcutaneously injected sheep thyroid extract, followed shortly after by an oral formulation. Purified thyroxine was introduced in 1914 and in the 1930s synthetic thyroxine became available, although desiccated animal thyroid extract remained widely used. Liothyronine was identified in 1952.
Early attempts at titrating therapy for hypothyroidism proved difficult. After hypothyroidism was found to cause a lower basal metabolic rate, this was used as a marker to guide adjustments in therapy in the early 20th century (around 1915). However, a low basal metabolic rate was known to be non-specific, also present in malnutrition. The first laboratory test to be helpful in assessing thyroid status was the serum protein bound iodine, which came into use around the 1950s.
In 1971, the thyroid stimulating hormone (TSH) radioimmunoassay was developed, which was the most specific marker for assessing thyroid states in patients. Many people who were being treated based on basal metabolic rate, minimizing hypothyroid symptoms, or based on serum protein bound iodine, were found to have excessive thyroid hormone. The following year, in 1972, a T3 radioimmunoassay was developed, and in 1974, a T4 radioimmunoassay was developed.
Other animals.
In veterinary practice, dogs are the species most commonly affected by hypothyroidism. The majority of cases occur as a result of primary hypothyroidism, of which two types are recognized: lymphocytic thyroiditis, which is probably immune-driven and leads to destruction and fibrosis of the thyroid gland, and idiopathic atrophy, which leads to the gradual replacement of the gland by fatty tissue. There is often lethargy, cold intolerance, exercise intolerance, and weight gain. Furthermore, skin changes and fertility problems are seen in dogs with hypothyroidism, as well as a number of other symptoms. The signs of myxedema can be seen in dogs, with prominence of skin folds on the forehead, and cases of myxedema coma are encountered. The diagnosis can be confirmed by blood test, as the clinical impression alone may lead to overdiagnosis. Lymphocytic thyroiditis is associated with detectable antibodies against thyroglobulin, although they typically become undetectable in advanced disease. Treatment is with thyroid hormone replacement.
Other species that are less commonly affected include cats and horses, as well as other large domestic animals. In cats, hypothyroidism is usually the result of other medical treatment such as surgery or radiation. In young horses, congenital hypothyroidism has been reported predominantly in Western Canada and has been linked with the mother's diet.

</doc>
<doc id="65847" url="https://en.wikipedia.org/wiki?curid=65847" title="Vitiligo">
Vitiligo

Vitiligo is a chronic skin condition characterized by portions of the skin losing their pigment. It occurs when skin pigment cells die or are unable to function. Aside from cases of contact with certain chemicals, the cause of vitiligo is unknown. Research suggests vitiligo may arise from autoimmune, genetic, oxidative stress, neural, or viral causes. Vitiligo is typically classified into two main categories: segmental and non-segmental vitiligo. Half of those affected show the disorder before age 20, though most develop it before age 40. 
The global incidence of vitiligo is less than 1%, with some populations averaging 2–3% and rarely as high as 16%. Autoimmune diseases such as Addison's disease, Hashimoto's thyroiditis, and type 1 diabetes mellitus tend to occur more often in people who have vitiligo. There is no known cure for vitiligo but many treatment options are available including topical steroids, calcineurin inhibitors, and phototherapy.
Classification.
Classification attempts to quantify vitiligo have been analyzed as being somewhat inconsistent, while recent consensus have agreed to a system of segmental vitiligo (SV) and non-segmental vitiligo (NSV). NSV is the most common type of vitiligo.
Non-segmental.
In non-segmental vitiligo (NSV), there is usually some form of symmetry in the location of the patches of depigmentation. New patches also appear over time and can be generalized over large portions of the body or localized to a particular area. Vitiligo where little pigmented skin remains is referred to as "vitiligo universalis". NSV can come about at any age (unlike segmental vitiligo, which is far more prevalent in teenage years).
Classes of non-segmental vitiligo include the following:
Segmental.
Segmental vitiligo (SV) differs in appearance, cause and prevalence from associated illnesses. Its treatment is different from that of NSV. It tends to affect areas of skin that are associated with dorsal roots from the spinal cord and is most often unilateral. It spreads much more rapidly than NSV and, without treatment, it is much more stable/static in course and its association with autoimmune diseases appears to be weaker than that of generalized vitiligo. SV is a very treatable condition that responds to topical treatment.
Signs and symptoms.
The only sign of vitiligo is the presence of pale patchy areas of depigmented skin which tend to occur on the extremities. The patches are initially small, but often grow and change shape. When skin lesions occur, they are most prominent on the face, hands and wrists. The loss of skin pigmentation is particularly noticeable around body orifices, such as the mouth, eyes, nostrils, genitalia and umbilicus. Some lesions have increased skin pigment around the edges. Patients who are stigmatized for their condition may experience depression and similar mood disorders.
Causes.
Although multiple hypotheses have been suggested as potential triggers that cause vitiligo, studies strongly imply that changes in the immune system are responsible for the condition. Vitiligo has been proposed to be a multifactorial disease with genetic susceptibility and environmental factors both thought to play a role. 
The TYR gene encodes the protein tyrosinase, which is not a component of the immune system, but is an enzyme of the melanocyte that catalyzes melanin biosynthesis, and a major autoantigen in generalized vitiligo. Some state the sunburns can cause the disease but there is not good evidence to support this.
Immune.
Variations in genes that are part of the immune system or part of melanocytes have both been associated with vitiligo. It is also thought to be caused by the immune system attacking and destroying the melanocytes of the skin. A genomewide association study found approximately 36 independent susceptibility loci for generalized vitiligo.
Autoimmune associations.
Vitiligo is sometimes associated with autoimmune and inflammatory diseases such as Hashimoto's thyroiditis, scleroderma, rheumatoid arthritis, type 1 diabetes mellitus, psoriasis, Addison's disease, pernicious anemia, alopecia areata, and systemic lupus erythematosus.
Among the inflammatory products of NALP1 are caspase 1 and caspase 7, which activate the inflammatory cytokine interleukin-1β. Interleukin-1β is expressed at high levels in patients with vitiligo. In one of the mutations, the amino acid leucine in the NALP1 protein was replaced by histidine (Leu155->His). The original protein and sequence is highly conserved in evolution, and is found in humans, chimpanzee, rhesus monkey, and the bush baby. Addison's disease (typically an autoimmune destruction of the adrenal glands) may also be seen in individuals with vitiligo.
Diagnosis.
An ultraviolet light can be used in the early phase of this disease for identification and to determine effectiveness of treatment. Skin with vitiligo, when exposed to a blacklight, will glow blue. In contrast, healthy skin will have no reaction.
Differential diagnosis.
Conditions with similar symptoms include the following:
Treatment.
There is no cure for vitiligo but several treatment options are available. The best evidence is for applied steroids and the combination of ultraviolet light in combination with creams. Due to the higher risks of skin cancer, the United Kingdom's National Health Service suggests phototherapy only be used if primary treatments are ineffective. Lesions located on the hands, feet, and joints are the most difficult to repigment; those on the face are easiest to return to the natural skin color.
Immune mediators.
Topical preparations of immune suppressing medications including glucocorticoids (such as 0.05% clobetasol or 0.10% betamethasone) and calcineurin inhibitors (such as tacrolimus or pimecrolimus) are considered to be first-line vitiligo treatments.
Phototherapy.
Phototherapy is considered a second-line treatment for vitiligo. Exposing the skin to light from UVB lamps is the most common treatment for vitiligo. The treatments can be done at home with an UVB lamp or in a clinic. The exposure time is managed so that the skin doesn't suffer overexposure. Treatment can take a few weeks if the spots are on the neck and face and if they existed not more than 3 years. If the spots are on the hands and legs and have been there more than 3 years, it can take a few months. Phototherapy sessions are done 2–3 times a week. Spots on a large area of the body may require full body treatment in a clinic or hospital. UVB broadband and narrowband lamps can be used, but narrowband ultraviolet picked around 311 nm is the choice. It has been constitutively reported that combination of UVB phototherapy with other topical treatments improves repigmentation.
Ultraviolet light (UVA) treatments are normally carried out in a hospital clinic. Psoralen and ultraviolet A light (PUVA) treatment involves taking a drug that increases the skin's sensitivity to ultraviolet light, then exposing the skin to high doses of UVA light. Treatment is required twice a week for 6–12 months or longer. Because of the high doses of UVA and psoralen, PUVA may cause side effects such as sunburn-type reactions or skin freckling.
Narrowband ultraviolet B (NBUVB) phototherapy lacks the side-effects caused by psoralens and is as effective as PUVA. As with PUVA, treatment is carried out twice weekly in a clinic or every day at home, and there is no need to use psoralen.
Skin camouflage.
In mild cases, vitiligo patches can be hidden with makeup or other cosmetic camouflage solutions. If the affected person is pale-skinned, the patches can be made less visible by avoiding tanning of unaffected skin.
De-pigmenting.
Most vitiligo is idiopathic; however, in cases where it is triggered by skin bleaching or other substances, it is said to be chemical after being treated with bleaching agents. In cases of extensive vitiligo the option to de-pigment the unaffected skin with topical drugs like monobenzone, mequinol, or hydroquinone may be considered to render the skin an even colour. The removal of all the skin pigment with monobenzone is permanent and vigorous. Sun-safety must be adhered to for life to avoid severe sun burn and melanomas. Depigmentation takes about a year to complete.
History.
Descriptions of a disease believed to be vitiligo date back to a passage in the medical text Ebers Papyrus circa 1500 BC in ancient Egypt. Mentions of whitening of the skin was also present circa 1400 BC in sacred Indian texts such as Atharvaveda as well as Shinto prayers in East Asia circa 1200 BC. The Hebrew word "Zora'at" from the Old Testament book of Leviticus dating to 1280 BCE (or 1312 BCE) described a group of skin disease associated with white spots, and a subsequent translation to Greek led to continued conflation of those with vitiligo with leprosy and spiritual uncleanliness. Medical sources in the ancient world such as Hippocrates often did not differentiate between vitiligo and leprosy, often grouping these diseases together. In Arabic literature, the word "alabras" has been associated with vitiligo, with this word found in the Koran. The name "vitiligo" was first used by the Roman physician Aulus Cornelius Celsus in his classic medical text "De Medicina".
Etymology.
The etymology of the term "vitiligo" is believed to be derived from "vitium", meaning "defect" or "blemish".
Research.
Afamelanotide is in phase II and III clinical trials for vitiligo and other skin diseases.
A medication for rheumatoid arthritis, tofacitinib, has been tested for the treatment of vitiligo.
In October 1992, a scientific report was published of successfully transplanting melanocytes to vitiligo affected areas, effectively repigmenting the region. The procedure involved taking a thin layer of pigmented skin from the patient's gluteal region. Melanocytes were then separated out to a cellular suspension that was expanded in culture. The area to be treated was then denuded with a dermabrader and the melanocytes graft applied. Between 70 and 85 percent of patients experienced nearly complete repigmentation of their skin. The longevity of the repigmentation differed from person to person. By now, a number of transplantation techniques has been developed, including transplantation of melanocyte precursors derived from hair follicles. Transplantation procedures are frequently used to treat segmental vitiligo which is poorly responsive to other types of treatment. In non-segmental vitiligo, success is achieved when treating patches that are not expanding (so called stable vitiligo).

</doc>
<doc id="65849" url="https://en.wikipedia.org/wiki?curid=65849" title="Pituitary tumour">
Pituitary tumour

The anterior pituitary secretes a number of hormones including:
Tumours arising from such glandular tissue is known as an adenoma. As the pituitary is a highly glandular tissue the most common type of tumour seen is an adenoma, though other cell types are capable of tumourigenesis at this site also. Most pituitary tumours are benign though this may be somewhat of a misnomer. Whilst their malignant potential is negligible they may not be indolent and can cause symptoms. These symptoms may depend on if the adenoma is secretory or not and its potential for local growth.
As stated above the pituitary makes lots of hormones. Pituitary adenomas may secrete one or more (usually just one) of these in supraphysiologic amounts or they may be non-secretatory. 
The most common tumours secrete growth hormone (see acromegaly) or prolactin (see prolactinoma or hyperprolactinaemia). 
Thus phenotypical and physiological symptoms can be caused by these increases in hormones. Other symptoms can occur from local expansive growth of the tumour in the pituitary fossa. Classical symptoms include raised ICP and bilateral temporal hemianopia due to localised compression of the optic nerve. 
Once tumours were categorised by their light microscopic appearance but now tumours are more reliably categorised by immunoperoxidase studies.
Blood tests for the hormones are important diagnostic tools.
CT scans, MRI and other imaging techniques are important for determining size (and seriousness of tumours), growth over time and treatment options.
Most tumours are benign but are quite serious because of their position close to important brain structures.
Treatment includes:

</doc>
<doc id="65852" url="https://en.wikipedia.org/wiki?curid=65852" title="Idiopathic hypoglycemia">
Idiopathic hypoglycemia

Idiopathic hypoglycemia is, literally, a medical condition in which the glucose level in the blood (blood glucose) is abnormally low due to an undeterminable cause. This is considered an incomplete and unsatisfactory diagnosis by physicians and is rarely used by endocrinologists, as it implies an unfinished diagnostic evaluation. In general, the more severe the hypoglycemia and the more clearly it is proven, the less likely it is to remain "idiopathic".
"Idiopathic hypoglycemia" can also be a synonym for reactive hypoglycemia or for hypoglycemia that is not diagnosed by a physician and does not fulfill the Whipple triad criteria. A more precise term for that condition is idiopathic postprandial syndrome.

</doc>
<doc id="65854" url="https://en.wikipedia.org/wiki?curid=65854" title="Admiral Kuznetsov">
Admiral Kuznetsov

Admiral Kuznetsov may refer to:

</doc>
<doc id="65858" url="https://en.wikipedia.org/wiki?curid=65858" title="Eliezer Yudkowsky">
Eliezer Yudkowsky

Eliezer Shlomo Yudkowsky (born September 11, 1979) is an American artificial intelligence researcher known for popularizing the idea of friendly artificial intelligence. He is a Research Fellow and co-founder at the Machine Intelligence Research Institute, a private research nonprofit based in Berkeley, California.
Work in artificial intelligence safety.
Goal learning and incentives in software systems.
Yudkowsky's views on the safety challenges posed by future generations of AI systems are discussed in the standard undergraduate textbook in AI, Stuart Russell and Peter Norvig's "". Noting the difficulty of formally specifying general-purpose goals by hand, Russell and Norvig cite Yudkowsky's proposal that autonomous and adaptive systems be designed to learn correct behavior over time:
Citing Steve Omohundro's idea of instrumental convergence, Russell and Norvig caution that autonomous decision-making systems with poorly designed goals would have default incentives to treat humans adversarially, or as dispensable resources, unless specifically designed to counter such incentives: "even if you only want your program to play chess or prove theorems, if you give it the capability to learn and alter itself, you need safeguards".
In response to the instrumental convergence concern, Yudkowsky and other MIRI researchers have recommended that work be done to specify software agents that converge on safe default behaviors even when their goals are misspecified. The Future of Life Institute (FLI) summarizes this research program in the Open Letter on Artificial Intelligence research priorities document:
Yudkowsky argues that as AI systems become increasingly intelligent, new formal tools will be needed in order to avert default incentives for harmful behavior, as well as to inductively teach correct behavior. These lines of research are discussed in MIRI's 2015 technical agenda.
System reliability and transparency.
Yudkowsky studies decision theories that achieve better outcomes than causal decision theory in Newcomblike problems. This includes decision procedures that allow agents to cooperate with equivalent reasoners in the one-shot prisoner's dilemma. Yudkowsky has also written on theoretical prerequisites for self-verifying software.
Yudkowsky argues that it is important for advanced AI systems to be cleanly designed and transparent to human inspection, both to ensure stable behavior and to allow greater human oversight and analysis. Citing papers on this topic by Yudkowsky and other MIRI researchers, the FLI research priorities document states that work on defining correct reasoning in embodied and logically non-omniscient agents would be valuable for the design, use, and oversight of AI agents.
Capabilities forecasting.
In their discussion of Omohundro and Yudkowsky's work, Russell and Norvig cite I. J. Good's 1965 prediction that when computer systems begin to outperform humans in software engineering tasks, this may result in a feedback loop of increasingly capable AI systems. This raises the possibility that AI's impact could increase very quickly after it reaches a certain level of capability.
In the intelligence explosion scenario inspired by Good's hypothetical, recursively self-improving AI systems quickly transition from subhuman general intelligence to superintelligent. Nick Bostrom's 2014 book "" sketches out Good's argument in greater detail, while making a broader case for expecting AI systems to eventually outperform humans across the board. Bostrom cites writing by Yudkowsky on inductive value learning and on the risk of anthropomorphizing advanced AI systems, e.g.: "AI might make an "apparently" sharp jump in intelligence purely as the result of anthropomorphism, the human tendency to think of 'village idiot' and 'Einstein' as the extreme ends of the intelligence scale, instead of nearly indistinguishable points on the scale of minds-in-general."
The Open Philanthropy Project, an offshoot of the charity evaluator GiveWell, credits Yudkowsky and Bostrom with several (paraphrased) arguments for expecting future AI advances to have a large societal impact:
Russell and Norvig raise the objection that there are known limits to intelligent problem-solving from computational complexity theory; if there are strong limits on how efficiently algorithms can solve various computer science tasks, then intelligence explosion may not be possible. Yudkowsky has debated the likelihood of intelligence explosion with economist Robin Hanson, who argues that AI progress is likely to accelerate over time, but is not likely to be localized or discontinuous.
In a 2013 report, "Intelligence Explosion Microeconomics", Yudkowsky suggests that more formalized microeconomic model of cognition and self-improvement could yield better predictions about whether intelligence explosion is feasible. In contrast, Yudkowsky expresses skepticism about our ability to predict when AI algorithms are likely to exceed humans in general intelligence, even if we attempt to improve our timelines with more formal models.
Rationality writing.
Between 2006 and 2009, Yudkowsky and Hanson were the principal contributors to "Overcoming Bias", a cognitive and social science blog sponsored by the Future of Humanity Institute of Oxford University. In February 2009, Yudkowsky founded "LessWrong", a "community blog devoted to refining the art of human rationality". "Overcoming Bias" has since functioned as Hanson's personal blog. "LessWrong" has been covered in depth in "Business Insider", and core concepts from "LessWrong" have been referenced in columns in "The Guardian".
Yudkowsky has also written several works of fiction. His fan fiction story, "Harry Potter and the Methods of Rationality", uses plot elements from J.K. Rowling's "Harry Potter" series to illustrate topics in science. "The New Yorker" describes "Harry Potter and the Methods of Rationality" as a retelling of Rowling's original "in an attempt to explain Harry's wizardry through the scientific method".
Over 300 blogposts by Yudkowsky have been released as six books, collected in a single ebook titled "Rationality: From AI to Zombies" by the Machine Intelligence Research Institute in 2015.

</doc>
<doc id="65861" url="https://en.wikipedia.org/wiki?curid=65861" title="Lu Xun">
Lu Xun

Lu Xun or Lu Hsün (Wade-Giles), was the pen name of Zhou Shuren (25 September 1881 – 19 October 1936), a leading figure of modern Chinese literature. Writing in Vernacular Chinese as well as Classical Chinese, Lu Xun was a short story writer, editor, translator, literary critic, essayist, and poet. In the 1930s he became the titular head of the League of Left-Wing Writers in Shanghai.
Lu Xun was born into a family of landlords and government officials in Shaoxing, Zhejiang; the family's financial resources declined over the course of his youth. Lu aspired to take the imperial civil service exam; but, due to his family's relative poverty, was forced to attend government-funded schools teaching "Western education". Upon graduation, Lu went to medical school in Japan, but later dropped out. He became interested in studying literature, but was eventually forced to return to China due to his family's lack of funds. After returning to China, Lu worked for several years teaching at local secondary schools and colleges before finally finding a job at the national Ministry of Education.
After the 1919 May Fourth Movement, Lu Xun's writing began to exert a substantial influence on Chinese literature and popular culture. Like many leaders of the May Fourth Movement, he was primarily a leftist and liberal. He was highly acclaimed by the Chinese government after 1949, when the People's Republic of China was founded, and Mao Zedong himself was a lifelong admirer of Lu Xun's writing. Though sympathetic to socialist ideas, Lu Xun never joined the Chinese Communist Party.
Biography.
Early life.
Lu Xun was born in Shaoxing, Zhejiang. As was common in pre-modern China, Lu Xun had many names. His birth name was "Zhou Zhangshou". His courtesy name was "Yushan", but he later changed his courtesy name to "Yucai". In 1898, before he went to the Jiangnan Naval Academy, he took the given name "Shuren", which means, figuratively, "to be an educated man". The name that he is best known as in English, "Lu Xun", was a literary pseudonym that he chose when his fiction was first published, in 1918.
By the time Lu Xun was born, the Zhou family had been prosperous for centuries, and had become wealthy through landowning, pawnbroking, and by having several family members promoted to government positions. His paternal grandfather, Zhou Fuqing, was appointed to the Imperial Hanlin Academy in Beijing: the highest position possible for aspiring civil servants at that time. Lu's early education was based on the Confucian classics, in which he studied poetry, history, and philosophy, but later reflected were neither useful nor interesting to him. Instead, he enjoyed folk stories and traditions: local operas, the mythological creatures and stories in the "Classic of Mountains and Seas", and the ghost stories told to him by an illiterate servant who raised him, Ah Chang (who he called "Mother Chang"). Zhou's mother was a member of the same gentry class as Lu Xun's father, from a slightly smaller town in the countryside (Anqiaotou, Zhejiang). Because formal education was not considered socially appropriate for girls, she did not receive any, but she still taught herself how to read and write. The surname "Lu" in Zhou Shouren's pen name, "Lu Xun", was the same as his mother's surname, "Lu".
By the time Lu was born, his family's prosperity had already been declining. His father, Zhou Boyi, had been successful at passing the lowest, county-level imperial examinations (the route to wealth and social success in imperial China), but was unsuccessful in writing the more competitive provincial-level examinations. In 1893 Zhou Boyi was discovered attempting to bribe an examination official. Lu Xun's grandfather was implicated, and was arrested and sentenced to beheading for his son's crime. The sentence was later commuted, and he was imprisoned in Hangzhou instead. After the affair Zhou Boyi was stripped of his position in the government and forbidden to write the civil service examinations ever again. The Zhou family prevented Lu's grandfather from being executed only through regular, expensive bribes to authorities, until he was finally released in 1901.
After the family's attempt at bribery was discovered, Zhou Boyi engaged in heavy drinking and opium use, and his health declined. Local Chinese doctors attempted to cure him through a series of expensive prescriptions of traditional Chinese cures, including monogamous crickets, sugar cane that had survived frost three times, ink, and the skin from a drum. Despite these expensive medical treatments, Zhou Boyi died of an asthma attack in 1896. He might have suffered from dropsy.
Education.
Lu Xun half-heartedly participated in one civil service examination, in 1899, but then abandoned pursuing a traditional Confucian education or career. He intended to study at a prestigious school, the "Seeking Affirmation Academy", in Hangzhou, but was forced by his family's poverty to study at a tuition-free military school, the "Jiangnan Naval Academy", in Nanjing, instead. As a consequence of Lu's decision to attend a military school specializing in Western education, his mother wept, he was instructed to change his name (to avoid disgracing his family), and some of his relatives began to look down on him. Lu attended the Jiangnan Naval Academy for half a year, and left after it became clear that he would be assigned to work in an engine room, below deck, which he considered degrading. He later wrote that he was dissatisfied with the quality of teaching at the academy. After leaving the school, Lu sat for the lowest level of the civil service exams, and finished 137th of 500. He intended to sit for the next-highest level, but became upset when one of his younger brothers died, and abandoned his plans.
Lu Xun transferred to another government-funded school, the "School of Mines and Railways", and graduated from that school in 1902. The school was Lu's first exposure to Western literature, philosophy, history, and science, and he studied English and German intensely. Some of the influential authors that he read during that period include T. H. Huxley, John Stuart Mill, Yan Fu, and Liang Qichao. His later social philosophy may have been influenced by several novels about social conflict that he read during the period, including "Ivanhoe" and "Uncle Tom's Cabin".
He did very well at the school with relatively little effort, and occasionally experienced racism directed at him from resident Manchu bannermen. The racism he experienced may have influenced his later sense of Han Chinese nationalism. After graduating Lu Xun planned to become a Western doctor.
In 1902, Lu Xun left for Japan on a Qing government scholarship to pursue an education in Western medicine. After arriving in Japan he attended the "Kobun Institute", a preparatory language school for Chinese students attending Japanese universities. After encouragement from a classmate, he cut off his queue (which all Han Chinese were legally forced to wear in China) and practice some jujutsu in his free time. He had an ambiguous attitude towards Chinese revolutionary politics during the period, and it is not clear whether he joined any of the revolutionary parties (such as the Tongmenghui) that were popular among Chinese expatriates in Japan at that time. He experienced anti-Chinese racism, but was simultaneously disgusted with the behaviour of some Chinese who were living in Japan. His earliest surviving essays, written in Classical Chinese, were published while he was attending this school, and he published his first Chinese translations of famous and influential Western novels, including Jules Verne's "Journey to the Moon" and "Twenty Thousand Leagues under the Sea".
In 1904, Lu began studying at the Sendai Medical Academy, in northern Honshu, but remained there for less than two years. He generally found his studies at the school tedious and difficult, partially due to his imperfect Japanese. While studying in Sendai he befriended one of his professors, Fujino Genkurō (藤野厳九郎), who helped him prepare class notes. Because of their friendship Lu was accused by his classmates of receiving special assistance from Fujino. Lu later recalled his mentor respectfully and affectionately in an essay, "Mr Fujino", published in "Dawn Blossoms Plucked at Dusk". Fujino later repaid Lu's respect in an obituary essay on his death, in 1937. The Sendai Medical Academy is now the medical school of Tohoku University.
While Lu Xun was attending medical school, the Russo-Japanese War (1904–1905) broke out. Part of the war was fought on disputed Chinese land. While the war was being fought it became common for lecturers to show slides of pictures from the war to their students after their classes had ended. After one of his biology classes Lu was shown a scene in which a Japanese soldier was about to behead a Chinese man who had allegedly spied for the Russians, surrounded by Chinese who were apathetic to the scene. In his preface to "Nahan", the first collection of his short stories, Lu explained how viewing this scene influenced him to quit studying Western medicine, and to become a literary physician to what he perceived to be China's spiritual problems instead:
In March 1906, Lu Xun abruptly and secretly terminated his pursuit of the degree and left college. At the time he told no one. After arriving in Tokyo he made sure that the Chinese embassy would not cancel his scholarship and registered at the local German Institute, but was not required to take classes there. He began to read Nietzsche, and wrote a number of essays in the period that were influenced by his philosophy.
In June 1906, Lu's mother heard a rumor that he had married a Japanese girl and had a child with her, and feigned illness as a pretext to ask Lu to return home, where she would then force him to take part in an arranged marriage she had agreed to several years before. The girl, Zhu An, had little in common with Lu, was illiterate, and had bound feet. Lu Xun married her, but they never had a romantic relationship. He never loved her, but took care of her material needs for the rest of his life. Several days after the ceremony Lu sailed back to Japan with his younger brother, Zuoren, and left behind his new wife.
After returning to Japan he took informal classes in literature and history, published several essays in student-run journals, and in 1907 he briefly took Russian lessons. He attempted to found a literary journal with his brother, "New Life", but before its first publication its other writers and its financial backers all abandoned the project, and it failed. In 1909 Lu published a translation of Eastern European fiction, "Tales from Abroad", but the book sold only 41 copies of the 1,500 copies that were printed. The publication failed for many reasons: it was sold only in Tokyo (which did not have a large Chinese population) and a single silk shop in Shanghai; Chinese readers may not have been interested in Eastern European culture; and, Lu wrote in Classical Chinese, which was very difficult for ordinary people to read.
Early career.
Lu intended to study in Germany in 1909, but did not have sufficient funds, and was forced to return home. Between 1909–1911 he held a number of brief teaching positions at local colleges and secondary schools that he felt were unsatisfying, partly to support his brother Zuoren's studies in Japan.
Lu spent these years in traditional Chinese literary pursuits: collecting old books, researching pre-modern Chinese fiction, reconstructing ancient tombstone inscriptions, and compiling the history of his native town, Shaoxing. He explained to an old friend that his activities were not "scholarship", but "a substitute for 'wine and women'". In his personal letters he expressed disappointment about his own failure, China's political situation, and his family's continuing impoverishment. In 1911 he returned to Japan to retrieve his brother, Zuoren, so that Zuoren could help with the family finances. Zuoren wanted to remain in Japan to study French, but Lu wrote that "French... does not fill stomachs." He encouraged another brother, Jianren, to become a botanist. He began to drink heavily, a habit he continued for the rest of his life. In 1911 he wrote his first short story, "Nostalgia", but he was so disappointed with it that he threw it away. Zuoren saved it, and had it successfully published two years later under his own name.
In February 1912, shortly after the Xinhai Revolution that ended the Qing dynasty and nominally founded the Republic of China, Lu gained a position at the national Ministry of Education. He was hired in Nanjing, but then moved with the ministry to Beijing, where he lived from 1912–1926. At first, his work consisted almost completely of copying books, but he was later appointed Section Head of the Social Education Division, and eventually to the position of Assistant Secretary. Two of his major accomplishments in office were the renovation and expansion of the Beijing Library, the establishment of the Natural History Museum, and the establishment of the Library of Popular Literature.
Together with Qian Daosun and Xu Shoushang he designed the Twelve Symbols national emblem in 1912.
Between 1912–1917 he was a member of an ineffectual censorship committee, informally studied Buddhist sutras, lectured on fine arts, wrote and self-published a book on the history of Shaoxing, and edited and self-published a collection of Tang and Song dynasty folk stories. He collected and self-published an authoritative book on the work of an ancient poet, Ji Kang, and wrote a "A Brief History of Chinese Fiction", a work which, because traditional scholars had not valued fiction, had little precedent in China. After Yuan Shikai declared himself the Emperor of China in 1915, Lu was briefly forced to participate in rituals honoring Confucius, which he ridiculed in his diaries.
In 1917, an old friend of Lu's, Qian Xuantong, invited Lu to write for a radical populist literary magazine that had recently been founded by Chen Duxiu, "New Youth". At first Lu was skeptical that his writing could serve any social purpose, and told Qian: "Imagine an iron house: without windows or doors, utterly indestructible, and full of sound sleepers – all about to suffocate to death. Let them die in their sleep, and they will feel nothing. Is it right to cry out, to rouse the light sleepers among them, causing them inconsolable agony before they die?" Qian replied that it was, because if the sleepers were awoken, "there was still hope – hope that the iron house may one day be destroyed". Shortly afterwards, in 1918 Lu wrote the first short story published in his name, "Diary of a Madman", for the magazine.
After the publication of "Diary of a Madman", the story was praised for its anti-traditionalism, its synthesis of Chinese and foreign conventions and ideas, and its skillful narration, and Lu himself was recognized as one of the leading writers of the New Culture Movement. Lu continued writing for the magazine, and produced his most famous stories for "New Youth" between 1917–1921. These stories were collected and re-published in "Nahan" (""Outcry"") in 1923.
In 1919, Lu moved his family from Shaoxing to a large compound in Beijing, where he lived with his mother, his two brothers, and their Japanese wives. This living arrangement lasted until 1923, when Lu had a falling out with his brother, Zuoren, after which Zuoren moved with his wife and mother to a separate house. Neither Lu nor Zuoren ever publicly explained the reason for their disagreement, but Zuoren's wife later accused Lu of making sexual advances towards her. Some writers have speculated that their relationship may have worsened as a result of issues related to money, that Lu walked in on Zuoren's wife bathing, or that Lu had an inappropriate "relationship" with Zuoren's wife in Japan that Zuoren later discovered. After the falling out with Zuoren, Lu became depressed.
In 1920, Lu began to lecture part-time at several colleges, including Peking University, Beijing Normal University, and Beijing Women’s College, where he taught traditional fiction and literary theory. His lecture notes were later collected and published as a "A Brief History of Chinese Fiction". He was able to work part-time because he only worked at the Education Ministry three days a week for three hours a day. In 1923 he lost his front teeth in a rickshaw accident, and in 1924 he developed the first symptoms of tuberculosis. In 1925 he founded a journal, "Wilderness", and established the "Weiming Society" in order to support young writers and encourage the translation of foreign literature into Chinese.
In 1925, Lu began what may have been his first meaningful romantic relationship, with one of his students at the Beijing Women's College, Xu Guangping. In March 1926 there was a mass student protest against the warlord Feng Yuxiang's collaboration with the Japanese. The protests degenerated into a massacre, in which two of Lu's students from Beijing Women's College were killed. Lu's public support for the protesters forced him to flee from the local authorities. Later in 1926, when the warlord troops of Zhang Zuolin and Wu Peifu took over Beijing, Lu left northern China and fled to Xiamen.
After arriving in Xiamen, later in 1926, Lu began a teaching position at Xiamen University, but he was disappointed by the petty disagreements and unfriendliness of the university's faculty. During the short time he lived in Xiamen, Lu wrote his last collection of fiction, "Old Tales Retold" (which was not published until several years later), and most of his autobiography, published as "Dawn Blossoms Plucked at Dusk". He also published a collection of prose poetry, "Wild Grass".
In January 1927, he and Xu moved to Guangzhou, where he was hired as the head of the Zhongshan University Chinese literature department. His first act in his position was to hire Xu as his "personal assistant", and to hire one of his old classmates from Japan, Xu Shoushang, as a lecturer. While in Guangzhou, he edited numerous poems and books for publication, and served as a guest lecturer at Whampoa Academy. He made contacts within the Kuomintang and the Chinese Communist Party through his students. After the Shanghai massacre in April 1927, he attempted to secure the release of several students through the university, but failed. His failure to save his students led him to resign from his position at the university, and he left for the foreign settlement of Shanghai in September 1927. By the time he left Guangzhou, he was one of the most famous intellectuals in China.
In 1927 Lu was considered for the Nobel Prize in Literature, for the short story "The True Story of Ah Q", despite a poor English translation and annotations that were nearly double the size of the text. Lu rejected the possibility of accepting the nomination. Later, he renounced writing fiction or poetry in response to China's deteriorating political situation and his own poor emotional state, and restricted himself to writing argumentative essays.
Late career.
In 1929, he visited his dying mother, and reported that she was pleased at the news of Guangping's pregnancy. Xu Guangping gave birth to a son, Haiying, on September 27, 1929. She was in labor with the baby for 27 hours. The child's name meant simply "Shanghai infant". His parents chose the name thinking that he could change it himself later, but he never did so. Haiying was Lu Xun's only child.
After moving to Shanghai, Lu rejected all regular teaching positions (though he sometimes gave guest lectures at different campuses), and for the first time was able to make a living solely as a professional writer, with a monthly income of roughly 500 yuan. He was also appointed by the a government as a "specially appointed writer" by the national Ministry of Higher Education, which brought him an additional 300 yuan/month. He began to study and identify with Marxist political theory, made contact with local Communist Party members, and became involved in literary disputes with other leftist writers in the city. In 1930 Lu became one of the co-founders of the League of Left-Wing Writers, but shortly after he moved to Shanghai other leftist writers accused him of being "an evil feudal remnant", the "best spokesman of the bourgeoisie", and "a counterrevolutionary split personality". The CCP may have secretly initiated these attacks, but later called them off. The League continued in various forms until 1936, when the constant disputes among its members led the CCP to dissolve it.
In January 1931, the Kuomintang passed new, stricter censorship laws, allowing for writers producing literature deemed "endangering the public" or "disturbing public order" to be imprisoned for life or executed. Later that month he went into hiding. In early February, less than a month later, the Kuomintang executed twenty-four local writers (including five that belonged to the League) that they had arrested under this law. After the execution of the "24 Longhua Martyrs" (in addition to other students, friends, and associates), Lu's political views became distinctly anti-Kuomintang. In 1933 Lu met Edgar Snow. Snow asked Lu if there were any Ah Q's left in China. Lu responded, "It's worse now. Now it's Ah Q's who are running the country."
Although he had renounced writing fiction years before, in 1934 he published his last collection of short stories, "Old Tales Retold". In 1935 he sent a telegram to Communist forces in Shaanxi congratulating them on the recent completion of their Long March. The Communist Party requested that he write a novel about the communist revolution set in rural China, but he declined, citing his lack of background and understanding of the subject.
Lu sent a telegram congratulating the CCP on their completion of the Long March in February 1936. He was a heavy smoker, which may have contributed to the deterioration of his health throughout the year. By 1936 he had developed chronic tuberculosis, and in March of that year he was stricken with bronchitic asthma and a fever. The treatment for this involved draining 300 grams of fluid in the lungs through a puncture. From June to August, he was again sick, and his weight dropped to only 83 pounds. He recovered somewhat, and wrote two essays in the fall reflecting on mortality. These included "Death", and "This Too Is Life". A month before his death, he wrote: "Hold the funeral quickly... do not stage any memorial services. Forget about me, and care about your own life – you're a fool if you don't." Regarding his son, he wrote: "On no account let him become a good-for-nothing writer or artist."
At 3:30 am on the morning of October 18, the author woke with great difficulty breathing. Dr. Sudo, his physician, was summoned, and Lu Xun took injections to relieve the pain. His wife was with him throughout that night, but Lu Xun was found without a pulse at 5:11 am the next morning, October 19. Lu's remains were interred in a mausoleum within Lu Xun Park in Shanghai. Mao Zedong later made the calligraphic inscription above his tomb. He was survived by his son, Zhou Haiying. He was posthumously made a member of the Communist Party for his contributions to the May Fourth Movement.
Legacy.
Shortly after Lu Xun's death, Mao Zedong called him "the saint of modern China," but used his legacy selectively to promote his own political goals. In 1942 he quoted Lu out of context to tell his audience to be "a willing ox" like Lu Xun was, but told writers and artists who believed in freedom of expression that, because Communist areas were already "free", they did not need to be like Lu Xun. After the People's Republic of China was established in 1949, Communist Party literary theorists portrayed his work as orthodox examples of communist literature, yet every one of Lu's close disciples from the 1930s was purged. Mao admitted that, had Lu survived until the 1950s, he would "either have gone silent or gone to prison".
Party leaders depicted him as "drawing the blueprint of the communist future" and Mao Zedong defined him as the "chief commander of China's Cultural Revolution," although Lu did not join the party. During the 1920s and 1930s Lu Xun and his contemporaries often met informally for freewheeling intellectual discussions, but after the founding of the People's Republic in 1949 the Party sought more control over intellectual life in China, and this type of intellectual independence was suppressed, often violently. Finally, Lu Xun's satirical and ironic writing style itself was discouraged, ridiculed, then as often as possible destroyed. Mao wrote that "the style of the essay should not simply be like Lu Xun's. a Communist society we can shout at the top of our voices and have no need for veiled and round-about expressions, which are hard for the people to understand". During the Cultural Revolution, the Communist Party both hailed Lu Xun as one of the fathers of communism in China, yet ironically suppressed the very intellectual culture and style of writing that he represented. Some of his essays and writings are now part of the primary school and middle school compulsory curriculum in China, but in 2007 some of his bleaker works were removed from school textbooks. Julia Lovell, who has translated Lu Xun's writing, speculated that "perhaps also it was an attempt to discourage the youth of today from Lu Xun's inconveniently fault-finding habits."
Lu completed volumes of translations, notably from Russian. He particularly admired Nikolai Gogol and made a translation of "Dead Souls". His own first story's title, "Diary of a Madman", was inspired by a work of Gogol of the same name. As a left-wing writer, Lu played an important role in the development of modern Chinese literature. His books were and remain highly influential and popular today, both in China and internationally. Lu Xun's works appear in high school textbooks in both China and Japan. He is known to Japanese by the name Rojin (ロジン in Katakana or in Kanji).
Because of his leftist political involvement and of the role his works played in the subsequent history of the People's Republic of China, Lu Xun's works were banned in Taiwan until the late 1980s. He was among the early supporters of the Esperanto movement in China.
Lu Xun has been described by Nobel laureate Kenzaburō Ōe as "The greatest writer Asia produced in the twentieth century." Lu Xun's importance to modern Chinese literature lies in the fact that he contributed significantly to nearly every modern literary medium during his lifetime. He wrote in a clear lucid style which was to influence many generations, in stories, prose poems and essays. Lu Xun's two short story collections, "Nahan" ("A Call to Arms" or "Outcry") and "Panghuang" ("Wandering"), are often taken to mark the beginning of modern Chinese literature, and are established classics. Lu Xun's translations were important in a time when Western literature was seldom read, and his literary criticisms remain acute and persuasively argued.
The work of Lu Xun has also received attention outside of China. In 1986, Fredric Jameson cited "A Madman's Diary" as the "supreme example" of the "national allegory" form that all Third World literature takes. Gloria Davies compares Lu Xun to Nietzsche, saying that both were "trapped in the construction of a modernity which is fundamentally problematic". According to Leonardo Vittorio Arena, Lu Xun cultivates an ambiguous standpoint towards Nietzsche, a mixture of attraction and repulsion, the latter because of Nietzsche's excesses in style and content.
Style and thought.
Lu Xun was a versatile writer. He wrote using both traditional Chinese conventions and 19th century European literary forms. His style has been described in equally broad terms, conveying both "sympathetic engagement" and "ironic detachment" at different moments. His essays are often very incisive in his societal commentary, and in his stories his mastery of the vernacular language and tone make some of his literary works (like "The True Story of Ah Q") hard to convey through translation. In them, he frequently treads a fine line between criticizing the follies of his characters and sympathizing with those very follies. Lu Xun was a master of irony and satire (as can been seen in "The True Story of Ah Q") and yet can write impressively direct with simple engagement ("My Old Home", "A Little Incident").
Lu Xun is typically regarded by Mao Zedong as the most influential Chinese writer who was associated with the May Fourth Movement. He produced harsh criticism of social problems in China, particularly in his analysis of the "Chinese national character". He was sometimes called a "champion of common humanity."
Lu Xun felt that the 1911 Xinhai Revolution had been a failure. In 1925 he opined, "I feel the so-called Republic of China has ceased to exist. I feel that, before the revolution, I was a slave, but shortly after the revolution, I have been cheated by slaves and have become their slave." He even recommended that his readers heed the critique of Chinese culture in "Chinese Characteristics", by the missionary writer Arthur Smith. His disillusionment with politics led him to conclude in 1927 that "revolutionary literature" alone could not bring about radical change. Rather, "revolutionary men" needed to lead a revolution using force. In the end, he experienced profound disappointment with the new Nationalist government, which he viewed as ineffective and even harmful to China.
Works.
Translations into English.
Lu Xun's works became known to English readers as early as 1926 with the publication in Shanghai of The True Story of Ah Q, translated by George Kin Leung, and more widely beginning in 1936 with an anthology edited by Edgar Snow and Nym Wales "Living China, Modern Chinese Short Stories," in which Part One included seven of Lu Xun's stories and a short biography based on Snow's talks with Lu Xun. However, there was not a complete translation of the fiction until the four volume set of his writings, which included "Selected Stories of Lu Hsun" translated by Yang Hsien-yi and Gladys Yang. Another full selection was William A. Lyell. "Diary of a Madman and Other Stories." (Honolulu: University of Hawaii Press, 1990). In 2009, Penguin Classics published a complete translation by Julia Lovell of his fiction, "The Real Story of Ah-Q and Other Tales of China: The Complete Fiction of Lu Xun" which the scholar Jeffrey Wasserstrom said "could be considered the most significant Penguin Classic ever published."
"The Lyrical Lu Xun: a Study of his Classical-style Verse"—a book by Jon Eugene von Kowallis (Honolulu: University of Hawaii Press, 1996) — includes a complete introduction to Lu Xun's poetry in the classical style, with Chinese characters, literal and verse translations, and a biographical introduction which summarizes his life in relation to his poetry.
"Capturing Chinese: Short Stories from Lu Xun's Nahan", edited by Kevin Nadolny, includes short summaries to Lu Xun's stories, the Chinese text in simplified characters, pinyin, and definitions for difficult vocabulary.

</doc>
<doc id="65862" url="https://en.wikipedia.org/wiki?curid=65862" title="Angina pectoris">
Angina pectoris

Angina pectoris, commonly known as angina, is the sensation of chest pain, pressure, or squeezing, often due to ischemia of the heart muscle from obstruction or spasm of the coronary arteries. While angina pectoris can derive from anemia, abnormal heart rhythms and heart failure, its main cause is coronary artery disease, an atherosclerotic process affecting the arteries feeding the heart. The term derives from the Latin "angere" ("to strangle") and "pectus" ("chest"), and can therefore be translated as "a strangling feeling in the chest".
There is a weak relationship between severity of pain and degree of oxygen deprivation in the heart muscle (i.e., there can be severe pain with little or no risk of a myocardial infarction (heart attack) and a heart attack can occur without pain). In some cases, angina can be quite severe, and in the early 20th century this was known to be a signal of impending death. However, given current medical therapies, the outlook has improved substantially. People with an average age of 62 years, who have moderate to severe degrees of angina (grading by classes II, III and IV) have a 5-year mortality rate of approximately 8%.
Worsening ("crescendo") angina attacks, sudden-onset angina at rest, and angina lasting more than 15 minutes are symptoms of "unstable angina" (usually grouped with similar conditions as the acute coronary syndrome). As these may precede a heart attack, they require urgent medical attention and are, in general, treated in similar fashion to myocardial infarction.
Classification.
Stable angina.
Also known as "effort angina", this refers to the classic type of angina related to myocardial ischemia. A typical presentation of stable angina is that of chest discomfort and associated symptoms precipitated by some activity (running, walking, etc.) with minimal or non-existent symptoms at rest or after administration of sublingual nitroglycerin. Symptoms typically abate several minutes after activity and recur when activity resumes. In this way, stable angina may be thought of as being similar to intermittent claudication symptoms. Other recognized precipitants of stable angina include cold weather, heavy meals, and emotional stress.
Unstable angina.
Unstable angina (UA) (also "crescendo angina"; this is a form of acute coronary syndrome) is defined as angina pectoris that changes or worsens.
It has at least one of these three features:
UA may occur unpredictably at rest, which may be a serious indicator of an impending heart attack. What differentiates stable angina from unstable angina (other than symptoms) is the pathophysiology of the atherosclerosis. The pathophysiology of unstable angina is the reduction of coronary flow due to transient platelet aggregation on apparently normal endothelium, coronary artery spasms, or coronary thrombosis. The process starts with atherosclerosis, progresses through inflammation to yield an active unstable plaque, which undergoes thrombosis and results in acute myocardial ischemia, which, if not reversed, results in cell necrosis (infarction). Studies show that 64% of all unstable anginas occur between 10 PM and 8 AM when patients are at rest.
In stable angina, the developing atheroma is protected with a fibrous cap. This cap may rupture in unstable angina, allowing blood clots to precipitate and further decrease the area of the coronary vessel's lumen. This explains why, in many cases, unstable angina develops independently of activity.
Cardiac syndrome X.
Cardiac syndrome X, sometimes known as microvascular angina is characterized by angina-like chest pain, in the context of normal epicardial coronary arteries (the largest vessels on the surface of the heart, prior to significant branching) on angiography. The original definition of cardiac syndrome X also mandated that ischemic changes on exercise (despite normal coronary arteries) were displayed, as shown on cardiac stress tests. The primary cause of cardiac syndrome X is unknown, but factors which appear to be involved are endothelial dysfunction and reduced flow (perhaps due to spasm) in the tiny "resistance" blood vessels of the heart. Since microvascular angina is not characterized by major arterial blockages, it is harder to recognize and diagnose. Microvascular angina was previously felt to be a rather benign condition, but more recent data has changed this attitude. Studies including the Women's Ischemia Syndrome Evaluation (WISE) suggest that microvascular angina is part of the pathophysiology of ischemic heart disease, perhaps explaining the higher rates of angina in women than in men, as well as their predilection towards ischemia and acute coronary syndromes in the absence of obstructive coronary artery disease.
Signs and symptoms.
Angina pectoris can be quite painful, but many patients with angina complain of chest discomfort rather than actual pain: the discomfort is usually described as a pressure, heaviness, tightness, squeezing, burning, or choking sensation. Apart from chest discomfort, anginal pains may also be experienced in the epigastrium (upper central abdomen), back, neck area, jaw, or shoulders. This is explained by the concept of referred pain, and is due to the fact that the spinal level that receives visceral sensation from the heart simultaneously receives cutaneous sensation from parts of the skin specified by that spinal nerve's dermatome, without an ability to discriminate the two. Typical locations for referred pain are arms (often inner left arm), shoulders, and neck into the jaw. Angina is typically precipitated by exertion or emotional stress. It is exacerbated by having a full stomach and by cold temperatures. Pain may be accompanied by breathlessness, sweating, and nausea in some cases. In this case, the pulse rate and the blood pressure increases. Chest pain lasting only a few seconds is normally not angina (such as precordial catch syndrome).
Myocardial ischemia comes about when the myocardia (the heart muscles) receive insufficient blood and oxygen to function normally either because of increased oxygen demand by the myocardia or because of decreased supply to the myocardia. This inadequate perfusion of blood and the resulting reduced delivery of oxygen and nutrients are directly correlated to blocked or narrowed blood vessels.
Some experience "autonomic symptoms" (related to increased activity of the autonomic nervous system) such as nausea, vomiting, and pallor.
Major risk factors for angina include cigarette smoking, diabetes, high cholesterol, high blood pressure, sedentary lifestyle, and family history of premature heart disease.
A variant form of angina (Prinzmetal's angina) occurs in patients with normal coronary arteries or insignificant atherosclerosis. It is thought to be caused by spasms of the artery. It occurs more in younger women.
Cause.
Major risk factors.
Routine counselling of adults to advise them to improve their diet and increase their physical activity has not been found to significantly alter behaviour, and thus is not recommended.
One study found that smokers with coronary artery disease had a significantly increased level of sympathetic nerve activity when compared to those without. This is in addition to increases in blood pressure, heart rate, and peripheral vascular resistance associated with nicotine, which may lead to recurrent angina attacks. In addition, the Centers for Disease Control and Prevention (CDC) reports that the risk of CHD (Coronary heart disease), stroke, and PVD (Peripheral vascular disease) is reduced within 1–2 years of smoking cessation. In another study, it was found that, after one year, the prevalence of angina in smoking men under 60 after an initial attack was 40% less in those having quit smoking compared to those that continued. Studies have found that there are short-term and long-term benefits to smoking cessation.
Other cardiac problems.
Myocardial ischemia can result from:
Atherosclerosis is the most common cause of stenosis (narrowing of the blood vessels) of the heart's arteries and, hence, angina pectoris. Some people with chest pain have normal or minimal narrowing of heart arteries; in these patients, vasospasm is a more likely cause for the pain, sometimes in the context of Prinzmetal's angina and syndrome X.
Myocardial ischemia also can be the result of factors affecting blood composition, such as reduced oxygen-carrying capacity of blood, as seen with severe anemia (low number of red blood cells), or long-term smoking.
Pathophysiology.
Angina results when there is an imbalance between the heart's oxygen demand and supply. This imbalance can result from an increase in demand (e.g., during exercise) without a proportional increase in supply (e.g., due to obstruction or atherosclerosis of the coronary arteries).
However, the pathophysiology of angina in females varies significantly as compared to males. Non-obstructive coronary disease is more common in females.
Diagnosis.
Angina should be suspected in people presenting with tight, dull, or heavy chest discomfort that is:
Some people present with atypical symptoms, including breathlessness, nausea, or epigastric discomfort or burping.
These atypical symptoms are particularly likely in older people, women, and those with diabetes.
Anginal pain is not usually sharp or stabbing or influenced by respiration. Antacids and simple analgesia do not usually relieve the pain. If chest discomfort (of whatever site) is precipitated by exertion, relieved by rest, and relieved by glyceryl trinitrate, the likelihood of angina is increased.
In angina patients momentarily not feeling any chest pain, an electrocardiogram (ECG) is typically normal, unless there have been other cardiac problems in the past. During periods of pain, depression, or elevation of the ST segment may be observed. To elicit these changes, an exercise ECG test ("treadmill test") may be performed, during which the patient exercises to his/her maximum ability before fatigue, breathlessness, or pain intervenes; if characteristic ECG changes are documented (typically more than 1 mm of flat or downsloping ST depression), the test is considered diagnostic for angina. Even constant monitoring of the blood pressure and the pulse rate can lead us to some conclusion regarding the angina. The exercise test is also useful in looking for other markers of myocardial ischaemia: blood pressure response (or lack thereof, in particular a drop in systolic pressure), dysrhythmia and chronotropic response. Other alternatives to a standard exercise test include a thallium scintigram or sestamibi scintigram (in patients unable to exercise enough for the purposes of the treadmill tests, e.g., due to asthma or arthritis or in whom the ECG is too abnormal at rest) or Stress Echocardiography.
In patients in whom such noninvasive testing is diagnostic, a coronary angiogram is typically performed to identify the nature of the coronary lesion, and whether this would be a candidate for angioplasty, coronary artery bypass graft (CABG), treatment only with medication, or other treatments. There has been research that concludes that a frequency is attained when there is increase in the blood pressure and the pulse rate. This frequency varies normally but the range is 45–50 kHz for the cardiac arrest or for the heart failure. In patients in hospital with unstable angina (or the newer term of "high-risk acute coronary syndromes"), those with resting ischaemic ECG changes or those with raised cardiac enzymes such as troponin may undergo coronary angiography directly.
Treatment.
The most specific medicine to treat angina is nitroglycerin. It is a potent vasodilator that decreases myocardial oxygen demand by decreasing the heart's workload. Beta blockers and calcium channel blockers act to decrease the heart's workload, and thus its requirement for oxygen. Nitroglycerin should not be given if certain inhibitors such as Sildenafil (Viagra), Tadalafil (Cialis), or Vardenafil (Levitra) have been taken within the previous 12 hours as the combination of the two could cause a serious drop in blood pressure. Treatments for angina are balloon angioplasty, in which the balloon is inserted at the end of a catheter and inflated to widen the arterial lumen. Stents to maintain the arterial widening are often used at the same time. Coronary bypass surgery involves bypassing constricted arteries with venous grafts. This is much more invasive than angioplasty.
The main goals of treatment in angina pectoris are relief of symptoms, slowing progression of the disease, and reduction of future events, especially heart attacks and death. Beta blockers (e.g., carvedilol, propranolol, atenolol) have a large body of evidence in morbidity and mortality benefits (fewer symptoms, less disability and longer life) and short-acting nitroglycerin medications have been used since 1879 for symptomatic relief of angina. Calcium channel blockers (such as nifedipine (Adalat) and amlodipine), isosorbide mononitrate and nicorandil are vasodilators commonly used in chronic stable angina. A new therapeutic class, called If inhibitor, has recently been made available: Ivabradine provides pure heart rate reduction leading to major anti-ischemic and antianginal efficacy. ACE inhibitors are also vasodilators with both symptomatic and prognostic benefit. Statins are the most frequently used lipid/cholesterol modifiers, which probably also stabilize existing atheromatous plaque. Low-dose aspirin decreases the risk of heart attack in patients with chronic stable angina, and was part of standard treatment. However, in patients without established cardiovascular disease, the increase in haemorrhagic stroke and gastrointestinal bleeding offsets any benefits and it is no longer advised unless the risk of myocardial infarction is very high.
Exercise is also a very good long-term treatment for the angina (but only particular regimens - gentle and sustained exercise rather than intense short bursts), probably working by complex mechanisms such as improving blood pressure and promoting coronary artery collateralisation.
Identifying and treating risk factors for further coronary heart disease is a priority in patients with angina. This means testing for elevated cholesterol and other fats in the blood, diabetes and hypertension (high blood pressure), and encouraging smoking cessation and weight optimisation.
The calcium channel blocker nifedipine prolongs cardiovascular event- and procedure-free survival in patients with coronary artery disease. New overt heart failures were reduced by 29% compared to placebo; however, the mortality rate difference between the two groups was statistically insignificant.
Microvascular angina in women.
Aggressive risk factor modification is required for effective treatment of microvascular angina where exercise plays a major role. Several other treatment strategies including b-blockers, angiotensin-converting enzyme inhibitors, ranolazine, l-arginine, statin drugs and potentially estrogen replacement therapy have been shown to relieve anginal symptoms as well as improve vascular function. Nitrates may be effective for symptom relief. Further studies are required to determine whether specific treatments are associated with improved survival as well as decreased symptoms.
Suspected angina.
Hospital admission for people with the following symptoms is recommended, as they may have unstable angina: pain at rest (which may occur at night), pain on minimal exertion, angina that seems to be progressing rapidly despite increasing medical treatment. All people with suspected angina should be urgently referred to a chest pain evaluation service, for confirmation of the diagnosis and assessment of the severity of coronary heart disease.
Epidemiology.
As of 2010, angina due to ischemic heart disease affects approximately 112 million people (1.6% of the population) being slightly more common in men than women (1.7% to 1.5%).
In the United States, 10.2 million are estimated to experience angina with approximately 500,000 new cases occurring each year. Angina is more often the presenting symptom of coronary artery disease in women than in men. The prevalence of angina rises with increasing age, with a mean age of onset of 62.3 years. After five years post-onset, 4.8% of individuals with angina subsequently died from coronary heart disease. Men with angina were found to have an increased risk of subsequent acute myocardial infarction and coronary heart disease related death than women. Similar figures apply in the remainder of the Western world. All forms of coronary heart disease are much less-common in the Third World, as its risk factors are much more common in Western and Westernized countries; it could, therefore, be termed a disease of affluence. The adoption of a rich, Westernized diet and subsequent increase of smoking, obesity, and other risk factors has led to an increase in angina and related diseases in countries such as China.
History.
It embodies all the essential components of present day definition, i.e. site, nature, aggravating and relieving factors and referral. According to him angina is chest pain which is precordial, temporary, exertional, emotional, burning like and relieved by rest. He also linked this kind of pain to obesity (medoroga).

</doc>
<doc id="65865" url="https://en.wikipedia.org/wiki?curid=65865" title="Poland, Ohio">
Poland, Ohio

Poland is a village located about southeast of Youngstown in Mahoning County, Ohio, United States. The population was 2,555 at the 2010 census. It is part of the "Youngstown-Warren-Boardman, OH-PA Metropolitan Statistical Area".
History.
In 1796, Poland Township was the first charted township in the Connecticut Western Reserve, being the southeasternmost portion (Township 1, Range 1). The township was founded by Jonathan Fowler, who fell in love with Yellow Creek which flows through Poland. Fowler owned an inn near the river which still stands as the oldest building in Poland. The historical buildings of Poland can be identified by a sign in the shape of Ohio located by the front door of the building.
The Village of Poland was founded in 1802. Poland Seminary was originally a private secondary school, Poland Academy, and then a liberal arts college founded in1849. Its main building has been incorporated into Poland Middle School on College Street. Its dormitory is incorporated into the Poland Public Library on Main Street. Former distinguished faculty include the journalist Ida Tarbell; graduates include William McKinley, President of the United States. 
The former medical school and Ohio Law College is now a private residence also on College Street. 
Poland is the home to the Poland Seminary High School Bulldogs. The girls softball team won the OHSAA Division II state championship in 2011.
Schools and education.
The Poland Local School District currently has three elementary schools: North, Dobbins, and Union. Poland also has an elementary school for 5th and 6th graders, Poland McKinley, named after the former U.S. President William McKinley. Poland Middle School is home for the 7th and 8th grade classes and Poland Seminary High School, often referred to as PSHS, houses the 9th through 12th grades.
A Catholic school located in Poland, Holy Family, serves children in pre-kindergarten through the 8th grade.
Poland has been designated 'Top School' in Ohio along with Canfield, Struthers, Niles, Springfield, Austintown, Sebring, Youngstown, South Range, and Beaver Local.
Geography.
According to the United States Census Bureau, the village has a total area of , of which is land and is water.
Demographics.
2010 census.
As of the census of 2010, there were 2,555 people, 1,066 households, and 765 families residing in the village. The population density was . There were 1,135 housing units at an average density of . The racial makeup of the village was 98.5% White, 0.2% African American, 0.4% Asian, and 0.9% from two or more races. Hispanic or Latino of any race were 1.1% of the population.
There were 1,066 households of which 27.8% had children under the age of 18 living with them, 59.8% were married couples living together, 8.4% had a female householder with no husband present, 3.5% had a male householder with no wife present, and 28.2% were non-families. 25.7% of all households were made up of individuals and 13.6% had someone living alone who was 65 years of age or older. The average household size was 2.40 and the average family size was 2.88.
The median age in the village was 46.3 years. 21.4% of residents were under the age of 18; 5.9% were between the ages of 18 and 24; 21.1% were from 25 to 44; 30.4% were from 45 to 64; and 21.3% were 65 years of age or older. The gender makeup of the village was 48.2% male and 51.8% female.
2000 census.
As of the census of 2000, there were 2,990 people, 1,086 households, and 822 families residing in the village. The population density was 2,303.2 people per square mile (892.4/km²). There were 1,123 housing units at an average density of 902.5 per square mile (349.7/km²). The racial makeup of the village was 99.16% White, 0.24% African American, 0.10% Asian, 0.17% from other races, and 0.31% from two or more races. Hispanic or Latino of any race were 0.98% of the population.
There were 1,086 households out of which 34.3% had children under the age of 18 living with them, 63.4% were married couples living together, 9.3% had a female householder with no husband present, and 24.3% were non-families. 22.5% of all households were made up of individuals and 11.1% had someone living alone who was 65 years of age or older. The average household size was 2.57 and the average family size was 3.01.
In the village the population was spread out with 24.6% under the age of 18, 5.4% from 18 to 24, 25.6% from 25 to 44, 24.8% from 45 to 64, and 19.6% who were 65 years of age or older. The median age was 42 years. For every 100 females there were 92.2 males. For every 100 females age 18 and over, there were 87.3 males.
The median income for a household in the village was $47,273, and the median income for a family was $55,486. Males had a median income of $42,857 versus $23,603 for females. The per capita income for the village was $23,924. About 4.5% of families and 6.1% of the population were below the poverty line, including 8.8% of those under age 18 and 13.2% of those age 65 or over.

</doc>
<doc id="65866" url="https://en.wikipedia.org/wiki?curid=65866" title="Operation TIPS">
Operation TIPS

Operation TIPS, where the last part is an acronym for the "Terrorism Information and Prevention System", was a domestic intelligence-gathering program designed by President George W. Bush to have United States citizens report suspicious activity. The program's website implied that US workers who had access to private citizens' homes, such as cable installers and telephone repair workers, would be reporting on what was in people's homes if it were deemed "suspicious."
It came under intense scrutiny in July 2002 when the "Washington Post" alleged in an editorial that the program was vaguely defined, and investigative political journalist Ritt Goldstein observed in Australia's "Sydney Morning Herald" that TIPS would provide America with a higher percentage of 'citizen spies' than the former East Germany had under the notorious Stasi secret police. Goldstein later observed that he broke news of Operation TIPS on March 10 in Spain's second largest daily, "El Mundo", but that he struggled until July before finding a major English language paper which would print the story.
In the days immediately following Goldstein's revelation, publications such as the libertarian magazine "Reason", and then the "Boston Globe", emphasized the Stasi analogy, widely highlighting Operation TIPS' shortcomings. TIPS was subsequently cancelled after concerns over civil liberties violations.
Overview.
The program's website implied that US workers who had access to private citizens' homes, such as cable installers and telephone repair workers, would be reporting on what was in people's homes if it were deemed "suspicious." The initial start of the program was to be August 2002 and would have included one million workers in ten US cities and then to be expanded.
Operation TIPS was accused of doing an "end run" around the United States Constitution, and the original wording of the website was subsequently changed. President Bush's then-Attorney General, John Ashcroft denied that private residences would be surveilled by private citizens operating as government spies. Mr. Ashcroft nonetheless defended the program, equivocating on whether the reports by citizens on fellow citizens would be maintained in government databases. While saying that the information would not be in a central database as part of Operation TIPS, he maintained that the information would still be kept in databases by various law enforcement agencies.
The databases were an explicit concern of various civil liberties groups (on both the left and the right) who felt that such databases could include false information about citizens with no way for those citizens to know that such information was compiled about them, nor any way for them to correct the information, nor any way for them to confront their accusers.
The United States Postal Service, after at first seeming supportive of the program, later resisted its personnel being included in this program, reasoning that if mail carriers became perceived as law enforcement personnel that they would be placed in danger at a level for which they could not reasonably be expected to be prepared, and that the downside of the program hence vastly outweighed any good that it could accomplish. The National Association of Letter Carriers, a postal labor union, was especially outspoken in its opposition.
Attempted passage.
Both Congressional Representative Dick Armey (Republican, Texas) and Senator Patrick Leahy (Democrat, Vermont) raised concerns. Senator Leahy said that it was similar to J. Edgar Hoover's misuse of the FBI during the 1960s when Hoover hired citizens to spy on neighbors who were political protesters. Rep. Armey included legislation in the House's Homeland Security Bill that explicitly prohibited the creation of Operation TIPS; but Joe Lieberman blocked the program's removal from the Senate version of the bill. The Senate, however, essentially passed the House version that eliminated the program.
Operation TIPS was officially cancelled when the Homeland Security Act was passed by Congress in November 2002. Section 880 explicitly prohibited the program.
Terrorism Liaison Officers.
On June 30, 2008, the "Denver Post" reported that 181 individuals, including police officers, paramedics, firefighters, utility workers, and railroad employees had been trained as Terrorism Liaison Officers to report suspicious information which could be signs of terrorist activity. The article also stated that TLOs were already active in six other states and the District of Columbia.

</doc>
<doc id="65867" url="https://en.wikipedia.org/wiki?curid=65867" title="Eileen Chang">
Eileen Chang

Eileen Chang () (September 30, 1920 – September 8, 1995) was one of the most influential modern Chinese writers.
Chang is noted for her fiction writings that deal with the tensions between men and women in love, and are considered by some scholars to be among the best Chinese literature of the period. Chang's portrayal of life in 1940s Shanghai and Japanese-occupied Hong Kong is remarkable in its focus on everyday life and the absence of the political subtext which characterised many other writers of the period. The Taiwanese author Yuan Qiongqiong drew inspiration from Chang. The poet and University of Southern California professor Dominic Cheung commented "had it not been for the political division between the Nationalist and Communist Chinese, she would have almost certainly won a Nobel Prize".
Chang's enormous popularity and famed image were in distinct contrast to her personal life, which was marred by disappointment, tragedy, increasing reclusiveness, and ultimately her sudden death from cardiovascular disease at age 74.
Life and career.
Childhood and youth.
Chang was born Zhang Ying () in Shanghai, China. She was the first child of Zhang Zhiyi () (1896–1953) and Huang Suqiong () (1893–1957). Chang's maternal great-grandfather, Huang Yisheng () (1818–1894), was a prominent naval commander. Chang's paternal grandfather, Zhang Peilun () (1848–1903), was son-in-law to Li Hongzhang (李鴻章) (1823–1901), who became an influential court official during the Qing Dynasty and married her paternal grandmother, Li Juou () (1866–1916). Her childhood had also been shared with paternal aunt Zhang Maoyuan () (1898–1991).
In 1922, when Chang was two years old, the family relocated to Tianjin, but in 1923, her mother left for the United Kingdom after her father took in a concubine and later became addicted to opium. Their marriage had been arranged, and despite having bound feet, Chang's mother managed to ski in the Swiss Alps. She returned in 1927, as her husband had promised to end the turmoil with drug usage and his personal affairs, and the family settled back to Shanghai in 1928. Her parents divorced in 1930. Chang and her younger brother Zhang Zijing () (1921–1997) were raised by their father.
Upon graduation from high school, Chang had a fight with her stepmother and father. Eventually, she contracted dysentery. Instead of receiving treatment, her father beat her and forced her to stay in her bedroom for six months. Chang ran away to live with her mother shortly after her 18th birthday, where they remained in a new apartment for nearly two years, until she began to attend university and briefly lived in Hong Kong.
Education.
Chang started school at age 4. Besides her native Chinese, Chang studied and professed a high ability in English. Although she says her family was not religious, she graduated from an all-female Christian high school, St. Mary's Hall, Shanghai in 1937.
In 1939, Chang was accepted to the University of London on a full scholarship, but because of the ongoing Second Sino-Japanese War, she was not able to attend. Instead, she studied English Literature at the University of Hong Kong, where she met her lifelong friend, Fatima Mohideen () (died 1995). When Chang was one semester short of earning her degree in December 1941, Hong Kong fell to the Empire of Japan. Chang made the decision to return to China. Her original plan was to finish her bachelor's degree at Saint John's University, but she chose to drop out after several weeks due to financial issues.
Early work.
At the age of 10, Chang's mother renamed her Ailing, a transliteration of Eileen, in preparation for her entrance into an English school. While in high school, Chang read "Dream of the Red Chamber", one of the Four Great Classical Novels of Chinese literature, which would influence her work throughout her career. Chang already displayed great literary talent and her writings were published in the school magazine. The following year, she wrote her debut short novel at the age of 12.
In 1943, Chang was introduced to the prominent editor Zhou Shoujuan (), and gave him a few pieces of her writing. With Zhou's backing, Chang soon became the most popular new writer in Shanghai. Within the next two years, she wrote some of her most acclaimed works, including "Qing Cheng Zhi Lian" () and "Jin Suo Ji" (). Her literary maturity was said to be beyond her age.
In the early days of her career, Chang was famously associated with this comment:
Other activities.
Chang migrated back to Hong Kong in 1952, where she worked as a translator for the United States Information Service for three years. While in Hong Kong, she wrote "The Rice Sprout Song", which was her first novel written entirely in English. She then left for the United States in 1955, never to return to mainland China again. Chang became a US citizen in 1960 and briefly headed to Taiwan for more opportunities, returning to the US in 1962.
In 1963, Chang finished her English semi-biographical novels, "The Fall of the Pagoda" and "The Book of Change". Both were believed to be her attempts to offer an alternative writing style to mainstream America; she did not succeed. The full-length novels were not published until 2010, 15 years after her death. In 1967, Chang held a short-term job at Radcliffe College and would later transfer to UC Berkeley in 1969 before leaving the university in 1972, when she relocated to Los Angeles. In 1975, she completed the English translation of "Shanghai Flowers", a celebrated Qing novel written in Wu Chinese by Han Bangqing (). Among her papers retrieved from the University of Southern California, the manuscript for the translated English version was found after her death and published.
She has been listed as one of China's four women geniuses together with Lü Bicheng, Xiao Hong and Shi Pingmei.
Marriages.
Chang met her first husband Hu Lancheng in 1943, when she was 23 and he was 37. They were married the following year in a private ceremony. Fatima Mohideen was the sole attendee. In the few months that he courted Chang, Hu was still married to his third wife. Despite Hu being labelled a traitor for collaborating with the Japanese during the ongoing World War II, Chang continued to remain loyal to Hu. Shortly thereafter, Hu chose to move to Wuhan to work for a newspaper. While staying at a local hospital, he seduced a 17-year-old nurse, Zhou Xunde (), who soon moved in with him. When Japan was defeated in 1945, Hu used another identity and hid in the neighboring Wenzhou, where he married Fan Xiumei (). Chang and Hu divorced in 1947.
While in MacDowell Colony, New Hampshire, Chang met and became involved with the American screenwriter Ferdinand Reyher, a native of Philadelphia, Pennsylvania. During the time they were briefly apart in New York (Chang in New York City, Reyher in Saratoga), Chang wrote to Reyher that she was pregnant with his child. Reyher wrote back to propose. Although Chang did not receive the letter, she telephoned the following morning to inform Reyher she was arriving in Saratoga. Reyher had a chance to propose to her in person, but insisted that he did not want the child. Chang had an abortion shortly after. On August 14, 1956, the couple married in New York City. After the wedding, the couple moved back to New Hampshire. After suffering a series of strokes, Reyher eventually became paralyzed before his death on October 8, 1967.
Death.
On September 8, 1995, Chang was found dead in her apartment on Rochester Avenue in Westwood, Los Angeles, by her landlord. That she was found days after her death testifies to her seclusion. Her death certificate states that she died from cardiovascular disease. According to Chang's will, she was cremated without any memorial services and her ashes were released into the Pacific Ocean.
She willed all her possessions to Stephen Soong () and his wife Mae Fong Soong () in Hong Kong, but they later died. Their daughter Elaine and son Roland inherited the estate of Chang's works. Roland, who writes the influential EastSouthWestNorth blog in Hong Kong, has spoken about her works.
Chang's brother, Zijing, died in 1997. Neither he nor his sister had any children.
Films.
Chang wrote several film scripts. Some of her works have been filmed and shown on the silver screen as well.
The following are the scripts that Eileen Chang wrote as a screenwriter:
The following are films adapted from Eileen Chang's novels:

</doc>
<doc id="65870" url="https://en.wikipedia.org/wiki?curid=65870" title="Qian Zhongshu">
Qian Zhongshu

Qian Zhongshu (November 21, 1910 – December 19, 1998) was a Chinese literary scholar and writer, known for his wit and erudition.
He is best known for his satirical novel "Fortress Besieged". His works of non-fiction are characterised by their large amount of quotations in both Chinese and Western languages (including English, French, German, Italian, Spanish, and Latin). He also played an important role in digitizing Chinese classics late in his life.
Life.
Most of what is known about Qian's early life relies on an essay written by his wife Yang Jiang. Born in Wuxi, Qian Zhongshu was the son of Qian Jibo (T: , S: ), a conservative Confucian scholar. By family tradition, Qian Zhongzhu grew up under the care of his eldest uncle, who did not have a son. Qian was initially named Yangxian ( "respect the ancients"), with the courtesy name Zheliang ( "sagacious and upright"). However, when he was one year old, according to a tradition practised in many parts of China, he was given a few objects laid out in front of him for his "grabbing". He grabbed a book. His uncle then renamed him Zhongshu, literally "being fond of books", and Yangxian became his intimate name. Qian was a talkative child. His father later changed his courtesy name to Mocun (), literally "to keep silent", in the hope that he would talk less.
Both Qian's name and courtesy name predicted his future life. While he remained talkative when talking about literature with friends, he kept silent most time on politics and social activities. Qian was indeed very fond of books. When he was young, his uncle often brought him along to tea houses during the day. There Qian was left alone to read storybooks on folklore and historical events, which he would repeat to his cousins upon returning home.
When Qian was 10, his uncle died. He continued living with his widowed aunt, even though their living conditions worsened drastically as her family's fortunes dwindled. Under the severe teaching of his father, Qian mastered classical Chinese. At the age of 14, Qian left home to attend an English-speaking missionary school in Suzhou, where he manifested his talent in language.
Despite failing in mathematics, Qian was accepted into the Department of Foreign Languages of Tsinghua University in 1929 because of his excellent performance in Chinese and English languages. At Tsinghua, Qian earned the admiration of many prominent scholars, but rarely socialized and was considered arrogant by his peers; one of his few friends was the budding Sinologist and comparatist Achilles Fang. Qian also frequently cut classes, though he more than made up for this in Tsinghua's large library, which he boasted of having "read through." It was probably in his college days that Qian began his lifelong habit of collecting quotations and taking reading notes. At Tsinghua, Qian met his wife, Yang Jiang, who was to become a successful playwright and translator; they married in 1935. For the biographical facts of Qian's following years, the two memoirs by his wife can be consulted. Yang Jiang wrote, "Zhongshu's 'foolishness' could not be contained in books, but just had to gush forth'".
In that same year, Qian received government sponsorship to further his studies abroad. Together with his wife, Qian headed for the University of Oxford in Britain. After spending two years at Exeter College, Oxford, he received a "Baccalaureus Litterarum" (Bachelor of Literature). Shortly after his daughter Qian Yuan (T: , S: ) was born, he studied for one more year in the University of Paris in France, before returning to China in 1938, in which year he was, as an exception, appointed a professor at Tsinghua University at the age of 28.
Owing to the unstable situation during the Second Sino-Japanese War, Qian did not hold any long-term jobs until the People's Republic of China was founded in 1949. However, he wrote extensively during the decade.
In 1949, Qian was ranked on the list of National First-class Professors (T: , S: ) and commenced his academic work in his Alma Mater. Four years later, an administrative adjustment saw Tsinghua changed into a science and technology-based institution, with its Arts departments merged into Peking University (PKU). Qian was relieved of teaching duties and worked entirely in the Institute of Literary Studies (T: , S: ) under PKU. He also worked in an agency in charge of the translation of Mao Zedong's works for a time.
During the Cultural Revolution, like many other prominent intellectuals of the time, Qian suffered persecution. Appointed to be a janitor, he was robbed of his favorite pastime, reading. Having no access to books, he had to read his reading notes. He began to form the plan to write "Guan Zhui Bian" (T: , S: ) during this period. Qian and his wife and daughter survived the hardships of Cultural Revolution, but his son-in-law, a history teacher, was driven to suicide.
After the Cultural Revolution, Qian returned to research. From 1978 to 1980, he visited several universities in Italy, the United States and Japan, impressing his audience with his wit and erudition. In 1982, he was instated as the deputy director of the Chinese Academy of Social Sciences. He then began working on "Guan Zhui Bian", which occupied the next decade of his life.
While "Guan Zhui Bian" established his fame in the academic field, his novel "Fortress Besieged" introduced him to the public. "Fortress Besieged" was reprinted in 1980, and became a best-seller. Many illegal reproductions and "continuations" followed. Qian's fame rose to its height when the novel was adapted into a TV serial in 1990.
Qian returned to research, but escaped from social activities. Most of his late life was confined to his reading room. He consciously kept a distance from the mass media and political figures. Readers kept visiting the secluded scholar, and the anecdote goes that Qian asked an elderly British lady, who loved the novel and phoned the author, "Is it necessary for one to know the hen if one loves the eggs it lays?"
Qian entered a hospital in 1994, and never came out. His daughter also became ill soon after, and died of cancer in 1997. On December 19, 1998, he died in Beijing.
Works.
Qian lived in Shanghai from 1941 to 1945, which was then under Japanese occupation. Many of his works were written or published during this chaotic period of time. A collection of short essays, "Marginalias of Life" (Traditional: , Simplified: ) was published in 1941. "Men, Beasts and Ghosts" (T: , S: ), a collection of short stories, mostly satiric, was published in 1946. His most celebrated work "Fortress Besieged" appeared in 1947. "On the Art of Poetry" (T: , S: ), written in classical Chinese, was published in 1948.
Besides rendering Mao Zedong's selected works into English, Qian was appointed to produce an anthology of poetry of the Song Dynasty when he was working in the Institute of Literary Studies. The "Selected and Annotated Song Dynasty Poetry" (T: , S: ) was published in 1958. Despite Qian's quoting the Chairman, and his selecting a considerable number of poems that reflect class struggle, the work was criticized for not being Marxist enough. The work was praised highly by the overseas critics, though, especially for its introduction and footnotes. In a new preface for the anthology written in 1988, Qian said that the work was an embarrassing compromise between his personal taste and the then prevailing academic atmosphere.
"Seven Pieces Patched Together" (T: , S: ), a collection of seven pieces of literary criticism written (and revised) over years in vernacular Chinese, was published in 1984. This collection includes the famous essay "Lin Shu's Translation" (T: , S: ).
Qian's "magnum opus" is the five-volume "Guan Zhui Bian", literally the "Pipe-Awl Collection", translated into English as "Limited Views". Begun in the 1980s and published in its current form in the mid-1990s, it is an extensive collection of notes and short essays on poetics, semiotics, literary history and related topics written in classical Chinese.
Qian's command of the cultural traditions of classical and modern Chinese, ancient Greek (in translations), Latin, English, German, French, Italian, and Spanish allowed him to construct a towering structure of polyglot and cross-cultural allusions. He took as the basis of this work a range of Chinese classical texts, including "I-Ching", "Classic of Poetry", "Chuci", "Zuozhuan", "Shiji", "Tao Te Ching", "Liezi", "Jiaoshi Yilin", "Taiping Guangji" and the "Complete Prose of the Pre-Tang Dynasties" (T: , S: ).
Familiar with the whole Western history of ideas, Qian shed new lights on the Chinese classical texts by comparing them with Western works, showing their likeness, or more often their apparent likeness and essential differences.
Qian Zhongshu was one of the Chinese authors best known to the Western world. "Fortress Besieged" has been translated into English, French, German, Russian, Japanese and Spanish.
Besides being one of the great masters of written vernacular Chinese in the 20th century, Qian was also one of the last authors to produce substantial works in classical Chinese. Some regard his choice of writing "Guan Zhui Bian" in classical Chinese as a challenge to the assertion that classical Chinese is incompatible with modern and Western ideas, an assertion often heard during the May Fourth Movement.
Posthumous publications.
A 13-volume edition of "Works of Qian Zhongshu" (Traditional: , Simplified: ) was published in 2001 by the Joint Publishing, a hard-covered "deluxe" edition, in contrast to all of Qian's works published during his lifetime which are cheap paperbacks. The publisher claimed that the edition had been proofread by many experts. One of the most valuable parts of the edition, titled "Marginalias on the Marginalias of Life" (T: , S: ), is a collection of Qian's writings previously scattered in periodicals, magazines and other books. The writings collected there are, however, arranged without any visible order.
Other posthumous publications of Qian's works have drawn harsh criticism. The 10-volume "Supplements to and Revisions of Songshi Jishi" (T: 宋詩紀事補正, S:宋诗纪事补正), published in 2003, was criticized as a shoddy publication. The editor and the publisher have been criticized for neglecting such obvious printing mistakes. A facsimile of Qian's holograph (known as 宋詩紀事補訂(手稿影印本) in Chinese) has been published in 2005, by another publisher. The facsimiles of parts of Qian's notebooks appeared in 2004, and have similarly drawn criticism on account of blatant inadvertency. In 2005, a collection of Qian's English works was published. Again, it was lashed for its editorial incompetence.
Further reading.
Innumerable biographies and memoirs in Chinese have been published since Qian's death.
An introduction to Qian's style of thinking can be found in the English (selected) translation of "Guan Zhui Bian":
An essay about Qian's critical vision and early writings:
Five of Qian's essays on poetry have been translated into French:

</doc>
<doc id="65873" url="https://en.wikipedia.org/wiki?curid=65873" title="Sweden Democrats">
Sweden Democrats

Sweden Democrats or Swedish Democrats (, SD) is a political party in Sweden that was founded in 1988. The party describes itself as social conservative with a nationalist foundation, though it has been characterized by some as far-right, right-wing populist, national-conservative, and anti-immigration. Since 2005 its party chairman has been Jimmie Åkesson. Richard Jomshof has been party secretary since 2015 and Mattias Karlsson has been the parliamentary group leader since 2014. An "Anemone hepatica" flower () has been the official SD logo since 2006.
SD is divided into eighteen district party associations, and various local or municipal associations, throughout Sweden. Until recently, young members were organised in the Sweden Democratic Youth (SDU), founded in 1998; however, the youth organisation was expelled from the party in 2015 due to accusations of racism and connections to extremist groups. Shortly after expelling SDU, the party formed a new youth wing called Ungsvenskarna SDU (Young Swedes SDU), a name similar to the old name of the youth wing of the Swedish Moderate party. The party also publishes a magazine, "SD-Kuriren", which is distributed to its members.
The party first gained wide notability, beyond local and/or niche coverage, in the Swedish mass media after the 2006 general election, when it achieved electoral success in many municipal and county council elections (which in Sweden are held simultaneously with the parliamentary election). This was particularly noticeable in the counties of Scania and Blekinge, in the far south of the country. In Malmö, Sweden's third-largest city, the party won more than 13% of the total votes, and in Helsingborg, around 15%.
Following the 2006 municipal elections and the party's success in the southern counties of Sweden, national support for the Sweden Democrats began to grow. Although the party's earliest roots were not in any specific region, support for it has grown from the south towards the north and the capital, Stockholm. In the 2010 general election the Sweden Democrats crossed for the first time the four per cent threshold necessary for parliamentary representation. This increase in popularity has been compared by international media to other similar anti-immigration movements in Europe. The party polled 5.7% and won 20 parliamentary seats.
The Sweden Democrats continued this success in the 2014 general election, polling 12.9% and winning 49 seats in the Riksdag, a 14%share of the seats. With a vote share of22.16% in the constituency of Scania County North & East, Sweden Democrats out-polled one of the two major parties for the first time in one of the 29constituencies where parliamentary seats are distributed. The Sweden Democrats, however, remain isolated in the Riksdag because the other parties are staunchly maintaining a policy of refusing cooperation with them.
History.
Early years (1988–1995).
The Sweden Democrats party was founded in 1988 as a successor to the Sweden Party, which in turn had been formed in 1986 by the merger of "Bevara Sverige Svenskt" (BSS, English: "Keep Sweden Swedish") and a faction of the Swedish Progress Party. SD claims 6 February 1988 as the date of its foundation, although observers tend to see the party's foundation as part of a complex decade-long series of events, with some even calling into question whether a meeting took place on 6 February. The party had its roots in Swedish fascism and was a part of the white supremacy movement; initially, it was characterized by right-wing extremism and activism. SD's logo from the 1990s until 2006 was a version of the torch used by the UK National Front.
While opinions on the early SD vary, it is generally agreed (also by the Swedish Committee Against Antisemitism and by "Expo") thatSD has never been a Nazi party, although various connections have existed through their members. The party sponsored music of the nationalist Viking rock band Ultima Thule, and various party officials today acknowledge that being fans of Ultima Thule's music factored prominently in their decision to become politically engaged. The party's first auditor, Gustaf Ekström, was a "Waffen-SS" veteran and had been a member of the national socialist party "Svensk Socialistisk Samling" in the1940s, and early chairman Anders Klarström had been active in "Nordiska rikspartiet" ("Nordic Reich Party"). Early on, the party sought international connections with the National Democratic Party of Germany, the American National Association for the Advancement of White People (founded by David Duke) and publications like the Nazi "Nation Europa" and "", a newspaper that advocates racial biology.
Moderation and growth (1995–2010).
From 1995 onwards the party's new leader, Mikael Jansson (a former member of the Centre Party), strove to make the party more respectable and, after photographs surfaced of some members posing in Nazi uniforms at party meetings, the wearing of any kind of uniform was formally banned in 1996. During the 1990s, the party became more influenced by the French National Front, as well as the Freedom Party of Austria, the Danish People's Party, German The Republicans and Italian National Alliance. SDreceived economic support for the 1998 election from the French National Front, and was active in Le Pen's Euronat from the same time. SD,however, in 1999 left its membership in Euronat to its youth organisation. In 2001 the most extreme faction was expelled from the party, leading to the formation of the more radical National Democrats.
During the 2000s the so-called "Scania gang", or "Gang of Four"Jimmie Åkesson (party leader since 2005), Björn Söder, Mattias Karlsson and Richard Jomshofcontinued and expanded the moderation policy, which included ousting openly extremist members. Before the 2002 election, former Moderate Party MP Sten Christer Andersson defected to SD, citing that the party had gotten rid of its extreme-right elements. In 2003 the party declared the Universal Declaration of Human Rights to be a cornerstone of its policies. In 2006 the party changed its logo from the torch to one featuring an "Anemone hepatica", reminiscent of the party's very first, but short-lived, logo (a stylized "Myosotis scorpioides").
Entrance into parliament (2010–2014).
In the 2010 general election, SDwon representation in the Swedish Riksdag for the first time, with5.7% of the vote and 20MPs.
Sweden Democrat MP William Petzäll was persuaded to leave the party on 26 September 2011 while still retaining his parliamentary seat. This was done because of Petzäll's substance abuse and the problems this might cause for SD'spublic image. Petzäll later died of an overdose and his seat was turned over to Stellan Bojerud in September 2012.
In November 2012, videos from August 2010 were released, in segments, over the course of three days by Swedish newspaper "Expressen" (a year earlier, "Expressen" had released the same videos without making much noise). This came to be known as the Iron pipe scandal, although the same videos had already been released on YouTube by Erik Almqvist in 2010. The videos, recorded by MP Kent Ekeroth, featured him along with fellow Sweden Democrats MP Erik Almqvist and Christian Westling. The videos show Almqvist arguing with comedian Soran Ismail: Almqvist is referring to Sweden as "my country, not your country", as an insult to Ismail. They are also shown arguing with a drunken man. A woman can also be seen approaching Kent Ekeroth while filming; he calls her a whore and pushes her out of the way. A few minutes later they are seen picking up iron bars. Coming only a month after party leader Åkesson had instated a zero-tolerance policy towards racism in the party, the release of the video caused Almqvist to leave his position as the party's economic policy spokesperson and his place in the executive committee on 14 November. He excused himself as having been under a lot of pressure and threats of violence at the time. As more segments of the video were released, revealing the other two men's involvement, the party announced on 15 November that Ekeroth would take a break from his position as the party's justice policy spokesman. Almqvist and Ekeroth both took time off from their parliament seats. Sweden Democratic Youth president Gustav Kasselstrand and vice president William Hahne criticized the decision to remove Almqvist and Ekeroth in an op-ed in Dagens Nyheter, arguing that the party should not give in to media pressure.
Only two weeks after Almqvist and Ekeroth were forced to step down, fellow MP Lars Isovaara reported being robbed of his backpack and pushed out of his wheelchair by "two unknown men of an immigrant background". When trying to get into the Riksdag, Isovaara was himself reported by the police for racial abuse against safety guards. The Sweden Democrats initially defended Isovaara, but backed down when "Expressen" revealed that Isovaara had actually forgotten his backpack at a restaurant, and that the two men had helped him when he fell out of his wheelchair. He left his seat in the Riksdag on 29 November, and was replaced by Markus Wiechel.
Rise in support (2014– ).
In the 2014 election the Sweden Democrats received 12.9%of the votes, doubling their support and becoming the third-largest party. The party remained big in Scania and Blekinge; for example in Malmö the party received 14% of the votes, in Landskrona it received 19% of the votes and in Sjöbo a total of 30%. Other parties, however, remained firm in their decision to isolate them from exerting influence. Some time after that, Åkesson announced he would go on sick leave due to burnout. Mattias Karlsson was appointed to temporarily take over Åkesson's duties as party leader.
On Monday, 23 March 2015, it was announced that Åkesson would return from his leave of absence to resume his duties as party leader following an interview to be broadcast on the Friday, 27 March instalment of the "Skavlan" program on SVT, and a subsequent press conference with the Swedish media.
Amid media coverage regarding the high immigration figures and the European migrant crisis, the Sweden Democrats soared in all opinion polls during the summer of 2015, even topping web-based polls from YouGov and Sentio in late summer, with a little over a quarter of the vote. The party also saw rising support in phone-based polls, although the swing was lower. This was the first time in 100 years that any party other than the Social Democrats and the Moderates had topped official opinion polls.
Ideology and political positions.
The Sweden Democrats' party programme is based on nationalism and social conservatism. The Sweden Democrats' ideological basis is described in their manifesto, first published on 4 May 2003 during the Jansson leadership and then revised on 8 May 2005 (one day after Åkesson became the new chairman). Nordic Studies scholar Benjamin Teitelbaum has called them radical nationalist. The party has been described by sociologist Jens Rydgren and others as xenophobic, racist and right-wing populist. In 2013 a Sveriges Radio journalist called the party xenophobic, which resulted in a complaint lodged to the broadcasting regulator. The Swedish Broadcasting Commission determined that this description was acceptable to use.
Immigration.
The Sweden Democrats believe that the current Swedish immigration and integration policies have been a failure. They oppose integration because they believe that integration involves "meeting in the middle" and do not think that the indigenous Swedish people should have to bear the burden of what they see as a reckless immigration policy. SDfeels that the current situation, with a large number of immigrants living in cultural enclaves, is not beneficial for the country. They argue that the immigrants themselves are rootless, that there have been rising antagonistic tensions between various population groups (socially, ethnically, religiously and culturally), and the immigration in itself, SD says, has caused social and economic strains on the country.
As the party considers Sweden to have had too much immigration in recent years, which it claims has seriously threatened national identity and societal cohesion, SDwants to reinstate a common Swedish national identity, which in turn would mean a stronger inner solidarity. SDrejects the policy of multiculturalism, but accepts a multiethnic society where cultural assimilation is promoted. SD wishes to strongly restrict immigration, and give generous support for immigrants who instead of wanting to assimilate in Sweden voluntarily prefer to emigrate back to their country of origin. As more state funds are made free from funding mass immigration, SD believes that Sweden will be better able to help refugees in their own nearby locations.
SD has referred to the recommendations from the United Nations High Commissioner for Refugees (UNHCR) which state that the return of refugees should be the solution to refugee problems. Torbjörn Kastell (former party secretary from 2003 to 2004) said in 2002 that the party wanted "a multicultural world, not a multicultural society". In a 2008 survey, a significant minority of 39% of all Swedes thought that there were "too many foreigners in the country", and in 2007 a survey showed that 49% of all Swedes wanted to restrict the number of asylum-seekers. In recent years SDhas tried to approach the immigration policy of the Danish People's Party, which from 2001 to 2011 provided parliamentary support for the former Danish liberal/conservative government in return for a tightening of Danish immigration policies and stricter naturalization laws.
According to "Aftonbladet", 14%of SD members are of immigrant origin, which corresponds to the proportion of foreign-born in Sweden. For the 2010 election in the municipality of Södertälje (Stockholm County), SDwas the only party with a majority of immigrants on its electoral list, mostly Assyrians from the Middle East. Polling7.31%(3,447 votes), SD'smunicipal list in Södertälje got 5of the65 municipal seats. Nader Helawi and four other Swedes of immigrant origin will sit as municipal councilors.
The elderly.
SD wishes to lower the tax rate for the elderly, as well as increase subsidized housing for the elderly. SD also wishes to allocate additional resources to municipalities in order to provide seniors with greater food assistance and, in general, improve their quality of life. SD has also emphasized a desire to crack down on abuses and crimes of which the elderly are particular targets.
Sami people.
The Sweden Democrats are critical of the special rights given to the indigenous Sami people of northern Sweden. In 2008 the party accepted a motion against the rights to reindeer husbandry. They have argued that those "who do not involve themselves with reindeer husbandry are treated as second class citizens" and that the privileges the herders have are "undemocratic". They want to restructure the councils and funds that are used to benefit the Sami population, so that they are used "regardless of ethnic identity and business operations". They also want to abolish the Sami Parliament, which claims special privileges for an "ethnic minority while the society claims equal rights for others".
Views on national identity.
In an interview for "Dagens Nyheter", Second Deputy Speaker of the Riksdag and then-party secretary Björn Söder elaborated on the SDparty programme with respect to its views on national identity by saying that he personally did not think people with dual national identities in Sweden would necessarily identify themselves as Swedish. Although an immigrant of any ethnic background in theory can become a Swedish citizen, they would have to adapt and be assimilated in order to be considered Swedish in the cultural sense. Björn Söder stated that the officially recognized Swedish minority peoples (e.g. Sami, Tornedalians and Jews) in many cases have dual cultural identities and that they probably would be proud of both heritages. It was widely interpreted that Söder had stated in the interview that Jews cannot be Swedish unless they abandon their Jewish identity. Söder's comments were understood to be anti-semitic and caused Swedish parliamentary groups and party leaders to call for Björn Söder's resignation. The Simon Wiesenthal Center listed the statement as number six on their list of the top ten most anti-semitic events of 2014. Söder responded to the allegations of anti-semitism with a column of his own that appeared in the "Jerusalem Post" on 5 January 2015, writing in part, "In a biased article in one of Sweden’s largest newspapers, Dagens Nyheter, some of my statements were dramatically taken out of context to erroneously credit me with opinions that do not correspond with reality. Politically biased journalists and political opponents have further distorted the statements, resulting in a presentation virtually the complete opposite of my actual statements and opinions. This is now distributed in the international press, such as in the Post, which therefore necessitates a clarification on my part." 
Law and order.
SD wishes to instate the possibility of life without parole for the worst crimes and to repatriate foreign citizens found guilty of serious crime (which already is general practice in Sweden, though the repatriation is usually limited to a few years after which the offenders are able to reapply for asylum). SD also wants to establish a public register of convicted pedophiles.
Foreign policy.
The Sweden Democrats in their foreign policy reject joining the Economic and Monetary Union of the European Union, are opposed to the accession of Turkey to the European Union, and want to renegotiate Swedish membership in the European Union.
Culture.
The Sweden Democrats advocate a cultural policy that would strip funding for multicultural initiatives and strengthen support for traditional Swedish culture. This agenda has often manifest as opposition to state funding of immigrant cultural organizations and festivals, and support for traditional Swedish craft, folk music, and folk dance groups. The party also tends to oppose state support for cultural initiatives deemed provocative or elitist. A 2014 letter signed by 52Swedish anthropologists, including Ronald Stade, critiziced the Sweden Democrats' use of the terms "culture" (kultur) and "anthropology" (antropologi), claiming their views on culture were "essentialist and obsolete", saying that culture is "dynamic" and "in constant change".
Family.
The Sweden Democrats consider children raised in a traditional nuclear family as the preferred option. Those not raised by their biological parents should have the right to associate or at least find out about who they were. In their Family Policy, SD opposes any government sanctioned adoption and insemination to single people, same-sex couples and polyamorous relationships unless the adopting party are close relatives or already have a close relationship with the child. They also state that children who live with a same-sex couple should be adopted to a same-sex couple if they become orphans and there is no pre-determined legal guardian. 
Although SDstrongly criticizes what it calls a "Homosex Lobby", the party claims that it is not hostile to homosexuals. Furthermore, party leader Jimmie Åkesson expressed concern that what he describes as Islamization of Sweden will eventually lead to the rights of sexual minorities being violated.
Published by SD Party secretary Björn Söder on 1 August 2007, a blog article titled "Botten måste snart vara nådd" (Soon enough we'll hit rock bottom) led to intense debate and criticism.
Controversies.
During the 1990s many outspoken far-right advocates were involved with the party. The party had flyers printed by the French National Front in the 1998 general election, and was financially backed for the 2004 European election by Belgian Bernard Mengal.
Media boycott.
The Sweden Democrats have complained about difficulties buying advertising space due to the media banning the party from advertisement, which has been criticised by free speech organisations. On 16 June 2006, "Dagens Nyheter" and "Svenska Dagbladet" decided to stop their boycott. "Expressen", however, still retains a ban on Sweden Democrat advertising.
Muhammad cartoon debate.
After the Danish daily newspaper "Jyllands-Posten" published twelve cartoons depicting Muhammad and ignited a controversy during the2005 autumn and winter, the Sweden Democrats gave their unreserved support to the publication with reference to the freedom of speech. SDstated that it saw no reason why a Danish newspaper should be forced to abide by Muslim rules and prohibitions regarding expression. When the boycott of Danish products was launched in the Middle East, SDlaunched a "Buy Danish" campaign in support of Danish workers. In 2006 SD entered the Muhammad cartoon debate by publishing a cartoon depicting Muhammad on its youth league (SDU) and "SD-Kuriren" websites. The cartoon showed Muhammad from behind holding a mirror in front of his face. However, instead of any facial features, the mirror showed only a blank head. The cartoon was captioned "Muhammad's Face" ().
The publication attracted the attention of the Swedish government, which informed internet service provider "Levonline" about the SD's publications. Subsequently, "Levonline" shut down SD's web page. The Minister for Foreign Affairs, Laila Freivalds, denied any direct interference. However, at the same time, Laila Freivalds condemned the publication as a provocation. Freivalds then resigned from the Persson Cabinet after being accused of interference with press freedom and lying about such actions.
This event spurred debate on government censorship in Sweden. The Sweden Democrats also had a hate speech charge filed against it due to the posted caricature. Similar hate speech charges were filed against other Swedish publishers who had depicted Muhammad. However, these charges were immediately deemed to be unfounded by the Swedish Chancellor of Justice ("Justitiekanslern").
The Sweden Democrats originally planned to publish a set of cartoons in their newspaper "SD-Kuriren". However, after the controversy erupted, Jimmie Åkesson issued a statement on SD's website on 9 February 2006, stating that it would refrain from further publications online and in print, due to concerns that it might spur hostile actions against Swedes and Swedish interests.
The shutdown of the Sweden Democrats' websites was reported to the Committee on the Constitution by the Liberal People's Party leader Lars Leijonborg. SD filed charges against the Security Service ("Säpo") and the Ministry for Foreign Affairs with the "Justitiekansler" and "Justitieombudsmannen", alleging that the government's interference was unconstitutional. The spokesperson of the Green Party, Peter Eriksson, also expressed concern over possible government involvement in the event.

</doc>
<doc id="65880" url="https://en.wikipedia.org/wiki?curid=65880" title="Compact Cassette">
Compact Cassette

The Compact Cassette or Musicassette (MC), also commonly called cassette tape, audio cassette, or simply tape or cassette, is a magnetic tape recording format for audio recording and playback released by Philips in 1962. Compact cassettes come in two forms, either already containing content as a pre-recorded cassette, or as a fully recordable "blank" cassette. It was designed originally for dictation machines, but improvements in fidelity led the Compact Cassette to supplant the Stereo 8-track cartridge and reel-to-reel tape recording in most non-professional applications. Its uses ranged from portable audio to home recording to data storage for early microcomputers. Between the early 1970s and the early 2000s, the cassette was one of the two most common formats for prerecorded music, first alongside the LP record and later the compact disc (CD).
Compact Cassettes contain two miniature spools, between which a magnetically coated, polyester-type plastic film is passed and wound. These spools and their attendant parts are held inside a protective plastic shell. Two stereo pairs of tracks (four total) or two monaural analog audio tracks are available on the tape; one stereo pair or one monophonic track is played or recorded when the tape is moving in one direction and the second pair when moving in the other direction. This reversal is achieved either by manually flipping the cassette, or by having the machine itself change the direction of tape movement and head respectively ("auto-reverse").
History.
Before the Compact Cassette.
In 1935, decades before the introduction of the Compact Cassette, AEG released the first reel-to-reel tape recorder (in German: "Tonbandgerät"), with the commercial name "Magnetophon", based on the invention of the magnetic tape (1928) by Fritz Pfleumer, which used similar technology but with open reels (for which the tape was manufactured by BASF). These instruments were very expensive and relatively difficult to use and were therefore used mostly by professionals in radio stations and recording studios.
In 1958, following four years of development, RCA Victor introduced the stereo, quarter-inch, reversible, reel-to-reel RCA tape cartridge. However, it was a large cassette (5" × 7"), and offered few pre-recorded tapes. Despite the multiple versions, it failed.
Consumer use of tape only took off in the early 1960s, after playback machines reached a comfortable, user-friendly design. This was achieved primarily by the introduction of transistors which replaced the bulky, fragile, and costly vacuum tubes of earlier designs. Reel-to-reel tape then became more suitable to household use, but still remained an esoteric product.
Introduction of the Compact Cassette.
In 1962, Philips invented the Compact Cassette medium for audio storage, introducing it in Europe on 30 August 1963 at the Berlin Radio Show, and in the United States (under the "Norelco" brand) in November 1964, with the trademark name "Compact Cassette". The team at Philips was led by Lou Ottens in Hasselt, Belgium.
"Philips was competing with Telefunken and Grundig in a race to establish its cassette tape as the worldwide standard, and it wanted support from Japanese electronics manufactureres." However, the Philips' Compact Cassette became dominant as a result of Philips' decision (under pressure from Sony) to license the format free of charge. Philips also released the Norelco "Carry-Corder 150" recorder/player in the US in November 1964. By 1966 over 250,000 recorders had been sold in the US alone and Japan soon became the major source of recorders. By 1968, 85 manufacturers had sold over 2.4 million players. By the end of the 1960s, the cassette business was worth an estimated 150 million dollars.
In the early years sound quality was mediocre, but it improved dramatically by the early 1970s when it caught up with the quality of 8-track tape and kept improving. The Compact Cassette went on to become a popular (and re-recordable) alternative to the 12-inch vinyl LP during the late 1970s.
Popularity of music cassettes.
The mass production of blank (recordable) Compact Cassettes began in 1964 in Hanover, Germany. Prerecorded music cassettes (also known as Music-Cassettes, and later just Musicassettes; M.C. for short) were launched in Europe in late 1965. The Mercury Record Company, a US affiliate of Philips, introduced M.C. to the US in July 1966. The initial offering consisted of 49 titles.
However, the system had been designed initially for dictation and portable use, with the audio quality of early players not well suited for music. Some early models also had unreliable mechanical design. In 1971 the Advent Corporation introduced their Model 201 tape deck that combined Dolby type B noise reduction and chromium dioxide (CrO2) tape, with a commercial-grade tape transport mechanism supplied by the Wollensak camera division of 3M Corporation. This resulted in the format being taken more seriously for musical use, and started the era of high fidelity cassettes and players.
Although the birth and growth of the cassette began in the 1960s, its cultural moment took place during the 1970s and '80s. The cassette's popularity grew during these years as a result of being a more effective, convenient and portable way of listening to music. Stereo tape decks and boom boxes became some of the most highly sought-after consumer products of both decades. Portable pocket recorders and high-fidelity ("hi-fi") players, such as Sony's Walkman (1979), also enabled users to take their music with them anywhere with ease. The increasing user-friendliness of the cassette led to its popularity around the globe. The body of the Walkman was not much larger than the cassette tape itself, with mechanical keys on one side, or electronic buttons or a display on the face. Sony's WM-10 was even smaller than the cassette itself, and expanded to hold and play a cassette.
Like the transistor radio in the 1950s and 1960s, the portable CD player in the 1990s, and the MP3 player in the 2000s, the Walkman defined the portable music market for the decade of the '80s, with cassette sales overtaking those of LPs. Total vinyl record sales remained higher well into the 1980s due to greater sales of singles, although cassette singles achieved popularity for a period in the 1990s. Another barrier to cassettes overtaking vinyl in sales was shoplifting; compact cassettes were small enough that a thief could easily place one inside a pocket and walk out of a store without being noticed. To prevent this, retailers would place cassettes inside oversized "spaghetti box" containers or locked display cases, either of which would significantly inhibit browsing, thus reducing cassette sales. During the early 1980s some record labels sought to solve this problem by introducing new, larger packages for cassettes which would allow them to be displayed alongside vinyl records and compact discs, or giving them a further market advantage over vinyl by adding bonus tracks. Williem Andriessen wrote that the development in technology allowed "hardware designers to [...] discover and satisfy one of the collective desires of human beings all over the world, independent of region, climate, religion, culture, race, sex, age and education: the desire to enjoy music at any time, at any place, [...] in any desired sound quality and almost at any wanted price.
Apart from the purely technological advances cassettes brought, they also served as catalysts for social change. Their durability and ease of copying helped bring underground rock and punk music behind the Iron Curtain, creating a foothold for Western culture among the younger generations. For similar reasons, cassettes became popular in developing nations.
One of the most famous political uses of cassette tapes was the dissemination of sermons by the Ayatollah Khomeini throughout Iran before the 1979 Iranian Revolution, in which Khomeini urged the overthrow of the regime of the Shah, Mohammad Reza Pahlavi. In 1970s India, cassettes were blamed for bringing unwanted Christian and Islamic influences into traditionally Sikh and Hindu areas. Cassette technology was a booming market for pop music in India, drawing criticism from conservatives while at the same time creating a huge market for legitimate recording companies, as well as pirated tapes.
Between 1985 and 1992, the cassette tape was the most popular format in the UK and wealthy record labels experimented with innovative packaging designs. A designer during the era explained: "There was so much money in the industry at the time, we could try anything with design." The introduction of the cassette single, called a "cassingle", was also part of this era and featured a music single in Compact Cassette form. Until 2005, cassettes remained the dominant medium for purchasing and listening to music in some developing countries, but compact disc (CD) technology had superseded the Compact Cassette in the vast majority of music markets throughout the world by this time.
Decline.
In Western Europe and North America, the market for cassettes declined sharply after its peak in the late 1980s. This was particularly noticeable with pre-recorded cassettes, the sales of which were overtaken by those of CDs during the early 1990s. By 1993, annual shipments of CD players had reached 5 million, up 21% from the year before; while cassette player shipments had dropped 7% to approximately 3.4 million. The decline continued, and in 2001 cassettes accounted for only 4% of all music sold. Since then, further decline occurred, with very few retailers stocking them because they are no longer issued by the major music labels.
Sales of pre-recorded music cassettes in the U.S. dropped from 442 million in 1990 to 274,000 by 2007. Another record low was registered in 2009, with 34,000 cassettes sold. Most of the major U.S. music companies discontinued production of cassette tapes by late 2001. However, , blank cassettes were still being produced and are still sold at some retail stores, while facilities for cassette duplication remain available. Cassette recorders and players are becoming gradually scarcer, but are still widely available and feature in some hi-fi systems. A company named National Audio Company (NAC) in Missouri oversaw the mass production of the film "Guardians of the Galaxy" soundtrack in 2014.
Cassettes remained popular for specific applications, such as car audio and telephone answering machines, well into the 1990s. Cassettes and their players were typically more rugged and resistant to dust, heat, and shocks than the main digital competitor, the CD. Their lower fidelity was not considered a serious drawback. However, the advent of "shock proof" buffering technology in CD players, the general heightening of consumer expectations, and the introduction of CD auto-changers meant that, by the early 2000s, the CD player rapidly replaced the cassette player as the default audio component in the majority of new vehicles in Europe and America. The last new car with an available cassette player was a 2010 Lexus SC 430.
While digital voice recorders are now common, Compact Cassette—or microcassette—recorders may be cheaper and of sufficient quality to serve as adjuncts or substitutes for note taking in business and educational settings. Audiobooks, church services, and other spoken word material are still frequently sold on cassette, as lower fidelity generally is not a drawback for such content, and some people prefer the convenience of the tape controls for rewinding to repeat a missed passage. While most publishers sell CD audiobooks, they usually also offered a cassette version at the same price well into the 2000s. In the audiobooks application, where recordings may span several hours, cassettes also have the advantage of holding up to 150 minutes of material, whereas the average CD holds less than 80.
21st century use.
Although portable digital recorders are most common today, analog tape remains a desirable option for certain artists and consumers. Older genres like "dansband" may favor the format most familiar to their fans. Some musicians and DJs in the independent music community maintain a tradition of using and releasing cassettes due to its low cost and ease of use. Underground and DIY communities release regularly, and sometimes exclusively, on cassette format, particularly in experimental music circles and to a lesser extent in hardcore punk, death metal, and black metal circles, out of a fondness for the format. Even among major label stars, the form has its devotees: Eminem has made his favor known, and Thurston Moore claimed in 2009, "I only listen to cassettes."
In 2010, Botswana-based Diamond Studios announced plans for establishing a plant to mass-produce cassettes in a bid to combat piracy.
In South Korea, the early English education boom for toddlers encourages a continuous demand for English language cassettes, as of 2011, due to the affordable cost.
In 2011, the Oxford English Dictionary removed the word "cassette player" from its 12th edition Concise version. Some media sources mistakenly claimed that the word "cassette tape" was being removed and this caused some media backlash. The term was removed to help make room for more than 400 new words being added to the dictionary.
In India, film and devotional music continues to be released in the cassette format due to its low cost.
In 2015, National Audio Company, the largest and one of the few remaining manufacturers of audiocassettes in the U.S., reported they produced more than 10 million tapes in 2014 and sales are up 20 percent this year, it has had its best year since it opened in 1969. In 2013, 2014 and again in 2015 a "Cassette Store Day" was used to promote recent cassette tape releases by indie, self-issued and DIY music bands. This demonstrates that the cassette has made a resurgence in recent years, albeit much smaller and niche compared to vinyl records.
Features.
The cassette was a great step forward in convenience from reel-to-reel audio tape recording, although, because of the limitations of the cassette's size and speed, it initially compared poorly in quality. Unlike the 4-track stereo open-reel format, the two stereo tracks of each side lie adjacent to each other, rather than being interleaved with the tracks of the other side. This permitted monaural cassette players to play stereo recordings "summed" as mono tracks and permitted stereo players to play mono recordings through both speakers. The tape is wide, with each stereo track 0.6 mm wide and an unrecorded guard band between each track. The tape moves at (1⅞ inch/s) from left to right. For comparison, the typical open-reel format in consumer use was (¼ inch) wide, each stereo track nominally 1.0 mm (1⁄25 inch) wide, and running at either twice or four times the speed of a cassette.
Cassette types.
Cassette tapes are made of a polyester type plastic film with a magnetic coating. The original magnetic material was based on gamma ferric oxide (Fe2O3). Circa 1970, 3M Company developed a cobalt "volume-doping" process combined with a double-coating technique to enhance overall tape output levels. This product was marketed as "High Energy" under its Scotch brand of recording tapes. Inexpensive cassettes commonly are labeled "low-noise," but typically are not optimized for high frequency response. For this reason, some low-grade IEC Type I tapes have been marketed specifically as better suited for data storage than for sound recording.
At about the same time, chromium dioxide (CrO2) tape, later designated Type II, was introduced by DuPont, the inventor of the particle, and BASF, the inventor and longtime manufacturer of magnetic recording tape. Next, coatings using magnetite (Fe3O4) such as TDK's Audua were produced in an attempt to approach or exceed the sound quality of vinyl records. Cobalt-"absorbed" iron oxide (Avilyn) was introduced by TDK in 1974 and proved very successful. "Type IV" tapes using pure metal particles (as opposed to oxide formulations) were introduced in 1979 by 3M under the trade name Metafine. The tape coating on most cassettes sold today as either "Normal" or "Chrome" consists of ferric oxide and cobalt mixed in varying ratios (and using various processes); there are very few cassettes on the market that use a pure (CrO2) coating.
Simple voice recorders and earlier cassette decks are designed to work with standard ferric formulations. Newer tape decks usually are built with switches and later detectors for the different bias and equalization requirements for higher grade tapes. The most common, iron oxide tapes (defined by the IEC 60094 standard, as "Type I") use 120 µs equalization, while chrome and cobalt-absorbed tapes (IEC Type II) require 70 µs equalization. The recording "bias" equalizations also were different (and had a much longer time constant). BASF and Sony tried a dual layer tape with both ferric oxide and chrome dioxide known as 'ferrichrome' (FeCr) (IEC Type III), but these were available for only a short time in the 1970s. These also use 70 µs, just like Type II did. Metal Cassettes (IEC Type IV) also use 70 µs equalization, and provide still further improvement in sound quality as well as durability. The quality normally is reflected in the price; Type I cassettes generally are the cheapest, and Type IV are usually the most expensive. BASF chrome tape used in commercially pre-recorded cassettes used type I equalization to allow greater high-frequency dynamic range for better sound quality, but the greater selling point for the music labels was that the Type I cassette shell could be used for both ferric and for chrome music cassettes.
Notches on top of the cassette shell indicate the type of tape. Type I cassettes have only write-protect notches, Type II have an additional pair next to the write protection ones, and Type IV (metal) have a third set in the middle of the cassette shell. These allow later cassette decks to detect the tape type automatically and select the proper bias and equalization. Virtually all recent hi-fi systems (with cassette decks) lack this feature; only a small niche of cassette decks (hi-fi separates) have the tape type selector. Playing Type II and IV tapes on a player without detection will produce exaggerated treble, but it may not be noticeable because such devices typically have amplifiers that lack extended high-frequency output. Recording on these units, however, results in very low sound reproduction, and sometimes distortion and noise is heard. Also, these cheaper units cannot erase high bias or metal bias tapes. Attempting to do so will result in an incomplete erasure. This is due to the fact that the bias levels for high and metal position requires greater levels.
An exception to this standard were mechanical storytelling dolls from the 1980s (e.g. Teddy Ruxpin) which used the Type IV Metal configuration cassette shell but had normal Type I voice grade tape inside. These toys used the Type IV notches to detect that a specially coded tape had been inserted, where the audio of the story is stored on the left channel and various cue tones to tell the doll's servos how and when to move along with the story on the right channel.
Most pre-recorded chrome cassettes require 120 µs equalisation and are treated as Type I (with notches as Type I ferric cassettes), to ensure compatibility with budget equipment.
Tape length.
Tape length usually is measured in minutes of total playing time. The most popular varieties are C46 (23 minutes per side), C60 (30 minutes per side), C90, and C120. The C46 and C60 lengths typically are 15–16 µm thick, but C90s are 10–11 µm and (the less common) C120s are just 9 µm thick, rendering them more susceptible to stretching or breakage. BASF declared the C60 with . Some vendors are more generous than others, providing rather than of tape for a C90 cassette. Even C180 tapes were available at one time, but these were extremely thin and fragile and suffered from such effects as print-through, which made them unsuitable for general use. 150 minute length is still available from Maxell (UR 150), Sony (CDixI 150) and TDK (TDK AE 150).
Although the TDK-D C180 was produced for a decade (1972–82), it is very rare, because of several technical flaws. The tape had to be so thin that it was nearly transparent and therefore had fewer particles to magnetize, resulting in a poor sound quality and even worse durability. It required a strong motor to be driven, and had high wow-and-flutter. Finally, it took a relatively long time to rewind.
Other lengths are (or were) also available from some vendors, including C10 and C15 (useful for saving data from early home computers and in telephone answering machines), C30, C40, C50, C54, C64, C70, C74, C80, C84, C100, C105, and C110. As late as 2010, Thomann still offered C10, C20, C30 and C40 IEC Type II tape cassettes for use with 4- and 8-track portastudios.
Some companies included a complimentary blank cassette with their portable cassette recorders in the early 1980s. Panasonic's was a C14 and came with a song recorded on side one, and a blank side two. Except for C74 and C100, such non-standard lengths always have been hard to find, and tend to be more expensive than the more popular lengths. Home taping enthusiasts may have found certain lengths useful for fitting an album neatly on one or both sides of a tape. For instance, the initial maximum playback time of Compact Discs was 74 minutes, explaining the relative popularity of C74 cassettes.
Track width.
The full tape width is 3.8 mm. For mono recording the track width were 1.5 mm wide. In stereo mode each channel has width of 0.6 mm with 0.3 mm separation to avoid crosstalk.
The head is 2 µm which gives a theoretical maximum frequency of about 12 kHz. Larger gap gives higher frequency limit but also weaker magnetization.
Write-protection.
All cassettes include a write protection mechanism to prevent re-recording and accidental erasure of important material. Each side of the cassette has a plastic tab on the top that may be broken off, leaving a small indentation in the shell. This indentation allows the entry of a sensing lever that prevents the operation of the recording function when the cassette is inserted into a cassette deck. If the cassette is held with one of the labels facing the user and the tape opening at the bottom, the write-protect tab for the corresponding side is at the top-left. Occasionally, manufacturers provided a movable panel that could be used to enable or disable write-protect on tapes.
If later required, a piece of adhesive tape can be placed over the indentation to bypass the protection, or (on some decks), the lever can be manually depressed to record on a protected tape. Extra care is required to avoid covering the additional indents on high bias or metal bias tape cassettes adjacent to the write-protect tabs.
Tape leaders.
In most cassettes, the magnetic tape was attached to each spool with a leader, usually made of strong plastic (see right-hand image). This leader protected the weaker magnetic tape from the shock occurring when the tape reached the end. Leaders can be complex: a plastic slide-in wedge anchors a short fully opaque plastic tape to the take-up hub; one or more tinted semi-opaque plastic segments follow; the clear leader (a tintless semi-opaque plastic segment) follows, which wraps almost all the way around the supply reel, before splicing to the magnetic tape itself. The clear leader spreads the shock load to a long stretch of tape instead of to the microscopic splice. Various patents have been issued detailing leader construction and associated tape player mechanisms to detect leaders. Cassette tape users would also use spare leaders to repair broken tapes.
The disadvantage with tape leaders was that the sound recording or playback did not start at the beginning of the tape, forcing the user to cue forward to the start of the magnetic section. For certain applications, such as dictation, special cassettes containing leaderless tapes were made, typically with stronger material and for use in machines that had more sophisticated end-of-tape prediction. Home computers that made use of cassettes prior to the advent of floppy discs (i.e. Apple II, Commodore PET) were designed to not start writing or reading data until leaders had spooled past.
Endless loop cassette.
Some cassettes were made to play a continuous loop of tape without stopping. Lengths available are from around 30 seconds to a standard full length. They are used in situations where a short message or musical jingle is to be played, either continuously or whenever a device is triggered, or whenever continuous recording or playing is needed. Some include a sensing foil on the tape to allow tape players to re-cue. From as early as 1969 various patents have been issued, covering such uses as uni-directional, bi-directional, and compatibility with auto-shut-off and anti-tape-eating mechanisms. One variant has a half-width loop of tape for an answering machine outgoing message, and another half-width tape on spools to record incoming messages.
Flaws.
While ubiquitous and accessible, cassette playback suffered from some flaws frustrating to both professionals and home recording enthusiasts. Tape speed could vary between devices, resulting in pitch that was too low or too high. Speed often was calibrated at the factory, and could not be changed by users. The slow tape speed increased tape hiss and noise, and in practice delivered higher values of wow and flutter. Different tape formulation and noise reduction schemes artificially boosted or cut high frequencies and inadvertently elevated noise levels. Noise reduction also adds some artifacts to the sound, which a trained ear can hear sometimes quite easily.
Cassette players and recorders.
The first cassette machines (e.g. the Philips EL 3300, introduced in August 1963) were simple mono-record and -playback units. Early machines required attaching an external dynamic microphone. Most units from the 1980s onwards also incorporated built-in condenser microphones, which have extended high-frequency response, but may also pick up noises from the recorder motor. A portable recorder format still common today is a long box, the width of a cassette, with a speaker at the top, a cassette bay in the middle, and "piano key" controls at the bottom edge. Another format is only slightly larger than the cassette, also adapted for stereo "Walkman" player applications. The markings of "piano key" controls soon were standardized, and are a legacy still emulated on many software control panels. These symbols are commonly a square for "stop", a vertically pointed triangle with a line under it for "eject", a right-pointing triangle for "play", double triangles for "fast-forward" and "rewind", a red dot for "record", and a vertically divided square (two rectangles side-by-side) for "pause".
Stereo recorders eventually evolved into high fidelity and were known as cassette decks, after the reel-to-reel decks. Hi-Fi cassette decks, in contrast to cassette recorders and cassette players, often didn't have built-in amplification or speakers. Many formats of cassette players and recorders have evolved over the years. Initially all were top loading, usually with cassette on one side, and VU meters and recording level controls on the other side. Older models used combinations of levers and sliding buttons for control.
A major innovation was the front-loading arrangement. Pioneer's angled cassette bay and the exposed bays of some Sansui models eventually were standardized as a front-loading door into which a cassette would be loaded. Later models would adopt electronic buttons, and replace conventional meters (which could be "pegged" when overloaded) with electronic LED or vacuum fluorescent displays, with level controls typically being controlled by either rotary controls or side-by-side sliders. BIC and Marantz briefly offered models that could be run at double speeds, but Nakamichi was widely recognized as one of the first companies to create decks that rivaled reel-to-reel decks with frequency response from the full 20–20,000 Hz range, low noise, and very low wow and flutter. The 3-head closed-loop dual capstan Nakamichi 1000 (1973) is one early example. Unlike typical cassette decks that use a single head for both record and playback plus a second head for erasing, the Nakamichi 1000, like the better reel-to-reel recorders, used three separate heads to optimize these functions.
Other contenders for the highest "HiFi" quality on this medium were two companies already widely known for their excellent quality reel-to-reel tape recorders: Tandberg and Revox (consumer brand of the Swiss professional Studer company for studio equipment). Tandberg started with combi-head machines, such as the TCD 300, and continued with the TCD 3x0 series with separate playback and recording heads. All TCD-models possessed dual-capstan drives, belt-driven from a single capstan motor and two separate reel motors. Frequency range extended to 18 kHz. After a disastrous overinvestment in colour television production, Tandberg folded and revived without the HiFi-branch these came from.
Revox went one step further: after much hesitation about whether to accept cassettes as a medium capable for meeting their strict standards from reel-to-reel recorders at all, they produced their B710MK I (Dolby B) and MK II (Dolby B&C) machines. Both cassette units possessed double capstan drives, but with two independent, electronically controlled capstan motors and two separate reel motors. The head assembly moved by actuating a damped solenoid movement, eliminating all belt drives and other wearable parts. These machines rivaled the Nakamichi in frequency and dynamic range. The B710MKII also achieved 20–20,000 Hz and dynamics of over 72 dB with Dolby C on chrome and slightly less dynamic range, but greater headroom with metal tapes and Dolby C. Revox adjusted the frequency range on delivery with many years of use in mind: when new, the frequency curve went upwards a few dB at 15–20 kHz, aiming for flat response after 15 years of use, and headwear to match.
A last step taken by Revox produced even more-advanced cassette drives with electronic finetuning of bias and equalization during recording. Revox also produced amplifiers, a very expensive FM tuner, and a pickup with a special parallel-arm mechanism of their own design. After releasing that product, Studer encountered financial difficulties. It had to save itself by folding its Revox-branch and all its consumer products (except their last reel-to-reel recorder, the B77).
While some might say that Nakamichi violated the tape recording standards to achieve the highest dynamics possible, producing non-compatible cassettes for playback on other machines, the reasons for this are more complex than they appear on the surface. Different interpretations of the cassette standard resulted in a 4 dB ambiguity at 16 kHz. Technically, both camps in this debate were still within the original cassette specification as no tolerance for frequency response was provided above 12.5 kHz and all calibration tones above 12.5 kHz are considered optional. But also Nakamichi is not error-prone. Decreasing noise at 16 kHz also decreases the maximum signal level at 16 kHz, the HighFrequency-Dynamics stay almost constant.
A third company, the Danish Bang & Olufsen improved the Dolby HX "head room extension" system for reliably reducing tape saturation effects at short wavelength (high frequencies) despite higher bias levels. This advanced method was called Dolby HX Pro in full and patented. HX Pro was adopted by many other high-end manufacturers.
As they became aimed at more casual users, fewer decks had microphone inputs. Dual decks became popular and incorporated into home entertainment systems of all sizes for tape dubbing. Although the quality would suffer each time a source was copied, there are no mechanical restrictions on copying from a record, radio, or another cassette source. Even as CD recorders are becoming more popular, some incorporate cassette decks for professional applications.
Another format that made an impact on culture in the 1980s was the radio-cassette, aka the "boom box" (a name used commonly only in the USA), which combined the portable cassette deck with a radio tuner and speakers capable of producing significant sound levels. These devices became synonymous with urban youth culture in entertainment, which led to the somewhat derisive nickname "ghetto blaster." The boom box allowed people to enjoy music on the go and share it with friends. This also led to such cultural practises as breakdancing.
Applications for car stereos varied widely. Auto manufacturers in the U.S. typically would fit a cassette slot into their standard large radio faceplates. Europe and Asia would standardize on DIN and double DIN sized faceplates. In the 1980s, a high-end installation would have a Dolby AM/FM cassette deck, and they rendered the 8-track cartridge obsolete in car installations because of space, performance, and audio quality. In the 1990s and 2000s, as the cost of building CD players declined, many manufacturers offered a CD player. The CD player eventually supplanted the cassette deck as standard equipment, but some cars, especially those targeted at older drivers, were offered with the option of a cassette player, either by itself or sometimes in combination with a CD slot. Most new cars can still accommodate aftermarket cassette players, and the auxiliary jack advertised for MP3 players can be used also with portable cassette players, but 2011 is the first model year for which no manufacturer offered factory-installed cassette players.
Although the cassettes themselves were relatively durable, the players required regular maintenance to perform properly. Head cleaning may be done with long swabs, soaked with isopropyl alcohol, or cassette-shaped devices that could be inserted into a tape deck to remove buildup of iron-oxide from the heads, tape-drive capstan, and pinch-roller. Some otherwise normal recording tapes included sections of leader that could clean the tape heads. One of the concerns of the time however was the use of abrasive cleaning tape. Some of the cleaning tapes actually felt rough to the touch and were considered damaging to the heads.
Similarly shaped demagnetizers used magnets to degauss the deck, which kept sound from becoming distorted. A common mechanical problem occurred when a worn-out or dirty player rotated the supply spool faster than the take-up spool or failed to release the heads from the tape upon ejection. This would cause the magnetic tape to be fed out through the bottom of the cassette and become tangled in the mechanism of the player. In these cases the player was said to have "eaten" the tape, and it often destroyed the playability of the cassette altogether, and resulted in the common sight of tangled tape on the side of the road. Cutting blocks, analogous to those used for open-reel 1/4" tape were readily available, though used mainly for retrieving valued recordings, could be used to remove the damaged portion, or repair the break in the tape. Creation of compilations usually was by re-recording rather than splicing sections of songs because of the much smaller tape area.
Applications.
Audio.
The Compact Cassette originally was intended for use in dictation machines. In this capacity, some later-model cassette-based dictation machines could also run the tape at half speed (15⁄16 in/s) as playback quality was not critical. The cassette soon became a popular medium for distributing prerecorded music—initially through The Philips Record Company (and subsidiary labels Mercury and Philips in the U.S.). As of 2009, one still finds cassettes used for a variety of purposes, such as journalism, oral history, meeting and interview transcripts, audio-books, and so on. Police are still big buyers of cassette tapes, as some lawyers "don't trust digital technology for interviews". However, they are starting to give way to Compact Discs and more "compact" digital storage media. Prerecorded cassettes were also employed as a way of providing chemotherapy information to recently diagnosed cancer patients as studies found anxiety and fear often gets in the way of the information processing.
The cassette quickly found use in the commercial music industry. One artifact found on some commercially produced music cassettes was a sequence of test tones, called SDR (Super Dynamic Range, also called XDR, or eXtended Dynamic Range) soundburst tones, at the beginning and end of the tape, heard in order of low frequency to high. These were used during SDR/XDR's duplication process to gauge the quality of the tape medium. Many consumers objected to these tones since they were not part of the recorded music.
Broadcasting.
News reporting, documentary, and human interest broadcast operations often used portable Marantz PMD-series recorders for the recording of speech interviews. The key advantages of the Marantz portable recorders were the accommodation of professional microphones with an XLR connector, normal and double tape speed recording for extended frequency response, Dolby and dbx noise reduction systems, manual or automatic gain control (AGC) level control, peak limiter, multiple tape formulation accommodation, microphone and line level input connections, unbalanced RCA stereo input and output connections, live or tape monitoring, VU meter, headphone jack, playback pitch control, and operation on AC power or batteries optimized for long duration. Unlike less-expensive portable recorders that were limited to automatic gain control (AGC) recording schemes, the manual recording mode preserved low noise dynamics and avoided the automatic elevation of noise.
Home studio.
Beginning in 1979, Tascam introduced the Portastudio line of four- and eight-track cassette recorders for home-studio use.
In the simplest configuration, rather than playing a pair of stereo channels of each side of the cassette, the typical "portastudio" used a four-track tape head assembly to access four tracks on the cassette at once (with the tape playing in one direction). Each track could be recorded to, erased, or played back individually, allowing musicians to overdub themselves and create simple multitrack recordings easily, which could then be mixed down to a finished stereo version on an external machine. To increase audio quality in these recorders, the tape speed sometimes was doubled to 3¾ inches per second, in comparison to the standard 17⁄8 ips; additionally, dbx, Dolby B or Dolby C noise reduction provided compansion (compression of the signal during recording with equal and opposite expansion of the signal during playback), which yields increased dynamic range by lowering the noise level and increasing the maximum signal level before distortion occurs. Multi-track cassette recorders with built-in mixer and signal routing features ranged from easy-to-use beginner units up to professional-level recording systems.
Although professional musicians typically used multitrack cassette machines only as "sketchpads", Bruce Springsteen's "Nebraska" was recorded entirely on a four-track cassette tape.
Home dubbing.
Most cassettes were sold blank, and used for recording (dubbing) the owner's records (as backup, to play in the car, or to make mixtape compilations), their friends' records, or music from the radio. This practice was condemned by the music industry with such alarmist slogans as "Home Taping Is Killing Music". However, many claimed that the medium was ideal for spreading new music and would increase sales, and strongly defended their right to copy at least their own records onto tape. For a limited time in the early 1980s Island Records sold chromium dioxide “One Plus One” cassettes that had an album prerecorded on one side and the other was left blank for the purchaser to use. Cassettes were also a boon to people wishing to tape concerts (unauthorized or authorized) for sale or trade, a practice tacitly or overtly encouraged by many bands, such as the Grateful Dead, with a more counterculture bent. Blank cassettes also were an invaluable tool to spread the music of unsigned acts, especially within tape trading networks.
Various legal cases arose surrounding the dubbing of cassettes. In the UK, in the case of CBS Songs v. Amstrad (1988), the House of Lords found in favor of Amstrad that producing equipment that facilitated the dubbing of cassettes, in this case a high-speed twin cassette deck that allowed one cassette to be copied directly onto another, did not constitute the infringement of copyright. In a similar case, a shop owner who rented cassettes and sold blank tapes was not liable for copyright infringement even though it was clear that his customers likely were dubbing them at home. In both cases, the courts held that manufacturers and retailers could not be held accountable for the actions of consumers.
As an alternative to home dubbing, in the late 1980s, the Personics company installed booths in record stores across America that allowed customers to make personalized mixtapes from a digitally encoded back-catalogue with customised printed covers.
Institutional duplication.
Educational, religious, corporate, military, and broadcasting institutions benefited from messaging proliferation through accessibly priced duplicators, offered by Telex Communications, Wollensak, Sony, and others. The duplicators would operate at double (or greater) tape speed. Systems were scalable, enabling the user to purchase initially one "master" unit (typically with 3 "copy" bays) and add "slave" units for expanded duplication abilities.
Data recording.
The Hewlett Packard HP 9830 was one of the first desktop computers in the early 1970s to use automatically controlled cassette tapes for storage. It could save and find files by number, using a clear leader to detect the end of tape. These would be replaced by specialized cartridges, such as the 3M DC-series. Many of the earliest microcomputers implemented the Kansas City standard for digital data storage. Most home computers of the late 1970s and early 1980s could use cassettes for data storage as a cheaper alternative to floppy disks, though users often had to manually stop and start a cassette recorder. Even the first version of the IBM PC of 1981 had a cassette port and a command in its ROM BASIC programming language to use it. However, IBM cassette tape was seldom used, as by 1981 floppy drives had become commonplace in high-end machines.
Nintendo's Famicom had an available cassette data recorder, used for saving programs created with the hardware's version of BASIC and saving progress in some Famicom games. It was never released outside Japan, but the North American versions of some of the compatible games can technically be used with it, since many early copies of two of the games ("Excitebike" and "Wrecking Crew") are actually just the Japanese versions in a different shell, and Nintendo intentionally included compatibility in later prints of those titles and in other games since they were planning on releasing the recorder in the region anyway.
The typical encoding method for computer data was simple FSK, which resulted in data rates of typically 500 to 2000 bit/s, although some games used special, faster-loading routines, up to around 4000-bit/s. A rate of 2000-bit/s equates to a capacity of around 660 kilobytes per side of a 90-minute tape.
Among home computers that used primarily data cassettes for storage in the late 1970s were Commodore PET (early models of which had a cassette drive built-in), TRS-80 and Apple II, until the introduction of floppy disk drives and hard drives in the early 1980s made cassettes virtually obsolete for day-to-day use in the US. However, they remained in use on some portable systems such as the TRS-80 Model 100 line—often in microcassette form—until the early 1990s.
Floppy disk storage had become the standard data storage medium in the United States by the mid-1980s; for example, by 1983 the majority of software sold by Atari Program Exchange was on floppy. Cassette remained more popular for 8-bit computers such as the Commodore 64, ZX Spectrum, MSX, and Amstrad CPC 464 in many countries such as the United Kingdom (where 8-bit software was mostly sold on cassette until that market disappeared altogether in the early 1990s.) Reliability of cassettes for data storage is inconsistent, with gamers recalling repeated attempts to load video games. In some countries, including the United Kingdom, Poland, Hungary, and the Netherlands, cassette data storage was so popular that some radio stations would broadcast computer programs that listeners could record onto cassette and then load into their computer. See BASICODE.
The use of better modulation techniques, such as QPSK or those used in modern modems, combined with the improved bandwidth and signal to noise ratio of newer cassette tapes, allowed much greater capacities (up to 60 MB) and data transfer speeds of 10 to 17 kB/s on each cassette. They found use during the 1980s in data loggers for scientific and industrial equipment.
The cassette was adapted into what is called a streamer cassette, a version dedicated solely for data storage, and used chiefly for hard disk backups and other types of data. Streamer cassettes look almost exactly the same as a standard cassette, with the exception of having a notch about 1/4 inch wide and deep situated slightly off-center at the top edge of the cassette. Streamer cassettes also have a re-usable write-protect tab on only one side of the top edge of the cassette, with the other side of the top edge having either only an open rectangular hole, or no hole at all. This is due to the whole 1/8 inch width of the tape loaded inside being used by a streamer cassette drive for the writing and reading of data, hence only one side of the cassette being used. Streamer cassettes can hold anywhere from 50 to 160 megabytes of data.
Successors.
Elcaset was a short-lived audio format created by Sony in 1976 that was about twice the size, using larger tape and a higher recording speed. Unlike the original cassette, the Elcaset was designed from the outset for sound quality. It was never widely accepted, as the quality of standard cassette decks rapidly approached high fidelity.
Technical development of the cassette effectively ceased when digital recordable media, such as DAT and MiniDisc, were introduced in the late 1980s and early 1990s. Anticipating the switch from analog to digital format, major companies, such as Sony, shifted their focus to new media. In 1992, Philips introduced the Digital Compact Cassette (DCC), a DAT-like tape in almost the same shell as a Compact Cassette. It was aimed primarily at the consumer market. A DCC deck could play back both types of cassettes. Unlike DAT, which was accepted in professional usage because it could record without lossy compression effects, DCC failed in both home and mobile environments, and was discontinued in 1996.
The microcassette has in many cases supplanted the full-sized cassette in situations where voice-level fidelity is all that is required, such as in dictation machines and answering machines. Even these, in turn, are starting to give way to digital recorders of various descriptions. Since the rise of cheap CD-R discs, and flash memory-based digital audio players, the phenomenon of "home taping" has effectively switched to recording to Compact Disc or downloading from commercial or music sharing Web sites.
Because of consumer demand, the cassette has remained influential on design, more than a decade after its decline as a media mainstay. As the Compact Disc grew in popularity, cassette-shaped audio adapters were developed to provide an economical and clear way to obtain CD functionality in vehicles equipped with cassette decks. A portable CD player would have its analog line-out connected to the adapter, which in turn fed the signal to the head of the cassette deck. These adapters continue to function with MP3 players as well, and generally are more reliable than the FM transmitters that must be used to adapt CD players and digital audio players to car stereo systems. Digital audio players shaped as cassettes have also become available, which can be inserted into any cassette player and communicate with the head as if they were normal cassettes.

</doc>
<doc id="65883" url="https://en.wikipedia.org/wiki?curid=65883" title="Dave Thompson (comedian)">
Dave Thompson (comedian)

Dave Thompson (born 1 January 1960) is a British comedian and actor who hit the headlines in July 1997 after being removed from the role of Tinky Winky in the children's television series "Teletubbies" after 70 episodes. The BBC said in a letter to Thompson that his "interpretation of the role was not acceptable".
In an interview for The Pod Delusion, Thompson revealed that his contract had already been terminated at the end of the first season's filming – some time before the media allegations concerning Tinky Winky had emerged. He also claimed that a number of other Ragdoll Productions staff had been fired at the same time, the implication being that the mainstream media account of his termination was incorrect.
Thompson recounted bursting into tears upon learning the news of his termination. In interviews, he supposed that the fact his voice was dubbed, unlike the other actors, was an indication of dissatisfaction over his performance.
Thompson later appeared in the 2000 Ben Elton film "Maybe Baby" as Dave the Comedian/Mrs. Furblob. He also appeared in Harry Hill's live tour "Hooves" as the minor characters of the Horse, the ballboy and the Greek man who grabs Harry's neck. The original role of Thompson for the show was to carry Harry onto the stage at the beginning of the performance, but was unable to do so throughout the tour due to having a bad back. Thompson made brief appearances in each night of the "Hooves" tour.

</doc>
<doc id="65886" url="https://en.wikipedia.org/wiki?curid=65886" title="Microphone">
Microphone

A microphone, colloquially nicknamed mic or mike (), is an acoustic-to-electric transducer or sensor that converts sound into an electrical signal. Electromagnetic transducers facilitate the conversion of acoustic signals into electrical signals. Microphones are used in many applications such as telephones, hearing aids, public address systems for concert halls and public events, motion picture production, live and recorded audio engineering, two-way radios, megaphones, radio and television broadcasting, and in computers for recording voice, speech recognition, VoIP, and for non-acoustic purposes such as ultrasonic checking or knock sensors.
Most microphones today use electromagnetic induction (dynamic microphones), capacitance change (condenser microphones) or piezoelectricity (piezoelectric microphones) to produce an electrical signal from air pressure variations. Microphones typically need to be connected to a preamplifier before the signal can be amplified with an audio power amplifier and a speaker or recorded.
History.
In order to speak to larger groups of people, there was a desire to increase the volume of the spoken word. The earliest known device to achieve this dates to 600 BC with the invention of masks with specially designed mouth openings that acoustically augmented the voice in amphitheatres. In 1665, the English physicist Robert Hooke was the first to experiment with a medium other than air with the invention of the "lovers' telephone" made of stretched wire with a cup attached at each end.
German inventor Johann Philipp Reis designed an early sound transmitter that used a metallic strip attached to a vibrating membrane that would produce intermittent current. Better results were achieved with the "liquid transmitter" design in Scottish-American Alexander Graham Bell's telephone of 1876 – the diaphragm was attached to a conductive rod in an acid solution. These systems, however, gave a very poor sound quality.
Carbon microphone.
The first microphone that enabled proper voice telephony was the (loose-contact) carbon microphone (then called transmitter). This was independently developed by David Edward Hughes in England and Emile Berliner and Thomas Edison in the US. Although Edison was awarded the first patent (after a long legal dispute) in mid-1877, Hughes had demonstrated his working device in front of many witnesses some years earlier, and most historians credit him with its invention.
Hughes' device used loosely packed carbon granules – the varying pressure exerted on the granules by the diaphragm from the acoustic waves caused the resistance of the carbon to vary proportionally, allowing a relatively accurate electrical reproduction of the sound signal. He demonstrated his apparatus to the Royal Society by magnifying the sound of insects scratching through a sound box. Contrary to Edison, Hughes decided not to take out a patent; instead he gave his invention as a gift to the world.
The carbon microphone is the direct prototype of today's microphones and was critical in the development of telephony, broadcasting and the recording industries.
Thomas Edison refined the carbon microphone into his carbon-button transmitter of 1886. This microphone was employed at the first ever radio broadcast, a performance at the New York Metropolitan Opera House in 1910.
Further developments.
In 1916, C. Wente of Bell Labs developed the next breakthrough with the first condenser microphone.
In 1923, the first practical moving coil microphone was built. "The Marconi Skykes" or "magnetophon", developed by Captain H. J. Round, was the standard for BBC studios in London. This was improved in 1930 by Alan Blumlein and Herbert Holman who released the HB1A and was the best standard of the day.
In the same year, the ribbon microphone was introduced, another electromagnetic type, believed to have been developed by Harry F. Olson, who essentially reverse-engineered a ribbon speaker. Over the years these microphones were developed by several companies, most notably RCA that made large advancements in pattern control, to give the microphone directionality. With television and film technology booming there was demand for high fidelity microphones and greater directionality. Electro-Voice responded with their Academy Award-winning shotgun microphone in 1963.
During the second half of 20th century development advanced quickly with the Shure Brothers bringing out the SM58 and SM57. Digital was pioneered by Milab in 1999 with the DM-1001. The latest research developments include the use of fibre optics, lasers and interferometers.
Components.
The sensitive transducer element of a microphone is called its "element" or "capsule." Except in thermophone based microphones, sound is first converted to mechanical motion by means of a diaphragm, the motion of which is then converted to an electrical signal. A complete microphone also includes a housing, some means of bringing the signal from the element to other equipment, and often an electronic circuit to adapt the output of the capsule to the equipment being driven. A wireless microphone contains a radio transmitter.
Varieties.
Microphones are referred to by their transducer principle, such as condenser, dynamic, etc., and by their directional characteristics. Sometimes other characteristics such as diaphragm size, intended use or orientation of the principal sound input to the principal axis (end- or side-address) of the microphone are used to describe the microphone.
Condenser microphone.
The condenser microphone, invented at Bell Labs in 1916 by E. C. Wente, is also called a capacitor microphone or electrostatic microphone—capacitors were historically called condensers. Here, the diaphragm acts as one plate of a capacitor, and the vibrations produce changes in the distance between the plates. There are two types, depending on the method of extracting the audio signal from the transducer: DC-biased microphones, and radio frequency (RF) or high frequency (HF) condenser microphones. With a DC-biased microphone, the plates are biased with a fixed charge ("Q"). The voltage maintained across the capacitor plates changes with the vibrations in the air, according to the capacitance equation (C = ), where Q = charge in coulombs, C = capacitance in farads and V = potential difference in volts. The capacitance of the plates is inversely proportional to the distance between them for a parallel-plate capacitor. The assembly of fixed and movable plates is called an "element" or "capsule".
A nearly constant charge is maintained on the capacitor. As the capacitance changes, the charge across the capacitor does change very slightly, but at audible frequencies it is sensibly constant. The capacitance of the capsule (around 5 to 100 pF) and the value of the bias resistor (100 MΩ to tens of GΩ) form a filter that is high-pass for the audio signal, and low-pass for the bias voltage. Note that the time constant of an RC circuit equals the product of the resistance and capacitance.
Within the time-frame of the capacitance change (as much as 50 ms at 20 Hz audio signal), the charge is practically constant and the voltage across the capacitor changes instantaneously to reflect the change in capacitance. The voltage across the capacitor varies above and below the bias voltage. The voltage difference between the bias and the capacitor is seen across the series resistor. The voltage across the resistor is amplified for performance or recording. In most cases, the electronics in the microphone itself contribute no voltage gain as the voltage differential is quite significant, up to several volts for high sound levels. Since this is a very high impedance circuit, current gain only is usually needed, with the voltage remaining constant.
RF condenser microphones use a comparatively low RF voltage, generated by a low-noise oscillator. The signal from the oscillator may either be amplitude modulated by the capacitance changes produced by the sound waves moving the capsule diaphragm, or the capsule may be part of a resonant circuit that modulates the frequency of the oscillator signal. Demodulation yields a low-noise audio frequency signal with a very low source impedance. The absence of a high bias voltage permits the use of a diaphragm with looser tension, which may be used to achieve wider frequency response due to higher compliance. The RF biasing process results in a lower electrical impedance capsule, a useful by-product of which is that RF condenser microphones can be operated in damp weather conditions that could create problems in DC-biased microphones with contaminated insulating surfaces. The Sennheiser "MKH" series of microphones use the RF biasing technique.
Condenser microphones span the range from telephone transmitters through inexpensive karaoke microphones to high-fidelity recording microphones. They generally produce a high-quality audio signal and are now the popular choice in laboratory and recording studio applications. The inherent suitability of this technology is due to the very small mass that must be moved by the incident sound wave, unlike other microphone types that require the sound wave to do more work. They require a power source, provided either via microphone inputs on equipment as phantom power or from a small battery. Power is necessary for establishing the capacitor plate voltage, and is also needed to power the microphone electronics (impedance conversion in the case of electret and DC-polarized microphones, demodulation or detection in the case of RF/HF microphones). Condenser microphones are also available with two diaphragms that can be electrically connected to provide a range of polar patterns (see below), such as cardioid, omnidirectional, and figure-eight. It is also possible to vary the pattern continuously with some microphones, for example the Røde NT2000 or CAD M179.
A valve microphone is a condenser microphone that uses a vacuum tube (valve) amplifier. They remain popular with enthusiasts of tube sound.
Electret condenser microphone.
An electret microphone is a type of capacitor microphone invented by Gerhard Sessler and Jim West at Bell laboratories in 1962.
The externally applied charge described above under condenser microphones is replaced by a permanent charge in an electret material. An electret is a ferroelectric material that has been permanently electrically charged or "polarized". The name comes from "electr"ostatic and magn"et"; a static charge is embedded in an electret by alignment of the static charges in the material, much the way a magnet is made by aligning the magnetic domains in a piece of iron.
Due to their good performance and ease of manufacture, hence low cost, the vast majority of microphones made today are electret microphones; a semiconductor manufacturer estimates annual production at over one billion units. Nearly all cell-phone, computer, PDA and headset microphones are electret types. They are used in many applications, from high-quality recording and lavalier use to built-in microphones in small sound recording devices and telephones. Though electret microphones were once considered low quality, the best ones can now rival traditional condenser microphones in every respect and can even offer the long-term stability and ultra-flat response needed for a measurement microphone. Unlike other capacitor microphones, they require no polarizing voltage, but often contain an integrated preamplifier that does require power (often incorrectly called polarizing power or bias). This preamplifier is frequently phantom powered in sound reinforcement and studio applications. Monophonic microphones designed for personal computer (PC) use, sometimes called multimedia microphones, use a 3.5 mm plug as usually used, without power, for stereo; the ring, instead of carrying the signal for a second channel, carries power via a resistor from (normally) a 5 V supply in the computer. Stereophonic microphones use the same connector; there is no obvious way to determine which standard is used by equipment and microphones.
Only the best electret microphones rival good DC-polarized units in terms of noise level and quality; electret microphones lend themselves to inexpensive mass-production, while inherently expensive non-electret condenser microphones are made to higher quality.
Dynamic microphone.
Dynamic microphones (also known as magneto-dynamic microphones) work via electromagnetic induction. They are robust, relatively inexpensive and resistant to moisture. This, coupled with their potentially high gain before feedback, makes them ideal for on-stage use.
Moving-coil microphones use the same dynamic principle as in a loudspeaker, only reversed. A small movable induction coil, positioned in the magnetic field of a permanent magnet, is attached to the diaphragm. When sound enters through the windscreen of the microphone, the sound wave moves the diaphragm. When the diaphragm vibrates, the coil moves in the magnetic field, producing a varying current in the coil through electromagnetic induction. A single dynamic membrane does not respond linearly to all audio frequencies. Some microphones for this reason utilize multiple membranes for the different parts of the audio spectrum and then combine the resulting signals. Combining the multiple signals correctly is difficult and designs that do this are rare and tend to be expensive. There are on the other hand several designs that are more specifically aimed towards isolated parts of the audio spectrum. The AKG D 112, for example, is designed for bass response rather than treble. In audio engineering several kinds of microphones are often used at the same time to get the best results.
Ribbon microphone.
Ribbon microphones use a thin, usually corrugated metal ribbon suspended in a magnetic field. The ribbon is electrically connected to the microphone's output, and its vibration within the magnetic field generates the electrical signal. Ribbon microphones are similar to moving coil microphones in the sense that both produce sound by means of magnetic induction. Basic ribbon microphones detect sound in a bi-directional (also called figure-eight, as in the diagram below) pattern because the ribbon is open on both sides. Also, because the ribbon is much less mass it responds to the air velocity rather than the sound pressure. Though the symmetrical front and rear pickup can be a nuisance in normal stereo recording, the high side rejection can be used to advantage by positioning a ribbon microphone horizontally, for example above cymbals, so that the rear lobe picks up only sound from the cymbals. Crossed figure 8, or Blumlein pair, stereo recording is gaining in popularity, and the figure-eight response of a ribbon microphone is ideal for that application.
Other directional patterns are produced by enclosing one side of the ribbon in an acoustic trap or baffle, allowing sound to reach only one side. The classic RCA Type 77-DX microphone has several externally adjustable positions of the internal baffle, allowing the selection of several response patterns ranging from "figure-eight" to "unidirectional". Such older ribbon microphones, some of which still provide high quality sound reproduction, were once valued for this reason, but a good low-frequency response could only be obtained when the ribbon was suspended very loosely, which made them relatively fragile. Modern ribbon materials, including new nanomaterials have now been introduced that eliminate those concerns, and even improve the effective dynamic range of ribbon microphones at low frequencies. Protective wind screens can reduce the danger of damaging a vintage ribbon, and also reduce plosive artifacts in the recording. Properly designed wind screens produce negligible treble attenuation. In common with other classes of dynamic microphone, ribbon microphones don't require phantom power; in fact, this voltage can damage some older ribbon microphones. Some new modern ribbon microphone designs incorporate a preamplifier and, therefore, do require phantom power, and circuits of modern passive ribbon microphones, "i.e.", those without the aforementioned preamplifier, are specifically designed to resist damage to the ribbon and transformer by phantom power. Also there are new ribbon materials available that are immune to wind blasts and phantom power.
Carbon microphone.
A carbon microphone, also known as a carbon button microphone (or sometimes just a button microphone), uses a capsule or button containing carbon granules pressed between two metal plates like the Berliner and Edison microphones. A voltage is applied across the metal plates, causing a small current to flow through the carbon. One of the plates, the diaphragm, vibrates in sympathy with incident sound waves, applying a varying pressure to the carbon. The changing pressure deforms the granules, causing the contact area between each pair of adjacent granules to change, and this causes the electrical resistance of the mass of granules to change. The changes in resistance cause a corresponding change in the current flowing through the microphone, producing the electrical signal. Carbon microphones were once commonly used in telephones; they have extremely low-quality sound reproduction and a very limited frequency response range, but are very robust devices. The Boudet microphone, which used relatively large carbon balls, was similar to the granule carbon button microphones.
Unlike other microphone types, the carbon microphone can also be used as a type of amplifier, using a small amount of sound energy to control a larger amount of electrical energy. Carbon microphones found use as early telephone repeaters, making long distance phone calls possible in the era before vacuum tubes. These repeaters worked by mechanically coupling a magnetic telephone receiver to a carbon microphone: the faint signal from the receiver was transferred to the microphone, where it modulated a stronger electric current, producing a stronger electrical signal to send down the line. One illustration of this amplifier effect was the oscillation caused by feedback, resulting in an audible squeal from the old "candlestick" telephone if its earphone was placed near the carbon microphone.
Piezoelectric microphone.
A crystal microphone or piezo microphone uses the phenomenon of piezoelectricity—the ability of some materials to produce a voltage when subjected to pressure—to convert vibrations into an electrical signal. An example of this is potassium sodium tartrate, which is a piezoelectric crystal that works as a transducer, both as a microphone and as a slimline loudspeaker component. Crystal microphones were once commonly supplied with vacuum tube (valve) equipment, such as domestic tape recorders. Their high output impedance matched the high input impedance (typically about 10 megohms) of the vacuum tube input stage well. They were difficult to match to early transistor equipment, and were quickly supplanted by dynamic microphones for a time, and later small electret condenser devices. The high impedance of the crystal microphone made it very susceptible to handling noise, both from the microphone itself and from the connecting cable.
Piezoelectric transducers are often used as contact microphones to amplify sound from acoustic musical instruments, to sense drum hits, for triggering electronic samples, and to record sound in challenging environments, such as underwater under high pressure. Saddle-mounted pickups on acoustic guitars are generally piezoelectric devices that contact the strings passing over the saddle. This type of microphone is different from magnetic coil pickups commonly visible on typical electric guitars, which use magnetic induction, rather than mechanical coupling, to pick up vibration.
Fiber optic microphone.
A fiber optic microphone converts acoustic waves into electrical signals by sensing changes in light intensity, instead of sensing changes in capacitance or magnetic fields as with conventional microphones.
During operation, light from a laser source travels through an optical fiber to illuminate the surface of a reflective diaphragm. Sound vibrations of the diaphragm modulate the intensity of light reflecting off the diaphragm in a specific direction. The modulated light is then transmitted over a second optical fiber to a photo detector, which transforms the intensity-modulated light into analog or digital audio for transmission or recording. Fiber optic microphones possess high dynamic and frequency range, similar to the best high fidelity conventional microphones.
Fiber optic microphones do not react to or influence any electrical, magnetic, electrostatic or radioactive fields (this is called EMI/RFI immunity). The fiber optic microphone design is therefore ideal for use in areas where conventional microphones are ineffective or dangerous, such as inside industrial turbines or in magnetic resonance imaging (MRI) equipment environments.
Fiber optic microphones are robust, resistant to environmental changes in heat and moisture, and can be produced for any directionality or impedance matching. The distance between the microphone's light source and its photo detector may be up to several kilometers without need for any preamplifier or other electrical device, making fiber optic microphones suitable for industrial and surveillance acoustic monitoring.
Fiber optic microphones are used in very specific application areas such as for infrasound monitoring and noise-canceling. They have proven especially useful in medical applications, such as allowing radiologists, staff and patients within the powerful and noisy magnetic field to converse normally, inside the MRI suites as well as in remote control rooms. Other uses include industrial equipment monitoring and audio calibration and measurement, high-fidelity recording and law enforcement.
Laser microphone.
Laser microphones are often portrayed in movies as spy gadgets, because they can be used to pick up sound at a distance from the microphone equipment. A laser beam is aimed at the surface of a window or other plane surface that is affected by sound. The vibrations of this surface change the angle at which the beam is reflected, and the motion of the laser spot from the returning beam is detected and converted to an audio signal.
In a more robust and expensive implementation, the returned light is split and fed to an interferometer, which detects movement of the surface by changes in the optical path length of the reflected beam. The former implementation is a tabletop experiment; the latter requires an extremely stable laser and precise optics.
A new type of laser microphone is a device that uses a laser beam and smoke or vapor to detect sound vibrations in free air. On 25 August 2009, U.S. patent 7,580,533 issued for a Particulate Flow Detection Microphone based on a laser-photocell pair with a moving stream of smoke or vapor in the laser beam's path. Sound pressure waves cause disturbances in the smoke that in turn cause variations in the amount of laser light reaching the photo detector. A prototype of the device was demonstrated at the 127th Audio Engineering Society convention in New York City from 9 through 12 October 2009.
Liquid microphone.
Early microphones did not produce intelligible speech, until Alexander Graham Bell made improvements including a variable-resistance microphone/transmitter. Bell's liquid transmitter consisted of a metal cup filled with water with a small amount of sulfuric acid added. A sound wave caused the diaphragm to move, forcing a needle to move up and down in the water. The electrical resistance between the wire and the cup was then inversely proportional to the size of the water meniscus around the submerged needle. Elisha Gray filed a caveat for a version using a brass rod instead of the needle. Other minor variations and improvements were made to the liquid microphone by Majoranna, Chambers, Vanni, Sykes, and Elisha Gray, and one version was patented by Reginald Fessenden in 1903. These were the first working microphones, but they were not practical for commercial application. The famous first phone conversation between Bell and Watson took place using a liquid microphone.
MEMS microphone.
The MEMS (MicroElectrical-Mechanical System) microphone is also called a microphone chip or silicon microphone. A pressure-sensitive diaphragm is etched directly into a silicon wafer by MEMS processing techniques, and is usually accompanied with integrated preamplifier. Most MEMS microphones are variants of the condenser microphone design. Digital MEMS microphones have built in analog-to-digital converter (ADC) circuits on the same CMOS chip making the chip a digital microphone and so more readily integrated with modern digital products. Major manufacturers producing MEMS silicon microphones are Wolfson Microelectronics (WM7xxx) now Cirrus Logic, InvenSense (product line sold by Analog Devices ), Akustica (AKU200x), Infineon (SMM310 product), Knowles Electronics, Memstech (MSMx), NXP Semiconductors (division bought by Knowles ), Sonion MEMS, Vesper, AAC Acoustic Technologies, and Omron.
More recently, there has been increased interest and research into making piezoelectric MEMS microphones which are a significant architectural and material change from existing condenser style MEMS designs.
Speakers as microphones.
A loudspeaker, a transducer that turns an electrical signal into sound waves, is the functional opposite of a microphone. Since a conventional speaker is constructed much like a dynamic microphone (with a diaphragm, coil and magnet), speakers can actually work "in reverse" as microphones. The result, though, is a microphone with poor quality, limited frequency response (particularly at the high end), and poor sensitivity. In practical use, speakers are sometimes used as microphones in applications where high quality and sensitivity are not needed such as intercoms, walkie-talkies or video game voice chat peripherals, or when conventional microphones are in short supply.
However, there is at least one other practical application of this principle: Using a medium-size woofer placed closely in front of a "kick drum" (bass drum) in a drum set to act as a microphone. The use of relatively large speakers to transduce low frequency sound sources, especially in music production, is becoming fairly common. A product example of this type of device is the Yamaha Subkick, a woofer shock-mounted into a 10" drum shell used in front of kick drums. Since a relatively massive membrane is unable to transduce high frequencies, placing a speaker in front of a kick drum is often ideal for reducing cymbal and snare bleed into the kick drum sound. Less commonly, microphones themselves can be used as speakers, almost always as tweeters. Microphones, however, are not designed to handle the power that speaker components are routinely required to cope with. One instance of such an application was the STC microphone-derived 4001 super-tweeter, which was successfully used in a number of high quality loudspeaker systems from the late 1960s to the mid-70s.
Capsule design and directivity.
The inner elements of a microphone are the primary source of differences in directivity. A pressure microphone uses a diaphragm between a fixed internal volume of air and the environment, and responds uniformly to pressure from all directions, so it is said to be omnidirectional. A pressure-gradient microphone uses a diaphragm that is at least partially open on both sides. The pressure difference between the two sides produces its directional characteristics. Other elements such as the external shape of the microphone and external devices such as interference tubes can also alter a microphone's directional response. A pure pressure-gradient microphone is equally sensitive to sounds arriving from front or back, but insensitive to sounds arriving from the side because sound arriving at the front and back at the same time creates no gradient between the two. The characteristic directional pattern of a pure pressure-gradient microphone is like a figure-8. Other polar patterns are derived by creating a capsule that combines these two effects in different ways. The cardioid, for instance, features a partially closed backside, so its response is a combination of pressure and pressure-gradient characteristics.
Microphone polar patterns.
(Microphone facing top of page in diagram, parallel to page):
A microphone's directionality or polar pattern indicates how sensitive it is to sounds arriving at different angles about its central axis. The polar patterns illustrated above represent the locus of points that produce the same signal level output in the microphone if a given sound pressure level (SPL) is generated from that point. How the physical body of the microphone is oriented relative to the diagrams depends on the microphone design. For large-membrane microphones such as in the Oktava (pictured above), the upward direction in the polar diagram is usually perpendicular to the microphone body, commonly known as "side fire" or "side address". For small diaphragm microphones such as the Shure (also pictured above), it usually extends from the axis of the microphone commonly known as "end fire" or "top/end address".
Some microphone designs combine several principles in creating the desired polar pattern. This ranges from shielding (meaning diffraction/dissipation/absorption) by the housing itself to electronically combining dual membranes.
Omnidirectional.
An omnidirectional (or nondirectional) microphone's response is generally considered to be a perfect sphere in three dimensions. In the real world, this is not the case. As with directional microphones, the polar pattern for an "omnidirectional" microphone is a function of frequency. The body of the microphone is not infinitely small and, as a consequence, it tends to get in its own way with respect to sounds arriving from the rear, causing a slight flattening of the polar response. This flattening increases as the diameter of the microphone (assuming it's cylindrical) reaches the wavelength of the frequency in question. Therefore, the smallest diameter microphone gives the best omnidirectional characteristics at high frequencies.
The wavelength of sound at 10 kHz is 1.4" (3.5 cm). The smallest measuring microphones are often 1/4" (6 mm) in diameter, which practically eliminates directionality even up to the highest frequencies. Omnidirectional microphones, unlike cardioids, do not employ resonant cavities as delays, and so can be considered the "purest" microphones in terms of low coloration; they add very little to the original sound. Being pressure-sensitive they can also have a very flat low-frequency response down to 20 Hz or below. Pressure-sensitive microphones also respond much less to wind noise and plosives than directional (velocity sensitive) microphones.
An example of a nondirectional microphone is the round black "eight ball".
Unidirectional.
A unidirectional microphone is primarily sensitive to sounds from only one direction. The diagram above illustrates a number of these patterns. The microphone faces upwards in each diagram. The sound intensity for a particular frequency is plotted for angles radially from 0 to 360°. (Professional diagrams show these scales and include multiple plots at different frequencies. The diagrams given here provide only an overview of typical pattern shapes, and their names.)
Cardioid, Hypercardioid, Supercardioid.
The most common unidirectional microphone is a cardioid microphone, so named because the sensitivity pattern is "heart-shaped", i.e. a cardioid. The cardioid family of microphones are commonly used as vocal or speech microphones, since they are good at rejecting sounds from other directions. In three dimensions, the cardioid is shaped like an apple centred around the microphone which is the "stem" of the apple. The cardioid response reduces pickup from the side and rear, helping to avoid feedback from the monitors. Since these directional transducer microphones achieve their patterns by sensing pressure gradient, putting them very close to the sound source (at distances of a few centimeters) results in a bass boost due to the increased gradient. This is known as the proximity effect. The SM58 has been the most commonly used microphone for live vocals for more than 50 years demonstrating the importance and popularity of cardioid mics.
A cardioid microphone is effectively a superposition of an omnidirectional and a figure-8 microphone; for sound waves coming from the back, the negative signal from the figure-8 cancels the positive signal from the omnidirectional element, whereas for sound waves coming from the front, the two add to each other. A hyper-cardioid microphone is similar, but with a slightly larger figure-8 contribution leading to a tighter area of front sensitivity and a smaller lobe of rear sensitivity. A super-cardioid microphone is similar to a hyper-cardioid, except there is more front pickup and less rear pickup. While any pattern between omni and figure 8 is possible by adjusting their mix, common definitions state that a hypercardioid is produced by combining them at a 3:1 ratio, producing nulls at 109.5°, while supercardioid is produced with a 5:3 ratio, with nulls at 126.9°.
Bi-directional.
"Figure 8" or bi-directional microphones receive sound equally from both the front and back of the element. Most ribbon microphones are of this pattern. In principle they do not respond to sound pressure at all, only to the "change" in pressure between front and back; since sound arriving from the side reaches front and back equally there is no difference in pressure and therefore no sensitivity to sound from that direction. In more mathematical terms, while omnidirectional microphones are scalar transducers responding to pressure from any direction, bi-directional microphones are vector transducers responding to the gradient along an axis normal to the plane of the diaphragm. This also has the effect of inverting the output polarity for sounds arriving from the back side.
Shotgun and parabolic microphones.
Shotgun microphones are the most highly directional of simple first-order unidirectional types. At low frequencies they have the classic polar response of a hypercardioid but at medium and higher frequencies an interference tube gives them an increased forward response. This is achieved by a process of cancellation of off-axis waves entering the longitudinal array of slots. A consequence of this technique is the presence of some rear lobes that vary in level and angle with frequency, and can cause some coloration effects. Due to the narrowness of their forward sensitivity, shotgun microphones are commonly used on television and film sets, in stadiums, and for field recording of wildlife.
Boundary or "PZM".
Several approaches have been developed for effectively using a microphone in less-than-ideal acoustic spaces, which often suffer from excessive reflections from one or more of the surfaces (boundaries) that make up the space. If the microphone is placed in, or very close to, one of these boundaries, the reflections from that surface have the same timing as the direct sound, thus giving the microphone a hemispherical polar pattern and improved intelligibility. Initially this was done by placing an ordinary microphone adjacent to the surface, sometimes in a block of acoustically transparent foam. Sound engineers Ed Long and Ron Wickersham developed the concept of placing the diaphragm parallel to and facing the boundary. While the patent has expired, "Pressure Zone Microphone" and "PZM" are still active trademarks of Crown International, and the generic term "boundary microphone" is preferred. While a boundary microphone was initially implemented using an omnidirectional element, it is also possible to mount a directional microphone close enough to the surface to gain some of the benefits of this technique while retaining the directional properties of the element. Crown's trademark on this approach is "Phase Coherent Cardioid" or "PCC," but there are other makers who employ this technique as well.
Application-specific designs.
A lavalier microphone is made for hands-free operation. These small microphones are worn on the body. Originally, they were held in place with a lanyard worn around the neck, but more often they are fastened to clothing with a clip, pin, tape or magnet. The lavalier cord may be hidden by clothes and either run to an RF transmitter in a pocket or clipped to a belt (for mobile use), or run directly to the mixer (for stationary applications).
A wireless microphone transmits the audio as a radio or optical signal rather than via a cable. It usually sends its signal using a small FM radio transmitter to a nearby receiver connected to the sound system, but it can also use infrared waves if the transmitter and receiver are within sight of each other.
A contact microphone picks up vibrations directly from a solid surface or object, as opposed to sound vibrations carried through air. One use for this is to detect sounds of a very low level, such as those from small objects or insects. The microphone commonly consists of a magnetic (moving coil) transducer, contact plate and contact pin. The contact plate is placed directly on the vibrating part of a musical instrument or other surface, and the contact pin transfers vibrations to the coil. Contact microphones have been used to pick up the sound of a snail's heartbeat and the footsteps of ants. A portable version of this microphone has recently been developed. A throat microphone is a variant of the contact microphone that picks up speech directly from a person's throat, which it is strapped to. This lets the device be used in areas with ambient sounds that would otherwise make the speaker inaudible.
A parabolic microphone uses a parabolic reflector to collect and focus sound waves onto a microphone receiver, in much the same way that a parabolic antenna (e.g. satellite dish) does with radio waves. Typical uses of this microphone, which has unusually focused front sensitivity and can pick up sounds from many meters away, include nature recording, outdoor sporting events, eavesdropping, law enforcement, and even espionage. Parabolic microphones are not typically used for standard recording applications, because they tend to have poor low-frequency response as a side effect of their design.
A stereo microphone integrates two microphones in one unit to produce a stereophonic signal. A stereo microphone is often used for broadcast applications or field recording where it would be impractical to configure two separate condenser microphones in a classic X-Y configuration (see microphone practice) for stereophonic recording. Some such microphones have an adjustable angle of coverage between the two channels.
A noise-canceling microphone is a highly directional design intended for noisy environments. One such use is in aircraft cockpits where they are normally installed as boom microphones on headsets. Another use is in live event support on loud concert stages for vocalists involved with live performances. Many noise-canceling microphones combine signals received from two diaphragms that are in opposite electrical polarity or are processed electronically. In dual diaphragm designs, the main diaphragm is mounted closest to the intended source and the second is positioned farther away from the source so that it can pick up environmental sounds to be subtracted from the main diaphragm's signal. After the two signals have been combined, sounds other than the intended source are greatly reduced, substantially increasing intelligibility. Other noise-canceling designs use one diaphragm that is affected by ports open to the sides and rear of the microphone, with the sum being a 16 dB rejection of sounds that are farther away. One noise-canceling headset design using a single diaphragm has been used prominently by vocal artists such as Garth Brooks and Janet Jackson. A few noise-canceling microphones are throat microphones.
Powering.
Microphones containing active circuitry, such as most condenser microphones, require power to operate the active components. The first of these used vacuum-tube circuits with a separate power supply unit, using a multi-pin cable and connector. With the advent of solid-state amplification, the power requirements were greatly reduced and it became practical to use the same cable conductors and connector for audio and power. During the 1960s several powering methods were developed, mainly in Europe. The two dominant methods were initially defined in German DIN 45595 as :de:Tonaderspeisung or T-power and DIN 45596 for phantom power. Since the 1980s, phantom power has become much more common, because the same input may be used for both powered and unpowered microphones. In consumer electronics such as DSLRs and camcorders, "plug-in power" is more common, for microphones using a 3.5 mm phone plug connector. Phantom, T-power and plug-in power are described in international standard IEC 61938.
Connectors.
The most common connectors used by microphones are:
Some microphones use other connectors, such as a 5-pin XLR, or mini XLR for connection to portable equipment. Some lavalier (or "lapel", from the days of attaching the microphone to the news reporters suit lapel) microphones use a proprietary connector for connection to a wireless transmitter, such as a radio pack. Since 2005, professional-quality microphones with USB connections have begun to appear, designed for direct recording into computer-based software.
Impedance-matching.
Microphones have an electrical characteristic called impedance, measured in ohms (Ω), that depends on the design. In passive microphones, this value describes the electrical resistance of the magnet coil (or similar mechanism). In active microphones, this value describes the output resistance of the amplifier circuitry. Typically, the "rated impedance" is stated. Low impedance is considered under 600 Ω. Medium impedance is considered between 600 Ω and 10 kΩ. High impedance is above 10 kΩ. Owing to their built-in amplifier, condenser microphones typically have an output impedance between 50 and 200 Ω.
The output of a given microphone delivers the same power whether it is low or high impedance. If a microphone is made in high and low impedance versions, the high impedance version has a higher output voltage for a given sound pressure input, and is suitable for use with vacuum-tube guitar amplifiers, for instance, which have a high input impedance and require a relatively high signal input voltage to overcome the tubes' inherent noise. Most professional microphones are low impedance, about 200 Ω or lower. Professional vacuum-tube sound equipment incorporates a transformer that steps up the impedance of the microphone circuit to the high impedance and voltage needed to drive the input tube. External matching transformers are also available that can be used in-line between a low impedance microphone and a high impedance input.
Low-impedance microphones are preferred over high impedance for two reasons: one is that using a high-impedance microphone with a long cable results in high frequency signal loss due to cable capacitance, which forms a low-pass filter with the microphone output impedance. The other is that long high-impedance cables tend to pick up more hum (and possibly radio-frequency interference (RFI) as well). Nothing is damaged if the impedance between microphone and other equipment is mismatched; the worst that happens is a reduction in signal or change in frequency response.
Some microphones are designed "not" to have their impedance matched by the load they are connected to. Doing so can alter their frequency response and cause distortion, especially at high sound pressure levels. Certain ribbon and dynamic microphones are exceptions, due to the designers' assumption of a certain load impedance being part of the internal electro-acoustical damping circuit of the microphone.
Digital microphone interface.
The AES 42 standard, published by the Audio Engineering Society, defines a digital interface for microphones. Microphones conforming to this standard directly output a digital audio stream through an XLR or XLD male connector, rather than producing an analog output. Digital microphones may be used either with new equipment with appropriate input connections that conform to the AES 42 standard, or else via a suitable interface box. Studio-quality microphones that operate in accordance with the AES 42 standard are now available from a number of microphone manufacturers.
Measurements and specifications.
Because of differences in their construction, microphones have their own characteristic responses to sound. This difference in response produces non-uniform phase and frequency responses. In addition, microphones are not uniformly sensitive to sound pressure, and can accept differing levels without distorting. Although for scientific applications microphones with a more uniform response are desirable, this is often not the case for music recording, as the non-uniform response of a microphone can produce a desirable coloration of the sound. There is an international standard for microphone specifications, but few manufacturers adhere to it. As a result, comparison of published data from different manufacturers is difficult because different measurement techniques are used. The Microphone Data Website has collated the technical specifications complete with pictures, response curves and technical data from the microphone manufacturers for every currently listed microphone, and even a few obsolete models, and shows the data for them all in one common format for ease of comparison.[http://www.microphone-data.com/]. Caution should be used in drawing any solid conclusions from this or any other published data, however, unless it is known that the manufacturer has supplied specifications in accordance with IEC 60268-4.
A frequency response diagram plots the microphone sensitivity in decibels over a range of frequencies (typically 20 Hz to 20 kHz), generally for perfectly on-axis sound (sound arriving at 0° to the capsule). Frequency response may be less informatively stated textually like so: "30 Hz–16 kHz ±3 dB". This is interpreted as meaning a nearly flat, linear, plot between the stated frequencies, with variations in amplitude of no more than plus or minus 3 dB. However, one cannot determine from this information how "smooth" the variations are, nor in what parts of the spectrum they occur. Note that commonly made statements such as "20 Hz–20 kHz" are meaningless without a decibel measure of tolerance. Directional microphones' frequency response varies greatly with distance from the sound source, and with the geometry of the sound source. IEC 60268-4 specifies that frequency response should be measured in "plane progressive wave" conditions (very far away from the source) but this is seldom practical. "Close talking" microphones may be measured with different sound sources and distances, but there is no standard and therefore no way to compare data from different models unless the measurement technique is described.
The self-noise or equivalent input noise level is the sound level that creates the same output voltage as the microphone does in the absence of sound. This represents the lowest point of the microphone's dynamic range, and is particularly important should you wish to record sounds that are quiet. The measure is often stated in dB(A), which is the equivalent loudness of the noise on a decibel scale frequency-weighted for how the ear hears, for example: "15 dBA SPL" (SPL means sound pressure level relative to 20 micropascals). The lower the number the better. Some microphone manufacturers state the noise level using ITU-R 468 noise weighting, which more accurately represents the way we hear noise, but gives a figure some 11–14 dB higher. A quiet microphone typically measures 20 dBA SPL or 32 dB SPL 468-weighted. Very quiet microphones have existed for years for special applications, such the Brüel & Kjaer 4179, with a noise level around 0 dB SPL. Recently some microphones with low noise specifications have been introduced in the studio/entertainment market, such as models from Neumann and Røde that advertise noise levels between 5–7 dBA. Typically this is achieved by altering the frequency response of the capsule and electronics to result in lower noise within the A-weighting curve while broadband noise may be increased.
The maximum SPL the microphone can accept is measured for particular values of total harmonic distortion (THD), typically 0.5%. This amount of distortion is generally inaudible, so one can safely use the microphone at this SPL without harming the recording. Example: "142 dB SPL peak (at 0.5% THD)". The higher the value, the better, although microphones with a very high maximum SPL also have a higher self-noise.
The clipping level is an important indicator of maximum usable level, as the 1% THD figure usually quoted under max SPL is really a very mild level of distortion, quite inaudible especially on brief high peaks. Clipping is much more audible. For some microphones the clipping level may be much higher than the max SPL.
The dynamic range of a microphone is the difference in SPL between the noise floor and the maximum SPL. If stated on its own, for example "120 dB", it conveys significantly less information than having the self-noise and maximum SPL figures individually.
Sensitivity indicates how well the microphone converts acoustic pressure to output voltage. A high sensitivity microphone creates more voltage and so needs less amplification at the mixer or recording device. This is a practical concern but is not directly an indication of the microphone's quality, and in fact the term sensitivity is something of a misnomer, "transduction gain" being perhaps more meaningful, (or just "output level") because true sensitivity is generally set by the noise floor, and too much "sensitivity" in terms of output level compromises the clipping level. There are two common measures. The (preferred) international standard is made in millivolts per pascal at 1 kHz. A higher value indicates greater sensitivity. The older American method is referred to a 1 V/Pa standard and measured in plain decibels, resulting in a negative value. Again, a higher value indicates greater sensitivity, so −60  dB is more sensitive than −70 dB.
Measurement microphones.
Some microphones are intended for testing speakers, measuring noise levels and otherwise quantifying an acoustic experience. These are calibrated transducers and are usually supplied with a calibration certificate that states absolute sensitivity against frequency. The quality of measurement microphones is often referred to using the designations "Class 1," "Type 2" etc., which are references not to microphone specifications but to sound level meters. A more comprehensive standard for the description of measurement microphone performance was recently adopted.
Measurement microphones are generally scalar sensors of pressure; they exhibit an omnidirectional response, limited only by the scattering profile of their physical dimensions. Sound intensity or sound power measurements require pressure-gradient measurements, which are typically made using arrays of at least two microphones, or with hot-wire anemometers.
Microphone calibration.
To take a scientific measurement with a microphone, its precise sensitivity must be known (in volts per pascal). Since this may change over the lifetime of the device, it is necessary to regularly calibrate measurement microphones. This service is offered by some microphone manufacturers and by independent certified testing labs. All microphone calibration is ultimately traceable to primary standards at a national measurement institute such as NPL in the UK, PTB in Germany and NIST in the United States, which most commonly calibrate using the reciprocity primary standard. Measurement microphones calibrated using this method can then be used to calibrate other microphones using comparison calibration techniques.
Depending on the application, measurement microphones must be tested periodically (every year or several months, typically) and after any potentially damaging event, such as being dropped (most such microphones come in foam-padded cases to reduce this risk) or exposed to sounds beyond the acceptable level.
Microphone array and array microphones.
A microphone array is any number of microphones operating in tandem. There are many applications:
Typically, an array is made up of omnidirectional microphones distributed about the perimeter of a space, linked to a computer that records and interprets the results into a coherent form.
Microphone windscreens.
Windscreens (or windshields – the terms are interchangeable) provide a method of reducing the effect of wind on microphones. While pop-screens give protection from unidirectional blasts, foam “hats” shield wind into the grille from all directions, and blimps / zeppelins / baskets entirely enclose the microphone and protect its body as well. This last point is important because, given the extreme low frequency content of wind noise, vibration induced in the housing of the microphone can contribute substantially to the noise output.
The shielding material used – wire gauze, fabric or foam – is designed to have a significant acoustic impedance. The relatively low particle-velocity air pressure changes that constitute sound waves can pass through with minimal attenuation, but higher particle-velocity wind is impeded to a far greater extent. Increasing the thickness of the material improves wind attenuation but also begins to compromise high frequency audio content. This limits the practical size of simple foam screens. While foams and wire meshes can be partly or wholly self supporting, soft fabrics and gauzes require stretching on frames, or laminating with coarser structural elements.
Since all wind noise is generated at the first surface the air hits, the greater the spacing between shield periphery and microphone capsule, the greater the noise attenuation. For an approximately spherical shield, attenuation increases by (approximately) the cube of that distance. Thus larger shields are always much more efficient than smaller ones. With full basket windshields there is an additional pressure chamber effect, first explained by Joerg Wuttke, which, for two-port (pressure gradient) microphones, allows the shield/microphone combination to act as a high-pass acoustic filter.
Since turbulence at a surface is the source of wind noise, reducing gross turbulence can add to noise reduction. Both aerodynamically smooth surfaces, and ones that prevent powerful vortices being generated, have been used successfully. Historically, artificial fur has proved very useful for this purpose since the fibres produce micro-turbulence and absorb energy silently. If not matted by wind and rain, the fur fibres are very transparent acoustically, but the woven or knitted backing can give significant attenuation. As a material it suffers from being difficult to manufacture with consistency, and to keep in pristine condition on location. Thus there is an interest (DPA 5100, Rycote Cyclone) to move away from its use.
In the studio and on stage, pop-screens and foam shields can be useful for reasons of hygiene, and protecting microphones from spittle and sweat. They can also be useful coloured idents. On location the basket shield can contain a suspension system to isolate the microphone from shock and handling noise.
Stating the efficiency of wind noise reduction is an inexact science, since the effect varies enormously with frequency, and hence with the bandwidth of the microphone and audio channel. At very low frequencies (10–100 Hz) where massive wind energy exists, reductions are important to avoid overloading of the audio chain – particularly the early stages. This can produce the typical “wumping” sound assocaited with wind, which is often syllabic muting of the audio due to LF peak limiting. At higher frequencies – 200 Hz to ~3 kHz – the aural sensitivity curve allows us to hear the effect of wind as an addition to the normal noise floor, even though it has a far lower energy content. Simple shields may allow the wind noise to be 10 dB less apparent; better ones can achieve nearer to a 50 dB reduction. However the acoustic transparency, particularly at HF, should also be indicated, since a very high level of wind attenuation could be associated with very muffled audio.

</doc>
<doc id="65888" url="https://en.wikipedia.org/wiki?curid=65888" title="Electromagnetic induction">
Electromagnetic induction

Electromagnetic induction is the production of an electromotive force across a conductor exposed to time varying magnetic fields. Michael Faraday, who mathematically described Faraday's law of induction, is generally credited with its discovery in 1831.
History.
Electromagnetic induction was first discovered by Michael Faraday, who made his discovery public in 1831. It was discovered independently by Joseph Henry in 1832.
In Faraday's first experimental demonstration (August 29, 1831), he wrapped two wires around opposite sides of an iron ring or "torus" (an arrangement similar to a modern toroidal transformer).
Faraday explained electromagnetic induction using a concept he called lines of force. However, scientists at the time widely rejected his theoretical ideas, mainly because they were not formulated mathematically. An exception was James Clerk Maxwell, who used Faraday's ideas as the basis of his quantitative electromagnetic theory. In Maxwell's model, the time varying aspect of electromagnetic induction is expressed as a differential equation, which Oliver Heaviside referred to as Faraday's law even though it is slightly different from Faraday's original formulation and does not describe motional EMF. Heaviside's version (see Maxwell–Faraday equation below) is the form recognized today in the group of equations known as Maxwell's equations.
Heinrich Lenz formulated the law named after him in 1834 to describe the "flux through the circuit". Lenz's law gives the direction of the induced EMF and current resulting from electromagnetic induction (elaborated upon in the examples below).
Following the understanding brought by these laws, many kinds of device employing magnetic induction have been invented.
Theory.
The law of physics describing the process of electromagnetic induction is known as Faraday's law of induction and the most widespread version of this law states that the induced electromotive force in any closed circuit is equal to the rate of change of the magnetic flux enclosed by the circuit. Or mathematically,
where formula_2 is the electromotive force (EMF) and ΦB is the magnetic flux. The direction of the electromotive force is given by Lenz's law. This version of Faraday's law strictly holds only when the closed circuit is a loop of infinitely thin wire, and is invalid in some other circumstances. A different version, the Maxwell–Faraday equation (discussed below), is valid in all circumstances.
For a tightly wound coil of wire, composed of "N" identical turns, each with the same magnetic flux going through them, the resulting EMF is given by
Faraday's law of induction makes use of the magnetic flux ΦB through a hypothetical surface Σ whose boundary is a wire loop. Since the wire loop may be moving, we write Σ("t") for the surface. The magnetic flux is defined by a surface integral:
where "d"A is an element of surface area of the moving surface Σ("t"), B is the magnetic field, and B·"d"A is a vector dot product (the infinitesimal amount of magnetic flux). In more visual terms, the magnetic flux through the wire loop is proportional to the number of magnetic flux lines that pass through the loop.
When the flux changes—because B changes, or because the wire loop is moved or deformed, or both—Faraday's law of induction says that the wire loop acquires an EMF, formula_2, defined as the energy available from a unit charge that has travelled once around the wire loop. Equivalently, it is the voltage that would be measured by cutting the wire to create an open circuit, and attaching a voltmeter to the leads.
According to the Lorentz force law (in SI units),
the EMF on a wire loop is:
where E is the electric field, B is the magnetic field (aka magnetic flux density, magnetic induction), dℓ is an infinitesimal arc length along the wire, and the line integral is evaluated along the wire (along the curve coincident with the shape of the wire).
Maxwell–Faraday equation.
The Maxwell–Faraday equation is a generalisation of Faraday's law that states that a time-varying magnetic field is always accompanied by a spatially-varying, non-conservative electric field, and vice versa. The Maxwell–Faraday equation is
(in SI units) where formula_8 is the curl operator and again E(r, "t") is the electric field and B(r, "t") is the magnetic field. These fields can generally be functions of position r and time "t".
The Maxwell–Faraday equation is one of the four Maxwell's equations, and therefore plays a fundamental role in the theory of classical electromagnetism. It can also be written in an integral form by the Kelvin-Stokes theorem:
where, as indicated in the figure:
Both dℓ and dA have a sign ambiguity; to get the correct sign, the right-hand rule is used, as explained in the article Kelvin-Stokes theorem. For a planar surface Σ, a positive path element "d"ℓ of curve ∂Σ is defined by the right-hand rule as one that points with the fingers of the right hand when the thumb points in the direction of the normal n to the surface Σ.
The integral around ∂Σ is called a "path integral" or "line integral".
Applications.
The principles of electromagnetic induction are applied in many devices and systems, including:
Electrical generator.
The EMF generated by Faraday's law of induction due to relative movement of a circuit and a magnetic field is the phenomenon underlying electrical generators. When a permanent magnet is moved relative to a conductor, or vice versa, an electromotive force is created. If the wire is connected through an electrical load, current will flow, and thus electrical energy is generated, converting the mechanical energy of motion to electrical energy. For example, the "drum generator" is based upon the figure to the right. A different implementation of this idea is the Faraday's disc, shown in simplified form on the right.
In the Faraday's disc example, the disc is rotated in a uniform magnetic field perpendicular to the disc, causing a current to flow in the radial arm due to the Lorentz force. It is interesting to understand how it arises that mechanical work is necessary to drive this current. When the generated current flows through the conducting rim, a magnetic field is generated by this current through Ampère's circuital law (labeled "induced B" in the figure). The rim thus becomes an electromagnet that resists rotation of the disc (an example of Lenz's law). On the far side of the figure, the return current flows from the rotating arm through the far side of the rim to the bottom brush. The B-field induced by this return current opposes the applied B-field, tending to "decrease" the flux through that side of the circuit, opposing the "increase" in flux due to rotation. On the near side of the figure, the return current flows from the rotating arm through the near side of the rim to the bottom brush. The induced B-field "increases" the flux on this side of the circuit, opposing the "decrease" in flux due to rotation. Thus, both sides of the circuit generate an EMF opposing the rotation. The energy required to keep the disc moving, despite this reactive force, is exactly equal to the electrical energy generated (plus energy wasted due to friction, Joule heating, and other inefficiencies). This behavior is common to all generators converting mechanical energy to electrical energy.
Electrical transformer.
When the electric current in a loop of wire changes, the changing current creates a changing magnetic field. A second wire in reach of this magnetic field will experience this change in magnetic field as a change in its coupled magnetic flux, "d" ΦB / "d t". Therefore, an electromotive force is set up in the second loop called the induced EMF or transformer EMF. If the two ends of this loop are connected through an electrical load, current will flow.
Magnetic flow meter.
Faraday's law is used for measuring the flow of electrically conductive liquids and slurries. Such instruments are called magnetic flow meters. The induced voltage ℇ generated in the magnetic field "B" due to a conductive liquid moving at velocity "v" is thus given by:
where ℓ is the distance between electrodes in the magnetic flow meter.
Eddy currents.
Conductors (of finite dimensions) moving through a uniform magnetic field, or stationary within a changing magnetic field, will have currents induced within them. These induced eddy currents can be undesirable, since they dissipate energy in the resistance of the conductor.
There are a number of methods employed to control these undesirable inductive effects.
Electromagnet laminations.
Eddy currents occur when a solid metallic mass is rotated in a magnetic field, because the outer portion of the metal cuts more lines of force than the inner portion, hence the induced electromotive force not being uniform, tends to set up currents between the points of greatest and least potential. Eddy currents consume a considerable amount of energy and often cause a harmful rise in temperature.
Only five laminations or plates are shown in this example, so as to show the subdivision of the eddy currents. In practical use, the number of laminations or punchings ranges from 40 to 66 per inch, and brings the eddy current loss down to about one percent. While the plates can be separated by insulation, the voltage is so low that the natural rust/oxide coating of the plates is enough to prevent current flow across the laminations.
This is a rotor approximately 20mm in diameter from a DC motor used in a Note the laminations of the electromagnet pole pieces, used to limit parasitic inductive losses.
Parasitic induction within inductors.
In this illustration, a solid copper bar inductor on a rotating armature is just passing under the tip of the pole piece N of the field magnet. Note the uneven distribution of the lines of force across the bar inductor. The magnetic field is more concentrated and thus stronger on the left edge of the copper bar (a,b) while the field is weaker on the right edge (c,d). Since the two edges of the bar move with the same velocity, this difference in field strength across the bar creates whorls or current eddies within the copper bar.
High current power-frequency devices, such as electric motors, generators and transformers, use multiple small conductors in parallel to break up the eddy flows that can form within large solid conductors. The same principle is applied to transformers used at higher than power frequency, for example, those used in switch-mode power supplies and the intermediate frequency coupling transformers of radio receivers.

</doc>
<doc id="65889" url="https://en.wikipedia.org/wiki?curid=65889" title="UVF">
UVF

UVF may refer to:

</doc>
<doc id="65890" url="https://en.wikipedia.org/wiki?curid=65890" title="Magnetic flux">
Magnetic flux

In physics, specifically electromagnetism, the magnetic flux (often denoted or ) through a surface is the surface integral of the normal component of the magnetic field B passing through that surface. The SI unit of magnetic flux is the weber (Wb) (in derived units: volt-seconds), and the CGS unit is the maxwell. Magnetic flux is usually measured with a fluxmeter, which contains measuring coils and electronics, that evaluates the change of voltage in the measuring coils to calculate the magnetic flux.
Description.
The magnetic interaction is described in terms of a vector field, where each point in space (and time) is associated with a vector that determines what force a moving charge would experience at that point (see Lorentz force). Since a vector field is quite difficult to visualize at first, in elementary physics one may instead visualize this field with field lines. The magnetic flux through some surface, in this simplified picture, is proportional to the number of field lines passing through that surface (in some contexts, the flux may be defined to be precisely the number of field lines passing through that surface; although technically misleading, this distinction is not important). Note that the magnetic flux is the "net" number of field lines passing through that surface; that is, the number passing through in one direction minus the number passing through in the other direction (see below for deciding in which direction the field lines carry a positive sign and in which they carry a negative sign).
In more advanced physics, the field line analogy is dropped and the magnetic flux is properly defined as the surface integral of the normal component of the magnetic field passing through a surface. If the magnetic field is constant, the magnetic flux passing through a surface of vector area S is
where B is the magnitude of the magnetic field (the magnetic flux density) having the unit of Wb/m2 (tesla), S is the area of the surface, and "θ" is the angle between the magnetic field lines and the normal (perpendicular) to S. For a varying magnetic field, we first consider the magnetic flux through an infinitesimal area element dS, where we may consider the field to be constant: 
A generic surface, S, can then be broken into infinitesimal elements and the total magnetic flux through the surface is then the surface integral
From the definition of the magnetic vector potential A and the fundamental theorem of the curl the magnetic flux may also be defined as:
where the line integral is taken over the boundary of the surface "S", which is denoted ∂"S".
Magnetic flux through a closed surface.
Gauss's law for magnetism, which is one of the four Maxwell's equations, states that the total magnetic flux through a closed surface is equal to zero. (A "closed surface" is a surface that completely encloses a volume(s) with no holes.) This law is a consequence of the empirical observation that magnetic monopoles have never been found.
In other words, Gauss's law for magnetism is the statement:
for any closed surface "S".
Magnetic flux through an open surface.
While the magnetic flux through a closed surface is always zero, the magnetic flux through an open surface need not be zero and is an important quantity in electromagnetism. For example, a change in the magnetic flux passing through a loop of conductive wire will cause an electromotive force, and therefore an electric current, in the loop. The relationship is given by Faraday's law:
where
The two equations for the EMF are, firstly, the work per unit charge done against the Lorentz force in moving a test charge around the (possibly moving) surface boundary ∂Σ and, secondly, as the change of magnetic flux through the open surface Σ. This equation is the principle behind an electrical generator.
Comparison with electric flux.
By way of contrast, Gauss's law for electric fields, another of Maxwell's equations, is
where
Note that the flux of E through a closed surface is "not" always zero; this indicates the presence of "electric monopoles", that is, free positive or negative charges.

</doc>
<doc id="65891" url="https://en.wikipedia.org/wiki?curid=65891" title="Boyle's law">
Boyle's law

Boyle's law (sometimes referred to as the Boyle–Mariotte law, or Mariotte's law) is an experimental gas law that describes how the pressure of a gas tends to increase as the volume of a gas decreases. A modern statement of Boyle's law is
The absolute pressure exerted by a given mass of an ideal gas is inversely proportional to the volume it occupies if the temperature and amount of gas remain unchanged within a closed system.
Mathematically, Boyle's law can be stated as
or
where "P" is the pressure of the gas, "V" is the volume of the gas, and "k" is a constant.
The equation states that product of pressure and volume is a constant for a given mass of confined gas as long as the temperature is constant. For comparing the same substance under two different sets of condition, the law can be usefully expressed as
The equation shows that, as volume increases, the pressure of the gas decreases in proportion. Similarly, as volume decreases, the pressure of the gas increases. The law was named after chemist and physicist Robert Boyle, who published the original law in 1662.
History.
This relationship between pressure and volume was first noted by Richard Towneley and Henry Power. Robert Boyle confirmed their discovery through experiments and published the results. According to Robert Gunther and other authorities, it was Boyle's assistant, Robert Hooke, who built the experimental apparatus. Boyle's law is based on experiments with air, which he considered to be a fluid of particles at rest in between small invisible springs. At that time, air was still seen as one of the four elements, but Boyle disagreed. Boyle's interest was probably to understand air as an essential element of life; for example, he published works on the growth of plants without air. Boyle used a closed J-shaped tube and after pouring mercury from one side he forced the air on the other side to contract under the pressure of mercury. After repeating the experiment several times and using different amounts of mercury he found that under controlled conditions, the pressure of a gas is inversely proportional to the volume occupied by it. The French physicist Edme Mariotte (1620–1684) discovered the same law independent of Boyle in 1679, but Boyle had already published it in 1662. Thus this law is sometimes referred to as Mariotte's law or the Boyle–Mariotte law. Later, in 1687 in the Philosophiæ Naturalis Principia Mathematica, Newton showed mathematically that if an elastic fluid consisting of particles at rest, between which are repulsive forces inversely proportional to their distance, the density would be directly proportional to the pressure, but this mathematical treatise is not the physical explanation for the observed relationship. Instead of a static theory a kinetic theory is needed, which was provided two centuries later by Maxwell and Boltzmann.
This law was the first physical law to be expressed in the form of an equation describing the dependence of two variable quantities.
Definition.
The law itself can be stated as follows:
Or Boyle's law is a gas law, stating that the pressure and volume of a gas have an inverse relationship, when temperature is held constant. If volume increases, then pressure decreases and vice versa, when temperature is held constant.
Therefore, when the volume is halved, the pressure is doubled; and if the volume is doubled, the pressure is halved.
Relation to kinetic theory and ideal gases.
Boyle’s law states that "at constant temperature" for a fixed mass, the absolute pressure and the volume of a gas are inversely proportional. The law can also be stated in a slightly different manner, that the product of absolute pressure and volume is always constant.
Most gases behave like ideal gases at moderate pressures and temperatures. The technology of the 17th century could not produce high pressures or low temperatures. Hence, the law was not likely to have deviations at the time of publication. As improvements in technology permitted higher pressures and lower temperatures, deviations from the ideal gas behavior became noticeable, and the relationship between pressure and volume can only be accurately described employing real gas theory. The deviation is expressed as the compressibility factor.
Boyle (and Mariotte) derived the law solely on experimental grounds. The law can also be derived theoretically based on the presumed existence of atoms and molecules and assumptions about motion and perfectly elastic collisions (see kinetic theory of gases). These assumptions were met with enormous resistance in the positivist scientific community at the time however, as they were seen as purely theoretical constructs for which there was not the slightest observational evidence.
Daniel Bernoulli in 1737-1738 derived Boyle's law using Newton's laws of motion with application on a molecular level. It remained ignored until around 1845, when John Waterston published a paper building the main precepts of kinetic theory; this was rejected by the Royal Society of England. Later works of James Prescott Joule, Rudolf Clausius and in particular Ludwig Boltzmann firmly established the kinetic theory of gases and brought attention to both the theories of Bernoulli and Waterston.
The debate between proponents of Energetics and Atomism led Boltzmann to write a book in 1898, which endured criticism up to his suicide in 1906. Albert Einstein in 1905 showed how kinetic theory applies to the Brownian motion of a fluid-suspended particle, which was confirmed in 1908 by Jean Perrin.
Equation.
The mathematical equation for Boyle's law is:
where:
So long as temperature remains constant the same amount of energy given to the system persists throughout its operation and therefore, theoretically, the value of "k" will remain constant. However, due to the derivation of pressure as perpendicular applied force and the probabilistic likelihood of collisions with other particles through collision theory, the application of force to a surface may not be infinitely constant for such values of "v", but will have a limit when differentiating such values over a given time. Forcing the volume "V" of the fixed quantity of gas to increase, keeping the gas at the initially measured temperature, the pressure "p" must decrease proportionally. Conversely, reducing the volume of the gas increases the pressure. Boyle's law is used to predict the result of introducing a change, in volume and pressure only, to the initial state of a fixed quantity of gas.
The initial and final volumes and pressures of the fixed amount of gas, where the initial and final temperatures are the same (heating or cooling will be required to meet this condition), are related by the equation:
Here "P"1 and "V"1 represent the original pressure and volume, respectively, and "P"2 and "V"2 represent the second pressure and volume.
Boyle's law, Charles's law, and Gay-Lussac's law form the combined gas law. The three gas laws in combination with Avogadro's law can be generalized by the ideal gas law.

</doc>
<doc id="65892" url="https://en.wikipedia.org/wiki?curid=65892" title="Weber">
Weber

Weber (, or ; German: ) is a surname of German origin, derived from the noun meaning "weaver". In some cases, following migration to English-speaking countries, it has been anglicised to the English surname 'Webber' or even 'Weaver'.
Notable people with the surname include:

</doc>
<doc id="65894" url="https://en.wikipedia.org/wiki?curid=65894" title="Electromotive force">
Electromotive force

Electromotive force, also called emf (denoted formula_1 and measured in volt), is the voltage developed by any source of electrical energy such as a battery or dynamo. It is generally defined as the electrical potential for a source in a circuit. A device that supplies electrical energy is called a seat of electromotive force or emf. Emfs convert chemical, mechanical, and other forms of energy into electrical energy. The product of such a device is also known as emf.
The word "force" in this case is not used to mean mechanical force, measured in newtons, but a potential, or energy per unit of charge, measured in volts.
In electromagnetic induction, emf can be defined around a closed loop as the electromagnetic work that would be done on a charge if it travels once around that loop.
(While the charge travels around the loop, it can simultaneously lose the energy gained via resistance into thermal energy.) For a time-varying magnetic flux linking a loop, the electric potential scalar field is not defined due to circulating electric vector field, but nevertheless an emf does work that can be measured as a virtual electric potential around that loop.
In the case of a two-terminal device (such as an electrochemical cell or electromagnetic generator) which is modeled as a Thévenin's equivalent circuit, the equivalent emf can be measured as the open-circuit potential difference or voltage between the two terminals. This potential difference can drive a current if an external circuit is attached to the terminals.
Devices that can provide emf include electrochemical cells, thermoelectric devices, solar cells, photodiodes, electrical generators, transformer and even Van de Graaff generators. In nature, emf is generated whenever magnetic field fluctuations occur through a surface. The shifting of the Earth's magnetic field during a geomagnetic storm, induces currents in the electrical grid as the lines of the magnetic field are shifted about and cut across the conductors.
In the case of a battery, the charge separation that gives rise to a voltage difference between the terminals is accomplished by chemical reactions at the electrodes that convert chemical potential energy into electromagnetic potential energy. A voltaic cell can be thought of as having a "charge pump" of atomic dimensions at each electrode, that is:
A source of emf can be thought of as a kind of "charge pump" that acts to move positive charge from a point of low potential through its interior to a point of high potential. … By chemical, mechanical or other means, the source of emf performs work "dW" on that charge to move it to the high potential terminal. The emf "ℰ" of the source is defined as the work "dW" done per charge "dq": "ℰ" = "dW/dq".
Around 1830, Michael Faraday established that the reactions at each of the two electrode–electrolyte interfaces provide the "seat of emf" for the voltaic cell, that is, these reactions drive the current and are not an endless source of energy as was initially thought. In the open-circuit case, charge separation continues until the electrical field from the separated charges is sufficient to arrest the reactions. Years earlier, Alessandro Volta, who had measured a contact potential difference at the metal–metal (electrode–electrode) interface of his cells, had held the incorrect opinion that contact alone (without taking into account a chemical reaction) was the origin of the emf.
In the case of an electrical generator, a time-varying magnetic field inside the generator creates an electric field via electromagnetic induction, 
which in turn creates a voltage difference between the generator terminals. Charge separation takes place within the generator, with electrons flowing away from one terminal and toward the other, until, in the open-circuit case, sufficient electric field builds up to make further charge separation impossible. Again the emf is countered by the electrical voltage due to charge separation. If a load is attached, this voltage can drive a current. The general principle governing the emf in such electrical machines is Faraday's law of induction.
Notation and units of measurement.
Electromotive force is often denoted by formula_1 or "ℰ" (script capital E, Unicode U+2130).
In a device without internal resistance, if an electric charge "Q" passes through that device, and gains an energy "W", the net emf for that device is the energy gained per unit charge, or "W"/"Q". Like other measures of energy per charge, emf has SI units of volts, equivalent to joules per coulomb.
Electromotive force in electrostatic units is the statvolt (in the centimeter gram second system of units equal in amount to an erg per electrostatic unit of charge).
Formal definitions of electromotive force.
"Inside" a source of emf that is open-circuited, the conservative electrostatic field created by separation of charge exactly cancels the forces producing the emf. Thus, the emf has the same value but opposite sign as the integral of the electric field aligned with an internal path between two terminals "A" and "B" of a source of emf in open-circuit condition (the path is taken from the negative terminal to the positive terminal to yield a positive emf, indicating work done on the electrons moving in the circuit). 
Mathematically:
where Ecs is the conservative electrostatic field created by the charge separation associated with the emf, dℓ is an element of the path from terminal "A" to terminal "B", and ‘·’ denotes the vector dot product. This equation applies only to locations "A" and "B" that are terminals, and does not apply to paths between points "A" and "B" with portions outside the source of emf. This equation involves the electrostatic electric field due to charge separation Ecs and does not involve (for example) any non-conservative component of electric field due to Faraday's law of induction.
In the case of a closed path in the presence of a varying magnetic field, the integral of the electric field around a closed loop may be nonzero; one common application of the concept of emf, known as ""induced emf"" is the voltage induced in a such a loop. The ""induced emf"" around a stationary closed path "C" is:
where now E is the entire electric field, conservative and non-conservative, and the integral is around an arbitrary but stationary closed curve "C" through which there is a varying magnetic field. The electrostatic field does not contribute to the net emf around a circuit because the electrostatic portion of the electric field is conservative (that is, the work done against the field around a closed path is zero).
This definition can be extended to arbitrary sources of emf and moving paths "C":
which is a conceptual equation mainly, because the determination of the "effective forces" is difficult.
Electromotive force in thermodynamics.
When multiplied by an amount of charge "dQ" the emf ℰ yields a thermodynamic work term ℰ"dQ" that is used in the formalism for the change in Gibbs energy when charge is passed in a battery:
where "G" is the Gibb's free energy, "S" is the entropy, "V" is the system volume, "P" is its pressure and "T" is its absolute temperature.
The combination ( ℰ, "Q" ) is an example of a conjugate pair of variables. At constant pressure the above relationship produces a Maxwell relation that links the change in open cell voltage with temperature "T" (a measurable quantity) to the change in entropy "S" when charge is passed isothermally and isobarically. The latter is closely related to the reaction entropy of the electrochemical reaction that lends the battery its power. This Maxwell relation is:
If a mole of ions goes into solution (for example, in a Daniell cell, as discussed below) the charge through the external circuit is:
where "n"0 is the number of electrons/ion, and "F"0 is the Faraday constant and the minus sign indicates discharge of the cell. Assuming constant pressure and volume, the thermodynamic properties of the cell are related strictly to the behavior of its emf by:
where Δ"H" is the enthalpy of reaction. The quantities on the right are all directly measurable.
Electromotive force and voltage difference.
An electrical voltage difference is sometimes called an emf. The points below illustrate the more formal usage, in terms of the distinction between emf and the voltage it generates:
In the case of an open circuit, the electric charge that has been separated by the mechanism generating the emf creates an electric field opposing the separation mechanism. For example, the chemical reaction in a voltaic cell stops when the opposing electric field at each electrode is strong enough to arrest the reactions. A larger opposing field can reverse the reactions in what are called "reversible" cells.
The electric charge that has been separated creates an electric potential difference that can be measured with a voltmeter between the terminals of the device. The magnitude of the emf for the battery (or other source) is the value of this 'open circuit' voltage. When the battery is charging or discharging, the emf itself cannot be measured directly using the external voltage because some voltage is lost inside the source.
It can, however, be inferred from a measurement of the current "I" and voltage difference "V", provided that the internal resistance "r" already has been measured: "ℰ" = "V" + "Ir".
Electromotive force generation.
Chemical sources.
[[File:Reaction path.JPG|thumb|380px|A typical reaction path requires the initial reactants to cross an energy barrier, enter an intermediate state and finally emerge in a lower energy configuration. If charge separation is involved, this energy difference can result in an emf. See Bergmann "et al." and Transition state.]]
The question of how batteries (galvanic cells) generate an emf is one that occupied scientists for most of the 19th century. The "seat of the electromotive force" was eventually determined by Walther Nernst to be primarily at the interfaces between the electrodes and the electrolyte.
Molecules are groups of atoms held together by chemical bonds, and these bonds consist of electrical forces between electrons (negative) and protons (positive). The molecule in isolation is a stable entity, but when different molecules are brought together, some types of molecules are able to steal electrons from others, resulting in charge separation. This redistribution of charge is accompanied by a change in energy of the system, and a reconfiguration of the atoms in the molecules. The gain of an electron is termed "reduction" and the loss of an electron is termed "oxidation". Reactions in which such electron exchange occurs (which are the basis for batteries) are called reduction-oxidation reactions or redox reactions. In a battery, one electrode is composed of material that gains electrons from the solute, and the other electrode loses electrons, because of these fundamental molecular attributes. The same behavior can be seen in atoms themselves, and their ability to steal electrons is referred to as their electronegativity.
As an example, a Daniell cell consists of a zinc anode (an electron collector), is oxidized as it dissolves into a zinc sulfate solution, the dissolving zinc leaving behind its electrons in the electrode according to the oxidation reaction ("s" = solid electrode; "aq" = aqueous solution):
The zinc sulfate is the electrolyte in that half cell. It is a solution which contains zinc cations formula_13, and sulfate anions formula_14 with charges that balance to zero.
In the other half cell, the copper cations in a copper sulfate electrolyte are drawn to the copper cathode to which they attach themselves as they adopt electrons from the copper electrode by the reduction reaction:
in effect leaving a deficit of electrons on the copper cathode. The difference of excess electrons on the anode and deficit of electrons on the cathode creates an electrical potential between the two electrodes. (A detailed discussion of the microscopic process of electron transfer between an electrode and the ions in an electrolyte may be found in Conway.)
If the cathode and anode are connected by an external conductor, electrons would pass through that external circuit (light bulb in figure), while the ions pass through the salt bridge to maintain charge balance until such a time as the anode and cathode reach electrical equilibrium of zero volts as chemical equilibrium is reached in the cell. In the process the zinc anode is dissolved while the copper electrode is plated with copper. The so-called "salt bridge" is not made of salt but could be made of material able to wick the cations and anions (salts) in the solutions, where the flow of positively charged cations along the "bridge" amounts to the same number of negative charges flowing in the opposite direction.
If the light bulb is removed (open circuit) the emf between the electrodes is opposed by the electric field due to charge separation, and the reactions stop.
For this particular cell chemistry, at 298 K (room temperature), the emf "ℰ" = 1.0934 V, with a temperature coefficient of d"ℰ"/d"T" = −4.53×10−4 V/K.
Voltaic cells.
Volta developed the voltaic cell about 1792, and presented his work March 20, 1800. Volta correctly identified the role of dissimilar electrodes in producing the voltage, but incorrectly dismissed any role for the electrolyte. Volta ordered the metals in a 'tension series', “that is to say in an order such that any one in the list becomes positive when in contact with any one that succeeds, but negative by contact with any one that precedes it.” A typical symbolic convention in a schematic of this circuit ( –||– ) would have a long electrode 1 and a short electrode 2, to indicate that electrode 1 dominates. Volta's law about opposing electrode emfs implies that, given ten electrodes (for example, zinc and nine other materials), 45 unique combinations of voltaic cells (10 × 9/2) can be created.
Electromotive force of cells.
The electromotive force produced by primary (single-use) and secondary (rechargeable) cells is usually of the order of a few volts. The figures quoted below are nominal, because emf varies according to the size of the load and the state of exhaustion of the cell.
Electromagnetic induction.
The principle of electromagnetic induction, noted above, states that a time-dependent magnetic field produces a circulating electric field. A time-dependent magnetic field can be produced either by motion of a magnet relative to a circuit, by motion of a circuit relative to another circuit (at least one of these must be carrying a current), or by changing the current in a fixed circuit. The effect on the circuit itself, of changing the current, is known as self-induction; the effect on another circuit is known as mutual induction.
For a given circuit, the electromagnetically induced emf is determined purely by the rate of change of the magnetic flux through the circuit according to Faraday's law of induction.
An emf is induced in a coil or conductor whenever there is change in the flux linkages. Depending on the way in which the changes are brought about, there are two types: When the conductor is moved in a stationary magnetic field to procure a change in the flux linkage, the emf is "statically induced". The electromotive force generated by motion is often referred to as "motional emf". When the change in flux linkage arises from a change in the magnetic field around the stationary conductor, the emf is "dynamically induced." The electromotive force generated by a time-varying magnetic field is often referred to as "transformer emf".
Contact potentials.
When solids of two different materials are in contact, thermodynamic equilibrium requires one of the solids assume a higher electrical potential than the other. This is called the "contact potential". Dissimilar metals in contact produce what is known also as a contact electromotive force or Galvani potential. The magnitude of this potential difference is often expressed as a difference in Fermi levels in the two solids when they are at charge neutrality, where the Fermi level (a name for the chemical potential of an electron system) describes the energy necessary to remove an electron from the body to some common point (such as ground). If there is an energy advantage in taking an electron from one body to the other, such a transfer will occur. The transfer causes a charge separation, with one body gaining electrons and the other losing electrons. This charge transfer causes a potential difference between the bodies, which partly cancels the potential originating from the contact, and eventually equilibrium is reached. At thermodynamic equilibrium, the Fermi levels are equal (the electron removal energy is identical) and there is now a built-in electrostatic potential between the bodies.
The original difference in Fermi levels, before contact, is referred to as the emf.
The contact potential cannot drive steady current through a load attached to its terminals because that current would involve a charge transfer. No mechanism exists to continue such transfer and, hence, maintain a current, once equilibrium is attained.
One might inquire why the contact potential does not appear in Kirchhoff's law of voltages as one contribution to the sum of potential drops. The customary answer is that any circuit involves not only a particular diode or junction, but also all the contact potentials due to wiring and so forth around the entire circuit. The sum of "all" the contact potentials is zero, and so they may be ignored in Kirchhoff's law.
Solar cell.
Operation of a solar cell can be understood from the equivalent circuit at right. Light, of sufficient energy (greater than the bandgap of the material), creates mobile electron–hole pairs in a semiconductor. Charge separation occurs because of a pre-existing electric field associated with the p-n junction in thermal equilibrium (a contact potential creates the field). This charge separation between positive holes and negative electrons across a p-n junction (a diode) yields a "forward voltage", the "photo voltage", between the illuminated diode terminals. As has been noted earlier in the terminology section, the photo "voltage" is sometimes referred to as the photo "emf", rather than distinguishing between the effect and the cause. The charge separation causes a photo voltage that drives current through any attached load.
The current available to the external circuit is limited by internal losses I0=ISH + I
D:
formula_16
Losses limit the current available to the external circuit. The light-induced charge separation eventually creates a current (called a forward current) ISH through the cell's junction in the direction opposite that the light is driving the current. In addition, the induced voltage tends to forward bias the junction. At high enough levels, this forward bias of the junction will cause a forward current, I
D in the diode opposite that induced by the light. Consequently, the greatest current is obtained under short-circuit conditions, and is denoted as "I"L (for light-induced current) in the equivalent circuit.
Approximately, this same current is obtained for forward voltages up to the point where the diode conduction becomes significant.
The current delivered by the illuminated diode, to the external circuit is:
where "I"0 is the reverse saturation current. Where the two parameters that depend on the solar cell construction and to some degree upon the voltage itself are "m", the ideality factor, and "kT/q" the thermal voltage (about 0.026 V at room temperature). This relation is plotted in the figure using a fixed value "m" = 2. Under open-circuit conditions (that is, as "I" = 0), the open-circuit voltage is the voltage at which forward bias of the junction is enough that the forward current completely balances the photocurrent. Solving the above for the voltage V and designating it the open-circuit voltage of the "I–V" equation as:
which is useful in indicating a logarithmic dependence of "V"oc upon the light-induced current. Typically, the open-circuit voltage is not more than about 0.5 V.
When driving a load, the photo voltage is variable. As shown in the figure, for a load resistance "R"L, the cell develops a voltage that is between the short-circuit value "V" = 0, "I" = "I"L and the open-circuit value "V"oc, "I" = 0, a value given by Ohm's law "V = I R"L, where the current "I" is the difference between the short-circuit current and current due to forward bias of the junction, as indicated by the equivalent circuit (neglecting the parasitic resistances).
In contrast to the battery, at current levels delivered to the external circuit near "I"L, the solar cell acts more like a "current source" rather than a voltage source( near vertical part of the two illustrated curves). 
The current drawn is nearly fixed over a range of load voltages, to one electron per converted photon. The quantum efficiency, or probability of getting an electron of photocurrent per incident photon, depends not only upon the solar cell itself, but upon the spectrum of the light.
The diode possesses a "built-in potential" due to the contact potential difference between the two different materials on either side of the junction. This built-in potential is established when the junction is manufactured and that voltage a by-product of thermodynamic equilibrium within the cell. Once established, this potential difference cannot drive a current, however, as connecting a load does not upset this equilibrium. In contrast, the accumulation of excess electrons in one region and of excess holes in another, due to illumination, results in a photo voltage that does drive a current when a load is attached to the illuminated diode. As noted above, this photo voltage also forward biases the junction, and so "reduces" the pre-existing field in the depletion region.

</doc>
<doc id="65896" url="https://en.wikipedia.org/wiki?curid=65896" title="Bidirectional">
Bidirectional

Bidirectional may refer to:

</doc>
<doc id="65897" url="https://en.wikipedia.org/wiki?curid=65897" title="Wire wrap">
Wire wrap

Wire wrap is a method to construct electronic circuit boards. Electronic components mounted on an insulating board are interconnected by lengths of insulated wire run between their terminals, with the connections made by wrapping several turns around a component lead or a socket pin. Wires can be wrapped by hand or by machine, and can be hand-modified afterwards. It was popular for large-scale manufacturing in the 60s and early 70s, and continues to be used for short runs and prototypes. The method eliminates the design and fabrication of a printed circuit board. Wire wrapping is unusual among other prototyping technologies since it allows for complex assemblies to be produced by automated equipment, but then easily repaired or modified by hand.
Wire wrap construction can produce assemblies which are more reliable than printed circuits: connections are less prone to fail due to vibration or physical stresses on the base board, and the lack of solder precludes soldering faults such as corrosion, cold joints and dry joints. The connections themselves are firmer and have lower electrical resistance due to cold welding of the wire to the terminal post at the corners.
Wire wrap was used for assembly of high frequency prototypes and small production runs, including gigahertz microwave circuits and super computers. It is unique among automated prototyping techniques in that wire lengths can be exactly controlled, and twisted pairs or magnetically shielded twisted quads can be routed together.
Wire wrap construction became popular around 1960 in circuit board manufacturing, and use has now sharply declined. Surface-mount technology has made the technique much less useful than in previous decades. Solder-less breadboards and the decreasing cost of professionally made PCBs have nearly eliminated this technology.
Overview.
A correctly made wire-wrap connection is seven turns of bare wire with half to one and a half turns of insulated wire at the bottom for strain relief. The square hard-gold-plated post thus forms 28 redundant contacts. The silver-plated wire coating cold-welds to the gold. If corrosion occurs, it occurs on the outside of the wire, not on the gas-tight contact where oxygen cannot penetrate to form oxides. A correctly designed wire-wrap tool applies up to twenty tons of force per square inch on each joint.
The electronic parts sometimes plug into sockets. The sockets are attached with cyanoacrylate (or silicone adhesive) to thin plates of glass-fiber-reinforced epoxy (fiberglass).
The sockets have square posts. The usual posts are square, high, and spaced at intervals. Premium posts are hard-drawn beryllium copper alloy plated with a of gold to prevent corrosion. Less-expensive posts are bronze with tin plating.
30 gauge silver-plated soft copper wire is insulated with a fluorocarbon that does not emit dangerous gases when heated. The most common insulation is "Kynar".
The 30 AWG Kynar wire is cut into standard lengths, then one inch of insulation is removed on each end.
A "wire wrap tool" has two holes. The wire and of insulated wire are placed in a hole near the edge of the tool. The hole in the center of the tool is placed over the post.
The tool is rapidly twisted. The result is that 1.5 to 2 turns of insulated wire are wrapped around the post, and above that, 7 to 9 turns of bare wire are wrapped around the post. The post has room for three such connections, although usually only one or two are needed. This permits manual wire-wrapping to be used for repairs.
The turn and a half of insulated wire helps prevent wire fatigue where it meets the post.
Above the turn of insulated wire, the bare wire wraps around the post. The corners of the post bite in with pressures of tons per square inch. This forces all the gases out of the area between the wire's silver plate and the post's gold or tin corners. Further, with 28 such connections (seven turns on a four-cornered post), a very reliable connection exists between the wire and the post. Furthermore, the corners of the posts are quite "sharp": they have a quite-small radius of curvature.
There are three ways of placing wires on a board.
In professionally built wire-wrap boards, long wires are placed first so that shorter wires mechanically secure the long wires. Also, to make an assembly more repairable, wires are applied in layers. The ends of each wire are always at the same height on the post, so that at most three wires need to be replaced to replace a wire. Also, to make the layers easier to see, they are made with different colors of insulation. In space-rated or airworthy wire-wrap assemblies, the wires are boxed, and may be conformally coated with wax to reduce vibration. Epoxy is never used for the coating because it makes an assembly unrepairable.
Application considerations.
Wire-wrap works well with digital circuits with few discrete components, but is less convenient for analog systems with many discrete resistors, capacitors or other components (such elements can be soldered to a header and plugged into a wire wrap socket). The sockets are an additional cost compared to directly inserting integrated circuits into a printed circuit board, and add size and mass to a system. Multiple strands of wire may introduce cross-talk between circuits, of little consequence for digital circuits but a limitation for analog systems. The interconnected wires can radiate electromagnetic interference and have less predictable impedance than a printed circuit board. Wire-wrap construction cannot provide the ground planes and power distribution planes possible with multilayer printed circuit boards, increasing the possibility of noise.
History.
Wire wrapping comes from the tradition of rope splicing. Early wire wrapping was performed manually; a slow and careful process. Wire wrapping was used for splices and for finishing cable ends in suspension bridge wires and other wire rope rigging, usually with a smaller diameter wire wrapped around a larger wire or bundle of wires. Such techniques were purely mechanical, to add strength or prevent fraying.
In the late 19th century, telegraph linemen developed methods of making a wire splice that would be strong mechanically and also carry electricity. The Western Union splice was the strongest of such wire-wrapped splices. The wraps could be coated in solder for even greater strength and to prevent oxidation between the wires.
Manually wrapped wires were common in early 20th century point-to-point electronic construction methods in which a strong connection was needed to hold the components in place. Wires were wrapped by hand around binding posts or spade lugs and then soldered.
Modern wire wrapping technology was developed after WWII at Bell Laboratories as a means of making electrical connections in a new relay being designed for use in the Bell Telephone system. A design team headed by Arthur C. Keller developed the “Keller Wrap Gun”, and the entire wrap system was passed over to Western Electric for execution. After a “make or buy” committee at Western Electric decided to have the hand tool manufactured by an outside vendor, Western Electric sent the tool contract out for bids. Keller Tool of Grand Haven, Michigan, a supplier of rotary hand tools to Western Electric, won the contract and made several design changes to make the tool easier to manufacture and to use. Keller began manufacturing the tools in 1953, and subsequently obtained a license from Western Electric allowing sale of the technology on the open market. The tool was marketed under its original name – since the name of the manufacturer was coincidentally the same as the name of the inventor.
IBM's first transistorized computers, introduced the late 1950s, were built with the IBM Standard Modular System that used wire-wrapped backplanes.
Manual wire wrap.
A manual wire wrap tool resembles a small pen. It is convenient for minor repairs. Wire wrap is one of the most repairable systems for assembling electronics. Posts can be rewrapped up to ten times without appreciable wear, provided that new wire is used each time. Slightly larger jobs are done with a manual "wire wrap gun" having a geared and spring-loaded squeeze grip to spin the bit rapidly.
Such tools were used in large numbers in American telephone exchanges in the last third of the 20th century, usually with a bigger bit to handle 22 or 24 AWG wire rather than the smaller 28 or 30 AWG used in circuit boards and backplanes. The larger posts can be rewrapped hundreds of times. They persisted into the 21st century in distribution frames where insulation-displacement connectors had not taken over entirely. Larger, hand held, high speed electric wrap guns replaced soldering in the late 1960s for permanent wiring, when installing exchange equipment. In the middle 1980s they were gradually replaced by connectorized cables.
The Apollo Guidance Computer with its short production run and stringent reliability requirement was one of the early applications of wire wrap to computer assembly.
Semiautomated wire wrap.
Semiautomated powered wire-wrap systems place "wire-wrap guns" on arms moved in two dimensions by computer-controlled motors. The guns are manually pulled down, and the trigger pressed to make a wrap. The wires are inserted into the gun manually. This system allows the operator to place wires without concern about whether they are on the correct pin, since the computer places the gun correctly.
Semi-automated wire wrapping is unique among prototyping systems because it can place twisted pairs, and twisted magnetically shielded quads, permitting the assembly of complex radar and high speed digital circuits.
Automated wire wrapping.
Automated wire-wrap machines, as manufactured by the Gardner Denver Company in the 1960s and 1970s, were capable of automatically routing, cutting, stripping and wrapping wires onto an electronic "backplane" or "circuit board". The machines were driven by wiring instructions encoded onto punched cards, Mylar punched hole tape, and early micro computers.
The earliest machines (14FB and 14FG models, for example) were initially configured as "horizontal", which meant that the wire wrap board was placed upside down (pins up) onto a horizontal tooling plate, which was then rolled into the machine and locked onto a rotating (TRP table rotational position of four positions) and shifting (PLP = pallet longitudinal position of 11 positions) pallet assembly. These machines included very large hydraulic units for powering the servos that drove the ball screw mounted "A" and "B" drive carriages, a tall electronics cabinet loaded with hundreds of IBM control relays, many dozens of solenoids for controlling the various pneumatic mechanical subsystems, and an IBM 029 card reader for positioning instructions. The automatic wire wrap machines themselves were quite large, tall and square. Servicing the machines was extremely complex, and often meant climbing inside them just to work on them. This could be quite dangerous if safety interlocks were not maintained properly; there were rumors throughout the industry that some fatalities/serious injuries had actually occurred.
Later, somewhat smaller machines were "vertical" (14FV) which meant the boards were placed onto a tooling plate with pins facing the machine operator. Gone were the hydraulic units, in favor of direct drive motors to rotate the ball screws, with rotary encoders to provide positioning feedback. This generally provided better visibility of the product for the operator, although maximum wrap area was significantly less than the Horizontal machines. Top speeds on horizontal machines were generally around 500-600 wires per hour, while the vertical machines could reach rates as high as 1200 per hour, depending on board quality and wiring configurations.
Design automation.
In wire-wrapping, electronic design automation can design the board, and optimize the order in which wires are placed.
The first stage was that a schematic was encoded into a netlist. This step is now done automatically by EDA programs that perform "schematic capture". A netlist is conceptually a list of pins, with each pin having an associated signal name.
The next step was to encode the pin positions of each device. The easy way to do this is to encode lettered rows and numbered columns where the devices should go. The computer then assigns pin 1 of each device in the bill of materials to an intersection, and renames the devices in the bill of materials by their row and column.
The computer would then "explode" the device list into a complete pin list for the board by using templates for each type of device. A template is map of a device's pins. It can be encoded once, and then shared by all devices of that type.
Some systems optimized the design by experimentally swapping the positions of parts and logic gates to reduce the wire length. After each movement, the associated pins in the netlist would be renamed. Some systems could also automatically discover power pins in the devices, and generate wires to the nearest power pins.
The computer program then merges the netlist (sorted by pin name) with the pin list (sorted by pin name), transferring the physical coordinates of the pin list to the netlist. The netlist is then resorted, by net name.
The programs then try to reorder each net in the signal-pin list to "route" each signal in the shortest way. The routing problem is equivalent to the travelling salesman problem, which is NP complete, and therefore not amenable to a perfect solution on a reasonable time scale. One practical routing algorithm is to pick the pin farthest from the center of the board, then use a greedy algorithm to select the next-nearest pin with the same signal name.
Once routed, each pair of nodes in a net becomes a wire, in a "wire list".
The computer then reads incidental information (wire color, order in the net, length of the wire, etc.) in the netlist and interprets it to renumber the wire list to optimize the ordering and direction of wires during production. The wire list is then resorted by the wire numbers.
For example, wires are always "top and bottomed". That is, wires alternate between high and low as they connect a series of pins. This lets a repair or modification occur with the removal of at most three wires.
Long wires are usually placed first within a level, so that shorter wires will hold longer wires down. This reduces vibration of the longer wires, making the board more rugged in a vibrating environment such as a vehicle.
Placing all the wires of a certain size makes it easier for a manual or semiautomated wire-wrapping machine to use precut wire. This especially speeds up manual wrapping.
Wires of different colors can also be placed together. Most wires are blue. Power and ground wires are often made with red and black. Clock wires (or other wires needing special routing) are often made yellow or white. Twisted pairs are usually black and white.
Another optimization is that within each size and color of wire, the computer selects the next wire so that the wrap head moves to the nearest pin. This can save up to 40% of the wrap time, almost getting two wire-wrap machines for the price of one. It also reduces wear on the wire-wrap machines.
Finally, the direction of placing a wire can be optimized for right-handed wire-wrap people, so that wires are placed from right to left. In a semi-automated wire-wrap system, this means that the wrap head moves away from the user's hand when placing a wire. The user can then use their strong hand and eye to route the wire.
Lastly, the sorted, optimized wire list is then printed out for use by machine operators, and turned into a tape or card deck for the machine. Machine-readable copies of this valuable production data are often archived at the same time.
Telecommunications.
In telecommunications wire wrap is in common high volume use in modern communications networks for cross connects of copper wiring. For example, most phone lines from the outside plant go to wire wrap panels in a central office, whether used for POTS, DSL or T1 lines. Typically at a main distribution frame Internal Cross Facilities Assignments and External Cross Facilities Assignments, are connected together via jumpers that are wire wrapped. Wire wrap is popular in telecommunications since it is one of the most secure ways to attach wires, and provides excellent and consistent data layer contact. Wirewrap panels are rated for high quality data services, including Cat 5 grade wiring. The principal competitor in this application is punch blocks, which are quicker but less secure.

</doc>
<doc id="65899" url="https://en.wikipedia.org/wiki?curid=65899" title="Digital Audio Tape">
Digital Audio Tape

Digital Audio Tape (DAT or R-DAT) is a signal recording and playback medium developed by Sony and introduced in 1987. In appearance it is similar to a Compact Cassette, using 3.81 mm / 0.15" (commonly referred to as 4mm) magnetic tape enclosed in a protective shell, but is roughly half the size at 73 mm × 54 mm × 10.5 mm. As the name suggests, the recording is digital rather than analog. DAT has the ability to record at higher, equal or lower sampling rates than a CD (48, 44.1 or 32 kHz sampling rate respectively) at 16 bits quantization. If a digital source is copied then the DAT will produce an exact clone, unlike other digital media such as Digital Compact Cassette or non-Hi-MD MiniDisc, both of which use a lossy data reduction system.
Like most formats of videocassette, a DAT cassette may only be recorded and played in one direction, unlike an analog compact audio cassette, although many DAT recorders had the capability to record program numbers and IDs, which can be used to select an individual track like on a CD player.
Although intended as a replacement for analog audio compact cassettes, the format was never widely adopted by consumers because of issues of expense and concerns from the music industry about unauthorized high-quality copies. The format saw moderate success in professional markets and as a computer storage medium, which was developed into the Digital Data Storage format. As Sony has ceased production of new recorders, it will become more difficult to play archived recordings in this format unless they are copied to other formats or hard drives.
History.
Development.
The technology of DAT is closely based on that of video recorders, using a rotating head and helical scan to record data. This prevents DATs from being physically edited in the cut-and-splice manner of analog tapes, or open-reel digital tapes like ProDigi or DASH.
The DAT standard allows for four sampling modes: 32 kHz at 12 bits, and 32 kHz, 44.1 kHz or 48 kHz at 16 bits. Certain recorders operate outside the specification, allowing recording at 96 kHz and 24 bits (HHS). Some early machines aimed at the consumer market did not operate at 44.1 kHz when recording so they could not be used to 'clone' a compact disc. Since each recording standard uses the same tape, the quality of the sampling has a direct relation to the duration of the recording – 32 kHz at 12 bits will allow six hours of recording onto a three-hour tape while HHS will only give 90 minutes from the same tape. Included in the signal data are subcodes to indicate the start and end of tracks or to skip a section entirely; this allows for indexing and fast seeking. Two-channel stereo recording is supported under all sampling rates and bit depths, but the R-DAT standard does support 4-channel recording at 32 kHz.
DAT tapes are between 15 and 180 minutes in length, a 120-minute tape being 60 metres in length. DAT tapes longer than 60 metres tend to be problematic in DAT recorders due to the thinner media. DAT machines running at 48 kHz and 44.1 kHz sample rates transport the tape at 8.15 mm/s. DAT machines running at 32 kHz sample rate transport the tape at 4.075 mm/s.
Predecessor formats.
DAT was not the first digital audio tape; pulse-code modulation (PCM) was used in Japan by Denon in 1972 for the mastering and production of analogue phonograph records, using a 2-inch Quadruplex-format videotape recorder for its transport, but this was not developed into a consumer product. Denon's development dated from its work with Japan's NHK Broadcasting; NHK developed the first high-fidelity PCM audio recorder in the late 1960s. Denon continued development of their PCM recorders that used professional video machines as the storage medium, eventually building 8-track units used for, among other productions, a series of jazz records made in New York in the late 1970s.
In 1976, another digital audio tape format was developed by Soundstream, using wide reel-to-reel tape loaded on an instrumentation recorder manufactured by Honeywell acting as a transport, which in turn was connected to outboard digital audio encoding and decoding hardware of Soundstream's own design. Soundstream's format was improved through several prototypes and when it was developed to 50 kHz sampling rate at 16 bits, it was deemed good enough for professional classical recording by the company's first client, Telarc Records of Cleveland, Ohio. Telarc's April, 1978 recording of the Holst Suites for Band by Fred Fennell and the Cleveland Wind Ensemble was a landmark release, and ushered in digital recording for America's classical music labels. Soundstream's system was also used by RCA.
Starting in 1978, 3M introduced its own line and format of digital audio tape recorders for use in a recording studio. One of the first prototypes of 3M's system was installed in the studios of Sound 80 in Minneapolis, Minnesota. This system was used in June 1978 to record Aaron Copland's "Appalachian Spring" by the St. Paul Chamber Orchestra conducted by Dennis Russell Davies. That record was the first Grammy-winning digital recording. The production version of the 3M Digital Mastering System was used in 1979 to record the first all-digital rock album, Ry Cooder's "Bop Till You Drop," made at Warner Brothers Studio in California.
The first consumer-oriented PCM format used consumer video tape formats (Beta and VHS) as the storage medium. These systems used the EIAJ digital format, which sampled at 44.056 kHz at 14 bits. The Sony PCM-F1 system debuted in 1981, and Sony from the start offered the option of 16-bit wordlength. Other systems were marketed by Akai, JVC, Nakamichi and others. Panasonic, via its Technics division, briefly sold a digital recorder that combined an EIAJ digital adapter with a VHS video transport, the SV-P100. These machines were marketed by consumer electronics companies to consumers, but they were very pricey compared to cassette or even reel-to-reel decks of the time. They did catch on with the more budget conscious professional recordists, and some boutique-label professional releases were recorded using these machines.
Starting in the early 1980s, professional systems using a PCM adaptor were also common as mastering formats. These systems digitized an analog audio signal and then encoded the resulting digital stream into an analog video signal so that a conventional VCR could be used as a storage medium.
One of the most significant examples of a PCM adaptor-based system was the Sony PCM-1600 digital audio mastering system, introduced in 1978. The PCM-1600 used a U-Matic-format VCR for its transport, connected to external digital audio processing hardware. It (and its later versions such as the PCM-1610 and 1630) was widely used for the production and mastering of some of the first Digital Audio CDs in the early 1980s. Once CD's were commercially introduced in 1982, tapes recorded on the PCM-1600 were sent to the CD pressing plants to be used to make the glass master disc for CD replication.
Other examples include dbx, Inc.'s Model 700 system, which, similar to modern Super Audio CDs, used a high sample-rate delta-sigma modulation rather than PCM; Decca's 1970s PCM system, which used a videotape recorder manufactured by IVC for a transport; and Mitsubishi's X-80 digital recorder, a 6.4 mm (¼ in) open reel digital mastering format that used a very unusual sampling rate of 50.4 kHz.
For high-quality studio recording, all of these formats were effectively made obsolete in the early 1980s by two competing reel-to-reel formats with stationary heads: Sony's DASH format and Mitsubishi's continuation of the X-80 recorder, which was improved upon to become the ProDigi format. (In fact, one of the first ProDigi-format recorders, the Mitsubishi X-86C, was playback-compatible with tapes recorded on an X-80.) Both of these formats remained popular as an analog alternative until the early 1990s, when hard disk recorders rendered them obsolete.
R-DAT and S-DAT (DCC).
The DAT recorder mechanism was considerably more complex and expensive than an analogue cassette deck mechanism due to the rotary helical scan head, therefore Philips & Panasonic Corporation developed a rival digital tape recorder system with a stationary head based on the analogue compact cassette. The DCC was cheaper and simpler mechanically than DAT, but did not make perfect digital copies as it used a lossy compression technique called PASC. (Lossy compression was necessary to reduce the data rate to a level that the DCC head could record successfully at the linear tape speed of 4.75 cm/s that the compact cassette system uses.) DCC was never a competitor to DAT in recording studios, because DAT was already established, and as it was launched at the same time as Sony's Minidisc format (which has random access and editing features), it was not successful with consumers either. However, DCC proved that high quality digital recording could be achieved with a cheap simple mechanism using stationary heads.
Anti-DAT lobbying.
In the late 1980s, the Recording Industry Association of America (RIAA) unsuccessfully lobbied against the introduction of DAT devices into the U.S. Initially, the organization threatened legal action against any manufacturer attempting to sell DAT machines in the country. It later sought to impose restrictions on DAT recorders to prevent them from being used to copy LPs, CDs, and prerecorded cassettes. One of these efforts, the Digital Audio Recorder Copycode Act of 1987 (introduced by Sen. Al Gore and Rep. Waxman), instigated by CBS Records president Walter Yetnikoff, involved a technology called CopyCode and required DAT machines to include a chip to detect attempts to copy material recorded with a notch filter, meaning that copyrighted prerecorded music, whether analog or digital, whether on LP, cassette, or DAT, would have distorted sound resulting from the notch filter applied by the publisher at the time of mastering for mass reproduction. A National Bureau of Standards study showed that not only were the effects plainly audible, but that it was not even effective at preventing copying.
This opposition by CBS softened after Sony, a DAT manufacturer, bought CBS Records in January 1988. By June 1989, an agreement was reached, and the only concession the RIAA would receive was a more practical recommendation from manufacturers to Congress that legislation be enacted to require that recorders have a Serial Copy Management System to prevent digital copying for more than a single generation. This requirement was enacted as part of the Audio Home Recording Act of 1992, which also imposed taxes on DAT recorders and blank media. However, the computer industry successfully lobbied to have personal computers exempted from that act, setting the stage for massive consumer copying of copyrighted material on materials like recordable CDs and by extension, filesharing systems such as Napster.
Uses of DAT.
Professional recording industry.
DAT was used professionally in the 1990s by the professional audio recording industry as part of an emerging all-digital production chain also including digital multi-track recorders and digital mixing consoles that was used to create a fully digital recording. In this configuration, it is possible for the audio to remain digital from the first AD converter after the mic preamp until it is in a CD player.
Pre-recorded DAT.
In December 1987, The Durutti Column album, "The Guitar And Other Machines" became the first commercial release on DAT. Later in May 1988, Wire released their album "The Ideal Copy" on the format. Several other albums from multiple record labels were also released as pre-recorded DAT tapes in the first few years of the format's existence, in small quantities as well. Factory Records released a small number of albums on the format, but many planned releases were cancelled.
Amateur and home use.
DAT was envisaged by proponents as the successor format to analogue audio cassettes in the way that the compact disc was the successor to vinyl-based recordings. It sold well in Japan, where high-end consumer audio stores stocked DAT recorders and tapes into the 2010s and second-hand stores generally continued to offer a wide selection of mint condition machines. However, there and in other nations, the technology was never as commercially popular as CD or cassette. DAT recorders proved to be comparatively expensive and few commercial recordings were available. Globally, DAT remained popular, for a time, for making and trading recordings of live music (see bootleg recording), since available DAT recorders predated affordable CD recorders.
Computer data storage medium.
The format was designed for audio use, but through the ISO Digital Data Storage standard was adopted for general data storage, storing from 1.3 to 80 GB on a 60 to 180 meter tape depending on the standard and compression. It is a sequential-access medium and is commonly used for backups. Due to the higher requirements for capacity and integrity in data backups, a computer-grade DAT was introduced, called DDS (Digital Data Storage). Although functionally similar to audio DATs, only a few DDS and DAT drives (in particular, those manufactured by Archive for SGI workstations) are capable of reading the audio data from a DAT cassette. SGI DDS4 drives no longer have audio support; SGI removed the feature due to "lack of demand".
Future of DAT.
In November 2005, Sony announced that its remaining DAT machine models would be discontinued the following month. Sony has sold around 660,000 DAT products since its introduction in 1987. However, the DAT format still finds regular use in film and television recording, principally due to the support in some recorders for SMPTE time code synchronisation, and sometimes by audio enthusiasts as a way of backing up vinyl, compact cassette and CD collections to a digital format to then be transferred to PC. Although it is being superseded by modern hard disk recording or memory card equipment, which offers much more flexibility and storage, Digital Data Storage tapes, which are broadly similar to DATs, apart from tape length and thickness on some variants, and are still manufactured today unlike DAT cassettes, are often used as substitutes in many situations.
In 2004, Sony introduced the Hi-MD Walkman with the ability to record in linear PCM. Although the Hi-MD format did find some favour as a disc-based DAT alternative for field recordings and general portable playback, Hi-MD manufacture ended in 2012.

</doc>
<doc id="65900" url="https://en.wikipedia.org/wiki?curid=65900" title="Picric acid">
Picric acid

Picric acid is the chemical compound formally called 2,4,6-trinitrophenol (TNP). This yellow crystalline solid is one of the most acidic phenols and is vinylogous to nitric acid. Like other highly nitrated compounds such as TNT, picric acid is an explosive. Its name comes from the Greek πικρός ("pikros"), meaning "bitter", reflecting its bitter taste.
Its primary use, now outdated, is as an explosive. It has also been used in medicine (antiseptic, burn treatments), dyes, and as a chemistry agent.
History.
Picric acid was probably first mentioned in the alchemical writings of Johann Rudolf Glauber in 1742. Initially, it was made by nitrating substances such as animal horn, silk, indigo, and natural resin, the synthesis from indigo first being performed by Peter Woulfe in 1771. Its synthesis from phenol, and the correct determination of its formula, were successfully accomplished in 1841. Not until 1830 did chemists think to use picric acid as an explosive. Before then, chemists assumed that only the salts of picric acid were explosive, not the acid itself. In 1871 Hermann Sprengel proved it could be detonated and most military powers used picric acid as their main high explosive material. Picric acid is also used in the analytical chemistry of metals, ores, and minerals.
Picric acid was the first high explosive nitrated organic compound widely considered suitable to withstand the shock of firing in conventional artillery. Nitroglycerine and guncotton were available earlier but shock sensitivity sometimes caused detonation in the artillery barrel at the time of firing. In 1885, based on research of Hermann Sprengel, French chemist Eugène Turpin patented the use of pressed and cast picric acid in blasting charges and artillery shells. In 1887 the French government adopted a mixture of picric acid and guncotton under the name Melinite. In 1888, Britain started manufacturing a very similar mixture in Lydd, Kent, under the name Lyddite. Japan followed with an "improved" formula known as shimose powder. In 1889, a similar material, a mixture of ammonium cresylate with trinitrocresol, or an ammonium salt of trinitrocresol, started to be manufactured under the name ecrasite in Austria-Hungary. By 1894 Russia was manufacturing artillery shells filled with picric acid. Ammonium picrate (known as Dunnite or explosive D) was used by the United States beginning in 1906. However, shells filled with picric acid become highly unstable if the compound reacts with metal shell or fuze casings to form metal picrates which are more sensitive than the parent phenol. The sensitivity of picric acid was demonstrated in the Halifax Explosion. Picric acid was used in the Battle of Omdurman, Second Boer War, the Russo-Japanese War, and World War I. Germany began filling artillery shells with TNT in 1902. Toluene was less readily available than phenol, and TNT is less powerful than picric acid, but improved safety of munitions manufacturing and storage caused replacement of picric acid by TNT for most military purposes between the World Wars.
Efforts to control the availability of phenol, the precursor to picric acid, emphasize its importance in World War I. Germans are reported to have bought US supplies of phenol and converted it to acetylsalicylic acid, i.e., aspirin, to keep it from the Allies. At the time, phenol was obtained from coal as a co-product of coke ovens and the manufacture of gas for gas lighting. Laclede Gas reports being asked to expand production of phenol (and toluene) to support the war effort. Both Monsanto and Dow Chemical undertook manufacture of synthetic phenol in 1915. Dow was the leading producer. Dow describes picric acid as “the main battlefield explosive used by the French. Large amounts [of phenol also went to Japan, where it was made into picric acid sold to the Russians.”
Synthesis.
The aromatic ring of phenol is highly activated towards electrophilic substitution reactions, and attempted nitration of phenol, even with dilute nitric acid, results in the formation of high molecular weight tars. In order to minimize these side reactions, anhydrous phenol is sulfonated with fuming sulfuric acid, and the resulting "p"-hydroxyphenylsulfonic acid is then nitrated with concentrated nitric acid. During this reaction, nitro groups are introduced, and the sulfonic acid group is displaced. The reaction is highly exothermic, and careful temperature control is required.
Uses.
By far, the largest use has been in munitions and explosives. Explosive D, also known as Dunnite, is the ammonium salt of picric acid—more powerful but less stable than the more common explosive, TNT (which is produced in a similar process to picric acid but with toluene as the feedstock). Picramide, formed by aminating picric acid (typically beginning with Dunnite), can be further aminated to produce the highly stable explosive TATB.
It has found some use in organic chemistry for the preparation of crystalline salts of organic bases (picrates) for the purpose of identification and characterization.
In metallurgy, a picric acid etch has been commonly used in optical metallography to reveal prior austenite grain boundaries in ferritic steels. The hazards associated with picric acid has meant it has largely been replaced with other chemical etchants. However, it is still used to etch magnesium alloys, such as AZ31.
Bouin solution is a common picric-acid-containing fixative solution used for histology specimens. It improves the staining of acid dyes, but it can also lead to hydrolysis of any DNA in the sample.
Clinical chemistry lab testing utilizes picric acid for the Jaffe reaction to test for creatinine. It forms a colored complex that can be measured using spectroscopy.
Much less commonly, wet picric acid has been used as a skin dye, or temporary branding agent. It reacts with proteins in the skin to give a dark brown color that may last as long as a month.
In the early 20th century, picric acid was stocked in pharmacies as an antiseptic and as a treatment for burns, malaria, herpes, and smallpox. Picric acid-soaked gauze was also commonly stocked in first aid kits from that period as a burn treatment. It was most notably used for the treatment of burns suffered by victims of the Hindenburg disaster in 1937. Picric acid was also used as a treatment for trench foot suffered by soldiers stationed on the Western Front during World War I.
Picric acid emits a high-pitched whine during combustion in air and this has led to its widespread use in fireworks.
Picric acid has been used for many years by fly tyers to dye mole skins and feathers a dark olive green. Its popularity has been tempered by its toxic nature.
Safety.
Modern safety precautions recommend storing picric acid wet. Dry picric acid is relatively sensitive to shock and friction, so laboratories that use it store it in bottles under a layer of water, rendering it safe. Glass or plastic bottles are required, as picric acid can easily form metal picrate salts that are even more sensitive and hazardous than the acid itself. Industrially, picric acid is especially hazardous because it is volatile and slowly sublimes even at room temperature. Over time, the buildup of picrates on exposed metal surfaces can constitute a grave hazard.
Picric acid gauze, if found in antique first aid kits, presents a safety hazard because picric acid of that vintage (60–90 years old) will have become crystallized and unstable, and may have formed metal picrates from long storage in a metal first aid case.
Bomb disposal units are often called to dispose of picric acid if it has dried out.
Munitions containing picric acid may be found in sunken warships. However, the buildup of metal picreates over time renders them shock-sensitive and extremely hazardous. It is recommended that wrecks that contain such munitions not be disturbed in any way. The hazard may subside when the shells become corroded enough to admit seawater as these materials are water-soluble.

</doc>
