<doc id="59868" url="https://en.wikipedia.org/wiki?curid=59868" title="Interpreter (computing)">
Interpreter (computing)

In computer science, an interpreter is a computer program that directly executes, i.e. "performs", instructions written in a programming or scripting language, without previously compiling them into a machine language program. An interpreter generally uses one of the following strategies for program execution:
Early versions of Lisp programming language and Dartmouth BASIC would be examples of the first type. Perl, Python, MATLAB, and Ruby are examples of the second, while UCSD Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for JIT systems). Some systems, such as Smalltalk, contemporary versions of BASIC, Java and others may also combine two and three.
While interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms "interpreted language" or "compiled language" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high level language is ideally an abstraction independent of particular implementations.
History.
The first interpreted high-level language was Lisp. Lisp was first implemented in 1958 by Steve Russell on an IBM 704 computer. Russell had read John McCarthy's paper, and realized (to McCarthy's surprise) that the Lisp "eval" function could be implemented in machine code. The result was a working Lisp interpreter which could be used to run Lisp programs, or more properly, "evaluate Lisp expressions".
Compilers versus interpreters.
Programs written in a high level language are either directly executed by some kind of interpreter or converted into machine code by a compiler (and assembler and linker) for the CPU to execute.
While compilers (and assemblers) generally produce machine code directly executable by computer hardware, they can often (optionally) produce an intermediate form called object code. This is basically the same machine specific code but augmented with a symbol table with names and tags to make executable blocks (or modules) identifiable and relocatable. Compiled programs will typically use building blocks (functions) kept in a library of such object code modules. A linker is used to combine (pre-made) library files with the object file(s) of the application to form a single executable file. The object files that are used to generate an executable file are thus often produced at different times, and sometimes even by different languages (capable of generating the same object format).
A simple interpreter written in a low level language (e.g. assembly) may have similar machine code blocks implementing functions of the high level language stored, and executed when a function's entry in a look up table points to that code. However, an interpreter written in a high level language typically uses another approach, such as generating and then walking a parse tree, or by generating and executing intermediate software-defined instructions, or both.
Thus, both compilers and interpreters generally turn source code (text files) into tokens, both may (or may not) generate a parse tree, and both may generate immediate instructions (for a stack machine, quadruple code, or by other means). The basic difference is that a compiler system, including a (built in or separate) linker, generates a stand-alone "machine code" program, while an interpreter system instead "performs" the actions described by the high level program.
A compiler can thus make almost all the conversions from source code semantics to the machine level once and for all (i.e. until the program has to be changed) while an interpreter has to do "some" of this conversion work every time a statement or function is executed. However, in an efficient interpreter, much of the translation work (including analysis of types, and similar) is factored out and done only the first time a program, module, function, or even statement, is run, thus quite akin to how a compiler works. However, a compiled program still runs much faster, under most circumstances, in part because compilers are designed to optimize code, and may be given ample time for this. This is especially true for simpler high level languages without (much) dynamic data structures, checks or typing.
In traditional compilation, the executable output of the linkers (.exe files or .dll files or a library, see picture) is typically relocatable when run under a general operating system, much like the object code modules are but with the difference that this relocation is done dynamically at run time, i.e. when the program is loaded for execution. On the other hand, compiled and linked programs for small embedded systems are typically statically allocated, often hard coded in a NOR flash memory, as there are often no secondary storage and no operating system in this sense.
Historically, most interpreter-systems have had a self-contained editor built in. This is becoming more common also for compilers (then often called an IDE), although some programmers prefer to use an editor of their choice and run the compiler, linker and other tools manually. Historically, compilers predate interpreters because hardware at that time could not support both the interpreter and interpreted code and the typical batch environment of the time limited the advantages of interpretation.
Development cycle.
During the software development cycle, programmers make frequent changes to source code. When using a compiler, each time a change is made to the source code, they must wait for the compiler to translate the altered source files and link all of the binary code files together before the program can be executed. The larger the program, the longer the wait. By contrast, a programmer using an interpreter does a lot less waiting, as the interpreter usually just needs to translate the code being worked on to an intermediate representation (or not translate it at all), thus requiring much less time before the changes can be tested. Effects are evident upon saving the source code and reloading the program. Compiled code is generally less readily debugged as editing, compiling, and linking are sequential processes that have to be conducted in the proper sequence with a proper set of commands. For this reason, many compilers also have an executive aid, known as a Make file and program. The Make file lists compiler and linker command lines and program source code files, but might take a simple command line menu input (e.g. "Make 3") which selects the third group (set) of instructions then issues the commands to the compiler, and linker feeding the specified source code files.
Distribution.
A compiler converts source code into binary instruction for a specific processor's architecture, thus making it less portable. This conversion is made just once, on the developer's environment, and after that the same binary can be distributed to the user's machines where it can be executed without further translation. A cross compiler can generate binary code for the user machine even if it has a different processor than the machine where the code is compiled.
An interpreted program can be distributed as source code. It needs to be translated in each final machine, which takes more time but makes the program distribution independent of the machine's architecture. However, the portability of interpreted source code is dependent on the target machine actually having a suitable interpreter. If the interpreter needs to be supplied along with the source, the overall installation process is more complex than delivery of a monolithic executable since the interpreter itself is part of what need be installed.
The fact that interpreted code can easily be read and copied by humans can be of concern from the point of view of copyright. However, various systems of encryption and obfuscation exist. Delivery of intermediate code, such as bytecode, has a similar effect to obfuscation, but bytecode could be decoded with a decompiler or disassembler.
Efficiency.
The main disadvantage of interpreters is that an interpreted program typically runs slower than if it had been compiled. The difference in speeds could be tiny or great; often an order of magnitude and sometimes more. It generally takes longer to run a program under an interpreter than to run the compiled code but it can take less time to interpret it than the total time required to compile and run it. This is especially important when prototyping and testing code when an edit-interpret-debug cycle can often be much shorter than an edit-compile-run-debug cycle.
Interpreting code is slower than running the compiled code because the interpreter must analyze each statement in the program each time it is executed and then perform the desired action, whereas the compiled code just performs the action within a fixed context determined by the compilation. This run-time analysis is known as "interpretive overhead". Access to variables is also slower in an interpreter because the mapping of identifiers to storage locations must be done repeatedly at run-time rather than at compile time.
There are various compromises between the development speed when using an interpreter and the execution speed when using a compiler. Some systems (such as some Lisps) allow interpreted and compiled code to call each other and to share variables. This means that once a routine has been tested and debugged under the interpreter it can be compiled and thus benefit from faster execution while other routines are being developed. Many interpreters do not execute the source code as it stands but convert it into some more compact internal form. Many BASIC interpreters replace keywords with single byte tokens which can be used to find the instruction in a jump table. A few interpreters, such as the PBASIC interpreter, achieve even higher levels of program compaction by using a bit-oriented rather than a byte-oriented program memory structure, where commands tokens occupy perhaps 5 bits, nominally "16-bit" constants are stored in a variable-length code requiring 3, 6, 10, or 18 bits, and address operands include a "bit offset". Many BASIC interpreters can store and read back their own tokenized internal representation.
An interpreter might well use the same lexical analyzer and parser as the compiler and then interpret the resulting abstract syntax tree.
Example data type definitions for the latter, and a toy interpreter for syntax trees obtained from C expressions are shown in the box.
Regression.
Interpretation cannot be used as the sole method of execution: even though an interpreter can itself be interpreted and so on, a directly executed program is needed somewhere at the bottom of the stack because the code being interpreted is not, by definition, the same as the machine code that the CPU can execute.
Variations.
Bytecode interpreters.
There is a spectrum of possibilities between interpreting and compiling, depending on the amount of analysis performed before the program is executed. For example, Emacs Lisp is compiled to bytecode, which is a highly compressed and optimized representation of the Lisp source, but is not machine code (and therefore not tied to any particular hardware). This "compiled" code is then interpreted by a bytecode interpreter (itself written in C). The compiled code in this case is machine code for a virtual machine, which is implemented not in hardware, but in the bytecode interpreter. The same approach is used with the Forth code used in Open Firmware systems: the source language is compiled into "F code" (a bytecode), which is then interpreted by a virtual machine.
Control tables - that do not necessarily ever need to pass through a compiling phase - dictate appropriate algorithmic control flow via customized interpreters in similar fashion to bytecode interpreters.
Abstract Syntax Tree interpreters.
In the spectrum between interpreting and compiling, another approach is to transform the source code into an optimized abstract syntax tree (AST), then execute the program following this tree structure, or use it to generate native code just-in-time. In this approach, each sentence needs to be parsed just once. As an advantage over bytecode, the AST keeps the global program structure and relations between statements (which is lost in a bytecode representation), and when compressed provides a more compact representation. Thus, using AST has been proposed as a better intermediate format for just-in-time compilers than bytecode. Also, it allows the system to perform better analysis during runtime.
However, for interpreters, an AST causes more overhead than a bytecode interpreter, because of nodes related to syntax performing no useful work, of a less sequential representation (requiring traversal of more pointers) and of overhead visiting the tree.
Just-in-time compilation.
Further blurring the distinction between interpreters, byte-code interpreters and compilation is just-in-time compilation (JIT), a technique in which the intermediate representation is compiled to native machine code at runtime. This confers the efficiency of running native code, at the cost of startup time and increased memory use when the bytecode or AST is first compiled. Adaptive optimization is a complementary technique in which the interpreter profiles the running program and compiles its most frequently executed parts into native code. Both techniques are a few decades old, appearing in languages such as Smalltalk in the 1980s.
Just-in-time compilation has gained mainstream attention amongst language implementers in recent years, with Java, the .NET Framework, most modern JavaScript implementations, and Matlab now including JITs.
Self-interpreter.
A self-interpreter is a programming language interpreter written in a programming language which can interpret itself; an example is a BASIC interpreter written in BASIC. Self-interpreters are related to self-hosting compilers.
If no compiler exists for the language to be interpreted, creating a self-interpreter requires the implementation of the language in a host language (which may be another programming language or assembler). By having a first interpreter such as this, the system is bootstrapped and new versions of the interpreter can be developed in the language itself. It was in this way that Donald Knuth developed the TANGLE interpreter for the language WEB of the industrial standard TeX typesetting system.
Defining a computer language is usually done in relation to an abstract machine (so-called operational semantics) or as a mathematical function (denotational semantics). A language may also be defined by an interpreter in which the semantics of the host language is given. The definition of a language by a self-interpreter is not well-founded (it cannot define a language), but a self-interpreter tells a reader about the expressiveness and elegance of a language. It also enables the interpreter to interpret its source code, the first step towards reflective interpreting.
An important design dimension in the implementation of a self-interpreter is whether a feature of the interpreted language is implemented with the same feature in the interpreter's host language. An example is whether a closure in a Lisp-like language is implemented using closures in the interpreter language or implemented "manually" with a data structure explicitly storing the environment. The more features implemented by the same feature in the host language, the less control the programmer of the interpreter has; a different behavior for dealing with number overflows cannot be realized if the arithmetic operations are delegated to corresponding operations in the host language.
Some languages have an elegant self-interpreter, such as Lisp or Prolog. Much research on self-interpreters (particularly reflective interpreters) has been conducted in the Scheme programming language, a dialect of Lisp. In general, however, any Turing-complete language allows writing of its own interpreter. Lisp is such a language, because Lisp programs are lists of symbols and other lists. XSLT is such a language, because XSLT programs are written in XML. A sub-domain of meta-programming is the writing of domain-specific languages (DSLs).
Clive Gifford introduced a measure quality of self-interpreter (the eigenratio), the limit of the ratio between computer time spent running a stack of "N" self-interpreters and time spent to run a stack of "N"−1 self-interpreters as "N" goes to infinity. This value does not depend on the program being run.
The book "Structure and Interpretation of Computer Programs" presents examples of meta-circular interpretation for Scheme and its dialects. Other examples of languages with a self-interpreter are Forth and Pascal.
Punched card interpreter.
The term "interpreter" often referred to a piece of unit record equipment that could read punched cards and print the characters in human-readable form on the card. The IBM 550 Numeric Interpreter and IBM 557 Alphabetic Interpreter are typical examples from 1930 and 1954, respectively.

</doc>
<doc id="59874" url="https://en.wikipedia.org/wiki?curid=59874" title="Schrödinger equation">
Schrödinger equation

In quantum mechanics, the Schrödinger equation is a partial differential equation that describes how the quantum state of a quantum system changes with time. It was formulated in late 1925, and published in 1926, by the Austrian physicist Erwin Schrödinger.
In classical mechanics Newton's second law, (), is used to mathematically predict what a given system will do at any time after a known initial condition. In quantum mechanics, the analogue of Newton's law is Schrödinger's equation for a quantum system (usually atoms, molecules, and subatomic particles whether free, bound, or localized). It is not a simple algebraic equation, but in general a linear partial differential equation, describing the time-evolution of the system's wave function (also called a "state function").
The concept of a wavefunction is a fundamental postulate of quantum mechanics. Although Schrödinger's equation is often presented as a separate postulate, some authors show that some properties resulting from Schrödinger's equation may be deduced just from symmetry principles alone, for example the commutation relations. Generally, “derivations” of the Schrödinger equation demonstrate its mathematical plausibility for describing wave-particle duality, but to date there are no universally accepted derivations of Schrödinger's equation from appropriate axioms.
In the Copenhagen interpretation of quantum mechanics, the wave function is the most complete description that can be given of a physical system. Solutions to Schrödinger's equation describe not only molecular, atomic, and subatomic systems, but also macroscopic systems, possibly even the whole universe. The Schrödinger equation, in its most general form, is consistent with both classical mechanics and special relativity, but the original formulation by Schrödinger himself was non-relativistic.
The Schrödinger equation is not the only way to make predictions in quantum mechanics—other formulations can be used, such as Werner Heisenberg's matrix mechanics, and Richard Feynman's path integral formulation.
Equation.
Time-dependent equation.
The form of the Schrödinger equation depends on the physical situation (see below for special cases). The most general form is the time-dependent Schrödinger equation, which gives a description of a system evolving with time:
where is the imaginary unit, is the Planck constant divided by , the symbol indicates a partial derivative with respect to time , (the Greek letter psi) is the wave function of the quantum system, and is the Hamiltonian operator (which characterizes the total energy of any given wave function and takes different forms depending on the situation).
The most famous example is the non-relativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field; see the Pauli equation):
where is the particle's "reduced mass", is its potential energy, is the Laplacian (a differential operator), and is the wave function (more precisely, in this context, it is called the "position-space wave function"). In plain language, it means "total energy equals kinetic energy plus potential energy", but the terms take unfamiliar forms for reasons explained below.
Given the particular differential operators involved, this is a linear partial differential equation. It is also a diffusion equation, but unlike the heat equation, this one is also a wave equation given the imaginary unit present in the transient term.
The term ""Schrödinger equation"" can refer to both the general equation (first box above), or the specific nonrelativistic version (second box above and variations thereof). The general equation is indeed quite general, used throughout quantum mechanics, for everything from the Dirac equation to quantum field theory, by plugging in various complicated expressions for the Hamiltonian. The specific nonrelativistic version is a simplified approximation to reality, which is quite accurate in many situations, but very inaccurate in others (see relativistic quantum mechanics and relativistic quantum field theory).
To apply the Schrödinger equation, the Hamiltonian operator is set up for the system, accounting for the kinetic and potential energy of the particles constituting the system, then inserted into the Schrödinger equation. The resulting partial differential equation is solved for the wave function, which contains information about the system.
Time-independent equation.
The time-independent Schrödinger equation predicts that wave functions can form standing waves, called stationary states (also called "orbitals", as in atomic orbitals or molecular orbitals). These states are important in their own right, and if the stationary states are classified and understood, then it becomes easier to solve the time-dependent Schrödinger equation for "any" state. The "time-independent Schrödinger equation" is the equation describing stationary states. (It is only used when the Hamiltonian itself is not dependent on time. However, even in this case the total wave function still has a time dependency.)
In words, the equation states:
The time-independent Schrödinger equation is discussed further below. In linear algebra terminology, this equation is an eigenvalue equation.
As before, the most famous manifestation is the non-relativistic Schrödinger equation for a single particle moving in an electric field (but not a magnetic field):
with definitions as above.
Implications.
The Schrödinger equation and its solutions introduced a breakthrough in thinking about physics. Schrödinger's equation was the first of its type, and solutions led to consequences that were very unusual and unexpected for the time.
Total, kinetic, and potential energy.
The "overall" form of the equation is "not" unusual or unexpected as it uses the principle of the conservation of energy. The terms of the nonrelativistic Schrödinger equation can be interpreted as total energy of the system, equal to the system kinetic energy plus the system potential energy. In this respect, it is just the same as in classical physics.
Quantization.
The Schrödinger equation predicts that if certain properties of a system are measured, the result may be "quantized", meaning that only specific discrete values can occur. One example is "energy quantization": the energy of an electron in an atom is always one of the quantized energy levels, a fact discovered via atomic spectroscopy. (Energy quantization is discussed below.) Another example is quantization of angular momentum. This was an "assumption" in the earlier Bohr model of the atom, but it is a "prediction" of the Schrödinger equation.
Another result of the Schrödinger equation is that not every measurement gives a quantized result in quantum mechanics. For example, position, momentum, time, and (in some situations) energy can have any value across a continuous range.
Measurement and uncertainty.
In classical mechanics, a particle has, at every moment, an exact position and an exact momentum. These values change deterministically as the particle moves according to Newton's laws. Under the Copenhagen interpretation of quantum mechanics, particles do not have exactly determined properties, and when they are measured, the result is randomly drawn from a probability distribution. The Schrödinger equation predicts what the probability distributions are, but fundamentally cannot predict the exact result of each measurement.
The Heisenberg uncertainty principle is the statement of the inherent measurement uncertainty in quantum mechanics. It states that the more precisely a particle's position is known, the less precisely its momentum is known, and vice versa.
The Schrödinger equation describes the (deterministic) evolution of the wave function of a particle. However, even if the wave function is known exactly, the result of a specific measurement on the wave function is uncertain.
Quantum tunneling.
In classical physics, when a ball is rolled slowly up a large hill, it will come to a stop and roll back, because it doesn't have enough energy to get over the top of the hill to the other side. However, the Schrödinger equation predicts that there is a small probability that the ball will get to the other side of the hill, even if it has too little energy to reach the top. This is called quantum tunneling. It is related to the distribution of energy: although the ball's assumed position seems to be on one side of the hill, there is a chance of finding it on the other side.
Particles as waves.
The nonrelativistic Schrödinger equation is a type of partial differential equation called a wave equation. Therefore, it is often said particles can exhibit behavior usually attributed to waves. In some modern interpretations this description is reversed – the quantum state, i.e. wave, is the only genuine physical reality, and under the appropriate conditions it can show features of particle-like behavior. However, Ballentine shows that such an interpretation has problems. Ballentine points out that whilst it is arguable to associate a physical wave with a single particle, there is still only "one" Schrödinger wave equation for many particles. He points out:
Two-slit diffraction is a famous example of the strange behaviors that waves regularly display, that are not intuitively associated with particles. The overlapping waves from the two slits cancel each other out in some locations, and reinforce each other in other locations, causing a complex pattern to emerge. Intuitively, one would not expect this pattern from firing a single particle at the slits, because the particle should pass through one slit or the other, not a complex overlap of both.
However, since the Schrödinger equation is a wave equation, a single particle fired through a double-slit "does" show this same pattern (figure on right). Note: The experiment must be repeated many times for the complex pattern to emerge. Although this is counterintuitive, the prediction is correct; in particular, electron diffraction and neutron diffraction are well understood and widely used in science and engineering.
Related to diffraction, particles also display superposition and interference.
The superposition property allows the particle to be in a quantum superposition of two or more quantum states at the same time. However, it is noted that a "quantum state" in QM means the "probability" that a system will be, for example at a position , not that the system will actually be at position . It does not infer that the particle itself may be in two classical states at once. Indeed, QM is generally unable to assign values for properties prior to measurement at all.
Multiverse.
In Dublin in 1952 Erwin Schrödinger gave a lecture in which at on point he jocularly warned his audience that what he was about to say might "seem lunatic". It was that, when his Nobel equations seem to be describing several different histories, they are "not alternatives but all really happen simultaneously". This is the earliest known reference to the multiverse.
Interpretation of the wave function.
The Schrödinger equation provides a way to calculate the wave function of a system and how it changes dynamically in time. However, the Schrödinger equation does not directly say "what", exactly, the wave function is. Interpretations of quantum mechanics address questions such as what the relation is between the wave function, the underlying reality, and the results of experimental measurements.
An important aspect is the relationship between the Schrödinger equation and wavefunction collapse. In the oldest Copenhagen interpretation, particles follow the Schrödinger equation "except" during wavefunction collapse, during which they behave entirely differently. The advent of quantum decoherence theory allowed alternative approaches (such as the Everett many-worlds interpretation and consistent histories), wherein the Schrödinger equation is "always" satisfied, and wavefunction collapse should be explained as a consequence of the Schrödinger equation.
Historical background and development.
Following Max Planck's quantization of light (see black body radiation), Albert Einstein interpreted Planck's quanta to be photons, particles of light, and proposed that the energy of a photon is proportional to its frequency, one of the first signs of wave–particle duality. Since energy and momentum are related in the same way as frequency and wavenumber in special relativity, it followed that the momentum of a photon is inversely proportional to its wavelength , or proportional to its wavenumber .
where is Planck's constant. Louis de Broglie hypothesized that this is true for all particles, even particles which have mass such as electrons. He showed that, assuming that the matter waves propagate along with their particle counterparts, electrons form standing waves, meaning that only certain discrete rotational frequencies about the nucleus of an atom are allowed.
These quantized orbits correspond to discrete energy levels, and de Broglie reproduced the Bohr model formula for the energy levels. The Bohr model was based on the assumed quantization of angular momentum according to: 
According to de Broglie the electron is described by a wave and a whole number of wavelengths must fit along the circumference of the electron's orbit:
This approach essentially confined the electron wave in one dimension, along a circular orbit of radius .
In 1921, prior to de Broglie, Arthur C. Lunn at the University of Chicago had used the same argument based on the completion of the relativistic energy–momentum 4-vector to derive what we now call the de Broglie relation. Unlike de Broglie, Lunn went on to formulate the differential equation now known as the Schrödinger equation, and solve for its energy eigenvalues for the hydrogen atom. Unfortunately the paper was rejected by the Physical Review, as recounted by Kamen.
Following up on de Broglie's ideas, physicist Peter Debye made an offhand comment that if particles behaved as waves, they should satisfy some sort of wave equation. Inspired by Debye's remark, Schrödinger decided to find a proper 3-dimensional wave equation for the electron. He was guided by William R. Hamilton's analogy between mechanics and optics, encoded in the observation that the zero-wavelength limit of optics resembles a mechanical system — the trajectories of light rays become sharp tracks that obey Fermat's principle, an analog of the principle of least action. A modern version of his reasoning is reproduced below. The equation he found is:
However, by that time, Arnold Sommerfeld had refined the Bohr model with relativistic corrections. Schrödinger used the relativistic energy momentum relation to find what is now known as the Klein–Gordon equation in a Coulomb potential (in natural units):
He found the standing waves of this relativistic equation, but the relativistic corrections disagreed with Sommerfeld's formula. Discouraged, he put away his calculations and secluded himself in an isolated mountain cabin in December 1925.
While at the cabin, Schrödinger decided that his earlier non-relativistic calculations were novel enough to publish, and decided to leave off the problem of relativistic corrections for the future. Despite the difficulties in solving the differential equation for hydrogen (he had sought help from his friend the mathematician Hermann Weyl) Schrödinger showed that his non-relativistic version of the wave equation produced the correct spectral energies of hydrogen in a paper published in 1926. In the equation, Schrödinger computed the hydrogen spectral series by treating a hydrogen atom's electron as a wave , moving in a potential well , created by the proton. This computation accurately reproduced the energy levels of the Bohr model. In a paper, Schrödinger himself explained this equation as follows:
This 1926 paper was enthusiastically endorsed by Einstein, who saw the matter-waves as an intuitive depiction of nature, as opposed to Heisenberg's matrix mechanics, which he considered overly formal.
The Schrödinger equation details the behavior of but says nothing of its "nature". Schrödinger tried to interpret it as a charge density in his fourth paper, but he was unsuccessful. In 1926, just a few days after Schrödinger's fourth and final paper was published, Max Born successfully interpreted as the probability amplitude, whose absolute square is equal to probability density. Schrödinger, though, always opposed a statistical or probabilistic approach, with its associated discontinuities—much like Einstein, who believed that quantum mechanics was a statistical approximation to an underlying deterministic theory— and never reconciled with the Copenhagen interpretation.
Louis de Broglie in his later years proposed a real valued wave function connected to the complex wave function by a proportionality constant and developed the De Broglie–Bohm theory.
The wave equation for particles.
The Schrödinger equation is a diffusion equation, the "solutions" are functions which describe wave-like motions. Wave equations in physics can normally be derived from other physical laws – the wave equation for mechanical vibrations on strings and in matter can be derived from Newton's laws – where the wave function represents the displacement of matter, and electromagnetic waves from Maxwell's equations, where the wave functions are electric and magnetic fields. The basis for Schrödinger's equation, on the other hand, is the energy of the system and a separate postulate of quantum mechanics: the wave function is a description of the system. The Schrödinger equation is therefore a new concept in itself; as Feynman put it:
The foundation of the equation is structured to be a linear differential equation based on classical energy conservation, and consistent with the De Broglie relations. The solution is the wave function , which contains all the information that can be known about the system. In the Copenhagen interpretation, the modulus of is related to the probability the particles are in some spatial configuration at some instant of time. Solving the equation for can be used to predict how the particles will behave under the influence of the specified potential and with each other.
The Schrödinger equation was developed principally from the De Broglie hypothesis, a wave equation that would describe particles, and can be constructed as shown informally in the following sections. For a more rigorous description of Schrödinger's equation, see also Resnick "et al".
Consistency with energy conservation.
The total energy of a particle is the sum of kinetic energy and potential energy , this sum is also the frequent expression for the Hamiltonian in classical mechanics: 
Explicitly, for a particle in one dimension with position , mass and momentum , and potential energy which generally varies with position and time :
For three dimensions, the position vector and momentum vector must be used:
This formalism can be extended to any fixed number of particles: the total energy of the system is then the total kinetic energies of the particles, plus the total potential energy, again the Hamiltonian. However, there can be interactions between the particles (an -body problem), so the potential energy can change as the spatial configuration of particles changes, and possibly with time. The potential energy, in general, is "not" the sum of the separate potential energies for each particle, it is a function of all the spatial positions of the particles. Explicitly:
Linearity.
The simplest wavefunction is a plane wave of the form:
where the is the amplitude, the wavevector, and the angular frequency, of the plane wave. In general, physical situations are not purely described by plane waves, so for generality the superposition principle is required; any wave can be made by superposition of sinusoidal plane waves. So if the equation is linear, a linear combination of plane waves is also an allowed solution. Hence a necessary and separate requirement is that the Schrödinger equation is a linear differential equation.
For discrete the sum is a superposition of plane waves:
for some real amplitude coefficients , and for continuous the sum becomes an integral, the Fourier transform of a momentum space wavefunction:
where is the differential volume element in -space, and the integrals are taken over all -space. The momentum wavefunction arises in the integrand since the position and momentum space wavefunctions are Fourier transforms of each other.
Consistency with the De Broglie relations.
Einstein's light quanta hypothesis (1905) states that the energy of a photon is proportional to the frequency (or angular frequency, ) of the corresponding quantum wavepacket of light:
Likewise De Broglie's hypothesis (1924) states that any particle can be associated with a wave, and that the momentum of the particle is inversely proportional to the wavelength of such a wave (or proportional to the wavenumber, ), in one dimension, by:
while in three dimensions, wavelength is related to the magnitude of the wavevector :
The Planck–Einstein and de Broglie relations illuminate the deep connections between energy with time, and space with momentum, and express wave–particle duality. In practice, natural units comprising are used, as the De Broglie "equations" reduce to "identities": allowing momentum, wavenumber, energy and frequency to be used interchangeably, to prevent duplication of quantities, and reduce the number of dimensions of related quantities. For familiarity SI units are still used in this article.
Schrödinger's insight, late in 1925, was to express the phase of a plane wave as a complex phase factor using these relations:
and to realize that the first order partial derivatives were:
with respect to space:
with respect to time:
Another postulate of quantum mechanics is that all observables are represented by linear Hermitian operators which act on the wavefunction, and the eigenvalues of the operator are the values the observable takes. The previous derivatives are consistent with the energy operator, corresponding to the time derivative,
where are the energy eigenvalues, and the momentum operator, corresponding to the spatial derivatives (the gradient ),
where is a vector of the momentum eigenvalues. In the above, the "hats" ( ) indicate these observables are operators, not simply ordinary numbers or vectors. The energy and momentum operators are "differential operators", while the potential energy function is just a multiplicative factor.
Substituting the energy and momentum operators into the classical energy conservation equation obtains the operator:
so in terms of derivatives with respect to time and space, acting this operator on the wavefunction immediately led Schrödinger to his equation:
Wave–particle duality can be assessed from these equations as follows. The kinetic energy is related to the square of momentum . As the particle's momentum increases, the kinetic energy increases more rapidly, but since the wavenumber increases the wavelength decreases. In terms of ordinary scalar and vector quantities (not operators):
The kinetic energy is also proportional to the second spatial derivatives, so it is also proportional to the magnitude of the "curvature" of the wave, in terms of operators:
As the curvature increases, the amplitude of the wave alternates between positive and negative more rapidly, and also shortens the wavelength. So the inverse relation between momentum and wavelength is consistent with the energy the particle has, and so the energy of the particle has a connection to a wave, all in the same mathematical formulation.
Wave and particle motion.
Schrödinger required that a wave packet solution near position with wavevector near will move along the trajectory determined by classical mechanics for times short enough for the spread in (and hence in velocity) not to substantially increase the spread in . Since, for a given spread in , the spread in velocity is proportional to Planck's constant , it is sometimes said that in the limit as approaches zero, the equations of classical mechanics are restored from quantum mechanics. Great care is required in how that limit is taken, and in what cases.
The limiting short-wavelength is equivalent to tending to zero because this is limiting case of increasing the wave packet localization to the definite position of the particle (see images right). Using the Heisenberg uncertainty principle for position and momentum, the products of uncertainty in position and momentum become zero as :
where denotes the (root mean square) measurement uncertainty in and (and similarly for the and directions) which implies the position and momentum can only be known to arbitrary precision in this limit.
The Schrödinger equation in its general form
is closely related to the Hamilton–Jacobi equation (HJE)
where is action and is the Hamiltonian function (not operator). Here the generalized coordinates for (used in the context of the HJE) can be set to the position in Cartesian coordinates as .
Substituting
where is the probability density, into the Schrödinger equation and then taking the limit in the resulting equation, yields the Hamilton–Jacobi equation.
The implications are:
Non-relativistic quantum mechanics.
The quantum mechanics of particles without accounting for the effects of special relativity, for example particles propagating at speeds much less than light, is known as non-relativistic quantum mechanics. Following are several forms of Schrödinger's equation in this context for different situations: time independence and dependence, one and three spatial dimensions, and one and particles.
In actuality, the particles constituting the system do not have the numerical labels used in theory. The language of mathematics forces us to label the positions of particles one way or another, otherwise there would be confusion between symbols representing which variables are for which particle.
Time independent.
If the Hamiltonian is not an explicit function of time, the equation is separable into a product of spatial and temporal parts. In general, the wavefunction takes the form:
where is a function of all the spatial coordinate(s) of the particle(s) constituting the system only, and is a function of time only.
Substituting for into the Schrödinger equation for the relevant number of particles in the relevant number of dimensions, solving by separation of variables implies the general solution of the time-dependent equation has the form:
Since the time dependent phase factor is always the same, only the spatial part needs to be solved for in time independent problems. Additionally, the energy operator can always be replaced by the energy eigenvalue , thus the time independent Schrödinger equation is an eigenvalue equation for the Hamiltonian operator:
This is true for any number of particles in any number of dimensions (in a time independent potential). This case describes the standing wave solutions of the time-dependent equation, which are the states with definite energy (instead of a probability distribution of different energies). In physics, these standing waves are called "stationary states" or "energy eigenstates"; in chemistry they are called "atomic orbitals" or "molecular orbitals". Superpositions of energy eigenstates change their properties according to the relative phases between the energy levels.
The energy eigenvalues from this equation form a discrete spectrum of values, so mathematically energy must be quantized. More specifically, the energy eigenstates form a basis – any wavefunction may be written as a sum over the discrete energy states or an integral over continuous energy states, or more generally as an integral over a measure. This is the spectral theorem in mathematics, and in a finite state space it is just a statement of the completeness of the eigenvectors of a Hermitian matrix.
One-dimensional examples.
For a particle in one dimension, the Hamiltonian is:
and substituting this into the general Schrödinger equation gives:
This is the only case the Schrödinger equation is an ordinary differential equation, rather than a partial differential equation. The general solutions are always of the form:
For particles in one dimension, the Hamiltonian is:
where the position of particle is . The corresponding Schrödinger equation is:
so the general solutions have the form:
For non-interacting distinguishable particles, the potential of the system only influences each particle separately, so the total potential energy is the sum of potential energies for each particle:
and the wavefunction can be written as a product of the wavefunctions for each particle:
For non-interacting identical particles, the potential is still a sum, but wavefunction is a bit more complicated - it is a sum over the permutations of products of the separate wavefunctions to account for particle exchange. In general for interacting particles, the above decompositions are "not" possible.
Free particle.
For no potential, , so the particle is free and the equation reads:
which has oscillatory solutions for (the are arbitrary constants):
and exponential solutions for 
The exponentially growing solutions have an infinite norm, and are not physical. They are not allowed in a finite volume with periodic or fixed boundary conditions.
See also free particle and wavepacket for more discussion on the free particle.
Constant potential.
For a constant potential, , the solution is oscillatory for and exponential for , corresponding to energies that are allowed or disallowed in classical mechanics. Oscillatory solutions have a classically allowed energy and correspond to actual classical motions, while the exponential solutions have a disallowed energy and describe a small amount of quantum bleeding into the classically disallowed region, due to quantum tunneling. If the potential grows to infinity, the motion is classically confined to a finite region. Viewed far enough away, every solution is reduced an exponential; the condition that the exponential is decreasing restricts the energy levels to a discrete set, called the allowed energies.
Harmonic oscillator.
The Schrödinger equation for this situation is
It is a notable quantum system to solve for; since the solutions are exact (but complicated – in terms of Hermite polynomials), and it can describe or at least approximate a wide variety of other systems, including vibrating atoms, molecules, and atoms or ions in lattices, and approximating other potentials near equilibrium points. It is also the basis of perturbation methods in quantum mechanics.
There is a family of solutions – in the position basis they are
where , and the functions are the Hermite polynomials.
Three-dimensional examples.
The extension from one dimension to three dimensions is straightforward, all position and momentum operators are replaced by their three-dimensional expressions and the partial derivative with respect to space is replaced by the gradient operator.
The Hamiltonian for one particle in three dimensions is:
generating the equation:
with stationary state solutions of the form:
where the position of the particle is . Two useful coordinate systems for solving the Schrödinger equation are Cartesian coordinates so that and spherical polar coordinates so that , although other orthogonal coordinates are useful for solving the equation for systems with certain geometric symmetries.
For particles in three dimensions, the Hamiltonian is:
where the position of particle is and the gradient operators are partial derivatives with respect to the particle's position coordinates. In Cartesian coordinates, for particle , the position vector is while the gradient and Laplacian operator are respectively:
generating the equation:
For particles in three dimensions, the Hamiltonian is:
where the position of particle is , generating the equation:
This last equation is in a very high dimension, so the solutions are not easy to visualize.
Solution methods.
General techniques:
Methods for special cases:
Properties.
The Schrödinger equation has the following properties: some are useful, but there are shortcomings. Ultimately, these properties arise from the Hamiltonian used, and solutions to the equation.
Linearity.
In the development above, the Schrödinger equation was made to be linear for generality, though this has other implications. If two wave functions and are solutions, then so is any linear combination of the two:
where and are any complex numbers (the sum can be extended for any number of wavefunctions). This property allows superpositions of quantum states to be solutions of the Schrödinger equation. Even more generally, it holds that a general solution to the Schrödinger equation can be found by taking a weighted sum over all single state solutions achievable. For example, consider a wave function such that the wave function is a product of two functions: one time independent, and one time dependent. If states of definite energy found using the time independent Schrödinger equation are given by with amplitude and time dependent phase factor is given by
is the probability current (flow per unit area).
Hence predictions from the Schrödinger equation do not violate probability conservation.
Positive energy.
If the potential is bounded from below, meaning there is a minimum value of potential energy, the eigenfunctions of the Schrödinger equation have energy which is also bounded from below. This can be seen most easily by using the variational principle, as follows. (See also below).
For any linear operator bounded from below, the eigenvector with the smallest eigenvalue is the vector that minimizes the quantity
over all which are normalized. In this way, the smallest eigenvalue is expressed through the variational principle. For the Schrödinger Hamiltonian bounded from below, the smallest eigenvalue is called the ground state energy. That energy is the minimum value of
(using integration by parts). Due to the complex modulus of (which is positive definite), the right hand side always greater than the lowest value of . In particular, the ground state energy is positive when is everywhere positive.
For potentials which are bounded below and are not infinite over a region, there is a ground state which minimizes the integral above. This lowest energy wavefunction is real and positive definite – meaning the wavefunction can increase and decrease, but is positive for all positions. It physically cannot be negative: if it were, smoothing out the bends at the sign change (to minimize the wavefunction) rapidly reduces the gradient contribution to the integral and hence the kinetic energy, while the potential energy changes linearly and less quickly. The kinetic and potential energy are both changing at different rates, so the total energy is not constant, which can't happen (conservation). The solutions are consistent with Schrödinger equation if this wavefunction is positive definite.
The lack of sign changes also shows that the ground state is nondegenerate, since if there were two ground states with common energy , not proportional to each other, there would be a linear combination of the two that would also be a ground state resulting in a zero solution.
Analytic continuation to diffusion.
The above properties (positive definiteness of energy) allow the analytic continuation of the Schrödinger equation to be identified as a stochastic process. This can be interpreted as the Huygens–Fresnel principle applied to De Broglie waves; the spreading wavefronts are diffusive probability amplitudes.
For a free particle (not subject to a potential) in a random walk, substituting into the time-dependent Schrödinger equation gives:
which has the same form as the diffusion equation, with diffusion coefficient .
Relativistic quantum mechanics.
Relativistic quantum mechanics is obtained where quantum mechanics and special relativity simultaneously apply. In general, one wishes to build relativistic wave equations from the relativistic energy–momentum relation
instead of classical energy equations. The Klein–Gordon equation and the Dirac equation are two such equations. The Klein–Gordon equation,
was the first such equation to be obtained, even before the non-relativistic one, and applies to massive spinless particles. The Dirac equation arose from taking the "square root" of the Klein–Gordon equation by factorizing the entire relativistic wave operator into a product of two operators – one of these is the operator for the entire Dirac equation.
The general form of the Schrödinger equation remains true in relativity, but the Hamiltonian is less obvious. For example, the Dirac Hamiltonian for a particle of mass and electric charge in an electromagnetic field (described by the electromagnetic potentials and ) is:
in which the and are the Dirac gamma matrices related to the spin of the particle. The Dirac equation is true for all particles, and the solutions to the equation are spinor fields with two components corresponding to the particle and the other two for the antiparticle.
For the Klein–Gordon equation, the general form of the Schrödinger equation is inconvenient to use, and in practice the Hamiltonian is not expressed in an analogous way to the Dirac Hamiltonian. The equations for relativistic quantum fields can be obtained in other ways, such as starting from a Lagrangian density and using the Euler–Lagrange equations for fields, or use the representation theory of the Lorentz group in which certain representations can be used to fix the equation for a free particle of given spin (and mass).
In general, the Hamiltonian to be substituted in the general Schrödinger equation is not just a function of the position and momentum operators (and possibly time), but also of spin matrices. Also, the solutions to a relativistic wave equation, for a massive particle of spin , are complex-valued spinor fields.
Quantum field theory.
The general equation is also valid and used in quantum field theory, both in relativistic and non-relativistic situations. However, the solution is no longer interpreted as a "wave", but should be interpreted as an operator acting on states existing in a Fock space.

</doc>
<doc id="59877" url="https://en.wikipedia.org/wiki?curid=59877" title="Gas constant">
Gas constant

The gas constant (also known as the molar, universal, or ideal gas constant, denoted by the symbol or "") is a physical constant which is featured in many fundamental equations in the physical sciences, such as the ideal gas law and the Nernst equation.
It is equivalent to the Boltzmann constant, but expressed in units of energy (i.e. the pressure-volume product) per temperature increment per "mole" (rather than energy per temperature increment per "particle"). The constant is also a combination of the constants from Boyle's law, Charles's law, Avogadro's law, and Gay-Lussac's law.
Physically, the gas constant is the constant of proportionality that happens to relate the energy scale in physics to the temperature scale, when a mole of particles at the stated temperature is being considered. Thus, the value of the gas constant ultimately derives from historical decisions and accidents in the setting of the energy and temperature scales, plus similar historical setting of the value of the molar scale used for the counting of particles. The last factor is not a consideration in the value of the Boltzmann constant, which does a similar job of equating linear energy and temperature scales.
The gas constant value is
The two digits in parentheses are the uncertainty (standard deviation) in the last two digits of the value. The relative uncertainty is 5.7.
Some have suggested that it might be appropriate to name the symbol "R" the Regnault constant in honour of the French chemist Henri Victor Regnault, whose accurate experimental data were used to calculate the early value of the constant; however, the exact reason for the original representation of the constant by the letter "R" is elusive.
The gas constant occurs in the ideal gas law, as follows:
where "P" is the absolute pressure (SI unit pascals), "V" is the volume of gas (SI unit cubic metres), "n" is the amount of gas (SI unit moles), "m" is the mass (SI unit kilograms) contained in "V", and "T" is the thermodynamic temperature (SI unit kelvins). The gas constant is expressed in the same physical units as molar entropy and molar heat capacity.
Dimensions of "R".
From the general equation "PV" = "nRT" we get:
where "P" is pressure, "V" is volume, "n" is number of moles of a given substance, and "T" is temperature.
As pressure is defined as force per unit area, the gas equation can also be written as:
Area and volume are (length)2 and (length)3 respectively. Therefore:
Since force × length = work:
The physical significance of "R" is work per degree per mole. It may be expressed in any set of units representing work or energy (such as joules), other units representing degrees of temperature (such as degrees Celsius or Fahrenheit), and any system of units designating a mole or a similar pure number that allows an equation of macroscopic mass and fundamental particle numbers in a system, such as an ideal gas (see Avogadro's number).
Instead of a mole the constant can be expressed by considering the normal cubic meter.
Relationship with the Boltzmann constant.
The Boltzmann constant "k"B (often abbreviated "k") may be used in place of the gas constant by working in pure particle count, "N", rather than amount of substance, "n", since
where "N"A is the Avogadro constant.
For example, the ideal gas law in terms of Boltzmann's constant is
where "N" is the number of particles (molecules in this case), or to generalize to an inhomogeneous system the local form holds:
where "n" is the number density.
Measurement.
As of 2006, the most precise measurement of "R" is obtained by measuring the speed of sound "c"a("p", "T") in argon at the temperature "T" of the triple point of water (used to define the kelvin) at different pressures "p", and extrapolating to the zero-pressure limit "c"a(0, "T"). The value of "R" is then obtained from the relation
where:
Specific gas constant.
The specific gas constant of a gas or a mixture of gases ("R"specific) is given by the molar gas constant divided by the molar mass ("M") of the gas/mixture.
Just as the ideal gas constant can be related to the Boltzmann constant, so can the specific gas constant by dividing the Boltzmann constant by the molecular mass of the gas.
Another important relationship comes from thermodynamics. Mayer's relation relates the specific gas constant to the specific heats for a calorically perfect gas and a thermally perfect gas.
where "cp" is the specific heat for a constant pressure and "cv" is the specific heat for a constant volume.
It is common, especially in engineering applications, to represent the specific gas constant by the symbol "R". In such cases, the universal gas constant is usually given a different symbol such as R to distinguish it. In any case, the context and/or units of the gas constant should make it clear as to whether the universal or specific gas constant is being referred to.
U.S. Standard Atmosphere.
The U.S. Standard Atmosphere, 1976 (USSA1976) defines the gas constant "R"* as:
Note the use of kmol units resulting in the factor of 1000 in the constant. The USSA1976 acknowledges that this value is not consistent with the cited values for the Avogadro constant and the Boltzmann constant. This disparity is not a significant departure from accuracy, and USSA1976 uses this value of "R*" for all the calculations of the standard atmosphere. When using the ISO value of "R", the calculated pressure increases by only 0.62 pascal at 11 kilometers (the equivalent of a difference of only 17.4 centimeters or 6.8 inches) and an increase of 0.292 Pa at 20 km (the equivalent of a difference of only 0.338 m or 13.2 in).

</doc>
<doc id="59879" url="https://en.wikipedia.org/wiki?curid=59879" title="1938 FIFA World Cup">
1938 FIFA World Cup

The 1938 FIFA World Cup was the third staging of the World Cup, and was held in France from 4 to 19 June 1938. Italy retained the championship (and thus became the only team to have won two FIFA World Cups under the same coach, or Vittorio Pozzo), beating Hungary 4–2 in the final.
Host selection.
France was chosen as hosts by FIFA in Berlin on August 13, 1936. France defeated Argentina and Germany in the first round of voting. The decision caused outrage in South America where it was believed that the venue would alternate between the two continents; instead, it was the second tournament in a row to be played in Europe. This was the last World Cup to be staged before the outbreak of the Second World War.
Qualification.
Because of anger over the decision to hold a second successive World Cup in Europe, neither Uruguay nor Argentina entered the competition, while Spain became the first country to be prevented from competing because of it being at war.
It was the first time that the hosts (France) and the title holders (Italy) qualified automatically. Title holders were given an automatic entry into the World Cup until 2006 when this was abolished.
Of the 14 remaining places, eleven were allocated to Europe, two to the Americas, and one to Asia. As a result, only three non-European nations took part: Brazil, Cuba and the Dutch East Indies. This is the smallest ever number of teams from outside the host continent to compete at a FIFA World Cup.
Austria qualified for the World Cup, but after qualification was complete, the Anschluss united Austria with Germany. Austria subsequently withdrew from the tournament, with some Austrian players joining the German squad (not including Austrian star player Matthias Sindelar, who refused to play for the unified team). Latvia was the runner-up in Austria's qualification group, but was not invited to participate; instead Austria's place remained empty, and Sweden, which would have been Austria's initial opponent, progressed directly to the second-round by default.
This tournament saw the first, and the only, participation in a World Cup tournament from Cuba and the Dutch East Indies (now Indonesia). It also saw the World Cup debuts of Poland and Norway. Poland and the Netherlands would not reappear at a finals tournament until 1974, while Norway would not qualify for another World Cup finals until 1994. A unified Germany team would not appear again until 1994.
Format.
The knockout format from 1934 was retained. If a match was tied after 90 minutes, then 30 minutes of extra time were played. If the score was still tied after extra time, the match would be replayed. This was the last of the two World Cup tournaments that used a straight knockout format.
Summary.
Germany, France, Italy, Czechoslovakia, Hungary, Cuba and Brazil were seeded for draw taking place in Paris, on 5 March 1938.
Five of the seven first round matches required extra time to break the deadlock; two games still went to a replay. In one replay, Cuba advanced to the next round at the expense of Romania. In the other replay, Germany, which had led 1–0 in the first game against Switzerland, led 2–0 but eventually was beaten 2–4. This loss, which took place in front of a hostile, bottle-throwing crowd in Paris, was blamed by German coach Sepp Herberger on a defeatist attitude from the five Austrian players he had been forced to include; a German journalist later commented that "Germans and Austrians prefer to play against each other even when they're in the same team". This remains, , the only time in World Cup history in which Germany failed to advance to the final eight (they did not enter in 1930 and had been re-admitted only after the 1950 WC).
Sweden advanced directly to the quarter-finals as a result of Austria's withdrawal, and they proceeded to beat Cuba 8–0. The hosts, France, were beaten by the holders, Italy, and Switzerland were seen off by Hungary. Czechoslovakia took Brazil to extra time in a notoriously feisty match in Bordeaux before succumbing in a replay; the South Americans proved too strong for the depleted Czechoslovak side (both Oldřich Nejedlý and František Plánička had suffered broken bones in the first game) and won 2–1. This was the last ever match to be replayed in a World Cup, with all winners of replay matches in 1938 having been eliminated in the next round.
Hungary destroyed Sweden in one of the semi-finals 5–1, while Italy and Brazil had the first of their many important World Cup clashes in the other. The Brazilians rested their star player Leônidas confident that they would qualify for the final, but the Italians won 2–1. Brazil topped Sweden 4–2 for third place.
Rumour has it, before the finals Benito Mussolini was to have sent a telegram to the team, saying "Vincere o morire!" (literally translated as "Win or die!"). This should not have been meant as a literal threat, but instead just an encouragement to win. However, no record remains of such a telegram, and World Cup player Pietro Rava said, when interviewed, "No, no, no, that's not true. He sent a telegram wishing us well, but no never 'win or die'."
The final itself took place at the Stade Olympique de Colombes in Paris. Vittorio Pozzo's Italian side took the lead early, but Hungary equalised within two minutes. The Italians took the lead again shortly after, and by the end of the first half were leading the Hungarians 3–1. Hungary never really got back into the game. With the final score favouring the Italians 4–2, Italy became the first team to successfully defend the title and were once more crowned World Cup winners.
Because of World War II, the World Cup would not be held for another 12 years, until 1950. As a result, Italy were the reigning World Cup holders for a record 16 years, from 1934 to 1950. The Italian Vice-President of FIFA, Dr. Ottorino Barassi, hid the trophy in a shoe-box under his bed throughout the Second World War and thus saved it from falling into the hands of occupying troops.
Venues.
Ten cities were planned to host the tournament:
Of these, all but Lyon ultimately hosted matches. Lyon did not due to Austria's withdrawal.
Squads.
For a list of all squads that appeared in the final tournament, see "1938 FIFA World Cup squads".
Goalscorers.
With seven goals, Leônidas is the top scorer in the tournament. In total, 84 goals were scored by 42 different players, with two of them credited as own goals.
FIFA retrospective ranking.
In 1986, FIFA published a report that ranked all teams in each World Cup up to and including 1986, based on progress in the competition, overall results and quality of the opposition. The rankings for the 1938 tournament were as follows:

</doc>
<doc id="59881" url="https://en.wikipedia.org/wiki?curid=59881" title="Ideal gas law">
Ideal gas law

The ideal gas law is the equation of state of a hypothetical ideal gas. It is a good approximation to the behavior of many gases under many conditions, although it has several limitations. It was first stated by Émile Clapeyron in 1834 as a combination of Boyle's law, Charles' law and Avogadro's Law. The ideal gas law is often written as:
where: 
It can also be derived microscopically from kinetic theory, as was achieved (apparently independently) by August Krönig in 1856 and Rudolf Clausius in 1857.
Equation.
The state of an amount of gas is determined by its pressure, volume, and temperature. The modern form of the equation relates these simply in two main forms. The temperature used in the equation of state is an absolute temperature: in the SI system of units, Kelvin.
Common form.
The most frequently introduced form is
where: 
In SI units, "P" is measured in pascals, "V" is measured in cubic metres, "n" is measured in moles, and "T" in Kelvin (The Kelvin scale is a shifted Celsius scale where 0.00 Kelvin = -273.15 degrees Celsius, the lowest possible temperature). R has the value 8.314 J·K−1·mol−1 or 0.08206 L·atm·mol−1·K−1or ≈2 calories if using pressure in standard atmospheres (atm) instead of pascals, and volume in litres instead of cubic metres.
Molar form.
How much gas is present could be specified by giving the mass instead of the chemical amount of gas. Therefore, an alternative form of the ideal gas law may be useful. The chemical amount ("n") (in moles) is equal to the mass ("m") (in grams) divided by the molar mass ("M") (in grams per mole):
By replacing "n" with "m / M", and subsequently introducing density "ρ" = "m"/"V", we get:
Defining the specific gas constant "R"specific as the ratio "R"/"M",
This form of the ideal gas law is very useful because it links pressure, density, and temperature in a unique formula independent of the quantity of the considered gas. Alternatively, the law may be written in terms of the specific volume "v", the reciprocal of density, as
It is common, especially in engineering applications, to represent the specific gas constant by the symbol "R". In such cases, the universal gas constant is usually given a different symbol such as R to distinguish it. In any case, the context and/or units of the gas constant should make it clear as to whether the universal or specific gas constant is being referred to.
Statistical mechanics.
In statistical mechanics the following molecular equation is derived from first principles:
where "P" is the absolute pressure of the gas measured in pascals; "N" is the number of molecules in the given volume "V". The number density is given by the ratio "N"/"V"; "kB" is the Boltzmann constant relating temperature and energy; and "T" is the absolute temperature in Kelvin .
The "number density" contrasts to the other formulation, which uses "n", the "number of moles" and "V", the volume. This relation implies that "R"="NAkB" where NA is Avogadro's constant, and the consistency of this result with experiment is a good check on the principles of statistical mechanics.
From this we can notice that for an average particle mass of "μ" times the atomic mass constant "m"u (i.e., the mass is "μ" u)
and since "ρ" = "mn", we find that the ideal gas law can be rewritten as:
In SI units, "P" is measured in pascals; "V" in cubic metres; "Y" is a dimensionless number; and "T" in Kelvin.
"k" has the value 1.38·10−23 J·K−1 in SI units.
Applications to thermodynamic processes.
The table below essentially simplifies the ideal gas equation for a particular processes, thus making this equation easier to solve using numerical methods.
A thermodynamic process is defined as a system that moves from state 1 to state 2, where the state number is denoted by subscript. As shown in the first column of the table, basic thermodynamic processes are defined such that one of the gas properties ("P", "V", "T", or "S") is constant throughout the process.
For a given thermodynamics process, in order to specify the extent of a particular process, one of the properties ratios (which are listed under the column labeled "known ratio") must be specified (either directly or indirectly). Also, the property for which the ratio is known must be distinct from the property held constant in the previous column (otherwise the ratio would be unity, and not enough information would be available to simplify the gas law equation).
In the final three columns, the properties ("P", "V", or "T") at state 2 can be calculated from the properties at state 1 using the equations listed.
Deviations from ideal behavior of real gases.
The equation of state given here applies only to an ideal gas, or as an approximation to a real gas that behaves sufficiently like an ideal gas. There are in fact many different forms of the equation of state. Since the ideal gas law neglects both molecular size and intermolecular attractions, it is most accurate for monatomic gases at high temperatures and low pressures. The neglect of molecular size becomes less important for lower densities, i.e. for larger volumes at lower pressures, because the average distance between adjacent molecules becomes much larger than the molecular size. The relative importance of intermolecular attractions diminishes with increasing thermal kinetic energy, i.e., with increasing temperatures. More detailed "equations of state", such as the van der Waals equation, account for deviations from ideality caused by molecular size and intermolecular forces.
A residual property is defined as the difference between a real gas property and an ideal gas property, both considered at the same pressure, temperature, and composition.
Derivations.
Empirical.
The ideal gas law can be derived from combining two empirical gas laws: the combined gas law and Avogadro's law. The combined gas law states that
where "C" is a constant which is directly proportional to the amount of gas, "n" (Avogadro's law). The proportionality factor is the universal gas constant, "R", i.e. "C" = "nR".
Hence the ideal gas law
Theoretical.
Kinetic theory.
The ideal gas law can also be derived from first principles using the kinetic theory of gases, in which several simplifying assumptions are made, chief among which are that the molecules, or atoms, of the gas are point masses, possessing mass but no significant volume, and undergo only elastic collisions with each other and the sides of the container in which both linear momentum and kinetic energy are conserved.
Statistical mechanics.
Let q = ("q"x, "q"y, "q"z) and p = ("p"x, "p"y, "p"z) denote the position vector and momentum vector of a particle of an ideal gas, respectively. Let F denote the net force on that particle. Then the time-averaged potential energy of the particle is:<br>
where the first equality is Newton's second law, and the second line uses Hamilton's equations and the equipartition theorem. Summing over a system of "N" particles yields
By Newton's third law and the ideal gas assumption, the net force of the system is the force applied by the walls of the container, and this force is given by the pressure "P" of the gas. Hence
where dS is the infinitesimal area element along the walls of the container. Since the divergence of the position vector q is
the divergence theorem implies that
where "dV" is an infinitesimal volume within the container and "V" is the total volume of the container.
Putting these equalities together yields
which immediately implies the ideal gas law for "N" particles:
where "n" = "N"/"N"A is the number of moles of gas and "R" = "N"A"k"B is the gas constant.

</doc>
<doc id="59886" url="https://en.wikipedia.org/wiki?curid=59886" title="Blast beat">
Blast beat

A blast beat is a drum beat that originated in death metal, and is often associated with extreme metal and grindcore (and more recently black metal and thrash). It is utilised by many different styles of metal. In Adam MacGregor's definition, "the blast-beat generally comprises a repeated, sixteenth-note figure played at a very fast tempo, and divided uniformly among the bass drum, snare, and ride, crash, or hi-hat cymbal." Blast beats have been described as, "maniacal percussive explosions, less about rhythm per se than sheer sonic violence".
Napalm Death is said to have coined the term, though this style of drumming had previously been practiced by D.R.I., Repulsion and others. Blast beats are made with rapid alternating or coinciding strokes primarily on the bass and snare drum. Diverse patterns and timings are also frequently used by more technical players, such as Gene Hoglan (Dethklok/Death/Dark Angel/Strapping Young Lad/Fear Factory/Testament), Alex Hernandez (Immolation), Max Duhamel (Kataklysm) and Flo Mounier (Cryptopsy). Alternative styles of blast beats include performing two strokes of the bass drum followed by one stroke of the snare drum. Pete Sandoval frequently uses this technique.
History.
The English band Napalm Death coined the term "blast beat", though this style of drumming had previously been practiced by others. Daniel Ekeroth argues that the blast beat was first performed by the Swedish group Asocial on their 1982 demo. D.R.I. ("No Sense"), Sepultura ("Antichrist"), S.O.D. ("Milk"), Sarcófago ("Satanas"), and Repulsion also included the technique prior to Napalm Death's emergence. Blast beats originated in performances by jazz drummers of the 1950s, 60s and 70s such as Tony Williams, Angelo Spampinato, and Sunny Murray, in particular his 3/28/1965 Greenwich Village recording of "Holy Ghost" with Albert Ayler. AllMusic contributor Thom Jurek credits Williams as the "true inventor of the blastbeat" in 1979. In 1970, the band Attila used a blast beat on their song "Brain Invasion" starting at the 2:04 mark and lasting about seven seconds. Blast roots in hardcore punk can be traced to recordings such as D.R.I's "No Sense" on their first EP (1983) and Beastie Boys "Riot Fight" on their first EP, "Pollywog Stew". Other examples include Heart Attack, Cryptic Slaughter and Lärm.
The blast beat as we know it today originated in the European hardcore and grindcore scenes in the 1980s. Contrary to popular belief, blast beats originated from punk and hardcore music, not metal music. In the UK punk and hardcore scene of the early 1980s there were many bands attempting to play as fast as possible. In 1985 emerging grindcore band Napalm Death replaced their former drummer Miles "Rat" Ratledge with Mick Harris, who brought to the band a whole new level of speed. Harris became the official drummer of Napalm Death and is credited for developing the term "blast beat", describing the fast notes played on the kick and snare. Harris started using the blast beat as a fundamental aspect of Napalm Death's early musical compositions. It was finally with Napalm Deaths first full-length album "Scum" (1987) that blast beat started to evolve into a distinct musical expression of its own. Blast beats became music from the mid to late 1980s popular in extreme music. The original use in metal music is generally attributed to Igor Cavalera (Sepultura), Mike Browning (Morbid Angel, Nocturnus), D.D. Crazy (Sarcófago), Dave 'Grave' Hollingshead (Repulsion) and Charlie Benante (Anthrax, SOD). Benante showcased the technique by a double-handed blast beat in the track "Milk" on the SOD album "Speak English or Die", later played single-handed on the live album "Live at Budokan". Although even earlier usage dates back to demos by Death from 1984, with drummer and vocalist Kam Lee showcasing usage in songs such as "Reign Of Terror and Curse Of The Priest". Members from Repulsion (back when they were known as Genocide) temporarily joined Death in 1985, so it's been speculated that they started their trademark widespread usage after first hearing it during their short tenure with Death. The blast beat evolved into its modern from as it was developed in the American death metal and grindcore scene of the late 1980s and early 1990s. Pete Sandoval, drummer of Terrorizer and later Morbid Angel, gave the blast beat a solid time signature and thus gave it a musical characteristic.
Blast beats eventually appeared in commercially successful metal music, beginning with Fear Factory's album "Demanufacture" and Slipknot's album "Iowa".
Characteristics.
A blast beat is traditionally played as an alternating single-stroke roll broken up between the kick drum and the snare drum. Blast beats are counted in 32nd or 16th notes. In a modern musical context blast beats are usually regarded as such when played at a minimum of above 90 beats per minute 32nd notes, or 180 bpm 16th notes. Early blast beats were generally quite slow and less precise compared to today's standards. Nowadays, a blast beat is normally played from 180 bpm 16th notes up to such high tempos as in the range of 250-280 bpm 16th notes (or even higher). There is also the "gravity blast", not to be confused with the one-handed gravity roll. This technique uses the rim of the snare drum as a fulcrum, allowing two snare hits with one downward motion (essentially doing the work of two hands with only one).
Typical blast beats consist of 8th-note patterns between both the bass and snare drum alternately, with the hi-hat or the ride synced. Variations exist such as displacing hi-hat/ride, snare and bass drum hits and/or using other cymbals such as splashes, crashes, chinas and even tambourines for accenting, for example when using odd time or playing progressively. While playing 8th or 8th note triplets some drummers choose to play in sync with one foot while others split the 8th notes between both feet. In blast beats in general, the notes on the kick drum can be played either with one foot only or by alternating both feet, referred to as a "two-foot" or "economy" blast.
Different drummers use different foot or hand techniques. Certain drummers, such as George Kollias, prefer to only use one foot while performing blast beats (known as "pushing"), as it gives them extra precision that is not easily attainable with two feet, and leaves the left foot free to add in more subdivisions, turning 16th note blasts with the feet into 32nd note blasts. Others, such as Trym Torson, prefer using two feet, as it allows for extra power. Drummers also will either use their wrists, their fingers, or a combination of both to control their drumsticks.
As blast beats have evolved, different types and interpretations have emerged. There are four main variations of the blast beat: the traditional blast, the bomb blast, the hammer blast and the freehand blast.
The traditional blast beat is a single-stroke roll alternating between the snare drum and kick drum. The ride hand is usually playing in unison with the kick drum. The traditional blast beat is structurally very similar to the skank beat, which can be regarded as a predecessor and a half time variation of the traditional blast beat. The skank beat originated in the early punk and thrash metal scene as a drum beat for extreme music. The skank beat is similar to the blast beat as it alternates between the kick and the snare, with the difference that the ride hand plays notes in unison with both kick and snare. A skank beat is in other words a sped up 2/4 rock or polka beat. In the US the skank beat was early on also referred to as the "Slayer" or "thrash" beat due to its popularity among thrash metal bands such as Slayer.
The bomb blast is essentially a combination of blast beat and double bass drumming. When measured in 16th notes a bomb blast consists of 8th notes on the snare played above a 16th notes kick drum line. Most drummers play this beat by leading with the snare, while the traditional blast beat is usually led with the kick. The bomb blast became popular among 1990s death metal bands such as Cannibal Corpse, which is why the bomb blast is also referred to as the "Cannibal" blast.
The hammer blast is played with the kick and snare in unison. Instead of playing 8th notes kick and snare in alternation and thus creating a 16th notes roll, the hammer blast is played as a straight 8th notes roll on the kick and snare simultaneously. The advantage of the hammer blast is that you only need one fast hand, which usually is your leading hand (right for right-handed and left for left-handed). If your weaker hand can't keep up with the 8th notes snare line, you can play quarter notes. The kick drum line can be played with one foot as well as a two-footed economy blast. When played at an extremely fast tempo, the hammer blast can be referred to as a "hyper blast". The hammer blast became popular in death metal music of the early 1990s.
The freehand blast, also known as the gravity blast, utilizes the gravity roll technique in a blast beat context. Of all the main blast beat variations, this one is the most recent to have emerged. The snare line is played as a 16th notes single stroke roll, also known as a gravity roll or single handed roll. The roll is played with an up and down motion in which you push and pull the drumstick on and off the snare drum. By using the snare rim as a fulcrum you create a stroke each time you push and pull the drumstick up and down. The concept behind the gravity roll is not new, but is noted for being brought into modern music by drummer Johhny Rabb. Rabb has published the book "The Official Freehand Technique", which covers the gravity roll technique. 
Examples.
Examples of the four main blast beat variations in drum tab:
The first example is a hammer blast. The second example shows a traditional blast beat - essentially a skank beat played at a high tempo (this particular one leads with the bass drum, but the snare can lead as well). Example #3 shows a blast beat with double bass, known as a bomb blast. Example #4 illustrates a freehand blast, also known as a gravity blast.

</doc>
<doc id="59888" url="https://en.wikipedia.org/wiki?curid=59888" title="IEC">
IEC

IEC may refer to:

</doc>
<doc id="59889" url="https://en.wikipedia.org/wiki?curid=59889" title="Real Academia Española">
Real Academia Española

The Real Academia Española (English: Royal Spanish Academy), generally abbreviated as RAE, is the official royal institution responsible for overseeing the Spanish language. It is based in Madrid, Spain, but is affiliated with national language academies in twenty-one other hispanophone (Spanish-speaking) nations through the Association of Spanish Language Academies. The RAE's emblem is a fiery crucible, and its motto is "" ("cleans, [it fixes, and casts splendour").
The RAE dedicates itself to language planning by applying linguistic prescription aimed at promoting linguistic unity within and between the various territories, to ensure a common standard in accordance with Article 1 of its founding charter: "... to ensure the changes that the Spanish language undergoes [...] do not break the essential unity it enjoys throughout the Spanish-speaking world."
The proposed language guidelines are shown in a number of works. The priorities are the "" (Dictionary of Spanish Language of the Royal Spanish Academy or DRAE), edited periodically twenty-three times since 1780, and its grammar, last edited in October 2014. The Academy has a formal procedure for admitting words to its publications.
The headquarters, opened in 1894, is located at Calle Felipe IV, 4, in the ward of Jerónimos, next to the Museo del Prado. The Center for the Studies of the Royal Spanish Academy, opened in 2007, is located at Calle Serrano 187–189.
History.
The was founded in 1713, modelled after the Italian (1582) and the French (1635), with the purpose "to fix the voices and vocabularies of the Castilian language with propriety, elegance, and purity". King Philip V approved its constitution on 3 October 1714, placing it under the Crown's protection.
Its aristocratic founder, , Marquis of Villena and Duke of Escalona, described its aims as "to assure that Spanish speakers will always be able to read Cervantes" – by exercising a progressive up-to-date maintenance of the formal language.
The RAE began establishing rules for the orthography of Spanish beginning in 1741 with the first edition of the ' (spelled ' from the second edition onwards). The proposals of the Academy became the official norm in Spain by royal decree in 1844, and they were also gradually adopted by the Spanish-speaking countries of America.
Several reforms were introduced in the ' (1959), and since then the rules have undergone continued adjustment, in consultation with the other national language academies. The current rules and practical recommendations are presented in the latest edition of the ' (1999).
In 1994, the RAE ruled that the Spanish consonants ""CH"" () and ""LL"" () would hence be alphabetized under "C" and under "L", respectively, and not as separate, discrete letters, as in the past. The RAE eliminated monosyllabic accented vowels where the accent did not serve in changing the word's meaning, examples include: """" ("gave"), """" ("saw"), both had an acutely accented vowel """"; yet the monosyllabic word """" ("I know", the first person, singular, present of """", "to know"; and the singular imperative of """", "to be") retains its acutely accented vowel in order to differentiate it from the reflexive pronoun """".
Composition.
Members of the Academy are known as ' (), chosen from among prestigious persons in the arts and sciences, including several Spanish-language authors, known as ' (), similarly to their counterparts. The "Números" are elected for life by the other academicians. Each academician holds a seat labeled with a letter from the Spanish alphabet; upper- and lower-case letters are separate seats.

</doc>
<doc id="59890" url="https://en.wikipedia.org/wiki?curid=59890" title="Institut d'Estudis Catalans">
Institut d'Estudis Catalans

The Institut d'Estudis Catalans (, English: "Institute for Catalan Studies"), also known by the acronym IEC, is an academic institution which seeks to undertake research and study into "all elements of Catalan culture". 
The IEC is known principally for its work in standardizing the Catalan language. The Institute's current president is Salvador Giner, elected to the office for four years in June 2005, and to a second term in 2009. The IEC is based in Barcelona, the capital of Catalonia, and the second largest city in Spain. 
Enric Prat de la Riba, who was to become the first President of the Commonwealth of Catalonia, signed the founding document of the Institute, as president of the Barcelona County Council on June 17, 1907. The IEC is one of a number of cultural and scientific institutions created at that time to lend greater prestige to the Catalan language and culture; others include the "Biblioteca de Catalunya" (Library of Catalonia), the "Escola Industrial" (Industrial School), the "Escola Superior de Belles Arts" (Higher School of Fine Arts) and the "Escola del Treball" (School of Labour), "el Centre de Recerca Matemàtica". Prat de la Riba also founded the "Escola de l'Administració Local" (School of Local Administration), in order to create a body of Catalan civil servants for the regional government. 
The IEC was admitted to the Union Académique Internationale in 1922, shortly after the establishment of the latter.
During the dictatorship of Franco, along with many other Catalan cultural institutions, the Institut lived a semiclandestine existence, and was not officially restored to its previous status in the field of language standardisation until a 1991 bill was passed by the (also restored) Catalan Parliament. 
The IEC inspired the creation of the Institut d'Estudis Occitans in Occitania. Occitania is an area in southern France where Occitan (often called "Provençal") has been historically spoken.
Philological Section.
The IEC's Philological Section was founded in 1911. Antoni Maria Alcover served as its first president. Along with Pompeu Fabra, the Philological Section worked to establish a series of spelling norms that were approved by members in 1913. These became the foundation of modern written Catalan which are still in use today. Similarly, in 1917, the "Diccionari Ortogràfic de l'Institut" was published; it soon became a dictionary of spelling norms irredeemably tied to the reputation of former Institute Director Pompeu Fabra. The dictionary went through several editions, with the last released in 1937. This work and others were the basis of Fabra's "Diccionari General de la Llengua Catalana" published in 1932, a general-purpose dictionary that became a standard reference work throughout the various Catalan-speaking territories.
Officially the IEC provides standards for the language as a whole: the Philological Section has members from Catalonia proper, Northern Catalonia (located in France), the Balearic Islands, Valencia, Alghero in Sardinia and the Principality of Andorra (the only country where Catalan is the sole official language). However, the Valencian Region south of Catalonia has its own language academy, the "Acadèmia Valenciana de la Llengua", which nevertheless formally acknowledges that theirs is one variant of the common language. In an area known as the "Franja de Ponent", the eastern edge of Aragon adjacent to Catalonia where Catalan is spoken by about 44,000 inhabitants, the rules are followed "de facto" although Catalan is not an official language in that region. 
Other IEC works of note include the "Diccionari de la Llengua Catalana" published in 1995, and the regionally sensitive "Diccionari Català-Valencià-Balear" (Catalan-Valencian-Balearic Dictionary). Notable members of the Philological Section include Josep Carner, Àngel Guimerà and Joan Maragall.

</doc>
<doc id="59891" url="https://en.wikipedia.org/wiki?curid=59891" title="Vedea River">
Vedea River

The Vedea River () in southern Romania flows from the Cotmeana Plateau and runs into the Danube, having a total length of 224 km, of which 33 km is regulated.
It flows in Argeș, Olt and Teleorman counties. The towns Alexandria and Roșiori de Vede lie in the vicinity of the river.
The name of the river is Dacian in origin, from Indo-European *"wed", "water".
Towns and villages.
The following towns and villages are situated along the river Vedea, from source to mouth: Făgețelu, Spineni, Tătulești, Optași, Corbu, Nicolae Titulescu, Văleni, Stejaru, Roşiorii de Vede, Vedea, Peretu, Plosca, Mavrodin, Buzescu, Alexandria, Poroschia, Brânceni, Smârdioasa, Cervenia, Conțești, Bragadiru, Bujoru.
Tributaries.
The following rivers are tributaries to the river Vedea:
Left: Vedița, Cotmeana, Tecuci, Burdea, Râul Câinelui, Teleorman
Right: Eiul, Brătășanul, Tisăr, Ciorâca, Cupen, Plapcea, Florișoru, Dorofei, Bratcov, Bărâcea, Gearanta, Valea Găuriciu, Valea Viei

</doc>
<doc id="59892" url="https://en.wikipedia.org/wiki?curid=59892" title="List of highest-grossing films">
List of highest-grossing films

Films generate income from several revenue streams, including theatrical exhibition, home video, television broadcast rights and merchandising. However, theatrical box office earnings are the primary metric for trade publications (such as Box Office Mojo and "Variety") in assessing the success of a film, mostly because of the availability of the data compared to sales figures for home video and broadcast rights, but also because of historical practice. Included on the list are charts of the top box office earners (ranked by both the nominal and real value of their revenue), a chart of high-grossing films by calendar year, a timeline showing the transition of the highest-grossing film record, and a chart of the highest-grossing film franchises and series. All charts are ranked by international theatrical box office performance where possible, excluding income derived from home video, broadcasting rights and merchandise.
Traditionally, war films, musicals and historical dramas have been the most popular genres, but franchise films have been among the best performers in the 21st century. Seven out of eight films from the "Harry Potter" franchise and all the films from Peter Jackson's "Middle-earth" series are included in the nominal earnings chart, while the "Transformers" and "Pirates of the Caribbean" franchises both feature prominently. There is also continued interest in the superhero genre: "Batman" and "Superman" from DC Comics and films based on the Marvel Comics brand, such as "Spider-Man", "X-Men" and films in the Marvel Cinematic Universe, have generally done well. The only films in the top ten that are not adapted from a pre-existing property or a sequel are the top two, "Avatar" and "Titanic", both directed by James Cameron. Animated family films have performed consistently well, with Disney films enjoying lucrative re-releases prior to the home-video era. Disney also enjoyed later success with films such as "Frozen" (the highest-grossing animated film) and "The Lion King", as well as with its Pixar brand, of which the "Toy Story" films and "Finding Nemo" have been the best performers. Beyond Pixar animation, the "Shrek", "Ice Age", "Madagascar" and "Despicable Me" series have met with the most success.
While inflation has eroded away the achievements of most films from the 1960s and 1970s, there are franchises originating from that period that are still active. Besides "Superman", "James Bond" and "Star Trek" films are still being released periodically, while the "Star Wars" saga and "Planet of the Apes" were reprised after a lengthy hiatus; "Indiana Jones" also saw a successful comeback after lying dormant for nearly twenty years. All six are still among the highest-grossing franchises, despite starting over thirty years ago. Some of the older films that held the record of highest-grossing film still have respectable grosses by today's standards, but no longer compete numerically against today's top-earners in an era of much higher individual ticket prices. When properly adjusted for inflation, however, on that comparative scale "Gone with the Wind"—which was the highest-grossing film outright for twenty-five years—is still the highest-grossing film of all-time. All grosses on the list are expressed in U.S. dollars at their nominal value, except where stated otherwise.
Highest-grossing films.
With a worldwide box-office gross of over $2.7 billion, "Avatar" is often proclaimed to be the "highest-grossing" film, but such claims usually refer to theatrical revenues only and do not take into account home video and television income, which can form a significant portion of a film's earnings. Once revenue from home entertainment is factored in it is not immediately clear which film is the most successful. "Titanic" earned $1.2 billion from video and DVD sales and rentals, in addition to the $2.2 billion it grossed in theaters. While complete sales data are not available for "Avatar", it earned $345 million from the sale of sixteen million DVD and Blu-ray units in North America, and ultimately sold a total of thirty million DVD and Blu-ray units worldwide. After home video income is accounted for, both films have earned over $3 billion. Television broadcast rights will also substantially add to a film's earnings, with a film often earning as much as 20–25% of its theatrical box-office for a couple of television runs on top of pay-per-view revenues; "Titanic" earned a further $55 million from the NBC and HBO broadcast rights, equating to about 9% of its North American gross.
When a film is highly exploitable as a commercial property, its ancillary revenues can dwarf its income from direct film sales. "The Lion King" earned over $2 billion in box-office and home video sales, but this pales in comparison to the $6 billion earned at box offices around the world by the stage adaptation. Merchandising can be extremely lucrative too: "The Lion King" also sold $3 billion of merchandise, while Pixar's "Cars"—which earned $462 million in theatrical revenues and was only a modest hit by comparison to other Pixar films—generated global merchandise sales of over $8 billion in the five years after its 2006 release. Pixar also had another huge hit with "Toy Story 3", which generated almost $10 billion in merchandise retail sales in addition to the $1 billion it earned at the box office.
On this chart, films are ranked by the revenues from theatrical exhibition at their nominal value, along with the highest positions they attained. Twenty-four films in total have grossed in excess of $1 billion worldwide, of which three have grossed over $2 billion, with "Avatar" ranked in the top position. All of the films have had a theatrical run (including re-releases) in the 21st century, and films that have not played during this period do not appear on the chart because of ticket-price inflation, population size and ticket purchasing trends not being considered.
Highest-grossing films adjusted for inflation.
Because of the long-term effects of inflation, notably the significant increase of movie theater ticket prices, the list unadjusted for inflation gives far more weight to later films. The unadjusted list, while commonly found in the press, is therefore largely meaningless for comparing films widely separated in time, as many films from earlier eras will never appear on a modern unadjusted list, despite achieving higher commercial success when adjusted for price increases. To compensate for the devaluation of the currency, some charts make adjustments for inflation, but not even this practice fully addresses the issue since ticket prices and inflation do not necessarily parallel one another. For example, in 1970 tickets cost $1.55 or about $6.68 in inflation-adjusted 2004 dollars; by 1980, prices had risen to about $2.69, a drop to $5.50 in inflation-adjusted 2004 dollars. Ticket prices have also risen at different rates of inflation around the world, further complicating the process of adjusting worldwide grosses.
Another complication is release in multiple formats for which different ticket prices are charged. One notable example of this phenomenon is "Avatar", which was also released in 3D and IMAX: almost two-thirds of tickets for that film were for 3D showings with an average price of $10, and about one-sixth were for IMAX showings with an average price over $14.50, compared to a 2010 average price of $7.61 for 2D films. Social and economic factors such as population change and the growth of international markets also impact on the number of people purchasing theater tickets, along with audience demographics where some films sell a much higher proportion of discounted children's tickets, or perform better in big cities where tickets cost more.
The measuring system for gauging a film's success is based on unadjusted grosses, mainly because historically this is the way it has always been done because of the practices of the film industry: the box office receipts are compiled by theaters and relayed to the distributor, which in turn releases them to the media. Converting to a more representative system that counts ticket sales rather than gross is also fraught with problems because the only data available for older films are the sale totals. As the motion picture industry is highly oriented towards marketing currently released films, unadjusted figures are always used in marketing campaigns so that new blockbuster films can much more easily achieve a high sales ranking, and thus be promoted as a "top film of all time", so there is little incentive to switch to a more robust analysis from a marketing or even newsworthy point of view.
Despite the inherent difficulties in accounting for inflation, several attempts have been made. Estimates depend on the price index used to adjust the grosses, and the exchange rates used to convert between currencies can also impact upon the calculations, both of which can have an effect on the ultimate rankings of an inflation adjusted list. "Gone with the Wind"—first released in 1939—is generally considered to be the most successful film, with "Guinness World Records" in 2014 estimating its adjusted global gross at $3.4 billion. Estimates for "Gone with the Wind"s adjusted gross have varied substantially: its owner, Turner Entertainment, estimated its adjusted earnings at $3.3 billion in 2007, a few years earlier than the "Guinness" estimate; other estimates fall either side of this amount, with one putting its gross just under $3 billion in 2010, while another provided an alternative figure of $3.8 billion in 2006. Which film is "Gone with the Wind"s nearest rival depends on the set of figures used: "Guinness" have "Avatar" in second place with $3 billion, while other estimates see "Titanic" in the runner-up spot with first-run worldwide earnings of almost $2.9 billion at 2010 prices. The only other film that all sources agree grossed in excess of $2 billion at current prices is "Star Wars"; according to "Guinness" it has earned $2.8 billion at 2014 price levels, while other sources from 2010/2011 put its adjusted earnings at $2.2–2.6 billion.
High-grossing films by year.
Audience tastes were fairly eclectic during the 20th century, but several trends did emerge. During the silent era, films with war themes were popular with audiences, with "The Birth of a Nation" (American Civil War), "The Four Horsemen of the Apocalypse", "The Big Parade" and "Wings" (all World War I) becoming the most successful films in their respective years of release, with the trend coming to an end with "All Quiet on the Western Front" in 1930. With the advent of sound in 1927, the musical—the genre best placed to showcase the new technology—took over as the most popular type of film with audiences, with 1928 and 1929 both being topped by musical films. The genre continued to perform strongly in the 1930s, but the outbreak of World War II saw war themed films dominate again during this period, starting with "Gone with the Wind" (American Civil War) in 1939, and finishing with "The Best Years of Our Lives" (World War II) in 1946. "Samson and Delilah" (1949) saw the beginning of a trend of increasingly expensive historical dramas set during Ancient Rome/biblical times throughout the 1950s as cinema competed with television for audiences, with "Quo Vadis", "The Robe", "The Ten Commandments", "Ben-Hur" and "Spartacus" all becoming the highest-grossing film of the year during initial release, before the genre started to wane after several high-profile failures. The success of "White Christmas" and "South Pacific" in the 1950s foreshadowed the comeback of the musical in the 1960s with "West Side Story", "Mary Poppins", "My Fair Lady", "The Sound of Music" and "Funny Girl" all among the top films of the decade. The 1970s saw a shift in audience tastes to high concept films, with six such films made by either George Lucas or Steven Spielberg topping the chart during the 1980s. The 21st century has seen an increasing dependence on franchises and adaptations, with "Avatar" in 2009 being the only chart-topper forming an original work.
Steven Spielberg is the most represented director on the chart with six films to his credit, occupying the top spot in 1975, 1981, 1982, 1984, 1989 and 1993. Cecil B. DeMille (1932, 1947, 1949, 1952 and 1956) and William Wyler (1942, 1946, 1959 and 1968) are in second and third place with five and four films respectively, while D. W. Griffith (1915, 1916 and 1920), George Roy Hill (1966, 1969 and 1973) and James Cameron (1991, 1997 and 2009) all feature heavily with three films apiece. George Lucas directed two chart-toppers in 1977 and 1999, but also served in a strong creative capacity as a producer and writer in 1980, 1981, 1983, 1984 and 1989 as well. The following directors have also all directed two films on the chart: Frank Lloyd, King Vidor, Frank Capra, Michael Curtiz, Leo McCarey, Alfred Hitchcock, David Lean, Stanley Kubrick, Guy Hamilton, Mike Nichols, William Friedkin, Peter Jackson, Gore Verbinski and Michael Bay; Mervyn LeRoy, Ken Annakin and Robert Wise are each represented by one solo credit and one shared credit, and John Ford co-directed two films. Disney films are usually co-directed and some directors have served on several winning teams: Wilfred Jackson, Hamilton Luske, Clyde Geronimi, David Hand, Ben Sharpsteen, Wolfgang Reitherman and Bill Roberts have all co-directed at least two films on the list. Only five directors have topped the chart in consecutive years: McCarey (1944 and 1945), Nichols (1966 and 1967), Spielberg (1981 and 1982), Jackson (2002 and 2003) and Verbinski (2006 and 2007).
Because of release schedules—especially in the case of films released towards the end of the year—and different release patterns across the world, many films can do business in two or more calendar years; therefore the grosses documented here are not confined to just the year of release. Grosses are not limited to original theatrical runs either, with many older films often being re-released periodically so the figures represent all the business a film has done since its original release; a film's first-run gross is included in brackets after the total if known. In the cases where estimates conflict both films are recorded, and in cases where a film has moved into first place because of being re-released the previous record-holder is also retained. Because of incomplete data it cannot be known for sure how much money some films have made and when they made it, but generally the chart chronicles the films from each year that went on to earn the most. At least one film every year has generated $100 million in gross revenue at the box office since 1967, and from 2008 each year has succeeded in producing a billion dollar grossing film.
( ... ) Since grosses are not limited to original theatrical runs, a film's first-run gross is included in brackets after the total if known.
Timeline of highest-grossing films.
At least ten films have held the record of 'highest-grossing film' since "The Birth of a Nation" assumed the top spot in 1915. Both "The Birth of a Nation" and "Gone with the Wind" spent twenty-five consecutive years apiece as the highest-grosser, with films directed by Steven Spielberg holding the record on three occasions and James Cameron—the current holder—twice. Spielberg became the first director to break his own record when "Jurassic Park" overtook "E.T.", and Cameron emulated the feat when "Avatar" broke the record set by "Titanic".
Some sources claim that "The Big Parade" superseded "The Birth of a Nation" as highest-grossing film, eventually being replaced by "Snow White and the Seven Dwarfs", which in turn was quickly usurped by "Gone with the Wind". Exact figures are not known for "The Birth of a Nation", but contemporary records put its worldwide earnings at $5.2 million as of 1919. Its international release was delayed by World War I, and it was not released in many foreign territories until the 1920s; coupled with further re-releases in the United States, its $10 million earnings as reported by "Variety" in 1932 are consistent with the earlier figure. At this time, "Variety" still had "The Birth of a Nation" ahead of "The Big Parade" ($6,400,000) on distributor rentals and—if its estimate is correct—"Snow White and the Seven Dwarfs" ($8,500,000) would not have earned enough on its first theatrical run to take the record; although it would have been the highest-grossing 'talkie', displacing "The Singing Fool" ($5,900,000). Although received wisdom holds that it is unlikely "The Birth of a Nation" was ever overtaken by a silent-era film, the record would fall to 1925's "Ben-Hur" ($9,386,000) if "The Birth of a Nation" earned significantly less than its estimated gross. In addition to its gross rental earnings through public exhibition, "The Birth of a Nation" played at a large number of private, club and organizational engagements which figures are unavailable for. It was hugely popular with the Ku Klux Klan who used it to drive recruitment, and at one point "Variety" estimated its total earnings to stand at around $50 million. Despite later retracting the claim, the sum has been widely reported even though it has never been substantiated. While it is generally accepted that "Gone with the Wind" took over the record of highest-grossing film on its initial release—which is true in terms of public exhibition—it is likely it did not overtake "The Birth of a Nation" in total revenue until a much later date, with it still being reported as the highest earner up until the 1960s. "Gone with the Wind" itself may have been briefly overtaken by "The Ten Commandments" (1956), which closed at the end of 1960 with worldwide rentals of $58–60 million compared to "Gone with the Wind'"s $59 million; if it did claim the top spot its tenure there was short-lived, since "Gone with the Wind" was re-released the following year and increased its earnings to $67 million. Depending on how accurate the estimates are, the 1959 remake of "Ben-Hur" may also have captured the record from "Gone with the Wind": as of the end of 1961 it had earned $47 million worldwide, and by 1963 it was trailing "Gone with the Wind" by just $2 million with international takings of $65 million, ultimately earning $66 million from its initial release.
Another film purported to have been the highest-grosser is the 1972 pornographic film, "Deep Throat". In 1984, Linda Lovelace testified to a United States Senate Judiciary Subcommittee on juvenile justice that the film had earned $600 million; this figure has been the subject of much speculation, since if it is accurate then the film would have made more money than "Star Wars", and finished the 1970s as the highest-grossing film. The main argument against this figure is that it simply did not have a wide enough release to sustain the sort of sums that would be required for it to ultimately gross this amount. Exact figures are not known, but testimony in a federal trial in 1976—about four years into the film's release—showed the film had grossed over $25 million. Roger Ebert has reasoned it possibly did earn as much as $600 million on paper, since mobsters owned most of the adult movie theaters during this period and would launder income from drugs and prostitution through them, so probably inflated the box office receipts for the film.
"The Birth of a Nation", "Gone with the Wind", "The Godfather", "Jaws", "Star Wars", "E.T." and "Avatar" all increased their record grosses with re-releases. The grosses from their original theatrical runs are included here along with totals from re-releases up to the point that they lost the record; therefore the total for "The Birth of a Nation" includes income from its reissues up to 1940; the total for "Star Wars" includes revenue from the late 1970s and early 1980s reissues but not from the 1997 Special Edition; the total for "E.T." incorporates its gross from the 1985 reissue but not from 2002; the total for "Avatar"—as the current record-holder—includes all its earnings at the present time. "Gone with the Wind" is represented twice on the chart: the 1940 entry includes earnings from its staggered 1939–1942 release (roadshow/general release/second-run) along with all of its revenue up to the 1961 reissue prior to losing the record to "The Sound of Music" in 1966; its 1971 entry—after it took back the record—includes income from the 1967 and 1971 reissues but omitting later releases. "The Godfather" was re-released in 1973 after its success at the 45th Academy Awards, and "Jaws" was released again in 1976, and their grosses here most likely include earnings from those releases. "The Sound of Music", "The Godfather", "Jaws", "Jurassic Park" and "Titanic" increased their earnings with further releases in 1973, 1997, 1979, 2013 and 2012 respectively, but they are not included in the totals here since they had already conceded the record prior to being re-released.
Highest-grossing franchises and film series.
Prior to 2000, only seven film series had grossed over $1 billion at the box office: "James Bond", "Star Wars", "Indiana Jones", "Rocky", "Batman", "Jurassic Park" and "Star Trek". Since the turn of the century that number has increased to over forty, excluding one-off hits such as "Avatar", "Titanic", "Frozen" and "Alice in Wonderland". This is partly due to inflation and market growth, but also to Hollywood's adoption of the franchise model: films that have built-in brand recognition, such as being based on a well known literary source or an established character. The methodology is based on the concept that films associated with things audiences are already familiar with can be more effectively marketed to them, and as such are known as "pre-sold" films within the industry.
The films in the cross-franchise Marvel Cinematic Universe have collectively grossed the most, amassing over $9 billion at the box office, although the Eon "James Bond" films have earned over $14 billion in total when adjusted to current prices. The "Harry Potter" franchise is the highest-grossing series based on a single property, earning nearly $8 billion at the box office; "Harry Potter" has also generated at least $3.5 billion in home video revenue, taking total consumer spending on the films to over $11 billion. If ancillary income from merchandise is included, then "Star Wars" is the most lucrative property; it holds the "Guinness" world record for the "most successful film merchandising franchise" and was valued at £19.51 billion in 2012 (approximately $30 billion). The Marvel Cinematic Universe is the only series where three films have grossed in excess of $1 billion, and the two "Avengers" films comprise the only franchise where each installment has grossed over $1 billion. "Avengers" is also the only franchise to have a series average of over $1 billion per film, although the "Star Wars", "Harry Potter", "Pirates of the Caribbean" and "Jurassic Park" franchises and Peter Jackson's "Middle-earth" adaptation also average over $1 billion adjusted for inflation.

</doc>
<doc id="59893" url="https://en.wikipedia.org/wiki?curid=59893" title="1950 FIFA World Cup">
1950 FIFA World Cup

The 1950 FIFA World Cup, held in Brazil from 24 June to 16 July 1950, was the fourth FIFA World Cup. It was the first World Cup since 1938, the planned 1942 and 1946 competitions having been cancelled owing to World War II. It was won by Uruguay, who had won the inaugural competition in 1930, clinching the cup by beating the hosts Brazil 2–1 in the deciding match of the four-team final group (this was the only tournament not decided by a one-match final). It was also the first tournament where the trophy was referred to as the Jules Rimet Cup, to mark the 25th anniversary of Jules Rimet's presidency of FIFA.
Host selection.
Because of World War II, the World Cup had not been staged since 1938; the planned World Cups of 1942 and 1946 were both cancelled. After the war, FIFA were keen to resurrect the competition as soon as possible, and they began making plans for a World Cup tournament to take place. In the aftermath of the war, much of Europe lay in ruins. As a result, FIFA had some difficulties finding a country interested in hosting the event, since many governments believed that their scarce resources ought to be devoted to more urgent priorities than a sporting celebration.
The World Cup was at risk of not being held for sheer lack of interest from the international community, until Brazil presented a bid at the 1946 FIFA Congress, offering to host the event on condition that the tournament take place in 1950 (it was originally planned to take place in 1949). Brazil and Germany had been the leading bidders to host the cancelled 1942 World Cup; since both the 1934 and 1938 tournaments had been held in Europe, football historians generally agree that the 1942 event would most likely have been awarded to a South American host country. Brazil's new bid was very similar to the mooted 1942 bid and was quickly accepted.
Qualification.
Having secured a host nation, FIFA would still dedicate some time to persuading countries to send their national teams to compete.
Italy was of particular interest as the long-standing defending champions, having won the two previous tournaments in 1934 and 1938; however, Italy's national team was weakened severely as most of its starting line-up perished in the Superga air disaster one year before the start of the tournament. The Italians were eventually persuaded to attend, but travelled by boat rather than by plane.
Because Brazil and Italy qualified automatically, there were 14 remaining places available. Of these, seven were allocated to Europe, six to the Americas, and one to Asia.
Banned teams.
Both Germany (still occupied and partitioned) and Japan (still occupied) were not permitted to participate in qualification. The French-occupied Saarland had been accepted by FIFA two weeks before the World Cup, several months before the German Football Association (DFB) was reinstated, while Soviet-occupied East Germany had not yet founded a football association. Italy, Austria, and other countries that had been involved in the World War as allies of Germany, were not subject to international sanctions, and Italy qualified automatically as defending champions of 1938.
British nations.
The British nations were invited to take part, having rejoined FIFA four years earlier, after 17 years of self-imposed exile. It was decided to use the 1949–50 British Home Championship as a qualifying group, with the top two teams qualifying. England finished first and Scotland second.
Teams refusing to participate.
A number of teams refused to participate in the qualifying tournament, including most nations behind the Iron Curtain, such as the Soviet Union, 1938 finalists Hungary and 1934 finalists Czechoslovakia.
Withdrawals during qualification.
Argentina, Ecuador and Peru in South America withdrew after the qualifying draw (in Argentina's case because of a dispute with the Brazilian Football Confederation). This meant that Chile, Bolivia, Paraguay and Uruguay qualified from South America by default. In Asia, the Philippines, Indonesia and Burma all withdrew, leaving India to qualify by default. In Europe, Austria withdrew, claiming its team was too inexperienced. Belgium also withdrew from the qualification tournament. These withdrawals meant that Switzerland and Turkey qualified without having to play their final round of matches.
Withdrawals after qualification.
Before the qualification competition, George Graham, chairman of the Scottish Football Association (SFA), had said that Scotland would only travel to Brazil as winners of the Home Championship. (England, by contrast, had committed to attending, even if they finished in second place). After Scotland ended up in second place behind England, the Scottish captain George Young, encouraged by England captain Billy Wright, pleaded with the SFA to change its mind and accept the place in Brazil; however, Graham refused to change his position and so Scotland withdrew from the tournament.
Turkey also withdrew, citing financial problems and the cost of travelling to South America. FIFA invited Portugal and France, who had both been eliminated in qualifying, to fill the gaps left by Scotland and Turkey. Portugal refused, but France initially accepted, and was entered into the draw.
Draw and subsequent withdrawals.
The draw, held in Rio on 22 May 1950, allocated the fifteen remaining teams into four groups:
After the draw, the Indian football association AIFF decided against going to the World Cup, citing travel costs (although FIFA had agreed to bear a major part of the travel expenses), lack of practice time, team selection issues and valuing the Olympics over the FIFA World Cup.
Although FIFA had imposed a rule banning barefoot play following the 1948 Olympics where India had played barefoot, the Indian captain at the time, Sailen Manna, claimed that this was not part of the AIFF's decision.
France also withdrew, citing the amount of travel that would be required in Group 4. There was not enough time to invite further replacement teams or to reorganise the groups, so the tournament featured only thirteen teams, with just two nations in Group 4.
Of the thirteen teams that competed, only one, England, was making its debut. Several of the Latin American teams were competing for the first time since the inaugural 1930 tournament—this included undefeated Uruguay, as well as Mexico, Chile, Paraguay and Bolivia. Yugoslavia was also making its first appearance since 1930. This would be the United States' last appearance at the World Cup finals until 1990, and Bolivia's last until 1994.
Format.
The Brazilian organizers of the tournament proposed a new format in which the 16 teams were divided into four first round groups (or "pools" as they were then called) of four teams, with the four group winners advancing to a final group stage, playing in round-robin format to determine the winner. The main reason for this choice was money: the organizers had spent a great deal on stadium and infrastructure investment. A straight knockout tournament, as had been used in 1934 and 1938, would feature only sixteen games (including the third-place playoff), while the proposed two-group format would guarantee thirty games, and thus more ticket revenue. In addition, this format would guarantee each team at least three games, and thus provide more incentive for European teams to make the journey to South America and compete. FIFA originally resisted this proposal, but backed down after Brazil threatened to back out of hosting the tournament if this format was not used.
In each group teams were awarded 2 points for a win and 1 point for a draw. Had there been a tie on points for first place in a group, then a playoff would have been held to determine the group winner.
The entire tournament was arranged in such a way that the four first round groups had no geographical basis. Hence, several teams were obliged to cover large distances to complete their program, although Brazil was allowed to play two of its three group matches in Rio de Janeiro while its other group game was held in the relatively nearby city of São Paulo.
Summary.
A combined Great Britain team had recently beaten the rest of Europe 6–1 in an exhibition match and England went into the competition as one of the favourites; however, it was not to be, as they went crashing out on 29 June in a shock 1–0 defeat by the United States which, combined with their 1–0 defeat by Spain, led to England being eliminated. Italy, the defending champions, lost their unbeaten record at the World Cup finals when the team was defeated 3–2 by Sweden in its opening match. Because of this defeat, Italy failed to progress to the second round. The final match in group 1 between the Switzerland and the Mexico it was the first time a national team did not play in their own kit. Both teams arrived with only their red kits. There was a meeting held between the Brazilian Football Confederation they tossed a coin and Mexico won so they would get to play in their own kit however they gave a nice gesture to the Swiss by allowing Switzerland to wear their kit so Mexico did the honors of changing. The local team that lent out their shirts was Esporte Clube Cruzeiro a team in Porto Allegre. The shirts had vertical blue and white stripes. .<ref>

</doc>
<doc id="59896" url="https://en.wikipedia.org/wiki?curid=59896" title="Willie Davenport">
Willie Davenport

William "Willie" D. Davenport (June 8, 1943 – June 17, 2002) was an American sprint runner. He attended Howland High School and a college at Southern University and A&M College in Baton Rouge, Louisiana. He competed in the 110 m hurdlers at the 1964, 1968, 1972 and 1976 Summer Olympics, winning a gold medal in 1968 and a bronze in 1976, and finishing fourth in 1972. In 1980 he took part in the Winter Olympics as a runner for the American bobsleigh team. Because of the boycott, and the quirk of participating in the Winter Olympics, he was the only U.S. track and field athlete to participate in the 1980 Olympics.
Davenport took part in his first Olympics in 1964, but injured his thigh and was eliminated in the semifinals. In Mexico City in 1968, he reached the final and won: "From the first step, the gun, I knew I had won the race." In 1972 he finished fourth, and in his third consecutive Olympic 110 m hurdles final, in 1976, he won a bronze medal. At his last Olympics in 1980 he was a bobsleigh runner, ending up 12th in the four-man competition. Davenport's other achievements include five national championships in the 60 yard hurdles indoor event.
By participating in the 1980 bobsleigh competition, Willie became the first African American to compete in the Winter Olympics for the USA.
Davenport was a U.S. Army private at the time of his first Olympic participation, he was a Colonel in the United States Army National Guard at the time of his death. He died of a heart attack at age 59 at Chicago's O'Hare International Airport on June 17, 2002. He was survived by daughter Tanya, sons Willie and Mark and fiancee Barbara Henry.
In 1977 he was inducted into the Mt. SAC Relays Hall of Fame, and in 1982 into the National Track and Field Hall of Fame. 

</doc>
<doc id="59897" url="https://en.wikipedia.org/wiki?curid=59897" title="Mamo Wolde">
Mamo Wolde

Degaga ("Mamo") Wolde (Afaan Oromo: Maammo Woldee) (Amharic: ደጋጋ ("ማሞ") ዎልደ; June 12, 1932 – May 26, 2002) was an Ethiopian long distance runner who competed in track and road running events. He was also the winner of the marathon at the 1968 Summer Olympics.
Wolde was born in Diri Jille to an Oromo family. In 1951, he moved to Addis Ababa and joined the Imperial Bodyguard. Wolde later served as a peacekeeper in Korea from 1953 to 1955.
At his first Olympic appearance in 1956, Wolde competed in the 800 m, 1,500 m and the 4x400 relay. He didn't compete in the 1960 Summer Olympics, when Abebe Bikila became the first Ethiopian to win a gold medal. Wolde claimed his absence was due to the government's desire to send him on a peacekeeping mission to the Congo during the Congo Crisis. According to him, in the government's ensuing conflict with the Ethiopian Olympic Committee, who wanted him to compete, he didn't get sent to either event. However, athlete Said Moussa Osman, who represented Ethiopia in the 800 m at the 1960 Olympics, stated that Wolde lost at the trials and didn't make it on the team.
Beginning in the 1960s, Wolde's focus changed from middle distance races to long distances. He placed fourth in the 10,000 m at the 1964 Summer Olympics, which was won by Billy Mills of the United States in one of the biggest upsets in the history of Olympic competition. After Abebe Bikila had won the 1964 Olympic marathon, four years later at the 1968 Summer Olympics, Wolde became the second Ethiopian to win the title in the marathon. Earlier in the same Olympics, Wolde had already won the silver medal in the 10,000 m. In 1972, Wolde won a third Olympic medal at the age of 40, winning bronze in the marathon. He blamed his third place showing on ill-fitting shoes forced on him by Ethiopian officials. Nonetheless, he became only the second person in Olympic history (Bikila was the first) to medal in successive Olympic marathons. Both medalists ahead of him in 1972, Frank Shorter and Karel Lismont would repeat the feat in 1976 behind Waldemar Cierpinski who would do it in 1980. Erick Wainaina was the most recent and only other to do it in 2000. Wolde also won the marathon race of 1973 All-Africa Games.
In 1993, Wolde was arrested on the accusation that he participated in a Red Terror execution during the regime of the dictator Mengistu Haile Mariam. He argued that although he was present at the killing, he was not a direct participant. The IOC pressured the Ethiopian government to release him. In early 2002 he was convicted to six years of imprisonment, but released because he had spent nine years in detention already waiting for his trial. 
Wolde died of liver cancer at age 69 a few months after his release. He had been married twice and had three children; a son with his first wife, Samuel, and two children, Addis Alem and Tabor, with his second wife. Mamo Wolde is interred in Saint Joseph's Church Cemetery in Addis Ababa, beside his fellow countryman Abebe Bikila.

</doc>
<doc id="59898" url="https://en.wikipedia.org/wiki?curid=59898" title="Carbondale">
Carbondale

Carbondale may refer to:

</doc>
<doc id="59899" url="https://en.wikipedia.org/wiki?curid=59899" title="Chatham, Kent">
Chatham, Kent

Chatham ( ) is one of the Medway towns located within the Medway unitary authority, in North Kent, in South East England.
Chatham Royal Dockyard closed in 1984, but major naval buildings remain as the focus for a flourishing tourist industry. Following closure, part of the site became a commercial port, other parts were redeveloped for business and residential use, and part became the Chatham Historic Dockyard museum, which features the submarine HMS Ocelot among a good many other attractions. Chatham also has military connections; several Army barracks were located here, together with 19th-century forts which provided a defensive shield for the dockyard. Brompton Barracks, located in the town, remains the headquarters of the Corps of Royal Engineers.
The town has important road links and the railway and bus stations are the main interchanges for the area. It is the administrative headquarters of Medway unitary authority, as well as its principal shopping centre.
History.
The name "Chatham" was first recorded as "Cetham" in 880. The Domesday Book records the place as "Ceteham".
Most books explain this name as a British root "ceto" (like Welsh "coed") plus Old English "ham", thus meaning a forest settlement.
However, the river-valley situation of Chatham is more consistent with "cet" being an Old English survival of the element "catu" that was common in Roman-era names and meant 'basin' or 'valley'.
Chatham stands on the A2 road along the line of the ancient Celtic route, which was paved by the Romans, and named Watling Street by the Anglo-Saxons. Among finds have been the remains of a Roman cemetery.
It long remained a small village on the banks of the river, but by the 16th century warships were being moored at Jillingham water {Gillingham}, because of its strategic sheltered location between London and the Continent. It was established as a Royal Dockyard by Queen Elizabeth I in 1568 and most of the dockyard actually lies within Gillingham. Initially a refitting base, it became a shipbuilding yard; from then until the late 19th century, further expansion of the yard took place. In its time, many thousands of men were employed at the dockyard, and many hundreds of vessels were launched there, including "HMS Victory" which was built there in the 1760s. After World War I many submarines were also built in Chatham Dockyard.
In addition to the dockyard itself, defensive fortifications were built to protect it from attack. Upnor Castle had been built in 1567, but had proved ineffectual; the Dutch Raid on the Medway in 1667 showed that more was required. The fortifications, which became more elaborate as the threat of invasion grew, were begun in 1756 as a complex across the neck of the peninsula formed by the bend in the River Medway, and included Fort Amherst. The threat of a land-based attack from the south during the 19th century led to the construction of even more forts.
The second phase of fort-building (1806–1819) included Fort Pitt (later used as a hospital and the site of the first Army Medical School). The 1859 Royal Commission on the Defence of the United Kingdom ordered, "inter alia", a third outer ring of forts: these included Fort Luton, Fort Bridgewood, and Fort Borstal. These fortifications all required military personnel to man them and Army barracks to house those men. These included Kitchener Barracks (c 1750–1780), the Royal Marine Barracks (c 1780). Brompton Artillery Barracks (1806) and Melville Barracks (opened 1820 as a Naval hospital, RM barracks from 1905). H.M.S. Collingwood and H.M.S. Pembroke were both naval barracks.
In response to the huge manpower needs, the village of Chatham and other nearby villages and towns grew commensurately. Trams, and later buses, linked those places to bring in the workforce. The area between the High Street and Luton village illustrates part of that growth, with its many streets of Victorian terraces.
The importance of Chatham dockyard gradually declined as Britain's naval resources were reduced or moved to other locations, and eventually, in 1984, it was closed completely. The dockyard buildings were preserved as the historic site Chatham Historic Dockyard (operated by Chatham Historic Dockyard Trust), which was under consideration as a World Heritage Site the site is being used for other purposes. Part of the St Mary's Island section is now used as a marina, and the remainder is being developed for housing, commercial and other uses, branded as "Chatham Maritime".
Governance.
Chatham lost its independence as a borough under the Local Government Act 1972, by which, on 1 April 1974, it became part of the Borough of Medway, a non-metropolitan district of the county of Kent; under subsequent renaming the Borough became the Borough of Rochester-upon-Medway (1979); and, from 1982, the City of Rochester-upon-Medway. Under the most recent change, in 1998, and with the addition of the Borough of Gillingham, the Borough of Medway became a unitary authority area, administratively separate from Kent. It remains part of the county of Kent for ceremonial purposes.
Medway Council has recently relocated its main administration building to Gun Wharf, the site of the earliest part of the Dockyard, a former Lloyd's office building.
Chatham is currently part of the parliamentary constituency of Chatham and Aylesford. Prior to 1997, Chatham had been included in the constituencies of Mid Kent, Rochester and Chatham and Chatham.
Like several other Kent constituencies, Chatham has proven to be a marginal seat, swinging backwards and forwards on the political tide and almost always following the national trend. Since 1945, the members of parliament for Chatham have been as follows:
Geography.
Chatham is situated where the lower part of the dip slope of the North Downs meets the River Medway which at this point is flowing in a south-north direction. This gives the right bank, where the town stands, considerable advantages from the point of view of river use. Compared with opposite bank, the river is fast-flowing and deep; the illustration (1), an early print of the settlement, is taken from the point where Fort Pitt now stands. The town lies below at river level, curving round to occupy a south-easterly trending valley (The Brook"), in which lies the High Street. Beyond the dockyard was marshy land, now called St Mary’s Island, and has several new developments of housing estates. The New Road crosses the scene below the vantage point of the illustration.
Illustration (2) is taken from the opposite side of the valley: the Pentagon Centre is to the right, with the building on the ridge left of centre, Fort Pitt and Rochester lies beyond that ridge; and Frindsbury is on the rising ground in the right distance.
The valley continues southeastwards as the Luton Valley, in which is the erstwhile village of that name; and Capstone Valley. The "Darland Banks", the northern slopes of the valley above these valleys, are unimproved chalk grassland. The photograph (3), taken from the Banks and looking south, shows the village in the centre, with the rows of Victorian terraced housing, which unusually follow the contour lines. The opposite slopes are the ‘’Daisy Banks'’ and ‘’Coney Banks'’, along which some of the defensive forts were built (including Fort Luton, in the trees to the left)
Until the start of the 20th century, most of the south part of the borough was entirely rural, with a number of farms and large tracts of woodland. The beginning of what is now Walderslade was when a speculative builder began to build the core of the village in "Walderslade Bottoms".
Demography.
Chatham became a market town in its own right in the 19th century, and a municipal borough in 1890. By 1831 its population had reached more than 16,000. By 1961 it had reached 48,800.
Economy.
The closure of the Dockyard has had the effect of changing the employment statistics of the town. About 7000 people lost their jobs. The unemployment rate went up to 23.5%. There has been a concerted effort to revitalise the Thames Gateway area and one of the largest employers in Chatham is now Vanquis Bank Ltd, a subsidiary of Provident Financial.
Landmarks.
The Chatham Naval Memorial commemorates the 18,500 officers, ranks and ratings of the Royal Navy who were lost or buried at sea in the two world wars. It stands on the Great Lines, the escarpment ridge between Chatham and Gillingham. The Grade II listed building Chatham Town Hall was built in 1900; it stands in the Brook, and is of a unique architectural design. With the town being part of Medway conurbation, it took on a new role as an arts centre. In 1996, it became the "Brook Theatre". The "Pentagon Centre" stands in the town centre and serviced the old bus station that was closed in 2011. Chatham Waterfront bus station opened in October 2011, replacing the town's previous Pentagon bus station which was built in the 1970s and was considered an unwelcoming environment for passengers.
Transport.
The Medway, apart from Chatham Dockyard, has always had an important role in communication: historically it provided a means for the transport of goods to and from the interior of Kent. Stone, timber and iron from the Weald for shipbuilding and agricultural produce were among the cargoes. Sun Pier in Chatham was one of many such along the river. By 1740, barges of forty tons could navigate as far upstream as Tonbridge. Today its use is confined to tourist traffic; apart from the marina, there are many yacht moorings on the river itself.
Chatham's position on the road network began with the building of the Roman road (Watling Street, which passed through the town. Turnpike trusts were established locally, so that the length from Chatham to Canterbury was turnpiked in 1730; and the Chatham to Maidstone road (now the A230) was also turnpiked before 1750. The High Street was bypassed in 1769, by the "New Road" (see illustration (1)) leading from the top of Star Hill Rochester, to the bottom of Chatham Hill at Luton Arches. This also became inadequate for the London cross-channel traffic and the "Medway Towns Bypass", the M2 motorway, was constructed to divert through traffic south of the Medway Towns.
Chatham is the hub of the Medway Towns. This fact means that the existing road system has always proved inadequate for the amount of traffic it has to handle, and various schemes have been tried to alleviate the congestion. The High Street itself is traffic-free, so all traffic has to skirt around it. The basic west-east routes are The Brook to the north and New Road to the south, but the additional problems caused by the situation of the Pentagon Bus Station meant that conflicting traffic flows were the result. In the 1980s the Chatham town centre was remodelled and an inner ring road – a one-way system – was constructed. This was completed with the construction of the "Sir John Hawkins Flyover" opened in 1989 carrying the south to north traffic over the High Street.
In September 2006, the one-way system was abandoned and two-way traffic reintroduced on most of the ring-road system. Further work on the road system commenced early in 2009, and as of early 2010, the demolition of the "Sir John Hawkins Flyover" has been completed. It was replaced by a street-level, buses only, road coupled with repositioning of the bus station. The new Waterfront bus station opened in October 2011 – be warned the sat-nav will try and take you down the bus road and a week later an automated fine will arrive in the post for making a wrong turn.
Chatham railway station, opened in 1858, serves both the North Kent and the Chatham Main Lines, and is the interchange between the two lines. It lies in the valley between the Fort Pitt and the Chatham Tunnels. There are three trains an hour to London Victoria, two trains an hour to London Charing Cross and two High Speed trains an hour to St Pancras International. The former services run to Dover and Ramsgate; the Charing Cross services terminate at Gillingham, Kent and the High Speed services terminate at Faversham.
Part of the industrial railway in what is now Chatham Historic Dockyard is still in operation, run by the North Kent Industrial Locomotive Society.
Buses are operated by Arriva Southern Counties and Nu-Venture to various destinations. They serve other towns in Medway including Gillingham, Grain, Strood and Rochester and also to other towns in Kent including Maidstone, Gravesend, Blue Bell Hill and Sittingbourne. There is also an express bus via Strood and Rochester and A2 to Bluewater Shopping Centre in Greenhithe.
Religion.
In the 19th century the ecclesiastical parish of Chatham included Luton and Brompton and also Chatham Intra (land on the river that was administered by the City of Rochester).
Chatham's parish church, St Marys, which stood on Dock Road, was rebuilt in 1788. St John's was a Waterloo church built in 1821 by Robert Smirke, and restructured in 1869 by Gordon Macdonald Hills; it ceased being an active church in 1964, and is currently used as an art project. St Paul's New Road was built in 1854; declared redundant in 1974, it has been demolished. St Peter's Troy Town was built in 1860. Christchurch Luton was built in 1843, replaced in 1884. The Royal Dockyard church (1806) was declared redundant in 1981.
St Michael's is a Roman Catholic church, that was built in 1863. There is a Unitarian Chapel built in 1861.
Chatham is reputed to be the home of the first Baptist chapel in north Kent, the Zion Baptist Chapel in Clover Street. The first known pastor was Edward Morecock who settled there in the 1660s. During Cromwell's time Morecock had been a sea-captain and had been injured in battle. His knowledge of the River Medway is reputed to have preserved him from persecution in the reign of King Charles II.
There was a second Baptist chapel founded about 1700. The Ebenezer Chapel dates back to 1662.
Chatham Memorial Synagogue was built by Simon Magnus in 1867 on the Chatham end of Rochester High Street in Rochester.
Education.
For a full list of schools serving Chatham visit List of schools in Medway
Sports.
The town's Association Football club, Chatham Town F.C., plays in the Isthmian League Division One South. Lordswood F.C. play in the Southern Counties East League. The defunct Chatham Excelsior F.C. were one of the early pioneers of football in Southern England.
Football league side Gillingham F.C. are seen to represent Medway as a whole.
Holcombe Hockey Club is one of the largest in the country, and are based in Chatham. The men's 1st XI are part of the England Hockey League.
Kite Flying is possible, especially power kiting on the Great Lines Heritage Park (between Gillingham and Chatham) and at Capstone Farm Country Park.
Skiing is also possible near Capstone Farm Country Park at Capstone Ski Slope and Snowboard Centre.
Popular culture.
On a cultural level, Chatham gave birth to several movements in literature, art and music. In the period from 1977 until 1982 the Medway Delta Sound emerged. The term was coined as a joke by Chatham born writer painter and musician Billy Childish after Russ Wilkins' Medway based record label, Empire Records, used the phrase "from the Medway Delta". Several Medway Delta bands gained international recognition, including The Milkshakes, The Prisoners (see also James Taylor Quartet), The Daggermen, The Dentists, Christopher Broderick and The Singing Loins. In the mid- to late 1980s a scene of more contemporary indie bands emerged, centred on Churchill's pub, and organising themselves within the "Medway Bands Co-operative".
Out of the Kent Institute of Art & Design (KIAD), now the University for the Creative Arts (UCA) came the band known as Wang Chung. The vocalist and guitarist with Wang Chung, Jeremy Ryder, who is better known as Jack Hues attended KIAD. Also Alan Denman, who was a well established lecturer at KIAD formed the Electronic Town as an urban theatre movement in the Medway Towns in 1984. Alan Denman also helped to form The Medway Poets with Billy Childish, Robert Earl, Bill Lewis, Sexton Ming and Charles Thomson. The Medway Poets used to meet regularly at the Railway Tavern that was known as The York during the period from 1974 to 1985 in Ordnance Street in Chatham. So Chatham has had a strong musical and creative arts heritage that was centred on local groups, many of which were part of KIAD. Sexton Ming went onto to create the artistic movement known as Stuckism in 1999.
There was a resurgence in the live music scene in early 2001, with an initial focus on the Tap 'n' Tin venue in Chatham. The spirit of the original greatness of the Medway Delta Sound was revived by music and poetry evenings promoted by David Wise's Urban Fox Press, which also published several books by Medway poets and artists. In 2008 the independent arts organisation Medway Eyes was founded, specialising in music and photography. Medway Eyes has promoted several arts exhibitions and gigs at The Barge in Gillingham and The Nags Head in Rochester.
The Medway Poets were formed in 1977 and disbanded in 1982 having performed at major literary festivals and on TV and Radio. They became a major influence to writers in the Medway Towns. From the core of this group the anti conceptual/pro painting movements of Stuckism and Remodernism came into being.
Recent Medway artists of note include Kid Harpoon, Crybaby Special and The Monsters, Red Light, Underground Heroes, Tyrannosaurus Alan, Pete Molinari, Lupen Crook, Stuart Turner and Theatre Royal.
The term 'chav', research suggests, does not derive from Chatham's name ("Chatham average"), but is derived from the romany word for 'youngster'.
Local media.
Newspapers.
Local newspapers for Chatham include "Medway News" and "Medway Standard", both published by Kent Regional News and Media; and the "Medway Messenger", published by the KM Group. The town also has free newspapers in the "Medway Extra" (KM Group) and "yourmedway" (KOS Media).
Radio.
The local commercial radio station for Chatham is KMFM Medway, owned by the KM Group. Medway is also served by community radio station Radio Sunlightbased in Richmond road between the high street and the River Medway. The area can also receive the county wide stations BBC Radio Kent, Heart and Gold, as well as many radio stations in Essex and Greater London.
Notable people.
Charles Dickens lived in the town as a boy, both in 'The Brook, Chatham' and in Ordnance Terrace before Chatham railway station was built just opposite. He subsequently described it as the happiest period of his childhood, and eventually returned to the area in adulthood when he bought a house in nearby Gad's Hill. Medway features in his novels. He then moved to Rochester, a nearby town, also part of the Medway Towns.
Others who were born or who lived or live in Chatham:

</doc>
<doc id="59900" url="https://en.wikipedia.org/wiki?curid=59900" title="College Park">
College Park

College Park may refer to:

</doc>
<doc id="59901" url="https://en.wikipedia.org/wiki?curid=59901" title="Concord">
Concord

Concord may refer to:

</doc>
<doc id="59902" url="https://en.wikipedia.org/wiki?curid=59902" title="Duluth (disambiguation)">
Duluth (disambiguation)

Duluth, Minnesota is a city in the United States
Duluth or DuLuth may also refer to:

</doc>
<doc id="59903" url="https://en.wikipedia.org/wiki?curid=59903" title="Elkton">
Elkton

Elkton may refer to:

</doc>
<doc id="59904" url="https://en.wikipedia.org/wiki?curid=59904" title="Eureka">
Eureka

Eureka may refer to:

</doc>
<doc id="59905" url="https://en.wikipedia.org/wiki?curid=59905" title="Fayetteville">
Fayetteville

Fayetteville is the name of a number of places in the United States of America. Many are named for General Gilbert du Motier, marquis de Lafayette, a French officer who fought under General George Washington in the American Revolutionary War. 

</doc>
<doc id="59906" url="https://en.wikipedia.org/wiki?curid=59906" title="Ernst &amp; Young">
Ernst &amp; Young

Ernst & Young (trading as EY) is a multinational professional services firm headquartered in London, United Kingdom. It is one of the "Big Four" audit firms and is the third largest professional services firm in the world by aggregated revenue in 2014, after PwC and Deloitte.
The organization operates as a network of member firms which are separate legal entities in individual countries. It has 212,000 employees in over 700 offices around 150 countries in the world. It provides assurance (including financial audit), tax, consulting and advisory services to companies.
The firm dates back to 1849 with the founding of Harding & Pullein in England. The current firm was formed by a merger of Ernst & Whinney and Arthur Young & Co. in 1989. It was known as Ernst & Young until 2013, when it underwent a rebranding to EY. The acronym "EY" was already an informal name for the firm prior to its official adoption.
As of 2015, EY was the 11th largest privately owned organization in the United States.
History.
Early history.
EY is the result of a series of mergers of ancestor organizations. The oldest originating partnership was founded in 1849 in England as Harding & Pullein. In that year the firm was joined by Frederick Whinney. He was made a partner in 1859 and with his sons in the business it was renamed Whinney Smith & Whinney in 1894.
In 1903, the firm of Ernst & Ernst was established in Cleveland by Alwin C. Ernst and his brother Theodore and in 1906, Arthur Young & Co. was set up by the Scotsman Arthur Young in Chicago.
As early as 1924, these American firms allied with prominent British firms, Young with Broads Paterson & Co. and Ernst with Whinney Smith & Whinney. In the year 1979, this led to the formation of Anglo-American Ernst & Whinney, creating the fourth largest accountancy firm in the world. Also in 1979, the European offices of Arthur Young merged with several large local European firms, which became member firms of Arthur Young International.
Mergers.
In 1989, the number four firm Ernst & Whinney merged with the then number five, Arthur Young, on a global basis to create Ernst & Young.
In October 1997, EY announced plans to merge its global practices with KPMG to create the largest professional services organization in the world, coming on the heels of another merger plan announced in September 1997 by Price Waterhouse and Coopers & Lybrand. The merger plans were abandoned in February 1998 due to client opposition, antitrust issues, cost problems and difficulty of merging the two diverse companies and cultures.
EY had built up its consultancy arm heavily during the 1980s and 1990s. The U.S. Securities and Exchange Commission and members of the investment community began to raise concerns about potential conflicts of interest between the consulting and auditing work amongst the Big Five and in May 2000, EY was the first of the firms to formally and fully separate its consulting practices via a sale to the French IT services company Cap Gemini for $11 billion, largely in stock, creating the new company of Cap Gemini Ernst & Young, which was later renamed Capgemini.
21st century: Expansion and Future.
In 2002, EY took over many of the ex-Arthur Andersen practices around the world, although not those in the UK, China, or the Netherlands.
In 2006, EY became the only member of the Big Four to have two member firms in the United States, with the inclusion of Mitchell & Titus, LLP, the largest minority-owned accounting firm in the United States.
In 2010, EY acquired Terco, the Brazilian member firm of Grant Thornton.
In 2013, EY agreed to pay federal prosecutors $123 million to settle criminal tax avoidance charges stemming from $2 billion in unpaid taxes from about 200 wealthy individuals advised by four Ernst & Young senior partners between 1999 and 2004.
In 2013, EY changed its brand name from Ernst & Young to EY and tagline to "Building a better working world".
In 2013, the Pope of the Roman Catholic church hired EY to help review Vatican City State's finances and help “verify and consult” the institution’s administration, including the museums, post office and tax-free department store. EY expanded further and acquired all of KPMG Denmark's operations including its 150 partners, 1500 employees and 21 offices. 
In 2015, EY opened its first ever global Security Operations Centre at Thiruvananthapuram, Kerala in India and will invest $20 million over 5 years to combat increasing threat of cybercrimes.
Global structure.
EY is the most globally managed of the Big Four firms. EY Global sets global standards and oversees global policy and consistency of service, with client work performed by its member firms.
Each EY member country is organized as part of one of four areas. This is different from other professional services networks, which are more centrally managed.
The four areas are:
Each area has an identical business structure and a management team, which is led by an Area Managing Partner who is part of the Global Executive board. The aim of this structure is to effectively cater for an increasingly global clientele, who have multinational interests.
Services.
EY has four main service lines and share of revenues in 2014:
Tax avoidance.
In 2014 tax arrangements negotiated by EY for The Walt Disney Company, Koch Industries, Skype, and other multinational corporations became public in the so-called "Luxembourg Leaks". The disclosure of these and other tax arrangements led to controversial discussions about tax avoidance.
Staff.
EY was ranked No. 1 in "Forbes" magazine's "The Best Accounting Firms to Work For" in 2012, claiming that EY treats its employees better than other large firms do. It was ranked 57 overall.
The firm was ranked No. 1 in "BusinessWeek"'s annual list of "Best Places To Launch a Career" for 2008.
The firm was ranked No. 44 in the "Fortune" list of "100 Best Companies to Work For", and the highest among the "Big Four", for 2009.
EY was ranked 4th in Universum's America's "Ideal Employers" list 2011 and 3rd in its Global Top Employers list.
The firm was No. 34 in "ComputerWorld"'s "100 Best Places To Work For In IT" for 2009.
The firm was also placed among the top 50 places in the "Where Women Want to Work" awards for 2007.
The firm was named as one of the "10 Best Companies for Working Mothers" by "Working Mothers" magazine in 2012 for the 7th straight year.
In April 2009, Reuters reported that EY. launched an initiative encouraging its staff in China to take 40 days of low-pay leave between July 2009 and June 2010. Those who participated got 20% of regular salary plus benefits of full-time employee. The initiative applied to employees in Hong Kong, Macau and mainland China, where the firm employs 8,500 in total.
In early 2012, it was reported that EY has 10,000 staff in mainland China and Hong Kong, which has quadrupled in a decade. It has about 11,200 staff in the UK.
In 2012, the firm was ranked number 1 in the "Stonewall Top 100 Workplace Equality Index", a list of Britain's top 100 gay-friendly employers. In 2013, the firm was ranked number 6 in the same Workplace Equality Index.
In 2013, EY earned 100% rating on the "Human Rights Campaign Corporate Equality Index".
In 2013, EY was named one of "DiversityInc" magazine's Top 50 companies for diversity.
In 2013, EY was ranked 4th in "Universum Top 100 IDEAL™ Employer".
In 2014, EY was ranked 2nd in Universum World's Most Attractive Employers.
Logo.
The current EY logo was introduced in July 2013, to coincide with the firm changing its trading name from Ernst & Young to EY.
Criticisms.
Accounting scandals.
Ernst and Young has been in accounting scandals - Informix Corporation (1996), Sybase (1997), Cendant (1998), One.Tel (2001), AOL (2002), HealthSouth Corporation (2003), Chiquita Brands International (2004), Lehman Brothers (2010), Sino-Forest Corporation (2011) and Olympus Corporation (2011).
Equitable Life (2004).
In April 2004, Equitable Life, a UK life assurance company, sued EY after nearly collapsing but abandoned the case in September 2005. EY described the case as "a scandalous waste of time, money and resources for all concerned."
Bally Total Fitness (2008).
Following allegations by the Securities and Exchange Commission that EY had committed accounting fraud in its work auditing the books of Bally Total Fitness, EY reached two settlements in 2008, including a fine of $8.5million.
Anglo Irish Bank (2009).
In 2009, in the Anglo Irish Bank hidden loans controversy, EY was criticised by politicians and the shareholders of Anglo Irish Bank for failing to detect large loans to Sean FitzPatrick, its Chairman, during its audits. The Irish Government had to subsequently take full ownership of the Bank at a cost of €28 billion. The Irish Chartered Accountants Regulatory Board appointed John Purcell to investigate. EY said it "fundamentally disagrees with the decision to initiate a formal disciplinary process" and that "there has been no adverse finding made against EY in respect of the audit of Anglo Irish Bank."
Sons of Gwalia (2009).
In 2009, EY, the former auditors of Sons of Gwalia, agreed to a $125m settlement over their role in the gold miner’s collapse in 2004. Ferrier Hodgson, the company's administrator, had claimed EY was negligent over the accounting of gold and dollar hedging contracts. However, EY said that the proposed settlement was not an admission of any liability.
Akai Holdings (2009) and Moulin Global Eyecare (2010).
In 2009, EY agreed to pay US$200m out of court to settle a negligence claim by the liquidators of Akai Holdings. It was alleged that EY falsified dozens of documents to cover up the theft of over US$800m by Akai's chairman, James Ting - in some cases documents had been painted over with correcting fluid and then written over by hand. In a separate lawsuit a former EY partner, Cristopher Ho, made a "substantial payment" to Akai creditors in his role as chairman of the company that had bought Akai just before it went bust in 2000. Police raided the Hong Kong office and arrested an EY partner who had been an audit manager on the Akai account from December 1997, although audit documents had been doctored dating back to 1994. The EY partner for the Akai account between 1991 and 1999, David Sun Tak-kei, faced no charges and went on to become co-managing partner for EY China. A few months later EY settled a similar claim of up to HK$300m from the liquidators of Moulin Global Eyecare, an audit client of the Hong Kong affiliate between 2002 and 2004. The liquidators described the Moulin accounts as a "morass of dodginess".
Lehman Brothers (2010).
The Valukas Report issued in 2010 charged that Lehman Brothers engaged in a practice known as repo 105 and that EY, Lehman's auditor, was aware of it. New York prosecutors announced in 2010 that they have sued the firm. EY said that its last audit of Lehman Brothers was for the fiscal year ending 30 November 2007 and that Lehman’s financial statements were fairly presented in accordance with Generally Accepted Accounting Principles. In March 2015, EY settled Lehman-related lawsuits in New Jersey and California.
Standard Water (2013).
EY Hong Kong resigned from the audit of Standard Water on when it emerged that although EY Hong Kong had signed off the audit, it had been effectively outsourced to the affiliate in mainland China, which had received 99.98% of the fee. This was important because shareholders have less confidence in mainland auditors and because audit papers on the mainland are subject to state secrecy laws and can be withheld from outside regulators. EY's quality and risk management leader (Greater China) even testified in the Court of First Instance that he was not sure whether there was a formal agreement covering the relationship between the two EY entities. The court case in 2013 came as US regulators were taking an interest in similar cases of accounting fraud in mainland China.
Hellas Telecommunications (2015).
In 2015, Britain's accounting regulator, the Institute of Chartered Accountants in England and Wales (ICAEW), reprimanded Ernst & Young (EY) for its involvement in the Wind Hellas scandal involving mainly TPG, Apax Partners & Nikesh Arora. EY agreed to become administrator of Hellas Telecommunications even though EY had been its accountant for the previous three years. The company was fined £250, 000 ($390, 850) for ethical breaches.
Corporate affairs and culture.
EY's publicity activity includes its World Entrepreneur Of The Year program, held in over 60 countries.
EY UK also publicizes itself by sponsoring exhibitions of works by famous artists, such as Cézanne, Picasso, Bonnard, Monet, Rodin and Renoir. The most recent of these was Maharaja: the Splendour of India's Royal Courts at the Victoria and Albert Museum.
In addition, EY publicizes itself by sponsoring the educational children's show "Cyberchase" on PBS Kids under the PBS Kids GO! television brand, in an effort to improve mathematics literacy in children.
EY in the UK sponsors the ITEM club.
EY in the UK has set up the National Equality Standard, an initiative developed for business which sets clear equality, diversity and inclusion (EDI) criteria against which companies are assessed. The National Equality Standard (NES) is currently the only industry recognised national standard for EDI in the UK.
EY in the UK has set up EY Foundation, a new UK charity set up to support young people and entrepreneurs.
Sports sponsorship.
On 8 September 2011, Rio 2016 made the announcement that EY will be an official sponsor of the XXXI Olympic Summer Games to be held in Brazil, as the exclusive provider of professional services – consulting and auditing – for Rio 2016 organizing committee.
EY has also been an Official Partner to The 2012 and the 2014 Ryder Cups.
EY also has a longstanding relationship with the 2011 Tour de France winner Cadel Evans.

</doc>
<doc id="59908" url="https://en.wikipedia.org/wiki?curid=59908" title="Swindon">
Swindon

Swindon () is a large town within the Borough of Swindon and ceremonial county of Wiltshire, in South West England. It is midway between Bristol, to the west and Reading, to the east. London is to the east. In the 2011 census, the population of the built-up area of Swindon was 185,609. The larger borough had a population of 209,000, including the small town of Highworth and the large village of Wroughton, an increase of 16.2% since 2001.
Swindon was named an Expanded Town under the Town Development Act 1952 and this led to a major increase in its population. Swindon railway station is on the line from London Paddington to Bristol. Swindon Borough Council is a unitary authority, independent of Wiltshire Council since 1997. Residents of Swindon are known as Swindonians. Swindon is home to the Bodleian Library's book depository, which contains of bookshelves.
History.
Early history.
The original Anglo-Saxon settlement of Swindon sat in a defensible position atop a limestone hill. It is referred to in the "Domesday Book" as Suindune, believed to be derived from the Old English words "swine" and "dun" meaning "pig hill" or possibly Sweyn's hill, where Sweyn is a personal name.
Swindon was a small market town, mainly for barter trade, until roughly 1848. This original market area is on top of the hill in central Swindon, now known as Old Town.
The Industrial Revolution was responsible for an acceleration of Swindon's growth. It started with the construction of the Wilts and Berks Canal in 1810 and the North Wilts Canal in 1819. The canals brought trade to the area and Swindon's population started to grow.
Railway town.
Between 1841 and 1842, Isambard Kingdom Brunel's Swindon Works was built for the repair and maintenance of locomotives on the Great Western Railway (GWR). The GWR built a small railway village to house some of its workers. The Steam Railway Museum and English Heritage, including the English Heritage Archive, now occupy part of the old works. In the village were the GWR Medical Fund Clinic at Park House and its hospital, both on Faringdon Road, and the 1892 health centre in Milton Road – which housed clinics, a pharmacy, laundries, baths, Turkish baths and swimming pools – was almost opposite.
From 1871, GWR workers had a small amount deducted from their weekly pay and put into a healthcare fund – its doctors could prescribe them or their family members free medicines or send them for medical treatment. In 1878 the fund began providing artificial limbs made by craftsmen from the carriage and wagon works, and nine years later opened its first dental surgery. In his first few months in post the dentist extracted more than 2000 teeth. From the opening in 1892 of the Health Centre, a doctor could also prescribe a haircut or even a bath. The cradle-to-grave extent of this service was later used as a blueprint for the NHS.
The Mechanics' Institute, formed in 1844, moved into a building looking rather like a church and included a covered market, on 1 May 1855. The New Swindon Improvement Company, a co-operative, raised the funds for this path self-improvement and paid the GWR £40 a year for its new home on a site at the heart of the railway village. It was a groundbreaking organisation that transformed the railway's workforce into some of the country's best-educated manual workers.
It had the UK's first lending library, and a range of improving lectures, access to a theatre and a range of activities from ambulance classes to xylophone lessons. A former Institute secretary formed the New Swindon Co-operative Society in 1853 which, after a schism in the society's membership, spawned the New Swindon Industrial Society that ran a retail business from a stall in the market at the Institute. The Institute also nurtured pioneering trades unionists and encouraged local democracy.
When tuberculosis hit the new town, the Mechanics' Institute persuaded the industrial pioneers of North Wiltshire to agree that the railway's former employees should continue to receive medical attention from the doctors of GWR Medical Society Fund, which the Institute had played a role in establishing and funding.
Swindon's 'other' railway, the Swindon, Marlborough and Andover Railway, merged with the Swindon and Cheltenham Extension Railway to form the Midland & South Western Junction Railway, which set out to join the London & South Western Railway with the Midland Railway at Cheltenham. The Swindon, Marlborough & Andover had planned to tunnel under the hill on which Swindon's Old Town stands but the money ran out and the railway ran into Swindon Town railway station, off Devizes Road in the Old Town, skirting the new town to the west, intersecting with the GWR at Rushey Platt and heading north for Cirencester, Cheltenham and the LMS, whose 'Midland Red' livery the M&SWJR adopted.
During the second half of the 19th century, Swindon New Town grew around the main line between London and Bristol. In 1900, the original market town, Old Swindon, merged with its new neighbour at the bottom of the hill to become a single town.
On 1 July 1923, the GWR took over the largely single-track M&SWJR and the line northwards from Swindon Town was diverted to Swindon Junction station, leaving the Town station with only the line south to Andover and Salisbury. The last passenger trains on what had been the SM&A ran on 10 September 1961, 80 years after the railway's first stretch opened.
During the first half of the 20th century, the railway works was the town's largest employer and one of the biggest in the country, employing more than 14,500 workers. Alfred Williams (1877–1930) wrote about his life as a hammerman at the works.
The works' decline started in 1960, when it rolled out "Evening Star", the last steam engine to be built in the UK. The works lost its locomotive building role and took on rolling stock maintenance for British Rail. In the late 1970s, much of the works closed and the rest followed in 1986.
The Community Centre in the Railway Village was originally the Barrack accommodation for Railway Employees of the GWR. The building became the Railway Museum in the 1960s, until the opening of the STEAM Museum in the 2000s.
Modern period.
David Murray John, Swindon's town clerk from 1938 to 1974, is seen as a pioneering figure in Swindon's post-war regeneration; his last act before retirement was to sign the contract for Swindon's tallest building, which is now named after him. His successor was David Maxwell Kent, appointed by the Swindon/Highworth Joint Committee in 1973. He had worked closely with David Murray John and continued similar policies for a further twenty years. The Greater London Council withdrew from the Town Development Agreement and the local council continued the development on its own.
There was the problem of the Western Development and of Lydiard Park being in the new North Wiltshire district, but this was resolved by a boundary change to take in part of North Wiltshire. Another factor limiting local decision-taking was the continuing role of Wiltshire County Council in the administration of Swindon. Together with like-minded councils, a campaign was launched to bring an updated form of county borough status to Swindon. This was successful in 1997, and Wiltshire is now divided into two Unitary Councils, both of equal status. One is Wiltshire Council, succeeding the former Wiltshire County Council and the Wiltshire district councils other than Thamesdown, while the other is Swindon Borough Council, covering the area of the former Thamesdown and the former Highworth Rural District Council.
The closure of the railway works (which had been in decline for many years) was a major blow to Swindon.
Because of this and the major growth in population diversification was continued at a rapid pace and theTown now has all the features of a successful urban/rural Council in the Outer South East Zone.
In February 2008 "The Times" named Swindon as one of "The 20 best places to buy a property in Britain". Only Warrington had a lower ratio of house prices to household income in 2007, with the average household income in Swindon among the highest in the country.
In October 2008 Swindon made a controversial move to ban fixed point speed cameras. The move was branded as reckless by some but by November 2008 Portsmouth, Walsall, and Birmingham councils were also considering the move.
In 2001 construction began on Priory Vale, the third and final instalment in Swindon's 'Northern Expansion' project, which began with Abbey Meads and continued at St Andrew's Ridge. In 2002 the New Swindon Company was formed with the remit of regenerating the town centre, to improve Swindon's regional status. The main areas targeted are Union Square, The Promenade, The Hub, Swindon Central, North Star Village, The Campus and the Public Realm.
Swindon hosted Radio 1's Big Weekend in May 2009 at Lydiard Park. Building on the work of Radio 1, Swindon Borough Council organised the Big Arts Day in 2010. Aiming to be an annual event celebrating the arts it was held at Lydiard Park in July for three consecutive years before being cancelled due to lack of funding.
Governance.
The local council was created in 1974 as the Borough of Thamesdown, out of the areas of Swindon Borough and Highworth Rural District. It was not initially called Swindon, because the borough covers a larger area than the town. It was renamed as the Borough of Swindon in 1997. The borough became a unitary authority on 1 April 1997, following a review by the Local Government Commission for England. The town is therefore no longer under the auspices of Wiltshire Council.
The borough consists of parished and non-parished areas. The non-parished areas include the former pre-1974 municipal borough of Swindon, and West Swindon which is a large town expansion area developed from the 1970s to the 1990s with land ceded from North Wiltshire district in the parishes of Lydiard Tregoze and Lydiard Millicent. Parished areas include Bishopstone (with Hinton Parva), Blunsdon St Andrew, Castle Eaton, Chiseldon, Covingham, Hannington, Haydon Wick, Highworth, Inglesham, Liddington, South Marston, Stanton Fitzwarren, Stratton St Margaret, Wanborough and Wroughton. In 2014 Nythe obtained independence from Stratton St Margaret, becoming a new parish in its own right with effect from 1 April 2015.
The executive comprises a leader and a cabinet, currently made up from the Conservative Group. The council as of the 2011 election has a majority of Conservative councillors.
Swindon is represented in the national parliament by two MPs. Robert Buckland (Conservative) was elected for the South Swindon seat in May 2010 with a 5.5% swing from Labour and Justin Tomlinson, also Conservative, represents North Swindon after a 10.1% swing at the same election. Both increased their majorities at the May 2015 election. Prior to 1997 there was a single seat for Swindon, although much of what is now in Swindon was then part of the Devizes seat.
Geography.
The town has an area of about .
The landscape is dominated by the chalk hills of the Wiltshire Downs to the south and east. The Old Town stands on a hill of Purbeck and Portland stone; this was quarried from Roman times until the 1950s. The area that was known as New Swindon is made up of mostly Kimmeridge clay with outcrops of Corrallian clay in the areas of Penhill and Pinehurst. Oxford clay makes up the rest of the borough. The River Ray forms the town's western boundary, including its tributary, the River Cole.
Climate.
Swindon has a maritime climate type, like all of the British Isles, with comparatively mild winters and comparatively cool summers considering its latitude. The nearest official weather station is RAF Lyneham, about west south west of Swindon town centre. The weather station's elevation is 145 metres, compared to the typical 100 metres encountered around Swindon town centre, so is likely to be marginally cooler throughout the year.
The absolute maximum is 34.9C (94.8F) recorded during August 1990. In an average year the warmest day should reach 28.7C (83.7F) and 10.3 days should register a temperature of 25.1C (77.2F) or above
The absolute minimum is −16.0C (3.0F), recorded in January 1982, and in an average year 45.2 nights of air frost can be expected.
Sunshine, at 1565 hours a year, is typical for inland parts of Southern England, although significantly higher than most areas further north.
Annual rainfall averages slightly under per year, with 123 days reporting over 1 mm of rain.
Demography.
The 2001 census shows there were 180,061 people and 75,154 occupied houses in the Swindon Unitary Authority. The average household size was 2.38 people. The population density was 780/km² (2020.19/mi²). 20.96% of the population were 0–15 years old, 72.80% 16–74 and the remaining 6.24% were 75 years old or over. For every 100 females there were 98.97 males. Approximately 300,000 people live within 20 minutes of Swindon town centre.
It is forecast that there will be a 70,000 (38.9%) increase in Swindon's population by 2026 from the current 180,000, to 250,000. The ethnic make-up of the town was 95.2% white, 1.3% Indian and 3.5% other. 92.4% were born in the UK, 2.7% in the EU and 4.9% elsewhere.
The majority of Swindonians (70.3%) identify themselves as Christians. This is followed by those of no religion (19.2%), Muslims (1.0%), Sikhs (0.6%), Hindus (0.6%), other (0.2%) and Jews (0.1%). In addition, 8.0% of people chose not to answer this question in the 2001 census.
In May 2007, 65.3% of households in Swindon had broadband Internet access, the highest in the UK, up 5.5% from June 2006.
In 2015, Public Health England found that 70.4% of the population was either overweight or obese with a BMI greater than 25.
Places of Worship.
There are numerous places of worship in Swindon, some of which are listed buildings. Until 1845, the only church in Swindon was the Holy Rood Church, a Grade II listed building. That year, St Mark's Church was built. In 1851, Christ Church was built. Later in the year, the first Roman Catholic chapel was opened in the city and was also named Holy Rood. In 1866, Cambria Baptist Chapel was built. In the 1880s, Bath Road Methodist Chapel was built. In 1885, St Barnabas Church was built. In 1907, St Augustine's Church in Even Swindon was built. Various churches and places of worship were built in the town by other denominations and faiths.
Polish community.
After the end of World War II, Polish refugees were temporarily housed in barracks at Fairford RAF base about north. Around 1950, some settled in Scotland and others in Swindon rather than stay in the barracks or hostels they were offered.
The 2001 UK Census found that most of the Polish-born people had stayed or returned after serving with British forces during World War II. Swindon and Nottingham were parts of this settlement. Data from that census showed that 566 Swindonians were Poland-born. Notes to those data read: ‘The Polish Resettlement Act of 1947, which was designed to provide help and support to people who wished to settle here, covered about 190,000 people ... at the time Britain did not recognise many of the professional gained overseas ... [but many did find work after the war; some went down the mines, some worked on the land or in steel works. Housing was more of a problem and many Poles were forced to live in barracks previously used for POWs ... The first generation took pains to ensure that their children grew up with a strong sense of Polish identity.'
In 2004, NHS planners devising services for senior citizens estimated that 5 percent of Swindon's population were not 'ethnically British' and most of those were culturally Polish.
The town's Polish ex-servicemen's club, which had run a football team for 45 years, closed in 2012. Barman Jerzy Trojan blamed the decline of both club and team on the children and grandchildren of the original refugees losing their Polish identity.
Economy.
Major employers include the Honda car production plant at the former Vickers-Armstrongs Supermarine aircraft factory on the former South Marston aerodrome, BMW/Mini (formerly Pressed Steel Fisher) in Stratton, Dolby Labs, international engineering consultancy firm Halcrow, and retailer W H Smith's distribution centre and headquarters. The electronics company Intel has its European head office on the south side of the town. Insurance and financial services companies such as Nationwide Building Society and Zurich Financial Services, the energy company RWE npower, the fuel card and fleet management company Arval, pharmaceutical companies such as Canada's Patheon and the United States-based Catalent Pharma Solutions and French medical supplies manufacturer Vygon (UK) Ltd have their UK divisions headquartered in the town. Swindon also has the head office of the National Trust.
Other employers include all of the national Research Councils, the British Computer Society, divisions of Tyco International, consumer goods supplier Reckitt Benckiser, Software Test Labs a dynamic test consultancy and managed testing services company and a branch of Becton Dickinson.
The town is currently the location of the UK Space Agency headquarters.
Transport.
At the junction of two Roman roads, the town has developed into a transport hub over the centuries. It is on the historical GWR and on canals. It also has two junctions (15 and 16) on the M4 motorway.
Swindon railway station opened in 1842 as Swindon Junction, and until 1895 every train stopped for at least 10 minutes to change locomotives. As a result, the station hosted the first recorded railway refreshment rooms.
Swindon bus operators are Thamesdown and Stagecoach. The local council acknowledges the need for more car parking as part of its vision for 2010. Swindon is one of the locations for an innovative scheme called Car share. It was set up as a joint venture between Wiltshire County Council and a private organisation, and now has over 300,000 members registered. It is a car pool or ride-sharing rather than a car share scheme, seeking to link people willing to share transport.
The town contains a large roundabout called Magic Roundabout. There are five mini-roundabouts within this roundabout and at its centre is a contra-rotational hub. It is the junction of five roads: (clockwise from South) Drove Road, Fleming Way, County Road, Shrivenham Road and Queens Drive. It is built on the site of Swindon wharf on the abandoned Wilts & Berks Canal, near the County Ground. The official name used to be County Islands, although it was colloquially known as the Magic Roundabout and the official name was changed in the late 1990s to match its nickname.
Media.
Print.
Swindon has a daily newspaper, the "Swindon Advertiser", with daily sales of about 21,000. Other newspapers covering the area include Bristol's daily "Western Daily Press" and the "Swindon Advertiser"s weekly, the "Gazette and Herald"; the "Wiltshire Ocelot" (a free listings magazine), "Swindon Star", "Hungry Monkeys" (a comic), "Stratton Outlook", "Frequency" (an arts and cultural magazine), "Great Swindon Magazine", "Swindon Business News", "Swindon Link" and "Highworth Link".
Radio.
Local radio stations include Sam FM and Heart Wiltshire in the commercial sector, with BBC Radio Wiltshire as a publicly funded alternative. The town has its own 24-hour community radio station, Swindon 105.5, which was given the Queen's Award for Voluntary Service in 2014, the highest award which can be given to a voluntary group.
Television.
Between 1973 and 1982 Swindon had its own cable television channel. It was called Swindon Viewpoint, a community television project run mainly by enthusiasts from studios in Victoria Hill and then Media Arts at the Town Hall Studios. It was followed by the more commercial Swindon's Local Channel, which included pay-per-view films. NTL (later Virgin Media) took over the channel's parent company, ComTel, and closed the station.
Regional news programmes covering Swindon include "Thames Valley Tonight" replaced by "Meridian Tonight" for the second time in February 2009 and "The West Tonight" from regional ITV1 stations and "South Today (Oxford)" and "Points West" from BBC One's regional variants.
Education.
The borough of Swindon has many primary schools, 12 secondary schools and two purpose built sixth-form colleges. Two secondary schools also have sixth forms. There is one independent school, Maranatha Christian School at Sevenhampton.
Further education.
New College and Swindon College cater for the town's further education and higher education requirements, mainly for 16- to 21-year-olds. Swindon College is one of the largest FE-HE colleges in southwestern England, situated at a purpose-built campus in North Star, Swindon.
Swindon also has a foundation learning programme called Include, which is situated in the Gorse Hill area. This is for 16- to 19-year-olds who are currently not in education or work.
Higher education.
Swindon is the UK's largest centre of population without its own university (by comparison, there are two universities in nearby Bath, which is half Swindon's size). In March 2008, a proposal was put forward by former Swindon MP, Anne Snelgrove, for a university-level institution to be established in the town within a decade, culminating in a future 'University of Swindon' (with some touting the future institution to be entitled 'The Murray John University, Swindon', after the town's most distinguished post-war civic leader). In October 2008, plans were announced for a possible University of Swindon campus to be built in east Swindon to the south of the town's Great Western Hospital, close to the M4-A419 interchange. However, these plans are currently mothballed.
Since 1999 Oxford Brookes University has had its Ferndale Campus in north-central Swindon, offering degrees and diplomas in Adult Nursing. The main OBU campus is 35 miles (56 km) northeast of Swindon. The university also sponsors UTC Swindon, which opened in 2014.
Between 2000 and 2008 the University of Bath had a campus in Walcot, east Swindon.
Sports.
Football.
Swindon Town F.C. play at the County Ground near the town centre. They have been Football League members since joining the then new Third Division (southern section) in 1920, and won promotion to the Second Division for the first time in 1963. They won their only major trophy to date, the Football League Cup, in 1969 beating Arsenal 3-1, at Wembley Stadium, and won the Anglo-Italian Cup the following year as the Football Association forbade Swindon from competing in the European Cup because they were in Division 3. They won promotion to the First Division in 1990, but stayed in the Second Division due to financial irregularities, only to reach the top flight (by then the Premier League) three years later. Their spell in the top flight lasted just one season, and then came a second successive relegation. A brief spite saw them promoted at the first attempt as champions of the new Division Two, but they were relegated again four years later and in 2006 fell back into the fourth tier for the first time since 1986, although promotion was gained at the first attempt. They were relegated again four years later. Notable former players of the club include John Trollope, Don Rogers, John Moncur, Fraser Digby, Duncan Shearer, Paul Bodin, Alan McLoughlin, Paul Rideout, Mike Summerbee, Shaun Taylor, Neil Ruddock, Jan Åge Fjørtoft and Phil King. Notable former managers include Lou Macari, Ossie Ardiles, Glenn Hoddle, John Gorman, Steve McMahon, Jimmy Quinn (a former player of the club), Colin Todd, Roy Evans, Andy King, Dennis Wise and Paul Sturrock. Under the charismatic reign of manager Paolo Di Canio, Swindon became League Two champions in 2011–12 and currently play in League One, the third-highest tier.
The town also has two non-league clubs: Swindon Supermarine F.C., playing in Southern League Division One South and West, and Highworth Town F.C., based in Highworth and playing in the Hellenic Football League.
Ice Hockey.
The Swindon Wildcats play in the second-tier English Premier Ice Hockey League (EPIHL). Since their inception, the Wildcats have played their home games at the 2800 capacity Link Centre. The club was founded in 1986.
In popular culture.
Books set in Swindon include "The Curious Incident of the Dog in the Night-Time" by Mark Haddon, and the "Thursday Next" novels by Jasper Fforde. Fforde's "Thursday Next" novels feature an alternative-universe Swindon that includes a parodic "Seven Wonders of Swindon". Robert Goddard's "Into the Blue", "Out of the Sun" and "Never Go Back" feature the central character of Harry Barnett from Swindon, and all three novels start in the town. Terry Jones, the former Monty Python member gave Swindon a backhanded reference in one of the short stories in his 2011 collection, "Evil Machines". The story "The Lift that Took People to Places They Didn't Want to Go" ends with the section "...But actually... the evil elevator hadn't changed at all. In fact it went on secretly taking people to places they didn't want to go. For every time the lift took the inhabitants of Swindon back down to the ground floor, they stepped out of the department store and onto the streets of Swindon, and so found themselves somewhere they didn't want to be."
Sherlock Holmes and Dr Watson lunched in Swindon in The Boscombe Valley Mystery "We lunch at Swindon, and I see that we shall be there in twenty minutes."
Swindon is home to the punk/pop band XTC. The band charted in both the UK and the US, their most notable hits including "Making Plans for Nigel", "Senses Working Overtime", "Ballad of Peter Pumpkinhead" and "The Mayor of Simpleton" amongst others.

</doc>
<doc id="59909" url="https://en.wikipedia.org/wiki?curid=59909" title="Danzig Research Society">
Danzig Research Society

The Danzig Research Society (, , ) was founded in 1743 in the city of Danzig (Gdańsk), in the Polish-Lithuanian Commonwealth, and continued in existence until 1936. The "Societas Physicae Experimentalis" (Experimental Physics Society) is thus considered as one of the oldest research societies in Central and Eastern Europe.
Already in 1670, the physician Israel Conradi (1634–1715) had tried to organize a scientific society in the city, without success. Several others tried after him, until Daniel Gralath (1708–1767) finally succeeded. His father-in-law was Jacob Theodor Klein (1685–1759), a city secretary and also a very distinguished scientist, nicknamed "Gedanensium Plinius".
At the end of 1742, Gralath had gathered a group of learned men for his purpose, an "Experimental Physics Society" (Societas Physicae Experimentalis), one of the oldest research societies of its type. The first organizing meeting took place on 7 November 1742, the first scientific meeting was called on 2 January 1743. The aim of the Society was to practice and popularize science, among others through weekly public demonstrations of the most interesting experiments in physics. Often the effects of electricity were studied, with the help of the Leyden jar. Since 1746 these took place in the Great Hall of the "Green Gate" building. Gralath also became "Ratsherr" (councilman) and, in 1763, "Bürgermeister" (mayor) of Danzig.
Known members were Nathanael Matthaeus von Wolf, Michael Christoph Hanow, Gottfried Lengnich, Johann Jacob Mascov, who wrote the "Geschichte der Teutschen", also Daniel Gabriel Fahrenheit and the prince-bishop Adam Stanisław Grabowski.
The sessions of the Society were also attended by many famous persons of the Polish-Lithuanian Commonwealth like Great Lithuanian Hetman Michał Kazimierz "Rybeńko" Radziwiłł, August Fryderyk Moszyński, Joachim Chreptowicz.
In 1840 Alexander von Humboldt accompanied Prussian King Frederick William IV on the way to Königsberg, and Humboldt received an honorary membership in the Society. Later, the society offered Humboldt stipends. The collections of the Society were displayed in the West-Prussian Provincial Museum located at the "Green Gate".
In 1845 the society was located in a Renaissance-era building at the Mottlau (Motława), an arm of the Vistula River.
After 200 years of existence, the society ceased to exist in 1936. The building and many priceless valuables were destroyed during the Soviet offensive in 1945, two years after the 200th anniversary.
The building at the Motława river was rebuilt after the war. It houses an Archaeological Museum today.

</doc>
<doc id="59916" url="https://en.wikipedia.org/wiki?curid=59916" title="Erysimum">
Erysimum

Erysimum (wallflower) is a genus of flowering plants in the botanical family Brassicaceae, that includes about 180 species, both popular garden plants and many wild forms. The genus "Cheiranthus" is sometimes included here in whole or in part. "Erysimum" has recently been ascribed to a monogeneric cruciferous tribe, Erysimeae. This tribe is characterized by sessile, stellate and/or malpighiaceous trichomes, yellow to orange flowers and multiseeded siliques.
Morphology.
Wallflowers are annuals, herbaceous perennials or sub-shrubs. The perennial species are short-lived and in cultivation treated as biennials. Most species have stems erect, somewhat winged, canescent with an indumentum of 2-fid hairs, usually 25 ± 53 cm x 2–3 mm in size, and t-shaped trichomes. The leaves are narrow and sessile. The lower leaves are linear to oblanceolate pinnatifid with backwardly directed lobes, acute, 50–80 mm x 0.5–3 mm. Stem leaves are linear, entire, all canescent with 2-fid hairs; 21–43 mm x 1.5–2 mm. Inflorescences are produced in racemes, with bright yellow to red or pink bilateral and hermaphodite, hypogynous and ebracteate flowers. Flowering occurs during spring and summer. One species, "Erysimum semperflorens", native to Morocco and Algeria, has white flowers. The floral pedicel ranges from 4 to 7 mm. Four free sepals somewhat saccate, light green, 5–7 mm x 1.5–2 mm.
Distribution.
Wallflowers are native to southwest Asia, the Mediterranean, Europe, Africa (Cabo Verde), Micronesia, and North America through Costa Rica. Many wallflowers are endemic to small areas, such as:-
Cultivation.
Most wallflower garden cultivars (e.g. "Erysimum" 'Chelsea Jacket') are derived from "E. cheiri" (often placed in "Cheiranthus"), from southern Europe. They are often attacked by fungal and bacterial disease, so they are best grown as biennials and discarded after flowering. They are also susceptible to clubroot, a disease of Brassicaceae. Growth is best in dry soils with very good drainage, and they are often grown successfully in loose wall mortar, hence the vernacular name. There is a wide range of flower colour in the warm spectrum, including white, yellow, orange, red, pink, maroon, purple and brown. The flowers, appearing in spring, usually have a strong fragrance. Wallflowers are often associated in spring bedding schemes with tulips and forget-me-nots.
The cultivar 'Bowles's Mauve' has gained the Royal Horticultural Society's Award of Garden Merit.
Ecology.
"Erysimum" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including the Garden Carpet ("Xanthorhoe fluctuata"). In addition, some species of weevils, like "Ceutorhynchus chlorophanus", live inside the fruits feeding on the developing seeds. Many species of beetles, bugs and grasshoppers eat on the leaves and stalks. Some mammalian herbivores, for example Mule Deer ("Odocoileus hemionus") in North America, Argali ("Ovis ammon") in Mongolia, Red Deer ("Cervus elaphus") in Central Europe, or Spanish Ibex ("Capra pyrenaica") in the Iberian Peninsula, feed on wallflower flowering and fruiting stalks.
Most wallflowers are pollinator-generalists, their flowers being visited by many different species of bees, bee flies, hoverflies, butterflies, beetles, and ants. However, there are some specialist species. For example, Teide wallflower is pollinated almost exclusively by "Anthophora alluadii".

</doc>
<doc id="59917" url="https://en.wikipedia.org/wiki?curid=59917" title="Erysimum 'Chelsea Jacket'">
Erysimum 'Chelsea Jacket'

Small and short-lived, "Erysimum" 'Chelsea Jacket' is a perennial cultivar of "Erysimum cheiri" usually grown for its long-lasting and brightly coloured flowers. It is closely related to "Cheiranthus". This plant has been given an RHS Award of Garden Merit. Normal height is about 30 cm, prostrate habit.
The plant flourishes in full sun and well-drained soil and is fairly hardy. It flowers continually from late spring to late summer. Flowers start off yellow then fade through tangerine to purple, often with a succession of colours on the same flower head. Some claim that it can be seen changing colors from day to day

</doc>
<doc id="59919" url="https://en.wikipedia.org/wiki?curid=59919" title="Pheasant">
Pheasant

Pheasants () are birds of several genera within the subfamily Phasianinae, of the family Phasianidae in the order Galliformes. The family's native range is restricted to Asia.
Pheasants are characterised by strong sexual dimorphism, males being highly decorated with bright colors and adornments such as wattles and long tails. Males are usually larger than females and have longer tails. Males play a part in rearing the young. Pheasants typically eat seeds and some insects.
The best-known is the common pheasant, which is widespread throughout the world in introduced feral populations and in farm operations. Various other pheasant species are popular in aviaries, such as the golden pheasant ("Chrysolophus pictus").
Species in taxonomic order.
This list is ordered to show presumed relationships between species.
Previous classifications.
"Euplocamus" and "Gennceus" are older names more or less corresponding to the current "Lophura".
These old genera were used for:

</doc>
<doc id="59920" url="https://en.wikipedia.org/wiki?curid=59920" title="Alfred Tarski">
Alfred Tarski

Alfred Tarski (; January 14, 1901 – October 26, 1983) was a Polish logician, mathematician and philosopher. Educated at the University of Warsaw and a member of the Lwów–Warsaw school of logic and the Warsaw school of mathematics and philosophy, he immigrated to the USA in 1939 where he became a naturalized citizen in 1945, and taught and carried out research in mathematics at the University of California, Berkeley from 1942 until his death.
A prolific author best known for his work on model theory, metamathematics, and algebraic logic, he also contributed to abstract algebra, topology, geometry, measure theory, mathematical logic, set theory, and analytic philosophy.
His biographers Anita and Solomon Feferman state that, "Along with his contemporary, Kurt Gödel, he changed the face of logic in the twentieth century, especially through his work on the concept of truth and the theory of models."
Life.
Alfred Tarski was born Alfred Teitelbaum (Polish spelling: "Tajtelbaum"), to parents who were Polish Jews in comfortable circumstances. He first manifested his mathematical abilities while in secondary school, at Warsaw's "Szkoła Mazowiecka". Nevertheless, he entered the University of Warsaw in 1918 intending to study biology.
After Poland regained independence in 1918, Warsaw University came under the leadership of Jan Łukasiewicz, Stanisław Leśniewski and Wacław Sierpiński and quickly became a world-leading research institution in logic, foundational mathematics, and the philosophy of mathematics. Leśniewski recognized Tarski's potential as a mathematician and encouraged him to abandon biology. Henceforth Tarski attended courses taught by Łukasiewicz, Sierpiński, Stefan Mazurkiewicz and Tadeusz Kotarbiński, and became the only person ever to complete a doctorate under Leśniewski's supervision. Tarski and Leśniewski soon grew cool to each other. However, in later life, Tarski reserved his warmest praise for Kotarbiński, as was mutual.
In 1923, Alfred Teitelbaum and his brother Wacław changed their surname to "Tarski." (Years later, Alfred met another Alfred Tarski in northern California.) The Tarski brothers also converted to Roman Catholicism, Poland's dominant religion. Alfred did so even though he was an avowed agnostic. Tarski was a Polish nationalist who saw himself as a Pole and wished to be fully accepted as such — later, in America, he spoke Polish at home.
After becoming the youngest person ever to complete a doctorate at Warsaw University, Tarski taught logic at the Polish Pedagogical Institute, mathematics and logic at the University, and served as Łukasiewicz's assistant. Because these positions were poorly paid, Tarski also taught mathematics at a Warsaw secondary school; before World War II, it was not uncommon for European intellectuals of research caliber to teach high school. Hence between 1923 and his departure for the United States in 1939, Tarski not only wrote several textbooks and many papers, a number of them ground-breaking, but also did so while supporting himself primarily by teaching high-school mathematics. In 1929 Tarski married fellow teacher Maria Witkowska, a Pole of Catholic background. She had worked as a courier for the army in the Polish-Soviet War. They had two children; a son Jan who became a physicist, and a daughter Ina who married the mathematician Andrzej Ehrenfeucht.
Tarski applied for a chair of philosophy at Lwów University, but on Bertrand Russell's recommendation it was awarded to Leon Chwistek. In 1930, Tarski visited the University of Vienna, lectured to Karl Menger's colloquium, and met Kurt Gödel. Thanks to a fellowship, he was able to return to Vienna during the first half of 1935 to work with Menger's research group. From Vienna he traveled to Paris to present his ideas on truth at the first meeting of the Unity of Science movement, an outgrowth of the Vienna Circle. In 1937, Tarski applied for a chair at Poznań University but the chair was abolished. Tarski's ties to the Unity of Science movement likely saved his life, because they resulted in his being invited to address the Unity of Science Congress held in September 1939 at Harvard University. Thus he left Poland in August 1939, on the last ship to sail from Poland for the United States before the German and Soviet invasion of Poland and the outbreak of World War II. Tarski left reluctantly, because Leśniewski had died a few months before, creating a vacancy which Tarski hoped to fill. Oblivious to the Nazi threat, he left his wife and children in Warsaw. He did not see them again until 1946. During the war, nearly all his extended family died at the hands of the German occupying authorities.
Once in the United States, Tarski held a number of temporary teaching and research positions: Harvard University (1939), City College of New York (1940), and thanks to a Guggenheim Fellowship, the Institute for Advanced Study in Princeton (1942), where he again met Gödel. In 1942, Tarski joined the Mathematics Department at the University of California, Berkeley, where he spent the rest of his career. Tarski became an American citizen in 1945. Although emeritus from 1968, he taught until 1973 and supervised Ph.D. candidates until his death. At Berkeley, Tarski acquired a reputation as an awesome and demanding teacher, a fact noted by many observers:
His seminars at Berkeley quickly became famous in the world of mathematical logic. His students, many of whom became distinguished mathematicians, noted the awesome energy with which he would coax and cajole their best work out of them, always demanding the highest standards of clarity and precision.
Tarski was extroverted, quick-witted, strong-willed, energetic, and sharp-tongued. He preferred his research to be collaborative — sometimes working all night with a colleague — and was very fastidious about priority.
A charismatic leader and teacher, known for his brilliantly precise yet suspenseful expository style, Tarski had intimidatingly high standards for students, but at the same time he could be very encouraging, and particularly so to women — in contrast to the general trend. Some students were frightened away, but a circle of disciples remained, many of whom became world-renowned leaders in the field.
Tarski supervised twenty-four Ph.D. dissertations including (in chronological order) those of Andrzej Mostowski, Bjarni Jónsson, Julia Robinson, Robert Vaught, Solomon Feferman, Richard Montague, James Donald Monk, Haim Gaifman, Donald Pigozzi and Roger Maddux, as well as Chen Chung Chang and Jerome Keisler, authors of "Model Theory" (1973), a classic text in the field. He also strongly influenced the dissertations of Alfred Lindenbaum, Dana Scott, and Steven Givant. Five of Tarski's students were women, a remarkable fact given that men represented an overwhelming majority of graduate students at the time.
Tarski lectured at University College, London (1950, 1966), the Institut Henri Poincaré in Paris (1955), the Miller Institute for Basic Research in Science in Berkeley (1958–60), the University of California at Los Angeles (1967), and the Pontifical Catholic University of Chile (1974–75). Among many distinctions garnered over the course of his career, Tarski was elected to the United States National Academy of Sciences, the British Academy and the Royal Netherlands Academy of Arts and Sciences in 1958, received honorary degrees from the Pontifical Catholic University of Chile in 1975, from Marseilles' Paul Cézanne University in 1977 and from the University of Calgary, as well as the Berkeley Citation in 1981. Tarski presided over the Association for Symbolic Logic, 1944–46, and the International Union for the History and Philosophy of Science, 1956–57. He was also an honorary editor of "Algebra Universalis".
Mathematician.
Tarski's mathematical interests were exceptionally broad. His collected papers run to about 2,500 pages, most of them on mathematics, not logic. For a concise survey of Tarski's mathematical and logical accomplishments by his former student Solomon Feferman, see "Interludes I–VI" in Feferman and Feferman.
Tarski's first paper, published when he was 19 years old, was on set theory, a subject to which he returned throughout his life. In 1924, he and Stefan Banach proved that, if one accepts the Axiom of Choice, a ball can be cut into a finite number of pieces, and then reassembled into a ball of larger size, or alternatively it can be reassembled into two balls whose sizes each equal that of the original one. This result is now called the Banach–Tarski paradox.
In "A decision method for elementary algebra and geometry", Tarski showed, by the method of quantifier elimination, that the first-order theory of the real numbers under addition and multiplication is decidable. (While this result appeared only in 1948, it dates back to 1930 and was mentioned in Tarski (1931).) This is a very curious result, because Alonzo Church proved in 1936 that Peano arithmetic (the theory of natural numbers) is "not" decidable. Peano arithmetic is also incomplete by Gödel's incompleteness theorem. In his 1953 "Undecidable theories", Tarski et al. showed that many mathematical systems, including lattice theory, abstract projective geometry, and closure algebras, are all undecidable. The theory of Abelian groups is decidable, but that of non-Abelian groups is not.
In the 1920s and 30s, Tarski often taught high school geometry. Using some ideas of Mario Pieri, in 1926 Tarski devised an original axiomatization for plane Euclidean geometry, one considerably more concise than Hilbert's. Tarski's axioms form a first-order theory devoid of set theory, whose individuals are points, and having only two primitive relations. In 1930, he proved this theory decidable because it can be mapped into another theory he had already proved decidable, namely his first-order theory of the real numbers.
In 1929 he showed that much of Euclidean solid geometry could be recast as a first-order theory whose individuals are "spheres" (a primitive notion), a single primitive binary relation "is contained in", and two axioms that, among other things, imply that containment partially orders the spheres. Relaxing the requirement that all individuals be spheres yields a formalization of mereology far easier to exposit than Lesniewski's variant. Near the end of his life, Tarski wrote a very long letter, published as Tarski and Givant (1999), summarizing his work on geometry.
"Cardinal Algebras" studied algebras whose models include the arithmetic of cardinal numbers. "Ordinal Algebras" sets out an algebra for the additive theory of order types. Cardinal, but not ordinal, addition commutes.
In 1941, Tarski published an important paper on binary relations, which began the work on relation algebra and its metamathematics that occupied Tarski and his students for much of the balance of his life. While that exploration (and the closely related work of Roger Lyndon) uncovered some important limitations of relation algebra, Tarski also showed (Tarski and Givant 1987) that relation algebra can express most axiomatic set theory and Peano arithmetic. For an introduction to relation algebra, see Maddux (2006). In the late 1940s, Tarski and his students devised cylindric algebras, which are to first-order logic what the two-element Boolean algebra is to classical sentential logic. This work culminated in the two monographs by Tarski, Henkin, and Monk (1971, 1985).
Logician.
Tarski's student, Vaught, has ranked Tarski as one of the four greatest logicians of all time — along with Aristotle, Gottlob Frege, and Kurt Gödel. However, Tarski often expressed great admiration for Charles Sanders Peirce, particularly for his pioneering work in the logic of relations.
Tarski produced axioms for "logical consequence", and worked on deductive systems, the algebra of logic, and the theory of definability. His semantic methods, which culminated in the model theory he and a number of his Berkeley students developed in the 1950s and 60s, radically transformed Hilbert's proof-theoretic metamathematics.
In [Tarski's] view, metamathematics became similar to any mathematical discipline. Not only its concepts and results can be mathematized, but they actually can be integrated into mathematics. ... Tarski destroyed the borderline between metamathematics and mathematics. He objected to restricting the role of metamathematics to the foundations of mathematics.
Tarski's 1936 article "On the concept of logical consequence" argued that the conclusion of an argument will follow logically from its premises if and only if every model of the premises is a model of the conclusion. In 1937, he published a paper presenting clearly his views on the nature and purpose of the deductive method, and the role of logic in scientific studies. His high school and undergraduate teaching on logic and axiomatics culminated in a classic short text, published first in Polish, then in German translation, and finally in a 1941 English translation as "Introduction to Logic and to the Methodology of Deductive Sciences".
Tarski's 1969 "Truth and proof" considered both Gödel's incompleteness theorems and Tarski's undefinability theorem, and mulled over their consequences for the axiomatic method in mathematics.
Truth in formalized languages.
In 1933, Tarski published a very long paper in Polish, titled "Pojęcie prawdy w językach nauk dedukcyjnych", setting out a mathematical definition of truth for formal languages. The 1935 German translation was titled "Der Wahrheitsbegriff in den formalisierten Sprachen", "The concept of truth in formalized languages", sometimes shortened to "Wahrheitsbegriff". An English translation appeared in the 1956 first edition of the volume "Logic, Semantics, Metamathematics". This collection of papers from 1923 to 1938 is an event in 20th-century analytic philosophy, a contribution to symbolic logic, semantics, and the philosophy of language. For a brief discussion of its content, see Convention T (and also T-schema).
Some recent philosophical debate examines the extent to which Tarski's theory of truth for formalized languages can be seen as a correspondence theory of truth. The debate centers on how to read Tarski's condition of material adequacy for a truth definition. That condition requires that the truth theory have the following as theorems for all sentences p of the language for which truth is being defined:
The debate amounts to whether to read sentences of this form, such as
as expressing merely a deflationary theory of truth or as embodying truth as a more substantial property (see Kirkham 1992). It is important to realize that Tarski's theory of truth is for formalized languages, so examples in natural language are not illustrations of the use of Tarski's theory of truth.
Logical consequence.
In 1936, Tarski published Polish and German versions of a lecture he had given the preceding year at the International Congress of Scientific Philosophy in Paris. A new English translation of this paper, Tarski (2002), highlights the many differences between the German and Polish versions of the paper, and corrects a number of mistranslations in Tarski (1983).
This publication set out the modern model-theoretic definition of (semantic) logical consequence, or at least the basis for it. Whether Tarski's notion was entirely the modern one turns on whether he intended to admit models with varying domains (and in particular, models with domains of different cardinalities). This question is a matter of some debate in the current philosophical literature. John Etchemendy stimulated much of the recent discussion about Tarski's treatment of varying domains.
Tarski ends by pointing out that his definition of logical consequence depends upon a division of terms into the logical and the extra-logical and he expresses some skepticism that any such objective division will be forthcoming. "What are Logical Notions?" can thus be viewed as continuing "On the Concept of Logical Consequence".
Work on logical notions.
Another theory of Tarski's attracting attention in the recent philosophical literature is that outlined in his "What are Logical Notions?" (Tarski 1986). This is the published version of a talk that he gave originally in 1966 in London and later in 1973 in Buffalo; it was edited without his direct involvement by John Corcoran. It became the most cited paper in the journal "History and Philosophy of Logic".
In the talk, Tarski proposed a demarcation of the logical operations (which he calls "notions") from the non-logical. The suggested criteria were derived from the Erlangen programme of the German 19th century Mathematician, Felix Klein. Mautner, in 1946, and possibly an article by the Portuguese mathematician Sebastiao e Silva, anticipated Tarski in applying the Erlangen Program to logic.
That program classified the various types of geometry (Euclidean geometry, affine geometry, topology, etc.) by the type of one-one transformation of space onto itself that left the objects of that geometrical theory invariant. (A one-to-one transformation is a functional map of the space onto itself so that every point of the space is associated with or mapped to one other point of the space. So, "rotate 30 degrees" and "magnify by a factor of 2" are intuitive descriptions of simple uniform one-one transformations.) Continuous transformations give rise to the objects of topology, similarity transformations to those of Euclidean geometry, and so on.
As the range of permissible transformations becomes broader, the range of objects one is able to distinguish as preserved by the application of the transformations becomes narrower. Similarity transformations are fairly narrow (they preserve the relative distance between points) and thus allow us to distinguish relatively many things (e.g., equilateral triangles from non-equilateral triangles). Continuous transformations (which can intuitively be thought of as transformations which allow non-uniform stretching, compression, bending, and twisting, but no ripping or glueing) allow us to distinguish a polygon from an annulus (ring with a hole in the centre), but do not allow us to distinguish two polygons from each other.
Tarski's proposal was to demarcate the logical notions by considering all possible one-to-one transformations (automorphisms) of a domain onto itself. By domain is meant the universe of discourse of a model for the semantic theory of a logic. If one identifies the truth value True with the domain set and the truth-value False with the empty set, then the following operations are counted as logical under the proposal:
In some ways the present proposal is the obverse of that of Lindenbaum and Tarski (1936), who proved that all the logical operations of Russell and Whitehead's "Principia Mathematica" are invariant under one-to-one transformations of the domain onto itself. The present proposal is also employed in Tarski and Givant (1987).
Solomon Feferman and Vann McGee further discussed Tarski's proposal in work published after his death. Feferman (1999) raises problems for the proposal and suggests a cure: replacing Tarski's preservation by automorphisms with preservation by arbitrary homomorphisms. In essence, this suggestion circumvents the difficulty Tarski's proposal has in dealing with sameness of logical operation across distinct domains of a given cardinality and across domains of distinct cardinalities. Feferman's proposal results in a radical restriction of logical terms as compared to Tarski's original proposal. In particular, it ends up counting as logical only those operators of standard first-order logic without identity.
McGee (1996) provides a precise account of what operations are logical in the sense of Tarski's proposal in terms of expressibility in a language that extends first-order logic by allowing arbitrarily long conjunctions and disjunctions, and quantification over arbitrarily many variables. "Arbitrarily" includes a countable infinity.

</doc>
<doc id="59922" url="https://en.wikipedia.org/wiki?curid=59922" title="Peter Scott">
Peter Scott

Sir Peter Markham Scott (14 September 1909 – 29 August 1989) was a British ornithologist, conservationist, painter, naval officer and sportsman.
Scott was knighted in 1973 for his contribution to the conservation of wild animals. He had been a founder of the World Wide Fund for Nature, founded the Wildfowl & Wetlands Trust (November 1946), and was an influence on international conservation. He received the WWF Gold Medal and the J. Paul Getty Prize for his work.
Early life.
Scott was born in London, the only child of Antarctic explorer Robert Falcon Scott and sculptor Kathleen Bruce. He was only two years old when his father died. Robert Scott, in a last letter to his wife, advised her to "make the boy interested in natural history if you can; it is better than games." He was named after Sir Clements Markham, mentor of Scott's polar expeditions, and his godfather was J. M. Barrie, creator of Peter Pan.
He was educated at Oundle School and Trinity College, Cambridge, initially reading Natural Sciences but graduating in the History of Art in 1931. Whilst at Cambridge he shared digs with John Berry and the two shared many views.
Art and sports.
Like his mother, he displayed a strong artistic talent and he became known as a painter of wildlife, particularly birds; he had his first exhibition in London in 1933. His wealthy background allowed him to follow his interests in art, wildlife and many sports, including wildfowling, sailing and ice skating. He represented Great Britain and Northern Ireland at sailing in the 1936 Berlin Olympic Games, winning a bronze medal in the O-Jolle dinghy class.
Second World War.
During the Second World War, Scott served in the Royal Navy Volunteer Reserve. As a Sub-Lieutenant, during the failed evacuation of the 51st Highland Division he was the British Naval officer sent ashore at Saint-Valery-en-Caux in the early hours of 11 June 1940 to evacuate some of the wounded. This was the last evacuation of British troops from the port area of St Valery that was not disrupted by enemy fire.
Then he served in destroyers in the North Atlantic but later moved to commanding the First (and only) Squadron of Steam Gun Boats against German E-boats in the English Channel. He was awarded the Distinguished Service Cross for bravery.
Scott is credited with designing the Western Approaches ship camouflage scheme, which disguised the look of ship superstructure. In July 1940, he managed to get the destroyer "HMS Broke (D83)" in which he was serving experimentally camouflaged, differently on the two sides. To starboard, the ship was painted blue-grey all over, but with white in naturally shadowed areas as countershading, following the ideas of Abbott Handerson Thayer from the First World War. To port, the ship was painted in "bright pale colours" to combine some disruption of shape with the ability to fade out during the night, again with shadowed areas painted white. However, he later wrote that compromise was fatal to camouflage, and that invisibility at night (by painting ships in white or other pale colours) had to be the sole objective. By May 1941, all ships in the Western Approaches (the North Atlantic) were ordered to be painted in Scott's camouflage scheme. The scheme was said to be so effective that several British ships including "HMS Broke" collided with each other. The effectiveness of Scott's and Thayer's ideas was demonstrated experimentally by the Leamington Camouflage Centre in 1941. Under a cloudy overcast sky, the tests showed that a white ship could approach six miles (9.6 km) closer than a black-painted ship before being seen. For this work he was appointed a Member of the Order of the British Empire.
Post-war life.
He stood as a Conservative candidate unsuccessfully in the 1945 general election in Wembley North. In 1946, he founded the organisation with which he was ever afterwards closely associated, the Severn Wildfowl Trust (now the Wildfowl and Wetlands Trust) with its headquarters at Slimbridge in Gloucestershire, where he saved the nene or Hawaiian goose, from extinction in the 1950s, through a captive breeding programme. In the years that followed, he led ornithological expeditions worldwide, and became a television personality, popularising the study of wildfowl and wetlands.
His BBC natural history series, "Look", ran from 1955 to 1981 and made him a household name. It included the first BBC natural history film to be shown in colour, "The Private Life of the Kingfisher" (1968), which he narrated. He wrote and illustrated several books on the subject, including his autobiography, "The Eye of the Wind" (1961). In the 1950s, he also appeared regularly on BBC radio's Children's Hour, in the series, "Nature Parliament".
Scott took up gliding in 1956 and became a British champion in 1963. He was chairman of the British Gliding Association (BGA) for two years from 1968 and was president of the Bristol & Gloucestershire Gliding Club. He was responsible for involving Prince Philip in gliding; the Prince is still patron of the BGA.
He was the subject of "This Is Your Life" in 1956 when he was surprised by Eamonn Andrews at the King's Theatre, Hammersmith, London.
As a member of the Species Survival Commission of the International Union for Conservation of Nature and Natural Resources, he helped create the Red Data books, the group's lists of endangered species.
From 1973 to 1983, Scott was Chancellor of the University of Birmingham. In 1979, he was awarded an Honorary Degree (Doctor of Science) from the University of Bath.
He died of a heart attack on 29 August 1989 in Bristol, two weeks before his 80th birthday.
He was the founder President of the Society of Wildlife Artists and President of the Nature in Art Trust (a role in which Philippa succeeded him). Scott tutored numerous artists including Paul Karslake.
Sailing.
Scott also continued with his love of sailing, skippering the 12 metre yacht "Sovereign" in the 1964 challenge for the America's Cup which was held by the United States. "Sovereign" suffered a whitewash 4–0 defeat in a one-sided competition where the American boat had a faster design. From 1955 – 1969 he was the president of the International Sailing Federation
World Wide Fund for Nature.
He was one of the founders of the World Wide Fund for Nature (formerly called the World Wildlife Fund), and designed its panda logo. His pioneering work in conservation also contributed greatly to the shift in policy of the International Whaling Commission and signing of the Antarctic Treaty, the latter inspired by his visit to his father's base on Ross Island in Antarctica.
Loch Ness Monster.
In 1962, he co-founded the Loch Ness Phenomena Investigation Bureau with the then Conservative MP David James, who had previously been Polar Adviser on the 1948 movie based on his late father's polar expedition "Scott of the Antarctic". In 1975 Scott proposed the scientific name of "Nessiteras rhombopteryx" for the Loch Ness Monster (based on a blurred underwater photograph of a supposed fin) so that it could be registered as an endangered species. The name was based on the Ancient Greek for "the monster of Ness with the diamond shaped fin", but it was later pointed out by "The Daily Telegraph" to be an anagram of "Monster hoax by Sir Peter S". Nessie researcher Robert H. Rines, who took two supposed pictures of the monster in the 1970s, responded by pointing out that the letters could also be read as an anagram for, "Yes, both pix are monsters, R."
British Naturalists' Association.
Scott was a long-time Vice-President of the British Naturalists' Association, whose "Peter Scott Memorial Award" was instituted after his death, to commemorate his achievements.
Television documentaries.
In June 2004, Scott and Sir David Attenborough were jointly profiled in the second of a three part BBC Two series, "The Way We Went Wild", about television wildlife presenters and were described as being largely responsible for the way that the British and much of the world views wildlife.
In 1996 Scott's life and work in wildlife conservation was celebrated in a major BBC Natural World documentary, produced by Andrew Cooper and narrated by Sir David Attenborough. Filmed across three continents from Hawaii to the Russian arctic, "In the Eye of the Wind" was the BBC Natural History Unit's tribute to Scott and the organisation he founded, the Wildfowl and Wetland Trust, on its 50th anniversary.
Scott's life was also the subject of a BBC Four documentary called "Peter Scott – A Passion for Nature" produced in 2006 by Available Light Productions (Bristol).
Cultural references.
Scott appears as a minor character in the novel "The Plague Dogs" by Richard Adams. The fictional Scott assists in rescuing the protagonists from their final peril, ably assisted by Ronald Lockley.
Scott is cited as a member of the eclectic (and fictional) "orchestra" in the Bonzo Dog Doo-Dah Band's recording, "The Intro and the Outro", where he is credited—appropriately—with playing a duck call.
Personal life.
Scott married the novelist Elizabeth Jane Howard in 1942 and had a daughter, Nicola, born a year later. Howard left Scott in 1946 and they were divorced in 1951. Howard, like Scott a Secretary in Robert Aickman's Inland Waterways Association, had an affair with Aickman.
In 1951, Scott married his assistant, Philippa Talbot-Ponsonby, while on an expedition to Iceland in search of the breeding grounds of the pink-footed goose. A daughter, Dafila, was born later in the same year ("dafila" is the old scientific name for a pintail). She, too, became an artist, painting birds. A son, Falcon, was born in 1954. Falcon Scott has worked part of the year giving lectures on educational cruises for Quark Expeditions.
Honours and decorations.
On 8 July 1941, it was announced that Scott had been Mentioned in Despatches "for good services in rescuing survivors from a burning Vessel" while serving on HMS "Broke". On 2 October 1942, it was announced that he had been further Mentioned in Despatches "for gallantry, daring and skill in the combined attack on Dieppe". On 1 June 1943, he was awarded the Distinguished Service Cross (DSC) "for skill and gallantry in action with enemy light forces".
He was appointed Member of the Order of the British Empire (MBE) in the 1942 King's Birthday Honours. He was promoted to Commander of the Order of the British Empire (CBE) in the 1953 Coronation Honours. In the 1987 Queen's Birthday Honours, he was appointed to the Order of the Companions of Honour (CH) "for services to conservation". He was knighted by Queen Elizabeth II at Buckingham Palace on 27 February 1973.
References.
The Wild Geese of the Newgrounds by Paul Walkden. Published by the Friends of WWT Slimbridge, 2009.ISBN 978-0-9561070-0-8
Illustrated with colour plates and ink drawing by Peter Scott. Includes chronology.

</doc>
<doc id="59925" url="https://en.wikipedia.org/wiki?curid=59925" title="J. F. C. Fuller">
J. F. C. Fuller

Major-General John Frederick Charles "Boney" Fuller, CB, CBE, DSO (1 September 1878 – 10 February 1966) was a British Army officer, military historian, and strategist, notable as an early theorist of modern armoured warfare, including categorizing principles of warfare. With 45 books and many articles, he was a highly prolific author whose ideas reached army officers and the interested public. He explored the business of fighting, in terms of the relationship between warfare and social, political, and economic factors in the civilian sector. Fuller emphasized the potential of new weapons, especially tanks and aircraft, to stun a surprised enemy psychologically. 
Fuller was highly controversial in British politics because of his support for the organized fascist movement. He was also an occultist and Thelemite who wrote a number of works on esotericism and mysticism.
Early life.
Fuller was born in 1878 at Chichester in West Sussex. After moving to Lausanne with his parents as a boy, he returned to England at the age of 11 without them; three years later, at "the somewhat advanced age of 14", he began attending Malvern College and, later trained for an army career at the Royal Military College, Sandhurst, from 1897 to 1898. His nickname of "Boney", which he was to retain, is said to have come either from an admiration for Napoleon Bonaparte, or from an imperious manner combined with military brilliance which resembled Napoleon's.
Career.
Fuller was commissioned into the 1st Battalion of the Oxfordshire Light Infantry (the old 43rd Foot), and served in South Africa from 1899 to 1902. In the spring of 1904 Fuller was sent with his unit to India, where he contracted typhoid fever in autumn of 1905; he returned to England the next year on sick-leave, where he met the woman he married in December 1906. Instead of returning to India, he was reassigned to units in England, serving as an adjutant to the 2nd South Middlesex Volunteers (amalgamated into the 7th Middlesex during the Haldane Reforms) and helping form the 10th Middlesex, until he was accepted into the Staff College at Camberley in 1913, starting work there in January 1914.
During the First World War, Fuller was a staff officer with the Home Forces and with 7 Corps in France, and from 1916 in the Headquarters of the Machine-Gun Corps' Heavy Branch which was later to become the Tank Corps. He helped plan the tank attack at the 20 November 1917 Battle of Cambrai and the tank operations for the Autumn offensives of 1918. His Plan 1919 for a fully mechanised offensive against the German army was never implemented. After 1918 he held various leading positions, notably as a commander of an experimental brigade at Aldershot.
After the war Fuller collaborated with his junior B. H. Liddell Hart in developing new ideas for the mechanisation of armies, 
launching a crusade for the mechanization and modernization of the British Army. Chief instructor of Camberly Staff College from 1923, he became military assistant to the chief of the Imperial General Staff in 1926. 
In what came to be known as the "Tidworth Incident", Fuller turned down the command of the Experimental Mechanized Force, which was formed on August 27, 1927. The appointment also carried responsibility for a regular infantry brigade and the garrison of Tidworth Camp on Salisbury Plain. Fuller believed he would be unable to devote himself to the Experimental Mechanized Force and the development of mechanized warfare techniques without extra staff to assist him with the additional extraneous duties, which the War Office refused to allocate.
He was promoted to major general in 1930 and retired three years later to devote himself entirely to writing.
Retirement and Fascism.
After retirement, Fuller served as a reporter during the Italian invasion of Ethiopia (1935) and the Spanish Civil War (1936–39). On his retirement in 1933, impatient with what he considered the inability of democracy to adopt military reforms, Fuller became involved with Sir Oswald Mosley and the British Fascist movement. As a member of the British Union of Fascists he sat on the party's Policy Directorate and was considered one of Mosley's closest allies. He was also a member of the clandestine far right group the Nordic League.
Fuller's ideas on mechanized warfare continued to be influential in the lead-up to the Second World War, ironically less with his countrymen than with the Nazis, notably Heinz Guderian who spent his own money to have Fuller's "Provisional Instructions for Tank and Armoured Car Training" translated. In the 1930s the German Army implemented tactics similar in many ways to Fuller's analysis, which became known as Blitzkrieg. Like Fuller, theorists of Blitzkrieg partly based their approach on the theory that areas of large enemy activity should be bypassed to be eventually surrounded and destroyed. Blitzkrieg-style tactics were used by several nations throughout the Second World War, predominantly by the Germans in the invasion of Poland (1939), Western Europe (1940), and the Soviet Union (1941). While Germany and to some degree the Western Allies adopted Blitzkrieg ideas, they were not much used by the Red Army, which developed its armored warfare doctrine based on deep operations, which were developed by Soviet military theorists Marshal M. N. Tukhachevsky et al. in the 1920s based on their experiences in the First World War and the Russian Civil War.
Fuller was the only foreigner present at Nazi Germany’s first armed manoeuvres in 1935. Fuller frequently praised Adolf Hitler in his speeches and articles, once describing him as "that realistic idealist who has awakened the common sense of the British people by setting out to create a new Germany". On April 20, 1939 Fuller was an honoured guest at Hitler's 50th birthday parade, watching as "for three hours a completely mechanised and motorised army roared past the Führer." Afterwards Hitler asked, "I hope you were pleased with your children?" Fuller replied, "Your Excellency, they have grown up so quickly that I no longer recognise them."
During World War II, 1939-45, Fuller was under suspicion for his Nazi sympathies. He continued to speak out in favour of a peaceful settlement with Germany. Alan Brooke (in his war diaries, p. 201) comments that "the Director of Security called on him to discuss Boney Fuller and his Nazi activities", though Alanbrooke commented that he did not think Fuller "had any unpatriotic intentions". Although he was not interned or arrested, he was the only officer of his rank not invited to return to service during the World War II. There was some suspicion that he was not incarcerated in May 1940 along with other leading officials of the BUF because of his association with General Edmund Ironside and other senior officers. Mosley himself admitted to "a little puzzlement" as to why Fuller had not been imprisoned.
Fuller died in Falmouth, Cornwall in 1966.
Military theories.
Fuller was a vigorous, expressive, and opinionated writer of military history and of controversial predictions of the future of war, publishing "On Future Warfare" in 1928. Seeing his teachings largely vindicated by World War II, he published "Machine Warfare: An Enquiry into the Influence of Mechanics on the Art of War" in 1942.
"The Foundations of the Science of War (1926)".
Fuller is perhaps best known today for his "Nine Principles of War" which have formed the foundation of much of modern military theory since the 1930s, and which were originally derived from a convergence of Fuller's mystical and military interests. The Nine Principles went through several iterations; Fuller stated that 
"...the system evolved from six principles in 1912, rose to eight in 1915, to, virtually, nineteen in 1923, and then descended to nine in 1925..." For example, notice how his analysis of general Ulysses S. Grant was presented in 1929.
The United States Army modified Fuller's list and issued its first list of the principles of war in 1921, making it the basis of advanced training for officers into the 1990s, when it finally reconceptualized its training.
The Nine Principles of War.
The Nine Principles involve the uses of Force (combat power). They have been expressed in various ways, but Fuller's 1925 arrangement is as follows:
Triads and Trichotomies.
Cabalistic influences on his theories can be evidenced by his use of the "Law of Threes" throughout his work. Fuller didn't believe the Principles stood alone as is thought today, but that they complemented and overlapped each other as part of a whole, forming the Law of Economy of Force.
Organization of Force.
These Principles were further grouped into the categories of "Control" (command / co-operation), "Pressure" (attack / activity) and "Resistance" (protection / stability). The Principles of Control guides the dual Principles of Pressure and of Resistance, which in turn create the Principles of Control.
The Unity of the Principles of War.
They were also grouped into Cosmic ("Spiritual"), Mental ("Mind / Thought / Reason"), Moral ("Soul / Sensations / Emotions"), and Physical ("Body / Musculature / Action") Spheres, in which two Principles (like the double-edged point of an arrowhead) combine to create or manifest a third, which in turn guides the first and second Principles (like the fletches on an arrow's tail). Each Sphere leads to the creation of the next until it returns to the beginning and repeats the circular cycle with reassessments of the "Object" and "Objective" to redefine the uses of "Force". The Cosmic Sphere is seen as outside the other three Spheres, like the Heavens are outside the Realm of Man. They influence it indirectly in ways that cannot be controlled by the commander, but they are a factor in the use of Force. Force resides in the center of the pattern, as all of these elements revolve around it. 
These Principles of War have been adopted and further refined by the military forces of several nations, most notably within NATO, and continue to be applied widely to modern strategic thinking. Recently they have also been applied to business tactics and hobby wargaming.
"Armament and History (1945)".
Fuller also developed the idea of the "Constant Tactical Factor". This states that every improvement in warfare is checked by a counter-improvement, causing the advantage to shift back and forth between the offensive and the defensive. Fuller's firsthand experience in the First World War saw a shift from the defensive power of the machine gun to the offensive power of the tank.
Magic and mysticism.
Fuller had an occultist side that oddly mixed with his military side. He was an early disciple of English poet and magician Aleister Crowley, and was very familiar with his and other forms of magick and mysticism. While serving in the First Oxfordshire Light Infantry he had entered and won a contest to write the best review of Crowley's poetic works, after which it turned out that he was the only entrant. This essay was later published in book form in 1907 as "The Star in the West". After this he became an enthusiastic supporter of Crowley, joining his magical order, the A∴A∴. within which he became a leading member, editing order documents and its journal, "The Equinox". During this period he wrote "The Treasure House of Images", edited early sections of Crowley's magical autobiography "The Temple of Solomon the King" and produced highly regarded paintings dealing with A∴A∴ teachings: these paintings have been used in recent years as the covers of the journal's revival, "The Equinox, Volume IV".
After the "Jones vs. The Looking Glass" case, in which a great deal was made of Aleister Crowley's bisexuality (although Crowley himself was not a party to the case), Fuller became worried that his association with Crowley might be a hindrance to his career. Crowley writes in chapter 67 of his book, "The Confessions of Aleister Crowley":
...to my breathless amazement he fired pointblank at my head a document in which he agreed to continue his co-operation on condition that I refrain from mentioning his name in public or private under penalty of paying him a hundred pounds for each such offence. I sat down and poured in a broadside at close quarters.
"My dear man," I said in effect, "do recover your sense of proportion, to say nothing of your sense of humour. Your contribution, indeed! I can do in two days what takes you six months, and my real reason for ever printing your work at all is my friendship for you. I wanted to give you a leg up the literary ladder. I have taken endless pain to teach you the first principles of writing. When I met you, you were not so much as a fifth-rate journalist, and now you can write quite good prose with no more than my blue pencil through two out of every three adjectives, and five out of every six commas. Another three years with me and I will make you a master, but please don't think that either I or the Work depend on you, any more than J.P. Morgan depends on his favourite clerk."
After this, contact between the two men faded rapidly. The front pages of the 1913 issues of the "Equinox" (Volume 1, nos. 9 and 10), which gave general directions to A∴A∴ members, included a notice on the subject of Fuller, who was described as a "former Probationer"; the notice disparaged Fuller's magical accomplishments and warned A∴A∴ members to accept no magical training from him. However, Fuller continued to be fascinated with occult subjects and in later years he would write about topics such as the Qabalah and yoga.
Works.
Fuller was a prolific writer and published 45+ books.
External links.
For examples of the use of Fuller's campaign theories in the business world see:

</doc>
<doc id="59926" url="https://en.wikipedia.org/wiki?curid=59926" title="Albert I of Belgium">
Albert I of Belgium

Albert I (8 April 1875 – 17 February 1934) reigned as King of the Belgians from 1909 to 1934. This was an eventful period in the History of Belgium, which included the period of World War I (1914–1918), when 90 percent of Belgium was overrun, occupied, and ruled by the German Empire. Other crucial issues included the adoption of the Treaty of Versailles, the ruling of the Belgian Congo as an overseas possession of the Kingdom of Belgium along with the League of Nations mandate of Ruanda-Urundi, the reconstruction of Belgium following the war, and the first five years of the Great Depression (1929–1934). King Albert died in a mountaineering accident in eastern Belgium in 1934, at the age of 58, and he was succeeded by his son Leopold.
Early life.
Born Albert Léopold Clément Marie Meinrad in Brussels, he was the fifth child and second son of Prince Philippe, Count of Flanders, and his wife, Princess Marie of Hohenzollern-Sigmaringen. Prince Philippe was the third (second surviving) son of Leopold I, the first King of the Belgians, and his wife, Marie-Louise of France, and the younger brother of King Leopold II of Belgium. Princess Marie was a relative of Kaiser Wilhelm II of Germany, and a member of the non-reigning, Catholic branch of the Hohenzollern family. Albert grew up in the Palace of Flanders, initially as third in the line of succession to the Belgian throne as his reigning Uncle Leopold II's son had already died. When, however, Albert's older brother, Prince Baudouin of Belgium, who had been subsequently prepared for the throne, also died young, Albert, at the age of 16, unexpectedly became second in line (after his father) to the Belgian Crown.
Retiring and studious, Albert prepared himself strenuously for the task of kingship. In his youth, Albert was seriously concerned with the situation of the working classes in Belgium, and personally traveled around working class districts incognito, to observe the living conditions of the people. Shortly before his accession to the throne in 1909, Albert undertook an extensive tour of the Belgian Congo, which had been annexed by Belgium in 1908 (after having been previously owned by King Leopold II of Belgium as his personal property), finding the country in poor condition. Upon his return to Belgium, he recommended reforms to protect the native population and to further technological progress in the colony.
He was the 1,152nd Knight of the Order of the Golden Fleece in Austria and the 851st Knight of the Order of the Garter in 1914.
Marriage.
Albert was married in Munich on 2 October 1900 to Duchess Elisabeth Gabrielle Valérie Marie in Bavaria, a Wittelsbach princess whom he had met at a family funeral. A daughter of Karl-Theodor, Duke in Bavaria, and his wife, the Infanta Maria Josepha of Portugal, she was born at Possenhofen Castle, Bavaria, Germany, on 25 July 1876, and died on 23 November 1965. Based on the letters written during their engagement and marriage (cited extensively in the memoirs of their daughter, Marie-José) the young couple appear to have been deeply in love. The letters express a deep mutual affection based on a rare affinity of spirit. They also make clear that Albert and Elisabeth continually supported and encouraged each other in their challenging roles as king and queen. The spouses shared an intense commitment to their country and family and a keen interest in human progress of all kinds. Together, they cultivated the friendship of prominent scientists, artists, mathematicians, musicians, and philosophers, turning their court at Laeken into a kind of cultural salon.
Children.
Albert and Elisabeth had three children:
Accession.
Following the death of his uncle, Leopold II, Albert succeeded to the Belgian throne in December 1909, since Albert's own father had already died in 1905. Previous Belgian kings had taken the royal accession oath only in French; Albert innovated by taking it in Dutch as well. He and his wife, Queen Elisabeth, were popular in Belgium due to their simple, unassuming lifestyle and their harmonious family life, which stood in marked contrast to the aloof, autocratic manner and the irregular private life of Leopold II. An important aspect of the early years of Albert's reign was his institution of many reforms in the administration of the Belgian Congo, Belgium's only colonial possession.
Religion.
King Albert was a devout Catholic. Many stories illustrate his deep and tender piety. For instance, when his former tutor General De Grunne, in his old age, entered the Benedictine monastery of Maredsous in Belgium, King Albert wrote a letter to him in which he spoke of the joy of giving oneself to God. He said: "May you spend many years at Maredsous in the supreme comfort of soul that is given, to natures touched by grace, by faith in God's infinite power and confidence in His goodness." To another friend, a Chinese diplomat, who became a Catholic monk, Albert wrote: "Consecrating oneself wholly to the service of Our Lord gives, to those touched by grace, the peace of soul which is the supreme happiness here below." Albert used to tell his children: "As you nourish your body, so you should nourish your soul." In an interesting meditation on what he viewed as the harm that would result if Christian ideals were abandoned in Belgium, he said: "Every time society has distanced itself from the Gospel, which preached humility, fraternity, and peace, the people have been unhappy, because the pagan civilization of ancient Rome, which they wanted to replace it with, is based only on pride and the abuse of force" (Commemorative speech for the war dead of the Battle of the Yser, given by Dom Marie-Albert, Abbot of Orval Abbey, Belgium, in 1936 ).
World War I.
At the start of World War I, Albert refused to comply with Germany's request for safe passage for its troops through Belgium in order to attack France, which the Germans alleged was about to advance into Belgium en route to attacking Germany in support of Russia. In fact, the French government had told its army commander not to go into Belgium before a German invasion. The German invasion brought Britain into the war as one of the guarantors of Belgian neutrality under the Treaty of 1839. King Albert, as prescribed by the Belgian constitution, took personal command of the Belgian army, and held the Germans off long enough for Britain and France to prepare for the Battle of the Marne (6–9 September 1914). He led his army through the Siege of Antwerp and the Battle of the Yser, when the Belgian army was driven back to a last, tiny strip of Belgian territory near the North Sea. Here the Belgians, in collaboration with the armies of the Triple Entente, took up a war of position, in the trenches behind the River Yser, remaining there for the next four years. During this period, King Albert fought with his troops and shared their dangers, while his wife, Queen Elisabeth, worked as a nurse at the front. During his time on the front, rumors spread on both sides of the lines that the German soldiers never fired upon him out of respect for him being the highest ranked commander in harm's way, while others feared risking punishment by the Kaiser himself. The king also allowed his 14-year-old son, Prince Leopold, to enlist in the Belgian army as a private and fight in the ranks.
The war inflicted great suffering on Belgium, which was subjected to a harsh German occupation. The king, fearing the destructive results of the war for Belgium and Europe and appalled by the huge casualty rates, worked through secret diplomatic channels for a negotiated peace between Germany and the Entente based on the "no victors, no vanquished" concept. He considered that such a resolution to the conflict would best protect the interests of Belgium and the future peace and stability of Europe. Since, however, neither Germany nor the Entente were favorable to the idea, tending instead to seek total victory, Albert's attempts to further a negotiated peace were unsuccessful. At the end of the war, as commander of the Army Group Flanders, consisting of Belgian, British and French divisions, Albert led the final offensive of the war that liberated occupied Belgium. King Albert, Queen Elisabeth, and their children then re-entered Brussels to a hero's welcome.
Post-war years.
Upon his return to Brussels, King Albert made a speech in which he outlined the reforms he desired to see implemented in Belgium, including universal suffrage and the establishment of a Flemish University in Ghent.
Trip to the United States.
From 23 September through 13 November 1919, King Albert, Queen Elisabeth of Bavaria, and their son Prince Leopold took an official visit to the United States. During a visit of the historic Indian pueblo of Isleta Pueblo, New Mexico, King Albert decorated Father Anton Docher with the Order of Léopold. Docher offered the king a turquoise cross mounted in silver made by the Tiwas Indians. Ten thousand people traveled to Isleta for this occasion.
Introduction of universal male suffrage.
In 1918, King Albert forged a post-war "Government of National Union" made up of members of the three main parties in Belgium, the Catholics, the Liberals, and the Socialists. Albert I remembered the Belgian general strike of 1913, and the promise following that of a Constitutional reform for an actual "one man, one vote" universal suffrage.
On 18 April 1893, at the end of the Belgian general strike of 1893, universal suffrage, approved by the Belgian Parliament, gave plural votes to individual men based on their wealth, education, and age, but this was clearly not a universal suffrage.
Albert attempted to mediate between the parties in favor of universal male suffrage and those opposed to it in order to bring about "one man, one vote" universal suffrage for men. He succeeded. Some people have named this the ""conspiracy of Loppem"" because the "one man, one vote" suffrage was effected without changing the Constitution of Belgium.
Paris Peace Conference.
The Belgian government sent the king to the Paris Peace Conference in April 1919, where he met with the leaders of France, Britain and the United States. He had four strategic goals: (1) to restore and expand the Belgian economy using cash reparations from Germany; (2) to assure Belgium's security by the creation of a new buffer state on the left bank of the Rhine; (3) to revise the obsolete treaty of 1839; (4) to promote a 'rapprochement' between Belgium and the Grand Duchy of Luxemburg. He strongly advised against a harsh, restrictive treaty against Germany to prevent future German aggression. He also considered that the dethronement of the princes of Central Europe and, in particular, the dissolution of the Habsburg Empire would constitute a serious menace to peace and stability on the continent. The Allies considered Belgium to be the chief victim of the war, and it aroused enormous popular sympathy, but the king's advice played a small role in Paris.
Albert spent much of the remainder of his reign assisting in the post-war reconstruction of Belgium.
Albert was a committed conservationist and in 1925, influenced by the ideas of Carl E. Akeley, he founded Africa's first national park, now known as Virunga National Park, in what is now Democratic Republic of Congo. During this period, he was also the first European monarch to visit the United States.
Death.
A passionate alpinist, King Albert I died in a mountaineering accident while climbing alone on the Roche du Vieux Bon Dieu at Marche-les-Dames, in the Ardennes region of Belgium near Namur. His death shocked the world and he was deeply mourned, both in Belgium and abroad. Because King Albert was an expert climber, some questioned the official version of his death and suggested that the king was murdered (or even committed suicide) somewhere else and that his body has never been at Marche-les-Dames, or that it was deposited over there. Several of those hypotheses with criminal motives were already investigated by the juridical authorities but the doubts have been increased ever since, today still being the subject to popular novels, books and documentaries. Nonetheless, rumors of murder have been dismissed by most historians. There are two possible explanations for his death according to the official juridical investigations: the first was he leaned against a boulder at the top of the mountain, which became dislodged; or two, the pinnacle to which his rope was belayed had broken, causing him to fall about sixty feet. King Albert is interred in the Royal Crypt at the Church of Our Lady of Laeken in Brussels.
In 1935, prominent Belgian author Emile Cammaerts published a widely acclaimed biography of King Albert I, titled "Albert of Belgium: Defender of Right". In 1993, a close climbing companion of the king, Walter Amstutz, founded the King Albert I Memorial Foundation, an association based in Switzerland and dedicated to honoring distinguished individuals in the mountaineering world.
Celebrating 175 years of Belgian Dynasty and the 100th anniversary of his accession, Albert I was selected as the main motif of a high-value collectors' coin: the Belgian 12.5 euro Albert I commemorative coin, minted in 2008. The obverse shows a portrait of the king.
Honors and awards.
Albert was Grand Master of several Belgian Orders: Order of Leopold (also Grand Cross decorated with Grand Cordon), Order of the African Star, Royal Order of the Lion, Order of the Crown and Order of Leopold II.
He was also the recipient of foreign awards:
Ancestry.
Arms.
On his birth, Albert was granted a coat of arms. These were those of the king, differenced by a label gules, with one crescent argent on the central point. When his father died in 1905, the crescent was removed. When he acceded as King, he gained the royal arms (Belgium with inescutcheon of the shield of Saxony), undifferenced. Finally, after the abolition of monarchy in Germany and the subsequent loss of his saxonian titles, Albert had the inescutcheon removed in 1921.

</doc>
<doc id="59928" url="https://en.wikipedia.org/wiki?curid=59928" title="130 BC">
130 BC

__NOTOC__
Year 130 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Lentulus/Pulcher and Perperna (or, less frequently, year 624 "Ab urbe condita"). The denomination 130 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
</onlyinclude>

</doc>
<doc id="59930" url="https://en.wikipedia.org/wiki?curid=59930" title="133 BC">
133 BC

__NOTOC__
Year 133 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Scaevola and Frugi (or, less frequently, year 621 "Ab urbe condita"). The denomination 133 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="59931" url="https://en.wikipedia.org/wiki?curid=59931" title="131 BC">
131 BC

__NOTOC__
Year 131 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Mucianus and Flaccus (or, less frequently, year 623 "Ab urbe condita"). The denomination 131 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59932" url="https://en.wikipedia.org/wiki?curid=59932" title="128 BC">
128 BC

__NOTOC__
Year 128 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Octavius and Rufus (or, less frequently, year 626 "Ab urbe condita"). The denomination 128 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="59933" url="https://en.wikipedia.org/wiki?curid=59933" title="Charmander">
Charmander

Charmander, known as in Japan, is a Pokémon species in Nintendo and Game Freak's "Pokémon" franchise. Created by Ken Sugimori, Charmander first appeared in the video games Pokémon Red and Blue and subsequent sequels, later appearing in various merchandise, spinoff titles and animated and printed adaptations of the franchise. The end of a Charmander's tail is alight with a flame, and the flame's size reflects both the physical health and the emotions of the individual.
Charley was created as one of the first Pokémon and is a starter Pokémon the player can choose from at the beginning of "Pokémon Red" and "Blue", and their remakes, "Pokémon FireRed" and "LeafGreen". In the anime, Ash acquires a Charmander early in the series, and it became one of his most used Pokemon. In the "Pokémon Adventures" manga, Blue receives a Charmander from his grandfather Professor Oak. Since it appeared in the "Pokémon" series, Charmander has received generally positive reception.
Charmander evolves into Charmeleon who then evolves into Charizard, which was originally its last form. Since the release of "Pokémon X" and "Y", Charizard can mega evolve into Mega Charizard for the duration of a battle.
Design and characteristics.
Charmander was one of several different designs conceived by Game Freak's character development team and finalized by Ken Sugimori for the first generation of "Pocket Monsters" games "Red" and "Green", which were localized outside Japan as "Pokémon Red" and "Blue". Originally called "Hitokage" in Japanese, Nintendo decided to give the various Pokémon species "clever and descriptive names" related to their appearance or features when translating the game for western audiences as a means to make the characters more relatable to American children. As a result, the species was renamed "Charmander", a combination of "char", meaning burnt, and "salamander".
Charmander is known as the Dragon Pokémon. Charmander are small, bipedal lizard-like Pokémon native to Kanto. They have blue eyes, red-orange skin, three-clawed toes, yellow bellies, and yellow soles under their feet. The end of a Charmander's tail is alight with a flame, and the flame's size reflects both the physical health and the emotions of the individual. When it rains, steam is said to spout from the tip of its tail. If the flame were to ever go out, the Charmander would die. When Charmander receives enough experience from battles, it evolves into Charmeleon (at level 16 in the video games), and later Charizard. With the help of the Mega Stone it could further evolve into Mega Charizard X/Mega Charizard Y. The idea to feature Charmander and the other "Red" and "Blue" starters in a significant role in "Pokémon X" and "Y" came about a year and a half into the development of the games. The Mega Evolutions for the three Pokémon's final forms were created, and the designers decided that they should give players an opportunity to find one of these Pokémon in order to see their Mega Evolved form.
Appearances.
In the video games.
Charmander is a starter Pokémon the player can choose from at the beginning of "Pokémon Red" and "Blue", and their remakes, "Pokémon FireRed" and "LeafGreen". Charmander and the other starters from "Red" and "Blue" are replaced by Pikachu in "Pokémon Yellow", the only starter available in it. Instead, they are each obtained from certain NPCs. In "Pokémon HeartGold" and "SoulSilver", as a reward from Professor Oak after defeating the final boss, Red, the player can choose from Bulbasaur, Charmander, and Squirtle. In "Pokémon X" and "Y", players can also choose between Bulbasaur, Charmander, and Squirtle near the start of the game shortly after having chosen the games' new starter Pokémon. Outside of the main series, Charmander has appeared in "Hey You, Pikachu!", "Pokémon Snap", "Pokémon Puzzle League", the "Pokémon Mystery Dungeon" games, the "Pokémon Ranger" games, and "". A Pokémon stage in Super Smash Bros. called "Saffron City" features an area where various Pokémon pop out to attack players; one such Pokémon is a Charmander that sometimes uses Flamethrower.
In anime.
In the anime, Ash acquires a Charmander early in the series. Ash's Charmander originally belonged to a trainer named Damian, who believed it was weak and cruelly abandoned it, telling it to stay in one spot until he "returned." The Pokémon was very loyal to its trainer and risked its life sitting in the rain, waiting for a trainer who'd never come back to it. Ash, Brock, and Misty had to rush it to a Pokémon Center to keep it alive. Upon seeing Damian's true colors, Charmander joined Ash. It was immediately one of Ash's most used Pokémon, defeating such opponents as Koga's Golbat, Erika's Weepinbell, and helped capture Ash's Primeape. Charmander evolved into Charmeleon during a battle against an army of Exeggutor. His personality temporarily changed, disobeying Ash and fighting only when and how he pleased.
In an anime adaption of , a Charmander and a female Chikorita work alongside a young boy who transformed into a Squirtle in helping fellow Pokémon.
In other media.
In the Electric Tale of Pikachu manga, the circumstances in which Ash captures a Charmander appear to be different. While Damian appears, he was separated from his Charmander because he was injured, not because he abandoned it. At the end of the chapter, the two reunite. Despite this difference, Ash is still seen owning a Charmander, whose capture is not shown. Later in the manga, Ash's Charmander reappears as a Charizard. In the "Pokémon Adventures" manga, Blue receives a Charmander from his grandfather Professor Oak. Blue tried using it against Mew but failed and withdrew his Pokémon. It is later shown to have evolved into a Charmeleon. In the Pokémon Pocket Monsters manga series, Isamu Akai's rival Kai Midorikawa, chose Charmander as his starter Pokémon. Kai's Charmander is mischievous and has a rivalry with Isamu Akai's Clefairy.
Reception.
Since it appeared in the "Pokémon" series, Charmander has received generally positive reception. It has appeared in several pieces of merchandise, including figures, plush toys, and the "Pokémon Trading Card Game". It has been noted as a popular Halloween costume for the year of 1999. Also in 1999, it was speculated by analysts that Pokémon species, in particular Charmander and others, would become sought-after toys.
IGN readers ranked Charmander at #37 among the best Pokémon ever. "Game Informer"s O'Dell Harmon ranked Charmander - along with Bulbasaur and Squirtle - as the third best Pokémon. He noted that the choice between the three was "one of the most important decisions to ever be made in "Pokémon" history." "GamesTM" noted that Charmander was the worst starting Pokémon in "Red" and "Blue". In the book "Dragonlore: From the Archives of the Grey School of Wizardry", author Ash Dekirk described Charmander as a "fire-breathing dragon". Author Loredana Lipperini cited Charmander as a popular Pokémon, suggesting that its popularity comes from its fiery tail. Author Mark Jacobson found the transition from Charmander to Charizard to be odd, questioning how a "baby" Pokémon can grow into a "two-hundred-pound monster whose breath can melt boulders." GamesRadar commented that while Charmander seems pitiful due to its flame tail, which burn more brightly depending on his mood/health, it grows into the cool-looking Charizard. GamesRadar editor Brett Elston stated that while it lacks the nuances of later similar starting Pokémon, it has "cutesy appeal" to it. "The Escapist" editor John Funk described Charmander as "cute", using its evolution into Charizard as an example of an extreme evolutionary change in the series. "Chicago Tribune" editor Darryl E. Owens described Charmander as "adorable". "San Antonio-Express News" editor Susan Yerkes described Charmander as "disgustingly cute". "Teen Ink" editor Kathryn J. called Charmander her favorite Pokémon.

</doc>
<doc id="59934" url="https://en.wikipedia.org/wiki?curid=59934" title="ECA">
ECA

ECA may refer to:

</doc>
<doc id="59937" url="https://en.wikipedia.org/wiki?curid=59937" title="136 BC">
136 BC

__NOTOC__
Year 136 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Philus and Serranus (or, less frequently, year 618 "Ab urbe condita"). The denomination 136 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
China.
</onlyinclude>

</doc>
<doc id="59938" url="https://en.wikipedia.org/wiki?curid=59938" title="126 BC">
126 BC

__NOTOC__
Year 126 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Lepidus and Orestes (or, less frequently, year 628 "Ab urbe condita"). The denomination 126 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Syria.
</onlyinclude>

</doc>
<doc id="59940" url="https://en.wikipedia.org/wiki?curid=59940" title="Falconidae">
Falconidae

The falcons and caracaras are around 60 species of diurnal birds of prey that make up the family Falconidae. The family is divided into two subfamilies, Polyborinae, which includes the caracaras and forest falcons, and Falconinae, the falcons, kestrels and falconets. They differ from the eagles of Accipitridae, in that falcons kill with their beaks instead of their taloned feet. They have a "tooth" on the side of their beak for the purpose.
Description.
Falcons and caracaras are small to medium-sized birds of prey, ranging in size from the black-thighed falconet, which can weight as little as , to the gyrfalcon, which can weigh as much as . They have strongly hooked bills, sharply curved talons and excellent eyesight. The plumage is usually composed of browns, whites, chestnut, black and grey, often with barring of patterning. There is little difference in the plumage of males and females, although a few species have some sexual dimorphism in boldness of plumage.
Distribution and habitat.
The family has a cosmopolitan distribution across the world, absent only from the densest forest of central Africa, some remote oceanic islands, the high Arctic and Antarctica. Some species have exceptionally wide ranges, particularly the cosmopolitan peregrine falcon, which ranges from Greenland to Fiji and has the widest natural breeding distribution of any bird. Other species have more restricted distributions, particularly island endemics like the Mauritius kestrel. Most habitat types are occupied, from tundra to rainforest and deserts, although they are generally more birds of open country and even forest species tend to prefer broken forest and forest edges. Some species, mostly in the genus "Falco", are fully migratory, with some species summering in Eurasia and wintering entirely in Africa, other species may be partly migratory. The Amur falcon has one of the longest migrations, moving from East Asia to southern Africa.
Behaviour.
Diet and feeding.
Falcons and caracaras are carnivores, feeding on birds, small mammals including bats, reptiles, insects and carrion. In popular imagination the falconids are fast flying predators, and while this is true of the genus "Falco" and some falconets other species, particularly the caracaras are more sedentary in their feeding. The forest falcons of the Neotropics are generalist forest hunters. Several species, particularly the true falcons, will stash food supplies in caches. They are solitary hunters and pairs guard territories, although they may form large flocks during migration. Some species are specialists, the laughing falcon specialises in snakes, others are more generalist.
Breeding.
The falcons and caracaras are generally solitary breeders, although around 10% of species are colonial, for example the red-footed falcon. They are monogamous, although some caracaras may also employ alloparenting strategies, where younger birds help adults (usually their parents) in raising the next brood of chicks. Nests are generally not built (except by the caracaras), but are co opted from other birds, for example pygmy falcons nest in the nests of weavers, or on the ledges on cliffs. Around 2-4 eggs are laid, and mostly incubated by the female. Incubation times vary from species to species and are correlated with body size, lasting 28 days in smaller species and up to 35 days in larger species. Chicks fledge after 28–49 days, again varying with size.
Relations with humans.
Falcons and caracaras have a complicated relationship with humans. In ancient Egypt they were deified in the form of Horus, the sky and sun god who was the ancestor of the pharaohs. Caracaras also formed part of the legends of the Aztecs, and are today the national emblems of Mexico. Falcons were important in the (formerly often royal) sport of falconry. They have also been persecuted for their predation on game and farm animals, and that persecution has led to the extinction of at least one species, the Guadalupe caracara. Several insular species have declined dramatically, none more so than the Mauritius kestrel, which at one time numbered no more than four birds. Around five species of falcon are considered vulnerable to extinction by the IUCN, including the saker falcon.
Taxonomy and systematics.
Families.
Traditionally, the raptors were grouped into four families in the single order Falconiformes, but many thought this group to be paraphyletic and not to share a common ancestor to the exclusion of all other birds.
First, multiple lines of evidence in the 1970s and 1980s suggested that the New World vultures Cathartidae were closer related to storks and herons (Ciconiiformes), though more recent place them outside that group as well. Consequently, New World vultures are now often raised to the rank of an independent order Cathartiformes not closely associated with either birds of prey or storks or herons. In 2007, the American Ornithologists' Union's North American checklist moved Cathartidae back into the lead position in Falconiformes, but with an asterisk that indicates it is a taxon "that is probably misplaced in the current phylogenetic listing but for which data indicating proper placement are not yet available".
In Europe, it has become common to split the remaining raptors into two: the falcons and caracaras remain in the order Falconiformes (about 60 species in 4 groups), and the remaining 220-odd species (including the Accipitridae eagles, hawks, Old World vultures, etc.) are put in the separate order Accipitriformes. An unplaced prehistoric family known only from fossils are the Horusornithidae.
In agreement with the split of Falconiformes and Accipitriformes, comparative genome analysis published in 2008 suggested that falcons are more closely related to the parrots and passerines than to other birds including the Accipitridae, so that the traditional Falconiformes are paraphyletic even if the Cathartidae are excluded. Indeed, a 2011 analysis of transposable element insertions shared between the genomes of falcons, passerines, and parrots, but not present in the genomes of other birds, confirmed that falcons are a sister group of the combined parrot/passerine group, together forming the clade Eufalconimorphae.
Subfamilies.
The clade Falconidae is composed of three main branches: the falconets and true falcons, the caracaras, and the forest falcons. Differences exist between authorities in how these are grouped into subfamilies. Also, the placement of the laughing falcon ("Herpetotheres") and the spot-winged falconet ("Spiziapteryx") varies. 
One common approach uses two subfamilies Polyborinae and Falconinae.
The first contains the caracaras, forest falcons, and laughing falcon. All species in this group are native to the Americas. 
The composition of Falconidae is disputed, and Polyborninae is not featured in the American Ornithologists' Union checklists for North and South American birds that are produced by its Classification Committees (NACC and SACC). The Check-list of North American Birds considers the laughing falcon a true falcon (Falconinae) and replaces Polyborinae with Caracarinae and Micrasturinae. On the other hand, the Check-list of South American Birds classifies all caracaras as true falcons and puts the laughing falcon and forest falcons into the subfamily Herpetotherinae.
Based on genetic research from the late 1990s to 2015, Boyd uses three subfamilies. 
He places the laughing falcon ("Herpetotheres") with the forest falcons ("Micrastur") into Herpetotherinae (similar to SACC). Caracarinae is separate (similar to NACC), but also contains the spot-winged falconet ("Spiziapteryx"). The other falcons are placed in Falconinae. 
Falconinae, in its traditional classification, contains the falcons, falconets, and pygmy falcons. Depending on the authority, Falconinae may also include the caracaras and/or the laughing falcon. Boyd further divides the Falconinae into two tribes: Polyhieracini containing the "Microhierax" falconets, plus Falconini containing the "Falco" falcons. The pygmy falcon and the white-rumped (pygmy) falcon are split into separate genera ("Polyhierax" and "Neohierax"), with the former placed into Polyhieracini and the latter into Falconini.
Genera in taxonomic order.
Family: Falconidae

</doc>
<doc id="59941" url="https://en.wikipedia.org/wiki?curid=59941" title="Nyaya">
Nyaya

Nyaya school's epistemology accepts four out of six "Pramanas" as reliable means of gaining knowledge – "Pratyakṣa" (perception), "Anumāṇa" (inference), "Upamāṇa" (comparison and analogy) and "Śabda" (word, testimony of past or present reliable experts).
In its metaphysics, Nyaya school is closer to Vaisheshika school of Hinduism than others. It holds that human suffering results from mistakes/defects produced by activity under wrong knowledge (notions and ignorance). Moksha (liberation), it states, is gained through right knowledge. This premise led Nyaya to concern itself with epistemology, that is the reliable means to gain correct knowledge and to remove wrong notions. False knowledge is not merely ignorance to Naiyyayikas, it includes delusion. Correct knowledge is discovering and overcoming one's delusions, and understanding true nature of soul, self and reality.
Naiyyayika scholars approached philosophy as a form of direct realism, stating that anything that really exists is in principle humanly knowable. To them, correct knowledge and understanding is different than simple, reflexive cognition; it requires "Anuvyavasaya" (अनुव्यवसाय, cross-examination of cognition, reflective cognition of what one thinks one knows). An influential collection of texts on logic and reason is the Nyayasutras, written by Aksapada Gautama about 2nd century CE.
Nyaya school shares some of its methodology and human suffering foundations with Buddhism; however, a key difference between the two is that Buddhism believes that there is neither a soul nor self;Steven Collins (1994), Religion and Practical Reason (Editors: Frank Reynolds, David Tracy), State Univ of New York Press, ISBN 978-0791422175, page 64; Quote: "Central to Buddhist soteriology is the doctrine of not-self (Pali: anattā, Sanskrit: anātman, the opposed doctrine of ātman is central to Brahmanical thought). Put very briefly, this is the doctrine that human beings have no soul, no self, no unchanging essence.";Edward Roer (Translator), , pages 2-4Katie Javanaud (2013), [https://philosophynow.org/issues/97/Is_The_Buddhist_No-Self_Doctrine_Compatible_With_Pursuing_Nirvana Is The Buddhist 'No-Self' Doctrine Compatible With Pursuing Nirvana?, Philosophy Now;John C. Plott et al (2000), Global History of Philosophy: The Axial Age, Volume 1, Motilal Banarsidass, ISBN 978-8120801585, page 63, Quote: "The Buddhist schools reject any Ātman concept. As we have already observed, this is the basic and ineradicable distinction between Hinduism and Buddhism".</ref> Nyaya school like other schools of Hinduism believes that there is a soul and self, with liberation (moksha) as a state of removal of ignorance, wrong knowledge, the gain of correct knowledge and unimpeded continuation of self.
Etymology.
"Nyaya" (न्याय) is a Sanskrit word which means method, rule, specially a collection of general or universal rules. In some contexts, it means model, axiom, plan, legal proceeding, judicial sentence, or judgment. In the theory of logic, and Indian texts discussing it, the term also refers to an argument consisting of an enthymeme or sometimes for any syllogism. In philosophical context, "Nyaya" encompasses propriety, logic and method.
"Nyaya" is related to several other concepts and words used in Indian philosophies: "Hetu-vidya" (science of causes), "Anviksiki" (science of inquiry, systematic philosophy), "Pramana-sastra" (epistemology, science of correct knowledge), "Tattva-sastra" (science of categories), "Tarka-vidya" (science of reasoning, innovation, synthesis), "Vadartha" (science of discussion) and "Phakkika-sastra" (science of uncovering sophism, fraud, error, finding fakes). Some of these subsume or deploy the tools of "Nyaya".
Overview.
The historical development of Nyaya school is unclear, although "Nasadiya" hymns of Book 10 Chapter 129 of Rigveda recite its spiritual questions in logical propositions. In early centuries BCE, states Clooney, the early Nyaya scholars began compiling the science of rational, coherent inquiry and pursuit of knowledge. By 2nd century CE, Aksapada Gautama had composed "Nyayasutras", a foundational text for Nyaya school, that primarily discusses logic, methodology and epistemology. The Nyaya scholars that followed refined it, expanded it, and applied it to spiritual questions. While the early Nyaya scholars published little to no analysis on whether supernatural power or God exists, they did apply their insights into reason and reliable means to knowledge to the questions of nature of existence, spirituality, happiness and moksha. Later Nyaya scholars, such as Udayana, examined various arguments on theism and attempted to prove existence of God. Other Nyaya scholars offered arguments to disprove the existence of God.
The most important contribution made by the Nyaya school to Hindu thought has been its treatises on epistemology and system of logic that, subsequently, has been adopted by the majority of the other Indian schools.
Sixteen "Padārthas or Categories.
The Nyaya metaphysics recognizes sixteen "padartha"s or categories and includes all six (or seven) categories of the Vaisheshika in the second one of them, called "prameya". These sixteen categories are "pramāṇa" (valid means of knowledge), "prameya" (objects of valid knowledge), "saṁśaya" (doubt), "prayojana" (aim), "dṛṣṭānta" (example), "siddhānta" (conclusion), "avayava" (members of syllogism), "tarka" (hypothetical reasoning), "nirṇaya" (settlement), "vāda" (discussion), "jalpa" (wrangling), "vitaṇḍā" (cavilling), "hetvābhāsa" (fallacy), "chala" (quibbling), "jāti" (sophisticated refutation) and "nigrahasthāna" (point of defeat).
Epistemology.
The Nyaya school of Hinduism developed and refined many treatises on epistemology that widely influenced other schools of Hinduism. Nyaya treated it as theory of knowledge, and its scholars developed it as "Pramana-sastras". "Pramana", a Sanskrit word, literally is "means of knowledge". It encompasses one or more reliable and valid means by which human beings gain accurate, true knowledge. The focus of Pramana is how correct knowledge can be acquired, how one knows, how one doesn't, and to what extent knowledge pertinent about someone or something can be acquired.
The Naiyayikas (the Nyaya scholars) accepted four valid means ("pramaṇa") of obtaining valid knowledge ("pramana") - perception ("pratyakṣa"), inference ("anumāna"), comparison ("upamāna") and word/testimony of reliable sources ("śabda"). The Nyaya scholars, along with those from other schools of Hinduism, also developed a theory of error, to methodically establish means to identify errors and the process by which errors are made in human pursuit of knowledge. These include "Saṁśaya" (समस्या, problems, inconsistencies, doubts) and "Viparyaya" (विपर्यय, contrariness, errors) which can be corrected or resolved by a systematic process of "Tarka" ( तर्क, reasoning, technique).
Perception.
"Pratyakṣa" (perception) occupies the foremost position in the Nyaya epistemology. Perception can be of two types, "laukika" (ordinary) and "alaukika" (extraordinary). Ordinary perception is defined by Akṣapāda Gautama in his "Nyaya Sutra" (I,i.4) as a 'non-erroneous cognition which is produced by the intercourse of sense-organs with the objects'.
Indian texts identify four requirements for correct perception: "Indriyarthasannikarsa" (direct experience by one's sensory organ(s) with the object, whatever is being studied), "Avyapadesya" (non-verbal; correct perception is not through hearsay, according to ancient Indian scholars, where one's sensory organ relies on accepting or rejecting someone else's perception), "Avyabhicara" (does not wander; correct perception does not change, nor is it the result of deception because one's sensory organ or means of observation is drifting, defective, suspect) and "Vyavasayatmaka" (definite; correct perception excludes judgments of doubt, either because of one's failure to observe all the details, or because one is mixing inference with observation and observing what one wants to observe, or not observing what one does not want to observe).
Ordinary perception to Nyaya scholars was based on direct experience of reality by eyes, ears, nose, touch and taste. Extraordinary perception included "yogaja" or "pratibha" (intuition), "samanyalaksanapratyaksa" (a form of induction from perceived specifics to a universal), and "jnanalaksanapratyaksa" (a form of perception of prior processes and previous states of a 'topic of study' by observing its current state).
Determinate and indeterminate perception.
The Naiyyayika maintains two modes or stages in perception. The first is called "nirvikalpa" (indeterminate), when one just perceives an object without being able to know its features, and the second "savikalpa" (determinate), when one is able to clearly know an object. All laukika and alaukika pratyakshas are "savikalpa", but it is necessarily preceded by an earlier stage when it is indeterminate. Vātsāyana says that if an object is perceived with its name we have determinate perception but if it is perceived without a name, we have indeterminate perception. Jayanta Bhatta says that indeterminate perception apprehends substance, qualities and actions and universals as separate and indistinct something and also it does not have any association with name, while determinate perception aprrehends all these together with a name. There is yet another stage called "Pratyabhijñā", when one is able to re-recognise something on the basis of memory.
Inference.
"Anumāna" (inference) is one of the most important contributions of the Nyaya. It can be of two types: inference for oneself ("Svarthanumana", where one does not need any formal procedure, and at the most the last three of their 5 steps), and inference for others ("Parathanumana", which requires a systematic methodology of 5 steps). Inference can also be classified into 3 types: "Purvavat" (inferring an unperceived effect from a perceived cause), "Sheshavat" (inferring an unperceived cause from a perceived effect) and "Samanyatodrishta" (when inference is not based on causation but on uniformity of co-existence). A detailed anaysis of error is also given, explaining when anumana could be false.
Theory of inference.
The methodology of inference involves a combination of induction and deduction by moving from particular to particular via generality. It has five steps, as in the example shown:
In Nyāya terminology for this example, the hill would be called as "paksha" (minor term), the fire is called as "sādhya" (major term), the smoke is called as "hetu", and the relationship between the smoke and the fire is called as "vyapti" (middle term). Hetu further has five characteristics: (1) It must be present in the Paksha, (2) It must be present in all positive instances, (3) It must be absent in all negative instances, (4) It must not incompatible with the minor term or Paksha and (5) All other contradictions by other means of knowledge should be absent. The fallacies in Anumana ("hetvābhasa") may occur due to the following:
Comparison, analogy.
"Upamāna" (उपमान) means comparison and analogy. "Upamana", states Lochtefeld, may be explained with the example of a traveller who has never visited lands or islands with endemic population of wildlife. He or she is told, by someone who has been there, that in those lands you see an animal that sort of looks like a cow, grazes like cow but is different from a cow in such and such way. Such use of analogy and comparison is, state the Indian epistemologists, a valid means of conditional knowledge, as it helps the traveller identify the new animal later. The subject of comparison is formally called "upameyam", the object of comparison is called "upamanam", while the attribute(s) are identified as "samanya". Thus, explains Monier Williams, if a boy says "her face is like the moon in charmingness", "her face" is "upameyam", the moon is "upamanam", and charmingness is "samanya". The 7th century text Bhaṭṭikāvya in verses 10.28 through 10.63 discusses many types of comparisons and analogies, identifying when this epistemic method is more useful and reliable, and when it is not. In various ancient and medieval texts of Hinduism, 32 types of "Upanama" and their value in epistemology are debated.
Word, testimony.
"Śabda" (शब्द) means relying on word, testimony of past or present reliable experts. Hiriyanna explains "Sabda-pramana" as a concept which means testimony of a reliable and trustworthy person ("āptavākya"). The schools of Hinduism which consider it epistemically valid suggest that a human being needs to know numerous facts, and with the limited time and energy available, he can learn only a fraction of those facts and truths directly. He must rely on others, his parent, family, friends, teachers, ancestors and kindred members of society to rapidly acquire and share knowledge and thereby enrich each other's lives. This means of gaining proper knowledge is either spoken or written, but through "Sabda" (words). The reliability of the source is important, and legitimate knowledge can only come from the "Sabda" of reliable sources. The disagreement between the schools of Hinduism has been on how to establish reliability. Some schools, such as Carvaka, state that this is never possible, and therefore "Sabda" is not a proper pramana. Other schools debate means to establish reliability.
Testimony can be of two types, "Vaidika" (Vedic), which are the words of the four sacred Vedas, and "Laukika", or words and writings of trustworthy human beings. "Vaidika" testimony is preferred over "Laukika" testimony. Laukika-sourced knowledge must be questioned and revised as more trustworthy knowledge becomes available.
Comparison with other schools of Hinduism.
Each school of Hinduism has its own treatises on epistemology, with different number of "Pramanas". For example, compared to Nyaya school's four "pramanas", Carvaka school has just one (perception), while Advaita Vedanta school recognizes six means to reliable knowledge.
The Nyaya theory of causation.
A "cause" is defined as an unconditional and invariable antecedent of an "effect" and an effect as an unconditional and invariable consequent of a cause. The same cause produces the same effect; and the same effect is produced by the same cause. The cause is "not" present in any hidden form whatsoever in its effect.
The following conditions should be met:
Nyaya recognizes five kinds of accidental antecedents 
Nyaya recognizes three kinds of cause:
Anyathakyativada of Nyaya.
The Nyaya theory of error is similar to that of Kumarila's Viparita-khyati (see Mimamsa). The Naiyyayikas also believe like Kumarila that error is due to a wrong synthesis of the presented and the represented objects. The represented object is confused with the presented one. The word 'anyatha' means 'elsewise' and 'elsewhere' and both these meanings are brought out in error. The presented object is perceived elsewise and the represented object exists elsewhere. They further maintain that knowledge is not intrinsically valid but becomes so on account of extraneous conditions ("paratah pramana" during both validity and invalidity).
Nyaya on God and salvation.
Early Naiyyayikas wrote very little about Ishvara (literally, the Supreme Soul). Evidence available so far suggests that early Nyaya scholars were non-theistic or atheists. Later, and over time, Nyaya scholars tried to apply some of their epistemological insights and methodology to the question: does God exist? Some offered arguments against and some in favor.
Arguments that God does not exist.
In Nyayasutra's Book 4, Chapter 1, verses 19-21, postulates God exists, states a consequence, then presents contrary evidence, and from contradiction concludes that the postulate must be invalid.
A literal interpretation of the three verses suggests that Nyaya school rejected the need for a God for the efficacy of human activity. Since human action and results do not require assumption or need of the existence of God, sutra IV.1.21 is seen as a criticism of the "existence of God and theism postulate". The context of the above verses includes various efficient causes. Nyayasutra verses IV.1.22 to IV.1.24, for example, examine the hypothesis that "random chance" explains the world, after these Indian scholars had rejected God as the efficient cause.
Arguments that God exists.
Udayana's "Nyayakusumanjali" gave the following nine arguments to prove the existence of creative God:
Salvation.
The Naiyyayikas believe that the bondage of the world is due to false knowledge, which can be removed by constantly thinking of its opposite ("pratipakshabhavana"), namely, the true knowledge. So the opening aphorism of the "" states that only the true knowledge lead to "niḥśreyasa" (Salvation). But the Nyaya school also maintains that the God's grace is essential for obtaining true knowledge. Jayanta, in his "Nyayamanjari" describes salvation as a passive stage of self in its natural purity, unassociated with pleasure, pain, knowledge and willingness.
Literature of Nyaya.
The earliest text of the Nyāya School is the ' of Akṣapāda Gautama. The text is divided into five books, each having two sections. Vātsāyana's ' is a classic commentary on the '. Udyotakara's ' (6th century CE) is written to defend against the attacks made by . Vacaspati Misra's ' (9th century CE) is the next major exposition of this school. Two other texts, ' and ' are also attributed to him. Udayana's (984 CE) ' is an important commentary on 's treatise. His ' is the first systematic account of theistic '. His other works include ', ' and '. Jayanta Bhatta's ' (10th century CE) is basically an independent work. 's ' (10th century CE) is a survey of ' philosophy.
The later works on ' accepted the ' categories and 's ' (12th century CE) is a notable treatise of this syncretist school. 's ' (13th century CE) is another important work of this school.
's ' (12th century CE) is the first major treatise of the new school of '. His son, 's ' (1225 CE), though a commentary on Udayana's ', incorporated his father's views. Jayadeva wrote a commentary on ' known as ' (13th century CE). 's ' (16th century CE) is first great work of Navadvipa school of . Raghunatha Siromani's ' and ' are the next important works of this school. Visvanatha Panchanana Bhattacharya's ' (17th century CE) is also a notable work. The Commentaries on "" by Jagadish Tarkalankar (17th century CE) and Gadadhar Bhattacharya (17th century CE) are the last two notable works of this school.
Annambhatta (17th century CE) tried to develop a consistent system by combining the ancient and the new schools, ' and ' and ' to develop the ' school. His ' and ' are the popular manuals of this school.
Further reading.
Navya-nyaya school

</doc>
<doc id="59944" url="https://en.wikipedia.org/wiki?curid=59944" title="Program counter">
Program counter

The program counter (PC), commonly called the instruction pointer (IP) in Intel x86 and Itanium microprocessors, and sometimes called the instruction address register (IAR), the instruction counter, or just part of the instruction sequencer, is a processor register that indicates where a computer is in its program sequence.
In most processors, the PC is incremented after fetching an instruction, and holds the memory address of ("points to") the next instruction that would be executed. (In a processor where the incrementation precedes the fetch, the PC points to the current instruction being executed.)
Instructions are usually fetched sequentially from memory, but control transfer instructions change the sequence by placing a new value in the PC. These include branches (sometimes called jumps), subroutine calls, and returns. A transfer that is conditional on the truth of some assertion lets the computer follow a different sequence under different conditions.
A branch provides that the next instruction is fetched from somewhere else in memory. A subroutine call not only branches but saves the preceding contents of the PC somewhere. A return retrieves the saved contents of the PC and places it back in the PC, resuming sequential execution with the instruction following the subroutine call.
Hardware implementation.
In a typical central processing unit (CPU), the PC is a digital counter (which is the origin of the term "program counter") that may be one of many registers in the CPU hardware. The instruction cycle begins with a fetch, in which the CPU places the value of the PC on the address bus to send it to the memory. The memory responds by sending the contents of that memory location on the data bus. (This is the stored-program computer model, in which executable instructions are stored alongside ordinary data in memory, and handled identically by it). Following the fetch, the CPU proceeds to execution, taking some action based on the memory contents that it obtained. At some point in this cycle, the PC will be modified so that the next instruction executed is a different one (typically, incremented so that the next instruction is the one starting at the memory address immediately following the last memory location of the current instruction).
Like other processor registers, the PC may be a bank of binary latches, each one representing one bit of the value of the PC. The number of bits (the width of the PC) relates to the processor architecture. For instance, a “32-bit” CPU may use 32 bits to be able to address 232 units of memory. If the PC is a binary counter, it may increment when a pulse is applied to its COUNT UP input, or the CPU may compute some other value and load it into the PC by a pulse to its LOAD input.
To identify the current instruction, the PC may be combined with other registers that identify a segment or page. This approach permits a PC with fewer bits by assuming that most memory units of interest are within the current vicinity.
Consequences in machine architecture.
Use of a PC that normally increments assumes that what a computer does is execute a usually linear sequence of instructions. Such a PC is central to the von Neumann architecture. Thus programmers write a sequential control flow even for algorithms that do not have to be sequential. The resulting “von Neumann bottleneck” led to research into parallel computing, including non-von Neumann or dataflow models that did not use a PC; for example, rather than specifying sequential steps, the high-level programmer might specify desired function and the low-level programmer might specify this using combinatory logic.
This research also led to ways to making conventional, PC-based, CPUs run faster, including:
Consequences in high-level programming.
Modern high-level programming languages still follow the sequential-execution model and, indeed, a common way of identifying programming errors is with a “procedure execution” in which the programmer's finger identifies the point of execution as a PC would. The high-level language is essentially the machine language of a virtual machine, too complex to be built as hardware but instead emulated or interpreted by software.
However, new programming models transcend sequential-execution programming:

</doc>
<doc id="59945" url="https://en.wikipedia.org/wiki?curid=59945" title="History of logic">
History of logic

The history of logic is the study of the development of the science of valid inference (logic). Formal logics were developed in ancient times in China, India, and Greece. Greek methods, particularly Aristotelian logic (or term logic) as found in the "Organon", found wide application and acceptance in science and mathematics for millennia. The Stoics, especially Chrysippus, were the first to develop predicate logic.
Aristotle's logic was further developed by Christian and Islamic philosophers in the Middle Ages, such as Boethius or William of Ockham, reaching a high point in the mid-fourteenth century. The period between the fourteenth century and the beginning of the nineteenth century was largely one of decline and neglect, and is regarded as barren by at least one historian of logic. Empirical methods seemed to rule the day, as evidenced by Bacon's "Novum Organon".
Logic was revived in the mid-nineteenth century, at the beginning of a revolutionary period when the subject developed into a rigorous and formal discipline whose exemplar was the exact method of proof used in mathematics, a hearkening back to the Greek tradition. The development of the modern "symbolic" or "mathematical" logic during this period by the likes of Boole, Frege, Russell, and Peano is the most significant in the two-thousand-year history of logic, and is arguably one of the most important and remarkable events in human intellectual history.
Progress in mathematical logic in the first few decades of the twentieth century, particularly arising from the work of Gödel and Tarski, had a significant impact on analytic philosophy and philosophical logic, particularly from the 1950s onwards, in subjects such as modal logic, temporal logic, deontic logic, and relevance logic.
Logic in the West.
Prehistory of logic.
Valid reasoning has been employed in all periods of human history. However, logic studies the "principles" of valid reasoning, inference and demonstration. It is probable that the idea of demonstrating a conclusion first arose in connection with geometry, which originally meant the same as "land measurement". The ancient Egyptians discovered geometry, including the formula for the volume of a truncated pyramid. Ancient Babylon was also skilled in mathematics. Esagil-kin-apli's medical "Diagnostic Handbook" in the 11th century BC was based on a logical set of axioms and assumptions, while Babylonian astronomers in the 8th and 7th centuries BC employed an internal logic within their predictive planetary systems, an important contribution to the philosophy of science.
Ancient Greece before Aristotle.
While the ancient Egyptians empirically discovered some truths of geometry, the great achievement of the ancient Greeks was to replace empirical methods by demonstrative proof. Both Thales and Pythagoras of the Pre-Socratic philosophers seem aware of geometry's methods.
Fragments of early proofs are preserved in the works of Plato and Aristotle, and the idea of a deductive system was probably known in the Pythagorean school and the Platonic Academy. The proofs of Euclid of Alexandria are a paradigm of Greek geometry. The three basic principles of geometry are as follows:
Further evidence that early Greek thinkers were concerned with the principles of reasoning is found in the fragment called "dissoi logoi", probably written at the beginning of the fourth century BC. This is part of a protracted debate about truth and falsity. In the case of the classical Greek city-states, interest in argumentation was also stimulated by the activities of the Rhetoricians or Orators and the Sophists, who used arguments to defend or attack a thesis, both in legal and political contexts.
Thales.
It is said Thales, most widely regarded as the first philosopher in the Greek tradition, measured the height of the pyramids by their shadows at the moment when his own shadow was equal to his height. Thales was said to have had a sacrifice in celebration of discovering Thales' Theorem just as Pythagoras had the Pythagorean Theorem.
Thales is the first known individual to use deductive reasoning applied to geometry, by deriving four corollaries to his theorem, and the first known individual to whom a mathematical discovery has been attributed. Indian and Babylonian mathematicians knew his theorem for special cases before he proved it. It is believed that Thales learned that an angle inscribed in a semicircle is a right angle during his travels to Babylon.
Pythagoras.
Before 520 BC, on one of his visits to Egypt or Greece, Pythagoras might have met the c. 54 years older Thales. The systematic study of proof seems to have begun with the school of Pythagoras (i. e. the Pythagoreans) in the late sixth century BC. Indeed, the Pythagoreans, believing all was number, are the first philosophers to emphasize "form" rather than "matter".
Heraclitus and Parmenides.
The writing of Heraclitus (c. 535 – c. 475 BC) was the first place where the word "logos" was given special attention in ancient Greek philosophy, Heraclitus held that everything changes and all was fire and conflicting opposites, seemingly unified only by this "Logos". He is known for his obscure sayings.
In contrast to Heraclitus, Parmenides held that all is one and nothing changes. He may have been a dissident Pythagorean, disagreeing that One (a number) produced the many. "X is not" must always be false or meaningless. What exists can in no way not exist. Our sense perceptions with its noticing of generation and destruction are in grievous error. Instead of sense perception, Parmenides advocated "logos" as the means to Truth. He has been called the discoverer of logic,
Zeno of Elea, a pupil of Parmenides, had the idea of a standard argument pattern found in the method of proof known as "reductio ad absurdum". This is the technique of drawing an obviously false (that is, "absurd") conclusion from an assumption, thus demonstrating that the assumption is false. Therefore Zeno and his teacher are seen as the first to apply the art of logic. Plato's dialogue Parmenides portrays Zeno as claiming to have written a book defending the monism of Parmenides by demonstrating the absurd consequence of assuming that there is plurality. Zeno famously used this method to develop his paradoxes in his arguments against motion. Such "dialectic" reasoning later became popular. The members of this school were called "dialecticians" (from a Greek word meaning "to discuss").
Plato.
None of the surviving works of the great fourth-century philosopher Plato (428–347 BC) include any formal logic, but they include important contributions to the field of philosophical logic. Plato raises three questions:
The first question arises in the dialogue "Theaetetus", where Plato identifies thought or opinion with talk or discourse ("logos"). The second question is a result of Plato's theory of Forms. Forms are not things in the ordinary sense, nor strictly ideas in the mind, but they correspond to what philosophers later called universals, namely an abstract entity common to each set of things that have the same name. In both the "Republic" and the "Sophist", Plato suggests that the necessary connection between the assumptions of a valid argument and its conclusion corresponds to a necessary connection between "forms". The third question is about definition. Many of Plato's dialogues concern the search for a definition of some important concept (justice, truth, the Good), and it is likely that Plato was impressed by the importance of definition in mathematics. What underlies every definition is a Platonic Form, the common nature present in different particular things. Thus, a definition reflects the ultimate object of understanding, and is the foundation of all valid inference. This had a great influence on Plato's student Aristotle, in particular Aristotle's notion of the essence of a thing.
Aristotle.
The logic of Aristotle, and particularly his theory of the syllogism, has had an enormous influence in Western thought. His logical works, called the "Organon", are the earliest formal study of logic that have come down to modern times. Though it is difficult to determine the dates, the probable order of writing of Aristotle's logical works is:
These works are of outstanding importance in the history of logic. Aristotle was the first logician to attempt a systematic analysis of logical syntax, of noun (or "term"), and of verb. He was the first "formal logician", in that he demonstrated the principles of reasoning by employing variables to show the underlying logical form of an argument. He was looking for relations of dependence which characterize necessary inference, and distinguished the validity of these relations, from the truth of the premises (the soundness of the argument). He was the first to deal with the principles of contradiction and excluded middle in a systematic way.
In the "Categories", he attempts to discern all the possible things to which a term can refer; this idea underpins his philosophical work "Metaphysics", which itself had a profound influence on Western thought. The "Prior Analytics" contains his exposition of the "syllogism", where three important principles are applied for the first time in history: the use of variables, a purely formal treatment, and the use of an axiomatic system. He also developed a theory of non-formal logic ("i.e.," the theory of fallacies), which is presented in "Topics" and "Sophistical Refutations".
Stoics.
The other great school of Greek logic is that of the Stoics. Stoic logic traces its roots back to the late 5th century BC philosopher Euclid of Megara, a pupil of Socrates and slightly older contemporary of Plato, probably following in the tradition of Parmenides and Zeno. His pupils and successors were called "Megarians", or "Eristics", and later the "Dialecticians". The two most important dialecticians of the Megarian school were Diodorus Cronus and Philo, who were active in the late 4th century BC. The Stoics adopted the Megarian logic and systemized it. The most important member of the school was Chrysippus (c. 278–c. 206 BC), who was its third head, and who formalized much of Stoic doctrine. He is supposed to have written over 700 works, including at least 300 on logic, almost none of which survive. Unlike with Aristotle, we have no complete works by the Megarians or the early Stoics, and have to rely mostly on accounts (sometimes hostile) by later sources, including prominently Diogenes Laertius, Sextus Empiricus, Galen, Aulus Gellius, Alexander of Aphrodisias, and Cicero.
Three significant contributions of the Stoic school were (i) their account of modality, (ii) their theory of the Material conditional, and (iii) their account of meaning and truth.
Medieval logic.
Logic in the Middle East.
The works of Al-Kindi, Al-Farabi, Avicenna, Al-Ghazali, Averroes and other Muslim logicians were based on Aristotelian logic and were important in communicating the ideas of the ancient world to the medieval West. Al-Farabi (Alfarabi) (873–950) was an Aristotelian logician who discussed the topics of future contingents, the number and relation of the categories, the relation between logic and grammar, and non-Aristotelian forms of inference. Al-Farabi also considered the theories of conditional syllogisms and analogical inference, which were part of the Stoic tradition of logic rather than the Aristotelian.
Ibn Sina (Avicenna) (980–1037) was the founder of Avicennian logic, which replaced Aristotelian logic as the dominant system of logic in the Islamic world, and also had an important influence on Western medieval writers such as Albertus Magnus. and on the propositional calculus, which were both part of the Stoic logical tradition. He developed an original "temporally modalized" syllogistic theory, involving temporal logic and modal logic. He also made use of inductive logic, such as the methods of agreement, difference, and concomitant variation which are critical to the scientific method. One of Avicenna's ideas had a particularly important influence on Western logicians such as William of Ockham: Avicenna's word for a meaning or notion ("ma'na"), was translated by the scholastic logicians as the Latin "intentio"; in medieval logic and epistemology, this is a sign in the mind that naturally represents a thing. This was crucial to the development of Ockham's conceptualism: A universal term ("e.g.," "man") does not signify a thing existing in reality, but rather a sign in the mind ("intentio in intellectu") which represents many things in reality; Ockham cites Avicenna's commentary on "Metaphysics" V in support of this view.
Fakhr al-Din al-Razi (b. 1149) criticised Aristotle's "first figure" and formulated an early system of inductive logic, foreshadowing the system of inductive logic developed by John Stuart Mill (1806–1873). Al-Razi's work was seen by later Islamic scholars as marking a new direction for Islamic logic, towards a Post-Avicennian logic. This was further elaborated by his student Afdaladdîn al-Khûnajî (d. 1249), who developed a form of logic revolving around the subject matter of conceptions and assents. In response to this tradition, Nasir al-Din al-Tusi (1201–1274) began a tradition of Neo-Avicennian logic which remained faithful to Avicenna's work and existed as an alternative to the more dominant Post-Avicennian school over the following centuries.
The Illuminationist school was founded by Shahab al-Din Suhrawardi (1155–1191), who developed the idea of "decisive necessity", which refers to the reduction of all modalities (necessity, possibility, contingency and impossibility) to the single mode of necessity. Ibn al-Nafis (1213–1288) wrote a book on Avicennian logic, which was a commentary of Avicenna's "Al-Isharat" ("The Signs") and "Al-Hidayah" ("The Guidance"). Ibn Taymiyyah (1263–1328), wrote the "Ar-Radd 'ala al-Mantiqiyyin", where he argued against the usefulness, though not the validity, of the syllogism and in favour of inductive reasoning. Ibn Taymiyyah also argued against the certainty of syllogistic arguments and in favour of analogy; his argument is that concepts founded on induction are themselves not certain but only probable, and thus a syllogism based on such concepts is no more certain than an argument based on analogy. He further claimed that induction itself is founded on a process of analogy. His model of analogical reasoning was based on that of juridical arguments. This model of analogy has been used in the recent work of John F. Sowa.
The "Sharh al-takmil fi'l-mantiq" written by Muhammad ibn Fayd Allah ibn Muhammad Amin al-Sharwani in the 15th century is the last major Arabic work on logic that has been studied. However, "thousands upon thousands of pages" on logic were written between the 14th and 19th centuries, though only a fraction of the texts written during this period have been studied by historians, hence little is known about the original work on Islamic logic produced during this later period.
Logic in medieval Europe.
"Medieval logic" (also known as "Scholastic logic") generally means the form of Aristotelian logic developed in medieval Europe throughout roughly the period 1200–1600. For centuries after Stoic logic had been formulated, it was the dominant system of logic in the classical world. When the study of logic resumed after the Dark Ages, the main source was the work of the Christian philosopher Boethius, who was familiar with some of Aristotle's logic, but almost none of the work of the Stoics. Until the twelfth century, the only works of Aristotle available in the West were the "Categories", "On Interpretation", and Boethius's translation of the Isagoge of Porphyry (a commentary on the Categories). These works were known as the "Old Logic" ("Logica Vetus" or "Ars Vetus"). An important work in this tradition was the "Logica Ingredientibus" of Peter Abelard (1079–1142). His direct influence was small, but his influence through pupils such as John of Salisbury was great, and his method of applying rigorous logical analysis to theology shaped the way that theological criticism developed in the period that followed.
By the early thirteenth century, the remaining works of Aristotle's "Organon" (including the "Prior Analytics", "Posterior Analytics", and the "Sophistical Refutations") had been recovered in the West. Logical work until then was mostly paraphrasis or commentary on the work of Aristotle. The period from the middle of the thirteenth to the middle of the fourteenth century was one of significant developments in logic, particularly in three areas which were original, with little foundation in the Aristotelian tradition that came before. These were:
The last great works in this tradition are the "Logic" of John Poinsot (1589–1644, known as John of St Thomas), the "Metaphysical Disputations" of Francisco Suarez (1548–1617), and the "Logica Demonstrativa" of Giovanni Girolamo Saccheri (1667–1733).
Traditional logic.
The textbook tradition.
"Traditional logic" generally means the textbook tradition that begins with Antoine Arnauld's and Pierre Nicole's "Logic, or the Art of Thinking", better known as the "Port-Royal Logic". Published in 1662, it was the most influential work on logic after Aristotle until the nineteenth century. The book presents a loosely Cartesian doctrine (that the proposition is a combining of ideas rather than terms, for example) within a framework that is broadly derived from Aristotelian and medieval term logic. Between 1664 and 1700, there were eight editions, and the book had considerable influence after that. The Port-Royal introduces the concepts of extension and intension. The account of propositions that Locke gives in the "Essay" is essentially that of the Port-Royal: "Verbal propositions, which are words, the signs of our ideas, put together or separated in affirmative or negative sentences. So that proposition consists in the putting together or separating these signs, according as the things which they stand for agree or disagree."
Dudley Fenner helped popularize Ramist logic, a reaction against Aristotle. Another influential work was the "Novum Organum" by Francis Bacon, published in 1620. The title translates as "new instrument". This is a reference to Aristotle's work known as the "Organon". In this work, Bacon rejects the syllogistic method of Aristotle in favor of an alternative procedure "which by slow and faithful toil gathers information from things and brings it into understanding". This method is known as inductive reasoning, a method which starts from empirical observation and proceeds to lower axioms or propositions; from these lower axioms, more general ones can be induced. For example, in finding the cause of a "phenomenal nature" such as heat, 3 lists should be constructed:
Then, the "form nature" (or cause) of heat may be defined as that which is common to every situation of the presence list, and which is lacking from every situation of the absence list, and which varies by degree in every situation of the variability list.
Other works in the textbook tradition include Isaac Watts's "Logick: Or, the Right Use of Reason" (1725), Richard Whately's "Logic" (1826), and John Stuart Mill's "A System of Logic" (1843). Although the latter was one of the last great works in the tradition, Mill's view that the foundations of logic lie in introspection influenced the view that logic is best understood as a branch of psychology, a view which dominated the next fifty years of its development, especially in Germany.
Logic in Hegel's philosophy.
G.W.F. Hegel indicated the importance of logic to his philosophical system when he condensed his extensive "Science of Logic" into a shorter work published in 1817 as the first volume of his "Encyclopaedia of the Philosophical Sciences." The "Shorter" or "Encyclopaedia" "Logic", as it is often known, lays out a series of transitions which leads from the most empty and abstract of categories—Hegel begins with "Pure Being" and "Pure Nothing"—to the "Absolute, the category which contains and resolves all the categories which preceded it. Despite the title, Hegel's "Logic" is not really a contribution to the science of valid inference. Rather than deriving conclusions about concepts through valid inference from premises, Hegel seeks to show that thinking about one concept compels thinking about another concept (one cannot, he argues, possess the concept of "Quality" without the concept of "Quantity"); this compulsion is, supposedly, not a matter of individual psychology, because it arises almost organically from the content of the concepts themselves. His purpose is to show the rational structure of the "Absolute"—indeed of rationality itself. The method by which thought is driven from one concept to its contrary, and then to further concepts, is known as the Hegelian dialectic.
Although Hegel's "Logic" has had little impact on mainstream logical studies, its influence can be seen elsewhere:
Logic and psychology.
Between the work of Mill and Frege stretched half a century during which logic was widely treated as a descriptive science, an empirical study of the structure of reasoning, and thus essentially as a branch of psychology. The German psychologist Wilhelm Wundt, for example, discussed deriving "the logical from the psychological laws of thought", emphasizing that "psychological thinking is always the more comprehensive form of thinking." This view was widespread among German philosophers of the period:
Such was the dominant view of logic in the years following Mill's work. This psychological approach to logic was rejected by Gottlob Frege. It was also subjected to an extended and destructive critique by Edmund Husserl in the first volume of his "Logical Investigations" (1900), an assault which has been described as "overwhelming". Husserl argued forcefully that grounding logic in psychological observations implied that all logical truths remained unproven, and that skepticism and relativism were unavoidable consequences.
Such criticisms did not immediately extirpate what is called "psychologism". For example, the American philosopher Josiah Royce, while acknowledging the force of Husserl's critique, remained "unable to doubt" that progress in psychology would be accompanied by progress in logic, and vice versa.
Rise of modern logic.
The period between the fourteenth century and the beginning of the nineteenth century had been largely one of decline and neglect, and is generally regarded as barren by historians of logic. The revival of logic occurred in the mid-nineteenth century, at the beginning of a revolutionary period where the subject developed into a rigorous and formalistic discipline whose exemplar was the exact method of proof used in mathematics. The development of the modern "symbolic" or "mathematical" logic during this period is the most significant in the 2000-year history of logic, and is arguably one of the most important and remarkable events in human intellectual history.
A number of features distinguish modern logic from the old Aristotelian or traditional logic, the most important of which are as follows: Modern logic is fundamentally a "calculus" whose rules of operation are determined only by the "shape" and not by the "meaning" of the symbols it employs, as in mathematics. Many logicians were impressed by the "success" of mathematics, in that there had been no prolonged dispute about any truly mathematical result. C.S. Peirce noted that even though a mistake in the evaluation of a definite integral by Laplace led to an error concerning the moon's orbit that persisted for nearly 50 years, the mistake, once spotted, was corrected without any serious dispute. Peirce contrasted this with the disputation and uncertainty surrounding traditional logic, and especially reasoning in metaphysics. He argued that a truly "exact" logic would depend upon mathematical, i.e., "diagrammatic" or "iconic" thought. "Those who follow such methods will ... escape all error except such as will be speedily corrected after it is once suspected". Modern logic is also "constructive" rather than "abstractive"; i.e., rather than abstracting and formalising theorems derived from ordinary language (or from psychological intuitions about validity), it constructs theorems by formal methods, then looks for an interpretation in ordinary language. It is entirely symbolic, meaning that even the logical constants (which the medieval logicians called "syncategoremata") and the categoric terms are expressed in symbols.
Modern logic.
The development of modern logic falls into roughly five periods:
Embryonic period.
The idea that inference could be represented by a purely mechanical process is found as early as Raymond Llull, who proposed a (somewhat eccentric) method of drawing conclusions by a system of concentric rings. The work of logicians such as the Oxford Calculators led to a method of using letters instead of writing out logical calculations ("calculationes") in words, a method used, for instance, in the "Logica magna" by Paul of Venice. Three hundred years after Llull, the English philosopher and logician Thomas Hobbes suggested that all logic and reasoning could be reduced to the mathematical operations of addition and subtraction. The same idea is found in the work of Leibniz, who had read both Llull and Hobbes, and who argued that logic can be represented through a combinatorial process or calculus. But, like Llull and Hobbes, he failed to develop a detailed or comprehensive system, and his work on this topic was not published until long after his death. Leibniz says that ordinary languages are subject to "countless ambiguities" and are unsuited for a calculus, whose task is to expose mistakes in inference arising from the forms and structures of words; hence, he proposed to identify an alphabet of human thought comprising fundamental concepts which could be composed to express complex ideas, and create a "calculus ratiocinator" that would make all arguments "as tangible as those of the Mathematicians, so that we can find our error at a glance, and when there are disputes among persons, we can simply say: Let us calculate."
Gergonne (1816) said that reasoning does not have to be about objects about which one has perfectly clear ideas, because algebraic operations can be carried out without having any idea of the meaning of the symbols involved. Bolzano anticipated a fundamental idea of modern proof theory when he defined logical consequence or "deducibility" in terms of variables:Hence I say that propositions formula_1, formula_2, formula_3,… are "deducible" from propositions formula_4, formula_5, formula_6, formula_7,… with respect to variable parts formula_8, formula_9,…, if every class of ideas whose substitution for formula_8, formula_9,… makes all of formula_4, formula_5, formula_6, formula_7,… true, also makes all of formula_1, formula_2, formula_3,… true. Occasionally, since it is customary, I shall say that propositions formula_1, formula_2, formula_3,… "follow", or can be "inferred" or "derived", from formula_4, formula_5, formula_6, formula_7,…. Propositions formula_4, formula_5, formula_6, formula_7,… I shall call the "premises", formula_1, formula_2, formula_3,… the "conclusions."This is now known as semantic validity.
Algebraic period.
Modern logic begins with what is known as the "algebraic school", originating with Boole and including Peirce, Jevons, Schröder, and Venn. Their objective was to develop a calculus to formalise reasoning in the area of classes, propositions, and probabilities. The school begins with Boole's seminal work "Mathematical Analysis of Logic" which appeared in 1847, although De Morgan (1847) is its immediate precursor. The fundamental idea of Boole's system is that algebraic formulae can be used to express logical relations. This idea occurred to Boole in his teenage years, working as an usher in a private school in Lincoln, Lincolnshire. For example, let x and y stand for classes let the symbol "=" signify that the classes have the same members, xy stand for the class containing all and only the members of x and y and so on. Boole calls these "elective symbols", i.e. symbols which select certain objects for consideration. An expression in which elective symbols are used is called an "elective function", and an equation of which the members are elective functions, is an "elective equation". The theory of elective functions and their "development" is essentially the modern idea of truth-functions and their expression in disjunctive normal form.
Boole's system admits of two interpretations, in class logic, and propositional logic. Boole distinguished between "primary propositions" which are the subject of syllogistic theory, and "secondary propositions", which are the subject of propositional logic, and showed how under different "interpretations" the same algebraic system could represent both. An example of a primary proposition is "All inhabitants are either Europeans or Asiatics." An example of a secondary proposition is "Either all inhabitants are Europeans or they are all Asiatics." These are easily distinguished in modern propositional calculus, where it is also possible to show that the first follows from the second, but it is a significant disadvantage that there is no way of representing this in the Boolean system.
In his "Symbolic Logic" (1881), John Venn used diagrams of overlapping areas to express Boolean relations between classes or truth-conditions of propositions. In 1869 Jevons realised that Boole's methods could be mechanised, and constructed a "logical machine" which he showed to the Royal Society the following year. In 1885 Allan Marquand proposed an electrical version of the machine that is still extant (picture at the Firestone Library).
The defects in Boole's system (such as the use of the letter "v" for existential propositions) were all remedied by his followers. Jevons published "Pure Logic, or the Logic of Quality apart from Quantity" in 1864, where he suggested a symbol to signify exclusive or, which allowed Boole's system to be greatly simplified. This was usefully exploited by Schröder when he set out theorems in parallel columns in his "Vorlesungen" (1890–1905). Peirce (1880) showed how all the Boolean elective functions could be expressed by the use of a single primitive binary operation, "neither ... nor ..." and equally well "not both ... and ...", however, like many of Peirce's innovations, this remained unknown or unnoticed until Sheffer rediscovered it in 1913. Boole's early work also lacks the idea of the logical sum which originates in Peirce (1867), Schröder (1877) and Jevons (1890), and the concept of inclusion, first suggested by Gergonne (1816) and clearly articulated by Peirce (1870).
The success of Boole's algebraic system suggested that all logic must be capable of algebraic representation, and there were attempts to express a logic of relations in such form, of which the most ambitious was Schröder's monumental "Vorlesungen über die Algebra der Logik" ("Lectures on the Algebra of Logic", vol iii 1895), although the original idea was again anticipated by Peirce.
Boole's unwavering acceptance of Aristotle's logic is emphasized by the historian of logic John Corcoran in an accessible introduction to "Laws of Thought" Corcoran also wrote a point-by-point comparison of "Prior Analytics" and "Laws of Thought". According to Corcoran, Boole fully accepted and endorsed Aristotle's logic. Boole's goals were "to go under, over, and beyond" Aristotle's logic by 1) providing it with mathematical foundations involving equations, 2) extending the class of problems it could treat — from assessing validity to solving equations — and 3) expanding the range of applications it could handle — e.g. from propositions having only two terms to those having arbitrarily many.
More specifically, Boole agreed with what Aristotle said; Boole's 'disagreements', if they might be called that, concern what Aristotle did not say. 
First, in the realm of foundations, Boole reduced the four propositional forms of Aristotelian logic to formulas in the form of equations — by itself a revolutionary idea. 
Second, in the realm of logic's problems, Boole's addition of equation solving to logic — another revolutionary idea — involved Boole's doctrine that Aristotle's rules of inference (the "perfect syllogisms") must be supplemented by rules for equation solving. 
Third, in the realm of applications, Boole's system could handle multi-term propositions and arguments whereas Aristotle could handle only two-termed subject-predicate propositions and arguments. For example, Aristotle's system could not deduce "No quadrangle that is a square is a rectangle that is a rhombus" from "No square that is a quadrangle is a rhombus that is a rectangle" or from "No rhombus that is a rectangle is a square that is a quadrangle".
Logicist period.
After Boole, the next great advances were made by the German mathematician Gottlob Frege. Frege's objective was the program of Logicism, i.e. demonstrating that arithmetic is identical with logic. Frege went much further than any of his predecessors in his rigorous and formal approach to logic, and his calculus or Begriffsschrift is important. Frege also tried to show that the concept of number can be defined by purely logical means, so that (if he was right) logic includes arithmetic and all branches of mathematics that are reducible to arithmetic. He was not the first writer to suggest this. In his pioneering work "Die Grundlagen der Arithmetik" (The Foundations of Arithmetic), sections 15–17, he acknowledges the efforts of Leibniz, J.S. Mill as well as Jevons, citing the latter's claim that "algebra is a highly developed logic, and number but logical discrimination."
Frege's first work, the "Begriffsschrift" ("concept script") is a rigorously axiomatised system of propositional logic, relying on just two connectives (negational and conditional), two rules of inference ("modus ponens" and substitution), and six axioms. Frege referred to the "completeness" of this system, but was unable to prove this. The most significant innovation, however, was his explanation of the quantifier in terms of mathematical functions. Traditional logic regards the sentence "Caesar is a man" as of fundamentally the same form as "all men are mortal." Sentences with a proper name subject were regarded as universal in character, interpretable as "every Caesar is a man". At the outset Frege abandons the traditional "concepts "subject" and "predicate"", replacing them with "argument" and "function" respectively, which he believes "will stand the test of time. It is easy to see how regarding a content as a function of an argument leads to the formation of concepts. Furthermore, the demonstration of the connection between the meanings of the words "if, and, not, or, there is, some, all," and so forth, deserves attention". Frege argued that the quantifier expression "all men" does not have the same logical or semantic form as "all men", and that the universal proposition "every A is B" is a complex proposition involving two "functions", namely ' – is A' and ' – is B' such that whatever satisfies the first, also satisfies the second. In modern notation, this would be expressed as
In English, "for all x, if Ax then Bx". Thus only singular propositions are of subject-predicate form, and they are irreducibly singular, i.e. not reducible to a general proposition. Universal and particular propositions, by contrast, are not of simple subject-predicate form at all. If "all mammals" were the logical subject of the sentence "all mammals are land-dwellers", then to negate the whole sentence we would have to negate the predicate to give "all mammals are "not" land-dwellers". But this is not the case. This functional analysis of ordinary-language sentences later had a great impact on philosophy and linguistics.
This means that in Frege's calculus, Boole's "primary" propositions can be represented in a different way from "secondary" propositions. "All inhabitants are either men or women" is
whereas "All the inhabitants are men or all the inhabitants are women" is
As Frege remarked in a critique of Boole's calculus:
As well as providing a unified and comprehensive system of logic, Frege's calculus also resolved the ancient problem of multiple generality. The ambiguity of "every girl kissed a boy" is difficult to express in traditional logic, but Frege's logic resolves this through the different scope of the quantifiers. Thus
means that to every girl there corresponds some boy (any one will do) who the girl kissed. But
means that there is some particular boy whom every girl kissed. Without this device, the project of logicism would have been doubtful or impossible. Using it, Frege provided a definition of the ancestral relation, of the many-to-one relation, and of mathematical induction.
This period overlaps with the work of what is known as the "mathematical school", which included Dedekind, Pasch, Peano, Hilbert, Zermelo, Huntington, Veblen and Heyting. Their objective was the axiomatisation of branches of mathematics like geometry, arithmetic, analysis and set theory. Most notable was Hilbert's Program, which sought to ground all of mathematics to a finite set of axioms, proving its consistency by "finitistic" means and providing a procedure which would decide the truth or falsity of any mathematical statement. The standard axiomatization of the natural numbers is named the Peano axioms in his honor. Peano maintained a clear distinction between mathematical and logical symbols. While unaware of Frege's work, he independently recreated his logical apparatus based on the work of Boole and Schröder.
The logicist project received a near-fatal setback with the discovery of a paradox in 1901 by Bertrand Russell. This proved Frege's naive set theory led to a contradiction. Frege's theory contained the axiom that for any formal criterion, there is a set of all objects that meet the criterion. Russell showed that a set containing exactly the sets that are not members of themselves would contradict its own definition (if it is not a member of itself, it is a member of itself, and if it is a member of itself, it is not). This contradiction is now known as Russell's paradox. One important method of resolving this paradox was proposed by Ernst Zermelo. Zermelo set theory was the first axiomatic set theory. It was developed into the now-canonical Zermelo–Fraenkel set theory (ZF). Russell's paradox symbolically is as follows:
The monumental Principia Mathematica, a three-volume work on the foundations of mathematics, written by Russell and Alfred North Whitehead and published 1910–13 also included an attempt to resolve the paradox, by means of an elaborate system of types: a set of elements is of a different type than is each of its elements (set is not the element; one element is not the set) and one cannot speak of the "set of all sets". The "Principia" was an attempt to derive all mathematical truths from a well-defined set of axioms and inference rules in symbolic logic.
Metamathematical period.
The names of Gödel and Tarski dominate the 1930s, a crucial period in the development of metamathematics – the study of mathematics using mathematical methods to produce metatheories, or mathematical theories about other mathematical theories. Early investigations into metamathematics had been driven by Hilbert's program. Work on metamathematics culminated in the work of Gödel, who in 1929 showed that a given first-order sentence is deducible if and only if it is logically valid – i.e. it is true in every structure for its language. This is known as Gödel's completeness theorem. A year later, he proved two important theorems, which showed Hibert's program to be unattainable in its original form. The first is that no consistent system of axioms whose theorems can be listed by an effective procedure such as an algorithm or computer program is capable of proving all facts about the natural numbers. For any such system, there will always be statements about the natural numbers that are true, but that are unprovable within the system. The second is that if such a system is also capable of proving certain basic facts about the natural numbers, then the system cannot prove the consistency of the system itself. These two results are known as Gödel's incompleteness theorems, or simply "Gödel's Theorem". Later in the decade, Gödel developed the concept of set-theoretic constructibility, as part of his proof that the axiom of choice and the continuum hypothesis are consistent with Zermelo–Fraenkel set theory.
In proof theory, Gerhard Gentzen developed natural deduction and the sequent calculus. The former attempts to model logical reasoning as it 'naturally' occurs in practice and is most easily applied to intuitionistic logic, while the latter was devised to clarify the derivation of logical proofs in any formal system. Since Gentzen's work, natural deduction and sequent calculi have been widely applied in the fields of proof theory, mathematical logic and computer science. Gentzen also proved normalization and cut-elimination theorems for intuitionistic and classical logic which could be used to reduce logical proofs to a normal form.
Alfred Tarski, a pupil of Łukasiewicz, is best known for his definition of truth and logical consequence, and the semantic concept of logical satisfaction. In 1933, he published (in Polish) "The concept of truth in formalized languages", in which he proposed his semantic theory of truth: a sentence such as "snow is white" is true if and only if snow is white. Tarski's theory separated the metalanguage, which makes the statement about truth, from the object language, which contains the sentence whose truth is being asserted, and gave a correspondence (the T-schema) between phrases in the object language and elements of an interpretation. Tarski's approach to the difficult idea of explaining truth has been enduringly influential in logic and philosophy, especially in the development of model theory. Tarski also produced important work on the methodology of deductive systems, and on fundamental principles such as completeness, decidability, consistency and definability. According to Anita Feferman, Tarski "changed the face of logic in the twentieth century".
Alonzo Church and Alan Turing proposed formal models of computability, giving independent negative solutions to Hilbert's "Entscheidungsproblem" in 1936 and 1937, respectively. The "Entscheidungsproblem" asked for a procedure that, given any formal mathematical statement, would algorithmically determine whether the statement is true. Church and Turing proved there is no such procedure; Turing's paper introduced the halting problem as a key example of a mathematical problem without an algorithmic solution.
Church's system for computation developed into the modern λ-calculus, while the Turing machine became a standard model for a general-purpose computing device. It was soon shown that many other proposed models of computation were equivalent in power to those proposed by Church and Turing. These results led to the Church–Turing thesis that any deterministic algorithm that can be carried out by a human can be carried out by a Turing machine. Church proved additional undecidability results, showing that both Peano arithmetic and first-order logic are undecidable. Later work by Emil Post and Stephen Cole Kleene in the 1940s extended the scope of computability theory and introduced the concept of degrees of unsolvability.
The results of the first few decades of the twentieth century also had an impact upon analytic philosophy and philosophical logic, particularly from the 1950s onwards, in subjects such as modal logic, temporal logic, deontic logic, and relevance logic.
Logic after WWII.
After World War II, mathematical logic branched into four inter-related but separate areas of research: model theory, proof theory, computability theory, and set theory.
In set theory, the method of forcing revolutionized the field by providing a robust method for constructing models and obtaining independence results. Paul Cohen introduced this method in 1963 to prove the independence of the continuum hypothesis and the axiom of choice from Zermelo–Fraenkel set theory. His technique, which was simplified and extended soon after its introduction, has since been applied to many other problems in all areas of mathematical logic.
Computability theory had its roots in the work of Turing, Church, Kleene, and Post in the 1930s and 40s. It developed into a study of abstract computability, which became known as recursion theory. The priority method, discovered independently by Albert Muchnik and Richard Friedberg in the 1950s, led to major advances in the understanding of the degrees of unsolvability and related structures. Research into higher-order computability theory demonstrated its connections to set theory. The fields of constructive analysis and computable analysis were developed to study the effective content of classical mathematical theorems; these in turn inspired the program of reverse mathematics. A separate branch of computability theory, computational complexity theory, was also characterized in logical terms as a result of investigations into descriptive complexity.
Model theory applies the methods of mathematical logic to study models of particular mathematical theories. Alfred Tarski published much pioneering work in the field, which is named after a series of papers he published under the title "Contributions to the theory of models". In the 1960s, Abraham Robinson used model-theoretic techniques to develop calculus and analysis based on infinitesimals, a problem that first had been proposed by Leibniz. 
In proof theory, the relationship between classical mathematics and intuitionistic mathematics was clarified via tools such as the realizability method invented by Georg Kreisel and Gödel's "Dialectica" interpretation. This work inspired the contemporary area of proof mining. The Curry-Howard correspondence emerged as a deep analogy between logic and computation, including a correspondence between systems of natural deduction and typed lambda calculi used in computer science. As a result, research into this class of formal systems began to address both logical and computational aspects; this area of research came to be known as modern type theory. Advances were also made in ordinal analysis and the study of independence results in arithmetic such as the Paris–Harrington theorem.
This was also a period, particularly in the 1950s and afterwards, when the ideas of mathematical logic begin to influence philosophical thinking. For example, tense logic is a formalised system for representing, and reasoning about, propositions qualified in terms of time. The philosopher Arthur Prior played a significant role in its development in the 1960s. Modal logics extend the scope of formal logic to include the elements of modality (for example, possibility and necessity). The ideas of Saul Kripke, particularly about possible worlds, and the formal system now called Kripke semantics have had a profound impact on analytic philosophy. His best known and most influential work is "Naming and Necessity" (1980). Deontic logics are closely related to modal logics: they attempt to capture the logical features of obligation, permission and related concepts. Although some basic novelties syncretizing mathematical and philosophical logic were shown by Bolzano in the early 1800s, it was Ernst Mally, a pupil of Alexius Meinong, who was to propose the first formal deontic system in his "Grundgesetze des Sollens", based on the syntax of Whitehead's and Russell's propositional calculus.
Another logical system founded after World War II was fuzzy logic by Azerbaijani mathematician Lotfi Asker Zadeh in 1965.
Logic in the East.
Logic in India.
Logic began independently in ancient India and continued to develop to early modern times without any known influence from Greek logic. Medhatithi Gautama (c. 6th century BC) founded the "anviksiki" school of logic. The "Mahabharata" (12.173.45), around the 5th century BC, refers to the "anviksiki" and "tarka" schools of logic. (c. 5th century BC) developed a form of logic (to which Boolean logic has some similarities) for his formulation of Sanskrit grammar. Logic is described by Chanakya (c. 350-283 BC) in his "Arthashastra" as an independent field of inquiry.
Two of the six Indian schools of thought deal with logic: Nyaya and Vaisheshika. The Nyaya Sutras of Aksapada Gautama (c. 2nd century AD) constitute the core texts of the Nyaya school, one of the six orthodox schools of Hindu philosophy. This realist school developed a rigid five-member schema of inference involving an initial premise, a reason, an example, an application, and a conclusion. The idealist Buddhist philosophy became the chief opponent to the Naiyayikas. Nagarjuna (c. 150-250 AD), the founder of the Madhyamika ("Middle Way") developed an analysis known as the catuṣkoṭi (Sanskrit), a "four-cornered" system of argumentation that involves the systematic examination and rejection of each of the 4 possibilities of a proposition, "P":
However, Dignaga (c 480-540 AD) is sometimes said to have developed a formal syllogism, and it was through him and his successor, Dharmakirti, that Buddhist logic reached its height; it is contested whether their analysis actually constitutes a formal syllogistic system. In particular, their analysis centered on the definition of an inference-warranting relation, "vyapti", also known as invariable concomitance or pervasion. To this end, a doctrine known as "apoha" or differentiation was developed. This involved what might be called inclusion and exclusion of defining properties.
The difficulties involved in this enterprise, in part, stimulated the neo-scholastic school of Navya-Nyāya, which developed a formal analysis of inference in the sixteenth century. This later school began around eastern India and Bengal, and developed theories resembling modern logic, such as Gottlob Frege's "distinction between sense and reference of proper names" and his "definition of number," as well as the Navya-Nyaya theory of "restrictive conditions for universals" anticipating some of the developments in modern set theory. Since 1824, Indian logic attracted the attention of many Western scholars, and has had an influence on important 19th-century logicians such as Charles Babbage, Augustus De Morgan, and particularly George Boole, as confirmed by his wife Mary Everest Boole, who wrote in 1901 an "open letter to Dr Bose", which was titled "Indian Thought and Western Science in the Nineteenth Century" and stated: "Think what must have been the effect of the intense Hinduizing of three such men as Babbage, De Morgan and George Boole on the mathematical atmosphere of 1830-1865".
Dignāga's famous "wheel of reason" ("Hetucakra") is a method of indicating when one thing (such as smoke) can be taken as an invariable sign of another thing (like fire), but the inference is often inductive and based on past observation. Matilal remarks that Dignāga's analysis is much like John Stuart Mill's Joint Method of Agreement and Difference, which is inductive.
In addition, the traditional five-member Indian syllogism, though deductively valid, has repetitions that are unnecessary to its logical validity. As a result, some commentators see the traditional Indian syllogism as a rhetorical form that is entirely natural in many cultures of the world, and yet not as a logical form—not in the sense that all logically unnecessary elements have been omitted for the sake of analysis.
Logic in China.
In China, a contemporary of Confucius, Mozi, "Master Mo", is credited with founding the Mohist school, whose canons dealt with issues relating to valid inference and the conditions of correct conclusions. In particular, one of the schools that grew out of Mohism, the Logicians, are credited by some scholars for their early investigation of formal logic. Due to the harsh rule of Legalism in the subsequent Qin Dynasty, this line of investigation disappeared in China until the introduction of Indian philosophy by Buddhists.

</doc>
<doc id="59949" url="https://en.wikipedia.org/wiki?curid=59949" title="Anglo-Catholicism">
Anglo-Catholicism

The terms Anglo-Catholicism, Anglican Catholicism and Catholic Anglicanism refer to people, beliefs and practices within Anglicanism that emphasise the Catholic heritage and identity of the various Anglican churches.
The term "Anglo-Catholic" was coined in the early 19th century, although movements emphasising the Catholic nature of Anglicanism had already existed. Particularly influential in the history of Anglo-Catholicism were the Caroline Divines of the seventeenth century and later the leaders of the Oxford Movement, which began at the University of Oxford in 1833 and ushered in a period of Anglican history known as the "Catholic Revival".
A minority of Anglo-Catholics, sometimes called Anglican Papalists, consider themselves under papal supremacy even though they are not in communion with the Roman Catholic Church. Such Anglo-Catholics, especially in England, often celebrate Mass according to the contemporary Roman Catholic rite and are concerned with seeking reunion with the Roman Catholic Church.
In addition, members of the personal ordinariates for former Anglicans created by Pope Benedict XVI are sometimes unofficially referred to as "Anglican Catholics".
History.
Following the passing of the Act of Supremacy and Henry VIII's break with the Roman Catholic Church, the Church of England continued to adhere to traditional Catholic teachings and did not initially make any alterations to doctrine. The "Ten Articles" were published in 1536 and constitute the first official Anglican articles of faith. The articles for the most part concurred with the pre-Reformation teachings of the Church in England and defended, among other things, the Real Presence of Christ in the Eucharist, the sacrament of confession, the honouring and invocation of saints and prayer for the dead. Belief in purgatory, however, was made non-essential. This was followed by the "Bishops' Book" in 1537, a combined effort by numerous clergy and theologians which, though not strongly Protestant in its inclinations, showed a slight move towards Reformed positions and was unpopular with conservative sections of the Church and quickly grew to be disliked by Henry VIII as well. The "Six Articles", released two years later, moved away from all Reformed ideas and strongly affirmed Catholic positions regarding matters such as transubstantiation and Mass for the dead. The "King's Book", the official article of religion written by Henry in 1543, likewise expressed Catholic sacramental theology and encouraged prayer for the dead.
A major shift in Anglican doctrine came in the reign of Henry's son, Edward VI, who repealed the "Six Articles" and under whose rule the Church of England became more identifiably Protestant. Though the Church's practices and approach to the sacraments became strongly influenced by those of continental reformers, it nevertheless retained episcopal church structure. The Church of England was then briefly reunited with the Roman Catholic Church under Mary, before separating again under Elizabeth I. The Elizabethan Religious Settlement was an attempt to end the religious divisions among Christians in England, and is often seen as an important event in Anglican history, ultimately laying the foundations for the ""via media"" concept of Anglicanism.
The nature of early Anglicanism was to be of great importance to the Anglo-Catholics of the 19th century, who would argue that their beliefs and practices were common during this period and were inoffensive to the earliest members of the Church of England.
Caroline Divines.
The Caroline Divines were a group of influential Anglican theologians active in the 17th century who opposed Calvinism and Puritanism and stressed the importance of episcopal polity, apostolic succession and the sacraments. The Caroline Divines also favoured elaborate liturgy (in some cases favouring the liturgy of the pre-Reformation church) and aesthetics. Their influence saw a revival in the use of images and statues in churches.
The leaders of the Anglo-Catholic revival in the 19th century would draw heavily from the works of the Caroline Divines.
Oxford Movement.
The modern Anglo-Catholic movement began with the Oxford Movement in the Victorian era, sometimes termed "Tractarianism".
In the early 19th century, various factors caused misgivings among English church people, including the decline of church life and the spread of unconventional practices in the Church of England. The British government's action in 1833 of beginning a reduction in the number of Church of Ireland bishoprics and archbishoprics inspired a sermon from John Keble in the University Church in Oxford on the subject of "National Apostasy". This sermon marked the inception of what became known as the Oxford Movement.
The principal objective of the Oxford Movement was the defence of the Church of England as a divinely-founded institution, of the doctrine of apostolic succession and of the "Book of Common Prayer" as a "rule of faith". The key idea was that Anglicanism was not a Protestant denomination but a branch of the historic Catholic Church, along with the Roman Catholic Church and the Eastern Orthodox churches. It was argued that Anglicanism had preserved the historical apostolic succession of priests and bishops and thus the Catholic sacraments. These ideas were promoted in a series of ninety "Tracts for the Times".
The principal leaders of the Oxford Movement were John Keble, John Henry Newman and Edward Bouverie Pusey. The movement gained influential support, but it was also attacked by the latitudinarians within the University of Oxford and by bishops of the church. Within the movement there gradually arose a much smaller group which tended towards submission to the supremacy of the Roman Catholic Church. In 1845 the university censured the "Ideal of a Christian Church" and its author, "Ideal Ward", the pro-Roman Catholic theologian W. G. Ward. The year 1850 saw the victory of the Evangelical cleric George Cornelius Gorham in a celebrated legal action against church authorities. Consequently, some Anglicans of Anglo-Catholic churchmanship, including John Henry Newman, were received into the Roman Catholic Church, while others, such as Mark Pattison, embraced Latitudinarian Anglicanism, and yet others, such James Anthony Froude, became sceptics. The majority of adherents of the movement, however, remained in the Church of England and, despite hostility in the press and in government, the movement spread. Its liturgical practices were influential, as were its social achievements (including its slum settlements) and its revival of male and female monasticism within Anglicanism.
Recent developments.
Since at least the 1970s, Anglo-Catholicism has been dividing into two distinct camps, along a fault-line which can perhaps be traced back to Bishop Charles Gore's work in the 19th century.
The Oxford Movement had been inspired in the first place by a rejection of liberalism and latitudinarianism in favour of the traditional faith of the "Church Catholic", defined by the teachings of the Church Fathers and the common doctrines of the historical eastern and western Christian churches. Until the 1970s, therefore, most Anglo-Catholics rejected liberalising development such as the conferral of holy orders on women. Present-day "traditionalist" Anglo-Catholics seek to maintain tradition and to keep Anglican doctrine in line with that of the Roman Catholic and Eastern Orthodox churches. They often ally themselves with Evangelicals to defend traditional teachings on sexual morality. The main organisation in the Church of England that opposes the ordination of women, Forward in Faith, is largely composed of Anglo-Catholics.
Gore's work, however, bearing the mark of liberal Protestant higher criticism, paved the way for an alternative form of Anglo-Catholicism influenced by liberal theology. Thus in recent years many Anglo-Catholics have accepted the ordination of women, the use of inclusive language in Bible translations and the liturgy, and progressive attitudes towards homosexuality and the blessing of same sex unions. Such Anglicans often refer to themselves as "Liberal Catholics". The more "progressive" or "liberal" style of Anglo-Catholicism is represented by Affirming Catholicism and the Society of Catholic Priests.
A third strand of Anglican Catholicism criticises elements of both liberalism and conservatism, drawing instead on the 20th century Roman Catholic "Nouvelle Théologie", especially Henri de Lubac. John Milbank and others within this strand have been instrumental in the creation of the ecumenical (though predominantly Anglican and Roman Catholic) movement known as Radical Orthodoxy.
Some traditionalist Anglo-Catholics have left official Anglicanism to form "continuing Anglican churches" such as those in the Anglican Catholic Church and Traditional Anglican Communion. Others have left Anglicanism altogether for the Roman Catholic or Eastern Orthodox churches, in the belief that liberal doctrinal changes in the Anglican churches have resulted in Anglicanism no longer being a true branch of the "Church Catholic".
Anglican ordinariates.
In late 2009 with the publication of the apostolic constitution "Anglicanorum Coetibus", traditionalist Anglicans were invited into unity with the Holy See. This action was in response to requests from various groups of Anglicans around the world to be received into full communion with the Holy See while retaining liturgical, musical, theological and other aspects of the Anglican patrimony.
An apostolic constitution is the highest level of papal legislation and is not time-limited. In other words, groups of Anglicans may apply for reception by the Holy See at any time and enter into what are termed "Anglican ordinariates" i.e. regional groupings of Anglican Catholics which come under the jurisdiction of an "ordinary", i.e. a bishop or priest appointed by Rome to oversee the community, which, while being in a country or region which is part of the Latin Rite of the Roman Catholic Church, retains aspects of the Anglican patrimony, e.g. married priests, traditional English choral music and liturgy.
Some have drawn parallels with the Eastern Catholic churches, but though there are some commonalities, Anglican ordinariates are intended to be part of the Western or Latin Rite of the Roman Catholic Church, as they had been before the breach with Rome following the reign of Mary I of England.
The first Anglican ordinariate, known as the Personal Ordinariate of Our Lady of Walsingham, was established on 15 January 2011 in the United Kingdom. The second Anglican ordinariate, known as the Personal Ordinariate of the Chair of Saint Peter, was established on 1 January 2012 in the United States. The already existing Anglican Use parishes in the United States, which have existed since the 1980s, will form a portion of the first American Anglican ordinariate. These parishes are already in communion with Rome and use modified Anglican liturgies approved by the Holy See. They will be joined by other groups and parishes of Episcopalians and some other Anglicans.
Practices and beliefs.
Theology.
Historically, Anglo-Catholics have valued "highly the tradition of the early, undivided Church, they saw its authority as co-extensive with Scripture. They re-emphasized the Church's institutional history and form. Anglo-Catholicism was emotionally intense, and yet drawn to aspects of the pre-Reformation Church, including the revival of religious orders, the reintroduction of the language and symbolism of the eucharistic sacrifice," and "the revival of private confession. Its spirituality was Evangelical in spirit, but High Church in content and form." At the same time, Anglo-Catholics held that "the Roman Catholic has corrupted the original ritualism; and she Anglican Church claims that the ritualism which she presents is a revival in purity of the original ritualism of the Catholic Church." The spirituality of Anglo-Catholics is drawn largely from the teachings of the early Church, in addition to the Caroline Divines. Archbishop of Canterbury Matthew Parker, in 1572, published "De Antiquitate Britannicæ Ecclesiæ", which traced the roots of the Anglican Church, arguing "that the early British Church differed from Roman Catholicism in key points and thus provided an alternative model for patristic Christianity," a view repeated by many Anglo-Catholics such as Charles Chapman Grafton, Bishop of the Diocese of Fond du Lac. In addition, Anglo-Catholics hold that the Anglican churches have maintained "catholicity and apostolicity." In the same vein, Anglo-Catholics emphasize the doctrines of apostolic succession and the threefold order, holding that these were retained by the Anglican Church after it went through the English Reformation. As an Anglican cleric and leader in the Oxford Movement, John Henry Newman summarized the beliefs of Anglo-Catholicism, defining "the differences between the Anglican position and the Roman Catholic one as antiquity vs. catholicity":
In agreement with the Eastern Orthodox Church and Oriental Orthodox Churches, Anglo-Catholics—along with Old-Catholics and Lutherans—generally appeal to the "canon" (or rule) of St Vincent of Lerins: "What everywhere, what always, and what by all has been believed, that is truly and properly Catholic."
The Anglican Thirty-Nine Articles make distinctions between Anglican and Roman Catholic understandings of doctrine; in the eyes of Anglo-Catholics, the Thirty-Nine Articles are Catholic, containing statements that profess the universal faith of the early Church. As the Articles were intentionally written in such a way as to be open to a range of interpretations, Anglo-Catholics have defended their practices and beliefs as being consistent with the Thirty-Nine Articles. A recent trend in Anglo-Catholic thought related to the Thirty-Nine Articles has included the New Perspective on Paul.
Anglo-Catholic priests often hear private confessions and anoint the sick, regarding these practices as sacraments. The classic Anglican aphorism regarding private confession is: "All may, some should, none must." Anglo-Catholics also offer prayers for the departed and the intercession of the saints; C.S. Lewis, often considered an Anglo-Catholic in his theological sensibilities, was once quoted as stating that, "Of course I pray for the dead. The action is so spontaneous, so all but inevitable, that only the most compulsive theological case against it would deter me. And I hardly know how the rest of my prayers would survive if those for the dead were forbidden. At our age, the majority of those we love best are dead. What sort of intercourse with God could I have if what I love best were unmentionable to him?" Anglicans of Anglo-Catholic churchmanship also believe in the real objective presence of Christ in the Eucharist and understand the way He is manifest in the sacrament to be a mystery of faith. Like the Eastern Orthodox, Anglo-Catholics, with the exception of the minority of Anglican Papalists, reject the Roman doctrines of the papal supremacy and papal infallibility, with Walter Herbert Stowe, an Anglo-Catholic cleric, explaining the Anglican position on these issues:
Liturgical practices.
Anglo-Catholics are often identified by their liturgical practices and ornaments. These have traditionally been characterised by the "six points" of the later Catholic Revival's eucharistic practice:
Many other traditional Catholic practices are observed within Anglo-Catholicism, including eucharistic adoration. Most of these Anglo-Catholic "innovations" have since been accepted by mainstream Anglican churches, if not by Evangelical or Low Church Anglicans.
Various liturgical strands exist within Anglo-Catholicism:
Preferences for Elizabethan English and modern English texts vary within the movement.
In the United States a group of Anglo-Catholics in the Episcopal Church published, under the rubrics of the 1979 Book of Common Prayer, the "Anglican Service Book" as "a traditional language adaptation of the 1979 Book of Common Prayer together with the Psalter or Psalms of David and additional devotions." This book is based on the 1979 Book of Common Prayer but includes offices and devotions in the traditional language of the 1928 Prayer Book that are not in the 1979 edition. The book also draws from sources such as the Anglican Missal.

</doc>
<doc id="59952" url="https://en.wikipedia.org/wiki?curid=59952" title="Thirty-Nine Articles">
Thirty-Nine Articles

The Thirty-Nine Articles of Religion are the historically defining statements of doctrines of the Church of England with respect to the controversies of the English Reformation. First established in 1563, the articles served to define the doctrine of the Church of England as it related to Calvinist doctrine and Roman-Catholic practice. The full name for the articles is commonly abbreviated as the Thirty-Nine Articles or the XXXIX Articles.
At the time, the Church of England was searching its doctrinal position in relation to the Roman-Catholic Church and the continental Protestant movements. A series of defining documents were written and replaced over a period of 30 years as the doctrinal and political situation changed from the excommunication of Henry VIII in 1533, to the excommunication of Elizabeth I in 1570.
Prior to King Henry's death in 1547, several statements of position were issued. The first attempt was the Ten Articles in 1536, which showed some slightly Protestant leanings—the result of an English desire for a political alliance with the German Lutheran princes. The next revision was the Six Articles in 1539 which swung away from all reformed positions, and the "King's Book" in 1543 which re-established almost in full the earlier Roman-Catholic doctrines. Then, during the reign of Edward VI in 1552, the Forty-Two Articles were written under the direction of Archbishop Thomas Cranmer. It was in this document that Calvinist thought reached the zenith of its influence in the English Church. These articles were never put into action, due to the king's death and the reunion of the English Church with Rome under Queen Mary I.
Finally, upon the coronation of Elizabeth and the re-establishment of the separate Church of England the Thirty-Nine Articles of Religion were established by the Convocation of 1563, under the direction of Matthew Parker ( then Archbishop of Canterbury ). It pulled back from some of the more extreme Calvinist thinking and created the peculiar English reformed doctrine.
The articles, finalised in 1571, were to have a lasting effect on religion in the United Kingdom and elsewhere through their incorporation into and propagation through the Book of Common Prayer.
Ten Articles (1536).
The Ten Articles were first published in 1536 by Thomas Cranmer. They were the first guidelines of the Church of England as it became independent of Rome.
In summary, the Ten Articles asserted:
The emerging doctrines of the autonomous Church of England were followed by further explication in "The Institution of the Christian Man".
"Bishops' Book" (1537).
"The Institution of the Christian Man" (also called "The Bishops' Book"), published in 1537, was written by a committee of 46 divines and bishops headed by Thomas Cranmer. The purpose of the work, along with the Ten Articles of the previous year, was to implement the reforms of Henry VIII in separating from the Catholic Church and reforming the "Ecclesia Anglicana". It was considered reformatory in basic orientation, though it was not strongly Lutheran. The work functioned as an official formulary of the reformed Anglican faith in England. It was later superseded by other creedal and official statements during the successive reigns of Edward VI and Elizabeth I, as the Anglican Church moved toward a more Reformed theological position. It would evolve into the "King's Book". "The work was a noble endeavor on the part of the bishops to promote unity, and to instruct the people in Church doctrine.""'
Authorship.
The list of the 46 divines as they appear in the "Bishop's Book" included all of the bishops, eight archdeacons and 17 other Doctors of Divinity, some of whom were later involved with translating the Bible and compiling the Prayer Book:
Thomas Cranmer –
Edward Lee –
John Stokesley –
Cuthbert Tunstall –
Stephen Gardiner –
Robert Aldrich –
John Voysey –
John Longland –
John Clerk –
Royland Lee –
Thomas Goodrich –
Nicholas Shaxton –
John Bird –
Edward Foxe –
Hugh Latimer –
John Hilsey –
Richard Sampson –
William Repps –
William Barlowe –
Robert Partew –
Robert Holgate –
Richard Wolman –
William Knight –
John Bell –
Edmond Bonner –
William Skip –
Nicholas Heath –
Cuthbert Marshal –
Richard Curren –
William Cliffe –
William Downes –
Robert Oking –
Ralph Bradford –
Richard Smyth –
Simon Matthew –
John Pryn –
William Buckmaster –
William May –
Nicholas Wotton –
Richard Cox –
John Edmunds –
Thomas Robertson –
John Baker –
Thomas Barett –
John Hase –
John Tyson
Six Articles (1539).
In 1538 three German theologians – Francis Burkhardt, vice-chancellor of Saxony; George von Boyneburg, doctor of law; and Friedrich Myconius, superintendent of the church of Gotha – were sent to London and held conferences with the Anglican bishops and clergy in the archbishop's palace at Lambeth for several months. The Germans presented, as a basis of agreement, a number of Articles based on the Lutheran Confession of Augsburg. Bishops Tunstall, Stokesley and others were not won over by these Protestant arguments and did everything they could to avoid agreement. They were willing to separate from Rome, but their plan was to unite with the Greek Church and not with the evangelical Protestants on the continent. The bishops also refused to eliminate what the Germans called the "Abuses" (e.g. private Masses, celibacy of the clergy, worship of angels) allowed by the Anglican Church. Stokesley considered these customs to be essential because the Greek Church practised them. In opposition, Cranmer favoured a union with German Protestants. The king, unwilling to break with Catholic practices, dissolved the conference.
Henry had felt uneasy about the appearance of the Lutheran doctors and their theology within his kingdom. On 28 April 1539 Parliament met for the first time in three years. On 5 May, the House of Lords created a committee with the customary religious balance to examine and determine doctrine. Eleven days later, the Duke of Norfolk noted that the committee had not agreed on anything and proposed that the Lords examine six doctrinal questions which eventually became the basis of the Six Articles. The articles reaffirmed traditional Roman-Catholic doctrine on key issues:
Penalties under the Act, "the whip with six strings", ranged from imprisonment and fine to death. However, its severity was reduced by an act of 1540, which retained the death penalty only for denial of transubstantiation, and a further act limited its arbitrariness. The Catholic emphasis of the doctrine commended in the articles is not matched by the ecclesiastical reforms Henry undertook in the following years, such as the enforcement of the necessity of the English Bible and the insistence upon the abolition of all shrines, both in 1541.
As the "Act of the Six Articles" neared passage in Parliament, Cranmer moved his wife and children out of seclusion, probably in Ford Palace in Kent, and out of England. The Act passed Parliament at the end of June; subsequently bishops Latimer and Nicholas Shaxton, outspoken opponents of the measure, resigned their dioceses. After Henry's death the articles were repealed by his son, Edward VI.
"King's Book" (1543).
"The Necessary Doctrine and Erudition for Any Christian Man", also known as the "King's Book", was published in 1543, and attributed to Henry VIII. It was a revision of "The Institution of the Christian Man", and defended transubstantiation and the Six Articles. It also encouraged preaching and attacked the use of images.
Forty-Two Articles (1552).
The Forty-Two Articles were intended to summarise Anglican doctrine, as it now existed under the reign of Edward VI, who favoured a Protestant faith. Largely the work of Thomas Cranmer, they were to be short formularies that would demonstrate the faith revealed in Scripture and the existing Catholic creeds. Completed in 1552, they were issued by Royal Mandate on 19 June 1553. The articles were claimed to have received the authority of a Convocation, although this is doubtful. With the coronation of Queen Mary I and the reunion of the Church of England with the Catholic Church, the Articles were never enforced. However, after Mary's death, they became the basis of the Thirty-Nine Articles. In 1563, Convocation met under Archbishop Parker to revise the articles. Convocation passed only 39 of the 42, and Elizabeth reduced the number to 38 by throwing out Article XXIX to avoid offending her subjects with Catholic leanings. In 1571, the Article XXIX, despite the opposition of Bishop Edmund Guest, was inserted, to the effect that the wicked do not eat the Body of Christ. This was done following the queen's excommunication by the Pope Pius V in 1570. That act destroyed any hope of reconciliation with Rome and it was no longer necessary to fear that Article XXIX would offend Catholic sensibilities. The Articles, increased to Thirty-nine, were ratified by the Queen, and the bishops and clergy were required to assent.
Thirty-Nine Articles (1563).
The Thirty-Nine Articles were not intended as a complete statement of the Christian faith, but of the position of the Church of England in relation to the Catholic Church and dissident Protestants. The Articles argue against some Anabaptist positions such as the holding of goods in common and the necessity of believer's baptism. The motivation for their production and enactment was the absence of a general consensus on matters of faith following the separation from Rome. There was a concern that dissenters who wanted the reforms to go much further (for example, to abolish hierarchies of bishops) would increase in influence. Wishing to pursue Elizabeth's agenda of establishing a national church that would maintain the indigenous apostolic faith and incorporate some of the insights of Protestantism, the Articles were intended to incorporate a balance of theology and doctrine. This allowed them to appeal to the broadest domestic opinion, Catholic and otherwise. In this sense, the Articles are a revealing window into the ethos and character of Anglicanism, in particular in the way the document works to navigate a "via media" ("middle path") between the beliefs and practices of the Catholic Church and of the English Puritans, thus lending the Church of England a mainstream Reformed air. The "via media" was expressed so adroitly in the Articles that some Anglican scholars have labelled their content as an early example of the idea that the doctrine of Anglicanism is one of "Reformed Catholicism".
Content.
The Articles highlight the Anglican positions with regard to orthodox Catholic teachings, to Puritanism, and to Anabaptist thought.
They are divided, in compliance with the command of Queen Elizabeth, into four sections: Articles 1–8, "The Catholic Faith"; Articles 9–18, "Personal Religion"; Articles 19–31, "Corporate Religion"; and Articles 32–39, "Miscellaneous." The articles were issued both in English and in Latin, and both are of equal authority.
Summary.
Articles I–VIII: The Catholic Articles:
The first five articles articulate the Catholic credal statements concerning the nature of God, manifest in the Holy Trinity. Articles VI and VII deal with scripture, while Article VIII discusses the essential creeds.
Articles IX—XVIII: The Protestant and Reformed Articles:
These articles dwell on the topics of sin, justification, and the eternal disposition of the soul. Of particular focus is the major Reformation topic of justification by faith.
Articles XIX–XXXI: The Anglican Articles:
This section focuses on the expression of faith in the public venue – the institutional church, the councils of the church, worship, ministry, and sacramental theology.
Articles XXXII—XXXIX: Miscellaneous:
These articles concern clerical celibacy, excommunication, traditions of the Church, and other issues not covered elsewhere.
Article XXXVII additionally states among other things that the Bishop of Rome has no jurisdiction in the realm of England.
Interpretation.
In 1628 Charles I of England prefixed a royal declaration to the articles, which demands a literal interpretation of them, threatening discipline for academics or churchmen teaching any personal interpretations or encouraging debate about them. It states: "no man hereafter shall either print or preach, to draw the Article aside any way, but shall submit to it in the plain and Full meaning thereof: and shall not put his own sense or comment to be the meaning of the Article, but shall take it in the literal and grammatical sense."
However, what the Articles truly mean has been a matter of debate in the Church since before they were issued. The evangelical wing of the Church has taken the Articles at face value. In 2003, evangelical Anglican clergyman Chris Pierce wrote:
This view has never been held by the whole church. In 1643, Archbishop of Armagh John Bramhall laid out the core argument against the Articles:
This divergence of opinion became overt during the Oxford Movement of the 19th century. The stipulations of Articles XXV and XXVIII were regularly invoked by evangelicals to oppose the reintroduction of certain beliefs, customs, and acts of piety with respect to the sacraments. In response, Cardinal John Henry Newman's Tract 90 attempted to show that the Articles could be interpreted in a way less hostile to Roman-Catholic doctrine.
History and influence.
Adherence to the Articles was made a legal requirement by the English Parliament in 1571. They are printed in the Book of Common Prayer and other Anglican prayer books. The Test Act of 1672 made adherence to the Articles a requirement for holding civil office in England until its repeal in 1824. Students at Oxford University were still expected to sign up to them until the passing of the University Reform Act of 1854.
In the past, in numerous national churches and dioceses, those entering Holy Orders had to make an oath of subscription to the Articles. Clergy of the Church of England are required to affirm their loyalty to the Articles and other historic formularies (the Book of Common Prayer and the Ordering of Bishops, Priests and Deacons). The Church of Ireland has a similar declaration for its clergy, while some other churches of the Anglican Communion make no such requirement.
The influence of the Articles on Anglican thought, doctrine and practice has been profound. Although Article VIII itself states that the three Catholic creeds are a sufficient statement of faith, the Articles have often been perceived as the nearest thing to a supplementary confession of faith possessed by the tradition.
A revised version was adopted in 1801 by the US Episcopal Church. Earlier, John Wesley, founder of the Methodists, adapted the Thirty-Nine Articles for use by American Methodists in the 18th century. The resulting Articles of Religion remain official United Methodist doctrine.
In Anglican discourse, the Articles are regularly cited and interpreted to clarify doctrine and practice. Sometimes they are used to prescribe support of Anglican comprehensiveness. An important concrete manifestation of this is the Chicago-Lambeth Quadrilateral, which incorporates Articles VI, VIII, XXV, and XXXVI in its broad articulation of fundamental Anglican identity. In other circumstances they delineate the parameters of acceptable belief and practice in proscriptive fashion.
The Articles continue to be invoked today in the Anglican Church. For example, in the ongoing debate over homosexual activity and the concomitant controversies over episcopal authority, Articles VI, XX, XXIII, XXVI, and XXXIV are regularly cited by those of various opinions.
Each of the 44 member churches in the Anglican Communion is, however, free to adopt and authorise its own official documents, and the Articles are not officially normative in all Anglican Churches (neither is the Athanasian Creed). The only doctrinal documents agreed upon in the Anglican Communion are the Apostles' Creed, the Nicene Creed of AD 381, and the Chicago-Lambeth Quadrilateral. Beside these documents, authorised liturgical formularies, such as Prayer Book and Ordinal, are normative. The several provincial editions of Prayer Books (and authorised alternative liturgies) are, however, not identical, although they share a greater or smaller amount of family resemblance. No specific edition of the Prayer Book is therefore binding for the entire Communion.

</doc>
<doc id="59953" url="https://en.wikipedia.org/wiki?curid=59953" title="Van Allen radiation belt">
Van Allen radiation belt

A radiation belt is a layer of energetic charged particles that is held in place around a magnetized planet, such as the Earth, by the planet's magnetic field. The Earth has two such belts and sometimes others may be temporarily created. The discovery of the belts is credited to James Van Allen, and as a result the Earth's belts are known as the Van Allen belts. The main belts extend from an altitude of about 1,000 to 60,000 kilometers above the surface in which region radiation levels vary. Most of the particles that form the belts are thought to come from solar wind and other particles by cosmic rays. The belts are located in the inner region of the Earth's magnetosphere. The belts contain energetic electrons and protons. Other nuclei, such as alpha particles, are less prevalent. The belts endanger satellites, which must protect their sensitive components with adequate shielding if they spend significant time in the radiation belts. In 2013, NASA reported that the Van Allen Probes had discovered a transient, third radiation belt, which was observed for four weeks until destroyed by a powerful, interplanetary shock wave from the Sun.
Discovery.
Kristian Birkeland, Carl Størmer, and Nicholas Christofilos had investigated the possibility of trapped charged particles before the Space Age. Explorer 1 and Explorer 3 confirmed the existence of the belt in early 1958 under James Van Allen at the University of Iowa. The trapped radiation was first mapped out by Explorer 4, Pioneer 3 and Luna 1.
The term "Van Allen belts" refers specifically to the radiation belts surrounding Earth; however, similar radiation belts have been discovered around other planets. The Sun itself does not support long-term radiation belts, as it lacks a stable, global dipole field. The Earth's atmosphere limits the belts' particles to regions above 200–1,000 km, while the belts do not extend past 7 Earth radii "RE". The belts are confined to a volume which extends about 65° from the celestial equator.
Research.
The NASA Van Allen Probes mission aims at understanding (to the point of predictability) how populations of relativistic electrons and ions in space form or change in response to changes in solar activity and the solar wind.
NASA Institute for Advanced Concepts–funded studies have proposed magnetic scoops to collect antimatter that naturally occurs in the Van Allen belts of Earth, although only about 10 micrograms of antiprotons are estimated to exist in the entire belt.
The Van Allen Probes mission successfully launched on August 30, 2012. The primary mission is scheduled to last two years with expendables expected to last four. NASA's Goddard Space Flight Center manages the overall Living With a Star program of which the Van Allen Probes is a project, along with Solar Dynamics Observatory (SDO). The Applied Physics Laboratory is responsible for the overall implementation and instrument management for the Van Allen Probes.
Radiation belts exist around other planets and moons in the solar system that have magnetic fields powerful enough to sustain them. To date most of these radiation belts have been poorly mapped. The Voyager Program (namely Voyager 2) only nominally confirmed the existence of similar belts on Uranus and Neptune.
Outer belt.
The outer belt consists mainly of high energy (0.1–10 MeV) electrons trapped by the Earth's magnetosphere. It is almost toroidal in shape, extending from an altitude of about three to ten Earth radii ("RE") or above the Earth's surface. Its greatest intensity is usually around 4–5 "RE". The outer electron radiation belt is mostly produced by the inward radial diffusion and local acceleration due to transfer of energy from whistler-mode plasma waves to radiation belt electrons. Radiation belt electrons are also constantly removed by collisions with atmospheric neutrals, losses to magnetopause, and the outward radial diffusion. The gyroradii for energetic protons would be large enough to bring them into contact with the Earth's atmosphere. The electrons here have a high flux and at the outer edge (close to the magnetopause), where geomagnetic field lines open into the geomagnetic "tail", fluxes of energetic electrons can drop to the low interplanetary levels within about , a decrease by a factor of 1,000.
In 2014 it was discovered that the inner edge of the outer belt is characterized by a very sharp edge, below which highly relativistic electrons (> 5MeV) cannot penetrate. The reason for this shield-like behavior is not well understood.
The trapped particle population of the outer belt is varied, containing electrons and various ions. Most of the ions are in the form of energetic protons, but a certain percentage are alpha particles and O+ oxygen ions, similar to those in the ionosphere but much more energetic. This mixture of ions suggests that ring current particles probably come from more than one source.
The outer belt is larger than the inner belt and its particle population fluctuates widely. Energetic (radiation) particle fluxes can increase and decrease dramatically as a consequence of geomagnetic storms, which are themselves triggered by magnetic field and plasma disturbances produced by the Sun. The increases are due to storm-related injections and acceleration of particles from the tail of the magnetosphere.
On February 28, 2013, a third radiation belt, consisting of high-energy ultrarelativistic charged particles, was reported to be discovered. In a news conference by NASA's Van Allen Probe team, it was stated that this third belt is generated when a mass coronal ejection is created by the Sun. It has been represented as a separate creation which splits the Outer Belt, like a knife, on its outer side, and exists separately as a storage container for a month's time, before merging once again with the Outer Belt.
The unusual stability of this third, transient belt has been explained as due to a 'trapping' by the Earth's magnetic field of ultrarelativistic particles as they are lost from the second, traditional outer belt. While the outer zone, which forms and disappears over a day, is highly variable owing to interactions with the atmosphere, the ultrarelativistic particles of the third belt are thought to not scatter into the atmosphere, as they are too energetic to interact with atmospheric waves at low latitudes. This absence of scattering and the trapping allows them to persist for a long time, finally only being destroyed by an unusual event, such as the shock wave from the sun which eventually destroyed it.
Inner belt.
The inner Van Allen Belt extends typically from an altitude of 0.2 to 2 Earth radii (L values of 1 to 3) or 600 miles (1,000 km) to 3,700 miles (6,000 km) above the Earth. In certain cases when solar activity is stronger or in geographical areas such as the South Atlantic Anomaly (SAA), the inner boundary may go down to roughly 200 kilometers above the Earth's surface. The inner belt contains high concentrations of electrons in the range of hundreds of keV and energetic protons with energies exceeding 100 MeV, trapped by the strong (relative to the outer belts) magnetic fields in the region.
It is believed that proton energies exceeding 50 MeV in the lower belts at lower altitudes are the result of the beta decay of neutrons created by cosmic ray collisions with nuclei of the upper atmosphere. The source of lower energy protons is believed to be proton diffusion due to changes in the magnetic field during geomagnetic storms.
Due to the slight offset of the belts from Earth's geometric center, the inner Van Allen belt makes its closest approach to the surface at the South Atlantic Anomaly.
On March 2014, a pattern resembling 'zebra stripes' was observed in the radiation belts by the Radiation Belt Storm Probes Ion Composition Experiment (RBSPICE) onboard Van Allen Probes. The reason reported was that due to the tilt in Earth's magnetic field axis, the planet’s rotation generated an oscillating, weak electric field that permeates through the entire inner radiation belt. It was later demonstrated that the zebra stripes were in fact an imprint of ionospheric winds on radiation belts.
Flux values.
In the belts, at a given point, the flux of particles of a given energy decreases sharply with energy.
At the magnetic equator, electrons of energies exceeding 500 keV (resp. 5 MeV) have omnidirectional fluxes ranging from 1.2×106 (resp. 3.7×104) up to 9.4×109 (resp. 2×107) particles per square centimeter per second.
The proton belts contain protons with kinetic energies ranging from about 100 keV (which can penetrate 0.6 µm of lead) to over 400 MeV (which can penetrate 143 mm of lead).
Most published flux values for the inner and outer belts may not show the maximum probable flux densities that are possible in the belts. There is a reason for this discrepancy: the flux density and the location of the peak flux is variable (depending primarily on solar activity), and the number of spacecraft with instruments observing the belt in real time has been limited. The Earth has not experienced a solar storm of Carrington event intensity and duration while spacecraft with the proper instruments have been available to observe the event.
Regardless of the differences of the flux levels in the Inner and Outer Van Allen belts, the beta radiation levels would be dangerous to humans if they were exposed for an extended period of time. The Apollo missions minimised hazards for astronauts by sending spacecraft at high speeds through the thinner areas of the upper belts, bypassing inner belts completely.
Antimatter confinement.
In 2011, a study confirmed earlier speculation that the Van Allen belt could confine antiparticles. The PAMELA experiment detected orders of magnitude higher levels of antiprotons than are expected from normal particle decays while passing through the SAA. This suggests the Van Allen belts confine a significant flux of antiprotons produced by the interaction of the Earth's upper atmosphere with cosmic rays. The energy of the antiprotons has been measured in the range from 60–750 MeV.
Implications for space travel.
Spacecraft travelling beyond low Earth orbit leave the protection of earth's geomagnetic field and transit the Van Allen belts. Beyond these, they face additional hazards from cosmic rays and solar flares. A region between the inner and outer Van Allen belts lies at two to four Earth radii and is sometimes referred to as the "safe zone".
Solar cells, integrated circuits, and sensors can be damaged by radiation. Geomagnetic storms occasionally damage electronic components on spacecraft. Miniaturization and digitization of electronics and logic circuits have made satellites more vulnerable to radiation, as the total electric charge in these circuits is now small enough so as to be comparable with the charge of incoming ions. Electronics on satellites must be hardened against radiation to operate reliably. The Hubble Space Telescope, among other satellites, often has its sensors turned off when passing through regions of intense radiation. A satellite shielded by 3 mm of aluminium in an elliptic orbit () passing the radiation belts will receive about 2,500 rem (25 Sv) per year (for comparison, a full-body dose of 5 Sv is deadly). Almost all radiation will be received while passing the inner belt.
The Apollo missions marked the first event where humans traveled through the Van Allen belts, which was one of several radiation hazards known by mission planners. The astronauts had low exposure in the Van Allen belts due to the short period of time spent flying through them. Apollo flight trajectories bypassed the inner belts completely to send spacecraft through only the thinner areas of the outer belts. The command module's inner structure was an aluminum "sandwich" consisting of a welded aluminium inner skin, a thermally bonded honeycomb core, and a thin aluminium "face sheet". The steel honeycomb core and outer face sheets were thermally bonded to the inner skin.
Astronauts' overall exposure was actually dominated by solar particles once outside Earth's magnetic field. The total radiation received by the astronauts varied from mission to mission but was measured to be between 0.16 and 1.14 rads (1.6 and 11.4 mGy), much less than the standard of 5 rem (50 mSv) per year set by the United States Atomic Energy Commission for people who work with radioactivity.
Causes.
It is generally understood that the inner and outer Van Allen belts result from different processes. The inner belt, consisting mainly of energetic protons, is the product of the decay of so-called "albedo" neutrons which are themselves the result of cosmic ray collisions in the upper atmosphere. The outer belt consists mainly of electrons. They are injected from the geomagnetic tail following geomagnetic storms, and are subsequently energized through wave-particle interactions.
In the inner belt, particles that originate from the sun are trapped in the Earth's nonlinear magnetic field. Particles gyrate and move along field lines. As particles encounter regions of larger density of magnetic field lines, their "longitudinal" velocity is slowed and can be reversed, reflecting the particle. This causes the particles to bounce back and forth between the Earth's poles. Globally, the motion of these trapped particles is chaotic.
A gap between the inner and outer Van Allen belts, sometimes called safe zone or safe slot, is caused by the Very Low Frequency (VLF) waves which scatter particles in pitch angle which results in the gain of particles to the atmosphere. Solar outbursts can pump particles into the gap but they drain again in a matter of days. The radio waves were originally thought to be generated by turbulence in the radiation belts, but recent work by James L. Green of the Goddard Space Flight Center comparing maps of lightning activity collected by the Microlab 1 spacecraft with data on radio waves in the radiation-belt gap from the IMAGE spacecraft suggests that they are actually generated by lightning within Earth's atmosphere. The radio waves they generate strike the ionosphere at the right angle to pass through it only at high latitudes, where the lower ends of the gap approach the upper atmosphere. These results are still under scientific debate.
Proposed removal.
High Voltage Orbiting Long Tether, or HiVOLT, is a concept proposed by Russian physicist V.V. Danilov and further refined by Robert P. Hoyt and Robert L. Forward for draining and removing the radiation fields of the Van Allen radiation belts that surround the Earth. A proposed configuration consists of a system of five 100 km long conducting tethers deployed from satellites, and charged to a large voltage. This would cause charged particles that encounter the tethers to have their pitch angle changed, thus over time dissolving the inner belts. Hoyt and Forward's company, Tethers Unlimited, performed a preliminary analysis simulation in 2011, and produced a chart depicting a theoretical radiation flux reduction, to less than 1% of current levels within two months for the inner belts that threaten LEO objects.

</doc>
<doc id="59955" url="https://en.wikipedia.org/wiki?curid=59955" title="Trojan Horse">
Trojan Horse

The Trojan Horse is a tale from the Trojan War about the subterfuge that the Greeks used to enter the city of Troy and win the war. In the canonical version, after a fruitless 10-year siege, the Greeks constructed a huge wooden horse, and hid a select force of men inside. The Greeks pretended to sail away, and the Trojans pulled the horse into their city as a victory trophy. That night the Greek force crept out of the horse and opened the gates for the rest of the Greek army, which had sailed back under cover of night. The Greeks entered and destroyed the city of Troy, decisively ending the war.
Metaphorically a "Trojan Horse" has come to mean any trick or stratagem that causes a target to invite a foe into a securely protected bastion or place. A malicious computer program which tricks users into willingly running it is also called a "Trojan horse".
The main ancient source for the story is the "Aeneid" of Virgil, a Latin epic poem from the time of Augustus. The event is referred to in Homer's "Iliad". In the Greek tradition, the horse is called the "Wooden Horse" (Δούρειος Ἵππος, "Doúreios Híppos", in the Homeric Ionic dialect).
Literary accounts.
According to Quintus Smyrnaeus, Odysseus thought of building a great wooden horse (the horse being the emblem of Troy), hiding an elite force inside, and fooling the Trojans into wheeling the horse into the city as a trophy. Under the leadership of Epeios, the Greeks built the wooden horse in three days. Odysseus' plan called for one man to remain outside the horse; he would act as though the Greeks had abandoned him, leaving the horse as a gift for the Trojans. An inscription was engraved on the horse reading: "For their return home, the Greeks dedicate this offering to Athena". Then they burned their tents and left to Tenedos by night. Greek soldier Sinon was "abandoned", and was to signal to the Greeks by lighting a beacon. In Virgil's poem, Sinon, the only volunteer for the role, successfully convinces the Trojans that he has been left behind and that the Greeks are gone. Sinon tells the Trojans that the Horse is an offering to the goddess Athena, meant to atone for the previous desecration of her temple at Troy by the Greeks, and ensure a safe journey home for the Greek fleet. Sinon tells the Trojans that the Horse was built to be too large for them to take it into their city and gain the favor of Athena for themselves.
While questioning Sinon, the Trojan priest Laocoön guesses the plot and warns the Trojans, in Virgil's famous line "Timeo Danaos et dona ferentes" ("I fear Greeks, even those bearing gifts"), Danai (accusative "Danaos") or Danaans (Homer's name for the Greeks) being the ones who had built the Trojan Horse. However, the god Poseidon sends two sea serpents to strangle him and his sons Antiphantes and Thymbraeus before any Trojan heeds his warning. According to Apollodorus the two serpents were sent by Apollo, whom Laocoon had insulted by sleeping with his wife in front of the "divine image". In the Odyssey, Homer says that Helen of Troy also guesses the plot and tries to trick and uncover the Greek soldiers inside the horse by imitating the voices of their wives, and Anticlus attempts to answer, but Odysseus shuts his mouth with his hand. King Priam's daughter Cassandra, the soothsayer of Troy, insists that the horse will be the downfall of the city and its royal family. She too is ignored, hence their doom and loss of the war.
This incident is mentioned in the "Odyssey":
The most detailed and most familiar version is in Virgil's "Aeneid", Book II (trans. A. S. Kline).
"After many years have slipped by, the leaders of the Greeks,<br>
"opposed by the Fates, and damaged by the war,<br>
"build a horse of mountainous size, through Pallas's divine art,<br>
"and weave planks of fir over its ribs:<br>
"they pretend it's a votive offering: this rumour spreads.<br>
"They secretly hide a picked body of men, chosen by lot,<br>
"there, in the dark body, filling the belly and the huge<br>
"cavernous insides with armed warriors.
[...]<br>
"Then Laocoön rushes down eagerly from the heights<br>
"of the citadel, to confront them all, a large crowd with him,<br> 
"and shouts from far off: "O unhappy citizens, what madness?<br> 
"Do you think the enemy's sailed away? Or do you think<br>
"any Greek gift's free of treachery? Is that Ulysses's reputation?<br> 
"Either there are Greeks in hiding, concealed by the wood,<br> 
"or it's been built as a machine to use against our walls,<br> 
"or spy on our homes, or fall on the city from above,<br> 
"or it hides some other trick: Trojans, don't trust this horse.<br> 
"Whatever it is, I'm afraid of Greeks even those bearing gifts."<br> 
Book II includes Laocoön saying: ""Equo ne credite, Teucri. Quidquid id est, timeo Danaos et dona ferentes."" ("Do not trust the horse, Trojans! Whatever it is, I fear the Greeks, even bringing gifts.")
Well before Virgil, the story is also alluded to in Greek classical literature. In Euripides' play "Trojan Women", written in 415 BC, the god Poseidon proclaims: "For, from his home beneath Parnassus, Phocian Epeus, aided by the craft of Pallas, framed a horse to bear within its womb an armed host, and sent it within the battlements, fraught with death; whence in days to come men shall tell of 'the wooden horse,' with its hidden load of warriors."
Men in the horse.
Thirty soldiers hid in the Trojan horse's belly and two spies in its mouth. Other sources give different numbers: The "Bibliotheca" 50; Tzetzes 23; and Quintus Smyrnaeus gives the names of 30, but says there were more. In late tradition the number was standardized at 40. Their names follow:
Factual explanations.
There has been speculation that the Trojan Horse may have been a battering ram resembling, to some extent, a horse, and that the description of the use of this device was then transformed into a myth by later oral historians who were not present at the battle and were unaware of that meaning of the name. Assyrians at the time used siege machines with animal names, often covered with dampened horse hides to protect against flaming arrows; it is possible that the Trojan Horse was such. Pausanias, who lived in the 2nd century AD, wrote in his book "Description of Greece" "That the work of Epeius was a contrivance to make a breach in the Trojan wall is known to everybody who does not attribute utter silliness to the Phrygians" where, by Phrygians, he means the Trojans.
Some authors have suggested that the gift was not a horse with warriors hiding inside, but a boat carrying a peace envoy, and it has also been noted that the terms used to put men in the horse are those used when describing the embarkation of men on a ship.
Images.
There are three known surviving classical depictions of the Trojan Horse. The earliest is on a fibula brooch dated about 700 BC. The other two are on relief pithos vases from the adjoining Grecian islands Mykonos and Tinos, both usually dated between 675 and 650 BC, the one from Mykonos being known as the Mykonos Vase. Historian Michael Wood, however, dates the Mykonos Vase to the 8th century BC, some 500 years after the supposed time of the war, but before the written accounts attributed by tradition to Homer. Wood concludes from that evidence that the story of the Trojan Horse was in existence prior to the writing of those accounts.

</doc>
<doc id="59957" url="https://en.wikipedia.org/wiki?curid=59957" title="Smart card">
Smart card

[[File:Differentsmartcardpadlayouts.jpg|thumb|Contact-type smart cards may have many different contact pad layouts, such as these 
SIMs]]
A smart card, chip card, or integrated circuit card (ICC) is any pocket-sized card that has embedded integrated circuits. Smart cards are made of plastic, generally polyvinyl chloride, but sometimes polyethylene terephthalate based polyesters, acrylonitrile butadiene styrene or polycarbonate.
Since April 2009, a Japanese company has manufactured reusable financial smart cards made from paper.
Smart cards can be either contact or contactless smart card. Smart cards can provide personal identification, authentication, data storage, and application processing. Smart cards may provide strong security authentication for single sign-on (SSO) within large organizations.
History.
Invention.
In 1968 and 1969 Helmut Gröttrup and Jürgen Dethloff jointly filed patents for the automated chip card. Roland Moreno patented the memory card concept in 1974. An important patent for smart cards with a microprocessor and memory as used today was filed by Jürgen Dethloff in 1976 and granted as USP 4105156 in 1978.
In 1977, Michel Ugon from Honeywell Bull invented the first microprocessor smart card. In 1978, Bull patented the self-programmable one-chip microcomputer (SPOM) that defines the necessary architecture to program the chip. Three years later, Motorola used this patent in its "CP8". At that time, Bull had 1,200 patents related to smart cards. In 2001, Bull sold its CP8 division together with its patents to Schlumberger, who subsequently combined its own internal smart card department and CP8 to create Axalto. In 2006, Axalto and Gemplus, at the time the world's top two smart card manufacturers, merged and became Gemalto. In 2008 Dexa Systems spun off from Schlumberger and acquired Enterprise Security Services business, which included the smart card solutions division responsible for deploying the first large scale public key infrastructure (PKI) based smart card management systems.
The first mass use of the cards was as a telephone card for payment in French pay phones, starting in 1983.
Carte Bleue.
After the Télécarte, microchips were integrated into all French "Carte Bleue" debit cards in 1992. Customers inserted the card into the merchant's point of sale (POS) terminal, then typed the personal identification number (PIN), before the transaction was accepted. Only very limited transactions (such as paying small highway tolls) are processed without a PIN.
Smart-card-based "electronic purse" systems store funds on the card so that readers do not need network connectivity. They entered European service in the mid-1990s. They have been common in Germany (Geldkarte), Austria (Quick Wertkarte), Belgium (Proton), France (Moneo), the Netherlands (Chipknip Chipper (decommissioned in 2001)), Switzerland ("Cash"), Norway ("Mondex"), Sweden ("Cash", decommissioned in 2004), Finland ("Avant"), UK ("Mondex"), Denmark ("Danmønt") and Portugal ("Porta-moedas Multibanco").
Since the 1990s, smart-cards have been the Subscriber Identity Modules (SIMs) used in European GSM mobile phone equipment. Mobile phones are widely used in Europe, so smart cards have become very common.
EMV.
Europay MasterCard Visa (EMV)-compliant cards and equipment are widespread. The United States started using the EMV technology in 2014. Typically, a country's national payment association, in coordination with MasterCard International, Visa International, American Express and Japan Credit Bureau (JCB), jointly plan and implement EMV systems.
Historically, in 1993 several international payment companies agreed to develop smart-card specifications for debit and credit cards. The original brands were MasterCard, Visa, and Europay. The first version of the EMV system was released in 1994. In 1998 the specifications became stable.
EMVCo maintains these specifications. EMVco's purpose is to assure the various financial institutions and retailers that the specifications retain backward compatibility with the 1998 version. EMVco upgraded the specifications in 2000 and 2004.
EMV compliant cards were accepted into the United States in 2014. MasterCard was the first company that has been allowed to use the technology in the United States. The United States has felt pushed to use the technology because of the increase in identity theft. The credit card information stolen from Target in late 2013 was one of the largest indicators that American credit card information is not safe. Target has made the decision on April 30, 2014 that they are going to try and implement the smart chip technology in order to protect themselves from future credit card identity theft.
Before 2014, the consensus in America was that there was enough security measures to avoid credit card theft and that the smart chip was not necessary. The cost of the smart chip technology was significant, which was why most of the corporations did not want to pay for it in the United States. The debate came when online credit theft was insecure enough for the United States to invest in the technology. The adaptation of EMV's increased significantly in 2015 when the liability shifts occurred in October by the credit card companies.
Development of contactless systems.
"Contactless" smart cards do not require physical contact between a card and reader. They are becoming more popular for payment and ticketing. Typical uses include mass transit and motorway tolls. Visa and MasterCard implemented a version deployed in 2004–2006 in the U.S. Most contactless fare collection systems are incompatible, though the MIFARE Standard card from NXP Semiconductors has a considerable market share in the US and Europe.
Smart cards are also being introduced for identification and entitlement by regional, national, and international organizations. These uses include citizen cards, drivers’ licenses, and patient cards. In Malaysia, the compulsory national ID MyKad enables eight applications and has 18 million users. Contactless smart cards are part of ICAO biometric passports to enhance security for international travel.
Design.
A smart card may have the following generic characteristics:
Contact smart cards.
Contact smart cards have a contact area of approximately , comprising several gold-plated contact pads. These pads provide electrical connectivity when inserted into a reader, which is used as a communications medium between the smart card and a host (e.g., a computer, a point of sale terminal) or a mobile telephone. Cards do not contain batteries; power is supplied by the card reader.
The ISO/IEC 7810 and ISO/IEC 7816 series of standards define:
Because the chips in financial cards are the same as those used in subscriber identity modules (SIMs) in mobile phones, programmed differently and embedded in a different piece of PVC, chip manufacturers are building to the more demanding GSM/3G standards. So, for example, although the EMV standard allows a chip card to draw 50 mA from its terminal, cards are normally well below the telephone industry's 6 mA limit. This allows for smaller and cheaper financial card terminals.
Communication protocols for contact smart cards include T=0 (character-level transmission protocol, defined in ISO/IEC 7816-3) and T=1 (block-level transmission protocol, defined in ISO/IEC 7816-3).
Contactless smart cards.
A second card type is the "contactless smart card", in which the card communicates with and is powered by the reader through RF induction technology (at data rates of 106–848 kbit/s). These cards require only proximity to an antenna to communicate.
Like smart cards with contacts, contactless cards do not have an internal power source. Instead, they use an inductor to capture some of the incident radio-frequency interrogation signal, rectify it, and use it to power the card's electronics.
APDU transmission via a contactless interface is defined in ISO/IEC 14443-4.
Hybrids.
Dual-interface cards implement contactless and contact interfaces on a single card with some shared storage and processing. An example is Porto's multi-application transport card, called Andante, which uses a chip with both contact and contactless (ISO/IEC 14443 Type B) interfaces.
USB.
The CCID (Chip Card Interface Device) is a USB protocol that allows a smartcard to be connected to a Computer, using a standard USB interface. This allows the smartcard to be used as a security token for authentication and data encryption such as Bitlocker. CCID devices typically look like a standard USB dongle and may contain a SIM card inside the USB dongle.
Applications.
Financial.
Smart cards serve as credit or ATM cards, fuel cards, mobile phone SIMs, authorization cards for pay television, household utility pre-payment cards, high-security identification and access-control cards, and public transport and public phone payment cards.
Smart cards may also be used as electronic wallets. The smart card chip can be "loaded" with funds to pay parking meters, vending machines or merchants. Cryptographic protocols protect the exchange of money between the smart card and the machine. No connection to a bank is needed. The holder of the card may use it even if not the owner. Examples are Proton, Geldkarte, Chipknip and Moneo. The German Geldkarte is also used to validate customer age at vending machines for cigarettes.
These are the best known payment cards (classic plastic card):
Roll-outs started in 2005 in the U.S. Asia and Europe followed in 2006. Contactless (non-PIN) transactions cover a payment range of ~$5–50. There is an ISO/IEC 14443 PayPass implementation. Some, but not all PayPass implementations conform to EMV.
Non-EMV cards work like magnetic stripe cards. This is common in the U.S. (PayPass Magstripe and Visa MSD). The cards do not hold or maintain the account balance. All payment passes without a PIN, usually in off-line mode. The security of such a transaction is no greater than with a magnetic stripe card transaction.
EMV cards can have either contact or contactless interfaces. They work as if they were a normal EMV card with a contact interface. Via the contactless interface they work somewhat differently, in that the card commands enabled improved features such as lower power and shorter transaction times.
SIM.
The subscriber identity modules used in mobile-phone systems are reduced-size smart cards, using otherwise identical technologies.
Identification.
Smart-cards can authenticate identity. Usually, they employ a public key infrastructure (PKI). The card stores an encrypted digital certificate issued from the PKI provider along with other relevant information. Examples include the U.S. Department of Defense (DoD) Common Access Card (CAC), and other cards used by other governments for their citizens. If they include biometric identification data, cards can provide superior two- or three-factor authentication.
Smart cards are not always privacy-enhancing, because the subject may carry incriminating information on the card. Contactless smart cards that can be read from within a wallet or even a garment simplify authentication; however, criminals may access data from these cards.
Cryptographic smart cards are often used for single sign-on. Most advanced smart cards include specialized cryptographic hardware that uses algorithms such as RSA and Digital Signature Algorithm (DSA). Today's cryptographic smart cards generate key pairs on board, to avoid the risk from having more than one copy of the key (since by design there usually isn't a way to extract private keys from a smart card). Such smart cards are mainly used for digital signatures and secure identification.
The most common way to access cryptographic smart card functions on a computer is to use a vendor-provided PKCS#11 library. On Microsoft Windows the Cryptographic Service Provider (CSP) API is also supported.
The most widely used cryptographic algorithms in smart cards (excluding the GSM so-called "crypto algorithm") are Triple DES and RSA. The key set is usually loaded (DES) or generated (RSA) on the card at the personalization stage.
Some of these smart cards are also made to support the National Institute of Standards and Technology (NIST) standard for Personal Identity Verification, FIPS 201.
Turkey implemented the first smart card driver's license system in 1987. Turkey had a high level of road accidents and decided to develop and use digital tachograph devices on heavy vehicles, instead of the existing mechanical ones, to reduce speed violations. Since 1987, the professional driver's licenses in Turkey have been issued as smart cards. A professional driver is required to insert his driver's license into a digital tachograph before starting to drive. The tachograph unit records speed violations for each driver and gives a printed report. The driving hours for each driver are also being monitored and reported. In 1990 the European Union conducted a feasibility study through BEVAC Consulting Engineers, titled "Feasibility study with respect to a European electronic drivers license (based on a smart-card) on behalf of Directorate General VII". In this study, chapter seven describes Turkey's experience.
Argentina's Mendoza province began using smart card driver's licenses in 1995. Mendoza also had a high level of road accidents, driving offenses, and a poor record of recovering fines. Smart licenses hold up-to-date records of driving offenses and unpaid fines. They also store personal information, license type and number, and a photograph. Emergency medical information such as blood type, allergies, and biometrics (fingerprints) can be stored on the chip if the card holder wishes. The Argentina government anticipates that this system will help to collect more than $10 million per year in fines.
In 1999 Gujarat was the first Indian state to introduce a smart card license system. As of 2005, it has issued 5 million smart card driving licenses to its people.
In 2002, the Estonian government started to issue smart cards named ID Kaart as primary identification for citizens to replace the usual passport in domestic and EU use.
As of 2010 about 1 million smart cards have been issued (total population is about 1.3 million) and they are widely used in internet banking, buying public transport tickets, authorization on various websites etc.
By the start of 2009 the entire population of Spain and Belgium became eID cards that are used for identification. These cards contain two certificates: one for authentication and one for signature. This signature is legally enforceable. More and more services in these countries use eID for authorization.
On August 14, 2012, the ID cards in Pakistan were replaced. The Smart Card is a third generation chip-based identity document that is produced according to international standards and requirements. The card has over 36 physical security features and has the latest encryption codes. This smart card replaced the NICOP (the ID card for overseas Pakistani).
Smart cards may identify emergency responders and their skills. Cards like these allow first responders to bypass organizational paperwork and focus more time on the emergency resolution. In 2004, The Smart Card Alliance expressed the needs: "to enhance security, increase government efficiency, reduce identity fraud, and protect personal privacy by establishing a mandatory, Government-wide standard for secure and reliable forms of identification". emergency response personnel can carry these cards to be positively identified in emergency situations. WidePoint Corporation, a smart card provider to FEMA, produces cards that contain additional personal information, such as medical records and skill sets.
In 2007, the Open Mobile Alliance (OMA) proposed a new standard defining V1.0 of the Smart Card Web Server (SCWS), an HTTP server embedded in a SIM card intended for a smartphone user. The non-profit trade association SIMalliance has been promoting the development and adoption of SCWS. SIMalliance states that SCWS offers end-users a familiar, OS-independent, browser-based interface to secure, personal SIM data. As of mid-2010, SIMalliance had not reported widespread industry acceptance of SCWS. The OMA has been maintaining the standard, approving V1.1 of the standard in May 2009, and V1.2 is expected was approved in October 2012.
Public transit.
Smart cards and integrated ticketing are used by many public transit operators. Card users may also make small purchases using the cards. Some operators offer points for usage, exchanged at retailers or for other benefits. Examples include Singapore's CEPAS, Toronto's Presto card, Hong Kong's Octopus Card, London's Oyster Card, Dublin's Leap card, Brussels' MoBIB, Québec's OPUS card, San Francisco's Clipper card, Auckland's AT Hop, Brisbane's go card, Perth's SmartRider and Sydney's Opal card. However, these present a privacy risk because they allow the mass transit operator (and the government) to track an individual's movement. In Finland, for example, the Data Protection Ombudsman prohibited the transport operator Helsinki Metropolitan Area Council (YTV) from collecting such information, despite YTV's argument that the card owner has the right to a list of trips paid with the card. Earlier, such information was used in the investigation of the Myyrmanni bombing.
The UK's Department for Transport mandated smart cards to administer travel entitlements for elderly and disabled residents. These schemes let residents use the cards for more than just bus passes. They can also be used for taxi and other concessionary transport. One example is the "Smartcare go" scheme provided by Ecebs. The UK systems use the ITSO Ltd specification.
Computer security.
Smart cards can be used as a security token.
The Mozilla Firefox web browser can use smart cards to store certificates for use in secure web browsing.
Some disk encryption systems, such as Microsoft’s BitLocker, can use smart cards to securely hold encryption keys, and also to add another layer of encryption to critical parts of the secured disk.
GnuPG, the well known encryption suite, also supports storing keys in a smart card.
Smart cards are also used for single sign-on to log on to computers.
Schools.
Smart cards are being provided to students at some schools and colleges. Uses include:
Healthcare.
Smart health cards can improve the security and privacy of patient information, provide a secure carrier for portable medical records, reduce health care fraud, support new processes for portable medical records, provide secure access to emergency medical information, enable compliance with government initiatives (e.g., organ donation) and mandates, and provide the platform to implement other applications as needed by the health care organization.
Other uses.
Smart cards are widely used to protect digital television streams. VideoGuard is a specific example of how smart card security worked.
Multiple-use systems.
The Malaysian government promotes MyKad as a single system for all smart-card applications. MyKad started as identity cards carried by all citizens and resident non-citizens. Available applications now include identity, travel documents, drivers license, health information, an electronic wallet, ATM bank-card, public toll-road and transit payments, and public key encryption infrastructure. The personal information inside the MYKAD card can be read using special APDU commands.
Security.
Smart cards have been advertised as suitable for personal identification tasks, because they are engineered to be tamper resistant. The chip usually implements some cryptographic algorithm. There are, however, several methods for recovering some of the algorithm's internal state.
Differential power analysis involves measuring the precise time and electric current required for certain encryption or decryption operations. This can deduce the on-chip private key used by public key algorithms such as RSA. Some implementations of symmetric ciphers can be vulnerable to timing or power attacks as well.
Smart cards can be physically disassembled by using acid, abrasives, solvents, or some other technique to obtain unrestricted access to the on-board microprocessor. Although such techniques may involve a risk of permanent damage to the chip, they permit much more detailed information (e.g., photomicrographs of encryption hardware) to be extracted.
Benefits.
The benefits of smart cards are directly related to the volume of information and applications that are programmed for use on a card. A single contact/contactless smart card can be programmed with multiple banking credentials, medical entitlement, driver’s license/public transport entitlement, loyalty programs and club memberships to name just a few. Multi-factor and proximity authentication can and has been embedded into smart cards to increase the security of all services on the card. For example, a smart card can be programmed to only allow a contactless transaction if it is also within range of another device like a uniquely paired mobile phone. This can significantly increase the security of the smart card.
Governments and regional authorities save money because of improved security, better data and reduced processing costs. These savings help reduce public budgets or enhance public services. There are many examples in the UK, many using a common open LASSeO specification.
Individuals have better security and more convenience with using smart cards that perform multiple services. For example, they only need to replace one card if their wallet is lost or stolen. The data storage on a card can reduce duplication, and even provide emergency medical information.
Advantages.
The first main advantage of smart cards is their flexibility. Smart cards have multiple functions which simultaneously can be an ID, a credit card, a stored-value cash card, and a repository of personal information such as telephone numbers or medical history. The card can be easily replaced if lost, and, the requirement for a PIN (or other form of security) provides additional security from unauthorised access to information by others. At the first attempt to use it illegally, the card would be deactivated by the card reader itself.
The second main advantage is security. Smart cards can be electronic key rings, giving the bearer ability to access information and physical places without need for online connections. They are encryption devices, so that the user can encrypt and decrypt information without relying on unknown, and therefore potentially untrustworthy, appliances such as ATMs. Smart cards are very flexible in providing authentication at different level of the bearer and the counterpart. Finally, with the information about the user that smart cards can provide to the other parties, they are useful devices for customizing products and services.
Other general benefits of smart cards are:
Smart cards and electronic commerce.
Smart cards can be used in electronic commerce, over the Internet, though the business model used in current electronic commerce applications still cannot use the full potential of the electronic medium. An advantage of smart cards for electronic commerce is their use customize services. For example, in order for the service supplier to deliver the customized service, the user may need to provide each supplier with their profile, a boring and time-consuming activity. A smart card can contain a non-encrypted profile of the bearer, so that the user can get customized services even without previous contacts with the supplier.
Disadvantages.
The plastic card in which the chip is embedded is fairly flexible. The larger the chip, the higher the probability that normal use could damage it. Cards are often carried in wallets or pockets, a harsh environment for a chip. However, for large banking systems, failure-management costs can be more than offset by fraud reduction.
If the account holder's computer hosts malware, the smart card security model may be broken. Malware can override the communication (both input via keyboard and output via application screen) between the user and the application. Man-in-the-browser malware (e.g., the Trojan Silentbanker) could modify a transaction, unnoticed by the user. Banks like Fortis and Belfius in Belgium and Rabobank ("random reader") in the Netherlands combine a smart card with an unconnected card reader to avoid this problem. The customer enters a challenge received from the bank's website, a PIN and the transaction amount into the reader, The reader returns an 8-digit signature. This signature is manually entered into the personal computer and verified by the bank, preventing malware from changing the transaction amount.
Smart cards have also been the targets of security attacks. These attacks range from physical invasion of the card's electronics, to non-invasive attacks that exploit weaknesses in the card's software or hardware. The usual goal is to expose private encryption keys and then read and manipulate secure data such as funds. Once an attacker develops a non-invasive attack for a particular smart card model, he is typically able to perform the attack on other cards of that model in seconds, often using equipment that can be disguised as a normal smart card reader. While manufacturers may develop new card models with additional security, it may be costly or inconvenient for users to upgrade vulnerable systems. Tamper-evident and audit features in a smart card system help manage the risks of compromised cards.
Another problem is the lack of standards for functionality and security. To address this problem, the Berlin Group launched the ERIDANE Project to propose "a new functional and security framework for smart-card based Point of Interaction (POI) equipment".

</doc>
<doc id="59958" url="https://en.wikipedia.org/wiki?curid=59958" title="Power series">
Power series

In mathematics, a power series (in one variable) is an infinite series of the form
where "an" represents the coefficient of the "n"th term and "c" is a constant. This series usually arises as the Taylor series of some known function.
In many situations "c" (the "center" of the series) is equal to zero, for instance when considering a Maclaurin series. In such cases, the power series takes the simpler form
These power series arise primarily in analysis, but also occur in combinatorics (as generating functions, a kind of formal power series) and in electrical engineering (under the name of the Z-transform). The familiar decimal notation for real numbers can also be viewed as an example of a power series, with integer coefficients, but with the argument "x" fixed at . In number theory, the concept of p-adic numbers is also closely related to that of a power series.
Examples.
Any polynomial can be easily expressed as a power series around any center "c", although most of the coefficients will be zero since a power series has infinitely many terms by definition. For instance, the polynomial formula_3 can be written as a power series around the center formula_4 as
or around the center formula_6 as
or indeed around any other center "c". One can view power series as being like "polynomials of infinite degree," although power series are not polynomials.
The geometric series formula
which is valid for formula_10, is one of the most important examples of a power series, as are the exponential function
formula
and the sine formula
valid for all real x.
These power series are also examples of Taylor series.
Negative powers are not permitted in a power series; for instance, formula_13
is not considered a power series (although it is a Laurent series). Similarly, fractional powers such as formula_14 are not permitted (but see Puiseux series). The coefficients formula_15 are not allowed to depend on formula_16, thus for instance:
Radius of convergence.
A power series will converge for some values of the variable "x" and may diverge for others. All power series "f"("x") in powers of ("x"-"c") will converge at "x" = "c". (The correct value "f"("c") = "a"0 requires interpreting the expression 00 as equal to 1.) If "c" is not the only convergent point, then there is always a number "r" with 0 < "r" ≤ ∞ such that the series converges whenever |"x" − "c"| < "r" and diverges whenever |"x" − "c"| > "r". The number "r" is called the radius of convergence of the power series; in general it is given as
or, equivalently,
formula_19
(this is the Cauchy–Hadamard theorem; see limit superior and limit inferior for an explanation of the notation). A fast way to compute it is
if this limit exists.
The series converges absolutely for |"x" − "c"| < "r" and converges uniformly on every compact subset of {"x" : |"x" − "c"| < "r"}. That is, the series is absolutely and compactly convergent on the interior of the disc of convergence.
For |"x" − "c"| = "r", we cannot make any general statement on whether the series converges or diverges. However, for the case of real variables, Abel's theorem states that the sum of the series is continuous at "x" if the series converges at "x". In the case of complex variables, we can only claim continuity along the line segment starting at "c" and ending at "x".
Operations on power series.
Addition and subtraction.
When two functions "f" and "g" are decomposed into power series around the same center "c", the power series of the sum or difference of the functions can be obtained by termwise addition and subtraction. That is, if:
then
Multiplication and division.
With the same definitions above, for the power series of the product and quotient of the functions can be obtained as follows:
The sequence formula_27 is known as the convolution of the sequences formula_15 and formula_29.
For division, observe:
and then use the above, comparing coefficients.
Differentiation and integration.
Once a function is given as a power series, it is differentiable on the interior of the domain of convergence. It can be differentiated and integrated quite easily, by treating every term separately:
Both of these series have the same radius of convergence as the original one.
Analytic functions.
A function "f" defined on some open subset "U" of R or C is called analytic if it is locally given by a convergent power series. This means that every "a" ∈ "U" has an open neighborhood "V" ⊆ "U", such that there exists a power series with center "a" which converges to "f"("x") for every "x" ∈ "V".
Every power series with a positive radius of convergence is analytic on the interior of its region of convergence. All holomorphic functions are complex-analytic. Sums and products of analytic functions are analytic, as are quotients as long as the denominator is non-zero.
If a function is analytic, then it is infinitely often differentiable, but in the real case the converse is not generally true. For an analytic function, the coefficients "a""n" can be computed as
where formula_35 denotes the "n"th derivative of "f" at "c", and formula_36. This means that every analytic function is locally represented by its Taylor series.
The global form of an analytic function is completely determined by its local behavior in the following sense: if "f" and "g" are two analytic functions defined on the same connected open set "U", and if there exists an element "c"∈"U" such that "f" ("n")("c") = "g" ("n")("c") for all "n" ≥ 0, then "f"("x") = "g"("x") for all "x" ∈ "U".
If a power series with radius of convergence "r" is given, one can consider analytic continuations of the series, i.e. analytic functions "f" which are defined on larger sets than { "x" : |"x" − "c"| < "r" } and agree with the given power series on this set. The number "r" is maximal in the following sense: there always exists a complex number "x" with |"x" − "c"| = "r" such that no analytic continuation of the series can be defined at "x".
The power series expansion of the inverse function of an analytic function can be determined using the Lagrange inversion theorem.
Formal power series.
In abstract algebra, one attempts to capture the essence of power series without being restricted to the fields of real and complex numbers, and without the need to talk about convergence. This leads to the concept of formal power series, a concept of great utility in algebraic combinatorics.
Power series in several variables.
An extension of the theory is necessary for the purposes of multivariable calculus. A power series is here defined to be an infinite series of the form
where "j" = ("j"1, ..., "j""n") is a vector of natural numbers, the coefficients
"a"("j1...,jn") are usually real or complex numbers, and the center "c" = ("c"1, ..., "c""n") and argument "x" = ("x"1, ..., "x""n") are usually real or complex vectors. In the more convenient multi-index notation this can be written
The theory of such series is trickier than for single-variable series, with more complicated regions of convergence. For instance, the power series formula_39 is absolutely convergent in the set formula_40 between two hyperbolas. (This is an example of a "log-convex set", in the sense that the set of points formula_41, where formula_42 lies in the above region, is a convex set. More generally, one can show that when c=0, the interior of the region of absolute convergence is always a log-convex set in this sense.) On the other hand, in the interior of this region of convergence one may differentiate and integrate under the series sign, just as one may with ordinary power series.
Order of a power series.
Let α be a multi-index for a power series "f"("x"1, "x"2, …, "x""n"). The order of the power series "f" is defined to be the least value |α| such that "a"α ≠ 0, or 0 if "f" ≡ 0. In particular, for a power series "f"("x") in a single variable "x", the order of "f" is the smallest power of "x" with a nonzero coefficient. This definition readily extends to Laurent series.

</doc>
<doc id="59959" url="https://en.wikipedia.org/wiki?curid=59959" title="ISO/IEC 7816">
ISO/IEC 7816

ISO/IEC 7816 is an international standard related to electronic identification cards with contacts, especially smart cards, managed jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC).
It is edited by the Joint technical committee (JTC) 1 / Sub-Committee (SC) 17, Cards and personal identification.
The following describes the different parts of this standard.
7816-1: Physical characteristics.
Created in 1987, updated in 1998, amended in 2003.
This part describes the physical characteristics of the card, primarily by reference to ISO/IEC 7810 "Identification cards — Physical characteristics", but also with other characteristics such as mechanical strength.
7816-2: Cards with contacts — Dimensions and location of the contacts.
Created in 1988, updated in 1999, amended in 2004, updated in 2007.
7816-3: Cards with contacts — Electrical interface and transmission protocols.
Created in 1989, amended in 1992 (addition of the T=1 protocol), amended in 1994 (revision of Protocol Type Selection), updated in 1997 (including addition of 3 Volt operation), amended in 2002 (including addition of 1.8 Volt operation), last updated in 2006 (including removal of Vpp).
7816-4: Organization, security and commands for interchange.
Created in 1995, updated in 2013.
According to its abstract, it specifies:
It does not cover the internal implementation within the card or the outside world.
ISO/IEC 7816-4:2013 is independent of the physical interface technology, and applies equally to contact cards, proximity cards and vicinity cards.
7816-5: Registration of application providers.
Created in 1995, updated in 2004.
According to its abstract, ISO/IEC 7816-5 defines how to use an application identifier to ascertain the presence of and/or perform the retrieval of an application in a card.
ISO/IEC 7816-5:2004 shows how to grant the uniqueness of application identifiers through the international registration of a part of this identifier, and defines
7816-6: Interindustry data elements for interchange.
Created in 1996, updated in 2004.
According to its abstract, it specifies the Data Elements (DEs) used for interindustry interchange based on integrated circuit cards (ICCs) both with contacts and without contacts. It gives the identifier, name, description, format, coding and layout of each DE and defines the means of retrieval of DEs from the card.
7816-7: Interindustry commands for Structured Card Query Language (SCQL).
Created in 1999.
7816-8: Commands for security operations.
Created in 1995, updated in 2004.
According to its abstract, it specifies interindustry commands for integrated circuit cards (either with contacts or without contacts) that may be used for cryptographic operations. These commands are complementary to and based on the commands listed in ISO/IEC 7816-4.
Annexes are provided that give examples of operations related to digital signatures, certificates and the import and export of asymmetric keys.
The choice and conditions of use of cryptographic mechanisms may affect card exportability. The evaluation of the suitability of algorithms and protocols is outside the scope of ISO/IEC 7816-8.
7816-9: Commands for card management.
Created in 1995, updated in 2004.
According to its abstract, it specifies interindustry commands for integrated circuit cards (both with contacts and without contacts) for card and file management, e.g. file creation and deletion. These commands cover the entire life cycle of the card and therefore some commands may be used before the card has been issued to the cardholder or after the card has expired.
An annex is provided that shows how to control the loading of data (secure download) into the card, by means of verifying the access rights of the loading entity and protection of the transmitted data with secure messaging. The loaded data may contain, for example, code, keys and applets.
7816-10: Electronic signals and answer to reset for synchronous cards.
Created in 1999.
This part specifies the power, signal structures, and the structure for the answer to reset between an integrated circuit card(s) with synchronous transmission and an interface device such as a terminal.
7816-11 Personal verification through biometric methods.
Created in 2004.
According to its abstract, it specifies the usage of interindustry commands and data objects related to personal verification through biometric methods in integrated circuit cards. The interindustry commands used are defined in ISO/IEC 7816-4. The data objects are partially defined in this International Standard, partially imported from ISO/IEC 19785-1.
ISO/IEC 7816-11 also presents examples for enrollment and verification and addresses security issues.
7816-12 Cards with contacts — USB electrical interface and operating procedures.
Created in 2005.
According to its abstract, it specifies the operating conditions of an integrated circuit card that provides a USB interface. An integrated circuit card with a USB interface is named USB-ICC.
ISO/IEC 7816-12:2005 specifies:
ISO/IEC 7816-12:2005 provides two protocols for control transfers. This is to support the protocol T=0 (version A) or to use the transfer on APDU level (version B). ISO/IEC 7816-12:2005 provides the state diagrams for the USB-ICC for each of the transfers (bulk transfers, control transfers version A and version B). Examples of possible sequences which the USB-ICC must be able to handle are given in an informative annex.
The USB CCID device class defines a standard for communicating with ISO/IEC 7816 smart cards over USB.
7816-13: Commands for application management in multi-application environment.
This part specifies commands for application management in a multi-application environment.
7816-15: Cryptographic information application.
Created in 2004, amended in 2004, 2007, 2008.
According to its abstract, it specifies a card application. This application contains information on cryptographic functionality. Further, ISO/IEC 7816-15:2004 defines a common syntax (in ASN.1) and format for the cryptographic information and mechanisms to share this information whenever appropriate.
ISO/IEC 7816-15:2004 supports the following capabilities:

</doc>
<doc id="59961" url="https://en.wikipedia.org/wiki?curid=59961" title="Ra'anana">
Ra'anana

Ra'anana (, lit. "Fresh") is a city in the heart of the southern Sharon Plain of the Central District of Israel with a population of 80,000, . Ra'anana is bordered by Kfar Saba on the east and Herzliya on the southwest. While the majority of its residents are native-born Israelis, a large part of the population are immigrants from the Americas and Europe.
Ra'anana's high tech industrial park is home to many leading global companies and local start up companies. It was designated a "Green City" award by the World Health Organization in 2005.
History.
In 1912, the Company for Jewish Settlement in Israel formed the "Ahuza A – New York" group to purchase land in Israel for agricultural settlement. The First World War delayed their plans but on April 2, 1922, two wagons left the corner of Lilienblum and Herzl Streets in Tel Aviv carrying 4 "Ahuza" members, 3 laborers and 2 armed watchmen. After a 5-hour journey, they unloaded their baggage at the place destined to become Ra'anana.
In its early days, the settlement was called "Ahuza A – New York." The Arabs of the region called it "Little America" as most of its residents were English speakers and came from New York. Later it was renamed "Ra'anania" and finally the founding settlers chose "Ra'anana" as its official name. By the War of Independence, it was a village of 3,000 residents. 
By the late 1960s, it had a population of 8,500 spanning an area of . In the 1980s Ra'anana was declared a city.
Demographics.
Ra'anana has both a large English-speaking population and a Spanish-speaking population, mainly from Argentina. The number of French immigrants is also on the rise.
Though the majority of Ra'anana residents are secular, there is a sizeable religious community, mainly consisting of Modern Orthodox Jews, many of whom are immigrants from the US, UK, South Africa and France. There are nearly 100 synagogues in Ra'anana, ranging from small minyanim to large edifices, and including a wide range of traditions, including Progressive (Reform), Sefaradi, Ashkenazi, Yemenite and even Afghani, Libyan synagogues. Many of these synagogues cater to specific immigrant groups. There is also a small Hasidic community of Clevelander Hasidim, led by the Clevelander Rebbe of Ra'anana, Rabbi Yitzchok Rosenbaum. The orthodox chief rabbi of the city is Rabbi Yitzhak Peretz.
Industry and commerce.
Ra'anana has an industrial zone in the north of the city, which is home to Renanim shopping mall and many high-tech companies, including Emblaze, Hewlett-Packard, NICE Systems, SAP, NCR Corporation (formerly Retalix), Comverse, Red Hat, Waze (prior to Google acquisition) and Texas Instruments. In addition, Microsoft's head office in Israel and Amdocs are located in an office complex at the eastern edge of the city, close to Ra'anana Junction, where Highway 4 meets Ahuza Street, Ra'anana's main boulevard. Ahuza Street runs through the city from east to west and is lined with shops, restaurants and a cultural center.
Education.
Ra'anana has 12 elementary schools, 10 middle schools and 8 high-schools.
Educational programs for gifted students start in the third grade. A program for the encouragement of girls to study technological subjects has been developed as well as a technology-focused leadership development and information management program, the first of its kind in Israel. The program, created in conjunction with "Ness Technologies", uses advanced technology as a catalyst for developing skills.
Ra'anana has developed supplementary education programs for the afternoon and evening hours, which meet the needs of thousands of children, aged 5–18. These programs foster creativity, promote social involvement and cultivate leadership skills. The supplementary education projects include over 20 "Batei Talmid" citywide extracurricular programs, an afternoon daycare program, and music, dance, art and science centers. Other programs include summer camps and summer activities, university for youth, dance troupes, the Children’s Parliament, an acting school, a school for the performing arts, and gifted children programs, that serve as a model for many other cities.
Ra'anana is home to the Open University of Israel and Ra'anana College.
Parks and museums.
The park of Ra'anana is the largest urban park in the Sharon region. It offers walking and bike paths, sports fields, a zoo and children's petting corner and a lake in a clover shape reminiscent of Ra'anana's coat of arms. There are two fountains in the lake and pedestrians can cross over it on the bridge. The lake is surrounded by special gardens, including the Seven Species garden, and shaded walking paths. There is also a restaurant and small art gallery. The Founders Museum presents the story of Ra'anana's original settlers, from the arrival of the Ahuza Alef-New York Association until Ra'anana achieved local council status in 1936.
Ra'anana Park Amphitheatre has been the venue for musical acts such as Backstreet Boys, Evanescence, Lauryn Hill, Tori Amos, Chick Corea, Ian Anderson, Ziggy Marley, The Cranberries, and Pet Shop Boys.
Hospitals and medical facilities.
Ra'anana is home to the Loewenstein Hospital Rehabilitation Center. Loewenstein was established in 1958 and is the only rehabilitation hospital operated by Clalit Health Services, Israel's largest health care provider. Its current multi-floor building is situated in a large gardened area and accommodates 240 rehabilitative beds for short and long term hospital care. 
As a national rehabilitative center, patients are admitted from all parts of the country, all health funds, from the Ministry of Defense, the Ministry of Health, and from general hospitals and clinics, both from Israel and overseas.
Sports.
The main soccer club of the city is Hapoel Ra'anana. In basketball, the city is represented by Maccabi Ra'anana who play in the National League.
The Ra'anana Roosters are the local rugby team, and the area is a center of rugby union in Israel, with the Israel Rugby Union being based there.
With a large population of American expatriates, the Ra'anana Express are an inaugural team in the Israel Baseball League.
Twin towns — Sister cities.
Ra'anana has the following twin cities with:

</doc>
<doc id="59962" url="https://en.wikipedia.org/wiki?curid=59962" title="Discrete cosine transform">
Discrete cosine transform

A discrete cosine transform (DCT) expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. DCTs are important to numerous applications in science and engineering, from lossy compression of audio (e.g. MP3) and images (e.g. JPEG) (where small high-frequency components can be discarded), to spectral methods for the numerical solution of partial differential equations. The use of cosine rather than sine functions is critical for compression, since it turns out (as described below) that fewer cosine functions are needed to approximate a typical signal, whereas for differential equations the cosines express a particular choice of boundary conditions.
In particular, a DCT is a Fourier-related transform similar to the discrete Fourier transform (DFT), but using only real numbers. DCTs are equivalent to DFTs of roughly twice the length, operating on real data with even symmetry (since the Fourier transform of a real and even function is real and even), where in some variants the input and/or output data are shifted by half a sample. There are eight standard DCT variants, of which four are common.
The most common variant of discrete cosine transform is the type-II DCT, which is often called simply "the DCT". Its inverse, the type-III DCT, is correspondingly often called simply "the inverse DCT" or "the IDCT". Two related transforms are the discrete sine transform (DST), which is equivalent to a DFT of real and "odd" functions, and the modified discrete cosine transform (MDCT), which is based on a DCT of "overlapping" data.
Applications.
The DCT, and in particular the DCT-II, is often used in signal and image processing, especially for lossy compression, because it has a strong "energy compaction" property: in typical applications, most of the signal information tends to be concentrated in a few low-frequency components of the DCT. For strongly correlated Markov processes, the DCT can approach the compaction efficiency of the Karhunen-Loève transform (which is optimal in the decorrelation sense). As explained below, this stems from the boundary conditions implicit in the cosine functions.
A related transform, the "modified" discrete cosine transform, or MDCT (based on the DCT-IV), is used in AAC, Vorbis, WMA, and MP3 audio compression.
DCTs are also widely employed in solving partial differential equations by spectral methods, where the different variants of the DCT correspond to slightly different even/odd boundary conditions at the two ends of the array.
DCTs are also closely related to Chebyshev polynomials, and fast DCT algorithms (below) are used in Chebyshev approximation of arbitrary functions by series of Chebyshev polynomials, for example in Clenshaw–Curtis quadrature.
JPEG.
The DCT is used in JPEG image compression, MJPEG, MPEG, DV, Daala, and Theora video compression. There, the two-dimensional DCT-II of formula_1 blocks are computed and the results are quantized and entropy coded. In this case, formula_2 is typically 8 and the DCT-II formula is applied to each row and column of the block. The result is an 8 × 8 transform coefficient array in which the formula_3 element (top-left) is the DC (zero-frequency) component and entries with increasing vertical and horizontal index values represent higher vertical and horizontal spatial frequencies.
Informal overview.
Like any Fourier-related transform, discrete cosine transforms (DCTs) express a function or a signal in terms of a sum of sinusoids with different frequencies and amplitudes. Like the discrete Fourier transform (DFT), a DCT operates on a function at a finite number of discrete data points. The obvious distinction between a DCT and a DFT is that the former uses only cosine functions, while the latter uses both cosines and sines (in the form of complex exponentials). However, this visible difference is merely a consequence of a deeper distinction: a DCT implies different boundary conditions from the DFT or other related transforms.
The Fourier-related transforms that operate on a function over a finite domain, such as the DFT or DCT or a Fourier series, can be thought of as implicitly defining an "extension" of that function outside the domain. That is, once you write a function formula_4 as a sum of sinusoids, you can evaluate that sum at any formula_5, even for formula_5 where the original formula_4 was not specified. The DFT, like the Fourier series, implies a periodic extension of the original function. A DCT, like a cosine transform, implies an even extension of the original function.
However, because DCTs operate on "finite", "discrete" sequences, two issues arise that do not apply for the continuous cosine transform. First, one has to specify whether the function is even or odd at "both" the left and right boundaries of the domain (i.e. the min-"n" and max-"n" boundaries in the definitions below, respectively). Second, one has to specify around "what point" the function is even or odd. In particular, consider a sequence "abcd" of four equally spaced data points, and say that we specify an even "left" boundary. There are two sensible possibilities: either the data are even about the sample "a", in which case the even extension is "dcbabcd", or the data are even about the point "halfway" between "a" and the previous point, in which case the even extension is "dcbaabcd" ("a" is repeated).
These choices lead to all the standard variations of DCTs and also discrete sine transforms (DSTs). 
Each boundary can be either even or odd (2 choices per boundary) and can be symmetric about a data point or the point halfway between two data points (2 choices per boundary), for a total of 2 × 2 × 2 × 2 = 16 possibilities. Half of these possibilities, those where the "left" boundary is even, correspond to the 8 types of DCT; the other half are the 8 types of DST.
These different boundary conditions strongly affect the applications of the transform and lead to uniquely useful properties for the various DCT types. Most directly, when using Fourier-related transforms to solve partial differential equations by spectral methods, the boundary conditions are directly specified as a part of the problem being solved. Or, for the MDCT (based on the type-IV DCT), the boundary conditions are intimately involved in the MDCT's critical property of time-domain aliasing cancellation. In a more subtle fashion, the boundary conditions are responsible for the "energy compactification" properties that make DCTs useful for image and audio compression, because the boundaries affect the rate of convergence of any Fourier-like series.
In particular, it is well known that any discontinuities in a function reduce the rate of convergence of the Fourier series, so that more sinusoids are needed to represent the function with a given accuracy. The same principle governs the usefulness of the DFT and other transforms for signal compression: the smoother a function is, the fewer terms in its DFT or DCT are required to represent it accurately, and the more it can be compressed. (Here, we think of the DFT or DCT as approximations for the Fourier series or cosine series of a function, respectively, in order to talk about its "smoothness".) However, the implicit periodicity of the DFT means that discontinuities usually occur at the boundaries: any random segment of a signal is unlikely to have the same value at both the left and right boundaries. (A similar problem arises for the DST, in which the odd left boundary condition implies a discontinuity for any function that does not happen to be zero at that boundary.) In contrast, a DCT where "both" boundaries are even "always" yields a continuous extension at the boundaries (although the slope is generally discontinuous). This is why DCTs, and in particular DCTs of types I, II, V, and VI (the types that have two even boundaries) generally perform better for signal compression than DFTs and DSTs. In practice, a type-II DCT is usually preferred for such applications, in part for reasons of computational convenience.
Formal definition.
Formally, the discrete cosine transform is a linear, invertible function formula_8 (where formula_9 denotes the set of real numbers), or equivalently an invertible "N" × "N" square matrix. There are several variants of the DCT with slightly modified definitions. The "N" real numbers "x"0, ..., "x""N"-1 are transformed into the "N" real numbers "X"0, ..., "X""N"-1 according to one of the formulas:
DCT-I.
Some authors further multiply the "x"0 and "x""N"-1 terms by √2, and correspondingly multiply the "X"0 and "X""N"-1 terms by 1/√2. This makes the DCT-I matrix orthogonal, if one further multiplies by an overall scale factor of formula_11, but breaks the direct correspondence with a real-even DFT.
The DCT-I is exactly equivalent (up to an overall scale factor of 2), to a DFT of formula_12 real numbers with even symmetry. For example, a DCT-I of "N"=5 real numbers "abcde" is exactly equivalent to a DFT of eight real numbers "abcdedcb" (even symmetry), divided by two. (In contrast, DCT types II-IV involve a half-sample shift in the equivalent DFT.)
Note, however, that the DCT-I is not defined for "N" less than 2. (All other DCT types are defined for any positive "N".)
Thus, the DCT-I corresponds to the boundary conditions: "x""n" is even around "n"=0 and even around "n"="N"-1; similarly for "X""k".
DCT-II.
The DCT-II is probably the most commonly used form, and is often simply referred to as "the DCT".
This transform is exactly equivalent (up to an overall scale factor of 2) to a DFT of formula_14 real inputs of even symmetry where the even-indexed elements are zero. That is, it is half of the DFT of the formula_14 inputs formula_16, where formula_17, formula_18 for formula_19, formula_20, and formula_21 for formula_22.
Some authors further multiply the "X"0 term by 1/√2 and multiply the resulting matrix by an overall scale factor of formula_23 (see below for the corresponding change in DCT-III). This makes the DCT-II matrix orthogonal, but breaks the direct correspondence with a real-even DFT of half-shifted input. This is the normalization used by Matlab, for example. In many applications, such as JPEG, the scaling is arbitrary because scale factors can be combined with a subsequent computational step (e.g. the quantization step in JPEG), and a scaling that can be chosen that allows the DCT to be computed with fewer multiplications.
The DCT-II implies the boundary conditions: "x""n" is even around "n"=-1/2 and even around "n"="N"-1/2; "X""k" is even around "k"=0 and odd around "k"="N".
DCT-III.
Because it is the inverse of DCT-II (up to a scale factor, see below), this form is sometimes simply referred to as "the inverse DCT" ("IDCT").
Some authors divide the "x"0 term by √2 instead of by 2 (resulting in an overall "x"0/√2 term) and multiply the resulting matrix by an overall scale factor of formula_23 (see above for the corresponding change in DCT-II), so that the DCT-II and DCT-III are transposes of one another. This makes the DCT-III matrix orthogonal, but breaks the direct correspondence with a real-even DFT of half-shifted output.
The DCT-III implies the boundary conditions: "x""n" is even around "n"=0 and odd around "n"="N"; "X""k" is even around "k"=-1/2 and odd around "k"="N"-1/2.
DCT-IV.
The DCT-IV matrix becomes orthogonal (and thus, being clearly symmetric, its own inverse) if one further multiplies by an overall scale factor of formula_23.
A variant of the DCT-IV, where data from different transforms are "overlapped", is called the modified discrete cosine transform (MDCT) (Malvar, 1992).
The DCT-IV implies the boundary conditions: "x""n" is even around "n"=-1/2 and odd around "n"="N"-1/2; similarly for "X""k".
DCT V-VIII.
DCTs of types I-IV treat both boundaries consistently regarding the point of symmetry: they are even/odd around either a data point for both boundaries or halfway between two data points for both boundaries. By contrast, DCTs of types V-VIII imply boundaries that are even/odd around a data point for one boundary and halfway between two data points for the other boundary.
In other words,
DCT types I-IV are equivalent to real-even DFTs of even order (regardless of whether "N" is even or odd), since the corresponding DFT is of length 2("N"−1) (for DCT-I) or 4"N" (for DCT-II/III) or 8"N" (for DCT-IV). The four additional types of discrete cosine transform (Martucci, 1994) correspond essentially to real-even DFTs of logically odd order, which have factors of "N"±½ in the denominators of the cosine arguments.
However, these variants seem to be rarely used in practice. One reason, perhaps, is that FFT algorithms for odd-length DFTs are generally more complicated than FFT algorithms for even-length DFTs (e.g. the simplest radix-2 algorithms are only for even lengths), and this increased intricacy carries over to the DCTs as described below.
Inverse transforms.
Using the normalization conventions above, the inverse of DCT-I is DCT-I multiplied by 2/("N"-1). The inverse of DCT-IV is DCT-IV multiplied by 2/"N". The inverse of DCT-II is DCT-III multiplied by 2/"N" and vice versa.
Like for the DFT, the normalization factor in front of these transform definitions is merely a convention and differs between treatments. For example, some authors multiply the transforms by formula_23 so that the inverse does not require any additional multiplicative factor. Combined with appropriate factors of √2 (see above), this can be used to make the transform matrix orthogonal.
Multidimensional DCTs.
Multidimensional variants of the various DCT types follow straightforwardly from the one-dimensional definitions: they are simply a separable product (equivalently, a composition) of DCTs along each dimension.
For example, a two-dimensional DCT-II of an image or a matrix is simply the one-dimensional DCT-II, from above, performed along the rows and then along the columns (or vice versa). That is, the 2D DCT-II is given by the formula (omitting normalization and other scale factors, as above):
Technically, computing a two- (or multi-) dimensional DCT by sequences of one-dimensional DCTs along each dimension is known as a "row-column" algorithm (after the two-dimensional case). As with multidimensional FFT algorithms, however, there exist other methods to compute the same thing while performing the computations in a different order (i.e. interleaving/combining the algorithms for the different dimensions).
The inverse of a multi-dimensional DCT is just a separable product of the inverse(s) of the corresponding one-dimensional DCT(s) (see above), e.g. the one-dimensional inverses applied along one dimension at a time in a row-column algorithm.
The image to the right shows combination of horizontal and vertical frequencies for an 8 x 8 (formula_30) two-dimensional DCT.
Each step from left to right and top to bottom is an increase in frequency by 1/2 cycle.
For example, moving right one from the top-left square yields a half-cycle increase in the horizontal frequency. Another move to the right yields two half-cycles. A move down yields two half-cycles horizontally and a half-cycle vertically. The source data (8x8) is transformed to a linear combination of these 64 frequency squares.
Computation.
Although the direct application of these formulas would require O("N"2) operations, it is possible to compute the same thing with only O("N" log "N") complexity by factorizing the computation similarly to the fast Fourier transform (FFT). One can also compute DCTs via FFTs combined with O("N") pre- and post-processing steps. In general, O("N" log "N") methods to compute DCTs are known as fast cosine transform (FCT) algorithms.
The most efficient algorithms, in principle, are usually those that are specialized directly for the DCT, as opposed to using an ordinary FFT plus O("N") extra operations (see below for an exception). However, even "specialized" DCT algorithms (including all of those that achieve the lowest known arithmetic counts, at least for power-of-two sizes) are typically closely related to FFT algorithms—since DCTs are essentially DFTs of real-even data, one can design a fast DCT algorithm by taking an FFT and eliminating the redundant operations due to this symmetry. This can even be done automatically (Frigo & Johnson, 2005). Algorithms based on the Cooley–Tukey FFT algorithm are most common, but any other FFT algorithm is also applicable. For example, the Winograd FFT algorithm leads to minimal-multiplication algorithms for the DFT, albeit generally at the cost of more additions, and a similar algorithm was proposed by Feig & Winograd (1992) for the DCT. Because the algorithms for DFTs, DCTs, and similar transforms are all so closely related, any improvement in algorithms for one transform will theoretically lead to immediate gains for the other transforms as well .
While DCT algorithms that employ an unmodified FFT often have some theoretical overhead compared to the best specialized DCT algorithms, the former also have a distinct advantage: highly optimized FFT programs are widely available. Thus, in practice, it is often easier to obtain high performance for general lengths "N" with FFT-based algorithms. (Performance on modern hardware is typically not dominated simply by arithmetic counts, and optimization requires substantial engineering effort.) Specialized DCT algorithms, on the other hand, see widespread use for transforms of small, fixed sizes such as the formula_31 DCT-II used in JPEG compression, or the small DCTs (or MDCTs) typically used in audio compression. (Reduced code size may also be a reason to use a specialized DCT for embedded-device applications.)
In fact, even the DCT algorithms using an ordinary FFT are sometimes equivalent to pruning the redundant operations from a larger FFT of real-symmetric data, and they can even be optimal from the perspective of arithmetic counts. For example, a type-II DCT is equivalent to a DFT of size formula_14 with real-even symmetry whose even-indexed elements are zero. One of the most common methods for computing this via an FFT (e.g. the method used in FFTPACK and FFTW) was described by and , and this method in hindsight can be seen as one step of a radix-4 decimation-in-time Cooley–Tukey algorithm applied to the "logical" real-even DFT corresponding to the DCT II. (The radix-4 step reduces the size formula_14 DFT to four size-formula_2 DFTs of real data, two of which are zero and two of which are equal to one another by the even symmetry, hence giving a single size-formula_2 FFT of real data plus formula_36 butterflies.) Because the even-indexed elements are zero, this radix-4 step is exactly the same as a split-radix step; if the subsequent size-formula_2 real-data FFT is also performed by a real-data split-radix algorithm (as in ), then the resulting algorithm actually matches what was long the lowest published arithmetic count for the power-of-two DCT-II (formula_38 real-arithmetic operations). So, there is nothing intrinsically bad about computing the DCT via an FFT from an arithmetic perspective—it is sometimes merely a question of whether the corresponding FFT algorithm is optimal. (As a practical matter, the function-call overhead in invoking a separate FFT routine might be significant for small formula_2, but this is an implementation rather than an algorithmic question since it can be solved by unrolling/inlining.)
Example of IDCT.
Consider this 8x8 grayscale image of capital letter A.
DCT of the image.
Each basis function is multiplied by its coefficient and then this product is added to the final image.

</doc>
<doc id="59964" url="https://en.wikipedia.org/wiki?curid=59964" title="Senufo people">
Senufo people

The Senufo (the francophone spelling Senoufo is commonly used) are an ethnolinguistic group composed of diverse subgroups of Gur-speaking people living in an area spanning from southern Mali and the extreme western corner of Burkina Faso to Katiola in Ivory Coast. One group, the Nafana, is found in north-western Ghana. The Senufo number somewhere between 1.5 and 2.7 million and speak the various Senufo languages. Korhogo, an ancient town in northern Ivory Coast dating from the 13th century, is the capital of the Senufo people. The Senufo are predominantly an agricultural people cultivating millet, yams, peanut, and rice.
Daily life for the Senufo people revolves around the religious rituals that enable them to placate the deities they respect and fear through means of divination practices and the wearing of specially crafted brass jewelry. The Senufo to employ the Fo bracelet, which contains one of the culture’s most prominent designs, a python, in a variety of purposes to suit the spiritual and aesthetic needs of the society. The Sandogo is an authoritative women’s social order responsible for sustaining positive relationships with the spiritual world through divination and for protecting the purity of each kinship group. The Sandobele are diviners within the Sandogo society who diagnose and resolve issues within the community.
External links.
__NOTOC__

</doc>
<doc id="59969" url="https://en.wikipedia.org/wiki?curid=59969" title="140 BC">
140 BC

__NOTOC__
Year 140 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Sapiens and Caepio (or, less frequently, year 614 "Ab urbe condita"). The denomination 140 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="59970" url="https://en.wikipedia.org/wiki?curid=59970" title="139 BC">
139 BC

__NOTOC__
Year 139 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Piso and Laenas (or, less frequently, year 615 "Ab urbe condita"). The denomination 139 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Astronomy.
</onlyinclude>

</doc>
<doc id="59971" url="https://en.wikipedia.org/wiki?curid=59971" title="138 BC">
138 BC

__NOTOC__
Year 138 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Serapio and Callaicus (or, less frequently, year 616 "Ab urbe condita"). The denomination 138 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Arts and sciences.
</onlyinclude>

</doc>
<doc id="59972" url="https://en.wikipedia.org/wiki?curid=59972" title="137 BC">
137 BC

__NOTOC__
Year 137 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Porcina and Mancinus (or, less frequently, year 617 "Ab urbe condita"). The denomination 137 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59973" url="https://en.wikipedia.org/wiki?curid=59973" title="127 BC">
127 BC

__NOTOC__
Year 127 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Ravilla and Cinna (or, less frequently, year 627 "Ab urbe condita"). The denomination 127 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Parthia.
</onlyinclude>

</doc>
<doc id="59974" url="https://en.wikipedia.org/wiki?curid=59974" title="123 BC">
123 BC

__NOTOC__
Year 123 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Balearicus and Flamininus (or, less frequently, year 631 "Ab urbe condita"). The denomination 123 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59975" url="https://en.wikipedia.org/wiki?curid=59975" title="143 BC">
143 BC

__NOTOC__
Year 143 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Pulcher and Macedonicus (or, less frequently, year 611 "Ab urbe condita"). The denomination 143 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59977" url="https://en.wikipedia.org/wiki?curid=59977" title="145 BC">
145 BC

__NOTOC__
Year 145 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Ameilianus and Mancinus (or, less frequently, year 609 "Ab urbe condita"). The denomination 145 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Astronomy.
</onlyinclude>

</doc>
<doc id="59978" url="https://en.wikipedia.org/wiki?curid=59978" title="125 BC">
125 BC

__NOTOC__
Year 125 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Hypsaeus and Flaccus (or, less frequently, year 629 "Ab urbe condita"). The denomination 125 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59979" url="https://en.wikipedia.org/wiki?curid=59979" title="Roman surface">
Roman surface

The Roman surface or Steiner surface is a self-intersecting mapping of the real projective plane into three-dimensional space, with an unusually high degree of symmetry. This mapping is not an immersion of the projective plane; however, the figure resulting from removing six singular points is one. It's name arises because was discovered by Jakob Steiner when he was in Rome in 1844.
The simplest construction is as the image of a sphere centered at the origin under the map "f"("x","y","z") = ("yz","xz","xy"). This gives an implicit formula of
2"y"2 + "y"2"z"2 + "x"2"z"2 − "r"2"xyz" = 0 -->
Also, taking a parametrization of the sphere in terms of longitude (θ) and latitude (φ), gives parametric equations for the Roman surface as follows:
The origin is a triple point, and each of the "xy"-, "yz"-, and "xz"-planes are tangential to the surface there. The other places of self-intersection are double points, defining segments along each coordinate axis which terminate in six pinch points. The entire surface has tetrahedral symmetry. It is a particular type (called type 1) of Steiner surface, that is, a 3-dimensional linear projection of the Veronese surface.
Derivation of implicit formula.
For simplicity we consider only the case "r" = 1. Given the sphere defined by the points ("x", "y", "z") such that
we apply to these points the transformation "T" defined by formula_3 say.
But then we have
and so formula_5 as desired.
Conversely, suppose we are given ("U", "V", "W") satisfying
(*) formula_6
We prove that there exists ("x","y","z") such that
(**) formula_2
for which formula_8
with one exception: In case 3.b. below, we show this cannot be proved.
1. In the case where none of "U", "V", "W" is 0, we can set
It is easy to use (*) to confirm that (**) holds for "x", "y", "z" defined this way.
2. Suppose that "W" is 0. From (*) this implies formula_10
and hence at least one of "U", "V" must be 0 also. This shows that is it impossible for exactly one of "U", "V", "W" to be 0.
3. Suppose that exactly two of "U", "V", "W" are 0. Without loss of generality we assume
(***)formula_11
It follows that formula_12
a. In the subcase where
if we determine "x" and "y" by
and formula_18
this ensures that (*) holds. It is easy to verify that formula_19
and hence choosing the signs of "x" and "y" appropriately will guarantee formula_20
Since also formula_21
this shows that this subcase leads to the desired converse.
b. In this remaining subcase of the case 3., we have formula_22
Since formula_23
it is easy to check that formula_24
and thus in this case, where formula_25
there is no ("x", "y", "z") satisfying formula_26
Hence the solutions ("U", 0, 0) of the equation (*) with formula_27
and likewise, (0, "V", 0) with formula_28
and (0, 0, "W") with formula_29
(each of which is a noncompact portion of a coordinate axis, in two pieces) do not correspond to any point on the Roman surface.
4. If ("U", "V", "W") is the point (0, 0, 0), then if any two of "x", "y", "z" are zero and the third one has absolute value 1, clearly formula_30 as desired.
This covers all possible cases.
Derivation of parametric equations.
Let a sphere have radius "r", longitude "φ", and latitude "θ". Then its parametric equations are
Then, applying transformation "T" to all the points on this sphere yields
which are the points on the Roman surface. Let "φ" range from 0 to 2π, and let "θ" range from 0 to "π/2".
Relation to the real projective plane.
The sphere, before being transformed, is not homeomorphic to the real projective plane, "RP2". But the sphere centered at the origin has this property, that if point "(x,y,z)" belongs to the sphere, then so does the antipodal point "(-x,-y,-z)" and these two points are different: they lie on opposite sides of the center of the sphere.
The transformation "T" converts both of these antipodal points into the same point,
Since this is true of all points of S2, then it is clear that the Roman surface is a continuous image of a "sphere modulo antipodes". Because some distinct pairs of antipodes are all taken to identical points in the Roman surface, it is not homeomorphic to "RP2", but is instead a quotient of the real projective plane "RP2 = S2 / (x~-x)". Furthermore, the map T (above) from S2 to this quotient has the special property that it is locally injective away from six pairs of antipodal points. Or from RP2 the resulting map making this an immersion of RP2 — minus six points — into 3-space.
Structure of the Roman surface.
The Roman surface has four bulbous "lobes", each one on a different corner of a tetrahedron.
A Roman surface can be constructed by splicing together three hyperbolic paraboloids and then smoothing out the edges as necessary so that it will fit a desired shape (e.g. parametrization).
Let there be these three hyperbolic paraboloids:
These three hyperbolic paraboloids intersect externally along the six edges of a tetrahedron and internally along the three axes. The internal intersections are loci of double points. The three loci of double points: "x" = 0, "y" = 0, and "z" = 0, intersect at a triple point at the origin.
For example, given "x" = "yz" and "y" = "zx", the second paraboloid is equivalent to "x" = "y"/"z". Then
and either "y" = 0 or "z"2 = 1 so that "z" = ±1. Their two external intersections are
Likewise, the other external intersections are
Let us see the pieces being put together. Join the paraboloids "y" = "xz" and "x" = "yz". The result is shown in Figure 1.
The paraboloid "y = x z" is shown in blue and orange. The paraboloid "x = y z" is shown in cyan and purple. In the image the paraboloids are seen to intersect along the "z = 0" axis. If the paraboloids are extended, they should also be seen to intersect along the lines
The two paraboloids together look like a pair of orchids joined back-to-back.
Now run the third hyperbolic paraboloid, "z" = "xy", through them. The result is shown in Figure 2.
On the west-southwest and east-northeast directions in Figure 2 there are a pair of openings. These openings are lobes and need to be closed up. When the openings are closed up, the result is the Roman surface shown in Figure 3.
A pair of lobes can be seen in the West and East directions of Figure 3. Another pair of lobes are hidden underneath the third ("z" = "xy") paraboloid and lie in the North and South directions.
If the three intersecting hyperbolic paraboloids are drawn far enough that they intersect along the edges of a tetrahedron, then the result is as shown in Figure 4.
One of the lobes is seen frontally—head on—in Figure 4. The lobe can be seen to be one of the four corners of the tetrahedron.
If the continuous surface in Figure 4 has its sharp edges rounded out—smoothed out—then the result is the Roman surface in Figure 5.
One of the lobes of the Roman surface is seen frontally in Figure 5, and its bulbous – balloon-like—shape is evident.
If the surface in Figure 5 is turned around 180 degrees and then turned upside down, the result is as shown in Figure 6.
Figure 6 shows three lobes seen sideways. Between each pair of lobes there is a locus of double points corresponding to a coordinate axis. The three loci intersect at a triple point at the origin. The fourth lobe is hidden and points in the direction directly opposite from the viewer. The Roman surface shown at the top of this article also has three lobes in sideways view.
One-sidedness.
The Roman surface is non-orientable, i.e. one-sided. This is not quite obvious. To see this, look again at Figure 3.
Imagine an ant on top of the "third" hyperbolic paraboloid, "z = x y". Let this ant move North. As it moves, it will pass through the other two paraboloids, like a ghost passing through a wall. These other paraboloids only seem like obstacles due to the self-intersecting nature of the immersion. Let the ant ignore all double and triple points and pass right through them. So the ant moves to the North and falls off the edge of the world, so to speak. It now finds itself on the northern lobe, hidden underneath the third paraboloid of Figure 3. The ant is standing upside-down, on the "outside" of the Roman surface.
Let the ant move towards the Southwest. It will climb a slope (upside-down) until it finds itself "inside" the Western lobe. Now let the ant move in a Southeastern direction along the inside of the Western lobe towards the "z = 0" axis, always above the "x-y" plane. As soon as it passes through the "z = 0" axis the ant will be on the "outside" of the Eastern lobe, standing rightside-up.
Then let it move Northwards, over "the hill", then towards the Northwest so that it starts sliding down towards the "x = 0" axis. As soon as the ant crosses this axis it will find itself "inside" the Northern lobe, standing right side up. Now let the ant walk towards the North. It will climb up the wall, then along the "roof" of the Northern lobe. The ant is back on the third hyperbolic paraboloid, but this time under it and standing upside-down. (Compare with Klein bottle.)
Double, triple, and pinching points.
The Roman surface has four "lobes". The boundaries of each lobe are a set of three lines of double points. Between each pair of lobes there is a line of double points. The surface has a total of three lines of double points, which lie (in the parametrization given earlier) on the coordinate axes. The three lines of double points intersect at a triple point which lies on the origin. The triple point cuts the lines of double points into a pair of half-lines, and each half-line lies between a pair of lobes. One might expect from the preceding statements that there could be up to eight lobes, one in each octant of space which has been divided by the coordinate planes. But the lobes occupy alternating octants: four octants are empty and four are occupied by lobes.
If the Roman surface were to be inscribed inside the tetrahedron with least possible volume, one would find that each edge of the tetrahedron is tangent to the Roman surface at a point, and that each of these six points happens to be a "Whitney singularity". These singularities, or pinching points, all lie at the edges of the three lines of double points, and they are defined by this property: that there is no plane tangent to any surface at the singularity.

</doc>
<doc id="59980" url="https://en.wikipedia.org/wiki?curid=59980" title="142 BC">
142 BC

__NOTOC__
Year 142 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Calvus and Servilianus (or, less frequently, year 612 "Ab urbe condita"). The denomination 142 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Judea.
</onlyinclude>

</doc>
<doc id="59981" url="https://en.wikipedia.org/wiki?curid=59981" title="144 BC">
144 BC

__NOTOC__
Year 144 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Galba and Cotta (or, less frequently, year 610 "Ab urbe condita"). The denomination 144 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Parthia.
</onlyinclude>

</doc>
<doc id="59982" url="https://en.wikipedia.org/wiki?curid=59982" title="147 BC">
147 BC

__NOTOC__
Year 147 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Aemilianus and Drusus (or, less frequently, year 607 "Ab urbe condita"). The denomination 147 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Greece.
</onlyinclude>

</doc>
<doc id="59983" url="https://en.wikipedia.org/wiki?curid=59983" title="148 BC">
148 BC

__NOTOC__
Year 148 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Magnus and Caesoninus (or, less frequently, year 606 "Ab urbe condita"). The denomination 148 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59984" url="https://en.wikipedia.org/wiki?curid=59984" title="151 BC">
151 BC

__NOTOC__
Year 151 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Lucullus and Albinus (or, less frequently, year 603 "Ab urbe condita"). The denomination 151 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
India.
</onlyinclude>

</doc>
<doc id="59985" url="https://en.wikipedia.org/wiki?curid=59985" title="152 BC">
152 BC

__NOTOC__
Year 152 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Marcellus and Flaccus (or, less frequently, year 602 "Ab urbe condita"). The denomination 152 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Seleucid Empire.
</onlyinclude>

</doc>
<doc id="59986" url="https://en.wikipedia.org/wiki?curid=59986" title="122 BC">
122 BC

__NOTOC__
Year 122 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Ahenobarbus and Fannius (or, less frequently, year 632 "Ab urbe condita"). The denomination 122 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="59987" url="https://en.wikipedia.org/wiki?curid=59987" title="124 BC">
124 BC

__NOTOC__
Year 124 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Longinus and Calvinus (or, less frequently, year 630 "Ab urbe condita"). The denomination 124 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Egypt.
</onlyinclude>

</doc>
