<doc id="50326" url="https://en.wikipedia.org/wiki?curid=50326" title="Cognitive neuroscience">
Cognitive neuroscience

Cognitive neuroscience is an academic field concerned with the scientific study of biological substrates underlying cognition, with a specific focus on the neural substrates of mental processes. It addresses the questions of how psychological/cognitive functions are produced by neural circuits in the brain. Cognitive neuroscience is a branch of both psychology and neuroscience, overlapping with disciplines such as physiological psychology, cognitive psychology, and neuropsychology. Cognitive neuroscience relies upon theories in cognitive science coupled with evidence from neuropsychology, and computational modeling.
Due to its multidisciplinary nature, cognitive neuroscientists may have various backgrounds. Other than the associated disciplines just mentioned, cognitive neuroscientists may have backgrounds in neurobiology, bioengineering, psychiatry, neurology, physics, computer science, linguistics, philosophy, and mathematics.
Methods employed in cognitive neuroscience include experimental paradigms from psychophysics and cognitive psychology, functional neuroimaging, electrophysiology, cognitive genomics, and behavioral genetics. Studies of patients with cognitive deficits due to brain lesions constitute an important aspect of cognitive neuroscience. Theoretical approaches include computational neuroscience and cognitive psychology.
Cognitive neuroscience can look at the effects of damage to the brain and subsequent changes in the thought processes due to changes in neural circuitry resulting from the damage. Also, cognitive abilities based on brain development are studied and examined under the subfield of developmental cognitive neuroscience.
Historical origins.
Cognitive neuroscience is an interdisciplinary area of study that has emerged from many other fields, perhaps most significantly neuroscience, psychology, and computer science. There were several stages in these disciplines that changed the way researchers approached their investigations and that led to the field becoming fully established.
Although the task of cognitive neuroscience is to describe how the brain creates the mind, historically it has progressed by investigating how a certain area of the brain supports a given mental faculty. However, early efforts to subdivide the brain proved to be problematic. The phrenologist movement failed to supply a scientific basis for its theories and has since been rejected. The aggregate field view, meaning that all areas of the brain participated in all behavior, was also rejected as a result of brain mapping, which began with Hitzig and Fritsch’s experiments and eventually developed through methods such as positron emission tomography (PET) and functional magnetic resonance imaging (fMRI). Gestalt theory, neuropsychology, and the cognitive revolution were major turning points in the creation of cognitive neuroscience as a field, bringing together ideas and techniques that enabled researchers to make more links between behavior and its neural substrates.
Origins in philosophy.
Philosophers have always been interested in the mind. For example, Aristotle thought the brain was the body’s cooling system and the capacity for intelligence was located in the heart. It has been suggested that the first person to believe otherwise was the Roman physician Galen in the second century AD, who declared that the brain was the source of mental activity, although this has also been accredited to Alcmaeon. However, Galen believed that personality and emotion were not generated by the brain, but rather by other organs. Andreas Vesalius, an anatomist and physician, was the first to believe that the brain and the nervous system are the center of the mind and emotion. Psychology, a major contributing field to cognitive neuroscience, emerged from philosophical reasoning about the mind.
19th century.
Phrenology.
One of the predecessors to cognitive neuroscience was phrenology, a pseudoscientific approach that claimed that behavior could be determined by the shape of the scalp. In the early 19th century, Franz Joseph Gall and J. G. Spurzheim believed that the human brain was localized into approximately 35 different sections. In his book, The Anatomy and Physiology of the Nervous System in General, and of the Brain in Particular, Gall claimed that a larger bump in one of these areas meant that that area of the brain was used more frequently by that person. This theory gained significant public attention, leading to the publication of phrenology journals and the creation of phrenometers, which measured the bumps on a human subject's head. While phrenology remained a fixture at fairs and carnivals, it did not enjoy wide acceptance within the scientific community. The major criticism of phrenology is that researchers were not able to test theories empirically.
Localizationist view.
The localizationist view was concerned with mental abilities being localized to specific areas of the brain rather than on what the characteristics of the abilities were and how to measure them. Studies performed in Europe, such as those of John Hughlings Jackson, supported this view. Jackson studied patients with brain damage, particularly those with epilepsy. He discovered that the epileptic patients often made the same clonic and tonic movements of muscle during their seizures, leading Jackson to believe that they must be occurring in the same place every time. Jackson proposed that specific functions were localized to specific areas of the brain, which was critical to future understanding of the brain lobes.
Aggregate field view.
According to the aggregate field view, all areas of the brain participate in every mental function.
Pierre Flourens, a French experimental psychologist, challenged the localizationist view by using animal experiments. He discovered that removing the cerebellum in rabbits and pigeons affected their sense of muscular coordination, and that all cognitive functions were disrupted in pigeons when the cerebral hemispheres were removed. From this he concluded that the cerebral cortex, cerebellum, and brainstem functioned together as a whole. His approach has been criticised on the basis that the tests were not sensitive enough to notice selective deficits had they been present.
Emergence of neuropsychology.
Perhaps the first serious attempts to localize mental functions to specific locations in the brain was by Broca and Wernicke. This was mostly achieved by studying the effects of injuries to different parts of the brain on psychological functions. In 1861, French neurologist Paul Broca came across a man who was able to understand language but unable to speak. The man could only produce the sound "tan". It was later discovered that the man had damage to an area of his left frontal lobe now known as Broca's area. Carl Wernicke, a German neurologist, found a patient who could speak fluently but non-sensibly. The patient had been the victim of a stroke, and could not understand spoken or written language. This patient had a lesion in the area where the left parietal and temporal lobes meet, now known as Wernicke's area. These cases, which suggested that lesions caused specific behavioral changes, strongly supported the localizationist view.
Mapping the brain.
In 1870, German physicians Eduard Hitzig and Gustav Fritsch published their findings about the behavior of animals. Hitzig and Fritsch ran an electric current through the cerebral cortex of a dog, causing different muscles to contract depending on which areas of the brain were electrically stimulated. This led to the proposition that individual functions are localized to specific areas of the brain rather than the cerebrum as a whole, as the aggregate field view suggests.
Neuron doctrine.
In the early 20th century, Santiago Ramón y Cajal and Camillo Golgi began working on the structure of the neuron. Golgi developed a silver staining method that could entirely stain several cells in a particular area, leading him to believe that neurons were directly connected with each other in one cytoplasm. Cajal challenged this view after staining areas of the brain that had less myelin and discovering that neurons were discrete cells. Cajal also discovered that cells transmit electrical signals down the neuron in one direction only. Both Golgi and Cajal were awarded a Nobel Prize in Physiology or Medicine in 1906 for this work on the neuron doctrine.
Mid-late 20th century.
Several findings in the 20th century continued to advance the field, such as the discovery of ocular dominance columns, recording of single nerve cells in animals, and coordination of eye and head movements. Experimental psychology was also significant in the foundation of cognitive neuroscience. Some particularly important results were the demonstration that some tasks are accomplished via discrete processing stages, the study of attention, and the notion that behavioural data do not provide enough information by themselves to explain mental processes. As a result, some experimental psychologists began to investigate neural bases of behaviour. 
Wilder Penfield built up maps of primary sensory and motor areas of the brain by stimulating cortices of patients during surgery. Sperry and Gazzaniga’s work on split brain patients in the 1950s was also instrumental in the progress of the field.
Brain mapping.
New brain mapping technology, particularly fMRI and PET, allowed researchers to investigate experimental strategies of cognitive psychology by observing brain function. Although this is often thought of as a new method (most of the technology is relatively recent), the underlying principle goes back as far as 1878 when blood flow was first associated with brain function. Angelo Mosso, an Italian psychologist of the 19th century, had monitored the pulsations of the adult brain through neurosurgically created bony defects in the skulls of patients. He noted that when the subjects engaged in tasks such as mathematical calculations the pulsations of the brain increased locally. Such observations led Mosso to conclude that blood flow of the brain followed function.
Emergence of a new discipline.
Birth of cognitive science.
On September 11, 1956, a large-scale meeting of cognitivists took place at the Massachusetts Institute of Technology. George A. Miller presented his "The Magical Number Seven, Plus or Minus Two" paper while Noam Chomsky and Newell & Simon presented their findings on computer science. Ulric Neisser commented on many of the findings at this meeting in his 1967 book "Cognitive Psychology". The term "psychology" had been waning in the 1950s and 1960s, causing the field to be referred to as "cognitive science". Behaviorists such as Miller began to focus on the representation of language rather than general behavior. David Marr concluded that one should understand any cognitive process at three levels of analysis. These levels include computational, algorithmic/representational, and physical levels of analysis.
Combining neuroscience and cognitive science.
Before the 1980s, interaction between neuroscience and cognitive science was scarce. The term 'cognitive neuroscience' was coined by George Miller and Michael Gazzaniga toward the end of the 1970s. Cognitive neuroscience began to integrate the newly laid theoretical ground in cognitive science, that emerged between the 1950s and 1960s, with approaches in experimental psychology, neuropsychology and neuroscience. (Neuroscience was not established as a unified discipline until 1971). In the very late 20th century new technologies evolved that are now the mainstay of the methodology of cognitive neuroscience, including TMS (1985) and fMRI (1991). Earlier methods used in cognitive neuroscience includes EEG (human EEG 1920) and MEG (1968). Occasionally cognitive neuroscientists utilize other brain imaging methods such as PET and SPECT. An upcoming technique in neuroscience is NIRS which uses light absorption to calculate changes in oxy- and deoxyhemoglobin in cortical areas. In some animals Single-unit recording can be used. Other methods include microneurography, facial EMG, and eye-tracking. Integrative neuroscience attempts to consolidate data in databases, and form unified descriptive models from various fields and scales: biology, psychology, anatomy, and clinical practice. In 2014, Stanislas Dehaene, Giacomo Rizzolatti and Trevor Robbins, were awarded the Brain Prize "for their pioneering research on higher brain mechanisms underpinning such complex human functions as literacy, numeracy, motivated behaviour and social cognition, and for their efforts to understand cognitive and behavioural disorders". Brenda Milner, Marcus Raichle and John O'Keefe received the Kavli Prize in Neuroscience “for the discovery of specialized brain networks for memory and cognition" and O'Keefe shared the Nobel Prize in Physiology or Medicine in the same year with May-Britt Moser and Edvard Moser "for their discoveries of cells that constitute a positioning system in the brain".
Recent trends.
Recently the foci of research have expanded from the localization of brain area(s) for speciﬁc functions in the adult brain using a single technology, studies have been diverging in several different directions such as monitoring REM sleep via polygraphy, a machine that is capable of recording the electrical activity of a sleeping brain. Advances in non-invasive functional neuroimaging and associated data analysis methods have also made it possible to use highly naturalistic stimuli and tasks such as feature films depicting social interactions in cognitive neuroscience studies.
Methods.
Experimental methods of specific psychology fields include:

</doc>
<doc id="50329" url="https://en.wikipedia.org/wiki?curid=50329" title="Communication complexity">
Communication complexity

The notion of communication complexity was introduced by Yao in 1979,
who investigated the following problem involving two separated parties (Alice and Bob). Alice receives an n-bit string x and Bob another n-bit string y, and the goal is for one of them (say Bob) to compute a certain function f(x,y) with the least amount of communication between them. Note that here we are not concerned about the number of computational steps, or the size of the computer memory used. Communication complexity tries to quantify the amount of communication required for such distributed computations.
Of course they can always succeed by having Alice send her whole n-bit string to Bob, who then computes the function, but the idea here is to find clever ways of calculating f with fewer than n bits of communication.
This abstract problem is relevant in many contexts: in VLSI circuit design, for example, one wants to minimize energy used by decreasing the amount of electric signals required between the different components during a distributed computation. The problem is also relevant in the study of data structures, and in the optimization of computer networks. For a survey of the field, see the book by Kushilevitz and Nisan.
Formal definition.
Let formula_1: X formula_2 Y formula_3 Z where we assume in the typical case that formula_4 and formula_5. Alice draws an n-bit string formula_6 formula_7 X while Bob draws an n-bit string formula_8 formula_7 Y. By communicating to each other one bit at a time (adopting some "communication protocol"), Alice and Bob want to compute the value of formula_10 such that at least one party knows the value at the end of the communication. At this point the answer can be communicated back so that at the cost of one extra bit, both parties will know the answer. The worst case communication complexity of this communication protocol, denoted as formula_11, is then defined to be
Using the above definition, it is useful to think of the function formula_13 as a matrix formula_14 (called the "input matrix") where each row of the matrix corresponds to formula_6 formula_7 X and each column corresponds to formula_8 formula_7 Y. An entry in the input matrix is formula_19. Initially both Alice and Bob have a copy of the entire matrix A (assuming the function formula_13 is known to both). Then, the problem of computing the function value can be rephrased as "zeroing-in" on the corresponding matrix entry. This problem can be solved if either Alice or Bob knows both formula_6 and formula_8. At the start of communication, the number of choices for the value of the function on the inputs is the size of matrix, i.e. formula_23. Then, as and when each party communicates a bit to the other, the number of choices for the answer reduces as this eliminates a set of rows/columns resulting in a submatrix of A.
More formally, a set R formula_24 X formula_2 Y is called a "(combinatorial) rectangle" if whenever formula_26 formula_7 R and formula_28 formula_7 R then formula_30 formula_7 R. Equivalently, R can also be viewed as a submatrix of the input matrix A such that R = M formula_2 N where M formula_24 X and N formula_24 Y. Consider the case when formula_35 bits are already exchanged between the parties. Now, for a particular formula_36 formula_7 formula_38, let us define a matrix
Then, formula_42 formula_24 X formula_2 Y, and formula_42 is a rectangle and a submatrix of A.
Example: EQ.
We consider the case where Alice and Bob try to determine if they both have the same string. That is, we are trying to determine if formula_6 is equal to formula_8. It is easy to prove that the equality problem (EQ) will always require you to communicate formula_48 bits in the worst case if you want to be absolutely sure formula_6 and formula_8 are equal.
Consider the simple case of formula_6 and formula_8 being 3 bits. The equality function in this case can be represented by the matrix below. The rows representing all the possibilities of formula_6, the columns those of formula_8.
As you can see, the function only evaluates to 1 when formula_6 equals formula_8 (i.e., on the diagonal). It is also fairly easy to see how communicating a single bit divides your possibilities in half. If you know that the first bit of formula_8 is 1, you only need to consider half of the columns (where formula_8 can equal 100, 101, 110, or 111).
Theorem: formula_59.<br>
Proof. Assume that formula_60. This means that there exists an formula_61 and an formula_62 having the same history, formula_36. Since this history defines a rectangle, formula_64 must also be 1. By definition formula_65 and we know that equality is only true for formula_66 when formula_67. Thus, we have a contradiction.
Intuitively, for formula_68 less than formula_48, we need to be able to define a rectangle in the EQ matrix greater in size than a single cell. All of the cells in this rectangle must contain 1 for us to be able to generalize that this rectangle equals 1. It is not possible to form such a rectangle in the equality matrix.
Randomized communication complexity.
In the above definition, we are concerned with the number of bits that must be "deterministically" transmitted between two parties. If both the parties are given access to a random number generator, can they determine the value of formula_13 with much less information exchanged? Yao, in his seminal paper
answers this question by defining randomized communication complexity.
A randomized protocol formula_71 for a function formula_13 has two-sided error.
A randomized protocol is a deterministic protocol that uses an extra random string in addition to its normal input. There are two models for this: a "public string" is a random string that is known by both parties beforehand, while a "private string" is generated by one party and must be communicated to the other party. A theorem presented below shows that any public string protocol can be simulated by a private string protocol that uses "O(log n)" additional bits compared to the original.
Note that in the probability inequalities above, the outcome of the protocol is understood to depend "only" on the random string; both strings "x" and "y" remain fixed. In other words, if R(x,y) yields g(x,y,r) when using random string "r", then g(x,y,r) = f(x,y) for at least half of all choices for the string "r".
The randomized complexity is simply defined as the number of bits exchanged in such a protocol.
Note that it is also possible to define a randomized protocol with one-sided error, and the complexity is defined similarly.
Example: EQ.
Returning to the previous example of "EQ", if certainty is not required, Alice and Bob can check for equality using only "O(log n)" messages. Consider the following protocol: Assume that Alice and Bob both have access to the same random string formula_75. Alice computes formula_76 and sends this bit (call it "b") to Bob. (The formula_77 is the dot product in GF(2).) Then Bob compares "b" to formula_78. If they are the same, then Bob accepts, saying "x" equals "y". Otherwise, he rejects.
Clearly, if formula_79, then formula_80, so formula_81. If "x" does not equal "y", it is still possible that formula_80, which would give Bob the wrong answer. How does this happen?
If "x" and "y" are not equal, they must differ in some locations:
Where formula_6 and formula_8 agree, formula_88 so those terms affect the dot products equally. We can safely ignore those terms and look only at where formula_6 and formula_8 differ. Furthermore, we can swap the bits formula_91 and formula_92 without changing whether or not the dot products are equal. This means we can swap bits so that formula_6 contains only zeros and formula_8 contains only ones:
Note that formula_98 and formula_99. Now, the question becomes: for some random string formula_100, what is the probability that formula_101? Since each formula_102 is equally likely to be formula_103 or formula_104, this probability is just formula_105. Thus, when formula_6 does not equal formula_8,
formula_108. The algorithm can be repeated many times to increase its accuracy. This fits the requirements for a randomized communication algorithm.
This shows that "if Alice and Bob share a random string of length n", they can send one bit to each other to compute formula_109. In the next section, it is shown that Alice and Bob can exchange only "O(log n)" bits that are as good as sharing a random string of length "n". Once that is shown, it follows that "EQ" can be computed in "O(log n)" messages.
Public coins versus private coins.
It is easier to create random protocols when both parties have access to the same random string (shared string protocol). It is still possible to use these protocols even when the two parties don't share a random string (private string protocol) with a small communication cost. Any shared string random protocol using any number of random string can be simulated by a private string protocol that uses an extra "O(log n)" bits.
Intuitively, we can find some set of strings that has enough randomness in it to run the random protocol with only a small increase in error. This set can be shared beforehand, and instead of drawing a random string, Alice and Bob need only agree on which string to choose from the shared set. This set is small enough that the choice can be communicated efficiently. A formal proof follows.
Consider some random protocol "P" with a maximum error rate of 0.1. Let formula_71 be formula_111 strings of length "n", numbered formula_112. Given such an formula_71, define a new protocol formula_114 which randomly picks some formula_115 and then runs "P" using formula_115 as the shared random string. It takes "O"(log 100"n") = "O"(log "n") bits to communicate the choice of formula_115.
Let us define formula_118 and formula_119 to be the probabilities that formula_120 and formula_114 compute the correct value for the input formula_122.
For a fixed formula_122, we can use Hoeffding's inequality to get the following equation:
Thus when we don't have formula_122 fixed:
The last equality above holds because there are formula_23 different pairs formula_122. Since the probability does not equal 1, there is some formula_129 so that for all formula_122:
Since formula_120 has at most 0.1 error probability, formula_133 can have at most 0.2 error probability.
Quantum communication complexity.
Quantum communication complexity tries to quantify the communication reduction possible by using quantum effects during a distributed computation.
At least three quantum generalizations of communication complexity have been proposed; for a survey see the suggested text by G. Brassard.
The first one is the qubit-communication model, where the parties can use quantum communication instead of classical communication, for example by exchanging photons through an optical fiber.
In a second model the communication is still performed with classical bits, but the parties are allowed to manipulate an unlimited supply of quantum entangled states as part of their protocols. By doing measurements on their entangled states, the parties can save on classical communication during a distributed computation.
The third model involves access to previously shared entanglement in addition to qubit communication, and is the least explored of the three quantum models.
Nondeterministic communication complexity.
In nondeterministic communication complexity, Alice and Bob have access to an oracle. After receiving the oracle's word, the parties communicate to deduce "f(x,y)". The nondeterministic communication complexity is then the maximum over all pairs "(x,y)" over the sum of number of bits exchanged and the coding length of the oracle word.
Viewed differently, this amounts to covering all 1-entries of the 0/1-matrix by combinatorial 1-rectangles (i.e., non-contiguous, non-convex submatrices, whose entries are all one (see Kushilevitz and Nisan or Dietzfelbinger et al.)). The nondeterministic communication complexity is the binary logarithm of the "rectangle covering number" of the matrix: the minimum number of combinatorial 1-rectangles required to cover all 1-entries of the matrix, without covering any 0-entries.
Nondeterministic communication complexity occurs as a means to obtaining lower bounds for deterministic communication complexity (see Dietzfelbinger et al.), but also in the theory of nonnegative matrices, where it gives a lower bound on the nonnegative rank of a nonnegative matrix.
Open problems.
Considering a 0/1 input matrix formula_134, the minimum number of bits exchanged to compute formula_13 deterministically in the worst case, formula_136, is known to be bounded from below by the logarithm of the rank of the matrix formula_137. The log rank conjecture proposes that the communication complexity, formula_136, of formula_137 is bounded from above by a constant power of the logarithm of the rank of formula_137. Since D(f) is bounded from above and below by polynomials of log rankformula_141, we can say D(f) is polynomially related to log rankformula_141. Since the rank of a matrix is polynomial time computable in the size of the matrix, such an upper bound would allow the matrix's communication complexity to be approximated in polynomial time. Note, however, that the size of the matrix itself is exponential in the size of the input.
For a randomized protocol, the number of bits exchanged in the worst case, R(f), is conjectured to be polynomially related to the following formula:
Such log rank conjectures are valuable because they reduce the question of a matrix's communication complexity to a question of linearly independent rows (columns) of the matrix. This reveals that the essence of the communication complexity problem, for example in the EQ case above, is figuring out where in the matrix the inputs are, in order to find out if they're equivalent.
Applications.
Lower bounds in communication complexity can be used to prove lower bounds in decision tree complexity, VLSI circuits, data structures, streaming algorithms, space–time tradeoffs for Turing machines and more.

</doc>
<doc id="50331" url="https://en.wikipedia.org/wiki?curid=50331" title="Gerhard Armauer Hansen">
Gerhard Armauer Hansen

Gerhard Henrik Armauer Hansen (29 July 1841 – 12 February 1912) was a Norwegian physician, remembered for his identification of the bacterium "Mycobacterium leprae" in 1873 as the causative agent of leprosy.
Life.
Hansen was born in Bergen and studied medicine at the Royal Frederik's University (now the University of Oslo), gaining his degree in 1866. He served a brief internship at the National Hospital in Christiania (Oslo) and as a doctor in Lofoten. In 1868 Hansen returned to Bergen to study leprosy while working with Daniel Cornelius Danielssen, a noted expert.
Leprosy was regarded as largely hereditary or otherwise miasmic in origin. Hansen concluded on the basis of epidemiological studies that leprosy was a specific disease with a specific cause. In 1870–71 Hansen travelled to Bonn and Vienna to gain the training necessary for him to prove his hypothesis. In 1873, he announced the discovery of "Mycobacterium leprae" in the tissues of all sufferers, although he did not identify them as bacteria, and received little support. The discovery was done with a "new and better" microscope.
In 1879 he gave tissue samples to Albert Neisser who successfully stained the bacteria and announced his findings in 1880, claiming to have discovered the disease causing organism. There was some quarreling between Neisser and Hansen, Hansen as discoverer of the bacillus and Neisser as identifier of it as the etiological agent. Neisser put in some effort to downplay the assistance of Hansen. Hansen's claim was injured by his failure to produce a pure microbiological culture in an artificial medium or to prove that the rod-shaped organisms were infectious. Further Hansen had attempted to infect at least one female patient without consent and although no damage was caused, that case ended in court and Hansen lost his post at the hospital.
Hansen remained medical officer for leprosy in Norway and it was through his efforts that the leprosy acts of 1877 and 1885 were passed, leading to a steady decline of the disease in Norway from 1,800 known cases in 1875 to just 575 cases in 1901. His distinguished work was recognized at the International Leprosy Congress held at Bergen in 1909. 
Hansen had suffered from syphilis since the 1860s but died of heart disease.
Honors.
In Bergen, a medical museum that is often referred to as the "Leprosy Museum", has been dedicated to Hansen. The University of Bergen has dedicated a research facility to him—"Armauer Hansen Building"—located at Haukeland University Hospital in Bergen.
In Jerusalem, a 19th-century leprosarium has borne Hansen's name since 1950. It has been reconstructed into an art center while preserving the physician's surname in its title.

</doc>
<doc id="50332" url="https://en.wikipedia.org/wiki?curid=50332" title="Utrecht (province)">
Utrecht (province)

Utrecht () is a province of the Netherlands. It is located in the centre of the country, bordering the Eemmeer in the north, the province of Gelderland in the east, the river Rhine in the south, the province of South Holland in the west and the province of North Holland in the north-west. With an area of approximately , it is the smallest of the twelve provinces. Apart from its eponymous capital, major cities in the province are Amersfoort, Houten, Nieuwegein, Veenendaal and Zeist.
In the International Organization for Standardization world region code system Utrecht makes up one region with code -UT.
History.
The Bishopric of Utrecht was established in 695 when Saint Willibrord was consecrated bishop of the Frisians at Rome by Pope Sergius I. With the consent of the Frankish ruler, Pippin of Herstal, he settled in an old Roman fort in Utrecht. After Willibrord's death the diocese suffered greatly from the incursions of the Vikings. Better times appeared during the reign of the Saxon emperors, who frequently summoned the Bishops of Utrecht to attend the imperial councils and diets. In 1024 the bishops were made Princes of the Holy Roman Empire and the new Prince-Bishopric of Utrecht was formed. In 1122, with the Concordat of Worms, the Emperor's right of investiture was annulled, and the cathedral chapter received the right to elect the bishop. It was, however, soon obligated to share this right with the four other collegiate chapters in the city. The Counts of Holland and Guelders, between whose territories the lands of the Bishops of Utrecht lay, also sought to acquire influence over the filling of the episcopal see. This often led to disputes and consequently the Holy See frequently interfered in the election. After the middle of the 14th century the popes repeatedly appointed the bishop directly without regard to the five chapters.
During the Hook and Cod Wars, Utrecht was fought over by forces of the Duke of Burgundy leading to the First Utrecht Civil War (1470-1474) and Second Utrecht Civil War (1481-1483).
In 1527, the Bishop sold his territories, and thus his secular authority, to Holy Roman Emperor Charles V and the principality became an integral part of the Habsburg dominions, which already included most other Dutch provinces. The chapters transferred their right of electing the bishop to Charles V and his government, a measure to which Pope Clement VII gave his consent, under political pressure after the Sack of Rome. However, the Habsburg rule did not last long, as Utrecht joined in the Dutch Revolt against Charles' successor Philip II in 1579, becoming a part of the Dutch Republic.
In World War II, Utrecht was held by German forces until the general capitulation of the Germans in the Netherlands on May 5, 1945. It was occupied by Canadian Allied forces on May 7, 1945. The towns of Oudewater, Woerden and Vianen were transferred from the province of South Holland to Utrecht in 1970, 1989 and 2002 respectively. In February 2011, Utrecht, together with the provinces of North Holland and Flevoland, showed a desire to investigate the feasibility of a merger between the three provinces. This has been positively received by the Dutch cabinet, for the desire to create one Randstad province has already been mentioned in the coalition agreement. The province of South Holland, part of the Randstad urban area, visioned to be part of the Randstad province, and very much supportive of the idea of a merger into one province, is not named. With or without South Holland, if created, the new province would be the largest in the Netherlands in both area and population.
Geography.
In the east of Utrecht lies the Utrecht Hill Ridge (Dutch: Utrechtse Heuvelrug), a chain of hills left as lateral moraine by tongues of glacial ice after the Saline glaciation that preceded the last ice age. Because of the scarcity of nutrients in the fast-draining sandy soil, the greatest part of a landscape that was formerly heath has been planted with pine plantations. The south of the province is a river landscape. The west consists mostly of meadows. In the north are big lakes formed by the digging of peat from bogs formed after the last ice age.
Nature.
One of the most attractive natural areas in the province is the Vechtstreek ("Vecht region"), situated on either side of the Vecht river.
An international nature conservation organisation that has settled the head office of its Netherlands branch in this province (at Zeist) is the WWF.
"Natuur en Milieu" ("Nature and Environment") is a national nature protection organisation whose head office is in this province (at Utrecht city).
Municipalities.
The Province of Utrecht is divided into 26 municipalities.

</doc>
<doc id="50333" url="https://en.wikipedia.org/wiki?curid=50333" title="Utrecht">
Utrecht

Utrecht (; ) is the capital and most populous city in the Dutch province of Utrecht. It is located in the eastern corner of the Randstad conurbation and is the fourth largest city in the Netherlands with a population of in .
Utrecht's ancient city centre features many buildings and structures several dating as far back as the High Middle Ages. It has been the religious centre of the Netherlands since the 8th century. It lost the status of prince-bishopric but remains the main religious center in the country.
Utrecht was the most important city in the Netherlands until the Dutch Golden Age, when it was succeeded by Amsterdam as the country's cultural centre and most populous city.
Utrecht is host to Utrecht University, the largest university in the Netherlands, as well as several other institutes for higher education. Due to its central position within the country, it is an important transport hub for both rail and road transport. It has the second highest number of cultural events in the Netherlands, after Amsterdam.
In 2012, Lonely Planet included Utrecht in the top 10 of the world’s unsung places.
History.
Origins (until 650).
Although there is some evidence of earlier inhabitation in the region of Utrecht, dating back to the Stone Age (app. 2200 BCE) and settling in the Bronze Age (app. 1800–800 BCE), the founding date of the city is usually related to the construction of a Roman fortification ("castellum"), probably built in around 50 CE.
A series of such fortresses was built after the Roman emperor Claudius decided the empire should not expand north. To consolidate the border the limes Germanicus defense line was constructed along the main branch of the river Rhine, which at that time flowed through a more northern bed compared to today (what is now the Kromme Rijn). These fortresses were designed to house a cohort of about 500 Roman soldiers. Near the fort settlements would grow housing artisans, traders and soldiers' wives and children.
In Roman times, the name of the Utrecht fortress was simply "Traiectum", denoting its location at a possible Rhine crossing. Traiectum became Dutch Trecht; with the U from Old Dutch "uut" (downriver) added to distinguish U-trecht from Maas-tricht. In 11th-century official documents it was Latinized as Ultra Traiectum.
Around the year 200, the wooden walls of the fortification were replaced by sturdier tuff stone walls, remnants of which are still to be found below the buildings around Dom Square.
From the middle of the 3rd century Germanic tribes regularly invaded the Roman territories. Around 275 the Romans could no longer maintain the northern border and Utrecht was abandoned. Little is known about the next period 270–650. Utrecht is first spoken of again several centuries after the Romans left. Under the influence of the growing realms of the Franks, during Dagobert I's reign in the 7th century, a church was built within the walls of the Roman fortress. In ongoing border conflicts with the Frisians this first church was destroyed.
Centre of Christianity in the Netherlands (650–1579).
By the mid-7th century, English and Irish missionaries set out to convert the Frisians. The pope appointed their leader, Willibrordus, bishop of the Frisians. The tenure of Willibrordus is generally considered to be the beginning of the Bishopric of Utrecht. In 723, the Frankish leader Charles Martel bestowed the fortress in Utrecht and the surrounding lands as the base of the bishops. From then on Utrecht became one of the most influential seats of power for the Roman Catholic Church in the Netherlands.
The archbishops of Utrecht were based at the uneasy northern border of the Carolingian Empire. In addition, the city of Utrecht had competition from the nearby trading centre Dorestad. After the fall of Dorestad around 850, Utrecht became one of the most important cities in the Netherlands. The importance of Utrecht as a centre of Christianity is illustrated by the election of the Utrecht-born Adriaan Florenszoon Boeyens as pope in 1522 (the last non-Italian pope before John Paul II).
Prince-bishops.
When the Frankish rulers established the system of feudalism, the Bishops of Utrecht came to exercise worldly power as prince-bishops. The territory of the bishopric not only included the modern province of Utrecht (Nedersticht, 'lower Sticht'), but also extended to the northeast. The feudal conflict of the Middle Ages heavily affected Utrecht. The prince-bishopric was involved in almost continuous conflicts with the Counts of Holland and the Dukes of Guelders. The Veluwe region was seized by Guelders, but large areas in the modern province of Overijssel remained as the Oversticht.
Clerical buildings.
Several churches and monasteries were built inside, or close to, the city of Utrecht. The most dominant of these was the Cathedral of Saint Martin, inside the old Roman fortress. The construction of the present Gothic building was begun in 1254 after an earlier romanesque construction had been badly damaged by fire. The choir and transept were finished from 1320 and were followed then by the ambitious Dom tower. The last part to be constructed was the central nave, from 1420. By that time, however, the age of the great cathedrals had come to an end and declining finances prevented the ambitious project from being finished, the construction of the central nave being suspended before the planned flying buttresses could be finished.
Besides the cathedral there were four collegiate churches in Utrecht: St. Salvator's Church (demolished in the 16th century), on the Dom square, dating back to the early 8th century. Saint John (Janskerk), originating in 1040; Saint Peter, building started in 1039 and Saint Mary's church building started around 1090 (demolished in the early 19th century, cloister survives).
Besides these churches the city housed St. Paul's Abbey, the 15th-century beguinage of St. Nicholas, and a 14th-century chapter house of the Teutonic Knights.
Besides these buildings which belonged to the bishopric; an additional four parish churches were constructed in the city: the Jacobikerk (dedicated to Saint James), founded in the 11th century, with the current Gothic church dating back to the 14th century; the Buurkerk (Neighbourhood-church) of the 11th-century parish in the centre of the city; Nicolaichurch (dedicated to Saint Nicholas), from the 12th century and the 13th-century Geertekerk (dedicated to Saint Gertrude of Nivelles).
City of Utrecht.
The location on the banks of the river Rhine allowed Utrecht to become an important trade centre in the Northern Netherlands. The growing town Utrecht was granted city rights by Henry V in 1122.
When the main flow of the Rhine moved south, the old bed, which still flowed through the heart of the town became evermore canalized; and the wharf system was built as an inner city harbour system. On the wharfs storage facilities ("werfkelders") were built, on top of which the main street, including houses was constructed. The wharfs and the cellars are accessible from a platform at water level with stairs descending from the street level to form a unique structure. The relations between the bishop, who controlled many lands outside of the city, and the citizens of Utrecht was not always easy. The bishop, for example dammed the Kromme Rijn at Wijk bij Duurstede to protect his estates from flooding. This threatened shipping for the city and led the city of Utrecht to commission a canal to ensure access to the town for shipping trade: the Vaartse Rijn, connecting Utrecht to the Hollandse IJssel at IJsselstein.
The end of independence.
In 1528 the bishop lost secular power over both Neder- and Oversticht – which included the city of Utrecht – to Charles V, Holy Roman Emperor. Charles V combined the Seventeen Provinces (the current Benelux and the northern parts of France) as a personal union. This ended the prince-bishopric Utrecht, as the secular rule was now the lordship of Utrecht, with the religious power remaining with the bishop, although Charles V had gained the right to appoint new bishops. In 1559 the bishopric of Utrecht was raised to archbishopric to make it the religious center of the Northern ecclesiastical province in the Seventeen provinces.
The transition from independence to a relatively minor part of a larger union was not easily accepted. To quell uprisings Charles V was struggling to exert his power over the citizens of the city, who had struggled to gain a certain level of independence from the bishops and were not willing to cede this to their new lord. The heavily fortified castle Vredenburg was built to house a large garrison whose main task was to maintain control over the city. The castle would last less than 50 years before it was demolished in an uprising in the early stages of the Dutch Revolt.
Republic of the Netherlands (1579–1806).
In 1579 the northern seven provinces signed the Union of Utrecht, in which they decided to join forces against Spanish rule. The Union of Utrecht is seen as the beginning of the Dutch Republic. In 1580 the new and predominantly Protestant state abolished the bishoprics, including the archbishopric of Utrecht. The stadtholders disapproved of the independent course of the Utrecht bourgeoisie and brought the city under much more direct control of the republic; which shifted the power towards its dominant province Holland. This was the start of a long period of stagnation of trade and development in Utrecht. Utrecht remained an atypical city in the new republic with about 40% Catholic in the mid-17th-century, and even more among the elite groups, who included many rural nobility and gentry with town houses there.
The fortified city temporarily fell to the French invasion in 1672 (the Disaster Year); where the French invasion was only stopped west of Utrecht at the Old Hollandic Waterline. In 1674, only two years after the French left, the centre of Utrecht was struck by a tornado. The halt to building before construction of flying buttresses in the 15th century now proved to be the undoing of the central section of the cathedral of St Martin church which collapsed; creating the current Dom square between the tower and choir. In 1713, Utrecht hosted one of the first international peace negotiations when the Treaty of Utrecht settled the War of the Spanish Succession. Since 1723 Utrecht became the centre of the non-Roman Old Catholic Churches in the world.
Modern history (1815–present).
In the early 19th century, the role of Utrecht as a fortified town had become obsolete. The fortifications of the Nieuwe Hollandse Waterlinie were moved east of Utrecht. The town walls could now be demolished to allow for expansion. The moats remained intact and formed an important feature of the Zocher plantsoen, an English style landscape park that remains largely intact today. Growth of the city increased when, in 1843, a railway connecting Utrecht to Amsterdam was opened. After that, Utrecht gradually became the main hub of the Dutch railway network. With the industrial revolution finally gathering speed in the Netherlands and the ramparts taken down, Utrecht began to grow far beyond the medieval centre. In 1853, the Dutch government allowed the bishopric of Utrecht to be reinstated by Rome, and Utrecht became the centre of Dutch Catholicism once more. From the 1880s onward neighbourhoods such as Oudwijk, Wittevrouwen, Vogelenbuurt to the East, and Lombok to the West were developed. New middle class residential areas, such as Tuindorp and Oog in Al, were built in the 1920s and 1930s. During this period, several Jugendstil houses and office buildings were built, followed by Rietveld who built the Rietveld Schröder House (1924), and Dudok's construction of the city theater (1941).
During World War II, Utrecht was held by the Germans until the general German surrender of the Netherlands on 5 May 1945. Canadian troops that had surrounded the city entered it after that surrender, on 7 May 1945. After the end of World War II, the city has grown considerably when new neighbourhoods such as , Kanaleneiland, and Lunetten were built. Around 2000 the city the Leidsche Rijn housing area was developed as the next extension of the city to the west.
The area surrounding Utrecht Centraal railway station and the station itself were developed following modernist ideas of the 1960s, in a brutalist style. This led to the construction of the shopping mall , music centre Vredenburg (Hertzberger, 1979), and conversion of part of the ancient canal structure into a highway (Catherijnebaan). Protest against further modernisation of the city centre followed even before the last buildings were finalised. In the early 21st century the whole area is being redeveloped. The redeveloped music centre opened in 2014 where the original Vredenburg concert and rock and jazz halls are brought together in a single building. 
Geography.
Climate.
Utrecht experiences a temperate oceanic climate (Köppen climate classification "Cfb") similar to almost all of the Netherlands.
Population.
Demographics.
Utrecht city had a population of 296,305 in 2007. Utrecht is a growing municipality and projections are that the city's population will surpass 392,000 by 2025.
Utrecht has a young population, with many inhabitants in the age category from 20 and 30 years, due to the presence of a large university. About 52% of the population is female, 48% is male. The majority of households (52.5%) in Utrecht are single person households. About 29% of people living in Utrecht are either married, or have another legal partnership. About 3% of the population of Utrecht is divorced.
About 69% of the population is of Dutch ancestry. Approximately 10% of the population consists of immigrants from Western countries, while 21% of the population is of non-Western origin (9% Moroccan, 5% Turkish, 3% Surinamese and Dutch Caribbean and 5% of other countries). Some of the city's boroughs have a relatively high percentage of originally non-Dutch inhabitants – i.e. Kanaleneiland 83% and Overvecht 57%. Like Rotterdam, Amsterdam, The Hague and other large Dutch cities, Utrecht faces some socio-economic problems. About 38% percent of its population either earns a minimum income or is dependent on social welfare (17% of all households). Boroughs such as Kanaleneiland, Overvecht and Hoograven consist primarily of high-rise housing developments, and are known for relatively high poverty and crime rate.
Religion.
Utrecht has been the religious centre of the Netherlands since the 8th century. Currently it is the see of the Metropolitan Archbishop of Utrecht, the most senior Dutch Roman Catholic leader. His ecclesiastical province covers the whole kingdom.
Utrecht is also the see of the archbishop of the Old Catholic church, titular head of the Union of Utrecht, and the location of the offices of the main Protestant church.
Population centres and agglomeration.
The city of Utrecht is subdivided into 10 city quarters, which all have their own neighbourhood council and service center for civil affairs.
Utrecht is the centre of a densely populated area, which makes concise definitions of its agglomeration difficult, and somewhat arbitrary. The smaller Utrecht agglomeration of continuously built up areas counts some 420,000 inhabitants and includes Nieuwegein, IJsselstein and Maarssen. It is sometimes argued that the close by municipalities De Bilt, Zeist, Houten, Vianen, Driebergen-Rijsenburg (Utrechtse Heuvelrug), and Bunnik should also be counted towards the Utrecht agglomeration, bringing the total to 640,000 inhabitants. The larger region, including slightly more remote towns such as Woerden and Amersfoort counts up to 820,000 inhabitants.
Cityscape.
Utrecht's cityscape is dominated by the Dom Tower, the tallest belfry in the Netherlands and originally part of the Cathedral of Saint Martin. An ongoing debate is over whether any building in or near the centre of town should surpass the Dom Tower in height (). Nevertheless, some tall buildings are now being constructed that will become part of the skyline of Utrecht. The second tallest building of the city, the Rabobank-tower, was completed in 2010 and stands tall. Two antennas will increase that height to . Two other buildings were constructed around the Nieuw Galgenwaard stadium (2007). These buildings, the 'Kantoortoren Galghenwert' and 'Apollo Residence', stand and high respectively.
Another landmark is the old centre and the canal structure in the inner city. The Oudegracht is a curved canal, partly following the ancient main branch of the Rhine. It is lined with the unique wharf-basement structures that create a two-level street along the canals. The inner city has largely retained its Medieval structure, and the moat ringing the old town is largely intact. Because of the role of Utrecht as a fortified city, construction outside the medieval centre and its city walls was restricted until the 19th century. Surrounding the medieval core there is a ring of late 19th- and early 20th-century neighbourhoods, with newer neighbourhoods positioned farther out. The eastern part of Utrecht remains fairly open. The Dutch Water Line, moved east of the city in the early 19th century required open lines of fire, thus prohibiting all permanent constructions until the middle of the 20th century on the east side of the city.
Due to the past importance of Utrecht as a religious centre, several monumental churches were erected, many of which have survived. Most prominent is the Dom Church. Other notable churches include the romanesque St Peter's and St John's churches, the gothic churches of St James and St Nicholas, and the Buurkerk, now converted into a museum for automatically playing musical instruments
Transport.
Public transport.
Because of its central location, Utrecht is well connected to the rest of the Netherlands and has a well-developed public transport network.
Heavy and light rail.
Utrecht Centraal is the main railway station of Utrecht. There are regular intercity services to all major Dutch cities; direct services to Schiphol Airport. Utrecht Centraal is a station on the night service, providing 7 days a week an all night service to (among others) Schiphol Airport, Amsterdam and Rotterdam. International InterCityExpress (ICE) services to Germany (and further) through Arnhem call at Utrecht Centraal. Regular local trains to all areas surrounding Utrecht also depart from Utrecht Centraal; and service several smaller stations: Utrecht Lunetten, Utrecht Vaartsche Rijn, Utrecht Overvecht, Utrecht Leidsche Rijn, Utrecht Terwijde, Utrecht Zuilen and Vleuten. A former station Utrecht Maliebaan closed in 1939 and has since been converted into the Dutch Railway Museum.
The Utrecht sneltram is a light rail scheme running southwards from Utrecht Centraal to the suburbs of IJsselstein, Kanaleneiland, Lombok and Nieuwegein. The sneltram began operations in 1983 and is currently operated by the private transport company Qbuzz. In 2018 the new extension to the Uithof will start operating creating a direct mass transit connection from the central station to the main Utrecht university campus.
Utrecht is the location of the headquarters of Nederlandse Spoorwegen (English: "Dutch Railways") – the largest rail operator in the Netherlands – and ProRail – the state-owned company responsible for the construction and maintenance of the country's rail infrastructure.
Bus transport.
The main local and regional bus station of Utrecht is located adjacent to Utrecht Centraal railway station, at the East and West entrances. Due to large scale renovation and construction works at the railway station, the station's bus stops are changing frequently. As a general rule, westbound buses depart from the bus station on the west entrance, other buses from the east side station. Local buses in Utrecht are operated by Qbuzz – its services include a high-frequency service to the Uithof university district. The local bus fleet is one of Europe's cleanest, using only buses compliant with the Euro-VI standard as well as electric buses for inner city transport. Regional buses from the city are operated by Arriva and Connexxion.
The Utrecht Centraal railway station is also served by the pan-European services of Eurolines. Furthermore, it acts as departure and arrival place of many coach companies serving holiday resorts in Spain and France – and during winter in Austria and Switzerland.
Cycling.
Like most Dutch cities, Utrecht has an extensive network of cycle paths, making cycling safe and popular. 33% of journeys within the city are by bicycle, more than any other mode of transport. (Cars, for example, account for 30% of trips). Bicycles are used by young and old people, and by individuals and families. They are mostly traditional, upright, steel-framed bicycles, with few or no gears. There are also barrow bikes, for carrying shopping or small children. As thousands of bicycles are parked haphazardly in town, creating an eyesore but also impeding pedestrians, the City Council decided in 2014 to build the world's largest bicycle parking station, near the Central Railway Station. This 3-floor construction will cost an estimated 48 million Euro and will hold 12,500 bicycles. Completion is foreseen in 2018.
Road transport.
Utrecht is well-connected to the Dutch road network. Two of the most important major roads serve the city of Utrecht: the A12 and A2 motorways connect Amsterdam, Arnhem, The Hague and Maastricht, as well as Belgium and Germany. Other major motorways in the area are the Almere–Breda A27 and the Utrecht–Groningen A28. Due to the increasing traffic and the ancient city plan, traffic congestion is a common phenomenon in and around Utrecht, causing elevated levels of air pollutants. This has led to a passionate debate in the city about the best way to improve the city's air quality.
Shipping.
Utrecht has an industrial port located on the Amsterdam-Rijnkanaal. The container terminal has a capacity of 80,000 containers a year. In 2003, the port facilitated the transport of four million tons of cargo; mostly sand, gravel, fertilizer and fodder. Additionally, some tourist boat trips are organised from various places on the Oudegracht; and the city is connected to touristic shipping routes through sluices.
Economy.
Production industry constitutes a small part of the economy of Utrecht.
The economy of Utrecht depends for a large part on the several large institutions located in the city. It is the centre of the Dutch railroad network and the location of the head office of Nederlandse Spoorwegen. ProRail is headquartered in The "" (The Inkpot) – the largest brick building in the Netherlands (the "UFO" featured on its façade stems from an art program in 2000). Rabobank, a large bank, has its headquarters in Utrecht.
A large indoor shopping centre is located between Utrecht Centraal railway station and the city centre. The corridors are treated as public places like streets, and the route between the station and the city centre is open all night. In 20 years from 2004, parts of Hoog Catharijne will be redeveloped as part of the renovation of the larger station area. Parts of the city's network of canals, which were filled to create the shopping center and central station area, will be recreated. The Jaarbeurs, one of the largest convention centres in the Netherlands, is located at the west side of the central railway station.
Education.
Utrecht hosts several large institutions of higher education. The most prominent of these is Utrecht University (est. 1636), the largest university of the Netherlands with 30,449 students (). The university is partially based in the inner city as well as in the Uithof campus area, to the east of the city. According to Shanghai Jiaotong University's university ranking in 2014 it is the 57th best university in the world. Utrecht also houses the much smaller University of Humanistic Studies, which houses about 400 students.
Utrecht is home of one of the locations of TIAS School for Business and Society, focused on post-experience management education and the largest management school of its kind in the Netherlands. In 2008, its executive MBA program was rated the 24th best program in the world by the "Financial Times".
Utrecht is also home to two other large institutions of higher education: the vocational university Hogeschool Utrecht (37,000 students), with locations in the city and the Uithof campus, and the HKU Utrecht School of the Arts (3,000 students).
There are many schools for primary and secondary education, allowing parents to select from different philosophies and religions in the school as is inherent in the Dutch school system.
Culture.
Utrecht city has an active cultural life, and in the Netherlands is second only to Amsterdam. There are several theatres and theatre companies. The 1941 main city theatre was built by Dudok. Besides theatres there is a large number of cinemas including three arthouse cinemas. Utrecht is host to the international Early Music Festival (Festival Oude Muziek, for music before 1800) and the Netherlands Film Festival. The city has an important classical music hall Vredenburg (1979 by Herman Hertzberger). Its acoustics are considered among the best of the 20th-century original music halls. The original Vredenburg music hall has been redeveloped as part of the larger station area redevelopment plan and in 2014 has gained additional halls that allowed its merger with the rock club Tivoli and the SJU jazzpodium. There are several other venues for music throughout the city. Young musicians are educated in the conservatory, a department of the Utrecht School of the Arts. There is a specialised museum of automatically playing musical instruments.
There are many art galleries in Utrecht. There are also several foundations to support art and artists. Training of artists is done at the Utrecht School of the Arts. The Centraal Museum has many exhibitions on the arts, including a permanent exhibition on the works of Utrecht resident illustrator Dick Bruna, who is best known for creating Miffy ("Nijntje", in Dutch). Although street art is illegal in Utrecht, the Utrechtse Kabouter, a picture of a gnome with a red hat, became a common sight in 2004. Utrecht also houses one of the landmarks of modern architecture, the 1924 Rietveld Schröder House, which is listed on UNESCO's world heritage sites.
Every Saturday a paviour adds another letter to The Letters of Utrecht, an endless poem in the cobblestones of the Oude Gracht in Utrecht. With the "Letters", Utrecht has a social sculpture as a growing monument created for the benefit of future people.
To promote culture Utrecht city organizes cultural Sundays. During a thematic Sunday several organisations create a program, which is open to everyone without, or with a very much reduced, admission fee. There are also initiatives for amateur artists. The city subsidises an organisation for amateur education in arts aimed at all inhabitants (Utrechts Centrum voor de Kunsten), as does the university for its staff and students. Additionally there are also several private initiatives. The city council provides coupons for discounts to inhabitants who receive welfare to be used with many of the initiatives.
Sports.
Utrecht is home to the premier league (professional) football club FC Utrecht, which plays in Stadium Nieuw Galgenwaard. It is also the home of Kampong, the largest (amateur) sportsclub in the Netherlands (4,500 members), SV Kampong. Kampong features field hockey, association football, cricket, tennis, squash and boules. Kampong's men and women top hockey squads play in the highest Dutch hockey league, the Rabohoofdklasse. Utrecht is also home to baseball and softball club UVV, which plays in the highest Dutch baseball league: de Hoofdklasse. Utrecht's waterways are used by several rowing clubs. Viking is a large club open to the general public, and the student clubs Orca and Triton compete in the Varsity each year.
In the summer of 2013, Utrecht hosted the European Youth Olympic Festival, in which more than 2,000 young athletes competed in nine different olympic sports. In July 2015 Utrecht hosted the Grand Départ and first stage of the Tour de France.
Museums.
Utrecht has several smaller and larger museums. Many of those are located in the southern part of the old town, the Museumkwartier.
Music and events.
The city has several music venues such as TivoliVredenburg, Tivoli De Helling, ACU, EKKO, DBs and RASA. Utrecht hosts the yearly Utrecht Early Music Festival – . In Jaarbeurs it hosts Trance Energy. Every summer there used to be the "Summer Darkness" festival, which celebrated "goth" culture and music. In November the Le Guess Who? festival, focused on indie rock, art rock and experimental rock, takes place in many of the city's venues.
Theatre.
There are two main theaters in the city, the and the De parade, a travelling theatre festival performs in Utrecht in summer.The city also hosts the yearly Festival a/d Werf which offers a selection of contemporary international theatre, together with visual arts, public art and music.
Notable people from Utrecht.
Over the ages famous people have been born and raised in Utrecht.
Among the most famous Utrechters are: 
International relations.
Twin towns.
Utrecht is twinned with:

</doc>
<doc id="50335" url="https://en.wikipedia.org/wiki?curid=50335" title="John Polkinghorne">
John Polkinghorne

John Charlton Polkinghorne, (born 16 October 1930) is an English theoretical physicist, theologian, writer and Anglican priest. A prominent and leading voice explaining the relationship between science and religion, he was professor of Mathematical physics at the University of Cambridge from 1968 to 1979, when he resigned his chair to study for the priesthood, becoming an ordained Anglican priest in 1982. He served as the president of Queens' College, Cambridge from 1988 until 1996.
Polkinghorne is the author of five books on physics, and 26 on the relationship between science and religion; his publications include "The Quantum World" (1989), "Quantum Physics and Theology: An Unexpected Kinship" (2005), "Exploring Reality: The Intertwining of Science and Religion" (2007), and "Questions of Truth" (2009). "The Polkinghorne Reader" (edited by Thomas Jay Oord) provides key excerpts from Polkinghorne's most influential books. He was knighted in 1997 and in 2002 received the £1 million Templeton Prize, awarded for exceptional contributions to affirming life's spiritual dimension.
Early life and education.
Polkinghorne was born in Weston-super-Mare to Dorothy Charlton, the daughter of a groom and George Polkinghorne, who worked for the post office. John was the couple's third child. There was a brother, Peter, and a sister, Ann, who died when she was six, one month before John's birth. Peter died in 1942 while flying for the Royal Air Force during the Second World War.
He was educated at the local primary school in Street, Somerset, then was taught by a friend of the family at home, and later at a Quaker school. When he was 11 he went to Elmhurst Grammar School in Street, and when his father was promoted to head postmaster in Ely in 1945, Polkinghorne was transferred to The Perse School, Cambridge. Following National Service in the Royal Army Educational Corps from 1948 to 1949, he read Mathematics at Trinity College, Cambridge, graduating in 1952 as Senior Wrangler, then earned his PhD in physics in 1955, supervised by Nobel laureate Abdus Salam in the group led by Paul Dirac.
Career.
Physics.
He joined the Christian Union of UCCF while at Cambridge and met his future wife, Ruth Martin, another member of the Union and also a mathematics student. They married on 26 March 1955, and at the end of that year sailed from Liverpool to New York. Polkinghorne accepted a postdoctoral Harkness Fellowship with the California Institute of Technology, where he worked with Murray Gell-Mann. Toward the end of the fellowship he was offered a position as lecturer at the University of Edinburgh, which he took up in 1956.
After two years in Scotland, he returned to teach at Cambridge in 1958. He was promoted to reader in 1965, and in 1968 was offered a professorship in mathematical physics, a position he held until 1979, his students including Brian Josephson and Martin Rees. For 25 years, he worked on theories about elementary particles, played a role in the discovery of the quark, and researched the analytic and high-energy properties of Feynman integrals and the foundations of S-Matrix theory. While employed by Cambridge, he also spent time at Princeton, Berkeley, Stanford, and at CERN in Geneva. He was elected a Fellow of the Royal Society in 1974.
Priesthood and Queens' College.
Polkinghorne decided to train for the priesthood in 1977. He said in an interview that he felt he had done his bit for science after 25 years, and that his best mathematical work was probably behind him; Christianity had always been central to his life, so ordination offered an attractive second career. He resigned his chair in 1979 to study at Westcott House, Cambridge, an Anglican theological college, becoming an ordained priest on 6 June 1982 (Trinity Sunday). The ceremony was held at Trinity College, Cambridge and presided over by Bishop John A. T. Robinson. He worked for five years as a curate in south Bristol, then as vicar in Blean, Kent, before returning to Cambridge in 1986 as dean of chapel at Trinity Hall. He became the president of Queens' College that year, a position he held until his retirement in 1996. He served as canon theologian of Liverpool Cathedral from 1994 to 2005.
Awards.
In 1997 he was made a Knight Commander of the Order of the British Empire (KBE), although as an ordained priest in the Church of England, he is not styled as "Sir John Polkinghorne". He is an Honorary Fellow of St Chad's College, Durham and awarded an honorary doctorate by the University of Durham in 1998; and in 2002 was awarded the Templeton Prize for his contributions to research at the interface between science and religion. He spoke on "The Universe as Creation" at the Trotter Prize ceremony in 2003.
He has been a member of the BMA Medical Ethics Committee, the General Synod of the Church of England, the Doctrine Commission, and the Human Genetics Commission. He served as chairman of the governors of The Perse School from 1972 to 1981. He is a fellow of Queens' College, Cambridge and was for 10 years a canon theologian of Liverpool Cathedral. He is a founding member of the Society of Ordained Scientists and also of the International Society for Science and Religion, of which he was the first president. He was selected to give the prestigious Gifford Lectures in 1993–1994, which he later published as "The Faith of a Physicist".
In 2006 he was awarded an honorary doctorate by the Hong Kong Baptist University as part of their 50-year celebrations. This included giving a public lecture on "The Dialogue between Science and Religion and Its Significance for the Academy" and an "East–West Dialogue" with Yang Chen-ning, a nobel laureate in physics. He is a member of staff of the Psychology and Religion Research Group at Cambridge University.
Ideas.
Polkinghorne said in an interview that he believes his move from science to religion has given him binocular vision, though he understands that it has aroused the kind of suspicion "that might follow the claim to be a vegetarian butcher." He describes his position as critical realism and believes that science and religion address aspects of the same reality. It is a consistent theme of his work that when he "turned his collar around" he did not stop seeking truth. He believes the philosopher of science who has most helpfully struck the balance between the "critical" and "realism" aspects of this is Michael Polanyi. He argues that there are five points of comparison between the ways in which science and theology pursue truth: moments of enforced radical revision, a period of unresolved confusion, new synthesis and understanding, continued wrestling with unresolved problems, deeper implications.
Because scientific experiments try to eliminate extraneous influences, he believes they are atypical of what goes on in nature. He suggests that the mechanistic explanations of the world that have continued from Laplace to Richard Dawkins should be replaced by an understanding that most of nature is cloud-like rather than clock-like. He regards the mind, soul and body as different aspects of the same underlying reality—"dual aspect monism"—writing that "there is only one stuff in the world (not two—the material and the mental) but it can occur in two contrasting states (material and mental phases, a physicist might say) which explain our perception of the difference between mind and matter." He believes that standard physical causation cannot adequately describe the manifold ways in which things and people interact, and uses the phrase "active information" to describe how, when several outcomes are possible, there may be higher levels of causation that choose which one occurs.
Sometimes Christianity seems to him to be just too good to be true, but when this sort of doubt arises he says to himself, "All right then, deny it," and writes that he knows this is something he could never do.
On the existence of God.
Polkinghorne considers that "the question of the existence of God is the single most important question we face about the nature of reality" and quotes with approval Anthony Kenny: "After all, if there is no God, then God is incalculably the greatest single creation of the human imagination." He addresses the questions of "Does the concept of God make sense? If so, do we have reason for believing in such a thing?" He is "cautious about our powers to assess coherence," pointing out that in 1900 a "competent ... undergraduate could have demonstrated the 'incoherence'" of quantum ideas. He suggests that "the nearest analogy in the physical world God would be ... the Quantum Vacuum."
He suggests that God is the ultimate answer to Leibniz's great question "why is there something rather than nothing?" The atheist's "plain assertion of the world's existence" is a "grossly impoverished view of reality ... that theism explains more than a reductionist atheism can ever address." He is very doubtful of St Anselm's Ontological Argument. Referring to Gödel's incompleteness theory, he said: "If we cannot prove the consistency of arithmetic it seems a bit much to hope that God's existence is easier to deal with," concluding that God is "ontologically necessary, but not logically necessary." He "does not assert that God's existence can be demonstrated in a logically coercive way (any more than God's non-existence can) but that theism makes more sense of the world, and of human experience, than does atheism." He cites in particular:
On free will.
Polkinghorne regards the problem of evil as the most serious intellectual objection to the existence of God. He believes that "The well-known free will defence in relation to moral evil asserts that a world with a possibility of sinful people is better than one with perfectly programmed machines. The tale of human evil is such that one cannot make that assertion without a quiver, but I believe that it is true nevertheless. I have added to it the free-process defence, that a world allowed to make itself is better than a puppet theatre with a Cosmic Tyrant. I think that these two defences are opposite sides of the same coin, that our nature is inextricably linked with that of the physical world which has given us birth."
On creationism.
Polkinghorne accepts evolution. Following the resignation of Michael Reiss, the director of education at the Royal Society—who had controversially argued that school pupils who believed in creationism should be used by science teachers to start discussions, rather than be rejected "per se"—Polkinghorne argued in "The Times" that there is a distinction between believing in the mind and purpose of a divine creator, and what he calls creationism "in that curious North American sense," with a literal interpretation of Genesis 1 and the belief that evolution is wrong, a position he rejects.
Critical reception.
Nancy Frankenberry, Professor of Religion at Dartmouth College, has described Polkinghorne as the finest British theologian/scientist of our time, citing his work on the possible relationship between chaos theory and natural theology. Owen Gingerich, an astronomer and former Harvard professor, has called him a leading voice on the relationship between science and religion.
The British philosopher Simon Blackburn has criticized Polkinghorne for using primitive thinking and rhetorical devices instead of engaging in philosophy. When Polkinghorne argues that the minute adjustments of cosmological constants for life points towards an explanation beyond the scientific realm, Blackburn argues that this relies on a natural preference for explanation in terms of agency. Blackburn writes that he finished Polkinghorne's books in "despair at humanity's capacity for self-deception." Against this, Freeman J. Dyson called Polkinghorne's arguments on theology and natural science "polished and logically coherent." The novelist Simon Ings, writing in the "New Scientist", said Polkinghorne's argument for the proposition that God is real is cogent and his evidence elegant.
Richard Dawkins, formerly Professor for Public Understanding of Science at Oxford, writes that the same three names of British scientists who are also sincerely religious crop up with the "likable familiarity of senior partners in a firm of Dickensian lawyers": Arthur Peacocke, Russell Stannard, and John Polkinghorne, all of whom have either won the Templeton Prize or are on its board of trustees. Dawkins writes that he is not so much bewildered by their belief in a cosmic lawgiver, but by their beliefs in the minutiae of Christianity, such as the resurrection and forgiveness of sins, and that such scientists, in Britain and in the U.S., are the subject of bemused bafflement among their peers. Polkinghorne responded that "debating with Dawkins is hopeless, because there's no give and take. He doesn't give you an inch. He just says no when you say yes" and writes in "Questions of Truth" that he hopes Dawkins will be a bit less baffled once he reads it.
A.C. Grayling criticized the Royal Society for allowing its premises to be used in connection with the launch of "Questions of Truth", describing it as a scandal, and suggesting that Polkinghorne had exploited his fellowship there to publicize a "weak, casuistical and tendentious pamphlet." After implying that the book's publisher, Westminster John Knox, was a self-publisher, Grayling went on to write that Polkinghorne and others were eager to see the credibility accorded to scientific research extended to religious perspectives through association.
In contrast to Grayling, science historian Edward B. Davis praises "Questions of Truth", saying the book provides "the kind of technical information...that scientifically trained readers will appreciate—yet they can be read profitably by anyone interested in science and Christianity." Davis concludes, "It hasn’t been easy to steer a middle course between fundamentalism and modernism, particularly on issues involving science. Polkinghorne has done that very successfully for a generation, and for this he ought to be both appreciated and emulated."
Bibliography.
Polkinghorne has written 34 books, translated into 18 languages; 26 concern science and religion, often for a popular audience.

</doc>
<doc id="50337" url="https://en.wikipedia.org/wiki?curid=50337" title="Fructose">
Fructose

Fructose, or fruit sugar, is a simple ketonic monosaccharide found in many plants, where it is often bonded to glucose to form the disaccharide sucrose. It is one of the three dietary monosaccharides, along with glucose and galactose, that are absorbed directly into the bloodstream during digestion. Fructose was discovered by French chemist Augustin-Pierre Dubrunfaut in 1847. The name "fructose" was coined in 1857 by the English chemist William Miller. Pure, dry fructose is a very sweet, white, odorless, crystalline solid and is the most water-soluble of all the sugars.
Fructose is found in honey, tree and vine fruits, flowers, berries, and most root vegetables.
Commercially, fructose is frequently derived from sugar cane, sugar beets, and corn. Crystalline fructose is the monosaccharide, dried, ground, and of high purity. High-fructose corn syrup (HFCS) is a mixture of glucose and fructose as monosaccharides. Sucrose is a compound with one molecule of glucose covalently linked to one molecule of fructose. All forms of fructose, including fruits and juices, are commonly added to foods and drinks for palatability and taste enhancement, and for browning of some foods, such as baked goods.
About 240,000 tonnes of crystalline fructose are produced annually.
There are speculations that excessive fructose consumption is a cause of insulin resistance, obesity, elevated LDL cholesterol and triglycerides, leading to metabolic syndrome, type 2 diabetes, and cardiovascular disease However, the UK's Scientific Advisory Committee on Nutrition in 2015 disputed the claims, demonstrating that "there is insufficient evidence to demonstrate that fructose intake... leads to adverse health outcomes independent of any effects related to its presence as a component of total and free sugars."
Chemical properties.
Fructose is a 6-carbon polyhydroxyketone. Crystalline fructose adopts a cyclic six-membered structure owing to the stability of its hemiketal and internal hydrogen-bonding. This form is formally called D-fructopyranose. In solution, fructose exists as an equilibrium mixture of 70% fructopyranose and about 22% fructofuranose, as well as small amounts of three other forms, including the acyclic structure.
Reactions.
Fructose and fermentation.
Fructose may be anaerobically fermented by yeast or bacteria. Yeast enzymes convert sugar (glucose, or fructose) to ethanol and carbon dioxide. The carbon dioxide released during fermentation will remain dissolved in water, where it will reach equilibrium with carbonic acid, unless the fermentation chamber is left open to the air. The dissolved carbon dioxide and carbonic acid produce the carbonation in bottled fermented beverages.
Fructose and Maillard reaction.
Fructose undergoes the Maillard reaction, non-enzymatic browning, with amino acids. Because fructose exists to a greater extent in the open-chain form than does glucose, the initial stages of the Maillard reaction occur more rapidly than with glucose. Therefore, fructose has potential to contribute to changes in food palatability, as well as other nutritional effects, such as excessive browning, volume and tenderness reduction during cake preparation, and formation of mutagenic compounds.
Dehydration.
Fructose readily dehydrates to give hydroxymethylfurfural ("HMF").
This process, in the future, may become part of a low-cost, carbon-neutral system to produce replacements for petrol and diesel from plants.
Physical and functional properties.
Sweetness of fructose.
The primary reason that fructose is used commercially in foods and beverages, besides its low cost, is its high relative sweetness. It is the sweetest of all naturally occurring carbohydrates. In general, fructose is regarded as being 1.73 times as sweet as sucrose. However, it is the 6-membered ring form of fructose that is sweeter; the 5-membered ring form tastes about the same as usual table sugar. Warming fructose leads to formation of the 5-membered ring form.
The sweetness of fructose is perceived earlier than that of sucrose or glucose, and the taste sensation reaches a peak (higher than that of sucrose) and diminishes more quickly than that of sucrose. Fructose can also enhance other flavors in the system.
Fructose exhibits a sweetness synergy effect when used in combination with other sweeteners. The relative sweetness of fructose blended with sucrose, aspartame, or saccharin is perceived to be greater than the sweetness calculated from individual components.
Fructose solubility and crystallization.
Fructose has higher solubility than other sugars as well as other sugar alcohols. Fructose is, therefore, difficult to crystallize from an aqueous solution. Sugar mixes containing fructose, such as candies, are softer than those containing other sugars because of the greater solubility of fructose.
Fructose hygroscopicity and humectancy.
Fructose is quicker to absorb moisture and slower to release it to the environment than sucrose, glucose, or other nutritive sweeteners. Fructose is an excellent humectant and retains moisture for a long period of time even at low relative humidity (RH). Therefore, fructose can contribute a more palatable texture, and longer shelf life to the food products in which it is used.
Freezing point.
Fructose has a greater effect on freezing point depression than disaccharides or oligosaccharides, which may protect the integrity of cell walls of fruit by reducing ice crystal formation. However, this characteristic may be undesirable in soft-serve or hard-frozen dairy desserts.
Fructose and starch functionality in food systems.
Fructose increases starch viscosity more rapidly and achieves a higher final viscosity than sucrose because fructose lowers the temperature required during gelatinizing of starch, causing a greater final viscosity.
Although some artificial sweeteners are not suitable for home-baking, many traditional recipes use fructose.
Food sources.
Natural sources of fructose include fruits, vegetables (including sugar cane), and honey. Fructose is often further concentrated from these sources. The highest dietary sources of fructose, besides pure crystalline fructose, are foods containing table sugar (sucrose), high-fructose corn syrup, agave nectar, honey, molasses, maple syrup, fruit and fruit juices, as these have the highest percentages of fructose (including fructose in sucrose) per serving compared to other common foods and ingredients. Fructose exists in foods either as a free monosaccharide or bound to glucose as sucrose, a disaccharide. Fructose, glucose, and sucrose may all be present in a food; however, different foods will have varying levels of each of these three sugars.
The sugar contents of common fruits and vegetables are presented in Table 1. In general, in foods that contain free fructose, the ratio of fructose to glucose is approximately 1:1; that is, foods with fructose usually contain about an equal amount of free glucose. A value that is above 1 indicates a higher proportion of fructose to glucose, and below 1 a lower proportion. Some fruits have larger proportions of fructose to glucose compared to others. For example, apples and pears contain more than twice as much free fructose as glucose, while for apricots the proportion is less than half as much fructose as glucose.
Apple and pear juices are of particular interest to pediatricians because the high concentrations of free fructose in these juices can cause diarrhea in children. The cells (enterocytes) that line children's small intestines have less affinity for fructose absorption than for glucose and sucrose. Unabsorbed fructose creates higher osmolarity in the small intestine, which draws water into the gastrointestinal tract, resulting in osmotic diarrhea. This phenomenon is discussed in greater detail in the Health Effects section.
Table 1 also shows the amount of sucrose found in common fruits and vegetables. Sugarcane and sugar beet have a high concentration of sucrose, and are used for commercial preparation of pure sucrose. Extracted cane or beet juice is clarified, removing impurities; and concentrated by removing excess water. The end-product is 99.9%-pure sucrose. Sucrose-containing sugars include common table white granulated sugar and powdered sugar, as well as brown sugar.
All data with a unit of g (gram) are based on 100 g of a food item.
The fructose/glucose ratio is calculated by dividing the sum of free fructose plus half sucrose by the sum of free glucose plus half sucrose.
Fructose is also found in the synthetically manufactured sweetener, high-fructose corn syrup (HFCS). Hydrolyzed corn starch is used as the raw material for production of HFCS. Through the enzymatic treatment, glucose molecules are converted into fructose. There are three types of HFCS, each with a different proportion of fructose: HFCS-42, HFCS-55, and HFCS-90. The number for each HFCS corresponds to the percentage of synthesized fructose present in the syrup. HFCS-90 has the highest concentration of fructose, and typically, is used to manufacture HFCS-55; HFCS-55 is used as sweetener in soft drinks, whereas HFCS-42 is used in many processed foods and baked goods.
Carbohydrate content of commercial sweeteners (percent on dry basis).
Data obtained from Kretchmer, N. & Hollenbeck, CB (1991). Sugars and Sweeteners, Boca Raton, FL: CRC Press, Inc. for HFCS, and USDA for fruits and vegetables and the other refined sugars.
Cane and beet sugars have been used as the major sweetener in food manufacturing for centuries. However, with the development of HFCS, a significant shift occurred in the type of sweetener consumption in certain countries, particularly the United States. As seen in Figure 3, this change happened in the 1970s. Contrary to the popular belief, however, with the increase of HFCS consumption, the total fructose intake relative to the total glucose intake has not dramatically changed. Granulated sugar is 99.9%-pure sucrose, which means that it has equal ratio of fructose to glucose. The most commonly used forms of HFCS, HFCS-42, and HFCS-55 have a roughly equal ratio of fructose to glucose, with minor differences. HFCS has simply replaced sucrose as a sweetener. Therefore, despite the changes in the sweetener consumption, the ratio of glucose to fructose intake has remained relatively constant.
Fructose digestion and absorption in humans.
Fructose exists in foods either as a monosaccharide (free fructose) or as a unit of a disaccharide (sucrose). Free fructose is absorbed directly by the intestine. When fructose is consumed in the form of sucrose, it is digested (broken down) and then absorbed as free fructose. As sucrose comes into contact with the membrane of the small intestine, the enzyme sucrase catalyzes the cleavage of sucrose to yield one glucose unit and one fructose unit, which are then each absorbed. After absorption, it enters the hepatic portal vein and is directed toward the liver.
The mechanism of fructose absorption in the small intestine is not completely understood. Some evidence suggests active transport, because fructose uptake has been shown to occur against a concentration gradient. However, the majority of research supports the claim that fructose absorption occurs on the mucosal membrane via facilitated transport involving GLUT5 transport proteins. Since the concentration of fructose is higher in the lumen, fructose is able to flow down a concentration gradient into the enterocytes, assisted by transport proteins. Fructose may be transported out of the enterocyte across the basolateral membrane by either GLUT2 or GLUT5, although the GLUT2 transporter has a greater capacity for transporting fructose, and, therefore, the majority of fructose is transported out of the enterocyte through GLUT2.
Capacity and rate of absorption.
The absorption capacity for fructose in monosaccharide form ranges from less than 5 g to 50 g (per individual serving) and adapts with changes in dietary fructose intake. Studies show the greatest absorption rate occurs when glucose and fructose are administered in equal quantities. When fructose is ingested as part of the disaccharide sucrose, absorption capacity is much higher because fructose exists in a 1:1 ratio with glucose. It appears that the GLUT5 transfer rate may be saturated at low levels, and absorption is increased through joint absorption with glucose. One proposed mechanism for this phenomenon is a glucose-dependent cotransport of fructose.
In addition, fructose transfer activity increases with dietary fructose intake. The presence of fructose in the lumen causes increased mRNA transcription of GLUT5, leading to increased transport proteins. High-fructose diets (>2.4 g/kg body wt) increase transport proteins within three days of intake.
Malabsorption.
Several studies have measured the intestinal absorption of fructose using the hydrogen breath test. These studies indicate that fructose is not completely absorbed in the small intestine. When fructose is not absorbed in the small intestine, it is transported into the large intestine, where it is fermented by the colonic flora. Hydrogen is produced during the fermentation process and dissolves into the blood of the portal vein. This hydrogen is transported to the lungs, where it is exchanged across the lungs and is measurable by the hydrogen breath test. The colonic flora also produces carbon dioxide, short-chain fatty acids, organic acids, and trace gases in the presence of unabsorbed fructose. The presence of gases and organic acids in the large intestine causes gastrointestinal symptoms such as bloating, diarrhea, flatulence, and gastrointestial pain Exercise immediately after consumption can exacerbate these symptoms by decreasing transit time in the small intestine, resulting in a greater amount of fructose emptied into the large intestine.
Fructose metabolism.
All three dietary monosaccharides are transported into the liver by the GLUT2 transporter. Fructose and galactose are phosphorylated in the liver by fructokinase (Km= 0.5 mM) and galactokinase (Km = 0.8 mM). By contrast, glucose tends to pass through the liver (Km of hepatic glucokinase = 10 mM) and can be metabolised anywhere in the body. Uptake of fructose by the liver is not regulated by insulin. However, insulin is capable of increasing the abundance and functional activity of GLUT5 in skeletal muscle cells.
Fructolysis.
The initial catabolism of fructose is sometimes referred to as fructolysis, in analogy with glycolysis, the catabolism of glucose. In fructolysis, the enzyme fructokinase initially produces fructose 1-phosphate, which is split by aldolase B to produce the trioses dihydroxyacetone phosphate (DHAP) and glyceraldehyde [http://www.ajcn.org/cgi/content-nw/full/76/5/911/F3]. Unlike glycolysis, in fructolysis the triose glyceraldehyde lacks a phosphate group. A third enzyme, triokinase, is therefore required to phosphorylate glyceraldehyde, producing glyceraldehyde 3-phosphate. The resulting trioses are identical to those obtained in glycolysis and can enter the gluconeogenic pathway for glucose or glycogen synthesis, or be further catabolized through the lower glycolytic pathway to pyruvate.
Metabolism of fructose to DHAP and glyceraldehyde.
The first step in the metabolism of fructose is the phosphorylation of fructose to fructose 1-phosphate by fructokinase, thus trapping fructose for metabolism in the liver. Fructose 1-phosphate then undergoes hydrolysis by aldolase B to form DHAP and glyceraldehydes; DHAP can either be isomerized to glyceraldehyde 3-phosphate by triosephosphate isomerase or undergo reduction to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The glyceraldehyde produced may also be converted to glyceraldehyde 3-phosphate by glyceraldehyde kinase or further converted to glycerol 3-phosphate by glycerol 3-phosphate dehydrogenase. The metabolism of fructose at this point yields intermediates in the gluconeogenic pathway leading to glycogen synthesis as well as fatty acid and triglyceride synthesis.
Synthesis of glycogen from DHAP and glyceraldehyde 3-phosphate.
The resultant glyceraldehyde formed by aldolase B then undergoes phosphorylation to glyceraldehyde 3-phosphate. Increased concentrations of DHAP and glyceraldehyde 3-phosphate in the liver drive the gluconeogenic pathway toward glucose and subsequent glycogen synthesis. It appears that fructose is a better substrate for glycogen synthesis than glucose and that glycogen replenishment takes precedence over triglyceride formation. Once liver glycogen is replenished, the intermediates of fructose metabolism are primarily directed toward triglyceride synthesis.
Synthesis of triglyceride from DHAP and glyceraldehyde 3-phosphate.
Carbons from dietary fructose are found in both the free fatty acid and glycerol moieties of plasma triglycerides. High fructose consumption can lead to excess pyruvate production, causing a buildup of Krebs cycle intermediates. Accumulated citrate can be transported from the mitochondria into the cytosol of hepatocytes, converted to acetyl CoA by citrate lyase and directed toward fatty acid synthesis. In addition, DHAP can be converted to glycerol 3-phosphate, providing the glycerol backbone for the triglyceride molecule. Triglycerides are incorporated into very-low-density lipoproteins (VLDL), which are released from the liver destined toward peripheral tissues for storage in both fat and muscle cells.
Fructose synthesis.
Humans are capable of synthesising fructose from glucose via the polyol pathway. This pathway is used by the seminal vesicles to secrete fructose into semen where it is a major source of energy for sperm.
Potential health effects.
Digestion.
Fructose absorption occurs in the small intestine via the GLUT-5 (fructose only) transporter, and the GLUT2 transporter, for which it competes with glucose and galactose. Over-consumption of fructose, inhibition of GLUT2 by other phytochemicals, such as flavonoids, or other issues, may result in delivery of unabsorbed fructose into the large intestine, which will cause more water to be drawn into the large intestine through the process of osmosis causing diarrhea. In addition, the excessive fructose becomes a source of nutrients for the gut flora resulting in a higher production of short chain fatty acids, hydrogen, carbon dioxide and other gases due to fermentation. This increase of gas causes gastrointestinal side effects that mimic irritable bowel syndrome.
Weight Gain.
In a meta-analysis of clinical trials with controlled feeding — where test subjects were fed a fixed amount of energy rather than being allowed to choose the amount they ate — fructose was not an independent factor for weight gain. In one study of a diet with excessive calories, fructose consumption was associated with weight gain.
Cardiometabolic diseases.
Excess fructose consumption has been hypothesized to be a cause of insulin resistance, obesity, elevated LDL cholesterol and triglycerides, leading to metabolic syndrome. In preliminary research, fructose consumption was correlated with obesity. Fructose encourages visceral adipose tissue build-up and ectopic fat deposition both in non-human animal models and in humans.
There have been many studies which indicate that there may be an increased risk of cardiovascular disease from a high intake of fructose. Studies have associated high fructose consumption with increased incidence of hypertension, both acutely and in the long term in subjects without a history of hypertension. The mechanistic link for this is proposed to be through the increased level of uric acid leading to hyperuricemia which is a known predictor for hypertension. The elevation in uric acid is due to unregulated phosphorylation leading to depletion of ATP and subsequent ADP degradation to uric acid.
Another study in humans concluded that fructose and sucrose are metabolized similarly, whereas a different analysis "produced significantly higher fasting plasma triglyceride values than did the glucose diet in men" and "...if plasma triacylglycerols are a risk factor for cardiovascular disease, then diets high in fructose may be undesirable". A study in 2015 later confirmed this by showing that consuming beverages with high levels of high fructose corn syrup caused heightened levels of LDL cholesterol, non-HDL cholesterol, apolipoprotein B, all of which are lipid/lipoproteins risk factors for cardiovascular disease.
Although all simple sugars have nearly identical chemical formulae, each has distinct chemical properties. This can be illustrated with pure fructose. A journal article reports that, "...fructose given alone increased the blood glucose almost as much as a similar amount of glucose (78% of the glucose-alone area)".
Skin and bone damage.
Fructose is a reducing sugar. The spontaneous chemical reaction of simple sugar molecules binding to proteins is known as glycation. Another study using human proteins showed that the glycation caused by fructose appears to be equivalent to that caused by glucose and so does not seem to be a better answer for diabetes for this reason alone, save for the smaller quantities required to achieve equivalent sweetness in some foods. It also found evidence for glycation of human lens proteins caused by fructose.
Compared with sucrose.
Fructose is often recommended for diabetics because it does not trigger the production of insulin by pancreatic β cells, probably because β cells have low levels of GLUT5, although the net effect for both diabetics and non-diabetics is debated. Fructose has a very low glycemic index of 19 ± 2, compared with 100 for glucose and 68 ± 5 for sucrose. Fructose is also 73% sweeter than sucrose at room temperature, so diabetics can use less of it. Studies show that fructose consumed before a meal may even lessen the glycemic response of the meal.
Liver function.
Excessive fructose consumption may contribute to the development of non-alcoholic fatty liver disease.
Gout.
A 2008 study found a risk of incident gout associated with high consumption of fructose or fructose-rich foods.
Glycemic index.
Fructose has the lowest glycemic index (GI = 19) of all the natural sugars. In comparison, ordinary table sugar (sucrose, which is half fructose) has a GI of 65 and honey (usually about 50% fructose content) has a GI of 55.
Appetite control.
Compared with consumption of high glucose beverages, drinking high-fructose beverages with meals results in lower circulating insulin and leptin levels, and higher ghrelin levels after the meal. Since leptin and insulin decrease appetite and ghrelin increases appetite, some researchers suspect that eating large amounts of fructose increases the likelihood of weight gain.

</doc>
<doc id="50338" url="https://en.wikipedia.org/wiki?curid=50338" title="'s-Hertogenbosch">
's-Hertogenbosch

's-Hertogenbosch (, literally "The Duke's Forest" in English, and historically in French: Bois-le-Duc) is a city and municipality in the southern Netherlands. It is the capital of the province of North Brabant.
In speech, the Dutch seldom use the formal "'s-Hertogenbosch" but rather the colloquial Den Bosch . Den Bosch means "The Forest".
Population centres.
Bokhoven, Crevecoeur, Deuteren (former village), Dieskant, Empel, Engelen, Gewande,
's-Hertogenbosch, Hintham, Kruisstraat, Maliskamp, Meerwijk, Orthen (former village), Oud-Empel and Rosmalen.
History.
The city's official name is a contraction of the Dutch "des Hertogen bosch"—"the Duke's forest". The duke in question was Duke Henry I of Brabant, whose family had owned a large estate at nearby Orthen for at least four centuries. He founded a new town located on some forested dunes in the middle of a marsh. At age 26, he granted 's-Hertogenbosch city rights and the corresponding trade privileges in 1185. This is, however, the traditional date given by later chroniclers; the first mention in contemporaneous sources is 1196. The original charter has been lost. His reason for founding the city was to protect his own interests against encroachment from Gelre and Holland; from its first days, he conceived of the city as a fortress. It was destroyed in 1203 in a joint expedition of Gelre and Holland, but was soon rebuilt. Some remnants of the original city walls may still be seen. In the late 14th century, a much larger wall was erected to protect the greatly expanded settled area. Artificial waterways were dug to serve as a city moat, through which the rivers Dommel and Aa were diverted.
Until 1520, the city flourished, becoming the second largest population centre in the territory of the present Netherlands, after Utrecht. The birthplace and home of one of the greatest painters of the northern Renaissance, Hieronymus Bosch, the city was also a center of music, and composers, such as Jheronimus Clibano, received their training at its churches. Others held positions there: Matthaeus Pipelare was musical director at the Confraternity of Our Lady; and renowned Habsburg copyist and composer Pierre Alamire did much of his work at 's-Hertogenbosch.
Eighty Years' War.
The wars of the Reformation changed the course of the city's history. It became an independent bishopric. During the Eighty Years' War, the city took the side of the Habsburg (Catholic) authorities and thwarted a Calvinist coup. It was besieged several times by Prince Maurice of Orange, stadtholder of most of the Dutch Republic, who wanted to bring 's-Hertogenbosch under the rule of the rebel United Provinces. The city was successfully defended by Claude de Berlaymont, also known as Haultpenne.
Thirty Years' War.
In the years of Truce, before the renewed fighting after 1618, the fortifications were greatly expanded. The surrounding marshes made a siege of the conventional type impossible, and the fortress, deemed impregnable, was nicknamed the Marsh Dragon. The town was nevertheless finally conquered by Frederik Hendrik of Orange in 1629 in a typically Dutch stratagem: he diverted the rivers Dommel and Aa, created a polder by constructing a forty-kilometre dyke and then pumped out the water by mills. After a siege of three months, the city had to surrender—an enormous blow to Habsburg geo-political strategy during the Thirty Years' War. This surrender cut the town off from the rest of the duchy and the area was treated by the Republic as an occupation zone without political liberties (see also Generality Lands).
Louis XIV to Bonaparte.
After the Peace of Westphalia, the fortifications were again expanded. In 1672, the Dutch "rampjaar", the city held against the army of Louis XIV of France. In 1794, French revolutionary troops under command of Charles Pichegru took the city with hardly a fight: in the Batavian Republic, both Catholics and "Brabanders" at last gained equal rights.
From 1806, the city became part of the Kingdom of Holland and, from 1810, it was incorporated into the French Empire. It was captured by the Prussians in 1814.
Kingdom of the Netherlands.
The next year, 1815, when the United Kingdom of the Netherlands was established, it became the capital of North Brabant. Many newer and more modern fortresses were created in the vicinity of the city. A new canal was built, the 'Zuid-Willemsvaart', which gave the city an economic impulse. Trade, manufacturing and industry grew. Until 1878 it was forbidden to build outside the ramparts. This led to overcrowding and the highest infant mortality in the kingdom. At the end of the 19th century the very conservative city government prevented industrial investment—they didn't want the number of workers to grow—and the establishment of educational institutions—students were regarded as disorderly. As a result, the relative importance of the city diminished.
World War II and after.
One of the few official Nazi concentration camp complexes in western Europe located outside Germany and Austria was named after 's-Hertogenbosch. It operated from January 1943, to September 1944 and was known to the Germans as Herzogenbusch (see List of subcamps of Herzogenbusch). About 30,000 inmates were interned in the complex during this time, of whom about 12,000 were Jews. In the Netherlands, this camp is known as 'Kamp Vught', because the concentration camp was actually located at a heath near Vught, a village a few kilometres south of 's-Hertogenbosch.
Conquered by the Germans in World War II (1940), with its railway station bombed by planes of the Royal Air Force on 16 September 1944, it was liberated in 24–27 October 1944 by British soldiers of the 53rd (Welsh) Infantry Division after Major Donald Bremner of the 1st Battalion, East Lancashire Regiment, of 158th Infantry Brigade, had already routed the enemy on 23/24th.
Main sights.
's-Hertogenbosch was founded as a fortified city and that heritage can still be seen today. After World War II, plans were made to modernise the old city, by filling in the canals, removing or modifying some ramparts and redeveloping historic neighbourhoods. Before these plans could come to effect however, the central government declared the city a protected townscape. Most historic elements have been preserved. Because the main ramparts are crucial in keeping out the water, they have never been slighted, their usual fate in the Netherlands. In contrast to cities like Rotterdam, 's-Hertogenbosch also survived the Second World War relatively unscathed. Much of its historic heritage remains intact, and today there are always renovations going on in the city to preserve the many old buildings, fortifications, churches and statues for later generations. In 2004 the city was awarded the title "European Fortress City of the year". It is planned to restore the city defences to much of their old glory in the coming years. 's-Hertogenbosch also has the oldest remaining brick house in the Netherlands, 'de Moriaan', which was built at the beginning of the 13th century. In the 1960s, de Moriaan was renovated to its former glory based on a famous 16th-century Dutch painting called 'De Lakenmarkt van 's-Hertogenbosch' ('The fabric market of 's-Hertogenbosch'). In the north of the old city, the hexagonal powder arsenal, or "Kruithuis", still exists, one of only two of its kind in the country. The Townhall is an originally 14th-century Gothic building, transformed in the typical style of Dutch classicism in the 17th century. Around the city itself many other fortresses can still be seen. Until recently it was a major garrison town.
The old city of 's-Hertogenbosch is still almost completely surrounded by continuous ramparts. On the south side, this wall still borders on an old polder, kept intact as a nature reserve, that stretches all the way to Vught. These city walls are currently undergoing renovations. Hidden below the old city is a canal network called the Binnendieze that once spanned . It started out as a regular river, the Dommel, running through the city in medieval times but due to lack of space in the city, people started building their houses and roads over the river. In later times it functioned as a sewer and fell into disrepair. In recent decades, the remaining sixth of the old waterway system has been renovated, and it is possible to take several guided subterranean boat trips through it.
's-Hertogenbosch is also home to Saint John's Cathedral ("Sint Jans kathedraal" in Dutch), which dates from c. 1220 and is best known for its Brabantine Gothic design and the many sculptures of craftsmen that are sitting on almost every arc and rim along the outside of the cathedral. In 2010 an extensive restoration was completed, undoing the damage of many years of wear-and-tear and acid rain.
Museums are the Stedelijk Museum 's-Hertogenbosch, Noordbrabants Museum, Jheronimus Bosch Art Center and Museum Slager.
The painter Hieronymus Bosch (c. 1450–1516) remains probably the best known citizen of 's-Hertogenbosch.
The city is also the location of the "Bolwoningen" complex, an array of fifty experimental spherical houses designed by Dries Kreijkamp.
Economy.
The city of 's-Hertogenbosch has become a center of industry, education, administration and culture. It is currently the fourth city of Noord Brabant. It is home to many national and international businesses such as Heineken, Epic, Tyco International and many others.
Culture.
's-Hertogenbosch is home to a variety of events such as the theatre festival "Boulevard", "Jazz in Duketown", and hip hop in duketown, the start of the Tour de France (1996), Tour Feminin (1997), the International Vocal Competition, November Music (a contemporary music festival) and the UNICEF Open (formerly the Ordina Open) grass court tennis tournament (in the nearby town of Rosmalen). There are also over 350 restaurants, pubs and cafés to be found in the city.
's-Hertogenbosch is also home to the European Ceramic Work Centre. This is a juried international ceramic residency where they invite artists, designers and architects from around to the world to explore the medium of Ceramics. This program was initially started in 1991 and continues to this day.
The city has its own food speciality, the Bossche Bol—effectively a giant profiterole, somewhat larger than a tennis ball, which is filled with whipped cream and coated with chocolate.
The spoken language is Maaslands (The variant spoken in 's-Hertogenbosch is called "Bosch" which is placed among the Central North Brabantian dialects, although other classification systems also describe it as East Brabantian), which is very similar to colloquial Dutch.
De Toonzaal is a music venue for chamber music, improvised music, and experimental music. For popular music there is the venue W2 (or Willem II).
Sport.
The city has one professional football club, FC Den Bosch (first club of Dutch international player Ruud van Nistelrooy), and is also the home to top field hockey club HC Den Bosch, basketball team EiffelTowers Den Bosch and 2008 national rugby champion The Dukes. The city is also host to the Rosmalen Grass Court Championships, a combined ATP Tour and WTA Tour grass court tennis event played two weeks before the Wimbledon Championships.
Carnival celebrations.
Once a year, 's-Hertogenbosch changes its name to "Oeteldonk". Contrary to popular belief, "oetel" in the name "Oeteldonk" is not a referral to a frog but is a facetious reference to the 's-Hertogenbosch Bishop Adrianus Godschalk who came from the village of Den Dungen, and he often fulminated against the 'pagan' Carnival festivities. Van den Oetelaar was a very common name in Den Dungen at that time. "Donk" is a reference to a dry place in the marsh. The frog is however a symbol often used during Carnival, and it is a symbol of the Oeteldonk Marsh.
This change only lasts for the three days of Carnival even though this original meaning has disappeared to the background. The Mayor then hands over his duties temporarily to "Peer vaan den Muggenheuvel tot den Bobberd" during this three-day festival. "Peer vaan den Muggenheuvel tot den Bobberd" is the host of Prince Carnaval "Prince Amadeiro XXV" when he visits Oeteldonk.
Transport.
Like most Dutch cities, 's-Hertogenbosch is well adapted to the high number of cyclists. A large network of bike paths make it convenient to cycle to various destinations and within the town the bike is the most popular mean of transportation. In 2011, the city was chosen as "Fietsstad 2011" — the top BikeCity of the Netherlands for 2011.
As for trains, 's-Hertogenbosch has three railway stations:
As for buses, Arriva buslines serve the city and most of its suburbs.
Geography.
Climate.
Climate in this area has mild differences between highs and lows, and there is adequate rainfall year round. The Köppen Climate Classification subtype for this climate is "". (Marine West Coast Climate/Oceanic climate).

</doc>
<doc id="50339" url="https://en.wikipedia.org/wiki?curid=50339" title="Rauni">
Rauni

Rauni may mean:

</doc>
<doc id="50342" url="https://en.wikipedia.org/wiki?curid=50342" title="Thomas Harrison (soldier)">
Thomas Harrison (soldier)

Major-General Thomas Harrison (1606 – 13 October 1660) sided with Parliament in the English Civil War. During the Interregnum he was a leader of the Fifth Monarchists. In 1649 he signed the death warrant of Charles I and in 1660, shortly after the Restoration, he was found guilty of regicide and hanged, drawn and quartered.
Life and work.
The son of the mayor of Newcastle-under-Lyme, he moved to London where he was admitted to the Inns of Court as an attorney at Clifford's Inn. Whilst a lawyer he met Charles Fleetwood and Edmund Ludlow. He enlisted in Earl of Essex's lifeguard in 1642 and experienced his baptism of fire at the Battle of Powick Bridge
First English Civil War.
During the Civil War he declared for Parliament and served in the Earl of Manchester's army. He fought in many of the major battles of the war and joined the New Model Army in 1645. By the end of the conflict he had risen to the rank of major-general and was a noted friend and supporter of Oliver Cromwell.
He was elected to the Long Parliament for Wendover in 1646. His regiment maintained strong leveller sympathies, mutinying in 1647.
Second English Civil War.
When conflict resumed he was wounded at Appleby in July 1648. He had to return to London but was well enough to command the escort that brought the King to London in January 1649. Harrison sat as a commissioner (judge) at the trial and was the seventeenth of fifty-nine commissioners to sign the death warrant of King Charles I.
In 1650, Harrison was appointed to a military command in Wales where he was apparently extremely severe. He was promoted to the rank of Major-General in 1651 and commanded the army in England during . He fought at the battle of Knutsford in August and at Worcester in September 1651.
By the early 1650s Harrison was associated with the radical Fifth Monarchists and became one of their key speakers. He still supported Cromwell and aided in the dissolution of the Rump Parliament in April 1653. Harrison was a radical member of the Nominated Assembly (Barebones Parliament) that replaced the Parliament. When the Assembly was dissolved, Harrison and others refused to leave and had to be forced out by soldiers. Harrison was dismissed from the Army in December.
Like many, he was outraged by the formation of the Protectorate and the elevation of Cromwell to Lord Protector. Under the Protectorate (1653–60) Harrison was imprisoned four times.
Arrest and Trial.
After Cromwell's death Harrison remained quietly in his home, supporting none of the contenders for power. Following the Restoration, Harrison declined to flee and was arrested in May 1660.
He was tried on 11 October 1660. Edmond Ludlow described the trial in his memoirs, 
Execution.
Major-General Harrison was the first of the Regicides to be executed by being hanged, drawn and quartered on 13 October 1660. Harrison, after being hanged for several minutes and then cut open, was reported to have leaned across and hit his executioner—resulting in the swift removal of his head. His entrails were thrown onto a nearby fire.
Samuel Pepys wrote an eyewitness account of the execution at Charing Cross, in which Major General Harrison was dryly reported to be "looking as cheerful as any man could do in that condition". This account is also quoted on a plaque on the wall of the Hung, Drawn and Quartered public house near Pepys Street, where the diarist lived and worked in the Navy Office. In his final moments, as he was being led up the scaffold, the hangman asked for his forgiveness. Upon hearing his request Thomas Harrison replied, "I do forgive thee with all my heart... Alas poor man, thou doith it ignorantly, the Lord grant that this sin may be not laid to thy charge." Thomas Harrison then gave all of the money that remained in his pockets to his executioner and was thereafter executed.
Edmond Ludlow also provided an account of the execution at Charing Cross, "the sentence which had been pronounced in consequence of the verdict was executed upon Major-General Harrison at the place where Charing Cross formerly stood, that the King might have the pleasure of the spectacle, and inure himself to blood. According to Ludlow, "On the fifteenth (15 October 1660), Mr. John Carew suffered there also, even their enemies confessing that more steadiness of mind, more contempt of death, and more magnanimity could not be expressed. To all who were present with them either in prison or at the place where the sentence was executed, they owned that having engaged in the cause of God and their country, they were not at all ashamed to suffer in the manner their enemies thought fit, openly avowing the inward satisfaction of their minds when they reflected upon the actions for which they had been condemned, not doubting the revival of the same cause; and that a time should come when men would have better thoughts of their persons and proceedings."

</doc>
<doc id="50344" url="https://en.wikipedia.org/wiki?curid=50344" title="Frontier">
Frontier

A frontier is the political and geographical areas near or beyond a boundary. The term came from French in the 15th century, with the meaning "borderland"—the region of a country that fronts on another country (see also marches).
The word "frontier" also means a region at the edge of a settled area, especially in North American development. It is a transition zone where explorers, pioneers and settlers were arriving. A frontier can also be referred to as a "front".
Frederick Jackson Turner said that "the significance of the frontier" was that as pioneers moved into the "frontier zone", they were changed by the encounter. For example, Turner argues that in the United States in 1893, unlimited free land in this zone was available, and thus offered the psychological sense of unlimited opportunity. This, in turn, had many consequences such as optimism, future orientation, shedding of restraints due to land scarcity, and wasting of natural resources.
Colonial North America.
In the earliest days of European settlement of the Atlantic coast, the frontier was essentially any part of the forested interior of the continent lying beyond the fringe of existing settlements along the coast and the great rivers, such as the St. Lawrence, Connecticut, Hudson, Delaware, Susquehanna River and James.
English, French, Spanish and Dutch patterns of expansion and settlement were quite different. Only a few thousand French migrated to Canada. These habitants settled in villages along the St. Lawrence river, building communities that remained stable for long stretches, rather than leapfrogging west the way the English and later Americans did. Although French fur traders ranged widely through the Great Lakes and Mississippi River watersheds, as far as the Rocky Mountains, they did not usually settle down. Actual French settlement in these areas was limited to a few very small villages on the lower Mississippi and in the Illinois Country. Likewise, the Dutch set up fur trading posts in the Hudson River valley, followed by large grants of land to patroons, who brought in tenant farmers that created compact, permanent villages. They did not push westward.
In contrast, the English colonies generally pursued a more systematic policy of widespread settlement of the New World, for cultivation and exploitation of the land, which required the extension of European property rights to the new continent. The typical English settlements were quite compact and small—under . Conflict with the Native Americans arose out of political issues, i.e. who would rule. Early frontier areas east of the Appalachian Mountains included the Connecticut River valley. The French and Indian Wars of the 1760s resulted in a complete victory for the British, who took over the French colonial territory west of the Appalachians to the Mississippi River. Americans began moving across the Appalachians into areas such the Ohio Country and the New River Valley.
Most of the frontier movement was east to west, but there were other directions as well. The frontier in New England lay to the north; in Nevada to the east; in Florida to the south. Throughout American history, the expansion of settlement was largely from the east to the west, and thus the frontier is often identified with "the west". On the Pacific Coast, settlement moved eastward. In New England, it moved north.
United States.
Following the victory of the United States in the American Revolutionary War and the signing Treaty of Paris in 1783, the United States gained formal, if not actual, control of the British lands west of the Appalachians. Many thousands of settlers, typified by Daniel Boone, had already reached Kentucky and Tennessee and adjacent areas. Some areas, such as the Virginia Military District and the Connecticut Western Reserve (both in Ohio), were used by the states as rewards to veterans of the war. How to formally include these new frontier areas into the nation was an important issue in the Continental Congress of the 1780s and was partly resolved by the Northwest Ordinance (1787). The Southwest Territory saw a similar pattern of settlement pressure.
For the next century, the expansion of the nation into these areas, as well as the subsequently acquired Louisiana Purchase, Oregon Country, and Mexican Cession, attracted hundreds of thousands of settlers. The question of whether the Kansas frontier would become "slave" or "free" was a spark of the American Civil War. In general before 1860, Northern Democrats promoted easy land ownership and Whigs and Southern Democrats resisted. The Southerners resisted Homestead Acts because it supported the growth of a free farmer population that might oppose slavery.
When the Republican Party came to power in 1860 they promoted a free land policy — notably the Homestead Act of 1862, coupled with railroad land grants that opened cheap (but not free) lands for settlers. In 1890, the frontier line had broken up (Census maps defined the frontier line as a line beyond which the population density was under ).
The popular culture impact of the frontier was enormous, in dime novels, Wild West shows, and, after 1910, Western movies set on the frontier.
The American frontier was generally the most Western edge of settlements and typically more free-spirited in nature than the East because of its lack of social and political institutions. The idea that the frontier provided the core defining quality of the United States was elaborated by the historian Frederick Jackson Turner, who built his Frontier Thesis in 1893 around this notion. Subsequently, the frontier has also been described as the point of contact between two cultures, where contact led to exchanges that affected both cultures.
Canadian frontier.
A "Canadian frontier thesis" was developed by Canadian historians Harold Adams Innis and J. M. S. Careless. They emphasized the relationship between the center and periphery. Katerberg argues that "in Canada the imagined West must be understood in relation to the mythic power of the North." 2003 In Innis's 1930 work "The Fur Trade in Canada", he expounded on what became known as the Laurentian thesis: that the most creative and major developments in Canadian history occurred in the metropolitan centers of central Canada and that the civilization of North America is the civilization of Europe. Innis considered place as critical in the development of the Canadian West and wrote of the importance of metropolitan areas, settlements, and indigenous people in the creation of markets. Turner and Innis continue to exert influence over the historiography of the American and Canadian Wests. The Quebec frontier showed little of the individualism or democracy that Turner ascribed to the American zone to the south. The Nova Scotia and Ontario frontiers were rather more democratic than the rest of Canada, but whether that was caused by the need to be self-reliant at the frontier itself, or the presence of large numbers of American immigrants is debated.
The Canadian political thinker Charles Blattberg has argued that such events ought to be seen as part of a process in which Canadians advanced a "border" as distinct from a "frontier" – from east to west. According to Blattberg, a border assumes a significantly sharper contrast between the civilized and the uncivilized since, unlike a frontier process, the civilizing force is not supposed to be shaped by that which it is civilizing. Blattberg criticizes both the frontier and border "civilizing" processes.
Canadian prairies.
The pattern of settlement of the Canadian prairies began in 1896, when the American prairie states had already achieved statehood. Pioneers then headed north to the "Last Best West".
Before settlers began to arrive, North-West Mounted Police were dispatched to the region. When settlers began to arrive, a system of law and order was already in place and the Dakota lawlessness for which the American "Wild West" was famed did not occur in Canada. Before settlers arrived, the federal government also sent teams of negotiators to meet with the Native peoples of the region. In a series of treaties, the basis for peaceful relations was established and the long wars with the Natives that occurred in the United States largely did not spread to Canada. Like their American counterparts, the Prairie provinces supported populist and democratic movements in the early 20th century.
Australia.
The term frontier was frequently used in colonial Australia in the meaning of country that borders the unknown or uncivilised, the boundary, border country, the borders of civilisation, or as the land that forms the furthest extent of what was frequently termed "the inside" or "settled" districts. The "outside" was another term frequently used in colonial Australia, this term seemingly covered not only the frontier but the districts beyond. Settlers at the frontier thus frequently referred to themselves as 'the outsiders' or 'outside residents' and the area in which they lived as "the outside districts". At times one might hear the "fronter" described as "the outside borders". However the term "frontier districts" was seemingly used predominantly in the early Australian colonial newspapers whenever dealing with skirmishes between black and white in northern New South Wales and Queensland, and in newspaper reports from South Africa, whereas it was seemingly not so commonly used when dealing with affairs in Victoria, South Australia and southern New South Wales. The use of the word "frontier" was thus frequently connected to descriptions of frontier violence, as in a letter printed in the "Sydney Morning Herald" in December 1850 which described murder and carnage at the northern frontier and calling for the protection of the settlers saying: "...nothing but a strong body of Native Police will restore and keep order in the frontier districts, and as the squatters are taxed for the purpose of such protection". 
European Union.
In the European Union, the frontier is the region beyond the expanding borders of the European Union itself. The E.U. has designated the countries surrounding it as part of the European Neighbourhood. This is a region of primarily less-developed countries, many of which aspire to become part of the E.U. Current applicants include Turkey and many small countries in the Balkans and South Caucasus. Romania and Bulgaria joined the E.U. in 2007. Proposals to admit Turkey have been debated but are now currently stalled, partly on the ground that Turkey is beyond Europe's historic frontier and it is yet to comply with the 35 point policy areas set out by the E.U. If all or most East European states become members, the frontier may be the boundaries with Russia and Turkey.

</doc>
<doc id="50345" url="https://en.wikipedia.org/wiki?curid=50345" title="Urban design">
Urban design

Urban design is the process of designing and shaping cities, towns and villages. In contrast to architecture, which focuses on the design of individual buildings, urban design deals with the larger scale of groups of buildings, streets and public spaces, whole neighborhoods and districts, and entire cities, with the goal of making urban areas functional, attractive, and sustainable.
Urban design is an inter-disciplinary subject that utilizes elements of many built environment professions, including landscape architecture, urban planning, architecture, civil and municipal engineering. It is common for professionals in all these disciplines to practice in urban design. In more recent times different sub-strands of urban design have emerged such as strategic urban design, landscape urbanism, water-sensitive urban design, and sustainable urbanism.
Urban design demands a good understanding of a wide range of subjects from physical geography, through to social science, and an appreciation for disciplines, such as real estate development, urban economics, political economy and social theory.
Urban design is about making connections between people and places, movement and urban form, nature and the built fabric. Urban design draws together the many strands of place-making, environmental stewardship, social equity and economic viability into the creation of places with distinct beauty and identity. Urban design draws these and other strands together creating a vision for an area and then deploying the resources and skills needed to bring the vision to life.
Urban design theory deals primarily with the design and management of public space (i.e. the 'public environment', 'public realm' or 'public domain'), and the way public places are experienced and used. Public space includes the totality of spaces used freely on a day-to-day basis by the general public, such as streets, plazas, parks and public infrastructure. Some aspects of privately owned spaces, such as building facades or domestic gardens, also contribute to public space and are therefore also considered by urban design theory. Important writers on urban design theory include Christopher Alexander, Peter Calthorpe, Gordon Cullen, Andres Duany, Jane Jacobs, Mitchell Joachim, Jan Gehl, Allan B. Jacobs, Kevin Lynch, Aldo Rossi, Colin Rowe, Robert Venturi, William H. Whyte, Camillo Sitte, Bill Hillier (Space syntax), Elizabeth Plater-Zyberk and Kelvin Campbell.
History.
Although contemporary professional use of the term 'urban design' dates from the mid-20th century, urban design as such has been practiced throughout history. Ancient examples of carefully planned and designed cities exist in Asia, Africa, Europe and the Americas, and are particularly well-known within Classical Chinese, Roman and Greek cultures (see Hippodamus of Miletus).
European Medieval cities are often, and often erroneously, regarded as exemplars of undesigned or 'organic' city development. There are many examples of considered urban design in the Middle Ages (see, e.g., David Friedman, "Florentine New Towns: Urban Design in the Late Middle Ages", MIT 1988). In England, many of the towns listed in the 9th century Burghal Hidage were designed on a grid, examples including Southampton, Wareham, Dorset and Wallingford, Oxfordshire, having been rapidly created to provide a defensive network against Danish invaders. 12th century western Europe brought renewed focus on urbanisation as a means of stimulating economic growth and generating revenue. The burgage system dating from that time and its associated burgage plots brought a form of self-organising design to medieval towns. Rectangular grids were used in the Bastides of 13th and 14th century Gascony, and the new towns of England created in the same period.
Throughout history, design of streets and deliberate configuration of public spaces with buildings have reflected contemporaneous social norms or philosophical and religious beliefs (see, e.g., Erwin Panofsky, "Gothic Architecture and Scholasticism", Meridian Books, 1957). Yet the link between designed urban space and human mind appears to be bidirectional. Indeed, the reverse impact of urban structure upon human behaviour and upon thought is evidenced by both observational study and historical record. There are clear indications of impact through Renaissance urban design on the thought of Johannes Kepler and Galileo Galilei (see, e.g., Abraham Akkerman, "Urban planning in the founding of Cartesian thought," "Philosophy and Geography" 4(1), 2001). Already René Descartes in his "Discourse on the Method" had attested to the impact Renaissance planned new towns had upon his own thought, and much evidence exists that the Renaissance streetscape was also the perceptual stimulus that had led to the development of coordinate geometry (see, e.g., Claudia Lacour Brodsky, "Lines of Thought: Discourse, Architectonics, and the Origins of Modern Philosophy", Duke 1996).
The beginnings of modern urban design in Europe are associated with the Renaissance but, especially, with the Age of Enlightenment. Spanish colonial cities were often planned, as were some towns settled by other imperial cultures. These sometimes embodied utopian ambitions as well as aims for functionality and good governance, as with James Oglethorpe's plan for Savannah, Georgia. In the Baroque period the design approaches developed in French formal gardens such as Versailles were extended into urban development and redevelopment. In this period, when modern professional specialisations did not exist, urban design was undertaken by people with skills in areas as diverse as sculpture, architecture, garden design, surveying, astronomy, and military engineering. In the 18th and 19th centuries, urban design was perhaps most closely linked with surveyors (engineers) and architects. The increase in urban populations brought with it problems of epidemic disease, the response to which was a focus on public health, the rise in the UK of municipal engineering and the inclusion in British legislation of provisions such as minimum widths of street in relation to heights of buildings in order to ensure adequate light and ventilation.
Much of Frederick Law Olmsted's work was concerned with urban design, and the newly formed profession of landscape architecture also began to play a significant role in the late 19th century.
Modern urban design.
Modern urban design is a part of the wider discipline of Urban planning. Indeed, Urban planning began as a movement primarily occupied with matters of public health and urban design. 
Planning and architecture went through a paradigm shift at the turn of the 20th century. The industrialized cities of the 19th century had grown at a tremendous rate, with the pace and style of building largely dictated by private business concerns. The evils of urban life for the working poor were becoming increasingly evident as a matter for public concern. The laissez-faire style of government management of the economy, in fashion for most of the Victorian era, was starting to give way to a New Liberalism that championed intervention on the part of the poor and disadvantaged. Around 1900, theorists began developing urban planning models to mitigate the consequences of the industrial age, by providing citizens, especially factory workers, with healthier environments.
The first modern urban planning theorist was Sir Ebenezer Howard, who initiated the garden city movement in 1898. Howard's ideas, although utopian were also highly practical and were adopted around the world in the ensuing decades. His garden cities were intended to be planned, self-contained communities surrounded by parks, containing proportionate and separate areas of residences, industry and agriculture. Inspired by the Utopian novel "Looking Backward" and Henry George's work "Progress and Poverty", Howard published his book "Garden Cities of To-morrow" in 1898, commonly regarded as the most important book in the history of urban planning. His idealised garden city would house 32,000 people on a site of , planned on a concentric pattern with open spaces, public parks and six radial boulevards, wide, extending from the centre. The garden city would be self-sufficient and when it reached full population, another garden city would be developed nearby. Howard envisaged a cluster of several garden cities as satellites of a central city of 50,000 people, linked by road and rail.
The first garden cities were created at Letchworth and Welwyn Garden City in Hertfordshire. Sir Frederic Osborn extended the movement to regional planning.
Urban planning became professionalized at this period, with input from utopian visionaries as well as from the practical minded infrastructure engineers and local councilors combining to produce new design templates for political consideration. The Town and Country Planning Association was founded in 1899 and the first academic course on urban planning was offered by the University of Liverpool in 1909.
The first official consideration of these new trends was embodied in the Housing and Town Planning Act of 1909 that compelled local authorities to introduce coherent systems of town planning across the country using the new principles of the 'garden city', and to ensure that all housing construction conformed to specific building standards.
Following this Act, surveyors, civil engineers, architects, lawyers and others began working together within local government in the United Kingdom to draw up schemes for the development of land and the idea of town planning as a new and distinctive area of expertise began to be formed. In 1910, Thomas Adams was appointed as the first Town Planning Inspector at the Local Government Board, and began meeting with practitioners. The Town Planning Institute was established in 1914 with a mandate to advance the study of town-planning and civic design. The first university course in America was established at Harvard University in 1924.
The automobile was an important influence on the design of urban development in the 20th century, and the rise of the "urban design" movement can be seen in part as a reaction to the adverse impact of car-use and car orientated design. 'Urban design' was first used as a distinctive term when Harvard University hosted a series of Urban Design Conferences from 1956 . These conferences provided a platform for the launching of Harvard's Urban Design program in 1959-60. The writings of Jane Jacobs, Kevin Lynch, Gordon Cullen and Christopher Alexander became authoritative works for the school of Urban Design.
Gordon Cullen's "The Concise Townscape", first published in 1961, also had a great influence on many urban designers. Cullen examined the traditional artistic approach to city design of theorists such as Camillo Sitte, Barry Parker and Raymond Unwin. He created the concept of 'serial vision', defining the urban landscape as a series of related spaces.
Jane Jacobs' "The Death and Life of Great American Cities", published in 1961, was also a catalyst for interest in ideas of urban design. She critiqued the Modernism of CIAM, and asserted that the publicly unowned spaces created by the 'city in the park' notion of Modernists was one of the main reasons for the rising crime rate. She argued instead for an 'eyes on the street' approach to town planning, and the resurrection of main public space precedents, such as streets and squares, in the design of cities.
Kevin Lynch's "The Image of the City" of 1961 was also seminal to the movement, particularly with regards to the concept of legibility, and the reduction of urban design theory to five basic elements - paths, districts, edges, nodes, landmarks. He also made popular the use of mental maps to understanding the city, rather than the two-dimensional physical master plans of the previous 50 years.
Other notable works include Rossi's "Architecture of the City" (1966), Venturi’s "Learning from Las Vegas" (1972), Colin Rowe's "Collage City" (1978), and Peter Calthorpe's "The Next American Metropolis" (1993). Rossi introduced the concepts of 'historicism' and 'collective memory' to urban design, and proposed a 'collage metaphor' to understand the collage of new and older forms within the same urban space. Calthorpe, on the other hand, developed a manifesto for sustainable urban living via medium density living, as well as a design manual for building new settlements in accordance with his concept of Transit Oriented Development (TOD). Bill Hillier and Julienne Hanson in "The Social Logic of Space" (1984) introduced the concept of Space Syntax to predict how movement patterns in cities would contribute to urban vitality, anti-social behaviour and economic success. The popularity of these works resulted in terms such as 'historicism', 'sustainability', 'livability', 'high quality of urban components', etc. become everyday language in the field of urban planning.
Current trends.
Various current movements in urban design seek to create sustainable urban environments with long-lasting structures, buildings and a great liveability for its inhabitants. The most clearly defined form of walkable urbanism is known as the "Charter of New Urbanism". It is an approach for successfully reducing environmental impacts by altering the built environment to create and preserve smart cities which support sustainable transport. Residents in compact urban neighborhoods drive fewer miles, and have significantly lower environmental impacts across a range of measures, compared with those living in sprawling suburbs. The concept of Circular flow land use management has also been introduced in Europe to promote sustainable land use patterns, that strive for compact cities and a reduction of greenfield land taken by urban sprawl.
In sustainable construction the recent movement of New Classical Architecture promotes a sustainable approach towards urban construction that appreciates and develops smart growth, walkability, architectural tradition and classical design. This in contrast to modernist and globally uniform architecture, as well as opposing solitary housing estates and suburban sprawl. Both trends started in the 1980s.
Principles.
Public spaces are frequently subject to overlapping management responsibilities of multiple public agencies or authorities and the interests of nearby property owners, as well as the requirements of multiple and sometimes competing users. The design, construction and management of public spaces therefore typically demands consultation and negotiation across a variety of spheres. Urban designers rarely have the degree of artistic liberty or control sometimes offered in design professions such as architecture. It also typically requires interdisciplinary input with balanced representation of multiple fields including engineering, ecology, local history, and transport planning.
The scale and degree of detail considered varies depending on context and needs. It ranges from the layout of entire city regions, cities, as with l'Enfant's plan for Washington DC, Griffin and Mahony's plan for Canberra and Doxiadis' plan for Islamabad (although such opportunities are obviously rare), through 'managing the sense of a region' as described by Kevin Lynch, to the design of street furniture.
Urban design may encompass the preparation of design guidelines and regulatory frameworks, or even legislation to control development, advertising, etc. and in this sense overlaps with urban planning. It may encompass the design of particular spaces and structures and in this sense overlaps with architecture, landscape architecture, highway engineering and industrial design. It may also deal with ‘place management’ to guide and assist the use and maintenance of urban areas and public spaces.
Most urban design work is undertaken by urban planners, landscape architects and architects. There are also professionals who identify themselves specifically as urban designers. Many architecture, landscape and planning programs incorporate urban design theory and design subjects into their curricula and there are an increasing number of university programs offering degrees in urban design, usually at post-graduate level.
Urban design considers:
Equality issues.
Until the 1970s, the design of towns and cities took little account of the needs of people with disabilities. At that time, disabled people began to form movements demanding recognition of their potential contribution if social obstacles were removed. Disabled people challenged the 'medical model' of disability which saw physical and mental problems as an individual 'tragedy' and people with disabilities as 'brave' for enduring them. They proposed instead a 'social model' which said that barriers to disabled people result from the design of the built environment and attitudes of able-bodied people. 'Access Groups' were established composed of people with disabilities who audited their local areas, checked planning applications and made representations for improvements. The new profession of 'access officer' was established around that time to produce guidelines based on the recommendations of access groups and to oversee adaptations to existing buildings as well as to check on the accessibility of new proposals. Many local authorities now employ access officers who are regulated by the Access Association. A new chapter of the Building Regulations (Part M) was introduced in 1992. Although it was beneficial to have legislation on this issue the requirements were fairly minimal but continue to be improved with ongoing amendments. The Disability Discrimination Act 1995 continues to raise awareness and enforce action on disability issues in the urban environment.

</doc>
<doc id="50346" url="https://en.wikipedia.org/wiki?curid=50346" title="Museum of Science and Industry (Chicago)">
Museum of Science and Industry (Chicago)

The Museum of Science and Industry (MSI) is located in Chicago, Illinois, in Jackson Park, in the Hyde Park neighborhood between Lake Michigan and The University of Chicago. It is housed in the former Palace of Fine Arts from the 1893 World's Columbian Exposition. Initially endowed by Julius Rosenwald, the Sears, Roebuck and Company president and philanthropist, it was supported by the Commercial Club of Chicago and opened in 1933 during the Century of Progress Exposition.
It is the largest science museum in the western hemisphere. Among its diverse and expansive exhibits, the museum features a full-size replica coal mine, a German submarine (U-505) captured during World War II, a model railroad, the first diesel-powered streamlined stainless-steel passenger train ("Pioneer Zephyr"), and the Apollo 8 spacecraft that carried the first humans to orbit the Moon.
Based on 2009 attendance, the Museum of Science and Industry was the second largest cultural attraction in Chicago. David R. Mosena has been President and CEO of the Museum since 1998.
History.
The Palace of Fine Arts (also known as the Fine Arts Building) at the 1893 World's Columbian Exposition was designed by Charles B. Atwood for D. H. Burnham & Co. Unlike the other "White City" buildings, it was constructed with a brick substructure under its plaster facade. After the World's Fair, it initially housed the Columbian Museum, which evolved into the Field Museum of Natural History. When the Field Museum moved to a new building near downtown Chicago in 1920, the former site was left vacant.
Art Institute of Chicago professor Lorado Taft led a public campaign to restore the building and turn it into another art museum, one devoted to sculpture. The South Park Commissioners (now part of the Chicago Park District) won approval in a referendum to sell $5 million in bonds to pay for restoration costs, hoping to turn the building into a sculpture museum, a technical trade school, and other things. However, after a few years, the building was selected as the site for a new science museum.
At this time, the Commercial Club of Chicago was interested in establishing a science museum in Chicago. Julius Rosenwald, the Sears, Roebuck and Company president and philanthropist, energized his fellow club members by pledging to pay $3 million towards the cost of converting the Palace of Fine Arts (Rosenwald eventually contributed more than $5 million to the project). During its conversion into the MSI, the building's exterior was re-cast in limestone to retain its 1893 Beaux Arts look. The interior was replaced with a new one in Art Moderne style designed by Alfred P. Shaw.
Rosenwald established the museum organization in 1926 but declined to have his name on the building. For the first few years, the museum was often called the Rosenwald Industrial Museum. In 1928, the name of the museum was officially changed to the Museum of Science and Industry. Rosenwald's vision was to create a museum in the style of the Deutsches Museum in Munich, which he had visited in 1911 while in Germany with his family.
Sewell Avery, another businessman, had supported the museum within the Commercial Club and was selected as its first president of the board of directors. The museum conducted a nationwide search for the first director. MSI's Board of Directors selected Waldemar Kaempffert, then the science editor of "The New York Times", because he shared Rosenwald's vision.
He assembled the museum's curatorial staff and directed the organizing and constructing the exhibits. In order to prepare the museum, Kaempffert and his staff visited the Deutsches Museum in Munich, the Science Museum in Kensington, and the Technical Museum in Vienna, all of which served as models. Kaempffert was instrumental in developing close ties with the science departments of the University of Chicago, which supplied much of the scholarship for the exhibits. Kaempffert resigned in early 1931 amid growing disputes with the second president of the board of directors; they disagreed over the objectivity and neutrality of the exhibits and Kaempffert's management of the staff.
The new Museum of Science and Industry opened to the public in three stages between 1933 and 1940. The first opening ceremony took place during the Century of Progress Exposition. Two of the museum's presidents, a number of curators and other staff members, and exhibits came to MSI from the Century of Progress event.
For years visitors entered the museum through its original main entrance, but that entrance was no longer large enough to handle an increasing volume of visitors. The newer main entrance is a structure detached from the main museum building, through which visitors descend into an underground area and re-ascend into the main building, similar to the Louvre Pyramid.
For over 55 years, admission to the MSI was free. Fees began to be charged during the early 1990s, with general admission rates increasing from $13 in 2008 to $18 in 2015.
Many "free days" are offered throughout the year, but those free days are restricted to Illinois residents showing proof of residence in the state.
Exhibits.
The museum has over 2,000 exhibits, displayed in 75 major halls. The museum has several major permanent exhibits: The "Coal Mine" re-creates a working deep-shaft, bituminous coal mine inside the museum's Central Pavilion, using original equipment from Old Ben #17 circa 1933. Since 1954, the museum has had the "U-505" submarine, one of just two German submarines captured during World War II, and the only one on display in the western hemisphere. Access to several of the exhibits (including the coal mine and tour of U-505) require the payment of an additional fee.
The museum opened "The New U-505 Experience" on June 5, 2005. "Take Flight" features a Boeing 727 jet plane donated by United Airlines, with one wing removed and holes cut on the fuselage to facilitate visitor access. Silent-film star and stock-market investor Colleen Moore's Fairy Castle "dolls house" is on display. "The Great Train Story", a model railroad, recounts the story of transportation from Chicago to Seattle.
The Transportation Zone includes exhibits on air and land transportation, including the "999 Empire State Express" steam locomotive, which the museum claims is the first vehicle to exceed . The Transportation Zone also includes two World War II warplanes donated by the British government: a German Ju 87 R-2/Trop. Stuka divebomber — one of only two intact Stukas left in the world — and a British Supermarine Spitfire. The first diesel-powered streamlined stainless-steel train, the "Pioneer Zephyr", is on permanent display in the Great Hall, renamed the Entry Hall in 2008. A free tour goes through it every 10–20 minutes. Several US Navy warship models are on display, and a flight simulator for the new F-35 Lightning II is featured.
In keeping with Rosenwald's vision, many of the exhibits are interactive, ranging from "Genetics: Decoding Life", which looks at how genetics affect human and animal development as well as containing a chick hatchery composed of an incubator where baby chickens hatch from their eggs and a chick pen for those that have already hatched, to "ToyMaker 3000", a working assembly line that lets visitors order a toy top and watch as it is made. The interactive "Fab Lab MSI" is intended as an interactive lab where members can "build anything".
In March 2010, the museum opened "Science Storms" in the Allstate Court. This multilevel exhibit features a water vapor tornado, tsunami tank, Tesla coil, heliostat system, and a Wimshurst machine built by James Wimshurst in the late 19th century. All artifacts allow guests to explore the physics and chemistry of the natural world.
MSI's Henry Crown Space Center includes the Apollo 8 spacecraft, which flew the first mission beyond low earth orbit to the Moon, enabling its crew, Frank Borman, James Lovell and William Anders, to become the first human beings to see the Earth as a whole, as well as becoming the first to view the Moon up close (as well as the first to view its far side). Other exhibits include an OmniMax theater, Scott Carpenter's Mercury-Atlas 7 spacecraft, a lunar module trainer and a life-size mockup of a space shuttle.
The museum is known for unique and quirky permanent exhibits, such as a walk-through model of the human heart. It was removed in 2008 for the construction of "YOU! the Experience", which replaced it with a , interactive, 3D heart. Also well known are the "Body Slices" (two cadavers exhibited in slices) in the exhibit.
In spring 2013, the "Art of the Bicycle" exhibit opened, showcasing history of the bicycle, and how modern bikes are still continuing to evolve.
Other exhibits include "Yesterday's Mainstreet", a mock-up of a Chicago street from the early 20th century, complete with a cobblestone roadway, old-fashioned light fixtures, fire hydrants, and several shops, including the precursors to several Chicago-based businesses. Included are:
Unlike the other shops, Finnigan's Ice Cream Parlor and The Nickelodeon Cinema can be entered and are functional. Finnigan's serves an assortment of ice cream and The Cinema plays short silent films throughout the day.
A second transportation gallery is located on the museum's west wing, containing models of "Ships Through the Ages" and several historic racing cars.
The "FarmTech" exhibit showcases modern agricultural techniques and how farmers use modern technology like GPS systems to improve work on the farm.
Other upper level exhibits include "Reusable City", which focuses on recycling and other methods that could cut down harmful pollution and especially climate change and the Regenstein Hall of Science, containing a giant periodic table of the elements. Other main level exhibits include: "Fast Forward", which features some aspects of how technology will change in the future; "NetWorld", which focuses on the Internet and how it connects society together; "Earth Revealed", featuring a "Science on a Sphere" holographic globe; and a "Whispering Gallery".
Some areas aimed for younger children include the "Swiss Jollyball", the world's largest pinball machine built by a British man from Switzerland using nothing but salvaged junk; the "Idea Factory", a toddler water table play area; and the "Circus", featuring animated dioramas of a miniature circus as well as containing a shadow garden and several funhouse mirrors.
A F-104 Starfighter on loan to MSI from the US Air Force since 1978 was sent to the Mid-America Air Museum in Liberal, Kansas in 1993. In March 1995, Santa Fe Steam Locomotive 2903 was moved from outside the museum to the Illinois Railway Museum.
The museum holds the Junior Achievement's US Business Hall of Fame.
Exhibitions.
In addition to its three floors of standing exhibits, the museum hosts temporary and traveling exhibitions. Exhibitions last for five months or less and usually require a separate paid admission fee. Exhibitions at MSI have included "Titanic: The Exhibition", which was the largest display of relics from the wreck of RMS "Titanic", in 2000; Gunther von Hagens' "Body Worlds", a view into the human body through use of plastinated human specimens in 2005; also in 2005 was "Game On", which features the history and culture of video games; and "Leonardo da Vinci: Man, Inventor, Genius" in the summer of 2006. Past temporary exhibitions include ', "Robots Like Us", "City of the Future," "Canstruction" and ',and "The Glass Experience". "Harry Potter: The Exhibition" ran from April to September 2009. The fourth installment of "Smart Home: Green + Wired" reopened in March 2011 and ran through January 2012, featuring the work of green architect Michelle Kaufmann. In May 2015, the "Robot Revolution" exhibit opened, which is sponsored by Google.org and features numerous hands-on demonstrations and advice from experts for prospective future robot scientists and engineers.

</doc>
<doc id="50347" url="https://en.wikipedia.org/wiki?curid=50347" title="Multivariate normal distribution">
Multivariate normal distribution

In probability theory and statistics, the multivariate normal distribution or multivariate Gaussian distribution, is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions. One possible definition is that a random vector is said to be "k"-variate normally distributed if every linear combination of its "k" components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.
Notation and parametrization.
The multivariate normal distribution of a "k"-dimensional random vector can be written in the following notation:
or to make it explicitly known that "X" is "k"-dimensional,
with "k"-dimensional mean vector
and formula_7 covariance matrix
Definition.
A random vector is said to have the multivariate normal distribution if it satisfies the following equivalent conditions.
The covariance matrix is allowed to be singular (in which case the corresponding distribution has no density). This case arises frequently in statistics; for example, in the distribution of the vector of residuals in the ordinary least squares regression. Note also that the "X""i" are in general "not" independent; they can be seen as the result of applying the matrix A to a collection of independent Gaussian variables z.
Properties.
Density function.
Non-degenerate case.
The multivariate normal distribution is said to be "non-degenerate" when the symmetric covariance matrix formula_10 is positive definite. In this case the distribution has density
where formula_12 is a real "k"-dimensional column vector and formula_13 is the determinant of formula_10. Note how the equation above reduces to that of the univariate normal distribution if formula_10 is a formula_16 matrix (i.e. a single real number).
Note that the circularly-symmetric version of the complex normal distribution has a slightly different form.
Each iso-density locus—the locus of points in "k"-dimensional space each of which gives the same particular value of the density—is an ellipse or its higher-dimensional generalization; hence the multivariate normal is a special case of the elliptical distributions.
The descriptive statistic formula_17 in the non-degenerate multivariate normal distribution equation is known as the square of the Mahalanobis distance, which represents the distance of the test point formula_12 from the mean formula_19. Note that in case when formula_20, the distribution reduces to a univariate normal distribution and the Mahalanobis distance reduces to the standard score.
In the 2-dimensional nonsingular case (), the probability density function of a vector is:
formula_21
where "ρ" is the correlation between "X" and "Y" and 
where formula_22 and formula_23. In this case,
In the bivariate case, the first equivalent condition for multivariate normality can be made less restrictive: it is sufficient to verify that countably many distinct linear combinations of X and Y are normal in order to conclude that the vector is bivariate normal.
The bivariate iso-density loci plotted in the "x,y"-plane are ellipses. As the correlation parameter "ρ" increases, these loci appear to be squeezed to the following line :
This is because the above expression - but without "ρ" being inside a signum function - is the best linear unbiased prediction of "Y" given a value of "X".
Degenerate case.
If the covariance matrix formula_10 is not full rank, then the multivariate normal distribution is degenerate and does not have a density. More precisely, it does not have a density with respect to "k"-dimensional Lebesgue measure (which is the usual measure assumed in calculus-level probability courses). Only random vectors whose distributions are absolutely continuous with respect to a measure are said to have densities (with respect to that measure). To talk about densities but avoid dealing with measure-theoretic complications it can be simpler to restrict attention to a subset of formula_27 of the coordinates of formula_28 such that the covariance matrix for this subset is positive definite; then the other coordinates may be thought of as an affine function of the selected coordinates.
To talk about densities meaningfully in the singular case, then, we must select a different base measure. Using the disintegration theorem we can define a restriction of Lebesgue measure to the formula_27-dimensional affine subspace of formula_30 where the Gaussian distribution is supported, i.e. formula_31. With respect to this measure the distribution has density:
where formula_33 is the generalized inverse and det* is the pseudo-determinant.
Higher moments.
The "k"th-order moments of x are given by
where 
The "k"th-order central moments are as follows
(a) If "k" is odd, .
(b) If "k" is even with , then
where the sum is taken over all allocations of the set formula_36 into "λ" (unordered) pairs. That is, for a "k"th central moment, one sums the products of covariances (the expected value μ is taken to be 0 in the interests of parsimony):
This yields formula_38 terms in the sum (15 in the above case), each being the product of "λ" (in this case 3) covariances. For fourth order moments (four variables) there are three terms. For sixth-order moments there are 3 × 5 = 15 terms, and for eighth-order moments there are 3 × 5 × 7 = 105 terms.
The covariances are then determined by replacing the terms of the list formula_39 by the corresponding terms of the list consisting of "r"1 ones, then "r"2 twos, etc.. To illustrate this, examine the following 4th-order central moment case:
where formula_41 is the covariance of "Xi" and "Xj". With the above method one first finds the general case for a "k"th moment with "k" different "X" variables, formula_42, and then one simplifies this accordingly. For example, for formula_43, one lets and one uses the fact that formula_44.
Likelihood function.
If the mean and variance matrix are unknown, a suitable log likelihood function for a single observation x would be:
where "x" is a vector of real numbers. The circularly-symmetric version of the complex case, where "z" is a vector of complex numbers, would be
i.e. with the conjugate transpose (indicated by formula_47) replacing the normal transpose (indicated by formula_48). This is slightly different than in the real case, because the circularly-symmetric version of the complex normal distribution has a slightly different form.
A similar notation is used for multiple linear regression.
Entropy.
The differential entropy of the multivariate normal distribution is
where the bars denote the matrix determinant.
Kullback–Leibler divergence.
The Kullback–Leibler divergence from formula_50 to formula_51, for non-singular matrices Σ0 and Σ1, is:
where formula_53 is the dimension of the vector space.
The logarithm must be taken to base "e" since the two terms following the logarithm are themselves base-"e" logarithms of expressions that are either factors of the density function or otherwise arise naturally. The equation therefore gives a result measured in nats. Dividing the entire expression above by log"e" 2 yields the divergence in bits.
Mutual Information.
The mutual information of a distribution is a special case of the Kullback–Leibler divergence in which formula_54 is the full multivariate distribution and formula_55 is the product of the 1-dimensional marginal distributions. In the notation of the Kullback–Leibler divergence section of this article, formula_56 is a diagonal matrix with the diagonal entries of formula_57, and formula_58. The resulting formula for mutual information is:
where formula_60 is the correlation matrix constructed from formula_61.
In the bivariate case the expression for the mutual information is:
Cumulative distribution function.
The notion of cumulative distribution function (cdf) in dimension 1 can be extended in two ways to the multidimensional case, based on rectangular and ellipsoidal regions.
The first way is to define the cdf formula_63 of a random vector formula_64 as the probability that all components of formula_64 are less than or equal to the corresponding values in the vector formula_28: 
Though there is no closed form for formula_63, there are a number of algorithms that estimate it numerically.
Another way is to define the cdf formula_69 as the probability that a sample lies inside the ellipsoid determined by its Mahalanobis distance formula_70 from the Gaussian, a direct generalization of the standard deviation
In order to compute the values of this function, closed analytic formulae exist, as follows.
Interval.
The interval for the multivariate normal distribution yields a region consisting of those vectors x satisfying
Here formula_12 is a formula_73-dimensional vector, formula_19 is the known formula_73-dimensional mean vector, formula_10 is the known covariance matrix and formula_77 is the quantile function for probability formula_78 of the chi-squared distribution with formula_73 degrees of freedom.
When formula_80 the expression defines the interior of an ellipse and the chi-squared distribution simplifies to an exponential distribution with mean equal to two.
Joint normality.
Normally distributed and independent.
If "X" and "Y" are normally distributed and independent, this implies they are "jointly normally distributed", i.e., the pair ("X", "Y") must have multivariate normal distribution. However, a pair of jointly normally distributed variables need not be independent (would only be so if uncorrelated, formula_81 ).
Two normally distributed random variables need not be jointly bivariate normal.
The fact that two random variables "X" and "Y" both have a normal distribution does not imply that the pair ("X", "Y") has a joint normal distribution. A simple example is one in which X has a normal distribution with expected value 0 and variance 1, and "Y" = "X" if |"X"| > "c" and "Y" = −"X" if |"X"| < "c", where "c" > 0. There are similar counterexamples for more than two random variables. In general, they sum to a mixture model.
Correlations and independence.
In general, random variables may be uncorrelated but statistically dependent. But if a random vector has a multivariate normal distribution then any two or more of its components that are uncorrelated are independent. This implies that any two or more of its components that are pairwise independent are independent.
But it is not true that two random variables that are (separately, marginally) normally distributed and uncorrelated are independent. Two random variables that are normally distributed may fail to be "jointly" normally distributed, i.e., the vector whose components they are may fail to have a multivariate normal distribution.
Conditional distributions.
If N-dimensional x is partitioned as follows
and accordingly μ and Σ are partitioned as follows
then, the distribution of x1 conditional on x2 = "a" is multivariate normal where
and covariance matrix
This matrix is the Schur complement of Σ22 in Σ. This means that to calculate the conditional covariance matrix, one inverts the overall covariance matrix, drops the rows and columns corresponding to the variables being conditioned upon, and then inverts back to get the conditional covariance matrix. Here formula_87 is the generalized inverse of formula_88.
Note that knowing that alters the variance, though the new variance does not depend on the specific value of a; perhaps more surprisingly, the mean is shifted by formula_89; compare this with the situation of not knowing the value of a, in which case x1 would have distribution
formula_90.
An interesting fact derived in order to prove this result, is that the random vectors formula_91 and formula_92 are independent.
The matrix Σ12Σ22−1 is known as the matrix of regression coefficients.
Bivariate case.
In the bivariate case where x is partitioned into "X"1 and "X"2, the conditional distribution of "X"1 given "X"2 is
where formula_94 is the correlation coefficient between "X"1 and "X"2.
Bivariate conditional expectation.
In the general case.
The conditional expectation of X1 given X2 is:
formula_96
Proof: the result is obtained by taking the expectation of the conditional distribution formula_97 above.
In the case of unit variances.
The conditional expectation of "X"1 given "X"2 is
and the conditional variance is
thus the conditional variance does not depend on "x"2.
The conditional expectation of "X"1 given that "X"2 is smaller/bigger than "z" is (Maddala 1983, p. 367) :
where the final ratio here is called the inverse Mills ratio.
Proof: the last two results are obtained using the result formula_99, so that
Marginal distributions.
To obtain the marginal distribution over a subset of multivariate normal random variables, one only needs to drop the irrelevant variables (the variables that one wants to marginalize out) from the mean vector and the covariance matrix. The proof for this follows from the definitions of multivariate normal distributions and linear algebra.
"Example"
Let be multivariate normal random variables with mean vector and covariance matrix Σ (standard parametrization for multivariate normal distributions). Then the joint distribution of is multivariate normal with mean vector and covariance matrix
formula_105.
Affine transformation.
If is an affine transformation of formula_106 where c is an formula_107 vector of constants and B is a constant formula_108 matrix, then y has a multivariate normal distribution with expected value and variance BΣBT i.e., formula_109. In particular, any subset of the "xi" has a marginal distribution that is also multivariate normal.
To see this, consider the following example: to extract the subset ("x"1, "x"2, "x"4)T, use
which extracts the desired elements directly.
Another corollary is that the distribution of , where b is a constant vector with the same number of elements as x and the dot indicates the dot product, is univariate Gaussian with formula_111. This result follows by using
Observe how the positive-definiteness of Σ implies that the variance of the dot product must be positive.
An affine transformation of x such as 2x is not the same as the sum of two independent realisations of x.
Geometric interpretation.
The equidensity contours of a non-singular multivariate normal distribution are ellipsoids (i.e. linear transformations of hyperspheres) centered at the mean. Hence the multivariate normal distribution is an example of the class of elliptical distributions. The directions of the principal axes of the ellipsoids are given by the eigenvectors of the covariance matrix Σ. The squared relative lengths of the principal axes are given by the corresponding eigenvalues.
If is an eigendecomposition where the columns of U are unit eigenvectors and Λ is a diagonal matrix of the eigenvalues, then we have
Moreover, U can be chosen to be a rotation matrix, as inverting an axis does not have any effect on "N"(0, Λ), but inverting a column changes the sign of U's determinant. The distribution "N"(μ, Σ) is in effect "N"(0, I) scaled by Λ1/2, rotated by U and translated by μ.
Conversely, any choice of μ, full rank matrix U, and positive diagonal entries Λ"i" yields a non-singular multivariate normal distribution. If any Λ"i" is zero and U is square, the resulting covariance matrix UΛUT is singular. Geometrically this means that every contour ellipsoid is infinitely thin and has zero volume in "n"-dimensional space, as at least one of the principal axes has length of zero.
"The radius around the true mean in a bivariate normal random variable, re-written in polar coordinates (radius and angle), follows a Hoyt distribution.
Estimation of parameters.
The derivation of the maximum-likelihood estimator of the covariance matrix of a multivariate normal distribution is straightforward. See estimation of covariance matrices.
In short, the probability density function (pdf) of a multivariate normal is
and the ML estimator of the covariance matrix from a sample of "n" observations is
which is simply the sample covariance matrix. This is a biased estimator whose expectation is
An unbiased sample covariance is
The Fisher information matrix for estimating the parameters of a multivariate normal distribution has a closed form expression. This can be used, for example, to compute the Cramér–Rao bound for parameter estimation in this setting. See Fisher information for more details.
Bayesian inference.
In Bayesian statistics, the conjugate prior of the mean vector is another multivariate normal distribution, and the conjugate prior of the covariance matrix is an inverse-Wishart distribution formula_118 . Suppose then that "n" observations have been made
and that a conjugate prior has been assigned, where
where
and
Then,
where
Multivariate normality tests.
Multivariate normality tests check a given set of data for similarity to the multivariate normal distribution. The null hypothesis is that the data set is similar to the normal distribution, therefore a sufficiently small "p"-value indicates non-normal data. Multivariate normality tests include the Cox-Small test
and Smith and Jain's adaptation of the Friedman-Rafsky test.
Mardia's test is based on multivariate extensions of skewness and kurtosis measures. For a sample {x1, ..., x"n"} of "k"-dimensional vectors we compute
Under the null hypothesis of multivariate normality, the statistic "A" will have approximately a chi-squared distribution with degrees of freedom, and "B" will be approximately standard normal "N"(0,1).
Mardia's kurtosis statistic is skewed and converges very slowly to the limiting normal distribution. For medium size samples formula_126, the parameters of the asymptotic distribution of the kurtosis statistic are modified For small sample tests (formula_127) empirical critical values are used. Tables of critical values for both statistics are given by Rencher for "k"=2,3,4.
Mardia's tests are affine invariant but not consistent. For example, the multivariate skewness test is not consistent against
symmetric non-normal alternatives.
The BHEP test computes the norm of the difference between the empirical characteristic function and the theoretical characteristic function of the normal distribution. Calculation of the norm is performed in the L2("μ") space of square-integrable functions with respect to the Gaussian weighting function formula_128. The test statistic is
The limiting distribution of this test statistic is a weighted sum of chi-squared random variables, however in practice it is more convenient to compute the sample quantiles using the Monte-Carlo simulations.
A detailed survey of these and other test procedures is available.
Drawing values from the distribution.
A widely used method for drawing (sampling) a random vector x from the "N"-dimensional multivariate normal distribution with mean vector μ and covariance matrix Σ works as follows:

</doc>
<doc id="50350" url="https://en.wikipedia.org/wiki?curid=50350" title="Ella Fitzgerald">
Ella Fitzgerald

Ella Jane Fitzgerald (April 25, 1917 – June 15, 1996) was an American jazz singer often referred to as the First Lady of Song, Queen of Jazz and Lady Ella. She was noted for her purity of tone, impeccable diction, phrasing and intonation, and a "horn-like" improvisational ability, particularly in her scat singing.
After tumultuous teenage years, Fitzgerald found stability in musical success with the Chick Webb Orchestra, performing across the country, but most often associated with the Savoy Ballroom in Harlem. Fitzgerald's rendition of the nursery rhyme "A-Tisket, A-Tasket" helped boost both her and Webb to national fame. Taking over the band after Webb died, Fitzgerald left it behind in 1942 to start a solo career that would last effectively the rest of her life.
Signed with manager and Savoy co-founder Moe Gale from early in her career, she eventually gave managerial control for her performance and recording career to Norman Granz, who built up the label Verve Records based in part on Fitzgerald's vocal abilities. With Verve she recorded some of her more widely noted works, particularly her interpretation of the Great American Songbook.
While Fitzgerald appeared in movies and as a guest on popular television shows in the second half of the twentieth century, her musical collaborations with Louis Armstrong, Duke Ellington, and The Ink Spots were some of her most notable acts outside of her solo career. These partnerships produced recognizable songs like "Dream a Little Dream of Me", "Cheek to Cheek", "Into Each Life Some Rain Must Fall", and "It Don't Mean a Thing (If It Ain't Got That Swing)". In 1993, Fitzgerald capped off her sixty-year career with her last public performance. Three years later, she died at the age of 79, following years of decline in her health. After her passing, Fitzgerald's influence lived on through her fourteen Grammy Awards, National Medal of Arts, Presidential Medal of Freedom, and tributes in the form of stamps, music festivals, and theater namesakes.
Early life.
Fitzgerald was born on April 25, 1917, in Newport News, Virginia, the daughter of William Fitzgerald and Temperance "Tempie" Fitzgerald. Her parents were unmarried but lived together for at least two and a half years after she was born. In the early 1920s Fitzgerald's mother and her new partner, a Portuguese immigrant named Joseph Da Silva, moved to the city of Yonkers, in Westchester County, New York, as part of the first Great Migration of African Americans. Initially living in a single room, her mother and Da Silva soon found jobs. Her half-sister, Frances Da Silva, was born in 1923. By 1925, Fitzgerald and her family had moved to nearby School Street, then a predominantly poor Italian area. She began her formal education at the age of six and proved to be an outstanding student, moving through a variety of schools before attending Benjamin Franklin Junior High School from 1929.
Fitzgerald had been passionate about dancing from third grade, being a fan of Earl "Snakehips" Tucker in particular, and would perform for her peers on the way to school and at lunchtime. Fitzgerald and her family were Methodists and were active in the Bethany African Methodist Episcopal Church, and she regularly attended worship services, Bible study, and Sunday school. The church provided Fitzgerald with her earliest experiences in formal music making, and she may also have had a short series of piano lessons during this period.
During this period Fitzgerald listened to jazz recordings by Louis Armstrong, Bing Crosby, and The Boswell Sisters. Fitzgerald idolized the Boswell Sisters' lead singer Connee Boswell, later saying, "My mother brought home one of her records, and I fell in love with it...I tried so hard to sound just like her."
In 1932, her mother died from a heart attack when Fitzgerald was 15 years of age. This left her at first in the care of her stepfather but before the end of April 1933, she had moved in with her aunt in Harlem. This seemingly swift change in her circumstances, reinforced by what Fitzgerald biographer Stuart Nicholson describes as rumors of her stepfather's "ill treatment" of Fitzgerald, leaves him to speculate that Da Silva might have abused her.
Regardless, following these traumas, Fitzgerald began skipping school and letting her grades suffer. During this period she worked at times as a lookout at a bordello and with a Mafia-affiliated numbers runner. Ella Fitzgerald never talked publicly about this time in her life. When the authorities caught up with her, she was first placed in the Colored Orphan Asylum in Riverdale, Bronx. However, when the orphanage proved too crowded, she was moved to the New York Training School for Girls in Hudson, New York, a state reformatory located about 120 miles north of New York City. Eventually she escaped and for a time she was homeless.
Early career.
While she seems to have survived during 1933 and 1934 in part from singing on the streets of Harlem, Fitzgerald made her most important amateur singing debut at age 17 on November 21, 1934, in one of the earliest of the famous Amateur Nights at the Apollo Theater. She had originally intended to go on stage and dance, but, intimidated by a local dance duo called the Edwards Sisters, she opted to sing instead. Performing in the style of Connee Boswell, she sang "Judy" and "The Object of My Affection" and won the first prize of US $25.00. In theory, she also won the chance to perform at the Apollo for a week but, seemingly because of her disheveled appearance, the theater never gave her that part of her prize.
In January 1935 Fitzgerald won the chance to perform for a week with the Tiny Bradshaw band at the Harlem Opera House. Around this same time, she was introduced to the drummer and bandleader Chick Webb, who had asked his recently signed singer Charlie Linton to help find him a female singer. Though Webb was, as "The New York Times" later wrote, "reluctant to sign her...because she was gawky and unkempt, a 'diamond in the rough,'" he offered her the opportunity to test with his band when they played a dance at Yale University.
Met with approval by both audiences and her fellow musicians, Fitzgerald was asked to join Webb's orchestra and soon gained acclaim as part of the group's renowned performances at Harlem's Savoy Ballroom. Fitzgerald recorded several hit songs with them, including "Love and Kisses" and "(If You Can't Sing It) You'll Have to Swing It (Mr. Paganini)". But it was her 1938 version of the nursery rhyme, "A-Tisket, A-Tasket," a song she co-wrote, that brought her wide public acclaim.
Webb died on June 16, 1939, and his band was renamed Ella and her Famous Orchestra, with Fitzgerald taking on the role of nominal bandleader. Fitzgerald recorded nearly 150 songs with Webb's orchestra between 1935 and its final end in 1942. In her "New York Times" obituary of 1996, Stephen Holder echoed the conventional critical view of the time in describing "the majority" of her recordings during this period as "novelties and disposable pop fluff".
Decca years.
In 1942, Fitzgerald left the band to begin a solo career. Continuing under contract to the Decca label that she had worked with while part of Webb's orchestra, she had several popular hits while recording with such artists as Bill Kenny & the Ink Spots, Louis Jordan, and the Delta Rhythm Boys.
With Decca's Milt Gabler as her manager, Fitzgerald began working regularly for the jazz impresario Norman Granz and appeared regularly in his Jazz at the Philharmonic (JATP) concerts. Her relationship with Granz was further cemented when he became her manager, although it would be nearly a decade before he could record her on one of his many record labels.
With the demise of the Swing era and the decline of the great touring big bands, a major change in jazz music occurred. The advent of bebop led to new developments in Fitzgerald's vocal style, influenced by her work with Dizzy Gillespie's big band. It was in this period that Fitzgerald started including scat singing as a major part of her performance repertoire. While singing with Gillespie, Fitzgerald recalled, "I just tried to do my voice what I heard the horns in the band doing."
Her 1945 scat recording of "Flying Home" arranged by Vic Schoen would later be described by "The New York Times" as "one of the most influential vocal jazz records of the decade...Where other singers, most notably Louis Armstrong, had tried similar improvisation, no one before Miss Fitzgerald employed the technique with such dazzling inventiveness." Her bebop recording of "Oh, Lady Be Good!" (1947) was similarly popular and increased her reputation as one of the leading jazz vocalists.
Verve years.
Fitzgerald was still performing at Granz's JATP concerts by 1955. She left Decca and Granz, now her manager, created Verve Records around her. She later described the period as strategically crucial, saying, "I had gotten to the point where I was only singing be-bop. I thought be-bop was 'it', and that all I had to do was go some place and sing bop. But it finally got to the point where I had no place to sing. I realized then that there was more to music than bop. Norman ... felt that I should do other things, so he produced "The Cole Porter Songbook" with me. It was a turning point in my life."
On March 15, 1955 Ella Fitzgerald opened her initial engagement at the Mocambo nightclub in Hollywood, after Marilyn Monroe lobbied the owner for the booking. The booking was instrumental in Fitzgerald's career. Bonnie Greer dramatized the incident as the musical drama, "Marilyn and Ella", in 2008. It had previously been widely reported that Fitzgerald was the first black performer to play the Mocambo, following Monroe's intervention, but this is not true. African-American singers Herb Jefferies, Eartha Kitt, and Joyce Bryan all played the Mocambo in 1952 and 1953, according to stories published at the time in "Jet" magazine and "Billboard".
"Ella Fitzgerald Sings the Cole Porter Songbook", released in 1956, was the first of eight Songbook sets Fitzgerald would record for Verve at irregular intervals from 1956 to 1964. The composers and lyricists spotlighted on each set, taken together, represent the greatest part of the cultural canon known as the "Great American Songbook". Her song selections ranged from standards to rarities and represented an attempt by Fitzgerald to cross over into a non-jazz audience. The sets are the most well-known items in her discography.
"Ella Fitzgerald Sings the Duke Ellington Song Book" was the only Songbook on which the composer she interpreted played with her. Duke Ellington and his longtime collaborator Billy Strayhorn both appeared on exactly half the set's 38 tracks and wrote two new pieces of music for the album: "The E and D Blues" and a four-movement musical portrait of Fitzgerald (the only Songbook track on which Fitzgerald does not sing). The Songbook series ended up becoming the singer's most critically acclaimed and commercially successful work, and probably her most significant offering to American culture. "The New York Times" wrote in 1996, "These albums were among the first pop records to devote such serious attention to individual songwriters, and they were instrumental in establishing the pop album as a vehicle for serious musical exploration."
Days after Fitzgerald's death, "The New York Times" columnist Frank Rich wrote that in the Songbook series Fitzgerald "performed a cultural transaction as extraordinary as Elvis' contemporaneous integration of white and African American soul. Here was a black woman popularizing urban songs often written by immigrant Jews to a national audience of predominantly white Christians." Frank Sinatra, out of respect for Fitzgerald, prohibited Capitol Records from re-releasing his own recordings in separate albums for individual composers in the same way.
Fitzgerald also recorded albums exclusively devoted to the songs of Porter and Gershwin in 1972 and 1983; the albums being, respectively, "Ella Loves Cole" and "Nice Work If You Can Get It". A later collection devoted to a single composer was released during her time with Pablo Records, "Ella Abraça Jobim", featuring the songs of Antônio Carlos Jobim.
While recording the Songbooks and the occasional studio album, Fitzgerald toured 40 to 45 weeks per year in the United States and internationally, under the tutelage of Norman Granz. Granz helped solidify her position as one of the leading live jazz performers. In 1961 Fitzgerald bought a house in the Klampenborg district of Copenhagen, Denmark, after she began a relationship with a Danish man. Though the relationship ended after a year, Fitzgerald regularly returned to Denmark over the next three years, and even considered buying a jazz club there. The house was sold in 1963, and Fitzgerald permanently returned to the United States.
There are several live albums on Verve that are highly regarded by critics. "Ella at the Opera House" shows a typical JATP set from Fitzgerald. ' and "Twelve Nights in Hollywood" display her vocal jazz canon. ' is still one of her best selling albums; it includes a Grammy-winning performance of "Mack the Knife" in which she forgets the lyrics, but improvises magnificently to compensate.
Verve Records was sold to MGM in 1963 for $3 million and in 1967 MGM failed to renew Fitzgerald's contract. Over the next five years she flitted between Atlantic, Capitol and Reprise. Her material at this time represented a departure from her typical jazz repertoire. For Capitol she recorded "Brighten the Corner", an album of hymns, "Ella Fitzgerald's Christmas", an album of traditional Christmas carols, "Misty Blue", a country and western-influenced album, and "30 by Ella", a series of six medleys that fulfilled her obligations for the label. During this period, she had her last US chart single with a cover of Smokey Robinson's "Get Ready", previously a hit for the Temptations, and some months later a top-five hit for Rare Earth.
The surprise success of the 1972 album "Jazz at Santa Monica Civic '72" led Granz to found Pablo Records, his first record label since the sale of Verve. Fitzgerald recorded some 20 albums for the label. "Ella in London" recorded live in 1974 with pianist Tommy Flanagan, guitarist Joe Pass, bassist Keter Betts and drummer Bobby Durham, was considered by many to be some of her best work. The following year she again performed with Joe Pass on German television station NDR in Hamburg. Her years with Pablo Records also documented the decline in her voice. "She frequently used shorter, stabbing phrases, and her voice was harder, with a wider vibrato", one biographer wrote. Plagued by health problems, Fitzgerald made her last recording in 1991 and her last public performances in 1993.
Film and television.
In her most notable screen role, Fitzgerald played the part of singer Maggie Jackson in Jack Webb's 1955 jazz film "Pete Kelly's Blues". The film costarred Janet Leigh and singer Peggy Lee. Even though she had already worked in the movies (she had sung briefly in the 1942 Abbott and Costello film "Ride 'Em Cowboy"), she was "delighted" when Norman Granz negotiated the role for her, and, "at the time...considered her role in the Warner Brothers movie the biggest thing ever to have happened to her." Amid "The New York Times" pan of the film when it opened in August 1955, the reviewer wrote, "About five minutes (out of ninety-five) suggest the picture this might have been. Take the ingenious prologue ... take the fleeting scenes when the wonderful Ella Fitzgerald, allotted a few spoken lines, fills the screen and sound track with her strong mobile features and voice." Fitzgerald's race precluded major big-screen success. After "Pete Kelly's Blues", she appeared in sporadic movie cameos, in "St. Louis Blues" (1958), and "Let No Man Write My Epitaph" (1960). Much later, she appeared in the 1980s television drama "The White Shadow".
She made numerous guest appearances on television shows, singing on "The Frank Sinatra Show", "The Andy Williams Show", "The Pat Boone Chevy Showroom", and alongside other greats Nat King Cole, Dean Martin, Mel Tormé, and many others. She was also frequently featured on "The Ed Sullivan Show". Perhaps her most unusual and intriguing performance was of the "Three Little Maids" song from Gilbert and Sullivan's comic operetta "The Mikado" alongside Joan Sutherland and Dinah Shore on Shore's weekly variety series in 1963. A performance at Ronnie Scott's Jazz Club in London was filmed and shown on the BBC. Fitzgerald also made a one-off appearance alongside Sarah Vaughan and Pearl Bailey on a 1979 television special honoring Bailey. In 1980, she performed a medley of standards in a duet with Karen Carpenter on the Carpenters' television program "Music, Music, Music".
Fitzgerald also appeared in TV commercials, her most memorable being an ad for Memorex. In the commercials, she sang a note that shattered a glass while being recorded on a Memorex cassette tape. The tape was played back and the recording also broke the glass, asking: "Is it live, or is it Memorex?" She also starred in a number of commercials for Kentucky Fried Chicken, singing and scatting to the fast-food chain's longtime slogan, "We do chicken right!" Her final commercial campaign was for American Express, in which she was photographed by Annie Leibovitz.
Collaborations.
Fitzgerald's most famous collaborations were with the vocal quartet Bill Kenny & the Ink Spots, trumpeter Louis Armstrong, the guitarist Joe Pass, and the bandleaders Count Basie and Duke Ellington.
Fitzgerald had a number of famous jazz musicians and soloists as sidemen over her long career. The trumpeters Roy Eldridge and Dizzy Gillespie, the guitarist Herb Ellis, and the pianists Tommy Flanagan, Oscar Peterson, Lou Levy, Paul Smith, Jimmy Rowles, and Ellis Larkins all worked with Ella mostly in live, small group settings.
Possibly Fitzgerald's greatest unrealized collaboration (in terms of popular music) was a studio or live album with Frank Sinatra. The two appeared on the same stage only periodically over the years, in television specials in 1958 and 1959, and again on 1967's "A Man and His Music + Ella + Jobim", a show that also featured Antônio Carlos Jobim. Pianist Paul Smith has said, "Ella loved working with . Sinatra gave her his dressing-room on "A Man and His Music" and couldn't do enough for her." When asked, Norman Granz would cite "complex contractual reasons" for the fact that the two artists never recorded together. Fitzgerald's appearance with Sinatra and Count Basie in June 1974 for a series of concerts at Caesars Palace, Las Vegas, was seen as an important incentive for Sinatra to return from his self-imposed retirement of the early 1970s. The shows were a great success, and September 1975 saw them gross $1,000,000 in two weeks on Broadway, in a triumvirate with the Count Basie Orchestra.
Later life and death.
In 1985, Fitzgerald was hospitalized briefly for respiratory problems, in 1986 for congestive heart failure, and in 1990 for exhaustion. In March 1990 she appeared at the Royal Albert Hall in London, England with the Count Basie Orchestra for the launch of Jazz FM, plus a gala dinner at the Grosvenor House Hotel at which she performed. In 1993, she had to have both of her legs amputated below the knee due to the effects of diabetes. Her eyesight was affected as well.
In 1996, tired of being in the hospital, she wished to spend her last days at home. Confined to a wheelchair, she spent her final days in her backyard of her Beverly Hills mansion on Whittier, with her son Ray and 12-year-old granddaughter, Alice. "I just want to smell the air, listen to the birds and hear Alice laugh," she reportedly said. On her last day, she was wheeled outside one last time, and sat there for about an hour. When she was taken back in, she looked up with a soft smile on her face and said, "I'm ready to go now." She died in her home on June 15, 1996 at the age of 79. A few hours after her death, the Playboy Jazz Festival was launched at the Hollywood Bowl. In tribute, the marquee read: "Ella We Will Miss You." Her funeral was private, and she was buried at Inglewood Park Cemetery in Los Angeles.
Personal life.
Fitzgerald married at least twice, and there is evidence that she may have married a third time. Her first marriage was in 1941, to Benny Kornegay, a convicted drug dealer and local dockworker. The marriage was annulled in 1942.
Her second marriage was in December 1947, to the famous bass player Ray Brown, whom she had met while on tour with Dizzy Gillespie's band a year earlier. Together they adopted a child born to Fitzgerald's half-sister, Frances, whom they christened Ray Brown, Jr. With Fitzgerald and Brown often busy touring and recording, the child was largely raised by his mother's aunt, Virginia. Fitzgerald and Brown divorced in 1953, bowing to the various career pressures both were experiencing at the time, though they would continue to perform together.
In July 1957, Reuters reported that Fitzgerald had secretly married Thor Einar Larsen, a young Norwegian, in Oslo. She had even gone as far as furnishing an apartment in Oslo, but the affair was quickly forgotten when Larsen was sentenced to five months' hard labor in Sweden for stealing money from a young woman to whom he had previously been engaged.
Fitzgerald was also notoriously shy. Trumpet player Mario Bauzá, who played behind Fitzgerald in her early years with Chick Webb, remembered that "she didn't hang out much. When she got into the band, she was dedicated to her music...She was a lonely girl around New York, just kept herself to herself, for the gig." When, later in her career, the Society of Singers named an award after her, Fitzgerald explained, "I don't want to say the wrong thing, which I always do but I think I do better when I sing."
Fitzgerald was a quiet but ardent supporter of many charities and non-profit organizations, including the American Heart Association and the City of Hope Medical Center. In 1993, she established the Ella Fitzgerald Charitable Foundation.
Discography and collections.
The primary collections of Fitzgerald's media and memorabilia reside at and are shared between the Smithsonian Institution and the US Library of Congress 
Awards, citations and honors.
Fitzgerald won thirteen Grammy Awards, and received the Grammy Lifetime Achievement Award in 1967.
Other major awards and honors she received during her career were the Kennedy Center for the Performing Arts Medal of Honor Award, National Medal of Art, first Society of Singers Lifetime Achievement Award, named "Ella" in her honor, Presidential Medal of Freedom, and the George and Ira Gershwin Award for Lifetime Musical Achievement, UCLA Spring Sing. Across town at the University of Southern California, she received the USC "Magnum Opus" Award which hangs in the office of the Ella Fitzgerald Charitable Foundation. In 1990, she received an honorary doctorate of Music from Harvard University.
Tributes and legacy.
The career history and archival material from Ella's long career are housed in the Archives Center at the Smithsonian's National Museum of American History, while her personal music arrangements are at the Library of Congress. Her extensive cookbook collection was donated to the Schlesinger Library at Harvard University, and her extensive collection of published sheet music was donated to UCLA.
In 1997, Newport News, Virginia created a music festival with Christopher Newport University to honor Ella Fitzgerald in her birth city. Past performers at the week-long festival include: Diana Krall, Arturo Sandoval, Jean Carne, Phil Woods, Aretha Franklin, Victoria Wyndham, Charles Keating, Freda Payne, Cassandra Wilson, Ethel Ennis, David Sanborn, Jane Monheit, Dianne Reeves, Dee Dee Bridgewater, Ramsey Lewis, Patti Austin, Lalah Hathaway, Ledisi, Chrisette Michele, Natalie Cole, Freddie Jackson, Joe Harnell, Roy Ayers and Ann Hampton Callaway.
Callaway, Dee Dee Bridgewater, and Patti Austin have all recorded albums in tribute to Fitzgerald. Callaway's album "To Ella with Love" (1996) features fourteen jazz standards made popular by Fitzgerald, and the album also features the trumpeter Wynton Marsalis. Bridgewater's album "Dear Ella" (1997) featured many musicians that were closely associated with Fitzgerald during her career, including the pianist Lou Levy, the trumpeter Benny Powell, and Fitzgerald's second husband, double bassist Ray Brown. Bridgewater's following album, "Live at Yoshi's", was recorded live on April 25, 1998, what would have been Fitzgerald's 81st birthday.
Austin's album, "For Ella" (2002) features 11 songs most immediately associated with Fitzgerald, and a twelfth song, "Hearing Ella Sing" is Austin's tribute to Fitzgerald. The album was nominated for a Grammy. In 2007, "", was released, a tribute album recorded for the 90th anniversary of Fitzgerald's birth. It featured artists such as Michael Bublé, Natalie Cole, Chaka Khan, Gladys Knight, Diana Krall, k.d. lang, Queen Latifah, Ledisi, Dianne Reeves, Linda Ronstadt, and Lizz Wright, collating songs most readily associated with the "First Lady of Song". Folk singer Odetta's album "To Ella" (1998) is dedicated to Fitzgerald, but features no songs associated with her. Her accompanist Tommy Flanagan affectionately remembered Fitzgerald on his album "Lady be Good ... For Ella" (1994).
"Ella, elle l'a", a tribute to Fitzgerald written by Michel Berger and performed by French singer France Gall, was a hit in Europe in 1987 and 1988. Fitzgerald is also referred to in the 1976 Stevie Wonder hit "Sir Duke" from his album "Songs in the Key of Life", and the song "I Love Being Here With You", written by Peggy Lee and Bill Schluger. Sinatra's 1986 recording of "Mack the Knife" from his album "L.A. Is My Lady" (1984) includes a homage to some of the song's previous performers, including 'Lady Ella' herself. She is also honored in the song "First Lady" by Canadian artist Nikki Yanofsky.
In 2008, the Downing-Gross Cultural Arts Center in Newport News named its brand new 276-seat theater the Ella Fitzgerald Theater. The theater is located several blocks away from her birthplace on Marshall Avenue. The Grand Opening performers (October 11 and 12, 2008) were Roberta Flack and Queen Esther Marrow.
In 2012, Rod Stewart performed a "virtual duet" with Ella Fitzgerald on his Christmas album "Merry Christmas, Baby", and his television special of the same name.
There is a bronze sculpture of Fitzgerald in Yonkers, the city in which she grew up, created by American artist Vinnie Bagwell. It is located southeast of the main entrance to the Amtrak/Metro-North Railroad station in front of the city's old trolley barn. A bust of Fitzgerald is on the campus of Chapman University in Orange, California. On January 9, 2007, the United States Postal Service announced that Fitzgerald would be honored with her own postage stamp. The stamp was released in April 2007 as part of the Postal Service's Black Heritage series.

</doc>
<doc id="50352" url="https://en.wikipedia.org/wiki?curid=50352" title="Lexington, Kentucky">
Lexington, Kentucky

Lexington, consolidated with Fayette County, is the second-largest city in Kentucky and the 61st largest in the United States. Known as the "Horse Capital of the World", it is the heart of the state's Bluegrass region. With a mayor-alderman form of government, it is one of two cities in Kentucky designated by the state as first-class; the other is the state's largest city of Louisville. In the 2014 U.S. Census Estimate, the city's population was 310,797, anchoring a metropolitan area of 489,435 people and a combined statistical area of 708,677 people.
Lexington ranks tenth among US cities in college education rate, with 39.5% of residents having at least a bachelor's degree. It is the location of the Kentucky Horse Park, The Red Mile and Keeneland race courses, Rupp Arena, the world's largest basketball-specific arena, Transylvania University, the University of Kentucky and Bluegrass Community & Technical College.
History.
This area of fertile soil and abundant wildlife was long occupied by varying tribes of Native Americans. European explorers began to trade with them but settlers did not come in force until the late 18th century.
Lexington was founded by European Americans in June 1775, in what was then considered Fincastle County, Virginia, 17 years before Kentucky became a state. A party of frontiersmen, led by William McConnell, camped on the Middle Fork of Elkhorn Creek (now known as Town Branch and rerouted under Vine Street) at the site of the present-day McConnell Springs. Upon hearing of the colonists' victory in the Battles of Lexington and Concord on April 19, 1775, they named their campsite Lexington. It was the first of what would be many American places to be named after the Massachusetts town. The risk of Indian attacks delayed permanent settlement for four years.
In 1779, during the American Revolutionary War, Col. Robert Patterson and 25 companions came from Fort Harrod and erected a blockhouse. They built cabins and a stockade, establishing a settlement known as Bryan Station. In 1780, Lexington was made the seat of Virginia's newly organized Fayette County. Colonists defended it against a British and allied Shawnee attack in 1782, during the last part of the American Revolutionary War.
The town was chartered on May 6, 1782, by an act of the Virginia General Assembly. The First African Baptist Church was founded by Peter Durrett, a Baptist preacher and slave held by Joseph Craig. Durrett helped guide "The Traveling Church", a group migration of several hundred pioneers led by the preacher Lewis Craig and Captain William Ellis from Orange County, Virginia to Kentucky in 1781. It is the oldest black Baptist congregation in Kentucky and the third oldest in the United States.
In 1806, Lexington was a rising city of the vast territory to the west of the Appalachian Mountains; poet Josiah Espy described it in the following letter:
Lexington is the largest and most wealthy town in Kentucky, or indeed west of the Allegheny Mountains; the main street of Lexington has all the appearance of Market Street in Philadelphia on a busy day ... I would suppose it contains about five hundred dwelling houses was closer to three hundred, many of them elegant and three stories high. About thirty brick buildings were then raising, and I have little doubt but that in a few years it will rival, not only in wealth, but in population, the most populous inland town of the United States ... The country around Lexington for many miles in every direction, is equal in beauty and fertility to anything the imagination can paint and is already in a high state of cultivation.Residents have fondly continued to refer to Lexington as "The Athens of the West" since Espy's poem dedicated to the city.
In the early 19th century, planter John Wesley Hunt became the first millionaire west of the Alleghenies. The growing town was devastated by a cholera epidemic in 1833, which had spread throughout the waterways of the Mississippi and Ohio valleys: 500 of 7,000 Lexington residents died within two months, including nearly one-third of the congregation of Christ Church Episcopal. London Ferrill, second preacher of First African Baptist, was one of three clergy who stayed in the city to serve the suffering victims. Additional cholera outbreaks occurred in 1848–49 and the early 1850s. Cholera was spread by people using contaminated water supplies, but its transmission was not understood in those years. Often the wealthier people would flee town for outlying areas to try to avoid the spread of disease.
Planters held slaves for use as field hands, laborers, artisans, and domestic servants. In the city, slaves worked primarily as domestic servants and artisans, although they also worked with merchants, shippers, and in a wide variety of trades. Plantations raised commodity crops of tobacco and hemp, and thoroughbred horse breeding and racing became established in this part of the state. In 1850, one-fifth of the state's population were slaves, and Lexington had the highest concentration of slaves in the entire state. It also had a significant population of free blacks, who were usually of mixed race. By 1850, First African Baptist Church, led by London Ferrill, a free black from Virginia, had a congregation of 1,820 persons, the largest of any, black or white, in the entire state.
Many of 19th-century America's leading political and military figures spent part of their lives in the city, including U.S. President Abraham Lincoln and Confederate President Jefferson Davis (who attended Transylvania University in 1823 and 1824); Confederate general John Hunt Morgan; U.S. Senator and Vice President John C. Breckinridge; and Speaker of the House, U.S. Senator, and Secretary of State Henry Clay, who had a plantation nearby. Lincoln's wife Mary Todd Lincoln was born and raised in Lexington, and the couple visited the city several times after their marriage in 1842.
During the 19th century, migrants moved from central Kentucky to Tennessee and Missouri. They established their traditional crops and livestock in Middle Tennessee and an area of Missouri along the Missouri River. While Kentucky stayed in the Union during the American Civil War, the residents of different regions of the state had divided loyalties.
20th century to present.
In 1935 during the Great Depression, the Addiction Research Center (ARC) was created as a small research unit at the U.S. Public Health Service Hospital in Lexington. Founded as one of the first drug rehabilitation clinics in the nation, the ARC was affiliated with a Federal prison. Expanded as the first alcohol and drug rehabilitation hospital in the United States, it was known as "Narco" of Lexington. The hospital was later converted to operate as part of the federal prison system; it is known as the Federal Medical Center, Lexington and serves a variety of health needs for prisoners.
Lexington has continued as the center of thoroughbred horse breeding and racing in Kentucky, with major racing and sales facilities, as well as a museum of horses and the sport.
Geography.
Lexington, which includes all Fayette County, consists of , mostly gently rolling plateau, in the center of the inner Bluegrass Region. The area is noted for its fertile soil, excellent pastureland, and horse and stock farms. "Poa pratensis" (bluegrass) thrives on the limestone beneath the soil's surface, playing a major role in the development of champion horses; it is associated with the area's beauty of landscape. Numerous small creeks rise and flow into the Kentucky River.
The Lexington-Fayette Metro area includes five counties: Clark, Jessamine, Bourbon, Woodford, and Scott. This is the second-largest metro area in Kentucky after that of Louisville. According to the United States Census Bureau, the city has a total area of . of it is land and of it (0.35%) is water.
Cityscape.
Lexington features a diverse cityscape. "Forbes" has ranked Lexington as one of the world's seventeen cleanest cities.
Planning.
Lexington must manage a rapidly growing population while working to maintain the character of the surrounding horse farms that give the region its identity. In 1958 Lexington enacted the nation's first Urban Growth Boundary, restricting new development to an Urban Service Area. It set a strict minimum area requirement, currently , to maintain open space for landholdings in the Rural Service Area. In 1967, the Urban Service Area was decreased in area; various zoning regulations were also amended from the original 1958 issue. Several years later, in 1973, the first Lexington Comprehensive Plan was completed.
In 1980, the Comprehensive Plan was updated: the Urban Service Area was modified to include Urban Activity Centers and Rural Activity Centers. The Urban Activity Centers were commercial and light-industrial districts in urbanized areas, while Rural Activity Centers were retail trade and light-industrial centers clustered around the Interstate 64/Interstate 75 interchanges. In 1996, the Urban Service Area was expanded when of the Rural Service Area was acquired through the Expansion Area Master Plan. This was controversial: this first major update to the Comprehensive Plan in over a decade was accompanied by arguments among residents about the future of Lexington and the thoroughbred farms.
The Expansion Area Master Plan included new concepts of impact fees, assessment districts, neighborhood design concepts, design overlays, mandatory greenways, major roadway improvements, stormwater management, and open space mitigation for the first time. It also included a draft of the Rural Land Management Plan, which included large-lot zoning and traffic impact controls. A pre-zoning of the entire expansion area was refuted in the Plan. A minimum proposal was defeated. Discussion of this proposal appeared to stimulate the development of numerous subdivisions in the Rural Service Areas.
Three years after the expansion was initiated, the Rural Service Area Land Management Plan was adopted, which increased the minimum lot size in the agricultural rural zones to minimums. In 2000, a Purchase of Development Rights plan was adopted, granting the city the power to purchase the development rights of existing farms; in 2001, $40 million was allocated to the plan from a $25 million local, $15 million state grant. An Infill and Redevelopment study was also initiated during that time, along with design guidelines for the areas surrounding the new Fayette County courthouses.
Climate.
Lexington is in the northern periphery of the humid subtropical climate zone, with hot, humid summers, and cool winters with occasional mild periods; it falls in USDA hardiness zone 6b. The city and the surrounding Bluegrass region have four distinct seasons that include cool plateau breezes, moderate nights in the summer, and no prolonged periods of heat, cold, rain, wind, or snow. The monthly daily average temperature ranges from in January to in July, while the annual mean temperature is . On average, there are 23 days of + highs annually and 19 days per winter where the high fails to rise above freezing. Annual precipitation is , with the late spring and summer months being slightly wetter; snowfall averages per season. Extreme temperatures range from on January 24, 1963, up to on July 10 and 15, 1936.
Lexington is recognized as a high allergy area by the Asthma and Allergy Foundation of America. The results for the spring of 2008 rank Lexington as first among high-allergy cities.
Demographics.
The Lexington-Fayette Metropolitan Statistical Area (MSA) includes Bourbon, Clark, Fayette, Jessamine, Scott, and Woodford counties. The MSA population in 2013 was estimated at 489,435.
The Lexington-Fayette-Frankfort-Richmond, KY Combined Statistical Area had an estimated population of 703,271 in 2012. This includes the metro area and an additional seven counties.
As of the census of 2000, there were 260,512 people, 108,288 households, and 62,915 families residing in the city. The population density was 915.6 people per square mile (353.5/km²). There were 116,167 housing units at an average density of 408.3/mi² (157.6/km²). The racial makeup of the city was 81.04% White, 13.48% African American, 0.19% Native American, 2.46% Asian, 0.03% Pacific Islander, 1.21% from other races, and 1.58% from two or more races. Hispanic or Latino of any race were 3.29% of the population.
There were 108,288 households out of which 27.3% had children under the age of 18 living with them, 43.5% were married couples living together, 11.5% had a female householder with no husband present, and 41.9% were non-families. 31.7% of all households were made up of individuals and 7.5% had someone living alone who was 65 years of age or older. The average household size was 2.29 and the average family size was 2.90.
In the city the population was spread out with 21.3% under the age of 18, 14.6% from 18 to 24, 33.2% from 25 to 44, 20.9% from 45 to 64, and 10.0% who were 65 years of age or older. The median age was 33 years. For every 100 females there were 96.5 males. For every 100 females age 18 and over, there were 94.3 males.
The median income for a household in the city was $39,813, and the median income for a family was $53,264. Males had a median income of $36,166 versus $26,964 for females. The per capita income for the city was $23,109. About 8.2% of families and 12.9% of the population were below the poverty line, including 14.3% of those under the age of 18 and 8.6% of those ages 65 and older.
The table below illustrates the population growth of Fayette County since the first U.S. Census in 1790. Lexington city limits became coterminous with Fayette County in 1974.
Sources:
Economy.
Lexington has one of the nation's most stable economies. Lexington describes itself as having "a fortified economy, strong in manufacturing, technology and entrepreneurial support, benefiting from a diverse, balanced business base". The Lexington Metro Area had an unemployment rate of 3.7% in August 2015, lower than many cities of similar size. In 2011 Lexington was ranked as the 4th-best city for "Businesses and Careers" by "Forbes" magazine, the 5th-best city for Young Professionals in 2008, and 6th-Best "Value Cities" in 2011 by "Kiplinger."
The city is home to several large corporations. Sizable employment is generated by four Fortune 500 companies: Xerox (who acquired Affiliated Computer Services), Lexmark International, Lockheed-Martin, and IBM, employing 3,000, 2,800, 1,705, and 552, respectively. United Parcel Service, Trane, and Amazon.com, Inc. have large operations in the city, and Toyota Motor Manufacturing Kentucky is within the Lexington CSA, located in adjoining Georgetown. A Jif peanut butter plant located here produces more peanut butter than any other factory in the world.
Notable corporate headquarters include: Lexmark International, a manufacturer of printers and enterprise software; Big Ass Solutions, a manufacturer of large ceiling fans and lighting fixtures for industrial, commercial, agricultural, and residential use; A&W Restaurants, a restaurant chain known for root beer and root beer floats; Fazoli's, a fast food Italian-style chain that has expanded to more than twenty states; Tempur Sealy International, a manufacturer of mattresses; Florida Tile, a manufacturer of porcelain and ceramic tile; and the Forcht Group of Kentucky, a holding company that employs more than 2,100 people across Kentucky. Forcht Group operates several businesses in Lexington, including First Corbin Bancorp, Kentucky National Insurance Company, My Favorite Things, BSC, a bank data services company; and First Lab, among others.
The city's largest employer, the University of Kentucky, as of 2012, employs about 14,000. That number is expected to shrink due to reduced funding from the state. The University ranks as the 9th-largest economic company in the state of Kentucky, with an annual budget of $1.4 billion, and the College of Medicine within the University is the 21st-largest company in the state.
Other sizable employers include the Lexington-Fayette County government and other hospital facilities. The Fayette County Public Schools employ 5,374, and the Lexington-Fayette Urban County Government employs 2,699. Central Baptist Hospital, Saint Joseph Hospital, Saint Joseph East, and the Veterans Administration Hospital employ 7,000 persons in total.
Culture.
Annual cultural events and fairs.
Lexington is home to many thriving arts organizations including a professional orchestra, two ballet companies, professional theatre, several museums, several choral organizations, and a highly respected opera program at the University of Kentucky. In addition, several annual events and fairs draw people as attendees from throughout the Bluegrass region.
Mayfest Arts Fair is a free outdoor festival that takes place annually over Mother's Day weekend. Held in Gratz Park between the Carnegie Center and Transylvania University, the festival typically features up to 100 art and craft booths, live entertainment throughout the weekend, food, children's activities, adult activities and literary events, free carriage rides, a traditional Morris and Maypole dance and various demonstrations.
June has two popular music festivals: bluegrass and Broadway. The Festival of the Bluegrass, Kentucky's oldest bluegrass music festival, is in early June; it includes three stages for music and a "bluegrass music camp" for school children. For more than two decades, during the 2nd and 3rd weekends, UK Opera Theatre presents a Broadway medley "It's A Grand Night for Singing!"
Later in June, the Gay and Lesbian Services Organization hosts the Lexington Pride Festival, which celebrates pride in the lesbian, gay, bisexual, and transgender communities and welcomes allies. The festival offers live music, crafts, food, and informational booths from diverse service organizations. Lexington Mayor Jim Gray, elected in 2010 and openly gay, proclaimed June 29, 2013 as Pride Day. Lexington has one of the highest concentrations of gay and lesbian couples in the United States for a city its size.
Area residents gather downtown for the Fourth of July festivities which extend for several days. On July 3, the Gratz Park Historic District is transformed into an outdoor music hall when the Patriotic Music Concert is held on the steps of Morrison Hall at Transylvania University. The Lexington Singers and the Lexington Philharmonic Orchestra perform at this event. On the Fourth, events include a reading of the Declaration of Independence on the steps of the Old Courthouse, a waiters' race in Phoenix Park, a parade, a country music concert, street vendors for wares and food, and fireworks. The Fourth of July may be the biggest holiday in Lexington.
The Woodland Arts Fair, almost four decades old, is held in mid-August by The Lexington Art League. It features many local and national artists working in a variety of media; vendors also sell refreshments.
Since the turn of the 21st century, Festival Latino de Lexington, the biggest fiesta in September, has been the city's main event to celebrate Hispanic Heritage Month. During the festival, thousands of people, Hispanics and non-Hispanics alike, gather in downtown Lexington to enjoy the cultural displays, dancing presentations, live music, and a variety of food from different regions of Latin America.
Also in September, the Roots & Heritage Festival includes art exhibits, literary readings, film presentations, the Festival Ball and the ever-popular two-day street festival featuring live musical performances from internationally renowned artists.
"Southern Lights: Spectacular Sights on Holiday Nights", taking place from November 18 to December 31, is held at the Kentucky Horse Park. It includes a three-mile (5 km) drive through the park, showcasing numerous displays, many in character with the horse industry and history of Lexington. The "Mini-Train Express", an indoor petting zoo featuring exotic animals, the International Museum of the Horse, an exhibit showcasing the Bluegrass Railway Club's model train, and Santa Claus are other major highlights.
In 2002, Lexington became the first city to launch a ""Thriller" video" reenactment as a Halloween festivity. The video's storyline and dance sequences are faithfully recreated in a parade beginning outside the historic Kentucky Theatre. The hundreds of zombies faithfully rehearse at the nearby Mecca Live Studio in the weeks leading up to the parade.
The Lexington Christmas Parade is held usually the day after Thanksgiving. The parade route follows Main Street between Midland and Broadway. Festivities include a Holiday Market with over 25 arts and craft vendors, a stage with entertainment, food, and the annual tree lighting ceremony, which occur in Triangle Park.
Other events and fairs include:
Historical structures and museums.
Lexington is home to numerous museums and historical structures. One of the most famous is Ashland: The Henry Clay Estate along Richmond Road east of downtown. This two-story museum is a National Historic Landmark and was the former home of statesman Henry Clay.
Since the late 20th century, Lexington has demolished hundreds of historic structures to make way for hotels, banks and parking structures. The Lexington Public Library was constructed where the historic Phoenix Hotel once stood. An historic 1880s block located on Main Street was demolished. This lot is still vacant.
The Pope Villa, built in 1811, is one of the best surviving domestic designs by the architect Benjamin Henry Latrobe. It is being restored by the Bluegrass Trust for Historic Preservation, which also maintains and administers the Hunt-Morgan House.
Additional historic sites are the following:
The University of Kentucky Art Museum is the premier art museum for Lexington and the only accredited museum in the region. Its collection of over 4000 objects ranges from Old Masters to Contemporary. It regularly hosts special exhibitions.
The world's largest ceiling clock and a five-story Foucault pendulum are located inside the Lexington Public Library on East Main St. The Central Library is also home to an art gallery and the 138-seat Farish Theater. The city library has five branches located throughout the city: Beaumont (off Harrodsburg Rd.), Eagle Creek (off Richmond Rd.), Northside (Russell Cave Rd.), Tates Creek (off Tates Creek Rd.), and Village (Versailles Rd.). Lexington Public Library offers a variety of programs and services to the citizens of Fayette County and circulates 3,000,000 items per year.
The local Woolworth's building was listed on the National Register of Historic Places for its significance as a site of civil rights protests against segregation during the 1960s. Activists conducted sit-ins to gain integrated lunch service, full access to facilities, and more employment. The building was demolished by its owner in 2004 and the area paved for use as a parking lot until further development.
Sports.
College athletics.
The Kentucky Wildcats, the athletic program of the University of Kentucky, is Lexington's most popular sports entity. The school fields 22 varsity sports teams, most of which compete in the Southeastern Conference as a founding member. The men's basketball team is highly ranked, having won 8 NCAA championships. It is considered the "winningest program" in college basketball history, being the first team to reach 2000 wins. The Transylvania University Pioneers compete in NCAA Division III athletics.
Professional sports.
Lexington is home to the Lexington Legends, a Class A minor league affiliate of the Kansas City Royals as 2012. Since its inception in 2001, Lexington has produced numerous major leaguers including: 2009 National League All-Star Hunter Pence (Outfielder), John Buck (Catcher), Mike Gallo (Pitcher), and Josh Anderson, (Right fielder). The Legends belong to the South Atlantic League, have one league title (2001) and four playoff appearances since 2001. Roger Clemens pitched in one game for the Legends in 2006 as part of his preparation to return to the Houston Astros.
Horse racing and equestrian events.
This area has been known as a major center for Thoroughbred breeding since the late 18th century due to the high calcium content in the soils of the Inner Bluegrass Region. Horses raised on its grass develop stronger bones and endurance.
The city is home to two horse racing tracks, Keeneland and The Red Mile harness track. Keeneland, sporting live races in April and October, is steeped in tradition; little has changed since the track's opening in 1936. Keeneland hosted the 2015 Breeder's Cup which was won by Triple Crown winner American Pharoah. This track also has the world's largest thoroughbred auction house. 19 Kentucky Derby winners, 21 Preakness winners and 18 Belmont winners were graduates of Keeneland sales. Its most notable race is the Blue Grass Stakes which is considered an important prep for the Kentucky Derby. The Red Mile Harness Track is the oldest horse racing track in the city and the second oldest in the nation. There you can see horses pull two-wheeled carts called sulkies while racing, also known as harness racing. The Red Mile Harness Track and Keeneland announced a partnership in 2014.
The Kentucky Horse Park, located along scenic Iron Works Pike in northern Fayette County, is a comparative late-comer to Lexington, opening in 1978. Although commonly known as a tourist attraction and museum, it is also a museum and working horse farm with a farrier and famous retired horses such as 2003 Kentucky Derby winner Funny Cide. Since its opening in April 1978, the Kentucky Horse Park has hosted the Rolex Kentucky Three Day Event, which is one of the top 3 annual equestrian eventing competitions in the world and held immediately before the Kentucky Derby at Churchill Downs in Louisville. In September and October 2010, Lexington hosted the World Equestrian Games, the first time the games were held outside of Europe. Since October/November 2011, the Kentucky Horse Park has hosted the National Horse Show.
Parks and outdoor attractions.
City parks and facilities.
Lexington has over 100 parks ranging in size from the Smith Street Park to the Masterson Station Park. Among those parks are:
For the Kentucky Horse Park, see Sporting Events below.
Natural areas.
The city is home to Raven Run Nature Sanctuary, a nature preserve along the Kentucky River Palisades. There are of back-country hiking trails that range from wheelchair-accessible paved trails to difficult single-track trails. It is common to run across hopeful Appalachian Trail backpackers. The city has recently purchased land adjacent to the park which will make Raven Run the largest park in the city. Raven Run is home to over 56 species of trees, 600 species of plants, 200 species of birds, and other wildlife. Remains of a grist mill, homestead and limekiln remain. The preserve also has a nature center and various educational programs throughout the year. Such programs include seasonal wildflower walks, stargazing during the warmer months, evening insect tours, and historical walks and presentations.
The Arboretum is a preserve adjacent to the University of Kentucky. It features the "Arboretum Woods", a small, Bluegrass Woodland patch that is home to eighteen native Kentucky tree species, and more than 50 native Kentucky grasses and herbs. It also has 1,500 varieties of roses in the "Rose Garden", a "Home Demonstration Garden", and numerous paved paths and trails.
The city also plays host to the historic McConnell Springs, a park within the industrial confines off of Old Frankfort Pike. There are two miles (3 km) of trails that surround the namesake springs, historic dry-laid stone fences, and historical structures.
Government and politics.
Federally, Lexington is part of Kentucky's 6th congressional district, represented by Republican Andy Barr, elected in 2012. He defeated five-term Democrat Ben Chandler.
The state's senior member of the United States Senate is the Republican Floor Leader, Mitch McConnell, elected in 1984. The state's junior member of the United States Senate is Rand Paul, elected in 2010. The Governor of Kentucky is Republican Matt Bevin, elected in 2015.
In 1974, the governments of the city of Lexington and Fayette County, Kentucky, combined to create the current "Lexington-Fayette Urban County Government", often abbreviated "LFUCG". Lexington has an elected mayor and city council-style of government.
Mayor.
On November 2, 2010, former vice-mayor Jim Gray was elected mayor, becoming the city's first openly gay mayor. He was sworn into office on January 2, 2011, by Kentucky Supreme Court justice Mary Noble. Gray was preceded by Mayor Jim Newberry, and the two collaborated on developing improved industrial and agricultural opportunity for the City of Lexington and the State of Kentucky. Elections are officially nonpartisan.
Urban County Council.
The Urban County Council is a 15-member legislative group. Twelve of the members represent specific districts and serve two-year terms; three are elected city-wide as at-large council members and serve four-year terms. The at-large member receiving the highest number of votes in the general election automatically becomes the Vice Mayor who, in the absence of the Mayor, is the presiding officer of the Council. The current council members as of 2015 are:
Law enforcement.
Primary law enforcement duties within Lexington-Fayette County is the responsibility of the Lexington-Fayette Urban County Government Division of Police. The Division of Police resulted from the merger of the Lexington Police Department with the Fayette County Patrol in 1974. The Fayette County Sheriff's Office is responsible for court service, including court security, prisoner transport, process and warrant service, and property tax collection. The 1974, merger also consolidated the Office of City Jailer into the Office of County Jailer, a Constitutional position. In 1992 (effective 1993), the Kentucky General Assembly enabled a correctional services division to be established by ordinance, making employees civil service employees rather than political appointees. The University of Kentucky Police Department, the Transylvania University Department of Public Safety, Blue Grass Airport Public Safety, Lexington-Fayette Division of Community Corrections, Fayette County Schools Department of Law Enforcement, the Veterans Affairs Police Department and the Kentucky Horse Park Police also have jurisdiction within their geographic areas in Lexington-Fayette County. In addition, the Lexington-Fayette Animal Care & Control exercises law enforcement over animal control issues and the Kentucky State Police, KSP Division of Commercial Vehicle Enforcement and Kentucky Department of Fish and Wildlife Resources can be seen performing their respective law enforcement duties within the county. Several federal law enforcement agencies, such as the FBI and Secret Service, maintain satellite offices within Lexington. Prisons in Fayette County include the Federal Medical Center, Lexington, operated by the Federal Bureau of Prisons, and the Blackburn Correctional Complex, operated by the Kentucky Department of Corrections.
Education.
According to the United States Census, of Lexington's population over the age of twenty-five, 22.4% hold a bachelor's degree, 11.4% hold a master's degree, and 3.1% hold a professional degree. Just 2.6% hold a doctorate degree. Lexington was ranked 10th in a list of America's most educated cities with a population of more than 250,000, ranked by percentage of bachelor's degrees among residents 25 and older, according to the United States Census Bureau. In a report released by Jack Miller, president of Central Connecticut State University, Lexington ranks 13th in the United States in terms of literacy rate. The index was compiled through six indicators of literacy, including Internet sources, newspaper circulation, the number of bookstores, library resources, education and periodical resources.
The city is served by the Fayette County Public Schools (FCPS). The system currently consists of 5 high schools, 12 middle schools, one combined middle/high school, and 35 elementary schools, along with six private schools. FCPS is currently building two new elementary schools and one new high school; the elementary schools are scheduled to open in August 2016, with the high school to follow in August 2017. There are also two traditional colleges: the University of Kentucky, which is the state's flagship public university, and Transylvania University, which is the state's oldest four-year university and the first university west of the Alleghenies. Other institutions of higher learning include Bluegrass Community and Technical College, Sullivan University, Spencerian College, Medtech College, Strayer University, Commonwealth Baptist College, and a distance-learning extension of Indiana Wesleyan University. Seven other postsecondary institutions are within the Lexington Combined Statistical Area: Asbury University and Asbury Theological Seminary, separate though related institutions in Wilmore; Georgetown College in Georgetown; Midway University, with its main campus in Midway and an extension campus in Lexington; Eastern Kentucky University in Richmond; Berea College in Berea, and Kentucky State University in Frankfort.
Media.
Lexington's largest daily circulating newspaper is the Lexington Herald-Leader. The region is also served by eight primary television stations, including WLEX, WKYT, WDKY, WTVQ, The CW, WKLE, and MyNetworkTV, and online news agency KyForward.com. Business Lexington is a monthly business newspaper. The Chevy Chaser Magazine and the Southsider Magazine are two community publications. The state's public television network, Kentucky Educational Television (KET), is headquartered in Lexington and is one of the nation's largest public networks.
Transportation.
Highways.
The northeast border of Lexington has direct access to Interstate 64 and Interstate 75, but freeways do not run through downtown or other sections of the city. Instead, Lexington has two beltways: inner New Circle Road (KY 4) and outer Man o' War Boulevard (southern semi-circle), then numerous arterial highways/U.S. routes radiate from downtown to the surrounding suburbs and small towns.
Lexington suffers considerable traffic congestion for a city of its size due to the lack of freeways, the proximity of the University of Kentucky to downtown, and the substantial number of commuters from outlying towns. For traffic relief on northern New Circle Road, Citation Boulevard is planned to connect Leestown Road and Russell Cave Road. Other proposed projects include widening/closing certain roads and changing directions from one-way to two-way.
Air.
Blue Grass Airport is Lexington's primary commercial airport. It provides approximately 13 nonstop flights and a total of 86 flights daily from its two runways. Five major airlines operate connection service at Blue Grass, including Allegiant Air, American Eagle, Delta Air Lines, United Express, and US Airways Express.
On August 27, 2006, Comair Flight 5191 took off from the wrong runway, and crashed in a nearby field, killing 49 of the 50 passengers, leaving the first officer alive but badly burned. The aircraft involved was a 50-seat Bombardier Canadair Regional Jet CRJ-100ER, serial number 7472. On August 27, 2011, a memorial sculpture depicting 47 steel birds flying was unveiled and dedicated to the victims.
The airport is located four miles (six km) west of the city center, at the intersection of US 60 (Versailles Road) and Man o' War Boulevard.
In addition to commercial airline service, the airport is also home to several private (general aviation) operators including TACAir, which serves as the airport's FBO (Fixed Base Operator). Flight training and aircraft maintenance services are also offered. A brand new general aviation runway was dedicated August 4, 2010.
Public transportation.
Lexington is served by Lextran, a public transit bus agency operated by LFUCG and has been in existence since 1972. Lextran (officially the Transit Authority of the Lexington-Fayette Urban County Government) is a public transportation bus system serving Lexington, Kentucky. It runs bus routes throughout the city which mostly all converge in downtown at the Lexington Transit Center located at 220 East Vine. It provides public transportation in the form of buses and lift vans. It operates seven days a week on a total of 24 bus routes from 5:00 a.m. until 12:30 a.m. In addition to mainline and paratransit, Lextran contracts with the University of Kentucky and operates two routes around the campus. It also runs two routes to the Bluegrass Community and Technical College campuses. Even though Lexington and Fayette County are a consolidated government, Lextran does not provide service outside the Lexington city proper due to limited funding sources.
Transportation planning.
The Lexington Area Metropolitan Planning Organization is responsible for transportation planning for Fayette and Jessamine Counties. This includes activities such as carpool matching, administering a commuter vanpool program, air quality forecasting, bicycle and pedestrian planning, congestion management, and developing transportation plans and documents.
Sister cities.
All four are, like Lexington, major centers of the Thoroughbred breeding industry in their respective countries.

</doc>
<doc id="50354" url="https://en.wikipedia.org/wiki?curid=50354" title="Precautionary principle">
Precautionary principle

The precautionary principle (or precautionary approach) to risk management states that if an action or policy has a suspected risk of causing harm to the public, or to the environment, in the absence of scientific consensus (that the action or policy is not harmful), the burden of proof that it is "not" harmful falls on those taking an action that may or may not be a risk.
The principle is used by policy makers to justify discretionary decisions in situations where there is the possibility of harm from making a certain decision (e.g. taking a particular course of action) when extensive scientific knowledge on the matter is lacking. The principle implies that there is a social responsibility to protect the public from exposure to harm, when scientific investigation has found a plausible risk. These protections can be relaxed only if further scientific findings emerge that provide sound evidence that no harm will result.
In some legal systems, as in Law of the European Union, the application of the Precautionary Principle has been made a statutory requirement in some areas of law.
Regarding international conduct, the first endorsement of the principle was in 1982 when the World Charter for Nature was adopted by the United Nations General Assembly, while its first international implementation was in 1987 through the Montreal Protocol. Soon after, the Principle integrated with many other legally binding international treaties such as the Rio Declaration and Kyoto Protocol.
Origins and theory.
The concept "precautionary principle" is generally considered to have arisen in English from a translation of the German term "Vorsorgeprinzip" in the 1980s.
The concepts underpinning the precautionary principle pre-date the term's inception. For example, the essence of the principle is captured in a number of cautionary aphorisms such as "an ounce of prevention is worth a pound of cure", "better safe than sorry", and "look before you leap". The precautionary principle may also be interpreted as the evolution of the "ancient-medical principle" of "first, do no harm" to apply to institutions and institutional decision-making processes rather than individuals.
In economics, the Precautionary Principle has been analysed in terms of "the effect on rational decision-making", of "the interaction of irreversibility" and "uncertainty". Authors such as Epstein (1980) and Arrow and Fischer (1974) show that "irreversibility of possible future consequences" creates a "quasi-option effect" which should induce a "risk-neutral" society to favor current decisions that allow for more flexibility in the future. Gollier et al. (2000) conclude that "more scientific uncertainty as to the distribution of a future risk – that is, a larger variability of beliefs – should induce society to take stronger prevention measures today." 
Formulations.
Many definitions of the precautionary principle exist: Precaution may be defined as "caution in advance," "caution practised in the context of uncertainty," or informed prudence. Two ideas lie at the core of the principle:
One of the primary foundations of the precautionary principle, and globally accepted definitions, results from the work of the Rio Conference, or "Earth Summit" in 1992. Principle #15 of the Rio Declaration notes: "In order to protect the environment, the precautionary approach shall be widely applied by States according to their capabilities. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall not be used as a reason for postponing cost-effective measures to prevent environmental degradation."
The 1998 Wingspread Statement on the Precautionary Principle summarizes the principle this way: "When an activity raises threats of harm to human health or the environment, precautionary measures should be taken even if some cause and effect relationships are not fully established scientifically." The Wingspread Conference on the Precautionary Principle was convened by the Science and Environmental Health Network.
The February 2, 2000, Commission of the European Communities, "Communication from the Commission on the Precautionary Principle", noted that, "The precautionary principle is not defined in the Treaty, which prescribes it Precautionary Principle only once - to protect the environment. But in practice, its scope is much wider, and specifically where preliminary-objective-scientific-evaluation indicates that there are reasonable grounds for concern that potentially dangerous effects on the environment, human, animal or plant health may be inconsistent with the high level of protection [for what chosen for the Community."
The January 29, 2000 Cartagena Protocol on Biosafety says, in regard to controversies over GMOs: "Lack of scientific certainty due to insufficient relevant scientific information . . . shall not prevent the Party of mport, in order to avoid or minimize such potential adverse effects, from taking a decision, as appropriate, with regard to the import of the living modified organism in question."
Application.
The application of the precautionary principle is hampered by both lack of political will, as well as the wide range of interpretations placed on it. One study identified 14 different formulations of the principle in treaties and nontreaty declarations. R.B. Stewart (2002) reduced the precautionary principle to four basic versions:
In deciding how to apply the principle, analysis may use a cost-benefit analysis that factors in both the opportunity cost of not acting, and the option value of waiting for further information before acting. One of the difficulties of the application of the principle in modern policy-making is that there is often an irreducible conflict between different interests, so that the debate necessarily involves politics.
Strong vs. weak.
"Strong precaution" holds that regulation is required whenever there is a possible risk to health, safety, or the environment, even if the supporting evidence is speculative and even if the economic costs of regulation are high. In 1982, the United Nations World Charter for Nature gave the first international recognition to the strong version of the principle, suggesting that when "potential adverse effects are not fully understood, the activities should not proceed." The widely publicized Wingspread Declaration, from a meeting of environmentalists in 1998, is another example of the strong version. Strong precaution can also be termed as a "no-regrets" principle, where costs are not considered in preventative action.
"Weak precaution" holds that lack of scientific evidence does not preclude action if damage would otherwise be serious and irreversible. Humans practice weak precaution every day, and often incur costs, to avoid hazards that are far from certain: we do not walk in moderately dangerous areas at night, we exercise, we buy smoke detectors, we buckle our seatbelts.
According to a publication by the New Zealand Treasury Department,
International agreements and declarations.
The World Charter for Nature, which was adopted by the UN General Assembly in 1982, was the first international endorsement of the precautionary principle. The principle was implemented in an international treaty as early as the 1987 Montreal Protocol, and among other international treaties and declarations is reflected in the 1992 Rio Declaration on Environment and Development (signed at the United Nations Conference on Environment and Development).
"Principle" vs. "approach".
No introduction to the precautionary principle would be complete without brief reference to the difference between the precautionary principle and the precautionary approach. Principle 15 of the Rio Declaration 1992 states that: "in order to protect the environment, the precautionary approach shall be widely applied by States according to their capabilities. Where there are threats of serious or irreversible damage, lack of full scientific certainty shall be not used as a reason for postponing cost-effective measures to prevent environmental degradation." As Garcia (1995) pointed out, "the wording, largely similar to that of the principle, is subtly different in that: (1) it recognizes that there may be differences in local capabilities to apply the approach, and (2) it calls for cost-effectiveness in applying the approach, e.g., taking economic and social costs into account." The 'approach' is generally considered a softening of the 'principle'.
"As Recuerda has noted, the distinction between the 'precautionary principle' and a 'precautionary approach' is diffuse and, in some contexts, controversial. In the negotiations of international declarations, the United States has opposed the use of the term 'principle' because this term has special connotations in legal language, due to the fact that a 'principle of law` is a source of law. This means that it is compulsory, so a court can quash or confirm a decision through the application of the precautionary principle. In this sense, the precautionary principle is not a simple idea or a desideratum but a source of law. This is the legal status of the precautionary principle in the European Union. On the other hand, an 'approach' usually does not have the same meaning, although in some particular cases an approach could be binding. A precautionary approach is a particular 'lens' used to identify risk that every prudent person possesses (Recuerda, 2008)
European Commission.
On 2 February 2000, the European Commission issued a Communication on the precautionary principle, in which it adopted a procedure for the application of this concept, but without giving a detailed definition of it. Paragraph 2 of article 191 of the Lisbon Treaty states that
""Union policy on the environment shall aim at a high level of protection taking into account the diversity of situations in the various regions of the Union. It shall be based on the precautionary principle and on the principles that preventive action should be taken, that environmental damage should as a priority be rectified at source and that the polluter should pay.""
After the adoption of the European Commission's Communication on the precautionary principle, the principle has come to inform much EU policy, including areas beyond environmental policy. As of 2006 it had been integrated into EU laws "in matters such as general product safety, the use of additives for use in animal nutrition, the incineration of waste, and the regulation of genetically modified organisms." Through its application in case law, it has become a "general principle of EU law."
USA.
On July 18, 2005, the City of San Francisco passed a Precautionary Principle Purchasing ordinance, which requires the city to weigh the environmental and health costs of its $600 million in annual purchases – for everything from cleaning supplies to computers. Members of the Bay Area Working Group on the Precautionary Principle including the Breast Cancer Fund, helped bring this to fruition.
Japan.
In 1997, Japan tried to use the consideration of the precautionary principle in a WTO SPS Agreement on the Application of Sanitary and Phytosanitary Measures case, as Japan's requirement to test each variety of agricultural products (apples, cherries, peaches, walnuts, apricots, pears, plums and quinces) for the efficacy of treatment against codling moths was challenged.
This moth is a pest that does not occur in Japan, and whose introduction has the potential to cause serious damage. The United States claimed that it was not necessary to test each variety of a fruit for the efficacy of the treatment, and that this varietal testing requirement was unnecessarily burdensome.
Australia.
The most important Australian court case so far, due to its exceptionally detailed consideration of the precautionary principle, is Telstra Corporation Limited v Hornsby Shire Council needed. The case was heard in the New South Wales, Land and Environment Court under Justice CJ Preston (24 April 2006).
The Principle was summarized by reference to the NSW "Protection of the Environment Administration Act 1991", which itself provides a good definition of the principle:
"If there are threats of serious or irreversible environmental damage, lack of full scientific certainty should not be used as a reasoning for postponing measures to prevent environmental degradation. In the application of the principle… decisions should be guided by:
(i) careful evaluation to avoid, wherever practicable, serious or irreversible damage to the environment; and
(ii) an assessment of risk-weighted consequence of various options".
The most significant points of Justice Preston's decision are the following findings:
Philippines.
A petition filed May 17, 2013 by environmental group Greenpeace Southeast Asia and farmer-scientist coalition Masipag ("Magsasaka at Siyentipiko sa Pagpapaunlad ng Agrikultura") asked the Appellate court to stop the planting of Bt eggplant in test fields, saying the impacts of such an undertaking to the environment, native crops and human health are still unknown. The Court of Appeals granted the petition, citing the precautionary principle stating "when human activities may lead to threats of serious and irreversible damage to the environment that is scientifically plausible but uncertain, actions shall be taken to avoid or diminish the threat." 
Respondents filed a motion for reconsideration in June 2013 and on September 20, 2013 the Court of Appeals chose to uphold their May decision saying the "bt talong" field trials violate the people's constitutional right to a "balanced and healthful ecology." The Supreme Court on December 8, 2015 permanently stopped the field testing for Bt (Bacillus thuringiensis) talong (eggplant), upholding the decision of the Court of Appeals which stopped the field trials for the genetically modified eggplant. The court is the first in the world to adopt the precautionary principle regarding GMO products in its decision.
Corporate.
The Body Shop International, a UK-based cosmetics company, included the precautionary principle in their 2006 Chemicals Strategy.
Environment and health.
Fields typically concerned by the precautionary principle are the possibility of:
The precautionary principle is often applied to biological fields because changes cannot be easily contained and have the potential of being global. The principle has less relevance to contained fields such as aeronautics, where the few people undergoing risk have given informed consent (e.g., a test pilot). In the case of technological innovation, containment of impact tends to be more difficult if that technology can self-replicate. Bill Joy emphasized the dangers of replicating genetic technology, nanotechnology, and robotic technology in his article in "Wired", "Why the future doesn't need us", though he does not specifically cite the precautionary principle. The application of the principle can be seen in the public policy of requiring pharmaceutical companies to carry out clinical trials to show that new medications are safe.
Oxford based philosopher Nick Bostrom discusses the idea of a future powerful superintelligence, and the risks that we/it face should it attempt to gain atomic level control of matter.
Application of the principle modifies the status of innovation and risk assessment: it is not the risk that must be avoided or amended, but a potential risk that must be prevented. Thus, in the case of regulation of scientific research, there is a third party beyond the scientist and the regulator: the consumer.
In an analysis concerning application of the precautionary principle to nanotechnology, Chris Phoenix and Mike Treder posit that there are "two forms" of the principle, which they call the "strict form" and the "active form". The former "requires inaction when action might pose a risk", while the latter means "choosing less risky alternatives when they are available, and [...] taking responsibility for potential risks." Thomas Alured Faunce has argued for stronger application of the precautionary principle by chemical and health technology regulators particularly in relation to Ti02 and ZnO nanoparticles in sunscreens, biocidal nanosilver in waterways and products whose manufacture, handling or recycling exposes humans to the risk of inhaling multi-walled carbon nanotubes.
Resource management.
Several natural resources like fish stocks are now managed by precautionary approach, through Harvest Control Rules (HCR) based upon the precautionary principle. The figure indicates how the principle is implemented in the cod fisheries management proposed by the International Council for the Exploration of the Sea.
In classifying endangered species, the precautionary principle means that if there is doubt about an animal's or plant's exact , the one that would cause the strongest protective measures to be realized should be chosen. Thus, a species like the silvery pigeon that might exist in considerable numbers and simply be under-recorded or might just as probably be long extinct is not classified as "data deficient" or "extinct" (which both do not require any protective action to be taken), but as "critically endangered" (the conservation status that confers the need for the strongest protection), whereas the increasingly rare, but probably not yet endangered emerald starling is classified as "data deficient", because there is urgent need for research to clarify its status rather than for conservation action to save it from extinction.
If, for example, a large ground-water body that many people use for drinking water is contaminated by bacteria (e-coli 0157 H7, campylobacter or leptospirosis) and the source of contamination is strongly suspected to be dairy cows but the exact science is not yet able to provide absolute proof, the cows should be removed from the environment until they are proved, by the dairy industry, not to be the source or until that industry ensures that such contamination will not recur.
Criticisms.
Critics of the principle use arguments similar to those against other formulations of technological conservatism.
Internal inconsistency - applying strong PP risks causing harm.
Strong formulations of the precautionary principle, without regard to its most basic provisions that it is to be applied only where risks are potentially high AND not easily calculable, applied to the principle itself as a policy decision, may rule out its own use. The reason suggested is that preventing innovation from coming to market means that only current technology may be used, and current technology itself may cause harm or leave needs unmet; there is a risk of causing harm by blocking innovation. As Michael Crichton wrote in his novel "State of Fear": "The 'precautionary principle', properly applied, forbids the precautionary principle." For example, forbidding nuclear power plants based on concerns about risk means continuing to rely on power plants that burn fossil fuels, which continue to release greenhouse gases. In another example, the Hazardous Air Pollutant provisions in the 1990 amendments to the U.S. Clean Air Act are an example of the Precautionary Principle where the onus is now on showing a listed compound is harmless. Under this rule no distinction is made between those air Pollutants that provide a higher or lower risk, so operators tend to choose less-examined agents that are not on the existing list.
Blocking innovation and progress generally.
Because applications of strong formulations of the precautionary principle can be used to block innovation, a technology which brings advantages may be banned by precautionary principle because of its potential for negative impacts, leaving the positive benefits unrealized.
The precautionary principle has been ethically questioned on the basis that its application could block progress in developing countries.
Vagueness and plausibility.
The precautionary principle calls for inaction in the face of scientific uncertainty, but some formulations do not specify the minimal threshold of plausibility of risk that acts as a "triggering" condition, so that any indication that a proposed product or activity might harm health or the environment is sufficient to invoke the principle. In "Sancho vs. DOE", Helen Gillmor, Senior District Judge, wrote in a dismissal of Wagner's lawsuit which included a popular worry that the LHC could cause "destruction of the earth" by a black hole:

</doc>
<doc id="50355" url="https://en.wikipedia.org/wiki?curid=50355" title="Informed consent">
Informed consent

Informed consent is a process for getting permission before conducting a healthcare intervention on a person. A health care provider may ask a patient to consent to receive therapy before providing it, or a clinical researcher may ask a research participant before enrolling that person into a clinical trial. Informed consent is collected according to guidelines from the fields of medical ethics and research ethics.
An informed consent can be said to have been given based upon a clear appreciation and understanding of the facts, implications, and consequences of an action. To give informed consent, the individual concerned must have adequate reasoning faculties and be in possession of all relevant facts. Impairments to reasoning and judgment that may prevent informed consent include basic intellectual or emotional immaturity, high levels of stress such as PTSD or a severe intellectual disability, severe mental illness, intoxication, severe sleep deprivation, Alzheimer's disease, or being in a coma.
Some acts can take place because of a lack of informed consent. In cases where an individual is considered unable to give informed consent, another person is generally authorized to give consent on his behalf, e.g., parents or legal guardians of a child (though in this circumstance the child may be required to provide informed assent) and conservators for the mentally ill.
In cases where an individual is provided insufficient information to form a reasoned decision, serious ethical issues arise. Such cases in a clinical trial in medical research are anticipated and prevented by an ethics committee or Institutional Review Board.
Informed Consent Form Templates can be found on the World Health Organization Website for practical use.
Assessment of consent.
Informed consent can be complex to evaluate, because neither expressions of consent, nor expressions of understanding of implications, necessarily mean that full adult consent was in fact given, nor that full comprehension of relevant issues is internally digested. Consent may be implied within the usual subtleties of human communication, rather than explicitly negotiated verbally or in writing. In some cases consent cannot legally be possible, even if the person protests he does indeed understand and wish. There are also structured instruments for evaluating capacity to give informed consent, although no ideal instrument presently exists.
Thus, there is always a degree to which informed consent must be assumed or inferred based upon observation, or knowledge, or legal reliance. This especially is the case in sexual or relational issues. In medical or formal circumstances, explicit agreement by means of signature—normally relied on legally—regardless of actual consent, is the norm. This is the case with certain procedures, such as a "do not resuscitate" directive that a patient signed prior to their illness.
Brief examples of each of the above:
Elements of valid informed consent.
For an individual to give valid informed consent, three components must be present: disclosure, capacity and voluntariness.
Waiver of requirement.
Waiver of the consent requirement may be applied in certain circumstances where no foreseeable harm is expected to result from the study or when permitted by law, federal regulations, or if an ethical review committee has approved the non-disclosure of certain information.
Besides studies with minimal risk, waivers of consent may be obtained in a military setting. According to 10 USC 980, the United States Code for the Armed Forces, Limitations on the Use of Humans as Experimental Subjects, a waiver of advanced informed consent may be granted by the Secretary of Defense if a research project would:
While informed consent is a basic right and should be carried out effectively, if a patient is incapacitated due to injury or illness, it is still important that patients benefit from emergency experimentation. The Food and Drug Administration (FDA) and the Department of Health and Human Services (DHHS) joined together to create federal guidelines to permit emergency research, without informed consent. However, they can only proceed with the research if they obtain a waiver of informed consent (WIC) or an emergency exception from informed consent (EFIC).
History.
"Informed consent" is a technical term first used in a medical malpractice United States court case in 1957. In tracing its history, some scholars have suggested tracing the history of checking for any of these practices:
These practices are part of what constitutes informed consent, and their history is the history of informed consent. They combine to form the modern concept of informed consent—which rose in response to particular incidents in modern research. Whereas various cultures in various places practiced informed consent, the modern concept of informed consent was developed by people who drew influence from Western tradition.
Medical history.
Historians cite a series of medical guidelines to trace the history of informed consent in medical practice.
The Hippocratic Oath, a 500 BC Greek text, was the first set of Western writings giving guidelines for the conduct of medical professionals. It advises that physicians conceal most information from patients to give the patients the best care. The rationale is a beneficence model for care—the doctor knows better than the patient, and therefore should direct the patient's care, because the patient is not likely to have better ideas than the doctor.
Henri de Mondeville , a French surgeon who in the 14th century, wrote about medical practice. He traced his ideas to the Hippocratic Oath. Among his recommendations were that doctors "promise a cure to every patient" in hopes that the good prognosis would inspire a good outcome to treatment. Mondeville never mentioned getting consent, but did emphasize the need for the patient to have confidence in the doctor. He also advised that when deciding therapeutically unimportant details the doctor should meet the patients' requests "so far as they do not interfere with treatment".
Benjamin Rush was an 18th-century United States physician who was influenced by the Age of Enlightenment cultural movement. Because of this, he advised that doctors ought to share as much information as possible with patients. He recommended that doctors educate the public and respect a patient's informed decision to accept therapy. There is no evidence that he supported seeking a consent from patients. In a lecture titled "On the duties of patients to their physicians", he stated that patients should be strictly obedient to the physician's orders; this was representative of much of his writings. John Gregory, Rush's teacher, wrote similar views that a doctor could best practice beneficence by making decisions for the patients without their consent.
Thomas Percival was a British physician who published a book called "Medical Ethics" in 1803. Percival was a student of the works of Gregory and various earlier Hippocratic physicians. Like all previous works, Percival's "Medical Ethics" makes no mention of soliciting for the consent of patients or respecting their decisions. Percival said that patients have a right to truth, but when the physician could provide better treatment by lying or withholding information, he advised that the physician do as he thought best.
When the American Medical Association was founded they in 1847 produced a work called the first edition of the "American Medical Association Code of Medical Ethics". Many sections of this book are verbatim copies of passages from Percival's "Medical Ethics". A new concept in this book was the idea that physicians should fully disclose all patient details truthfully when talking to other physicians, but the text does not also apply this idea to disclosing information to patients. Through this text, Percival's ideas became pervasive guidelines throughout the United States as other texts were derived from them.
Worthington Hooker was an American physician who in 1849 published "Physician and Patient". This medical ethics book was radical demonstrating understanding of the AMA's guidelines and Percival's philosophy and soundly rejecting all directives that a doctor should lie to patients. In Hooker's view, benevolent deception is not fair to the patient, and he lectured widely on this topic. Hooker's ideas were not broadly influential.
Research history.
Historians cite a series of Human subject research experiments to trace the history of informed consent in research.
"Tearoom Trade" is the name of a book by American psychologist Laud Humphreys. In it he describes his research into male homosexual acts. In conducting this research he never sought consent from his research subjects and other researchers raised concerns that he violated the right to privacy for research participants.
The Milgram experiment is the name of a 1961 experiment conducted by American psychologist Stanley Milgram. In the experiment Milgram had an authority figure order research participants to commit a disturbing act of harming another person. After the experiment he would reveal that he had deceived the participants and that they had not hurt anyone, but the research participants were upset at the experience of having participated in the research. The experiment raised broad discussion on the ethics of recruiting participants for research without giving them full information about the nature of the research.
Medical procedures.
The doctrine of informed consent relates to professional negligence and establishes a breach of the duty of care owed to the patient (see duty of care, breach of the duty, and respect for persons). The doctrine of informed consent also has significant implications for medical trials of medications, devices, or procedures.
Requirements of the professional.
In the United Kingdom and countries such as Malaysia and Singapore, informed consent in medical procedures requires proof as to the standard of care to expect as a recognised standard of acceptable professional practice (the Bolam Test), that is, what risks would a medical professional usually disclose in the circumstances (see Loss of right in English law). Arguably, this is "sufficient consent" rather than "informed consent."
Medicine in the United States, Australia, and Canada take a more patient-centeric approach to "informed consent." Informed consent in these jurisdictions requires doctors to disclose significant risks, as well as risks of particular importance to that patient. This approach combines an objective (the reasonable patient) and subjective (this particular patient) approach.
The doctrine of informed consent should be contrasted with the general doctrine of medical consent, which applies to assault or battery. The consent standard here is only that the person understands, in general terms, the nature of and purpose of the intended intervention. As the higher standard of informed consent applies to negligence, not battery, the other elements of negligence must be made out. Significantly, causation must be shown: That had the individual been made aware of the risk he would not have proceeded with the operation (or perhaps with that surgeon).
Optimal establishment of an informed consent requires adaptation to cultural or other individual factors of the patient. For example, people from Mediterranean and Arab appear to rely more on the context of the delivery of the information, with the information being carried more by who is saying it and where, when, and how it's being said, rather than "what" is said, which is of relatively more importance in typical "Western" countries.
The informed consent doctrine is generally implemented through good healthcare practice: pre-operation discussions with patients and the use of medical consent forms in hospitals. However, reliance on a signed form should not undermine the basis of the doctrine in giving the patient an opportunity to weigh and respond to the risk. In one British case, a doctor performing routine surgery on a woman noticed that she had cancerous tissue in her womb. He took the initiative to remove the woman's womb; however, as she had not given informed consent for this operation, the doctor was judged by the General Medical Council to have acted negligently. The council stated that the woman should have been informed of her condition, and allowed to make her own decision.
Obtaining informed consents.
To capture and manage informed consents, hospital management systems typically use paper-based consent forms which are scanned and stored in a document handling system after obtaining the necessary signatures. Hospital systems and research organizations are adopting an electronic way of capturing informed consents to enable indexing, to improve comprehension, search and retrieval of consent data, thus enhancing the ability to honor to patient intent and identify willing research participants. More recently, Health Sciences South Carolina, a statewide research collaborative focused on transforming healthcare quality, health information systems and patient outcomes, developed an open-source system called Research Permissions Management System (RPMS).RPMS has been released as an open-source application.
Competency of the patient.
The ability to give informed consent is governed by a general requirement of competency. In common law jurisdictions, adults are presumed competent to consent. This presumption can be rebutted, for instance, in circumstances of mental illness or other incompetence. This may be prescribed in legislation or based on a common-law standard of inability to understand the nature of the procedure. In cases of incompetent adults, a health care proxy makes medical decisions. In the absence of a proxy, the medical practitioner is expected to act in the patient's best interests until a proxy can be found.
By contrast, 'minors' (which may be defined differently in different jurisdictions) are generally presumed incompetent to consent, but depending on their age and other factors may be required to provide Informed assent. In some jurisdictions (e.g. much of the U.S.), this is a strict standard. In other jurisdictions (e.g. England, Australia, Canada), this presumption may be rebutted through proof that the minor is ‘mature’ (the ‘Gillick standard’). In cases of incompetent minors, informed consent is usually required from the parent (rather than the 'best interests standard') although a parens patriae order may apply, allowing the court to dispense with parental consent in cases of refusal.
Deception.
Research involving deception is controversial given the requirement for informed consent. Deception typically arises in social psychology, when researching a particular psychological process requires that investigators deceive subjects. For example, in the Milgram experiment, researchers wanted to determine the willingness of participants to obey authority figures despite their personal conscientious objections. They had authority figures demand that participants deliver what they thought was an electric shock to another researcher. For the study to succeed, it was necessary to deceive the participants so they believed that the subject was a peer and that their electric shocks caused the peer actual pain.
Nonetheless, research involving deception prevents the subject/patient from exercising his/her basic right of autonomous informed decision-making and conflicts with the ethical principle of Respect for persons.
The Ethical Principles of Psychologists and Code of Conduct set by the American Psychological Association says that psychologists may not conduct research that includes a deceptive compartment unless they can justify the act by the value and importance of the study's results, and show they couldn't obtain the results by some other way. Moreover, the research should bear no potential harm to the subject as an outcome of deception, be it physical pain or emotional distress. Finally, the code requires a debriefing session, in which the experimenter tells the subject about the deception, and gives subjects the option of withdrawing their data.
Abortion.
In some U.S. States, informed consent laws (sometimes called "right to know" laws) require that a woman seeking an elective abortion receive factual information from the abortion provider about her legal rights, alternatives to abortion (such as adoption), available public and private assistance, and "medical facts" (some of which are disputed—see fetal pain), before the abortion is performed (usually 24 hours in advance of the abortion). Other countries with such laws (e.g. Germany) require that the information giver be properly certified to make sure that no abortion is carried out for the financial gain of the abortion provider and to ensure that the decision to have an abortion is not swayed by any form of incentive.
Some informed consent laws have been criticized for allegedly using "loaded language in an apparently deliberate attempt to 'personify' the fetus," but those critics acknowledge that abortion information provided pursuant to informed consent laws "most of the information in the materials about abortion comports with recent scientific findings and the principles of informed consent, some content is either misleading or altogether incorrect."
Children.
As children often lack the decision making ability or legal power (competence) to provide true informed consent for medical decisions, it often falls on parents or legal guardians to provide "informed permission" for medical decisions. This "consent by proxy" usually works reasonably well, but can lead to ethical dilemmas when the judgment of the parents or guardians and the medical professional differ with regard to what constitutes appropriate decisions "in the best interest of the child". Children who are legally emancipated, and certain situations such as decisions regarding sexually transmitted diseases or pregnancy, or for unemancipated minors who are deemed to have medical decision making capacity, may be able to provide consent without the need for parental permission depending on the laws of the jurisdiction the child lives in. The American Academy of Pediatrics encourages medical professionals also to seek the assent of older children and adolescents by providing age appropriate information to these children to help empower them in the decision making process.
Research on children has benefited society in many ways. The only effective way to establish normal patterns of growth and metabolism is to do research on infants and young children. When addressing the issue of informed consent with children, the primary response is parental consent. This is valid, although only legal guardians are able to consent for a child, not adult siblings. Additionally, parents may not order the termination of a treatment that is required to keep a child alive, even if they feel it is in the best interest. Guardians are typically involved in the consent of children, however a number of doctrines have developed that allow children to receive health treatments without parental consent. For example, emancipated minors may consent to medical treatment, and minors can also consent in an emergency.
No-victim laws.
It may not be legally possible to give consent to certain activities in certain jurisdictions. See the Operation Spanner case for an example of this in the UK that involved sadomasochistic activities such as branding. There are currently several legal challenges underway to address these issues of legality in several nations.
Research.
In medical research, the Nuremberg Code set a base international standard in 1947, which continued to develop, for example in response to the ethical violation in the Holocaust. Nowadays, medical research is overseen by an ethics committee that also oversees the informed consent process.
As the medical guidelines established in the Nuremberg Code were imported into the ethical guidelines for the social sciences, informed consent became a common part of the research procedure. However, while informed consent is the default in medical settings, it is not always required in the social science. Here, research often involves low or no risk for participants, unlike in many medical experiments. Second, the mere knowledge that they participate in a study can cause people to alter their behavior, as in the Hawthorne Effect: "In the typical lab experiment, subjects enter an environment in which they are keenly aware that their behavior is being monitored, recorded, and subsequently scrutinized."
In such cases, seeking informed consent directly interferes with the ability to conduct the research, because the very act of revealing that a study is being conducted is likely to alter the behavior studied. List exemplifies the potential dilemma that can result: "if one were interested in exploring whether, and to what extent, race or gender influences the prices that buyers pay for used cars, it would be difficult to measure accurately the degree of discrimination among used car dealers who know that they are taking part in an experiment." In cases where such interference is likely, and after careful consideration, a researcher may forgo the informed consent process. This is commonly done after weighting the risk to study participants versus the benefit to society and whether participants are present in the study out of their own wish and treated fairly. Researchers often consult with an Ethics Committee or institutional review board to render a decision.
The birth of new online media, such as social media, has complicated the idea of informed consent. In an online environment people pay little attention to Terms of Use agreements and can subject themselves to research without thorough knowledge. This issue came to the public light following a study conducted by Facebook in 2014 and published by Facebook and Cornell University. Facebook conducted a study where they altered the Facebook News Feeds of roughly 700,000 users to reduce either the amount of positive or negative posts they saw for a week. The study then analyzed if the users status updates changed during the different conditions. The study was published in the Proceedings of the National Academy of Sciences.
The lack of informed consent led to outrage among many researchers and users. Many believed that by potentially altering the mood of users by altering what posts they see, Facebook put at-risk individuals at higher dangers for depression and suicide. However, supports of Facebook claim that Facebook details that they have the right to use information for research in their terms of use. Others say the experiment is just a part of Facebook’s current work, which alters News Feeds algorithms continually to keep people interested and coming back to the site. Others pointed out that this specific study is not along but that news organizations constantly try out different headlines using algorithms to elicit emotions and garner clicks or Facebook shares. They say this Facebook study is no different than things people already accept. Still, others say that Facebook broke the law when conducting the experiment on user that didn’t give informed consent.
The Facebook study controversy raises numerous questions about informed consent and the differences in the ethical review process between publicly and privately funded research. Some say Facebook was within its limits and others see the need for more informed consent and/or the establishment of in-house private review boards.
Conflicts of Interest.
Other, long-standing controversies underscore the role for conflicts of interest among medical school faculty and researchers. For example, coverage of University of California (UC) medical school faculty members has included news of ongoing corporate payments to researchers and practitioners from companies that market and produce the very devices and treatments they recommend to patients. Robert Pedowitz, the former chairman of UCLA’s orthopedic surgery department, reported concern that his colleague’s financial conflicts of interest could negatively affect patient care or research into new treatments. In a subsequent lawsuit about whistleblower retaliation, the University provided a $10 million settlement to Pedowitz while acknowledging no wrongdoing. Consumer Watchdog, an oversight group, observed that University of CA policies were “either inadequate or unenforced…Patients in UC hospitals deserve the most reliable surgical devices and medication…and they shouldn’t be treated as subjects in expensive experiments.” Other UC incidences range from taking the eggs of women for implantation into other women without consent, to the illegal sale of donated cadavers, to the injection of live bacteria into human brains resulting in potentially premature deaths.

</doc>
<doc id="50356" url="https://en.wikipedia.org/wiki?curid=50356" title="975">
975

__NOTOC__
Year 975 (CMLXXV) was a common year starting on Friday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="50357" url="https://en.wikipedia.org/wiki?curid=50357" title="Larva">
Larva

A larva (plural "larvae" ) is a distinct juvenile form many animals undergo before metamorphosis into adults. Animals with indirect development such as insects, amphibians, or cnidarians typically have a larval phase of their life cycle.
The larva's appearance is generally very different from the adult form ("e.g." caterpillars and butterflies). A larva often has unique structures and organs that do not occur in the adult form. Their diet may also be considerably different.
Larvae are frequently adapted to environments separate from adults. For example, some larvae such as tadpoles live almost exclusively in aquatic environments, but can live outside water as adult frogs. By living in a distinct environment, larvae may be given shelter from predators and reduce competition for resources with the adult population.
Animals in the larval stage will consume food to fuel their transition into the adult form. In some species like barnacles, adults are immobile but their larvae are mobile, and use their mobile larval form to distribute themselves.
Some larvae are dependent on adults to feed them. In many eusocial Hymenoptera species, the larvae are fed by female workers. In "Ropalidia marginata" (a paper wasp) the males are also capable of feeding larvae but they are much less efficient, spending more time and getting less food to the larvae.
The larvae of some species (for example, some newts) can become pubescent and do not develop further into the adult form. This is a type of neoteny.
It is a misunderstanding that the larval form always reflects the group's evolutionary history. This could be the case, but often the larval stage has evolved secondarily, as in insects. In these cases the larval form may differ more than the adult form from the group's common origin.
Insect Larva.
Within Insects, only Endopterygotes show different types of larvae. Several classifications have been suggested by many entomologists, and following classification is based on Antonio Berlese classification in 1913. There are four main types of endopterygote larvae types:

</doc>
<doc id="50363" url="https://en.wikipedia.org/wiki?curid=50363" title="Indigo dye">
Indigo dye

Indigo dye is an organic compound with a distinctive blue color (see indigo). Historically, indigo was a natural dye extracted from plants, and this process was important economically because blue dyes were once rare. A large percentage of indigo dye produced today – several thousand tons each year – is synthetic. It is the blue often associated with blue jeans.
Uses.
The primary use for indigo is as a dye for cotton yarn, which is mainly for the production of denim cloth for blue jeans. On average, a pair of blue jean trousers requires 3–12 g of indigo. Small amounts are used for dyeing wool and silk.
Indigo carmine, or indigo, is an indigo derivative which is also used as a colorant. About 20 million kg are produced annually, again mainly for blue jeans. It is also used as a food colorant, and is listed in the United States as FD&C Blue No. 2.
Natural indigoes.
Plant sources.
A variety of plants have provided indigo throughout history, but most natural indigo was obtained from those in the genus "Indigofera", which are native to the tropics. The primary commercial indigo species in Asia was true indigo ("Indigofera tinctoria", also known as "I. sumatrana"). A common alternative used in the relatively colder subtropical locations such as Japan's Ryukyu Islands and Taiwan is "Strobilanthes cusia". In Central and South America, the two species grown are "I. suffruticosa" ("añil") and dyer's knotweed ("Polygonum tinctorum"), although the "Indigofera" species yield more dye.
Extraction.
The precursor to indigo is indican, a colorless, water-soluble derivative of the amino acid tryptophan. Indican readily hydrolyzes to release β-D-glucose and indoxyl. Oxidation by exposure to air converts indoxyl to indigo. Indican was obtained from the processing of the plant's leaves, which contain as much as 0.2–0.8% of this compound. The leaves were soaked in water and fermented to convert the glycoside indican present in the plant to the blue dye indigotin. The precipitate from the fermented leaf solution was mixed with a strong base such as lye, pressed into cakes, dried, and powdered. The powder was then mixed with various other substances to produce different shades of blue and purple.
History of natural indigo.
Indigo was used in India, which was also the earliest major center for its production and processing. The "I. tinctoria" species was domesticated in India. Indigo, used as a dye, made its way to the Greeks and the Romans, where it was valued as a luxury product.
Indigo is among the oldest dyes to be used for textile dyeing and printing. Many Asian countries, such as India, China, Japan, and Southeast Asian nations have used indigo as a dye (particularly silk dye) for centuries. The dye was also known to ancient civilizations in Mesopotamia, Egypt, Britain, Mesoamerica, Peru, Iran, and Africa.
India is believed to be the oldest center of indigo dyeing in the Old World. It was a primary supplier of indigo to Europe as early as the Greco-Roman era. The association of India with indigo is reflected in the Greek word for the dye, "indikón" (ινδικόν, Indian). The Romans latinized the term to "indicum", which passed into Italian dialect and eventually into English as the word indigo.
In Mesopotamia, a neo-Babylonian cuneiform tablet of the seventh century BC gives a recipe for the dyeing of wool, where lapis-colored wool ("uqnatu") is produced by repeated immersion and airing of the cloth. Indigo was most probably imported from India. The Romans used indigo as a pigment for painting and for medicinal and cosmetic purposes. It was a luxury item imported to the Mediterranean from India by Arab merchants.
Indigo remained a rare commodity in Europe throughout the Middle Ages. A chemically identical dye derived from the woad plant "(Isatis tinctoria)", was used instead. In the late 15th century, the Portuguese explorer Vasco da Gama discovered a sea route to India. This led to the establishment of direct trade with India, the Spice Islands, China, and Japan. Importers could now avoid the heavy duties imposed by Persian, Levantine, and Greek middlemen and the lengthy and dangerous land routes which had previously been used. Consequently, the importation and use of indigo in Europe rose significantly. Much European indigo from Asia arrived through ports in Portugal, the Netherlands, and England. Spain imported the dye from its colonies in South America. Many indigo plantations were established by European powers in tropical climates; it was a major crop in Jamaica and South Carolina, with much or all of the labor performed by enslaved Africans and African Americans. Indigo plantations also thrived in the Virgin Islands. However, France and Germany outlawed imported indigo in the 16th century to protect the local woad dye industry.
Indigo was the foundation of centuries-old textile traditions throughout West Africa. From the Tuareg nomads of the Sahara to Cameroon, clothes dyed with indigo signified wealth. Women dyed the cloth in most areas, with the Yoruba of Nigeria and the Mandinka of Mali particularly well known for their expertise. Among the Hausa male dyers, working at communal dye pits was the basis of the wealth of the ancient city of Kano, and they can still be seen plying their trade today at the same pits.
In Japan, indigo became especially important in the Edo period, when it was forbidden to use silk, so the Japanese began to import and plant cotton. It was difficult to dye the cotton fiber except with indigo. Even today indigo is very much appreciated as a color for the summer Kimono Yukata, as this traditional clothing recalls Nature and the blue sea.
Newton used "indigo" to describe one of the two new primary colors he added to the five he had originally named, in his revised account of the rainbow in "Lectiones Opticae" of 1675.
In North America indigo was introduced into colonial South Carolina by Eliza Lucas Pinckney, where it became the colony's second-most important cash crop (after rice). As a major export crop, indigo supported plantation slavery there. When Benjamin Franklin sailed to France in November 1776 to enlist France's support for the American Revolutionary War, 35 barrels of indigo were on board the "Reprisal", the sale of which would help fund the war effort. In colonial North America, three commercially important species are found: the native "I. caroliniana", and the introduced "I. tinctoria" and "I. suffruticosa".
Because of its high value as a trading commodity, indigo was often referred to as blue gold.
Peasants in Bengal revolted against unfair treatment by the East India Company traders/planters in what became known as the Indigo revolt in 1859, during the British Raj of India. In literature, the play "Nil Darpan" by Dinabandhu Mitra is based on the slavery and forced cultivation of indigo in India.
The demand for indigo in the 19th century is indicated by the fact that in 1897, were dedicated to the cultivation of indican-producing plants, mainly in India. By comparison, the country of Luxembourg is .
Era of synthetic indigo.
In 1897, 19,000 tons of indigo were produced from plant sources. Largely due to advances in organic chemistry, production by natural sources dropped to 1,000 tons by 1914 and continued to contract. These advances can be traced to 1865 when the German chemist Adolf von Baeyer began working on the synthesis of indigo. He described his first synthesis of indigo in 1878 (from isatin) and a second synthesis in 1880 (from 2-nitrobenzaldehyde). (It was not until 1883 that Baeyer finally determined the structure of indigo.) The synthesis of indigo remained impractical, so the search for alternative starting materials at BASF and Hoechst continued. Johannes Pfleger and Karl Heumann eventually came up with industrial mass production synthesis. The synthesis of N-(2-carboxyphenyl)glycine from the easy to obtain aniline provided a new and economically attractive route. BASF developed a commercially feasible manufacturing process that was in use by 1897. In 2002, 17,000 tons of synthetic indigo were produced worldwide.
Developments in dyeing technology.
Indigo white.
Indigo is a challenging dye because it is not soluble in water. To be dissolved, it must undergo a chemical change (reduction). Reduction converts indigo into "white indigo" (leuco-indigo). When a submerged fabric is removed from the dyebath, the white indigo quickly combines with oxygen in the air and reverts to the insoluble, intensely colored indigo. When it first became widely available in Europe in the 16th century, European dyers and printers struggled with indigo because of this distinctive property. It also required several chemical manipulations, some involving toxic materials, and had many opportunities to injure workers. In the 19th century, English poet William Wordsworth referred to the plight of indigo dye workers of his hometown of Cockermouth in his autobiographical poem "The Prelude". Speaking of their dire working conditions and the empathy that he feels for them, he wrote,
"Doubtless, I should have then made common cause"
"With some who perished; haply perished too"
"A poor mistaken and bewildered offering"
"Unknown to those bare souls of miller blue"
A preindustrial process for production of indigo white, used in Europe, was to dissolve the indigo in stale urine. A more convenient reductive agent is zinc. Another preindustrial method, used in Japan, was to dissolve the indigo in a heated vat in which a culture of thermophilic, anaerobic bacteria was maintained. Some species of such bacteria generate hydrogen as a metabolic product, which convert insoluble indigo into soluble indigo white. Cloth dyed in such a vat was decorated with the techniques of "shibori" (tie-dye), "kasuri, katazome", and "tsutsugaki". Examples of clothing and banners dyed with these techniques can be seen in the works of Hokusai and other artists.
Direct printing.
Two different methods for the direct application of indigo were developed in England in the 18th century and remained in use well into the 19th century. The first method, known as 'pencil blue' because it was most often applied by pencil or brush, could be used to achieve dark hues. Arsenic trisulfide and a thickener were added to the indigo vat. The arsenic compound delayed the oxidation of the indigo long enough to paint the dye onto fabrics.
The second method was known as 'China blue' due to its resemblance to Chinese blue-and-white porcelain. Instead of using an indigo solution directly, the process involved printing the insoluble form of indigo onto the fabric. The indigo was then reduced in a sequence of baths of iron(II) sulfate, with air-oxidation between each immersion. The China blue process could make sharp designs, but it could not produce the dark hues possible with the pencil blue method.
Around 1880, the 'glucose process' was developed. It finally enabled the direct printing of indigo onto fabric and could produce inexpensive dark indigo prints unattainable with the China blue method.
Since 2004, freeze-dried indigo, or instant indigo, has become available. In this method, the indigo has already been reduced, and then freeze-dried into a crystal. The crystals are added to warm water to create the dye pot. As in a standard indigo dye pot, care has to be taken to avoid mixing in oxygen. Freeze-dried indigo is simple to use, and the crystals can be stored indefinitely as long as they are not exposed to moisture.
Chemical properties.
Indigo is a dark blue crystalline powder that sublimes at . It is insoluble in water, alcohol, or ether, but soluble in DMSO, chloroform, nitrobenzene, and concentrated sulfuric acid. The chemical formula of indigo is C16H10N2O2.
The molecule absorbs light in the orange part of the spectrum (λmax = 613 nm). The compound owes its deep color to the conjugation of the double bonds, i.e. the double bonds within the molecule are adjacent and the molecule is planar. In indigo white, the conjugation is interrupted because the molecule is nonplanar.
Chemical synthesis.
Given its economic importance, indigo has been prepared by many methods. The Baeyer-Drewson indigo synthesis dates back to 1882. It involves an aldol condensation of o-nitrobenzaldehyde with acetone, followed by cyclization and oxidative dimerization to indigo. This route is highly useful for obtaining indigo and many of its derivatives on the laboratory scale, but was impractical for industrial-scale synthesis. Johannes Pfleger and Karl Heumann (de) eventually came up with industrial mass production synthesis. The first commercially practical route is credited to Pfleger in 1901. In this process, "N"-phenylglycine is treated with a molten mixture of sodium hydroxide, potassium hydroxide, and sodamide. This highly sensitive melt produces indoxyl, which is subsequently oxidized in air to form indigo. Variations of this method are still in use today. An alternative and also viable route to indigo is credited to Heumann in 1897. It involves heating "N"-(2-carboxyphenyl)glycine to in an inert atmosphere with sodium hydroxide. The process is easier than the Pfleger method, but the precursors are more expensive. Indoxyl-2-carboxylic acid is generated. This material readily decarboxylates to give indoxyl, which oxidizes in air to form indigo. The preparation of indigo dye is practiced in college laboratory classes according to the original Baeyer-Drewsen route.
Indigo derivatives.
The benzene rings in indigo can be modified to give a variety of related dyestuffs. Thioindigo, where the two NH groups are replaced by S atoms, is deep red. Tyrian purple is a dull purple dye that is secreted by a common Mediterranean snail. It was highly prized in antiquity. In 1909, its structure was shown to be 6,6'-dibromoindigo. It has never been produced on a commercial basis. The related Ciba blue (5,7,5′,7′-tetrabromoindigo) is, however, of commercial value. Indigo and its derivatives featuring intra- and intermolecular hydrogen bonding have very low solubility in organic solvents. They can be made soluble using transient protecting groups such as the tBOC group, which suppresses intermolecular bonding. Heating of the tBOC indigo results in efficient thermal deprotection and regeneration of the parent H-bonded pigment.
Treatment with sulfuric acid converts indigo into a blue-green derivative called indigo carmine (sulfonated indigo). It became available in the mid-18th century. It is used as a colorant for food, pharmaceuticals, and cosmetics.
Indigo as an organic semiconductor.
Indigo and some of its derivatives are known to be ambipolar organic semiconductors when deposited as thin films by vacuum evaporation.
Safety and the environment.
Indigo has a low oral toxicity, with an of 5000 mg/kg in mammals. In 2009, large spills of blue dyes had been reported downstream of a blue jeans manufacturer in Lesotho.

</doc>
<doc id="50368" url="https://en.wikipedia.org/wiki?curid=50368" title="Lambert Simnel">
Lambert Simnel

Lambert Simnel (ca. 1477 – ca. 1525) was a pretender to the throne of England. His claim to be the Earl of Warwick in 1487 threatened the newly established reign of King Henry VII (reigned 1485–1509). Simnel became the figurehead of a Yorkist rebellion organised by John de la Pole, Earl of Lincoln. The rebellion was crushed in 1487. Simnel was pardoned, and was thereafter employed in the Royal kitchens as a servant. 
Early life.
Simnel was born around 1477. His real name is not known – contemporary records call him John, not Lambert, and even his surname is suspect. Different sources have different claims of his parentage, from a baker and tradesman to organ builder. Most definitely, he was of humble origin. At the age of about ten, he was taken as a pupil by an Oxford-trained priest named Richard Simon (or Richard Symonds / Richard Simons / William Symonds) who apparently decided to become a kingmaker. He tutored the boy in courtly manners and contemporaries described the boy as handsome. He was taught the necessary etiquette and was well educated by Simon. One contemporary described him as "a boy so learned, that, had he ruled, he would have as a learned man."
Pretender.
Simon noticed a striking resemblance between Lambert and the sons of Edward IV, so he initially intended to present Simnel as Richard, Duke of York, son of King Edward IV, the younger of the vanished Princes in the Tower. However, when he heard rumours that the Earl of Warwick had died during his imprisonment in the Tower of London, he changed his mind. The real Warwick was a boy of about the same age and had a claim to the throne as the son of the Duke of Clarence, King Edward IV's brother.
According to James A. Williamson, Simnel was merely a figurehead for a rebellion that was already being planned by the Yorkists:
He was merely a commonplace tool to be used for important ends, and the attempt to overthrow Henry VII would have taken place had Simnel never existed. The Yorkist leaders were determined on a serious push, rising of their party in England supported by as great a force as possible from overseas. 
Simon spread a rumour that Warwick had actually escaped from the Tower and was under his guardianship. He gained some support from Yorkists. He took Simnel to Ireland where there was still support for the Yorkist cause, and presented him to the head of the Irish government, the Earl of Kildare. Kildare was willing to support the story and invade England to overthrow King Henry. Simnel was paraded through the streets, carried on the shoulders of "the tallest man of the time", an individual called D'Arcy of Platten. On 24 May 1487, Simnel was crowned in Christ Church Cathedral in Dublin as "King Edward VI". He was about ten years old. Lord Kildare collected an army of Irish soldiers under the command of his younger brother, Thomas FitzGerald of Laccagh.
The Earl of Lincoln, formerly the designated successor of the late King Richard III, joined the conspiracy against Henry VII. He fled to Burgundy, where Warwick's aunt Margaret of York, the Dowager Duchess of Burgundy, kept her court. Lincoln claimed that he had taken part in young Warwick's supposed escape. He also met Viscount Lovell, who had supported a failed Yorkist uprising in 1486. Margaret collected 2,000 Flemish mercenaries and shipped them to Ireland under the command of Martin Schwartz, a noted military leader of the time. They arrived in Ireland on 5 May. King Henry was informed of this and began to gather troops.
Simnel's army — mainly Flemish and Irish troops — landed on Piel Island in the Furness area of Lancashire on 5 June 1487 and were joined by some English supporters. However, most local nobles, with the exception of Sir Thomas Broughton, did not join them. They clashed with the King's army on 16 June at the Battle of Stoke Field and were defeated. Lincoln, Thomas FitzGerald and Sir Thomas Broughton were killed. Lovell went missing; there were rumours that he had escaped and hidden to avoid retribution. Simons avoided execution due to his priestly status, but was imprisoned for life. Kildare, who had remained in Ireland, was pardoned.
Later life.
King Henry pardoned young Simnel (probably because he had mostly been a puppet in the hands of adults) and gave him a job in the royal kitchen as a spit-turner. When he grew older, he became a falconer. Almost no information about his later life is known. He died some time between 1525 and 1535. He seems to have married, as he is probably the father of Richard Simnel, a canon of St Osyth's Priory in Essex during the reign of Henry VIII.
Cultural depictions.
In the 1972 BBC serial "The Shadow of the Tower" Simnel was portrayed by Gary Warren.

</doc>
<doc id="50371" url="https://en.wikipedia.org/wiki?curid=50371" title="Photios I of Constantinople">
Photios I of Constantinople

Photios I ( "Phōtios"; ), also spelled Photius () or Fotios, was the Ecumenical Patriarch of Constantinople from 858 to 867 and from 877 to 886; He is recognized in the Eastern Orthodox Church as St. Photios the Great.
Photios is widely regarded as the most powerful and influential Patriarch of Constantinople since John Chrysostom, and as the most important intellectual of his time, "the leading light of the ninth-century renaissance". He was a central figure in both the conversion of the Slavs to Christianity and the Photian schism, and is considered "he great systematic compiler of the Eastern Church, who occupies a similar position to that of Gratian in the West", and whose "collection in two parts...formed and still forms the classic source of ancient Church Law for the Greek Church."
Photios was a well-educated man from a noble Constantinopolitan family. Photius's great uncle was a previous Patriarch of Constantinople, Tarasius. He intended to be a monk, but chose to be a scholar and statesman instead. In 858, Emperor Michael III (r. 842–867) deposed Patriarch Ignatius of Constantinople, and Photios, still a layman, was appointed in his place. Amid power struggles between the pope and the Byzantine emperor, Ignatius was reinstated. Photios resumed the position when Ignatius died (877), by order of the Byzantine emperor. The new pope, John VIII, approved Photios's reinstatement. Catholics regard a Fourth Council of Constantinople (Roman Catholic) as anathematizing Photios as legitimate. Eastern Orthodox regard a second council named the Fourth Council of Constantinople (Eastern Orthodox), reversing the first, as legitimate. The contested Ecumenical Councils mark the end of unity represented by the first seven Ecumenical Councils.
Biography.
Secular life.
Most of the primary sources treating Photios's life are written by persons hostile to him. Modern scholars are thus cautious when assessing the accuracy of the information these sources provide. Little is known of Photios's origin and early years. It is known that he was born into a notable family and that his uncle Tarasios had been the Patriarch of Constantinople from 784–806 under both Empress Irene (r. 797–802) and Emperor Nikephoros I (r. 802–811). During the second Iconoclasm, his family suffered persecution since his father, Sergios, was a prominent iconophile. Sergios's family returned to favor only after the restoration of the icons in 842. Certain scholars assert that Photios was, at least in part, of Armenian descent while other scholars merely refer to him as a "Greek Byzantine". Byzantine writers also report that Emperor Michael III (r. 842–867) once angrily called Photios "Khazar-faced", but whether this was a generic insult or a reference to his ethnicity is unclear.
Although Photios had an excellent education, we have no information about how he received this education. The famous library he possessed attests to his enormous erudition (theology, history, grammar, philosophy, law, the natural sciences, and medicine). Most scholars believe that he never taught at Magnaura or at any other university; Vasileios N. Tatakes asserts that, even while he was patriarch, Photios taught "young students passionately eager for knowledge" at his home, which "was a center of learning".
Photios says that, when he was young, he had an inclination for the monastic life, but instead he started a secular career. The way to public life was probably opened for him by (according to one account) the marriage of his brother Sergios to Irene, a sister of the Empress Theodora, who upon the death of her husband Emperor Theophilos (r. 829–842) in 842, had assumed the regency of the Byzantine Empire. Photios became a captain of the guard ("prōtospatharios") and subsequently chief imperial secretary ("protasēkrētis"). At an uncertain date, Photios participated in an embassy to the Abbasids of Baghdad.
Patriarch of Constantinople.
Photios's ecclesiastical career took off spectacularly after "Caesar" Bardas and his nephew, the youthful Emperor Michael, put an end to the administration of the regent Theodora and the logothete of the drome Theoktistos in 856. In 858, Bardas found himself opposed by the then Patriarch Ignatios, who refused to admit him into Hagia Sophia, since it was believed that he was having an affair with his widowed daughter-in-law. In response, Bardas and Michael engineered Ignatios's deposition and confinement on the charge of treason, thus leaving the patriarchal throne empty. The throne was soon filled with a kinsman of Bardas, Photios himself; he was tonsured on December 20, 858, and on the four following days he was successively ordained lector, sub-deacon, deacon and priest. He was consecrated as Patriarch of Constantinople on Christmas Day.
The deposition of Ignatios and the sudden promotion of Photios caused scandal and ecclesiastical division on an ecumenical scale as the Pope and the rest of the western bishops took up the cause of Ignatios. The latter's deposition without a formal ecclesiastical trial meant that Photios's election was uncanonical, and eventually Pope Nicholas I sought to involve himself in determining the legitimacy of the succession. His legates were dispatched to Constantinople with instructions to investigate, but finding Photios well ensconced, they acquiesced in the confirmation of his election at a synod in 861. On their return to Rome, they discovered that this was not at all what Nicholas had intended, and in 863 at a synod in Rome the pope deposed Photios, and reappointed Ignatius as the rightful patriarch, triggering a schism. Four years later, Photios was to respond on his own part by calling a Council and excommunicating the pope on grounds of heresy – over the question of the double procession of the Holy Spirit. The situation was additionally complicated by the question of papal authority over the entire Church and by disputed jurisdiction over newly converted Bulgaria.
This state of affairs changed with the murder of Photios's patron Bardas in 866 and of Emperor Michael III in 867, by his colleague Basil the Macedonian, who now usurped the throne. Photios was deposed as patriarch, not so much because he was a protégé of Bardas and Michael, but because Basil I was seeking an alliance with the Pope and the western emperor. Photios was removed from his office and banished about the end of September 867, and Ignatios was reinstated on November 23. Photios was condemned by the Council of 869–870, thus putting an end to the schism. During his second patriarchate, however, Ignatios followed a policy not very different from that of Photios.
Not long after his condemnation, Photios had reingratiated himself with Basil, and became tutor to the Byzantine emperor's children. From surviving letters of Photios written during his exile at the Skepi monastery, it appears that the ex-patriarch brought pressure to bear on the Byzantine emperor to restore him. Ignatios's biographer argues that Photios forged a document relating to the genealogy and rule of Basil's family, and had it placed in the imperial library where a friend of his was a librarian. According to this document, the Byzantine emperor's ancestors were not mere peasants as everyone believed but descendants of the Arsacid Dynasty of Armenia. True or not, this story does reveal Basil's dependence on Photios for literary and ideological matters. Following Photios's recall, Ignatios and the ex-patriarch met, and publicly expressed their reconciliation. When Ignatios died on October 23, 877, it was a matter of course that his old opponent replaced him on the patriarchal throne three days later. Shaun Tougher asserts that from this point on Basil no longer simply depended on Photios, but in fact he was dominated by him.
Photios now obtained the formal recognition of the Christian world in a council convened at Constantinople in November 879. The legates of Pope John VIII attended, prepared to acknowledge Photios as legitimate patriarch, a concession for which the pope was much censured by Latin opinion. The patriarch stood firm on the main points contested between the Eastern and Western Churches, the demanded apology to the Pope, the ecclesiastical jurisdiction over Bulgaria, and the addition of the "filioque" to the Nicene creed by the Western church. Eventually, Photios refused to apologize or accept the "filioque", and the papal legates made do with his return of Bulgaria to Rome. This concession, however, was purely nominal, as Bulgaria's return to the Byzantine rite in 870 had already secured for it an autocephalous church. Without the consent of Boris I of Bulgaria (r. 852–889), the papacy was unable to enforce its claims.
During the altercations between Emperor Basil I and his heir Leo VI, Photios took the side of the Byzantine emperor. In 883, Basil accused Leo of conspiracy and confined the prince to the palace; he would have even have Leo blinded had he not been dissuaded by Photios and Stylianos Zaoutzes, the father of Zoe Zaoutzaina, Leo's mistress. In 886, Basil discovered and punished a conspiracy by the domestic of the "Hikanatoi" John Kourkouas the Elder and many other officials. In this conspiracy, Leo was not implicated, but Photios was possibly one of the conspirators against Basil's authority.
Basil died in 886 injured while hunting, according to the official story. Warren T. Treadgold believes that this time the evidence points to a plot on behalf of Leo VI, who became emperor, and deposed Photios, although the latter had been his tutor. Photios was replaced by the Byzantine emperor's brother Stephen, and sent into exile to the monastery of Bordi in Armenia. It is confirmed from letters to and from Pope Stephen that Leo extracted a resignation from Photios. In 887, Photios and his protégé, Theodore Santabarenos, were put on trial for treason before a tribuneal headed by senior officials, headed by Andrew the Scythian. Although the sources sympathetic to Photios give the impression that the trial ended without a conviction, the chronicle of Pseudo-Symeon clearly states that Photios was banished to the monastery of Gordon, where he later died. Yet it appears that he did not remain reviled for the remainder of his life.
Photios continued his career as a writer throughout his exile, and Leo probably rehabilitated his reputation within the next few years; in his "Epitaphios" on his brothers, a text probably written in 888, the Emperor presents Photios favorably, portraying him as the legitimate archbishop, and the instrument of ultimate unity, an image that jars with his attitude to the patriarch in the previous year. Confirmation that Photios was rehabilitated comes upon his death: according to some chronicles, his body was permitted to be buried in Constantinople. In addition, according to the anti-Photian biographer of Ignatius, partisans of the ex-patriarch after his death endeavored to claim for him the "honor of sainthood". Furthermore, a leading member of Leo's court, Leo Choirosphaktes, wrote poems commemorating the memory of several prominent contemporary figures, such as Leo the Mathematician and the Patriarch Stephen, and he also wrote one on Photios. Shaun Tougher notes, however, that "yet Photios's passing does seem rather muted for a great figure of Byzantine history [...] Leo [...] certainly did not allow him back into the sphere of politics, and it is surely his absence from this arena that accounts for his quiet passing."
The Eastern Orthodox Church venerates Photios as a saint; his feast day is February 6.
Assessments.
Photios is one of the most famous figures not only of 9th-century Byzantium but of the entire history of the Byzantine Empire. One of the most learned men of his age, and revered – even by some of his opponents and detractors – as the most prolific theologian of his time, he has earned his fame due to his part in ecclesiastical conflicts, and also for his intellect and literary works.
Analyzing his intellectual work, Tatakes regards Photios as "mind turned more to practice than to theory". He believes that, thanks to Photios, humanism was added to Orthodoxy as a basic element of the national consciousness of the Byzantines. Tatakes also argues that, having understood this national consciousness, Photios emerged as a defender of the Greek nation and its spiritual independence in his debates with the Western Church. Adrian Fortescue regards him as "the most wonderful man of all the Middle Ages", and stresses that "had not given his name to the great schism, he would always be remembered as the greatest scholar of his time".
Writings.
The most important of the works of Photios is his renowned "Bibliotheca" or "Myriobiblon", a collection of extracts and abridgements of 280 volumes of classical authors (usually cited as Codices), the originals of which are now to a great extent lost. The work is especially rich in extracts from historical writers.
Some older scholarship speculated that the "Bibliotheca" was in fact compiled in Baghdad at the time of Photius's embassy to the Abbasid court, since many of the mentioned works were rarely cited during the so-called Byzantine Dark Ages c. 630 – c. 800, and it was known that the Abbasids were interested in works of Greek science and philosophy. However, specialists of this period of Byzantine history, such as Paul Lemerle, have shown that Photius could not have compiled his "Bibliotheca" in Baghdad because he clearly states in both his introduction and his postscript that when he learned of his appointment to the embassy, he sent his brother a summary of books that he read "previously", "since the time I learned how to understand and evaluate literature" i.e. since his youth. Moreover, the Abbasids were interested only in Greek science, philosophy and medicine; they did not have Greek history, rhetoric, or other literary works translated; nor did they have Christian patristic writers translated. Yet the majority of works in "Bibliotheca" are by Christian patristic authors, and most of the secular texts in "Bibliotheca" are histories, grammars or literary works, usually rhetoric, rather than science, medicine or philosophy. This further indicates that the majority of the works cannot have been read while Photius was in the Abbasid empire.
To Photios, we are indebted for almost all we possess of Ctesias, Memnon of Heraclea, Conon, the lost books of Diodorus Siculus, and the lost writings of Arrian. Theology and ecclesiastical history are also very fully represented, but poetry and ancient philosophy are almost entirely ignored. It seems that he did not think it necessary to deal with those authors with whom every well-educated man would naturally be familiar. The literary criticisms, generally distinguished by keen and independent judgment, and the excerpts vary considerably in length. The numerous biographical notes are probably taken from the work of Hesychius of Miletus.
The "Lexicon" (Λέξεων Συναγωγή), published later than the "Bibliotheca", was probably in the main the work of some of his pupils. It was intended as a book of reference to facilitate the reading of old classical and sacred authors, whose language and vocabulary were out of date. For a long time, the only manuscripts of the "Lexicon" were the "Codex Galeanus", which passed into the library of Trinity College, Cambridge and Berolinensis graec. oct. 22, both of which were incomplete. But then in 1959, Linos Politis of the University of Thessaloniki discovered a complete manuscript in a monastery in western Macedonia, codex Zavordensis 95.
His most important theological work is the "Amphilochia", a collection of some 300 questions and answers on difficult points in Scripture, addressed to Amphilochius, archbishop of Cyzicus. Other similar works are his treatise in four books against the Manichaeans and Paulicians, and his controversy with the Latins on the Procession of the Holy Spirit. Photios also addressed a long letter of theological advice to the newly converted Boris I of Bulgaria. Numerous other "Epistles" also survive.
Photios is also the writer of two "mirrors of princes", addressed to Boris-Michael of Bulgaria (Epistula 1, ed. Terzaghi) and to Leo VI the Wise (Admonitory Chapters of Basil I).
The chief contemporary authority for the life of Photios is his bitter enemy, Nicetas the Paphlagonian, the biographer of his rival Ignatios.
The first English translation, by Holy Transfiguration Monastery, of the "Mystagogy of the Holy Spirit" by Photios was published in 1983. Another translation was published in 1987 with a preface by Archimandrite (now Archbishop) Chrysostomos of Etna.

</doc>
<doc id="50372" url="https://en.wikipedia.org/wiki?curid=50372" title="Paul Revere">
Paul Revere

Paul Revere (; December 21, 1734 O.S.May 10, 1818) was an American silversmith, engraver, early industrialist, and a Patriot in the American Revolution. He is best known for alerting the Colonial militia to the approach of British forces before the battles of Lexington and Concord, as dramatized in Henry Wadsworth Longfellow's poem, "Paul Revere's Ride" (1861).
Revere was a prosperous and prominent Boston silversmith, who helped organize an intelligence and alarm system to keep watch on the British military. Revere later served as a Massachusetts militia officer, though his service culminated after the Penobscot Expedition, one of the most disastrous campaigns of the American Revolutionary War, for which he was absolved of blame. Following the war, Revere returned to his silversmith trade and used the profits from his expanding business to finance his work in iron casting, bronze bell and cannon casting, and the forging of copper bolts and spikes. Finally in 1800 he became the first American to successfully roll copper into sheets for use as sheathing on naval vessels.
Early life and education.
Revere was born in the North End of Boston on December 21, 1734, according to the Old Style calendar then in use, or January 1, 1735, in the modern calendar. His father, a French Huguenot born Apollos Rivoire, came to Boston at the age of 13 and was apprenticed to the silversmith John Coney. By the time he married Deborah Hitchborn, a member of a long-standing Boston family that owned a small shipping wharf, in 1729, Rivoire had anglicized his name to Paul Revere. Their son, Paul Revere, was the third of 12 children and eventually the eldest surviving son. Revere grew up in the environment of the extended Hitchborn family, and never learned his father's native language. At 13 he left school and became an apprentice to his father. The silversmith trade afforded him connections with a cross-section of Boston society, which would serve him well when he became active in the American Revolution. As for religion, although his father attended Puritan services, Revere was drawn to the Church of England. Revere eventually began attending the services of the political and provocative Jonathan Mayhew at the West Church. His father did not approve, and as a result father and son came to blows on one occasion. Revere relented and returned to his father's church, although he did become friends with Mayhew, and returned to the West Church in the late 1760s.
Revere's father died in 1754, when Paul was legally too young to officially be the master of the family silver shop. In February 1756, during the French and Indian War (the North American theater of the Seven Years' War), he enlisted in the provincial army. Possibly he made this decision because of the weak economy, since army service promised consistent pay. Commissioned a second lieutenant in a provincial artillery regiment, he spent the summer at Fort William Henry at the southern end of Lake George in New York as part of an abortive plan for the capture of Fort St. Frédéric. He did not stay long in the army, but returned to Boston and assumed control of the silver shop in his own name. On August 4, 1757, he married Sarah Orne (1736–1773); their first child was born eight months later. He and Sarah had eight children, but two died young, and only one, Mary, survived her father.
1765–1774: the gathering storm of revolution.
Revere's business began to suffer when the British economy entered a recession in the years following the Seven Years' War, and declined further when the Stamp Act of 1765 resulted in a further downturn in the Massachusetts economy. Business was so poor that an attempt was made to attach his property in late 1765. To help make ends meet he even took up dentistry, a skill set he was taught by a practicing surgeon who lodged at a friend's house. One client was Doctor Joseph Warren, a local physician and political opposition leader with whom Revere formed a close friendship. Revere and Warren, in addition to having common political views, were also both active in the same local Masonic lodges.
Although Revere was not one of the "Loyal Nine"—organizers of the earliest protests against the Stamp Act—he was well connected with its members, who were laborers and artisans. Revere did not participate in some of the more raucous protests, such as the attack on the home of Lieutenant Governor Thomas Hutchinson. In 1765, a group of militants who would become known as the "Sons of Liberty" formed, of which Revere was a member. From 1765 on, in support of the dissident cause, he produced engravings and other artifacts with political themes. Among these engravings are a depiction of the arrival of British troops in 1768 (which he termed "an insolent parade") and a famous depiction of the March 1770 Boston Massacre (see illustration). Although the latter was engraved by Revere and he included the inscription, "Engraved, Printed, & Sold by Paul Revere Boston", it was modeled on a drawing by Henry Pelham, and Revere's engraving of the drawing was colored by a third man and printed by a fourth. Revere also produced a bowl commemorating the Massachusetts assembly's refusal to retract the Massachusetts Circular Letter. (This letter, adopted in response to the 1767 Townshend Acts, called for united colonial action against the acts. King George III had issued a demand for its retraction.)
In 1770 Revere purchased a house on North Square in Boston's North End. Now a museum, the house provided space for his growing family while he continued to maintain his shop at nearby Clark's Wharf. Sarah died in 1773, and on October 10 of that year Revere married Rachel Walker (1745–1813). They had eight children, three of whom died young.
In November 1773 the merchant ship "Dartmouth" arrived in Boston harbor carrying the first shipment of tea made under the terms of the Tea Act. This act authorized the British East India Company to ship tea (of which it had huge surpluses due to colonial boycotts organized in response to the Townshend Acts) directly to the colonies, bypassing colonial merchants. Passage of the act prompted calls for renewed protests against the tea shipments, on which Townshend duties were still levied. Revere and Warren, as members of the informal North End Caucus, organized a watch over the "Dartmouth" to prevent the unloading of the tea. Revere took his turns on guard duty, and was one of the ringleaders in the Boston Tea Party of December 16, when colonists (some disguised as Indians) dumped tea from the "Dartmouth" and two other ships into the harbor.
From December 1773 to November 1775, Revere served as a courier for the Boston Committee of Public Safety, traveling to New York and Philadelphia to report on the political unrest in Boston. Research has documented 18 such rides. Notice of some of them was published in Massachusetts newspapers, and British authorities received further intelligence of them from Loyalist Americans. In 1774, his cousin John on the island of Guernsey wrote to Paul that John had seen reports of Paul's role as an "express" (courier) in London newspapers.
In 1774, the military governor of Massachusetts, General Thomas Gage, dissolved the provincial assembly on orders from Great Britain. Governor Gage also closed the port of Boston and all over the city forced private citizens to quarter (provide lodging for) soldiers in their homes.
During this time, Revere and a group of 30 "mechanics" began meeting in secret at his favorite haunt, the "Green Dragon", to coordinate the gathering and dissemination of intelligence by "watching the Movements of British Soldiers". Around this time Revere regularly contributed politically charged engravings to the recently founded Patriot monthly, "Royal American Magazine".
He rode to Portsmouth, New Hampshire in December 1774 upon rumors of an impending landing of British troops there, a journey known in history as the Portsmouth Alarm. Although the rumors were false, his ride sparked a rebel success by provoking locals to raid Fort William and Mary, defended by just six soldiers, for its gunpowder supply.
"Midnight Ride".
When British Army activity on April 7, 1775 suggested the possibility of troop movements, Joseph Warren sent Revere to warn the Massachusetts Provincial Congress, then sitting in Concord, the site of one of the larger caches of Patriot military supplies. After receiving the warning, Concord residents began moving the military supplies away from the town.
One week later, on April 14, General Gage received instructions from Secretary of State William Legge, Earl of Dartmouth (dispatched on January 27), to disarm the rebels, who were known to have hidden weapons in Concord, among other locations, and to imprison the rebellion's leaders, especially Samuel Adams and John Hancock. Dartmouth gave Gage considerable discretion in his commands. Gage issued orders to Lieutenant Colonel Francis Smith to proceed from Boston "with utmost expedition and secrecy to Concord, where you will seize and destroy... all Military stores... But you will take care that the soldiers do not plunder the inhabitants or hurt private property." Gage did not issue written orders for the arrest of rebel leaders, as he feared doing so might spark an uprising.
Between 9 and 10 p.m. on the night of April 18, 1775, Joseph Warren told Revere and William Dawes that the king's troops were about to embark in boats from Boston bound for Cambridge and the road to Lexington and Concord. Warren's intelligence suggested that the most likely objectives of the regulars' movements later that night would be the capture of Adams and Hancock. They did not worry about the possibility of regulars marching to Concord, since the supplies at Concord were safe, but they did think their leaders in Lexington were unaware of the potential danger that night. Revere and Dawes were sent out to warn them and to alert colonial militias in nearby towns.
In the days before April 18, Revere had instructed Robert Newman, the sexton of the North Church, to send a signal by lantern to alert colonists in Charlestown as to the movements of the troops when the information became known. In what is well known today by the phrase "one if by land, two if by sea", one lantern in the steeple would signal the army's choice of the land route while two lanterns would signal the route "by water" across the Charles River (the movements would ultimately take the water route, and therefore two lanterns were placed in the steeple). Revere first gave instructions to send the signal to Charlestown. He then crossed the Charles River by rowboat, slipping past the British warship HMS "Somerset" at anchor. Crossings were banned at that hour, but Revere safely landed in Charlestown and rode to Lexington, avoiding a British patrol and later warning almost every house along the route. The Charlestown colonists dispatched additional riders to the north.
Riding through present-day Somerville, Medford, and Arlington, Revere warned patriots along his route, many of whom set out on horseback to deliver warnings of their own. By the end of the night there were probably as many as 40 riders throughout Middlesex County carrying the news of the army's advance. Revere did not shout the phrase later attributed to him ("The British are coming!"): His mission depended on secrecy, the countryside was filled with British army patrols, and most of the Massachusetts colonists (who were predominantly English in ethnic origin) still considered themselves British. Revere's warning, according to eyewitness accounts of the ride and Revere's own descriptions, was "The Regulars are coming out." Revere arrived in Lexington around midnight, with Dawes arriving about a half-hour later. They met with Samuel Adams and John Hancock, who were spending the night with Hancock's relatives (in what is now called the Hancock-Clarke House), and they spent a great deal of time discussing plans of action upon receiving the news. They believed that the forces leaving the city were too large for the sole task of arresting two men and that Concord was the main target. The Lexington men dispatched riders to the surrounding towns, and Revere and Dawes continued along the road to Concord accompanied by Samuel Prescott, a doctor who happened to be in Lexington "returning from a lady friend's house at the awkward hour of 1 a.m."
Revere, Dawes, and Prescott were detained by a British Army patrol in Lincoln at a roadblock on the way to Concord. Prescott jumped his horse over a wall and escaped into the woods; he eventually reached Concord. Dawes also escaped, though he fell off his horse not long after and did not complete the ride.
Revere was captured and questioned by the British soldiers at gunpoint. He told them of the army's movement from Boston, and that British army troops would be in some danger if they approached Lexington, because of the large number of hostile militia gathered there. He and other captives taken by the patrol were still escorted east toward Lexington, until about a half mile from Lexington they heard a gunshot. The British major demanded Revere explain the gunfire, and Revere replied it was a signal to "alarm the country". As the group drew closer to Lexington, the town bell began to clang rapidly, upon which one of the captives proclaimed to the British soldiers "The bell's a'ringing! The town's alarmed, and you're all dead men!" The British soldiers gathered and decided not to press further towards Lexington but instead to free the prisoners and head back to warn their commanders. The British confiscated Revere's horse and rode off to warn the approaching army column. Revere walked to Rev. Jonas Clarke's house, where Hancock and Adams were staying. As the battle on Lexington Green unfolded, Revere assisted Hancock and his family in their escape from Lexington, helping to carry a trunk of Hancock's papers.
The ride of the three men triggered a flexible system of "alarm and muster" that had been carefully developed months before, in reaction to the colonists' impotent response to the Powder Alarm of September 1774. This system was an improved version of an old network of widespread notification and fast deployment of local militia forces in times of emergency. The colonists had periodically used this system all the way back to the early years of Indian wars in the colony, before it fell into disuse in the French and Indian War. In addition to other express riders delivering messages, bells, drums, alarm guns, bonfires, and a trumpet were used for rapid communication from town to town, notifying the rebels in dozens of eastern Massachusetts villages that they should muster their militias because the regulars in numbers greater than 500 were leaving Boston with possible hostile intentions. This system was so effective that people in towns from Boston were aware of the army's movements while they were still unloading boats in Cambridge. Unlike in the Powder Alarm, the alarm raised by the three riders successfully allowed the militia to confront the British troops in Concord, and then harry them all the way back to Boston.
War years.
Because Boston was besieged after the battles of Lexington and Concord, Revere could not return to the city, which was now firmly in British hands. He boarded in Watertown, where he was eventually joined by Rachel and most of his children (Paul Jr., then 15, remained in Boston to mind the family properties). After he was denied a commission in the Continental Army, he tried to find other ways to be useful to the rebel cause. He was retained by the provincial congress as a courier, and he printed local currency which the congress used to pay the troops around Boston.
Since there was a desperate shortage of gunpowder, the provincial congress decided in November 1775 to send him to Philadelphia to study the working of the only powder mill in the colonies, in the hopes that he might be able to build a second one in Massachusetts. Revere called on the mill's owner, Oswald Eve, armed with a letter from Continental Congressmen Robert Morris and John Dickinson asking Eve to "Chearfully & from Public Spirited Motives give Mr. Revere such information as will inable him to Conduct the business on his return home." Eve showed Revere around the mill, but refused to give him detailed drawings unless he was first paid a substantial bribe. Despite this chilly reception, Revere was able to discern useful information from the visit. He also acquired, through the work of Samuel Adams, plans for another powder mill. This information enabled Revere to set up a powder mill at Stoughton (present-day Canton). The mill produced tons of gunpowder for the Patriot cause.
Revere's friend and compatriot Joseph Warren was killed in the Battle of Bunker Hill on June 17, 1775. Because soldiers killed in battle were often buried in mass graves without ceremony, Warren's grave was unmarked. On March 21, 1776, several days after the British army left Boston, Revere, Warren's brothers, and a few friends went to the battlefield and found a grave containing two bodies. After being buried for nine months, Warren's face was unrecognizable, but Revere was able to identify Warren's body because he had placed a false tooth in Warren's mouth, and recognized the wire he had used for fastening it. Warren was given a proper funeral and reburied in a marked grave.
Militia service.
Upon returning to Boston in 1776, Revere was commissioned a major of infantry in the Massachusetts militia in that April, and transferred to the artillery a month later. In November he was promoted to lieutenant colonel, and was stationed at Castle William, defending Boston harbor. He was generally second or third in the chain of command, and on several occasions he was given command of the fort. He applied his engineering skills to maintaining the fort's armaments, even designing and building a caliper to accurately measure cannonballs and cannon bore holes. The service at Castle William was relatively isolated, and personality friction prompted some men to file complaints against Revere. The boredom was alleviated in late August 1777 when Revere was sent with a troop of soldiers to escort prisoners taken in the Battle of Bennington to Boston, where they were confined on board prison ships, and again in September when he was briefly deployed to Rhode Island.
In August 1778 Revere's regiment served in a combined Franco-American expedition whose objective was to capture the British base at Newport, Rhode Island. His regiment was responsible for erecting and maintaining artillery batteries on Aquidneck Island. The attempt was abandoned by the French when their fleet was scattered in a storm, and Revere's regiment returned to Boston before the British sortied from Newport to force the Battle of Rhode Island.
Penobscot disaster.
The British in June 1779 established a new base on Penobscot Bay in present-day Maine (which was then part of Massachusetts). Massachusetts authorities called out the militia, pressed into service available shipping, and organized a major expedition to dislodge the British. The expedition was a complete fiasco: its land and naval commanders squabbled over control of the expedition, and could not agree on strategy or tactics. The arrival of British reinforcements led to the destruction of the entire Massachusetts fleet. Revere commanded the artillery units for the expedition, and was responsible for organizing the artillery train. He participated in the taking of Bank's Island, from which artillery batteries could reach the British ships anchored before Fort George. He next oversaw the transport of the guns from Bank's Island to a new position on the heights of the Bagaduce Peninsula that commanded the fort. Although Revere was in favor of storming the fort, Brigadier General Solomon Lovell opted for a siege instead. After further disagreements on how to proceed between Lovell and fleet commander Dudley Saltonstall, Lovell decided to return to the transports on August 12, a decision supported by Revere.
Late the next day British sails were spotted. A mad scramble ensued, and on the 14th the fleet was in retreat heading up the Penobscot River. Revere and his men were put ashore with their stores, and their transports destroyed. At one point Brigadier General Peleg Wadsworth ordered Revere to send his barge in an attempt to recover a ship drifting toward the enemy position. Revere at first resisted, but eventually complied, and Wadsworth told him to expect formal charges over the affair. The incident separated Revere from his men. Moving overland, he eventually managed to regroup most of his troops, and returned to Boston on August 26. A variety of charges were made against Revere, some of which were exaggerated assignments of blame made by enemies he had made in his command at Castle William. The initial hearings on the matter in September 1779 were inconclusive, but he was asked to resign his post. He repeatedly sought a full court-martial to clear his name, but it was not until February 1782 that a court martial heard the issue, exonerating him.
Business and social connections.
During the Revolutionary War, Revere continued his efforts to move upwards in society into the gentry. After his failed efforts to become a military officer he attempted to become a merchant, but was hindered by a number of factors: while he was a fairly well-off member of the artisan class, he did not have the resources to afford the goods he would have sold as a merchant, nor were lenders in England willing to lend him the required startup capital. Other American merchants of the time collaborated with colleagues in England. However, Revere's inexperience as a merchant meant that he had not yet established such relationships and was not able to communicate as effectively on unfamiliar matters. Another factor preventing Revere’s success as a merchant was the economic climate of the time period after the war known as the Critical Period; while the colonies had seen a time of economic growth before the war, the colonies experienced a severe post-war depression, constraining the overall success of his business.
Revere's increased efficiency left financial and human resources available for the exploration of other products, which was essential to overcoming the fluctuating post-war economic climate. In addition to increasing production, the flatting mill enabled Revere to move towards a more managerial position.
Later years: entrepreneurship, manufacturing, and politics.
After the war, Revere became interested in metal work beyond gold and silver. By 1788 he had invested some of the profits from his growing silverworking trade to construct a large furnace, which would allow him to work with larger quantities of metals at higher temperatures. He soon opened an iron foundry in Boston's North End that produced utilitarian cast iron items such as stove backs, fireplace tools, and window weights, marketed to a broad segment of Boston's population. Many of Revere’s business practices changed when he expanded his practice into ironworking, because he transitioned from just being an artisan to also being an entrepreneur and a manager. In order to make this transition successfully, Revere had to invest substantial quantities of investment capital and time in his foundry.
Technological practices.
The quasi-industrialization of his practice set Revere apart from his competition. “Revere’s rapid foundry success resulted from fortuitous timing, innate technical aptitude, thorough research, and the casting experience he gained from silverworking.” This technical proficiency allowed Revere to optimize his work and adapt to a new technological and entrepreneurial model. Revere’s location also benefited his endeavors. Revere was entering the field of iron casting in a time when New England cities were becoming centers of industry. The nature of technological advancement was such that many skilled entrepreneurs in a number of fields worked together, in what is known by Nathan Rosenberg as technological convergence, by which a number of companies work together on challenges in order to spur advances. By accessing the knowledge of other nearby metal workers, Revere was able to successfully explore and master new technologies throughout his career.
Labor practices.
One of the biggest changes for Revere in his new business was organization of labor. In his earlier days, Revere primarily utilized the apprenticeship model standard for artisan shops at this time, but as his business expanded he hired employees (wage laborers) to work for his foundry. Many manufacturers of the era found this transition from master to employer difficult because many employees at the onset of the Industrial Revolution identified themselves as skilled workers, and thus wanted to be treated with the respect and autonomy accorded to artisans. An artisan himself, Revere managed to avoid many of these labor conflicts by adopting a system of employment that still held trappings of the craft system in the form of worker freedoms such as work hour flexibility, wages in line with skill levels, and liquor on the job.
Manufacturing: church bells, cannon, and copper products.
After mastering the iron casting process and realizing substantial profits from this new product line, Revere identified a burgeoning market for church bells in the religious revival known as the Second Great Awakening that followed the war. Beginning in 1792 he became one of America's best-known bell casters, working with sons Paul Jr. and Joseph Warren Revere in the firm Paul Revere & Sons. This firm cast the first bell made in Boston and ultimately produced hundreds of bells, a number of which remain in operation to this day.
In 1794, Revere decided to take the next step in the evolution of his business, expanding his bronze casting work by learning to cast cannon for the federal government, state governments, and private clients. Although the government often had trouble paying him on time, its large orders inspired him to deepen his contracting and seek additional product lines of interest to the military.
By 1795, a growing percentage of his foundry's business came from a new product, copper bolts, spikes, and other fittings that he sold to merchants and the Boston naval yard for ship construction. In 1801, Revere became a pioneer in the production of rolled copper, opening North America's first copper mill south of Boston in Canton. Copper from the Revere Copper Company was used to cover the original wooden dome of the Massachusetts State House in 1802. His copper and brass works eventually grew, through sale and corporate merger, into a large corporation, Revere Copper and Brass, Inc.
Steps towards standardized production.
During his earlier days as an artisan, especially when working with silver products, Revere produced "bespoke" or customized goods. As he shifted to ironworking, he found the need to produce more standardized products, because this made production cheaper. To achieve the beginnings of standardization, Revere used identical molds for casting, especially in the fabrication of mass-produced items such as stoves, ovens, frames, and chimney backs. However, Revere did not totally embrace uniform production. For example, his bells and cannons were all unique products: these large objects required extensive fine-tuning and customization, and the small number of bells and cannon minimized the potential benefits of standardizing them. In addition, even the products that he made in large quantities could not be truly standardized due to technological and skill limitations. His products were rarely (if ever) identical, but his processes were well systematized. “He came to realize that the foundry oven melded the characteristics of tools and machines: it required skilled labor and could be used in a flexible manner to produce different products, but an expert could produce consistent output by following a standard set of production practices.”
Freemasonry.
Revere was the Grand Master of the Freemasons of Massachusetts when a box containing an assemblage of commemorative items was deposited under the cornerstone of the Massachusetts State House on 4 July 1795 by Governor Samuel Adams, assisted by Grand Master Revere and Deputy Grand Master, Colonel William Scollay.
Politics and final years.
Revere remained politically active throughout his life. His business plans in the late 1780s were often stymied by a shortage of adequate money in circulation. Alexander Hamilton's national policies regarding banks and industrialization exactly matched his dreams, and he became an ardent Federalist committed to building a robust economy and a powerful nation. He continued to participate in local discussions of political issues even after his retirement in 1811, and in 1814 circulated a petition offering the government the services of Boston's artisans in protecting Boston during the War of 1812. Revere died on May 10, 1818, at the age of 83, at his home on Charter Street in Boston. Originally buried at the Christ Church Cemetery in Boston, he is now buried in the Granary Burying Ground on Tremont Street.
Legacy.
After Revere's death, the family business was taken over by his oldest surviving son, Joseph Warren Revere. The copper works founded in 1801 continues today as the Revere Copper Company, with manufacturing divisions in Rome, New York and New Bedford, Massachusetts.
Revere's original silverware, engravings, and other works are highly regarded today, and can be found on display in museums including the Museum of Fine Arts, Boston and the Metropolitan Museum of Art. The Revere Bell, presented in 1843 to the Church of St. Andrew in Singapore by his daughter, Mrs. Maria Revere Balestier, wife of American consul Joseph Balestier, is now displayed in the National Museum of Singapore. This is the only bell cast by the Revere foundry that is outside the United States. For a time, it was displayed behind velvet ropes in the foyer of the United States Embassy in Singapore.
The communities of Revere, Massachusetts and Revere, Minnesota bear his name, as do Revere Beach in Revere, Massachusetts, Revere Avenue in The Bronx, New York City, Paul Revere Road in Arlington, Massachusetts, and Paul Revere Apartments in Seattle.
A 25-cent 1958 U.S. postage stamp in the Liberty Series honors Paul Revere, featuring the portrait by Gilbert Stuart. He also appears on the $5,000 Series EE U.S. Savings Bond.
The rock group Paul Revere and the Raiders had considerable popularity from the middle 1960s through the early 1970s.
Longfellow's poem.
In 1861, over 40 years after Revere's death, Henry Wadsworth Longfellow made the midnight ride the subject of his poem "Paul Revere's Ride" which opens:
Longfellow's poem is not historically accurate, but the inaccuracies were deliberate. Longfellow had researched the historical event, using such works as George Bancroft's "History of the United States", but he changed the facts for poetic effect. The poem was one of a series in which he sought to create American legends; earlier examples include "The Song of Hiawatha" (1855) and "The Courtship of Miles Standish" (1858). Longfellow was successful in creating a legend: Revere's stature rose significantly in the years following the poem's publication.
Parts of the ride route in Massachusetts are now posted with signs marked "Revere's Ride". The route follows Main Street in Charlestown, Broadway and Main Street in Somerville, Main Street and High Street in Medford, Medford Street to Arlington center, and Massachusetts Avenue the rest of the way through Lexington and into Lincoln. Revere's ride is reenacted annually.

</doc>
<doc id="50373" url="https://en.wikipedia.org/wiki?curid=50373" title="Purple (cipher machine)">
Purple (cipher machine)

In the history of cryptography, or , codenamed Purple by the United States, was a diplomatic cryptographic machine used by the Japanese Foreign Office just before and during World War II. The machine was an electromechanical stepping-switch device.
The information gained from decryptions was eventually code-named "Magic" within the US government.
The codename "Purple" referred to binders used by US cryptanalysts for material produced by various systems; it replaced the Red machine used by the Japanese Foreign Office. The Japanese also used Coral and JADE stepping-switch systems.
Development of Japanese cipher machines.
Overview.
The Japanese Navy did not cooperate with the Army in pre-war cipher machine development, and that lack of cooperation continued into World War II. The Navy believed the Purple machine was sufficiently difficult to break that it did not attempt to revise it to improve security. This seems to have been on the advice of a mathematician, , who lacked a background in cryptanalysis. The Ministry of Foreign Affairs was supplied Red and Purple by the Navy. No one in Japanese authority noticed weak points in both machines.
Just before the end of the war, the Army warned the Navy of a weak point of Purple, but the Navy failed to act on this advice.
The Army developed their own cipher machines on the same principle as Enigma, , and from 1932 to 1941. The Army judged that these machines had lower security than the Navy's Purple design, so the Army's two cipher machines were less used.
Prototype of Red.
Japanese diplomatic communications at negotiations for the Washington Naval Treaty were broken by the American Black Chamber in 1922, and when this became publicly known, there was considerable pressure to improve their security. In any case, the Japanese Navy had planned to develop their first cipher machine for the following London Naval Treaty. Japanese Navy Captain , of Section 10 (cipher & code) of the Japanese Navy General Staff Office, supervised the work.
The development of the machine was the responsibility of the Japanese Navy Institute of Technology, Electric Research Department, Section 6. In 1928, the chief designer and Navy Commander, developed a prototype of Red, .
The prototype used the same principle as the Kryha cipher machine, having a plug-board, and was used by the Japanese Navy and Ministry of Foreign Affairs at negotiations for the London Naval Treaty in 1930.
Red.
The prototype machine was finally completed as in 1931. The year 1931 was year 2591 in the Japanese Imperial calendar. Thus it was prefixed "91-shiki" from the year it was developed.
The "91-shiki injiki" Roman-letter model was also used by the Ministry of Foreign Affairs as , codenamed "Red" by United States cryptanalysts.
The Red machine was unreliable unless the contacts in its half-rotor switch were cleaned every day. It enciphered vowels (AEIOUY) and consonants separately, perhaps to reduce telegram costs, and this was a significant weak point. The Navy also used the "91-shiki injiki" Kana-letter model at its bases and on its vessels.
Purple.
In 1937, the Japanese completed the next generation . The Ministry of Foreign Affairs machine was the , codenamed Purple by United States cryptanalysts.
The chief designer of Purple was . His engineers were and . Eikichi Suzuki suggested the use of a stepping switch instead of the more troublesome half-rotor switch.
Clearly, the Purple machine was more secure than Red, but the Navy did not recognize that Red had already been broken. The Purple machine inherited a weakness from the Red machine that six letters of the alphabet were encrypted separately. It differed from RED in that the group of letters was changed and announced every nine days, whereas in RED they were permanently fixed as the Latin vowels 'a', 'e', 'i', 'o', 'u' and 'y'. Thus US Army SIS was able to break the cipher used for the six letters before it was able to break the one used for the 20 others.
Weaknesses and cryptanalysis.
In operation, the enciphering machine accepted typewritten input (in the Roman alphabet) and produced ciphertext output, and vice versa when deciphering messages. The result was a potentially excellent cryptosystem. In fact, operational errors, chiefly in key choice, made the system less secure than it could have been; in that way the Purple code shared the fate of the German Enigma machine. The cipher was broken by a team from the US Army Signals Intelligence Service, then directed by William Friedman in 1940. Reconstruction of the Purple machine was based on ideas of Larry Clark. Advances into the understanding of Purple keying procedures were made by Lt Francis A. Raven, USN. Raven discovered that the Japanese had divided the month into three 10-days periods, and within each period they used the keys of the first day with small predictable changes.
The Japanese believed it to be unbreakable throughout the war, and even for some time after the war, even though they had been informed otherwise by the Germans. In April 1941, Hans Thomsen, a diplomat at the German embassy in Washington, D.C., sent a message to Joachim von Ribbentrop, the German foreign minister, informing him that "an absolutely reliable source" had told Thomsen that the Americans had broken the Japanese diplomatic cipher (that is, Purple). That source apparently was Konstantin Umansky, the Soviet ambassador to the US, who had deduced the leak based upon communications from Sumner Welles. The message was duly forwarded to the Japanese; but use of the code continued.
The United States obtained portions of a Purple machine from the Japanese Embassy in Germany following Germany's defeat in 1945 (see image above) and discovered that the Japanese had used precisely the same "stepping switch" in its construction that Leo Rosen of SIS had chosen when building a "duplicate" (or Purple analog machine) in Washington in 1939 and 1940. The "stepping switch" was a uniselector - a standard element used in large quantities in automatic telephone exchanges in countries like the United States, Canada, the UK, Germany and Japan, which had extensive dial-telephone systems.
Apparently, all other Purple machines at Japanese embassies and consulates around the world (e.g. in Axis countries, Washington, London, Moscow, and in neutral countries) and in Japan itself, were destroyed and ground into particles by the Japanese. American occupation troops in Japan in 1945−52 searched for any remaining units.
The Purple machine itself was first used by Japan in June 1938, but U.S. and British cryptanalysts had broken some of its messages well before the attack on Pearl Harbor. U.S. cryptanalysts decrypted and translated Japan's 14-part message to its Washington Embassy (ominously) breaking off negotiations with the United States at 1 p.m. Washington time on 7 December 1941, before the Japanese Embassy in Washington had done so. Decryption and typing difficulties at the Embassy, coupled with ignorance of the importance of it being on time, were major reasons the "Nomura note" was delivered late.
Other factors.
During World War II, the Japanese embassy in Nazi Germany was kept well-informed on German military affairs. This information was reported to Tokyo in Purple-enciphered radio messages. These reports included, for example, details of the Atlantic Wall fortifications along the coasts of France and Belgium. Since these messages were being read by the Allies, this provided valuable intelligence about German military preparations against the forthcoming invasion of Western Europe.
The decrypted Purple traffic, and Japanese messages generally, was the subject of acrimonious hearings in Congress post-World War II in connection with an attempt to decide who, if anyone, had allowed the attack at Pearl Harbor to happen and who therefore should be blamed. It was during those hearings that the Japanese learned, for the first time, that the Purple cipher machine had indeed been broken.
The Russians also succeeded in breaking into the PURPLE system in late 1941, and traffic indicating that Japan was only going to attack the US and UK allowed Stalin to move considerable forces from the Far East to Moscow just in time to help stop the final German push to Moscow.

</doc>
<doc id="50375" url="https://en.wikipedia.org/wiki?curid=50375" title="Internet relationship">
Internet relationship

An internet relationship is a relationship between people who have met online, and in many cases know each other only via the Internet. Online relationships are similar in many ways to pen pal relationships. This relationship can be romantic, platonic, or even based on business affairs. An internet relationship (or online relationship) is generally sustained for a certain amount of time before being titled a relationship, just as in-person relationships. The major difference here is that an internet relationship is sustained via computer or online service, and the individuals in the relationship may or may not ever meet each other in person. Otherwise, the term is quite broad and can include relationships based upon text, video, audio, or even virtual character. This relationship can be between people in different states, different countries, different sides of the world, or even people who reside in the same area but do not communicate in person.
Technological advances.
According to J. Michael Jaffe, author of "Gender, Pseudonyms, and CMC: Masking Identities and Baring Souls", "the Internet was originally established to expedite communication between governmental scientists and defense experts, and was not at all intended to be the popular 'interpersonal mass medium' it has become", yet new and revolutionary devices enabling the mass public to communicate online are constantly being developed and released.
Rather than having many devices for different uses and ways of interacting, communicating online is more accessible and cheaper by having an Internet function built into one device, such as mobile phones, tablets, laptops, and smartphones. Other ways of communicating online with these devices are via services and applications such as Email, Skype, IChat, instant messaging programs, social networking services, asynchronous discussion groups, online games, virtual worlds and the World Wide Web.
Some of these ways of communicating online are asynchronous (meaning not in real time), such as YouTube and some are synchronous (immediate communication), such as Twitter. Synchronous communication occurs when two or more participants are interacting in real time via voice or text chat.
Types of relationships.
Many types of internet relationships are possible in today's world of technology.
Internet dating.
Internet dating is very relevant in the lives of many individuals worldwide. A major benefit in the rise of Internet dating is the decrease in prostitution. People no longer need to search on the streets to find casual relationships. They can find them online if that is what they desire. Internet dating websites offer matchmaking services for people to find love or whatever else they may be looking for. The creation of the internet and its progressive innovations have opened up doors for people to meet other people who they may very well have never met otherwise.
Dating website innovations.
Although the availability of uploading videos to the internet is not a new innovation, it has been made easier since 2008 thanks to YouTube. YouTube began the surge of video streaming sites in 2005 and within three years, smaller web developers started implementing video sharing on their sites. Internet dating sites have benefitted greatly since the surge in easiness and accessibility of picture and video uploading. Videos and pictures are equally important for most personal profiles. These profiles can be found on sites used for interpersonal relationships other than dating as well. "The body, although graphically absent, does not have to be any less present." Older and less advanced sites usually still allow, and often require, each user to upload a picture. Newer and more advanced sites offer the possibility of streaming media live via the user's profile for the site. The inclusion of videos and pictures has become almost a necessity for sexual social networking sites to maintain the loyalty of their members. It is appealing to internet users to be able to view and share videos, especially when forming relationships or friendships.
Who uses online dating sites.
According to an article in the "New York Times", mediated matchmaking has been around since the mid-1800s. Online dating was made available in the mid-1990s, with the creation of the first dating sites. These dating sites create a space for liberation of sexuality. According to Sam Yagan of OkCupid, "the period between New Year's Day and Valentine's Day is busiest six weeks of the year". Changes that online dating companies have created include not only the increase of pickiness in singles, but the rise in interracial marriages and spread the acceptance of homosexual individuals. Dating sites "are a place where sexual minorities, inter-sexed people and gay people are enjoying a newly found freedom". Several studies have shown the availability of online dating to produce a greater closeness and intimacy between individuals because it circumvents barriers that face-to-face interactions might have. "Participating in personal relationships online allow for almost full freedom from power relations in the offline/real world."
A plethora of virtual sexual identities are represented in online profiles. The amount of personal information users are being asked to provide is constantly increasing. More and more online users are starting to explore and experiment with aspects of their sexual identities, whereas before, they may have felt uncomfortable due to social constraints or fear of possible repercussions. Most internet sites containing personal profiles require individuals to fill in "personal information" sections. Often these sections include a series of multiple choice questions. Due to the anonymity of these virtual profiles, individuals are more frequent to ‘role’-play at being one of the predefined ‘types,’ although offline, reservations may inhibit the individual from sharing true answers.
There have also been many studies done to observe online daters and their reason for turning to the internet to look for romantic partners. According to Dr. Robert J. Brym and Dr. Rhonda L. Lenton, users of online games, websites, and other virtual communities are encouraged to conceal their identities and learn things about themselves that they never knew before. With a concealed identity, an online user can be whoever they want to be at that exact moment. They have the ability to venture outside of their comfort zone and act as someone completely different.
The Journal of Computer-Mediated Communication reports the results of a study conducted by Robert J. Stephure, Susan D. Boon, Stacy L. MacKinnon, and Vicki L. Deveau on types of relationships online participants were seeking. They concluded that "when asked what they were looking for in an online relationship, the considerable 
majority of participants expressed interest in seeking fun, companionship, and someone to talk to. Most also reported interests in developing casual friendships and dating relationships with online partners. Substantially fewer reported using the Internet for the speciﬁc purposes of identifying potential sexual or marital partners."
Faye Mishna, Alan McLuckie, and Michael Saini, co-authors of the Oxford Journal article "Real-World Dangers in an Online Reality: A Qualitative Study Examining Online Relationships and Cyber Abuse", reported the results of their research and observation of over 35,000 individuals between the ages of 6 and 24 who have been or currently are a part of an internet relationship. Of the final 346 posts chosen to be included in the study, the average age of online users sharing information about their online relationship(s) was 14 years old. The overwhelming result was that children and youth consider their online relationships to be just as "real" as their offline relationships. The study also showed that the internet plays a crucial role in most sexual and romantic experiences of adolescent users.
Success of dating websites and social networks.
Canaan Partners have reported that the dating industry brings in an estimate of 3-4 billion dollars yearly from membership fees and advertisements. The range of dating sites has expanded vastly over the past two decades. There are dating websites that focus on the matchmaking of certain groups of people based on religion, sexual preference, race, etc.
The average life expectancy has been on a rise, leaving many young singles feeling as if they have plenty of time to find a Life partner. This opens up time to travel and experience things without the burden of a relationship. As of 1996, more than 20% of Canadians "were not living in the same census subdivision as they were five years earlier" and as of 1998, more than half of employed Canadians worried "they not have enough time to spend with their family and friends". Due to an increase in many businesses requiring their employees to travel, singles, often young professionals, find online dating websites to be the perfect answer to their "problem", states Brym and Lenton.
Erik Shipmon, author of “Why Do People Date Online?", exclaims, "the Internet is the ultimate singles’ bar—without the noise, the drunks, and the high cost of all those not-so-happy hours. Nor, thanks to online dating membership sites, do you have to depend on your friends and family to hook you up with people they think would be perfect for you—and who wouldn’t be perfect for, well, anyone, which is why they are still unattached”.
Cybersex.
Some people who are in an online relationship also participate in cybersex, which is a virtual sex encounter in which two or more individuals who are connected remotely via computer network send each other sexually explicit messages describing a sexual experience. This can also include individuals communicating sexually via video or audio. Some websites offer a cybersex service, where a patron pays the website owner in exchange for an online sexual experience with another person.
Cybersex sometimes includes real life masturbation. The quality of a cybersex encounter typically depends upon the participants' abilities to evoke a vivid, visceral mental picture in the minds of their partners. Imagination and suspension of disbelief are also critically important. Cybersex can occur either within the context of existing or intimate relationships, e.g. among lovers who are geographically separated, or among individuals who have no prior knowledge of one another and meet in virtual spaces or cyberspaces and may even remain anonymous to one another. In some contexts cybersex is enhanced by the use of a webcam to transmit real-time video of the partners.
Social networking relationships.
Social networking has enabled people to connect with each other via the internet. Sometimes, members of a social networking service do know all, or many of their "friends" (Facebook) / "Connections" (LinkedIn) / etc. in person. However, sometimes internet relationships are formed through these services, including but not limited to:
Facebook,
Myspace,
Google Plus,
LinkedIn,
Twitter,
Instagram,
Pinterest,
DeviantArt and
Xanga.
"Social Networking Service" is a very broad term, branching out to websites based on many different aspects. One aspect that is possible on all social networking sites is the possibility of an internet relationship. These sites enable users to search for new connections based on location, education, experiences, hobbies, age, gender, and more. This allows individuals meeting each other to already have some characteristic in common. These sites usually allow for people who do not know each other to "add" each other as a connection or friend and to send each other messages. This connection can lead to more communication between two individuals. An immense amount of information about the individuals can be found directly on their social network profile. Proving those individuals include plentiful and accurate information about themselves, people in online relationships can find out much about each other by viewing profiles and "about me's." Communication between individuals can become more frequent, thus forming some type of relationship via the internet. This relationship can turn into an acquaintance, a friendship, a romantic relationship, or even a business partnership.
Online gaming.
Online gaming elicits the introduction of many different types of people in one interface. A common type of online game where individuals form relationships is the MMORPG, or a Massively Multiplayer Online Role-Playing Game. Some examples of MMORPGs are World of Warcraft, EverQuest, SecondLife, Final Fantasy Online, and Minecraft (see List of massively multiplayer online role-playing games.) These games enable individuals to create a character that represents them and interact with other characters played by real individuals, while at the same time carrying out the tasks and goals of the actual game.
Online games other than MMORPGs can elicit internet relationships as well. Card games such as poker and board games like Pictionary have been transformed into virtual interfaces that allow an individual to play against people across the internet, as well as chatting with them. Virtual pet sites such as Webkinz and Neopets are another type of popular online game that allow individuals to socialize with other players.
Games create social spaces for people of various ages, with userbases often crossing age brackets. Most of these games enable individuals to chat with each other, as well as form groups and clans. This interaction can lead to further communication, turning into a friendship or romantic relationship.
Online forums and chatrooms.
An Internet forum is a website that includes conversations in the form of posted messages. Forums can be for general chatting or can be broken down into categories and topics. They can be used to ask questions, post opinions, or debate topics. Forums include their own jargon, for example a conversation is a "thread." Different forums also have different lingo and styles of communicating.
There are religion forums, music forums, car forums, and countless other topics. These forums elicit communication between individuals no matter the location, gender, ethnicity, etc. although some do include age restrictions. Through these forums people may comment on each other's topics or threads, and with further communication form a friendship, partnership, or romantic relationship.
Professional relationships.
Even in work settings, the introduction of the internet has established easier and sometimes more practical forms of communicating. Compared to traditional communication in business, communication through internet can be more efficient in the aspect of time-saving. The internet is often referred to as a vehicle for investor relations or the "electronic highway" for business transactions in the United States. The Internet has increased organizational involvement by facilitating the flow of information between face-to-face meetings and allowing for people to arrange meetings at virtually any given time. Socially, it has stimulated positive change in people’s lives by creating new forms of online interaction and enhancing offline relationships worldwide, allowing for better and more efficient. In the real world, companies which are considered as leading companies in our world already introduced efficient ways of communication based on internet. business communication.
Advantages.
For more intimate relationships, research has shown that personal disclosures create a greater sense of intimacy. This gives a sense of trust and equality, which people search for in a relationship, and this is often easier to achieve online than face to face, although not all disclosures are responded to positively. Individuals are able to engage in more self disclosure than an average interaction, because a person can share their inner thoughts, feelings and beliefs and be met with less disapproval and fewer sanctions online than is the case in face-to-face encounters. Researcher Cooper, termed this type of relationship as a "Triple A Engine" implying that internet relationships are accessible, affordable, and anonymous.
Online, barriers that might stand in the way of relationship such as physical attractiveness, social anxiety and stuttering do not exist. Whereas those could hinder an individual in face-to-face encounters, an Internet interaction negates this and allows the individual freedom. Research has shown that stigmas such as these can make a large impact on first impressions in face-to-face meeting, and this does not apply with an online relationship. Furthermore, as the internet has become a worldwide phenomenon, many people can interact with others around the world, or find someone who fits their radar or their type, if there is no one who they find physically or emotionally attractive in their own area. The internet allows for interaction of many different people so there is greater chance of finding someone more attractive. The Internet "enhances face-to-face and telephone communication as network members become more aware of each others' needs and stimulate their relationships through more frequent contact".
According to Joseph Walter's Social Information Processing Theory, computer mediated communications can work for people. While online interactions take roughly four times longer than face to face interactions, this gives users the opportunity to evaluate and the time to think, making sure they say the perfect response. Thus, chronemics is the only verbal clue available to digital communications. With the focus on conversation and not appearance, overtime digital interactions will develop higher levels of intimacy than face-to-face interactions.
Disadvantages.
The Internet provides the opportunity for misrepresentation, particularly in the early stages of a relationship when commitment is low, and self-presentation and enhancement agendas are paramount. After receiving many complaints about his social networking site Ashley Madison, founder Noel Biderman responded to accusations that his and other similar cyber-dating sites are at fault for the" rising divorce rates and growth in casual dating". Biderman argued that the idea for Ashleymadison.com came to him when he realized the growing number of people on "mainstream dating sites" were married or in a relationship but posing as singles in order to start an affair.
In an empirical study of commitment and misrepresentation on the internet Cornwell and Lundgren (2001) surveyed 80 chat-room users. Half about their 'realspace' relationships, and half about their cyberspace relationships. They found that 'realspace' relationships were considered to be more serious, with greater feelings of commitment, than the cyber-relationship participants. Both groups, however, reported similar levels of satisfaction and potential for 'emotional growth' with regard to romantic relationships. Cornwell and Lundgren went on to ask about whether the participants had misrepresented themselves to their partner in a number of areas: their interests (e.g. hobbies, musical tastes); their age; their background; their appearance and 'mis-presentation of yourself in any other way' (p. 203). Participants responded using either yes or no to each question, and their score was summed into a misrepresentation measure. The results can be found below:
Dangers of Internet relationships.
An often forgotten aspect on online interactions is the possible danger present. The option for an individual to conceal their identity may be harmless in many cases, but it can also lead to extremely dangerous situations. Hidden identities are often used in cases of Cyber-bullying and Cyberstalking. Concealing your true identity is also a technique that can be used to manipulate your new online friend or lover into convincing them you are someone completely different. This is something most online predators do in order to prey on victims. Despite the awareness of dangers, Mishna et al. found children and youth to still partake in online relationships with little care or concern for negative effects. Brym and Lenton also claim that "although true identities are usually concealed, they sometimes decide to meet and interact in real life". From this dangers, people seriously have considered a kind of policy forcing people to use their real name only and open their personal information. By doing this, people are not going to do harmful to others because their information can be checked by others.
Engaging in internet relationships is also risky because the information placed online about an individual does not have to be accurate. An individual can formulate an entirely different persona and pose as this person as long as they desire. This can be hurtful to individuals who are honest about their identities and believe that they are in a positive relationship or friendship with the individual. This concept has been most recently illustrated on the television show, "".
Internet affairs.
Internet affairs offer a new perspective on the definition of an affair. Some people consider internet relationships to be classified as an affair while others claim contact affairs are much more serious. Trent Parker and Karen Wampler conducted a qualitative study to discover the different perceptions of internet relationships based on gender differences. Through their study they found internet affairs were considered less of an affair than a physical relationship. Through the results from the same study Parker and Wampler also concluded that women considered sexual internet activities such as internet porn much more severe than the men did. Internet affairs and physical contact affairs are similar because they both involve another partner. "The primary difference between an internet affair and an affair is that in an affair, the couple meet to engage in the relationship. With internet affairs, on the other hand, the couple rarely meet. This offers a unique advantage to internet affairs."
Effects on face-to-face interactions.
Since the creation of the Internet, communication has become one of it is prime uses. It has become a ubiquitous force in people’s everyday lives due to the increase in the regularity and quality of interaction. The internet has also created a new approach to human relationships, and it has changed the way people connect to one another in their social worlds. Online relationships have also changed which effective strategies we use to perform maintenance on our relationships, depending on the exclusivity of the internet the relationship. In the past, postal services made communication possible without the necessity of physical presence, and the invention of the telephone allowed synchronous communication between people across long distances. The internet combined the advantages of both mail and telephone, unifying the speed of the telephone with the written character of the mail service. The evolution of communication within the Internet has arguably changed the nature of individuals' relationships with one another. Some see a major negative impact resulting in an increased use of internet communication is of its diversion of true community because online interaction via computers is often regarded as a more impersonal communication medium than face- to- face communication. Others consider the incorporation of the internet allowing online activities to be "viewed as an extension of offline activities". The multiple techniques that humans use to communicate, such as taking turns or nodding in agreement, are absent in these settings. Without the body language cues present in a face-to-face conversation, such as pauses or gestures, participants in instant messaging may type over one another's messages without necessarily waiting for a cue to talk. Also, with or without the correct grammar, tone and context can be misunderstood. Recently people who already adapted internet-based communication have missed face-to-face interactions because this traditional way of communication is able to offer advancement in our relationships.
Early positive view.
In 1991, Stone argued that when virtual communities began forming, this process generated a new type of social space where people could still apparently meet face-to-face, but this required a re-definition of the terms “meet” and “face-to-face.” These virtual communities allowed people to effortlessly access others, and in many ways to feel better connected, feel that they receive greater support from others, and to obtain emotional satisfaction from their families, communities and society. However, it does have several obvious problems for people to communicate with others. The representative limitation of this way of communications is that it cannot contain people's diverse emotions completely, so it can cause diverse misunderstanding between people.
Pseudocommunity theory.
In 1987, this understanding of social spaces was challenged by scholars such as James R. Beniger. Beniger questioned whether these virtual communities were “real” or were pseudo communities, “a pattern relating that, while looking highly interpersonal interaction, is essentially impersonal.” He put forward the idea that in a society within the virtual world, participants lack the necessary honesty it would take to create a “real” virtual community.
Weakening of social ties.
In many cases the introduction of the Internet as a social instigator may cause a repercussion leading to a weakening of social ties. In a study conducted in 1998, Robert Kraut et al. discovered that Internet users were becoming less socially involved. They linked this to an increase in loneliness and depression in relation to use of the Internet. Though these findings may have been sound, in a later study, Kraut et al. revisited his original study with the idea of expanding his current initial sample and correlating it with new subsequently collected longitudinal data. This synthesis produced a different outcome than the one that Kraut had originally presented.
In this newer paper, Kraut stated that there were fewer negative affects than he had originally found, and in some cases the negative effect had vanished. In the second study he saw that small positive effects began to appear in social involvement and psychological well-being. Assessing the effect of the Internet over a period of time, he saw people’s use of the Internet increase in sophistication.
During the Kraut et al. study, the researchers asked reclusive people if they use the Internet to counteract the loss of social skills that are needed in face-to-face encounters. They also asked people with strong social skills whether they use the Internet to amplify their abilities to network amongst people. The study discovered that these people who already possessed strong social skills were the ones who received the most beneficial outcome to using the Internet. The concluding analysis was, that rather than helping to decrease the difference between those who already had social skills compared with those lacking in social skills, internet use had actually exacerbated the differences in the skill level needed for social interaction.
Assisting reclusive people.
This theory was later challenged in a study, by McKenna et al., that indicated that people who are more socially inept use the internet to create an initial contact which allows them to explore their “true self" within these interactions. These social interactions within cyberspace tend to lead to closer and high quality relationships which influence face-to-face encounters. In essence, these findings meant that although it is not clear whether the internet helps reclusive people develop better social skills, it does allow reclusive people to form relationships that may not have existed otherwise because of their lack of comfort with interpersonal situations in general. When these relationships emerge into face-to-face relationships it is hard to distinguish these relationships from those that started as face-to-face interactions. Future studies on this topic may allow scholars to define whether or not society is becoming too dependent on the Internet as a social tool. Those relationships are also found for people suffering from depression, suicidal ideation and other mental health problems. For example, suicidal people were more likely to go online in search of new interpersonal relationships and to seek interpersonal help. Similar findings were found for suicidal LGBT. These studies show that people who have trouble meeting similar others, not only the 'socially inept', are using the internet to create stronger and more extensive interpersonal relationships.

</doc>
<doc id="50376" url="https://en.wikipedia.org/wiki?curid=50376" title="Dragonflight">
Dragonflight

Dragonflight is a science fiction novel by the American-Irish author Anne McCaffrey. It is the first book in the "Dragonriders of Pern" series. "Dragonflight" was first published by Ballantine Books in July 1968. It is a fix-up of novellas, including two which made McCaffrey the first woman to win a Hugo or Nebula Award.
In 1987, "Locus: The magazine of the science fiction & fantasy field" ranked "Dragonflight" at number nine among the 33 "All-Time Best Fantasy Novels", based on a poll of subscribers.
Origins.
Two components of "Dragonflight" were award-winning novellas published by "Analog" science fiction magazine. The first segment, "Weyr Search", illustrated by John Schoenherr, had been the cover story for the October 1967 issue. The second segment, "Dragonrider", appeared in two parts, beginning in December 1967.
"Weyr Search" features a young woman named Lessa being recruited to establish a telepathic bond with a queen dragon at its hatching, thus becoming a dragonrider, and the leader of a Weyr community on the fictional planet Pern. "Dragonrider" features the growth of Lessa's queen dragon, Ramoth, and their training together. "Analog" editor John W. Campbell asked "to see dragons fighting Thread", Pern's menace from space, and he also suggested time travel. In response, McCaffrey wrote a third story titled "Crack Dust, Black Dust", which was not published separately, but provided crucial material for the novel.
Plot introduction.
"Dragonflight" takes place in the far future on Pern, a planet colonized by humans. The colonists had originally intended to gradually adopt a low-technology agrarian lifestyle, but were forced to move more quickly after they encountered the deadly Thread raining down from the sky. By harnessing and riding the indigenous, flying, fire-breathing dragons (with genetic alterations to make them larger and telepathic), the colonists destroyed the Thread in the skies over Pern, creating pockets of safety over its surface, before it was able to burrow into the land and breed. Humanity finally managed to find equilibrium and began to create a thriving culture, society, and economy, eventually expanding right across Pern's northern continent. However, when this narrative begins, an unusually long interval between Thread attacks has caused the general population to dismiss the threat as myth and gradually withdraw support from the Weyrs where dragons are bred and trained. By the time of this narrative, only one Weyr remains (the other five having mysteriously disappeared at the same time in the last quiet interval), maintaining a precarious existence.
Dragons are telepathic and are capable of forming a lifelong bond with one particular human in a process called "Impression". Tradition, established thousands of years before the narrative, dictates that selected young humans with empathetic and telepathic talents are taken to the Hatching Grounds as candidates for Impression. The dragons come in several colors which generally correlate with their sizes; blue males, green females, brown males, bronze males, and golden females – queens. Bronzes, the largest males, are by tradition the only ones who compete to win the queens in their mating flights. The green females are banned from breeding as they produce only small, less talented dragons. The golden queens are not only the largest dragons, they also hold a subtle control over their dragon communities Weyrs.
Plot summary.
"Dragonflight" chronicles the story of Lessa, the sole survivor of the noble ruling family of Ruatha Hold on the northern continent of Pern. When the rest of her family is killed by a cruel usurper, Fax, she survives by disguising herself as a drudge (a menial servant), partly through simply adopting a slovenly appearance, but also by using her hereditary telepathic abilities to make others see her as far older and less attractive than she actually is. Her only friend is a watch-wher, a somewhat telepathic animal related to dragons, that guards the Hold. Lessa psychically influences other Hold workers to do less than their best work, or to become clumsy or inefficient, in order to sabotage Ruatha as part of her strategy to make it economically unproductive, so that Fax will renounce it and she can retake her Hold.
F'lar, wingleader at Benden Weyr, and rider of the bronze dragon Mnementh, finds Lessa while searching for candidates to impress a new queen dragon. The current queen has a batch of eggs due to hatch shortly, including a crucial golden egg. After killing Fax in single combat, following the rules of the Pernese code duello, he realises that she manipulated him emotionally to kill Fax and engineered Fax's renouncement. F'lar recognizes that Lessa possesses both unusually strong psychic abilities and great strength of will. He recognizes her potential to be the strongest Weyrwoman in recent history, and the path to his own leadership at Benden Weyr. F'lar convinces a reluctant Lessa to give up her birthright as Lord Holder of Ruatha Hold for the larger domain of the dragonweyr and she agree to pass the title on to Fax's newborn son (who later features in "The White Dragon"). F'lar takes Lessa to Benden Weyr, where she Impresses the queen hatchling Ramoth and becomes the Weyrwoman, the new co-leader of the last active Weyr. On Ramoth's first mating flight, Mnementh catches her, and by Weyr tradition, this makes F'lar the Weyrleader.
One Weyr by itself is not enough to defend the planet; there had been six, but the other five Weyrs are now empty, deserted since the last Pass centuries before. In a desperate attempt to increase their numbers, a new queen, Prideth, and her rider, Kylara, are sent back "between" times (a recently rediscovered skill) ten turns, to allow Prideth time to mature and reproduce. Lessa travels four hundred turns into the past to bring the five 'missing' Weyrs forward to her present. This is a huge strain for both her and Ramoth. She convinces the dragonriders of the five Weyrs to go with her to their future, and they use the Red Star as a guide to make smaller, less strenuous hops forward in time. This not only provides much needed skilled reinforcements in the battle against Thread, but explains how and why the five Weyrs were abandoned: they came forward in time.
Awards.
"Dragonflight" includes the novellas "Weyr Search", which won the 1968 Hugo Award for Best Novella (voted by members of the annual World Science Fiction Convention) and "Dragonrider" which won the Nebula Award for Best Novella (voted annually by the Science Fiction Writers of America) in 1969. McCaffrey was the first woman to win either award.
In 1999, the American Library Association cited the two early Pern trilogies ("Dragonriders" and "Harper Hall"), along with "The Ship Who Sang", when McCaffrey received the annual Margaret A. Edwards Award for her lifetime contribution in writing for teens.
References.
Citations – books
Web sites

</doc>
<doc id="50378" url="https://en.wikipedia.org/wiki?curid=50378" title="High-speed rail">
High-speed rail

High-speed rail is a type of rail transport that operates significantly faster than traditional rail traffic, using an integrated system of specialized rolling stock and dedicated tracks. While there is no single standard that applies worldwide, new lines in excess of 250 km/h and existing lines in excess of 200 km/h are widely considered to be high-speed, with some extending the definition to include much lower speeds (e.g. 160 km/h) in areas for which these speeds still represent significant improvements. The first such system began operations in Japan in 1964 and was widely known as the bullet train. High-speed trains normally operate on standard gauge tracks of continuously welded rail on grade-separated right-of-way that incorporates a large turning radius in its design.
Many countries have developed high-speed rail to connect major cities, including Austria, Belgium, China, France, Germany, Italy, Japan, Poland, Portugal, Russia, South Korea, Spain, Sweden, Taiwan, Turkey, United Kingdom, United States and Uzbekistan. Only in Europe does HSR cross international borders. China has 19,000 km of HSR as of December 2015, accounting for two thirds of the world's total.
While high-speed rail is most often designed for passenger travel, some high-speed systems also offer freight service.
Definitions.
Multiple definitions for high-speed rail are in use worldwide.
The European Union Directive 96/48/EC, Annex 1 defines high-speed rail in terms of:
The International Union of Railways (UIC) prefers to use "definitions" (plural) because they consider that there is no single standard definition of high-speed rail, nor even standard usage of the terms ("high speed", or "very high speed"). They make use of the European EC Directive 96/48, stating that high speed is a combination of all the elements which constitute the system: infrastructure, rolling stock and operating conditions.
The International Union of Railways states that high-speed rail is a set of unique features, not merely a train travelling above a particular speed. Many conventionally hauled trains are able to reach in commercial service but are not considered to be high-speed trains. These include the French SNCF Intercités and German DB IC.
National domestic standards may vary from the international ones.
History.
Railways were the first form of rapid land transportation and had an effective monopoly on passenger traffic until the development of the motor car and airliners in the early-mid 20th century. Speed had always been an important factor for railroads and they constantly tried to achieve higher speeds and decrease journey times. Rail transportation in the late 19th Century was not much slower than non high-speed trains today and many railroads regularly operated relatively fast "express" trains which averaged speeds of around .
Early research.
First experiments.
High-speed rail development began in Germany in 1899 when the Prussian state railway joined with ten electrical and engineering firms and electrified of military owned railway between Marienfelde and Zossen. The line used three-phase current at 10 kilovolts and 45 Hz.
The Van der Zypen & Charlier company of Deutz, Cologne built two railcars, one fitted with electrical equipment from Siemens-Halske, the second with equipment from "Allgemeine Elektricitäts-Gesellschaft" (AEG), that were tested on the Marienfelde–Zossen line during 1902 and 1903.
On 23 October 1903, the S&H-equipped railcar achieved a speed of and on 27 October the AEG-equipped railcar achieved . These trains demonstrated the feasibility of electric high-speed rail; however, regularly scheduled electric high-speed rail travel was still more than 30 years away.
The high-speed dream.
After the breakthrough of electric railroads, it was clearly the infrastructure – especially the cost of it – which hampered the introduction of high-speed rail. Several disasters happened – derailments, head-on collisions on single-track lines, collisions with road traffic at grade crossings, etc. The physical laws were well-known, i.e. if the speed was doubled, the curve radius should be quadrupled; the same was true for the acceleration and braking distances. 
In 1891 the engineer Károly Zipernowsky proposed a high-speed line Vienna–Budapest, bound for electric railcars at . In 1893 Dr. Wellington Adams proposed an air-line from Chicago to St. Louis of . At a speed of only , he was more modest than Zipernowsky – and more realistic, according to General Electric.
Probably, the first high-speed line was the Marienfelde–Zossen military railway, owned by the Prussian State Railway, and from 1899 on used to test electric railcars. Alexander C. Miller had greater ambitions. In 1906, he launched the "Chicago-New York Electric Air Line Railroad" project to reduce the running time between the two big cities to ten hours by using electric locomotives. After seven years of effort, however, less than of arrow-straight track was finished. A part of the line is still used as one of USA's last interurbans.
The high-speed interurbans.
In the USA, some of the interurbans (i.e. trams or streetcars which run from city to city) of the early 20th century were really high-speed for their time (also Europe had and still does have some interurbans). Several high-speed rail technologicies have their origin on the interurban scene.
In 1903 – 30 years before the conventional railways started to streamline their trains – the officials of the Louisiana Purchase Exposition organized the Electric Railway Test Commission to conduct a series of tests to develop a carbody design that would reduce wind resistance at high speeds. A long series of tests was carried. In 1905, St. Louis Car Company built a railcar for the traction magnate Henry Huntington, capable of speeds approaching 100 mph. Once it ran 20 miles (32 km) between Los Angeles and Long Beach in 15 minutes, an average speed of 80 mph (almost 130 km/h). But it was too heavy for much of the tracks. Instead, Cincinnati Car Company, J.G.Brill and others pioneered lightweight constructions, use of aluminium alloys, and low-level bogies which could operate smoothly at extremely high speeds on rough interurban tracks. Westinghouse and General Electric designed motors compact enough to be mounted on the bogies. From 1930 on, the Red Devils from Cincinnati Car Company and a some other interurban railcars reached about 145 km/h (90 mph) in commercial traffic. The Red Devils weighed only 22 tons though they could seat 44 passengers.
Extensive wind tunnel research – the first in the railway industry – was done before J.G.Brill in 1931 built the Bullet cars for Philadelphia and Western Railroad (P&W). They were capable to run at 92 mph. Some of them were almost 60 years in service. P&W's Norristown High Speed Line is still in use, almost 110 years after P&W in 1907 opened their double-track Upper Darby–Strafford line without a single grade crossing with roads or other railways. The entire line was governed by an absolute block signal system.
Early German high-speed network.
On May 15, 1933, the Deutsche Reichsbahn-Gesellschaft company introduced the diesel-powered "Fliegender Hamburger" in regular service between Hamburg and Berlin (286 km), thereby achieving a new top speed for a regular service, with a top speed of . This train was a streamlined multi-powered unit, albeit diesel, and used Jakobs bogies.
Following the success of the Hamburg line, the steam-powered Henschel-Wegmann Train was developed and introduced in June 1936 for service from Berlin to Dresden, with a regular top speed of . Incidentally no train service since the cancelation of this express train in 1939 has traveled between the two cities in a faster time .
Further development allowed the usage of these "Fliegenden Züge" (flying trains) on a rail network across Germany.
The "Diesel-Schnelltriebwagen-Netz" had been in the planning since 1934 but it never reached its envisaged size.
All high-speed service stopped in August 1939 shortly before the outbreak of World War II.
The American Streamliners.
On 26 May 1934, one year after Fliegender Hamburger introduction, the Burlington Railroad set an average speed record on long distance with their new streamlined train, the Zephyr, at with peaks at . The Zephyr was made of stainless steel, and like the Fliegender Hamburger, was diesel powered, articulated with Jacobs bogies, and could reach as commercial speed.
The new service was inaugurated 11 November 1934, traveling between Kansas City and Lincoln, but at a lower speed than the record, on average speed .
In 1935, the Milwaukee Road introduced the Morning Hiawatha service, hauled at by steam locomotives. These were the last "high-speed" trains to use steam power. In 1936, the Twin Cities Zephyr entered service, from Chicago to Minneapolis, with an average speed of .
Many of these streamliners posted travel times comparable to or even better than their modern Amtrak successors, which are limited to 79 mph top speed on most of the network.
The Italian electric and the last steam record.
The German high-speed service was followed in Italy in 1938 with an electric-multiple-unit ETR 200, designed for , between Bologna and Naples. It too reached in commercial service, and achieved a world mean speed record of near Milan in 1938.
In Great Britain in the same year, the streamlined steam locomotive "Mallard" achieved the official world speed record for steam locomotives at . The external combustion engines and boilers on steam locomotives were large, heavy and time and labor-intensive to maintain, and the days of steam for high speed were numbered.
The introduction of the Talgo system.
In 1945, a Spanish engineer, Alejandro Goicoechea, developed a streamlined articulated train able to run on existing tracks at higher speeds than contemporary passenger trains. This was achieved by providing the locomotive and cars with a unique axle system that used one axle set per car end, connected by a Y-bar coupler. Amongst other advantages, the centre of mass was only half as high as usual. This system became famous under the name of Talgo (Tren Articulado Ligero Goicoechea Oriol), and for half a century was the main Spanish provider of high-speed trains.
The first above 300km/h developments.
In the early 1950s, the French National Railway started to receive their new powerful CC 7100 electric locomotives, and began to study and evaluate running at higher speeds. In 1954, the CC 7121 hauling a full train achieved a record during a test on standard track. The next year, two specially tuned electric locomotives, the CC 7107 and the prototype BB 9004, broke previous speed records, reaching respectively and , again on standard track. For the first time, was surpassed, allowing the idea of higher speed services to be developed and further engineering studies commenced. Especially, during the 1955 records, a dangerous hunting oscillation, the swaying of the bogies which leads to dynamic instability and potential derailment was discovered. This problem was solved by "yaw dampers" which enabled safe running at high speeds today. Research was also made about "current harnessing" at high-speed by the pantographs, that was solved 20 years later by the Zébulon TGV's prototype.
Breakthrough: The Shinkansen.
Japanese research and development.
With some 45 million people living in the densely populated Tokyo-to-Osaka corridor, congestion on road and rail became a serious problem after World War II, and the Japanese government began thinking seriously about a new high-speed rail service.
Japan in the 1950s was a populous, resource-limited nation that for security reasons did not want to import petroleum, but needed a way to transport its millions of people in and between cities.
Japanese National Railways (JNR) engineers then began to study the development of a high-speed regular mass transit service. In 1955, they were present at the Lille's Electrotechnology Congress in France, and during a 6-month visit, the head engineer of JNR accompanied the deputy director Marcel Tessier at the DETE (SNCF Electric traction study department). JNR engineers returned to Japan with a number of ideas and technologies they would use on their future trains, including alternating current for rail traction, and international standard gauge.
The first narrow-gauge Japanese high-speed service.
In 1957, the engineers at the private Odakyu Electric Railway in Greater Tokyo area launched the Odakyu 3000 series SE EMU. This EMU set a world record for narrow gauge trains at , giving the Odakyu engineers confidence they could safely and reliably build even faster trains at standard gauge. The original Japanese railways generally used narrow gauge, but the increased stability offered by widening the rails to standard gauge would make very high-speed rail much simpler, and thus standard gauge was adopted for high-speed service. With the sole exceptions of Russia and India all high speed rail lines in the world are still standard gauge, even in countries where the preferred gauge for legacy lines is different.
A new train on a new line.
The new service, named Shinkansen (meaning "new trunk line") would provide a new alignment, 25% wider standard gauge, continuously welded rails between Tokyo and Osaka using new rolling stock, designed for . However, the World Bank, whilst supporting the project, considered the design of the equipment as unproven for that speed, and set the maximum speed to .
After initial feasibility tests, the plan was fast-tracked and construction of the first section of the line started on 20 April 1959. In 1963, on the new track, test runs hit a top speed of . Five years after the beginning of the construction work, in October 1964, just in time for the Olympic Games, the first modern high-speed rail, the Tōkaidō Shinkansen, was opened between the two cities.
The first Shinkansen trains, the 0 Series Shinkansen, built by Kawasaki Heavy Industries—in English often called "Bullet Trains", after the original Japanese name —outclassed the earlier fast trains in commercial service. They traversed the distance in 3 hours 10 minutes, reaching a top speed of and sustaining an average speed of with stops at Nagoya and Kyoto.
High-speed rail for the masses.
Speed was only a part of the Shinkansen revolution: the Shinkansen offered high-speed rail travel to the masses. The first "Bullet trains" had 12 cars and later versions had up to 16, and double-deck trains further increased the capacity.
After three years, more than 100 million passengers had used the trains, and the milestone of the first one billion passengers was reached in 1976. In 1972, the line was extended a further , and further construction has resulted in the network expanding to as of March 2015, with a further of extensions currently under construction and due to open in stages between March 2016 and 2035. The cumulative patronage on the entire system since 1964 is over 10 billion, the equivalent of approximately 140% of the world's population, without a single train passenger fatality. (Suicides, passengers falling off the platforms, and industrial accidents have resulted in fatalities).
Since their introduction, Japan's Shinkansen systems have been undergoing constant improvement, not only increasing line speeds. Over a dozen train models have been produced, addressing diverse issues such as tunnel boom noise, vibration, aerodynamic drag, lines with lower patronage ("Mini shinkansen"), earthquake and typhoon safety, braking distance, problems due to snow, and energy consumption (newer trains are twice as energy efficient as the initial ones despite greater speeds).
Revival in Europe and North America.
First demonstrations at 200 km/h.
In Europe, high-speed rail began during the International Transport Fair in Munich in June 1965, when Dr. Öpfering, the director of Deutsche Bundesbahn (German Federal Railways), performed 347 demonstrations at between Munich and Augsburg by DB Class 103 hauled trains. The same year the Aérotrain, a French hovercraft monorail train prototype, reached within days of operation.
"Le Capitole".
After the successful introduction of the Japanese Shinkansen in 1964, at , the German demonstrations up to in 1965, and the proof-of-concept jet-powered Aérotrain, SNCF still ran its fastest trains at only .
In 1966, French Infrastructure Minister Edgard Pisani consulted engineers and gave the French National Railways twelve months to raise speeds to . The classic line Paris–Toulouse was chosen, and fitted, to support rather than . Some improvements were set, notably the signals system, development of on board "in-cab" signalling system, and curve revision.
The next year, in May 1967, a regular service at was inaugurated by the TEE "Le Capitole" between Paris and Toulouse, with specially adapted SNCF Class BB 9200 locomotives hauling classic UIC cars, and a full red livery.
At the same time, the Aérotrain prototype 02 reached on a half-scale experimental track. In 1969, it achieved on the same track. On 5 March 1974, the full-scale commercial prototype Aérotrain I80HV, jet powered, reached .
American Metroliner trains at 200 km/h.
In the United States, immediately following the creation of Japan's first high-speed Shinkansen, President Lyndon B. Johnson as part of his Great Society infrastructure building initiatives asked the Congress to devise a way to increase speeds on the railroads. The congress delivered the High Speed Ground Transportation Act of 1965 which passed with overwhelming bi-partisan support and helped to create regular Metroliner service between New York City and Washington, D.C.. The new service was inaugurated in 1969, with top speeds of and averaging along the route. In a 1967 competition with a GE powered Metroliner on Penn Central's main-line, the United Aircraft Corporation TurboTrain set a record of .
Europe at 200 km/h.
In 1976, British Rail introduced a high-speed service able to reach using the InterCity 125 diesel-electric train sets under the brand name of High Speed Train (HST). It was the fastest diesel-powered train in regular service and it improved upon its forerunners in speed and acceleration. The train was as a reversible multi-car set having driving power-cars at both ends and a fixed formation of passenger cars between them. Journey times were reduced by an hour for example on the East Coast Main Line, and passenger numbers increased.
The next year, in 1977, Germany finally introduced a new service at , on the Munich-Augsburg line. That same year, Italy inaugurated the first European High-Speed line, the "Direttissima" between Roma and Florence, designed for , but used by FS E444 hauled train at . This year also saw the abandonment for political reasons of the Aérotrain project, in favour of the TGV.
Evolution in Europe.
France.
High-speed rail research.
Following the 1955 records, two divisions of the SNCF began to study high-speed services. In 1964, the DETMT (petrol-engine traction studies department of SNCF) investigated the use of gas turbines: a diesel-powered railcar was modified with a gas-turbine, and was called "TGV" (Turbotrain Grande Vitesse). It reached in 1967, and served as a basis for the future Turbotrain and the real TGV. At the same time, the new "SNCF Research Department", created in 1966, was studying various projects, including one code-named "C03: Railways possibilities on new infrastructure (tracks)".
In 1969, the "C03 project" was transferred to public administration while a contract with Alsthom was signed for the construction of two gas-turbine high-speed train prototypes, named "TGV 001". The prototype consisted of a set of five carriages, plus a power car at each end, both powered by two gas-turbine engines. The sets used Jacobs bogies, which reduce drag and increase safety.
In 1970, the DETMT's Turbotrain began operations on the Paris-Cherbourg line, and operated at despite being designed for usage at . It used gas-turbine powered multiple elements and was the basis for future experimentation with TGV services, including shuttle services and regular high rate schedules.
In 1971, the "C03" project, now known as "TGV Sud-Est", was validated by the government, against Bertin's Aerotrain. Until this date, there was a rivalry between the French Land Settlement Commission (DATAR), supporting the Aérotrain, and the SNCF and its ministry, supporting conventional rail. The "C03 project" included a new High-Speed line between Paris and Lyon, with new multi-engined trains running at . At that time, the classic Paris-Lyon line was already congested and a new line was required; this busy corridor, neither too short (where high speeds give limited reductions in end to end times) nor too long (where planes are faster in city center to city center travel time), was the best choice for the new service.
The 1973 oil crisis substantially increases oil prices. In the continuity of the De Gaulle "energy self-sufficiency" and nuclear-energy policy, a ministry decision switched the future TGV from now costly gas-turbine to full electric energy in 1974. An electric railcar named "Zébulon" was developed for testing at very high speeds, reaching a speed of . It was used to develop pantographs capable of withstanding speeds of over .
A service above 250 km/h.
After intensive tests with the gas-turbine "TGV 001" prototype, and the electric "Zébulon", in 1977, the SNCF placed an order to the group Alstom–Francorail–MTE for 87 TGV Sud-Est trainsets.
They used the "TGV 001" concept, with a permanently coupled set of eight cars, sharing Jacobs bogies, and hauled by two electric power-cars, one at each end.
In 1981, the first section of the new Paris–Lyon High-Speed line was inaugurated, with a top speed (then soon after). Being able to use both dedicated high-speed and conventional lines, the TGV offered the ability to join every city in the country at shorter journey times. After the introduction of the TGV on some routes, air traffic on these routes decreased and some cases disappeared. The TGV set a publicised speed records in 1981 at , in 1990 at , and then in 2007 at .
Germany.
Following the French TGV, in 1991 Germany was the second country in Europe to inaugurate a high-speed rail service, with the launch of the Intercity-Express (ICE) on the new Hannover-Würzburg high-speed railway, operating at a top speed of . The German ICE train was similar to the TGV, with dedicated streamlined power cars at both ends, but a variable number of trailers between them. Unlike the TGV, the trailers had two conventional bogies per car, and could be uncoupled, allowing the train to be lengthened or shortened. This introduction was the result of ten years of study with the ICE-V prototype, originally called Intercity Experimental, which broke the world speed record in 1988, reaching .
Spain.
In 1992, just in time for the Barcelona Olympic Games and Seville Expo '92, the Madrid–Seville high-speed rail line opened in Spain with 25 kV AC electrification, and standard gauge, differing from all other Spanish lines which used Iberian gauge. This allowed the AVE rail service to begin operations using Class 100 train sets built by Alstom, directly derived in design from the French TGV trains. The service was very popular and development continued on high-speed rail in Spain.
In 2005, the Spanish Government announced an ambitious plan, (PEIT 2005–2020) envisioning that by 2020, 90 percent of the population would live within of a station served by AVE. Spain began building the largest HSR network in Europe: , five of the new lines have opened (Madrid-Zaragoza-Lleida-Tarragona-Barcelona, Córdoba- Malaga, Madrid-Toledo, Madrid-Segovia-Valladolid, Madrid-Cuenca-Valencia) and another were under construction.
Evolution in North America.
In 1992, the United States Congress authorized the Amtrak Authorization and Development Act to focus on Amtrak's service improvement on the segment between Boston and New York City of the Northeast Corridor. The primary objectives were to electrify the line north of New Haven, Connecticut and replace the then 30-year-old Metroliners with new trains to achieve shorter travel time.
Amtrak started testing two trains, the Swedish X2000 and the German ICE 1, in the same year along its fully electrified segment between New York City and Washington DC. The officials favored the X2000 as it had a tilting mechanism. However, the Swedish manufacturer never bid on the contract as the burdensome United States railroad regulations required them to heavily modify the train resulting in added weight, among other things. Eventually, a custom-made tilting train derived from TGV, manufactured by Alstom and Bombardier, won the contract and was put into service in December 2000.
The new service was named "Acela Express" and linked Boston, New York City, Philadelphia, Baltimore, and Washington DC. The service did not meet the 3-hour travel time objective, between Boston and New York City. The time was 3 hours and 24 minutes as it partially ran on regular lines, limiting its average speed, with a maximum speed of being reached on a small section of its route through Rhode Island and Massachusetts.
Expansion in East Asia.
For four decades from its opening in 1964, the Japanese Shinkansen was the only high-speed rail service outside of Europe. In the 2000s a number of new high-speed rail services started operating in East Asia.
The South Korean KTX.
In South Korea, Korea Train Express (KTX) services were launched on 1 April 2004, on the Seoul-Busan corridor, Korea's busiest traffic corridor, between the two largest cities. In 1982, it represented 65.8% of South Korea's population, a number that grew to 73.3% by 1995, along with 70% of freight traffic and 66% of passenger traffic. With both the Gyeongbu Expressway and Korail's Gyeongbu Line congested as of the late 1970s, the government saw the pressing need for another form of transportation.
Construction began on the high-speed line from Seoul to Busan in 1992 with the first commercial service launching in 2004. Top speed for trains in regular service is currently , though the infrastructure is designed for . The initial rolling stock was based on Alstom's TGV Réseau, and was partly built in Korea. The domestically developed HSR-350x, which achieved in tests, resulted in a second type of high-speed trains now operated by Korail, the KTX Sancheon. The next generation KTX train, HEMU-430X, achieved in 2013, making South Korea the world's fourth country after France, Japan, and China to develop a high-speed train running on conventional rail above .
The Chinese CRH.
High-speed rail was introduced to China only in the last 20 years but has rapidly developed into the world's most extensive network. As of December 2013, the country had of track in operation, accounting for about half of the world's total at the time. In 2013, high-speed trains carried 530 million passenger-trips, about one quarter of all train trips taken in China that year. By the end of 2014, China had 16,000 km of high-speed rail lines, accounting for 60% of the world's total. By the end of 2015, the total has risen to 19,000 km.
State planning for high-speed railway began in the early 1990s, and the country's first high-speed rail line, the Qinhuangdao–Shenyang Passenger Railway, was built in 1999 and opened to commercial operation in 2003. This line could accommodate commercial trains running at up to . Planners also considered Germany's Transrapid maglev technology and built the Shanghai Maglev Train, which runs on a track linking the city and its international airport. The maglev train service began operating in 2004 with trains reaching a top speed of , and remains the fastest high-speed service in the world. Maglev, however, was not adopted nationally and all subsequent expansion features high-speed rail on conventional tracks.
In the 1990s, China's domestic train production industry designed and produced a series of high-speed train prototypes but few were used in commercial operation and none were mass-produced. The Chinese Ministry of Railways (MOR) then arranged for the purchase of foreign high-speed trains from French, German, and Japanese manufacturers along with certain technology transfers and joint ventures with domestic trainmakers. In 2007, the MOR introduced the China Railways High-speed (CRH) service, also known as "Harmony Trains" (和谐号), using trains with foreign technology.
In 2008, high-speed trains began running at a top speed of on the passenger dedicated line between Beijing and Tianjin, which opened during the 2008 Beijing Summer Olympic Games. The following year, trains on the newly opened Wuhan–Guangzhou High-Speed Railway set a world record for average speed over an entire trip, at 312.5 km/h (194.2 mph) over 968 km (601 mi.). In July 2011, however, top train speeds were lowered to .
A collision of high-speed trains on 23 July 2011 in eastern China, which killed 40 and injured 195, raised concerns about operational safety. A credit crunch later that year slowed the construction of new lines. But by 2012, the high-speed rail boom had renewed with new lines and new rolling stock by domestic producers that had indigenized foreign technology. On 25 December 2012, China opened the world's longest high-speed rail line, which runs from the country's capital Beijing in the north to Shenzhen on the southern coast. The network is still rapidly expanding to create the 4+4 National High Speed Rail Grid by 2015.
Taiwan (THSR).
Taiwan High Speed Rail's first and only HSR line opened for service on 5 January 2007, using Japanese trains with a top speed of . The service traverses from Taipei Railway Station to Zuoying Station in as little as 96 minutes. Once THSR began operations, almost all passengers switched from airlines flying parallel routes while road traffic was also reduced.
Middle East/Central Asia.
Turkey.
In 2009, Turkey inaugurated a high-speed service between Ankara and Eskișehir. This has been followed up by an Ankara - Konya route, and the Eskișehir line has been extended to Istanbul (Asian part).
Uzbekistan.
Uzbekistan opened a slower Afrosiyob service from Tashkent to Samarkand beginning in 2011, which was upgraded in 2013 to an average operational speed of 161 km/h and peak speeds over 200 km/h.
Saudi Arabia.
Saudi Arabia plans to begin service on a high-speed line with the phased opening from Medina to King Abdullah Economic City in 2015 first, followed up with the rest of the line to Mecca the following year.
South Asia.
India.
Plans to introduce a High speed rail system in India have been proposed since the 1980s. Initial implementation started in 2009 when the Ministry of Railways submitted its "Vision 2020" proposal to the Parliament discussing the various routes to be built. A new corporation High Speed Rail corporation of India(HSRC) has been set up in 2013 to handle all efforts related to the building of a HSR network. The contract to build the railways has been given to Japan .Construction of the first phase of the Mumbai-Ahmedabad line, is expected to begin in 2017 and be completed in 2023.
Africa.
Morocco.
In November 2007 by the Moroccan government decided to undertake the construction of a high-speed rail line between the economic capital Casablanca and Tangier, one of the largest harbour city on the Strait of Gibraltar. The line will also serve Rabat the capital and Kenitra The first section of the line, Kenitra–Tangier high-speed rail line should be completed in 2018.
Network.
Technologies.
Continuous welded rail is generally used to reduce track vibrations and misalignment. Almost all high-speed lines are electrically driven via overhead cables, have in-cab signalling, and use advanced switches using very low entry and frog angles.
Road-rail parallel layout.
Road Rail Parallel Layout uses land beside highways for railway lines. Examples include Paris/Lyon and Köln - Frankfurt in which 15% and 70% of the track runs beside highways, respectively.
Track sharing.
In China, high-speed lines at speeds between may carry freight or passengers. Lines operating at speeds of are used only by passenger CRH trains.
Rolling stock.
Key technologies include tilting trainsets, aerodynamic designs (to reduce drag, lift, and noise), air brakes, regenerative braking, engine technology and dynamic weight shifting.
Comparison with other modes of transport.
Optimal distance.
While commercial high-speed trains have lower maximum speeds than jet aircraft, they offer shorter total trip times than air travel for short distances. They typically connect city centre rail stations to each other, while air transport connects airports that are typically farther from city centres.
High-speed rail (HSR) is best suited for journeys of 1 to 4½ hours (about ), for which the train can beat air and car trip time. For trips under about , the process of checking in and going through airport security, as well as traveling to and from the airport, makes the total air journey time equal to or slower than HSR. European authorities treat HSR as competitive with passenger air for HSR trips under 4½ hours.
HSR eliminated most air transport from between Paris-Lyon, Paris-Brussels, Cologne-Frankfurt, Madrid-Barcelona, Nanjing-Wuhan, Chongqing-Chengdu, Tokyo-Nagoya, Tokyo-Sendai and Tokyo-Niigata.
China Southern Airlines, China's largest airline, expects the construction of China's high-speed railway network to impact (through increased competition and falling revenues) 25% of its route network in the coming years.
Market shares.
European data indicate that air traffic is more sensitive than road traffic (car and bus) to competition from HSR, at least on journeys of and more – perhaps because cars and buses are far more flexible than planes, though cost certainly plays a part as well. TGV Sud-Est reduced the travel time Paris–Lyon from almost four to about two hours. Market share rose from 40 to 72%. Air and road market shares shrunk from 31 to 7% and from 29 to 21%, respectively. On the Madrid–Sevilla link, the AVE connection increased share from 16 to 52%; air traffic shrunk from 40 to 13%; road traffic from 44 to 36%, hence the rail market amounted to 80% of combined rail and air traffic. This figure increased to 89% in 2009, according to Spanish rail operator RENFE.
According to Peter Jorritsma, the rail market share "s", as compared to planes, can be computed approximately as a function of the travelling time in minutes "t" by the formula
According to this formula, a journey time of three hours yields 65% market share. However, market shares are also influenced by ticket prices.
In another study conducted about Japan's High-speed rail service, they found a "4-hour wall" in High-speed rail's market share, which if the high speed rail journey time exceeded 4 hours, then people would likely choose planes over high-speed rail. For instance, from Tokyo to Osaka where high-speed rail take 2h22m, high-speed rail have 85% market share whereas planes have 15%. From Tokyo to Hiroshima which high-speed rail take 3h44m, high-speed rail have 67% market share whereas planes have 33%. The situation is the reverse on the Tokyo to Fukuoka route where high-speed rail takes 4h47m and rail only has 10% market share and planes 90%.
Energy efficiency.
Travel by rail is more competitive in areas of higher population density or where gasoline is expensive, because conventional trains are more fuel-efficient than cars when ridership is high, similar to other forms of mass transit. Very few high-speed trains consume diesel or other fossil fuels but the power stations that provide electric trains with power can consume fossil fuels. In Japan and France, with very extensive high-speed rail networks, a large proportion of electricity comes from nuclear power. On the Eurostar, which primarily runs off the French grid, emissions from travelling by train from London to Paris are 90% lower than by flying. Even using electricity generated from coal or oil, high-speed trains are significantly more fuel-efficient per passenger per kilometre traveled than the typical automobile because of economies of scale in generator technology and trains themselves, as well as lower air friction and rolling resistance at the same speed. Rail networks, like highways, require large fixed capital investments and thus require a blend of high density and government investment to be competitive against existing capital infrastructure.
Automobiles and buses.
High-speed rail can accommodate more passengers at far higher speeds than automobiles. Generally, the longer the journey, the better the time advantage of rail over road if going to the same destination. However, high-speed rail can be competitive with cars on shorter distances, , for example for commuting, especially if the car users do experience road congestion or expensive parking fees. In Norway, the Gardermobanen has made the rail market share for passengers from Oslo to the airport (42 km) rise to 51% in 2014, compared to 17% for buses and 28% for private cars and taxis. On such short relations, and relations with several stops, the trains' acceleration may be more important than the maximum speed.
Moreover, typical passenger rail carries 2.83 times as many passengers per hour per metre width as a road. A typical capacity is the Eurostar, which provides capacity for 12 trains per hour and 800 passengers per train, totaling 9,600 passengers per hour in each direction. By contrast, the Highway Capacity Manual gives a maximum capacity of 2,250 passenger cars per hour per lane, excluding other vehicles, assuming an average vehicle occupancy of 1.57 people. A standard twin track railway has a typical capacity 13% greater than a 6-lane highway (3 lanes each way), while requiring only 40% of the land (1.0/3.0 versus 2.5/7.5 hectares per kilometre of direct/indirect land consumption). The Tokaido Shinkansen line in Japan, has a much higher ratio (with as many as 20,000 passengers per hour per direction). Similarly commuter roads tend to carry fewer than 1.57 persons per vehicle (Washington State Department of Transportation, for instance, uses 1.2 persons per vehicle) during commute times.
Air travel.
Although air transit moves at higher speeds than even high-speed rail (except for vactrains, which can reach speeds even higher than passenger aircraft but which so far have not been implemented anywhere), its total time to destination can be increased by check-in, baggage handling, security and boarding. These procedures also add cost to air travel. Trains are preferred in shorter, mid range distances since rail stations are typically closer to urban centers than airports. Likewise, air travel needs longer distances to have a speed advantage after accounting for both processing time and travel to the airport.
Rail travel also requires less weather dependency than air travel. A well designed and operated rail system can only be affected by severe weather conditions, such as heavy snow, heavy fog, and major storm. Flights however, generally face cancellations or delays under less severe conditions. High-speed trains also have comfort advantages, since train passengers are allowed to move freely about the train at any point in the journey. The seats are also less subject to weight restrictions than on planes, and as such may have more padding and legroom. Technology advances such as continuously welded rail have minimized the vibration found on slower railways, while air travel remains affected by turbulence when adverse wind conditions arise. Trains can also accommodate intermediate stops at lower time and energetic costs than planes, though this applies less to HSR than to the slower conventional trains.
On particular busy air-routes - those that HSR has historically been most successful on - trains are also less prone to delays due to congested airports. A train that is late by a couple of minutes will not have to wait for another slot to open up, unlike airplanes at congested airports. Furthermore, many airlines see short haul flights as increasingly uneconomic and in some countries airlines rely on high-speed rail instead of short haul flights for connecting services.
Safety.
HSR is much simpler to control due to its predictable course. High-speed rail systems reduce (but do not eliminate) collisions with automobiles or people, by using non-grade level track and eliminating grade-level crossings. To date the only two deadly accidents involving a high speed train on high speed tracks were the 1998 Eschede train disaster and the 2011 Wenzhou train collision (in which speed was not a factor).
Accidents.
High-speed rail is one of the safest modes of transportation. The first high-speed rail network, the Japanese Shinkansen has not had any fatal accidents involving passengers since it began operating in 1964.
Notable major accidents involving high-speed trains include the following.
The 1998 Eschede accident.
In 1998, after over thirty years of high-speed rail operations worldwide without fatal accidents, the Eschede accident occurred in Germany: a poorly designed ICE 1 wheel broke at near Eschede, resulting in the derailment and destruction of almost the entire full set of 16 cars and the subsequent death toll of 101 people.
The 2011 Wenzhou accident.
On 23 July 2011, 13 years after the Eschede train accident, a Chinese CRH2 traveling at collided with a CRH1 which was stopped on a viaduct in the suburbs of Wenzhou, Zhejiang province, China. The two trains derailed, and four cars fell off the viaduct. 40 people were killed, at least 192 were injured, 12 of which were severe injuries.
The disaster led to a number of changes in management and exploitation of high-speed rail in China. Despite the fact that high speed was not a factor in the accident, one of the major changes was the lowering by of all maximum speeds in China HST, becoming 300, becoming 200, and becoming 160.
The 2013 Santiago de Compostela accident.
In July 2013, a high-speed train in Spain attempted to round a curve which had a speed limit of at , leading to 78 fatalities. Normally high-speed rail has automatic speed limiting restrictions, but this track section is a conventional section and in this case the automatic speed limit was said to be disabled by the driver several kilometers before the station. A few days later, the train worker's union claimed that the speed limiter didn't work properly because of lack of proper funding, acknowledging the budget cuts made by the current government. Two days after the accident, the driver was provisionally charged with homicide by negligence. This is the first accident that occurred with a Spanish high-speed train, but it occurred in a section that was not high speed.
The 2015 Eckwersheim accident.
On 14 November 2015, a specialized TGV EuroDuplex was performing commissioning tests on the unopened second phase of the LGV Est high-speed line, when it entered a curve, overturned, and struck the parapet of a bridge over the Marne–Rhine Canal. The rear power car came to a rest in the canal, while the remainder of the train came to a rest in the grassy median between the northern and southern tracks. Approximately 50 people were on board, consisting of SNCF technicians and reportedly some unauthorized guests. 11 were killed and 37 were injured. The train was performing tests at 10% above the planned speed limit for the line and should have slowed from to before entering the curve. Officials have indicated that excessive speed may have caused the accident.
Records.
Speed.
There are several definitions of "maximum speed":
Absolute speed record.
Conventional rail.
Since the 1955 record, France has nearly continuously held the absolute world speed record. The latest record is held by a SNCF TGV POS trainset, which reached in 2007, on the newly constructed LGV Est high-speed line. This run was for proof of concept and engineering, not to test normal passenger service.
Unlike the conventional records, the TGV records have been made by heavily tuned trains, modified from commercial service trains.
Unconventional rail.
The speed record for a pre-production unconventional passenger train was set by a seven-car L0 series manned maglev train at on 21 April 2015.
Maximum speed in service.
Top ten trains are:
Levitation trains.
The Shanghai Maglev Train reaches during its daily service on its dedicated line, holding the speed record for commercial train service. 
Conventional rail.
The fastest operating conventional trains are the French TGV POS, German ICE 3, and Japanese E5 and E6 Series Shinkansen with a maximum commercial speed of , the former two on some French high-speed lines, and the latter on a part of Tohoku Shinkansen line.
In Spain, on the Madrid–Barcelona HSL, maximum speed is .
Since July 2011, in China, the maximum speed is officially , but a tolerance is accepted, and trains often reach . Before that, from August 2008 to July 2011, China Railway High-speed trains held the highest commercial operating speed record with on some lines (Beijing–Tianjin Intercity Railway, Wuhan–Guangzhou High-Speed Railway).
Due to high costs and safety concerns the top speeds in China were reduced to on 1 July 2011.
Service distance.
The China Railways G/1276/7-G1278/5 Harbin-Wuhan train (2446 km, 14 ½ hours), which began service on December 10, 2014, became the longest high-speed rail service in the world. It overtook the G1202 Harbin-Shanghai (2421 km, 12 hours), which had set the previous record on December 28, 2012.
Markets.
The early target areas, identified by France, Japan, Spain, and the U.S., were between pairs of large cities. In France, this was Paris–Lyon, in Japan, Tokyo–Osaka, in Spain, Madrid–Seville (then Barcelona). In European countries, South Korea and Japan, dense networks of city subways and railways provide connections with high-speed rail lines.
Asia.
China.
China has the largest network of high-speed railways in the world and in 2015 it encompassed 19,000 km of high-speed rail or 60% of the world's total. According to Railway Gazette, the trains between Shijiazhuang and Zhengzhou East have the fastest average operating speed in the world at .
Japan.
In Japan, intra-city rail daily usage per capita is the highest, with cumulative ridership of 6 billion passengers (as of 2003).
Taiwan.
Taiwan High Speed Rail is a high-speed rail system that has only one line. It is approximately 345 km (214 mi) long, along the west coast of Taiwan from the national capital Taipei to the southern city of Kaohsiung. The construction was managed by Taiwan High Speed Rail Corporation and the total cost of the project was US$18 billion. The private company operates the line fully, and the system is based primarily on Japan's Shinkansen technology.
Eight initial stations were built during the construction of the High Speed Rail system: Taipei, Banqiao, Taoyuan, Hsinchu, Taichung, Chiayi, Tainan, and Kaohsiung.
South Korea.
Since its opening in 2004, KTX has transferred over 360 million passengers until April 2013, accounting to one South Korean using it seven times. For any transportation involving travel above , the KTX secured a market share of 57% over other modes of transport, which is by far the largest.
Russia.
Other target areas include freight lines, such as the Trans-Siberian Railway in Russia, which would allow 3 day Far East to Europe service for freight, potentially fitting in between the months by ship and hours by air.
Americas.
United States.
The United States has domestic definitions for high-speed rail varying between jurisdictions.
, the Northeast Corridor's Acela Express (operated by Amtrak) is the only high-speed rail line in operation in the United States, linking Boston, New York City, Philadelphia, and Washington, D.C.. The California High-Speed Rail project is planned to have its first operating segment, between Fresno and Bakersfield, in 2021. No other segment is expected to be in service before 2025.
Europe.
France.
Market segmentation has principally focused on the business travel market. The French original focus on business travelers is reflected by the early design of the TGV trains. Pleasure travel was a secondary market; now many of the French extensions connect with vacation beaches on the Atlantic and Mediterranean, as well as major amusement parks and also the ski resorts in France and Switzerland. Friday evenings are the peak time for TGVs ("train à grande vitesse"). The system lowered prices on long distance travel to compete more effectively with air services, and as a result some cities within an hour of Paris by TGV have become commuter communities, increasing the market while restructuring land use.
On the Paris – Lyon service, the number of passengers grew sufficiently to justify the introduction of double-decker coaches. Later high-speed rail lines, such as the LGV Atlantique, the LGV Est, and most high-speed lines in France, were designed as feeder routes branching into conventional rail lines, serving a larger number of medium-sized cities.
Germany.
Germany's first high-speed lines ran north-south, for historical reasons, and later developed east-west after German unification.
Italy.
During the 1920s and '30s, Italy was one of the first countries to develop the technology for high-speed rail. The country constructed the "Direttissime" railways connecting major cities on dedicated electrified high-speed track (although not as high-speed as would nowadays be called high-speed rail) and developed the fast ETR 200 trainset. After the Second World War and the fall of the fascist regime, interest in high-speed rail dwindled, with the successive governments considering it too costly and developing the tilting Pendolino, to run at medium-high speed (up to ) on conventional lines, instead. The only exception was the "Direttissima" between Florence and Rome, but it was not conceived to be part of a high-speed line on large scale.
A true dedicated high-speed rail network was developed during the 80s and the 1990s, and in 2010 of high-speed rail were fully operational. Frecciarossa services are operated with ETR 500 non-tilting trains at 25kVAC, 50 Hz power. The operational speed of the service is of . ETR1000 trainsets are currently under construction and were developed by the consortium formed by AnsaldoBreda and Bombardier. Based on the Bombardier Zefiro trainset, it will operate up to on the existing high-speed rail system.
Over 100 million passengers used the Frecciarossa from the service introduction and the first months of 2012. Italian high-speed services are recording profits, encouraging Trenitalia to plan major investments and to cede a large part of local and regional services to other operators ( like Nuovo Trasporto Viaggiatori and Trenord) and focusing efforts on high-speed and long-distance services (also through the medium-speed Frecciargento, Frecciabianca and InterCity services, which run on conventional lines).
Norway.
Norway's fastest trains have (2015) a commercial top speed of 210 km/h and the FLIRT trains may attain 200 km/h. A velocity of 210 km/h is permitted on the 42 km Gardermobanen which links the Gardermoen airport to Oslo and a part of the main line northwards to Trondheim.
Some parts of the trunk railways around Oslo are renewed and built for 250 km/h:
Spain.
Spain has built an extensive high-speed rail network, (2013), the largest in Europe. It uses standard gauge in opposite to the Iberian gauge used in the most of the national railway network, meaning that the high-speed railways are separated and has almost only high-speed trains, no local trains and no freight. This network is from 2013 connected to the French network, with direct trains Paris-Barcelona.
Switzerland.
High-speed north–south freight lines in Switzerland are under construction, avoiding slow mountainous truck traffic, and lowering labour costs. The new lines, in particular the Gotthard Base Tunnel, are built for . But the short high-speed parts and the mix with freight will lower the average speeds. The limited size of the country gives fairly short domestic travel times anyway.
Turkey.
The Turkish State Railways started building high-speed rail lines in 2003. The first section of the line, between Ankara and Eskișehir, was inaugurated on March 13, 2009. It is a part of the Istanbul to Ankara high-speed rail line. A subsidiary of Turkish State Railways, Yüksek Hızlı Tren is the sole commercial operator of high-speed trains in Turkey.
The construction of three separate high-speed lines from Ankara to Istanbul, Konya and Sivas, as well as taking an Ankara–İzmir line to the launch stage, form part of the Turkish Ministry of Transport's strategic aims and targets. Turkey plans to construct a network of high-speed lines in the early part of the 21st century, targeting a network of high-speed lines by 2013 and a network by the year 2023.
United Kingdom.
The UK's fastest high-speed line (HS-1) connects London St Pancras with Brussels and Paris through the Channel Tunnel. It is the only high-speed line in Britain with an operating speed of more than .
The Great Western Main Line, South Wales Main Line, West Coast Main Line, Midland Main Line, Cross Country Route and East Coast Main Line all have maximum speed limits of on all or part of the line. Attempts to increase speeds to on both the West Coast Main Line and East Coast Main Line have failed because the trains on those lines do not have cab signaling, which is a legal requirement in the UK for trains to be permitted to operate at speeds greater than due to the impracticality of observing lineside signals at such speeds.

</doc>
<doc id="50379" url="https://en.wikipedia.org/wiki?curid=50379" title="Illinois and Michigan Canal">
Illinois and Michigan Canal

The Illinois and Michigan Canal connected the Great Lakes to the Mississippi River and the Gulf of Mexico. It ran from the Chicago River in Bridgeport, Chicago to the Illinois River at LaSalle-Peru. The canal crossed the Chicago Portage and helped establish Chicago as the transportation hub of the United States, before the railroad era. It was opened in 1848, its function was largely replaced by the wider and shorter Chicago Sanitary and Ship Canal in 1900 and it ceased transportation operations with the completion of the Illinois Waterway in 1933.
Illinois and Michigan Canal Locks and Towpath, a collection of eight engineering structures and segments of the canal between Lockport and LaSalle-Peru, was designated a National Historic Landmark in 1964.
Portions of the canal have been filled in. Much of the former canal, near the Heritage Corridor transit line, has been preserved as part of the Illinois and Michigan Canal National Heritage Corridor.
Significance.
Canals were the highways of the day. The Illinois and Michigan Canal connected the Mississippi Basin to the Great Lakes Basin. The canal influenced Illinois's north border. The Erie Canal and the Illinois and Michigan Canal cemented cultural and trade ties to the Northeast rather than the South. Prior to the canal, farming in the region was limited to subsistence farming. The canal made agriculture in northern Illinois profitable, opening up connections to eastern markets. With the expansion of agriculture, the canal created the city of Chicago. Without the initial stimulus of the canal, Chicago would not have attracted the populations, railroads and the industry that it did.
History.
Conception.
The first known Europeans to travel the area, Father Marquette and Louis Joliet went through the Chicago Portage on their return trip. Joliet remarked that with a canal they could remove the need to portage and the French create an empire spanning the continent.
Years later, the territory was now American. With several slave states recently admitted to the Union, Nathaniel Pope and Ninian Edwards saw the opportunity to make Illinois a state. They proposed moving the border northward from the southern tip of Lake Michigan to allow the canal to be within a single state. They believed that the canal would firmly align Illinois with the free states and so Congress granted them statehood even though Illinois did not meet the population requirements.
Construction.
In 1824, Samuel D. Lockwood, one of the first commissioners of the canal, was given the authorization to hire contractors to survey a route for the canal to follow.
Construction on the canal began in 1836, although it was stopped for several years due to an Illinois state financial crisis related to the Panic of 1837. The Canal Commission had a grant of of federal land which it sold at $1.25 per acre (309 $/km²) to finance the construction. Still, money had to be borrowed from eastern U.S. and British investors to finish the canal.
Most of the canal work was done by Irish immigrants who previously worked on the Erie Canal. The work was considered dangerous and many workers died, although no official records exist to indicate how many. The Irish immigrants who toiled to build the canal were often derided as a sub-class and were treated very poorly by other citizens of the city.
The canal was finished in 1848 at a total cost of $6,170,226. Chicago Mayor James Hutchinson Woodworth presided over the opening ceremony. Pumps were used to draw water to fill the canal near Chicago, which was soon supplemented by water from the Calumet Feeder Canal. The feeder was supplied by water from the Calumet River and originated in Blue Island, Il. The DuPage River provided water farther south. In 1871 the canal was deepened to speed up the current and to improve sewage disposal.
Completion.
The canal was eventually wide and deep, with towpaths constructed along each edge to permit mules to be harnessed to tow barges along the canal. Towns were planned out along the path of the canal spaced at intervals corresponding to the length that the mules could haul the barges. It had seventeen locks and four aqueducts to cover the height difference between Lake Michigan and the Illinois River. From 1848 to 1852 the canal was a popular passenger route, but passenger service ended in 1853 with the opening of the Chicago, Rock Island and Pacific Railroad that ran parallel to the canal. The canal had its peak shipping year in 1882 and remained in use until 1933.
Experiencing a remarkable recovery from the devastating Great Chicago Fire of 1871, Chicago rebuilt rapidly along the shores of the Chicago River. The river was especially important to the development of the city since all wastes from houses, farms, the stockyards, and other industries could be dumped into the river and carried out into Lake Michigan.
Decline & Replacement.
The lake, however, was also the source of drinking water. During a tremendous storm in 1885, the rainfall washed refuse from the river, especially from the highly polluted Bubbly Creek, far out into the lake (the city water intakes are located offshore). Although no epidemics occurred, the Chicago Sanitary District (now The Metropolitan Water Reclamation District) was created by the Illinois legislature in 1889 in response to this close call.
This new agency devised a plan to construct channels and canals to reverse the flow of the rivers away from Lake Michigan and divert the contaminated water downstream where it could be diluted as it flowed into the Des Plaines River and eventually the Mississippi.
In 1892, the direction of part of the Chicago River was reversed by the Army Corps of Engineers with the result that the river and much of Chicago's sewage flowed into the canal instead of into Lake Michigan. The complete reversal of the river's flow was accomplished when the Sanitary and Ship Canal was opened in 1900.
It was replaced in 1933 by the Illinois Waterway, which remains in use.
Rejuvenation.
The actual origin site of the Illinois and Michigan Canal has been converted into a nature park that integrates history, ecology and art to communicate the Canal's importance in the development of Chicago. In 2003 the Chicago Park District - in cooperation with the I & M Canal Association, hired Conservation Design Forum to develop plans to convert the brownfield site into a landscape that provided for passive recreational uses in a landscape setting with native plant species. Interpretive panels built into a wall along a bike trail were designed by local high school art students. also consulted on landscape stabilization techniques to repair a significantly degraded shoreline (water levels can fluctuate as much as 5 feet).
Today much of the canal is a long, thin linear park with canoeing and a hiking and biking trail (constructed on the alignment of the mule tow paths). It also includes museums and historical canal buildings. It was designated the first National Heritage Corridor by US Congress in 1984.
Adjacent Communities.
Many towns in Northern Illinois owe their existence directly to the Illinois and Michigan Canal. Chicago, Lockport, Morris, Ottawa, and LaSalle were platted by the Canal Commissioners to raise funds for the canal's construction. From east to west the towns along the path of the canal include:

</doc>
<doc id="50380" url="https://en.wikipedia.org/wiki?curid=50380" title="National Association for Research &amp; Therapy of Homosexuality">
National Association for Research &amp; Therapy of Homosexuality

The National Association for Research & Therapy of Homosexuality (NARTH), also known as the NARTH Institute, is an organization that offers conversion therapy and other regimens that purport to change the sexual orientation of people with same-sex attraction. NARTH has been described by a Christian ministry group as a ministry partner that is "a multi-disciplinary professional and scientific organization dedicated to the service of persons who experience unwanted homosexual (same-sex) attractions (SSA)". NARTH was founded in 1992 by Joseph Nicolosi, Benjamin Kaufman, and Charles Socarides. Its headquarters are in Encino, California, at the Thomas Aquinas Psychological Clinic. Julie Hamilton is the current president of NARTH. NARTH's leaders disagree with the view of the world's major mental health organizations that homosexuality is not a disorder.
History.
NARTH was founded in 1992 by Benjamin Kaufman, Charles Socarides, and Joseph Nicolosi. In an article titled "In Defense of the Need for Honest Dialogue", Kaufman wrote that the three of them founded NARTH because the American Psychiatric Association and similar professional organizations "had totally stifled the scientific inquiry that would be necessary to stimulate a discussion" about homosexuality. NARTH's leaders argue that the political atmosphere had changed, making it politically incorrect even to suggest the need for a dialogue that considers the question of the normality of homosexuality. Kaufman states that NARTH was formed in response to censorship of scientific investigation of politically unpopular views.
The organization had 501(c)(3) tax exempt status, which was revoked by the Internal Revenue Service in September 2012 due to ongoing failure to file required paperwork.
Activities.
NARTH claims to be a secular organization, differentiating it from other ex-gay groups that are primarily religious in nature. Nevertheless, NARTH often partners with religious groups, such as Jews Offering New Alternatives for Healing, , and Evergreen International in Positive Alternatives to Homosexuality. The NARTH website contains a resource list of theological articles.
In July 2011, NARTH failed to pay its dues to the California Board for Behavioral Sciences and was removed from the list of groups that provide continuing education credits to therapists in California. NARTH had been an approved continuing education provider since 1998.
Sigmund Freud Award/President's Award.
Beginning in 1996, NARTH has given an award in recognition of a researcher's outstanding work, called the NARTH Sigmund Freud Award. In some years, the award is called the NARTH President's Award. The award is presented at NARTH's annual conference.
Affiliations.
A. Dean Byrd is a past president. Notable members of the Scientific Advisory Committee include Hillel Goldberg, Nathaniel S. Lehrman and Jeffrey Satinover. Robert Perloff, former president of the American Psychological Association, was a notable supporter of NARTH.
PATH.
In 2003, PATH's leaders made NARTH a member of Positive Alternatives to Homosexuality.
Controversy.
Abba Goldberg.
In 2010 it was revealed that NARTH’s executive secretary, Abba Goldberg, was a con man who had served 18 months in prison.
Gerald Schoenewolf.
NARTH received criticism from the Southern Poverty Law Center for an essay titled "Gay Rights and Political Correctness: A Brief History", and written by Gerald Schoenewolf, a member of NARTH's Science Advisory Committee. SPLC called it an angry polemic that made outrageous historical claims. The article had drawn a letter of protest from the National Black Justice Coalition a year after its publication. A month later, NARTH removed the article from its website and posted a statement of apology. A later statement from NARTH said that SPLC had mis-labeled Schoenewolf as "ex-gay" and had made other erroneous claims about his essay.
Gerard J. M. van den Aardweg.
Gerard J. M. van den Aardweg is a member of NARTH's advisory panel. During the debate over Same-sex marriage in the Republic of Ireland Aardweg promoted opinions the "Irish Times" described as conspiracy theories, including that the Nazi party was "rooted" in homosexuals, and that homosexuality is being imposed on the world by "freemasonry international."

</doc>
<doc id="50387" url="https://en.wikipedia.org/wiki?curid=50387" title="Containerization">
Containerization

Containerization is a system of intermodal freight transport using intermodal containers (also called shipping containers and ISO containers) made of weathering steel. The containers have standardized dimensions. They can be loaded and unloaded, stacked, transported efficiently over long distances, and transferred from one mode of transport to another—container ships, rail transport flatcars, and semi-trailer trucks—without being opened. The handling system is completely mechanized so that all handling is done with cranes and special forklift trucks. All containers are numbered and tracked using computerized systems.
The system, developed after World War II, dramatically reduced transport costs, supported the post-war boom in international trade, and was a major element in globalization. Containerization did away with the manual sorting of most shipments and the need for warehousing. It displaced many thousands of dock workers who formerly handled break bulk cargo. Containerization also reduced congestion in ports, significantly shortened shipping time and reduced losses from damage and theft.
Origin.
Before containerization, goods were usually handled manually as break bulk cargo. Typically, goods would be loaded onto a vehicle from the factory and taken to a port warehouse where they would be offloaded and stored awaiting the next vessel. When the vessel arrived, they would be moved to the side of the ship along with other cargo to be lowered or carried into the hold and packed by dock workers. The ship might call at several other ports before off-loading a given consignment of cargo. Each port visit would delay the delivery of other cargo. Delivered cargo might then have been offloaded into another warehouse before being picked up and delivered to its destination. Multiple handling and delays made transport costly, time consuming and unreliable.
Containerization has its origins in early coal mining regions in England beginning in the late 18th century. In 1766 James Brindley designed the box boat 'Starvationer' with 10 wooden containers, to transport coal from Worsley Delph (quarry) to Manchester by Bridgewater Canal. In 1795, Benjamin Outram opened the Little Eaton Gangway, upon which coal was carried in wagons built at his Butterley Ironwork. The horse-drawn wheeled wagons on the gangway took the form of containers, which, loaded with coal, could be transshipped from canal barges on the Derby Canal, which Outram had also promoted.
By the 1830s, railroads on several continents were carrying containers that could be transferred to other modes of transport. The Liverpool and Manchester Railway in the United Kingdom was one of these. "Simple rectangular timber boxes, four to a wagon, they were used to convey coal from the Lancashire collieries to Liverpool, where they were transferred to horse-drawn carts by crane." Originally used for moving coal on and off barges, "loose boxes" were used to containerize coal from the late 1780s, at places like the Bridgewater Canal. By the 1840s, iron boxes were in use as well as wooden ones. The early 1900s saw the adoption of closed container boxes designed for movement between road and rail.
On 17 May 1917 Benjamin Franklin Fitch inaugurated exploitation of the experimental installation for transfer of the containers called the demountable bodies based on his own design in Cincinnati, Ohio in US. Later in 1919, his system was extended to over 200 containers serving 21 railway stations with 14 freight trucks.
Prior to the Second World War, many European countries independently developed container systems.
In 1919, Stanislaw Rodowicz, an engineer, developed the first draft of the container system in Poland. In 1920, he built a prototype of the biaxial wagon. The Polish-Bolshevik War stopped development of the container system in Poland.
In 1926, a regular connection of the luxury passenger train from London to Paris, Golden Arrow/Fleche d'Or, by Southern Railway and French Northern Railway, began. For transport of passengers' baggage four containers were used. These containers were loaded in London or Paris and carried to ports, Dover or Calais, on flat cars in the UK and “CIWL Pullman Golden Arrow Fourgon of CIWL” in France. At the Second World Motor Transport Congress in Rome, September 1928, Italian senator Silvio Crespi proposed the use of containers for road and railway transport systems, using collaboration rather than competition. This would be done under the auspices of an international organ similar to the Sleeping Car Company, which provided international carriage of passengers in sleeping wagons. In 1928 Pennsylvania Railroad (PRR) started regular container service in the northeast United States. After the Wall Street Crash of 1929 in New York and the subsequent Great Depression, many countries were without any means of transport for cargo. The railroads were sought as a possibility to transport cargo, and there was an opportunity to bring containers into broader use. Under auspices of the International Chamber of Commerce in Paris in Venice on September 30, 1931, on one of the platforms of the Maritime Station (Mole di Ponente), practical tests were done to assess the best construction for European containers as part of an international competition.
In the same year, 1931, in USA Benjamin Franklin Fitch designed the two largest and heaviest containers in existence anywhere at the time. One measured 17'6" by 8'0" by 8'0" with a capacity of 30,000 pounds in 890 cubic feet, and a second measured 20'0" by 8'0" by 8'0", with a capacity of 50,000 pounds in 1,000 cubic feet.
In November 1932 in Enola the first container terminal in the world was opened by PRR Pennsylvania RailRoad company. The Fitch hooking system was used for reloading of the containers.
The development of containerization was created in Europe and the US as a way to revitalize rail companies after the Wall Street Crash of 1929, which had caused economic collapse and reduction in use of all modes of transport 
In 1933 in Europe under the auspices of the International Chamber of Commerce the International Container Bureau (French: "Bureau International des Conteneurs", B.I.C.) was established. In June 1933, the B.I.C. decided on obligatory parameters for containers used in international traffic. Containers handled by means of lifting gear, such as cranes, overhead conveyors, etc. for traveling elevators (group I containers), constructed after July 1, 1933. Obligatory Regulations: 
In April 1935 BIC established second standard for European containers:
From 1926 to 1947 in the United States, the Chicago North Shore and Milwaukee Railway carried motor carrier vehicles and shippers' vehicles loaded on flatcars between Milwaukee, Wisconsin, and Chicago, Illinois. Beginning in 1929, Seatrain Lines carried railroad boxcars on its sea vessels to transport goods between New York and Cuba.
In the mid-1930s, the Chicago Great Western Railway and then the New Haven Railroad began "piggyback" service (transporting highway freight trailers on flatcars) limited to their own railroads. The Chicago Great Western Railway filed a US federal patent in 1938 on their method of securing each trailer to a flatcar using chains and turnbuckles. Other components included wheel chocks and ramps for loading and unloading the trailers from the flatcars. By 1953, the Chicago, Burlington and Quincy, the Chicago and Eastern Illinois, and the Southern Pacific railroads had joined the innovation. Most of the railcars used were surplus flatcars equipped with new decks. By 1955, an additional 25 railroads had begun some form of piggyback trailer service.
During World War II, the Australian Army used containers to help more easily deal with various breaks of gauge in the railroads. These non-stackable containers were about the size of the later 20-foot ISO container and perhaps made mainly of wood.
During World War II, the United States Army started to combine items of uniform size, lashing them onto a pallet, unitizing cargo to speed the loading and unloading of transport ships. In 1947 the Transportation Corps developed the "Transporter", a rigid, corrugated steel container with a carrying capacity, for shipping household goods of officers in the field. It was 8' 6" long, 6' 3" wide, and 6' 10" high (2.59 x 1.91 x 2.08 m), with double doors on one end, mounted on skids, and had lifting rings on the top four corners. During the Korean War the Transporter was evaluated for handling sensitive military equipment and, proving effective, was approved for broader use. Theft of material and damage to wooden crates convinced the army that steel containers were needed.
In April 1951, at Zürich Tiefenbrunnen railway station, the Swiss Museum of Transport and "Bureau International des Containers" (BIC) held demonstrations of container systems, with the aim of selecting the best solution for Western Europe. Present were representatives from France, Belgium, the Netherlands, Germany, Switzerland, Sweden, Great Britain, Italy and the United States. The system chosen for Western Europe was based on the Netherlands' system for consumer goods and waste transportation called "Laadkisten" (literally, "loading bins"), in use since 1934. This system used roller containers that were moved by rail, truck and ship, in various configurations up to a capacity of , and up to 3.1 x 2.3 x 2 metres in size. This became the first post World War II European railway standard UIC 590, known as "pa-Behälter." It was implemented in the Netherlands, Belgium, Luxembourg, West Germany, Switzerland, Sweden and Denmark.
With the popularization of the larger ISO containers, support for pa containers was phased out by the railways. In the 1970s they began to be widely used for transporting waste.
In 1952 the US army developed the Transporter into the CONtainer EXpress or "CONEX" box system. The size and capacity of the Conex were about the same as the Transporter, but the system was made "modular", by the addition of a smaller, half-size unit of 6' 3" long, 4' 3" wide and 6' 10½" high. CONEXes could be stacked three high, and protected their contents from the elements.
The first major shipment of CONEXes, containing engineering supplies and spare parts, was made by rail from the Columbus General Depot in Georgia to the Port of San Francisco, then by ship to Yokohama, Japan, and then to Korea, in late 1952; shipment times were almost halved. By the time of the Vietnam War the majority of supplies and materials were shipped by CONEX. By 1965 the US military used some 100,000 Conex boxes, and more than 200,000 in 1967. making this the first worldwide application of intermodal containers. After the US Department of Defense standardized an 8-foot by 8-foot cross section container in multiples of 10-foot lengths for military use, it was rapidly adopted for shipping purposes.
In 1955, former trucking company owner Malcom McLean worked with engineer Keith Tantlinger to develop the modern intermodal container. The challenge was to design a shipping container that could efficiently be loaded onto ships and would hold securely on long sea voyages. The result was a tall by wide box in -long units constructed from thick corrugated steel. The design incorporated a twistlock mechanism atop each of the four corners, allowing the container to be easily secured and lifted using cranes. After helping McLean create the successful design, Tantlinger convinced him to give the patented designs to industry; this began international standardization of shipping containers.
Purpose-built ships.
The first vessels purpose-built to carry containers had begun operation in 1926 for the regular connection of the luxury passenger train between London and Paris, the Golden Arrow/Fleche d'Or. Four containers were used for the conveyance of passengers' baggage. These containers were loaded in London or Paris and carried to the ports of Dover or Calais.
The next step was in Europe was after the Second World War. Vessels purpose-built to carry containers were used between UK and Netherlands and also in Denmark in 1951. In the United States, ships began carrying containers in 1951, between Seattle, Washington and Alaska. However, none of these services was particularly successful. First, the containers were rather small, with 52% of them having a volume of less than . Almost all European containers were made of wood and used canvas lids, and they required additional loading into rail or truck bodies.
The world's first purpose-built container ship were "vessels"? was the "Clifford J. Rodgers", built in Montreal in 1955 and owned by the White Pass and Yukon Route Hougen Group of Companies. Its first trip carried 600 containers between North Vancouver, British Columbia, and Skagway, Alaska, on November 26, 1955; in Skagway, the containers were unloaded to purpose-built railroad cars for transport north to the Yukon, in the first intermodal service using trucks, ships, and railroad cars. Southbound containers were loaded by shippers in the Yukon and moved by rail, ship, and truck to their consignees without opening. This first intermodal system operated from November 1955 until 1982.
The first truly successful container shipping company dates to April 26, 1956, when American trucking entrepreneur McLean put 58 "trailer vans" later called containers, aboard a refitted tanker ship, the , and sailed them from Newark, New Jersey to Houston, Texas. Independently of the events in Canada, McLean had the idea of using large containers that never opened in transit and that were transferable on an intermodal basis, among trucks, ships, and railroad cars. McLean had initially favored the construction of "trailerships"—taking trailers from large trucks and stowing them in a ship's cargo hold. This method of stowage, referred to as roll-on/roll-off, was not adopted because of the large waste in potential cargo space on board the vessel, known as broken stowage. Instead, McLean modified his original concept into loading just the containers, not the chassis, onto the ship; hence the designation "container ship" or "box" ship. (See also pantechnicon van and trolley and lift van.)
Toward standards.
During the first 20 years of containerization, many container sizes and corner fittings were used; there were dozens of incompatible container systems in the United States alone. Among the biggest operators, the Matson Navigation Company had a fleet of containers, while Sea-Land Service, Inc used containers. The standard sizes and fitting and reinforcement norms that now exist evolved out of a series of compromises among international shipping companies, European railroads, US railroads, and US trucking companies. Four important ISO (International Organization for Standardization) recommendations standardized containerization globally:
In the United States, containerization and other advances in shipping were impeded by the Interstate Commerce Commission (ICC), which was created in 1887 to keep railroads from using monopolist pricing and rate discrimination but fell victim to regulatory capture. By the 1960s, ICC approval was required before any shipper could carry different items in the same vehicle or change rates. The fully integrated systems in the United States today became possible only after the ICC's regulatory oversight was cut back (and abolished in 1995); trucking and rail were deregulated in the 1970s and maritime rates were deregulated in 1984.
Double-stacked rail transport, where containers are stacked two high on railway cars, was introduced in the United States. The concept was developed by Sea-Land and the Southern Pacific railroad. The first standalone double-stack container car (or single-unit 40-ft COFC well car) was delivered in July 1977. The 5-unit well car, the industry standard, appeared for the first time in 1981. Initially, these double-stack railway cars were deployed in regular train service. Ever since American President Lines initiated in 1984 a dedicated double-stack container train service between Los Angeles and Chicago, transport volumes increased rapidly.
Effects.
Containerization greatly reduced the expense of international trade and increased its speed, especially of consumer goods and commodities. It also dramatically changed the character of port cities worldwide. Prior to highly mechanized container transfers, crews of 20–22 longshoremen would pack individual cargoes into the hold of a ship. After containerization, large crews of longshoremen were no longer necessary at port facilities, and the profession changed drastically.
Meanwhile, the port facilities needed to support containerization changed. One effect was the decline of some ports and the rise of others. At the Port of San Francisco, the former piers used for loading and unloading were no longer required, but there was little room to build the vast holding lots needed for container transport. As a result, the Port of San Francisco virtually ceased to function as a major commercial port, but the neighboring port of Oakland emerged as the second largest on the US West Coast. A similar fate met the relation between the ports of Manhattan and New Jersey. In the United Kingdom, the Port of London and Port of Liverpool declined in importance. Meanwhile, Britain's Port of Felixstowe and Port of Rotterdam in the Netherlands emerged as major ports. In general, inland ports on waterways incapable of deep-draft ship traffic also declined from containerization in favor of seaports. With intermodal containers, the job of sorting and packing containers could be performed far from the point of embarking.
The effects of containerization rapidly spread beyond the shipping industry. Containers were quickly adopted by trucking and rail transport industries for cargo transport not involving sea transport. Manufacturing also evolved to adapt to take advantage of containers. Companies that once sent small consignments began grouping them into containers. Many cargoes are now designed to fit precisely into containers. The reliability of containers also made just in time manufacturing possible as component suppliers could deliver specific components on regular fixed schedules.
Twenty-first century.
, approximately 90% of non-bulk cargo worldwide is moved by containers stacked on transport ships; 26% of all container transshipment is carried out in China. For example, in 2009 there were 105,976,701 transshipments in China (both international and coastal, excluding Hong Kong), 21,040,096 in Hong Kong (which is listed separately), and only 34,299,572 in the United States. In 2005, some 18 million containers made over 200 million trips per year. Some ships can carry over , such as the "Emma Mærsk", long, launched in August 2006. It has been predicted that, at some point, container ships will be constrained in size only by the depth of the Straits of Malacca, one of the world's busiest shipping lanes, linking the Indian Ocean to the Pacific Ocean. This so-called Malaccamax size constrains a ship to dimensions of in length and wide.
However, few initially foresaw the extent of the influence of containerization on the shipping industry. In the 1950s, Harvard University economist Benjamin Chinitz predicted that containerization would benefit New York by allowing it to ship its industrial goods more cheaply to the Southern United States than other areas, but he did not anticipate that containerization might make it cheaper to import such goods from abroad. Most economic studies of containerization merely assumed that shipping companies would begin to replace older forms of transportation with containerization, but did not predict that the process of containerization itself would have a more direct influence on the choice of producers and increase the total volume of trade.
The widespread use of ISO standard containers has driven modifications in other freight-moving standards, gradually forcing removable truck bodies or swap bodies into standard sizes and shapes (though without the strength needed to be stacked), and changing completely the worldwide use of freight pallets that fit into ISO containers or into commercial vehicles.
Improved cargo security is also an important benefit of containerization. The cargo is not visible to the casual viewer and thus is less likely to be stolen; the doors of the containers are usually sealed so that tampering is more evident. Some containers are fitted with electronic monitoring devices and can be remotely monitored for changes in air pressure, which happens when the doors are opened. This reduced the thefts that had long plagued the shipping industry. Recent developments have focused on the use of intelligent logistics optimization to further enhance security.
The use of the same basic sizes of containers across the globe has lessened the problems caused by incompatible rail gauge sizes in different countries. The majority of the rail networks in the world operate on a gauge track known as standard gauge, but many countries (such as Russia, India, Finland, and Lithuania) use broader gauges, while many others in Africa and South America use narrower gauges on their networks. The use of container trains in all these countries makes transshipment between different trains of different gauges easier.
Containers have become a popular way to ship private cars and other vehicles overseas using 20- or 40-foot containers. Unlike roll-on/roll-off vehicle shipping, personal effects can be loaded into the container with the vehicle, allowing for easy international relocation.
Container standards.
ISO standard.
There are five common standard lengths: , , , , and . US domestic standard containers are generally and (rail and truck). Container capacity is often expressed in twenty-foot equivalent units (TEU, or sometimes "teu"). An equivalent unit is a measure of containerized cargo capacity equal to one standard (length) × (width) container. As this is an approximate measure, the height of the box is not considered. For instance, the "high cube" and the "half height" containers are also called one TEU.
The maximum gross mass for a dry cargo container is , and for a container (including the high cube) it is . Allowing for the tare mass of the container, the maximum payload mass is therefore reduced to approximately for , and for containers.
The original choice of height for ISO containers was made in part to suit a large proportion of railway tunnels, though some had to be modified. The current standard is high. With the arrival of even taller hi-cube containers at and double stacking rail cars, further enlargement of the rail loading gauge is proving necessary.
Air freight containers.
While major airlines use containers that are custom designed for their aircraft and associated ground handling equipment the IATA has created a set of standard aluminium container sizes of up to in volume.
Other container system standards.
Some other container systems (in date order) are:
Container loading.
Full container load.
A full container load (FCL) is an ISO standard container that is loaded and unloaded under the risk and account of one shipper and only one consignee. In practice, it means that the whole container is intended for one consignee. FCL container shipment tends to have lower freight rates than an equivalent weight of cargo in bulk. FCL is intended to designate a container loaded to its allowable maximum weight or volume, but FCL in practice on ocean freight does not always mean a full payload or capacity - many companies will prefer to keep a 'mostly' full container as a single container load to simplify logistics and increase security compared to sharing a container with other goods.
Less-than-container load.
Less-than-container load (LCL) is a shipment that is not large enough to fill a standard cargo container. The abbreviation LCL formerly applied to "less than (railway) car load" for quantities of material from different shippers or for delivery to different destinations carried in a single railway car for efficiency. LCL freight was often sorted and redistributed into different railway cars at intermediate railway terminals en route to the final destination.
LCL is "a quantity of cargo less than that required for the application of a carload rate. A quantity of cargo less than that fills the visible or rated capacity of an inter-modal container." It can also be defined as "a consignment of cargo which is inefficient to fill a shipping container. It is grouped with other consignments for the same destination in a container at a container freight station".
Issues.
Hazards.
Containers have been used to smuggle contraband. The vast majority of containers are never subjected to scrutiny due to the large number of containers in use. In recent years there have been increased concerns that containers might be used to transport terrorists or terrorist materials into a country undetected. The US government has advanced the Container Security Initiative (CSI), intended to ensure that high-risk cargo is examined or scanned, preferably at the port of departure.
Empty containers.
Containers are intended to be used constantly, being loaded with new cargo for a new destination soon after having been emptied of previous cargo. This is not always possible, and in some cases, the cost of transporting an empty container to a place where it can be used is considered to be higher than the worth of the used container. Shipping lines and container leasing companies have become expert at repositioning empty containers from areas of low or no demand, such as the US West Coast, to areas of high demand, such as China. Repositioning within the port hinterland has also been the focus of recent logistics optimization work. However, damaged or retired containers may also be recycled in the form of shipping container architecture, or the steel content salvaged. In the summer of 2010, a worldwide shortage of containers developed as shipping increased after the recession, while new container production had largely ceased.
Loss at sea.
Containers occasionally fall from ships, usually during storms; according to media sources, between 2,000 and 10,000 containers are lost at sea each year. The World Shipping Council states in a survey among freight companies that this claim is grossly excessive and calculated an average of 350 containers to be lost at sea each year, or 675 if including catastrophic events. For instance, on November 30, 2006, a container washed ashore on the Outer Banks of North Carolina, along with thousands of bags of its cargo of Doritos Chips. Containers lost in rough waters are smashed by cargo and waves, and often sink quickly. Although not all containers sink, they seldom float very high out of the water, making them a shipping hazard that is difficult to detect. Freight from lost containers has provided oceanographers with unexpected opportunities to track global ocean currents, notably a cargo of Friendly Floatees.
In 2007 the International Chamber of Shipping and the World Shipping Council began work on a code of practice for container storage, including crew training on parametric rolling, safer stacking, the marking of containers, and security for above-deck cargo in heavy swell.
In 2011, the MV Rena ran aground off the coast of New Zealand. As the ship listed, some containers were lost, while others were held on board at a precarious angle.
Trade union challenges.
Some of the biggest battles in the container revolution were waged in Washington, D.C.. Intermodal shipping got a huge boost in the early 1970s, when carriers won permission to quote combined rail-ocean rates. Later, non-vessel-operating common carriers won a long court battle with a US Supreme Court decision against contracts that attempted to require that union labor be used for stuffing and stripping containers at off-pier locations.
Other uses for containers.
Shipping container architecture is the use of containers as the basis for housing and other functional buildings for people, either as temporary or permanent housing, and either as a main building or as a cabin or workshop. Containers can also be used as sheds or storage areas in industry and commerce.
Tempo Housing in Amsterdam stacks containers for individual housing units.
Containers are also beginning to be used to house computer data centers, although these are normally specialized containers.
There is now a high demand for containers to be converted in the domestic market to serve specific purposes. As a result, a number of container-specific accessories have become available for a variety of applications, such as racking for archiving, lining/heating/lighting/powerpoints to create purpose-built secure offices, canteens and drying rooms, condensation control for furniture storage, and ramps for storage of heavier objects. Containers are also converted to provide equipment enclosures, pop-up cafes, exhibition stands, security huts, and more.
Public containerised transport is the concept, not yet implemented, of modifying motor vehicles to serve as personal containers in non-road passenger transport.
The ACTS roller container standards have become the basis of containerized firefighting equipment throughout Europe.
BBC tracking project.
On September 5, 2008, the BBC embarked on a year-long project to study international trade and globalization by tracking a shipping container on its journey around the world.

</doc>
