<doc id="48836" url="https://en.wikipedia.org/wiki?curid=48836" title="Kava">
Kava

Kava or kava-kava ("Piper methysticum": Latin "pepper" + Latinized Greek "intoxicating") is a crop of the western Pacific.
The name "kava(-kava)" is from Tongan and Marquesan; other names for kava include "awa" (Hawaii), "ava" (Samoa), "yaqona" (Fiji), "sakau" (Pohnpei), and "malok" or "malogu" (parts of Vanuatu).
The roots of the plant are used to produce a drink with sedative, anesthetic, and entheogenic properties. Kava is consumed throughout the Pacific Ocean cultures of Polynesia, including Hawaii, Vanuatu, Melanesia and some parts of Micronesia. (See canoe plants.) Kava is sedating and is primarily consumed to relax without disrupting mental clarity. Its active ingredients are called kavalactones. A Cochrane Collaboration systematic review of its evidence concluded it was likely to be more effective than placebo at treating short-term social anxiety.
Characteristics.
The several cultivars of kava vary in concentrations of primary and secondary psychoactive alkaloids. The largest number are grown in the Republic of Vanuatu, and so it is recognised as the "home" of kava. Kava was historically grown only in the Pacific islands of Hawaii, Federated States of Micronesia, Vanuatu, Fiji, the Samoas and Tonga. Some is grown in the Solomon Islands since World War II, but most is imported. Kava is a cash crop in Vanuatu and Fiji.
The kava shrub thrives in loose, well-drained soils where plenty of air reaches the roots. It grows naturally where rainfall is plentiful (over 2,000 mm/yr). Ideal growing conditions are and 70–100% relative humidity. Too much sunlight is harmful, especially in early growth, so kava is an understory crop.
Kava cannot reproduce sexually. Female flowers are especially rare and do not produce fruit even when hand-pollinated. Its cultivation is entirely by propagation from stem cuttings.
Traditionally, plants are harvested around four years of age, as older plants have higher concentrations of kavalactones. After reaching about 2 m height, plants grow a wider stalk and additional stalks, but not much taller. The roots can reach a depth of 60 cm.
Strains and origins.
One of the most potent strains is called "Isa" in Papua New Guinea, and also called "Tuday" in Hawaii. In Vanuatu, it is considered a type of "Tudei" kava, pronounced as "two-day" because it is said to have effects lasting two days due to its chemical profile being high in the kavalactone dihydromethysticin. The plant itself is a strong, very hardy, fast-growing variety with multiple light to dark green stems covered with raised dark spots.
In Vanuatu, exportation of kava is strictly regulated. Only strains they deem as "noble" varieties that are not too weak or too potent are allowed to be exported. Only the most desirable strains for everyday drinking are selected to be noble varieties to maintain quality control. In addition, their laws mandate that exported kava must be at least five years old and farmed organically. Their most popular noble strains are "Boroguu" or "Boronggoru" from Pentecost Island, "Melomelo" from Ambae island (called "sese" in North Pentecost), and "Palarasul" kava from Espiritu Santo Island. In Vanuatu, Tudei (two-days) kava is reserved for special ceremonial occasions and exporting it is not allowed. "Palisi" is a popular Tudei variety.
In Hawaii, there are many other strains of kava. Some of the most popular strains are the "Mahakea," "Mo'i," "Hiwa" and "Nene" varieties. The "Ali'i" (kings) of old Hawaii coveted the special kava they called "Mo'i" that had a strong cerebral effect due to a predominant amount of the kavalactone kavain. This sacred variety was so important to them that no one but royalty could ever experience it, "lest they suffer an untimely death". The reverence for Hiwa in old Hawai‘i is evident in this portion of a chant recorded by N.B. Emerson and quoted by Handy and Handy. "This refers to the cup of sacramental‘awa brewed from the strong, black ‘awa root (‘awa hiwa) which was drunk sacramentally by the kumu hula":
Winter describes a hula prayer for inspiration which contains the line, "He ‘ike pū ‘awa hiwa." Pukui and Elbert translated this as "a knowledge from kava offerings". Winter explains that ‘awa, especially of the Hiwa variety, was offered to hula deities in return for knowledge and inspiration. 34, Hawaiian 'Awa, Views of an Ethnobotanical Treasure, 2006.
Other strains are found in Fiji, Tonga, and Samoa.
Composition.
Fresh kava root contains on average 80% water. Dried root contains approximately 43% starch, 20% dietary fiber, 15% kavalactones, 12% water, 3.2% sugars, 3.6% protein, and 3.2% minerals. Kavalactone content is greatest in the roots and decreases higher up the plant. Relative concentrations of 15%, 10% and 5% have been observed in the root, stump, and basal stems, respectively.
The mature roots of the kava plant are harvested after a minimum of four years (at least five years ideally) for peak kavalactone content. Most kava plants produce around of root when they are harvested. Kava root is classified into two categories: crown root (or chips) and lateral root. Crown roots are the large-diameter pieces that look like ( diameter) wooden poker chips. Most kava plants consist of approximately 80% crown root upon harvesting. Lateral roots are smaller-diameter roots that look more like a typical root. A mature kava plant is about 20% lateral roots. Kava lateral roots have the highest content of kavalactones in the kava plant. "Waka" grade kava is made of lateral roots only.
Pharmacology.
Constituents.
A total of 18 different kavalactones (or kavapyrones) have been identified to date, at least 15 of which are active. However, six of them, including kavain, dihydrokavain, methysticin, dihydromethysticin, yangonin, and desmethoxyyangonin, have been determined to be responsible for about 96% of the plant's pharmacological activity. Some minor constituents, including three chalcones, flavokavain A, flavokavain B, and flavokavain C, have also been identified, as well as a toxic alkaloid, pipermethystine.
Pharmacodynamics.
The following pharmacological actions have been reported for kava and/or its major active constituents:
Receptor binding assays with botanical extracts have revealed direct interactions of leaf extracts of kava (which appear to be more active than root extracts) with the GABA (i.e., main) binding site of the GABAA receptor, the D2 receptor, the μ- and δ-opioid receptors, and the H1 and H2 receptors. Weak interaction with the 5-HT6 and 5-HT7 receptors and the benzodiazepine site of the GABAA receptor was also observed.
Potentiation of GABAA receptor activity may underlie the anxiolytic effects of kava, while elevation of dopamine levels in the nucleus accumbens likely underlie the moderately psychotropic effects the plant can produce. Changes in the activity of 5-HT neurons could explain the sleep-inducing action However, failure of the GABAA receptor inhibitor flumazenil to reverse the anxiolytic effects of kava in mice suggests that benzodiazepine-like effects are not contributing to the pharmacological profile of kava extracts.
Heavy, long-term use of kava has been found to not reduce ability in saccade and cognitive tests, but has been associated with elevated liver enzymes.
Detection.
Recent usage of kava has been documented in forensic investigations by quantitation of kavain in blood specimens. The principal urinary metabolite, conjugated 4'-OH-kavain, is generally detectable for up to 48 hours.
Preparation and consumption.
Traditional preparation.
Kava is consumed in various ways throughout the Pacific Ocean cultures of Polynesia, Vanuatu, Melanesia and some parts of Micronesia and Australia. Traditionally, it is prepared by either chewing, grinding or pounding the roots of the kava plant. Grinding is done by hand against a cone-shaped block of dead coral; the hand forms a mortar and the coral a pestle. The ground root/bark is combined with only a little water, as the fresh root releases moisture during grinding. Pounding is done in a large stone with a small log. The product is then added to cold water and consumed as quickly as possible.
The extract is an emulsion of kavalactone droplets in starch and buttermilk. The taste is slightly pungent, while the distinctive aroma depends on whether it was prepared from dry or fresh plant, and on the variety. The colour is grey to tan to opaque greenish.
Kava prepared as described above is much more potent than processed kava. Chewing produces the strongest effect because it produces the finest particles. Fresh, undried kava produces a stronger beverage than dry kava. The strength also depends on the species and techniques of cultivation. Many find mixing powdered kava with hot water makes the drink stronger.
In Vanuatu, a strong kava drink is normally followed by a hot meal or tea. The meal traditionally follows some time after the drink so the psychoactives are absorbed into the bloodstream quicker. Traditionally, no flavoring is added.
In Papua New Guinea, the locals in Madang province refer to their kava as "waild koniak" ("wild cognac" in English).
Fijians commonly share a drink called "grog" made by pounding sun-dried kava root into a fine powder, straining and mixing it with cold water. Traditionally, grog is drunk from the shorn half-shell of a coconut, called a "bilo". Grog is very popular in Fiji, especially among young men, and often brings people together for storytelling and socializing. Drinking grog for a few hours brings a numbing and relaxing effect to the drinker; grog also numbs the tongue and grog drinking typically is followed by a "chaser" or sweet or spicy snack to follow a "bilo".
Supplements and pharmaceutical preparations.
Pharmaceutical and herbal supplement companies extract kavalactones from the kava plant using solvents such as supercritical carbon dioxide, acetone and ethanol to produce pills standardized with between 30% and 90% kavalactones.
Kava culture.
Kava is used for medicinal, religious, political, cultural and social purposes throughout the Pacific. These cultures have a great respect for the plant and place a high importance on it. In Fiji, for example, a formal "yaqona" (kava) ceremony will often accompany important social, political, religious, etc. functions, usually involving a ritual presentation of the bundled roots as a "sevusevu" (gift), and drinking of the "yaqona" itself. 
Effects of consumption.
The effects of a kava drink vary widely with the particular selection of kava plant(s) and amount. A potent drink results in a faster onset with a lack of stimulation; the user's eyes become more sensitive, the person soon experiences sleepiness and then has deep, dreamless sleep within 30 minutes. Sleep is often restful and pronounced periods of sleepiness correlate to the amount and potency of kava consumed. Kava drinkers are often perceived as having lazy days after consumption of kava the night before, which can be expected as many active kavalactones have half lives of approximately 9 hours.
Efficacy.
Kava is renowned for its many health benefits, but the chiefly among them is its anxiolytic properties. A journal article titled ""Kava: A Comprehensive Review of Efficacy, Safety, And Psychopharmacology." from the "Australian
& New Zealand Journal of Psychiatry", reviewed all scholarly journal articles related to kava found in scholarly electronic databases, they found that "The current weight of evidence supports the use of kava in treatment of anxiety with a significant result occurring in four out of six studies reviewed" 
Other than its anxiolytic uses, kava also has a few therapeutic qualities. Kava is known for having a sleep inducing effect and is good for common restlessness and more serious insomnia. Another study found that kava consumption may reduce colon cancer risk based on their results with carcinogen
(a substance capable of causing cancer in living tissue) treated rats.
A few studies have shown that kava extracts possessed efficacious anxiolytic activity compared to placebo for the symptomatic treatment for anxiety. As well, kava extracts may be an effective alternative to tricyclic antidepressants and benzodiazepines for the treatment of anxiety disorders.
One study showed that 150 mg/day of the standardized WS 1490 Kava extract in divided doses for four weeks was effective at decreasing anxiety symptoms but was not superior to the alternative dose of 300 mg/day in divided doses used in other studies.
Most of these studies used a standardized WS 1490 Kava extract formulation, which is composed of 70% kavalactones.
Toxicity and safety.
In 2001, concerns were raised about the safety of kava, which led to restrictions and regulations in several countries, as well as reports by the United States CDC and FDA. Most of the concerns were related to a small number of reports indicating potential cases of hepatotoxicity caused by consumption of various commercial products derived from kava. A number of scientists and medical practitioners criticized the poor quality of the reports by pointing out that most of the reported rare cases of hepatotoxicity involved patients with a history of alcohol or prescription drugs abuse or concomitant use of medicines known as potentially hepatotoxic. On June 10, 2014, the German Administrative Court overturned the 2002 ban reinstating the regulatory requirements of 2001. The court stated that risk from kava exposure had not been clearly demonstrated, nor does it appear unusually high, an opinion presumably driven by the very small number of cases of reported toxicity (n ~ 3) with even a certain degree of causality linked to kava in a global kava-consuming community that may number in the millions of doses consumed daily.
According to a recent comprehensive review of the relevant literature by Showman et al. (2014): "Despite the link to kava and liver toxicity demonstrated in vivo and in vitro, in the history of Western kava use, toxicity is still considered relatively rare. Only a fraction of the handful of cases reviewed for liver toxicity could be, with any certainty, linked to kava consumption and most of those involved the coingestion of other medications/supplements. That means that the incident rate of liver toxicity due to kava is one in 60-125 million patients."
Various components of the plant have been labelled as potentially capable of causing toxicity, particularly pipermethystine and flavokavain B. Yet, based on a retrospective study in Germany, the alkaloid pipermethystine is an unlikely cause for the observed hepatotoxicity, as, although it occurs in qualities up to 0.2% of the leaves, it is found in quantities a hundred-fold less in Western extracts. Another suggested culprit, Flavokavain B, is also found in various preparations in the plant, although not in ethanol extracts of noble cultivars, and never in large enough amounts to cause any liver damage. The very rare cases of kava hepatotoxicity may have been due to contamination with aflatoxins or other mold hepatotoxins, but this requires further study.
Other data suggests that the rare cases of toxicity may be due to an intrinsic metabolic cause. Three possible mechanisms for kavalactone hepatotoxicity have been theorized: inhibition of cytochrome P450, possible reduction in liver glutathione content and, more remotely, inhibition of cyclooxygenase enzyme activity. The direct toxicity of kava extracts is relatively small under any analysis, yet the potential for drug interactions and/or the potentiation of the toxicity of other compounds is large.
In line with this reasoning, several adverse interactions with drugs have been documented, both prescription and nonprescription – including, but not limited to, anticonvulsants, alcohol, anxiolytics (CNS depressants such as benzodiazepines), antipsychotics, levodopa, diuretics, and drugs metabolized by CYP450 in the liver.
A few notable potential drug interactions are, but are not limited to:
The Australian studies that drove this debate focused on populations with heavy concomitant consumption of alcohol and overall poor health. In these studies, heavy kava use in an Aboriginal community in Arnhem Land was associated with overall poor health, a puffy face, scaly rash, and a slight increase in patellar reflexes. A 2012 analysis of cases worldwide proposed that mold was the primary cause of hepatotoxicity in kava products.
Regulation.
In 2002 the EU imposed a ban on imports of kava-based pharmaceutical products. The sale of kava plant is regulated in Switzerland, France, and the Netherlands. Some Pacific Island States who had been benefiting from the export of kava to the pharmaceutical companies have attempted to overturn the EU ban on kava-based pharmaceutical products by invoking international trade agreements at the WTO: Fiji, Samoa, Tonga and Vanuatu argued that the ban was imposed with insufficient evidence. The pressure prompted Germany to reconsider the evidence base for banning kava-based pharmaceutical products. On June 10, 2014, the German Administrative Court overturned the 2002 ban making selling kava as a medicine legal (personal possession of kava has never been illegal), albeit strictly regulated. In Germany, Kava-based pharmaceutical preparations are currently prescription drugs. Furthermore, patient and professional information brochures have been redesigned to warn from potential hepatotoxic side effects. These strict measures have been opposed by some of the leading kava scientists. In early 2016 a court case has been filed against the Bundesinstitut für Arzneimittel und Medizinprodukte (BfArM/German Federal Institute for Drugs and Medical Devices) arguing that the new regulatory regime is too strict and not justified. 
Poland is the only EU country with an "outright ban on kava" and where the mere possession of kava is prohibited and may result in a prison sentence.
In the United Kingdom it is a criminal offence to sell, supply or import any medicinal product containing kava. At the same time it is still legal to possess kava (for personal use) or import it for purposes other than human consumption.
In 2002 Health Canada issued an order prohibiting the sale of any product containing kava. While the restrictions on kava were lifted in 2012, Health Canada has not licensed any kava products for sale in Canada.
In Australia, the supply of kava is regulated through the National Code of Kava Management. The importation and licensing of kava is prohibited in Western Australia. In the Northern Territory, the police say that "the sale and, in majority of circumstances, possession of kava [...] is illegal". The Australian Therapeutic Goods Administration has recommended no more than 250 mg of kavalactones be taken in a 24‑hour period.
Legislation has been proposed to require that kava products be derived only from noble cultivars, which may be less toxic. Other acts have been passed based on the assumption that aqueous solutions are less harmful. Exclusion of certain aerial parts of the plant are also often required by law or convention, which contain less pipermethystine and other toxic compounds.

</doc>
<doc id="48837" url="https://en.wikipedia.org/wiki?curid=48837" title="Sidereal time">
Sidereal time

Sidereal time is a time-keeping system astronomers use to keep track of the direction to point their telescopes to view a given star in the night sky. Briefly, sidereal time is a "time scale that is based on the Earth's rate of rotation measured relative to the fixed stars" rather than the Sun.
From a given observation point, a star found at one location in the sky will be found at nearly the same location on another night at the same sidereal time. This is similar to how the time kept by a sundial can be used to find the location of the Sun. Just as the Sun and Moon appear to rise in the east and set in the west due to the rotation of the Earth, so do the stars. Both solar time and sidereal time make use of the regularity of the Earth's rotation about its polar axis, solar time following the Sun while sidereal time roughly follows the stars. More exactly, sidereal time is the angle, measured from the observer's meridian, along the celestial equator, to the great circle that passes through the March equinox and both poles, and is usually expressed in hours, minutes, and seconds. Common time on a typical clock measures a slightly longer cycle, accounting not only for the Earth's axial rotation but also for the Earth's annual revolution around the Sun of slightly less than 1 degree per day (in fact to the nearest arc-second, it takes 365.2422 days
to revolve therefore 360 degrees/365.2422 days = 0.9856 degrees or 59 arc-minutes, 8 arc-seconds per day, i.e., slightly 
less than 1 degree per day).
A mean sidereal day is 23 hours, 56 minutes, 4.0916 seconds (23.9344699 hours or 0.99726958 mean solar days), the time it takes the Earth to make one rotation relative to the vernal equinox. (Due to nutation, an actual sidereal day is not quite so constant.) The vernal equinox itself precesses slowly westward relative to the fixed stars, completing one revolution in about 26,000 years, so the misnamed sidereal day ("sidereal" is derived from the Latin "sidus" meaning "star") is some 0.0084 seconds shorter than the Earth's period of rotation relative to the fixed stars.
The longer "true" sidereal period is called a stellar day by the International Earth Rotation and Reference Systems Service (IERS). It is also referred to as the sidereal period of rotation, or simply as the period of rotation or the rotational period.
Maps of the stars in the night sky use declination and right ascension as coordinates. These correspond to latitude and longitude respectively. While declination is measured in degrees, right ascension is measured in units of time, because it was most natural to name locations in the sky in connection with the time when they crossed the meridian.
In the sky, the meridian is the imaginary north to south line that goes through the point directly overhead (the zenith). The right ascension of any object crossing the meridian is equal to the current local (apparent) sidereal time, ignoring for present purposes that part of the circumpolar region north of the north celestial pole (for an observer in the northern hemisphere) or south of the south celestial pole (for an observer in the southern hemisphere) that is crossing the meridian the other way.
Because the Earth orbits the Sun once a year, the sidereal time at any given place and time will gain about four minutes against local civil time, every 24 hours, until, after a year has passed, one additional sidereal "day" has elapsed compared to the number of solar days that have gone by.
Sidereal time and solar time.
Solar time is measured by the apparent diurnal motion of the Sun, and local noon in apparent solar time is the moment when the Sun is exactly due south or north (depending on the observer's latitude and the season). A mean solar day (what we normally measure as a "day") is the average time between local solar noons ("average" since this varies slightly over the year).
The Earth makes one rotation around its axis in a sidereal day; during that time it moves a short distance (about 1°) along its orbit around the Sun. So after a sidereal day has passed, the Earth still needs to rotate slightly more before the Sun reaches local noon according to solar time. A mean solar day is, therefore, nearly 4 minutes longer than a sidereal day.
The stars are so far away that the Earth's movement along its orbit makes nearly no difference to their apparent direction (see, however, parallax), and so they return to their highest point in a sidereal day.
Another way to see this difference is to notice that, relative to the stars, the Sun appears to move around the Earth once per year. Therefore, there is one fewer solar day per year than there are sidereal days. This makes a sidereal day approximately times the length of the 24-hour solar day, giving approximately 23 hours, 56 minutes, 4.1 seconds (86,164.1 seconds).
Precession effects.
The Earth's rotation is not a simple rotation around an axis that would always remain parallel to itself. The Earth's rotational axis itself rotates about a second axis, orthogonal to the Earth's orbit, taking about 25,800 years to perform a complete rotation. This phenomenon is called the precession of the equinoxes. Because of this precession, the stars appear to move around the Earth in a manner more complicated than a simple constant rotation.
For this reason, to simplify the description of Earth's orientation in astronomy and geodesy, it is conventional to chart the positions of the stars in the sky according to right ascension and declination, which are based on a frame that follows the Earth's precession, and to keep track of Earth's rotation, through sidereal time, relative to this frame as well. In this reference frame, Earth's rotation is close to constant, but the stars appear to rotate slowly with a period of about 25,800 years. It is also in this reference frame that the tropical year, the year related to the Earth's seasons, represents one orbit of the Earth around the sun. The precise definition of a sidereal day is the time taken for one rotation of the Earth in this precessing reference frame.
Definition.
Sidereal time, at any moment (and at a given locality defined by its geographical longitude), more precisely Local Apparent Sidereal Time (LAST), is defined as the hour angle of the vernal equinox at that locality: it has the same value as the right ascension of any celestial body that is crossing the local meridian at that same moment.
At the moment when the vernal equinox crosses the local meridian, Local Apparent Sidereal Time is 00:00. Greenwich Apparent Sidereal Time (GAST) is the hour angle of the vernal equinox at the prime meridian at Greenwich, England.
Local Sidereal Time at any locality differs from the Greenwich Sidereal Time value of the same moment, by an amount that is proportional to the longitude of the locality. When one moves eastward 15° in longitude, sidereal time is larger by one sidereal hour (note that it wraps around at 24 hours). Unlike the reckoning of local solar time in "time zones," incrementing by (usually) one hour, differences in local sidereal time are reckoned based on actual measured longitude, to the accuracy of the measurement of the longitude, not just in whole hours.
Apparent Sidereal Time (Local or at Greenwich) differs from Mean Sidereal Time (for the same locality and moment) by the Equation of the Equinoxes: This is a small difference in Right Ascension R.A. (formula_1) (parallel to the equator), not exceeding about ±1.2 seconds of time, due to nutation, the complex 'nodding' motion of the Earth's polar axis of rotation. It corresponds to the current amount of the nutation in (ecliptic) longitude (formula_2) and to the current obliquity (formula_3) of the ecliptic, so that formula_4 .
Greenwich Mean Sidereal Time (GMST) and UT1 differ from each other in rate, with the second of sidereal time a little shorter than that of UT1, so that (as at 2000 January 1 noon) 1.002 737 909 350 795 second of mean sidereal time was equal to 1 second of Universal Time (UT1). The ratio varies slightly with time, reaching 1.002 737 909 409 795 after a century.
To an accuracy within 0.1 second per century, Greenwich (Mean) Sidereal Time (in hours and decimal parts of an hour) can be calculated as 
where D is the interval, in UT1 days including any fraction of a day, since 2000 January 1, at 12h UT (interval counted positive if forwards to a later time than the 2000 reference instant), and the result is freed from any integer multiples of 24 hours to reduce it to a value in the range 0–24.
In other words, Greenwich Mean Sidereal Time exceeds mean solar time at Greenwich by a difference equal to the longitude of the fictitious mean Sun used for defining mean solar time (with longitude converted to time as usual at the rate of 1 hour for 15 degrees), plus or minus an offset of 12 hours (because mean solar time is reckoned from 0h midnight, instead of the pre-1925 astronomical tradition where 0h meant noon).
Sidereal time is used at astronomical observatories because sidereal time makes it very easy to work out which astronomical objects will be observable at a given time. Objects are located in the night sky using right ascension and declination relative to the celestial equator (analogous to longitude and latitude on Earth), and when sidereal time is equal to an object's right ascension the object will be at its highest point in the sky, or "culmination", at which time it is usually best placed for observation, as atmospheric extinction is minimised.
Sidereal time is a measure of the position of the Earth in its rotation around its axis, or time measured by the apparent diurnal motion of the vernal equinox, which is very close to, but not identical to, the motion of stars. They differ by the precession of the vernal equinox in right ascension relative to the stars.Earth's sidereal day also differs from its rotation period relative to the background stars by the amount of precession in right ascension during one day (8.4 ms). Its J2000 mean value is 23h56m4.090 530 833s.
Exact duration and its variation.
A mean sidereal day is about 23 h 56 m 4.1 s in length. However, due to variations in the rotation rate of the Earth, the rate of an ideal sidereal clock deviates from any simple multiple of a civil clock. In practice, the difference is kept track of by the difference UTC–UT1, which is measured by radio telescopes and kept on file and available to the public at the IERS and at the United States Naval Observatory.
Given a tropical year of 365.242 190 402 days from Simon et al. this gives a sidereal day of 86 400 × formula_5, or 86 164.090 53 seconds.
Aoki et al., defined UT1 such that the observed sidereal day at the beginning of 2000 would be times a UT1 day of 86 400 seconds, which gives 86 164.090 530 833 seconds of UT1. For times within a century of 1984, the ratio only alters in its 11th decimal place. This web-based sidereal time calculator uses a truncated ratio of .
Because this is the period of rotation in a precessing reference frame, it is not directly related to the mean rotation rate of the Earth in an inertial frame, which is given by ω=2π/T where T is the slightly longer stellar day given by Aoki et al. as 86 164.098 903 697 32 seconds. This can be calculated by noting that ω is the magnitude of the vector sum of the rotations leading to the sidereal day and the precession of that rotation vector. In fact, the period of the Earth's rotation varies on hourly to interannual timescales by around a millisecond, together with a secular increase in length of day of about 2.3 milliseconds per century, mostly from tidal friction slowing the Earth's rotation.
Sidereal days compared to solar days on other planets.
Of the eight solar planets, all but Venus and Uranus have prograde rotation—that is, they rotate more than once per year in the same direction as they orbit the sun, so the sun rises in the east. Venus and Uranus, however, have retrograde rotation. For prograde rotation, the formula relating the lengths of the sidereal and solar days is
or equivalently
On the other hand, the formula in the case of retrograde rotation is
or equivalently
All the solar planets more distant from the sun than Earth are similar to Earth in that, since they experience many rotations per revolution around the sun, there is only a small difference between the length of the sidereal day and that of the solar day—the ratio of the former to the latter never being less than Earth's ratio of .997 . But the situation is quite different for Mercury and Venus. Mercury's sidereal day is about two-thirds of its orbital period, so by the prograde formula its solar day lasts for two revolutions around the sun—three times as long as its sidereal day. Venus rotates retrograde with a sidereal day lasting about 243.0 earth-days, or about 1.08 times its orbital period of 224.7 earth-days; hence by the retrograde formula its solar day is about 116.8 earth-days, and it has about 1.9 solar days per orbital period.
By convention, rotation periods of planets are given in sidereal terms unless otherwise specified.

</doc>
<doc id="48838" url="https://en.wikipedia.org/wiki?curid=48838" title="Hour angle">
Hour angle

In astronomy and celestial navigation, the hour angle is one of the coordinates used in the equatorial coordinate system to give the direction of a point on the celestial sphere.
The hour angle of a point is the angle between two planes: one containing the Earth's axis and the zenith (the meridian plane), and the other containing the Earth's axis and the given point (the hour circle passing through the point).
The angle may be expressed as negative east of the meridian plane and positive west of the meridian plane, or as positive westward from 0° to 360°. The angle may be measured in degrees or in time, with 24h = 360° exactly.
In astronomy, hour angle is defined as the angular distance on the celestial sphere measured westward along the celestial equator from the meridian to the hour circle passing through a point. It may be given in degrees, time, or rotations depending on the application.
In celestial navigation, the convention is to measure in degrees westward from the prime meridian (Greenwich hour angle, GHA), the local meridian (local hour angle, LHA) or the first point of Aries (sidereal hour angle, SHA).
The hour angle is paired with the declination to fully specify the direction of a point on the celestial sphere in the equatorial coordinate system.
Relation with the right ascension.
The local hour angle (LHA) of an object in the observer's sky is
where LHAobject is the local hour angle of the object, LST is the local sidereal time, formula_3 is the object's right ascension, GST is Greenwich sidereal time and formula_4 is the observer's longitude (positive west from the prime meridian). These angles can be measured in time (24 hours to a circle) or in degrees (360 degrees to a circle)— one or the other, not both.
Negative hour angles indicate the time until the next transit across the meridian; an hour angle of zero means the object is on the meridian.
Solar hour angle.
Observing the sun from earth, the solar hour angle is an expression of time, expressed in angular measurement, usually degrees, from solar noon. At solar noon the hour angle is 0.000 degrees, with the time before solar noon expressed as negative degrees, and the local time after solar noon expressed as positive degrees. For example, at 10:30 AM local apparent time the hour angle is -22.5° (15° per hour times 1.5 hours before noon).
The cosine of the hour angle (cos("h")) is used to calculate the solar zenith angle. At solar noon, "h" = 0.000 so cos("h")=1, and before and after solar noon the cos(± "h") term = the same value for morning (negative hour angle) or afternoon (positive hour angle), i.e. the sun is at the same altitude in the sky at 11:00AM and 1:00PM solar time, etc.
Sidereal hour angle.
The sidereal hour angle (SHA) of a body on the celestial sphere is its angular distance west of the vernal equinox generally measured in degrees. The SHA of a star changes slowly, and the SHA of a planet doesn't change very quickly, so SHA is a convenient way to list their positions in an almanac. SHA is often used in celestial navigation and navigational astronomy.
The sidereal hour angle is defined as 360° minus the right ascension.

</doc>
<doc id="48839" url="https://en.wikipedia.org/wiki?curid=48839" title="Meridian">
Meridian

Meridian, or a meridian line may refer to

</doc>
<doc id="48842" url="https://en.wikipedia.org/wiki?curid=48842" title="Smallmouth bass">
Smallmouth bass

The "smallmouth bass" ("Micropterus dolomieu") is a species of freshwater fish in the sunfish family (Centrarchidae) of the order Perciformes. It is the type species of its genus. One of the black basses, it is a popular game fish sought by anglers throughout the temperate zones of North America, and has been spread by stock to many cool-water tributaries and lakes in Canada and more so introduced in the United States. It attains a length of up to 27 inches and 12 pounds. The smallmouth bass is native to the upper and middle Mississippi River basin, the Saint Lawrence River–Great Lakes system, and up into the Hudson Bay basin. Its common names include smallmouth, bronzeback, brown bass, brownie, smallie, bronze bass, "hog", and bareback bass.
Description.
The smallmouth bass is generally brown, appearing sometimes as black or green (seldom yellow) with red eyes, and dark brown vertical bands, rather than a horizontal band along the side. There are 13–15 soft rays in the dorsal fin. The upper jaw of smallmouth bass extends to the middle of the eye. The smallmouth's coloration and hue may vary according to environmental variables such as water clarity or prey diet.
Males are generally smaller than Females. The males tend to range around two pounds, while females can range from three to six pounds. Their average sizes can differ, depending on where they are found; those found in American waters tend to be larger due to the longer summers, which allow them to eat and grow for a longer period of time.
Their habitat plays a significant role in their color, weight, and shape. River water smallmouth that live in dark water tend to be rather torpedo-shaped and very dark brown to be more efficient for feeding. Lakeside smallmouth bass, however, that live in sandy areas, tend to be a light yellow-brown to adapt to the environment in a defensive state and are more oval-shaped.
They have been seen eating tadpoles, fish, aquatic insects, crayfish, anything that they could swallow, they will. They have been seen eating frogs, small mice,small birds, and even French fries. There are two recognized subspecies, the Northern Smallmouth Bass (M. dolomieui dolomieui) and the Neosho Smallmouth Bass (M. dolomieui velox).
Habitat.
The smallmouth bass is found in clearer water than the largemouth, especially streams, rivers, and the rocky areas and stumps and also sandy bottoms of lakes and reservoirs. The smallmouth prefers cooler water temperatures than its cousin the Largemouth bass, and may be found in both still and running water. Because it is intolerant of pollution, the smallmouth bass is a good natural indicator of a healthy environment, though it can better adjust to changes in water condition than most trout species. Carnivorous, its diet comprises crayfish, insects, and smaller fish; the young also feeding on zooplankton. 
The female can lay up to 21,100 eggs, which are guarded by the male in his nest. 
Migration.
When the weather gets colder, and the water temperature drops below 15 C (60 F), smallmouth will often migrate in search of deeper pools in which they enter a semi-hybernation state, moving sluggishly and feeding very little until the warm season returns. The migration patterns of smallmouth have been tracked and it is not unusual for a smallmouth to travel 12 miles in a single day in a stream, creek or river. The overall migration can exceed 60 miles.
Angling.
In the United States, smallmouth bass were first introduced outside of their native range with the construction of the Erie Canal in 1825, extending the fish's range into central New York state. During the mid-to-late 19th century, smallmouth were transplanted via the nation's rail system to lakes and rivers throughout the northern and western United States, as far as California. Shippers found that smallmouth bass were a hardy species that could be transported in buckets or barrels by rail, sometimes using the spigots from the railroad water tanks to aerate the fingerlings. They were introduced east of the Appalachians just before the Civil War, and afterwards transplanted to the states of New England.
With increased industrialization and development, many of the nation's eastern trout rivers were nasty, polluted, or allowed to silt up, raising water temperatures and killing off the native brook trout. Smallmouth bass were often introduced to northern rivers now too warm for native trout, and slowly became a popular gamefish with many anglers. Equally adaptable to large, cool-water impoundments and reservoirs, the smallmouth also spread far beyond its original native range. Later, smallmouth populations also began to decline after years of damage caused by overdevelopment and pollution, as well as a loss of river habitat caused by damming many formerly wild rivers to form lakes or reservoirs. In recent years, a renewed emphasis on preserving water quality and riparian habitat in the nation's rivers and lakes, together with stricter management practices, eventually benefited smallmouth populations and has caused a resurgence in their popularity with anglers.
Today, smallmouth bass are very popular game fish, frequently sought by anglers using conventional spinning and bait casting gear, as well as fly fishing tackle. The smallmouth bass is one of the toughest fighting freshwater fish in North America. In addition to wild populations, the smallmouth bass is stocked in cool rivers and lakes throughout Canada and the United States. In shallow streams, it is a wary fish, though usually not to the extent of most trout. The smallmouth is highly regarded for its topwater fighting ability when hooked – old fishing journals referred to the smallmouth bass as "ounce for ounce and pound for pound the gamest fish that swims". Smallmouth bass are taken for the table, with filets of white, firm flesh when cooked. Today, many fishermen practice catch-and-release fishing to improve fish populations.
The current all-tackle world record for a smallmouth bass is 11 lb 15 oz, caught by Casey Peters in the Dale Hollow Reservoir, on the Kentucky/Tennessee border, in 1955.
Tackle.
In conventional fishing, smallmouth may be successfully caught on a wide range of natural and artificial baits or lures, including crankbaits, hair jigs, plastic jerkbaits, artificial worms, spinnerbaits, and all types of soft plastic lures, including curly tail grubs or tubes with lead head jigs. Spinning reels or baitcasting reels may be used, with line strengths of 2 to 15 pounds typically utilised. Rods are usually of ultralight to medium-heavy action. They may also be caught with a fly rod using a dry or wet artificial fly, nymphs, streamers, or imitations of larger aquatic creatures, such as hellgrammites, crawfish, or leeches. Floating topwater popper fly patterns and buzz baits are also popular for smallmouth fishing.
For river fishing, spinning tackle or fly tackle has been the most popular angling tools for smallmouth in North America for many years.

</doc>
<doc id="48843" url="https://en.wikipedia.org/wiki?curid=48843" title="Largemouth bass">
Largemouth bass

The largemouth bass ("Micropterus salmoides") is a freshwater gamefish in the sunfish family, a species of black bass native to North America. It is known by a variety of regional names, such as the brown bass, widemouth bass, bigmouth bass, black bass, bucketmouth, Potter's fish, Florida bass, Florida largemouth, green bass, green trout, gilsdorf bass, linesides, Oswego bass, southern largemouth and (paradoxically) northern largemouth. The largemouth bass is the state fish of Alabama (official freshwater fish), Georgia, Mississippi, and Florida (state freshwater fish).
Description.
The largemouth bass is an olive-green fish, in the North East right after ice-out, it most often has a gray color, marked by a series of dark, sometimes black, blotches forming a jagged horizontal stripe along each flank. The upper jaw (maxilla) of a largemouth bass extends beyond the rear margin of the orbit. In comparison to age, a female bass is larger than a male. The largemouth is the largest of the black basses, reaching a maximum recorded overall length of and a maximum unofficial weight of . The fish lives 16 years on average (give or take a few years).
Feeding.
The juvenile largemouth bass consumes mostly small bait fish, scuds, small shrimp, and insects. Adults consume smaller fish (bluegill, banded killifish), snails, crawfish (crayfish), frogs, snakes, salamanders, bats and even small water birds, mammals, and baby alligators. In larger lakes and reservoirs, adult bass occupy deeper water than younger fish, and shift to a diet consisting almost entirely of smaller fish like shad, yellow perch, ciscoes, shiners, and sunfish. It also consumes younger members of larger fish species, such as pike, catfish, trout, walleye, white bass, striped bass, and even smaller black bass. Prey items can be as large as 50% of the bass's body length or larger.
Studies of prey utilization by largemouths show that in weedy waters, bass grow more slowly due to difficulty in acquiring prey. Less weed cover allows bass to more easily find and catch prey, but this consists of more open-water baitfish. With little or no cover, bass can devastate the prey population and starve or be stunted. Fisheries managers must consider these factors when designing regulations for specific bodies of water. Under overhead cover, such as overhanging banks, brush, or submerged structure, such as weedbeds, points, humps, ridges, and drop-offs, the largemouth bass uses its senses of hearing, sight, vibration, and smell to attack and seize its prey. Adult largemouth are generally apex predators within their habitat, but they are preyed upon by many animals while young.
Notably in the Great Lakes Region, "Micropterus salmoides" along with many other species of native fish have been known to prey upon the invasive round goby ("Neogobius melanostomus"). Remains of said fish have been found inside the stomachs of largemouth bass consistently. This feeding habit may impact the ecosystem positively, but more research must be conducted to verify this. Note that it is illegal to use "Neogobius melanostomus" as bait in the Great Lakes Region.
Angling.
Largemouth bass are keenly sought after by anglers and are noted for the excitement of their fight. The fish will often become airborne in their effort to throw the hook, but many say that their cousin species, the smallmouth bass, can beat them pound for pound. Anglers most often fish for largemouth bass with lures such as plastic worms (and other plastic baits), jigs, crankbaits and spinnerbaits. A recent trend is the use of large swimbaits to target trophy bass that often forage on juvenile rainbow trout in California. Fly fishing for largemouth bass may be done using both topwater and worm imitations tied with natural or synthetic materials. Live bait, such as nightcrawlers, minnows, frogs, or crawfish can also be productive. In fact, large golden shiners are a popular live bait used to catch trophy bass, especially when they are sluggish in the heat of summer or in the cold of winter. Largemouth bass usually hang around big patches of weeds and other shallow water cover. These fish are very capable of surviving in a wide variety of climates and waters. They are perhaps, one of North America's most tolerant fish. 
The world record largemouth according to IGFA is shared by Manabu Kurita and George W. Perry, Kurita's bass was caught from Lake Biwa in Japan on July 2, 2009 and weighed 10.12 kg (22 lbs 4oz.) Perry's bass was caught June 2, 1932 from Montgomery Lake in Georgia and weighed 10.09 kg (22 lbs 4oz.) This record is shared because the IGFA states a new record must beat the old record by 2 ounces.
Strong cultural pressure among largemouth bass anglers encourages the practice of catch and release, especially the larger specimens, mainly because larger specimens are usually breeding females that contribute heavily to future sport fishing stocks. Largemouth bass, if handled with care, respond well to catch and release. They have a white, slightly mushy meat, lower quality than that of the smallmouth bass, bluegill, yellow perch, crappie or walleye. Small largemouth, 10-14 inches, can be quite delicious when the water temperature is low but the large fish should be released.
Invasive species.
The largemouth bass has been introduced into many other regions and countries due to its popularity as a sport fish. It causes the decline, displacement or extinctions of species in its new habitat through predation and competition, for example in Namibia. They are an invasive species in the Canadian province of New Brunswick, and are a danger to native fish fry. They have also been blamed for the extinction of the Atitlan Grebe, a large waterbird which once inhabited Lake Atitlan, Guatemala. In 2011, researchers found that in streams and rivers in the Iberian Peninsula, juvenile largemouth bass were able to demonstrate trophic plasticity, meaning that they were able to adjust their feeding habits to obtain the necessary amount of energy needed to survive. The ability to do such, allows them to be successful as invasive species in relatively stable aquatic food webs. Similarly, a study done in Japan showed that the introduction of both largemouth bass and bluegill into farm ponds would have increases in the numbers of benthic organisms, resulting from the predation on fishes, crustaceans, and nymphal odonates by the bass. The largemouth bass has been causing sharp decreases in native fish populations in Japan since 1996, especially in bitterling fish in Lake Izunuma-Uchinuma.

</doc>
<doc id="48844" url="https://en.wikipedia.org/wiki?curid=48844" title="Redeye bass">
Redeye bass

The Redeye bass ("Micropterus coosae") is a species of freshwater fish in the sunfish family (Centrarchidae) native to the Coosa River system of Georgia, Alabama. The waters it is normally found in are cool streams and rivers in the foothills of mountains.
Systematics.
In 2013, "M. coosae" was split into five species with "M. coosae" restricted to the Coosa River system. "M. cahabae" of the Cahaba River system, "M. chattahoochae" of the Chattahoochee River system, "M. tallapoosae" of the Tallapoosa River system and "M. warriorensis" of the Black Warrior River system were all recognized as separate species.
Description.
The upper jaw (maxilla) extends to the back of the eye, which is usually red. The redeye or Coosa bass is an elongate, slender fish with a large mouth that extends to or slightly behind the rear margin of the eye. The dorsal fin contains nine to 11 (usually 10) spines and 11 to 13 (usually 12) rays, and the area between the two is only slightly notched. The anal fin contains three spines and nine to 11 (usually 10) rays. The complete lateral line has from 63 to 74 scales. Scales above the lateral line number 12 or 13. A small tooth patch is present on the tongue. The back and sides are generally olive to brown with darker brown mottling. Adults have several horizontal rows of dark spots on the lower sides and venter. Breeding males have a light bluish green color on the lower head and throat. On juveniles, the sides of the body usually have 10 to 12 dark blotches that do not join to form a lateral stripe. The upper and lower margins of the caudal fin are edged in white, a useful feature for separating redeye bass from both smallmouth bass and shoal bass.
Growing to a maximum reported overall length of , the redeye bass is one of the smaller black basses. The probable world record for Redeye bass is from Lake Jocassee in South Carolina. Many Redeye bass world record listings, especially those over are actually records for the shoal bass (Micropterus cataractae) which was commonly called "Redeye bass".
Its main food tends to be insects.
The rock bass ("Ambloplites rupestris"), a distinct species of Centrarchid, is sometimes called the redeye or redeye bass in Canada.

</doc>
<doc id="48845" url="https://en.wikipedia.org/wiki?curid=48845" title="Reims">
Reims

Reims (; also spelt Rheims; ), a city in the Alsace-Champagne-Ardenne-Lorraine region of France, lies east-northeast of Paris. The 2013 census recorded 182,592 inhabitants ("Rémoises" (feminine) and "Rémois" (masculine)) in the city of Reims proper (the "commune"), and 317,611 inhabitants in the metropolitan area ("aire urbaine").
Founded by the Gauls, it became a major city during the period of the Roman Empire. Reims played a prominent ceremonial role in French monarchical history as the traditional site of the crowning of the kings of France. The Cathedral of Reims (damaged by the Germans during the First World War but restored since) housed the Holy Ampulla ("Sainte Ampoule") containing the "Saint Chrême" (chrism), allegedly brought by a white dove (the Holy Spirit) at the baptism of Clovis in 496. It was used for the anointing, the most important part of the coronation of French kings.
Administration.
Reims functions as a subprefecture of the department of Marne, in the administrative region of Alsace-Champagne-Ardenne-Lorraine. Although Reims is by far the largest commune in both its region and department, Châlons-en-Champagne is the capital and prefecture of both.
History.
Before the Roman conquest of northern Gaul, Reims, founded circa 80 BC as "*Durocorteron" ("round fortress"; in Latin: "Durocortōrum"), served as the capital of the tribe of the Remi — whose name the town would subsequently echo. In the course of Julius Caesar's conquest of Gaul (58–51 BC), the Remi allied themselves with the Romans, and by their fidelity throughout the various Gallic insurrections secured the special favour of the imperial power. At its height in Roman times the city had a population in the range of 30,000 - 50,000 or perhaps up to 100,000.
Christianity had become established in the city by 260, at which period Saint Sixtus of Reims founded the bishopric of Reims. The consul Jovinus, an influential supporter of the new faith, repelled the Alamanni who invaded Champagne in 336; but the Vandals captured the city in 406 and slew Bishop Nicasius; and in 451 Attila the Hun put Reims to fire and sword.
In 496 – ten years after Clovis, King of the Salian Franks, won his victory at Soissons (486) — Remigius, the bishop of Reims, baptized him using the oil of the sacred phial – purportedly brought from heaven by a dove for the baptism of Clovis and subsequently preserved in the Abbey of Saint-Remi. For centuries the events at the crowning of Clovis I became a symbol used by the monarchy to claim the divine right to rule.
Meetings of Pope Stephen II (752–757) with Pepin the Short, and of Pope Leo III (795–816) with Charlemagne (died 814), took place at Reims; and here Pope Stephen IV crowned Louis the Debonnaire in 816. Louis IV gave the city and countship of Reims to the archbishop Artaldus in 940. Louis VII (reigned 1137–1180) gave the title of duke and peer to William of Champagne, archbishop from 1176 to 1202, and the archbishops of Reims took precedence over the other ecclesiastical peers of the realm.
By the 10th century Reims had become a centre of intellectual culture. Archbishop Adalberon (in office 969 to 988), seconded by the monk Gerbert (afterwards (from 999 to 1003) Pope Silvester II), founded schools which taught the classical "liberal arts". (Adalberon also played a leading role in the dynastic revolution which elevated the Capetian dynasty in the place of the Carolingians.)
The archbishops held the important prerogative of the consecration of the kings of France – a privilege which they exercised (except in a few cases) from the time of Philippe II Augustus (anointed 1179, reigned 1180–1223) to that of Charles X (anointed 1825). Louis VII granted the city a communal charter in 1139. The Treaty of Troyes (1420) ceded it to the English, who had made a futile attempt to take it by siege in 1360; but French patriots expelled them on the approach of Joan of Arc, who in 1429 had Charles VII consecrated in the cathedral. Louis XI cruelly suppressed a revolt at Reims, caused in 1461 by the salt tax. During the French Wars of Religion the city sided with the Catholic League (1585), but submitted to Henri IV after the battle of Ivry (1590).
In the invasions of the War of the Sixth Coalition in 1814, anti-Napoleonic allied armies captured and re-captured Reims; in 1870–1871, during the Franco-Prussian War, the victorious Germans made it the seat of a governor-general and impoverished it with heavy requisitions.
In August 1909 Reims hosted the first international aviation meet, the "Grande Semaine d'Aviation de la Champagne". Major aviation personages such as Glenn Curtiss, Louis Blériot and Louis Paulhan participated.
Hostilities in World War I greatly damaged the city. German bombardment and a subsequent fire in 1914 did severe damage to the cathedral. The ruined cathedral became one of the central images of anti-German propaganda produced in France during the war, which presented it, along with the ruins of the Cloth Hall at Ypres and the University Library in Louvain, as evidence that German aggression targeted cultural landmarks of European civilization.
From the end of World War I to the an international effort to restore the cathedral from the ruins has continued. The Palace of Tau, St Jacques Church and the Abbey of St Remi also were protected and restored. The collection of preserved buildings and Roman ruins remains monumentally impressive.
During World War II the city suffered additional damage. But in Reims, at 2:41 on the morning of 7 May 1945, General Eisenhower and the Allies received the unconditional surrender of the German Wehrmacht. General Alfred Jodl, German Chief-of-Staff, signed the surrender at the Supreme Headquarters Allied Expeditionary Force (SHAEF) as the representative for German President Karl Dönitz.
The British statesman Leslie Hore-Belisha died of a cerebral haemorrhage while making a speech at the Reims "hôtel de ville" (city hall) in February 1957.
Sights.
Streets and squares.
The principal squares of Reims include the Place Royale, with a statue of Louis XV, and the Place Cardinal-Luçon, with an equestrian statue of Joan of Arc. The Rue de Vesle, the main commercial street (continued under other names), traverses the city from southwest to northeast, passing through the Place Royale.
The Place Drouet d'Erlon in the city centre contains many lively restaurants and bars, as well as several attractive statues and fountains. During the summer it fills with people sitting outside the many cafés enjoying the summer sun, and in December it has a lively and charming Christmas market.
Gallo-Roman antiquities.
The oldest monument in Reims, the "Porte de Mars" ("Mars Gate", so called from a temple to Mars in the neighbourhood), a triumphal arch 108 feet in length by 43 in height, consists of three archways flanked by columns. Popular tradition tells that the Remi erected it in honour of Augustus when Agrippa made the great roads terminating at the city, but it probably belongs to the 3rd or 4th century. The Mars Gate was one of 4 Roman gates to the city walls, which were restored at the time of the Norman Invasion of northern France in the 9th century. In its vicinity a curious mosaic, measuring 36 feet by 26, with thirty-five medallions representing animals and gladiators, was discovered in 1860.
Note too the Gallo-Roman sarcophagus, allegedly that of the 4th-century consul Jovinus, preserved in the archaeological museum in the cloister of the abbey of Saint-Remi.
Cathedral of "Notre-Dame de Reims".
Many people know Reims for its cathedral, "Notre-Dame de Reims", formerly the place of coronation of the kings of France. The cathedral became a UNESCO World Heritage Site in 1991, along with the former Abbey of Saint-Remi and the Palace of Tau.
Palace of Tau.
The archiepiscopal palace, built between 1498 and 1509, and in part rebuilt in 1675, served as the residence of the kings of France on the occasion of their coronations. The salon ("salle du Tau"), where the royal banquet took place, has an immense stone chimney that dates from the 15th century. The chapel of the archiepiscopal palace consists of two storeys, of which the upper still () serves as a place of worship. Both the chapel and the "salle du Tau" have decorative tapestries of the 17th century, known as the Perpersack tapestries, after the Flemish weaver who executed them. The palace opened to the public in 1972 as a museum containing such exhibits as statues formerly displayed by the cathedral, treasures of the cathedral from past centuries, and royal attire from coronations of French kings. 
Saint Remi Basilica.
Saint Remi Basilica, about a mile from the Cathedral of Notre Dame of Reims, takes its name from the fifth-century Saint Remi — revered as the patron saint of the inhabitants of Reims for more than fifteen centuries. The basilica almost approaches the cathedral in size. Adjacent to the basilica stands an important abbey, formerly known as the Royal Abbey of St Remi. The abbey sought to trace its heritage back to St Remi, while the present abbey building dates back to the 17th and 18th centuries.
The Saint Remi Basilica dates from the 11th, 12th, 13th and 15th centuries. Most of the construction of the church finished in the 11th century, with additions made later. The nave and transepts, Romanesque in style, date mainly from the earliest, the façade of the south transept from the latest of those periods, the choir and apse chapels from the 12th and 13th centuries. The 17th and 19th centuries saw further additions. The building suffered greatly in World War I, and the meticulous restoration work of architect rebuilt it from its ruins over the following 40 years. it remains the seat of an active Catholic parish holding regular worship services and welcoming pilgrims. It has been classified as an historical monument since 1841 and is one of the pinnacles of the history of art and of the history of France.
Several royal and archepiscopal figures lie buried in the basilica, but in unidentified graves. They include:
The public can visit the abbey building, the Saint-Remi Museum. The abbey closed in the wake of the French Revolution (the government had all French monasteries dissolved in February 1790). The museum exhibits include tapestries from the 16th century donated by the archbishop Robert de Lenoncourt (uncle of the cardinal of the same name), marble capitals from the fourth century AD, furniture, jewellery, pottery, weapons and glasswork from the sixth to eighth centuries, medieval sculpture, the façade of the 13th-century musicians' House, remnants from an earlier abbey building, and also exhibits of Gallo-Roman arts and crafts and a room of pottery, jewellery and weapons from Gallic civilization, as well as an exhibit of items from the Palaeolithic to the Neolithic periods.
Another section of the museum features a permanent military exhibition.
Forts.
In 1874 the construction of a chain of detached forts started in the vicinity, the French Army having selected Reims as one of the chief defences of the northern approaches to Paris. Atop the ridge of St Thierry stands a fort of the same name, which with the neighbouring work of Chenay closes the west side of the place. To the north the hill of Brimont has three works guarding the Laon railway and the Aisne canal. Farther east, on the old Roman road, stands the Fort de Fresnes. Due east, the hills of Arnay are crowned with five large and important works which cover the approaches from the upper Aisne. Fort de la Pompelle, which hosts a World War I museum featuring a rich collection of German uniforms, and Montbré close the southeast side, and the Falaise hills on the southwest are open and unguarded. The perimeter of the defences measures just under 22 miles, and the forts are at a mean distance of from the centre of the city.
Monument to the Black Army of Reims.
The original monument was erected in 1924 where the Boulevard Henry Vasnier meets the Avenue du Général Giraud. The first stone was place by André Maginot, Minister of War on 29 October 1922. This ceremony was also addressed by Blaise Diagne, the Senegalese political leader. In July 1924 the monument was inaugurated with a Military and Sports fete presided over by Édouard Daladier, the Minister of the colonies. General Louis Archinard was the president of the committee that supervised the erection of the monument, highlighting the role of African troops of the 1st Colonial Infantry Corps in the defense of Reims from the German Army in 1918. They were particularly renowned for their tenacious defence of Fort de la Pompelle. 10,000 people attended the fete which was held immediately following the inauguration. The original monument, consisting of five figures, was a replica of a similar monument erected in Bamako, Mali in January 1924. The monument was dismantled during the German occupation in September 1940. In September 1958, on the occasion of the fortieth anniversary of the Defence of Reims, a new monument was started. This was completed in time for a second inauguration ceremony on 6 October 1963, with Pierre Messmer, Minister of Armies, Jean Sainteny, Minister of veterans, Jacques Foccart, secretary general of the Communauté et les affaires africaines et malgaches, and General Georges Catroux, grand chancellor of the Légion d'honneur. In 2008, on the occasion of the ninetieth anniversary of the defence of Reims, a major ceremony was held in remembrance of the Black Army of Reims, attended by Jean-Marie Bockel, Rama Yade and Adeline Hazan.
Other buildings.
The Church of St Jacques dates from the 13th to the 16th centuries. A few blocks from the cathedral, it stands in a neighborhood of shopping and restaurants. What remains of the Abbey of St. Denis has become a Fine Arts Museum. The old College of the Jesuits also survives as a museum. The churches of St Maurice (partly rebuilt in 1867), St André, and St Thomas (erected from 1847 to 1853, under the patronage of Cardinal Gousset, now buried within its walls) also draw tourists.
The Temple protestant de Reims was designed by Charles Letrosne in a flamboyant neo-Gothic style. Originally the walls were lavishly decorated in Art Deco style by Gustave Louis Jaulmes, but in 1973 the walls were painted white, giving an austere appearance.
The Foujita Chapel (1966), designed and decorated by the Japanese School of Paris artist Tsuguharu Foujita, became famed for its frescos. It was listed as an historic monument in 1992.
The city hall ("hôtel de ville"), erected in the 17th century and enlarged in the 19th, features a pediment with an equestrian statue of Louis XIII (reigned 1610 to 1643).
The Surrender Museum is the building in which on 7 May 1945, General Eisenhower and the Allies received the unconditional surrender of the German Wehrmacht.
The Carnegie library, the former public library built with money donated by Andrew Carnegie to the city of Reims after World War I, is a remarkable example of Art Deco in France.
Transport.
Reims is served by two main railway stations: Gare de Reims in the city centre, the hub for regional transport, and the new Gare de Champagne-Ardenne TGV southwest of the city with high-speed rail connections to Paris, Metz, Nancy and Strasbourg. The motorways A4 (Paris-Strasbourg), A26 (Calais-Langres) and A34 intersect near Reims.
Public transport within the city consists of buses and a tramway, the latter opened in 2011.
Wine.
Reims, along with Épernay and Ay, functions as one of the centres of champagne production. Many of the largest champagne-producing houses, known as "les grandes marques", have their headquarters in Reims, and most open for tasting and tours. Champagne ages in the many caves and tunnels under Reims, which form a sort of maze below the city. Carved from chalk, some of these passages date back to Roman times.
Sport.
Between 1925 and 1969 Reims hosted the "Grand Prix de la Marne" automobile race at the circuit of "Reims-Gueux". The French Grand Prix took place here 14 times between 1938 and 1966.
The city has hosted the Reims Marathon since 1984.
Notable residents.
Those born in Reims include:
Higher education.
The URCA (University of Rheims Champagne-Ardenne|Université de Reims Champagne-Ardenne) was created in 1548. This multidisciplinary university develops innovative, fundamental and applied research. It provides more than 18 000 students in Rheims (22 000 in Champagne-Ardenne) with a wide initial undergraduate studies program which corresponds to society's needs in all domains of the knowledge. The university also accompanies independent or company backed students in continuing professional development training.
The Institut d'Etudes politiques de Paris, the leading French university in social and political sciences, also known as Sciences Po, opened a new campus in the in 2010. This Euro-American campus will welcome more than 1500 students in 2015. In 2012 the first Reims Model United Nations was launched, which gathered 200 international students from all the Sciences Po campuses. Daniel Rondeau, the ambassador of France to UNESCO and a French writer, is the patron of the event. The Reims Management School is also one of the main schools in Reims.
International relations.
Twin towns – sister cities.
Reims is twinned with:

</doc>
<doc id="48846" url="https://en.wikipedia.org/wiki?curid=48846" title="Maya">
Maya

Maya may refer to:

</doc>
<doc id="48847" url="https://en.wikipedia.org/wiki?curid=48847" title="Deflation">
Deflation

In economics, deflation is a decrease in the general price level of goods and services. Deflation occurs when the inflation rate falls below 0% (a negative inflation rate). Inflation reduces the real value of money over time; conversely, deflation increases the real value of money the currency of a national or regional economy. This allows one to buy more goods with the same amount of money over time.
Economists generally believe that deflation is a problem in a modern economy because it increases the real value of debt, and may aggravate recessions and lead to a deflationary spiral.
Although the values of capital assets are often casually said to "deflate" when they decline, this usage does not comply with the definition of deflation; a more accurate description for a decrease in the value of a capital asset is economic depreciation. Another term, the accounting conventions of depreciation are standards to determine a decrease in values of capital assets when market values are not readily available or practical. Deflation is distinct from disinflation, a slow-down in the inflation rate, i.e. when inflation declines to lower levels.
Causes and corresponding types.
In the IS–LM model (investment and saving equilibrium liquidity preference and money supply equilibrium model), deflation is caused by a shift in the supply and demand curve for goods and services, particularly a fall in the aggregate level of demand. That is, there is a fall in how much the whole economy is willing to buy, and the going price for goods. Because the price of goods is falling, consumers have an incentive to delay purchases and consumption until prices fall further, which in turn reduces overall economic activity. Since this idles the productive capacity, investment also falls, leading to further reductions in aggregate demand. This is the deflationary spiral. An answer to falling aggregate demand is stimulus, either from the central bank, by expanding the money supply, or by the fiscal authority to increase demand, and to borrow at interest rates which are below those available to private entities.
In more recent economic thinking, deflation is related to risk: where the risk-adjusted return on assets drops to negative, investors and buyers will hoard currency rather than invest it, even in the most solid of securities. This can produce a liquidity trap or it may lead to shortages that entice investments yielding more jobs and commodity production. A central bank cannot, normally, charge negative interest for money, and even charging zero interest often produces less stimulative effect than slightly higher rates of interest. In a closed economy, this is because charging zero interest also means having zero return on government securities, or even negative return on short maturities. In an open economy it creates a carry trade, and devalues the currency. A devalued currency produces higher prices for imports without necessarily stimulating exports to a like degree.
In monetarist theory, deflation must be associated with either a reduction in the money supply, a reduction in the velocity of money or an increase in the number of transactions. But any of these may occur separately without deflation. It may be attributed to a dramatic contraction of the money supply, or to adherence to a gold standard or other external monetary base requirement.
However, deflation is the natural condition of hard currency economies when the supply of money does not grow as quickly as population and the economy. When this happens, the available amount of hard currency per person falls, in effect making money more scarce, and consequently, the purchasing power of each unit of currency increases. Deflation occurs when improvements in production efficiency lower the overall price of goods. Competition in the marketplace often prompts those producers to apply at least some portion of these cost savings into reducing the asking price for their goods. When this happens, consumers pay less for those goods, and consequently deflation has occurred, since purchasing power has increased.
Rising productivity and reduced transportation cost created structural deflation during the acceleration productivity era of from 1870–1900, but there was mild inflation for about a decade before the establishment of the Federal Reserve in 1913. There was inflation during World War I, but deflation returned again after that war and during the 1930s depression. Most nations abandoned the gold standard in the 1930s. There is less reason to expect deflation, aside from the collapse of speculative asset classes, under a fiat monetary system with low productivity growth.
In mainstream economics, deflation may be caused by a combination of the supply and demand for goods and the supply and demand for money, specifically the supply of money going down and the supply of goods going up. Historic episodes of deflation have often been associated with the supply of goods going up (due to increased productivity) without an increase in the supply of money, or (as with the Great Depression and possibly Japan in the early 1990s) the demand for goods going down combined with a decrease in the money supply. Studies of the Great Depression by Ben Bernanke have indicated that, in response to decreased demand, the Federal Reserve of the time decreased the money supply, hence contributing to deflation.
Demand-side causes are:
Supply-side causes are:
Debt deflation.
Debt deflation is a complicated phenomenon associated with the end of long-term credit cycles. It was proposed as a theory by Irving Fisher (1933) to explain the deflation of the Great Depression.
Money supply side deflation.
From a monetarist perspective, deflation is caused primarily by a reduction in the velocity of money and/or the amount of money supply per person.
A historical analysis of money velocity and monetary base shows an inverse correlation: for a given percentage decrease in the monetary base the result is nearly equal percentage increase in money velocity. This is to be expected because monetary base (MB), velocity of base money (VB), price level (P) and real output (Y) are related by definition: MBVB = PY. However, it is important to note that the monetary base is a much narrower definition of money than M2 money supply. Additionally, the velocity of the monetary base is interest rate sensitive, the highest velocity being at the highest interest rates.
In the early history of the United States there was no national currency and an insufficient supply of coinage. Banknotes were the majority of the money in circulation. During financial crises many banks failed and their notes became worthless. Also, banknotes were discounted relative to gold and silver, the discount depending on the financial strength of the bank.
In recent years changes in the money supply have historically taken a long time to show up in the price level, with a rule of thumb lag of at least 18 months. More recently Alan Greenspan cited the time lag as taking between 12 and 13 quarters. Bonds, equities and commodities have been suggested as reservoirs for buffering changes in money supply.
Credit deflation.
In modern credit-based economies, deflation may be caused by the central bank "initiating" higher interest rates (i.e., to 'control' inflation), thereby possibly popping an asset bubble. In a credit-based economy, a slow-down or fall in lending leads to less money in circulation, with a further sharp fall in money supply as confidence reduces and velocity weakens, with a consequent sharp fall-off in demand for employment or goods. The fall in demand causes a fall in prices as a supply glut develops. This becomes a deflationary spiral when prices fall below the costs of financing production, or repaying debt levels incurred at the prior price level. Businesses, unable to make enough profit no matter how low they set prices, are then liquidated. Banks get assets which have fallen dramatically in value since their mortgage loan was made, and if they sell those assets, they further glut supply, which only exacerbates the situation. To slow or halt the deflationary spiral, banks will often withhold collecting on non-performing loans (as in Japan, and most recently America and Spain). This is often no more than a stop-gap measure, because they must then restrict credit, since they do not have money to lend, which further reduces demand, and so on.
Historical examples of credit deflation.
In the early economic history of the United States, cycles of inflation and deflation correlated with capital flows between regions, with money being loaned from the financial center in the Northeast to the commodity producing regions of the -West and South. In a procyclical manner, prices of commodities rose when capital was flowing in, that is, when banks were willing to lend, and fell in the depression years of 1818 and 1839 when banks called in loans. Also, there was no national paper currency at the time and there was a scarcity of coins. Most money circulated as banknotes, which typically sold at a discount according to distance from the issuing bank and the bank's perceived financial strength. When banks failed their notes were redeemed for bank reserves, which often did not result in payment at par value, and sometimes the notes became worthless. Notes of weak surviving banks traded at steep discounts. During the Great Depression, people who owed money to a bank whose deposits had been frozen would sometimes buy bank books (deposits of other people at the bank) at a discount and use them to pay off their debt at par value.
Deflation occurred periodically in the U.S. during the 19th century (the most important exception was during the Civil War). This deflation was at times caused by technological progress that created significant economic growth, but at other times it was triggered by financial crises — notably the Panic of 1837 which caused deflation through 1844, and the Panic of 1873 which triggered the Long Depression that lasted until 1879. These deflationary periods preceded the establishment of the U.S. Federal Reserve System and its active management of monetary matters. Episodes of deflation have been rare and brief since the Federal Reserve was created (a notable exception being the Great Depression) while U.S. economic progress has been unprecedented.
A financial crisis in England in 1818 caused banks to call in loans and curtail new lending, draining specie out of the U.S. The Bank of the United States also reduced its lending. Prices for cotton and tobacco fell. The price of agricultural commodities also were pressured by a return of normal harvests following 1816, the "year without a summer", that caused large scale famine and high agricultural prices.
There were several causes of the deflation of the severe depression of 1839-43, which included an oversupply of agricultural commodities (importantly cotton) as new cropland came into production following large federal land sales a few years earlier, banks requiring payment in gold or silver, the failure of several banks, default by several states on their bonds and British banks cutting back on specie flow to the U.S.
This cycle has been traced out on the broad scale during the Great Depression. Partly because of overcapacity and market saturation and partly as a result of the Smoot-Hawley Tariff Act, international trade contracted sharply, severely reducing demand for goods, thereby idling a great deal of capacity, and setting off a string of bank failures. A similar situation in Japan, beginning with the stock and real estate market collapse in the early 1990s, was arrested by the Japanese government preventing the collapse of most banks and taking over direct control of several in the worst condition.
Scarcity of official money.
The United States had no national paper money until 1862 (greenbacks used to fund the Civil War), but these notes were discounted to gold until 1877. There was also a shortage of U.S. minted coins. Foreign coins, such as Mexican silver, were commonly used. At times banknotes were as much as 80% of currency in circulation before the Civil War. In the financial crises of 1818–19 and 1837–41, many banks failed, leaving their money to be redeemed below par value from reserves. Sometimes the notes became worthless, and the notes of weak surviving banks were heavily discounted. The Jackson administration opened branch mints, which over time increased the supply of coins. Following the 1848 finding of gold in the Sierra Nevada, enough gold came to market to devalue gold relative to silver. To equalize the value of the two metals in coinage, the U.S. mint slightly reduced the silver content of new coinage in 1853.
When structural deflation appeared in the years following 1870, a common explanation given by various government inquiry committees was a scarcity of gold and silver, although they usually mentioned the changes in industry and trade we now call productivity. However, David A. Wells (1890) notes that the U. S. money supply during the period 1879-1889 actually rose 60%, the increase being in gold and silver, which rose against the percentage of national bank and legal tender notes. Furthermore, Wells argued that the deflation only lowered the cost of goods that benefited from recent improved methods of manufacturing and transportation. Goods produced by craftsmen did not decrease in price, nor did many services, and the cost of labor actually increased. Also, deflation did not occur in countries that did not have modern manufacturing, transportation and communications.
By the end of the 19th century, the deflation ended and turned to mild inflation. William Stanley Jevons predicted rising gold supply would cause inflation decades before it actually did. Irving Fisher blamed the worldwide inflation of the pre WW I years on rising gold supply.
In economies with an unstable currency, barter and other alternate currency arrangements such as dollarization are common, and therefore when the 'official' money becomes scarce (or unusually unreliable), commerce can still continue (e.g., most recently in Zimbabwe). Since in such economies the central government is often unable, even if it were willing, to adequately control the internal economy, there is no pressing need for individuals to acquire official currency except to pay for imported goods. In effect, barter acts as a protective tariff in such economies, encouraging local consumption of local production. It also acts as a spur to mining and exploration, because one easy way to make money in such an economy is to dig it out of the ground.
Effects.
Deflation was present during most economic depressions in US history Deflation is generally regarded negatively, as it causes a transfer of wealth from borrowers and holders of illiquid assets, to the benefit of savers and of holders of liquid assets and currency, and because confused pricing signals cause malinvestment, in the form of under-investment.
In this sense it is the opposite of the more usual scenario of inflation, whose effect is to tax currency holders and lenders (savers) and use the proceeds to subsidize borrowers, including governments, and to cause malinvestment as overinvestment. Thus inflation encourages short term consumption and can similarly over-stimulate investment in projects that may not be worthwhile in real terms (for example the housing or Dot-com bubbles), while deflation retards investment even when there is a real-world demand not being met. In modern economies, deflation is usually caused by a drop in aggregate demand, and is associated with economic depression, as occurred in the Great Depression and the Long Depression.
Nobel laureate Friedrich Hayek, a libertarian Austrian Economist, stated about the Great Depression deflation:
While an increase in the purchasing power of one's money benefits some, it amplifies the sting of debt for others: after a period of deflation, the payments to service a debt represent a larger amount of purchasing power than they did when the debt was first incurred. Consequently, deflation can be thought of as an effective increase in a loan's interest rate. If, as during the Great Depression in the United States, deflation averages 10% per year, even an interest-free loan is unattractive as it must be repaid with money worth 10% more each year. Under normal conditions, the Fed and most other central banks implement policy by setting a target for a short-term interest rate the overnight federal funds rate in the U.S. and enforcing that target by buying and selling securities in open capital markets. When the short-term interest rate hits zero, the central bank can no longer ease policy by lowering its usual interest-rate target. With interest rates near zero, debt relief becomes an increasingly important tool in managing deflation.
In recent times, as loan terms have grown in length and loan financing (or leveraging) is common among many types of investments, the costs of deflation to lenders has grown larger. Deflation can discourage private investment, because there is reduced expectations on future profits when future prices are lower. Consequently, with reduced private investments, spiraling deflation can cause a collapse in aggregate demand. Without the "hidden risk of inflation", it may become more prudent for institutions to hold on to money, and not to spend or invest it (burying money). They are therefore rewarded by holding money. This "hoarding" behavior is seen as undesirable by most economists, as Hayek points out:
Some believe that, in the absence of large amounts of debt, deflation would be a welcome effect because the lowering of prices increases purchasing power.
Since deflationary periods disfavor debtors (including most farmers), they are often periods of rising populist backlash. For example, in the late 19th century, populists in the US wanted debt relief or to move off the new gold standard and onto a silver standard (the supply of silver was increasing relatively faster than the supply of gold, making silver less deflationary than gold), bimetal standard, or paper money like the recently ended Greenbacks.
Deflationary spiral.
A "deflationary spiral" is a situation where decreases in price lead to lower production, which in turn leads to lower wages and demand, which leads to further decreases in price. Since reductions in general price level are called deflation, a deflationary spiral occurs when reductions in price lead to a vicious circle, where a problem exacerbates its own cause. In science, this effect is also known as a positive feedback loop. Another economic example of this principle is a bank run.
The Great Depression was regarded by some as a deflationary spiral. A deflationary spiral is the modern macroeconomic version of the general glut controversy of the 19th century. Another related idea is Irving Fisher's theory that excess debt can cause a continuing deflation. Whether deflationary spirals can actually occur is controversial, with its possibility being disputed by freshwater economists (including the Chicago school of economics) and Austrian School economists.
Counteracting deflation.
During severe deflation, targeting an interest rate (the usual method of determining how much currency to create) may be ineffective, because even lowering the short-term interest rate to zero may result in a real interest rate which is too high to attract credit-worthy borrowers. In the 21st century negative interest rate has been tried, but it can't be too negative, since people might withdraw cash from bank accounts if they have negative interest rate. Thus the central bank must directly set a target for the quantity of money (called "quantitative easing") and may use extraordinary methods to increase the supply of money, e.g. purchasing financial assets of a type not usually used by the central bank as reserves (such as mortgage-backed securities). Before he was Chairman of the United States Federal Reserve, Ben Bernanke claimed in 2002, "...sufficient injections of money will ultimately always reverse a deflation", although Japan's deflationary spiral was not broken by this very sort of quantitative easing.
Until the 1930s, it was commonly believed by economists that deflation
would cure itself. As prices decreased, demand would naturally increase and the economic system would correct itself without outside intervention.
This view was challenged in the 1930s during the Great Depression. Keynesian economists argued that the economic system was not self-correcting with respect to deflation and that governments and central banks had to take active measures to boost demand through tax cuts or increases in government spending. Reserve requirements from the central bank were high compared to recent times. So were it not for redemption of currency for gold (in accordance with the gold standard), the central bank could have effectively increased money supply by simply reducing the reserve requirements and through open market operations (e.g., buying treasury bonds for cash) to offset the reduction of money supply in the private sectors due to the collapse of credit (credit is a form of money).
With the rise of monetarist ideas, the focus in fighting deflation was put on expanding demand by lowering interest rates (i.e., reducing the "cost" of money). This view has received a setback in light of the failure of accommodative policies in both Japan and the US to spur demand after stock market shocks in the early 1990s and in 2000–02, respectively. Austrian economists worry about the inflationary impact of monetary policies on asset prices. Sustained low real rates can cause higher asset prices and excessive debt accumulation. Therefore, lowering rates may prove to be only a temporary palliative, aggravating an eventual debt deflation crisis.
With interest rates near zero, debt relief becomes an increasingly important tool in managing deflation.
Special borrowing arrangements.
When the central bank has lowered nominal interest rates to zero, it can no longer further stimulate demand by lowering interest rates. This is the famous liquidity trap. When deflation takes hold, it requires "special arrangements" to lend money at a zero nominal rate of interest (which could still be a very high "real" rate of interest, due to the "negative" inflation rate) in order to artificially increase the money supply.
Historical examples.
In Hong Kong.
Following the Asian financial crisis in late 1997, Hong Kong experienced a long period of deflation which did not end until the 4th quarter of 2004. Many East Asian currencies devalued following the crisis. The Hong Kong dollar however, was pegged to the US dollar, leading to an adjustment instead by a deflation of consumer prices. The situation was worsened by the increasingly cheap exports from Mainland China, and "weak consumer confidence" in Hong Kong. This deflation was accompanied by an economic slump that was more severe and prolonged than those of the surrounding countries that devalued their currencies in the wake of the Asian financial crisis.
In Ireland.
In February 2009, Ireland's Central Statistics Office announced that during January 2009, the country experienced deflation, with prices falling by 0.1% from the same time in 2008. This is the first time deflation has hit the Irish economy since 1960. Overall consumer prices decreased by 1.7% in the month.
Brian Lenihan, Ireland's Minister for Finance, mentioned deflation in an interview with RTÉ Radio. According to RTÉ's account, "Minister for Finance Brian Lenihan has said that deflation must be taken into account when Budget cuts in child benefit, public sector pay and professional fees are being considered. Mr Lenihan said month-on-month there has been a 6.6% decline in the cost of living this year."
This interview is notable in that the deflation referred to is not discernibly regarded negatively by the Minister in the interview. The Minister mentions the deflation as an item of data helpful to the arguments for a cut in certain benefits. The alleged economic harm caused by deflation is not alluded to or mentioned by this member of government. This is a notable example of deflation in the modern era being discussed by a senior financial Minister without any mention of how it might be avoided, or whether it should be.
In Japan.
Deflation started in the early 1990s. The Bank of Japan and the government tried to eliminate it by reducing interest rates and 'quantitative easing', but did not create a sustained increase in broad money and deflation persisted. In July 2006, the zero-rate policy was ended.
Systemic reasons for deflation in Japan can be said to include:
In November 2009, Japan returned to deflation, according to the Wall Street Journal. Bloomberg L.P. reports that consumer prices fell in October 2009 by a near-record 2.2%.
In the United States.
Major deflations in the United States.
There have been four significant periods of deflation in the United States.
The first and most severe was during the depression from 1818-21 when prices of agricultural commodities declined by almost 50%. A credit contraction caused by a financial crisis in England drained specie out of the U.S. The Bank of the United States also contracted its lending. The price of agricultural commodities fell by almost 50% from the high in 1815 to the low in 1821, and did not recover until the late 1830s, although to a significantly lower price level. Most damaging was the price of cotton, the U.S.'s main export. Food crop prices, which had been high because of the famine of 1816 that was caused by the year without a summer, fell after the return of normal harvests in 1818. Improved transportation, mainly from turnpikes, and to a minor extent the introduction of steamboats, significantly lowered transportation costs.
The second was the depression of the late 1830s to 1843, following the Panic of 1837, when the currency in the United States contracted by about 34% with prices falling by 33%. The magnitude of this contraction is only matched by the Great Depression. (See: Historical examples of credit deflation) This "deflation" satisfies both definitions, that of a decrease in prices and a decrease in the available quantity of money. Despite the deflation and depression, GDP rose 16% from 1839-43.
The third was after the Civil War, sometimes called The Great Deflation. It was possibly spurred by return to a gold standard, retiring paper money printed during the Civil War.
The fourth was between 1930–33 when the rate of deflation was approximately 10 percent/year, part of the United States' slide into the Great Depression, where banks failed and unemployment peaked at 25%.
The deflation of the Great Depression occurred partly because there was an enormous contraction of credit (money), bankruptcies creating an environment where cash was in frantic demand, and when the Federal Reserve was supposed to accommodate that demand, it instead contracted the money supply by 30% in enforcement of its new real bills doctrine, so banks toppled one-by-one (because they were unable to meet the sudden demand for cash see fractional-reserve banking). From the standpoint of the Fisher equation (see above), there was a concomitant drop both in money supply (credit) and the velocity of money which was so profound that price deflation took hold despite the increases in money supply spurred by the Federal Reserve.
Minor deflations in the United States.
Throughout the history of the United States, inflation has approached zero and dipped below for short periods of time (negative inflation is deflation). This was quite common in the 19th century, and in the 20th century until the permanent abandonment of the gold standard for the Bretton Woods System in 1948. In the past 60 years, the United States has only experienced deflation two times. Once in 2009 with the financial crisis. The other minor deflation happened in 2015, where the CPI barely broke below 0% at -0.1%.
Some economists believe the United States may have experienced deflation as part of the financial crisis of 2007–10; compare the theory of debt-deflation. Year-on-year, consumer prices dropped for six months in a row to end-August 2009, largely due to a steep decline in energy prices. Consumer prices dropped 1 percent in October, 2008. This was the largest one-month fall in prices in the US since at least 1947. That record was again broken in November, 2008 with a 1.7% decline. In response, the Federal Reserve decided to continue cutting interest rates, down to a near-zero range as of December 16, 2008. In late 2008 and early 2009, some economists feared the US could enter a deflationary spiral. Economist Nouriel Roubini predicted that the United States would enter a deflationary recession, and coined the term "stag-deflation" to describe it. It is the opposite of stagflation, which was the main fear during the spring and summer of 2008. The United States then began experiencing measurable deflation, steadily decreasing from the first measured deflation of -0.38% in March, to July's deflation rate of -2.10%. On the wage front, in October 2009 the state of Colorado announced that its state minimum wage, which is indexed to inflation, is set to be cut, which would be the first time a state has cut its minimum wage since 1938.
In the United Kingdom.
During World War I the British pound sterling was removed from the gold standard. The motivation for this policy change was to finance World War I; one of the results was inflation, and a rise in the gold price, along with the corresponding drop in international exchange rates for the pound. When the pound was returned to the gold standard after the war it was done on the basis of the pre-war gold price, which, since it was higher than equivalent price in gold, required prices to fall to realign with the higher target value of the pound.
The UK experienced deflation of approx 10% in 1921, 14% in 1922, and 3 to 5% in the early 1930s.

</doc>
<doc id="48851" url="https://en.wikipedia.org/wiki?curid=48851" title="Quillback">
Quillback

The Quillback ("Carpiodes cyprinus") is a type of freshwater fish of the sucker family. It is deeper-bodied than most suckers, leading to a carplike appearance. It can be distinguished from carp by the lack of barbels around the mouth. 
Physical description.
The Quillback is a large, ectothermic, deep-bodied fish found throughout North America. It has a small head, humped back and deeply forked caudal fin. The compressed body of the Quillback makes it look flattened when viewed from the side. The Quillback has a subterminal mouth with no barbels, and no nipple-like protrusions on the bottom lip. It has large, reflective, silver cycloid scales that are responsible for giving the Quillback its characteristic silver color. They have a white belly with yellow or orange lower fins. The tail and dorsal fin are usually gray or silver. The Quillback gets its name from the long quill that is formed via the first several fin rays of the dorsal fin. Quillback are typically 15-20 inches on average, weighing between 1 and 4 pounds. However, they can grow up to 26 inches and weigh 10 pounds. The Quillback Carpsucker has a nearly straight, hyper-sensitive lateral line, composed of at least 37 lateral line scales. This helps the fish locate predators and prey.
Distribution and habitat.
The Quillback Carpsucker is found throughout much of North America, from Saskatchewan to Florida, and from South Dakota to Alabama. The Quillback occupies temperate, freshwater habitats. This includes many streams, lakes, channels and rivers. They prefer water that is clear, slow moving, highly productive and moderately deep. The Quillback can commonly be found in the Hudson Bay, Mississippi River Basins, the Great Lakes, and drainages from the Delaware River, Apalachicola River, and the Pearl River. They often comprise a large portion of the biomass of warmwater rivers, but they are very difficult to catch with traditional American angling methods. The quillback carpsucker is closely related to the highfin carpsucker and the river carpsucker. All three species are rarely caught by anglers due to their feeding habits, but they have been caught occasionally on worms, minnows, and artificial lures.
Diet.
Quillback Carpsuckers usually feed in schools. They are omnivores and bottom feeders that prefer lakes, rivers and streams in which the water is clear at the bottom. The school of Quillbacks move slowly over a sand or gravel bottom when they eat. Their typical diet consists of insect larvae and various aquatic vegetation, crustaceans, and protozoa including algae, leaves, mollusks, and clams.
Reproduction.
The Quillback reproduces once yearly, typically in late spring or early summer. The timing of reproduction depends on the water temperature. Ideal temperatures for reproduction are between 7-18 degrees Celsius. Spawning occurs upstream of the typical Quillback habitat, and they migrate in schools to the spawning site. The female Quillback produces between 15,000 to 60,000 eggs, and scatters them in shallow water over a sandy or mud bottom. Fertilization then happens externally, and left in quiet water. Since the Quillback is oviparous, the eggs are hatched outside of the fishes body. The Quillback possesses a polygynandrous mating system, meaning that two or more males have an exclusive sexual relationship with two or more females. The numbers of each sex can vary, and do not need to be equal.
Conservation status.
The Quillback is currently at risk for extinction in various states throughout North America including Vermont, New York and Michigan. Other places prove vulnerability to the species including Alberta, Saskatchewan, Quebec, South Dakota, Kansas, Oklahoma, Arkansas, Louisiana and North Carolina. Quillback Carpsuckers benefit the ecosystem they reside in because they are bottom feeders. Bottom feeders help keep their natural environment clean by feeding on the material at the bottom of the habitat. In Minnesota, and other areas of the United States, the Quillback does not provide any economic benefit to humans because it is not a commercial fish. The Quillback does serve some economic benefit to Mexico.

</doc>
<doc id="48852" url="https://en.wikipedia.org/wiki?curid=48852" title="Market economy">
Market economy

A market economy is an economy in which decisions regarding investment, production, and distribution are based on market determined supply and demand, and prices of goods and services are determined in a free price system. The major defining characteristic of a market economy is that investment decisions and the allocation of producer goods are mainly made by cooperative negotiation through markets. This is contrasted with a so-called planned economy, where investment and production decisions are embodied in a plan of production established by a state or other body with control over economic resources.
Market economies can range from regulated markets to various forms of state-owned interventionist variants. In reality, market economies and free markets do not exist in "pure" form, since societies and governments all regulate them to varying degrees. Different perspectives exist as to how strong a role the government should have in both guiding and regulating the market economies and addressing (or not addressing) the inequalities the market naturally produces since some producers are always "better" than others. Most existing market economies include a degree of state economic planning or state-directed activity, and are thus classified as mixed economies. The term "free-market economy" is sometimes used synonymously with market economy.
Market economies do not logically presuppose the existence of private ownership of the means of production. A market economy can and often does consist of a mix of various types of cooperatives, collectives, or autonomous state agencies that acquire and exchange capital goods in capital markets. These all utilize a market determined free price system to allocate capital goods and labor. There are many variations of market socialism, some of which involve employee-owned enterprises based on self-management; as well as models that involve public ownership of the means of production where capital goods are allocated through markets.
Capitalism.
Capitalism generally refers to economic system where the means of production are largely or entirely privately owned and operated for a profit, structured on the process of capital accumulation. In general, in capitalist systems investment, distribution, income, and prices are determined by markets, whether regulated or unregulated.
There are different variations of capitalism with different relationships to markets. In Laissez-faire and free market variations of capitalism, markets are utilized most extensively with minimal or no state intervention and regulation over prices and the supply of goods and services. In interventionist, welfare capitalism and mixed economies, markets continue to play a dominant role but are regulated to some extent by government in order to correct market failures or to promote social welfare. In state capitalist systems, markets are relied upon the least, with the state relying heavily on either indirect economic planning and/or state-owned enterprises to accumulate capital.
Capitalism has been dominant in the Western world since the end of feudalism, but most feel that the term "mixed economies" more precisely describes most contemporary economies, due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. For example, higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.
Laissez-faire.
Laissez-faire is synonymous with what was referred to as strict capitalist free market economy during the early and mid-19th century as a classical liberal (right-libertarian) ideal to achieve. It is generally understood that the necessary components for the functioning of an idealized free market include the complete absence of government regulation, subsidies, artificial price pressures, and government-granted monopolies (usually classified as coercive monopoly by free market advocates) and no taxes or tariffs other than what is necessary for the government to provide protection from coercion and theft, maintaining peace and property rights, and providing for basic public goods. Right-libertarian advocates of anarcho-capitalism see the state as morally illegitimate and economically unnecessary and destructive.
Free-market economy.
Free-market economy refers to an economic system where prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets, private ownership of productive enterprises. "Laissez-faire" is a more extensive form of free-market economy where the role of the state is limited to protecting property rights.
Welfare capitalism.
Welfare capitalism refers to a capitalist economy that includes a public policies favoring extensive provisions for social welfare services. The economic mechanism involves a free market and the predominance of privately owned enterprises in the economy, but public provision of universal welfare services aimed at enhancing individual autonomy and maximizing equality. Examples of contemporary welfare capitalism include the Nordic model of capitalism predominant in Northern Europe.
Regional models.
Anglo-Saxon model.
Anglo-Saxon capitalism refers to the form of capitalism predominant in Anglophone countries and typified by the economy of the United States. It is contrasted with European models of capitalism such as the continental "Social market" model and the "Nordic model". Anglo-Saxon capitalism refers to a macroeconomic policy regime and capital market structure common to the Anglophone economies. Among these characteristics are low rates of taxation, more open financial markets, lower labor market protections, and a less generous welfare state eschewing collective bargaining schemes found in the continental and northern European models of capitalism.
East Asian model.
The East Asian model of capitalism involves a strong role for state investment, and in some instances involves state-owned enterprises. The state takes an active role in promoting economic development through subsidies, the facilitation of "national champions", and an export-based model of growth. The actual practice of this model varies by country. This designation has been applied to the economies of Singapore, Japan, Taiwan, South Korea and the People's Republic of China.
A related concept in political science is the developmental state.
Social market economy.
This model was implemented by Alfred Müller-Armack and Ludwig Erhard after World War II in West Germany. The social market economic model (sometimes called "Rhine capitalism") is based upon the idea of realizing the benefits of a free market economy, especially economic performance and high supply of goods, while avoiding disadvantages such as market failure, destructive competition, concentration of economic power and anti-social effects of market processes. The aim of the social market economy is to realize greatest prosperity combined with best possible social security. One difference from the free market economy is that the state is not passive, but takes active regulatory measures. The social policy objectives include employment, housing and education policies, as well as a socio-politically motivated balancing of the distribution of income growth. Characteristics of social market economies are a strong competition policy and a contractionary monetary policy. The philosophical background is Neoliberalism or Ordoliberalism
Market socialism.
Market socialism refers to various types of economic systems where the means of production and the dominant economic institutions are either publicly owned or cooperatively owned but operated according to the rules of supply and demand. This type of market economy has its roots in classical economics and in the works of Adam Smith, the Ricardian socialists and Mutualist philosophers.
The distinguishing feature between non-market socialism and market socialism is the existence of a market for factors of production and the criteria of profitability for enterprises. Profits derived from publicly owned enterprises can variously be used to reinvest in further production, to directly finance government and social services, or be distributed to the public at large through a social dividend or basic income system.
Public ownership models.
In the 1930s, the economists Oskar Lange and Abba Lerner developed a model of socialism that posited that a public body (dubbed the "Central Planning Board") can set prices through a trial-and-error approach until they equaled the marginal cost of production in order to achieve perfect competition and pareto optimality. In this model of socialism, firms would be state-owned and managed by their employees, and the profits would be disbursed among the population in a social dividend. This model came to be referred to as "market socialism" because it involved the use of money, a price system and simulated capital markets, all of which are absent from traditional non-market conceptions of socialism.
A more contemporary model of market socialism is that put forth by the American economist John Roemer, referred to as "Economic democracy". In this model, social ownership is achieved through public ownership of equity in a market economy. A Bureau of Public Ownership (BPO) would own controlling shares in publicly listed firms, so that the profits generated would be used for public finance and the provision of a basic income.
Cooperative socialism.
Libertarian socialists and left-anarchists often promote a form of market socialism in which enterprises are owned and managed cooperatively by their workforce so that the profits directly remunerate the employee-owners. These cooperative enterprises would compete with each other in the same way private companies compete with each other in a capitalist market. The first major elaboration of this type of market socialism was made by Pierre Joseph Proudhon and was called "mutualism".
Self-managed market socialism was promoted in Yugoslavia by economists Branko Horvat and Jaroslav Vanek. In the self-managed model of socialism, firms would be directly owned by their employees and the management board would be elected by employees. These cooperative firms would compete with each other in a market for both capital goods and for selling consumer goods.
Socialist market economy.
Following the 1978 reforms, the People's Republic of China instituted what it calls a "socialist market economy", in which most of the economy is under state ownership, but the state enterprises are organized as joint-stock companies where various government agencies own controlling shares through a shareholder system. Prices are set by a largely free-price system and the state-owned enterprises are not subjected to micromanagement by a government planning agency. A similar system called "socialist-oriented market economy" has emerged in Vietnam following the Đổi Mới reforms in 1986.
However, this system is usually characterized as state capitalism instead of market socialism because there is no meaningful degree of employee self-management in firms, because the state enterprises retain their profits instead of distributing them to the workforce or government, and many function as de facto private enterprises. The profits neither finance a social dividend to benefit the population at large, nor do they accrue to their employees.
In the People's Republic of China, this economic model is presented as a "preliminary stage of socialism" to explain the dominance of capitalistic management practices and forms of enterprise organization in both the state and non-state sectors.
Criticisms.
The economist Joseph Stiglitz argues that markets suffer from informational inefficiency and the presumed efficiency of markets stems from the faulty assumptions of neoclassical welfare economics, particularly the assumption of perfect and costless information, and related incentive problems. Neoclassical economics assumes static equilibrium, and efficient markets require that there be no non-convexities, even though nonconvexities are pervasive in modern economies. Stiglitz's critique applies to both existing models of capitalism and to hypothetical models of market socialism. However, Stiglitz does not advocate replacing markets, but states that there is a significant role for government intervention to boost the efficiency of markets and to address the pervasive market failures that exist in contemporary economies.
Robin Hahnel and Michael Albert claim that "markets inherently produce class division." Albert states that even if everyone started out with a balanced job complex (doing a mix of roles of varying creativity, responsibility and empowerment) in a market economy, class divisions would arise.
"(...) Without taking the argument that far, it is evident that in a market system with uneven distribution of empowering work, such as Economic Democracy, some workers will be more able than others to capture the benefits of economic gain. For example, if one worker designs cars and another builds them, the designer will use his cognitive skills more frequently than the builder. In the long term, the designer will become more adept at conceptual work than the builder, giving the former greater bargaining power in a firm over the distribution of income. A conceptual worker who is not satisfied with his income can threaten to work for a company that will pay him more. The effect is a class division between conceptual and manual laborers, and ultimately managers and workers, and a de facto labor market for conceptual workers (...)". 
David McNally argues that the logic of the market inherently produces inequitable outcomes and leads to unequal exchanges, arguing that Adam Smith's moral intent and moral philosophy espousing equal exchange was undermined by the practice of the free markets he championed. The development of the market economy involved coercion, exploitation and violence that Adam Smith's moral philosophy could not countenance. McNally also criticizes market socialists for believing in the possibility of "fair" markets based on equal exchanges to be achieved by purging "parasitical" elements from the market economy, such as private ownership of the means of production. McNally argues that market socialism is an oxymoron when socialism is defined as an end to wage-based labor.

</doc>
<doc id="48853" url="https://en.wikipedia.org/wiki?curid=48853" title="Walleye">
Walleye

Walleye ("Sander vitreus", formerly "Stizostedion vitreum") is a freshwater perciform fish native to most of Canada and to the Northern United States. It is a North American close relative of the European pikeperch. The walleye is sometimes called the yellow walleye to distinguish it from the blue walleye, which is a subspecies that was once found in the southern Ontario and Quebec regions, but are now presumed extinct. However, recent genetic analysis of a preserved (frozen) 'blue walleye' sample suggests that the blue and yellow walleye were simply phenotypes within the same species and do not merit separate taxonomic classification.
In some parts of its range, the walleye is known as the walleyed pike, colored pike, yellow pike or pickerel (esp. in English-speaking Canada), although the fish is not related to other species of pikes which are members of the family Esocidae.
Walleyes show a fair amount of variation across watersheds. In general, fish within a watershed are quite similar and are genetically distinct from those of nearby watersheds. The species has been artificially propagated for over a century and has been planted on top of existing populations or introduced into waters naturally devoid of the species, sometimes reducing the overall genetic distinctiveness of populations.
Etymology.
The common name, "walleye", comes from the fact that the fish's eyes point outward, as if looking at the walls. This externally facing orientation of the eyes gives anglers an advantage in the dark because there is a certain eyeshine given off by the eye of the walleye in the dark, similar to that of lions and other night dwelling animals. This "eyeshine" is the result of a light-gathering layer in the eyes called the "tapetum lucidum", which allows the fish to see well in low-light conditions. In fact, many anglers look for walleyes at night since this is when major feeding patterns occur. The fish's eyes also allow them to see well in turbid waters (stained or rough, breaking waters), which gives them an advantage over their prey. Thus, walleye anglers will commonly look for locations where there is a good "walleye chop" ("i.e.", rough water). This excellent vision also allows the fish to populate the deeper regions in a lake, and they can often be found in deeper water, particularly during the warmest part of the summer.
Description.
Walleyes are largely olive and gold in color (hence the French common name: "doré"—golden). The dorsal side of a walleye is olive, grading into a golden hue on the flanks. The olive/gold pattern is broken up by five darker saddles that extend to the upper sides. The color shades to white on the belly. The mouth of a walleye is large and is armed with many sharp teeth. The first dorsal and anal fins are spinous, as is the Operculum. Walleyes are distinguished from their close cousin the sauger by the white coloration on the lower lobe of the caudal fin which is absent on the sauger. In addition, the two dorsals and the caudal fin of the sauger are marked with distinctive rows of black dots which are absent from or indistinct on the same fins of walleyes.
Length and weight.
Walleyes grow to about in length, and weigh up to about . The maximum recorded size for the fish is in length and in weight. The rate depends partly on where in their range they occur, with southern populations often growing faster and larger. In general, females grow larger than males. Walleyes may live for decades; the maximum recorded age is 29 years. In heavily fished populations, however, few walleye older than five or six years of age are encountered. In North America, where they are highly prized, their typical size when caught is on the order of , substantially below their potential size.
As walleye grow longer, they increase in weight. The relationship between length andnship between total length ("L") and total weight ("W") for nearly all species of fish can be expressed by an equation of the form
Invariably, "b" is close to 3.0 for all species, and "c" is a constant that varies among species. For walleye, "b" = 3.180 and "c" = 0.000228 (with units in inches and pounds).
The relationship described in this section suggests a walleye will weigh about , while a walleye will likely weigh about .
Reproduction.
In most of the species' range, male walleyes mature sexually between three and four years of age. Females normally mature about a year later. Adults migrate to tributary streams in late winter or early spring to lay eggs over gravel and rock, although there are open water reef or shoal spawning strains as well. Some populations are known to spawn on sand or vegetation. Spawning occurs at water temperatures of . A large female can lay up to 500,000 eggs, and no care is given by the parents to the eggs or fry. The eggs are slightly adhesive and fall into spaces between rocks. The incubation period for the embryos is temperature-dependent, but generally lasts from 12 to 30 days. After hatching, the free-swimming embryo spends about a week absorbing a relatively small amount of yolk. Once the yolk has been fully absorbed, the young walleye begins to feed on invertebrates, such as fly larvæ and zooplankton. After 40 to 60 days, juvenile walleyes become piscivorous. Thenceforth, both juvenile and adult walleyes eat fish almost exclusively, frequently yellow perch or ciscoes, moving onto bars and shoals at night to feed. Walleye also feed heavily on crayfish, minnows, and leeches.
As food.
The walleye is considered to be a quite palatable freshwater fish, and, consequently, is fished recreationally and commercially for food. Because of its nocturnal feeding habits, it is most easily caught at night using live minnows or lures that mimic small fish. In Minnesota the walleye is often fished for in the late afternoon on windy days or in the night. Most commercial fisheries for walleye are situated in the Canadian waters of the Great Lakes.
Fishing.
Because walleyes are popular with anglers, fishing for walleyes is regulated by most natural resource agencies. Management may include the use of quotas and length limits to ensure that populations are not over-exploited. For example, in the state of Michigan, walleye shorter than may not be legally kept, except in Lake St. Clair and the St. Clair River where fish as short as may be taken.
Since walleyes have excellent visual acuity under low illumination levels, they tend to feed more extensively at dawn and dusk, on cloudy or overcast days and under choppy conditions when light penetration into the water column is disrupted. Although anglers interpret this as light avoidance, it is merely an expression of the walleye's competitive advantage over its prey under those conditions. Similarly, in darkly stained or turbid waters, walleye tend to feed throughout the day. In the spring and fall walleye are located near the shallower areas due to the spawning grounds; and they are most often located in shallower areas during higher winds due to the murkier, higher oxygenated water at around six feet deep. On calm spring days the walleye are more often located at the deep side of the shoreline drop-off and around shore slopes around or deeper than ten feet.
"Walleye chop" is a term used by walleye anglers for rough water typically with winds of , and is one of the indicators for good walleye fishing due to the walleye's increased feeding activity during such conditions. In addition to fishing the "Walleye chop", night fishing with live bait can be very effective.
The current all-tackle world record for a walleye is held by Mabry Harper, who caught a 11.34 kg (25 lbs.) walleye in Old Hickory Lake in Tennessee, USA on August 2, 1960.
Cultural aspects.
The walleye is the state fish of Minnesota, Vermont and South Dakota and the official fish of Saskatchewan. It is very popular with Minnesota residents; more walleye is eaten in Minnesota than in any other jurisdiction of the United States. Both Garrison and Baudette, Minnesota, claim to be the "Walleye Capital of the World," each with a large statue of the fish.
Winnipeg, Manitoba considers the walleye (referred to locally as "pickerel") its most important local fish. Icelandic fishermen in Lake Winnipeg have traditionally supplied the Winnipeg market.
Walleye fishing records.
By information from International Game Fish Association IGFA the most outstanding records:

</doc>
<doc id="48854" url="https://en.wikipedia.org/wiki?curid=48854" title="Linguistic typology">
Linguistic typology

Linguistic typology is a field of linguistics that studies and classifies languages according to their structural and functional features. Its aim is to describe and explain the common properties and the structural diversity of the world's languages. It includes three subdisciplines: qualitative typology, which deals with the issue of comparing languages and within-language variance; quantitative typology, which deals with the distribution of structural patterns in the world’s languages; and theoretical typology, which explains these distributions.
Qualitative typology.
Qualitative typology develops cross-linguistically viable notions or types that provide a framework for the description and comparison of individual languages. A few examples appear below.
Typological systems.
Subject–verb–object positioning.
One set of types reflects the basic order of subject, verb, and direct object in sentences:
These labels usually appear abbreviated as "SVO" and so forth, and may be called "typologies" of the languages to which they apply.
Some languages split verbs into an auxiliary and an infinitive or participle, and put the subject and/or object between them. For instance, German ("Ich "habe" einen Fuchs im Wald "gesehen"" - *"I have a fox in-the woods seen"), Dutch ("Hans "vermoedde" dat Jan Marie "zag leren zwemmen"" - *"Hans suspected that Jan Marie saw teach swim") and Welsh (""Mae"'r gwirio sillafu wedi'i "gwblhau"" - *"Is the checking spelling after its to complete"). In this case, linguists base the typology on the non-analytic tenses (i.e. those sentences in which the verb is not split) or on the position of the auxiliary. German is thus SVO in main clauses and Welsh is VSO (and preposition phrases would go after the infinitive).
Many typologists classify both German and Dutch as V2 languages, as the verb invariantly occurs as the second element of a full clause.
Some languages allow varying degrees of freedom in their constituent order that pose a problem for their classification within the subject–verb–object schema. To define a basic constituent order type in this case, one generally looks at frequency of different types in declarative affirmative main clauses in pragmatically neutral contexts, preferably with only old referents. Thus, for instance, Russian is widely considered an SVO language, as this is the most frequent constituent order under such conditions—all sorts of variations are possible, though, and occur in texts. In many inflected languages, such as Russian, Latin, and Greek, departures from the default word-orders are permissible but usually imply a shift in focus, an emphasis on the final element, or some special context. In the poetry of these languages, the word order may also shift freely to meet metrical demands. Additionally, freedom of word order may vary within the same language—for example, formal, literary, or archaizing varieties may have different, stricter, or more lenient constituent-order structures than an informal spoken variety of the same language.
On the other hand, when there is no clear preference under the described conditions, the language is considered to have "flexible constituent order" (a type unto itself).
An additional problem is that in languages without living speech communities, such as Latin, Hellenic Greek, and Old Church Slavonic, linguists have only written evidence, perhaps written in a poetic, formalizing, or archaic style that mischaracterizes the actual daily use of the language. The daily spoken language of a Sophocles or a Cicero might have exhibited a different or much more regular syntax than their written legacy indicates.
Morphosyntactic alignment.
Another common classification distinguishes nominative–accusative alignment patterns and ergative–absolutive ones. In a language with cases, the classification depends on whether the subject (S) of an intransitive verb has the same case as the agent (A) or the patient (P) of a transitive verb. If a language has no cases, but the word order is AVP or PVA, then a classification may reflect whether the subject of an intransitive verb appears on the same side as the agent or the patient of the transitive verb. Bickel (2011) has argued that alignment should be seen as a construction-specific property rather than a language-specific property.
Many languages show mixed accusative and ergative behaviour (for example: ergative morphology marking the verb arguments, on top of an accusative syntax). Other languages (called "active languages") have two types of intransitive verbs—some of them ("active verbs") join the subject in the same case as the agent of a transitive verb, and the rest ("stative verbs") join the subject in the same case as the patient. Yet other languages behave ergatively only in some contexts (this "split ergativity" is often based on the grammatical person of the arguments or on the tense/aspect of the verb). For example, only some verbs in Georgian behave this way, and, as a rule, only while using the perfective (aorist).
Phonological systems.
Linguistic typology will also apply to the structure and spread of sound systems in languages world-wide in identifying patterns. Ultimately, the goal is to understand the patterns of relative frequency between sounds and their co-occurrences and why they are thus. An example of this relative spread can be seen in trying to explain why contrastive voicing commonly occurs with plosives, such as in English with “neat” and “need”, but much fewer have this occur in fricatives, such as the English “niece” and “knees”. According to a worldwide sample of 637 languages, 62% have the voicing contrast in stops but only 35% have this in fricatives. In the vast majority of those cases, the absence of voicing contrast occurs because there is a lack of voiced fricatives and because all languages have some form of plosive, but there are languages with no fricatives. Below is a chart showing the breakdown of these languages, showing the numbers as shown in this sample and how they relate to each other.
Languages worldwide also vary in the number of sounds that are used within them. These languages can go from very small phonemic inventories (Rotokas with six consonants and five vowels) to very large inventories (!Xóõ with 128 consonants and 28 vowels). An interesting phonological observation found with this data is that the larger a consonant inventory a language has, the more likely it is to contain a sound from a defined set of complex consonants (clicks, glottalized consonants, doubly articulated labial-velar stops, lateral fricatives and affricates, uvular and pharyngeal consonants, and dental or alveolar non-sibilant fricatives). Of this list, only about 26% of languages in a survey of over 600 with small inventories (less than 19 consonants) contain a member of this set, while 51% of average languages (19-25) contain at least one member and 69% of large consonant inventories (greater than 25 consonants) contain a member of this set. It is then seen that complex consonants are in proportion to the size of the inventory.
Vowels contain a more modest number of phonemes, with the average being 5-6, which 51% of the languages in the survey have. About a third of the languages have larger than average vowel inventories. Most interesting though is the lack of relationship between consonant inventory size and vowel inventory size. Below is a chart showing this lack of predictability between consonant and vowel inventory sizes in relation to each other.
Quantitative typology.
Quantitative typology deals with the distribution and co-occurrence of structural patterns in the languages of the world. Major types of non-chance distribution include:

</doc>
<doc id="48856" url="https://en.wikipedia.org/wiki?curid=48856" title="Sucker">
Sucker

Sucker may refer to: 

</doc>
<doc id="48859" url="https://en.wikipedia.org/wiki?curid=48859" title="Percidae">
Percidae

The Percidae are a family of perciform fish found in fresh and brackish waters of the Northern Hemisphere. The majority are Nearctic, but there are also Palearctic species. The family contains about 200 species in 10 genera. The darters, perches, and their relatives are in this family; well-known species include the walleye, sauger, ruffe, and three species of perch. However, small fish known as darters are also a part of this family.
This family is characterized by a greater or lesser degree of armour about the head, caused by the presence of teeth or spines on the cheeks and opercles (gill covers) or their edges, and by two narrow bands of numerous close-set teeth on the sides (palatines). Also, many percid fish have a heart-shaped plate of teeth on the roof of the mouth (vomer). The shape of these fish is usually somewhat slender and laterally compressed. Their scales are generally harsh and rough to the feel, or ciliate. Percid fish are among the most beautiful of the freshwater fish due to their brilliant colors (red, brown, orange, and yellow are the most predominant tints).
Species.
FishBase lists 204 species in 10 genera:
(Cope, 1870)-->

</doc>
<doc id="48860" url="https://en.wikipedia.org/wiki?curid=48860" title="Cirrus">
Cirrus

Cirrus may refer to:

</doc>
<doc id="48863" url="https://en.wikipedia.org/wiki?curid=48863" title="Centrarchidae">
Centrarchidae

The sunfish are a family (Centrarchidae) of freshwater ray-finned fish belonging to the order Perciformes. The type genus is "Centrarchus" (consisting solely of the flier, "C. macropterus"). The family's 37 species include many fish familiar to North Americans, including the rock bass, largemouth bass, bluegill, pumpkinseed, and crappies. All are native only to North America.
Family members are distinguished by having at least three anal spines. The dorsal spines are five to 13 in number, but most species have 10–12. The pseudobranch is small and concealed. Sizes of most are in the range. However, some are much smaller, with the black-banded sunfish at just in length, while the largemouth bass is reported to reach almost in extreme cases.
The male of most species builds a nest by hollowing out a depression using his tail, then guards the eggs.
Most sunfish are valued for sports fishing, and have been introduced in many areas outside their original ranges, sometimes becoming invasive species. While edible, they are not commercially marketed as a food fish.
Fossil record.
The earliest fossils of Centrarchidae are from Middle Miocene Nebraska, belonging to the redear sunfish (13.6–16.3 million years ago).
Habitat.
These fish prefer to live in and around aquatic vegetation so they can get adequate coverage from predators such as bass. They also prefer slow moving water such as lakes, or slow moving rivers. They are also social fish, as they prefer to live and travel in schools. They generally feed off of the bottom, but will rise to the surface to feed on insects. Their diet consists of plants and animals that are in the water in their habitat.
Classification.
Recent genetic evidence suggests the following taxonomy of the centrarchid genera:

</doc>
<doc id="48865" url="https://en.wikipedia.org/wiki?curid=48865" title="Gadidae">
Gadidae

The Gadidae are a family of marine fish, included in the order Gadiformes, known as the cods, codfishes or true cods. It contains several commercially important fishes, including the cod, haddock, whiting, and pollock.
Most gadid species are found in temperate waters of the Northern Hemisphere, but several range into subtropical, Subarctic and Arctic oceans, and a single (southern blue whiting) is found in the Southern Hemisphere. They are generally medium-sized fish, and are distinguished by the presence of three dorsal fins on the back and two anal fins on the underside. Most species have barbels on their chins, which they use while browsing on the sea floor. Gadids are carnivorous, feeding on smaller fish and crustaceans.
Gadids are highly prolific, producing several million eggs at each spawning. This contributes to their high population numbers, which, in turn, makes commercial fishing relatively easy.
Concepts differ about the contents of the family Gadidae. The system followed by FishBase includes a dozen genera. Alternatively, also fishes in the current Lotidae (with burbot, cusk) and Phycidae (hakes) have been included in Gadidae, as its subfamilies Lotinae and Phycidae.

</doc>
<doc id="48866" url="https://en.wikipedia.org/wiki?curid=48866" title="Gadiformes">
Gadiformes

Gadiformes are an order of ray-finned fish, also called the Anacanthini, that includes the cod and its allies. Many major food fish are in this order. They are found in marine waters throughout the world and the vast majority of the species are found in temperate or colder regions (tropical species are typically deep-water). A few species may enter estuaries and a single, the burbot ("Lota lota"), is a freshwater fish.
Common characteristics include the positioning of the pelvic fins (if present), below or in front of the pectoral fins. Gadiformes are physoclists, which means their gas bladders do not have a pneumatic duct. The fins are spineless. Gadiform fish range in size from the codlets, which may be as small as in adult length, to the Atlantic cod, "Gadus morhua", which reaches up to .

</doc>
<doc id="48867" url="https://en.wikipedia.org/wiki?curid=48867" title="Sciaenidae">
Sciaenidae

Sciaenidae is the family of fish commonly called drums or croakers in reference to the repetitive throbbing or drumming sounds they make. The family includes the weakfish, and consists of about 275 species in about 70 genera; it belongs to the order Perciformes.
Characteristics.
A sciaenid has a long dorsal fin reaching nearly to the , and a notch between the rays and spines of the dorsal, although the two parts are actually separate. Drums are somberly colored, usually in shades of brown, with a lateral line on each side that extends to the tip of the caudal fin. The anal fin usually has two spines, while the dorsal fins are deeply notched or separate. Most species have a rounded or pointed caudal fin. The mouth is set low and is usually inferior. Their croaking mechanism involves the beating of abdominal muscles against the swim bladder.
Sciaenids are found worldwide, in both fresh and salt water, and are typically benthic carnivores, feeding on invertebrates and smaller fish. They are small to medium-sized, bottom-dwelling fishes living primarily in estuaries, bays, and muddy river banks. Most of these fishes avoid clear waters, such as coral reefs and oceanic islands, with a few notable exceptions (i.e., reef croaker, high-hat, and spotted drum). They live in warm-temperate and tropical waters and are best represented in major rivers in Southeast Asia, northeast South America, the Gulf of Mexico, and the Gulf of California.
Fisheries.
They are excellent food and sport fish, and are commonly caught by surf and pier fishers. Some of them are important commercial fishery species, notably small yellow croaker with reported landings of 218,00–407,000 tonnes in 2000–2009; based on the FAO fishery statistics from 2009, it was the 25th most important fishery species worldwide. However, a large proportion of catches is not reported at species level; in the FAO fishery statistics, the category "Croakers, drums, not elsewhere included", is the largest one within sciaenids, with annual landings of 431,000–780,000 tonnes in 2000–2009, most of which were reported from the western Indian Ocean (FAO fishing area 51) and northwest Pacific (FAO fishing area 61).
Croaking Mechanism.
A notable trait of sciaenids is the ability to produce a "croaking" sound. However the pitch and use of croaking varies species to species. The croaking ability is a distinguishing characteristic of sciaenids. The croaking mechanism is used by males as a mating call in some species.
To produce the croaking sound, special muscles vibrate against the swim bladder. These muscles are called sonic muscle fibers, and run horizontally along the fish's body on both sides around the swim bladder and are connected to a central tendon which surrounds the swim bladder ventrally. These sonic muscle fibers are contracted against the swim bladder to produce the croaking sound that gives drum and croaker their common name. The swim bladder of species in sciaenids, is used as a resonating chamber. The large swim bladder is more expansive and branched than species outside of sciaenids, which aids in croaking. In some species the sonic muscle fibers are only present in males. These muscles strengthen during mating season and are allowed to atrophy when not in the mating season., causing the croaking mechanism to be inactive. In other species, most notably the Atlantic Croaker, the croaking mechanism is present in both genders and remains active year round. These species are theorized to use croaking for communication purposes such as announcing hazards and location when in turbid water.
Croaking in communication.
In some species croaking is used for communication aside from attracting mates. For those species that have year round croaking ability it is theorized that the croaks serve as a low-aggression warning during group feeding as well as to communicate location in turbid waters. In those species that lack the ability to croak year round, croaking is usually restricted to males for attracting mates. A disadvantage to the croaking ability is that it allows Bottlenose dolphin to easily locate large groups of croaker and drum as they broadcast their position, indicating large amounts of food for the dolphins.

</doc>
<doc id="48868" url="https://en.wikipedia.org/wiki?curid=48868" title="Sparidae">
Sparidae

The Sparidae are a family of fish in the order Perciformes, commonly called sea breams and porgies (North America). The sheepshead, scup, and red seabream are species in this family. Most sparids are deep-bodied compressed fish with a small mouth separated by a broad space from the eye, a single dorsal fin with strong spines and soft rays, a short anal fin, long pointed pectoral fins and rather large firmly attached scales. They are found in shallow temperate and tropical waters and are bottom-dwelling carnivores. Most species possess grinding, molar-like teeth. Some of the species, such as "Polysteganus undulosus", have been subject to overfishing, or exploitation beyond sustainable recovery.
Genera.
The family Sparidae contains about 155 species in 38 genera:
Cookery.
The most celebrated of the breams in cookery are the gilt-head bream and the common dentex.

</doc>
<doc id="48869" url="https://en.wikipedia.org/wiki?curid=48869" title="Clupeiformes">
Clupeiformes

Clupeiformes is the order of ray-finned fish that includes the herring family, Clupeidae, and the anchovy family, Engraulidae. The group includes many of the most important forage and food fish.
Clupeiformes are physostomes, which means that the gas bladder has a pneumatic duct connecting it to the gut. They typically lack a lateral line, but still have the eyes, fins and scales that are common to most fish, though not all fish have these attributes. They are generally silvery fish with streamlined, spindle-shaped, bodies, and they often school. Most species eat plankton which they filter from the water with their gill rakers.
Families.
The order includes about 405 species in seven families:
Order Clupeiformes

</doc>
<doc id="48870" url="https://en.wikipedia.org/wiki?curid=48870" title="Clupeidae">
Clupeidae

Clupeidae (Latin: "sardine") is the name of the fish family of the herrings, shads, sardines, hilsa, and menhadens. They include many of the most important food fishes in the world, and are also commonly caught for production of fish oil and fish meal. Many members of the family have a body protected with shiny cycloid scales (very smooth and uniform scales), a single dorsal fin, with a fusiform body evolved for quick, evasive swimming and pursuit of prey composed of small planktonic animals. Due to their small size, and position in the lower trophic level of many marine food webs, the levels of methylmercury they bioaccumulate are very low, reducing the risk of mercury poisoning when consumed.
Description and biology.
Clupeids are mostly marine forage fish, although a few species are found in fresh water. No species has scales on the head, and some are entirely scaleless. The lateral line is short or absent, and the teeth are unusually small where they are present at all. Clupeids typically feed on plankton, and range from 2 to 75 cm (0.8 to 30 in.) in length. The family arguably also contains the "Sundasalangidae", a paedomorphic taxon first thought to be distinct salmoniform family but then found to be deeply nested in Clupeidae. In the fossil record clupeids date back to the early Paleogene.
Clupeids spawn huge numbers of eggs (up to 200,000 in some species) near the surface of the water. After hatching, the larvae live among the plankton until they develop a swim bladder and transform into adults. These eggs and fry are not protected or tended to by parents. The adults typically live in large shoals, seeking protection from piscivorous predators such as birds, sharks and other predatory fish, tooth whales, marine mammals and jellyfish. They also form bait balls.
Commercially important species of Clupeidae include for instance the Atlantic menhaden ("Brevoortia tyrannus"), the Atlantic and Baltic herrings ("Clupea harengus"), the Pacific herring ("C. pallasii") and the sardine ("Sardina pilchardus").

</doc>
<doc id="48872" url="https://en.wikipedia.org/wiki?curid=48872" title="Scombridae">
Scombridae

The Scombridae family of the mackerels, tunas, and bonitos includes many of the most important and familiar food fishes. The family consists of 51 species in 15 genera and two subfamilies. All species are in the subfamily Scombrinae, except the butterfly kingfish, which is the sole member of subfamily Gasterochismatinae.
Scombrids have two dorsal fins and a series of finlets behind the rear dorsal fin and anal fin. The caudal fin is strongly divided and rigid, with a slender, ridged base. The first (spiny) dorsal fin and the pelvic fins are normally retracted into body grooves. Species lengths vary from the of the island mackerel to the recorded for the immense Atlantic bluefin tuna.
Scombrids are generally predators of the open ocean, and are found worldwide in tropical and temperate waters. They are capable of considerable speed, due to a highly streamlined body and retractable fins. Some members of the family, in particular the tunas, are notable for being partially endothermic (warm-blooded), a feature that also helps them to maintain high speed and activity. Other adaptations include a large amount of red muscle, allowing them to maintain activity over long periods. Two of the fastest recorded scombrids are the wahoo and the yellowfin tuna, which can each attain speeds of .
Classification.
Jordan, Evermann and Clark (1930) divide these fishes into the four families: Cybiidae, Katsuwonidae, Scombridae, and Thunnidae, but taxonomists later classified them all into a single family, the Scombridae.
The World Wildlife Fund and the Zoological Society of London jointly issued their "Living Blue Planet Report" on 16 September 2015 which states that a dramatic fall of 74% occurred in world-wide stocks of scombridae fish between 1970 and 2010, and the global overall "population sizes of mammals, birds, reptiles, amphibians and fish fell by half on average in just 40 years."
The 51 extant species are in 15 genera and two subfamilies – with the subfamily Scombrinae further grouped into four tribes, as:

</doc>
<doc id="48873" url="https://en.wikipedia.org/wiki?curid=48873" title="Serranidae">
Serranidae

The Serranidae are a large family of fishes belonging to the order Perciformes. The family contains about 450 species in 64 genera, including the sea basses and the groupers (subfamily Epinephelinae). Although many species are small, in some cases less than , the giant grouper ("Epinephelus lanceolatus") is one of the largest bony fishes in the world, growing to in length and in weight.
Characteristics.
Many serranid species are brightly colored, and many of the larger species are caught commercially for food. They are usually found over reefs, in tropical to subtropical waters along the coasts. Serranids are generally robust in form, with large mouths and small spines on the gill coverings. They typically have several rows of sharp teeth, usually with a pair of particularly large, canine-like teeth projecting from the lower jaw.
All serranids are carnivorous. Although some species, especially in the Anthiinae subfamily, only feed on zooplankton, the majority feed on fish and crustaceans. They are typically ambush predators, hiding in cover on the reef and darting out to grab passing prey. Their bright colours are most likely a form of disruptive camouflage, similar to the stripes of a tiger.
Many species are protogynous hermaphrodites, meaning they start out as females and change sex to male later in life. They produce large quantities of eggs and their larvae are planktonic, generally at the mercy of ocean currents until they are ready to settle into adult populations.
As other fish, serranids harbour parasites, including nematodes, cestodes, digeneans, monogeneans, isopods, and copepods. A study conducted in New Caledonia has shown that coral reef-associated serranids harbour about 10 species of parasites per fish species.
Classification.
Recent molecular classifications challenge the validity of the genera "Cromileptes" (sometimes spelled "Chromileptes") and "Anyperodon". Each of these two genera has a single species, which were included in the same clade as species of "Epinephelus" in a study based on five different genes.

</doc>
<doc id="48874" url="https://en.wikipedia.org/wiki?curid=48874" title="Hugo Chávez">
Hugo Chávez

Hugo Rafael Chávez Frías (; 28 July 1954 – 5 March 2013), commonly known as Hugo Chávez, was a Venezuelan politician who served as the 64th President of Venezuela from 1999 to 2013. He was also leader of the Fifth Republic Movement from its foundation in 1997 until 2007, when it merged with several other parties to form the United Socialist Party of Venezuela (PSUV), which he led until 2012.
Born into a working-class family in Sabaneta, Barinas, Chávez became a career military officer, and after becoming dissatisfied with the Venezuelan political system based on the Punto Fijo Pact, he founded the clandestine Revolutionary Bolivarian Movement-200 (MBR-200) in the early 1980s. Chávez led the MBR-200 in an unsuccessful coup d'état against the Democratic Action government of President Carlos Andrés Pérez in 1992, for which he was imprisoned. Released from prison after two years, he founded a political party known as the Fifth Republic Movement and was elected president of Venezuela in 1998. He was re-elected in 2000 and again in 2006 with over 60% of the votes. After winning his fourth term as president in the October 2012 presidential election, he was to be sworn in on 10 January 2013, but Venezuela's National Assembly postponed the inauguration to allow him time to recover from medical treatment in Cuba. Suffering a return of the cancer originally diagnosed in June 2011, Chávez died in Caracas on 5 March 2013 at the age of 58.
Following the adoption of a new constitution in 1999, Chávez focused on enacting social reforms as part of the Bolivarian Revolution, which is a type of socialist revolution. Using record-high oil revenues of the 2000s, his government nationalized key industries, created participatory democratic Communal Councils, and implemented social programs known as the Bolivarian Missions to expand access to food, housing, healthcare, and education. With Venezuela receiving high oil profits in the mid-2000s, improvements in areas such as poverty, literacy, income equality, and quality of life occurred primarily between 2003 and 2007. At the end of Chávez's presidency in the early 2010s, economic actions performed by his government during the preceding decade such as overspending and price controls proved to be unsustainable, with Venezuela's economy faltering while poverty, inflation and shortages in Venezuela increased. Chávez's presidency also saw significant increases in the country's murder rate and corruption within the police force and government. His use of enabling acts and his government's use of Bolivarian propaganda was also controversial.
Internationally, Chávez aligned himself with the Marxist–Leninist governments of Fidel and then Raúl Castro in Cuba, and the socialist governments of Evo Morales (Bolivia), Rafael Correa (Ecuador), and Daniel Ortega (Nicaragua). His presidency was seen as a part of the socialist "pink tide" sweeping Latin America. Chávez described his policies as anti-imperialist, being a prominent adversary of the United States's foreign policy as well as a vocal critic of US-supported neoliberalism and laissez-faire capitalism. He described himself as a Marxist. He supported Latin American and Caribbean cooperation and was instrumental in setting up the pan-regional Union of South American Nations, the Community of Latin American and Caribbean States, the Bolivarian Alliance for the Americas, the Bank of the South, and the regional television network TeleSUR. Chavez's ideas, programs, and style form the basis of "Chavismo", a political ideology closely associated with Bolivarianism and socialism of the 21st Century.
Early life.
He was born on 28 July 1954 in his paternal grandmother Rosa Inéz Chávez's home, a modest three-room house located in the rural village Sabaneta, Barinas State. The Chávez family were of Amerindian, Afro-Venezuelan, and Spanish descent. His parents, Hugo de los Reyes Chávez, described as a proud COPEI member, and Elena Frías de Chávez, were schoolteachers who lived in the small village of Los Rastrojos.
Hugo was born the second of seven children. Hugo described his childhood as "poor... very happy", though his childhood of supposed poverty has been disputed as Chávez possibly changed the story of his background for political reasons. Attending the Julián Pino Elementary School, Chávez was particularly interested in the 19th-century federalist general Ezequiel Zamora, in whose army his own great-great-grandfather had served. With no high school in their area, Hugo's parents sent Hugo and his older brother Adán to live with their grandmother Rosa, who lived in a lower middle class subsidized home provided by the government, where they attended Daniel O'Leary High School in the mid-1960s. Hugo later described his grandmother as being "a pure human being... pure love, pure kindness." She was a devout Roman Catholic, and Hugo was an altar boy at a local church. His father, despite having the salary of a teacher, helped pay college for Chávez and his siblings.
Military Academy: 1971–1975.
Aged seventeen, Chávez studied at the Venezuelan Academy of Military Sciences in Caracas, following a curriculum known as the Andrés Bello Plan, instituted by a group of progressive, nationalistic military officers. This new curriculum encouraged students to learn not only military routines and tactics but also a wide variety of other topics, and to do so civilian professors were brought in from other universities to give lectures to the military cadets.
Living in Caracas, he saw more of the endemic poverty faced by working class Venezuelans, and said that this experience only made him further committed to achieving social justice. He also began to get involved in activities outside of the military school, playing baseball and softball with the "Criollitos de Venezuela" team, progressing with them to the Venezuelan National Baseball Championships. He also wrote poetry, fiction, and drama, and painted, and he researched the life and political thought of 19th-century South American revolutionary Simón Bolívar. He also became interested in the Marxist revolutionary Che Guevara (1928–67) after reading his memoir "The Diary of Che Guevara". In 1974, he was selected to be a representative in the commemorations for the 150th anniversary of the Battle of Ayacucho in Peru, the conflict in which Simon Bolívar's lieutenant, Antonio José de Sucre, defeated royalist forces during the Peruvian War of Independence. In Peru, Chávez heard the leftist president, General Juan Velasco Alvarado (1910–1977), speak, and inspired by Velasco's ideas that the military should act in the interests of the working classes when the ruling classes were perceived as corrupt, he "drank up the books had written, even memorising some speeches almost completely."
Befriending the son of Maximum Leader Omar Torrijos, the leftist dictator of Panama, Chávez visited Panama, where he met with Torrijos, and was impressed with his land reform program that was designed to benefit the peasants. Influenced by Torrijos and Velasco he saw the potential for military generals to seize control of a government when the civilian authorities were perceived as serving the interests of only the wealthy elites. In contrast to Torrijos and Velasco, Chávez became highly critical of Augusto Pinochet, the right-wing general who had recently seized control in Chile with the aid of the American CIA. Chávez later said, "With Torrijos, I became a Torrijist. With Velasco I became a Velasquist. And with Pinochet, I became an anti-Pinochetist". In 1975, Chávez graduated from the military academy as one of the top graduates of the year.
Early military career: 1976–1981.
Following his graduation, Chávez was stationed as a communications officer at a counterinsurgency unit in Barinas, although the Marxist–Leninist insurgency which the army was sent to combat had already been eradicated from that state. At one point he found a stash of Marxist literature that apparently had belonged to insurgents many years before. He went on to read these books, which included titles by Karl Marx, Vladimir Lenin, and Mao Zedong, but his favourite was a work entitled "The Times of Ezequiel Zamora", written about the 19th-century federalist general whom Chávez had admired as a child. These books further convinced Chávez of the need for a leftist government in Venezuela: "By the time I was 21 or 22, I made myself a man of the left".
In 1977, Chávez's unit was transferred to Anzoátegui, where they were involved in battling the Red Flag Party, a Marxist–Hoxhaist insurgency group. After intervening to prevent the beating of an alleged insurgent by other soldiers, Chávez began to have his doubts about the army and their methods in using torture. At the same time, he was becoming increasingly critical of the corruption in the army and in the civilian government, coming to believe Venezuela's poor were not benefiting from the oil wealth, and began to sympathize with the Red Flag Party and their cause and their violent methods.
In 1977, he founded a revolutionary movement together with Luis R. Gonzalez an William Jimenez, within the armed forces, in the hope that he could one day introduce a leftist government to Venezuela: the Venezuelan People's Liberation Army ("", or ELPV), consisted of him and a handful of his fellow soldiers who had no immediate plans for direct action, though they knew they wanted a middle way between the right wing policies of the government and the far left position of the Red Flag. Nevertheless, hoping to gain an alliance with civilian leftist groups in Venezuela, Chávez set up clandestine meetings with various prominent Marxists, including Alfredo Maneiro (the founder of the Radical Cause) and Douglas Bravo. At this time, Chávez married a working-class woman named Nancy Colmenares, with whom he had three children: Rosa Virginia (born September 1978), Maria Gabriela (born March 1980) and Hugo Rafael (born October 1983).
Later military career and the Bolivarian Revolutionary Army-200: 1982–1991.
Five years after his creation of the ELPV, Chávez went on to form a new secretive cell within the military, the Bolivarian Revolutionary Army-200 (EBR-200), later redesignated the Revolutionary Bolivarian Movement-200 (MBR-200). He was inspired by Ezequiel Zamora (1817–1860), Simón Bolívar (1783–1830) and Simón Rodríguez (1769–1854), who became known as the "three roots of the tree" of the MBR-200. Later, Chávez said that "the Bolivarian movement that was being born did not propose political objectives... Its goals were imminently internal. Its efforts were directed in the first place to studying the military history of Venezuela as a source of a military doctrine of our own, which up to then didn't exist". However, he always hoped for the Bolivarian Movement to become a politically dominant party that would "accept all kinds of ideas, from the right, from the left, from the ideological ruins of those old capitalist and communist systems." Indeed, Irish political analyst Barry Cannon noted that the MBR's early ideology "was a doctrine in construction, a heterogeneous amalgam of thoughts and ideologies, from universal thought, capitalism, Marxism, but rejecting the neoliberal models currently being imposed in Latin America and the discredited models of the old Soviet Bloc."
In 1981, Chávez, by now a captain, was assigned to teach at the military academy where he had formerly trained. Here he introduced new students to his so-called "Bolivarian" ideals and recruited some of them. By the time they had graduated, at least thirty out of 133 cadets had joined his cause. In 1984 he met Herma Marksman, a recently divorced history teacher with whom he had an affair that lasted several years. During this time Francisco Arias Cárdenas , a soldier interested in liberation theology, also joined MBR-200. Cárdenas rose to a significant position within the group, although he came into ideological conflict with Chávez, with Chávez believing that they should begin direct military action in order to overthrow the government, something Cárdenas thought was reckless.
After some time, some senior military officers became suspicious of Chávez and reassigned him so that he would not be able to gain any more fresh new recruits from the academy. He was sent to take command of the remote barracks at Elorza in Apure State, where he organized social events for the community and contacted the local indigenous tribal peoples, the Cuiva and Yaruro. Distrustful as they were because of the mistreatment at the hands of the Venezuelan army in previous decades, Chávez gained their trust by joining the expeditions of an anthropologist to meet with them. Chávez said his experiences with them later led him to introduce laws protecting the rights of indigenous tribal peoples. In 1988, after being promoted to the rank of major, the high-ranking General Rodríguez Ochoa took a liking to Chávez and employed him to be his assistant at his office in Caracas.
"Operation Zamora" coup attempt: 1992.
In 1989, centrist Carlos Andrés Pérez (1922–2010) was elected President, and though he had promised to oppose the United States government's Washington Consensus and the International Monetary Fund's policies, he opposed neither once he got into office, following instead the neoliberal economic policies supported by the United States and the IMF, angering the public. In an attempt to stop the widespread protests and looting that followed his social spending cuts, Pérez initiated Plan Ávila and a violent repression of protesters, known as "El Caracazo" unfolded. Though members of Chávez's MBR-200 movement had allegedly participated in the crackdown, Chávez did not participate since he was then hospitalized with chicken pox and later condemned the event as "genocide".
Chávez began preparing for a military coup d'état known as Operation Zamora. The plan involved inside members of the military, the overwhelming of military locations along with communication installations and the establishment of Rafael Caldera in power following the capture and assassination of President Perez. Initially prepared for December, Chávez delayed the MBR-200 coup until the early twilight hours of 4 February 1992.
On that date, five army units under Chávez's command moved into urban Caracas. Despite years of planning, the coup quickly encountered trouble since Chávez could command the loyalty of less than 10% of Venezuela's military forces. After numerous betrayals, defections, errors, and other unforeseen circumstances, Chávez and a small group of rebels found themselves hiding in the Military Museum, unable to communicate with other members with Pérez managing to escape Miraflores Palace. Fourteen soldiers were killed, and fifty soldiers and some eighty civilians injured during the ensuing violence. Another unsuccessful coup against the government occurred in November, with the fighting during the coups resulting in the deaths of at least 143 people and perhaps as many as several hundred.
Chávez gave himself up to the government and appeared on television, in uniform, to call on remaining coup members to lay down their arms. Many viewers noted that Chávez in his speech had remarked that he had failed only ""por ahora"" (for now), and many Venezuelans, particularly poor ones, began seeing him as someone who stood up against government corruption and kleptocracy.
Chávez was arrested and imprisoned at the San Carlos military stockade, where he remained wracked with guilt, feeling responsible for the coup's failure. Pro-Chávez demonstrations that took place outside of San Carlos led to his being transferred to Yare prison soon after. The government meanwhile began a temporary crackdown on media supportive of Chávez and the coup. Pérez himself was then impeached a year later for malfeasance and misappropriation of funds for illegal activities.
Political rise: 1992–1998.
While Chávez and the other senior members of the MBR-200 were in prison, his relationship with Herma Marksman broke up in July 1993. In 1994, Rafael Caldera (1916–2009) of the centrist National Convergence Party and who had knowledge of the coup was elected president, and soon after freed Chávez and the other imprisoned MBR-200 members, though Caldera banned them from returning to the military. Chávez went on a 100-day tour of the country, promoting his Bolivarian cause of social revolution. On his tours around the country he met Marisabel Rodríguez, who would give birth to their daughter shortly before becoming his second wife in 1997.
Travelling around Latin America in search of foreign support for his Bolivarian movement, he visited Argentina, Uruguay, Chile, Colombia, and finally Cuba, where he met Castro and became friends with him. During his stay in Colombia, he spent six months receiving guerilla training and establishing contacts with the FARC and ELN terrorist groups, and even adopted a nom de guerre, Comandante Centeno. After his return to Venezuela, Chávez was critical of President Caldera and his neoliberal economic policies. A drop in per capita income, coupled with increases in poverty and crime, "led to gaps emerging between rulers and ruled which favoured the emergence of a populist leader".
By now Chávez was a supporter of taking military action, believing that the oligarchy would never allow him and his supporters to win an election, while Francisco Arias Cárdenas insisted that they take part in the representative democratic process. Indeed, Cárdenas soon joined the Radical Cause socialist party and won the December 1995 election to become governor of the oil-rich Zulia State. As a result, Chávez and his supporters founded a political party, the Fifth Republic Movement (MVR – "Movimiento Quinta República") in July 1997 in order to support Chávez's candidature in the Venezuelan presidential election, 1998.
1998 election.
At the start of the election run-up, front runner Irene Sáez was backed by one of Venezuela's two primary political parties, Copei. Chávez's revolutionary rhetoric gained him support from "Patria Para Todos" (Fatherland for All), the "Partido Comunist Venezolano" (Venezeuelan Communist Party) and the "Movimiento al Socialismo" (Movement for Socialism). Chávez's promises of widespread social and economic reforms won the trust and favor of a primarily poor and working class. By May 1998, Chávez's support had risen to 30% in polls, and by August he was registering 39%. With his support increasing, and Sáez's decreasing, both the main two political parties, Copei and Democratic Action, put their support behind Henrique Salas Römer, a Yale University-educated economist who represented the Project Venezuela party.
Voter turnout in the election is the subject of dispute. Voter turnout was at 63.45%, with Chávez winning the election with 56.20% of the vote. Academic analysis of the election showed that Chávez's support had come primarily from the country's poor and the "disenchanted middle class", whose standard of living had decreased rapidly in the previous decade, and much of the middle and upper class vote went Römer.
Presidency: 1999–2013.
First presidential term: 2 February 1999 – 10 January 2001.
Chávez's presidential inauguration took place on 2 February 1999, and during the usual presidential oath he deviated from the prescribed words to proclaim that "I swear before God and my people that upon this moribund constitution I will drive forth the necessary democratic transformations so that the new republic will have a Magna Carta befitting these new times." He appointed new figures to a number of government posts, including promoting various leftist allies to key positions; he for instance gave one of the founders of MBR, Jesús Urdaneta, the position in charge of the Bolivarian Intelligence Agency; and made one of the 1992 coup leaders, Hernán Grüber Ódreman, governor of the Federal District of Caracas.
Chávez also appointed some conservative, centrist and centre-right figures to government positions as well, reappointing Caldera's economy minister Maritza Izaquirre to that same position and also appointing the businessman Roberto Mandini to be president of the state-run oil company Petroleos de Venezuela. His critics referred to this group of government officials as the "Boliburguesía" or "Bolivarian bourgeoisie", and highlighted the fact that it "included few people with experience in public administration." The involvement of a number of his immediate family members in Venezuelan politics led to accusations of nepotism. In June 2000 he separated from his wife Marisabel, and their divorce was finalised in January 2004.
The Chávez government's initial policies were moderate, capitalist and centre-left, having much in common with those of contemporary Latin American leftists like Brazil's president Lula da Silva. Chávez initially believed that capitalism was still a valid economic model for Venezuela, but only Rhenish capitalism, not the US-supported neoliberalism of former governments. He followed the economic guidelines recommended by the International Monetary Fund and continued to encourage foreign corporations to invest in Venezuela, even visiting the New York Stock Exchange in the United States in an attempt to convince wealthy investors to do so.
Chávez set into motion a social welfare program called Plan Bolívar 2000, which he organised to begin on 27 February 1999, the tenth anniversary of the "Caracazo" massacre. Chávez said he would set aside $20.8 million for the plan, though some state that the program costed $113 million. Plan Bolívar 2000 involved 70,000 soldiers, sailors and members of the air force going out into the streets of Venezuela where they would repair roads and hospitals, remove stagnant water that offered breeding areas for disease-carrying mosquitoes, offer free medical care and vaccinations, and sell food at low prices.
In May 2000 he launched his own Sunday morning radio show, "Aló Presidente" ("Hello, President"), on the state radio network, as well as a Thursday night television show, "De Frente con el Presidente" ("Face to Face with the President"). He followed this with his own newspaper, "El Correo del Presidente" ("The President's Post"), founded in July, for which he acted as editor-in-chief, but which was later shut amidst accusations of corruption in its management. In his television and radio shows, he answered calls from citizens, discussed his latest policies, sang songs and told jokes, making it unique not only in Latin America but the entire world.
Constitutional reform.
Chávez then called for a public referendum which he hoped would support his plans to form a constitutional assembly, composed of representatives from across Venezuela, as well as from indigenous tribal groups, which would be able to rewrite the nation's constitution. Using the momentum of support he had received in the previous elections, the referendum went ahead on 25 April 1999, and was a success for Chávez, with 88% of voters supporting the proposal.
Then Chávez called for an election to take place on 25 July, in which the members of the constitutional assembly would be voted into power. Of the 1,171 candidates standing for election to the assembly, over 900 of them were opponents of Chávez. Despite the large number of opposition candidates, Chavez's supporters won another overwhelming electoral victory creating "a very pro-Chávez Constitutional Assembly", with his supporters taking 125 seats (95% of the total), including all of those belonging to indigenous tribal groups, whereas the opposition were voted into only 6 seats.
On 12 August 1999, the new constitutional assembly voted to give themselves the power to abolish government institutions and to dismiss officials who were perceived as being corrupt or operating only in their own interests. Opponents of the Chávez regime argued that it was therefore dictatorial. Most jurists believed that the new constitutional assembly became the country's "supreme authority" and that all other institutions were subordinate to it. The assembly also declared a "judicial emergency", granting itself the power to overhaul the judicial system. The Supreme Court, which ruled that the assembly did indeed have such authority, was eventually replaced by the 1999 Constitution, which created the "Supreme Tribunal of Justice" in its place.
The constituent assembly, filled with Chávez's supporters, put together a new constitution, and a referendum on the issue of whether to adopt it was held in December 1999; the referendum saw an abstention vote of over 50%, although among those voting, 72% approved the new constitution's adoption. The constitution included progressive language of environment and indigenous protection, socioeconomic guarantees with state benefits, but it also gave greater powers to Chávez. The assembly granted the presidency more power by extending their term and getting rid of the two houses of the Congress, while also granting the power to legislate on citizen rights, to promote military officers and to oversee economic and financial matters. It also gave the military a role in the government by providing it with the mandated role of ensuring public order and aiding national development, something it had been expressely forbidden from doing under the former constitution. As a part of the new constitution, the country, which was then officially known as the Republic of Venezuela, was renamed the Bolivarian Republic of Venezuela (República Bolivariana de Venezuela) at Chávez's request.
With the creation of the 1999 Venezuelan constitution by the pro-Chávez constituent assembly, checks and balances were eliminated and Chávez's government controlled every branch of Venezuelan government for over 15 years until the Venezuelan parliamentary election in 2015.
Second presidential term: 10 January 2001 – 10 January 2007.
Under the new constitution, it was legally required that new elections be held in order to re-legitimize the government and president. This presidential election in July 2000 would be a part of a greater "megaelection", the first time in the country's history that the president, governors, national and regional congressmen, mayors and councilmen would be voted for on the same day. Going into the elections, Chávez had control of all three branches of government. For the position of president, Chávez's closest challenger proved to be his former friend and co-conspirator in the 1992 coup, Francisco Arias Cárdenas, who since becoming governor of Zulia state had turned towards the political centre and begun to denounce Chávez as autocratic. Although some of his supporters feared that he had alienated those in the middle class and the Roman Catholic Church hierarchy who had formerly supported him, Chávez was re-elected with 59.76% of the vote (the equivalent of 3,757,000 people), a larger majority than his 1998 electoral victory, again primarily receiving his support from the poorer sectors of Venezuelan society.
That year, Chávez helped to further cement his geopolitical and ideological ties with the Cuban government of Fidel Castro by signing an agreement under which Venezuela would supply Cuba with 53,000 barrels of oil per day at preferential rates, in return receiving 20,000 trained Cuban medics and educators. In the ensuing decade, this would be increased to 90,000 barrels a day (in exchange for 40,000 Cuban medics and teachers), dramatically aiding the Caribbean island's economy and standard of living after its "Special Period" of the 1990s. However, Venezuela's growing alliance with Cuba came at the same time as a deteriorating relationship with the United States: in late 2001, just after the American-led invasion of Afghanistan in retaliation for 11 September attacks against the U.S. by Islamist militants, Chávez showed pictures of Afghan children killed in a bomb attack on his television show. He commented that "They are not to blame for the terrorism of Osama Bin Laden or anyone else", and called on the American government to end "the massacre of the innocents. Terrorism cannot be fought with terrorism." The U.S. government responded negatively to the comments, which were picked up by the media worldwide.
Meanwhile, the 2000 elections had led to Chávez's supporters gaining 101 out of 165 seats in the Venezuelan National Assembly, and so in November 2001 they voted to allow him to pass 49 social and economic decrees. This move antagonized the opposition movement particularly strongly.
At the start of the 21st century, Venezuela was the world's fifth largest exporter of crude oil, with oil accounting for 85.3% of the country's exports, therefore dominating the country's economy. Previous administrations had sought to privatise this industry, with U.S. corporations having a significant level of control, but the Chávez administration wished to curb this foreign control over the country's natural resources by nationalising much of it under the state-run oil company, Petróleos de Venezuela S.A. (PdVSA). In 2001, the government introduced a new Hydrocarbons Law through which they sought to gain greater state control over the oil industry: they did this by raising royalty taxes on the oil companies and also by introducing the formation of "mixed companies", whereby the PdVSA could have joint control with private companies over industry. By 2006, all of the 32 operating agreements signed with private corporations during the 1990s had been converted from being primarily or solely corporate-run to being at least 51% controlled by PdVSA.
Opposition and the CD.
During Chávez's first term in office, the opposition movement had been "strong but reasonably contained, complaints centering mainly on procedural aspects of the implementation of the constitution".
The first organized protest against the Bolivarian government occurred in January 2001, when the Chávez administration tried to implement educational reforms through the proposed Resolution 259 and Decree 1.011, which would have seen the publication of textbooks with a heavy Bolivarian bias. The protest movement, which was primarily by middle class parents whose children went to privately run schools, marched to central Caracas shouting out the slogan "Don't mess with my children." Although the protesters were denounced by Chávez, who called them "selfish and individualistic," the protest was successful enough for the government to retract the proposed education reforms and instead enter into a consensus-based educational program with the opposition.
Later into 2001, an organization known as the "" (CD) was founded, under which the Venezuelan opposition political parties, corporate powers, most of the country's media, the Venezuelan Federation of Chambers of Commerce, the and the Central Workers Union all united to oppose Chávez's regime. The prominent businessman Pedro Carmona (1941–) was chosen as the CD's leader. They received support from various foreign sources. The CD and other opponents of Chávez's Bolivarian government accused it of trying to turn Venezuela from a democracy into a dictatorship by centralising power amongst its supporters in the Constituent Assembly and granting Chávez increasingly autocratic powers. Many of them pointed to Chávez's personal friendship with Cuba's Fidel Castro and the one-party socialist government in Cuba as a sign of where the Bolivarian government was taking Venezuela. Others did not hold such a strong view but still argued that Chávez was a "free-spending, authoritarian populist" whose policies were detrimental to the country.
Coup, strikes and the recall referendum.
On 11 April 2002, during mass protests in Caracas against the Bolivarian government, twenty people were killed, and over 110 were wounded. A group of high-ranking anti-Chávez military officers had been planning to launch a coup against Chávez and used the civil unrest as an opportunity. After the plotters gained significant power, Chávez agreed to be detained and was transferred by army escort to La Orchila; business leader Pedro Carmona declared himself president of an interim government. Carmona abolished the 1999 constitution and appointed a small governing committee to run the country. Protests in support of Chávez along with insufficient support for Carmona's regime, which some felt was implementing totalitarian measures, quickly led to Carmona's resignation, and Chávez was returned to power on 14 April.
Chávez's response was to moderate his approach, implementing a new economic team that appeared to be more centrist and reinstated the old board of directors and managers of the state oil company Petróleos de Venezuela S.A. (PDVSA), whose replacement had been one of the reasons for the coup. At the same time, the Bolivarian government began increased the country's military capacity, purchasing 100,000 AK-47 assault rifles and several helicopters from Russia, as well as a number of Super Tucano light attack and training planes from Brazil. Troop numbers were also increased.
In 2002, after appointing political allies to head the PDVSA and replacing the company's board of directors with loyalists who had "little or no experience in the oil industry", Chávez faced a two-month management strike at the PDVSA. The Chávez government's response was to fire about 19,000 striking employees for illegally abandoning their posts and then employing retired workers, foreign contractors, and the military to do their jobs instead. According to one observer, this move further damaged the strength of Chávez's opposition by removing the many managers in the oil industry who had been supportive of their cause to overthrow Chávez.
The 1999 constitution had introduced the concept of a recall referendum into Venezuelan politics, so the opposition called for such a referendum to take place. A 2004 referendum to recall Chávez was defeated. 70% of the eligible Venezuelan population turned out to vote, with 59% of voters deciding to keep the president in power. Unlike his original 1998 election victory, this time Chávez's electoral support came almost entirely from the poorer working classes rather than the middle classes, who "had practically abandoned Chávez" after he "had consistently moved towards the left in those five and a half years".
"Socialism of the 21st century".
The various attempts at overthrowing the Bolivarian government from power had only served to further radicalize Chávez. In January 2005, he began openly proclaiming the ideology of "socialism of the 21st Century", something that was distinct from his earlier forms of Bolivarianism, which had been social democratic in nature, merging elements of capitalism and socialism. He used this new term to contrast the democratic socialism, which he wanted to promote in Latin America from the Marxist–Leninist socialism that had been spread by socialist states like the Soviet Union and the People's Republic of China during the 20th century, arguing that the latter had not been truly democratic, suffering from a lack of participatory democracy and an excessively authoritarian governmental structure.
In May 2006, Chávez visited Europe in a private capacity, where he announced plans to supply cheap Venezuelan oil to poor working class communities in the continent. The Mayor of London Ken Livingstone welcomed him, describing him as "the best news out of Latin America in many years".
Third presidential term: 10 January 2007 – 10 January 2013.
In the presidential election of December 2006, which saw a 74% voter turnout, Chávez was once more elected, this time with 63% of the vote, beating his closest challenger Manuel Rosales, who conceded his loss. The election was certified as being free and legitimate by the Organization of American States (OAS) and the Carter Center. After this victory, Chávez promised an "expansion of the revolution."
United Socialist Party of Venezuela and domestic policy.
On 15 December 2006, Chávez publicly announced that those leftist political parties who had continually supported him in the Patriotic Pole would unite into one single, much larger party, the United Socialist Party of Venezuela ("Partido Socialista Unido de Venezuela", PSUV). In the speech which he gave announcing the PSUV's creation, Chávez declared that the old parties must "forget their own structures, party colours and slogans, because they are not the most important thing for the fatherland." According to political analyst Barry Cannon, the purpose of creating the PSUV was to "forge unity amongst the disparate elements the Bolivarian movement, providing grassroots input into policy and leadership formation, uniting the grassroots and leadership into one single body." It was hoped that by doing so, it would decrease the problems of clientelism and corruption and also leave the movement less dependent on its leadership: as Chávez himself declared, "In this new party, the bases will elect the leaders. This will allow real leaders to emerge."
Chávez had initially proclaimed that those leftist parties which chose to not dissolve into the PSUV would have to leave the government, however, after several of those parties supporting him refused to do so, he ceased to issue such threats. There was initially much grassroots enthusiasm for the creation of the PSUV, with membership having risen to 5.7 million people by 2007, making it the largest political group in Venezuela. The United Nations' International Labour Organization however expressed concern over some voters' being pressured to join the party.
In 2007, the Bolivarian government set up a constitutional commission in order to review the 1999 constitution and suggest potential amendments to be made to it. Led by the prominent pro-Chávez intellectual Luis Britto García, the commission came to the conclusion that the constitution could include more socially progressive clauses, such as the shortening of the working week, a constitutional recognition of Afro Venezuelans and the elimination of discrimination on the grounds of sexual orientation. It also suggested measures that would have increased many of the president's powers, for instance increasing the presidential term limit to seven years, allowing the president to run for election indefinitely and centralizing powers in the executive. The government put the suggested changes to a public referendum in December 2007. Abstention rate was high however, with 43.95% of registered voters not turning out, and in the end the proposed changes were rejected by 50.65% of votes. This would prove to the first electoral loss that Chávez had faced in the thirteen electoral contests held since he took power, something analysts argued was due to the top-down nature of the changes, as well as general public dissatisfaction with "the absence of internal debate on its content, as well as dissatisfaction with the running of the social programmes, increasing street crime, and with corruption within the government."
In order to ensure that his Bolivarian Revolution became socially engrained in Venezuela, Chávez discussed his wish to stand for re-election when his term ran out in 2013, and spoke of ruling beyond 2030. Under the 1999 constitution, he could not legally stand for re-election again, and so brought about a referendum on 15 February 2009 to abolish the two-term limit for all public offices, including the presidency. Approximately 70% of the Venezuelan electorate voted, and they approved this alteration to the constitution with over 54% in favor, allowing any elected official the chance to try to run indefinitely.
Fourth presidential term: 10 January 2013 – 5 March 2013.
On 7 October 2012, Chávez won election as president for a fourth time, his third six-year term. He defeated Henrique Capriles with 54% of the votes versus 45% for Capriles, which was a lower victory margin than in his previous presidential wins, in the 2012 Venezuelan presidential election Turnout in the election was 80%, with a hotly contested election between the two candidates. There was significant support for Chávez amongst the Venezuelan lower class. Chávez's opposition blamed him for unfairly using state funds to spread largesse before the election to bolster Chavez's support among his primary electoral base, the lower class.
The inauguration of Chávez's new term was scheduled for 10 January 2013, but as he was undergoing medical treatment at the time in Cuba, he was not able to return to Venezuela for that date. The National Assembly president Diosdado Cabello proposed to postpone the inauguration and the Supreme Court decided that, being just another term of the sitting president and not the inauguration of a new one, the formality could be bypassed. The Venezuelan Bishops Conference opposed the verdict, stating that the constitution must be respected and the Venezuelan government had not been transparent regarding details about Chávez's health.
Acting executive officials produced orders of government signed by Chávez, which were suspected of forgery by some opposition politicians, who claimed that Chávez was too sick to be in control of his faculties. Guillermo Cochez, recently dismissed from the office of Panamanian ambassador to the Organization of American States, even claimed that Chávez had been brain-dead since 31 December 2012. Near to Chavez's death, two American attachés were expelled from the country for allegedly undermining Venezuelan democracy.
Due to the death of Chávez, Vice President Nicolas Maduro took over the presidential powers and duties for the remainder of Chávez's abbreviated term until presidential elections were held. Venezuela's constitution specifies that the speaker of the National Assembly, Diosdado Cabello, should assume the interim presidency if a president cannot be sworn in.
Political ideology.
Chávez propagated what he called "socialism for the 21st century", but according to the pro-Chavez academic Gregory Wilpert, "Chávez has not clearly defined twenty-first century socialism, other than to say that it is about establishing liberty, equality, social justice, and solidarity. He has also indicated that it is distinctly different from state socialism", as implemented by the governments of the Soviet Union and the People's Republic of China. As a part of his socialist ideas, he emphasised the role of so-called "participatory democracy", which he claimed increased democratic participation, and was implemented through the foundation of the Venezuelan Communal Councils and Bolivarian Circles which he cited as examples of grassroots and participatory democracy.
Bolivarianism.
Hugo Chávez defined his political position as Bolivarianism, an ideology he developed from that of Simón Bolívar (1783–1830) and others. Bolívar was a 19th-century general who led the fight against the colonialist Spanish authorities and who is widely revered across Latin America today. Along with Bolívar, the other two primary influences upon Bolivarianism are Simón Rodríguez (1769–1854), a philosopher who was Bolívar's tutor and mentor, and Ezequiel Zamora, (1817–1860), the Venezuelan Federalist general. Chávez's ideology originating from Bolívar has also received some criticism because Chávez had occasionally described himself as being influenced by Karl Marx, a critic of Bolívar. Beddow and Thibodeaux noted the complications between Bolívar and Marx, stating that "[describing Bolivar as a socialist warrior in the class struggle, when he was actually member of the aristocratic 'criollos,' is peculiar when considering Karl Marx's own writings on Bolivar, whom he dismissed as a false liberator who merely sought to preserve the power of the old Creole nobility which he belonged".
Marxism.
Chávez's connection to Marxism was a complex one, though he had described himself as a Marxist on some occasions. In May 1996, he gave an interview with Agustín Blanco Muñoz in which he remarked that "I am not a Marxist, but I am not anti-Marxist. I am not communist, but I am not anti-communist." In a 2009 speech to the national assembly, he said: "I am a Marxist to the same degree as the followers of the ideas of Jesus Christ and the liberator of America, Simon Bolivar." He was well versed in many Marxist texts, having read the works of many Marxist theoreticians, and often publicly quoted them. Various international Marxists supported his government, believing it to be a sign of proletariat revolution as predicted in Marxist theory. In 2010, Hugo Chávez proclaimed support for the ideas of Marxist Leon Trotsky, saying "When I called him (former Minister of Labour, José Ramón Rivero)" Chávez explained, "he said to me: 'President I want to tell you something before someone else tells you ... I am a Trotskyist', and I said, 'well, what is the problem? I am also a Trotskyist! I follow Trotsky's line, that of permanent revolution," and then cited Marx and Lenin.
Other influences.
Chávez's early heroes were nationalist military dictators that included former Peruvian president Juan Velasco Alvarado and former Panamanian "Maximum Leader" Omar Torrijos. One dictator Chávez admired was Marcos Pérez Jiménez, a former president of Venezuela that he praised for the public works he performed. Chávez praised Pérez Jiménez in order to vilify preceding democratic governments, stating that "General Pérez Jiménez was the best president Venezuela had in a long time ... He was much better than Rómulo Betancourt, much better than all of those others. They hated him because he was a soldier."
Chávez was also well acquainted with the various traditions of Latin American socialism, espoused by such figures as Colombian politician Jorge Eliécer Gaitán and former Chilean president Salvador Allende. Early in his presidency, Chávez was advised and influenced by the Argentine fascist Norberto Ceresole. Cuban Communist revolutionaries Che Guevara and Fidel Castro also influenced Chávez, especially with Castro's government assistance with the Bolivarian Missions. Other indirect influences on Chávez's political philosophy are the writings of American linguist Noam Chomsky and the Gospel teachings of Jesus Christ. Other inspirations of Chávez's political view are Giuseppe Garibaldi, Antonio Gramsci and Antonio Negri.
Policy overview.
Economic and social policy.
From his election in 1998 until his death in March 2013, Chávez's administration proposed and enacted democratic socialist economic policies. Domestic policies included redistribution of wealth, land reform, and democratization of economic activity via workplace self-management and creation of worker-owned cooperatives.
With increasing oil prices in the early 2000s and funds not seen in Venezuela since the 1980s, Chávez created the Bolivarian Missions, aimed at providing public services to improve economic, cultural, and social conditions so he could maintain political power. The Missions entailed the construction of thousands of free medical clinics for the poor, and the enactment of food and housing subsidies. A 2010 OAS report indicated achievements in addressing illiteracy, healthcare and poverty, and economic and social advances. The quality of life for Venezuelans had also improved according to a UN Index. Teresa A. Meade wrote that Chávez's popularity strongly depended "on the lower classes who have benefited from these health initiatives and similar policies."
The Gini coefficient, a measure of income inequality, also dropped from .495 in 1998 to .39 in 2011, putting Venezuela behind only Canada in the Western Hemisphere. Venezuelans aged 15 and older, 95.2% could also read and write, with Venezuela having one of the highest literacy rates in the region, though some scholars have disputed that literacy improvements during Chavez's presidency resulted from his administration's policies. The poverty rate fell from 48.6% in 1999 to 32.1% in 2013, according to the Venezuelan government's National Statistics Institute (INE). The drop of Venezuela's poverty rate compared to poverty in other South American countries was slightly behind that of Peru, Brazil and Panama with the poverty rate becoming higher than the Latin American average in 2013 according to the UN. In the two years following Chávez's death, the poverty rate returned to where it had been before his presidency.
The social works initiated by Chávez's government relied on oil products, the keystone of the Venezuelan economy, with Chávez's administration suffering from Dutch disease as a result. Economist Mark Weisbrot, in a 2009 analysis of the Chávez administration stated that economic expansion during Chávez's tenure "began when the government got control over the national oil company in the first quarter of 2003". Chávez gained a reputation as a price hawk in OPEC, pushing for stringent enforcement of production quotas and higher target oil prices. According to Cannon, the state income from oil revenue grew "from 51% of total income in 2000 to 56% 2006"; oil exports increased "from 77% in 1997 [...] to 89% in 2006"; and his administration's dependence on petroleum sales was "one of the chief problems facing the Chávez government". In 2012, the World Bank also explained that Venezuela's economy is "extremely vulnerable" to changes in oil prices since in 2012 "96% of the country's exports and nearly half of its fiscal revenue" relied on oil production, while by 2008, according to "Foreign Policy", exports of everything but oil "collapsed". The Chávez administration then used such oil prices on his populist policies and for voters.
Economists say that the Venezuelan government's overspending on social programs and strict business policies contributed to imbalances in the country's economy, contributing to rising inflation, poverty, low healthcare spending and shortages in Venezuela going into the final years of his presidency. Such occurrences, especially the risk of default and the unfriendliness toward private businesses, led to a lack of foreign investment and stronger foreign currencies, though the Venezuelan government argued that the private sector had remained relatively unchanged during Chavez's presidency despite several nationalizations. In January 2013 near the end of Chávez's presidency, the Heritage Foundation and the Wall Street Journal gave Venezuela's economic freedom a low score of 36.1, twenty points lower than 56.1 in 1999, ranking its freedom very low at 174 of 177 countries, with freedom on a downward trend. Nicholas Kozloff, Chávez's biographer, stated of Chávez's economic policies: "Chávez has not overturned capitalism, he has done much to challenge the more extreme, neo-liberal model of development." According to analysts, the economic woes Venezuela suffered under President Nicolás Maduro would have still occurred with or without Chávez.
Food and products.
In the 1980s and 1990s health and nutrition indexes in Venezuela were generally low, and social inequality in access to nutrition was high. Chávez made it his stated goal to lower inequality in the access to basic nutrition, and to achieve food sovereignty for Venezuela. The main strategy for making food available to all economic classes was a controversial policy of fixing price ceilings for basic staple foods implemented in 2003. Between 1998 and 2006 malnutrition related deaths fell by 50%. In October 2009, the Executive Director of the National Institute of Nutrition (INN) Marilyn Di Luca reported that the average daily caloric intake of the Venezuelan people had reached 2790 calories, and that malnutrition had fallen from 21% in 1998 to 6%. Chávez also expropriated and redistributed 5 million acres of farmland from large landowners.
Price controls initiated by Chávez created shortages of goods since merchants could no longer afford to import necessary goods. Chávez blamed "speculators and hoarders" for these scarcities and strictly enforced his price control policy, denouncing anyone who sold food products for higher prices as "speculators". In 2011, food prices in Caracas were nine times higher than when the price controls were put in place and resulted in shortages of cooking oil, chicken, powdered milk, cheese, sugar and meat. The price controls increased the demand for basic foods while making it difficult for Venezuela to import goods causing increased reliance on domestic production. Economists believe this policy increased shortages. Shortages of food then occurred throughout the rest of Chávez's presidency with food shortage rates between 10% and 20% from 2010 to 2013. One possible reason for shortages is the relationship between inflation and subsidies, where no profitability due to price regulations affect operations. In turn, the lack of dollars made it difficult to purchase more food imports. Chávez's strategy in response to food shortages consisted of attempting to increase domestic production through nationalizing large parts of the food industry, though such nationalizations allegedly did the opposite and caused decreased production instead.
As part of his strategy of food security Chávez started a national chain of supermarkets, the Mercal network, which had 16,600 outlets and 85,000 employees that distributed food at highly discounted prices, and ran 6000 soup kitchens throughout the country. Simultaneously Chávez expropriated many private supermarkets. According to Commerce Minister Richard Canan, "The average for the basic food bundle (at the Mercal Bicentennial markets) is around 30%. There are some products, for example cheese and meat, which reach a savings of 50 to 60% compared with capitalist markets." The Mercal network was criticized by some commentators as being a part of Chávez's strategy to brand himself as a provider of cheap food, and the shops feature his picture prominently. The Mercal network was also subject to frequent scarcities of basic staples such as meat, milk and sugar – and when scarce products arrived, shoppers had to wait in lines.
Communes.
After his election in 1998, more than 100,000 state-owned cooperatives – which claimed to represent some 1.5 million people – were formed with the assistance of government start-up credit and technical training; and the creation and maintenance, as of September 2010, of over 30,000 communal councils, examples of localised participatory democracy; which he intended to be integrated into regional umbrella organizations known as ""Communes in Construction"".
In 2010, Chávez supported the construction of 184 communes, housing thousands of families, with $23 million in government funding. The communes produced some of their own food, and were able to make decisions by popular assembly of what to do with government funds. In September 2010, Chávez announced the location of 876 million bolivars ($203 million) for community projects around the country, specifically communal councils and the newly formed communes. Chávez also criticised the bureaucracy still common in Venezuela saying, when in discussion with his Communes Minister Isis Ochoa, that "All of the projects must be carried out by the commune, not the bureaucracy." The Ministry for Communes, which oversees and funds all communal projects, was initiated in 2009. Despite such promises, the Venezuelan government often failed to construct the number of homes they had proposed. According to Venezuela's "El Universal", one of the Chávez administration's outstanding weaknesses is the failure to meet its goals of construction of housing.
Currency controls.
In the first few years of Chavez's office, his newly created social programs required large payments in order to make the desired changes. On February 5, 2003, the government created CADIVI, a currency control board charged with handling foreign exchange procedures. Its creation was to control capital flight by placing limits on individuals and only offering them so much of a foreign currency. This limit to foreign currency led to a creation of a currency black market economy since Venezuelan merchants rely on foreign goods that require payments with reliable foreign currencies. As Venezuela printed more money for their social programs, the bolívar continued to devalue for Venezuelan citizens and merchants since the government held the majority of the more reliable currencies.
The implied value or "black market value" is what Venezuelans believe the Bolivar Fuerte is worth compared to the United States dollar. The high rates in the black market make it difficult for businesses to purchase necessary goods since the government often forces these businesses to make price cuts.
This leads to businesses selling their goods and making a low profit. Since businesses make low profits, this leads to shortages since they are unable to import the goods that Venezuela is reliant on.
Crime and punishment.
During the 1980s and 1990s there was a steady increase in crime in Latin America. The countries of Colombia, El Salvador, Venezuela, and Brazil all had homicide rates above the regional average. During his terms as president, hundreds of thousands of Venezuelans were murdered due to violent crimes occurring in the country. Gareth A. Jones and Dennis Rodgers stated in their book "Youth violence in Latin America: Gangs and Juvenile Justice in Perspective" that, "With the change of political regime in 1999 and the initiation of the Bolivarian Revolution, a period of transformation and political conflict began, marked by a further increase in the number and rate of violent deaths" showing that in four years, the murder rate had increased to 44 per 100,000 people. Kidnappings also rose tremendously during Chavez's tenure, with the number of kidnappings over 20 times higher in 2011 than when Chavez was elected. Documentary filmmaker James Brabazon, stated "kidnapping crimes had skyrocketed ... after late Venezuelan President Hugo Chavez freed thousands of violent prisoners as part of controversial criminal justice system reforms" while kidnappings and murders also increased due to Colombian organized crime activity as well. He further explained that common criminals felt that the Venezuelan government did not care for the problems of the higher and middle classes, which in turn gave them a sense of impunity that created a large business of kidnapping-for-ransom.
Under Chávez's administration, crimes were so prevalent that by 2007 the government no longer produced crime data. Homicide rates in Venezuela more than tripled, with one NGO finding the rate to have nearly quadrupled. The majority of the deaths occur in crowded slums in Caracas. The NGO found that the number of homicides in the country increased from 6,000 in 1999 to 24,763 in 2013. In 2010 Caracas had the highest murder rate in the world. According to the United Nations Office on Drugs and Crime, in 2012 there were 13,080 murders in Venezuela.
In leaked government INE data for kidnappings in the year 2009, the number of kidnappings were at an estimated 16,917, contrasting the CICPCs number of only 673, before the Venezuelan government blocked the data. According to the leaked INE report, only 1,332 investigations for kidnappings were opened or about 7% of the total kidnapping cases, with 90.4% of the kidnappings happening away from rural areas, 80% of all being express kidnappings and the most common victim being lower-middle or middle class Venezuelans and middle-aged men. Also in 2009, it was reported that Venezuelan authorities would assign judicial police to Caracas area morgues to speak with families. At that time, they would advise families not to report the murder of their family member to the media in exchange for expediting the process of releasing the victim's body.
In September 2010, responding to escalating crime rates in the country, Chávez stated that Venezuela is no more violent than it was when he first took office. An International Crisis Group report that same year stated that when Chávez took office, there were some factors beyond his control that led to the crime epidemic throughout Venezuela, but that Chávez ignored it as well as corruption in the country; especially among fellow state officials. The report also stated that international organised crime filters between Colombia and Venezuela with assistance from "the highest spheres of government" in Venezuela, leading to higher rates of kidnapping, drug trafficking, and homicides. Chávez supporters stated that the Bolivarian National Police has reduced crime and also said that the states with the highest murder rates were controlled by the opposition. According to the publications "El Espectador" and "Le Monde diplomatique", rising crime in rural and urban areas of Venezuela was partly due to increased cross-border activity by Colombian right-wing paramilitary groups like "Águilas Negras".
Prisons.
During Chávez's presidency, there were reports of prisoners having easy access to firearms, drugs, and alcohol. Carlos Nieto—head of Window to Freedom—alleges that heads of gangs acquire military weapons from the state, saying: "They have the types of weapons that can only be obtained by the country's armed forces. ... No one else has these." Use of internet and mobile phones are also a commonplace where criminals can take part in street crime while in prison. One prisoner explained how, "If the guards mess with us, we shoot them" and that he had "seen a man have his head cut off and people play football with it."
Edgardo Lander, a sociologist and professor at the Central University of Venezuela with a PhD in sociology from Harvard University explained that Venezuelan prisons were "practically a school for criminals" since young inmates come out "more sort of trained and hardened than when they went in". He also explained that prisons are controlled by gangs and that "very little has been done" to control them.
Democracy under Chávez.
The electoral processes surrounding Venezuela's democracy under Chávez were often observed controversially. Given the protests and strikes, some of which were quite big, like in 10 December 2001, the largest in the history of Venezuela, some confidential cables published on Wikileaks tried to explain the discrepancy between Chávez's relatively low popularity and his overwhelming electoral victory.
According to the cables, Hugo Chávez used "practically unlimited state resources" for propaganda activities, and high oil prices facilitated his success. The opposition, on the contrary, was divided into different parties, which ran for the same office, and the limited financial resources were badly invested. Chávez lavished huge amounts of money in exchange for votes. He reportedly mobilized the lower class Venezuelan voters who had historically abstained from elections for years, providing both undocumented Venezuelans and foreigners with identity cards; 200,000 foreigners were naturalized before August 2004 and around 3,000-4,000 foreigners per year might have been naturalized thereafter. Most of them purportedly voted for him.
According to the same cable, Chávez had control over the CNE (National Electoral Council) "and, by extension, the international observer missions." Moreover, "The CNE's decision to use fingerprinting machines "cazahuellas" to verify a voter's identity led to the widespread belief that a person's vote would not be secret".
Finally, Chávez allegedly used the judiciary in order to detain or intimidate opposition politicians or NGOs accused of receiving money from the United States (through the National Endowment for Democracy - NED) purportedly in order to overthrow the government. According to the same source, the received money amounts to $30,000. He reportedly also put pressure in the attorney general's office in order to replace three key employees and have any case that might damage the government or Chávez himself undisclosed.
Corruption.
In December 1998, Hugo Chávez declared three goals for the new government; "convening a constituent assembly to write a new constitution, eliminating government corruption, and fighting against social exclusion and poverty". However, during Hugo Chávez's time in power, corruption has become widespread throughout the government due to impunity towards members of the government, bribes and the lack of transparency. In 2004, Hugo Chávez and his allies took over the Supreme Court, filling it with supporters of Chávez and made new measures so the government could dismiss justices from the court. According to the libertarian "Cato Institute", the National Electoral Council of Venezuela was under control of Chávez where he tried to "push a constitutional reform that would have allowed him unlimited opportunities for reelection". The Corruption Perceptions Index, produced annually by the Berlin-based NGO Transparency International (TNI), reported that in the later years of Chávez's tenure, corruption worsened; it was 158th out of 180 countries in 2008, and 165th out of 176 (tied with Burundi, Chad, and Haiti). Most Venezuelans believed the government's effort against corruption was ineffective; that corruption had increased; and that government institutions such as the judicial system, parliament, legislature, and police were the most corrupt.
In Gallup Poll's 2006 Corruption Index, Venezuela ranked 31st out of 101 countries according to how widespread the population perceive corruption as being in the government and in business. The index listed Venezuela as the second least corrupt nation in Latin America, behind Chile. Some criticism came from Chávez's supporters, as well. Chávez's own political party, Fifth Republic Movement (MVR), had been criticized as being riddled with the same cronyism, political patronage, and corruption that Chávez alleged were characteristic of the old "Fourth Republic" political parties. Venezuela's trade unionists and indigenous communities participated in peaceful demonstrations intended to impel the government to facilitate labor and land reforms. These communities, while largely expressing their sympathy and support for Chávez, criticized what they saw as Chávez's slow progress in protecting their interests against managers and mining concerns, respectively.
Aiding FARC.
According to the International Institute for Strategic Studies (IISS), "Chavez's government funded FARC's office in Caracas and gave it access to Venezuela's intelligence services" and said that during the 2002 coup attempt that "FARC also responded to requests from intelligence service to provide training in urban terrorism involving targeted killings and the use of explosives". The IISS continued saying that "the archive offers tantalizing but ultimately unproven suggestions that FARC may have undertaken assassinations of Chavez's political opponents on behalf of the Venezuelan state". Venezuelan diplomats denounced the IISS' findings saying that they had "basic inaccuracies".
In 2007, authorities in Colombia claimed that through laptops they had seized on a raid against Raúl Reyes, they found in documents that Hugo Chávez offered payments of as much as $300 million to the FARC "among other financial and political ties that date back years" along with other documents showing "high-level meetings have been held between rebels and Ecuadorean officials" and some documents claiming that FARC had "bought and sold uranium".
In 2015, Chávez's former bodyguard Leamsy Salazar stated in "Bumerán Chávez" that Chávez met with the high command of FARC in 2007 somewhere in rural Venezuela. Chávez created a system in which the FARC would provide the Venezuelan government with drugs that would be transported in live cattle and the FARC would receive money and weaponry from the Venezuelan government. According to Salazar, this was done in order to weaken Colombian President Álvaro Uribe, an enemy of Chávez.
Human rights.
1999 Venezuelan Constitution.
In the 1999 Venezuelan constitution, 116 of the 350 articles were concerned with human rights; these included increased protections for indigenous peoples and women, and established the rights of the public to education, housing, healthcare, and food. It called for dramatic democratic reforms such as ability to recall politicians from office by popular referendum, increased requirements for government transparency, and numerous other requirements to increase localized, participatory democracy, in favor of centralized administration. It gave citizens the right to timely and impartial information, community access to media, and a right to participate in acts of civil disobedience.
Criticisms.
Shortly after Hugo Chávez's election, ratings for freedom in Venezuela dropped according to political and human rights group Freedom House and Venezuela was rated "partly free". In 2004, Amnesty International criticized President Chavez's administration of not handling the 2002 coup in a proper manner, saying that violent incidents "have not been investigated effectively and have gone
unpunished" and that "impunity enjoyed by the perpetrators encourages further human rights violations in a particularly volatile political climate". Amnesty International also criticized the Venezuelan National Guard and the Direccion de Inteligencia Seguridad y Prevención (DISIP) stating that they "allegedly used excessive force to control the situation on a number of
occasions" during protests involving the 2004 Venezuela recall. It was also noted that many of the protesters detained seemed to not be "brought before a judge within the legal time limit".
In 2008, Human Rights Watch released a report reviewing Chávez's human rights record over his first decade in power. The report praises Chávez's 1999 amendments to the constitution which significantly expanded human rights guarantees, as well as mentioning improvements in women's rights and indigenous rights, but noted a "wide range of government policies that have undercut the human rights protections established" by the revised constitution. In particular, the report accused Chávez and his administration of engaging in discrimination on political grounds, eroding the independence of the judiciary, and of engaging in "policies that have undercut journalists' freedom of expression, workers' freedom of association, and civil society's ability to promote human rights in Venezuela." The Venezuelan government retaliated for the report by expelling members of Human Rights Watch from the country. Subsequently, over a hundred Latin American scholars signed a joint letter with the Council on Hemispheric Affairs, a leftist NGO that would defend Chávez and his movement, with the individuals criticizing the Human Rights Watch report for its alleged factual inaccuracy, exaggeration, lack of context, illogical arguments, and heavy reliance on opposition newspapers as sources, amongst other things.
The International Labour Organization of the United Nations had also expressed concern over voters being pressured to join the party.
In 2010, Amnesty International criticized the Chávez administration for targeting critics following several politically motivated arrests. Freedom House listed Venezuela as being "partly free" in its 2011 Freedom in the World annual report, noting a recent decline in civil liberties. A 2010 Organization of American States report found concerns with freedom of expression, human rights abuses, authoritarianism, press freedom, threats to democracy, as well as erosion of separation of powers, the economic infrastructure and ability of the president to appoint judges to federal courts. OAS observers were denied access to Venezuela; Chávez rejected the OAS report, pointing out that its authors did not even come to Venezuela. He said Venezuela should boycott the OAS, which he felt is dominated by the United States; a spokesperson said, "We don't recognize the commission as an impartial institution". He disclaimed any power to influence the judiciary. A Venezuelan official said the report distorted and took statistics out of context, and said that "human rights violations in Venezuela have decreased". Venezuela said it would not accept an IACHR/OAS visit as long as Santiago Cantón remains its Executive Secretary, unless the IACHR apologizes for what he described as its support of the 2002 coup.
In November 2014, Venezuela appeared before the United Nations Committee Against Torture over cases between 2002 and 2014. Human rights expert of the UN committee, Felice D. Gaer, noted that in "only 12 public officials have been convicted of human rights violations in the last decade when in the same period have been more than 5,000 complaints". The United Nations stated that there were 31,096 complaints of human rights violations received between the years 2011 and 2014. Of the 31,096 complaints, only 3.1% of the cases resulted in only in an indictment by the Venezuelan Public Ministry.
Allegations of Anti-semitism.
Chavez's opposition to Zionism and close relations with Iran led to accusations of antisemitism Such claims were made by the Venezuelan Jewish community at a World Jewish Congress Plenary Assembly in Jerusalem. Claims of antisemitism were prompted by various remarks Chávez made, including in a 2006 Christmas speech where he complained that "a minority, the descendants of the same ones that crucified Christ", now had "taken possession of all of the wealth of the world". In 2009, attacks on a synagogue in Caracas were alleged to be influenced by "vocal denunciations of Israel" by the Venezuelan state media and Hugo Chávez even though Chavez promptly condemned the attacks blaming an "oligarchy". A weeklong CICPC investigation revealed the synagogue attack to be an 'inside job', the motive apparently being robbery rather than anti-semitism.
Media and the press.
Human Rights Watch criticized Chávez for engaging in "often discriminatory policies that have undercut journalists' freedom of expression". Freedom House listed Venezuela's press as being "Not Free" in its 2011 Map of Press Freedom, noting that "gradual erosion of press freedom in Venezuela continued in 2010." Reporters Without Borders criticized the Chávez administration for "steadily silencing its critics". In the group's 2009 Press Freedom Index, Reporters Without Borders noted that "Venezuela is now among the region’s worst press freedom offenders."
In July 2005 Chávez inaugurated TeleSUR, a Pan-American equivalent of Al Jazeera that sought to challenge the present domination of Latin American television news by Univision and the United States-based CNN en Español. In 2006 Chávez inaugurated a state-funded movie studio called "Villa del Cine" (English: Cinema City).
Chávez also had a Twitter account with more than 3,200,000 followers as of August 2012. A team of 200 people sorted through suggestions and comments sent via Twitter. Chávez said Twitter was "another mechanism for contact with the public, to evaluate many things and to help many people", and that he saw Twitter as "a weapon that also needs to be used by the revolution".
Foreign policy.
Though Chávez inspired other movements in Latin America to follow his model of "chavismo" in an attempt to reshape South America, it was later seen as being erratic and his influence internationally became exaggerated. He refocused Venezuelan foreign policy on Latin American economic and social integration by enacting bilateral trade and reciprocal aid agreements, including his so-called "oil diplomacy" making Venezuela more dependent on using oil, its main commodity, and increasing it longterm vulnerability. Chávez also aligned himself with authoritarian nations and radical movements that were seen as being anti-Western, with relations with Cuba and Iran becoming a particular importance. Chávez focused on a variety of multinational institutions to promote his vision of Latin American integration, including Petrocaribe, Petrosur, and TeleSUR. Bilateral trade relationships with other Latin American countries also played a major role in his policy, with Chávez increasing arms purchases from Brazil, forming oil-for-expertise trade arrangements with Cuba, and creating unique barter arrangements that exchange Venezuelan petroleum for cash-strapped Argentina's meat and dairy products. He also befriended pariah states such as Belarus and Iran. Domestic mishandling of the country under Chávez prevented Venezuela from strengthening its position in the world.
Personal life.
Chávez married twice. He first wed Nancy Colmenares, a woman from a poor family in Chávez's hometown of Sabaneta. Chávez and Colmenares remained married for 18 years, during which time they had three children: Rosa Virginia, María Gabriela, and Hugo Rafael, the latter of whom suffers from behavioural problems. The couple separated soon after Chávez's 1992 coup attempt. During his first marriage, Chávez had an affair with historian Herma Marksman; their relationship lasted nine years. Chávez's second wife was journalist Marisabel Rodríguez de Chávez, with whom he separated in 2002 and divorced in 2004. Through that marriage, Chávez had another daughter, Rosinés. Both María and Rosa provided Chávez with grandchildren. When Chávez was released from prison, he initiated in affairs with women that had been his followers. Allegations were also made that Chávez was a womanizer throughout both his marriages, having encounters with actresses, journalists, ministers, and minsters' daughters. The allegations remained unproven and are contradicted by statements provided by other figures close to him, though one retired aide shared that while Chávez was married to Marisabel and afterward, he participated in liaisons with women and gave them gifts, with some rumors among his aides stating that some of the women bore children of Chávez.
Those who were very close to Chávez felt that he had a bipolar disorder. Salvador Navarrete, a physician that treated Chávez during his first years in the presidency believed that Chávez was bipolar. In 2010, Alberto Müller Rojas, then vice president of Chávez's party, PSUV, stated that Chávez had "a tendency toward cyclothymia––mood swings that range from moments of extreme euphoria to moments of despondence". A different explanation was that such behavior was a tactic used by Chávez in order to attack opponents and polarize.
Chávez was a Catholic. He intended at one time to become a priest. He saw his socialist policies as having roots in the teachings of Jesus Christ, (liberation theology) and he publicly used the slogan of "Christ is with the Revolution!" Although he traditionally kept his own faith a private matter, Chávez over the course of his presidency became increasingly open to discussing his religious views, stating that he interpreted Jesus as a Communist. He was, in general, a liberal Catholic, some of whose declarations were disturbing to the religious community of his country. In 2008 he expressed his skepticism of an afterlife, saying that such idea was false. He also would declare his belief in Darwin's theory of evolution, stating that "it is a lie that God created man from the ground." Among other things, he cursed the state of Israel, and he had some disputes with both the Venezuelan Catholic clergy and Protestant groups like the New Tribes Mission, whose evangelical leader he "condemned to hell". In addition, he showed syncretistic practices such as the worship of the Venezuelan goddess María Lionza. In his last years, after he discovered he had cancer, Chávez became more attached to the Catholic Church.
Illness.
In June 2011, Chávez revealed in a televised address from Havana, Cuba, that he was recovering from an operation to remove an abscessed tumor with cancerous cells. Vice President Elías Jaua declared that the President remained in "full exercise" of power and that there was no need to transfer power due to his absence from the country. On 3 July, the Venezuelan government denied, however, that Chávez's tumour had been completely removed, further stating that he was heading for "complete recovery". On 17 July 2011, television news reported that Chávez had returned to Cuba for further cancer treatments.
Chávez gave a public appearance on 28 July 2011, his 57th birthday, in which he stated that his health troubles had led him to radically reorient his life towards a "more diverse, more reflective and multi-faceted" outlook, and he went on to call on the middle classes and the private sector to get more involved in his Bolivarian Revolution, something he saw as "vital" to its success. Soon after this speech, in August Chávez announced that his government would nationalize Venezuela's gold industry, taking it over from Russian-controlled company Rusoro, while at the same time also moving the country's gold stocks, which were largely stored in western banks, to banks in Venezuela's political allies like Russia, China and Brazil.
On 9 July 2012, Chávez declared himself fully recovered from cancer just three months before the 2012 Venezuelan presidential election, which he won, securing a fourth term as president. In November 2012, Chávez announced plans to travel to Cuba for more medical treatment for cancer.
On 8 December 2012, Chávez announced he would undergo a new operation after doctors in Cuba detected malignant cells; the operation took place on 11 December 2012. Chávez suffered a respiratory infection after undergoing the surgery but it was controlled. It was announced 20 December by the country's vice-president that Chávez had suffered complications following his surgery. It was announced on 3 January 2013 that Chávez had a severe lung infection that had caused respiratory failures following a strict treatment regimen for respiratory insufficiency. However he was reported to have overcome this later that month, and it was reported that he was then undergoing further treatment. On 18 February 2013, Chávez returned to Venezuela after 2 months of cancer treatment in Cuba. On 1 March 2013, Vice President Nicolás Maduro said that Chávez had been receiving chemotherapy in Venezuela following his surgery in Cuba. On 4 March, it was announced by the Venezuelan government that Chávez's breathing problems had worsened and he was suffering a new, severe respiratory infection.
Death.
On 5 March 2013, Vice President Nicolás Maduro announced on state television that Chávez had died in a military hospital in Caracas at 16:25 VET (20:55 UTC). The Vice President said Chávez died "after battling a tough illness for nearly two years." According to the head of Venezuela's presidential guard, Chávez died from a massive heart attack, and his cancer was very advanced when he died. Gen. Jose Ornella said that near the end of his life Chávez could not speak aloud, but mouthed his last words: ""Yo no quiero morir, por favor no me dejen morir"" (I don't want to die. Please don't let me die). Chávez is survived by four children and four grandchildren.
Suggestions of American foul play, implying that Chávez had been poisoned or infected with a cancer virus (arguing a plot reminiscent to the Yasser Arafat death controversy and the attempts against Fidel Castro), were strongly denied by the U.S. Department of State as "absurd".
His death triggered a constitutional requirement that a presidential election be called within 30 days. Chavez's Vice President, Maduro, was elected president on April 14, 2013.
Honours and awards.
Recognition.
The United States-based "Time" magazine included Hugo Chávez among their list of the world's 100 most influential people in 2005 and 2006, noting the spreading of his anti-globalization efforts and anti-US sentiment throughout Latin America. In a 2006 list compiled by the leftist British magazine "New Statesman", he was voted eleventh in the list of "Heroes of our time". In 2010 the magazine included Chávez in its annual "The World's 50 Most Influential Figures". His biographers Marcano and Tyszka believed that within only a few years of his presidency, he "had already earned his place in history as the president most loved and most despised by the Venezuelan people, the president who inspired the greatest zeal and the deepest revulsion at the same time."
In 2008 Chávez was awarded the "Blue Planet Award" by the Ethecon Foundation, one of the comparatively very few 'grass-root' foundations.
Honorary degrees.
Chávez was awarded the following honorary degrees:

</doc>
<doc id="48877" url="https://en.wikipedia.org/wiki?curid=48877" title="Code talker">
Code talker

Code talkers are people in the 20th century who used obscure languages as a means of secret communication during wartime. The term is now usually associated with the United States soldiers during the world wars who used their knowledge of Native American languages as a basis to transmit coded messages. In particular, there were approximately 400–500 Native Americans in the United States Marine Corps whose primary job was the transmission of secret tactical messages. Code talkers transmitted these messages over military telephone or radio communications nets using formal or informally developed codes built upon their native languages. Their service improved the speed of encryption of communications at both ends in front line operations during World War II.
The name "code talkers" is strongly associated with bilingual Navajo speakers specially recruited during World War II by the Marines to serve in their standard communications units in the Pacific Theater. Code talking, however, was pioneered by Cherokee and Choctaw Indians during World War I.
Other Native American code talkers were deployed by the United States Army during World War II, including Lakota, Meskwaki, and Comanche soldiers. Soldiers of Basque ancestry were also used for code talking by the U.S. Marines during World War II in areas where other Basque speakers were not expected to be operating.
Assiniboine code talkers.
Members of the Assiniboine served as code talkers during the war, utilizing the Assiniboine language to encrypt communications. The code talkers included Gilbert Horn Sr., who grew up in the Fort Belknap Indian Reservation of Montana and later became a tribal judge and politician.
Cherokee code talkers.
The first known use of Native Americans in the American military to transmit messages under fire was a group of Cherokee troops used by the American 30th Infantry Division serving alongside the British during the Second Battle of the Somme. According to the Division Signal Officer, this took place in September 1918. Their unit was under British command at the time.
Choctaw code talkers.
In the days of World War I, company commander Captain Lawrence of the U.S. Army overheard Solomon Louis and Mitchell Bobb conversing in the Choctaw language. He found eight Choctaw men in the battalion. Eventually, fourteen Choctaw men in the Army's 36th Infantry Division trained to use their language in code. They helped the American Expeditionary Forces win several key battles in the Meuse-Argonne Offensive in France, during the final large German push of the war. Within 24 hours of the Choctaw language being pressed into service, the tide of the battle had turned. In less than 72 hours, the Germans were retreating and the Allies were in full attack. These soldiers are now known as the Choctaw code talkers.
Comanche code talkers.
Adolf Hitler knew about the successful use of code talkers during World War I. He sent a team of some thirty anthropologists to the United States to learn Native American languages before the outbreak of World War II. It proved too difficult for them to learn the many languages and dialects that existed. Because of Nazi German anthropologists' efforts to learn the languages, the U.S. Army did not implement a large-scale code talker program in the European Theater of the war. Fourteen Comanche code talkers did take part in the Invasion of Normandy, and continued to serve in the 4th Infantry Division during additional European operations. Comanche of the 4th Signal Company compiled a vocabulary of over 100 code terms using words or phrases in their own language. Using a substitution method similar to the Navajo, the Comanche code word for tank was "turtle", bomber was "pregnant airplane", machine gun was "sewing machine" and Adolf Hitler was referred to as "crazy white man".
Two Comanche code-talkers were assigned to each regiment, the rest to 4th Infantry Division headquarters. Shortly after landing on Utah Beach on June 6, 1944, the Comanche began transmitting messages. Some were wounded but none killed.
In 1989, the French government awarded the Comanche code-talkers the Chevalier of the National Order of Merit. On November 30, 1999, the United States Department of Defense presented Charles Chibitty with the Knowlton Award.
Meskwaki code talkers.
Meskwaki men used their language against the Germans while fighting in the US Army in North Africa. Twenty-seven Meskwaki, then 16% of Iowa's Meskwaki population, enlisted in the U.S. Army together in January 1941.
Basque code talkers.
In May 1942, upon meeting about 60 U.S. Marines of Basque ancestry in a San Francisco camp, Captain Frank D. Carranza conceived the idea of using the Basque language for codes. His superiors were wary as there were known settlements of Basque in the Pacific region. There were 35 Basque Jesuits in Hiroshima, led by Pedro Arrupe. In China and the Philippines, there was a colony of Basque jai alai players, and there were Basque supporters of Falange in Asia. The American Basque code talkers were kept away from these theaters; they were initially used in tests and in transmitting logistic information for Hawaii and Australia.
On August 1, 1942, Lieutenants Nemesio Aguirre, Fernández Bakaicoa and Juanna received a Basque-coded message from San Diego for Admiral Chester Nimitz, warning him of the upcoming Operation Apple to remove the Japanese from the Solomon Islands. They also translated the start date, August 7, for the attack on Guadalcanal. As the war extended over the Pacific, there was a shortage of Basque speakers and the US military came to prefer the parallel program based on the use of Navajo speakers.
Navajo code talkers.
Philip Johnston, a civil engineer for the city of Los Angeles, proposed the use of Navajo to the United States Marine Corps at the beginning of World War II. Johnston, a World War I veteran, was raised on the Navajo reservation as the son of a missionary to the Navajo. He was one of the few non-Navajo who spoke the language fluently.
Because Navajo has a complex grammar, it is not nearly mutually intelligible enough with even its closest relatives within the Na-Dene family to provide meaningful information. It was still an unwritten language, and Johnston thought Navajo could satisfy the military requirement for an undecipherable code. Navajo was spoken only on the Navajo lands of the American Southwest. Its syntax and tonal qualities, not to mention dialects, made it unintelligible to anyone without extensive exposure and training. One estimate indicates that at the outbreak of World War II, fewer than 30 non-Navajo could understand the language.
Early in 1942, Johnston met with Major General Clayton B. Vogel, the commanding general of Amphibious Corps, Pacific Fleet, and his staff. Johnston staged tests under simulated combat conditions which demonstrated that Navajo men could encode, transmit, and decode a three-line English message in 20 seconds, versus the 30 minutes required by machines at that time. The idea was accepted, with Vogel recommending that the Marines recruit 200 Navajo. The first 29 Navajo recruits attended boot camp in May 1942. This first group created the Navajo code at Camp Pendleton, Oceanside, California.
The Navajo code was formally developed and modeled on the Joint Army/Navy Phonetic Alphabet that uses agreed-upon English words to represent letters. The Navajo Code Talkers were mainly Marines. As it was determined that phonetically spelling out all military terms letter by letter into words—while in combat—would be too time-consuming, some terms, concepts, tactics and instruments of modern warfare were given uniquely formal descriptive nomenclatures in Navajo (for example, the word for "shark" being used to refer to a destroyer, or "silver oak leaf" to the rank of lieutenant colonel). Several of these coinages, such as "gofasters" referring to running shoes or "ink sticks" for pens, entered Marine Corps vocabulary. They are commonly used today to refer to the appropriate objects.
A codebook was developed to teach the many relevant words and concepts to new initiates. The text was for classroom purposes only, and was never to be taken into the field. The code talkers memorized all these variations and practiced their rapid use under stressful conditions during training. Uninitiated Navajo speakers would have no idea what the code talkers' messages meant; they would hear only truncated and disjointed strings of individual, unrelated nouns and verbs.
The Navajo code talkers were commended for their skill, speed, and accuracy demonstrated throughout the war. At the Battle of Iwo Jima, Major Howard Connor, 5th Marine Division signal officer, had six Navajo code talkers working around the clock during the first two days of the battle. These six sent and received over 800 messages, all without error. Connor later stated, "Were it not for the Navajos, the Marines would never have taken Iwo Jima."
As the war progressed, additional code words were added on and incorporated program-wide. In other instances, informal short-cut code words were devised for a particular campaign and not disseminated beyond the area of operation. To ensure a consistent use of code terminologies throughout the Pacific Theater, representative code talkers of each of the U.S. Marine divisions met in Hawaii to discuss shortcomings in the code, incorporate new terms into the system, and update their codebooks. These representatives in turn trained other code talkers who could not attend the meeting. For example, the Navajo word for buzzard, , was used for bomber, while the code word used for submarine, , meant iron fish in Navajo. The last of the original 29 Navajo code talkers who developed the code, Chester Nez, died on June 4, 2014.
The deployment of the Navajo code talkers continued through the Korean War and after, until it was ended early in the Vietnam War. The Navajo code is the only spoken military code never to have been deciphered.
Seminole code talkers.
The last surviving Seminole code talker, Edmond Harjo of the Seminole Nation of Oklahoma, died on March 31, 2014, at the age of 96. Harjo had served as far afield as Normandy and the Battle of Iwo Jima during the war. His biography was recounted by Speaker of the United States House of Representatives John Boehner at the Congressional Gold Medal ceremony honoring Harjo and other code talkers at the U.S. Capitol on November 20, 2013.
Wenzhou code talkers.
China used Wenzhounese speaking people as code talkers during war.
Nubian code talkers.
In the 1973 Arab–Israeli War, Egypt employed Nubian-speaking Nubian people as codetalkers.
Cryptographic properties.
Non-speakers would find it extremely difficult to accurately distinguish unfamiliar sounds used in these languages. Additionally, a speaker who has acquired a language during their childhood sounds distinctly different from a person who acquired the same language in later life, thus reducing the chance of successful impostors sending false messages. Finally, the additional layer of an alphabet cypher was added to prevent interception by native speakers not trained as code talkers, in the event of their capture by the Japanese. A similar system employing Welsh was used by British forces, but not to any great extent during World War II. Welsh was used more recently in the Yugoslav Wars for non-vital messages.
Navajo was an attractive choice for code use because few people outside the Navajo had learned to speak the language. Virtually no books in Navajo had been published. Outside of the language, the Navajo spoken code was not very complex by cryptographic standards. It would likely have been broken if a native speaker and trained cryptographers could have worked together effectively. The Japanese had an opportunity to attempt this when they captured Joe Kieyoomia in the Philippines in 1942 during the Bataan Death March. Kieyoomia, a Navajo sergeant in the U.S. Army, but not a code talker, was ordered to interpret the radio messages later in the war. However, since Kieyoomia had not participated in the code training, the messages made no sense to him. When he reported that he could not understand the messages, his captors tortured him. The Japanese Imperial Army and Navy never cracked the spoken code.
Post-war recognition.
The Navajo code talkers received no recognition until the declassification of the operation in 1968. In 1982, the code talkers were given a Certificate of Recognition by U.S. President Ronald Reagan, who also named August 14, 1982 as "Navajo Code Talkers Day".
On December 21, 2000 the U.S. Congress passed, and President Bill Clinton signed, Public Law 106-554, 114 Statute 2763, which awarded the Congressional Gold Medal to the original twenty-nine World War II Navajo code talkers, and Silver Medals to each person who qualified as a Navajo code talker (approximately 300). In July 2001, U.S. President George W. Bush personally presented the Medal to four surviving original code talkers (the fifth living original code talker was not able to attend) at a ceremony held in the Capitol Rotunda in Washington, D.C. Gold medals were presented to the families of the deceased 24 original code talkers.
On September 17, 2007, 18 Choctaw code talkers were posthumously awarded the Texas Medal of Valor from the Adjutant General of the State of Texas for their World War II service.
On November 15, 2008, The Code Talkers Recognition Act of 2008 (Public Law 110-420), was signed into law by President George W. Bush, which recognizes every Native American code talker who served in the United States military during WWI or WWII (with the exception of the already-awarded Navajo) with a Congressional Gold Medal, designed as distinct for each tribe, with silver duplicates awarded to the individual code talkers or their next-of-kin. As of 2013, 33 tribes have been identified and were honored at a ceremony at Emancipation Hall at the U.S. Capitol Visitor Center. One surviving code talker was present, Edmond Harjo.

</doc>
<doc id="48878" url="https://en.wikipedia.org/wiki?curid=48878" title="Salmonidae">
Salmonidae

Salmonidae is a family of ray-finned fish, the only living family currently placed in the order Salmoniformes. It includes salmon, trout, chars, freshwater whitefishes, and graylings, which collectively are known as the salmonids. The Atlantic salmon and trout of the genus "Salmo" give the family and order their names.
Salmonids have a relatively primitive appearance among the teleost fish, with the pelvic fins being placed far back, and an adipose fin towards the rear of the back. They are slender fish, with rounded scales and forked tails. Their mouths contain a single row of sharp teeth. Although the smallest species is just long as an adult, most are much larger, with the largest reaching .
All salmonids spawn in fresh water, but in many cases, the fish spend most of their lives at sea, returning to the rivers only to reproduce. This lifecycle is described as anadromous. They are predators, feeding on small crustaceans, aquatic insects, and smaller fish.
Evolution.
Current salmonids comprise three lineages, taxonomically treated as subfamilies: whitefish (Coregoninae), graylings (Thymallinae), and the char, trout, and salmons (Salmoninae). Generally, all three lineages are accepted to share a suite of derived traits indicating a monophyletic group.
The Salmonidae first appear in the fossil record in the middle Eocene with the fossil "Eosalmo driftwoodensis", which was first described from fossils found at Driftwood Creek, central British Columbia. This genus shares traits found in the Salmoninae, whitefish, and grayling lineages. Hence, "E. driftwoodensis" is an archaic salmonid, representing an important stage in salmonid evolution.
A gap appears in the salmonine fossil record after "E. driftwoodensis" until about seven million years ago (mya), in the late Miocene, when trout-like fossils appear in Idaho, in the Clarkia Lake beds. Several of these species appear to be "Oncorhynchus"—the current genus for Pacific salmon and some trout. The presence of these species so far inland established that "Oncorhynchus" was not only present in the Pacific drainages before the beginning of the Pliocene (~5–6 mya), but also that rainbow and cutthroat trout, and Pacific salmon lineages had diverged before the beginning of the Pliocene. Consequently, the split between "Oncorhynchus" and "Salmo" (Atlantic salmon) must have occurred well before the Pliocene. Suggestions have gone back as far as the early Miocene (about 20 mya).
Classification.
Together with the closely related Esociformes (the pikes and related fishes), Osmeriformes (e.g. smelts), and Argentiniformes, the Salmoniformes comprise the superorder Protacanthopterygii.
The Salmonidae are divided into three subfamilies and around 10 genera. The concepts of the number of species recognised vary among researchres and authorities; the numbers presented below represent the higher estimates of diversity:
Order Salmoniformes
Hybrid crossbreeding.
The following table shows results of hybrid crossbreeding combination in Salmonidae.
note :- : The identical kind, O : (survivability) , X : (Fatality)

</doc>
<doc id="48881" url="https://en.wikipedia.org/wiki?curid=48881" title="Esociformes">
Esociformes

The Esociformes are a small order of ray-finned fish, with two families, the Umbridae (mudminnows) and the Esocidae (pikes). The pikes of genus "Esox" give the order its name. The 10 species are found as five in each family. 
This order is closely related to the Salmoniformes, the two comprising the superorder Protacanthopterygii, and are often included in their order. The esociform fishes first appeared in the mid-Cretaceous — early products of the Euteleost radiation of that time. Today, they are found only in fresh water in North America and the northern Eurasian regions.
Esocidae.
The pikes tend to be lie-in-wait, ambush predators, with elongated snouts, long, well-muscled torsos, forked tails, and dorsal and anal fins set well back and opposite each other for rapid acceleration along a straight line, allowing the fish to quickly emerge from cover to capture their prey. Prey capture is facilitated by the impaling of the prey animal on the sharp teeth, after which the pike retreats to cover, turns the prey around, and swallows it, head first. 
Anatomically, the pikes are characterized by the presence of shark-like, maxillary teeth, a mesocoracoid, and the absence of an adipose fin, breeding tubercules, and pyloric cecae.
The two more prominent species of Esocidae are "Esox lucius", the northern pike, a popular sport fish that may reach lengths as great 1.5 m (4.6 ft), and the muskellunge or "muskie", "E. masquinongy", which grows even larger.
Umbridae.
Mudminnows are much smaller than the related pikes, with usual lengths of less than 20 cm. However, they are also extremely efficient, lie-in-wait, ambush predators, feeding mostly on the invertebrates commonly found in warm backwaters. Of the three North American species of genus "Umbra", one, "U. limi", possesses a limited capability for air breathing. "Umbra" spp. are most commonly found in the Atlantic coast regions of North America, along the marshy, low-oxygen areas of the Mississippi River, and in similar environments in Europe.

</doc>
<doc id="48884" url="https://en.wikipedia.org/wiki?curid=48884" title="Sculpin">
Sculpin

A sculpin is a type of fish that belongs to the superfamily Cottoidea in the order Scorpaeniformes. As of 2006, this superfamily contains 11 families, 149 genera, and 756 species.
Sculpins occur in many types of habitat, including ocean and freshwater zones. They live in rivers, submarine canyons, kelp forests, and shallow littoral habitat types, such as tidepools.
Sculpins are benthic fish, dwelling on the bottoms of water bodies. Their pectoral fins are smooth on the upper edge and webbed with sharp rays along the lower edge, a modification that makes them specialized for gripping the substrate. This adaptation helps the fish anchor in fast-flowing water.
Families.
Families include:

</doc>
<doc id="48885" url="https://en.wikipedia.org/wiki?curid=48885" title="Scorpaeniformes">
Scorpaeniformes

The Scorpaeniformes are a diverse order of ray-finned fish, but have also been called the Scleroparei. It is one of the five largest orders of bony fishes by number of species with over 1,320.
They are known as "mail-cheeked" fishes due to their distinguishing characteristic, the suborbital stay: a backwards extension of the third circumorbital bone (part of the lateral head/cheek skeleton, below the eye socket) across the cheek to the preoperculum, to which it is connected in most species.
Scorpaeniform fishes are carnivorous, mostly feeding on crustaceans and on smaller fish. Most species live on the sea bottom in relatively shallow waters, although species are known from deep water, from the midwater, and even from fresh water. They typically have spiny heads, and rounded pectoral and caudal fins. Most species are less than in length, but the full size range of the order varies from the velvetfishes, which can be just long as adults, to the lingcod, which can reach in length.
Classification.
The division of Scorpaeniformes into families is not settled; accounts range from 26 to 35 families.
Order Scorpaeniformes

</doc>
<doc id="48891" url="https://en.wikipedia.org/wiki?curid=48891" title="Joe DiMaggio">
Joe DiMaggio

Joseph Paul DiMaggio (November 25, 1914 – March 8, 1999), nicknamed "Joltin' Joe" and "the Yankee Clipper", was an American Major League Baseball center fielder who played his entire 13-year career for the New York Yankees. He is perhaps best known for his 56-game hitting streak (May 15 – July 16, 1941), a record that still stands.
DiMaggio was a three-time MVP winner and an All-Star in each of his 13 seasons. During his tenure with the Yankees, the club won ten American League pennants and nine World Series championships.
At the time of his retirement, he ranked fifth in career home runs (361) and sixth in career slugging percentage (.579). He was inducted into the Baseball Hall of Fame in 1955, and was voted the sport's greatest living player in a poll taken during the baseball centennial year of 1969.
His brothers Vince (1912–1986) and Dom (1917–2009) also were major league center fielders.
Early life.
DiMaggio was born on November 25, 1914 in Martinez, California, the eighth of nine children born to Sicilian immigrants Giuseppe (1872–1949) and Rosalia (Mercurio) DiMaggio (1878–1951). He was delivered by a midwife identified on his birth certificate as Mrs. J. Pico. He was named after his father; "Paolo" was in honor of Giuseppe's favorite saint, Saint Paul. The family moved to nearby San Francisco when Joe was a year old.
Giuseppe was a fisherman, as were generations of DiMaggios before him. According to statements from Joe's brother Tom to biographer Maury Allen, Rosalia's father wrote to her with the advice that Giuseppe could earn a better living in California than in their native Isola delle Femmine, a northwestern Sicilian village in the province of Palermo.
After being processed on Ellis Island, Giuseppe worked his way across America, eventually settling near Rosalia's father in Pittsburg, California on the east side of the San Francisco Bay Area. After four years, he earned enough money to send to Italy for Rosalia and their daughter, who was born after he had left for the United States.
Giuseppe hoped that his five sons would become fishermen. DiMaggio recalled that he would do anything to get out of cleaning his father's boat, as the smell of dead fish nauseated him. Giuseppe called him "lazy" and "good for nothing".
DiMaggio was playing semi-pro ball when older brother Vince DiMaggio, playing for the San Francisco Seals of the Pacific Coast League (PCL), talked his manager into letting DiMaggio fill in at shortstop. Joe DiMaggio made his professional debut on October 1, 1932. From May 27 to July 25, 1933, he hit safely in 61 consecutive games, a PCL-record. "Baseball didn't really get into my blood until I knocked off that hitting streak", he said. "Getting a daily hit became more important to me than eating, drinking or sleeping."
In 1934, DiMaggio suffered a career-threatening knee injury when he tore ligaments while stepping out of a jitney. Scout Bill Essick of the New York Yankees, convinced that the injury would heal, pestered his club to give him another look. After DiMaggio passed a physical examination in November, the Yankees purchased his contract for $50,000 and five players. He remained with the Seals for the 1935 season and batted .398 with 154 runs batted in (RBIs) and 34 home runs. His team won the 1935 PCL title, and DiMaggio was named the league's Most Valuable Player.
Major league career.
DiMaggio made his major league debut on May 3, 1936, batting ahead of Lou Gehrig. The Yankees had not been to the World Series since 1932, but they won the next four Fall Classics. In total, DiMaggio led the Yankees to nine titles in 13 years.
In 1939, DiMaggio was nicknamed the "Yankee Clipper" by Yankee's stadium announcer Arch McDonald, when he likened DiMaggio's speed and range in the outfield to the then-new Pan American airliner.
DiMaggio was pictured with his son on the cover of the inaugural issue of "SPORT" magazine in September 1946.
In 1947, Boston Red Sox owner Tom Yawkey and Yankees GM Larry MacPhail verbally agreed to trade DiMaggio for Ted Williams, but MacPhail refused to include Yogi Berra.
In the September 1949 issue of "SPORT", Hank Greenberg said that DiMaggio covered so much ground in center field that the only way to get a hit against the Yankees was "to hit 'em where Joe wasn't." DiMaggio also stole home five times in his career.
On February 7, 1949, DiMaggio signed a record contract worth $100,000 ($ in current dollar terms) ($70,000 plus bonuses), and became the first baseball player to break $100,000 in earnings. By 1950, he was ranked the second-best center fielder by the "Sporting News", after Larry Doby. After a poor 1951 season, a scouting report by the Brooklyn Dodgers that was turned over to the New York Giants and leaked to the press, and various injuries, DiMaggio announced his retirement at age 37 on December 11, 1951. When remarking on his retirement to the "Sporting News" on December 19, 1951, he said:
I feel like I have reached the stage where I can no longer produce for my club, my manager, and my teammates. I had a poor year, but even if I had hit .350, this would have been my last year. I was full of aches and pains and it had become a chore for me to play. When baseball is no longer fun, it's no longer a game, and so, I've played my last game.
Through May 2009, DiMaggio was tied with Mark McGwire for third place all-time in home runs over the first two calendar years in the major leagues (77), behind Phillies Hall of Famer Chuck Klein (83), and Milwaukee Brewers' Ryan Braun (79). Through 2011, he was one of seven major leaguers to have had at least four 30-homer, 100-RBI seasons in their first five years, along with Chuck Klein, Ted Williams, Ralph Kiner, Mark Teixeira, Albert Pujols, and Ryan Braun. DiMaggio holds the record for most seasons with more home runs than strikeouts (minimum 20 home runs), a feat he accomplished seven times, and five times consecutively from 1937–1941. DiMaggio would likely have exceeded 500 home runs and 2,000 RBIs had he not served in the military.
DiMaggio might have had better power-hitting statistics had his home park not been Yankee Stadium. As "The House That Ruth Built", its nearby right field favored the Babe's left-handed power. For right-handed hitters, its deep left and center fields made home runs almost impossible. Mickey Mantle recalled that he and Whitey Ford witnessed many DiMaggio blasts that would have been home runs anywhere other than Yankee Stadium (Ruth himself fell victim to that problem, as he also hit many long flyouts to center). Bill James calculated that DiMaggio lost more home runs due to his home park than any other player in history. Left-center field went as far back as 457 ft where left-center rarely reaches 380 ft [116 m in today's ballparks. Al Gionfriddo's famous catch in the 1947 World Series, which was close to the 415-foot mark [126 m] in left-center, would have been a home run in the Yankees' current ballpark. DiMaggio hit 148 home runs in 3,360 at-bats at home versus 213 home runs in 3,461 at-bats on the road. His slugging percentage at home was .546, and on the road, it was .610. Expert statistician Bill Jenkinson made a statement on these statistics:
For example, Joe DiMaggio was acutely handicapped by playing at Yankee Stadium. Every time he batted in his home field during his entire career, he did so knowing that it was physically impossible for him to hit a home run to the half of the field directly in front of him. If you look at a baseball field from foul line to foul line, it has a 90-degree radius. From the power alley in left center field (430 in Joe's time) to the fence in deep right center field (407 ft), it is 45-degrees. And Joe DiMaggio never hit a single home run over the fences at Yankee Stadium in that 45-degree graveyard. It was just too far. Joe was plenty strong; he routinely hit balls in the 425-foot range. But that just wasn't good enough in cavernous Yankee Stadium. Like Ruth, he benefited from a few easy homers each season due to the short foul line distances. But he lost many more than he gained by constantly hitting long fly outs toward center field. Whereas most sluggers perform better on their home fields, DiMaggio hit only 41 percent of his career home runs in the Bronx. He hit 148 homers at Yankee Stadium. If he had hit the same exact pattern of batted balls with a typical modern stadium as his home, he would have belted about 225 homers during his home field career.
Joe became eligible for the Baseball Hall of Fame in 1953 but he was not elected until 1955. The Hall of Fame rules on the post-retirement induction waiting period had been revised in the interim, extending the waiting period from one to five years, but DiMaggio and Ted Lyons were exempted from the rule. DiMaggio told "Baseball Digest" in 1963 that the Brooklyn Dodgers had offered him their managerial job in 1953, but he turned it down. After being out of baseball since his retirement as a player, Joe became the first hitting instructor of the newly relocated Oakland Athletics from 1968 to 1970.
Hitting streak.
DiMaggio's most famous achievement is his MLB record-breaking 56-game hitting streak in 1941. The streak began on May 15, 1941, a couple of weeks before the death of Lou Gehrig, when DiMaggio went one-for-four against Chicago White Sox pitcher Eddie Smith. Major newspapers began to write about DiMaggio's streak early on, but as he approached George Sisler's modern era record of 41 games, it became a national phenomenon. Initially, DiMaggio showed little interest in breaking Sisler's record, saying "I'm not thinking a whole lot about it... I'll either break it or I won't." As he approached Sisler's record, DiMaggio showed more interest, saying, "At the start I didn't think much about it... but naturally I'd like to get the record since I am this close." On June 29, 1941, DiMaggio doubled in the first game of a doubleheader against the Washington Senators at Griffith Stadium to tie Sisler's record, and then singled in the nightcap to extend his streak to 42.
A Yankee Stadium crowd of 52,832 fans watched DiMaggio tie the all-time hitting streak record (44 games, Wee Willie Keeler in 1897) on July 1. The next day against the Boston Red Sox, he homered into Yankee Stadium's left field stands to extend his streak to 45, setting a new record. DiMaggio recorded 67 hits in 179 at-bats during the first 45 games of his streak, while Keeler recorded 88 hits in 201 at-bats. DiMaggio continued hitting after breaking Keeler's record, reaching 50 straight games on July 11 against the St. Louis Browns. On July 17 at Cleveland Stadium, DiMaggio's streak was finally snapped at 56 games, thanks in part to two backhand stops by Indians third baseman Ken Keltner. DiMaggio batted .408 during the streak, with 15 home runs and 55 RBI. The day after the streak ended, DiMaggio started another streak that lasted 16 games. The distinction of hitting safely in 72 of 73 games is also a record. The closest anyone has come to equaling DiMaggio since 1941 is Pete Rose, who hit safely in 44 straight games in 1978. During the streak, DiMaggio played in seven doubleheaders. The Yankees' record during the streak was 41–13–2.
Some consider DiMaggio's streak a uniquely outstanding and unbreakable record, and a statistical near-impossibility. Nobel Prize-winning physicist and sabermetrician Edward Mills Purcell calculated that, to have the likelihood of a hitting streak of 50 games occurring in the history of baseball up to the late 1980s be greater than 50%, fifty-two .350 lifetime hitters would have to have existed instead of the actual three (Ty Cobb, Rogers Hornsby, and Shoeless Joe Jackson). His Harvard colleague Stephen Jay Gould, citing Purcell's work, called DiMaggio's 56-game achievement "the most extraordinary thing that ever happened in American sports". Samuel Arbesman and Steven Strogatz of Cornell University disagree; they conducted 10,000 computer simulations of Major League Baseball from 1871 to 2005, 42% of which produced streaks as long or longer, with record streaks ranging from 39 to 109 games and typical record streaks between 50 and 64 games.
Wartime.
DiMaggio enlisted in the United States Army Air Forces on February 17, 1943, rising to the rank of sergeant. He was stationed at Santa Ana, California, Hawaii, and Atlantic City, New Jersey, as a physical education instructor. He was released on medical discharge in September 1945, due to chronic stomach ulcers. Other than now being paid $21 a month, DiMaggio's service was as comfortable as a soldier's life could be. He spent most of his career playing for baseball teams and in exhibition games against fellow Major Leaguers and minor league players, and superiors gave him special privileges due to his prewar fame. DiMaggio ate so well from an athlete-only diet that he gained 10 pounds, and while in Hawaii he and other players mostly tanned on the beach and drank. Embarrassed by his lifestyle, DiMaggio demanded combat duty in 1943, but was turned down.
Parents as "enemy aliens".
Giuseppe and Rosalia DiMaggio were among the thousands of German, Japanese and Italian immigrants classified as "enemy aliens" by the government after Pearl Harbor was bombed by Japan. They carried photo ID booklets at all times, and were not allowed to travel outside a five-mile radius from their home without a permit. Giuseppe was barred from the San Francisco Bay, where he had fished for decades, and his boat was seized. Rosalia became an American citizen in 1944, followed by Giuseppe in 1945.
Married life.
Dorothy Arnold.
In January 1937, DiMaggio met actress Dorothy Arnold on the set of "Manhattan Merry-Go-Round", in which he had a minor role and she was an extra. They married at San Francisco's St. Peter and Paul Church on November 19, 1939, as 20,000 well-wishers jammed the streets. Their son, Joseph Paul DiMaggio III, was born at Doctors Hospital on October 23, 1941. The couple divorced in 1944.
Marilyn Monroe.
According to her autobiography, Marilyn Monroe originally did not want to meet DiMaggio, fearing that he was a stereotypical arrogant athlete. They eloped at San Francisco City Hall on January 14, 1954.
An incident between the couple is supposed to have occurred immediately after the skirt-blowing scene in "The Seven Year Itch" which was filmed on September 14, 1954, in front of Manhattan's Trans-Lux 52nd Street Theater. Then-20th Century Fox's East Coast correspondent Bill Kobrin told the "Palm Springs Desert Sun" that it was director Billy Wilder's idea to turn the shoot into a media circus. The couple then had a "yelling battle" in the theater lobby. A month later, she contracted the services of celebrity attorney Jerry Giesler and filed for divorce on grounds of mental cruelty 274 days after the wedding. After the failure of their marriage, DiMaggio had undergone therapy, stopped drinking alcohol and expanded his interests beyond baseball: he and Monroe read poetry together in their later years.
On August 1, 1956, an International News wire photo of DiMaggio with Lee Meriwether speculated that the couple was engaged, but Cramer wrote that it was a rumor started by Walter Winchell. Monroe biographer Donald Spoto claimed that DiMaggio was "very close to marrying" 1957 Miss America Marian McKnight, who won the crown with a Marilyn Monroe act, but McKnight denied it. He was also linked to Liz Renay, Cleo Moore, Rita Gam, Marlene Dietrich, and Gloria DeHaven during this period, and to Elizabeth Ray and Morgan Fairchild years later, but he never publicly confirmed any involvement with any woman.
DiMaggio re-entered Monroe's life as her marriage to Arthur Miller was ending. On February 10, 1961, he secured her release from Payne Whitney Psychiatric Clinic. She joined him in Florida where he was a batting coach for the Yankees. Their "just friends" claim did not stop remarriage rumors from flying. Reporters staked out her Manhattan apartment building. Bob Hope "dedicated" Best Song nominee "The Second Time Around" to them at the 33rd Academy Awards.
According to Maury Allen's biography, DiMaggio was alarmed at how Monroe had fallen in with people he felt were detrimental to her well-being. Val Monette, owner of a military post-exchange supply company, told Allen that DiMaggio left his employ on August 1, 1962, because he had decided to ask Monroe to remarry him.
She was found dead in her Brentwood, Los Angeles, home on August 5 after housekeeper Eunice Murray telephoned Monroe's psychiatrist, Dr. Ralph Greenson. DiMaggio's son had spoken to Monroe on the phone the night of her death and claimed that she seemed fine. Her death was deemed a probable suicide by "Coroner to the Stars" Thomas Noguchi, but has been the subject of numerous conspiracy theories.
Devastated, DiMaggio claimed her body and arranged for her funeral at Westwood Village Memorial Park Cemetery, barring Hollywood's elite. He had a half-dozen red roses delivered three times a week to her crypt for 20 years. He refused to talk about her publicly or otherwise exploit their relationship. He never married again. When he died in 1999, his last words were "I'll finally get to see Marilyn."
Advertising.
In the 1970s, DiMaggio became a spokesman for Mr. Coffee and would be the face of the electric, drip coffee makers for over 20 years. Vincent Marotta, the CEO of North American Systems, which manufactured Mr. Coffee at the time, recruited DiMaggio for the advertising campaign. DiMaggio's spots proved successful with consumers. In a 2007 interview with the "Columbus Dispatch", Marotta joked that "millions of kids grew up thinking Joe DiMaggio was a famous appliance salesman." Despite his commercials for Mr. Coffee, Joe DiMaggio rarely drank coffee due to ulcers. However, when he did drink coffee, DiMaggio preferred Sanka instant coffee, rather than the coffee brewed by Mr. Coffee machines.
In 1972, DiMaggio became a spokesman for The Bowery Savings Bank. With the exception of a five-year hiatus in the 1980s, DiMaggio regularly made commercials for the financial institution until 1992.
Death.
DiMaggio, a heavy smoker for much of his adult life, was admitted to Memorial Regional Hospital in Hollywood, Florida, on October 12, 1998, for lung cancer surgery, and remained there for 99 days. He returned to his Hollywood, Florida home on January 19, 1999, where he died on March 8.
DiMaggio's funeral was held on March 11, 1999, at Sts. Peter and Paul Roman Catholic Church in San Francisco. DiMaggio's son died the following August at age 57. DiMaggio is interred at Holy Cross Cemetery in Colma, California.
Sports legacy.
At his death, "The New York Times" called DiMaggio's 1941 56-game hitting streak "perhaps the most enduring record in sports".
In an article in 1976 in "Esquire" magazine, sportswriter Harry Stein published an "All Time All-Star Argument Starter", consisting of five ethnic baseball teams. Joe DiMaggio was the center fielder on Stein's Italian team.
On September 17, 1992, the doors were opened at Joe DiMaggio Children's Hospital at Memorial Regional Hospital in Hollywood, Florida, for which he raised over $4,000,000.
On April 13, 1998, DiMaggio was given the Sports Legend Award at the 13th annual American Sportscasters Association Hall of Fame Awards Dinner in New York City. Dr. Henry Kissinger, former Secretary of State and a longtime fan of DiMaggio's, made the presentation to the Yankee great. The event was one of DiMaggio's last public appearances before taking ill.
Yankee Stadium's fifth monument was dedicated to DiMaggio on April 25, 1999, and the West Side Highway was officially renamed in his honor. The Yankees wore DiMaggio's number 5 on the left sleeves of their uniforms for the 1999 season. He is ranked No. 11 on "The Sporting News"' list of the 100 Greatest Baseball Players, and he was elected by fans to the Major League Baseball All-Century Team. In addition to his number 5 being retired by the New York Yankees, DiMaggio's number was also retired by the Florida Marlins, who retired it in honor of their first team president, Carl Barger, who died five months before the team took the field for the first time in 1993. DiMaggio had been his favorite player.
An auction of DiMaggio's personal items was held by the adopted daughters of DiMaggio's son in May 2006. Highlights included the ball hit to break Wee Willie Keeler's hitting-streak record ($63,250); his 2,000th career hit ball ($29,900); his 1947 Most Valuable Player Award ($281,750); the uniform worn in the 1951 World Series ($195,500); his Hall of Fame ring ($69,000); a photograph Marilyn autographed "I love you Joe" ($80,500); her passport ($115,000); and their marriage certificate ($23,000). Lot 758, DiMaggio's white 1991 Mercedes 420 SEL sedan, which was a gift from the New York Yankees commemorating the 50th anniversary of DiMaggio's 1941 season, sold for $18,000. The event netted a total of $4.1 million.
On August 8, 2011, the United States Postal Service announced that DiMaggio would appear on a stamp for the first time. It was issued as part of the "Major League Baseball All-Star Stamp Series" which came out in July 2012.
In popular culture.
DiMaggio's popularity during his career was such that he was referenced in film, television, literature, art, and music both during his career and decades after he retired.

</doc>
<doc id="48892" url="https://en.wikipedia.org/wiki?curid=48892" title="Dandenong Ranges National Park">
Dandenong Ranges National Park

The Dandenong Ranges National Park is a national park located in the Greater Melbourne region of Victoria, Australia. The national park is situated from at its western most points at Ferntree Gully and Boronia to at it easternmost point at Silvan, east of the Melbourne city centre.
The park was proclaimed on , amalgamating the Ferntree Gully National Park, Sherbrooke Forest and Doongalla Estate. In 1997 the Olinda State Forest, Mt. Evelyn and Montrose Reserve were formally added to the national park.
Features.
Dandenong Ranges National Park is divided into five sections:
Feral pests.
Because the park is located in an urban area, the park has a long history of major problems with feral and roaming animals. A cat-curfew was introduced in the entire Dandenong Ranges area some years ago, and since then the numbers and variety of lyrebirds and other native species have climbed dramatically.

</doc>
<doc id="48894" url="https://en.wikipedia.org/wiki?curid=48894" title="Hooke">
Hooke

Hooke may refer to:

</doc>
<doc id="48896" url="https://en.wikipedia.org/wiki?curid=48896" title="Lyrebird">
Lyrebird

A lyrebird is either of two species of ground-dwelling Australian birds, that form the genus, Menura, and the family Menuridae. They are most notable for their superb ability to mimic natural and artificial sounds from their environment. As well as their extraordinary mimicking ability, lyrebirds are notable because of the striking beauty of the male bird's huge tail when it is fanned out in display; and also because of their courtship display. Lyrebirds have unique plumes of neutral-coloured tailfeathers and are among Australia's best-known native birds.
Taxonomy and systematics.
The classification of lyrebirds was the subject of much debate after the first specimens reached European scientists after 1798. The superb lyrebird was first illustrated and described scientifically as "Menura superba" by Major-General Thomas Davies in 1800 to the Linnean Society of London. He based his work on specimens sent from New South Wales to England.
Lyrebirds were thought to be Galliformes like the broadly similar looking partridge, junglefowl, and pheasants that Europeans were familiar with, and this was reflected in the early names the superb lyrebird had, including native pheasant. They were also called peacock-wrens and Australian birds-of-paradise. The idea that they were related to the pheasants was abandoned when the first chicks, which are altricial, were described. They were not placed with the passerines until a paper was published in 1840, 12 years after they were first placed in their own family, Menuridae. Within that family they are placed in a single genus, "Menura".
It is generally accepted that the lyrebird family is most closely related to the scrub-birds (Atrichornithidae) and some authorities combine both in a single family, but evidence that they are also related to the bowerbirds remains controversial.
Lyrebirds are ancient Australian animals: the Australian Museum has fossils of lyrebirds dating back to about 15 million years ago. The prehistoric "Menura tyawanoides" has been described from Early Miocene fossils found at the famous Riversleigh site.
Species.
There are two extant species of lyrebird:
Description.
The lyrebirds are large passerine birds, amongst the largest in the order. They are ground living birds with strong legs and feet and short rounded wings. They are generally poor fliers and rarely take to the air except for periods of downhill gliding. The superb lyrebird is the larger of the two species. Females are 74–84 cm long, and the males are a larger 80–98 cm long—making them the third-largest passerine bird after the thick-billed raven and the common raven. Albert's lyrebird is slightly smaller at a maximum of 90 cm (male) and 84 cm (female) (around 30–35 inches) They have smaller, less spectacular lyrate feathers than the superb lyrebird, but are otherwise similar.
Distribution and habitat.
The superb lyrebird is found in areas of rainforest in Victoria, New South Wales and south-east Queensland, as well as in Tasmania where it was introduced in the 19th century. Many superb lyrebirds live in the Dandenong Ranges National Park and Kinglake National Park around Melbourne, the Royal National Park and Illawarra region south of Sydney and in many other parks along the east coast of Australia as well as non protected bushland. Albert's lyrebird is found in only a very small area of Southern Queensland rainforest.
Behaviour and ecology.
Lyrebirds are shy and difficult to approach, particularly the Albert's lyrebird, which means that there is little information about its behaviour. When lyrebirds detect potential danger they will pause and scan their surroundings, then give an alarm call. Having done so, they will either flee the vicinity on foot, or seek cover and freeze. Also, firefighters sheltering in mine shafts during bushfires have been joined by lyrebirds.
Diet and feeding.
Lyrebirds feed on the ground and as individuals. A range of invertebrate prey is taken, including insects such as cockroaches, beetles (both adults and larvae), earwigs, fly larvae, and the adults and larvae of moths. Other prey taken includes centipedes, spiders, earthworms. Less commonly taken prey includes stick insects, bugs, amphipods, lizards, frogs and occasionally, seeds. They find food by scratching with their feet through the leaf-litter.
Breeding.
The breeding cycle of the lyrebirds is long, and lyrebirds are long-lived birds, capable of living up to thirty years. They also start breeding later in life than other passerine birds. Female superb lyrebirds start breeding at the age of five or six, and males at the age of six to eight. Males defend territories from other males, and these territories may contain the breeding territories of up to eight females. Within the male territories, the males create or use display platforms; in the case of the superb lyrebird, it is a mound of bare soil; in the Albert's lyrebird it is simply a pile of twigs on the forest floor.
Male lyrebirds call mostly during winter, when they construct and maintain an open arena-mound in dense bush, on which they sing and dance in courtship, to display to potential mates, of which the male lyrebird has several. The female builds an untidy nest, usually low to the ground in a moist gully, where she lays a single egg. She is the sole parent who incubates the egg over 50 days until it hatches, and she is also the sole carer of the lyrebird chick.
Vocalizations and mimicry.
A lyrebird's song is one of the more distinctive aspects of its behavioural biology. Lyrebirds sing throughout the year, but the peak of the breeding season, from June to August, is when they sing with the most intensity. During this peak they may sing for four hours of the day, almost half the hours of daylight. The song of the superb lyrebird is a mixture of seven elements of its own song and any number of other mimicked songs and noises. The lyrebird's syrinx is the most complexly-muscled of the Passerines (songbirds), giving the lyrebird extraordinary ability, unmatched in vocal repertoire and mimicry. Lyrebirds render with great fidelity the individual songs of other birds and the chatter of flocks of birds, and also mimic other animals such as koalas and dingos. The lyrebird is capable of imitating almost any sound and they have been recorded mimicking human sounds such as a mill whistle, a cross-cut saw, chainsaws, car engines and car alarms, fire alarms, rifle-shots, camera shutters, dogs barking, crying babies, music, mobile phone ring tones, and even the human voice. However, while the mimicry of human noises is widely reported, the extent to which it happens is exaggerated and the phenomenon is quite unusual.
The superb lyrebird's mimicked calls are learned from the local environment, including from other superb lyrebirds. An instructive example of this is the population of superb lyrebirds in Tasmania, which have retained the calls of species not native to Tasmania in their repertoire, but have also added some local Tasmanian endemic bird noises. It takes young birds about a year to perfect their mimicked repertoire. The female lyrebirds of both species are also mimics, and will sing on occasion but the females do so with less skill than the males. A recording of a superb lyrebird mimicking sounds of an electronic shooting game, workmen and chainsaws was added to the National Film and Sound Archive's Sounds of Australia registry in 2013.
One researcher, Sydney Curtis, has recorded flute-like lyrebird calls in the vicinity of the New England National Park. Similarly, in 1969, a park ranger, Neville Fenton, recorded a lyrebird song which resembled flute sounds in the New England National Park, near Dorrigo in northern coastal New South Wales. After much detective work by Fenton, it was discovered that in the 1930s, a flute player living on a farm adjoining the park used to play tunes near his pet lyrebird. The lyrebird adopted the tunes into his repertoire, and retained them after release into the park. Neville Fenton forwarded a tape of his recording to Norman Robinson. Because a lyrebird is able to carry two tunes at the same time, Robinson filtered out one of the tunes and put it on the phonograph for the purposes of analysis. The song represents a modified version of two popular tunes in the 1930s: ""The Keel Row"" and ""Mosquito's Dance"". Musicologist David Rothenberg has endorsed this information.
Status and conservation.
Lyrebirds are not endangered in the short to medium term. Albert's lyrebird has a very restricted habitat and had been listed as vulnerable by the IUCN, but due to careful management of the species and its habitat the species was re-assessed to near threatened in 2009. The superb lyrebird, once seriously threatened by habitat destruction, is now classified as common. Even so, lyrebirds are vulnerable to cats and foxes, and it remains to be seen if habitat protection schemes will stand up to increased human population pressure.
Lyrebird emblems and logos.
The lyrebird has been featured as a symbol and emblem many times, especially in New South Wales and Victoria (where the superb lyrebird has its natural habitat), and in Queensland (where Albert's lyrebird has its natural habitat).
Painting by John Gould.
The lyrebird is so called because the male bird has a spectacular tail, consisting of 16 highly modified feathers (two long slender "lyrates" at the centre of the plume, two broader "medians" on the outside edges and twelve "filamentaries" arrayed between them), which was originally thought to resemble a lyre. This happened when a superb lyrebird specimen (which had been taken from Australia to England during the early 19th century) was prepared for display at the British Museum by a taxidermist who had never seen a live lyrebird. The taxidermist mistakenly thought that the tail would resemble a lyre, and that the tail would be held in a similar way to that of a peacock during courtship display, and so he arranged the feathers in this way. Later, John Gould (who had also never seen a live lyrebird), painted the lyrebird from the British Museum specimen.
Although very beautiful, the male lyrebird's tail is not held as in John Gould's painting. Instead, the male lyrebird's tail is fanned over the lyrebird during courtship display, with the tail completely covering his head and back—as can be seen in the image in the 'breeding' section of this page, and also the image of the 10 cent coin, where the superb lyrebird's tail (in courtship display) is portrayed accurately.

</doc>
<doc id="48898" url="https://en.wikipedia.org/wiki?curid=48898" title="SIMSCRIPT II.5">
SIMSCRIPT II.5

SIMSCRIPT II.5 is the latest incarnation of SIMSCRIPT, one of the oldest computer simulation languages. Although military contractor CACI released it in 1971, it still enjoys wide use in large-scale military and air-traffic control simulations.

</doc>
<doc id="48899" url="https://en.wikipedia.org/wiki?curid=48899" title="Simulation language">
Simulation language

A computer simulation language describes the operation of a simulation on a computer. There are two major types of simulation: continuous and discrete event though more modern languages can handle more complex combinations most languages also have a graphical interface and at least a simple statistic gathering capability for the analysis of the results. An important part of discrete-event languages is the ability to generate pseudo-random numbers and variants from different probability distributions. Examples are:

</doc>
<doc id="48900" url="https://en.wikipedia.org/wiki?curid=48900" title="Atomic radius">
Atomic radius

The atomic radius of a chemical element is a measure of the size of its atoms, usually the mean or typical distance from the center of the nucleus to the boundary of the surrounding cloud of electrons. Since the boundary is not a well-defined physical entity, there are various non-equivalent definitions of atomic radius. Three widely used definitions of atomic radius are: Van der Waals radius, ionic radius, and covalent radius.
Depending on the definition, the term may apply only to isolated atoms, or also to atoms in condensed matter, covalently bound in molecules, or in ionized and excited states; and its value may be obtained through experimental measurements, or computed from theoretical models. The value of the radius may depend on the atom's state and context.
Electrons do not have definite orbits, or sharply defined ranges. Rather, their positions must be described as probability distributions that taper off gradually as one moves away from the nucleus, without a sharp cutoff. Moreover, in condensed matter and molecules, the electron clouds of the atoms usually overlap to some extent, and some of the electrons may roam over a large region encompassing two or more atoms.
Under most definitions the radii of isolated neutral atoms range between 30 and 300 pm (trillionths of a meter), or between 0.3 and 3 ångströms. Therefore, the radius of an atom is more than 10,000 times the radius of its nucleus (1–10 fm), and less than 1/1000 of the wavelength of visible light (400–700 nm).
For many purposes, atoms can be modeled as spheres. This is only a crude approximation, but it can provide quantitative explanations and predictions for many phenomena, such as the density of liquids and solids, the diffusion of fluids through molecular sieves, the arrangement of atoms and ions in crystals, and the size and shape of molecules.
Atomic radii vary in a predictable and explicable manner across the periodic table. For instance, the radii generally decrease along each period (row) of the table, from the alkali metals to the noble gases; and increase down each group (column). The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period. These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory. The atomic radii decrease across the Periodic Table because as the atomic number increases, the number of protons increases across the period, but the extra electrons are only added to the same quantum shell. Therefore, the effective nuclear charge towards the outermost electrons increases, drawing the outermost electrons closer. As a result, the electron cloud contracts and the atomic radius decreases.
History.
In 1920, shortly after it had become possible to determine the sizes of atoms using X-ray crystallography, it was suggested that all atoms of the same element have the same radii. However, in 1923, when more crystal data had become available, it was found that the approximation of an atom as a sphere does not necessarily hold when comparing the same atom in different crystal structures.
Definitions.
Widely used definitions of atomic radius include:
Empirically measured atomic radii.
The following table shows empirically measured covalent radii for the elements, as published by J. C. Slater in 1964. The values are in picometers (pm or 1×10−12 m), with an accuracy of about 5 pm. The shade of the box ranges from red to yellow as the radius increases; gray indicates lack of data.
Explanation of the general trends.
The increasing nuclear charge is partly counterbalanced by the increasing number of electrons, a phenomenon that is known as shielding; which explains why the size of atoms usually increases down each column. However, there is one notable exception, known as the lanthanide contraction: the 5d block of elements are much smaller than one would expect, due to the shielding caused by the 4f electrons.
The following table summarizes the main phenomena that influence the atomic radius of an element:
Lanthanide contraction.
The electrons in the 4f-subshell, which is progressively filled from cerium ("Z" = 58) to lutetium ("Z" = 71), are not particularly effective at shielding the increasing nuclear charge from the sub-shells further out. The elements immediately following the lanthanides have atomic radii which are smaller than would be expected and which are almost identical to the atomic radii of the elements immediately above them. Hence hafnium has virtually the same atomic radius (and chemistry) as zirconium, and tantalum has an atomic radius similar to niobium, and so forth. The effect of the lanthanide contraction is noticeable up to platinum ("Z" = 78), after which it is masked by a relativistic effect known as the inert pair effect.
Due to lanthanide contraction, the 5 following observations can be drawn:
d-Block contraction.
The d-block contraction is less pronounced than the lanthanide contraction but arises from a similar cause. In this case, it is the poor shielding capacity of the 3d-electrons which affects the atomic radii and chemistries of the elements immediately following the first row of the transition metals, from gallium ("Z" = 31) to bromine ("Z" = 35).
Calculated atomic radii.
The following table shows atomic radii computed from theoretical models, as published by Enrico Clementi and others in 1967. The values are in picometres (pm).

</doc>
<doc id="48901" url="https://en.wikipedia.org/wiki?curid=48901" title="Conversion factor">
Conversion factor

A conversion factor originally known as unity bracket method, is a mathematical tool for converting between units of measurement. It is sometimes referred to as a unit multiplier, and consists of a fraction in which the denominator is equal to the numerator.
A "conversion factor" is used to change the units of a measured quantity without changing its value. Because of the identity property of multiplication, the value of a number will not change as long as it is multiplied by one. Also, if the numerator and denominator of a fraction are equal to each other, then the fraction is equal to one. So as long as the numerator and denominator of the fraction are equivalent, they will not affect the value of the measured quantity.
For example,
Days are converted to hours, by multiplying the days by the conversion factor as 24. The conversion can be reversed by dividing, the hours, by 24 to get days; however, the reciprocal 1/24 could be considered the reverse conversion factor for an hours-to-days conversion, where 1/24 ~= 0.0416666666667. Hence, the term ""conversion factor"" is the multiplier which yields the result, not a divisor from that viewpoint. To yield hours, the conversion factor is 24, not 1/24, so: hours = days × 24 (multiplying by the factor).
Examples of Conversion Factors

</doc>
<doc id="48902" url="https://en.wikipedia.org/wiki?curid=48902" title="Kilogram per cubic metre">
Kilogram per cubic metre

Kilogram per cubic metre is an SI derived unit of density, defined by mass in kilograms divided by volume in cubic metres. The official SI symbolic abbreviation is kg·m−3, or equivalently either kg/m3 or formula_1.
Conversions.
1 kg/m3 is equivalent to:
1 g/cm3 = 1000 kg/m3 = 1,000,000 g/m3 (exactly)
1 lb/ft3 ≈ 16.02 kg/m3 (approximately)
1 oz/gal ≈ 7.489 kg/m3 (approximately)
Relation to other measures.
The kilogram was originally based on the mass of one litre of water, thus the density of water is about 1000 kg/m3 or 1 g/cm3. In chemistry, g/cm3 is more commonly used.

</doc>
<doc id="48903" url="https://en.wikipedia.org/wiki?curid=48903" title="Nucleosynthesis">
Nucleosynthesis

Nucleosynthesis is the process that creates new atomic nuclei from pre-existing nucleons, primarily protons and neutrons. The first nuclei were formed about three minutes after the Big Bang, through the process called Big Bang nucleosynthesis. It was then that hydrogen and helium formed to become the content of the first stars, and this primeval process is responsible for the present hydrogen/helium ratio of the cosmos.
With the formation of stars, heavier nuclei were created from hydrogen and helium by stellar nucleosynthesis, a process that continues today. Some of these elements, particularly those lighter than iron, continue to be delivered to the interstellar medium when low mass stars eject their outer envelope before they collapse to form white dwarfs. The remains of their ejected mass form the planetary nebulae observable throughout our galaxy.
Supernova nucleosynthesis within exploding stars by fusing carbon and oxygen is responsible for the abundances of elements between magnesium (atomic number 12) and nickel (atomic number 28). Supernova nucleosynthesis is also thought to be responsible for the creation of rarer elements heavier than iron and nickel, in the last few seconds of a type II supernova event. The synthesis of these heavier elements absorbs energy (endothermic) as they are created, from the energy produced during the supernova explosion. Some of those elements are created from the absorption of multiple neutrons (the R process) in the period of a few seconds during the explosion. The elements formed in supernovas include the heaviest elements known, such as the long-lived elements uranium and thorium.
Cosmic ray spallation, caused when cosmic rays impact the interstellar medium and fragment larger atomic species, is a significant source of the lighter nuclei, particularly 3He, 9Be and 10,11B, that are not created by stellar nucleosynthesis.
In addition to the fusion processes responsible for the growing abundances of elements in the universe, a few minor natural processes continue to produce very small numbers of new nuclides on Earth. These nuclides contribute little to their abundances, but may account for the presence of specific new nuclei. These nuclides are produced via radiogenesis (decay) of long-lived, heavy, primordial radionuclides such as uranium and thorium. Cosmic ray bombardment of elements on Earth also contribute to the presence of rare, short-lived atomic species called cosmogenic nuclides.
Timeline.
It is thought that the primordial nucleons themselves were formed from the quark–gluon plasma during the Big Bang as it cooled below two trillion degrees. A few minutes afterward, starting with only protons and neutrons, nuclei up to lithium and beryllium (both with mass number 7) were formed, but the abundances of other elements dropped sharply with growing atomic mass. Some boron may have been formed at this time, but the process stopped before significant carbon could be formed, as this element requires a far higher product of helium density and time than were present in the short nucleosynthesis period of the Big Bang. That fusion process essentially shut down at about 20 minutes, due to drops in temperature and density as the universe continued to expand. This first process, Big Bang nucleosynthesis, was the first type of nucleogenesis to occur in the universe.
The subsequent nucleosynthesis of the heavier elements requires the extreme temperatures and pressures found within stars and supernovas. These processes began as hydrogen and helium from the Big Bang collapsed into the first stars at 500 million years. Star formation has occurred continuously in the galaxy since that time. The elements found on Earth, the so-called primordial elements, were created prior to Earth's formation by stellar nucleosynthesis and by supernova nucleosynthesis. They range in atomic numbers from Z=6 (carbon) to Z=94 (plutonium). Synthesis of these elements occurred either by nuclear fusion (including both rapid and slow multiple neutron capture) or to a lesser degree by nuclear fission followed by beta decay.
A star gains heavier elements by combining its lighter nuclei, hydrogen, deuterium, beryllium, lithium, and boron, which were found in the initial composition of the interstellar medium and hence the star. Interstellar gas therefore contains declining abundances of these light elements, which are present only by virtue of their nucleosynthesis during the Big Bang. Larger quantities of these lighter elements in the present universe are therefore thought to have been restored through billions of years of cosmic ray (mostly high-energy proton) mediated breakup of heavier elements in interstellar gas and dust. The fragments of these cosmic-ray collisions include the light elements Li, Be and B.
History of nucleosynthesis theory.
The first ideas on nucleosynthesis were simply that the chemical elements were created at the beginning of the universe, but no rational physical scenario for this could be identified. Gradually it became clear that hydrogen and helium are much more abundant than any of the other elements. All the rest constitute less than 2% of the mass of the Solar System, and of other star systems as well. At the same time it was clear that oxygen and carbon were the next two most common elements, and also that there was a general trend toward high abundance of the light elements, especially those composed of whole numbers of helium-4 nuclei.
Arthur Stanley Eddington first suggested in 1920, that stars obtain their energy by fusing hydrogen into helium and raised the possibility that the heavier elements may also form in stars. This idea was not generally accepted, as the nuclear mechanism was not understood. In the years immediately before World War II, Hans Bethe first elucidated those nuclear mechanisms by which hydrogen is fused into helium.
Fred Hoyle's original work on nucleosynthesis of heavier elements in stars, occurred just after World War II. His work explained the production of all heavier elements, starting from hydrogen. Hoyle proposed that hydrogen is continuously created in the universe from vacuum and energy, without need for universal beginning.
Hoyle's work explained how the abundances of the elements increased with time as the galaxy aged. Subsequently, Hoyle's picture was expanded during the 1960s by contributions from William A. Fowler, Alastair G. W. Cameron, and Donald D. Clayton, followed by many others. The seminal 1957 review paper by E. M. Burbidge, G. R. Burbidge, Fowler and Hoyle is a well-known summary of the state of the field in 1957. That paper defined new processes for the transformation of one heavy nucleus into others within stars, processes that could be documented by astronomers.
The Big Bang itself had been proposed in 1931, long before this period, by Georges Lemaître, a Belgian physicist, who suggested that the evident expansion of the Universe in time required that the Universe, if contracted backwards in time, would continue to do so until it could contract no further. This would bring all the mass of the Universe to a single point, a "primeval atom", to a state before which time and space did not exist. Hoyle later gave Lemaître's model the derisive term of Big Bang, not realizing that Lemaître's model was needed to explain the existence of deuterium and nuclides between helium and carbon, as well as the fundamentally high amount of helium present, not only in stars but also in interstellar space. As it happened, both Lemaître and Hoyle's models of nucleosynthesis would be needed to explain the elemental abundances in the universe.
The goal of the theory of nucleosynthesis is to explain the vastly differing abundances of the chemical elements and their several isotopes from the perspective of natural processes. The primary stimulus to the development of this theory was the shape of a plot of the abundances versus the atomic number of the elements. Those abundances, when plotted on a graph as a function of atomic number, have a jagged sawtooth structure that varies by factors up to ten million. A very influential stimulus to nucleosynthesis research was an abundance table created by Hans Suess and Harold Urey that was based on the unfractionated abundances of the non-volatile elements found within unevolved meteorites. Such a graph of the abundances is displayed on a logarithmic scale below, where the dramatically jagged structure is visually suppressed by the many powers of ten spanned in the vertical scale of this graph. See "Handbook of Isotopes in the Cosmos" for more data and discussion of abundances of the isotopes.
Processes.
There are a number of astrophysical processes which are believed to be responsible for nucleosynthesis. The majority of these occur in shells within stars, and the chain of those nuclear fusion processes are known as hydrogen burning (via the proton-proton chain or the CNO cycle), helium burning, carbon burning, neon burning, oxygen burning and silicon burning. These processes are able to create elements up to and including iron and nickel. This is the region of nucleosynthesis within which the isotopes with the highest binding energy per nucleon are created. Heavier elements can be assembled within stars by a neutron capture process known as the s-process or in explosive environments, such as supernovae, by a number of other processes. Some of those others include the r-process, which involves rapid neutron captures, the rp-process, and the p-process (sometimes known as the gamma process), which results in the photodisintegration of existing nuclei.
The major types of nucleosynthesis.
Big Bang nucleosynthesis.
Big Bang nucleosynthesis occurred within the first three minutes of the beginning of the universe and is responsible for much of the abundance of 1H (protium), 2H (D, deuterium), 3He (helium-3), and 4He (helium-4). Although 4He continues to be produced by stellar fusion and alpha decays and trace amounts of 1H continue to be produced by spallation and certain types of radioactive decay, most of the mass of the isotopes in the universe are thought to have been produced in the Big Bang. The nuclei of these elements, along with some 7Li and 7Be are considered to have been formed between 100 and 300 seconds after the Big Bang when the primordial quark–gluon plasma froze out to form protons and neutrons. Because of the very short period in which nucleosynthesis occurred before it was stopped by expansion and cooling (about 20 minutes), no elements heavier than beryllium (or possibly boron) could be formed. Elements formed during this time were in the plasma state, and did not cool to the state of neutral atoms until much later.
Stellar nucleosynthesis.
Stellar nucleosynthesis is the nuclear process by which new nuclei are produced. It occurs in stars during stellar evolution. It is responsible for the galactic abundances of elements from carbon to iron. Stars are thermonuclear furnaces in which H and He are fused into heavier nuclei by increasingly high temperatures as the composition of the core evolves. Of particular importance is carbon, because its formation from He is a bottleneck in the entire process. Carbon is produced by the triple-alpha process in all stars. Carbon is also the main element that causes the release of free neutrons within stars, giving rise to the s-process, in which the slow absorption of neutrons converts iron into elements heavier than iron and nickel.
The products of stellar nucleosynthesis are generally dispersed into the interstellar gas through mass loss episodes and the stellar winds of low mass stars. The mass loss events can be witnessed today in the planetary nebulae phase of low-mass star evolution, and the explosive ending of stars, called supernovae, of those with more than eight times the mass of the Sun.
The first direct proof that nucleosynthesis occurs in stars was the astronomical observation that interstellar gas has become enriched with heavy elements as time passed. As a result, stars that were born from it late in the galaxy, formed with much higher initial heavy element abundances than those that had formed earlier. The detection of technetium in the atmosphere of a red giant star in 1952, by spectroscopy, provided the first evidence of nuclear activity within stars. Because technetium is radioactive, with a half-life much less than the age of the star, its abundance must reflect its recent creation within that star. Equally convincing evidence of the stellar origin of heavy elements, is the large overabundances of specific stable elements found in stellar atmospheres of asymptotic giant branch stars. Observation of barium abundances some 20-50 times greater than found in unevolved stars is evidence of the operation of the s-process within such stars. Many modern proofs of stellar nucleosynthesis are provided by the isotopic compositions of stardust, solid grains that have condensed from the gases of individual stars and which have been extracted from meteorites. Stardust is one component of cosmic dust, and is frequently called presolar grains. The measured isotopic compositions in stardust grains demonstrate many aspects of nucleosynthesis within the stars from which the grains condensed during the star's late-life mass-loss episodes.
Explosive nucleosynthesis.
Supernova nucleosynthesis occurs in the energetic environment in supernovae, in which the elements between silicon and nickel are synthesized in quasiequilibrium established during fast fusion that attaches by reciprocating balanced nuclear reactions to 28Si. Quasiequilibrium can be thought of as "almost equilibrium" except for a high abundance of the 28Si nuclei in the feverishly burning mix. This concept was the most important discovery in nucleosynthesis theory of the intermediate-mass elements since Hoyle's 1954 paper because it provided an overarching understanding of the abundant and chemically important elements between silicon (A=28) and nickel (A=60). It replaced the incorrect although much cited alpha process of the B2FH paper, which inadvertently obscured Hoyle's better 1954 theory. Further nucleosynthesis processes can occur, in particular the r-process (rapid process) described by the B2FH paper and first calculated by Seeger, Fowler and Clayton, in which the most neutron-rich isotopes of elements heavier than nickel are produced by rapid absorption of free neutrons. The creation of free neutrons by electron capture during the rapid compression of the supernova core along with assembly of some neutron-rich seed nuclei makes the r-process a "primary process", and one that can occur even in a star of pure H and He. This is in contrast to the B2FH designation of the process as a "secondary process". This promising scenario, though generally supported by supernova experts, has yet to achieve a totally satisfactory calculation of r-process abundances. The primary r-process has been confirmed by astronomers who have observed old stars born when galactic metallicity was still small, that nonetheless contain their complement of r-process nuclei; thereby demonstrating that the metallicity is a product of an internal process. The r-process is responsible for our natural cohort of radioactive elements, such as uranium and thorium, as well as the most neutron-rich isotopes of each heavy element.
The rp-process (rapid proton) involves the rapid absorption of free protons as well as neutrons, but its role and its existence are less certain.
Explosive nucleosynthesis occurs too rapidly for radioactive decay to decrease the number of neutrons, so that many abundant isotopes with equal and even numbers of protons and neutrons are synthesized by the silicon quasiequilibrium process. During this process, the burning of oxygen and silicon fuses nuclei that themselves have equal numbers of protons and neutrons to produce nuclides which consist of whole numbers of helium nuclei, up to 15 (representing 60Ni). Such multiple-alpha-particle nuclides are totally stable up to 40Ca (made of 10 helium nuclei), but heavier nuclei with equal and even numbers of protons and neutrons are tightly bound but unstable. The quasiequilibrium produces radioactive isobars 44Ti, 48Cr, 52Fe, and 56Ni, which (except 44Ti) are created in abundance but decay after the explosion and leave the most stable isotope of the corresponding element at the same atomic weight. The most abundant and extant isotopes of elements produced in this way are 48Ti, 52Cr, and 56Fe. These decays are accompanied by the emission of gamma-rays (radiation from the nucleus), whose spectroscopic lines can be used to identify the isotope created by the decay. The detection of these emission lines were an important early product of gamma-ray astronomy.
The most convincing proof of explosive nucleosynthesis in supernovae occurred in 1987 when those gamma-ray lines were detected emerging from supernova 1987A. Gamma ray lines identifying 56Co and 57Co nuclei, whose radioactive half-lives limit their age to about a year, proved that they were created by their radioactive cobalt parents. This nuclear astronomy observation was predicted in 1969 as a way to confirm explosive nucleosynthesis of the elements, and that prediction played an important role in the planning for NASA's Compton Gamma-Ray Observatory.
Other proofs of explosive nucleosynthesis are found within the stardust grains that condensed within the interiors of supernovae as they expanded and cooled. Stardust grains are one component of cosmic dust. In particular, radioactive 44Ti was measured to be very abundant within supernova stardust grains at the time they condensed during the supernova expansion. This confirmed a 1975 prediction of the identification of supernova stardust (SUNOCONs), which became part of the pantheon of presolar grains. Other unusual isotopic ratios within these grains reveal many specific aspects of explosive nucleosynthesis.
Cosmic ray spallation.
Cosmic ray spallation process reduces the atomic weight of interstellar matter by the impact with cosmic rays, to produce some of the lightest elements present in the universe (though not a significant amount of deuterium). Most notably spallation is believed to be responsible for the generation of almost all of 3He and the elements lithium, beryllium, and boron, although some and are thought to have been produced in the Big Bang. The spallation process results from the impact of cosmic rays (mostly fast protons) against the interstellar medium. These impacts fragment carbon, nitrogen, and oxygen nuclei present. The process results in the light elements beryllium, boron, and lithium in cosmos at much greater abundances than they are within solar atmospheres. The light elements 1H and 4He nuclei are not a product of spallation and are represented in the cosmos with approximately primordial abundance.
Beryllium and boron are not significantly produced by stellar fusion processes, due to the instability of any 8Be formed from two 4He nuclei.
Empirical evidence.
Theories of nucleosynthesis are tested by calculating isotope abundances and comparing those results with observed results. Isotope abundances are typically calculated from the transition rates between isotopes in a network. Often these calculations can be simplified as a few key reactions control the rate of other reactions.
Minor mechanisms and processes.
Very small amounts of certain nuclides are produced on Earth by artificial means. Those are our primary source, for example, of technetium. However, some nuclides are also produced by a number of natural means that have continued after primordial elements were in place. These often act to produce new elements in ways that can be used to date rocks or to trace the source of geological processes. Although these processes do not produce the nuclides in abundance, they are assumed to be the entire source of the existing natural supply of those nuclides.
These mechanisms include:
In addition to artificial processes, it is postulated that neutron star collision is the main source of elements heavier than iron.

</doc>
<doc id="48906" url="https://en.wikipedia.org/wiki?curid=48906" title="People's Bank of China">
People's Bank of China

The People's Bank of China (PBC or PBOC, ) is the central bank of the People's Republic of China with the power to control monetary policy and regulate financial institutions in mainland China. The People's Bank of China has more financial assets than any single public institution, and is second only to the Federal Reserve System of the United States in terms of overall central bank assets.
History.
The bank was established on December 1, 1948, based on the consolidation of the Huabei Bank, the Beihai Bank and the Xibei Farmer Bank. The headquarters was first located in Shijiazhuang, Hebei, and then moved to Beijing in 1949. Between 1950 and 1978 the PBC was the only bank in the People's Republic of China and was responsible for both central banking and commercial banking operations. All other banks within Mainland China such as the Bank of China were either organized as divisions of the PBC or were non-deposit taking agencies.
In the 1980s, as part of economic reform, the commercial banking functions of the PBC were split off into four independent but state-owned banks and in 1983, the State Council promulgated that the PBC would function as the central bank of China. Mr. Chen Yuan was instrumental in modernizing the bank in the early 1990s. Its central bank status was legally confirmed on March 18, 1995 by the 3rd Plenum of the 8th National People's Congress. In 1998, the PBC underwent a major restructuring. All provincial and local branches were abolished, and the PBC opened nine regional branches, whose boundaries did not correspond to local administrative boundaries. In 2003, the Standing Committee of the Tenth National People's Congress approved an amendment law for strengthening the role of PBC in the making and implementation of monetary policy for safeguarding the overall financial stability and provision of financial services.
The People's Bank of China (PBC) is the largest central bank at US$3.201 trillion.
Management.
The top management of the PBC is composed of the governor and a certain number of deputy governors. The governor of the PBC is appointed into or removed by the National People's Congress or its Standing Committee. The candidate for the governor of the PBC is nominated by the Premier of the People's Republic of China and approved by the National People's Congress. When the National People's Congress is in adjournment, the Standing Committee sanctions the candidacy for the governor of the PBC. The deputy governors of the PBC are appointed to or removed from office by the Premier of the State Council.
The PBC adopts a governor responsibility system under which the governor supervises the overall work of the PBC while the deputy governors provide assistance to the governor to fulfill his or her responsibility.
The current governor is Zhou Xiaochuan:. Deputy governors of the management team include: Yi Gang, Wang Huaqing, Pan Gongsheng, Fan Yifei, Guo Qingping, Zhang Xiaohui, and Yang Ziqiang. Former top-level managers include: Ms. Hu Xiaolian, Liu Shiyu, Li Dongrong and Ms. Jin Qi.
Structure.
The PBC has established 9 regional branches, one each in Tianjin, Shenyang, Shanghai, Nanjing, Jinan, Wuhan, Guangzhou, Chengdu and Xi'an, 2 operations offices in Beijing and Chongqing, 303 municipal sub-branches and 1809 county-level sub-branches.
It has 6 overseas representative offices (PBC Representative Office for America, PBC Representative Office (London) for Europe, PBC Tokyo Representative Office, PBC Frankfurt Representative Office, PBC Representative Office for Africa, Liaison Office of the PBC in the Caribbean Development Bank).
The PBC consists of 18 functional departments (bureaus) as below:
The following enterprises and institutions are directly under the PBC:
Financial inclusion.
The PBOC is active in promoting financial inclusion policy and a member of the Alliance for Financial Inclusion.
Interest rates.
Previously, interest rates set by the bank were always divisible by nine, instead of by 25 as in the rest of the world.
However, this is no longer applied since the central bank started changing rates by 0.25 percentage points on October 19, 2010 (which was a rate increase).
PBC latest interest rate changes
Reserve Requirement Ratio (RRR).
PBC latest Reserve Requirement Ratio changes
Foreign-exchange reserves.
Foreign-exchange reserves from 2004

</doc>
<doc id="48907" url="https://en.wikipedia.org/wiki?curid=48907" title="Rules of chess">
Rules of chess

The rules of chess (also known as the laws of chess) are rules governing the play of the game of chess. While the exact origins of chess are unclear, modern rules first took form during the Middle Ages. The rules continued to be slightly modified until the early 19th century, when they reached essentially their current form. The rules also varied somewhat from place to place. Today chess's international governing body FIDE ("Fédération Internationale des Échecs") sets the standard rules, with slight modifications made by some national organizations for their own purposes. There are variations of the rules for fast chess, correspondence chess, online chess, and chess variants.
Chess is a game played by two people on a chessboard, with sixteen pieces (of six types) for each player. Each type of piece moves in a distinct way. The goal of the game is to checkmate, that is, to threaten the opponent's king with inevitable capture. Games do not necessarily end with checkmate – players often resign if they believe they will lose. In addition, there are several ways that a game can end in a draw.
Besides the basic movement of the pieces, rules also govern the equipment used, the time control, the conduct and ethics of players, accommodations for physically challenged players, the recording of moves using chess notation, as well as provide procedures for resolving irregularities which can occur during a game.
Initial setup.
Chess is played on a chessboard, a square board divided into 64 squares (eight-by-eight) of alternating color, which is similar to that used in draughts (checkers) . No matter what the actual colors of the board, the lighter-colored squares are called "light" or "white", and the darker-colored squares are called "dark" or "black". Sixteen "white" and sixteen "black" pieces are placed on the board at the beginning of the game. The board is placed so that a white square is in each player's near-right corner. Horizontal rows are called ranks and vertical rows are called files.
Each player controls sixteen pieces:
At the beginning of the game, the pieces are arranged as shown in the diagram: for each side one king, one queen, two rooks, two bishops, two knights, and eight pawns. The pieces are placed, one on a square, as follows:
Popular mnemonics used to remember the setup are "queen on her own color" and "white on right". The latter refers to setting up the board so that the square closest to each player's right is white .
Gameplay.
The player controlling the white pieces is named "White"; the player controlling the black pieces is named "Black". White moves first, then players alternate moves. Making a move is required; it is not legal to skip a move, even when having to move is detrimental. Play continues until a king is checkmated, a player resigns, or a draw is declared, as explained below. In addition, if the game is being played under a time control players who exceed their time limit lose the game.
The official chess rules do not include a procedure for determining who plays White. Instead, this decision is left open to tournament-specific rules (e.g. a Swiss system tournament or Round-robin tournament) or, in the case of non-competitive play, mutual agreement, in which case some kind of random choice is often employed. A common method is for one player to conceal a piece (usually a pawn) of each color in either hand; the other player chooses a hand to open and reveal their color. Play then commences with white.
Movement.
Basic moves.
Each type of chess piece has its own method of movement. A piece moves to a vacant square except when capturing an opponent's piece.
Except for any move of the knight and castling, pieces cannot jump over other pieces. A piece is "captured" (or "taken") when an attacking enemy piece replaces it on its square ("en passant" is the only exception). The captured piece is thereby permanently removed from the game. The king can be put in check but cannot be captured (see below).
Castling.
Castling consists of moving the king two squares towards a rook, then placing the rook on the other side of the king, adjacent to it. Castling is only permissible if all of the following conditions hold:
"En passant".
When a pawn advances two squares from its original square and ends the turn adjacent to a pawn of the opponent's on the same rank, it may be captured by that pawn of the opponent's, as if it had moved only one square forward. This capture is only legal on the opponent's next move immediately following the first pawn's advance. The diagrams on the right demonstrate an instance of this: if the white pawn moves from a2 to a4, the black pawn on b4 can capture it "en passant", moving from b4 to a3 while the white pawn on a4 is removed from the board.
Pawn promotion.
If a player advances a pawn to its eighth rank, the pawn is then "promoted" (converted) to a queen, rook, bishop, or knight of the same color at the choice of the player (a queen is usually chosen). The choice is not limited to previously captured pieces. Hence it is theoretically possible for a player to have up to nine queens or up to ten rooks, bishops, or knights if all of their pawns are promoted. If the desired piece is not available, the player should call the arbiter to provide the piece .
Check.
A king is "in check" when it is under attack by at least one enemy piece. A piece unable to move because it would place its own king in check (it is pinned against its own king) may still deliver check to the opposing player.
A player may not make any move which places or leaves his king in check. The possible ways to get out of check are:
If it is not possible to get out of check, the king is checkmated and the game is over (see the next section).
In informal games, it is customary to announce "check" when making a move that puts the opponent's king in check. However, in formal competitions check is rarely announced .
Competition rules.
These rules apply to games played "over the board". There are special rules for correspondence chess, blitz chess, computer chess, and for handicapped players.
Act of moving the pieces.
The movement of pieces is to be done with one hand. Once the hand is taken off a piece after moving it, the move cannot be retracted unless the move is illegal. When castling, the player should first move the king with one hand and then move the rook with the same hand .
In the case of a pawn promotion, if the player releases the pawn on the eighth rank, the player must promote the pawn. After the pawn has moved, the player may touch any piece not on the board and the promotion is not finalized until the new piece is released on the promotion square .
Touch-move rule.
In serious play, if a player having the move touches a piece as if having the intention of moving it, then the player must move it if it can be legally moved. So long as the hand has not left the piece on a new square, any legal move can be made with the piece. If a player touches one of the opponent's pieces then that piece must be captured if there is a legal move that does so. If none of the touched pieces can be moved or captured there is no penalty .
When castling, the king must be the first piece touched. If the player touches the rook at the same time as touching the king, the player must castle with that rook if it is legal to do so. If the player completes a two-square king move without touching a rook, the player must move the correct rook accordingly if castling in that direction is legal. If a player starts to castle illegally, another legal king move must be made if possible, including castling with the other rook .
If a player moves a pawn to its eighth rank, it cannot be substituted for a different move of the pawn when the player has stopped touching it. However, the move is not complete until the promoted piece is released on that square.
If a player wishes to touch a piece with the intention of adjusting its position on a square, the player must first alert the opponent of this intention by saying "J'adoube" or "I adjust". Once the game has started, only the player with the move may touch the pieces on the board .
Timing.
Tournament games are played under time constraints, called time controls, using a chess clock. Each player must make his moves within the time control or forfeit the game. There are different types of time controls. In some cases each player will have a certain amount of time to make a certain number of moves. In other cases each player will have a limited amount of time to make all of his moves. Also, the player may gain a small amount of additional time for each move made, either by a small increment added for each move made, or by the clock delaying a small amount of time each time it is started after the opponent's move .
The United States Chess Federation (USCF) rule is different. USCF Rule 14E defines "insufficient material to win on time", that is lone king, king plus knight, king plus bishop, and king plus two knights opposed by no pawns, and there is no forced win in the final position. Hence to win on time with this material, the USCF rule requires that a win can be forced from that position, while the FIDE rule merely requires a win to be possible. (See Monika Soćko rules appeal in 2008 and Women's World Chess Championship 2008 for a famous instance of this rule.)
If a player believes that his opponent is attempting to win the game on time and not by normal means (i.e. checkmate), if it is a sudden death time control and the player has less than two minutes remaining, the player may stop the clocks and claim a draw with the arbiter. The arbiter may declare the game a draw or postpone the decision and allot the opponent two extra minutes .
Recording moves.
Each square of the chessboard is identified with a unique pair of a letter and a number. The vertical "files" are labeled a through h, from White's left (i.e. the queenside) to White's right. Similarly, the horizontal "ranks" are numbered from 1 to 8, starting from the one nearest White's side of the board. Each square of the board, then, is uniquely identified by its file letter and rank number. The white king, for example, starts the game on square e1. The black knight on b8 can move to a6 or c6.
In formal competition, each player is obliged to record each move as it is played in a chess notation in order to settle disputes about illegal positions, overstepping time control, and making claims of draws by the fifty-move rule or repetition of position. Algebraic chess notation is the accepted standard for recording games today. There are other systems such as ICCF numeric notation for international correspondence chess and the obsolete descriptive chess notation. The current rule is that a move must be made on the board before it is written on paper or recorded with an electronic device.
Both players should indicate offers of a draw by writing "=" at that move on their scoresheet . Notations about the time on the clocks can be made. If a player has less than five minutes left to complete all of their moves, they are not required to record the moves (unless a delay of at least thirty seconds per move is being used). The scoresheet must be made available to the arbiter at all times. A player may respond to an opponent's move before writing it down .
Irregularities.
Illegal move.
A player who makes an illegal move must retract that move and make a legal move. That move must be made with the same piece if possible, because the touch-move rule applies. If the illegal move was an attempt to castle, the touch-move rule applies to the king but not to the rook. The arbiter should adjust the time on the clock according to the best evidence. If the mistake is only noticed later on, the game should be restarted from the position in which the error occurred . Some regional organizations have different rules.
If blitz chess is being played (in which both players have a small, limited time, e.g. five minutes) the rule varies. A player may correct an illegal move if the player has not pressed their clock. If a player has pressed their clock, the opponent may claim a win if he or she hasn't moved. If the opponent moves, the illegal move is accepted and without penalty .
According to the FIDE Laws of Chess, the first completed illegal move is penalized by awarding the opponent two additional minutes on the clock. The second completed illegal move by the same player results in the loss of the game, unless the position is such that it is impossible for the opponent to win by any series of legal moves (e.g. if the opponent has a bare king) in which case the game is drawn. A move is completed when it has been made and the player has pressed the clock. In rapid chess and blitz chess, the first completed illegal move results in a loss.
Illegal position.
If it is discovered during the game that the starting position was incorrect, the game is restarted. If it is discovered during the game that the board is oriented incorrectly, the game is continued with the pieces transferred to a correctly oriented board. If the game starts with the colors of the pieces reversed, the game continues (unless the arbiter rules otherwise) . Some regional organizations have different rules.
If a player knocks over pieces, it is their responsibility to restore them to their correct position on their time. If it is discovered that an illegal move has been made, or that pieces have been displaced, the game is restored to the position before the irregularity. If that position cannot be determined, the game is restored to the last known correct position .
Conduct.
Players may not use any notes, outside sources of information (including computers), or advice from other people. Analysis on another board is not permitted. Scoresheets are to record objective facts about the game only, such as time on the clock or draw offers. Players may not leave the competition area without permission of the arbiter .
High standards of etiquette and ethics are expected. Players should shake hands before and after the game. Generally a player should not speak during the game, except to offer a draw, resign, or to call attention to an irregularity. An announcement of "check" is made in amateur games but should not be used in officially sanctioned games. A player may not distract or annoy another player by any means, including repeatedly offering a draw .
Due to increasing concerns about the use of chess engines and outside communication, in 2014 FIDE banned all mobile phones from the playing area during chess competitions, under penalty of forfeiture of the game or even expulsion from the tournament. However, the rules also allow for less rigid enforcement in amateur events.
Equipment.
The size of the squares of the chessboard should be approximately 1.25 to 1.3 times the diameter of the base of the king, or 50 to 65 mm. Squares of approximately 57 mm ( inches) normally are well-suited for pieces with the kings in the preferred size range. The darker squares are usually brown or green and the lighter squares are off-white or buff.
Pieces of the Staunton chess set design are the standard and are usually made of wood or plastic. They are often black and white; other colors may be used (like a dark wood or even red for the dark pieces) but they would still be called the "white" and "black" pieces (see White and Black in chess). The height of the king should be 85 to 105 millimetres (3.35–4.13 inches). A height of approximately 95 to 102 mm (–4 inches) is preferred by most players. The diameter of the king should be 40 to 50% of its height. The size of the other pieces should be in proportion to the king. The pieces should be well balanced .
In games subject to time control, a chess clock is used, consisting of two adjacent clocks and buttons to stop one clock while starting the other, such that the two component clocks never run simultaneously. The clock can be analog or digital though a digital clock is highly preferred under both USCF and FIDE rulesets. This is since most tournaments now include either a time delay (a countdown to when a clock starts again) or add (extra time being added prior or after the move) to their time controls. Before the start of the game, either the arbiter decides, or whomever is playing Black, where the chess clock is placed.
End of the game.
Checkmate.
If a player's king is placed in check and there is no legal move that player can make to escape check, then the king is said to be "checkmated", the game ends, and that player loses . Unlike other pieces, the king is never actually captured or removed from the board because checkmate ends the game .
The diagram shows an example checkmate position. The white king is threatened by the black queen; the square to which the king could move is also threatened; it cannot capture the queen, because it would then be in check by the rook.
Resigning.
Either player may "resign" at any time and their opponent wins the game. This normally happens when the player believes he or she is very likely to lose the game. A player may resign by saying it verbally or by indicating it on their scoresheet in any of three ways: (1) by writing "resigns", (2) by circling the result of the game, or (3) by writing "1–0" if Black resigns or "0–1" if White resigns . Tipping over the king also indicates resignation, but it is not frequently used (and should be distinguished from accidentally knocking the king over). Stopping both clocks is not an indication of resigning, since clocks can be stopped to call the arbiter. An offer of a handshake is not necessarily a resignation either, since one player could think they are agreeing to a draw .
Draws.
The game ends in a draw if any of these conditions occur:
The player having the move may claim a draw by declaring that one of the following conditions exists, or by declaring an intention to make a move which will bring about one of these conditions:
If the claim is proven true, the game is drawn .
At one time, if a player was able to check the opposing king continually (perpetual check) and the player indicated their intention to do so, the game was drawn. This rule is no longer in effect; however, players will usually agree to a draw in such a situation, since either the rule on threefold repetition or the fifty-move rule will eventually be applicable , .
Time control.
A game played under time control will end as a loss for a player who uses up all of their allotted time, unless the opponent cannot possibly checkmate him (see the Timing section above). There are different types of time control. Players may have a fixed amount of time for the entire game or they may have to make a certain number of moves within a specified time. Also, a small increment of time may be added for each move made.
History.
The rules of chess have evolved much over the centuries from the early chess-like games played in India in the 6th century. For much of that time the rules have varied from area to area. The modern rules first took form in Italy during the 13th century, giving more mobility to pieces that previously had more restricted movement (such as the queen and bishop). Such modified rules entered into an accepted form during the late 15th century or early 16th century . The basic moves of the king, rook, and knight are unchanged. Pawns originally did not have the option of moving two squares on their first move and promoted only to a queen if they reached the eighth rank. The queen was originally the "fers" or "farzin", which could move one square diagonally in any direction. In European chess it became able to leap two squares diagonally, forwards, backwards, or to left or right on its first move; some area also gave this right to a newly promoted pawn. In the Persian and Arabic game the bishop was a "pīl" (Persian) or "fīl" (Arabic) (meaning "elephant") which moved two squares diagonally with jump . In the Middle Ages the pawn could only be promoted to the equivalent of a queen (which at that time was the weakest piece) if it reached its eighth rank . During the 12th century the squares on the board sometimes alternated colors, and this became the standard in the 13th century , whence the word "chequered"/"checkered".
Between 1200 and 1600 several laws emerged that drastically altered the game. Checkmate became a requirement to win; a player could not win by capturing all of the opponent's pieces. Stalemate was added, although the outcome has changed several times (see History of the stalemate rule). Pawns gained the option of moving two squares on their first move, and the en passant rule was a natural consequence of that new option. The king and rook acquired the right to castle (see Variations throughout history of castling for different versions of the rule).
Between 1475 and 1500 the queen and the bishop also acquired their current moves, which made them much stronger pieces . When all of these changes were accepted the game was in essentially its modern form .
The rules for pawn promotion have changed several times. As stated above, originally the pawn could only be promoted to the queen, which at that time was a weak piece. When the queen acquired its current move and became the most powerful piece, the pawn could then be promoted to a queen or a rook, bishop, or knight. In the 18th century rules allowed only the promotion to a piece already captured, e.g. the rules published in 1749 by François-André Danican Philidor. In the 19th century this restriction was lifted, which allowed for a player to have more than one queen, e.g. the 1828 rules by Jacob Sarratt .
Two new rules concerning draws were introduced, each of which have changed through the years:
Another group of new laws included (1) the touch-move rule and the accompanying "j'adoube/adjust" rule; (2) that White moves first (in 1889); (3) the orientation of the board; (4) the procedure if an illegal move was made; (5) the procedure if the king had been left in check for some moves; and (6) issues regarding the behavior of players and spectators. The Staunton chess set was introduced in 1849 and it became the standard style of pieces. The size of pieces and squares of the board was standardized .
Until the middle of the 19th century, chess games were played without any time limit. In an 1834 match between Alexander McDonnell and Louis-Charles Mahé de La Bourdonnais, McDonnell took an inordinate amount of time to move, sometimes up to 1½ hours. In 1836 Pierre Charles Fournier de Saint-Amant suggested a time limit, but no action was taken. At the 1851 London tournament, Staunton blamed his loss in his match against Elijah Williams on Williams' slow play; one game was adjourned for the day after only 29 moves. The next year a match between Daniel Harrwitz and Johann Löwenthal used a limit of 20 minutes per move.The first use of a modern-style time limit was in an 1861 match between Adolph Anderssen and Ignác Kolisch .
Codification.
The first known publication of chess rules was in a book by Luis Ramírez de Lucena about 1497, shortly after the movement of the queen, bishop, and pawn were changed to their modern form . In the 16th and 17th centuries, there were differences of opinion concerning rules such as castling, pawn promotion, stalemate, and "en passant". Some of these differences existed until the 19th century . Ruy López de Segura gave rules of chess in his 1561 book Libro de la invencion liberal y arte del juego del axedrez .
As chess clubs arose and tournaments became common, there was a need to formalize the rules. In 1749 Philidor (1726–1795) wrote a set of rules that were widely used, as well as rules by later writers such as the 1828 rules by Jacob Sarratt (1772–1819) and rules by George Walker (1803–1879). In the 19th century, many major clubs published their own rules, including The Hague in 1803, London in 1807, Paris in 1836, and St. Petersburg in 1854. In 1851 Howard Staunton (1810–1874) called for a "Constituent Assembly for Remodeling the Laws of Chess" and proposals by Tassilo von Heydebrand und der Lasa (1818–1889) were published in 1854. Staunton had published rules in "Chess Player's Handbook" in 1847, and his new proposals were published in 1860 in "Chess Praxis"; they were generally accepted in English-speaking countries. German-speaking countries usually used the writings of chess authority Johann Berger (1845–1933) or Handbuch des Schachspiels by Paul Rudolf von Bilguer (1815–1840), first published in 1843.
In 1924, Fédération Internationale des Échecs (FIDE) was formed and in 1929 it took up the task of standardizing the rules. At first FIDE tried to establish a universal set of rules, but translations to various languages differed slightly. Although FIDE rules were used for international competition under their control, some countries continued to use their own rules internally . In 1952 FIDE created the Permanent Commission for the Rules of Chess (also known as the Rules Commission) and published a new edition of the rules. The third official edition of the laws was published in 1966. The first three editions of the rules were published in French, with that as the official version. In 1974 FIDE published the English version of the rules (which was based on an authorized 1955 translation). With that edition, English became the official language of the rules. Another edition was published in 1979. Throughout this time, ambiguities in the laws were handed by frequent interpretations that the Rules Commission published as supplements and amendments. In 1982 the Rules Commission rewrote the laws to incorporate the interpretations and amendments . In 1984 FIDE abandoned the idea of a universal set of laws, although FIDE rules are the standard for high-level play . With the 1984 edition, FIDE implemented a four-year moratorium between changes to the rules. Other editions were issued in 1988 and 1992 , .
The rules of national FIDE affiliates (such as the United States Chess Federation, or USCF) are based on the FIDE rules, with slight variations . Kenneth Harkness published popular rulebooks in the United States starting in 1956, and the USCF continues to publish rulebooks for use in tournaments it sanctions.
In 2008 FIDE added the variant Chess960 to the appendix of the laws of chess. Chess960 uses a random initial set-up of main pieces, with the conditions that the king is placed somewhere between the two rooks, and bishops on opposite-color squares. The castling rules are extended to cover all these positions.
Variations.
One case of a minor extra rule being added for a particular match is "no drawing or resigning during the first 30 moves" in the London Chess Classic on 8–15 December 2009 at Olympia, London.
See also.
Specific rules
References.
Bibliography

</doc>
<doc id="48908" url="https://en.wikipedia.org/wiki?curid=48908" title="Apparent retrograde motion">
Apparent retrograde motion

While the terms direct and prograde are equivalent in this context, the former is the traditional term in astronomy. The earliest recorded use of prograde was in the early 18th century, although the term is now less common.
Etymology.
The term "retrograde" is from the Latin word "retrogradus" – "backward-step", the affix "retro-" meaning "backwards" and "gradus" "step". "Retrograde" is most commonly an adjective used to describe the path of a planet as it travels through the night sky, with respect to the zodiac, stars, and other bodies of the celestial canopy. In this context, the term refers to planets, as they appear from Earth, to stop briefly and reverse direction at certain times though in reality, of course, we now understand that they perpetually orbit in the same uniform direction.
"Mercury in retrograde" is an example of the term used as a noun for retrograde "motion". "Retrograde" is also sometimes used as an intransitive verb meaning to become, to appear, to behave—or appear to move—in a retrograde fashion.
Although planets can sometimes be mistaken for stars as one observes the night sky, the planets actually change position from night to night in relation to the stars. Retrograde (backward) and prograde (forward) are observed as though the stars revolve around the Earth. Ancient Greek astronomer Ptolemy in 150 AD believed that the Earth was the center of the Solar System and therefore used the terms "retrograde" and "prograde" to describe the movement of the planets in relation to the stars. Although it is known today that the planets revolve around the sun, the same terms continue to be used in order to describe the movement of the planets in relation to the stars as they are observed from Earth. Like the sun, the planets appear to rise in the East and set in the West. When a planet travels eastward in relation to the stars, it is called "prograde". When the planet travels westward in relation to the stars (opposite path) it is called "retrograde".
Apparent motion.
From Earth.
When we observe the sky, the Sun, Moon, and stars appear to move from east to west because of the rotation of Earth (so-called diurnal motion). However, orbiters such as the Space Shuttle and many artificial satellites appear to move from west to east. These are direct satellites (they actually orbit Earth in the same direction as the Moon), but they orbit Earth faster than Earth itself rotates, and so appear to move in the opposite direction of the Moon. Mars has a natural satellite Phobos, with a similar orbit. From the surface of Mars it appears to move in the opposite direction because its orbital period is less than a Martian day. There are also smaller numbers of truly retrograde artificial satellites orbiting Earth which counter-intuitively appear to move westward, in the same direction as the Moon.
As seen from Earth, all the other planets, asteroids and all objects in the Solar System appear to periodically switch direction as they cross the sky. Though all stars and planets appear to move from east to west on a nightly basis in response to the rotation of Earth, the outer planets generally drift slowly eastward relative to the stars. Asteroids and Kuiper Belt Objects (including Pluto) exhibit apparent retrogradation. This motion is normal for the planets, and so is considered direct motion. However, since Earth completes its orbit in a shorter period of time than the planets outside its orbit, it periodically overtakes them, like a faster car on a multi-lane highway. When this occurs, the planet being passed will first appear to stop its eastward drift, and then drift back toward the west. Then, as Earth swings past the planet in its orbit, it appears to resume its normal motion west to east. Inner planets Venus and Mercury appear to move in retrograde in a similar mechanism, but as they can never be in opposition to the Sun as seen from Earth, their retrograde cycles are tied to their inferior conjunctions with the Sun. They are unobservable in the Sun's glare and in their "new" phase, with mostly their dark sides toward Earth; they occur in the transition from morning star to evening star.
The more distant planets retrograde more frequently, as they don't move as far in their orbits while Earth completes an orbit itself. The center of the retrograde motion occurs when the body is exactly opposite the sun, and therefore high in the ecliptic at local midnight. The retrogradation of a hypothetical extremely distant (and nearly non-moving) planet would take place during a half-year, with the planet's apparent yearly motion being reduced to a parallax ellipse.
The period between the center of such retrogradations is the synodic period of the planet.
This apparent retrogradation puzzled ancient astronomers, and was one reason they named these bodies 'planets' in the first place: 'Planet' comes from the Greek word for 'wanderer'. In the geocentric model of the Solar System proposed by Apollonius in the third century BCE, retrograde motion was explained by having the planets travel in deferents and epicycles. It was not understood to be an illusion until the time of Copernicus, although the Greek astronomer Aristarchus in 240 BCE proposed a heliocentric model for the Solar System.
Interestingly, Galileo's drawings show that he first observed Neptune on December 28, 1612, and again on January 27, 1613. On both occasions, Galileo mistook Neptune for a fixed star when it appeared very close—in conjunction—to Jupiter in the night sky, hence, he is not credited with Neptune's discovery. During the period of his first observation in December 1612, Neptune was stationary in the sky because it had just turned retrograde that very day. Since Neptune was only beginning its yearly retrograde cycle, the motion of the planet was far too slight to be detected with Galileo's small telescope.
From Mercury.
At specific points on Mercury's surface, an observer would be able to see the Sun rise part way, then reverse and set before rising again, all within the same Mercurian day. This apparent retrograde motion of the Sun occurs because, from approximately four Earth days before perihelion until approximately four Earth days after it, Mercury's angular orbital speed exceeds its angular rotational velocity. Mercury's elliptical orbit is farther from circular than that of any other planet in the Solar System, resulting in a substantially higher orbital speed near perihelion.

</doc>
<doc id="48909" url="https://en.wikipedia.org/wiki?curid=48909" title="Zenith">
Zenith

Zenith (, ) refers to an imaginary point directly "above" a particular location, on the imaginary celestial sphere. "Above" means in the vertical direction opposite to the apparent gravitational force at that location. The opposite direction, i.e. the direction in which gravity pulls, is toward the nadir. The zenith is the "highest" point on the celestial sphere (meaning it is the farthest up from the gravitational force). Zenith is sometimes used as a given name, most commonly for males. Its meaning, "highest point," evokes success and power.
Origin.
The word "zenith" derives from an inaccurate reading of the Arabic expression (), meaning "direction of the head" or "path above the head", by Medieval Latin scribes in the Middle Ages (during the 14th century), possibly through Old Spanish. It was reduced to 'samt' ("direction") and miswritten as 'senit'/'cenit', as the "m" was misread as an "ni". Through the Old French 'cenith', 'zenith' first appeared in the 17th century.
Relevance and use.
The term "zenith" is sometimes used to refer to the highest point, way or level reached by a celestial body during its apparent orbit around a given point of observation. This sense of the word is often used to describe the location of the Sun ("The sun reached its zenith..."), but to an astronomer the sun does not have its own zenith, and is at the zenith only if it is directly overhead.
In a scientific context, the zenith is the direction of reference for measuring the zenith angle, the angle between a direction of interest (e.g., a star) and the local zenith.
In astronomy, the altitude in the horizontal coordinate system and the zenith angle are complementary angles, with the horizon perpendicular to the zenith. The astronomical meridian is also determined by the zenith, and is defined as a circle on the celestial sphere that passes through the zenith, nadir, and the celestial poles. A zenith telescope is a type of telescope designed to point straight up at or near the zenith, and used for precision measurement of star positions, to simplify telescope construction, or both. The NASA Orbital Debris Observatory and the Large Zenith Telescope are both zenith telescopes since the use of liquid mirrors meant these telescopes could only point straight up.

</doc>
<doc id="48910" url="https://en.wikipedia.org/wiki?curid=48910" title="Horizon">
Horizon

The horizon or skyline is the apparent line that separates earth from sky, the line that divides all visible directions into two categories: those that intersect the Earth's surface, and those that do not. At many locations, the true horizon is obscured by trees, buildings, mountains, etc., and the resulting intersection of earth and sky is called the "visible horizon". When looking at a sea from a shore, the part of the sea closest to the horizon is called the "offing".
The word "horizon" derives from the Greek "ὁρίζων κύκλος" "horizōn kyklos", "separating circle", from the verb ὁρίζω "horizō", "to divide", "to separate", and that from "ὅρος" ("oros"), "boundary, landmark".
Appearance and usage.
Historically, the distance to the visible horizon at sea has been extremely important as it represented the maximum range of communication and vision before the development of the radio and the telegraph. Even today, when flying an aircraft under Visual Flight Rules, a technique called attitude flying is used to control the aircraft, where the pilot uses the visual relationship between the aircraft's nose and the horizon to control the aircraft. A pilot can also retain his or her spatial orientation by referring to the horizon.
In many contexts, especially perspective drawing, the curvature of the Earth is disregarded and the horizon is considered the theoretical line to which points on any horizontal plane converge (when projected onto the picture plane) as their distance from the observer increases. For observers near sea level the difference between this "geometrical horizon" (which assumes a perfectly flat, infinite ground plane) and the "true horizon" (which assumes a spherical Earth surface) is imperceptible to the naked eye (but for someone on a 1000-meter hill looking out to sea the true horizon will be about a degree below a horizontal line).
In astronomy the horizon is the horizontal plane through (the eyes of) the observer. It is the fundamental plane of the horizontal coordinate system, the locus of points that have an altitude of zero degrees. While similar in ways to the geometrical horizon, in this context a horizon may be considered to be a plane in space, rather than a line on a picture plane.
Distance to the horizon.
Please note: We typically see further along the Earths curved surface than a simple geometric calculation allows for because of the refraction error. If the ground, or water surface, is colder then the air above it, a cold, dense layer of air forms close to the surface, causing light to be refracted downward as it travels, and therefore, to some extent, to go around the curvature of the Earth. The reverse happens if the ground is hotter than the air above it, as often happens in deserts, producing mirages. As an approximate compensation for refraction, surveyors measuring longer distances than 300 feet subtract 14% from the calculated curvature error and ensure lines of sight are at least 5 feet from the ground to reduce random errors created by refraction. 
However, ignoring the effect of atmospheric refraction, distance to the horizon from an observer close to the Earth's surface is about
where "d" is in kilometres and "h" is height above ground level in metres.
Examples:
With "d" in miles
and "h" in feet,
Examples, assuming no refraction:
Geometrical model.
If the Earth is assumed to be a sphere with no atmosphere then the distance to the horizon can easily be calculated.
The secant-tangent theorem states that
Make the following substitutions:
The formula now becomes
or
where "R" is the radius of the Earth.
The equation can also be derived using the Pythagorean theorem.
Since the line of sight is a tangent to the Earth, it is perpendicular to the radius at the horizon. This sets up a right triangle, with the sum of the radius and the height as the hypotenuse. With
referring to the second figure at the right leads to the following:
Another relationship involves the distance "s" along the curved surface of the Earth to the horizon; with "γ" in radians,
then
Solving for "s" gives
The distance "s" can also be expressed in terms of the line-of-sight distance "d"; from the second figure at the right,
substituting for "γ" and rearranging gives
The distances "d" and "s" are nearly the same when the height of the object is negligible compared to the radius (that is, "h" ≪ "R").
Approximate geometrical formulas.
If the observer is close to the surface of the earth, then it is valid to disregard "h" in the term , and the formula becomes
Using kilometres for "d" and "R", and metres for "h", and taking the radius of the Earth as 6371 km, the distance to the horizon is
Using imperial units, with "d" and "R" in statute miles (as commonly used on land), and "h" in feet, the distance to the horizon is
If "d" is in nautical miles, and "h" in feet, the constant factor is about 1.06, which is close enough to 1 that it is often ignored, giving:
These formulas may be used when "h" is much smaller than the radius of the Earth (6371 km or 3959 mi), including all views from any mountaintops, airplanes, or high-altitude balloons. With the constants as given, both the metric and imperial formulas are precise to within 1% (see the next section for how to obtain greater precision).
Exact formula for a spherical Earth.
If "h" is significant with respect to "R", as with most satellites, then the approximation made previously is no longer valid, and the exact formula is required:
where "R" is the radius of the Earth ("R" and "h" must be in the same units). For example,
if a satellite is at a height of 2000 km, the distance to the horizon is ;
neglecting the second term in parentheses would give a distance of , a 7% error.
Objects above the horizon.
To compute the greatest distance at which an observer can see the top of an object above the horizon, compute the distance to the horizon for a hypothetical observer on top of that object, and add it to the real observer's distance to the horizon. For example, for an observer with a height of 1.70 m standing on the ground, the horizon is 4.65 km away. For a tower with a height of 100 m, the horizon distance is 35.7 km. Thus an observer on a beach can see the top of the tower as long as it is not more than 40.35 km away. Conversely, if an observer on a boat () can just see the tops of trees on a nearby shore (), the trees are probably about 16 km away.
Referring to the figure at the right, the top of the lighthouse will be visible to a lookout in a crow's nest at the top of a mast of the boat if
where "D"BL is in kilometres and "h"B and "h"L are in metres.
As another example, suppose an observer, whose eyes are two metres above the level ground, uses binoculars to look at a distant building which he knows to consist of thirty storeys, each 3.5 metres high. He counts the storeys he can see, and finds there are only ten. So twenty storeys or 70 metres of the building are hidden from him by the curvature of the Earth. From this, he can calculate his distance from the building:
which comes to about 35 kilometres.
It is similarly possible to calculate how much of a distant object is visible above the horizon. Suppose an observer's eye is 10 metres above sea level, and he is watching a ship that is 20 km away. His horizon is:
kilometres from him, which comes to about 11.3 kilometres away. The ship is a further 8.7 km away. The height of a point on the ship that is just visible to the observer is given by:
which comes to almost exactly six metres. The observer can therefore see that part of the ship that is more than six metres above the level of the water. The part of the ship that is below this height is hidden from him by the curvature of the Earth. In this situation, the ship is said to be hull-down.
Effect of atmospheric refraction.
If the Earth were an airless world like the Moon, the above calculations would be accurate. However, Earth has an atmosphere of air, whose density and refractive index vary considerably depending on the temperature and pressure. This makes the air refract light to varying extents, affecting the appearance of the horizon. Usually, the density of the air just above the surface of the Earth is greater than its density at greater altitudes. This makes its refractive index greater near the surface than higher, which causes light that is travelling roughly horizontally to be refracted downward. This makes the actual distance to the horizon greater than the distance calculated with geometrical formulas. With standard atmospheric conditions, the difference is about 8%. This changes the factor of 3.57, in the metric formulas used above, to about 3.86. This correction can be, and often is, applied as a fairly good approximation when conditions are close to standard. When conditions are unusual, this approximation fails. Refraction is strongly affected by temperature gradients, which can vary considerably from day to day, especially over water. In extreme cases, usually in springtime, when warm air overlies cold water, refraction can allow light to follow the Earth's surface for hundreds of kilometres. Opposite conditions occur, for example, in deserts, where the surface is very hot, so hot, low-density air is below cooler air. This causes light to be refracted upward, causing mirage effects that make the concept of the horizon somewhat meaningless. Calculated values for the effects of refraction under unusual conditions are therefore only approximate. Nevertheless, attempts have been made to calculate them more accurately than the simple approximation described above.
Outside the visual wavelength range, refraction will be different. For radar (e.g. for wavelengths 300 to 3 mm i.e. frequencies between 1 and 100 GHz) the radius of the Earth may be multiplied by 4/3 to obtain an effective radius giving a factor of 4.12 in the metric formula i.e. the radar horizon will be 15% beyond the geometrical horizon or 7% beyond the visual. The 4/3 factor is not exact, as in the visual case the refraction depends on atmospheric conditions.
Integration method—Sweer
If the density profile of the atmosphere is known, the distance "d" to the horizon is given by
where "R"E is the radius of the Earth, "ψ" is the dip of the horizon and "δ" is the refraction of the horizon. The dip is determined fairly simply from
where "h" is the observer's height above the Earth, "μ" is the index of refraction of air at the observer's height, and "μ"0 is the index of refraction of air at Earth's surface.
The refraction must be found by integration of
where formula_26 is the angle between the ray and a line through the center of the Earth. The angles "ψ" and formula_26 are related by
Simple method—Young
A much simpler approach, which produces essentially the same results as the first-order approximation described above, uses the geometrical model but uses a radius . The distance to the horizon is then
Taking the radius of the Earth as 6371 km, with "d" in km and "h" in m,
with "d" in mi and "h" in ft,
Results from Young's method are quite close to those from Sweer's method, and are sufficiently accurate for many purposes.
Curvature of the horizon.
From a point above the surface the horizon appears slightly bent (it is a circle). There is a basic geometrical relationship between this visual curvature formula_32, the altitude and the Earth's radius. It is
The curvature is the reciprocal of the curvature angular radius in radians. A curvature of 1 appears as a circle of an angular radius of 45° corresponding to an altitude of approximately 2640 km above the Earth's surface. At an altitude of 10 km (33,000 ft, the typical cruising altitude of an airliner) the mathematical curvature of the horizon is about 0.056, the same curvature of the rim of circle with a radius of 10 m that is viewed from 56 cm directly above the center of the circle. However, the apparent curvature is less than that due to refraction of light in the atmosphere and because the horizon is often masked by high cloud layers that reduce the altitude above the visual surface.
The Horizon curves by: sqrt(radius^2 + distance^2)-radius, equivalent to distance^2/R*2. At 100 km, it descends 784m.
Vanishing points.
The horizon is a key feature of the picture plane in the science of graphical perspective. Assuming the picture plane stands vertical to ground, and "P" is the perpendicular projection of the eye point "O" on the picture plane, the horizon is defined as the horizontal line through "P". The point "P" is the vanishing point of lines perpendicular to the picture. If "S" is another point on the horizon, then it is the vanishing point for all lines parallel to "OS". But Brook Taylor (1719) indicated that the horizon plane determined by "O" and the horizon was like any other plane:
The peculiar geometry of perspective where parallel lines converge in the distance, stimulated the development of projective geometry which posits a point at infinity where parallel lines meet. In her 2007 book "Geometry of an Art", Kirsti Andersen described the evolution of perspective drawing and science up to 1800, noting that vanishing points need not be on the horizon. In a chapter titled "Horizon", John Stillwell recounted how projective geometry has led to incidence geometry, the modern abstract study of line intersection. Stillwell also ventured into foundations of mathematics in a section titled "What are the Laws of Algebra ?" The "algebra of points", originally given by Karl von Staudt deriving the axioms of a field was deconstructed in the twentieth century, yielding a wide variety of mathematical possibilities. Stillwell states

</doc>
<doc id="48911" url="https://en.wikipedia.org/wiki?curid=48911" title="Prince regent">
Prince regent

A prince regent, or prince-regent, is a prince who rules a monarchy as regent instead of a monarch, e.g., as a result of the Sovereign's incapacity (minority or illness) or absence (remoteness, such as exile or long voyage, or simply no incumbent). 
While the term itself can have the generic meaning and refer to any prince who fills the role of regent, historically it has mainly been used to describe a small number of individual Princes who were Regents.
Prince Regent in the United Kingdom.
In the English language the title "Prince Regent" is most commonly associated with George IV, who held the style "HRH" The Prince Regent during the incapacity, by dint of mental illness, of his father, George III (see Regent for other regents). Regent's Park, Regent Street and Regent's Canal (which he commissioned) in London were all named in honour of him.
This period is known as the British Regency, or just the Regency.
The title was conferred by the Regency Act on February 5, 1811. Subject to certain limitations for a period, the Prince Regent was able to exercise the full powers of the King. The precedent of the Regency Crisis of 1788 (from which George III recovered before it was necessary to appoint a Regent) was followed. The Prince of Wales continued as regent until his father's death in 1820, when he became George IV.
Prince Regent in Germany.
In Germany, the title "Prinzregent" (literally prince regent) is most commonly associated with Prince Luitpold of Bavaria, who served as regent for two of his nephews, King Ludwig II of Bavaria, who was declared mentally incompetent in 1886, and King Otto of Bavaria (who had been declared insane in 1875) from 1886 until 1912.
The years of Luitpold's regency were marked by tremendous artistic and cultural activity in Bavaria, where they are known after the regencies as the "Prinzregentenjahre" or the "Prinzregentenzeit". Numerous streets in Bavarian cities and towns are called "Prinzregentenstraße". Many institutions are named in Luitpold's honour, "e.g.", the "Prinzregententheater" in Munich. "Prinzregententorte" is a multi-layered cake with chocolate butter cream named in Luitpold's honour.
At Luitpold's death in 1912, his son Prince Ludwig succeeded as Prince Regent. Ludwig held the title for less than a year, since the Bavarian Legislature decided to recognise him as king.
Prince Regent in Bulgaria.
Prince Kiril of Bulgaria was appointed head of a regency council by the Bulgarian parliament following the death of his brother, Tsar Boris III on 28 August 1943, to act as Head of State until the late Tsar's son and successor, Tsar Simeon II, reached the age of 18 years. On 5 September 1944 the Soviet Union declared war on the Kingdom of Bulgaria and on the 8th - Soviet armies crossed the Romanian border and occupied the country. On 1 February 1945 the prince regent Kyril, and the two other former regents - Professor Bogdan Filov and General Nikola Mikhov, as well as a range of former cabinet ministers, royal advisors and 67 MPs were executed.
Prince Lieutenant in Luxembourg.
The heir-apparent or heir-presumptive to the grand duke of Luxembourg may be titled "prince-lieutenant" ('prince deputy') during a period in which the incumbent remains formally on the grand ducal throne, but (progressively, most) functions of the crown are performed by the 'monarch apprentice', as prince Jean did 4 May 1961 – 12 November 1964 in the last years of his mother Charlotte's reign until she abdicated and he succeeded to the grand ducal throne (she lived until 1985), and Jean's own son prince Henri 3 March 1998 – 7 October 2000 until his father abdicated and he succeeded.
Queen regent.
It has also been known throughout history that when a king is unable to reign or is out of the country for long periods of time, sometimes the consort will step up and will temporarily do the duties of a regent. In the Kingdom of Swaziland, queen mothers have temporarily stepped in when the sovereign was either a minor or unable to reign for other reasons.

</doc>
<doc id="48913" url="https://en.wikipedia.org/wiki?curid=48913" title="Caroline of Brunswick">
Caroline of Brunswick

Caroline Amelia Elizabeth of Brunswick-Wolfenbüttel (17 May 1768 – 7 August 1821), best known as Caroline of Brunswick, was Queen of the United Kingdom as the wife of King George IV from 29 January 1820 until her death in 1821. She was the Princess of Wales from 1795 to 1820.
Her father was the ruler of Brunswick-Wolfenbüttel in Germany, and her mother, Princess Augusta, was the sister of George III. In 1794, she was engaged to her first-cousin and George III's eldest son and heir George, Prince of Wales, although they had never met and George was already married illegally to Maria Fitzherbert. George and Caroline married the following year, and nine months later Caroline had a child, Princess Charlotte of Wales.
Shortly after Charlotte's birth, George and Caroline separated. By 1806, rumours that Caroline had taken lovers and had an illegitimate child led to an investigation into her private life. The dignitaries who led the investigation concluded that there was "no foundation" to the rumours, but Caroline's access to her daughter was restricted.
In 1814, Caroline moved to Italy, where she employed Bartolomeo Pergami as a servant. Pergami soon became Caroline's closest companion, and it was widely assumed that they were lovers. In 1817, Caroline was devastated when her daughter Charlotte died in childbirth; she heard the news from a passing courier as George had refused to write and tell her. He was determined to divorce Caroline, and set up a second investigation to collect evidence of her adultery.
In 1820, George became king of the United Kingdom and Hanover. George hated her, vowed she would never be the queen, and insisted on a divorce, which she refused. A legal divorce was possible but difficult to obtain. Caroline returned to Britain to assert her position as queen. She was wildly popular and the new king was despised for his immoral behaviour. On the basis of the evidence collected against her, George attempted to divorce her by introducing the Pains and Penalties Bill to Parliament, but George and the bill were so unpopular, and Caroline so popular with the masses, that it was withdrawn by the Tory government. In July 1821, Caroline was barred from the coronation on the orders of her husband. She fell ill in London and died three weeks later; her funeral procession passed through London on its way to her native Brunswick, where she was buried.
Early life.
Caroline was born as Princess of Brunswick, with the courtesy title of Duchess of Brunswick-Wolfenbüttel on 17 May 1768 at Braunschweig (known in English as "Brunswick") in Germany. She was the daughter of Charles William, Duke of Brunswick-Wolfenbüttel, and his wife Princess Augusta of Great Britain, eldest sister of George III.
John Stanley, later Lord Stanley of Alderley, saw her in 1781, and noted that she was an attractive girl with curly, fair hair. Caroline could understand English and French, but her father admitted that she was lacking in education. According to Caroline's mother, who was British, all German princesses learned English in the hope that they would be chosen to marry George, Prince of Wales, George III's eldest son and heir apparent and Caroline's first cousin.
Engagement.
In 1794, Caroline and the Prince of Wales were engaged. They had never met—George had agreed to marry her because he was heavily in debt, and if he contracted a marriage with an eligible princess, Parliament would increase his allowance. Caroline seemed eminently suitable: she was a Protestant of royal birth, and the marriage would ally Brunswick and Britain. Although Brunswick was only a small country, Britain was at war with revolutionary France so eager to obtain allies on the European mainland. Brunswick was ruled by the esteemed soldier Charles William Ferdinand, Duke of Brunswick, who himself had married Princess Augusta, the sister of George III. On 20 November 1794, Lord Malmesbury arrived at Brunswick to escort Caroline to her new life in Britain. In his diary, Malmesbury recorded his reservations about Caroline's suitability as a bride for the prince: she lacked judgement, decorum and tact, spoke her mind too readily, acted indiscreetly, and often neglected to wash, or change her dirty clothes. He went on to say that she had "some natural but no acquired morality, and no strong innate notions of its value and necessity." However, Malmesbury was impressed by her bravery; on the journey to England, the party heard cannon fire, as they were not far from the French lines. While Caroline's mother, who was accompanying them to the coast as chaperone, was concerned for their safety, Caroline was unfazed.
On 28 March 1795, Caroline and Malmesbury left Cuxhaven in the "Jupiter". Delayed by poor weather, they landed a week later, on Easter Sunday, 5 April, at Greenwich. There, she met Frances Villiers, Countess of Jersey, George's mistress, who had been appointed Caroline's Lady of the Bedchamber. Smith concludes that:
On meeting his future wife for the first time, George called for a glass of brandy. He was evidently disappointed. Similarly, Caroline told Malmesbury, "Prince is very fat and he's nothing like as handsome as his portrait." At dinner that evening, the Prince was appalled by Caroline's garrulous nature and her jibes at the expense of Lady Jersey. She was upset and disappointed by George's obvious partiality for Lady Jersey over her.
Troubled marriage.
Caroline and George were married on 8 April 1795 at the Chapel Royal, St. James's Palace, in London. At the ceremony, George was drunk. He regarded Caroline as unattractive and unhygienic, and told Malmesbury that he suspected that she was not a virgin when they married. He, of course, was not. He had himself already secretly married Maria Fitzherbert; however, his marriage to Fitzherbert violated the Royal Marriages Act 1772 and thus was not legally valid.
In a letter to a friend, the prince claimed that the couple only had sexual intercourse three times: twice the first night of the marriage, and once the second night. He wrote, "it required no small to conquer my aversion and overcome the disgust of her person." Caroline claimed George was so drunk that he "passed the greatest part of his bridal night under the grate, where he fell, and where I left him".
Nine months after the wedding, Caroline gave birth to Princess Charlotte Augusta, George's only legitimate child, at Carlton House on 7 January 1796. Charlotte was second in the line of succession to the British throne after her father. Just three days after Charlotte's birth, George made out a new will. He left all his property to "Maria Fitzherbert, my wife", while to Caroline he left one shilling.
Gossip about Caroline and George's troubled marriage was already circulating. The newspapers claimed that Lady Jersey opened, read and distributed the contents of Caroline's private letters. She despised Lady Jersey and could not visit or travel anywhere without George's permission. The press vilified George for his extravagance and luxury at a time of war and portrayed Caroline as a wronged wife. She was cheered in public and gained plaudits for her "winning familiarity" and easy, open nature. George was dismayed at her popularity and his own unpopularity, and felt trapped in a loveless marriage with a woman he loathed. He wanted a separation.
In April 1796, George wrote to Caroline, "We have unfortunately been oblig'd to acknowledge to each other that we cannot find happiness in our union. ... Let me therefore beg you to make the best of a situation unfortunate for us both." In June, Lady Jersey resigned as Caroline's Lady of the Bedchamber. George and Caroline were already living separately, and in August 1797 Caroline moved to a private residence: The Vicarage or Old Rectory in Charlton, London. Later, she moved to Montagu House in Blackheath. No longer constrained by her husband, or, according to rumour, her marital vows, she entertained whomever she pleased. She flirted with Admiral Sir Sidney Smith and Captain Thomas Manby, and may have had a fling with the politician George Canning.
Her daughter Charlotte was placed in the care of a governess, in a mansion near Montagu House in the summers, and Caroline visited her often. It seems that a single daughter was not sufficient to sate Caroline's maternal instincts, and she adopted eight or nine poor children who were fostered out to people in the district. In 1802, she adopted a three-month-old boy, William Austin, and took him into her home. By 1805, Caroline had fallen out with her near neighbours, Sir John and Lady Douglas, who claimed that Caroline had sent them obscene and harassing letters. Lady Douglas accused Caroline of infidelity, and alleged that William Austin was Caroline's illegitimate son.
Delicate Investigation.
In 1806, a secret commission was set up, known as the "Delicate Investigation", to examine Lady Douglas's claims. The commission comprised four of the most eminent men in the country: Prime Minister Lord Grenville, the Lord Chancellor Lord Erskine, the Lord Chief Justice of England and Wales Lord Ellenborough and the Home Secretary Lord Spencer. Lady Douglas testified that Caroline herself had admitted to her in 1802 that she was pregnant, and that Austin was her son. She further alleged that Caroline had been rude about the royal family, touched her in an inappropriately sexual way, and had admitted that any woman friendly with a man was sure to become his lover. In addition to Smith, Manby and Canning, artist Thomas Lawrence and Henry Hood (the son of Lord Hood) were also mentioned as potential paramours. Caroline's servants could or would not confirm that these gentlemen were her lovers, nor that she had been pregnant, and said that the child had been brought to Caroline's house by his true mother, Sophia Austin. Sophia was summoned before the commissioners, and testified that the child was hers.
The commissioners decided that there was "no foundation" for the allegations, but despite being a supposedly secret investigation, it proved impossible to prevent gossip from spreading, and news of the investigation leaked to the press. Caroline's conduct with her gentlemen friends was considered improper, but there was no direct proof that she had been guilty of anything more than flirtation. Perhaps Caroline had told Lady Douglas that she was pregnant out of frustrated maternal desire, or as part of a foolish prank that, unfortunately for her, backfired. Later in the year, Caroline received further bad news as Brunswick was overrun by the French, and her father was killed in the battle of Jena-Auerstadt. Her mother and brother, Frederick William, Duke of Brunswick-Wolfenbüttel, fled to England. Caroline had wanted to return to Brunswick and leave Britain behind her, but with much of Europe controlled by the French she had no safe haven to run to.
During the Delicate Investigation, Caroline was not permitted to see her daughter, and afterwards her visits were essentially restricted to once a week and only in the presence of Caroline's own mother, the Dowager Duchess of Brunswick. Meetings took place at either Blackheath or an apartment in Kensington Palace designated for Caroline's use.
Social isolation.
By the end of 1811, King George III had become permanently insane, and the Prince of Wales was appointed as Regent. He restricted Caroline's access to Princess Charlotte further, and Caroline became more socially isolated as members of high society chose to patronise George's extravagant parties rather than hers. She moved her London residence to Connaught House in Bayswater. Caroline needed a powerful ally to help her oppose George's increasing ability to prevent her from seeing her daughter. In league with Henry Brougham, an ambitious Whig politician who favoured reform, she began a propaganda campaign against George. George countered by leaking Lady Douglas's testimony from the "Delicate Investigation", which Brougham repudiated by leaking the testimonies of the servants and Mrs Austin. Charlotte favoured her mother's point of view, as did most of the public. Jane Austen wrote of Caroline: "Poor woman, I shall support her as long as I can, because she "is" a Woman and because I hate her Husband."
In 1814, after Napoleon's defeat, nobility from throughout Europe attended celebrations in London, but Caroline was excluded. George's relationship with his daughter was also deteriorating, as Charlotte sought greater freedom from her father's strictures. On 12 July, he informed Charlotte that she would henceforth be confined at Cranbourne Lodge, Windsor, that her household would be replaced, and that she could have no visitors except her grandmother, Queen Charlotte, once a week. Horrified, Charlotte ran away to her mother's house in Bayswater. After an anxious night, Charlotte was eventually persuaded to return to her father by Brougham, since legally she could be placed in her father's care and there was a danger of public disorder against George, which might prejudice Charlotte's position if she continued to disobey him.
Caroline, unhappy at her situation and treatment in Britain, negotiated a deal with the Foreign Secretary, Lord Castlereagh. She agreed to leave the country in exchange for an annual allowance of £35,000. Both Brougham and Charlotte were dismayed by Caroline's decision, as they both realised that Caroline's absence would strengthen George's power and weaken theirs. On 8 August 1814, Caroline left Britain.
Exile.
After a two-week visit to Brunswick, Caroline headed for Italy through Switzerland. Along the way, possibly in Milan, she hired Bartolomeo Pergami as a servant. Pergami soon rose to the head of Caroline's household, and managed to get his sister, Angelica, Countess of Oldi, appointed as Caroline's lady-in-waiting. In mid-1815, Caroline bought a house, Villa d'Este, on the shores of Lake Como, even though her finances were stretched.
From early 1816, she and Pergami went on a cruise around the Mediterranean, visiting Napoleon's former palace on Elba, and Sicily, where Pergami obtained the Order of Malta and a barony. By this time, Caroline and Pergami were eating their meals together openly, and it was widely rumoured that they were lovers. They visited Tunis, Malta, Milos, Athens, Corinth, Constantinople, and Nazareth. Caroline entered Jerusalem riding on a donkey in a convoy of camels. Pergami was made a Knight of the Order of Jerusalem. Caroline instituted the Order of St Caroline, nominating Pergami its Grand Master. In August, they returned to Italy, stopping at Rome to visit the Pope.
By this time, gossip about Caroline was everywhere. Lord Byron wrote to his publisher that Caroline and Pergami were lovers, and Baron Friedrich Ompteda, a Hanoverian spy, bribed one of Caroline's servants so that he could search her bedroom for proof of adultery. He found none. By August 1817, Caroline's debts were growing, so she sold Villa d'Este and moved to the smaller Villa Caprile near Pesaro. Pergami's mother, brother and daughter, but not his wife, joined Caroline's household.
The previous year, Caroline's daughter, Princess Charlotte, had married Prince Leopold of Saxe-Coburg-Saalfeld, and the future of the British monarchy looked bright. Then tragedy struck: in November 1817, Charlotte died after giving birth to her only child, a stillborn son. For the most part, Charlotte had been immensely popular with the public, and her death was a blow to the country. George refused to write to Caroline to inform her, leaving it for their son-in-law Leopold to do, but Leopold was deep in grief and delayed writing. George did, however, write to the pope of the tragedy, and by chance the courier carrying the letter passed by Pesaro, and so it was that Caroline heard the devastating news. Caroline had lost her daughter, but she had also lost any chance of regaining position through the succession of her daughter to the throne.
George was determined to press ahead with a divorce and set up a commission chaired by the Vice-Chancellor John Leach to gather evidence of Caroline's adultery. Leach sent three commissioners to Milan to interrogate Caroline's former servants, including Theodore Majocchi and Caroline's maid, Louise Demont. In London, Brougham was still acting as Caroline's agent. Concerned that the "Milan commission" might threaten Caroline, he sent his brother James to Caroline's villa in the hope of establishing whether George had any grounds for divorce. James wrote back to his brother of Caroline and Pergami, "they are to all appearances man and wife, never was anything so obvious." The Milan commission was assembling more and more evidence, and by 1819 Caroline was worried. She informed James Brougham that she would agree to a divorce in exchange for money. However, at this time in England divorce by mutual consent was illegal; it was only possible to divorce if one of the partners admitted or was found guilty of adultery. Caroline said it was "impossible" for her to admit that, so the Broughams advised that only formal separation was possible. Both keen to avoid publicity, the Broughams and the Government discussed a deal where Caroline would be called by a lesser title, such as "Duchess of Cornwall" rather than "Princess of Wales". As the negotiations continued at the end of 1819, Caroline travelled to France, which gave rise to speculation that she was on her way back to England. In January 1820, however, she made plans to return to Italy, but then on 29 January 1820 King George III died. Caroline's husband became king and, at least nominally, she was queen of the United Kingdom.
Queen consort.
Instead of being treated like a queen, Caroline found that her estranged husband's accession paradoxically made her position worse. On visiting Rome, the pope refused her an audience, and the pope's minister Cardinal Consalvi insisted that she be greeted only as a duchess of Brunswick, and not as a queen. In an attempt to assert her rights, she made plans to return to Britain. The King demanded that his ministers get rid of her. He successfully persuaded them to remove her name from the liturgy of the Church of England, but they would not agree to a divorce because they feared the effect of a public trial. The government was weak and unpopular, and a trial detailing salacious details of both Caroline's and George's separate love lives was certain to destabilise it further. Rather than run the risk, the government entered into negotiations with Caroline, and offered her an increased annuity of £50,000 if she stayed abroad.
By the beginning of June, Caroline had travelled north from Italy, and was at St Omer near Calais. Acting on the advice of Alderman Matthew Wood and her lady-in-waiting Lady Anne Hamilton, she rejected the government's offer. She bid farewell to Pergami, and embarked for England. When she arrived on 5 June, riots broke out in support of her. Caroline was a figurehead for the growing Radical movement that demanded political reform and opposed the unpopular king. Nevertheless, the King still adamantly desired a divorce, and the following day, he submitted the evidence gathered by the Milan commission to Parliament in two green bags. On 15 June, the Guards in the King's Mews mutinied. The mutiny was contained, but the government was fearful of further unrest. Examination of the bags of evidence was delayed as Parliament debated the form of the investigation, but eventually, on 27 June, they were opened and examined in secret by 15 peers. The peers considered the contents scandalous, and a week later, after their report to the House, the government introduced a bill in Parliament, the Pains and Penalties Bill 1820, to strip Caroline of the title of queen consort and dissolve her marriage. It was claimed that Caroline had committed adultery with a low-born man: Bartolomeo Pergami. Various witnesses, such as Theodore Majocchi, were called during the reading of the bill, which was effectively a public trial of the Queen. The trial caused a sensation, as details of Caroline's familiarity with Pergami were revealed. Witnesses said the couple had slept in the same room, kissed, and been seen together in a state of undress. The bill passed the House of Lords, but was not submitted to the House of Commons as there was little prospect that the Commons would pass it. To her friends, Caroline joked that she had indeed committed adultery once—with the husband of Mrs. Fitzherbert, the King.
Even during the trial, the Queen remained immensely popular, as witnessed by over 800 petitions and nearly a million signatures that favoured her cause. As a figurehead of the opposition movement demanding reform, many revolutionary pronouncements were made in Caroline's name.
But with the end of the trial her alliance with the radicals came to an end. The government again extended the offer of £50,000 a year, this time without preconditions, and Caroline accepted.
Death.
Despite the King's best attempts, Caroline retained a strong popularity amongst the masses, and pressed ahead with plans to attend the coronation service on 19 July 1821 as queen. Lord Liverpool told Caroline that she should not go to the service, but she turned up anyway. George had Caroline turned away from the coronation at the doors of Westminster Abbey. Refused entry at both the doors to the East Cloister and the doors to the West Cloister, Caroline attempted to enter via Westminster Hall, where many guests were gathered before the service began. A witness described how the Queen stood at the door fuming as bayonets were held under her chin until the Deputy Lord Chamberlain had the doors slammed in her face. Caroline then proceeded back to an entrance near Poets' Corner, where she was met by Sir Robert Inglis, who held the office of "Gold Staff". Inglis persuaded the Queen to return to her carriage, and she left. Caroline lost support through her exhibition at the coronation; the crowds jeered her as she rode away, and even Brougham recorded his distaste at her undignified behaviour.
That night, Caroline fell ill and took a large dose of milk of magnesia and some drops of laudanum. Over the next three weeks she suffered more and more pain as her condition deteriorated. She realised she was nearing death and put her affairs in order. Her papers, letters, memoirs, and notebooks were burned. She wrote a new will, and settled her funeral arrangements: she was to be buried in her native Brunswick in a tomb bearing the inscription "Here lies Caroline, the Injured Queen of England". She died at Brandenburg House in Hammersmith at 10:25 p.m. on 7 August 1821 at the age of 53. Her physicians thought she had an intestinal obstruction, but she may have had cancer, and there were rumours at the time that she had been poisoned.
Afraid that a procession of the funeral bier through London could spark public unrest, Lord Liverpool decided the Queen's cortège would avoid the city, passing to the north on the way to Harwich and Brunswick. The crowd accompanying the procession was incensed and blocked the intended route with barricades to force a new route through Westminster and London. The scene soon descended into chaos; the soldiers forming the honour guard opened fire and rode through the crowd with drawn sabres. People in the crowd threw cobblestones and bricks at the soldiers, and two members of the public—Richard Honey, a carpenter, and George Francis, a bricklayer—were killed. Eventually, the Chief Metropolitan Magistrate, Sir Robert Baker, ordered that the official route be abandoned, and the cortège passed through the city. As a result, Baker was dismissed from office.
The final route (in heavy rain) took the following course: Hammersmith, Kensington (blocked), Kensington Gore (blocked), Hyde Park, Park Lane (blocked), return to Hyde Park where soldiers forced the gates open, Cumberland Gate (blocked), Edgware Road, Tottenham Court Road, Drury Lane, the Strand, and from there through the City of London, then by way of Romford, Chelmsford, and Colchester, to the seaport of Harwich.
The body was placed on a ship on 16 August and reached Brunswick on the 24th. Caroline was buried in Brunswick Cathedral on the 25th.
Legacy.
Historian Thomas Laqueur emphasizes that the sordid royal squabble captivated all Britons: 
The story of Caroline's marriage to George and her battle to be recognised as queen consort served as the basis for the 1996 BBC docudrama "A Royal Scandal" with Susan Lynch as Caroline and Richard E. Grant as George IV. The 2008 radio play "The People's Princess", with Alex Jennings as George IV and Rebecca Saire as Caroline, drew parallels with the marriage and divorce of Charles, Prince of Wales, and Diana, Princess of Wales. Caroline is the subject of Richard Condon's 1977 novel "The Abandoned Woman".
The final conflict between the newly ascended George IV and the soon-to-die Caroline is the subject of Bernard Bastable's novel "Dead, Mr. Mozart", where an alternate Wolfgang Mozart, who had a long life in England instead of a short life in Austria, is drawn against his will into a cover-up of information reflecting badly on all sides.
Titles, styles and arms.
Arms.
The royal coat of arms of the United Kingdom are impaled with her father's arms as Duke of Brunswick. The arms were Quarterly of twelve, 1st, Or, a semé of hearts Gules, a lion rampant Azure (Lüneburg); 2nd, Gules, two lions passant guardant Or (Brunswick); 3rd, Azure, a lion rampant Argent crowned Or (Eberstein); 4th, Gules a lion rampant Or, within a border componé Argent and Azure (Homburg); 5th, Or, a lion rampant Gules crowned Azure (Diepholz); 6th, Gules, a lion rampant Or (Lauterberg); 7th, Per fess, in chief Or, two bears' paws erect Sable (Hoya), in the base a gyronny, Argent and Azure (Old Bruckhausen); 8th, Azure, an eagle displayed Argent, langued, beaked and membered Gules (Diepholz eagle); 9th, Chequy Argent and Gules (Hohnstein); 10th, Argent, a stag's attire in bend Gules (Regenstein); 11th, Argent, a stag trippant Sable (Klettenburg); 12th, Argent, a stag's attire in bend sinister Sable (Blankenburg).
As Princess of Wales she used the arms of her husband (the royal arms with a label of three points Argent) impaled with those of her father, the whole surmounted by a coronet of the heir apparent.

</doc>
<doc id="48916" url="https://en.wikipedia.org/wiki?curid=48916" title="Model (people)">
Model (people)

A model (from Middle French "modelle") is a person with a role either to promote, display, or advertise commercial products (notably fashion clothing) or to serve as a visual aide for people who are creating works of art or to pose for photography.
Modelling ("modeling" in American English) is considered to be different from other types of public performance, such as acting or dancing. Although the difference between modelling and performing is not always clear, appearing in a film or a play is not generally considered to be "modelling".
Types of modelling include: fashion, glamour, fitness, bikini, fine art, body-part, and commercial print models. Models are featured in a variety of media formats including: books, magazines, films, newspapers, and TV. Fashion models are sometimes featured in films: ("Looker"), reality TV shows ("America's Next Top Model", "The Janice Dickinson Modeling Agency"), and music videos: ("Freedom! '90", "Wicked Game", "Daughters", and "Blurred Lines").
Celebrities, including actors, singers, sports personalities and reality TV stars, frequently take modelling contracts in addition to their regular work.
History.
Early years.
Modelling as a profession was first established in 1853 by Charles Frederick Worth, the "father of haute couture", when he asked his wife, Marie Vernet Worth, to model the clothes he designed. The term "house model" was coined to describe this type of work. Eventually, this became a common practice for Parisian fashion houses. There were no standard physical measurement requirements for a model, and most designers would use women of varying sizes to demonstrate variety in their designs.
With the development of fashion photography, the modelling profession expanded to photo modelling. Models remained fairly anonymous, and relatively poorly paid, until the late 1950s. One of the first well-known models was Lisa Fonssagrives, who was very popular in the 1930s. Fonssagrives appeared on over 200 "Vogue" covers, and her name recognition led to the importance of "Vogue" in shaping the careers of fashion models. In 1946, Ford Models was established by Eileen and Gerard Ford in New York; it is one of the oldest model agencies in the world. One of the most popular models during the 1940s was Jinx Falkenburg who was paid $25 per hour, a large sum at the time. During the 1940s and 1950s, Wilhelmina Cooper, Jean Patchett, Dovima, Dorian Leigh, Suzy Parker, Evelyn Tripp, Carmen Dell'Orefice, and Lisa Fonssagrives dominated fashion. Dorothea Church was among the first black models in the industry to gain notoriety in Paris. However, these models were unknown outside the fashion community. Compared to today's models, the models of the 1950s were more voluptuous. Wilhelmina Cooper's measurements were 38"-24"-36" whereas Chanel Iman's measurements are 32"-23"-33".
The 1960s and the beginning of the industry.
In the 1960s, the modelling world began to establish modelling agencies. Throughout Europe, secretarial services acted as models' agents charging them weekly rates for their messages and bookings. For the most part, models were responsible for their own billing. In Germany, agents were not allowed to work for a percentage of a person's earnings, so referred to themselves as secretaries. With the exception of a few models travelling to Paris or New York, travelling was relatively unheard of for a model. Most models only worked in one market due to different labor laws governing modelling in various countries. In the 1960s, Italy had many fashion houses and fashion magazines but was in dire need of models. Italian agencies would often coerce models to return to Italy without work visas by withholding their pay. They would also pay their models in cash, which models would have to hide from customs agents. It was not uncommon for models staying in hotels such as La Louisiana in Paris or the Arena in Milan to have their hotel rooms raided by the police looking for their work visas. It was rumoured that competing agencies were behind the raids. This led many agencies to form worldwide chains; for example, the Marilyn Agency has branches in Paris and New York.
By the late 1960s, London was considered the best market in Europe due to its more organised and innovative approach to modelling. It was during this period that models began to become household names. Models like: Jean Shrimpton, Joanna Lumley, Tania Mallet, Celia Hammond, Twiggy, Penelope Tree, and Pauline Stone dominated the London fashion scene and were well paid, unlike their predecessors. Twiggy became The Face of '66 at the age of 16. At this time, model agencies were not as restrictive about the models they represented, although it was uncommon for them to sign shorter models. Twiggy, who stood at with a 32" bust and had a boy's haircut, is credited with changing model ideals. At that time,the lady she earned £80 an hour when the average wage was £15 a week.
In 1967, seven of the top model agents in London formed the Association of London Model Agents. The formation of this association helped legitimize modelling and changed the fashion industry. Even with a more professional attitude towards modelling, models were still expected to have their hair and makeup done before they arrived at a shoot. Meanwhile, agencies took responsibility for a model's promotional materials and branding. That same year, former top fashion model Wilhelmina Cooper opened up her own fashion agency with her husband called Wilhelmina Models. By 1968, FM Agency and Models 1 were established and represented models in a similar way that agencies do today. By the late 1960s, models were treated better and were making better wages. One of the innovators, Ford Models, was the first agency to advance models money they were owed and would often allow teen models, who did not live locally, to reside in their house, a precursor to model housing.
The 1970s and 1980s.
The innovations of the 1960s flowed into the 1970s fashion scene. As a result of model industry associations and standards, model agencies became more business minded, and more thought went into a model's promotional materials. By this time, agencies were starting to pay for a model's publicity. In the early 1970s, Scandinavia had many tall, leggy, blonde-haired, blue-eyed models and not enough clients. It was during this time that Ford Models pioneered scouting. They would spend time working with agencies holding modelling contests. This was the precursor to the Ford Models Supermodel of the World competition which was established in 1980. Ford also focused their attentions on Brazil which had a wide array of seemingly "exotic" models, which eventually led to establishment of Ford Models Brazil. It was also during this time that the "Sports Illustrated Swimsuit Issue" debuted. The magazine set a trend by photographing "bigger and healthier" California models, and printing their names by their photos, thus turning many of them into household names and establishing the issue as a hallmark of supermodel status.
The 1970s marked numerous milestones in fashion. Beverly Johnson was the first African American to appear on the cover of U.S. "Vogue" in 1974. Models, including Grace Jones, Donyale Luna, Minah Bird, Naomi Sims, and Toukie Smith were some of the top black fashion models who paved the way for black women in fashion. In 1975, Margaux Hemingway landed a then-unprecedented million-dollar contract as the face of Fabergé's Babe perfume and the same year appeared on the cover of "Time" magazine, labelled one of the "New Beauties," giving further name recognition to fashion models.
Many of the world's most prominent modelling agencies were established in the 1970s and early 1980s. These agencies created the standard by which agencies now run. In 1974, Nevs Models was established in London with only a men's board, the first of its kind. Elite Models was founded in Paris in 1975 as well as Friday's Models in Japan. The next year Cal-Carries was established in Singapore, the first of a chain of agencies in Asia. In 1977, Select Model Management opened its doors as well as Why Not Models in Milan. By the 1980s, agencies such as Premier Model Management, Storm Models, Mikas, Marilyn, and Metropolitan Models had been established.
By the 1980s, most models were able to make modelling a full-time career. It was common for models to travel abroad and work throughout Europe. As modelling became global, numerous agencies began to think globally. In 1980, Ford Models, the innovator of scouting, introduced the Ford Models Supermodel of the World contest. That same year, John Casablancas opened Elite Models in New York. In 1981, cosmetics companies began contracting top models to lucrative endorsement deals. By 1983, Elite developed its own contest titled the Elite Model Look competition. In New York during the 1980s there were so-called "model wars" in which the Ford and Elite agencies fought over models and campaigns. Models were jumping back and forth between agencies such Elite, Wilhelmina, and Ford. In New York, the late 1980s trend was the boyish look in which models had short cropped hair and looked androgynous. In Europe, the trend was the exact opposite. During this time, a lot of American models who were considered more feminine looking moved abroad. By the mid-1980s, big hair was made popular by some musical groups, and the boyish look was out. The curvaceous models who had been popular in the 1950s and early 1970s were in style again. Models like Patti Hansen earned $200 an hour for print and $2,000 for television plus residuals. It was estimated that Hansen earned about $300,000 a year during the 1980s.
The 1990s to present.
The early 1990s were dominated by the supermodels of the late 1980s. In 1990, Linda Evangelista famously said to "Vogue", "we don't wake up for less than $10,000 a day". Evangelista and her contemporaries, Naomi Campbell, Cindy Crawford, Christy Turlington and Stephanie Seymour, became arguably the most recognizable models in the world, earning the moniker of "supermodel", and were boosted to global recognition and new heights of wealth for the industry. In 1991, Turlington signed a contract with Maybelline that paid her $800,000 for twelve days' work each year.
By the mid‑1990s, the new "heroin chic" movement became popular amongst New York and London editorial clients. While the heroin chic movement was inspired by model Jaime King, who suffered from a heroin addiction, it was Kate Moss who became its poster child through her ads for Calvin Klein. In spite of the heroin chic movement, model Claudia Schiffer earned $12 million. With the popularity of lingerie retailer Victoria's Secret, and the "Sports Illustrated Swimsuit Issue", there was a need for healthier-looking supermodels such as Tyra Banks and Heidi Klum to meet commercial modelling demand. The mid‑1990s also saw many Asian countries establishing modelling agencies.
By the late 1990s, the heroin chic era had run its course. Teen-inspired clothing infiltrated mainstream fashion, teen pop music was on the rise, and artists such as Britney Spears and Christina Aguilera popularized pleather and bare midriffs. As fashion changed to a more youthful demographic, the models who rose to fame had to be sexier for the digital age. Following Gisele Bundchen's breakthrough, a wave of Brazilian models including Adriana Lima, Alessandra Ambrosio, and Ana Beatriz Barros rose to fame on runways and became popular in commercial modelling throughout the 2000s. Some attribute this to decisions by magazines to replace models with celebrities their covers.
In the late 2000s, the Brazilians fell out of favour on the runways. Editorial clients were favouring models with a china-doll or alien look to them, such as Gemma Ward and Lily Cole. During the 2000s, Ford Models and NEXT Model Management were engaged in a legal battle, with each agency alleging that the other was stealing its models.
However, the biggest controversy of the 2000s was the health of high-fashion models participating in fashion week. While the health of models had been a concern since the 1970s, there were several high-profile news stories surrounding the deaths of young fashion models due to eating disorders and drug abuse. The British Fashion Council subsequently asked designers to sign a contract stating they would not use models under the age of sixteen. On March 3, 2012, "Vogue" banned models under the age of sixteen as well as models who appeared to have an eating disorder. Similarly, other countries placed bans on unhealthy, and underage models, including Spain, Italy, and Israel, which all enacted a minimum body mass index (BMI) requirement. In 2013, New York toughened its child labor law protections for models under the age of eighteen by passing New York Senate Bill No. 5486, which gives underage models the same labor protections afforded to child actors. Key new protections included the following: underage models are not to work before 5:00 pm or after 10:00 pm on school nights, nor were they to work later than 12:30 am on non-school nights; the models may not return to work less than twelve hours after they leave; a pediatric nurse must be on site; models under sixteen must be accompanied by an adult chaperone; parents or guardians of underage models must create a trust fund account into which employers will transfer a minimum of 15% of the child model's gross earnings; and employers must set aside time and a dedicated space for educational instruction.
Types of models.
Fashion modelling.
Runway modelling.
Runway models showcase clothes from fashion designers, fashion media, and consumers. They are also called "live models" and are self-employed. Runway models work in different locations, constantly travelling between those cities where fashion is well known—London, Milan, New York City, and Paris. Second-tier international fashion center cities include: Rome, Florence, Venice, Brescia, Barcelona, Los Angeles, Tokyo, and Moscow. Cities where catalog work comprises the bulk of fashion packaging, merchandising and marketing work are: Miami, San Francisco, Sydney, Chicago, Toronto, Mexico City, Tokyo, Hamburg, London, and Beijing.
The criteria for runway models include certain height and weight requirements. During runway shows, models have to constantly change clothes and makeup. Models walk, turn, and stand in order to demonstrate a garment's key features. Models also go to interviews (called "go and sees") to present their portfolios. The more experience a model has, the more likely she/he is to be hired for a fashion show. A runway model can also work in other areas, such as department store fashion shows, and the most successful models sometimes create their own product lines or go into acting.
The British Association of Model Agents (AMA) says that female models should be around 34"-24"-34" and between and tall. The average model is very slender. Those who do not meet the size requirement may try to become a plus-size model. According to the New York Better Business Career Services website, the preferred dimensions for a male model are a height of 5 ft 11 in (180 cm) to 6 ft 2 in (189 cm), a waist of and a chest measurement of . Male runway models are notably skinny and well toned.
Male and female models must also possess clear skin, healthy hair, and attractive facial features. Stringent weight and body proportion guidelines form the selection criteria by which established, and would‑be, models are judged for their placement suitability, on an ongoing basis. There can be some variation regionally, and by market tier, subject to current prevailing trends at any point, in any era, by agents, agencies and end-clients.
Formerly, the required measurements for models were 35"-23.5"-35" in (90-60-90 cm), the alleged measurements of Marilyn Monroe. Today's fashion models tend to have measurements closer to the AMA-recommended shape, but some - such as Afghan model Zohre Esmaeli - still have 35"-23.5"-35" measurements. Although in some fashion centres, a size 00 is more ideal than a size 0.
The often thin shape of many fashion models has been criticized for warping girls' body image and encouraging eating disorders. Organisers of a fashion show in Madrid in September 2006 turned away models who were judged to be underweight by medical personnel who were on hand. In February 2007, six months after her sister, Luisel Ramos, also a model, died, Uruguayan model Eliana Ramos became the third fashion model to die of malnutrition in six months. The second victim was Ana Carolina Reston. Luisel Ramos died of heart failure caused by anorexia nervosa just after stepping off the catwalk. In 2015, France passed a law requiring models to be declared healthy by a doctor in order to participate in fashion shows. The law also requires re-touched images to be marked as such in magazines.
Supermodels.
Supermodels are highly paid, high-profile, fashion models having the greatest prominence in the industry. They have celebrity status and appear on top fashion magazine covers, national advertisements such as commercials, magazine spreads, and at fashion shows. Additionally, their appearance in advertising amounts to an endorsement which attracts greater financial rewards, especially when they conclude deals to advertise a brand exclusively, becoming ""the face"" of that brand or a brand ambassador.
Plus-size models.
Plus-size models are models who generally have larger measurements than editorial fashion models. The primary use of plus-size models is to appear in advertising and runway shows for plus-size labels. Plus-size models are also engaged in work that is not strictly related to selling large-sized clothing, e.g., stock photography and advertising photography for cosmetics, household and pharmaceutical products and sunglasses, footwear and watches. Therefore, plus-size models do not exclusively wear garments marketed as plus-size clothing. This is especially true when participating in fashion editorials for mainstream fashion magazines. Some plus-size models have appeared in runway shows and campaigns for mainstream retailers and designers such as Gucci, Guess, Jean-Paul Gaultier, Levi's and Versace Jeans.
Fit models.
A fit model works as a sort of live mannequin to give designers and pattern makers feedback on the fit, feel, movement, and drape of a garment to be produced in a given size.
Glamour models.
Glamour modelling focuses solely on sexuality so there are no requirements to be a glamour model other than the ability to pose seductively. Glamour models can be any size or shape. There is no industry standard for glamour modelling and it varies greatly by country. For the most part, glamour models are limited to modelling in calendars, men's magazines, such as "Playboy", bikini modelling, lingerie modelling, fetish modelling, music videos, and extra work in films. However, some extremely popular glamour models transition into commercial print modelling, appearing in swimwear, bikini and lingerie campaigns.
It is widely considered that England created the market for glamour modelling when "The Sun" established Page 3 in 1969, a section in their newspaper which now features topless models. In the beginning, the newspaper featured sexually suggestive images of "Penthouse" and "Playboy" models. It was not until 1970 that models appeared topless. In the 1980s, "The Sun" competitors followed suit and produced their own Page 3 sections. It was during this time that glamour models first came to prominence with the likes of Samantha Fox. As a result, the United Kingdom has a very large glamour market and has numerous glamour modelling agencies to this day.
It was not until the 1990s that modern glamour modelling was established. During this time, the fashion industry was promoting models with waif bodies and androgynous looking women, which left a void. Several fashion models, who were deemed too commercial, and too curvaceous, were frustrated with industry standards, and took a different approach. Models such as Victoria Silvstedt left the fashion world and began modelling for men's magazines. In the previous decades, posing nude for "Playboy" resulted in models losing their agencies and endorsements. "Playboy" was a stepping stone which catapulted the careers of Victoria Silvstedt, Pamela Anderson, and Anna Nicole Smith. Pamela Anderson became so popular from her "Playboy" spreads that she was able to land roles on "Home Improvement" and "Baywatch".
In the mid-1990s, a series of men's magazines were established such as "Maxim", "FHM", and "Stuff". At the same time, magazines including Sweden's "Slitz" re-branded themselves as men's magazines. Pre-internet, these magazines were popular among men in their late teens and early twenties because they were considered to be more tasteful than their predecessors. With the glamour market growing, fashion moved away from the waifs and onto Brazilian bombshells. The glamour market, which consisted mostly of commercial fashion models and commercial print models, became its own genre due to its popularity. Even in a large market like the United Kingdom, however, glamour models are not usually signed exclusively to one agency as they can not rely financially on one agency to provide them with enough work. It was, and still is, a common practice for glamour models to partake in kiss-and-tell interviews about their dalliances with famous men. The notoriety of their alleged bed-hopping often propels their popularity and they are often promoted by their current or former fling. With Page 3 models becoming fixtures in the British tabloids, glamour models such as Jordan, now known as Katie Price, became household names. By 2004, Page 3 regulars earned anywhere from £30,000 to 40,000, where the average salary of a non-Page 3 model, as of 2011, was between £10,000 and 20,000. In the early 2000s, glamour models, and aspiring glamour models, appeared on reality television shows such as "Big Brother" to gain fame. Several "Big Brother" alumni parlayed their fifteen minutes of fame into successful glamour modelling careers. However, the glamour market became saturated by the mid-2000s, and numerous men's magazines including "Arena", "Stuff" and "FHM" in the United States went under. During this time, there was a growing trend of glamour models, including Kellie Acreman and Lauren Pope, becoming DJs to supplement their income. In a 2012 interview, Keeley Hazell said that going topless is not the best way to achieve success and that " was lucky to be in that 1% of people that get that, and become really successful."
Alternative models.
An alternative model is any model who does not fit into the conventional model types and may include punk, goth, fetish, and tattooed models or models with distinctive attributes. This type of modelling is usually a cross between glamour modelling and art modelling. Publishers such as Goliath Books in Germany introduced alternative models and punk photography to larger audiences. Then transgendered, Billi Gordon, was the top greeting card model in the world and inspired a cottage industry around his image including greeting cards, T-shirts, fans, stationery, gift bags, etc.
Parts models.
Some models are employed for their body parts. For example, hand models may be used to promote products held in the hand and nail-related products. (e.g. rings, other jewelry or nail polish). They are frequently part of television commercials. Many parts models have exceptionally attractive body parts, but there is also demand for unattractive or unusual looking body parts for particular campaigns.
Hands are the most in-demand body parts. Feet models are also in high demand, particularly those who fit sample size shoes. Models are also successful modelling other specific parts including abs, arms, back, bust or chest, legs, and lips. Some petite models (females who are under and do not qualify as fashion models) have found success in women's body part modelling.
Parts model divisions can be found at agencies worldwide. Several agencies solely represent parts models, including Hired Hands in London, Body Parts Models in Los Angeles, Carmen Hand Model Management in New York and Parts Models in New York. Parts Models is the largest parts agency, representing over 300 parts models.
Fitness models.
Fitness modelling focuses on displaying a healthy, toned physique. Fitness models usually have defined muscle groups. The model's body weight is heavier due to muscle weighing more than fat; however, they have a lower body fat percentage because the muscles are toned and sculpted. Fitness models are often used in magazine advertising. Sometimes they are certified personal fitness trainers. However, other fitness models are also athletes and compete as professionals in fitness and figure competitions. There are several agencies in large markets such as New York, London, Germany that have fitness modelling agencies. While there is a large market for these models, most of these agencies are a secondary agency promoting models who typically earn their primary income as commercial models.
Commercial print and on-camera models.
Commercial print models generally appear in print ads for non-fashion products, and in television commercials. Commercial print models can earn up to $250 an hour. Commercial print models are usually non-exclusive, and primarily work in one location.
There are several large fashion agencies that have commercial print divisions, including Ford Models in the United States.
Promotional models.
A promotional model is a model hired to drive consumer demand for a product, service, brand, or concept by directly interacting with potential consumers. The vast majority of promotional models tend to be attractive in physical appearance. They serve to provide information about the product or service and make it appealing to consumers. While the length of interaction may be short, the promotional model delivers a live experience that reflects on the product or service he or she is representing. This form of marketing touches fewer consumers for the cost than traditional advertising media (such as print, radio, and television); however, the consumer's perception of a brand, product, service, or company is often more profoundly affected by a live person-to-person experience.
Marketing campaigns that make use of promotional models may take place in stores or shopping malls, at tradeshows, special promotional events, clubs, or even at outdoor public spaces. They are often held at high traffic locations to reach as many consumers as possible, or at venues at which a particular type of target consumer is expected to be present.
Spokesmodels.
"Spokesmodel" is a term used for a model who is employed to be associated with a specific brand in advertisements. A spokesmodel may be a celebrity used only in advertisements (in contrast to a brand ambassador who is also expected to represent the company at various events), but more often the term refers to a model who is not a celebrity in their own right. A classic example of the spokesmodel are the models hired to be the Marlboro Man between 1954 and 1999.
Trade show models.
Trade show models work a trade show floorspace or booth, and represent a company to attendees. Trade show models are typically not regular employees of the company, but are freelancers hired by the company renting the booth space. They are hired for several reasons: trade show models can make a company's booth more visibly distinguishable from the hundreds of other booths with which it competes for attendee attention. They are articulate and quickly learn and explain or disseminate information on the company and its product(s) and service(s). And they can assist a company in handling a large number of attendees which the company might otherwise not have enough employees to accommodate, possibly increasing the number of sales or leads resulting from participation in the show.
Atmosphere models.
Atmosphere models are hired by the producers of themed events to enhance the atmosphere or ambience of their event. They are usually dressed in costumes exemplifying the theme of the event and are often placed strategically in various locations around the venue. It is common for event guests to have their picture taken with atmosphere models. For example, if someone is throwing a "Brazilian Day" celebration, they would hire models dressed in samba costumes and headdresses to stand or walk around the party.
Podium models.
Podium models differ from runway models in that they don't walk down a runway, but rather just stand on an elevated platform. They are kind of like live mannequins placed in various places throughout an event. Attendees can walk up to the models and inspect and even feel the clothing. Podium Modeling is a practical alternative way of presenting a fashion show when space is too limited to have a full runway fashion show. 
Art models.
Art models pose for any visual artist as part of the creative process. Art models are often paid professionals who provide a reference or inspiration for a work of art that includes the human figure. The most common types of art created using models are figure drawing, figure painting, sculpture and photography, but almost any medium may be used. Although commercial motives dominate over aesthetics in illustration, its artwork commonly employs models. Models are most frequently employed for art classes or by informal groups of experienced artists that gather to share the expense of a model.

</doc>
<doc id="48918" url="https://en.wikipedia.org/wiki?curid=48918" title="Alexandra of Denmark">
Alexandra of Denmark

Alexandra of Denmark (Alexandra Caroline Marie Charlotte Louise Julia; 1 December 1844 – 20 November 1925) was Queen consort of the United Kingdom of Great Britain and Ireland and Empress consort of India as the wife of King-Emperor Edward VII.
Her family had been relatively obscure until her father, Prince Christian of Schleswig-Holstein-Sonderburg-Glücksburg, was chosen with the consent of the great powers to succeed his distant cousin, Frederick VII, to the Danish throne. At the age of sixteen, she was chosen as the future wife of Albert Edward, Prince of Wales, the heir apparent of Queen Victoria. They married eighteen months later in 1863, the same year her father became king of Denmark as Christian IX and her brother was appointed to the vacant Greek throne as George I. She was Princess of Wales from 1863 to 1901, the longest anyone has ever held that title, and became generally popular; her style of dress and bearing were copied by fashion-conscious women. Largely excluded from wielding any political power, she unsuccessfully attempted to sway the opinion of British ministers and her husband's family to favour Greek and Danish interests. Her public duties were restricted to uncontroversial involvement in charitable work.
On the death of Queen Victoria in 1901, Albert Edward became king-emperor as Edward VII, with Alexandra as queen-empress consort. She held the status until Edward's death in 1910. She greatly distrusted her nephew, German Emperor Wilhelm II, and supported her son during World War I, in which Britain and its allies fought Germany.
Early life.
Princess Alexandra Caroline Marie Charlotte Louise Julia, or "Alix", as her immediate family knew her, was born at the Yellow Palace, an 18th-century town house at 18 Amaliegade, right next to the Amalienborg Palace complex in Copenhagen. Her father was Prince Christian of Schleswig-Holstein-Sonderburg-Glücksburg and her mother was Princess Louise of Hesse-Kassel. Although she was of royal blood, her family lived a comparatively normal life. They did not possess great wealth; her father's income from an army commission was about £800 per year and their house was a rent-free grace and favour property. Occasionally, Hans Christian Andersen was invited to call and tell the children stories before bedtime.
In 1848, King Christian VIII of Denmark died and his only son, Frederick ascended the throne. Frederick was childless, had been through two unsuccessful marriages, and was assumed to be infertile. A succession crisis arose as Frederick ruled in both Denmark and Schleswig-Holstein, and the succession rules of each territory differed. In Holstein, the Salic law prevented inheritance through the female line, whereas no such restrictions applied in Denmark. Holstein, being predominantly German, proclaimed independence and called in the aid of Prussia. In 1852, the great powers called a conference in London to discuss the Danish succession. An uneasy peace was agreed, which included the provision that Prince Christian of Schleswig-Holstein-Sonderburg-Glücksburg would be Frederick's heir in all his dominions and the prior claims of others (who included Christian's own mother-in-law, brother-in-law and wife) were surrendered.
Prince Christian was given the title Prince of Denmark and his family moved into a new official residence, Bernstorff Palace. Although the family's status had risen, there was little or no increase in their income and they did not participate in court life at Copenhagen as they refused to meet Frederick's third wife and former mistress, Louise Rasmussen, because she had an illegitimate child by a previous lover. Alexandra shared a draughty attic bedroom with her sister, Dagmar (later Empress of Russia), made her own clothes and waited at table along with her sisters. Alexandra and Dagmar were given swimming lessons by the Swedish pioneer of women's swimming, Nancy Edberg. At Bernstorff, Alexandra grew into a young woman; she was taught English by the English chaplain at Copenhagen and was confirmed in Christiansborg Palace. She was devout throughout her life, and followed High Church practice.
Marriage and family.
Queen Victoria and her husband, Prince Albert, were already concerned with finding a bride for their son and heir, Albert Edward, the Prince of Wales. They enlisted the aid of their daughter, Crown Princess Victoria of Prussia, in seeking a suitable candidate. Alexandra was not their first choice, since the Danes were at loggerheads with the Prussians over the Schleswig-Holstein Question and most of the British royal family's relations were German. Eventually, after rejecting other possibilities, they settled on her as "the only one to be chosen".
On 24 September 1861, Crown Princess Victoria introduced her brother Albert Edward to Alexandra at Speyer. Almost a year later on 9 September 1862 (after his affair with Nellie Clifden and the death of his father) Albert Edward proposed to Alexandra at the Royal Palace of Laeken, the home of his great-uncle, King Leopold I of Belgium.
A few months later, Alexandra travelled from Denmark to Britain aboard the royal yacht "Victoria and Albert II" and arrived in Gravesend, Kent, on 7 March 1863. Sir Arthur Sullivan composed music for her arrival and Poet Laureate Alfred, Lord Tennyson, wrote an ode in Alexandra's honour:
Thomas Longley, the Archbishop of Canterbury, married the couple on 10 March 1863 at St George's Chapel, Windsor Castle. The choice of venue was criticised widely. As the ceremony took place outside London, the press complained that large public crowds would not be able to view the spectacle. Prospective guests thought it awkward to get to and, as the venue was small, some people who had expected invitations were disappointed. The Danes were dismayed because only Alexandra's closest relations were invited. The British court was still in mourning for Prince Albert, so ladies were restricted to wearing grey, lilac or mauve. As the couple left Windsor for their honeymoon at Osborne House on the Isle of Wight, they were cheered by the schoolboys of neighbouring Eton College, who included Lord Randolph Churchill.
By the end of the following year, Alexandra's father had ascended the throne of Denmark, her brother George had become King of the Hellenes, her sister Dagmar was engaged to the Tsesarevich of Russia, and Alexandra had given birth to her first child. Her father's accession gave rise to further conflict over the fate of Schleswig-Holstein. The German Confederation successfully invaded Denmark, reducing the area of Denmark by two-fifths. To the great irritation of Queen Victoria and the Crown Princess of Prussia, Alexandra and Albert Edward supported the Danish side in the war. The Prussian conquest of former Danish lands heightened Alexandra's profound dislike of the Germans, a feeling which stayed with her for the rest of her life.
Alexandra's first child, Albert Victor, was born two months premature in early 1864. Alexandra showed devotion to her children: "She was in her glory when she could run up to the nursery, put on a flannel apron, wash the children herself and see them asleep in their little beds." Albert Edward and Alexandra had six children in total: Albert Victor, George, Louise, Victoria, Maud, and John. All of Alexandra's children were apparently born prematurely; biographer Richard Hough thought Alexandra deliberately misled Queen Victoria as to her probable delivery dates, as she did not want the queen to be present at their births. During the birth of her third child in 1867, the added complication of a bout of rheumatic fever threatened Alexandra's life, and left her with a permanent limp.
In public, Alexandra was dignified and charming; in private, affectionate and jolly. She enjoyed many social activities, including dancing and ice-skating, and was an expert horsewoman and tandem driver. She also enjoyed hunting, to the dismay of Queen Victoria, who asked her to stop, but without success. Even after the birth of her first child, she continued to socialise much as before, which led to some friction between the queen and the young couple, exacerbated by Alexandra's loathing of Prussians and the queen's partiality towards them.
Princess of Wales.
Albert Edward and Alexandra visited Ireland in April 1868. After her illness the previous year, she had only just begun to walk again without the aid of two walking sticks, and was already pregnant with her fourth child. The royal couple undertook a six-month tour taking in Austria, Egypt and Greece over 1868 and 1869, which included visits to her brother King George I of Greece, to the Crimean battlefields and (for her only) to the harem of the Khedive Ismail. In Turkey she became the first woman to sit down to dinner with the Sultan (Abdülâziz).
The Waleses made Sandringham House their preferred residence, with Marlborough House their London base. Biographers agree that their marriage was in many ways a happy one; however, some have asserted that Albert Edward did not give his wife as much attention as she would have liked and that they gradually became estranged, until his attack of typhoid fever (the disease which was believed to have killed his father) in late 1871 brought about a reconciliation. This is disputed by others, who point out Alexandra's frequent pregnancies throughout this period and use family letters to deny the existence of any serious rift. Nevertheless, the prince was severely criticised from many quarters of society for his apparent lack of interest in her very serious illness with rheumatic fever. Throughout their marriage Albert Edward continued to keep company with other women, including the actress Lillie Langtry; Daisy Greville, Countess of Warwick; humanitarian Agnes Keyser; and society matron Alice Keppel. Alexandra knew about most of these relationships, and later permitted Alice Keppel to visit the king as he lay dying. Alexandra herself remained faithful throughout her marriage.
An increasing degree of deafness, caused by hereditary otosclerosis, led to Alexandra's social isolation; she spent more time at home with her children and pets. Her sixth and final pregnancy ended tragically when her infant son died only a day after his birth. Despite Alexandra's pleas for privacy, Queen Victoria insisted on announcing a period of court mourning, which led unsympathetic elements of the press to describe the birth as "a wretched abortion" and the funeral arrangements as "sickening mummery", even though the infant was not buried in state with other members of the royal family at Windsor, but in strict privacy in the churchyard at Sandringham, where he had lived out his brief life.
For eight months over 1875–76, the Prince of Wales was absent from Britain on a tour of India, but to her dismay Alexandra was left behind. The prince had planned an all-male group and intended to spend much of the time hunting and shooting. During the prince's tour, one of his friends who was travelling with him, Lord Aylesford, was told by his wife that she was going to leave him for another man: Lord Blandford, who was himself married. Aylesford was appalled and decided to seek a divorce. Meanwhile, Lord Blandford's brother, Lord Randolph Churchill, persuaded the lovers against an elopement. Now concerned by the threat of divorce, Lady Aylesford sought to dissuade her husband from proceeding but Lord Aylesford was adamant and refused to reconsider. In an attempt to pressure Lord Aylesford to drop his divorce suit, Lady Aylesford and Lord Randolph Churchill called on Alexandra and told her that if the divorce was to proceed they would subpoena her husband as a witness and implicate him in the scandal. Distressed at their threats, and following the advice of Sir William Knollys and the Duchess of Teck, Alexandra informed the queen, who then wrote to the Prince of Wales. The prince was incensed. Eventually, the Blandfords and the Aylesfords both separated privately. Although Lord Randolph Churchill later apologised, for years afterwards the Prince of Wales refused to speak to or see him.
Alexandra spent the spring of 1877 in Greece recuperating from a period of ill health and visiting her brother King George of the Hellenes. During the Russo-Turkish War, Alexandra was clearly partial against Turkey and towards Russia, where her sister was married to the Tsarevitch, and she lobbied for a revision of the border between Greece and Turkey in favour of the Greeks. Alexandra and her two sons spent the next three years largely parted from each other's company as the boys were sent on a worldwide cruise as part of their naval and general education. The farewell was very tearful and, as shown by her regular letters, she missed them dreadfully. In 1881, Alexandra and Albert Edward travelled to Saint Petersburg after the assassination of Alexander II of Russia, both to represent Britain and so that Alexandra could provide comfort to her sister, who was now the Tsarina.
Alexandra undertook many public duties; in the words of Queen Victoria, "to spare me the strain and fatigue of functions. She opens bazaars, attends concerts, visits hospitals in my place ... she not only never complains, but endeavours to prove that she has enjoyed what to another would be a tiresome duty." She took a particular interest in the London Hospital, visiting it regularly. Joseph Merrick, the so-called "Elephant Man", was one of the patients whom she met. Crowds usually cheered Alexandra rapturously, but during a visit to Ireland in 1885, she suffered a rare moment of public hostility when visiting the City of Cork, a hotbed of Irish nationalism. She and her husband were booed by a crowd of two to three thousand people brandishing sticks and black flags. She smiled her way through the ordeal, which the British press still portrayed in a positive light, describing the crowds as "enthusiastic". As part of the same visit, she received a Doctorate in Music from Trinity College, Dublin.
The death of her eldest son, Prince Albert Victor, Duke of Clarence and Avondale, in 1892 was a serious blow to Alexandra. His room and possessions were kept exactly as he had left them, much as those of Prince Albert were left after his death in 1861. She said, "I have buried my angel and with him my happiness." Surviving letters between Alexandra and her children indicate that they were mutually devoted. In 1894, her brother-in-law Alexander III of Russia died and her nephew Nicholas II of Russia became Tsar. Alexandra's widowed sister, the Dowager Empress, leant heavily on her for support; Alexandra slept, prayed, and stayed beside her sister for the next two weeks until Alexander's burial.
Queen Alexandra.
Queen consort.
With the death of her mother-in-law, Queen Victoria, in 1901, Alexandra became queen-empress consort to the new king. Just two months later, her surviving son George and daughter-in-law Mary left on an extensive tour of the empire, leaving their young children in the care of Alexandra and Edward, who doted on their grandchildren. On George's return, preparations for Edward and Alexandra's coronation were well in hand but just a few days before the scheduled coronation in June 1902 the king became seriously ill with appendicitis. Alexandra deputised for him at a military parade, and attended the Royal Ascot races without him, in an attempt to prevent public alarm. Eventually, the coronation had to be postponed and Edward had an operation performed by Frederick Treves of the London Hospital to drain the infected appendix. After his recovery, Alexandra and Edward were crowned together in August: he by the Archbishop of Canterbury, Frederick Temple, and she by the Archbishop of York, William Dalrymple Maclagan.
Despite being queen, Alexandra's duties changed little, and she kept many of the same retainers. Alexandra's Woman of the Bedchamber, Charlotte Knollys, the daughter of Sir William Knollys, served Alexandra loyally for many years. On 10 December 1903, Knollys woke to find her bedroom full of smoke. She roused Alexandra and shepherded her to safety. In the words of Grand Duchess Augusta of Mecklenburg-Strelitz, "We must give credit to old Charlotte for "really" saving [Alexandra's] life."
Alexandra again looked after her grandchildren when George and Mary went on a second tour, this time to British India, over the winter of 1905–06. Her father, King Christian IX of Denmark, died that January. Eager to retain their family links, to each other and to Denmark, in 1907 Alexandra and her sister, the Dowager Empress of Russia, purchased a villa north of Copenhagen, Hvidøre, as a private getaway.
Biographers have asserted that Alexandra was denied access to the king's briefing papers and excluded from some of his foreign tours to prevent her meddling in diplomatic matters. She was deeply distrustful of Germans, and invariably opposed anything that favoured German expansion or interests. For example, in 1890 Alexandra wrote a memorandum, distributed to senior British ministers and military personnel, warning against the planned exchange of the British North Sea island of Heligoland for the German colony of Zanzibar, pointing out Heligoland's strategic significance and that it could be used either by Germany to launch an attack, or by Britain to contain German aggression. Despite this, the exchange went ahead anyway. The Germans fortified the island and, in the words of Robert Ensor and as Alexandra had predicted, it "became the keystone of Germany's maritime position for offence as well as for defence". The "Frankfurter Zeitung" was outspoken in its condemnation of Alexandra and her sister, the Dowager Empress of Russia, saying that the pair were "the centre of the international anti-German conspiracy". She despised and distrusted her nephew, Wilhelm II of Germany, calling him in 1900 "inwardly our enemy".
In 1910, Alexandra became the first queen consort to visit the British House of Commons during a debate. In a remarkable departure from precedent, for two hours she sat in the Ladies' Gallery overlooking the chamber while the Parliament Bill, a bill to remove the right of the House of Lords to veto legislation, was debated. Privately, Alexandra disagreed with the bill. Shortly afterward, she left to visit her brother, King George I of Greece, in Corfu. While there, she received news that King Edward was seriously ill. Alexandra returned at once and arrived just the day before her husband died. In his last hours, she personally administered oxygen from a gas cylinder to help him breathe. She told Frederick Ponsonby, "I feel as if I had been turned into stone, unable to cry, unable to grasp the meaning of it all." Later that year, she moved out of Buckingham Palace to Marlborough House, but she retained possession of Sandringham. The new king, Alexandra's son George, soon faced a decision over the Parliament Bill. Despite her personal views, Alexandra supported her son's reluctant agreement to Prime Minister H. H. Asquith's request to create sufficient Liberal peers after a general election if the Lords continued to block the legislation.
Queen mother.
From Edward's death, Alexandra was queen mother, being a dowager queen and the mother of the reigning monarch. She was styled "Her Majesty Queen Alexandra". She did not attend her son's coronation in 1911 since it was not customary for a crowned queen to attend the coronation of another king or queen, but otherwise continued the public side of her life, devoting time to her charitable causes. One such cause included Alexandra Rose Day, where artificial roses made by people with disabilities were sold in aid of hospitals by women volunteers. During the First World War, the custom of hanging the banners of foreign princes invested with Britain's highest order of knighthood, the Order of the Garter, in St George's Chapel, Windsor Castle, came under criticism, as the German members of the Order were fighting against Britain. Alexandra joined calls to "have down those hateful German banners". Driven by public opinion, but against his own wishes, the king had the banners removed but to Alexandra's dismay he had down not only "those vile Prussian banners" but also those of her Hessian relations who were, in her opinion, "simply soldiers or vassals under that brutal German Emperor's orders". On 17 September 1916, she was at Sandringham during a Zeppelin air raid, but far worse was to befall other members of her family. In Russia, her nephew Tsar Nicholas II was overthrown and he, his wife and children were killed by revolutionaries. Her sister the Dowager Empress was rescued from Russia in 1919 by and brought to England, where she lived for some time with Alexandra.
Alexandra retained a youthful appearance into her senior years, but during the war her age caught up with her. She took to wearing elaborate veils and heavy makeup, which was described by gossips as having her face "enamelled". She made no more trips abroad, and suffered increasing ill health. In 1920, a blood vessel in her eye burst, leaving her with temporary partial blindness. Towards the end of her life, her memory and speech became impaired. She died on 20 November 1925 at Sandringham after suffering a heart attack, and was buried in an elaborate tomb next to her husband in St George's Chapel, Windsor Castle.
Legacy.
The Queen Alexandra Memorial by Alfred Gilbert was unveiled on Alexandra Rose Day 8 June 1932 at Marlborough Gate, London. An ode in her memory, "So many true princesses who have gone", composed by the then Master of the King's Musick Sir Edward Elgar to words by the Poet Laureate John Masefield, was sung at the unveiling and conducted by the composer.
Alexandra was highly popular with the British public. After she married the Prince of Wales in 1863, a new park and "People's Palace", a public exhibition and arts centre under construction in north London, were renamed the Alexandra Palace and park to commemorate her. There are at least sixty-seven roads and streets in the Greater London area alone called Alexandra Road, Alexandra Avenue, Alexandra Gardens, Alexandra Close or Alexandra Street, all named after her. Unlike her husband and mother-in-law, she was not castigated by the press. Funds that she helped to collect were used to buy a river launch, called "Alexandra", to ferry the wounded during the Sudan campaign, and to fit out a hospital ship, named "The Princess of Wales", to bring back wounded from the Boer War. During the Boer War, Queen Alexandra's Imperial Military Nursing Service, later renamed Queen Alexandra's Royal Army Nursing Corps, was founded under Royal Warrant.
Alexandra had little understanding of money. The management of her finances was left in the hands of her loyal comptroller, Sir Dighton Probyn VC, who undertook a similar role for her husband. In the words of her grandson, Edward VIII (later the Duke of Windsor), "Her generosity was a source of embarrassment to her financial advisers. Whenever she received a letter soliciting money, a cheque would be sent by the next post, regardless of the authenticity of the mendicant and without having the case investigated." Though she was not always extravagant (she had her old stockings darned for re-use and her old dresses were recycled as furniture covers), she would dismiss protests about her heavy spending with a wave of a hand or by claiming that she had not heard.
She hid a small scar on her neck, which was likely the result of a childhood operation, by wearing choker necklaces and high necklines, setting fashions which were adopted for fifty years. Alexandra's effect on fashion was so profound that society ladies even copied her limping gait, after her serious illness in 1867 left her with a stiff leg. This came to be known as the "Alexandra limp". She used predominantly the London fashion houses; her favourite was Redfern's, but she shopped occasionally at Doucet and Fromont of Paris.
Queen Alexandra has been portrayed on television by Deborah Grant and Helen Ryan in "Edward the Seventh", Ann Firbank in "Lillie", Maggie Smith in "All the King's Men", and Bibi Andersson in "The Lost Prince". She was portrayed in film by Helen Ryan again in the 1980 film "The Elephant Man", Sara Stewart in the 1997 film "Mrs Brown", and Julia Blake in the 1999 film "Passion". In a 1980 stage play by Royce Ryton, "Motherdear", she was portrayed by Margaret Lockwood in her last acting role.
Titles, styles, honours and arms.
Honours.
In 1901, she became the first woman since 1488 to be made a Lady of the Garter. Other honours she held included Member First Class of the Royal Order of Victoria and Albert, Lady of the Imperial Order of the Crown of India, and Lady of Justice of the Order of St. John of Jerusalem.
Among foreign honours received by Queen Alexandra was the Japanese Order of the Precious Crown, delivered to her on behalf of Emperor Meiji by Prince Komatsu Akihito when he visited the United Kingdom in June 1902 to attend the coronation.
Arms.
Queen Alexandra's arms upon the ascension of her husband in 1901 were the royal coat of arms of the United Kingdom impaled with the arms of her father, King Christian IX of Denmark. The shield is surmounted by the imperial crown, and supported by the crowned lion of England and a wild man or savage from the Danish royal arms.

</doc>
<doc id="48919" url="https://en.wikipedia.org/wiki?curid=48919" title="Prince Albert (disambiguation)">
Prince Albert (disambiguation)

Albert, Prince Consort (1819–1861) was the husband and consort of Queen Victoria.
Prince Albert may also refer to:

</doc>
<doc id="48921" url="https://en.wikipedia.org/wiki?curid=48921" title="Aspen, Colorado">
Aspen, Colorado

The City of Aspen is the Home Rule Municipality that is the county seat and the most populous municipality of Pitkin County, Colorado, United States. The city population was 6,658 at the 2010 United States Census. Aspen is situated in a remote area of the Rocky Mountains' Sawatch Range and Elk Mountains, along the Roaring Fork River at an elevation just below above sea level on the Western Slope, west of the Continental Divide.
Founded as a mining camp during the Colorado Silver Boom and later named "Aspen" because of the abundance of aspen trees in the area, the city boomed during the 1880s, its first decade of existence. That early era ended when the Panic of 1893 led to a collapse in the silver market, and the city began a half-century known as "the quiet years" during which its population steadily declined, reaching a nadir of less than a thousand by 1930. Aspen's fortunes reversed in the mid-20th century when neighboring Aspen Mountain was developed into a ski resort, and industrialist Walter Paepcke bought many properties in the city and redeveloped them. Today it is home to three renowned institutions, two of which Paepcke helped found, that have international importance: the Aspen Music Festival and School, the Aspen Institute, and the Aspen Center for Physics.
In the late 20th century, the city became a popular retreat for celebrities. Gonzo journalist Hunter S. Thompson worked out of a downtown hotel and ran unsuccessfully for county sheriff. Singer John Denver wrote two songs about Aspen after settling there. Both of them popularized Aspen among the countercultural youth of the 1970s as an ideal place to live, and the city continued to grow even as it gained notoriety for some of the era's hedonistic excesses as well, particularly its drug culture.
Today the musicians and movie stars have been joined by corporate executives. As a result of this influx of wealth, Aspen boasts the most expensive real estate prices in the United States and many middle-class residents can no longer afford to live there. It remains a popular tourist destination, with outdoor recreation in the surrounding White River National Forest serving as a summertime complement to the four ski areas in the vicinity.
History.
The city's roots are traced to the winter of 1879, when a group of miners ignored pleas by Frederick Pitkin, governor of Colorado, to return across the Continental Divide due to an uprising of the Ute Indians. Originally named Ute City, the small community was renamed Aspen in 1880, and, in its peak production years of 1891 and 1892, surpassed Leadville as the United States' most productive silver-mining district. Production expanded due to the passage of the Sherman Silver Purchase Act of 1890, which doubled the government's purchase of silver. By 1893, Aspen had banks, a hospital, a police department, two theaters, an opera house and electric lights. Economic collapse came with the Panic of 1893, when President Cleveland called a special session of Congress and repealed the act. Within weeks, many of the Aspen mines were closed and thousands of miners were put out of work. It was proposed that silver be recognized as legal tender and the People's Party (populists) adopted that as one of its main issues; Davis H. Waite, an Aspen newspaperman and agitator was elected governor of Colorado on the Democratic Ticket; but in time the movement failed.
Eventually, after wage cuts, mining revived somewhat, but production declined and by the 1930 census only 705 residents remained. Remaining, however, were fine stocks of old commercial buildings and residences, along with excellent snow. Aspen's development as a ski resort first flickered in the 1930s when investors conceived of a ski area, but the project was interrupted by World War II. Friedl Pfeifer, a member of the 10th Mountain Division who had trained in the area, returned to the area and linked up with industrialist Walter Paepcke and his wife Elizabeth. The Aspen Skiing Corporation was founded in 1946 and the city quickly became a well-known resort, hosting the FIS World Championships in 1950. Paepcke also played an important role in bringing the Goethe Bicentennial Convocation to Aspen in 1949, an event held in a newly designed tent by the architect Eero Saarinen. Aspen was now on the path to becoming an internationally known ski resort and cultural center, home of the Aspen Music Festival and School. The area would continue to grow with the development of three additional ski areas, Buttermilk (1958), Aspen Highlands (1958), and Snowmass (1967).
In 1977, notorious serial killer Ted Bundy, while in the Pitkin County Courthouse in Aspen for a pre-trial hearing, jumped from a second-story window and escaped. He remained free for six days, hiding out on Aspen Mountain, before he was arrested while attempting to drive a stolen car out of the city.
In 1977, Aspen was thoroughly photographed for the Aspen Movie Map project funded by the U.S. Department of Defense. The Movie Map is one of the earliest examples of virtual reality software.
Aspen is notable as the smallest radio market tracked by Arbitron, ranked at #302.
Local media in Aspen includes two radio stations: KSNO and KSPN; two daily newspapers: "The Aspen Times" and "The Aspen Daily News"; three local, lifestyle magazines: "Aspen Sojourner" "Aspen Magazine" [http://www.aspenmagazine.com/ and the bi-annual "Aspen Peak"; one digital magazine, Skollie Magazine's Aspen Edition; as well as one local, live, lifestyle television channel Aspen 82.
Government.
Aspen is a Home Rule Municipality under Colorado law. It has a council-manager government. An elected council of four members and the mayor supervise the city's operations, managed on a day-to-day basis by the city manager, an appointed official who serves at their pleasure. Steve Barwick has been city manager since 1999; Steve Skadron is the mayor.
The city's main office is at City Hall, the former Armory Hall listed on the National Register of Historic Places at the intersection of South Galena Street and East Hopkins Avenue. Because of its expansion in the late 20th century, it has outgrown that space. Several city departments are housed in satellite offices around the city.
Image.
The historic character of the city has been challenged in recent decades by skyrocketing property values and the proliferation of second homes, increasingly shutting low- and middle-income workers out of the city and creating a large pool of commuters from nearby bedroom communities such as Snowmass, Basalt, Carbondale and Glenwood Springs. At the same time, in stark contrast to its historic character, the city has emerged into international fame as a glitzy playground of the wealthy and famous. Aspen has become a second and third home to many international jet setters.
The downtown has been largely transformed into an upscale shopping district that includes high-end restaurants, salons, and boutiques. Aspen boasts Ralph Lauren, Dior, Louis Vuitton, Prada, Gucci, Fendi, Bvlgari, Burberry, Brioni, theory and Ermenegildo Zegna boutiques.
Real estate market.
Aspen is the most expensive place to buy real estate in the US. Aspen is a mixture of high-end luxury estates and condos intermixed with single-family homes and mobile home parks. As of March 2011, the lowest-priced single-family home on the market was a trailer for $559,000. As of June 2015, the median listing price for homes or condos for sale in Aspen is $5,081,388 or $897 a sq ft according to Trulia. It is not uncommon to see listing prices reaching the mid-eight figures. In a 2015 survey of U.S. ski resort towns, Aspen was listed as having the second most expensive rentals, with a one-bedroom averaging $1,750.
Geography.
The city sits along the southeast (upper) end of the Roaring Fork Valley, along the Roaring Fork River, a tributary of the Colorado River about south of Glenwood Springs, Colorado. It is surrounded by mountain and wilderness areas on three sides: Red Mountain to the north, Smuggler Mountain to the east, and Aspen Mountain to the south.
Aspen is located at , along State Highway 82.
According to the United States Census Bureau, the city has a total area of , all land.
Climate.
Under the Köppen climate classification, Aspen has humid continental climate (Köppen: "Dfb") owing to its high altitude. There is a large diurnal temperature variation between daytime and nighttime temperatures, rendering summer days to be moderately warm and winter nights to be very cold for the latitude. Summer lows and winter highs are relatively moderate, with frosts being rare in summer and winter days often averaging above freezing.
Demographics.
As of the census of 2003, there were 5,914 people, 2,903 households, and 1,082 families residing in the city. The population density was 1,675.4 people per square mile (646.9/km²). There were 4,354 housing units at an average density of 1,233.5 per square mile (476.2/km²). The racial makeup of the city was 94.94% White, 0.44% Black or African American, 0.24% Native American, 1.45% Asian, 0.08% Pacific Islander, 1.64% from other races, and 1.20% from two or more races. Hispanic or Latino of any race were 6.14% of the population.
There were 2,903 households out of which 16.5% had children under the age of 18 living with them, 28.8% were married couples living together, 5.6% had a female householder with no husband present, and 62.7% were non-families. 43.8% of all households were made up of individuals and 4.8% had someone living alone who was 65 years of age or older. The average household size was 1.94 and the average family size was 2.67.
In the city the population was spread out with 13.1% under the age of 18, 9.8% from 18 to 24, 42.1% from 25 to 44, 27.6% from 45 to 64, and 7.4% who were 65 years of age or older. The median age was 37 years. For every 100 females there were 115.1 males. For every 100 females age 18 and over, there were 117.5 males.
The median income for a household in the city was $53,750, and the median income for a family was $70,300. Males had a median income of $41,011 versus $32,023 for females. The per capita income for the city was $40,680. About 3.6% of families and 8.2% of the population were below the poverty line, including 4.4% of those under age 18 and 2.6% of those age 65 or over.
Culture.
Education.
As of 2012, based on data from the 2009–2010 school year, according to "U.S. News & World Report" Aspen High School, the only high school in the Aspen School District, is the top ranked high school in Colorado and ranked 59th in the United States. The high school has grades 9 to 12, 540 students and 41 teachers. Minorities, mostly Hispanic, make up 13% of the school's enrollment. 4% of the students are economically disadvantaged. The school has a high rate of participation in the International Baccalaureate program. As of 2012 the school was not included in the lists of top high schools published by "Newsweek and The Daily Beast" or The Washington Post".
Sports.
The Winter X Games sports event has been held in Aspen since 2002.
The Gentlemen of Aspen is the local rugby team. The Gentlemen of Aspen won the Rugby Super League several times: 1997, 2001, 2002.
Sister cities.
Aspen has seven sister cities, as designated by Sister Cities International:

</doc>
<doc id="48922" url="https://en.wikipedia.org/wiki?curid=48922" title="Leadville, Colorado">
Leadville, Colorado

Leadville is the statutory city that is the county seat and only incorporated municipality in Lake County, Colorado, United States. The city population was 2,602 at the 2010 United States Census. Situated at an elevation of , Leadville is the highest incorporated city and the second highest incorporated municipality in the United States. A former silver mining town that lies near the headwaters of the Arkansas River in the heart of the Rocky Mountains, the Leadville Historic District contains many historic structures and sites from its dynamic mining era. In the late 19th century, Leadville was the second most populous city in Colorado, after Denver.
History.
Settlement.
The Leadville area was first settled in 1859 when placer gold was discovered in California Gulch during the Pikes Peak Gold Rush. By 1860, a town, Oro City ("oro" is the Spanish word for gold) had sprung up and a year later its population had reached more than 5,000. But the boom was brief because the placer-mined gold soon ran out and Oro City never became a major settlement.
The early miners had noted that mining for placer gold was hampered by heavy black sand in the sluice boxes, and in 1874 it was discovered that the heavy sand that impeded gold recovery was the lead mineral cerussite, which has a high silver content. Prospectors traced the cerussite to its source, present day Leadville, and by 1876 had discovered several silver-lead lode deposits.
Horace Tabor, who became known as the "Leadville Silver King" and his wife Augusta were among the first prospectors to arrive in Oro City. Tabor tried his luck at prospecting while his wife worked as a camp cook, laundress, banker and postmistress.
Founding of Leadville.
Leadville was founded in 1877 by mine owners Horace Tabor and August Meyer at the start of the Colorado Silver Boom. The town was built on desolate flat land below the tree line. The first miners lived in a rough tented camp near the silver deposits in California Gulch. Initially the settlement was called "Slabtown" but when the residents petitioned for a post office the name Leadville was chosen. By 1880 Tabor and Meyer's new town had gas lighting, water mains and 28 miles of streets, five churches, three hospitals, six banks, and a school for 1,100 students. Many business buildings were constructed with bricks hauled in by wagons. 
The first post office was in Tabor's store at Oro, Augusta Tabor was the postmistress. Carriers went down to Denver one week and tried to come back the next. Postage was fifty cents a letter. In early 1878, Meyer, Harrison, Tabor established a post office in Leadville, with Henderson as postmaster. The post office and the telegraph office both prospered.
The town's first newspaper was "The Reville", a Republican weekly in 1878. Three months later, a competing Democratic weekly, "The Eclipse" emerged. The Chronicle was the town's first daily and first newspaper in America to employ a full-time female reporter. Like the Rocky Mountain News, The Chronicle took the lead in outing criminals and thieves, in an attempt to clean up the town's shady business culture. Despite violent threats, the Chronicle survived without major incident.
William Nye opened the first saloon early 1877 and it was followed by many others. The same year "The Coliseum Novelty" was the first theater to open. It offered sleeping rooms upstairs for a nightly rate and provided a variety of entertainments: dancing girls, dogfights, cockfighting, wrestling and boxing matches, as well as rooms for gambling. In June 1881, it burned to the ground. Ben Wood who arrived in Leadville in 1878, opened the first legitimate theatre,"Wood's Opera House" with a thousand seats. It was a first- class theater, where gentleman removed their hats and did not smoke or drink in the presence of a lady. Less than a year later, Wood opened the Windsor Hotel. His opera house was regarded as the largest and best theater constructed in the west, an honor it held until the opening of the Tabor Opera House. Horace Tabor's Opera House was the most costly structure in Colorado at the time. Building materials were brought by wagons from Denver. The massive three-story opera house, constructed of stone, brick, and iron, opened on 20 November 1879. Tabor, originally from Vermont, became the town's first mayor. After striking it rich, he had an estimated net worth of 10 million dollars and was known for his extravagant lifestyle.
In 1883 Horace Tabor divorced his wife of 25 years, and married Baby Doe McCourt, who was half his age. Tabor was by then a US senator and the divorce and marriage caused a scandal in Colorado and beyond. Tabor, one of the wealthiest men in Colorado, lost his fortune when the repeal of the Sherman Silver Purchase Act caused the Panic of 1893. He died destitute but convinced the price of silver would rebound, and according to legend told Baby Doe to "hold on the Matchless mine … it will make millions again when silver comes back." She returned to Leadville with her daughters, Silver Dollar and Lily, where she spent the rest of her life believing Tabor's prediction. At one time the "best dressed woman in the West", she lived in a cabin at the Matchless Mine for the last three decades of her life. After a snowstorm in March 1935, she was found frozen in her cabin, aged about 81 years.
Mining and smelting.
By 1880, three years after the town was founded, Leadville was one of the world's largest and richest silver camps, with a population of more than 15,000. Income from more than thirty mines and ten large smelting works producing gold, silver, and lead amounting to $15,000,000 annually.
According to Lincoln H. Hall, "The outpouring of the precious metal from Leadville transformed the struggling Centennial State into a veritable autocrat in the colony of states. As if by magic the rough frontier town of Denver became a metropolis; stately buildings arose on the site of shanties; crystal streams flowed through the arid plains and the desert blossomed and became fruitful. Poverty gave way to the annoyance of wealth and the fame of silver state spread throughout the world." (Conant, 106)
Swindles were not uncommon in the mining community. When the Little Pittsburg mine was exhausted of its rich ore body, its managers sold their shares while concealing the mine's actual condition from the other stockholders. "Chicken Bill" Lovell dumped a wheelbarrow load of silver-rich ore into a barren pit on his Chrysolite claim in order to sell it to Horace Tabor for a large price. Tabor had the last laugh when his miners dug a few feet farther and discovered a rich ore body. Some time later the manager of the Chrysolite mine fooled an outside mining engineer into overestimating the mine's ore reserves.
The city's fortunes declined with the repeal of the Sherman Silver Purchase Act in 1893, although afterwards there was another small gold boom. Mining companies came to rely increasingly on income from the lead and zinc. The district is credited with producing over 2.9 million troy ounces of gold, 240 million troy ounces of silver, 1 million short tons of lead, 785 thousand short tons of zinc, and 53 thousand short tons of copper.
A bitter strike by Leadville's hard rock miners in 1896–97 led to bloodshed, at least five deaths, and the burning of the Coronado Mine.
World War II caused an increase in the demand for molybdenum, used to harden steel. It was mined at the nearby Climax mine, which at one time produced 75 percent of the world's output. By 1980 The Climax Mine was the largest underground mine in the world. Taxes paid by the mine provided Leadville with good schools, libraries, and provided employment for many residents. When the market dropped in 1981, Leadville's economy suffered and many people lost their jobs. With little industry other than the tourist trade, most of the former miners left and the standard of living declined. Climax reopened in 2008 and started production in 2010. It currently is the most efficient mine producing the metal in Colorado and estimated to have a production life of thirty years.
Leadville's colorful past.
As the population boomed, by 1878 Leadville had the reputation as one of the most lawless towns in the West. The first city marshal was run out of town a few days after he was appointed and his replacement was shot dead within a month by one of his deputies. Fearing the town would be lost to the lawless element, Mayor Horace Tabor sent for Mart Duggan, who was living in Denver, as a replacement. Duggan is little-known today, but was well known as a fearless gunfighter. Using strong-arm and lawless tactics, during his two stints as marshal Duggan brought order to Leadville by 1880 when he stepped down. He was shot and killed in 1889 by an unknown assailant, most likely an enemy he had made when he was a Leadville marshal. Historian Robert Dearment writes: "Mart Duggan was a quick-shooting, hard-drinking, brawling tough Irish man, but he was exactly the kind of man a tough, hard-drinking, quick-shooting camp like Leadville needed in its earliest days. His name is all but forgotten today but the name 'Matt Dillon' is recognized around the world. Such are the vagaries of life."
Alice Ivers, better known as Poker Alice, was a card player and dealer of the Old West who learned her trade in Leadville. Born in Devonshire, her family moved to America when she was a small girl. They first settled in Virginia where she attended an elite girl's boarding school. When she was a teenager her family moved to Leadville when the silver boom drew hundreds of new residents to the area. At the age of twenty she married a mining engineer who, like many of the men at that time, frequented the numerous gambling halls in Leadville. Alice went along, at first just observing, but eventually she began to sit in on the games as well. After a few years of marriage her husband was killed in a mining accident and she turned to cards to support herself. Alice was attractive, dressed in the latest fashions, and was in great demand as a dealer. Eventually Alice left Leadville to travel the gambling circuit, as was common of the male gamblers of that time. She continued to dress in the latest fashions but took to smoking cigars. Well known throughout the West, gambling halls welcomed her because she was good for business. In her later years, Alice claimed to have won more than $250,000 at the gaming tables and never once cheated.
Texas Jack Omohundro, Confederate scout, cowboy and stage actor with "Buffalo Bill" Cody's travelling revue, died of pneumonia a month before his 34th birthday in summer 1880 in Leadville where he was living on a small estate with his wife, ballerina Giuseppina Morlacchi.
Around 1883, shortly after the gun fight at the O.K. Corral, outlaw Doc Holliday moved to Leadville. On 19 August 1884, he shot ex-Leadville policeman, Billy Allen, who had threatened him for failing to pay a $5 debt. Despite overwhelming evidence implicating him, a jury found Holliday not guilty of the shooting or attempted murder.
Gunfighter and professional gambler Luke Short also spent time in Leadville.
Margaret "Molly" Brown, who became known as "The Unsinkable Molly Brown", moved to Leadville when she was 15. In 1886 she married a mining engineer who was twice her age, James J. Brown. The Brown family acquired great wealth in 1893 when Brown was instrumental in the discovery of a substantial ore seam at the Little Jonny Mine owned by his employers, the Ibex Mining Company. Molly became famous because of her survival of the 1912 sinking of the RMS "Titanic", after exhorting the crew of to return to look for survivors. A 1960 Broadway musical based on her life was produced, along with a 1964 film adaptation of the musical, both titled "The Unsinkable Molly Brown".
Oscar Wilde appeared at the Tabor Opera House during his 1882 American Aesthetic Movement lecture tour. The reviews were mixed and the press satirized Wilde in cartoons as an English dandy decorated with sunflowers and lilies, the floral emblems of the Aesthetic Movement. A Kansas newspaper described the event:
Oscar Wilde's visit to Leadville excited a great deal of interest and curiosity. The Tabor-opera house where he lectured was packed full. It was rumored that an attempt would be make by a number of young men to ridicule him by coming to the lecture in exaggerated costume with enormous sunflowers and lilies and to introduce a number of characters in the costume of the Western "bad men". Probably, however, better council prevailed and no disturbance took place
Mayor David H. Dougan invited Wilde to tour the Matchless Mine and name its new lode, "The Oscar". Wilde later recounted a visit to a local saloon, "where I saw the only rational method of art criticism I have ever come across. Over the piano was printed a notice – 'Please do not shoot the pianist. He is doing his best.'"
Post-mining era.
The many years of mining left behind substantial contamination of the soil and water, so that the Environmental Protection Agency designated some former mining sites as Superfund sites. The town is now 98% cleaned up and the Superfund designation is about to expire.
The town has made major efforts to improve its economy by encouraging tourism and emphasizing its history and opportunities for outdoor recreation. The National Mining Museum and Hall of Fame opened in 1987 with a federal charter. The town's altitude and rugged terrain contributes to a number of challenging racing events, such as the Leadville Trail 100 series of races. It is often used as a base for altitude training and hosts a number of other events for runners and mountain bicyclists.
Geography.
Although a few higher unincorporated settlements exist, Leadville is the highest incorporated city in the United States. At an elevation of 10,152 ft (3,094 m), it lies right at the edge of timberline, which in Colorado is from 11,000 to 12,000 feet. The surrounding peaks are all well above 12,000, thus they are all bare of trees.
Leadville lies in a valley at the head waters of the Arkansas River which flows through the southern Rocky Mountains and eventually empties into the Mississippi River. It is situated between two mountain ranges, the Mosquito Range to the east and the Sawatch Range to the west; both of which include several nearby peaks with elevations above 14,000 feet, the so-called fourteeners. Mount Elbert, about 16 miles southwest of Leadville, is the highest summit of the Rocky Mountains of North America and the highest point in the Colorado and the entire Mississippi River drainage basin. An ultra-prominent fourteener, Mount Elbert is the highest summit of the Sawatch Range and the second-highest summit in the contiguous United States after Mount Whitney. Mount Massive, 10.6 miles (17.1 km) west-southwest of Leadville, at 14,428-foot (4,398 m) is the second highest summit in the Rocky Mountains and state of Colorado, and the third highest in the nation.
Turquoise Lake lies on the northern outskirts of Leadville. The surface available for recreation includes 780 acres. Turquoise Lake is a feature of the Fryingpan-Arkansas Project. Recreation is managed by the Forest Service for Reclamation as part of the San Isabel National Forest.
Climate.
Leadville has an alpine subarctic climate with cold winters and mild summers, bordering on a cold semi-arid climate. The average January temperatures are a maximum of and a minimum of . The average July temperatures are a maximum of and a minimum of . There are an average of 278 days annually with freezing temperatures, which can occur in any month of the year. The record high temperature was on August 12, 1903. The record low temperature was on January 28, 1948.
Average annual precipitation is . The wettest year was 1957 with and the driest year was 1994 with . The most precipitation in one month was in January 1996. The most precipitation in 24 hours was on December 24, 1983. Average annual snowfall is . The most snowfall in one year was in 1996. The most snowfall in one month was in February 1995.
Demographics.
As of the census of 2000, there were 2,821 people, 1,253 households, and 675 families residing in the city. The population density was 2,659.5 people per square mile (1,027.5/km²). There were 1,514 housing units at an average density of 1,427.3 per square mile (551.5/km²). The racial makeup of the city was 83.52% White, 0.14% African American, 1.28% Native American, 0.32% Asian, 0.11% Pacific Islander, 12.34% from other races, and 2.30% from two or more races. Hispanic or Latino of any race were 25.45% of the population.
There were 1,253 households out of which 24.7% had children under the age of 18 living with them, 40.7% were married couples living together, 8.5% had a female householder with no husband present, and 46.1% were non-families. 35.0% of all households were made up of individuals and 9.2% had someone living alone who was 65 years of age or older. The average household size was 2.23 and the average family size was 2.91.
In the city the population was spread out with 22.1% under the age of 18, 12.1% from 18 to 24, 34.4% from 25 to 44, 22.0% from 45 to 64, and 10.4% who were 65 years of age or older. The median age was 34 years. For every 100 females there were 109.1 males. For every 100 females age 18 and over, there were 107.8 males.
The median income for a household in the city was $36,714, and the median income for a family was $44,444. Males had a median income of $28,125 versus $23,512 for females. The per capita income for the city was $20,607. About 9.1% of families and 13.3% of the population were below the poverty line, including 18.5% of those under age 18 and 7.5% of those age 65 or over.
Historic sites and districts.
Leadville Historic District was designated a National Historic Landmark District in 1961. The district encompasses 67 mines east of the city up to the 12,000 foot (3658 m) elevation level, and a defined portion of the village area, with specific exclusions for various buildings. The principal historic buildings are: Tabor Grand Hotel, St George's Church, Temple Israel, the Annunciation Church, Tabor Opera House, City Hall, Healy House, Dexter Cabin, Engelbach House, and Tabor House, as well as mining structures and small homes.
The National Mining Hall of Fame is dedicated to commemorating the work of miners and people that work with natural resources. It is listed on the National Register of Historic Places. Major exhibits include an elaborate model railroad, a walk-through replica of an underground hardrock mine, the Gold Rush Room with specimens of native gold, a large collection of mineral specimens, and a mining art gallery. The site also includes the Matchless Mine and cabin, former home of Baby Doe Tabor.
Some historic sites are linked by Mineral Belt National Recreation Trail, an 11.6 mile all-season biking/walking trail that loops around Leadville and through its historic mining district. In part it follows old mining-camp railbeds. Interpretative kiosks recount the history and a photograph of what was on that particular site more than a century ago. The trail is well-marked with interpretive signs and altitude and mileage markers.
Camp Hale is located north of Leadville in the Eagle River valley near Tennessee Pass . It was a U.S. Army ski warfare training facility constructed in 1942 for what became the 10th Mountain Division. Some of the nation's finest skiers were employed as instructors. Soldiers were trained in mountain climbing, Alpine and Nordic skiing, and cold-weather survival, as well as various weapons and ordnance. When it was in full operation, approximately 15,000 soldiers were housed there. As the only source of recreation for the trainees, Leadville was persuaded to change its moral character, perceived "to be on a rather low plane" at the time. Today Camp Hale is designated as a National Historic Site offering a self-guiding tour with interpretive signs at ten stops and a larger interpretive site at the main entrance. Ten miles north of Leadville the old downhill training slope, Cooper Hill, located atop Tennessee Pass which traverses the continental divide, now operates as the ski area known as Ski Cooper. Much of the area is above the tree line, providing a panoramic view of the peaks of the Sawatch Range to visitors. A memorial to troops of the 10th Mountain Division is located at the summit.
Culture and sport.
Boom Days, held on the first full weekend of August, is a tribute to the city's mining past. The event has been honored by the United States Congress as a Local Legacy Event. The festivities held over three days include mining competitions and burro racing, motorcycle games, a rod and gun show, live music, a craft fair and parade. The annual Skijoring event and Crystal Carnival take place in March. This is a horse-drawn skiing for the family since the 1960s. The town has frequent, sometimes small parades held in the downtown area, such as the quirky "St Patrick's Day Practice Parade".
The Leadville Trail 100, an ultramarathon, takes place each August on an out-and-back course on trails around Turquoise Lake, over Hagerman Pass, the Colorado Trail, through Twin Lakes, across the Arkansas River, up and over Hope Pass, to the ghost town of Winfield. It then returns along the same course.
The Mineral Belt Trail is an 11.6 mile, two way non-motorized paved trail around the city. Five access points offer opportunities to walk shorter sections: Ice Palace Park, Lake County Middle School, Dutch Henry Hill, California Gulch, and the East 5th Street Bridge. Mineral Belt is completely ADA-accessible for wheelchairs and strollers, cyclists, runners and in-line skaters. http://mineralbelttrail.com/
The "Route of the Silver Kings" is a driving tour of the 20-square-milehistoric mining district. The tour passess mines, power plants, ghost towns and mining camps.
Outdoor recreation.
Situated within the San Isabel National Forest and surrounded by three wilderness areas, Leadville is popular with hikers and campers. The Mount Massive Wilderness, the Buffalo Peaks Wilderness, and the Collegiate Peaks Wilderness are all within a few miles of Leadville.
Turquoise Lake lies on the northern outskirts of Leadville. Recreation facilities consist of eight campgrounds and two boat-launching ramps. The surface available for recreation includes 780 acres. Primary recreation activities include camping and fishing. Fish species include mackinaw trout, rainbow trout, and brook trout. The facilities are closed in winter due to ice and snow, but they remain a popular area for ice fishing. Turquoise Lake is a feature of the Fryingpan-Arkansas Project. Recreation is managed by the Forest Service for Reclamation as part of the San Isabel National Forest.
The Top of the Rockies Byway, designated a National Scenic Byway in 1998, is a highway that travels 75 miles starting in Aspen and traveling through Leadville to either Minturn or Copper Mountain. Seldom dropping below 9,000 feet, it is literally at the top of the Rockies. It drives over three mountain passes that are above 10,000 feet and there are views of six mountains of over 14,000 feet. The Top of the Rockies byway runs through three National Forests: the Pike, Arapahoe, and White River National Forests. The Camp Hale Memorial is located along the byway, where soldiers trained on skis to fight in the Apennine Mountains of Italy during World War II. The road passes through the Arkansas Headwaters Recreation Area and the
Arkansas River Headwaters State Park.
Transportation.
Leadville is served by Lake County Airport. However, there are no scheduled airline services available from this airport. The closest airports to provide scheduled services are Eagle County Airport and Aspen-Pitkin County Airport, both located away.
All of the highways in Lake County are part of the Top of the Rockies Scenic and Historic Byway.
References.
20."Leadville: The Struggle to Revive An American Town" by Gillian Klucas p. 21
21. Conant Graff, Marshall. "A History of Leadville, Colorado." 1920.
22. Scanlon, Gretchen. "A History of Leadville Theatre: Opera Houses, Variety Acts and Burlesque Shows." 2012.
23. Kent, Lewis A. "Leadville: The City. Mines and Bullion Product. Personal Histories of Prominent Citizens, Facts and Figures Never Before Given to the Public." 1880.
24. "Holliday Bound Over to Appear at the Criminal Court in the Sum of Eight Thousand Dollars." Leadville Daily Herald. August 26, 1884. (P.4)

</doc>
<doc id="48923" url="https://en.wikipedia.org/wiki?curid=48923" title="Pike's Peak Gold Rush">
Pike's Peak Gold Rush

The Pike's Peak Gold Rush (later known as the Colorado Gold Rush) was the boom in gold prospecting and mining in the Pike's Peak Country of western Kansas Territory and southwestern Nebraska Territory of the United States that began in July 1858 and lasted until roughly the creation of the Colorado Territory on February 28, 1861. An estimated 100,000 gold seekers took part in one of the greatest gold rushes in North American history.
The participants in the gold rush were known as "Fifty-Niners" after 1859, the peak year of the rush and often used the motto Pike's Peak or Bust! But in fact the location of the Pike's Peak Gold Rush was north of Pike's Peak. It was only named Pike's Peak Gold Rush because of how well known and important Pike's Peak was.
Overview.
The Pike's Peak Gold Rush, which followed the California Gold Rush by approximately one decade, produced a dramatic but temporary influx of immigrants into the Pike's Peak Country of the Southern Rocky Mountains. The rush was exemplified by the slogan "Pike's Peak or Bust!", a reference to the prominent mountain at the eastern edge of the Rocky Mountains that guided many early prospectors to the region westward over the Great Plains. The prospectors provided the first major European-American population in the region.
The rush created a few mining camps such as Denver City and Boulder City that would develop into cities. Many smaller camps such as Auraria and Saint Charles City were absorbed by larger camps and towns. Scores of other mining camps have faded into ghost towns, but quite a few camps such as Central City, Black Hawk, Georgetown, and Idaho Springs survive.
Discovery.
For many years people had suspected the mountains had numerous rich gold deposits. In 1835, a French trapper by the name of Eustace Carriere had ended up losing his party and wandered through the mountains for many weeks. During these weeks he found many gold specimens which he later took back to New Mexico for examination. Upon examination, they turned out to be "pure gold". But when he tried to lead an expedition back to the location of where he found the gold, they came up short because he could not quite remember the location.
In 1849 and 1850, several parties of gold seekers bound for the California Gold Rush panned small amounts of gold from various streams in the South Platte River valley at the foot of the Rocky Mountains. The Rocky Mountain gold failed to impress or delay men with visions of unlimited wealth in California, and the discoveries were not reported for several years.
As the hysteria of the California Gold Rush faded, many discouraged gold seekers returned home. Rumors of gold in the Rocky Mountains persisted and several small parties explored the region. In the summer of 1857, a party of Spanish-speaking gold seekers from New Mexico worked a placer deposit along the South Platte River about 5 miles (8 kilometers) above Cherry Creek in what is today Denver.
William Greeneberry "Green" Russell was a Georgian who worked in the California gold fields in the 1850s. Russell was married to a Cherokee woman, and through his connections to the tribe, he heard about an 1849 discovery of gold along the South Platte River. Green Russell organized a party to prospect along the South Platte River, setting off with his two brothers and six companions in February 1858. They rendezvoused with Cherokee tribe members along the Arkansas River in present-day Oklahoma and continued westward along the Santa Fe Trail. Others joined the party along the way until their number reached 107.
Upon reaching Bent's Fort, they turned to the northwest, reaching the confluence of Cherry Creek and the South Platte on May 23. The site of their initial explorations is in present-day Confluence Park in Denver. They began prospecting in the river beds, exploring Cherry Creek and nearby Ralston Creek but without success. In the first week of July 1858, Green Russell and Sam Bates found a small placer deposit near the mouth of Little Dry Creek that yielded about 20 troy ounces (622 grams) of gold, the first significant gold discovery in the Rocky Mountain region. The site of the discovery is in the present-day Denver suburb of Englewood, just north of the junction of U.S. Highway 285 and U.S. Highway 85.
The initial boom.
The first decade of the boom was largely concentrated along the South Platte River at the base of the mountains, the canyon of Clear Creek in the mountains west of Golden City, at Breckenridge and in South Park at Como, Fairplay, and Alma. By 1860, Denver City, Golden City, and Boulder City were substantial towns serving the mines. Rapid population growth led to the creation of the Colorado Territory in 1861.
The Pike's Peak Gold Rush sent everyone into a frenzy. Anyone who could afford it was going to pack up their belongings and head out to Colorado. They gathered their supplies, wagons, mules, mining equipment, anything they could think of to sustain them when they arrived at their destination. As soon as the spring of 1859 came around people were in a race to get to Pike's Peak. Some brave adventures even dared to go out in the winter of 1858 to try and get a head start only to realize that they would have to wait until the snow melted to even begin their mining for gold.
Free gold.
Hardrock mining boomed for a few years, but then declined in the mid-1860s as the miners exhausted the shallow parts of the veins that contained free gold, and found that their amalgamation mills could not recover gold from the deeper sulfide ores. This problem was eventually solved and gold and silver mining in Colorado became a major industry.
Colorado produced 150,000 ounces of gold in 1861 and 225,000 troy ounces in 1862. This led Congress to establish the Denver Mint. Cumulative Colorado production by 1865 was 1.25 million ounces, of which sixty percent was placer gold.

</doc>
