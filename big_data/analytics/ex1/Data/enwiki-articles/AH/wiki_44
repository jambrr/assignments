<doc id="51470" url="https://en.wikipedia.org/wiki?curid=51470" title="Mycelium">
Mycelium

[[Image:Oyster mushroom (Pleurotus ostreatus) mycelium in petri dish on coffee grounds.JPG|thumb|right|Oyster mushroom 
(Pleurotus ostreatus) growing on coffee grounds]]
Mycelium is the vegetative part of a fungus, consisting of a mass of branching, thread-like hyphae. The mass of hyphae is sometimes called shiro, especially within the fairy ring fungi. Fungal colonies composed of mycelium are found in and on soil and many other substrates. A typical single spore germinates into a homokaryotic mycelium, which cannot reproduce sexually; when two compatible homokaryotic mycelia join and form a dikaryotic mycelium; that mycelium may form fruiting bodies such as mushrooms. A mycelium may be minute, forming a colony that is too small to see, or it may be extensive:
Through the mycelium, a fungus absorbs nutrients from its environment. It does this in a two-stage process. First, the hyphae secrete enzymes onto or into the food source, which break down biological polymers into smaller units such as monomers. These monomers are then absorbed into the mycelium by facilitated diffusion and active transport.
Mycelium is vital in terrestrial and aquatic ecosystems for their role in the decomposition of plant material. They contribute to the organic fraction of soil, and their growth releases carbon dioxide back into the atmosphere (see carbon cycle). Ectomycorrhizal extramatrical mycelium, as well as the mycelium of Arbuscular mycorrhizal fungi increase the efficiency of water and nutrient absorption of most plants and confers resistance to some plant pathogens. Mycelium is an important food source for many soil invertebrates.
"Mycelium", like "fungus", can be considered a mass noun, a word that can be either singular or plural. The term "mycelia", though, like "fungi", is often used as the preferred plural form.
Sclerotia are compact or hard masses of mycelium.
Uses.
One of the primary roles of fungi in an ecosystem is to decompose organic compounds. Petroleum products and some pesticides (typical soil contaminants) are organic molecules (i.e., they are built on a carbon structure), and thereby present a potential carbon source for fungi. Hence, fungi have the potential to eradicate such pollutants from their environment unless the chemicals prove toxic to the fungus. This biological degradation is a process known as bioremediation.
Mycelial mats have been suggested (see Paul Stamets) as having potential as biological filters, removing chemicals and microorganisms from soil and water. The use of fungal mycelium to accomplish this has been termed "mycofiltration".
Knowledge of the relationship between mycorrhizal fungi and plants suggests new ways to improve crop yields.
When spread on logging roads, mycelium can act as a binder, holding new soil in place and preventing washouts until woody plants can be established.
Since 2007, a company called Ecovative Design has been developing alternatives to polystyrene and plastic packaging by growing mycelium in agricultural waste. The two ingredients are mixed together and placed into a mold for 3–5 days to grow into a durable material. Depending on the strain of mycelium used, they make many different varieties of the material including water absorbent, flame retardant, and dielectric.
Fungi are essential for converting biomass into compost, as they decompose feedstock components such as lignin, which many other composting microorganisms cannot. Turning a backyard compost pile will commonly expose visible networks of mycelia that have formed on the decaying organic material within. Compost is an essential soil amendment and fertilizer for organic farming and gardening. Composting can divert a substantial fraction of municipal solid waste from landfill.

</doc>
<doc id="51472" url="https://en.wikipedia.org/wiki?curid=51472" title="Spore">
Spore

In biology, a spore is a unit of asexual reproduction that may be adapted for dispersal and for survival, often for extended periods of time, in unfavorable conditions. By contrast, gametes are units of sexual reproduction. Spores form part of the life cycles of many plants, algae, fungi and protozoa. Bacterial spores are not part of a sexual cycle but are resistant structures used for survival under unfavourable conditions. Myxozoan spores release amoebulae into their hosts for parasitic infection, but also reproduce within the hosts through the pairing of two nuclei within the plasmodium, which develops from the amoebula.
Spores are usually haploid and unicellular and are produced by meiosis in the sporangium of a diploid sporophyte. Under favourable conditions the spore can develop into a new organism using mitotic division, producing a multicellular gametophyte, which eventually goes on to produce gametes. Two gametes fuse to form a zygote which develops into a new sporophyte. This cycle is known as alternation of generations.
The spores of seed plants, however, are produced internally and the megaspores, formed within the ovules and the microspores are involved in the formation of more complex structures that form the dispersal units, the seeds and pollen grains.
Definition.
The term "spore" derives from the ancient Greek word σπορά "spora", meaning "seed, sowing," related to σπόρος "sporos", "sowing," and σπείρειν "speirein", "to sow."
In common parlance, the difference between a "spore" and a "gamete" (both together called gonites) is that a spore will germinate and develop into a sporeling, while a gamete needs to combine with another gamete to form a zygote before developing further.
The chief difference between spores and seeds as dispersal units is that spores are unicellular, while seeds contain within them a multicellular gametophyte that produces a developing embryo, the multicellular sporophyte of the next generation. Spores germinate to give rise to haploid gametophytes, while seeds germinate to give rise to diploid sporophytes.
Classification of spore-producing organisms.
Vascular plant spores are always haploid. Vascular plants are either homosporous (or isosporous) or heterosporous. Plants that are homosporous produce spores of the same size and type. Heterosporous plants, such as seed plants, spikemosses, quillworts, and some aquatic ferns produce spores of two different sizes: the larger spore (megaspore) in effect functioning as a "female" spore and the smaller (microspore) functioning as a "male".
Classification of spores.
Spores can be classified in several ways:
By spore-producing structure.
In fungi and fungus-like organisms, spores are often classified by the structure in which meiosis and spore production occurs. Since fungi are often classified according to their spore-producing structures, these spores are often characteristic of a particular taxon of the fungi.
By mobility.
Spores can be differentiated by whether they can move or not.
Anatomy.
Under high magnification, spores can be categorized as either monolete spores or trilete spores. In monolete spores, there is a single line on the spore indicating the axis on which the mother spore was split into four along a vertical axis. In trilete spores, all four spores share a common origin and are in contact with each other, so when they separate, each spore shows three lines radiating from a center pole.
Spore tetrads and trilete spores.
Envelope-enclosed spore tetrads are taken as the earliest evidence of plant life on land, dating from the mid-Ordovician (early Llanvirn, ~), a period from which no macrofossils have yet been recovered.
Individual trilete spores resembling those of modern cryptogamic plants first appeared in the fossil record at the end of the Ordovician period.
Dispersal.
In fungi, both asexual and sexual spores or sporangiospores of many fungal species are actively dispersed by forcible ejection from their reproductive structures. This ejection ensures exit of the spores from the reproductive structures as well as travelling through the air over long distances. Many fungi thereby possess specialized mechanical and physiological mechanisms as well as spore-surface structures, such as hydrophobins, for spore ejection. These mechanisms include, for example, forcible discharge of ascospores enabled by the structure of the ascus and accumulation of osmolytes in the fluids of the ascus that lead to explosive discharge of the ascospores into the air. The forcible discharge of single spores termed "ballistospores" involves formation of a small drop of water (Buller's drop), which upon contact with the spore leads to its projectile release with an initial acceleration of more than 10,000 g. Other fungi rely on alternative mechanisms for spore release, such as external mechanical forces, exemplified by puffballs. Attracting insects, such as flies, to fruiting structures, by virtue of their having lively colours and a putrid odour, for dispersal of fungal spores is yet another strategy, most prominently used by the stinkhorns.
In Common Smoothcap moss ("Atrichum undulatum"), the vibration of sporophyte has been shown to be an important mechanism for spore release.
In the case of spore-shedding vascular plants such as ferns, wind distribution of very light spores provides great capacity for dispersal. Also, spores are less subject to animal predation than seeds because they contain almost no food reserve; however they are more subject to fungal and bacterial predation. Their chief advantage is that, of all forms of progeny, spores require the least energy and materials to produce.
In the spikemoss "Selaginella lepidophylla", dispersal is achieved in part by an unusual type of diaspore, a tumbleweed.

</doc>
<doc id="51474" url="https://en.wikipedia.org/wiki?curid=51474" title="Seat belt">
Seat belt

A seat belt, also known as a safety belt, is a vehicle safety device designed to secure the occupant of a vehicle against harmful movement that may result during a collision or a sudden stop. A seat belt functions to reduce the likelihood of death or serious injury in a traffic collision by reducing the force of secondary impacts with interior strike hazards, by keeping occupants positioned correctly for maximum effectiveness of the airbag (if equipped) and by preventing occupants being ejected from the vehicle in a crash or if the vehicle rolls over. When driving, the driver and passengers are travelling at the same speed as the car. If the car suddenly stops or crashes, the driver and passengers continue at the same speed the car was going before it stopped. A seatbelt applies an opposite force to the driver and passengers to prevent them from falling out or making contact with the interior of the car.
Seatbelts are considered as Primary Restraint Systems (PRS) as they play a vital role in occupant safety.
Efficiency.
An analysis conducted in the United States in 1984 compared a variety of seat belt types alone and in combination with air bags. The range of fatality reduction for front seat passengers was broad, from 20% to 55%, as was the range of major injury, from 25% to 60%. More recently, the CDC has summarized this data by stating "seat belts reduce serious crash-related injuries and deaths by about half."
History.
Seat belts were invented by English engineer George Cayley in the mid-19th century, though Edward J. Claghorn of New York, was granted the first patent (, on February 10, 1885 for a safety belt). Claghorn was granted United States Patent #312,085 for a Safety-Belt for tourists, painters, firemen, etc. who are being raised or lowered, described in the patent as "designed to be applied to the person, and provided with hooks and other attachments for securing the person to a fixed object."
In 1911, Benjamin Foulois had the cavalry saddle shop fashion a belt for the seat of Wright Flyer Signal Corps 1. He wanted it to hold him firmly in his seat so he could better control his aircraft as he bounded along the rough field used for takeoff and landing. It was not until World War II that seat belts were fully adopted in military aircraft, and even then, it was mainly for safety reasons, not improved aircraft control.
In 1946, Dr. C. Hunter Shelden had opened a neurological practice at Huntington Memorial Hospital in Pasadena, California. In the early 1950s, Dr. Shelden had made a major contribution to the automotive industry with his idea of retractable seat belts. This came about greatly in part from the high number of head injuries coming through the emergency rooms. He investigated the early seat belts whose primitive designs were implicated in these injuries and deaths. His findings were published in the November 5, 1955 "Journal of the American Medical Association" (JAMA) in which he proposed not only the retractable seat belt, but also recessed steering wheels, reinforced roofs, roll bars, door locks and passive restraints such as the air bag. Subsequently, in 1959, Congress passed legislation requiring all automobiles to comply with certain safety standards.
American car manufacturers Nash (in 1949) and Ford (in 1955) offered seat belts as options, while Swedish Saab first introduced seat belts as standard in 1958. After the Saab GT 750 was introduced at the New York Motor Show in 1958 with safety belts fitted as standard, the practice became commonplace.
Glenn Sheren of Mason, Michigan submitted a patent application on March 31, 1955 for an automotive seat belt and was awarded US Patent 2,855,215 in 1958. This was a continuation of an earlier patent application that Mr. Sheren had filed on September 22, 1952.
However, the first modern three point seat belt (the so-called "CIR-Griswold restraint") used in most consumer vehicles today was patented in 1955 by the Americans Roger W. Griswold and Hugh DeHaven. Fatal car accidents were rapidly increasing in Sweden during the 1950s. When a study at Vattenfall of accidents among employees revealed that the majority of casualties came from car accidents, two Vattenfall engineers (Bengt Odelgard and Per-Olof Weman) started to develop the safety belt. Their work set the standard for safety belts in Swedish cars and was presented to Swedish manufacturer Volvo in the late 1950s. The 3-point seatbelt was developed to its modern form by Swedish inventor Nils Bohlin for Volvo—who introduced it in 1959 as standard equipment. In addition to designing an effective three-point belt, Bohlin demonstrated its effectiveness in a study of 28,000 accidents in Sweden. Unbelted occupants sustained fatal injuries throughout the whole speed scale, whereas none of the belted occupants were fatally injured at accident speeds below 60 mph. No belted occupant was fatally injured if the passenger compartment remained intact. Bohlin was granted for the device.
The world's first seat belt law was put in place in 1970, in the state of Victoria, Australia, making the wearing of a seat belt compulsory for drivers and front-seat passengers. This legislation was enacted after trialing Hemco seatbelts, designed by Desmond Hemphill (1926–2001), in the front seats of police vehicles, lowering the incidence of officer injury and death.
Types.
Two-point.
A 2-point belt attaches at its two endpoints, and was invented in the early 1900s by Jack Swearingen of Louisville, Kentucky.
Lap.
A lap belt is a strap that goes over the waist. This was the most commonly installed type of belt prior to legislation requiring 3-point belts, and is primarily found in older cars. Coaches are equipped with lap belts (although many newer coaches have three-point belts), as are passenger aircraft seats.
University of Minnesota Professor James J. (Crash) Ryan was the inventor of and held the patent on the automatic retractable lap safety belt. Ralph Nader cited Ryan's work in Unsafe at Any Speed and in 1966 President Lyndon Johnson signed two bills requiring safety belts in all passenger vehicles starting in 1968.
Until the 1980s, three-point belts were commonly available only in the front outboard seats of cars; the back seats were only often fitted with lap belts. Evidence of the potential of lap belts to cause separation of the lumbar vertebrae and the sometimes associated paralysis, or "seat belt syndrome", led to progressive revision of passenger safety regulations in nearly all developed countries to require 3-point belts first in all outboard seating positions and eventually in all seating positions in passenger vehicles. Since September 1, 2007, all new cars sold in the U.S. require a lap and shoulder belt in the center rear seat. 
Besides regulatory changes, "seat belt syndrome" has led to tremendous liability for vehicle manufacturers. One Los Angeles case resulted in a $45 million jury verdict against the Ford Motor Company; the resulting $30 million judgment (after deductions for another defendant who settled prior to trial) was affirmed on appeal in 2006.
Sash.
A "sash" or shoulder harness is a strap that goes diagonally over the vehicle occupant's outboard shoulder and is buckled inboard of his or her lap. The shoulder harness may attach to the lap belt tongue, or it may have a tongue and buckle completely separate from those of the lap belt. Shoulder harnesses of this separate or semi-separate type were installed in conjunction with lap belts in the outboard front seating positions of many vehicles in the North American market starting at the inception of the shoulder belt requirement of the U.S. National Highway Traffic Safety Administration's Federal Motor Vehicle Safety Standard 208 on 1 January 1968. However, if the shoulder strap is used without the lap belt, the vehicle occupant is likely to "submarine", or slide forward in the seat and out from under the belt, in a frontal collision. In the mid-1970s, 3-point belt systems such as Chrysler's "Uni-Belt" began to supplant the separate lap and shoulder belts in American-made cars, though such 3-point belts had already been supplied in European vehicles such as Volvos, Mercedes, and Saabs for some years.
Three-point.
A 3-point belt is a Y-shaped arrangement, similar to the separate lap and sash belts, but unitized. Like the separate lap-and-sash belt, in a collision the 3-point belt spreads out the energy of the moving body over the chest, pelvis, and shoulders. Volvo introduced the first production three-point belt in 1959. The first car with a three-point belt was a Volvo PV 544 that was delivered to a dealer in Kristianstad on August 13, 1959. However, the first car model to feature the three-point seat belt as a standard item was the 1959 Volvo 122, first outfitted with a two-point belt at initial delivery in 1958, replaced with the three-point seat belt the following year. The three-point belt was developed by Nils Bohlin who had earlier also worked on ejection seats at Saab. Volvo then made the new seat belt design patent open in the interest of safety and made it available to other car manufacturers for free.
Belt-in-Seat (BIS).
The BIS is a three-point harness with the shoulder belt attached to the seat itself, rather than to the vehicle structure. The first car using this system was the Range Rover Classic. Fitment was standard on the front seats from 1970. Some cars like the Renault Vel Satis use this system for the front seats. A General Motors assessment concluded seat-mounted 3-point belts offer better protection especially to smaller vehicle occupants, though GM did not find a safety performance improvement in vehicles with seat-mounted belts versus body-mounted belts.
BIS type belts have been used by automakers in convertibles and pillarless hardtops, where there is no "B" pillar to affix the upper mount of the belt. Chrysler and Cadillac are well known for using this design. Antique auto enthusiasts sometimes replace original seats in their cars with BIS-equipped front seats, providing a measure of safety not available when these cars were new. However, modern BIS systems typically use electronics that must be installed and connected with the seats and the vehicle's electrical system in order to function properly.
4-, 5-, and 6-point.
Five-point harnesses are typically found in child safety seats and in racing cars. The lap portion is connected to a belt between the legs and there are two shoulder belts, making a total of five points of attachment to the seat. A 4-point harness is similar, but without the strap between the legs, while a 6-point harness has two belts between the legs. In NASCAR, the 6-point harness became popular after the death of Dale Earnhardt, who was wearing a five-point harness when he suffered his fatal crash; as it was first thought that his belt had broken, and broke his neck at impact, some teams ordered a six-point harness in response.
Seven-point.
Aerobatic aircraft frequently use a combination harness consisting of a five-point harness with a redundant lap-belt attached to a different part of the air craft. While providing redundancy for negative-g maneuvers (which lift the pilot out of the seat); they also require the pilot to un-latch two harnesses if it is necessary to parachute from a failed aircraft.
Technology.
Locking retractors.
The purpose of locking retractors is to provide the seated occupant the convenience of some free movement of the upper torso within the compartment, while providing a method of limiting this movement in the event of a crash. Most modern seat belts are stowed on spring-loaded reels called "retractors" equipped with inertial locking mechanisms that stop the belt from extending off the reel during severe deceleration. There are two main types of inertial seat belt lock. A webbing-sensitive lock is based on a centrifugal clutch activated by rapid acceleration of the strap (webbing) from the reel. The belt can be pulled from the reel only slowly and gradually, as when the occupant extends the belt to fasten it. A sudden rapid pull of the belt — as in a sudden braking or collision event — causes the reel to lock, restraining the occupant in position.
A vehicle-sensitive lock is based on a pendulum swung away from its plumb position by rapid deceleration or rollover of the vehicle. In the absence of rapid deceleration or rollover, the reel is unlocked and the belt strap may be pulled from the reel against the spring tension of the reel. The vehicle occupant can move around with relative freedom while the spring tension of the reel keeps the belt taut against the occupant. When the pendulum swings away from its normal plumb position due to sudden deceleration or rollover, a pawl is engaged, the reel locks and the strap restrains the belted occupant in position. Dual-sensing locking retractors use both vehicle G-loading and webbing payout rate to initiate the locking mechanism.
Pretensioners and webclamps.
Seatbelts in many newer vehicles are also equipped with "pretensioners" or "web clamps", or both.
Pretensioners preemptively tighten the belt to prevent the occupant from jerking forward in a crash. Mercedes-Benz first introduced pretensioners on the 1981 S-Class. In the event of a crash, a pretensioner will tighten the belt almost instantaneously. This reduces the motion of the occupant in a violent crash. Like airbags, pretensioners are triggered by sensors in the car's body, and many pretensioners have used explosively expanding gas to drive a piston that retracts the belt. Pretensioners also lower the risk of "submarining", which occurs when a passenger slides forward under a loosely fitted seat belt.
Some systems also pre-emptively tighten the belt during fast accelerations and strong decelerations, even if no crash has happened. This has the advantage that it may help prevent the driver from sliding out of position during violent evasive maneuvers, which could cause loss of control of the vehicle. These pre-emptive safety systems may "prevent" some collisions from happening, as well as reducing injury in the event an actual collision occurs. Pre-emptive systems generally use electric pretensioners which can operate repeatedly and for a sustained period, rather than pyrotechnic pretensioners, which can only operate a single time.
Webclamps clamp the webbing in the event of an accident, and limit the distance the webbing can spool out (caused by the unused webbing tightening on the central drum of the mechanism). These belts also often incorporate an energy management loop ("rip stitching") in which a section of the webbing is looped and stitched with a special stitching. The function of this is to "rip" at a predetermined load, which reduces the maximum force transmitted through the belt to the occupant during a violent collision, reducing injuries to the occupant.
A study demonstrated that standard automotive 3-point restraints fitted with pyrotechnic or electric pretensioners were not able to eliminate all interior passenger compartment head strikes in rollover test conditions. Electric pretensioners are often incorporated on vehicles equipped with precrash systems; they are designed to reduce seat belt slack in a potential collision and assist in placing the occupants in a more optimal seating position. The electric pretensioners also can operate on a repeated or sustained basis, providing better protection in the event of an extended rollover or a multiple collision accident.
Inflatable.
The inflatable seatbelt was invented by Donald Lewis and tested at the Automotive Products Division of Allied Chemical Corporation. Inflatable seatbelts have tubular inflatable bladders contained within an outer cover. When a crash occurs the bladder inflates with a gas to increase the area of the restraint contacting the occupant and also shortening the length of the restraint to tighten the belt around the occupant, improving the protection. The inflatable sections may be shoulder-only or lap and shoulder. The system supports the head during the crash better than a web only belt. It also provides side impact protection. In 2013, Ford began offering rear seat inflatable seat belts on a limited set of models, such as the Explorer and Flex.
Automatic.
Seatbelts that automatically move into position around a vehicle occupant once the adjacent door is closed and/or the engine is started were developed as a countermeasure against low usage rates of manual seat belts, particularly in the United States. The first car to feature automatic shoulder belts as standard equipment was the 1981 Toyota Cressida, but the history of such belts goes back further.
The 1972 Volkswagen ESVW1 Experimental Safety Vehicle presented passive seat belts. Volvo tried to develop a passive three point seatbelt. In 1973 Volkswagen announced they had a functional passive seat belt. The first commercial car to use automatic seat belts was the 1975 Volkswagen Rabbit.
Automatic seat belts received a boost in the United States in 1977 when Brock Adams, United States Secretary of Transportation in the Carter Administration, mandated that by 1983 every new car should have either airbags or automatic seat belts despite strong lobbying from the auto industry. Adams was attacked by Ralph Nader, who said that the 1983 deadline was too late. Soon after, General Motors began offering automatic seat belts, first on the Chevrolet Chevette, but by early 1979 the VW Rabbit and the Chevette were the only cars to offer the safety feature, and GM was reporting disappointing sales. By early 1978, Volkswagen had reported 90,000 Rabbits sold with automatic seat belts. A study released in 1978 by the United States Department of Transportation claimed that cars with automatic seat belts had a fatality rate of .78 per 100 million miles, compared with 2.34 for cars with regular, manual belts.
In 1981, Drew Lewis, the first Transportation Secretary of the Reagan Administration, influenced by studies done by the auto industry, "killed" the previous administration's mandate; the decision was overruled in a federal appeals court the following year, and then by the Supreme Court. In 1984, the Reagan Administration reversed its course, though in the meantime the original deadline had been extended; Elizabeth Dole, then Transportation Secretary, proposed that the two passive safety restraints be phased into vehicles gradually, from vehicle model year 1987 to vehicle model year 1990, when all vehicles would be required to have either automatic seat belts or driver side air bags. Though more awkward for vehicle occupants, most manufacturers opted to use less expensive automatic belts rather than airbags during this time period.
When driver side airbags became mandatory on all passenger vehicles in model year 1995, most manufacturers stopped equipping cars with automatic seat belts. Exceptions include the 1995-1996 Ford Escort/Mercury Tracer and the Eagle Summit Wagon which had automatic safety belts along with dual airbags.
Disadvantages.
Automatic belt systems generally offer inferior occupant crash protection. In systems with belts attached to the door rather than a sturdier fixed portion of the vehicle body, a crash that causes the vehicle door to open leaves the occupant without belt protection. In such a scenario, the occupant may be thrown from the vehicle and suffer greater injury or death.
Because many automatic belt system designs compliant with the US passive-restraint mandate did not meet the safety performance requirements of Canada—which were not weakened to accommodate automatic belts—vehicle models which had been eligible for easy importation in either direction across the US-Canada border when equipped with manual belts became ineligible for importation in either direction once the US variants got automatic belts and the Canadian versions retained manual belts. Two such models were the Dodge Spirit and Plymouth Acclaim.
Automatic belt systems also present several operational disadvantages. Motorists who would normally wear seat belts must still fasten the manual lap belt, thus rendering redundant the automation of the shoulder belt. Those who do not fasten the lap belt wind up inadequately protected by only the shoulder belt; in a crash without a lap belt such a vehicle occupant is likely to "submarine" (be thrown forward under the shoulder belt) and be seriously injured. Motorized or door-affixed shoulder belts hinder access to the vehicle, making it difficult to enter and exit—particularly if the occupant is carrying items such as a box or a purse. Vehicle owners tend to disconnect the motorized or door-affixed shoulder belt to alleviate the nuisance of entering and exiting the vehicle, leaving only a lap belt for crash protection. Also, many automatic seat belt systems are incompatible with child safety seats, or compatible only with special modifications.
Homologation and testing.
Starting in 1971 and ending in 1972, the United States conducted a research project on seat belt effectiveness on a total of 40,000 vehicle occupants using car accident reports collected during that time. Of these 40,000 occupants, 18% were reported wearing lap belts, or two-point safety belts, 2% were reported wearing a three-point safety belt, and the remaining 80% was reported as wearing no safety belt. The results concluded that users of the two-point lap belt had a 73% lower fatality rate, a 53% lower serious injury rate, and a 38% lower injury rate, when compared with the occupants that were reported unrestrained. Similarly, users of the three-point safety belt had a 60% lower serious injury rate and a 41% lower rate of all other injuries. Out of the 2% described as wearing a three-point safety belt, no fatalities were reported.
This study and others led to the Restraint Systems Evaluation Program (RSEP), started by the National Highway Transport Safety Authority in 1975 to increase the reliability and authenticity of past studies. A study as part of this program used data taken from 15,000 tow-away accidents that involved only car models made between 1973 and 1975. The study found that for injuries considered “moderate” or worse, individuals wearing a three-point safety belt had a 56.5% lower injury rate than those wearing no safety belt. The study also concluded that the effectiveness of the safety belt did not differ with size of car. It was determined that the variation among results of the many studies conducted in the 1960s and 70s was due to the use of different methodologies, and could not be attributed to any significant variation in the effectiveness of safety belts.
Helping to improve safety apparatuses in vehicles, injury testing, and seat belt effectiveness are being tested today by Wayne State University’s Automotive Safety Research Group, as well as other researchers. Wayne State’s Bioengineering Center uses human cadavers in their crash test research. Albert King, the Center’s director, wrote in his 1995 article titled "Humanitarian Benefits of Cadaver Research on Injury Prevention” that that use of cadavers in this type of research since 1987 has saved and will continue to save nearly 8,500 lives each year, due to the vehicle safety improvements made possible by human cadaver testing. He also indicates that due to the improvements made to three-point safety belts an average of 61 lives are saved per year.
The New Car Assessment Program (NCAP) was put in place by the United States National Highway Traffic Safety Administration in 1979. The NCAP is a government program that evaluates vehicle safety designs and sets standards for foreign and domestic automobile companies. The agency has put in place a rating system and requires access to safety test results. , manufacturers are required to place a NCAP star rating on the automobile price sticker.
Experimental.
Research and development efforts are ongoing to improve the safety performance of vehicle seatbelts. Some experimental designs have included:
In rear seats.
In 1955 (as a 1956 package), Ford offered lap only seat belts in the rear seats as an option within the "Lifeguard" safety package. In 1967, Volvo started to install lap belts in the rear seats. In 1972, Volvo upgraded the rear seat belts to a three-point belt.
In crashes, unbelted rear passengers increase the risk of belted front seat occupants' death by nearly five times.
Child occupants.
As with adult drivers and passengers, the advent of seat belts was accompanied by calls for their use by child occupants, including legislation requiring such use. Generally children using adult seat belts suffer significantly lower injury risk when compared to non-buckled children.
The UK extended compulsory seatbelt wearing to child passengers under the age of 14 in 1989. It was observed that this measure was accompanied by a 10% "increase" in fatalities and a 12% "increase" in injuries among the target population. In crashes, small children who wear adult seatbelts can suffer "seat-belt syndrome" injuries including severed intestines, ruptured diaphragms and spinal damage. There is also research suggesting that children in inappropriate restraints are at significantly increased risk of head injury, one of the authors of this research has been quoted as claiming that: "The early graduation of kids into adult lap and shoulder belts is a leading cause of child-occupant injuries and deaths."
As a result of such findings, many jurisdictions now advocate or require child passengers to use specially designed child restraints. Such systems include separate child-sized seats with their own restraints and booster cushions for children using adult restraints. In some jurisdictions children below a certain size are forbidden to travel in front car seats."
Reminder chime and light.
In Europe and some other parts of the world, most modern cars include a seat-belt reminder light for the driver and some also include a reminder for the passenger, when present, activated by a pressure sensor under the passenger seat. Some cars will intermittently flash the reminder light and sound the chime until the driver (and sometimes the front passenger, if present) fasten their seatbelts.
In North America, cars sold since the early 1970s have included an audiovisual reminder system consisting of a tell-tale light on the dashboard and a buzzer or chime reminding the driver and passengers to fasten their belts. Originally, these lights were accompanied by a warning buzzer whenever the transmission was in any position except park if either the driver was not buckled up or, as determined by a pressure sensor in the passenger's seat, if there was a passenger there not buckled up. However, this was considered by many to be a major annoyance, as the light would be on and the buzzer would sound continuously if front-seat passengers were not buckled up. Therefore, people who did not wish to buckle up would defeat this system by fastening the seat belts with the seat empty and leaving them that way.
To combat this dangerous habit, in 1971 NHTSA amended Federal Motor Vehicle Safety Standard № 208 (FMVSS 208) to require a seat belt/starter interlock system to prevent passenger cars from being started with an unbelted front-seat occupant. This mandate applied to passenger cars built after August 1973, i.e., starting with the 1974 model year. The specifications required the system to permit the car to be started only if the belt of an occupied seat were fastened after the occupant sat down, so pre-buckling the belts would not defeat the system. 
The interlock systems used logic modules complex enough to require special diagnostic computers, and were not entirely dependable—an override button was provided under the hood of equipped cars, permitting one (but only one) "free" starting attempt each time it was pressed. However, the interlock system spurred severe backlash from an American public who largely rejected seat belts. In 1974, Congress acted to prohibit NHTSA from requiring or permitting a system that prevents a vehicle from starting or operating with an unbelted occupant, or that gives an audible warning of an unfastened belt for more than 8 seconds after the ignition is turned on. This prohibition took effect on 27 October 1974, shortly after the 1975 model year began.
In response to the Congressional action, NHTSA once again amended FMVSS 208, requiring vehicles to come with a seat belt reminder system that gives an audible signal for 4 to 8 seconds and a warning light for at least 60 seconds after the ignition is turned on if the driver's seat belt is not fastened. This is called a seat belt reminder (SBR) system. In the mid-1990s, an insurance company from Sweden called Folksam worked with Saab and Ford to determine the requirements for the most efficient seat belt reminder. One characteristic of the optimal SBR, according to the research, is that the audible warning becomes increasingly penetrating the longer the seat belt remains unfastened.
In 2003, the Transportation Research Board Committee, chaired by two psychologists, reported that ESBRs could save an additional 1,000 lives a year. Research by the Insurance Institute for Highway Safety found that Ford's ESBR, which provides an intermittent chime intermittently for up to five minutes if the driver is unbelted, sounding for 6 seconds then pausing for 30, increased seat belt use by 5 percent. Farmer and Wells found that driver fatality rates were 6% lower for vehicles with ESBR compared with otherwise-identical vehicles without.
Efficacy.
In 2001, Congress directed NHSTA to study the benefits of technology meant to increase the use of seat belts. NHSTA found that seat belt usage had increased to 73% since the initial introduction of the SBR system. In 2002, Ford demonstrated that seat belts were used more in Fords with seat belt reminders than in those without: 76% and 71% respectively. In 2007, Honda conducted a similar study and found that 90% of people who drove Hondas with seat belt reminders used a seat belt, while 84% of people who drove Hondas without seat belt reminders used a seat belt.
Legislation.
Observational studies of car crash morbidity and mortality, experiments using both crash test dummies and human cadavers indicate that wearing seat belts greatly reduces the risk of death and injury in the majority of car crashes.
This has led many countries to adopt mandatory seat belt wearing laws. It is generally accepted that, in comparing like-for-like accidents, a vehicle occupant not wearing a properly fitted seat belt has a significantly and substantially higher chance of death and serious injury. One large observation studying using US data showed that the odds ratio of crash death is 0.46 with a three-point belt, when compared with no belt. In another study that examined injuries presenting to the ER pre- and post-seat belt law introduction, it was found that 40% more escaped injury and 35% more escaped mild and moderate injuries.
The effects of seat belt laws are disputed by those who observe that their passage did not reduce road fatalities. There was also concern that instead of legislating for a general protection standard for vehicle occupants, laws that required a particular technical approach would rapidly become dated as motor manufacturers would tool up for a particular standard which could not easily be changed. For example, in 1969 there were competing designs for lap and 3-point seat belts, rapidly tilting seats, and air bags being developed. But as countries started to mandate seat belt restraints the global auto industry invested in the tooling and standardized exclusively on seat belts, and ignored other restraint designs such as air bags for several decades
As of 2016, seat belt laws can be divided into two categories: primary and secondary. A primary seat belt law allows an officer to issue and citation for lack of seatbelt use without any other citation, whereas a secondary seat belt law allows an officer to issue a seat belt citation only in the presence of a different violation. In the United States, fifteen states enforce secondary laws, while 34 states, as well as the District of Columbia, American Samoa, Guam, the Northern Mariana Islands, Puerto Rico and the Virgin Islands, enforce primary seat belt laws. New Hampshire lacks both a primary and secondary seat belt law.
Risk compensation.
Some have proposed that the number of deaths was influenced by the development of risk compensation, which says that drivers adjust their behavior in response to the increased sense of personal safety wearing a seat belt provides.
In one trial subjects were asked to drive go-karts around a track under various conditions. It was found that subjects who started driving unbelted drove consistently faster when subsequently belted. Similarly, a study of habitual non-seatbelt wearers driving in freeway conditions found evidence that they had adapted to seatbelt use by adopting higher driving speeds and closer following distances. 
A 2001 analysis of US crash data aimed to establish the effects of seatbelt legislation on driving fatalities and found that previous estimates of seatbelts effectiveness had been significantly overstated. According to the analysis used, seatbelts were claimed to have decreased fatalities by 1.35% for each 10% increase in seatbelt use. The study controlled for endogenous motivations of seat belt use, which it is claimed creates an artificial correlation between seat belt use and fatalities, leading to the conclusion that seatbelts cause fatalities. For example, drivers in high risk areas are more likely to use seat belts, and are more likely to be in accidents, creating a non-causal correlation between seatbelt use and mortality. After accounting for the endogeneity of seatbelt usage, Cohen and Einav found no evidence that the risk compensation effect makes seatbelt wearing drivers more dangerous, a finding at variance with other research.
Increased traffic.
Other statistical analyses have included adjustments for factors such as increased traffic, and other factors such as age, and based on these adjustments, a reduction of morbidity and mortality due to seat belt use has been claimed. However, Smeed's law predicts a fall in accident rate with increasing car ownership and has been demonstrated independently of seat belt legislation.
Mass transit considerations.
Buses.
School buses.
Six states—California, Florida, Louisiana, New Jersey, New York, and Texas—require seat belts on school buses.
Pros and cons had been alleged about the use of seatbelts in school buses.
School buses which are much bigger in size than the average vehicle allow for the mass transportation of students from place to place. The American School Bus Council states in a brief article saying that, “The children are protected like eggs in an egg carton – compartmentalized, and surrounded with padding and structural integrity to secure the entire container.” (ASBC). Although school buses are considered safe for mass transit of students this will not guarantee that the students will be injury free if an impact were to occur. Seatbelts in buses are sometimes believed to make recovering from a roll or tip harder for students and staff as they could be easily trapped in their own safety belt.
In 2015, for the first time, NHTSA endorsed seat belts on school buses.
Motor coaches.
In the European Union, all new long distance buses and coaches must be fitted with seat belts.
Australia has required lap/sash seat belts in new coaches since 1994. These must comply with Australian Design Rule 68, which requires the seat belt, seat and seat anchorage to withstand 20g deceleration and an impact by an unrestrained occupant to the rear.
In the United States, NHTSA has now required lap-shoulder seat belts in new "over-the-road" buses (includes most coaches) starting in 2016.
Trains.
The use of seatbelts in trains has been investigated. Concerns about survival space intrusion in train crashes and increased injuries to unrestrained or incorrectly restrained passengers led the researchers to discourage the use of seat belts in trains.
Airplanes.
Lap belts are found on all passenger aircraft. Many civil aviation authorities require a "fasten seat belt" sign in passenger aircraft that can be activated by a pilot during takeoff, turbulence, and landing. The International Civil Aviation Organization recommends the use of child restraints.

</doc>
<doc id="51478" url="https://en.wikipedia.org/wiki?curid=51478" title="Rolf Singer">
Rolf Singer

Rolf Singer (June 23, 1906 – January 18, 1994) was a German-born mycologist and one of the most important taxonomists of gilled mushrooms (agarics) in the 20th century.
After receiving his Ph.D. at the University of Vienna in 1931 he worked in Munich. By 1933, however, Singer was forced to flee Nazi Germany to Vienna. There he met his wife, Martha Singer. From Vienna, Singer and his wife went to Barcelona, Spain, where Singer was appointed Assistant Professor at the Autonomous University of Barcelona. Persecution by the Spanish authorities on behalf of the German government forced Singer to leave Spain for France in 1934. After a fellowship at the Museum d'Histoire Naturelle in Paris, Singer again moved, this time to Leningrad, where he was Senior Scientific Expert at the Botanical Garden of the Academy of Sciences of the USSR. During his time at the Academy, Singer made many expeditions to Siberia, the Altai Mountains, and Karelia. In 1941, Singer emigrated to the United States. He was offered a position at the Farlow Herbarium as a research associate, then as Assistant Curator, then as acting Curator following the death of Dr. David Linder. He spent a total of seven years at the Farlow. During this time, Singer also received a Guggenheim Fellowship for studies in Florida, and taught at the Mountain Lake Biological Station of the University of Virginia.
In 1948, Singer left Harvard to become professor at the Universidad Nacional de Tucuman in Argentina. Later, in 1961, Singer became professor at the Universidad de Buenos Aires. During his time in South America, Singer, his wife, and his daughter Heidi collected extensively. Singer's last faculty appointment was at the University of Illinois at Chicago, from 1968 to 1977.
Singer was a prolific writer, with more than 400 publications to his name. He was also known for his eagerness to aid other botanists, whether they were professionals or amateurs.
He wrote major books like "The Agaricales in Modern Taxonomy". He fled to various countries during the Nazi period, pursuing mycology in far-flung places like the Soviet Union, Argentina, and finally the United States, as mycologist at the Field Museum in Chicago.

</doc>
<doc id="51480" url="https://en.wikipedia.org/wiki?curid=51480" title="Hypha">
Hypha

A hypha (plural hyphae, from Greek ὑφή, huphḗ, “web”) is a long, branching filamentous structure of a fungus, oomycete, or actinobacterium. In most fungi, hyphae are the main mode of vegetative growth, and are collectively called a mycelium. Yeasts are unicellular fungi that do not grow as hyphae.
Structure.
A hypha consists of one or more cells surrounded by a tubular cell wall. In most fungi, hyphae are divided into cells by internal cross-walls called "septa" (singular septum). Septa are usually perforated by pores large enough for ribosomes, mitochondria and sometimes nuclei to flow between cells. The major structural polymer in fungal cell walls is typically chitin, in contrast to plants and oomycetes that have cellulosic cell walls. Some fungi have aseptate hyphae, meaning their hyphae are not partitioned by septa.
Growth.
Hyphae grow at their tips. During tip growth, cell walls are extended by the external assembly and polymerization of cell wall components, and the internal production of new cell membrane. The spitzenkörper is an intracellular organelle associated with tip growth. It is composed of an aggregation of membrane-bound vesicles containing cell wall components. The spitzenkörper is part of the endomembrane system of fungi, holding and releasing vesicles it receives from the Golgi apparatus. These vesicles travel to the cell membrane via the cytoskeleton and release their contents outside the cell by the process of exocytosis, where it can then be transported to where it is needed. Vesicle membranes contribute to growth of the cell membrane while their contents form new cell wall. The spitzenkörper moves along the apex of the hyphal strand and generates apical growth and branching; the apical growth rate of the hyphal strand parallels and is regulated by the movement of the spitzenkörper.
As a hypha extends, septa may be formed behind the growing tip to partition each hypha into individual cells. Hyphae can branch through the bifurcation of a growing tip, or by the emergence of a new tip from an established hypha.
Behavior.
The direction of hyphal growth can be controlled by environmental stimuli, such as the application of an electric field. Hyphae can sense reproductive units from some distance, and grow towards them. Hyphae can weave through a permeable surface to penetrate it.
Modifications.
Hyphae may be modified in many different ways to serve specific functions. Some parasitic fungi form haustoria that function in absorption within the host cells. The arbuscules of mutualistic mycorrhizal fungi serve a similar function in nutrient exchange, so are important in assisting nutrient and water absorption by plants. Ectomycorrhizal extramatrical mycelium greatly increases the soil area available for exploitation by plant hosts by funneling water and nutrients to ectomycorrhizas, complex fungal organs on the tips of plant roots. Hyphae are found enveloping the gonidia in lichens, making up a large part of their structure. In nematode-trapping fungi, hyphae may be modified into trapping structures such as constricting rings and adhesive nets. Mycelial cords can be formed to transfer nutrients over larger distances. Bulk fungal tissues, cords, and membranes, such as those of mushrooms and lichens, are mainly composed of felted and often anastomosed hyphae.
Types.
Classification based on cell wall and overall form.
Characteristics of hyphae can be important in fungal classification. In basidiomycete taxonomy, hyphae that comprise the fruiting body can be identified as generative, skeletal, or binding hyphae.
Based on the generative, skeletal and binding hyphal types, in 1932 E. J. H. Corner applied the terms monomitic, dimitic, and trimitic to hyphal systems, in order to improve the classification of polypores.
Fungi that form fusiform skeletal hyphae bound by generative hyphae are said to have sarcodimitic hyphal systems. A few fungi form fusiform skeletal hyphae, generative hyphae, and binding hyphae, and these are said to have sarcotrimitic hyphal systems. These terms were introduced as a later refinement by E. J. H. Corner in 1966.
Classification based on refractive appearance.
Hyphae are described as "gloeoplerous" ("gloeohyphae") if their high refractive index gives them an oily or granular appearance under the microscope. These cells may be yellowish or clear (hyaline). They can sometimes selectively be coloured by sulphovanillin or other reagents. The specialized cells termed cystidia can also be gloeoplerous.

</doc>
<doc id="51481" url="https://en.wikipedia.org/wiki?curid=51481" title="Jurist">
Jurist

A jurist (a word coming from medieval Latin), also known as legal scholar or legal theorist, is someone who researches and studies jurisprudence (theory of law). Such a person can work as an academic, legal writer or law lecturer. In the United Kingdom, Australia, New Zealand, South Africa, and in many other Commonwealth countries, the word "jurist" is sometimes used to refer to an illustrious barrister, whereas in the United States of America and Canada it is often used to refer to a judge.
Thus a "jurist," someone who studies, analyses and comments on law, stands in contrast with a "lawyer", someone who applies law on behalf of clients and thinks about it in practical terms.
Many legal scholars and authors have explained that a person may be both a lawyer and a jurist, but a jurist is not necessarily a lawyer, nor a lawyer necessarily a jurist. Both must possess an acquaintance with the term "law". The work of the jurist is the study, analysis and arrangement of the law — work which can be done wholly in the seclusion of the library. The work of the lawyer is the satisfaction of the wishes of particular human beings for legal assistance — work which requires dealing to some extent therefore with people in the office, in the court room, or in the market-place.
Any highly civilized society requires both lawyers and jurists, both philosophers and doers. As a mere matter of fact, there is a greater demand for doers than for philosophers, for lawyers than for jurists; but the number of persons which the interests of society require should engage in a particular occupation, has nothing to do with the question of the importance of the different kinds of work done by those persons. It is important however to note the fundamental difference between the work of the lawyer and that of the jurist.
The term "jurist "has another sense, which is wider, synonymous with "legal professional", i.e. anyone professionally involved with law and justice. In some other European languages, a word resembling "jurist" (such as Italian "giurista", German "Juristen", Norwegian "jurist", French "juriste", Spanish and Portuguese "jurista" etc.) is used in this major sense.
Islamic world.
In Sharia (Islamic law), jurists are known as Ulema, who specialize in Fiqh (Islamic jurisprudence). In order to become an Islamic jurist, it is required for a student to receive an "ijazat attadris wa 'l-ifa'" ("license to teach and issue legal opinions"), equivalent to the Juris Doctor and Doctor of Laws qualifications, from a "Madrasah" or "Jami'ah", equivalent to a college and university respectively. This system of legal education dates back to the 9th century, during the classical period of Islam.
Notable jurists.
"This is a sequential classification of some notable jurists."

</doc>
<doc id="51483" url="https://en.wikipedia.org/wiki?curid=51483" title="Mutualism (biology)">
Mutualism (biology)

Mutualism is the way two organisms of "different" species exist in a relationship in which each individual benefits from the activity of the other. Similar interactions "within" a species are known as co-operation. Mutualism can be contrasted with interspecific competition, in which each species experiences "reduced" fitness, and exploitation, or parasitism, in which one species benefits at the "expense" of the other. Symbiosis involves two species living in close proximity and includes relationships that are mutualistic, parasitic, and commensal. Symbiotic relationships are sometimes, but not always, mutualistic.
A well-known example of mutualism is the relationship between ungulates (such as bovines) and bacteria within their intestines. The ungulates benefit from the cellulase produced by the bacteria, which facilitates digestion; the bacteria benefit from having a stable supply of nutrients in the host environment. This can also be found in many many different symbiotic relationships.
Mutualism plays a key part in ecology. For example, mutualistic interactions are vital for terrestrial ecosystem function as more than 48% of land plants rely on mycorrhizal relationships with fungi to provide them with inorganic compounds and trace elements. In addition, mutualism is thought to have driven the evolution of much of the biological diversity we see, such as flower forms (important for pollination mutualisms) and co-evolution between groups of species. However mutualism has historically received less attention than other interactions such as predation and parasitism.
Measuring the exact fitness benefit to the individuals in a mutualistic relationship is not always straightforward, particularly when the individuals can receive benefits from a variety of species, for example most plant-pollinator mutualisms. It is therefore common to categorise mutualisms according to the closeness of the association, using terms such as obligate and facultative. Defining "closeness," however, is also problematic. It can refer to mutual dependency (the species cannot live without one another) or the biological intimacy of the relationship in relation to physical closeness ("e.g.", one species living within the tissues of the other species).
The term "mutualism" was introduced by Pierre-Joseph van Beneden in 1876.
Types of relationships.
Mutualistic transversals can be thought of as a form of "biological barter" in mycorrhizal associations between plant roots and fungi, with the plant providing carbohydrates to the fungus in return for primarily phosphate but also nitrogenous compounds. Other examples include rhizobia bacteria that fix nitrogen for leguminous plants (family Fabaceae) in return for energy-containing carbohydrates.
Service-resource relationships.
Service-resource relationships are also common.
Pollination in which nectar or pollen (food resources) are traded for pollen dispersal (a service) or ant protection of aphids, where the aphids trade sugar-rich honeydew (a by-product of their mode of feeding on plant sap) in return for defense against predators such as ladybugs.
Phagophiles feed (resource) on ectoparasites, thereby providing anti-pest service, as in cleaning symbiosis.
Elacatinus and Gobiosoma, genus of gobies, also feed on ectoparasites of their clients while cleaning them.
Zoochory is an example where animals disperse the seeds of plants. This is similar to pollination in that the plant produces food resources (for example, fleshy fruit, overabundance of seeds) for animals that disperse the seeds (service).
Service-service relationships.
Strict service-service interactions are very rare, for reasons that are far from clear. One example is the relationship between sea anemones and anemone fish in the family Pomacentridae: the anemones provide the fish with protection from predators (which cannot tolerate the stings of the anemone's tentacles) and the fish defend the anemones against butterflyfish (family Chaetodontidae), which eat anemones. However, in common with many mutualisms, there is more than one aspect to it: in the anemonefish-anemone mutualism, waste ammonia from the fish feed the symbiotic algae that are found in the anemone's tentacles. Therefore, what appears to be a service-service mutualism in fact has a service-resource component. A second example is that of the relationship between some ants in the genus "Pseudomyrmex" and trees in the genus "Acacia", such as the whistling thorn and bullhorn acacia. The ants nest inside the plant's thorns. In exchange for shelter, the ants protect acacias from attack by herbivores (which they frequently eat, introducing a resource component to this service-service relationship) and competition from other plants by trimming back vegetation that would shade the acacia.
In addition, another service-resource component is present, as the ants regularly feed on lipid-rich food-bodies called Beltian bodies that are on the "Acacia" plant.
In the neotropics, the ant, "Myrmelachista schumanni" makes its nest in special cavities in "Duroia hirsute". Plants in the vicinity that belong to other species are killed with formic acid. This selective gardening can be so aggressive that small areas of the rainforest are dominated by "Duroia hirsute". These peculiar patches are known by local people as "devil's gardens".
In some of these relationships, the cost of the ant’s protection can be quite expensive. "Cordia" sp. trees in the Amazonian rainforest have a kind of partnership with "Allomerus" sp. ants, which make their nests in modified leaves. To increase the amount of living space available, the ants will destroy the tree’s flower buds. The flowers die and leaves develop instead, providing the ants with more dwellings. Another type of "Allomerus" sp. ant lives with the "Hirtella" sp. tree in the same forests, but in this relationship the tree has turned the tables on the ants. When the tree is ready to produce flowers, the ant abodes on certain branches begin to wither and shrink, forcing the occupants to flee, leaving the tree’s flowers to develop free from ant attack.
The term "species group" can be used to describe the manner in which individual organisms group together. In this non-taxonomic context one can refer to "same-species groups" and "mixed-species groups." While same-species groups are the norm, examples of mixed-species groups abound. For example, zebra ("Equus burchelli") and wildebeest ("Connochaetes taurinus") can remain in association during periods of long distance migration across the Serengeti as a strategy for thwarting predators. "Cercopithecus mitis" and "Cercopithecus ascanius", species of monkey in the Kakamega Forest of Kenya, can stay in close proximity and travel along exactly the same routes through the forest for periods of up to 12 hours. These mixed-species groups cannot be explained by the coincidence of sharing the same habitat. Rather, they are created by the active behavioural choice of at least one of the species in question.
Humans and mutualism.
Humans also engage in mutualisms with other species, including their gut flora without which they would not be able to digest food efficiently. Infestations of head lice might have been beneficial for humans by fostering an immune response that helps to reduce the threat of body louse borne lethal diseases.
Some relationships between humans and domesticated animals and plants are to different degrees mutualistic. Agricultural varieties of maize are unable to reproduce without human intervention because the leafy sheath does not fall open, and the seedhead (the "corn on the cob") does not shatter to scatter the seeds naturally.
In traditional agriculture, some plants have mutualist as companion plants, providing each other with shelter, soil fertility and/or natural pest control. For example, beans may grow up cornstalks as a trellis, while fixing nitrogen in the soil for the corn, a phenomenon that is used in Three Sisters farming.
Boran people of Ethiopia and Kenya traditionally use a whistle to call the honeyguide bird, though the practice is declining. If the bird is hungry and within earshot, it guides them to a bees' nest. In exchange the Borans leave some food from the nest for the bird.
A population of bottlenose dolphins in Laguna, Brazil coordinates, via body language, with local net-using fishermen in order for both to catch schools of mullet.
One researcher has proposed that the key advantage "Homo sapiens" had over Neanderthals in competing over similar habitats was the former's mutualism with dogs.
Mathematical modeling.
Mathematical treatments of mutualisms, like the study of mutualisms in general, has lagged behind those of predation, or predator-prey, consumer-resource, interactions. Here we present two such approaches. In models of mutualisms, the terms "type I" and "type II" functional responses refer to the linear and saturating relationships, respectively, between "benefit" provided to an individual of species 1 ("y"-axis) on the "density" of species 2 ("x"-axis).
Type I functional response.
One of the simplest frameworks for modeling species interactions is the Lotka–Volterra equations. In this model, the change in population density of the two mutualists is quantified as:
where
Mutualism is in essence the logistic growth equation + mutualistic interaction. The mutualistic interaction term represents the increase in population growth of species one as a result of the presence of greater numbers of species two, and vice versa. As the mutualistic term is always positive, it may lead to unrealistic unbounded growth as it happens with the simple model. So, it is important to include a saturation mechanism to avoid the problem.
The type I functional response is visualized as the graph of formula_2 "vs." "M".
Type II functional response.
In 1989, David Hamilton Wright modified the Lotka–Volterra equations by adding a new term, "βM"/"K", to represent a mutualistic relationship. Wright also considered the concept of saturation, which means that with higher densities, there are decreasing benefits of further increases of the mutualist population. Without saturation, species' densities would increase indefinitely. Because that isn't possible due to environmental constraints and carrying capacity, a model that includes saturation would be more accurate. Wright's mathematical theory is based on the premise of a simple two-species mutualism model in which the benefits of mutualism become saturated due to limits posed by handling time. Wright defines handling time as the time needed to process a food item, from the initial interaction to the start of a search for new food items and assumes that processing of food and searching for food are mutually exclusive. Mutualists that display foraging behavior are exposed to the restrictions on handling time. Mutualism can be associated with symbiosis.
Handling time interactions
In 1959, C. S. Holling performed his classic disc experiment that assumed the following: that (1), the number of food items captured is proportional to the allotted searching time; and (2), that there is a variable of handling time that exists separately from the notion of search time. He then developed an equation for the Type II functional response, which showed that the feeding rate is equivalent to
where,
The equation that incorporates Type II functional response and mutualism is:
where
or, equivalently,
where
The model presented above is most effectively applied to free-living species that encounter a number of individuals of the mutualist part in the course of their existences. Of note, as Wright points out, is that models of biological mutualism tend to be similar qualitatively, in that the featured isoclines generally have a positive decreasing slope, and by and large similar isocline diagrams. Mutualistic interactions are best visualized as positively sloped isoclines, which can be explained by the fact that the saturation of benefits accorded to mutualism or restrictions posed by outside factors contribute to a decreasing slope.
The type II functional response is visualized as the graph of formula_6 "vs." "M".
The structure of mutualistic networks.
Mutualistic networks made up out of the interaction between plants and pollinators were found to have a similar structure in very different ecosystems on different continents, consisting of entirely different species. The structure of these mutualistic networks may have large consequences for the way in which pollinator communities respond to increasingly harsh conditions.
Mathematical models that examine the consequences of this network structure for the stability of pollinator communities suggest that the specific way in which plant-pollinator networks are organized minimizes competition between pollinators and may even lead to strong indirect facilitation between pollinators when conditions are harsh. This means that pollinator species together can survive under harsh conditions. But it also means that pollinator species collapse simultaneously when conditions pass a critical point. This simultaneous collapse occurs, because pollinator species depend on each other when surviving under difficult conditions.
Such a community-wide collapse, involving many pollinator species, can occur suddenly when increasingly harsh conditions pass a critical point and recovery from such a collapse might not be easy. The improvement in conditions needed for pollinators to recover, could be substantially larger than the improvement needed to return to conditions at which the pollinator community collapsed.

</doc>
<doc id="51485" url="https://en.wikipedia.org/wiki?curid=51485" title="German Workers' Party">
German Workers' Party

The German Workers' Party (, DAP) was a short-lived political party and the forerunner of the Nazi Party (, NSDAP (National Socialist German Workers' Party).
Origins.
The DAP was founded in Munich in the hotel "Fürstenfelder Hof" on January 5, 1919 by Anton Drexler. It developed out of the "Freier Arbeiterausschuss für einen guten Frieden" (Free Workers' Committee for a Good Peace) league, a branch of which Drexler had founded in 1918. Thereafter in 1918, Karl Harrer (a journalist and member of the Thule Society), convinced Drexler and several others to form the "Politischer Arbeiterzirkel" (Political Workers' Circle). The members met periodically for discussions with themes of nationalism and antisemitism directed against the Jews. Drexler was encouraged to form the DAP in December 1918 by his mentor, Dr. Paul Tafel. Tafel was a leader of the Alldeutscher Verband (Pan-Germanist Union), a director of the Maschinenfabrik Augsburg-Nürnberg, and a member of the Thule Society. Drexler's wish was for a political party which was both in touch with the masses and nationalist. In January 1919 with the DAP founding, Drexler was elected chairman and Harrer was made "Reich Chairman", an honorary title. On May 17, only ten members were present at the meeting; a later meeting in August only noted 38 members attending.
Adolf Hitler's membership.
After World War I ended, Adolf Hitler returned to Munich. Having no formal education and career prospects, he tried to remain in the army for as long as possible. In July 1919 he was appointed "Verbindungsmann" (intelligence agent) of an "Aufklärungskommando" (reconnaissance commando) of the "Reichswehr", to influence other soldiers and to infiltrate the German Workers' Party (DAP). While monitoring the activities of the DAP, Hitler became attracted to founder Anton Drexler's anti-Semitic, nationalist, anti-capitalist, and anti-Marxist ideas. While attending a party meeting at the "Sterneckerbräu" beer hall on September 12, 1919, Hitler became involved in a heated political argument with a visitor, a Professor Baumann, who questioned the soundness of Gottfried Feder's arguments against capitalism and proposed that Bavaria should break away from Prussia and found a new South German nation with Austria. In vehemently attacking the man's arguments he made an impression on the other party members with his oratory skills and, according to Hitler, the "professor" left the hall acknowledging unequivocal defeat. Impressed with Hitler, Drexler invited him to join the DAP. Hitler accepted on September 12, 1919, becoming the party's 55th member.
In less than a week, Hitler received a postcard from Drexler stating he had officially been accepted as a DAP member and he should come to a "committee" meeting to discuss it. Hitler attended the "committee" meeting held at the run-down Altes Rosenbad beer-house. Normally, enlisted army personnel were not allowed to join political parties. However in this case, Hitler had Captain Karl Mayr's permission to join the DAP. Further, Hitler was allowed to stay in the army and receive his weekly pay of 20 gold marks a week. At the time when Hitler joined the party there were no membership numbers or cards. It was in January 1920 when a numeration was issued for the first time: listed in alphabetical order, Hitler received the number 555. In reality he had been the 55th member, but the counting started at the number 501 in order to make the party appear larger. Hitler, in his work "Mein Kampf", later claimed to be the seventh party member (he was in fact the seventh executive member of the Party's central committee). After giving his first speech for the DAP on October 16 at the "Hofbräukeller", Hitler quickly became the party's most active orator. Hitler's considerable oratory and propaganda skills were appreciated by the party leadership as crowds began to "flock" to hear his speeches during 1919 and 1920. With the support of Anton Drexler, Hitler became chief of propaganda for the party in early 1920. Hitler preferred that role as he saw himself as the drummer for a national cause. He saw propaganda as the way to bring nationalism to the public.
From DAP to NSDAP.
The small number of party members were quickly won over to Hitler's political beliefs. He organized their biggest meeting yet of 2,000 people, for February 24, 1920 in the "Staatliches Hofbräuhaus in München". Further in an attempt to make the party more broadly appealing to larger segments of the population, the DAP was renamed the National Socialist German Workers' Party on February 24. Such was the significance of Hitler's particular move in publicity that Karl Harrer resigned from the party in disagreement. The new name was borrowed from a different Austrian party active at the time (Deutsche Nationalsozialistische Arbeiterpartei, German National Socialist Workers' Party), although Hitler earlier suggested the party to be renamed the "Social Revolutionary Party"; it was Rudolf Jung who persuaded Hitler to follow the NSDAP naming.
Membership.
Hitler was an early member of the party; the following are well-known early members:

</doc>
<doc id="51487" url="https://en.wikipedia.org/wiki?curid=51487" title="Salvador Allende">
Salvador Allende

Salvador Guillermo Allende Gossens (; 26 June 1908 – 11 September 1973), more commonly known as Salvador Allende, was a Chilean physician and politician, known as the first Marxist to become president of a Latin American country through open elections.
Allende's involvement in Chilean political life spanned a period of nearly forty years. As a member of the Socialist Party, he was a senator, deputy and cabinet minister. He unsuccessfully ran for the presidency in the 1952, 1958, and 1964 elections. In 1970, he won the presidency in a close three-way race. He was elected in a run-off by Congress as no candidate had gained a majority.
As president, Allende adopted a policy of nationalization of industries and collectivisation; due to these and other factors, increasingly strained relations between him and the legislative and judicial branches of the Chilean government—who did not share his enthusiasm for socialisation—culminated in a declaration by Congress of a "constitutional breakdown." A centre-right majority including the Christian Democrats, whose support had enabled Allende's election, denounced his rule as unconstitutional and called for his overthrow by force. On 11 September 1973, the military moved to oust Allende in a "coup d'état" sponsored by the United States Central Intelligence Agency (CIA). As troops surrounded La Moneda Palace, he gave his last speech vowing not to resign. Later that day, Allende shot himself dead with an assault rifle, according to an investigation conducted by a Chilean court with the assistance of international experts in 2011.
Following Allende's deposition, General Augusto Pinochet declined to return authority to the civilian government, and Chile was later ruled by a military junta that was in power up until 1990, ending almost 41 years of Chilean democratic rule. The military junta that took over dissolved the Congress of Chile and began a persecution of alleged dissidents, in which thousands of Allende's supporters were kidnapped, tortured, and murdered.
Early life.
Allende was born on 26 June 1908 in Santiago. He was the son of Salvador Allende Castro and Laura Gossens Uribe. Allende's family belonged to the Chilean upper middle class and had a long tradition of political involvement in progressive and liberal causes. His grandfather was a prominent physician and a social reformist who founded one of the first secular schools in Chile. Salvador Allende was of Belgian and Basque descent.
Allende attended high school at the Liceo Eduardo de la Barra in Valparaíso. As a teenager, his main intellectual and political influence came from the shoe-maker Juan De Marchi, an Italian-born anarchist. Allende was a talented athlete in his youth, being a member of the Everton de Viña del Mar sports club (named after the more famous English football club of the same name), where he is said to have excelled at the long jump. Allende then graduated with a medical degree in 1933 from the University of Chile. During the medical school Allende was influenced by Professor Max Westenhofer, a German pathologist who stressed on the social determinants of disease and social medicine
Political involvement up to 1970.
He co-founded a section of the Socialist Party of Chile (founded in 1933 with Marmaduque Grove and others) in Valparaíso and became its chairman. He married Hortensia Bussi with whom he had three daughters. He was a Freemason, a member of the Lodge Progreso No. 4 in Valparaíso. In 1933, he published his doctoral thesis "Higiene Mental y Delincuencia" (Crime and Mental Hygiene) in which he criticized Cesare Lombroso's proposals.
In 1938, Allende was in charge of the electoral campaign of the Popular Front headed by Pedro Aguirre Cerda. The Popular Front's slogan was "Bread, a Roof and Work!" After its electoral victory, he became Minister of Health in the Reformist Popular Front government which was dominated by the Radicals. While serving in this position, Allende was responsible for the passage of a wide range of progressive social reforms, including safety laws protecting workers in the factories, higher pensions for widows, maternity care, and free lunch programmes for schoolchildren.
Upon entering the government, Allende relinquished his congressional seat for Valparaíso, which he had won in 1937. Around that time, he wrote "La Realidad Médico Social de Chile" ("The social and medical reality of Chile"). After the Kristallnacht in Nazi Germany, Allende and other members of the Congress sent a telegram to Adolf Hitler denouncing the persecution of Jews. Following President Aguirre Cerda's death in 1941, he was again elected deputy while the Popular Front was renamed Democratic Alliance.
In 1945, Allende became senator for the Valdivia, Llanquihue, Chiloé, Aisén and Magallanes provinces; then for Tarapacá and Antofagasta in 1953; for Aconcagua and Valparaíso in 1961; and once more for Chiloé, Aisén and Magallanes in 1969. He became president of the Chilean Senate in 1966. During the Fifties, Allende introduced legislation that established the Chilean national health service, the first program in the Americas to guarantee universal health care.
His three unsuccessful bids for the presidency (in the 1952, 1958 and 1964 elections) prompted Allende to joke that his epitaph would be "Here lies the next President of Chile." In 1952, as candidate for the "Frente de Acción Popular" (Popular Action Front, FRAP), he obtained only 5.4% of the votes, partly due to a division within socialist ranks over support for Carlos Ibáñez. In 1958, again as the FRAP candidate, Allende obtained 28.5% of the vote. This time, his defeat was attributed to votes lost to the populist Antonio Zamorano.
Declassified documents show that from 1962 through 1964, the CIA spent a total of $2.6 millions to finance the campaign of Eduardo Frei and spent $3 millions in anti-Allende propaganda "to scare voters away from Allende's FRAP coalition". The CIA considered its role in the victory of Frei a great success. They argued that "the financial and organizational assistance given to Frei, the effort to keep Durán in the race, the propaganda campaign to denigrate Allende—were 'indispensable ingredients of Frei's success'", and they thought that his chances of winning and the good progress of his campaign would have been doubtful without the covert support of the Government of the United States. Thus, in 1964 Allende lost once more as the FRAP candidate, polling 38.6% of the votes against 55.6% for Christian Democrat Eduardo Frei. As it became clear that the election would be a race between Allende and Frei, the political rightwhich initially had backed Radical Julio Durán– settled for Frei as "the lesser evil".
Relationship with the Chilean Communist Party.
Allende had a close relationship with the Chilean Communist Party from the beginning of his political career. On his fourth (and successful) bid for the presidency, the Communist Party supported him as the alternate for its own candidate, the world-renowned poet Pablo Neruda.
During his presidential term, Allende shared positions held by the Communists, in opposition to the views of the socialists. Some argue, however, that this was reversed at the end of his period in office.
1970 election.
Allende won the 1970 Chilean presidential election as leader of the Unidad Popular ("Popular Unity") coalition. On 4 September 1970, he obtained a narrow plurality of 36.2 percent to 34.9 percent over Jorge Alessandri, a former president, with 27.8 percent going to a third candidate (Radomiro Tomic) of the Christian Democratic Party (PDC), whose electoral platform was similar to Allende's. According to the Chilean Constitution of the time, if no presidential candidate obtained a majority of the popular vote, Congress would choose one of the two candidates with the highest number of votes as the winner. Tradition was for Congress to vote for the candidate with the highest popular vote, regardless of margin. Indeed, former president Jorge Alessandri had been elected in 1958 with only 31.6 percent of the popular vote, defeating Allende.
One month after the election, on 20 October, while the senate had still to reach a decision and negotiations were actively in place between the Christian Democrats and the Popular Unity, General René Schneider, Commander in Chief of the Chilean Army, was shot resisting a kidnap attempt by a group led by General Roberto Viaux. Hospitalized, he died of his wounds three days later, on 23 October. Schneider was a defender of the "constitutionalist" doctrine that the army's role is exclusively professional, its mission being to protect the country's sovereignty and not to interfere in politics.
General Schneider's death was widely disapproved of and, for the time, ended military opposition to Allende, whom the congress finally chose on 24 October. On 26 October, President Eduardo Frei named General Carlos Prats as commander in chief of the army to replace René Schneider.
Allende assumed the presidency on 3 November 1970 after signing a "Statute of Constitutional Guarantees" proposed by the Christian Democrats in return for their support in Congress. In an extensive interview with Régis Debray in 1972, Allende explained his reasons for agreeing to the guarantees. Some critics have interpreted Allende's responses as an admission that signing the "Statute" was only a tactical move.
Presidency.
Upon assuming power, Allende began to carry out his platform of implementing a socialist programme called "La vía chilena al socialismo" ("the Chilean Path to Socialism"). This included nationalization of large-scale industries (notably copper mining and banking), and government administration of the health care system, educational system (with the help of a U.S. educator, Jane A. Hobson-Gonzalez from Kokomo, Indiana), a programme of free milk for children in the schools and shanty towns of Chile, and an expansion of the land seizure and redistribution already begun under his predecessor Eduardo Frei Montalva, who had nationalized between one-fifth and one-quarter of all the properties listed for takeover. Allende also intended to improve the socio-economic welfare of Chile's poorest citizens; a key element was to provide employment, either in the new nationalized enterprises or on public work projects.
In November 1970, 3,000 scholarships were allocated to Mapuche children in an effort to integrate the Indian minority into the educational system, payment of pensions and grants was resumed, an emergency plan providing for the construction of 120,000 residential buildings was launched, all part-time workers were granted rights to social security, a proposed electricity price increase was withdrawn, diplomatic relations were restored with Cuba, and political prisoners were granted an amnesty. In December that same year, bread prices were fixed, 55,000 volunteers were sent to the south of the country to teach writing and reading skills and provide medical attention to a sector of the population that had previously been ignored, a central commission was established to oversee a tri-partite payment plan in which equal place was given to government, employees and employers, and a protocol agreement was signed with the United Centre of Workers which granted workers representational rights on the funding board of the Social Planning Ministry. An obligatory minimum wage for workers of all ages (including apprentices) was established, free milk was introduced for expectant and nursing mothers and for children between the ages of 7 and 14, free school meals were established, rent reductions were carried out, and the construction of the Santiago subway was rescheduled so as to serve working-class neighbourhoods first. Workers benefited from increases in social security payments, an expanded public works program, and a modification of the wage and salary adjustment mechanism (which had originally been introduced in the Forties to cope with the country’s permanent inflation), while middle-class Chileans benefited from the elimination of taxes on modest incomes and property. In addition, state-sponsored programs distributed free food to the country’s neediest citizens, and in the countryside, peasant councils were established to mobilise agrarian workers and small proprietors. In the government’s first budget (presented to the Chilean congress in November 1970), the minimum taxable income level was raised, removing from the tax pool 35% of those who had paid taxes on earnings in the previous year. In addition, the exemption from general taxation was raised to a level equivalent to twice the minimum wage. Exemptions from capital taxes were also extended, which benefitted 330,000 small proprietors. The extra increases that Frei promised to the armed forces were also fully paid. According to one estimate, purchasing power went up by 28% between October 1970 and July 1971.
The rate of inflation fell from 36.1% in 1970 to 22.1% in 1971, while average real wages rose by 22.3% during 1971. Minimum real wages for blue-collar workers were increased by 56% during the first quarter of 1971, while in the same period real minimum wages for white-collar workers were increased by 23%, a development that decreased the differential ratio between blue- and white-collar workers’ minimum wage from 49% (1970) to 35% (1971). Central government expenditures went up by 36% in real terms, raising the share of fiscal spending in GDP from 21% (1970) to 27% (1971), and as part of this expansion, the public sector engaged in a huge housing program, starting to build 76,000 houses in 1971, compared to 24,000 for 1970. During a 1971 emergency program, over 89,000 houses were built, and during Allende’s three years as president an average of 52,000 houses were constructed annually. Although the acceleration of inflation in 1972 and 1973 eroded part of the initial increase in wages, they still rose (on average) in real terms during the 1971–73 period.
Allende’s first step in early 1971 was to raise minimum wages (in real terms) for blue-collar workers by 37%–41% and 8%–10% for white-collar workers. Educational, food, and housing assistance was significantly expanded, with public-housing starts going up twelvefold and eligibility for free milk extended from age 6 to age 15. A year later, blue-collar wages were raised by 27% in real terms and white-collar wages became fully indexed. Price controls were also set up, while the Allende Government introduced a system of distribution networks through various agencies (including local committees on supply and prices) to ensure that the new rules were adhered to by shopkeepers.
The new Minister of Agriculture, Jacques Chonchol, promised to expropriate all estates which were larger than eighty "basic" hectares. This promise was kept, with no farm in Chile exceeding this limit by the end of 1972. Within eighteen months, the Latifundia (extensive agricultural estates) had been abolished. The agrarian reform had involved the expropriation of 3,479 properties which, added to the 1,408 properties incorporated under the Frei Government, made up some 40% of the total agricultural land area in the country.
Particularly in rural areas, the Allende Government launched a campaign against illiteracy, while adult education programs expanded, together with educational opportunities for workers. From 1971 through to 1973, enrollments in kindergarten, primary, secondary, and postsecondary schools all increased. The Allende Government encouraged more doctors to begin their practices in rural and low-income urban areas, and built additional hospitals, maternity clinics, and especially neighborhood health centers that remained open longer hours to serve the poor. Improved sanitation and housing facilities for low-income neighborhoods also equalized health care benefits, while hospital councils and local health councils were established in neighborhood health centers as a means of democratizing the administration of health policies. These councils gave central government civil servants, local government officials, health service employees, and community workers the right to review budgetary decisions.
The Allende government also sought to bring the arts (both serious and popular) to the mass of the Chilean population by funding a number of cultural endeavours. With eighteen-year-olds and illiterates now granted the right to vote, mass participation in decision-making was encouraged by the Allende government, with traditional hierarchical structures now challenged by socialist egalitarianism. The Allende Government was able to draw upon the idealism of its supporters, with teams of "Allendistas" travelling into the countryside and shanty towns to perform volunteer work. The Allende Government also worked to transform Chilean popular culture through formal changes to school curriculum and through broader cultural education initiatives, such as state-sponsored music festivals and tours of Chilean folklorists and nueva canción musicians. In 1971, the purchase of a private publishing house by the state gave rise to "Editorial Quimantu," which became the center of the Allende Government’s cultural activities. In the space of 2 years, 12 million copies of books, magazines, and documents (8 million of which were books) specializing in social analysis, were published. Cheap editions of great literary works were produced on a weekly basis, and in most cases were sold out within a day. Culture came into the reach of the masses for the first time, who responded enthusiastically. "Editorial Quimantu" encouraged the establishment of libraries in community organizations and trade unions. Through the supply of cheap textbooks, it enabled the Left to progress through the ideological content of the literature made available to workers.
To improve social and economic conditions for women, the Women’s Secretariat was established in 1971, which took on issues such as public laundry facilities, public food programs, day-care centers, and women’s health care (especially prenatal care). The duration of maternity leave was extended from 6 to 12 weeks, while the Allende Government veered the educational system towards poorer Chileans by expanding enrollments through government subsidies. A "democratisation" of university education was carried out, making the system tuition-free. This led to an 89% rise in university enrollments between 1970 and 1973. The Allende Government also increased enrollment in secondary education from 38% in 1970 to 51% in 1974. Enrollment in education reached record levels, including 3.6 million young people, and 8 million school textbooks were distributed among 2.6 million pupils in primary education. An unprecedented 130,000 students were enrolled by the universities, which became accessible to peasants and workers. The illiteracy rate was reduced from 12% in 1970 to 10.8% in 1972, while the growth enrollment in primary school enrollment increased from an annual average of 3.4% in the period 1966–70 to 6.5% in 1971–1972. Secondary education grew at a rate of 18.2% in 1971–1972, and the average school enrollment of children between the ages of 6 and 14 rose from 91% (1966–70) to 99%.
Social spending was dramatically increased, particularly for housing, education, and health, while a major effort was made to redistribute wealth to poorer Chileans. As a result of new initiatives in nutrition and health, together with higher wages, many poorer Chileans were able to feed themselves and clothe themselves better than they had been able to before. Public access to the social security system was increased, while state benefits such as family allowances were raised significantly. The redistribution of income enabled wage and salary earners to increase their share of national income from 51.6% (the annual average between 1965 and 1970) to 65% while family consumption increased by 12.9% in the first year of the Allende Government. In addition, while the average annual increase in personal spending had been 4.8% in the period 1965–70, it reached 11.9% in 1971. During the first two years of Allende’s presidency, state expenditure on health rose from around 2% to nearly 3.5% of GDP. According to Jennifer E. Pribble, this new spending "was reflected not only in public health campaigns, but also in the construction of health infrastructure." Small programs targeted at women were also experimented with, such as cooperative laundries and communal food preparation, together with an expansion of child-care facilities.
The National Supplementary Food Program was extended to all primary school and to all pregnant women, regardless of their employment or income condition. Complementary nutritional schemes were applied to malnourished children, while antenatal care was emphasized. Under Allende, the proportion of children under the age of 6 with some form of malnutrition fell by 17%. Apart from the existing Supply and Prices councils (community-based bodies which controlled the distribution of essential groups in working-class districts, and were a popular, not government, initiative), community-based distribution centers and shops were developed, which sold directly in working-class neighborhoods. The Allende Government felt obliged to increase its intervention in marketing activities, and state involvement in grocery distribution reached 33%. The CUT (central labor confederation) was accorded legal recognition, and its membership grew from 700,000 to almost 1 million. In enterprises in the Area of Social Ownership, an assembly of the workers elected half of the members of the management council for each company. These bodies replaced the former board of directors.
Minimum pensions were increased by amounts equal to two or three times the inflation rate, and between 1970 and 1972, such pensions increased by a total of 550%. The incomes of 300,000 retirement pensioners were increased by the government from one-third of the minimum salary to the full amount. Labor insurance cover was extended to 200,000 market traders, 130,000 small shop proprietors, 30,000 small industrialists, small owners, transport workers, clergy, professional sportsmen, and artesans. The public health service was improved, with the establishment of a system of clinics in working-class neighborhoods on the peripheries of the major cities, providing a health center for every 40,000 inhabitants. Statistics for construction in general, and house-building in particular, reached some of the highest levels in the history of Chile. Four million square metres were completed in 1971–72, compared to an annual average of two-and-a-half million between 1965 and 1970. Workers were able to acquire goods which had previously been beyond their reach, such as heaters, refrigerators, and television sets. As further noted by Ricardo Israel Zipper,
"By now meat was no longer a luxury, and the children of working people were adequately supplied with shoes and clothing. The popular living standards were improved in terms of the employment situation, social services, consumption levels, and income distribution."
Chilean presidents were allowed a maximum term of six years, which may explain Allende's haste to restructure the economy. Not only was a major restructuring program organized (the Vuskovic Plan), he had to make it a success if a socialist successor to Allende was going to be elected. In the first year of Allende's term, the short-term economic results of Minister of the Economy Pedro Vuskovic's expansive monetary policy were highly favorable: 12% industrial growth and an 8.6% increase in GDP, accompanied by major declines in inflation (down from 34.9% to 22.1%) and unemployment (down to 3.8%). However, by 1972, the Chilean "escudo" had an inflation rate of 140%. The average Real GDP contracted between 1971 and 1973 at an annual rate of 5.6% ("negative growth"); and the government's fiscal deficit soared while foreign reserves declined. The combination of inflation and government-mandated price-fixing, together with the "disappearance" of basic commodities from supermarket shelves, led to the rise of black markets in rice, beans, sugar, and flour. The Chilean economy also suffered as a result of a US campaign against the Allende government.
The Allende government announced it would default on debts owed to international creditors and foreign governments. Allende also froze all prices while raising salaries. His implementation of these policies was strongly opposed by landowners, employers, businessmen and transporters associations, and some civil servants and professional unions. The rightist opposition was led by the National Party, the Roman Catholic Church (which in 1973 was displeased with the direction of educational policy), and eventually the Christian Democrats. There were growing tensions with foreign multinational corporations and the government of the United States.
Allende also undertook Project Cybersyn, a system of networked telex machines and computers. Cybersyn was developed by British cybernetics expert Stafford Beer. The network was supposed to transmit data from factories to the government in Santiago, allowing for economic planning in real time.
In 1971, Chile re-established diplomatic relations with Cuba, joining Mexico and Canada in rejecting a previously established Organization of American States convention prohibiting governments in the Western Hemisphere from establishing diplomatic relations with Cuba. Shortly afterward, Cuban president Fidel Castro made a month-long visit to Chile. Originally the visit was supposed to be one week; however, Castro enjoyed Chile and one week led to another.
In October 1972, the first of what were to be a wave of strikes was led first by truckers, and later by small businessmen, some (mostly professional) unions and some student groups. Other than the inevitable damage to the economy, the chief effect of the 24-day strike was to induce Allende to bring the head of the army, general Carlos Prats, into the government as Interior Minister. Allende also instructed the government to begin requisitioning trucks in order to keep the nation from coming to a halt. Government supporters also helped to mobilize trucks and buses but violence served as a deterrent to full mobilization, even with police protection for the strike-breakers. Allende's actions were eventually declared unlawful by the Chilean appeals court and the government was ordered to return trucks to their owners.
Throughout this presidency racial tensions between the poor descendants of indigenous people, who supported Allende's reforms, and the white elite increased.
Allende raised wages on a number of occasions throughout 1970 and 1971, but these wage hikes were negated by the in-tandem inflation of Chile's fiat currency. Although price rises had also been high under Frei (27% a year between 1967 and 1970), a basic basket of consumer goods rose by 120% from 190 to 421 escudos in one month alone, August 1972. In the period 1970–72, while Allende was in government, exports fell 24% and imports rose 26%, with imports of food rising an estimated 149%.
Export income fell due to a hard-hit copper industry: the price of copper on international markets fell by almost a third, and post-nationalization copper production fell as well. Copper is Chile's single most important export (more than half of Chile's export receipts were from this sole commodity). The price of copper fell from a peak of $66 per ton in 1970 to only $48–9 in 1971 and 1972. Chile was already dependent on food imports, and this decline in export earnings coincided with declines in domestic food production following Allende's agrarian reforms.
Throughout his presidency, Allende remained at odds with the Chilean Congress, which was dominated by the Christian Democratic Party. The Christian Democrats (who had campaigned on a socialist platform in the 1970 elections, but drifted away from those positions during Allende's presidency, eventually forming a coalition with the National Party), continued to accuse Allende of leading Chile toward a Cuban-style dictatorship, and sought to overturn many of his more radical policies. Allende and his opponents in Congress repeatedly accused each other of undermining the Chilean Constitution and acting undemocratically.
Allende's increasingly bold socialist policies (partly in response to pressure from some of the more radical members within his coalition), combined with his close contacts with Cuba, heightened fears in Washington. The Nixon administration continued exerting economic pressure on Chile via multilateral organizations, and continued to back Allende's opponents in the Chilean Congress. Almost immediately after his election, Nixon directed CIA and U.S. State Department officials to "put pressure" on the Allende government.
Foreign relations during Allende's presidency.
Allende's Popular Unity government tried to maintain normal relations with the United States. When Chile nationalized its copper industry, Washington cut off U.S. credits and increased its support to opposition. Forced to seek alternative sources of trade and finance, Chile gained commitments from the Soviet Union to invest some $400 million in Chile in the next six years. Allende's government was disappointed that it received far less economic assistance from the USSR than it hoped for. Trade between the two countries did not significantly increase and the credits were mainly linked to the purchase of Soviet equipment. Moreover, credits from the Soviet Union were much less than those provided to the People's Republic of China and countries of Eastern Europe. When Allende visited the USSR in late 1972 in search of more aid and additional lines of credit, after 3 years, he was turned down.
US involvement.
The United States opposition to Allende started several years before he was elected President of Chile. Declassified documents show that from 1962 through 1964, the CIA spent $3 millions in anti-Allende propaganda "to scare voters away from Allende's FRAP coalition", and spent a total of $2.6 millions to finance the presidential campaign of Eduardo Frei.
Declassified documents related to the military coup have shown that although the CIA didn't "instigate" the 1973 coup, they were well aware of it and knew about it in advance. However, the US refused to "provide any assistance" because it was "strictly an internal Chilean matter." According to CIA documents, the United States "probably appeared to condone coup," considering their intelligence collection and active participation in positively slanting propaganda in 1974 to place Pinochet and his military government in a positive light.
The possibility of Allende winning Chile's 1970 election was deemed a disaster by a US administration that wanted to protect US geopolitical interests by preventing the spread of Communism during the Cold War. In September 1970, President Nixon informed the CIA that an Allende government in Chile would not be acceptable and authorized $10 million to stop Allende from coming to power or unseat him. Henry Kissinger's 40 Committee and the CIA planned to impede Allende's investiture as President of Chile with covert efforts known as "Track I" and "Track II"; Track I sought to prevent Allende from assuming power via so-called "parliamentary trickery", while under the Track II initiative, the CIA tried to convince key Chilean military officers to carry out a coup.
Additionally, some point to the involvement of the Defense Intelligence Agency agents that allegedly secured the missiles used to bombard La Moneda Palace. In fact, open US military aid to Chile continued during the Allende administration, and the national government was very much aware of this, although there is no record that Allende himself believed that such assistance was anything but beneficial to Chile.
During Nixon's presidency, U.S. officials attempted to prevent Allende's election by financing political parties aligned with opposition candidate Jorge Alessandri and supporting strikes in the mining and transportation sectors. After the 1970 election, the Track I operation attempted to incite Chile's outgoing president, Eduardo Frei Montalva, to persuade his party (PDC) to vote in Congress for Alessandri. Under the plan, Alessandri would resign his office immediately after assuming it and call new elections. Eduardo Frei would then be constitutionally able to run again (since the Chilean Constitution did not allow a president to hold two consecutive terms, but allowed multiple non-consecutive ones), and presumably easily defeat Allende. The Chilean Congress instead chose Allende as President, on the condition that he would sign a "Statute of Constitutional Guarantees" affirming that he would respect and obey the Chilean Constitution and that his reforms would not undermine any of its elements.
Track II was aborted, as parallel initiatives already underway within the Chilean military rendered it moot.
During the second term of office of Democratic President Bill Clinton, the CIA acknowledged having played a role in Chilean politics before the coup, but its degree of involvement is debated. The CIA was notified by its Chilean contacts of the impending coup two days in advance but contends it "played no direct role in" the coup.
Much of the internal opposition to Allende's policies came from the business sector, and recently released U.S. government documents confirm that the U.S. indirectly funded the truck drivers' strike, which exacerbated the already chaotic economic situation before the coup.
The most prominent U.S. corporations in Chile before Allende's presidency were the Anaconda and Kennecott copper companies and ITT Corporation, International Telephone and Telegraph. Both copper corporations aimed to expand privatized copper production in the city of El Teniente in the Chilean Andes, the world's largest underground copper mine. At the end of 1968, according to US Department of Commerce data, U.S. corporate holdings in Chile amounted to $964 million. Anaconda and Kennecott accounted for 28% of U.S. holdings, but ITT had by far the largest holding of any single corporation, with an investment of $200 million in Chile. In 1970, before Allende was elected, ITT owned 70% of Chitelco, the Chilean Telephone Company and funded El Mercurio, a Chilean right-wing newspaper. Documents released in 2000 by the CIA confirmed that before the elections of 1970, ITT gave $700,000 to Allende's conservative opponent, Jorge Alessandri, with help from the CIA on how to channel the money safely. ITT president Harold Geneen also offered $1 million to the CIA to help defeat Allende in the elections.
After General Pinochet assumed power, United States Secretary of State Henry Kissinger told President Richard Nixon that the U.S. "didn't do it," but "we helped them...created the conditions as great as possible." (referring to the coup itself). Recent documents declassified under the Clinton administration's Chile Declassification Project show that the United States government and the CIA sought to overthrow Allende in 1970 immediately before he took office ("Project FUBELT"). Many documents regarding the U.S. intervention in Chile remain classified.
Relationships with the Soviet Union.
Salvador Allende is mentioned in a book written by the official historian of the British Intelligence MI5 Christopher Andrew. According to SIS and Andrew, the book is based on the handwritten notes of KGB archivist defector Vasili Mitrokhin. The book also named several Italians of the left as informants or KGB agents, and the right wing Prime Minister of the time, Silvio Berlusconi, opened an investigation to target his opponents. As Mitrokhin's information was very old, and most of the people in his files were dead or retired, he failed to find any evidence that any of the accused by the book were KGB agents or informants. Some Italian Ministers dismissed the archive as "not a dossier from the KGB but one about the KGB constructed by British counter-espionage agents based on the confession of an ex-agent, if there is one, and 'Mitrokhin' is just a codename for an MI5 operation". The Indian Congress party referred to the book as "pure sensationalism not even remotely based on facts or records" and pointed out that the book is not based on any official documents from the Soviet Union.
Christopher Andrew alleges that the KGB said that Allende "was made to understand the necessity of reorganizing Chile's army and intelligence services, and of setting up a relationship between Chile's and the USSR's intelligence services". But that claim is not accurate, because once Allende was elected President via the electoral ballot it became a new historical precedent. The Soviet Union observed closely if this alternative to Socialism could work and they did not interfere with the Chileans' decisions. Nikolai Leonov affirms that whenever he tried to give advice to Latin American leaders he was usually turned down by them, and he was told that they had their own understanding on how to conduct political business in their countries. Leonov adds that the relationships of KGB agents with Latin American leaders did not involve intelligence, because their intelligence target was the United States. Since many North Americans were living in the region, they were focusing in recruiting agents from the United States. Latin America was also a good region for KGB agents to get in touch with their informants from the CIA or other contacts from the United States than inside that country. Additionally, many scholars or experts in the field are skeptical about the reliability of Vasili Mitrokhin's claims, and believe that the origin of the source is doubtful or mysterious.
Political and moral support came mostly through the Communist Party and unions from the Soviet Union. For instance, Allende received the Lenin Peace Prize from the Soviet Union in 1972. However, there were some fundamental differences between Allende and Soviet political analysts who believed that some violence – or measures that those analysts "theoretically considered to be just" – should have been used. Declarations from KGB General Nikolai Leonov, former Deputy Chief of the First Chief Directorate of the KGB, confirmed that the Soviet Union supported Allende's government economically, politically and militarily. Leonov stated in an interview at the Chilean Center of Public Studies (CEP) that the Soviet economic support included over $100 million in credit, three fishing ships (that distributed 17,000 tons of frozen fish to the population), factories (as help after the 1971 earthquake), 3,100 tractors, 74,000 tons of wheat and more than a million tins of condensed milk.
In mid-1973 the USSR had approved the delivery of weapons (artillery, tanks) to the Chilean Army. However, when news of an attempt from the Army to depose Allende through a coup d'état reached Soviet officials, the shipment was redirected to another country.
Crisis.
On 29 June 1973, Colonel Roberto Souper surrounded the presidential palace, La Moneda, with his tank regiment but failed to depose the government. That failed "coup d’état" – known as the "Tanquetazo" ("tank putsch") – organised by the nationalist "Patria y Libertad" paramilitary group, was followed by a general strike at the end of July that included the copper miners of El Teniente.
In August 1973, a constitutional crisis occurred, and the Supreme Court of Chile publicly complained about the inability of Allende government to enforce the law of the land. On 22 August, the Chamber of Deputies (with the Christian Democrats uniting with the National Party) accused the government of unconstitutional acts through Allende's refusal to promulgate constitutional amendments, already approved by the Chamber, which would have prevented his government from continuing his massive nationalization plan and called upon the military to enforce constitutional order.
For months, Allende had feared calling upon the "Carabineros" ("Carabineers", the national police force), suspecting them of disloyalty to his government. On 9 August, President Allende appointed General Carlos Prats as Minister of Defence. On 24 August 1973, General Prats was forced to resign both as defense minister and as the commander-in-chief of the army, embarrassed by both the Alejandrina Cox incident and a public protest in front of his house by the wives of his generals. General Augusto Pinochet replaced him as Army commander-in-chief the same day.
According to Chilean political scientist Arturo Valenzuela (later becoming a U.S. citizen and Assistant Secretary of State for Hemispheric Affairs in the Obama administration), a greater share of the blame for the breakdown in Chilean democracy lay with the leftist Allende government. While each side increasingly distrusted the other, the extreme leftists accelerated the process and left less room for political moderation than the extreme rightists. He writes "By its actions, the revolutionary Left, which had always ridiculed the possibility of a socialist transformation through peaceful means, was engaged in a self-fulfilling prophecy."
Supreme Court's resolution.
On 26 May 1973, the Supreme Court of Chile unanimously denounced the Allende government's disruption of the legality of the nation in its failure to uphold judicial decisions, because of its continual refusal to permit police execution of judicial decisions contrary to the government's own measures.
Chamber of Deputies' resolution.
On 22 August 1973, the Christian Democrats and the National Party members of the Chamber of Deputies joined together to vote 81 to 47 in favor of a resolution that asked the authorities to "put an immediate end" to "breachof the Constitution. . . with the goal of redirecting government activity toward the path of law and ensuring the Constitutional order of our Nation, and the essential underpinnings of democratic co-existence among Chileans."
The resolution declared that Allende's government sought "to conquer absolute power with the obvious purpose of subjecting all citizens to the strictest political and economic control by the State. . . the goal of establishing. . . a totalitarian system" and claimed that the government had made "violations of the Constitution. . . a permanent system of conduct." Essentially, most of the accusations were about disregard by the Socialist government of the separation of powers, and arrogating legislative and judicial prerogatives to the executive branch of government.
Specifically, the Socialist government of President Allende was accused of:
Finally, the resolution condemned the creation and development of government-protected armed groups, which were said to be "headed towards a confrontation with the armed forces". President Allende's efforts to re-organize the military and the police forces were characterized as "notorious attempts to use the armed and police forces for partisan ends, destroy their institutional hierarchy, and politically infiltrate their ranks".
President Allende's response.
Two days later, on 24 August 1973, President Allende responded, characterising the Congress's declaration as "destined to damage the country’s prestige abroad and create internal confusion", predicting "It will facilitate the seditious intention of certain sectors." He noted that the declaration (passed 81–47 in the Chamber of Deputies) had not obtained the two-thirds Senate majority "constitutionally required" to convict the president of abuse of power: essentially, the Congress were "invoking the intervention of the armed forces and of Order against a democratically-elected government" and "subordinat political representation of national sovereignty to the armed institutions, which neither can nor ought to assume either political functions or the representation of the popular will."
Allende argued he had obeyed constitutional means for including military men to the cabinet at the service of civic peace and national security, defending republican institutions against insurrection and terrorism. In contrast, he said that Congress was promoting a "coup d’état" or a civil war with a declaration full of affirmations that had already been refuted beforehand and which, in substance and process (directly handing it to the ministers rather than directly handing it to the President) violated a dozen articles of the (then-current) Constitution. He further argued that the legislature was usurping the government's executive function.
President Allende wrote: "Chilean democracy is a conquest by all of the people. It is neither the work nor the gift of the exploiting classes, and it will be defended by those who, with sacrifices accumulated over generations, have imposed it...With a tranquil conscience...I sustain that never before has Chile had a more democratic government than that over which I have the honor to preside...I solemnly reiterate my decision to develop democracy and a state of law to their ultimate consequences...Congress has made itself a bastion against the transformations...and has done everything it can to perturb the functioning of the finances and of the institutions, sterilizing all creative initiatives."
Adding that economic and political means would be needed to relieve the country's current crisis, and that the Congress were obstructing said means; having already paralyzed the State, they sought to destroy it. He concluded by calling upon the workers, all democrats and patriots to join him in defending the Chilean Constitution and the revolutionary process.
Coup.
In early September 1973, Allende floated the idea of resolving the constitutional crisis with a plebiscite. His speech outlining such a solution was scheduled for September 11, but he was never able to deliver it. On September 11, 1973, the Chilean military, aided by the United States and its Central Intelligence Agency CIA, staged a coup against Allende.
Death.
Just before the capture of La Moneda (the Presidential Palace), with gunfire and explosions clearly audible in the background, Allende gave his farewell speech to Chileans on live radio, speaking of himself in the past tense, of his love for Chile and of his deep faith in its future. He stated that his commitment to Chile did not allow him to take an easy way out, and he would not be used as a propaganda tool by those he called "traitors" (he refused an offer of safe passage), clearly implying he intended to fight to the end.
Shortly afterwards, the coup plotters announced that Allende had committed suicide. An official announcement declared that the weapon he had used was an automatic rifle. Before his death he had been photographed several times holding an AK-47, a gift from Fidel Castro. He was found dead with this gun, according contemporaneous statements made by officials in the Pinochet regime.
Lingering doubts regarding the manner of Allende's death persisted throughout the dark years of the Pinochet regime—a regime that had kidnapped, tortured and murdered thousands of Chilean citizens. Given the regime's dismal record for truthtelling and its unfailing instinct for secrecy, many Chileans and independent observers refused to accept on faith the government's version of events amid speculation that Allende had been murdered by government agents. When in 2011 a Chilean court at last opened a criminal investigation into the circumstances of Allende's death, Pinochet had long since left power and Chile had meanwhile become one of the most stable democracies in the Americas according to "The Economist" magazine's democracy index.
The ongoing criminal investigation led to a May 2011 court order that Allende's remains be exhumed and autopsied by an international team of experts. Results of the autopsy were officially released in mid-July 2011. The team of experts concluded that the former president had shot himself with an AK-47 assault rifle. In December 2011 the judge in charge of the investigation affirmed the experts' findings and ruled Allende's death a suicide. On 11 September 2012, the 39th anniversary of Allende's death, a Chilean appeals court unanimously upheld the trial court's ruling, officially closing the case.
The Guardian, a leading UK newspaper, reported that a scientific autopsy of the remains had confirmed that "Salvador Allende committed suicide during the 1973 coup that toppled his socialist government." The Guardian went on to say that: 
According to Isabel Allende Bussi—the daughter of Salvador Allende and currently a member of the Chilean Senate—the Allende family has long accepted that the former President shot himself, telling the BBC that: "The report conclusions are consistent with what we already believed. When faced with extreme circumstances, he made the decision of taking his own life, instead of being humiliated."
The definitive and unanimous results produced by the 2011 Chilean judicial investigation appear to have laid to rest decades of nagging suspicions that Allende might have been assassinated by the Chilean Armed Forces. But public acceptance of the suicide theory had already been growing for much of the previous decade. In a post-junta Chile where restrictions on free speech were steadily eroding, independent and seemingly reliable witnesses at last began to tell their stories to the news media and to human rights researchers. The cumulative weight of the facts reported by these witnesses provided factual support for many previously unconfirmed details relating Allende's death.
The widespread acceptance of suicide as the cause Salvador Allende's death was, however, preceded by decades of speculation and controversializing about the circumstances surrounding his death. Several examples of pre-2011 speculation are shown below or on the Wikipedia page regarding the Death of Salvador Allende.
Alternate views regarding the death of Salvador Allende:
Family.
Well-known relatives of Salvador Allende include his daughter Isabel Allende (a politician) and his second niece Isabel Allende (a writer).
Memorials.
Memorials to Allende include a statue in front of the Palacio de la Moneda. The placement of the statue was not without controversy, as it is located facing the eastern edge of the Plaza de la Ciudadanía, this plaza containing memorials to a number of Chilean heroes. However, the statue is not located in the plaza, but rather on an surrounding sidewalk and facing an entrance to the plaza.
In Nicaragua the tourist port of the capital of Managua is named after him, the first socialist president of Latin America. The Salvador Allende Port is located near Downtown Managua.
The broken glasses of Allende were given to the Chilean National History Museum in 1996 by a woman who had found them in La Moneda in 1973.

</doc>
<doc id="51488" url="https://en.wikipedia.org/wiki?curid=51488" title="Rhone (disambiguation)">
Rhone (disambiguation)

Rhone can refer to:

</doc>
<doc id="51489" url="https://en.wikipedia.org/wiki?curid=51489" title="Poitiers">
Poitiers

Poitiers () is a city on the Clain river in west-central France. It is a commune and the capital of the Vienne department and also of the Poitou-Charentes region. Poitiers is a major university centre. The centre of town is picturesque and its streets include predominant historical architecture, especially religious architecture and especially from the Romanesque period. Two major military battles took place near the city: in 732, the Battle of Poitiers (also known as the Battle of Tours), in which the Franks commanded by Charles Martel halted the expansion of the Umayyad Caliphate, and in 1356, the Battle of Poitiers, a key victory for the English forces during the Hundred Years' War. This battle's consequences partly provoked the Jacquerie.
Geography.
Location.
The city of Poitiers is strategically situated on the Seuil du Poitou, a shallow gap between the Armorican and the Central Massif. The Seuil du Poitou connects the Aquitaine Basin to the South to the Paris Basin to the North. This area is an important geographic crossroads in France and Western Europe.
Situation.
Poitiers's primary site sits on a vast promontory between the valleys of the Boivre and the Clain. The old town occupies the slopes and the summit of a plateau which rises above the streams which surround it on three sides. Thus Poitiers benefits from a very strong tactical situation. This was an especially important factor before and throughout the Middle Ages.
Inhabitants and demography.
Inhabitants of Poitiers are referred as Pictaviens (male) and Pictaviennes (female) from Pictavis, which was the ancient name for the town. It is not uncommon for inhabitants of Poitiers to call themselves Poitevins or Poitevines, although this denomination can be used for anyone from the Poitou province.
, the population of Poitiers was 298,339. One out of three people in Poitiers is under the age of 30 and one out of four residents in Poitiers is a student.
Climate.
The climate in the Poitiers area is mild with mild temperature amplitudes, and adequate rainfall throughout the year. The Köppen Climate Classification subtype for this type of climate is "" (Marine West Coast Climate/Oceanic climate).
History.
Antiquity.
Poitiers was founded by the Celtic tribe of the Pictones and was known as the oppidum "Lemonum" before Roman influence. The name is said to have come from the Celtic word for elm, "Lemo". After Roman influence took over, the town became known as "Pictavium", or later "Pictavis", after the original Pictones inhabitants themselves.
There is a rich history of archeological finds from the Roman era in Poitiers. In fact until 1857 Poitiers hosted the ruins of a vast Roman amphitheatre, which was larger than that of Nîmes. Remains of Roman baths, built in the 1st century and demolished in the 3rd century, were uncovered in 1877.
In 1879 a burial-place and tombs of a number of Christian martyrs were discovered on the heights to the south-east of the town. The names of some of the Christians had been preserved in paintings and inscriptions. Not far from these tombs is a huge dolmen (the "Pierre Levée"), which is long, broad and high, and around which used to be held the great fair of Saint Luke.
The Romans also built at least three aqueducts. This extensive ensemble of Roman constructions suggests Poitiers was a town of first importance, possibly even the capital of the Roman province of "Gallia Aquitania" during the 2nd century.
As Christianity was made official and gradually introduced across the Roman Empire during the 3rd and 4th centuries, the first bishop of Poitiers from 350 to 367, Hilary of Poitiers or Saint Hilarius, proceeded to evangelize the town. Exiled by Constantius II, he risked death to return to Poitiers as Bishop after discovering that the Christian "Eastern" Church were not heretical as believed in Rome, but had, rather, reached many of the same conclusions about the Holy Trinity as had the Western Church. The first foundations of the Baptistère Saint-Jean can be traced to that era of open Christian evangelization. He was named "Doctor of The Church" by Pope Pius IX.
In the 4th century, a thick wall 6m wide and 10m high was built around the town. It was long and stood lower on the naturally defended east side and at the top of the promontory. Around this time, the town began to be known as Poitiers.
Fifty years later Poitiers fell into the hands of the Arian Visigoths, and became one of the principal residences of their kings. Visigoth King Alaric II was defeated by Clovis I at Vouillé, not far from Poitiers, in 507, and the town thus came under Frankish dominion.
Middle Ages.
During most of the Early Middle Ages, the town of Poitiers took advantage of its defensive tactical site and of its location, which was far from the centre of Frankish power. As the seat for an "évêché" (bishop) since the 4th century, the town was a centre of some importance and the capital of the Poitou county. At the height of their power, the Counts of Poitiers governed a large domain, including both Aquitaine and Poitou.
The first decisive victory of a Christian army over a Muslim power, the Battle of Tours, was fought by Charles Martel's men in the vicinity of Poitiers on 10 October 732. For many historians, it was one of the world's pivotal moments.
Eleanor of Aquitaine frequently resided in the town, which she embellished and fortified, and in 1199 entrusted with communal rights. In 1152 she married the future King Henry II of England in Poitiers Cathedral.
During the Hundred Years' War, the Battle of Poitiers, an English victory, was fought near the town of Poitiers on 19 September 1356. Later in the war In 1418, under duress, the royal parliament moved from Paris to Poitiers, where it remained in exile until the Plantagenets finally withdrew from the capital in 1436. During this interval, in 1429 Poitiers was the site of Joan of Arc's formal inquest.
The University of Poitiers was founded in 1431. During and after the Reformation, John Calvin had numerous converts in Poitiers and the town had its share of the violent proceedings which underlined the Wars of Religion throughout France.
In 1569 Poitiers was defended by Gui de Daillon, comte du Lude, against Gaspard de Coligny, who after an unsuccessful bombardment and seven weeks, retired from a siege he had laid to the town.
16th century.
The type of political organisation existing in Poitiers during the late medieval or early modern period can be glimpsed through a speech given on 14 July 1595 by Maurice Roatin, the town's mayor. He compared it to the Roman state, which combined three types of government: monarchy (rule by one person), aristocracy (rule by a few), and democracy (rule by the many). He said the Roman consulate corresponded to Poitiers' mayor, the Roman senate to the town's peers and "échevins", and the democratic element in Rome corresponded to the fact that most important matters "can not be decided except by the advice of the "Mois et Cent"" (broad council).1 The mayor appears to have been an advocate of a mixed constitution; not all Frenchmen in 1595 would have agreed with him, at least in public; many spoke in favour of absolute monarchy. The democratic element was not as strong as the mayor's words may seem to imply: in fact, Poitiers was similar to other French cities, Paris, Nantes, Marseille, Limoges, La Rochelle, Dijon, in that the town's governing body ("corps de ville") was "highly exclusive and oligarchical": a small number of professional and family groups controlled most of the city offices. In Poitiers many of these positions were granted for the lifetime of the office holder.2
The city government in Poitiers based its claims to legitimacy on the theory of government where the mayor and "échevins" held jurisdiction of the city's affairs in fief from the king: that is, they swore allegiance and promised support for him, and in return he granted them local authority. This gave them the advantage of being able to claim that any townsperson who challenged their authority was being disloyal to the king. Every year the mayor and the 24 "échevins" would swear an oath of allegiance "between the hands" of the king or his representative, usually the lieutenant général or the sénéchaussée. For example, in 1567, when Maixent Poitevin was mayor, king Henry III came for a visit, and, although some townspeople grumbled about the licentious behaviour of his entourage, Henry smoothed things over with a warm speech acknowledging their allegiance and thanking them for it.2
In this era, the mayor of Poitiers was preceded by sergeants wherever he went, consulted deliberative bodies, carried out their decisions, "heard civil and criminal suits in first instance", tried to ensure that the food supply would be adequate, visited markets.2
In the 16th century, Poitiers impressed visitors because of its large size, and important features, including "royal courts, university, prolific printing shops, wealthy religious institutions, cathedral, numerous parishes, markets, impressive domestic architecture, extensive fortifications, and castle."3
16th-century Poitiers is closely associated with the life of François Rabelais and with the community of Bitards.
17th century.
The town saw less activity during the Renaissance. Few changes were made in the urban landscape, except for laying way for the "rue de la Tranchée". Bridges were built where the inhabitants had used "gués". A few "hôtels particuliers" were built at that time, such as the hôtels Jean Baucé, Fumé and Berthelot. Poets Joachim du Bellay and Pierre Ronsard met at the University of Poitiers, before leaving for Paris.
During the 17th century, many people emigrated from Poitiers and the Poitou to the French settlements in the new world and thus many Acadians or Cajuns living in North America today can trace ancestry back to this region.
18th century.
During the 18th century, the town's activity mainly depended on its administrative functions as a regional centre: Poitiers served as the seat for the regional administration of royal justice, the évêché, the monasteries and the intendance of the "Généralité du Poitou".
The Vicomte de Blossac, intendant of Poitou from 1750 to 1784, had a French garden landscaped in Poitiers. He also had Aliénor d'Aquitaine's ancient wall razed and modern boulevards were built in its place.
19th century.
During the 19th century, many army bases were built in Poitiers because of its central and strategic location. Poitiers became a garrison town, despite its distance from France's borders.
The Poitiers train station was built in the 1850s, and connected Poitiers to the rest of France.
20th century and contemporary Poitiers.
Poitiers was bombed during World War II, particularly the area around the railway station which was heavily hit on 13 June 1944.
From the late 1950s until the late 1960s when Charles de Gaulle ended the American military presence, the U.S. Army and U.S. Air Force had an array of military installations in France, including a major Army logistics and communications hub in Poitiers, part of what was called the Communication Zone (ComZ), and consisting of a logistics headquarters and communications agency located at Aboville Caserne, a military compound situated on a hill above the city. Hundreds of graduates ("Military Brats") of Poitiers American High School, a school operated by the Department of Defense School System (DODDS), have gone on to successful careers, including the recent commander-in-chief of U.S. Special Forces Command, Army General Bryan (Doug) Brown. The Caserne also housed a full support community, with a theater, commissary, recreation facilities and an affiliate radio station of the American Forces Network, Europe, headquartered in Frankfurt (now Mannheim, Germany). 
The town benefited from industrial "décentralisation" in the 1970s, for instance with the installation during that decade of the Michelin and Compagnie des compteurs Schlumberger factories. The "Futuroscope" theme-park and research park project, built in 1986–1987 in nearby Chasseneuil-du-Poitou, after an idea by René Monory, consolidated Poitiers' place as a touristic destination and as a modern university centre, and opened the town to the era of information technology. 
Sports.
The Stade Poitevin, founded in 1900, is a multi-sports club, which fields several top-level teams in a variety of sports. These include a volleyball team that play in the French Pro A volleyball league, a basketball team, an amateur football team and a professional rugby team (as of the 2008–2009 season.)
The PB86 or Poitiers Basket 86 (www.pb86.fr) play in the French Pro A basketball league. In the 2009–10 season, three Americans played for PB86: Rasheed Wright, Kenny Younger and Tommy Gunn. The team played the French championship playoffs in the 2009–10 season and was the Pro B French Champion for the 2008–2009 season. The team's communication strategy is considered by some to be one of the best in the French basketball league.
Brian Joubert, the figure skating champion, practices at an ice rink in Poitiers and lives with his family in the city.
Tourism.
Historic churches, in particular Romanesque church buildings, are the main attraction inside Poitiers itself. The town's centre is picturesque, with generally well-preserved architecture and a recently re-zoned pedestrian area. There are numerous shops, cafes and restaurants in the town centre.
Since 1987, Poitiers' tourist industry has indirectly benefited from the "Futuroscope" theme-park and research park in nearby Chasseneuil-du-Poitou. The centre of town receives visits in complement to the theme-park and benefits from a larger proportion of European tourists, notably from the United Kingdom. In conjunction, Poitiers' tourism has directly benefited from the TGV high-speed rail link to Paris.
Transport.
Poitiers' railway station lies on the TGV Atlantique line between Paris and Bordeaux. The station is in the valley to the west of the old town centre. Services run to Angoulême, Limoges and La Rochelle in addition to Paris and Bordeaux. The direct TGV puts Poitiers 1h40 from Paris' Gare Montparnasse.
Poitiers - Biard Airport is located west of Poitiers with flights to Lyon-Saint Exupéry, London-Stansted, Edinburgh and Shannon, Ireland on Ryanair.
Urban transportation in Poitiers is provided by a company called Vitalis. Regional ground transportation in the department of the Vienne is provided by private bus companies such as "Ligne en Vienne". Rail transportation in the region is provided by the public TER Poitou-Charentes (regional express train).
From January 2009 to December 2012, Poitiers' town centre went through deep changes to make it less accessible to motor vehicles. The project, named "Projet Coeur d'Agglo", focused on re-thinking the way people use individual cars to access the town centre and as an everyday way of transportation. On 29 September 2010, 12 streets were permanently closed off to motor vehicles and transformed into an entirely pedestrian zone.
Eventually, a new line of fast buses will be added around 2017.
Education.
The city of Poitiers has a very old tradition as a university centre, starting in the Middle Ages. The University of Poitiers was established in 1431 as the second oldest university in France, and has welcomed many famous philosophers and scientists throughout the ages (notably François Rabelais; René Descartes; Francis Bacon; Samir Amin).
Today Poitiers is one of the biggest university towns in France; in fact it has more students per inhabitant than any other large town or city in France. All around, there are over 27,000 university students in Poitiers, nearly 4,000 of which are foreigners, hailing from 117 countries. The University covers all major fields from sciences to geography, history, languages economics and law.
The law degree at the University of Poitiers is considered to be one of the best in France. The program was ranked second by "l'Étudiant magazine" in 2005.
In addition to the University, Poitiers also hosts two engineering schools and two business schools:
Since 2001, the city of Poitiers has hosted the first cycle of "the South America, Spain and Portugal" program from the Paris Institute of Political Studies.
International relations.
Twin towns – Sister cities.
Poitiers is twinned with:
Notable people.
This is a list of people of interest who were born or resided in Poitiers:

</doc>
<doc id="51490" url="https://en.wikipedia.org/wiki?curid=51490" title="Rights">
Rights

Rights are legal, social, or ethical principles of freedom or entitlement; that is, rights are the fundamental normative rules about what is allowed of people or owed to people, according to some legal system, social convention, or ethical theory. Rights are of essential importance in such disciplines as law and ethics, especially theories of justice and deontology.
Rights are often considered fundamental to civilization, being regarded as established pillars of society and culture, and the history of social conflicts can be found in the history of each right and its development. According to the "Stanford Encyclopedia of Philosophy", "rights structure the form of governments, the content of laws, and the shape of morality as it is currently perceived."
Definitional issues.
There is considerable disagreement about what is meant precisely by the term "rights". It has been used by different groups and thinkers for different purposes, with different and sometimes opposing definitions, and the precise definition of this principle, beyond having something to do with normative rules of some sort or another, is controversial.
One way to get an idea of the multiple understandings and senses of the term is to consider different ways it is used. Many diverse things are claimed as rights: 
There are likewise diverse possible ways to categorize rights, such as:
There has been considerable debate about what this term means within the academic community, particularly within fields such as philosophy, law, deontology, logic, political science, and religion.
Natural rights versus legal rights.
Some thinkers see rights in only one sense while others accept that both senses have a measure of validity. There has been considerable philosophical debate about these senses throughout history. For example, Jeremy Bentham believed that legal rights were the essence of rights, and he denied the existence of natural rights; whereas Thomas Aquinas held that rights purported by positive law but not grounded in natural law were not properly rights at all, but only a facade or pretense of rights.8
Claim rights versus liberty rights.
Liberty rights and claim rights are the inverse of one another: a person has a liberty right permitting him to do something only if there is no other person who has a claim right forbidding him from doing so. Likewise, if a person has a claim right against someone else, then that other person's liberty is limited. For example, a person has a "liberty right" to walk down a sidewalk and can decide freely whether or not to do so, since there is no obligation either to do so or to refrain from doing so. But pedestrians may have an obligation not to walk on certain lands, such as other people's private property, to which those other people have a claim right. So a person's "liberty right" of walking extends precisely to the point where another's "claim right" limits his or her freedom.
Positive rights versus negative rights.
In one sense, a right is a permission to do something or an entitlement to a specific service or treatment from others, and these rights have been called "positive rights". However, in another sense, rights may allow or require inaction, and these are called "negative rights"; they permit or require doing nothing. For example, in some democracies e.g. the US, citizens have the "positive right" to vote and they have the "negative right" to not vote; people can choose not to vote in a given election without punishment. In other democracies e.g. Australia, however, citizens have a positive right to vote but they don't have a negative right to not vote, since voting is compulsory. Accordingly:
Though similarly named, positive and negative rights should not be confused with "active rights" (which encompass "privileges" and "powers") and "passive rights" (which encompass "claims" and "immunities").
Individual rights versus group rights.
The general concept of rights is that they are possessed by individuals in the sense that they are permissions and entitlements to do things which other persons, or which governments or authorities, can not infringe. This is the understanding of people such as the author Ayn Rand who argued that only individuals have rights, according to her philosophy known as Objectivism. However, others have argued that there are situations in which a group of persons is thought to have rights, or "group rights". Accordingly:
There can be tension between individual and group rights. A classic instance in which group and individual rights clash is conflicts between unions and their members. For example, individual members of a union may wish a wage higher than the union-negotiated wage, but are prevented from making further requests; in a so-called closed shop which has a union security agreement, only the union has a "right" to decide matters for the individual union members such as wage rates. So, do the supposed "individual rights" of the workers prevail about the proper wage? Or do the "group rights" of the union regarding the proper wage prevail? Clearly this is a source of tension.
The Austrian School of Economics holds that only individuals think, feel, and act whether or not members of any abstract group. The society should thus according to economists of the school be analyzed starting from the individual. This methodology is called methodological individualism and is used by the economists to justify individual rights.
Other senses.
Other distinctions between rights draw more on historical association or family resemblance than on precise philosophical distinctions. These include the distinction between civil and political rights and economic, social and cultural rights, between which the articles of the Universal Declaration of Human Rights are often divided. Another conception of rights groups them into three generations. These distinctions have much overlap with that between negative and positive rights, as well as between individual rights and group rights, but these groupings are not entirely coextensive.
Rights and politics.
Rights are often included in the foundational questions that governments and politics have been designed to deal with. Often the development of these socio-political institutions have formed a dialectical relationship with rights.
Rights about particular issues, or the rights of particular groups, are often areas of special concern. Often these concerns arise when rights come into conflict with other legal or moral issues, sometimes even other rights. Issues of concern have historically included labor rights, LGBT rights, reproductive rights, disability rights, patient rights and prisoners' rights. With increasing monitoring and the information society, information rights, such as the right to privacy are becoming more important.
Some examples of groups whose rights are of particular concern include animals, and amongst humans, groups such as children and youth, parents (both mothers and fathers), and men and women.
Accordingly, politics plays an important role in developing or recognizing the above rights, and the discussion about which behaviors are included as "rights" is an ongoing political topic of importance. The concept of rights varies with political orientation. Positive rights such as a "right to medical care" are emphasized more often by left-leaning thinkers, while right-leaning thinkers place more emphasis on negative rights such as the "right to a fair trial".
Further, the term "equality" which is often bound up with the meaning of "rights" often depends on one's political orientation. Conservatives and libertarians and advocates of free markets often identify equality with equality of opportunity, and want equal and fair rules in the process of making things, while agreeing that sometimes these fair rules lead to unequal outcomes. In contrast, socialists often identify equality with equality of outcome and see fairness when people have equal amounts of goods and services, and therefore think that people have a right to equal portions of necessities such as health care or economic assistance or housing.
Rights and philosophy.
In philosophy, meta-ethics is the branch of ethics that seeks to understand the nature of ethical properties, statements, attitudes, and judgments. Meta-ethics is one of the three branches of ethics generally recognized by philosophers, the others being normative ethics and applied ethics.
While normative ethics addresses such questions as "What should one do?", thus endorsing some ethical evaluations and rejecting others, meta-ethics addresses questions such as "What "is" goodness?" and "How can we tell what is good from what is bad?", seeking to understand the nature of ethical properties and evaluations.
Rights ethics is an answer to the meta-ethical question of "what normative ethics is concerned with". (Metaethics also includes a group of questions about how ethics comes to be known, true, etc. which is not directly addressed by rights ethics).
Rights ethics holds that normative ethics is concerned with rights. Alternative metaethical theories are that ethics is concerned with one of the following
Rights ethics has had considerable influence on political and social thinking. The Universal Declaration of Human Rights gives some concrete examples of widely accepted rights.
Criticism.
Some philosophers have criticised rights as ontologically dubious entities. For instance, although in favour of the extension of individual legal rights, the utilitarian philosopher Jeremy Bentham opposed the idea of natural law and natural rights, calling them "nonsense upon stilts". Further, one can question the ability of rights to actually bring about justice for all.
Etymology.
The Modern English word "right" derives from Old English "riht" or "reht", in turn from Proto-Germanic "*riχtaz" meaning "right" or "direct", and ultimately from Proto-Indo-European "*reg-to-" meaning "having moved in a straight line", in turn from "*(o)reg'(a)-" meaning "to straighten or direct". In several different Indo-European languages, a single word derived from the same root means both "right" and "law", such as French "droit", Spanish "derecho", German "Recht". and Italian "diritto".
Many other words related to normative or regulatory concepts derive from this same root, including "correct", "regulate", and "rex" (meaning "king"), whence "regal" and thence "royal". Likewise many more geometric terms derive from this same root, such as "erect" (as in "upright"), "rectangle" (literally "right angle"), "straight" and "stretch". Like "right", the English words "rule" and "ruler", deriving still from the same root, have both normative or regulatory and geometric meanings (e.g. a ruler as in a king, or a ruler as in a straightedge).
Several other roots have similar normative and geometric descendants, such as Latin "norma", whence "norm", "normal", and "normative" itself, and also geometric concepts such as "normal vectors"; and likewise Greek "ortho" and Latin "ordo", meaning either "right" or "correct" (as in "orthodox", meaning "correct opinion") or "straight" or "perpendicular" (as in "orthogonal", meaning "perpendicular angle"), and thence "order", "ordinary", etc.
History of rights.
The specific enumeration of rights has differed greatly in different periods of history. In many cases, the system of rights promulgated by one group has come into sharp and bitter conflict with that of other groups. In the political sphere, a place in which rights have historically been an important issue, constitutional provisions of various states sometimes address the question of who has what legal rights.
Historically, many notions of rights were authoritarian and hierarchical, with different people granted different rights, and some having more rights than others. For instance, the right of a father to respected from his son did not indicate a right from the son to receive a return from that respect; and the divine right of kings, which permitted absolute power over subjects, did not leave a lot of room for many rights for the subjects themselves.
In contrast, modern conceptions of rights often emphasize liberty and equality as among the most important aspects of rights, for example in the American Revolution and the French Revolution.
Important documents in the political history of rights include:
See also.
Outline of rights
Organisations:

</doc>
<doc id="51491" url="https://en.wikipedia.org/wiki?curid=51491" title="Samarkand">
Samarkand

Samarkand (from Sogdian: "Stone Fort" or "Rock Town"; ; ; Cyrillic/), alternatively Samarqand or Samarcand, is one of the oldest inhabited cities in Central Asia, prospering from its location on the Silk Road between China and the Mediterranean. At times Samarkand has been one of the greatest cities of Central Asia; traditionally it is the capital of Samarqand Region, and is Uzbekistan's third largest city, after fast-growing Namangan in the Ferghana Valley. The city is noted for being an Islamic centre for scholarly study. In the 14th century it became the capital of the empire of Timur (Tamerlane) and is the site of his mausoleum (the Gur-e Amir). The Bibi-Khanym Mosque (a modern replica) remains one of the city's most notable landmarks. The Registan was the ancient center of the city. The city has carefully preserved the traditions of ancient crafts: embroidery, gold embroidery, silk weaving, engraving on copper, ceramics, carving and painting on wood.
There is evidence of human activity in the area of the city from the late paleolithic era, though there is no direct evidence of when exactly Samarkand was founded, some theories are that it was founded between the 8th and 7th centuries BC. By the time of the Achaemenid dynasty of Persia, it was the capital of the Sogdian satrapy. The city was taken by Alexander the Great in 329 BC, when it was known by its Greek name of Marakanda. The city was ruled by a succession of Iranian, Persian, and Turkish peoples until the Mongols under Genghis Khan conquered Samarkand in 1220.
In 2001, UNESCO added the city to its World Heritage List as "Samarkand – Crossroads of Cultures".
Etymology.
The name probably originates in the Sogdian words "asmara", "stone", "rock" and "kand", "fort", "town".
Early history.
Along with Bukhara, Samarkand is one of the oldest inhabited cities in Central Asia, prospering from its location on the trade route between China and the Mediterranean (Silk Road). At times Samarkand has been one of the greatest cities of Central Asia.
Archeological excavations held within the city limits (Syob and midtown) as well as suburban areas (Hojamazgil, Sazag'on) unearthed evidence of human activity as early as 40,000 years old, which is late paleolithic era. A group of Mesolithic era (12-7 millennium BCE) archeological sites were discovered at Sazag'on-1, Zamichatosh, Okhalik (suburbs of the city). Syob and Darg'om canals, supplying with water the city and its suburbs appeared around the 7th to 5th centuries BCE (early Iron Age). There is no direct evidence of when exactly Samarkand was founded. Researchers of Institute of Archeology of Samarkand argue existence of the city between the 8th and 7th centuries BCE. Samarkand has been one of the main centres of Sogdian civilization from its early days. By the time of the Achaemenid dynasty of Persia, it had become the capital of the Sogdian satrapy.
Hellenistic period.
While settlement in the region goes well back into pre-historic times, by the 7th century BCE, the town seems to have housed a substantial center of craft production and already boasted an extensive irrigation system. It was one of the easternmost administrative centers for Achaemenid Persia and had a citadel and strong fortifications. 
Alexander the Great conquered Samarkand in 329 BCE. The city was known as Maracanda by the Greeks. Written sources offer small clues as to the subsequent system of government. They tell of an Orepius who became ruler "not from ancestors, but as a gift of Alexander".
While Samarkand suffered significant damage during Alexander's initial conquest, the city recovered rapidly and under the new Hellenic influence flourished. There were also major new construction techniques; oblong bricks were replaced with square ones and superior methods of masonry and plastering were introduced. It was later part of Seleucid Empire, Greco-Bactrian Kingdom and Kushan Empire successively. Alexander's conquests introduced into Central Asia Classical Greek culture; at least for a time the Greek models were followed closely by the local artisans. The Greek legacy lived on in the various "Graeco-Bactrian" kingdoms of the area and the Kushan Empire of the first centuries of the Common Era whose territories extended well down into what is today Pakistan and India. During the Kushan era, the city declined though; it did not really revive until the 5th century CE.
Pre-Mongol period.
Samarkand was conquered by the Sassanians around 260 CE. Under Sassanian rule, the region became an essential site for Manichaeism, and facilitated the dissemination of the religion throughout central Asia.
After the Hephtalites conquered Samarkand, they controlled it until the Göktürks, in an alliance with the Sassanid Persians, captured it during the Battle of Bukhara. The Turks ruled over Samarkand until they were defeated by the Sassanids during the Göktürk–Persian Wars. After the Islamic conquest of Iran the Turks conquered Samarkand and held it until the Turkic khaganate collapsed due to wars with the Chinese Tang Dynasty. During this time the city became a protectorate and paid tribute to the ruling Tang. The armies of the Umayyad Caliphate under Qutayba ibn Muslim captured the city in around 710 from Turks.
During this period, Samarkand was a diverse religious community and was home to a number of religions, including Buddhism, Zoroastrianism, Hinduism, Manichaeism, Judaism and Nestorian Christianity. However, after the Arab conquest of Sogdiana, Islam became the dominant religion in Samarkand, with much of the population converting.
Legend has it that during Abbasid rule, the secret of papermaking was obtained from two Chinese prisoners from the Battle of Talas in 751, which led to the foundation of the first paper mill of the Islamic world in Samarkand. The invention then spread to the rest of the Islamic world, and from there to Europe.
The Abbasid control of Samarkand soon dissipated and was replaced with that of the Samanids (862–999), though it must be noted that the Samanids were still nominal vassals of the Caliph during their control of Samarkand. Under Samanid rule the city became one of the capitals of the Samanid dynasty and an even more important link amongst numerous trade routes. The Samanids were overthrown by Karakhanids in around 1000. During the next two hundred years, Samarkand would be ruled by a succession of Turkish tribes, including the Seljuqs and the Khwarazm-Shahs.
The 10th-century Iranian author Istakhri, who travelled in Transoxiana, provides a vivid description of the natural riches of the region he calls "Smarkandian Sogd":
I know no place in it or in Samarkand itself where if one ascends some elevated ground one does not see greenery and a pleasant place, and nowhere near it are mountains lacking in trees or a dusty steppe...Samakandian Sogd...eight days travel through unbroken greenery and gardens...The greenery of the trees and sown land extends along both sides of the river [Sogd...and beyond these fields is pasture for flocks. Every town and settlement has a fortress...It is the most fruitful of all the countries of Allah; in it are the best trees and fruits, in every home are gardens, cisterns and flowing water...
Mongol period.
The Mongols conquered Samarkand in 1220. Although Genghis Khan "did not disturb the inhabitants the city in any way", according to Juvaini he killed all who took refuge in the citadel and the mosque. He also pillaged the city completely and conscripted 30,000 young men along with 30,000 craftsmen. Samarkand suffered at least one other Mongol sack by Khan Baraq to get treasure he needed to pay an army. It was part of Chagatai Khanate till 1370.
"The Travels" of Marco Polo, where Polo records his journey along the Silk Road, describes Samarkand as "a very large and splendid city..." Here also is related the story of a Christian church in Samarkand, which miraculously remained standing after a portion of its central supporting column was removed..
14th century.
In 1365, a revolt against Mongol control occurred in Samarkand.
In 1370 Timur, the founder and ruler of the Timurid Empire, made Samarkand his capital. During the next 35 years, he rebuilt most of the city and populated it with the great artisans and craftsmen from across the empire. Timur gained a reputation as a patron of the arts and Samarkand grew to become the centre of the region of Transoxiana. Timur’s commitment to the arts is evident in the way he was ruthless with his enemies but merciful towards those with special artistic abilities. He spared the lives of artists, craftmen and architects so that he could bring them to improve and beautify his capital. He was also directly involved in his construction projects and his visions often exceeded the technical abilities of his workers. Furthermore, the city was in a state of constant construction and Timur would often request buildings to be done and redone quickly if he was unsatisfied with the results. Timur made it so that the city could only be reached by roads and also ordered the construction of deep ditches and walls, that would run five miles () in circumference, separating the city from the rest of its surrounding neighbors. During this time the city had a population of about 150,000. This great period of reconstruction is incapsulated in the account of Henry III's ambassador, Ruy Gonzalez de Clavijo, who was stationed there between 1403 and 1406. During his stay the city was typically in a constant state of construction. "The Mosque which Timur had caused to be built in memory of the mother of his wife...seemed to us the noblest of all those we visited in the city of Samarkand, but no sooner had it been completed than he begun to find fault with its entrance gateway, which he now said was much too low and must forthwith be pulled down."
15th century.
Between 1424 and 1429, the great astronomer Ulugh Beg built the Samarkand Observatory. The sextant was 11 metres long and once rose to the top of the surrounding three-storey structure, although it was kept underground to protect it from earthquakes. Calibrated along its length, it was the world's largest 90-degree quadrant at the time. However, the observatory was destroyed by religious fanatics in 1449.
Modern history.
In 1500 the Uzbek nomadic warriors took control of Samarkand. The Shaybanids emerged as the Uzbek leaders at or about this time.
In the second quarter of 16th century, the Shaybanids moved their capital to Bukhara and Samarkand went into decline. After an assault by Nader Shah the city was abandoned in the 18th century, about 1720 or a few years later.
From 1599 to 1756, Samarkand was ruled by the Ashtarkhanid dynasty of Bukhara.
From 1756 to 1868, Samarkand was ruled by the Manghyt emirs of Bukhara.
The city came under Russian rule after the citadel had been taken by a force under Colonel Konstantin Petrovich von Kaufman in 1868. Shortly thereafter the small Russian garrison of 500 men were themselves besieged. The assault, which was led by Abdul Malik Tura, the rebellious elder son of the Bukharan Emir, as well as Baba Beg of Shahrisabz and Jura Beg of Kitab, was repelled with heavy losses. Alexander Abramov became the first Governor of the Military Okrug, which the Russians established along the course of the Zeravshan River, with Samarkand as the administrative centre. The Russian section of the city was built after this point, largely to the west of the old city.
In 1886, the city became the capital of the newly formed Samarkand Oblast of Russian Turkestan and grew in importance still further when the Trans-Caspian railway reached the city in 1888. It became the capital of the Uzbek SSR in 1925 before being replaced by Tashkent in 1930.
People.
According to various independent sources, Tajiks (Persian-speaking people) are the major ethnic group in the city, while ethnic Uzbeks form a growing minority. Exact figures are difficult to evaluate, since many people in Uzbekistan either identify as "Uzbek" even though they speak Eastern Persian as their first language, or because they are registered as Uzbeks by the central government despite their Eastern Persian language and identity. As explained by Paul Bergne:
During the census of 1926 a significant part of the Tajik population was registered as Uzbek. Thus, for example, in the 1920 census in Samarkand city the Tajiks were recorded as numbering 44,758 and the Uzbeks only 3301. According to the 1926 census, the number of Uzbeks was recorded as 43,364 and the Tajiks as only 10,716. In a series of kishlaks in the Khojand Okrug, whose population was registered as Tajik in 1920 e.g. in Asht, Kalacha, Akjar i Tajik and others, in the 1926 census they were registered as Uzbeks. Similar facts can be adduced also with regard to Ferghana, Samarkand, and especially the Bukhara oblasts.
Architecture.
Timur initiated the building of Bibi Khanum after his campaign in India in 1398-1399. Before its reconstruction after an earthquake in 1897, Bibi Khanum had around 450 marble columns that were established with the help of 95 elephants that Timur had brought back from Hindustan. Also from India, artisans and stonemasons designed the mosque’s dome, giving it its distinctiveness amongst the other buildings.
The best-known structure in Samarkand is the mausoleum known as Gur-i Amir. It exhibits many cultures and influences from past civilizations, neighboring peoples, and especially those of Islam. Despite how much devastation the Mongols caused in the past to all of the Islamic architecture that had existed in the city prior to Timur's succession, much of the destroyed Islamic influences were revived, recreated, and restored under Timur. The blueprint and layout of the mosque itself follows the Islamic passion of geometry and other elements of the structure had been precisely measured. The entrance to the Gur-i Amir is decorated with Arabic calligraphy and inscriptions, the latter being a common feature in Islamic architecture. The attention to detail and meticulous nature of Timur is especially obvious when looking inside the building. Inside, the walls have been covered in tiles through a technique, originally developed in Iran, called “mosaic faience,” a process where each tile is cut, colored, and fit into place individually. The tiles were also arranged in a specific way that would engrave words relating to the city's religiosity; words like "Muhammad" and "Allah" have been spelled out on the walls using the tiles.
The ornaments and decorations of the walls include floral and vegetal symbols which are used to signify gardens. Gardens are commonly interpreted as paradise in the Islamic religion and they were both inscribed in tomb walls and grown in the city itself. In the city of Samarkand, there were two major gardens, the New Garden and the Garden of Heart’s Delight, and these became the central areas of entertainment for ambassadors and important guests. A friend of Genghis Khan in 1218 named Yelü Chucai, reported that Samarkand was the most beautiful city of all where "it was surrounded by numerous gardens. Every household had a garden, and all the gardens were well designed, with canals and water fountains that supplied water to round or square-shaped ponds. The landscape included rows of willows and cypress trees, and peach and plum orchards were shoulder to shoulder." The floors of the mausoleum is entirely covered with uninterrupted patterns of tiles of flowers, emphasizing the presence of Islam and Islamic art in the city. In addition, Persian carpets with floral printings have been found in some of the Timurid buildings.
Turko-Mongol influence is also apparent in the architecture of the buildings in Samarkand. For instance, nomads previously used tents, or "yurts", to display the bodies of the dead before they were to engage in proper burial procedures. Similarly, it is believed that the melon-shaped domes of the tomb chambers are imitations of those very "yurts". Timur, naturally, used stronger materials, like bricks and wood, to establish these tents, but their purposes remain largely unchanged.
The color of the buildings in Samarkand also has significant meaning behind it. For instance, blue is the most common and dominant color that will be found on the buildings, which was used by Timur in order to symbolize a large range of ideas. For one, the blue shades seen in the Gur-i Amir are colors of mourning. Blue was the color of mourning in Central Asia at the time, as it is in many cultures even today, and its dominance in the city's mausoleum appears to be a very rational idea. In addition, blue was also seen as the color that would ward off "the evil eye" in Central Asia and the notion is evident in the number of doors in and around the city that were colored blue during this time. Furthermore, blue was representative of water, which was a particularly rare resource around the Middle East and Central Asia; coloring the walls blue symbolized the wealth of the city.
Gold also has a strong presence in the city. Timur's fascination with vaulting explains the excessive use of gold in the Gur-i Amir as well as the use of embroidered gold fabric in both the city and his buildings. The Mongols had great interests in Chinese- and Persian-style golden silk textiles as well as nasij woven in Iran and Transoxiana. Past Mongol leaders, like Ogodei, built textile workshops in their cities in order to be able to produce gold fabrics themselves.
There is evidence that Timur tried to preserve his Mongol roots. In the chamber in which his body was laid, "tuqs" were found. "Tuqs" are poles with horses' tails hanging at the top, which was symbolic of an ancient Turkic tradition where horses, which were valuable commodities, were sacrificed in order to honor the dead.
Climate.
Samarkand features a Mediterranean climate (Köppen climate classification "Csa") that closely borders on a semi-arid climate with hot, dry summers and relatively wet, variable winters that alternate periods of warm weather with periods of cold weather. July and August are the hottest months of the year with temperatures reaching, and exceeding, . Most of the sparse precipitation is received from December through April. January 2008 was particularly cold, and the temperature dropped to 
Notable people.
Also said to be the place of death of Muhammad b. Isma'il al-Bukhari, one of the six prominent collectors of hadith of Sunni Islam. "See" "Sahih Bukhari"

</doc>
<doc id="51493" url="https://en.wikipedia.org/wiki?curid=51493" title="Santiago (disambiguation)">
Santiago (disambiguation)

Santiago (pop. 5 million) is the capital city of Chile, named in honour of Saint James. Four other notable cities are also often referred to simply as "Santiago":
Santiago may also refer to:
Other places.
In Argentina:
In Brazil:
In Cape Verde:
In Chile:
In Colombia:
In Costa Rica:
In Cuba:
In Dominican Republic:
In Ecuador:
In Guatemala:
In Jamaica:
In Mexico:
In Nicaragua:
In Panama:
In Paraguay:
In Peru:
In the Philippines:
In Portugal:
In Spain:
In Taiwan
In the United States:
In Uruguay:
In Venezuela:

</doc>
<doc id="51497" url="https://en.wikipedia.org/wiki?curid=51497" title="Battle of Verdun">
Battle of Verdun

The Battle of Verdun ( , ), fought from 21 February to 18 December 1916, was one of the largest battles of the First World War on the Western Front between the German and French armies. The battle took place on the hills north of Verdun-sur-Meuse in north-eastern France. The German 5th Army attacked the defences of the (RFV) and those of the Second Army garrisons on the right bank of the Meuse, intending to rapidly capture the (Meuse Heights), from which Verdun could be overlooked and bombarded with observed artillery fire. The German strategy aimed to provoke the French to attack to drive the Germans off the heights. The Germans captured ground early in the battle but the French quickly contained the German advance and were able to recapture much of the lost territory towards the end of the year, despite the demands of the Battle of the Somme in Picardy to the north-west.
The German strategy assumed that the French would attempt to hold on to the east bank of the Meuse and then commit the French strategic reserve to recapture it. The French would suffer catastrophic losses from German artillery fire and the German infantry held positions that were easy to defend and suffer fewer losses. The German plan was based on the experience of the Second Battle of Champagne ( from September and October 1915), when after early success, the French offensive was defeated with more French than German casualties. Poor weather delayed the beginning of the German Verdun offensive (/Operation Judgement) until 21 February. French construction of defensive lines and the arrival of reinforcements before the opening attack, delayed the German advance despite many French losses. By 6 March, French divisions were in the RFV and a more extensive defence in depth had been constructed. Pétain ordered that no withdrawals were to be made and that counter-attacks were to be conducted, despite exposing French infantry to fire from the German artillery. By 29 March, French artillery on the west bank had begun a constant bombardment of German positions on the east bank, which caused many German infantry casualties.
In March, the German offensive was extended to the left (west) bank of the Meuse, to gain observation of the ground from which French artillery had been firing over the river, into the flank of the German infantry on the east bank. The German troops were able to advance at first but French reinforcements contained the attacks short of their objectives. In early May, the Germans changed tactics and made local attacks and counter-attacks, which gave the French an opportunity to begin an attack against Fort Douaumont. Part of the fort was occupied, until a German counter-attack recaptured the fort and took numerous prisoners. The Germans changed tactics again, alternating their attacks on both banks of the Meuse and in June captured Fort Vaux. The Germans continued the offensive beyond Vaux, towards the last geographical objectives of the original plan, at Fleury-devant-Douaumont and Fort Souville. German attacks drove a salient into the French defences, captured Fleury and came within of the Verdun citadel.
In July 1916, the German offensive was reduced to provide artillery and infantry reinforcements for the Somme front and during local operations, the village of Fleury changed hands sixteen times from 23 June to 17 August and a German attempt to capture Fort Souville in early July, was repulsed by artillery and small-arms fire. To supply reinforcements for the Somme front, the German offensive was reduced further and attempts were made to deceive the French into expecting more attacks, to keep French reinforcements away from the Somme. In August and December, French counter-offensives recaptured much of the ground lost on the east bank and recovered Fort Douaumont and Fort Vaux. An estimate in 2000 found a total of , and an average of a month; other recent estimates increase the number of casualties to with at Verdun during the war. The Battle of Verdun lasted for and became the longest and one of the most costly battles in human history.
Background.
Strategic developments.
After the German invasion of France had been halted at the First Battle of the Marne in September 1914, the war of movement ended at the Battle of the Yser and the First Battle of Ypres. The Germans built field fortifications to hold the ground captured in 1914 and the French began siege warfare to break through the German defences and recover the lost territory. In late 1914 and in 1915, offensives on the Western Front had failed to gain much ground and been extremely costly in casualties. According to his memoirs written after the war, the Chief of the German General Staff, Erich von Falkenhayn, believed that although victory might no longer be achieved by a decisive battle, the French army could still be defeated if it suffered a sufficient number of casualties. Falkenhayn offered five corps from the strategic reserve for an offensive at Verdun at the beginning of February 1916 but only for an attack on the east bank of the Meuse. Falkenhayn considered it unlikely the French would be complacent about Verdun; he thought that they might send all their reserves there and begin a counter-offensive elsewhere or fight to hold Verdun while the British launched a relief offensive. After the war, the Kaiser and Colonel Tappen, the Operations Officer at "Oberste Heeresleitung" (OHL: General Headquarters), wrote that Falkenhayn believed the last possibility was most likely.
By seizing or threatening to capture Verdun, the Germans anticipated that the French would send all their reserves, which would have to attack secure German defensive positions, which were supported by a powerful artillery reserve and be destroyed. In the Gorlice–Tarnów Offensive (), the German and Austro-Hungarian Armies attacked Russian defences frontally, after pulverising them with large amounts of heavy artillery. During the Second Battle of Champagne ( "autumn battle") of , the French suffered "extraordinary casualties" from the German heavy artillery, which Falkenhayn considered offered a way out of the dilemma of material inferiority and the growing strength of the Allies. In the north, a British relief offensive would wear down British reserves, to no decisive effect but create the conditions for a German counter-offensive near Arras.
Hints about Falkenhayn's thinking were picked up by Dutch military intelligence and passed on to the British in December. The German strategy was to create a favourable operational situation without a mass attack, which had been costly and ineffective when it had been tried by the Franco-British, by relying on the power of heavy artillery to inflict mass losses. A limited offensive at Verdun, would lead to the destruction of the French strategic reserve in fruitless counter-attacks and the defeat of British reserves in a futile relief offensive, leading to the French accepting a separate peace. If the French refused to negotiate, the second phase of the strategy would begin in which the German armies would attack terminally weakened Franco-British armies, mop up the remains of the French armies and expel the British from Europe. To fulfil this strategy, Falkenhayn needed to hold back enough of the strategic reserve for the Anglo-French relief offensives and then conduct a counter-offensive, which limited the number of divisions which could be sent to the 5th Army at Verdun, for (Operation Judgement).
The Fortified Region of Verdun (RFV) lay in a salient formed during the German invasion of 1914. The Commander-in-Chief of the French Army, General Joseph Joffre, had concluded from the swift capture of the Belgian fortresses at the Battle of Liège and at the Siege of Namur in 1914 that fixed defences had been made obsolete by German siege guns. In a directive of the General Staff of 5 August 1915, the RFV was to be stripped of batteries and of ammunition. Plans to demolish forts Douaumont and Vaux to deny them to the Germans were made and of explosives had been laid by the time of the German offensive on 21 February. The forts and other batteries around Verdun were left with fewer than and a small reserve of ammunition while their garrisons had been reduced to small maintenance crews. The railway line from the south into Verdun had been cut during the Battle of Flirey in 1914, with the loss of Saint-Mihiel; the line west from Verdun to Paris was cut at Aubréville in mid-July 1915 by the German Third Army, which had attacked southwards through the Argonne Forest for most of the year.
Région Fortifiée de Verdun.
For centuries, Verdun had played an important role in the defence of the hinterland from the strategic location of the city on the Meuse river. Attila the Hun failed to seize the town in the fifth century; when the empire of Charlemagne was divided under the Treaty of Verdun of 843, the town became part of the Holy Roman Empire and the Peace of Westphalia in 1648, awarded Verdun to France. The heart of the city of Verdun was a citadel built by Vauban in the 17th century. A double ring of and smaller works () had been built around Verdun on commanding ground, at least above the river valley, from the citadel at Verdun. The programme had been devised by Séré de Rivières in the 1870s to build two lines of fortresses from Belfort to Épinal and from Verdun to Toul as defensive screens and to enclose towns intended to be the bases for counter-attacks. Many of the Verdun forts had been modernised and made more resistant to artillery, with a reconstruction programme begun at Douaumont in the 1880s. A sand cushion and thick, steel-reinforced concrete tops up to thick, buried under of earth, were added. The forts and had been sited to overlook each other for mutual support and the outer ring had a circumference of . The outer forts had in shell-proof turrets and more than and machine-guns to protect the ditches around the forts. Six forts had in retractable turrets and fourteen had retractable twin .
In 1903, Douaumont was equipped with a new concrete bunker (), containing two 75 mm field guns to cover the south-western approach and the defensive works along the ridge to . More guns were added from in four retractable steel turrets. The guns could rotate for all-round defence and two smaller versions, at the north-eastern and north-western corners of the fort, housed twin Hotchkiss machine-guns. On the east side of the fort, an armoured turret with a gun faced north and north-east and another housed twin at the north end, to cover the intervals between forts. The fort at Douaumont formed part of a complex of the village, fort, six , five shelters, six concrete batteries, an underground infantry shelter, two ammunition depots and several concrete infantry trenches. The Verdun forts had a network of concrete infantry shelters, armoured observation posts, batteries, concrete trenches, command posts and underground shelters between the forts. The artillery comprised , with reserve and the forts and were linked by telephone and telegraph, a narrow-gauge railway system and a road network; on mobilisation, the RFV had a garrison of and rations for six months.
Prelude.
German offensive preparations.
Verdun was isolated on three sides and railway communications to the French rear had been cut except for a light railway; German-controlled railways lay only to the north of the front line. A corps was moved to the 5th Army to provide labour for the preparation of the offensive. Areas were emptied of French civilians and buildings requisitioned, thousands of kilometres of telephone cable were laid, thousands of tons of ammunition and rations were stored under cover with hundreds of guns installed and camouflaged. Ten new rail lines with twenty stations were built and vast underground shelters () were dug deep, each to accommodate up to infantry. The III Corps, VII Reserve Corps and XVIII Corps were transferred to the 5th Army, each corps being reinforced by troops and recruits. V Corps was placed behind the front line, ready to advance if necessary when the assault divisions were moving up and the XV Corps, with two divisions, was in the 5th Army reserve, ready to advance to mop up as soon as the French defence collapsed.
Special arrangements were made to maintain a high rate of artillery-fire during the offensive, munitions trains per day were to deliver ammunition sufficient for to be fired in the first six days and another in the next twelve. Five repair shops were built close to the front to reduce delays for maintenance; factories in Germany were made ready, rapidly to refurbish artillery needing more extensive repairs. A redeployment plan for the artillery was devised, for field guns and mobile heavy artillery to be moved forward, under the covering fire of mortars and the super-heavy artillery. A total of were massed on the Verdun front, two thirds of which were heavy and super-heavy artillery, which had been obtained by stripping the modern German artillery from the rest of the Western Front and substituting it with older types and captured Russian guns. The German artillery could fire into the Verdun salient from three directions, yet remain dispersed.
German plan of attack.
The 5th Army divided the attack front into areas, "A" occupied by the VII Reserve Corps, "B" by the XVIII Corps, "C" by the III Corps and "D" on the Woëvre plain by the XV Corps. The preliminary artillery bombardment was to begin in the morning of 12 February. At , the infantry in areas "A" to "C" would advance in open order, supported by grenade and flame-thrower detachments. Wherever possible, the French advanced trenches were to be occupied and the second position reconnoitred, for the artillery fire on the second day. Great emphasis was placed on limiting German infantry casualties, by sending them to follow up destructive bombardments by the artillery, which was to carry the burden of the offensive in a series of large "attacks with limited objectives", to maintain a relentless pressure on the French. The initial objectives were the Meuse Heights, on a line from Froide Terre to Fort Souville and Fort Tavannes, which would provide a secure defensive position from which to repel French counter-attacks. Relentless pressure was a term added by the 5th Army staff and created ambiguity about the purpose of the offensive. Falkenhayn wanted land to be captured, from which artillery could dominate the battlefield and the 5th Army wanted a quick capture of Verdun. The confusion caused by the ambiguity was left to the corps headquarters to sort out.
Control of the artillery was centralised by an "Order for the Activities of the Artillery and Mortars", which stipulated that the corps Generals of Foot Artillery were responsible for local target selection, while co-ordination of flanking fire by neighbouring corps and the fire of certain batteries, was determined by the 5th Army headquarters. French fortifications were to be engaged by the heaviest howitzers and enfilade fire. The heavy artillery was to maintain long-range bombardment of French supply routes and assembly areas; counter-battery fire was reserved for specialist batteries firing gas shells. Co-operation between the artillery and infantry was stressed, with accuracy of the artillery being given priority over rate of fire. The opening bombardment was to build up slowly and (a rate of fire so rapid that the sound of shell-explosions merged into a rumble) would not begin until the last hour. As the infantry advanced, the artillery would increase the range of the bombardment to destroy the French second position. Artillery observers were to advance with the infantry and communicate with the guns by field telephones, flares and coloured balloons. When the offensive began, the French were to be bombarded continuously, harassing fire being maintained at night.
French defensive preparations.
In 1915, and of ammunition in the forts of the RFV had been removed, leaving only the heavy guns in retractable turrets. The conversion of the RFV to a conventional linear defence, with trenches and barbed wire began but proceeded slowly, after resources were sent west from Verdun for the Second Battle of Champagne In October 1915, building began on trench lines known as the first, second and third positions and in January 1916, an inspection by General Noël de Castelnau, Chief of Staff at French General Headquarters (GQG), reported that the new defences were satisfactory, except for small deficiencies in three areas. The fortress garrisons had been reduced to small maintenance crews and some of the forts had been readied for demolition. The maintenance garrisons were responsible to the central military bureaucracy in Paris and when the XXX Corps commander, General Chrétien, attempted to inspect Fort Douaumont in January 1916, he was refused entry.
Douaumont was the largest fort in the RFV and by February 1916, the only artillery left in the fort were the and guns and light guns covering the ditch. The fort was used as a barracks by under the command of Warrant-Officer Chenot, the . One of the rotating turrets was partially manned and the other was left empty. The Hotchkiss machine-guns were stored in boxes and four in the casemates had already been removed. The drawbridge had been jammed in the down position by a German shell and had not been repaired. The (wall bunkers) with Hotchkiss revolver-cannons protecting the moats, were unmanned and over of explosive charges had been placed in the fort to demolish it.
In late January 1916, French intelligence had obtained an accurate assessment of German military capacity and intentions at Verdun but Joffre considered that an attack would be a diversion, because of the lack of an obvious strategic objective. By the time of the German offensive, Joffre expected a bigger attack elsewhere but ordered the VII Corps to Verdun on 23 January, to hold the north face of the west bank. XXX Corps held the salient east of the Meuse to the north and north-east and II Corps held the eastern face of the Meuse Heights; Herr had divisions in the front line, with divisions in close reserve. (GAC, General De Langle de Cary) had the I and XX corps with two divisions each in reserve, plus most of the 19th Division; Joffre had in the strategic reserve. French artillery reinforcements had brought the total at Verdun to guns and guns, against guns, two thirds of which were heavy and super heavy, including and some being . Eight specialist flame-thrower companies were also sent to the 5th Army.
Castelnau met De Langle de Cary on 25 February, who doubted the east bank could be held. Castelnau disagreed and ordered General Frédéric-Georges Herr the corps commander, to hold the right (east) bank of the Meuse at all costs. Herr sent a division from the west bank and ordered XXX Corps to hold a line from Bras to Douaumont, Vaux and Eix. Pétain took over command of the defence of the RFV at with Colonel Maurice de Barescut as chief of staff and Colonel Bernard Serrigny as head of operations, only to hear that Fort Douaumont had fallen. Pétain ordered for the remaining Verdun forts to be re-garrisoned. Four groups were established, under the command of generals Guillaumat, Balfourier and Duchêne on the right bank and Bazelaire on the left bank. A "line of resistance" was established on the east bank from Souville to Thiaumont, around Fort Douaumont to Fort Vaux, Moulainville and along the ridge of the Woëvre. On the west bank, the line ran from Cumières to Mort Homme, Côte 304 and Avocourt. A "line of panic" was planned in secret as a final line of defence north of Verdun, through forts Belleville, St. Michel and Moulainville. I Corps and XX Corps arrived from increasing the number of divisions in the RFV to . By 6 March, the arrival of the XIII, XXI, XIV and XXXIII corps had increased the total to divisions.
Battle.
First phase, 21 February – 1 March.
21–26 February.
By 22 February, German troops had advanced and captured , at the edge of the village of Flabas. Two French battalions led by Colonel Émile Driant had held the bois (wood) for two days but were forced back to Samogneux, Beaumont and Ornes. Driant was killed, fighting with the 56th and 59th and only the Chasseurs managed to escape. Poor communications meant that only then did the French High Command realise the seriousness of the attack. The Germans managed to take the village of Haumont but French forces repulsed a German attack on the village of . On 23 February, a French counter-attack at was repulsed. Fighting for continued until the Germans outflanked the French defenders from . The German attackers had many casualties during their attack on and the French held on Samogneux. German attacks continued on 24 February and the French XXX Corps was forced out of the second line of defence; XX Corps under General Balfourier arrived at the last minute and was rushed forward. That evening Castelnau advised Joffre that the French Second Army, under General Pétain, should be sent to the RFV. The Germans had captured Beaumont, and and were moving up which led to Fort Douaumont.
At on 25 February, infantry of Brandenburg Regiment 24 advanced with the II and III battalions side-by-side, each formed into two waves composed of two companies each. A delay in the arrival of orders to the regiments on the flanks, led to the III Battalion advancing without support on that flank. The Germans rushed French positions in the woods and on Côte 347, with the support of machine-gun fire from the edge of and took many prisoners, as the French on Côte 347 were outflanked on the right and withdrew to Douaumont village. The German infantry had reached their objectives in fewer than twenty minutes and pursued the French, until fired on by a machine-gun in Douaumont church. Some German troops took cover in woods and a ravine which led to the fort, when German artillery began to bombard the area, the gunners having refused to believe claims sent by field telephone, that the German infantry were within a few hundred metres of the fort. Several German parties were forced to advance to find cover from the German shelling and two parties independently made for the fort. They did not know that the French garrison was made up of only a small maintenance crew led by a warrant officer, since most of the Verdun forts had been partly disarmed, after the demolition of Belgian forts in 1914, by the German super-heavy Krupp 420 mm mortars.
The German party of tried to signal to the artillery with flares but twilight and falling snow obscured them from view. Some of the party began to cut through the wire around the fort, while French machine-gun fire from Douaumont village ceased. The French had seen the German flares and took the Germans on the fort to be Zouaves retreating from Côte 378. The Germans were able to reach the north-east end of the fort, before the French resumed firing. The German party found a way through the railings on top of the ditch and climbed down without being fired on, since the machine-gun bunkers at each corner of the ditch, had been left unmanned. The German parties continued and found a way inside the fort, through one of the unoccupied ditch bunkers and then reached the central . After quietly moving inside, the Germans heard voices and persuaded a French prisoner captured in an observation post, to lead them to the lower floor, where they found Warrant Officer Chenot and about troops, most of the skeleton garrison of the fort and took them prisoner. On 26 February, the Germans had advanced on a front; French losses were and German losses were A French counter-attack on Fort Douaumont failed and Pétain ordered that no more attempts were be made; existing lines were to be consolidated and other forts were to be occupied, rearmed and supplied to withstand a siege if surrounded.
27–29 February.
The German advance gained little ground on 27 February, after a thaw turned the ground into a swamp and the arrival of French reinforcements increased the effectiveness of the defence. Some German artillery became unserviceable and other batteries became stranded in the mud. German infantry began to suffer from exhaustion and unexpectedly high losses, being suffered in the fighting around Douaumont village. On 29 February, the German advance was contained at Douaumont by a heavy snowfall and the defence of French 33rd Infantry Regiment. Delays gave the French time to bring up and of ammunition from the railhead at Bar-le-Duc to Verdun. The swift German advance had gone beyond the range of artillery covering fire and the muddy conditions made it very difficult to move the artillery forward as planned. The German advance southwards, brought it into range of French artillery west of the Meuse, whose fire caused more German infantry casualties than in the earlier fighting, when French infantry on the east bank had fewer guns in support.
Second phase, 6 March – 15 April.
6–11 March.
Before the offensive, Falkenhayn had expected that French artillery on the west bank would be suppressed by counter-battery fire but this had failed. The Germans set up an artillery task-force, to counter French artillery-fire from the west bank but this also failed to reduce German infantry casualties. The 5th Army asked for more troops in late February but Falkenhayn refused, due to the rapid advance already achieved on the east bank and because he needed the rest of the OHL reserve for an offensive elsewhere, once the attack at Verdun had attracted and consumed French reserves. The pause in the German advance on 27 February led Falkenhayn to have second thoughts to decide between terminating the offensive or reinforcing it. On 29 February, Knobelsdorf, the 5th Army Chief of Staff, prised two divisions from the OHL reserve, with the assurance that once the heights on the west bank had been occupied, the offensive on the east bank could be completed. The VI Reserve Corps was reinforced with the X Reserve Corps, to capture a line from the south of Avocourt to Côte 304 north of Esnes, Mort-Homme, Bois des Cumières and Côte 205, from which the French artillery on the west bank could be destroyed.
The artillery of the two-corps assault group on the west bank was reinforced by artillery batteries, artillery command was centralised under one officer and arrangements were made for the artillery on the east bank to fire in support. The attack was planned by General Heinrich von Gossler in two parts, on Mort-Homme and Côte 265 on 6 March, followed by attacks on Avocourt and Côte 304 on 9 March. The German bombardment reduced the top of Côte 304 from a height of to ; Mort-Homme sheltered batteries of French field guns, which hindered German progress towards Verdun on the right bank; the hills also provided commanding views of the left bank. After storming the and then losing it to a French counter-attack, the Germans launched another assault on Mort-Homme on 9 March, from the direction of Béthincourt to the north-west. was captured again at great cost in casualties, before the Germans took parts of Mort-Homme, Côte 304, Cumières and Chattancourt on 14 March.
11 March – 9 April.
After a week, the German attack had reached the first-day objectives and then found that French guns behind Côte de Marre and Bois Borrous were still operational and continued to inflict many casualties on the east bank. German artillery moved to Côte 265, was subjected to systematic artillery-fire by the French, which left the Germans needing to implement the second part of the west bank offensive, to protect the gains of the first phase. German attacks changed from large operations on broad fronts, to narrow-front attacks with limited objectives. On 14 March a German attack captured Côte 265 at west end of Mort-Homme but the French 75th Infantry Brigade managed to hold Côte 295 at the east end. On 20 March, after a bombardment by mortar rounds, the 11th Bavarian and 11th Reserve divisions attacked and and reached their initial objectives easily. Gossler then paused the attack, to consolidate the captured ground and to prepare another big bombardment for the next day. On 22 March, two divisions attacked "Termite Hill" near Côte 304 but were met by a mass of artillery-fire, which also fell on assembly points and the German lines of communication, ending the German advance.
The limited German success had been costly and French artillery inflicted more casualties as the German infantry tried to dig in. By 30 March, Gossler had captured but had lost and the Germans were still short of Côte 304. On 30 March, the XXII Reserve Corps arrived as reinforcements and General Max von Gallwitz took command of a new . Malancourt village was captured on 31 March, Haucourt fell on 5 April and Béthincourt on 8 April. On the east bank, German attacks near Vaux reached and the Vaux–Fleury railway but were then driven back by the French 5th Division. An attack was made on a wider front along both banks by the Germans at noon on 9 April, with five divisions on the left bank but this was repulsed except at Mort-Homme, where the French 42nd Division was forced back from the north-east face. On the right bank an attack on failed.
In March the German attacks had no advantage of surprise and faced a determined and well-supplied adversary in superior defensive positions. German artillery could still devastate French defensive positions but could not prevent French artillery-fire from inflicting many casualties on German infantry and isolating them from their supplies. Massed artillery fire could enable German infantry to make small advances but massed French artillery-fire could do the same for French infantry when they counter-attacked, which often repulsed the German infantry and subjected them to constant losses, even when captured ground was held. The German effort on the west bank also showed that capturing a vital point was not sufficient, because it would be found to be overlooked by another terrain feature, which had to be captured to ensure the defence of the original point, which made it impossible for the Germans to terminate their attacks, unless they were willing to retire to the original front line of February 1916.
By the end of March the offensive had cost the Germans and Falkenhayn began to think of ending the offensive, lest it become another costly and indecisive engagement similar to the First Battle of Ypres in late 1914. The 5th Army staff requested more reinforcements from Falkenhayn on 31 March with an optimistic report claiming that the French were close to exhaustion and incapable of a big offensive. The 5th Army command wanted to continue the east bank offensive until a line from Ouvrage de Thiaumont, to Fleury, Fort Souville and Fort de Tavannes had been reached, while on the west bank the French would be destroyed by their own counter-attacks. On 4 April, Falkenhayn replied that the French had retained a considerable reserve and that German resources were limited and not sufficient to replace continuously men and munitions. If the resumed offensive on the east bank failed to reach the Meuse Heights, Falkenhayn was willing to accept that the offensive had failed and end it.
Third phase, 16 April – 1 July.
April.
The failure of German attacks in early April by , led Knobelsdorf to obtain reports from the 5th Army corps commanders, who unanimously wanted to continue. The German infantry were exposed to continuous artillery-fire from the flanks and behind, communications from the rear and reserve positions were equally vulnerable, which caused a constant drain of casualties. Defensive positions were difficult to build, because existing positions were on ground which had been swept clear by German bombardments early in the offensive, leaving German infantry with very little cover. The XV Corps commander, General Berthold von Deimling also wrote that French heavy artillery and gas bombardments. were undermining the morale of the German infantry, it was necessary to keep going to reach safer defensive positions. Knobelsdorf reported these findings to Falkenhayn on 20 April, adding that if the Germans did not go forward, they must go back to the start line of 21 February.
Knobelsdorf rejected the policy of limited piecemeal attacks tried by Mudra, while in command of and advocated a return to wide-front attacks with unlimited objectives, swiftly to reach the line from Ouvrage de Thiaumont to Fleury, Fort Souville and Fort de Tavannes. Falkenhayn was persuaded to agree to the change and by the end of April, most of the OHL reserve, had been sent to Verdun and troops had also been transferred from the Eastern Front. The resort to large, unlimited attacks was costly for both sides but the German advance proceeded only slowly. Rather than causing devastating French casualties by heavy artillery, with the infantry in secure defensive positions, which the French were compelled to attack, the Germans inflicted casualties by attacks which provoked French counter-attacks and assumed that the process inflicted five French casualties for two German losses.
In mid-March, Falkenhayn had reminded the 5th Army to use tactics intended to conserve infantry, after the corps commanders had been allowed discretion to choose between the cautious step-by-step tactics desired by Falkenhayn and maximum efforts, intended to obtain quick results. On the third day of the offensive, the 6th Division of the III Corps (General Ewald von Lochow), had ordered that Herbebois be taken "regardless of loss" and the 5th Division had attacked Wavrille to the accompaniment of its band. Falkenhayn urged the 5th Army to use (storm units) composed of two infantry squads and one of engineers, armed with automatic weapons, hand grenades, trench mortars and flame-throwers, to advance in front of the main infantry body. The would conceal their advance by shrewd use of terrain and capture any strong-points which remained after the artillery preparation. Strong-points which could not be taken, were to be by-passed and captured by follow-up troops. Falkenhayn ordered that the command of field and heavy artillery units was to be combined, with a commander at each corps headquarters. Common observers and communication systems would ensure that batteries in different places, could bring targets under converging fire, which would be allotted systematically to support divisions.
In mid-April, Falkenhayn ordered that infantry should advance close to the barrage, to exploit the neutralising effect of the shell-fire on surviving defenders, because fresh troops at Verdun had not been trained in these methods. Knobelsdorf persisted with attempts to maintain momentum, which was incompatible with the methods of casualty conservation, which could be implemented only with limited attacks, with pauses to consolidate and prepare. Mudra and other commanders who disagreed were sacked. Falkenhayn also intervened to change German defensive tactics, advocating a dispersed defence with the second line to be held as a main line of resistance and jumping-off point for counter-attacks. Machine-guns were to be set up with overlapping fields of fire and infantry given specific areas to defend. When French infantry attacked, they were to be isolated by (barrage-fire) on their former front line, to increase French infantry casualties. The changes desired by Falkenhayn had little effect, because the main cause of German casualties was artillery-fire, just as it was for the French.
4–24 May.
From 10 May German operations were limited to local attacks, either in reply to French counter-attacks on 11 April between Douaumont and Vaux and on 17 April between the Meuse and Douaumont, or local attempts to take points of tactical value. At the beginning of May, General Pétain was promoted to the command of (GAC) and General Robert Nivelle took over the Second Army at Verdun. From German attacks were made on the west bank around Mort-Homme and on 4 May, the north slope of Côte 304 was captured; French counter-attacks from were repulsed. The French defenders on the crest of Côte 304 were forced back on 7 May but German infantry were unable to occupy the ridge, because of the intensity of French artillery-fire. Cumieres and Caurettes fell on 24 May as a French counter-attack began at Fort Douaumont.
22–24 May.
In May General Nivelle who had taken over the Second Army, ordered General Charles Mangin, commander of the 5th Division to plan a counter-attack on Fort Douaumont. The initial plan was for an attack on a front but several minor German attacks captured and ravines on the south-eastern and western sides of the fort. A further attack took the ridge south of the , which gave the Germans better routes for counter-attacks and observation over the French lines to the south and south-west. Mangin proposed a preliminary attack to retake the area of the ravines, to obstruct the routes by which a German counter-attack on the fort could be made. More divisions were necessary but these were refused, to preserve the troops needed for the forthcoming offensive on the Somme; Mangin was limited to one division for the attack with one in reserve. Nivelle reduced the attack to an assault on Morchée Trench, Bonnet-d'Evèque, Fontaine Trench, Fort Douaumont, a machine-gun turret and Hongrois Trench, which would require an advance of on a front.
III Corps was to command the attack by the 5th Division and the 71st Brigade, with support from three balloon companies for artillery-observation and a fighter group. The main effort was to be conducted by two battalions of the 129th Infantry Regiment, each with a pioneer company and a machine-gun company attached. The 2nd Battalion was to attack from the south and the 1st Battalion was to move along the west side of the fort to the north end, taking Fontaine Trench and linking with the 6th Company. Two battalions of the 74th Infantry Regiment were to advance along the east and south-east sides of the fort and take a machine-gun turret on a ridge to the east. Flank support was arranged with neighbouring regiments and diversions were planned near Fort Vaux and the . Preparations for the attack included the digging of of trenches and the building of large numbers of depots and stores but little progress was made due to a shortage of pioneers. French troops captured on 13 May, disclosed the plan to the Germans, who responded by subjecting the area to more artillery harassing fire, which also slowed French preparations.
The French preliminary bombardment by four and guns, began on 17 May and by 21 May, the French artillery commander claimed that the fort had been severely damaged. During the bombardment the German garrison in the fort experienced great strain, as French heavy shells smashed holes in the walls and concrete dust, exhaust fumes from an electricity generator and gas from disinterred corpses polluted the air. Water ran short but until 20 May, the fort remained operational, reports being passed back and reinforcements moving forward until the afternoon, when the Bourges Casemate was isolated and the wireless station in the north-western machine-gun turret burnt down. Conditions for the German infantry in the vicinity of the fort were far worse and by 18 May, the French destructive bombardment had obliterated many defensive positions, the survivors taking post in shell-holes and dips on the ground. Communication with the rear was severed and food and water ran out by the time of the French attack on 22 May. The troops of Infantry Regiment 52 in front of Fort Douaumont had been reduced to near Thiaumont Farm and German counter-barrages inflicted similar losses on French troops. French aircraft attacked eight observation balloons and the 5th Army headquarters at Stenay on 22 May. Six balloons were shot down but the German artillery fire increased and twenty minutes before zero hour, a German bombardment began, which reduced the 129th Infantry Regiment companies to about each.
The assault began at on 22 May on a front. On the left flank the 36th Infantry Regiment attack quickly captured Morchée Trench and Bonnet-d'Evèque but was costly and the regiment could advance no further. The flank guard on the right was pinned down, except for one company which disappeared and in , a battalion of the 74th Infantry Regiment was unable to leave its trenches; the other battalion managed to reach its objectives at an ammunition depot, shelter "DV1" at the edge of and the machine-gun turret east of the fort, where the battalion found its flanks unsupported. Despite German small-arms fire, the 129th Infantry Regiment reached the fort in a few minutes and managed to get in through the west and south sides. By nightfall, about half of the fort had been recaptured and next day, the 34th Division was sent to reinforce the fort. The reinforcements were repulsed and German reserves managed to cut off the French troops in the fort and force them to surrender, prisoners being taken. After three days, the French had lost from the in the attack and German casualties in Infantry Regiment 52, Grenadier Regiment 12 and Leib-Grenadier Regiment 8 were 
30 May – 7 June.
Later in May 1916, the German attacks shifted from the left bank at Mort-Homme and Côte 304 and returned to the right bank, south of Fort Douaumont. A German offensive began to reach Fleury Ridge, the last French defensive line and take , Fleury, Fort Souville and Fort Vaux at the north-east extremity of the French line, which had been bombarded by a day since the beginning of the offensive. After a final assault on 1 June, by troops, the top of the fort was occupied on 2 June . Fighting went on underground until the garrison ran out of water and surrendered on 7 June. In five days the German attack had advanced for a loss of against casualties. When news of the loss of Fort Vaux reached Verdun, the Line of Panic was occupied and trenches were dug on the edge of the city. On the left bank, the German advanced from the line Côte 304, Mort-Homme and Cumières and threatened Chattancourt and Avocourt. Heavy rains slowed the German advance towards Fort Souville, where both sides attacked and counter-attacked for the next two months.
22–25 June.
On 22 June, German artillery fired over (Green Cross) gas shells at French artillery positions, which caused over and silenced much of the French artillery. Next day the German attack on a front at drove a salient into the French defences unopposed until when some French troops were able to fight a rearguard action. The Ouvrage de Thiaumont and the Ouvrage de Froidterre at the south end of the plateau were captured and the village of Fleury and Chapelle Sainte-Fine were overrun. The attack came close to Fort Souville, which since April had been hit by and brought the Germans to within of the Verdun citadel. Chapelle Sainte-Fine was quickly recaptured by a French counter-attack and the German advance was halted. The supply of water to the German infantry broke down, the salient was vulnerable to fire from three sides and the attack could not go on without Diphosgene ammunition. Chapelle Sainte-Fine became the furthest point reached by the German Verdun offensive and on 24 June, the Anglo-French preliminary bombardment began on the Somme. Fleury changed hands sixteen times from Four French divisions were diverted to Verdun from the Somme and the French artillery recovered sufficiently on 24 June, to cut off the German front line from the rear. By 25 June both sides were exhausted and Knobelsdorf suspended the attack.
Fourth phase 1 July – 17 December.
By the end of May French casualties at Verdun had risen to and in June German losses had reached The opening of the Battle of the Somme on 1 July, forced the Germans to withdraw some of their artillery from Verdun, which was the first strategic success of the Anglo-French offensive.
9–15 July.
Fort Souville dominated a crest south-east of Fleury and was one of the original objectives of the February offensive. The capture of the fort would give the Germans control of the heights overlooking Verdun and allow the infantry to dig in on commanding ground. A German preparatory bombardment began on 9 July, with an attempt to suppress French artillery with over shells, which had little effect since the French had been equipped with an improved M2 gas mask. Fort Souville and its approaches were bombarded with more than including shells on the fort. An attack by three German divisions began on 11 July but German infantry bunched on the path leading to Fort Souville and came under bombardment from French artillery. The surviving troops were fired on by sixty French machine-gunners, who emerged from the fort and took positions on the superstructure. Thirty soldiers of Infantry Regiment 140 managed to reach the top of the fort on 12 July, from where the Germans could see the roofs of Verdun and the spire of the cathedral. After a small French counter-attack, the survivors retreated to their start lines or surrendered. On the evening of 11 July, Crown Prince Wilhelm was ordered by Falkenhayn to go onto the defensive and on 15 July, the French conducted a larger counter-attack which gained no ground; for the rest of the month the French made only small attacks.
1 August – 17 September.
On 1 August a German surprise-attack advanced towards Fort Souville, which prompted French counter-attacks for two weeks, which were only able to retake a small amount of the captured ground. On 18 August, Fleury was recaptured and by September, French counter-attacks had recovered much of the ground lost in July and August. On 29 August Falkenhayn was replaced as Chief of the General Staff by Paul von Hindenburg and First Quartermaster-General Erich Ludendorff. On 3 September, an attack on both flanks at Fleury advanced the French line several hundred metres, against which German counter-attacks from failed. The French attacked again on Losses were light except at the Tavannes railway tunnel, where troops died in a fire which began on 4 September.
20 October – 2 November.
In October 1916 the French began the (First Offensive Battle of Verdun), to recapture Fort Douaumont, an advance of more than . Seven of the at Verdun were replaced by mid-October and French infantry platoons were reorganised to contain sections of riflemen, grenadiers and machine-gunners. In a six-day preliminary bombardment, the French artillery fired including field-gun shells, medium shells and super-heavy shells, from more than and howitzers. Two French Saint-Chamond railway guns, to the south-west at Baleycourt, fired the super-heavy shells, each weighing . At least the super-heavy shells hit Fort Douaumont, the sixth penetrating to the lowest level and exploding in a pioneer depot, starting a fire next to 
The 38th, 133rd and 74th divisions attacked at behind a creeping field-artillery barrage, moving at a rate of in two minutes, beyond which a heavy artillery barrage moved in lifts, as the field artillery barrage came within , to force the German infantry and machine-gunners to stay under cover. The Germans had partly evacuated Douaumont, which was recaptured on 24 October by French marines and colonial infantry; more than and fifteen guns were captured by 25 October but an attempt on Fort Vaux failed. The Haudromont quarries, Ouvrage de Thiaumont and Thiaumont Farm, Douaumont village, the northern end of Caillette Wood, Vaux pond, the eastern fringe of Bois Fumin and the Damloup battery were captured. The heaviest French artillery bombarded Fort Vaux for the next week and on 2 November, the Germans evacuated the fort, after a huge explosion caused by a French eavesdroppers overheard a German wireless message announcing the departure and a French infantry company entered the fort without firing a shot; on 5 November, the French reached the front line of 24 February and offensive operations ceased until December.
15–17 December 1916.
The "2ième Bataille Offensive de Verdun" (Second Offensive Battle of Verdun) by four divisions, with four more in reserve, was planned by Nivelle and executed by Mangin. The attack began at on 15 December, after a six-day bombardment by fired from The final French bombardment was directed from artillery-observation aircraft and fell on trenches, dug-out entrances and observation posts. Five German divisions supported by held the defensive position, which was deep, with of the infantry in the battle zone and the remaining in reserve back; two of the German divisions were understrength with only instead of their normal establishment of The infantry attack was preceded by a double creeping barrage, shrapnel-fire from field artillery in front of the infantry and a high-explosive barrage ahead, which moved towards a shrapnel bombardment along the German second line, laid to cut off the German retreat and block the advance of reinforcements. The German defence collapsed and of the the five front divisions were lost, most having been trapped under cover and taken prisoner when the French infantry arrived.
The French reached their objectives at Vacherauville and Louvemont which had been lost in February, along with Hardaumont and Côte du Poivre, despite attacking in very bad weather. German reserve battalions did not reach the front until the evening and two Eingreif divisions, which had been ordered forward the previous evening, were still away at noon. By the night of the French had consolidated a new line from Bezonvaux to Côte du Poivre, beyond Douaumont and north of Fort Vaux, before the German reserves and units could counter-attack. The at Douaumont had been repaired and fired in support of the French attack. The closest German point to Verdun had been pushed back and all the dominating observation points had been recaptured. The French took and Some German officers complained to Mangin about their lack of comfort in captivity and he replied, "We do regret it, gentlemen but then we did not expect so many of you". Lochow, the 5th Army commander and General von Zwehl, commander of XIV Reserve Corps, were sacked on 16 December.
Subsequent operations.
20–26 August 1917.
On 20 August 1917, the (Second Offensive Battle of Verdun) was carried out by the XIII, XVI, XV and XXXII corps, to capture Côte 304 and Mort Homme on the west bank and Côte Talou and Beaumont on the east bank. The plan required an advance of on a front. On 11 August, an artillery preparation by on a area began and by 20 August, the French artillery had fired including shells, along with a machine-gun bombardment fired on tracks, crossroads, supply lines and German artillery batteries. In four days, French troops captured Bois d'Avocourt, Mort-Homme, Bois Corbeaux and the Bismarck, Kronprinz and Gallwitz tunnels, which had connected the German front lines to the rear, underneath Mort-Homme and Côte 304. On the right bank, Bois Talou, Champneuville, Côte 344, part of Bois Fosse, Bois Chaume and Mormont Farm were captured. Next day Côte 304, Samogneux and Régnieville fell and on 26 August, the French reached the southern outskirts of Beaumont. By 26 August, the French had captured thirty guns, mortars and 
7 September 1917.
After the success of the attack in August, Guillaumat was ordered to plan an operation to capture several trenches and a more ambitious offensive to take the last ground from which German artillery-observers could see Verdun. Pétain questioned Guillaumat and Fayolle, who argued that the French could not remain in their present positions and must either advance or retire, advocating a limited advance to make German counter-attacks harder, improve conditions in the front line and deceive the Germans about French intentions. The two Corps on the east bank made small attacks, XV Corps on 7 September which failed and XXXII Corps the next day which was a costly success. The attack continued and the trenches necessary for a secure defensive position were taken but not the last German observation point. Further attempts to advance were met by massed artillery-fire and counter-attacks; the French commanders ended the operation.
Meuse–Argonne Offensive.
The French Fourth Army and the American First Army attacked on a front from Moronvilliers to the Meuse on 26 September 1918 at after a three-hour bombardment. American troops quickly captured Malancourt, Bethincourt and Forges on the left bank of the Meuse and by midday the Americans had reached Gercourt, Cuisy, the southern part of Montfaucon and Cheppy. German troops were able to repulse American attacks on Montfaucon ridge, until it was outflanked to the south and Montfaucon was surrounded. German counter-attacks from slowed the American advance but Ivoiry and Epinon-Tille were captured, after which Montfaucon ridge was taken along with and On the right bank of the Meuse, a combined Franco-American force under American command, took Brabant, Haumont, Bois d'Haumont and Bois des Caures and then crossed the front line of February 1916. By November, and several thousand machine-guns had been captured. A German retreat began and continued until the Armistice.
Aftermath.
Analysis.
Falkenhayn wrote in his memoir that he sent an appreciation of the strategic situation to the Kaiser in December 1915,
The German strategy in 1916 was to inflict mass casualties on the French, a goal achieved against the Russians from 1914 to 1915, to weaken the French Army to the point of collapse. The French Army had to be drawn into circumstances from which it could not escape, for reasons of strategy and prestige. The Germans planned to use a large number of heavy and super-heavy guns to inflict a greater number of casualties than French artillery, which relied mostly upon the gun. In 2007, Foley wrote that Falkenhayn intended an attrition battle from the beginning, contrary to the views of Krumeich, Förster and others but the lack of surviving documents had led to many interpretations of Falkenhayn's strategy. At the time, critics of Falkenhayn claimed that the battle demonstrated that he was indecisive and unfit for command; in 1937, Förster had proposed the view "forcefully". In 1994, Afflerbach questioned the authenticity of the "Christmas Memorandum" in his biography of Falkenhayn; after studying the evidence that had survived in the (Army Military History Research Institute) files, he concluded that the memorandum had been written after the war but that it was an accurate reflection of much of Falkenhayn's thinking in 1916.
Krumeich wrote that the Christmas Memorandum had been fabricated to justify a failed strategy and that attrition had been substituted for the capture of Verdun, only after the city was not taken quickly. Foley wrote that after the failure of the Ypres Offensive of 1914, Falkenhayn had returned to the pre-war strategic thinking of Moltke the Elder and Hans Delbrück on (attrition strategy), because the coalition fighting Germany was too powerful to be decisively defeated by military means. German strategy should aim to divide the Allies, by forcing at least one of the Entente powers into a negotiated peace. An attempt at attrition lay behind the offensive against Russia in 1915 but the Russians had refused to accept German peace feelers, despite the huge defeats inflicted by the Austro-Germans that summer.
With insufficient forces to break through the Western Front and to overcome the Entente reserves behind it, Falkenhayn attempted to force the French to attack instead, by threatening a sensitive point close to the front line. Falkenhayn chose Verdun as the place to force the French to begin a counter-offensive, which would be defeated with huge losses to the French, inflicted by German artillery on the dominating heights around the city. The 5th Army would begin a big offensive with limited objectives, to seize the Meuse Heights on the right bank of the river, from which German artillery could dominate the battlefield. By being forced into a counter-offensive against such formidable positions, the French Army would "bleed itself white". As the French were weakened, the British would be forced to launch a hasty relief offensive, which would also be a costly defeat. If such defeats were not enough to force negotiations on the French, a German offensive would mop up the last of the Franco-British armies and break the Entente "once and for all".
In a revised instruction to the French army of January 1916, the General Staff had stated that equipment could not be fought by men. Firepower could conserve infantry but a battle of material prolonged the war and consumed the troops which were preserved in each battle. In 1915 and early 1916, German industry quintupled the output of heavy artillery and doubled the production of super-heavy artillery. French production had also recovered since 1914 and by February 1916, the army had guns. In May 1916, Joffre implemented a plan to issue each division with two groups of 155  mm guns and each corps with four groups of long-range guns. Both sides at Verdun had the means to fire huge numbers of heavy shells to suppress defences, before risking infantry movements. At the end of May, the Germans had guns at Verdun against , which were sufficient to contain the Germans but not enough for a counter-offensive.
German infantry found that it was easier for the French to endure preparatory bombardments, since French positions tended to be on dominating ground, not always visible and sparsely occupied. As soon as German infantry attacked, the French positions "came to life" and the troops began machine-gun and rapid fire with field artillery. On 22 April, the Germans had suffered and in mid-April, the French fired artillery shells during an attack to the south-east of Fort Douaumont. A few days after taking over at Verdun, Pétain told the air commander, Commandant Charles Tricornot de Rose, to sweep away the German air service and to provide observation for the French artillery. German air superiority was challenged and eventually reversed, using eight-aircraft for artillery-observation, counter-battery and tactical support.
The fighting at Verdun was less costly to both sides than the war of movement in 1914, which cost the French the Germans from August to December. The 5th Army had a lower rate of loss than armies on the Eastern Front in 1915 and the French had a lower average rate of loss at Verdun than the rate over three weeks during the Second Battle of Champagne (September–October 1915), which were not fought as battles of attrition. German loss rates increased relative to French rates, from early 1915 to close to the end of the Battle of Verdun and rough parity continued during the Nivelle Offensive in 1917. The main cost of attrition tactics was indecision, because limited-objective attacks under an umbrella of massed heavy artillery-fire, could succeed but created unlimited duration.
Pétain used a "Noria" (rotation) system, to relieve French troops at Verdun after a short period, which brought most troops of the French army to the Verdun front but for shorter periods than for the German troops. French will to resist did not collapse, the symbolic importance of Verdun proved a rallying point and Falkenhayn was forced to conduct the offensive for much longer and commit far more infantry than intended. By the end of April, most of the German strategic reserve was at Verdun, suffering similar casualties to the French army. The Germans believed that they were inflicting losses at a rate of military intelligence thought that French casualties up to 11 March, had been and Falkenhayn was confident that German artillery could easily inflict another In May, Falkenhayn estimated that the French had lost against casualties and that the French strategic reserve had been reduced to Actual French losses were 1 May and the Noria system had enabled to be withdrawn and rested, when their casualties reached Of the battalions of the French metropolitan army, went to Verdun, against divisions, of the (western army). Afflerbach wrote that divisions fought at Verdun and that from February to August, the ratio of German to French losses was not the third of French losses assumed by Falkenhayn. By 31 August, 5th Army losses were French casualties numbered .
In June 1916, the amount of French artillery at Verdun had been increased to including field guns; the French and German armies fired , with a weight of from The German offensive had been contained by French reinforcements, difficulties of terrain and the weather by May, with the 5th Army infantry stuck in tactically dangerous positions, overlooked by the French on the east bank and the west bank, instead of secure on the Meuse Heights. Attrition of the French forces was inflicted by constant infantry attacks, which were vastly more costly than waiting for French counter-attacks and defeating them with artillery. The stalemate was broken by the Brusilov Offensive and the Anglo-French relief offensive on the Somme, which had been expected to lead to the collapse of the Anglo-French armies. Falkenhayn had begun to remove divisions from the armies on the Western Front in June, to rebuild the strategic reserve but only twelve divisions could be spared. Four divisions were sent to the 2nd Army on the Somme, which had dug a layered defensive system based on the experience of the . The situation before the beginning of the battle on the Somme was considered by Falkenhayn to be better than before previous offensives and a relatively easy defeat of the British offensive was anticipated. No divisions were moved from the 6th Army, which had divisions and a large amount of heavy artillery, ready for a counter-offensive when the British offensive had been defeated.
The strength of the Anglo-French offensive surprised Falkenhayn and the staff officers of OHL despite the losses inflicted on the British; the loss of artillery to "overwhelming" counter-battery fire and the policy of instant counter-attack against any Anglo-French advance, led to far more German infantry casualties than at the height of the fighting at Verdun, where had been suffered in the first ten days, against on the Somme. The Brusilov Offensive had recommenced as soon as Russian supplies had been replenished, which inflicted more losses on Austro-Hungarian and German troops during June and July, when the offensive was extended to the north. Falkenhayn was called on to justify his strategy to the Kaiser on 8 July and again advocated sending minimal reinforcements to the east and to continue the "decisive" battle in France, where the Somme offensive was the "last throw of the dice" for the Entente. Falkenhayn had already given up the plan for a counter-offensive near Arras, to reinforce the Russian front and the 2nd Army, with eighteen divisions moved from the reserve and the 6th Army front. By the end of August only one division remained in reserve. The 5th Army had been ordered to limit its attacks at Verdun in June but a final effort was made in July to capture Fort Souville. The effort failed and on 12 July, Falkenhayn ordered a strict defensive policy, permitting only small local attacks, to try to limit the number of troops the French took from the RFV to add to the Somme offensive.
Falkenhayn had underestimated the French, for whom victory at all costs was the only way to justify the sacrifices already made; the pressure imposed on the French army never came close to making the French collapse and triggering a premature British relief offensive. The ability of the German army to inflict disproportionate losses had also been exaggerated, in part because the 5th Army commanders had tried to capture Verdun and attacked regardless of loss; even when reconciled to Falkenhayn's attrition strategy, they continued to use the costly (strategy of annihilation) and tactics of (manoeuvre warfare). Failure to reach the Meuse Heights, forced the 5th Army to try to advance from poor tactical positions and to impose attrition by infantry attacks and counter-attacks. The unanticipated duration of the offensive made Verdun a matter of German prestige as much as it was for the French and Falkenhayn became dependent on a British relief offensive and a German counter-offensive to end the stalemate. When it came, the collapse of the southern front in Russia and the power of the Anglo-French attack on the Somme reduced the German armies to holding their positions as best they could. On 29 August, Falkenhayn was sacked and replaced by Hindenburg and Ludendorff, who ended the German offensive at Verdun on 2 September.
Casualties.
In 1980, Terraine gave casualties in of battle; Dupuy and Dupuy gave casualties in 1993. Heer and Naumann calculated and casualties, a monthly average of in 2000. Mason wrote in 2000 that there had been and casualties. In 2003, Clayton quoted casualties, of whom killed or missing and losses, or prisoners and Writing in 2005, Doughty gave French casualties at Verdun, from 21 February to 20 December 1916 as of at Verdun and the Somme; of Verdun casualties were known to have been killed, wounded and missing, many of whom were eventually presumed dead. Doughty wrote that other historians had followed Churchill (1927) who gave a figure of by mistakenly including all French losses on the Western Front. (In 2014, Philpott recorded casualties, of whom had been killed, German casualties were and a recent estimate of casualties at Verdun from 1914 to 1918 was ).
In the second edition of "The World Crisis" (1938), Churchill wrote that the figure of for other ranks and the figure of "probably" included officers. Churchill gave a figure of of whom killed and expressed dismay that French casualties had exceeded German by . Churchill also stated that an eighth needed to be deducted from his figures for both sides to account for casualties on other sectors, giving and casualties. Grant gave a figure of casualties in 2005. In 2005, Foley used calculations made by Wendt in 1931 to give German casualties at Verdun from 21 February to 31 August 1916 as casualties. Afflerbach used the same source in 2000 to give and casualties at Verdun, from February to December 1916.
In 2013, Jankowski wrote that since the beginning of the war, French army units had produced every five days for the Bureau of Personnel at GQG. The health service at the Ministry of War received daily counts of wounded taken in by hospitals and other services but casualty data was dispersed among regimental depots, GQG, the , which recorded deaths, the , which counted injuries and illnesses and the , which communicated with next of kin. Regimental depots were ordered to keep to record losses continuously and the of GQG began to compare the five-day field reports with the records of hospital admissions. The new system was used to calculate losses since August 1914, which took several months but the system had become established by February 1916. The were used to calculate casualty figures published in the , the French Official History and other publications.
The German armies compiled every ten days, which were published by the in the of 1924–1925. German medical units kept detailed records of medical treatment at the front and in hospital and in 1923, the published an amended edition of the lists produced during the war, incorporating medical service data not in the . Monthly figures of wounded and ill servicemen that were treated were published in 1934 in the . Using such sources for comparisons of losses during a battle is difficult, because the information recorded losses over time, rather than place. Losses calculated for particular battles could be inconsistent, as in the "Statistics of the Military Effort of the British Empire during the Great War 1914–1920" (1922). In the early 1920s, Louis Marin reported to the Chamber of Deputies but could not give figures per battle, except for some by using numerical reports from the armies, which were unreliable unless reconciled with the system established in 1916.
Some French data excluded those lightly wounded but some did not. In April 1917, GQG required that the discriminate between the lightly wounded, treated at the front over a period of and severely wounded evacuated to hospitals. Uncertainty over the criteria had not been resolved before the war ended, excluded lightly wounded and the records included them. Churchill revised German statistics, by adding for unrecorded wounded in "The World Crisis", written in the 1920s and the British official historian For the Battle of Verdun, the contained incomplete data for the Verdun area, did not define "wounded" and the 5th Army field reports exclude them. The Marin Report and covered different periods but included lightly wounded. Churchill used a figure of and took a figure of from the Marin Report, for March to June and November to December 1916, for all the Western Front.
The give French losses in a range from and in 1930, Wendt recorded French Second Army and German 5th Army casualties of respectively, from , not taking account of the inclusion or exclusion of lightly wounded. In 2006, McRandle and Quirk used the to adjust the by an increase of , which gave a total of casualties, compared to the French Official History record by 20 December 1916, of losses. A German record from the , which explicitly excluded lightly wounded, compared German losses at Verdun in 1916, which averaged for each with the 9th Army in Poland 1914 average of the 11th Army average in Galicia 1915 of , the 1st Army Somme 1916 average of and the 2nd Army average on the Somme of Jankowski estimated an equivalent figure for the French Second Army of , "including" lightly wounded. With a adjustment following McRandle and Quirk, to the German figure of to include lightly wounded. The loss rate is analogous to the estimate for French casualties.
Morale.
The concentration of so much fighting in such a small area devastated the land, resulting in miserable conditions for troops on both sides. Rain, combined with the constant tearing up of the ground, turned the clay of the area to a wasteland of mud full of human remains. Shell craters became filled with a liquid ooze, becoming so slippery that troops who fell into them or took cover in them could drown. Forests were reduced to tangled piles of wood by constant artillery-fire and eventually obliterated. The effect on soldiers in the battle was devastating and many broke down with shell-shock. Some French soldiers attempted to desert to Spain, those caught being court-martialled and shot. On 20 March, French deserters disclosed details of the French defences to the Germans, who were able to surround and force them to surrender.
A French lieutenant at Verdun, who would be killed by a shell, wrote in his diary on 23 May 1916, "Humanity is mad. It must be mad to do what it is doing. What a massacre! What scenes of horror and carnage! I cannot find words to translate my impressions. Hell cannot be so terrible. Men are mad!" Discontent began to spread among French troops at Verdun during the summer of 1916. Following the promotion of General Pétain from the Second Army on 1 June and his replacement by General Nivelle, five infantry regiments were affected by episodes of "collective indiscipline". Two French Lieutenants, Henri Herduin and Pierre Millant, were summarily shot on 11 June; Nivelle then published an Order of the Day forbidding French troops to surrender. In 1926, after an inquiry into the cause célèbre, Herduin and Millant were exonerated and their military records expunged.
Commemoration.
In April 1916, Pétain had issued an Order of the Day, ("Courage! We will get them") and on 23 June 1916, Nivelle ordered, "They shall not pass".
Nivelle had been concerned about diminished French morale at Verdun; after his promotion to lead the Second Army in June 1916, manifestations of indiscipline occurred in five front line regiments. reappeared in the French army mutinies that followed the Nivelle Offensive (April–May 1917).
Denizot published statistical tables including French troop movements, as well as monthly French artillery ammunition consumption by type of gun (German artillery ammunition consumption is reported in lesser detail) and period photographs show overlapping shell craters in an area of about . Forests planted in the 1930s have grown up and hide most of the "Zone rouge" (Red Zone) but the battlefield remains a vast graveyard, where the mortal remains of over soldiers lie, unless discovered by the French Forestry Service and laid in the Douaumont ossuary.
Pétain praised what he saw as the success of the fixed fortification system at Verdun in "La Bataille de Verdun" published in 1929 and in 1930, while construction of the Maginot Line began along the border with Germany. At Verdun, French field artillery in the open outnumbered turreted guns in the Verdun forts by at least It was the mass of French field artillery (over after May 1916) that inflicted about of German infantry casualties. In 1935, a number of mechanised and motorised units were deployed behind the Maginot line and plans were laid to send detachments to fight a mobile defence in front of the fortifications. Verdun remained a symbol and at the Battle of Dien Bien Phu (1953–1954), General Christian de Castries said that the situation was "somewhat like Verdun". French forces at Dien Bien Phu were supplied by transport aircraft, using a landing strip in range of Viet Minh artillery; the French forces at Verdun were supplied by road and rail, beyond the reach of German artillery.
[[File:Memorial de Verdun.jpg|thumb|Verdun Memorial on the battlefield near Fleury-devant-Douaumont, opened 1967: to the fallen soldiers and civilians
Verdun has become for the French the representative memory of World War I. Antoine Prost wrote, "Like Auschwitz, Verdun marks a transgression of the limits of the human condition". From 1918 to 1939, the French expressed two memories of the battle, a patriotic view embodied in memorials built on the battlefield and the memory of the survivors who recalled the death, suffering and sacrifice of others. In the 1960s, Verdun became a symbol of Franco-German reconciliation, through remembrance of common suffering and in the 1980s it became a capital of peace. Organisations were formed and old museums were dedicated to the ideals of peace and human rights. On 22 September 1984, the German Chancellor Helmut Kohl (whose father had fought near Verdun) and French President François Mitterrand (who had been taken prisoner nearby in World War II), stood at the Douaumont cemetery, holding hands for several minutes in driving rain as a gesture of Franco-German reconciliation.

</doc>
<doc id="51499" url="https://en.wikipedia.org/wiki?curid=51499" title="Western Front (World War I)">
Western Front (World War I)

The Western Front was the main theatre of war during World War I. Following the outbreak of war in August 1914, the German Army opened the Western Front first by invading Luxembourg and Belgium, then gaining military control of important industrial regions in France. The tide of the advance was dramatically turned with the Battle of the Marne. Following the race to the sea, both sides dug in along a meandering line of fortified trenches, stretching from the North Sea to the Swiss frontier with France. This line remained essentially unchanged for most of the war.
Between 1915 and 1917 there were several major offensives along this front. The attacks employed massive artillery bombardments and massed infantry advances. However, a combination of entrenchments, machine gun nests, barbed wire, and artillery repeatedly inflicted severe casualties on the attackers and counter-attacking defenders. As a result, no significant advances were made. Among the most costly of these offensives were the Battle of Verdun, in 1916, with a combined 700,000 casualties (estimated), the Battle of the Somme, also in 1916, with more than a million casualties (estimated), and the Battle of Passchendaele, in 1917, with roughly 600,000 casualties (estimated).
In an effort to break the deadlock, this front saw the introduction of new military technology, including poison gas, aircraft and tanks. But it was only after the adoption of improved tactics that some degree of mobility was restored. The German Army's Spring Offensive of 1918 was made possible by the Treaty of Brest-Litovsk that marked the end of the conflict on the Eastern Front. Using the recently introduced infiltration tactics, the German armies advanced nearly to the west, which marked the deepest advance by either side since 1914 and very nearly succeeded in forcing a breakthrough.
In spite of the generally stagnant nature of this front, this theatre would prove decisive. The inexorable advance of the Allied armies during the second half of 1918 persuaded the German commanders that defeat was inevitable, and the government was forced to sue for conditions of an armistice. The terms of peace were agreed upon with the signing of the Treaty of Versailles in 1919.
1914—German invasion of France and Belgium.
At the outbreak of the First World War, the German Army (consisting in the West of seven field armies) executed a modified version of the Schlieffen Plan, designed to quickly attack France through neutral Belgium before turning southwards to encircle the French army on the German border. Belgium's neutrality was guaranteed by Britain under the 1839 Treaty of London; this caused Britain to join the war at the expiration of its ultimatum at 11 pm GMT on 4 August. Armies under German generals Alexander von Kluck and Karl von Bülow attacked Belgium on 4 August 1914. Luxembourg had been occupied without opposition on 2 August. The first battle in Belgium was the Siege of Liège, which lasted from 5–16 August. Liège was well fortified and surprised the German army under von Bülow with its level of resistance. However, German heavy artillery was able to ruin the key forts within a few days. Following the fall of Liège, most of the Belgian army retreated to Antwerp and Namur, with the Belgian capital, Brussels, falling to the Germans on 20 August. Although the German army bypassed Antwerp, it remained a threat to their flank. Another siege followed at Namur, lasting from about 20–23 August.
For their part, the French had five Armies deployed on their borders. The pre-war French offensive plan, Plan XVII, was intended to capture Alsace-Lorraine following the outbreak of hostilities. On 7 August the VII Corps attacked Alsace with its objectives being to capture Mulhouse and Colmar. The main offensive was launched on 14 August with 1st and 2nd Armies attacking toward Sarrebourg-Morhange in Lorraine. In keeping with the Schlieffen Plan, the Germans withdrew slowly while inflicting severe losses upon the French. The French advanced the 3rd and 4th army toward the Saar River and attempted to capture Saarburg, attacking Briey and Neufchateau, before being driven back. The French VII Corps captured Mulhouse after a brief engagement on 7 August, but German reserve forces engaged them in the Battle of Mulhouse and forced a French retreat.
The German army swept through Belgium, executing civilians and razing villages. The application of "collective responsibility" against a civilian population further galvanised the allies, and newspapers condemned the German invasion and the army's violence against civilians and property, together called the "Rape of Belgium". (A modern author uses the term only in the narrower sense of describing the war crimes committed by the German army during this period.) After marching through Belgium, Luxembourg and the Ardennes, the German Army advanced, in the latter half of August, into northern France where they met both the French army, under Joseph Joffre, and the initial six divisions of the British Expeditionary Force, under Sir John French. A series of engagements known as the Battle of the Frontiers ensued. Key battles included the Battle of Charleroi and the Battle of Mons. In the former battle the French 5th Army was almost destroyed by the German 2nd and 3rd Armies and the latter delayed the German advance by a day. A general Allied retreat followed, resulting in more clashes such as the Battle of Le Cateau, the Siege of Maubeuge and the Battle of St. Quentin (Guise).
The German army came within of Paris, but at the First Battle of the Marne (6–12 September), French and British troops were able to force a German retreat by exploiting a gap which appeared between the 1st and 2nd Armies, ending the German advance into France. The German army retreated north of the Aisne River and dug in there, establishing the beginnings of a static western front that was to last for the next three years. Following this German setback, the opposing forces tried to outflank each other in the Race for the Sea, and quickly extended their trench systems from the North Sea to the Swiss frontier. The resulting German-occupied territory held 64% of France's pig-iron production, 24% of its steel manufacturing and 40% of the total coal mining capacity, dealing a serious, but not crippling setback to French industry.
On the Entente side, the final lines were occupied by the armies of the Allied countries, with each nation defending a part of the front. From the coast in the north, the primary forces were from Belgium, the British Empire and France. Following the Battle of the Yser in October, the Belgian forces controlled a 35 km length of Belgium's Flanders territory along the coast, known as the Yser Front, along the Yser river and the Yperlee canal, from Nieuwpoort to Boesinghe. Stationed to the south was the sector of the British Expeditionary Force (BEF). Here, from 19 October until 22 November, the German forces made their final breakthrough attempt of 1914 during the First Battle of Ypres. Heavy casualties were suffered on both sides but no breakthrough occurred. After the battle Erich von Falkenhayn reasoned that it was no longer possible for Germany to win the war, and on 18 November 1914 he called for a diplomatic solution, but Chancellor Theobald von Bethmann-Hollweg, Paul von Hindenburg and Erich Ludendorff disagreed. By Christmas, the BEF guarded a continual line from the La Bassée Canal to south of St. Eloi in the Somme valley. The greater part of the front, south to the border with Switzerland, was manned by French forces.
1915—Stalemate.
Between the coast and the Vosges was a westward bulge in the trench line, named the Noyon salient for the captured French town at the maximum point of advance near Compiègne. Joffre's plan for 1915 was to attack this German salient on both flanks to cut it off. The British would form the northern attack force by pressing eastward in Artois, while the French attacked in Champagne.
On 10 March, as part of what was intended as a larger offensive in the Artois region, the British army attacked at Neuve Chapelle in an effort to capture the Aubers Ridge. The assault was made by four divisions along a front. Preceded by a concentrated bombardment lasting 35 minutes, the initial assault made rapid progress and the village was captured within four hours. The advance then slowed because of problems with logistics and communications. The Germans then brought up reserves and counter-attacked, forestalling the attempt to capture the ridge. Since the British had used about one-third of their supply of artillery shells, General Sir John French blamed the failure on the shortage of shells, despite the success of the initial attack.
Gas warfare.
All sides signed treaties (the Hague Conventions of 1899 and 1907) which prohibited the use of chemical weapons in warfare before World War I. In spite of this, World War I saw large-scale chemical warfare.
Despite the German plans to maintain the stalemate with the French and British, German commanders planned an offensive at the Belgian town of Ypres, which the British had defended in November 1914. This Second Battle of Ypres was intended to divert attention from offensives in the Eastern Front while disrupting Franco-British planning and to test a new weapon: the second mass use of chemical weapons. (Ypres is frequently cited as the first use of gas but this had occurred at the Battle of Bolimów on the Eastern Front.) On 22 April, after a two-day bombardment, the Germans released of chlorine gas onto the battlefield. Being heavier than air, the gas crept across no man's land and drifted into the British trenches. The green-yellow cloud asphyxiated some defenders and those in the rear fled in panic, creating an undefended gap in the Allied line. The Germans were unprepared for the level of their success and lacked sufficient reserves to exploit the opening. Canadian troops quickly arrived and drove back the German advance.
The gas attack was repeated two days later and caused a withdrawal of the Franco-British line but the opportunity had been lost. The success of this attack would not be repeated, as the Allies countered by introducing gas masks and other countermeasures. An example of the success of these measures came a year later, on 27 April at Hulluch to the south of Ypres, where the 16th (Irish) Division withstood several German gas attacks.
Air warfare.
This year also saw the introduction of aeroplanes specifically modified for aerial combat. While planes had already been used in the war for scouting, on 1 April the French pilot Roland Garros became the first to shoot down an enemy plane by using a machine gun that shot forward through the propeller blades. This was achieved by crudely reinforcing the blades so bullets which hit them were deflected away.
Several weeks later Garros was forced to land behind German lines. His plane was captured and sent to Dutch engineer Anthony Fokker, who soon produced a significant improvement, the interrupter gear, in which the machine gun is synchronised with the propeller so it fires in the intervals when the blades of the propeller are out of the line of fire. This advance was quickly ushered into service, in the Fokker E.I ("Eindecker", or monoplane, Mark 1), the first single seat fighter aircraft to combine a reasonable maximum speed with an effective armament; Max Immelmann scored the first confirmed kill in an "Eindecker" on 1 August.
This started a back-and-forth arms race, as both sides developed improved weapons, engines, airframes and materials, which continued until the end of the war. It also inaugurated the cult of the ace, the most famous being the Red Baron. Contrary to the myth, antiaircraft fire claimed more kills than fighters.
Continued Entente attacks.
The final Entente offensive of the spring was fought at Artois, with the goal of trying to capture Vimy Ridge. The French 10th Army attacked on 9 May after a six-day bombardment and advanced . However, they retreated as they had come into sights of machine gun nests and the German reinforcements fired artillery at the attackers. By 15 May the advance had been stopped, although the fighting continued until 18 June.
In May the German army captured a French document at La Ville-aux-Bois describing a new system of defence. Rather than relying on a heavily fortified front line, the defence is arranged in a series of echelons. The front line would be a thinly manned series of outposts, reinforced by a series of strongpoints and a sheltered reserve. If a slope was available, troops were deployed along the rear side for protection. The defence became fully integrated with command of artillery at the divisional level. Members of the German high command viewed this new scheme with some favour and it later became the basis of an elastic defence in depth doctrine against Entente attacks.
During autumn of 1915, the "Fokker Scourge" began to have an effect on the battlefront as Allied spotter planes were nearly driven from the skies. These reconnaissance planes were used to direct gunnery and photograph enemy fortifications but now the Allies were nearly blinded by German fighters. However, the impact of German air superiority was diminished by their doctrinal reluctance to risk their pilots capture by fighting over Allied held territory.
In September 1915 the Entente allies launched another offensive, with the French attacking at Champagne and the British at Loos. The French had spent the summer preparing for this action, with the British assuming control of more of the front to release French troops for the attack. The bombardment, which had been carefully targeted by means of aerial photography, began on 22 September. The main French assault was launched on 25 September and at first made good progress, in spite of surviving wire entanglements and machine gun posts. Rather than retreating, the Germans adopted a new defence-in-depth scheme that consisted of a series of defensive zones and positions with a depth of up to .
On 25 September, the British began their assault at Loos, which was meant to supplement the larger Champagne attack. The attack was preceded by a four-day artillery bombardment of 250,000 shells and a release of 5,100 cylinders of chlorine gas. The attack involved two corps in the main assault and two more corps performing diversionary attacks at Ypres. The British suffered heavy losses, especially due to machine gun fire, during the attack and made only limited gains before they ran out of shells. A renewal of the attack on 13 October fared little better. In December, British Field Marshal Sir John French was replaced by General Douglas Haig as commander of the British forces.
1916—Artillery duels and attrition.
The German Chief of Staff, Erich von Falkenhayn, believed that a breakthrough might no longer be possible, and instead focused on forcing a French capitulation by inflicting massive casualties. His new goal was to "bleed France white".
As such, he adopted two new strategies. The first was the use of unrestricted submarine warfare to cut off Allied supplies arriving from overseas. The second would be targeted, high-casualty attacks against the French ground troops. To inflict the maximum possible casualties, he planned to attack a position from which the French could not retreat for reason of both strategic positions and national pride and thus trap the French. The town of Verdun was chosen for this because it was an important stronghold, surrounded by a ring of forts, that lay near the German lines and because it guarded the direct route to Paris. The operation was codenamed "Gericht", German for "court", but meant "place of execution".
Falkenhayn limited the size of the front to to concentrate their firepower and to prevent a breakthrough from a counteroffensive. He also kept tight control of the main reserve, feeding in just enough troops to keep the battle going. In preparation for their attack, the Germans had amassed a concentration of aircraft near the fortress. In the opening phase, they swept the air space of enemy spotters which allowed the accurate German artillery spotters and bombers to operate without interference. However, by May, the French countered by deploying "escadrilles de chasse" with superior Nieuport fighters. The tight air space over Verdun turned into an aerial battlefield, and illustrated the value of tactical air superiority, as each side sought to dominate air reconnaissance.
Battle of Verdun.
The Battle of Verdun began on 21 February 1916 after a nine-day delay due to snow and blizzards. After a massive eight-hour artillery bombardment, the Germans did not expect much resistance as they slowly advanced on Verdun and its forts. However, heavy French resistance was encountered. The French lost control of Fort Douaumont. Nonetheless, French reinforcements halted the German advance by 28 February.
The Germans turned their focus to Le Mort Homme to the north from which the French were successfully shelling them. After some of the most intense fighting of the campaign, the hill was taken by the Germans in late May. After a change in French command at Verdun from the defensive-minded Philippe Pétain to the offensive-minded Robert Nivelle the French attempted to re-capture Fort Douaumont on 22 May but were easily repulsed. The Germans captured Fort Vaux on 7 June and, with the aid of the gas diphosgene, came within of the last ridge over Verdun before stopping on 23 June.
Over the summer, the French slowly advanced. With the development of the rolling barrage, the French recaptured Fort Vaux in November, and by December 1916 they had pushed the Germans back from Fort Douaumont, in the process rotating 42 divisions through the battle. The Battle of Verdun—also known as the 'Mincing Machine of Verdun' or 'Meuse Mill'—became a symbol of French determination and self-sacrifice.
Battle of the Somme.
In the spring Allied commanders had been concerned about the ability of the French army to withstand the enormous losses at Verdun. The original plans for an attack around the river Somme were modified to let the British make the main effort. This would serve to relieve pressure on the French, as well as the Russians who had also suffered great losses. On 1 July, after a week of heavy rain, British divisions in Picardy launched an attack around the river Somme, supported by five French divisions on their right flank. The attack had been preceded by seven days of heavy artillery bombardment. The experienced French forces were successful in advancing but the British artillery cover had neither blasted away barbed wire, nor destroyed German trenches as effectively as was planned. They suffered the greatest number of casualties (killed, wounded, and missing) in a single day in the history of the British army, about 57,000.
Having assessed the air combat over Verdun, the Allies had new aircraft designed by Citroën engineer Andrew Sywy, for the attack in the Somme valley. The Verdun lesson learnt, the Allies' tactical aim became the achievement of air superiority and the German planes were, indeed, largely swept from the skies over the Somme. The success of the Allied air offensive caused a reorganisation of the German air arm, and both sides began using large formations of aircraft rather than relying on individual combat.
After regrouping, the battle continued throughout July and August, with some success for the British despite the reinforcement of the German lines. By August General Haig had concluded that a breakthrough was unlikely, and instead switched tactics to a series of small unit actions. The effect was to straighten out the front line, which was thought necessary in preparation for a massive artillery bombardment with a major push.
The final phase of the battle of the Somme saw the first use of the tank on the battlefield. The Allies prepared an attack that would involve 13 British and Imperial divisions and four French corps. The attack made early progress, advancing in places, but the tanks had little effect due to their lack of numbers and mechanical unreliability. The final phase of the battle took place in October and early November, again producing limited gains with heavy loss of life. All told, the Somme battle had made penetrations of only , and failed to reach the original objectives. The British had suffered about 420,000 casualties and the French around 200,000. It is estimated that the Germans lost 465,000, although this figure is controversial.
The Somme led directly to major new developments in infantry organisation and tactics; despite the terrible losses of 1 July, some divisions had managed to achieve their objectives with minimal casualties. In examining the reasons behind losses and achievements, the British, and the Colonial contingents, reintroduced the concept of the infantry platoon, following in the footsteps of the French and German armies who were already groping their way towards the use of small tactical units. At the time of the Somme, British senior commanders insisted that the company (120 men) was the smallest unit of manoeuvre; less than a year later, the section of 10 men would be so.
Hindenburg line.
In August 1916 the German leadership along the western front had changed as Falkenhayn resigned and was replaced by Generals Paul von Hindenburg and Erich Ludendorff. The new leaders soon recognised that the battles of Verdun and the Somme had depleted the offensive capabilities of the German army. They decided that the German army in the west would go over to the strategic defensive for most of 1917, while the Central powers would attack elsewhere.
During the Somme battle and through the winter months, the Germans created a prepared defensive position behind a section of their front that would be called the Hindenburg Line using the defensive principles elaborated since the defensive battles of 1915, including the use of Eingreif divisions. This was intended to shorten the German front, freeing 10 divisions for other duties. This line of fortifications ran from Arras south to St Quentin and shortened the front by about . British long-range reconnaissance aircraft first spotted the construction of the Hindenburg Line in November 1916.
1917—British offensives.
The Hindenburg Line was built between two and behind the German front line. On 9 February German forces retreated to the line and the withdrawal was completed 5 April, leaving behind a devastated territory to be occupied by the Allies. This withdrawal negated the French strategy of attacking both flanks of the Noyon salient, as it no longer existed. However, offensive advances by the British continued as the High Command claimed, with some justice, that this withdrawal resulted from the casualties the Germans received during the Battles of the Somme and Verdun, despite the Allies suffering greater losses.
Meanwhile, on 6 April the United States declared war on Germany. In early 1915, following the sinking of the , Germany had stopped its unrestricted submarine warfare in the Atlantic because of concerns of drawing the United States into the conflict. With the growing discontent of the German public due to the food shortages, however, the government resumed unrestricted submarine warfare in February 1917. They had calculated that a successful submarine and warship siege of Britain would force that country out of the war within six months, while American forces would take a year to become a serious factor on the Western Front. The submarine and surface ships had a long period of success before Britain resorted to the convoy system, bringing a large reduction in shipping losses.
By 1916–17, the size of the British Army on the Western Front had grown to two-thirds the total numbers in the French forces. In April 1917 the British Empire forces launched an attack starting the Battle of Arras. The Canadian Corps and the British 5th Division, attacked German lines at Vimy Ridge, capturing the heights. However, the rest of the offensive was halted with heavy losses. The Allied attack ended with the refusal to provide reinforcements to the region.
During the winter of 1916–17, German air tactics had been improved, a fighter training school was opened at Valenciennes and better aircraft with twin guns were introduced. The result was near disastrous losses for Allied air power, particularly for the British, Portuguese, Belgians, and Australians who were struggling with outmoded aircraft, poor training and weak tactics. As a result, the Allied air successes over the Somme would not be repeated, and heavy losses were inflicted by the Germans. During their attack at Arras, the British lost 316 air crews and the Canadians lost 114 compared to 44 lost by the Germans. This became known to the RFC as Bloody April.
French mutinies.
The same month, French General Robert Nivelle ordered a new offensive against the German trenches, promising that it would end the war within 48 hours. The 16 April attack, dubbed the Nivelle Offensive (also known as Chemin des Dames, after the area where the offensive took place), would be 1.2 million men strong, to be preceded by a week-long artillery bombardment and accompanied by tanks. However, the operation proceeded poorly as the French troops, with the help of two Russian brigades, had to negotiate rough, upward-sloping terrain. In addition, detailed planning had been dislocated by the voluntary German withdrawal to the Hindenburg Line, secrecy had been compromised, and German planes gained control of the sky making reconnaissance difficult. This allowed the creeping barrage to move too far ahead of the advancing troops. Within a week 100,000 French troops were dead. Despite the heavy casualties and his promise to halt the offensive if it did not produce a breakthrough, Nivelle ordered the attack continued into May.
On 3 May the weary French 2nd Colonial Division, veterans of the Battle of Verdun, refused their orders, arriving drunk and without their weapons. Lacking the means to punish an entire division, the officers of the division did not immediately implement harsh measures against the mutineers. Thereupon mutinies afflicted 54 French divisions and saw 20,000 men desert. Other Allied forces attacked but suffered massive casualties. Appeals to patriotism and duty followed, as did mass arrests and trials. The French soldiers returned to defend their trenches, but refused to participate in further offensive action. On 15 May Nivelle was removed from command, replaced by General Philippe Pétain who immediately suspended large-scale attacks. The French would go on the defensive for the following months to avoid high casualties and to restore confidence in the French High Command.
British offensives, American troops arrive.
On 7 June a British offensive was launched on Messines Ridge, south of Ypres, to retake the ground lost in the First and Second Battles of Ypres in 1914. Since 1915 specialist Royal Engineer tunnelling companies had been digging tunnels under the ridge, and about 500 tonnes (roughly 500,000 kg) of explosives had been planted in 21 mines under the enemy lines. Following four days of heavy bombardment, the explosives in 19 of these mines were detonated, resulting in the deaths of 10,000 Germans. The offensive that followed again relied on heavy bombardment which allowed the British infantry to capture the ridge in one day. The limited offensive was a great success, all German counter-attacks were defeated and the southern flank of the Gheluvelt plateau protected from German observation.
On 11 July 1917 during this battle, the Germans introduced a new weapon into the war when they fired gas shells delivered by artillery. The limited size of an artillery shell required that a more potent gas be deployed, and so the Germans employed mustard gas, a powerful blistering agent. The artillery deployment allowed heavy concentrations of the gas to be used on selected targets. Mustard gas was also a persistent agent, which could linger for up to several days at a site, an additional demoralising factor for their opponents. Along with phosgene, mustard gas would be used extensively by both German and Allied forces in later battles, as the Allies also began to increase production of gas for chemical warfare.
On 25 June the first US troops began to arrive in France, forming the American Expeditionary Force. However, the American units did not enter the trenches in divisional strength until October. The incoming troops required training and equipment before they could join in the effort, and for several months American units were relegated to support efforts. In spite of this, however, their presence provided a much-needed boost to Allied morale.
Beginning on 31 July and continuing to 10 November the struggle around Ypres was renewed with the Battle of Passchendaele (technically the Third Battle of Ypres, of which Passchendaele was the final phase). The battle had the original aim of capturing the ridges east of Ypres then advancing to Roulers and Thourout to close the main rail line supplying the German garrisons of the Western front and the Belgian coast then capturing the German submarine bases on the Belgian coast, but was later restricted to advancing the British Army onto the ridges around Ypres, as the unusually wet weather slowed British progress. Canadian veterans from the Battle of Vimy Ridge and the Battle of Hill 70 relieved the two ANZAC Corps and other British forces and took the village of Passchendaele on 6 November, despite extremely heavy rain and casualties. The offensive produced large numbers of casualties on both sides for relatively little gain of ground against dogged German resistance, yet that captured was of great tactical importance and the British made inexorable gains during periods of drier weather. The ground was generally muddy and pocked by shell craters, making supply missions and further advancement very difficult.
Both sides lost a combined total of over a half million men during this offensive. The battle has become a byword among some British historians for bloody and futile slaughter, whilst the Germans called Passchendaele "the greatest martyrdom of the War". It is one of the two battles (the other is the Battle of the Somme) which have done most to earn British Field Marshal Sir Douglas Haig his controversial reputation.
Battle of Cambrai.
On 20 November the British launched the first massed tank attack during the Battle of Cambrai. The Allies attacked with 324 tanks, with one-third held in reserve, and twelve divisions, against two German divisions. To maintain surprise, there was no preparatory bombardment; only a curtain of smoke was laid down before the tanks. The machines carried fascines on their fronts to bridge trenches and German tank traps. Special "grapnel tanks" towed hooks to pull away the German barbed wire. The initial attack was a success for the British. The British forces penetrated further in six hours than had been achieved at the Third Ypres in four months, and at a cost of only 4,000 British casualties.
However, the advance produced an awkward salient and a surprise German counteroffensive on 30 November drove the British back to their starting lines. Despite the reversal, the attack had been seen as a success by the Allies and Germans as it proved that tanks could overcome trench defences. The battle had also seen the first massed use of German "stosstruppen" on the Western front, who used infantry infiltration tactics to successfully penetrate the Allied lines, bypassing resistance and quickly advancing into the enemy's rear.
1918—Final offensives.
Following the successful Allied attack and penetration of the German defences at Cambrai, Ludendorff and Hindenburg determined that the only opportunity for German victory now lay in a decisive attack along the Western front during the spring, before American manpower became a significant presence. On 3 March 1918, the Treaty of Brest-Litovsk was signed, and Russia withdrew from the war. This would now have a dramatic effect on the conflict as 33 divisions were now released from the Eastern Front for deployment to the West. However, the Germans occupied almost as much Russian territory under the provisions of the Treaty of Brest-Litovsk as they did in the Second World War: this considerably restricted their troop redeployment. Nonetheless, they still had an advantage of 192 divisions to the Allied 178 divisions, which allowed Germany to pull veteran units from the line and retrain them as "sturmtruppen". In contrast, the Allies still lacked a unified command and suffered from morale and manpower problems: the British and French armies were sorely depleted, and American troops had not yet transitioned into a combat role.
Ludendorff's strategy would be to launch a massive offensive against the British and Commonwealth designed to separate them from the French and her allies, then drive them back to the channel ports. The attack would combine the new storm troop tactics with ground attack aircraft, tanks, and a carefully planned artillery barrage that would include gas attacks.
German spring offensives.
"Operation Michael", the first of the German Spring Offensives, very nearly succeeded in driving the Allied armies apart, advancing about during the first eight days and moving the front lines more than west, within shelling distance of Paris for the first time since 1914.
As a result of the battle, the Allies finally agreed on a unified system of command. General Ferdinand Foch was appointed commander of all Allied forces in France. The unified Allies were now better able to respond to each of the German drives, and the offensive turned into a battle of attrition.
In May, the American divisions also began to play an increasing role, winning their first victory in the Battle of Cantigny. By summer, 300,000 American soldiers were arriving every month. A total of 2.1 million American troops would be deployed on this front before the war came to an end. The rapidly increasing American presence served as a counter for the large numbers of redeployed German forces.
Final allied counter-offensives.
In July, Foch initiated a counter-offensive against the Marne salient produced during the German attacks, eliminating the salient by August. A second major offensive was launched two days after the first, ending at Amiens to the north. This attack included Franco-British forces, and was spearheaded by Australian and Canadian troops, along with 600 tanks and supported by 800 aircraft. The assault proved highly successful, leading Hindenburg to name 8 August as the "Black Day of the German Army". The Italian 2nd Army Corps, commanded by general Alberico Albricci, also participated in the operations around Reims.
The German army's manpower had been severely depleted after four years of war, and its economy and society were under great internal strain. The Entente now fielded a total of 216 divisions against 197 understrength German divisions. The Hundred Days Offensive beginning in August proved the final straw, and following this string of military defeats, German troops began to surrender in large numbers. As the Allied forces broke the German lines, Prince Maximilian of Baden was appointed as Chancellor of Germany in October to negotiate an armistice. Because of his opposition to the peace feelers, Ludendorff was forced to step aside and he fled to Sweden. Fighting was still continuing, but the German armies were in retreat when the German Revolution put a new government in power. An armistice was quickly signed, that stopped all fighting on the Western Front on Armistice Day (11 November 1918). The German Imperial Monarchy collapsed as General Groener (Ludendorff's successor) backed the moderate Social Democratic Government under Friedrich Ebert, rather than face the possibility of a revolution like that in Russia the previous year.
Consequences.
The war along the Western Front led the German government and its allies to sue for peace in spite of German success elsewhere. As a result, the terms of the peace were dictated by France, Britain and the United States, during the 1919 Paris Peace Conference. The result was the Treaty of Versailles, signed in June 1919 by a delegation of the new German government.
The terms of the treaty would effectively cripple Germany as an economic and military power. The Versailles treaty returned the border provinces of Alsace-Lorraine to France, thus limiting the coal required by German industry. The Saar, which formed the west bank of the Rhine, would be demilitarised and controlled by Britain and France, while the Kiel Canal opened to international traffic. The treaty also drastically reshaped Eastern Europe. It severely limited the German armed forces by restricting the size of the army to 100,000 and disallowing a navy or air force. The navy was sailed to Scapa Flow under the terms of surrender but was later scuttled, under the order of German admirals, as a reaction to the treaty.
Germany in 1919 was bankrupt, the people living in a state of semi-starvation, and having no commerce with the remainder of the world. The Allies occupied the Rhine cities of Cologne, Koblenz and Mainz, with restoration dependent on payment of reparations. Among the German populace, the myth arose—openly cultivated by the Army Chief of Staff Hindenburg—that the defeat was not the fault of the 'good core' of the army but due to certain left-wing groups within Germany; this would later be exploited by Nazi party propaganda to partly justify the overthrow of the Weimar Republic. "See" Stab-in-the-back legend.
France suffered heavy damage in the war. In addition to losing more casualties relative to its population than any other great power, the industrial north-east of the country had been devastated by the war. The provinces overrun by Germany had produced 40% of the nation's coal and 58% of its steel output. Once it was clear that Germany was going to be defeated, Ludendorff had ordered the destruction of the mines in France and Belgium. His goal was to cripple the industries of Germany's main European rival. To prevent similar German attacks in the future, France later built a massive series of fortifications along the German border known as the Maginot Line.
The war in the trenches of the Western Front had left a generation of maimed soldiers and war widows. The unprecedented loss of life had a lasting effect on popular attitudes toward war, resulting later in an Allied reluctance to pursue an aggressive policy toward Adolf Hitler (himself a decorated veteran of the war). The repercussions of that struggle are still being felt to this day.

</doc>
<doc id="51503" url="https://en.wikipedia.org/wiki?curid=51503" title="Progressive rock">
Progressive rock

Progressive rock (first known as progressive pop, later prog rock, prog, and sometimes art rock) is a rock music subgenre that originated in the United Kingdom with further developments in Germany, Italy, and France, throughout the mid-to-late 1960s and 1970s. It developed from psychedelic rock, and originated as an attempt to give greater artistic weight and credibility to rock music. Bands abandoned the short pop single in favor of instrumentation and compositional techniques more frequently associated with jazz or classical music in an effort to give rock music the same level of musical sophistication and critical respect.
Progressive rock sometimes abandons the danceable beat that defines earlier rock styles and is more likely to experiment with compositional structure, instrumentation, harmony, rhythm, and lyrical content. It may demand more effort on the part of the listener than other types of music. Musicians in progressive rock typically display a high degree of instrumental skill. Musical forms are blurred through the use of extended sections and of musical interludes that bridge separate sections, which results in classical-style suites. Early progressive rock groups expanded the timbral palette of the then-traditional rock instrumentation by adding instruments more typical of folk, jazz, or music in the classical tradition. A number of bands, especially at the genre's onset, recorded albums in which they performed with full orchestras.
Progressive rock artists are more likely to explore complex time signatures such as 5/8 and 7/8. Tempo, key, and time signature changes are common within progressive rock compositions.
Songs were replaced with musical suites that often stretched to 20 or 40 minutes in length and contained symphonic influences, extended musical themes, philosophical, mystical, or surreal lyrics, and complex orchestrations. The genre was criticized, however, as some reviewers found the concepts "pretentious" and the sounds "pompous" and "overblown".
Progressive rock saw a high level of popularity throughout the 1970s, especially in the middle of the decade. Bands such as Pink Floyd, Jethro Tull, the Moody Blues, Yes, King Crimson, Genesis, and Emerson, Lake & Palmer (ELP) were the genre's most influential groups and were among the most popular acts of the era, although there were many other, often highly influential, bands who experienced a lesser degree of commercial success. The genre faded in popularity during the second half of the decade. Conventional wisdom holds that the rise of punk rock caused this, although in reality a number of factors contributed to the decline. Progressive rock bands achieved commercial success well into the 1980s, albeit with changed lineups and more compact song structures.
The genre grew out of several 1960s rock bands. Most of the prominent bands from the genre's 1970s heyday fall into the "symphonic prog" category, in which classical orchestrations and compositional techniques are melded with rock music. Other subgenres exist, including the more accessible neo-progressive rock of the 1980s, the jazz-influenced Canterbury sound of the 1960s and 1970s, and the more political and experimental Rock in Opposition movement of the late 1970s and onward. Progressive rock has influenced genres such as krautrock and post-punk, and it has fused with other forms of rock music to create subgenres, such as neo-classical metal and progressive metal. A revival, often known as new prog, occurred at the turn of the 21st century and has since enjoyed a cult following.
Characteristics.
Progressive rock originally referred to progressive pop or "classical rock" in which a band performed together with an orchestra, but the term's use broadened over time to include Miles Davis-style jazz fusion, some metal and folk rock styles, and experimental German bands. It does not refer to a single style but to an approach that combines elements of diverse styles. Jerry Ewing, editor of "Prog Magazine", explains that "Prog is not just a sound, it's a mindset," and Dream Theater guitarist John Petrucci points out that it is defined by its very lack of stylistic boundaries.
The advent of the concept album and the genre's roots in psychedelia led albums and performances to be viewed as combined presentations of music, lyrics, and visuals. Progressive rock abandons the danceable beat that defines earlier rock styles and is more likely than other types of popular music to experiment with compositional structure, instrumentation, harmony and rhythm, and lyrical content. It may demand more effort on the part of the listener than other types of music.
Musicians in progressive rock typically display a high degree of instrumental skill, although this is not always the case. Neither Greg Lake nor Boz Burrell had ever been a bassist prior to filling that role in King Crimson. Jeffrey Hammond-Hammond joined Jethro Tull because of his social compatibility with the band rather than musical skills. "Jeffrey didn't get into the group because he was a good guitarist," said bandleader Ian Anderson, "because he could hardly play a note." Pink Floyd and Brian Eno are notable examples of artists who are able to build complex structures out of simple parts and who are virtuosos in the sense that their instrument is the recording studio.
Musical aspects.
Form.
Progressive rock songs often avoid common popular music song structures of verse/chorus form, and their extended lengths allow complex themes that cannot be fully developed within the span of a three-minute single. Musical forms are blurred through the use of extended sections and of musical interludes that bridge separate sections together, which results in classical-style suites. These large-scale compositions are similar to medleys, but there is typically more thematic unity between the sections. Transitions between electric and acoustic sections provide dynamic contrast. Extended instrumental passages often mix composed, classical-style sections with group improvisation. These sections emphasize group virtuosity rather than individual skill, and they are a break from other pop forms in which a single, dominant singer or soloist is accompanied by a band. Although many progressive rock songs are of three to five minutes in length, and bands such as Kraftwerk did adhere to pop songwriting principles, long-form pieces of twenty minutes or more are not uncommon.
These extended pieces are usually considered to be the result of experimentation with classical music forms, although an alternative viewpoint holds that they are explorations of the complexities possible within the popular music format. Many bands did, however, use compositional techniques borrowed from classical music. Gentle Giant, whose Kerry Minnear held a degree in composition from the Royal Academy of Music, often used counterpoint in their pieces. Kansas songs such as "Miracles out of Nowhere" often contain complex passages in which the violin and one or more keyboards and guitars all play separate contrapuntal parts. "Close to the Edge," by Yes, uses a classical compositional technique in which the arrangement is developed by the use of varied repetitions of a theme throughout the piece's structureand has elements of sonata form.
Elements of classical music are sometimes borrowed for the cultural significance they carry. Yes frequently used contrapuntal sections to create the impression of a baroque style, as in a fugue-like section at the eight-minute mark of "Close to the Edge" and in the harpsichord solo of "Siberian Khatru." Gentle Giant created a medieval feel through their use of the madrigal.
Instrumentation.
Early progressive rock groups expanded the timbral palette of the then-traditional rock instrumentation of guitar, keyboard, bass guitar, and drums by adding instruments more typical of folk music, jazz or music in the classical tradition. A number of bands, especially at the genre's onset, recorded albums in which they performed together with a full orchestra. The Moody Blues, who until then had been a blues-based British invasion band with a single hit to their credit, launched the trend with the huge success of their "Days of Future Passed" album. "Days" used arrangements that combined the band and orchestra, and it used orchestral interludes to bridge together the individual songs.
Electronic keyboards.
It was impractical to work together with an orchestra on a regular basis, so the Moody Blues turned to the Mellotron as a substitute. The Mellotron is a keyboard instrument that contains tape-recordings of individual notes of various instruments and voices, and plays back their sounds as the keyboard is pressed. Its sounds included woodwinds, choirs, brass and, perhaps most famously, strings. The technology available meant that its sounds were not exact reproductions of the instruments, but instead had a haunting quality that many bands prized. This instrument became the signature sound of the Moody Blues and was closely associated with many later progressive rock acts including Genesis, Strawbs, and King Crimson.
The Hammond organ is another instrument closely associated with progressive rock. It is a versatile instrument that can function like a pipe organ, can be played through a guitar amplifier for a distorted tone, is capable of sustained notes and rapid melodic runs, and can make percussive sounds. The ability to adjust its timbre while a note is held and its capabilities of vibrato and, when a rotating Leslie speaker is used, tremolo, make it a very expressive lead instrument. The use of organs and choirs reflects the background in Anglican church music shared by many of the genre's founders.
Various other electronic and electro-mechanical keyboard instruments were in common use. The RMI Electra-Piano was favored by Rick Wakeman of Yes, and Genesis keyboardist Tony Banks used its organ sounds to supplement those of the Hammond. RMI pianos could also substitute for harpsichords, as could the Clavinet. The Wurlitzer electric piano was a signature of Supertramp's sound. Some bands, notably Genesis, used Yamaha's electric grand piano, and string synthesizers were sometimes employed.
Synthesizers.
The birth of progressive rock roughly coincided with the commercial availability of synthesizers. Early modular synthesizers were large instruments that used patch cords to route the signal flow. Programming the instruments meant placing the patch cords to connect the individual modules. The Minimoog, a smaller, simplified synthesizer that needed no patch cords, began production in 1971 and provided keyboardists with a more-easily programmed instrument that could imitate other instruments, could create new sounds of its own, and was highly portable and affordable. Progressive rock was the genre in which the synthesizer first became established as a common part of popular music. Synthesizers could be used to play the rapid, virtuosic lines that changed the perception of keyboard instruments.
The reliance on the use of multiple keyboard sounds meant that keyboardists such as Rick Wakeman appeared onstage surrounded by ten or more keyboards. Modern digital synthesizers and samplers have reduced the need for huge keyboard stacks, as they typically allow sounds to be layered or for one keyboard to trigger another's sounds through a MIDI connection. They also provide a reliable alternative to instruments such as Mellotrons, whose delicate mechanical apparatus is prone to breakdowns, and are much more portable than bulky instruments such as the Hammond organ. Digital synthesizers are also suitable chordal instruments, unlike early analog synthesizers such as the Minimoog, Moog Taurus and ARP Odyssey, which could play only one note at a time and so were mainly suitable for drones, basslines and lead playing.
Electronic effects.
The concept of the studio as an instrument led certain audio effects units to become identified with progressive rock. Pink Floyd, especially in their early days, were noted for their heavy use of vocal delay. Robert Fripp and Brian Eno employed a tape-delay system using two 1/4" tape-recorders, and later dubbed "Frippertronics," that allowed self-accompaniment and the creation of textural, evolving soundscapes. Frippertronics debuted on Fripp & Eno's 1973 "No Pussyfooting" album, and was later incorporated into Fripp solo albums and mainstream works such as "Peter Gabriel" and Daryl Hall's 1977 "Sacred Songs". Progressive rock guitarists showed a distinct preference for Hiwatt amplifiers, with the exception of Yes guitarist Steve Howe, who used Fender Dual Showmans. Rush's transition from their early metal albums into their progressive rock phase was accompanied by guitarist Alex Lifeson's switch of amplification from Marshall to Hiwatt.
Advancements in recording technology were key in enabling the production of progressive rock albums. The Moody Blues were given access to an orchestra for the recording of "Days of Future Passed" because Deram Records wanted to showcase their production technology. As multitrack recording with as many as 64 separate tracks became available, bands took advantage of the additional tracks and created increasingly dense arrangements. Some artists, such as Yes and Brian Eno, later saw this as having been taken to excess and either simplified their arrangements or distanced themselves from the genre altogether.
Traditional instruments.
Progressive rock bands often use instruments in ways different from their traditional roles. The role of the bass may be expanded from its traditional rhythm section function into that of a lead instrument. Bassists often play contrapuntal lines that are more independent and melodic than conventional bass lines, which emphasize the chord root. This is often accompanied by the use of an instrument such as a Rickenbacker bass, whose sound contains an unusually large amount of treble frequencies. Some bassists use the Chapman Stick, which is operated with both hands on the fretboard and allows polyrhythmic and chordal playing. Treble may be emphasized by the choice of strings, by playing with a pick, and by use of the instrument's higher registers. Drum kits are frequently expanded with orchestral percussion such as timpani and gongs. Acoustic guitar becomes more prominent and often appears as interludes played in the classical style of Andrés Segovia. Piano is played in a style derived from the classical piano repertoire rather than from the blues or boogie-woogie styles previously in use. Guitar may be dispensed with altogether, and traditional rhythm guitar is almost never used, as chordal backgrounds are typically played on a keyboard instrument such as the Hammond organ. Genesis built huge, orchestral textures by blurring the lines between the roles of the keyboard and the guitar.
Virtuosity.
Virtuoso instrumental skills are so closely associated with progressive rock that authors such as Bill Martin consider it as a defining element and exclude bands such as Pink Floyd from consideration. Keith Emerson was acclaimed as "the Hendrix of the keyboard." Yes bassist Chris Squire helped to redefine his instrument's role in rock music and influenced bassists across a range of genres.
It is not uncommon for musicians to have received a higher-than-average level of formal training. Rick Wakeman studied at the Royal College of Music for a time, but left due to increasing demand for his services as a session musician. The Dixie Dregs were music students at the University of Miami, where their guitarist Steve Morse studied under Pat Metheny, and Dream Theater was formed by a group of Berklee School of Music students. Carl Palmer, of ELP, studied at the Guildhall School of Music and Drama. Annie Haslam, of Renaissance, was a classically trained soprano with a vocal range of five octaves. Genesis drummer (and later singer) Phil Collins and Curved Air vocalist Sonja Kristina performed in the London stage productions of "Oliver!" and "Hair", respectively.
Players from the genre frequently appear in readers' polls of publications that cater to musicians. The US magazine "Guitar Player" lists Yes guitarist Steve Howe, Mahavishnu Orchestra guitarist John McLaughlin, Rush bassist Geddy Lee, Dixie Dregs guitarist Steve Morse, onetime Soft Machine guitarist Andy Summers, and Frank Zappa in its "Gallery of the Greats," awarded for repeated wins in a readers' poll category. "Modern Drummer" magazine lists drummers Phil Collins; Stewart Copeland, formerly of Curved Air; Terry Bozzio, of Frank Zappa and U.K.; Vinnie Colaiuta, of Frank Zappa; Bill Bruford, of Yes and King Crimson; Carl Palmer, and Neil Peart of Rush in its reader-selected Hall of Fame. Editors of the US "Keyboard" magazine chose Dream Theater keyboardist Jordan Rudess and Jon Lord, the Deep Purple keyboardist who composed their "Concerto for Group and Orchestra", as founding members of their "Keyboard" Hall of Fame. Chris Squire was a frequent "Melody Maker" poll winner.
Rhythm, melody and harmony.
There is a tendency towards greater freedom of rhythm than exists in other forms of rock music. Progressive rock artists are more likely to explore complex time signatures such as 5/8 and 7/8. Tempo, key and time signature changes are common within progressive rock compositions. John Wetton, a veteran of several prominent progressive rock groups, later described frequent meter changes as an immature behavior that one grows out of. Yes keyboardist Rick Wakeman explained their use as necessary for matching the music to Jon Anderson's lyrics.
Complex time signatures are sometimes used to create a polyrhythmic effect, as in "The Journey," from Rick Wakeman's "Journey to the Centre of the Earth". An ostinato, played on a Clavinet in a meter subdivided as an unusual 2+2+2+3 pattern, is overlaid by a choral pattern in a time signature with the standard 3+3+3 subdivision. Robert Fripp has spoken of meters based on 5, 7 and 11 as "vital and energetic."
Progressive rock often discards the blues inflections and pentatonic scale-based melodies of mainstream rock in favor of modal melodies. Compositions draw inspiration from a wide range of genres including classical, jazz, folk music and world music. Melodies are more likely to comprise longer, developing passages than short, catchy ones.
Chords are typically standard triads, although many keyboardists would alter these triads by playing a nonchord tone in the bass. Quartal harmony, which uses chords built on intervals of fourths rather than thirds and was used heavily in the 1960s by John Coltrane pianist McCoy Tyner, is a key feature of Keith Emerson's style. ELP also use bitonality, or the use of two keys simultaneously, in "Infinite Space" and "The Endless Enigma." Some bands, such as King Crimson, incorporated atonality and free improvisation into their works. "Red" and "Fracture," two King Crimson pieces built on the octatonic scale and the whole tone scale, are two examples.
Chord changes are typically based on modes, as is typical of rock music, and deviate significantly from the tonality of music from the classical era. Unexpected chord changes in the style of impressionist composers like Claude Debussy are common. Jazz harmonies appear in the music of Canterbury groups such as Soft Machine.
Lyrical themes.
Progressive rock lyrics tend to avoid common rock and pop subjects such as love and dancing. Bands also avoid such youth-oriented themes as violence, nihilism, rebellion, and the macabre. Sex is not a common subject, although the occasionally leering lyrics of Jethro Tull and Frank Zappa are an exception. Themes found in classical literature, fantasy and folklore occur frequently, and intellectual topics such as psychological theories may be addressed. Romantic poetry and J. R. R. Tolkien are frequent sources of inspiration.
Medievalism and science fiction themes are common and often appear as metaphors for spiritual transformation and the quest for an ideal society. Magma's 1970s output is a single science fiction narrative spread out over several albums and written in the Kobaian language, which was invented for the purpose. Dystopian and apocalyptic themes drawn from science fiction criticize totalitarianism and the dehumanizing effects of society. These occur in Van der Graaf Generator's "Lemmings," Roger Waters' Pink Floyd lyrics in the mid-to-late 1970s and Rush's "2112". Bill Martin, author of several books on progressive rock, has noted that King Crimson's "21st Century Schizoid Man" anticipates cyberpunk by several years and carries a theme of technology run amok that is also found in ELP's "Tarkus" and "Brain Salad Surgery" albums.
Many early lyrics express utopian themes that reflect the genre's origins in psychedelic rock and address the subject of spiritual transformation. Spiritual and religious themes are common, as in Yes' "Close to the Edge", which is based on Hermann Hesse's "Siddhartha", and Aphrodite's Child's "666", an apocalyptic album with imagery drawn from the Biblical Book of Revelation.
Monty Python and Bonzo Dog Doo-Dah Band-influenced humour appears in some progressive rock lyrics. This is especially pronounced in the more eccentric, Dadaistic approach adopted by some of the Canterbury bands. Song titles such as Hatfield and the North's "Big Jobs (Poo Poo Extract)" reflect this. Puns are common, as in the Caravan album title "Cunning Stunts". The more serious symphonic prog bands occasionally recorded such comical tracks as "Jeremy Bender" by ELP, "Harold the Barrel" by Genesis, and "The Story of the Hare Who Lost His Spectacles", an interlude from Jethro Tull's album-length "A Passion Play".
Several groups valued lyrics so strongly as to employ a lyricist as a full-time band member. These include Peter Sinfield with King Crimson and Keith Reid with Procol Harum. Renaissance maintained a longtime relationship with lyricist Betty Thatcher. Hawkwind for a time featured lyrics by science fiction author Michael Moorcock.
Social commentary.
Social commentary is frequently present. The British class system is criticized in Genesis' "Selling England by the Pound", Gentle Giant's "Three Friends" and Jethro Tull's "Thick as a Brick", which also functions as a satire of the concept album. "Breakfast in America", by British expatriates Supertramp, questioned the American Dream. The Nice's instrumental "America" is considered to have made a similar point musically through a series of dissonant variations on the song's melody. Organized religion is criticized in Jethro Tull's "Aqualung", ELP's "The Only Way (Hymn)" and King Crimson's "The Great Deceiver."
Frank Zappa, a self-described conservative, used his concept album "Joe's Garage" to address themes such as individualism, sexuality, the danger of large government, and "the foolishness of white males".
Italian progressive rock bands, such as Premiata Forneria Marconi (PFM), had a greater tendency toward politicized lyrics. Bands and festivals in Italy were sometimes sponsored by the Italian Communist Party, and it was not uncommon for bands to hint, through either their lyrics or their actions, at support for armed revolutionary groups such as the Red Army Faction and the Palestine Liberation Organization. The very act of forming a band could be seen as politically subversive in Communist Eastern Europe, and acts such as Omega, in Hungary, and Aquarium, in the Soviet Union, initially existed as underground groups. Various members of the Czech band the Plastic People of the Universe endured prison sentences.
Henry Cow, an especially avant-garde British band with Marxist leanings, took the viewpoint that the major record labels were using their economic power to dictate which styles of music ever got heard by the public. The band organized a "Rock in Opposition" (RIO) festival to unite bands who similarly opposed music business practices. Italy's Stormy Six and Belgium's Univers Zero aligned themselves with the RIO movement, as did later bands such as the 5uu's and Thinking Plague.
Pastoralism and ecology.
Many progressive rock bands were strongly rooted in British folk music, and this resulted in a tendency toward pastoralism in the lyrics. Genesis, especially when Anthony Phillips was a member of the band, used mythological figures and fairytale worlds to create this effect in songs. After his departure the band did continue to explore these fantasy elements, yet often in a more diverse approach as songs began to combine fantasy with more dark and bizarrely surreal themes such as "The Musical Box" and "The Return of the Giant Hogweed." As social and economic problems increased in Britain within the 1970s, many artists gravitated away from pastoralism and ecology at varying degrees, with temporary to near-permanent shifts towards modernism, contemporary political satire, and realism. Jethro Tull, however, increasingly retreated into albums such as "Songs From the Wood", "Heavy Horses" and Stormwatch, whose lyrics emphasized nature.
Awareness of nature sometimes combined with social criticism to produce lyrics that expressed concern over the ecology. This appears on the major Yes albums of the early 1970s and their later "Don't Kill the Whale." Ecology also figures heavily in Magma's lyrical concept. Manfred Mann's Earth Band's 1974 album "The Good Earth" carried an ecological theme and included a coupon that entitled its purchasers to a square foot of mountain property in Wales. Ecological themes were sometimes carried out to an extent that even genre fans found embarrassing, and they were frequently satirized by Frank Zappa as naive.
Concept albums.
The late 1960s and early 1970s saw a general trend among rock and pop artists toward albums in which many or all of the songs shared a common theme. This tendency was especially pronounced in progressive rock. Experimentation with expanded musical forms contributed to this, as songs that were more or less thematically related were often combined into suites made up of several movements. This occurred as early as the 1966 album "Freak Out!", by the Mothers of Invention, in which the multi-part "The Return of the Son of Monster Magnet" occupied the entire fourth side of the album. Two influential examples followed in 1968: the title track of "Ars Longa Vita Brevis", by the Nice, and "In Held 'Twas in I," from Procol Harum's "Shine On Brightly", both of which used sonata-type forms.
These extended pieces carry on in the Romantic-era tradition of program music, which is intended to tell a story, and they often are inspired by works of literature. Pink Floyd's "Animals" is a concept album based on George Orwell's "Animal Farm". Genesis' "Selling England by the Pound" was influenced by T. S. Eliot's poem "The Waste Land". Rush's "2112" was inspired by Ayn Rand's "Anthem". Arthur C. Clarke's novel "Childhood's End" inspired both Pink Floyd's "Obscured by Clouds" and Genesis' "Watcher of the Skies."
"Darwin!", by Banco del Mutuo Soccorso, is a concept album based on Charles Darwin's theory of evolution. Gentle Giant's "The Power and the Glory" addressed current events, primarily the Watergate scandal. Story arcs are sometimes spread out over several albums, as was done with the "Chapters" on the first four Saga albums, Rush's Cygnus X-1 and Fear series, Magma's mythology and, more recently, the ongoing science fiction narrative of the Coheed and Cambria albums.
The advent of multi-part suites that occupy an entire LP side roughly coincided with the rise of FM radio and its practice of playing albums, or album sides, in their entirety. These extended works are at best, as with "Close to the Edge" and "2112," considered to be among the bands' greatest works. Some bands stretched the format beyond their audiences' capacity to tolerate. This was the case with Yes' "Tales from Topographic Oceans", a two-LP set that contained a single 20-minute song on each side. The album caused disagreements that led to keyboardist Rick Wakeman's departure from the band, as he compared the new material to a "padded bra" and protested the new songs by eating onstage instead of playing. In the punk era, "Tales" became a symbol of progressive rock self-indulgence.
Visual aspects.
Stage presentation.
Pink Floyd pioneered the concept of concerts as multimedia events, and they used sophisticated light shows meant to suggest or enhance the use of LSD. Their laser show was later replaced by even more sophisticated props such as aeroplane crashes, flying animals, and a giant wall that was constructed behind them and then torn down. Genesis took an operatic approach, as frontman Peter Gabriel used multiple costume changes to accent the theatrical nature of his lyrics. Their "The Lamb Lies Down on Broadway" tour reinforced this with a slideshow of as many as 1500 images.
Pink Floyd's interest in multimedia performances later led to soundtrack work on several films and ultimately expressed itself in the film "Pink Floyd – The Wall". Other progressive rock bands dabbled in film. Peter Gabriel collaborated with surrealist filmmaker Alejandro Jodorowsky in an attempt to write a "Lamb Lies Down on Broadway" screenplay, and the Italian band Goblin was noted for their soundtrack work on "Dawn of the Dead", "Profondo rosso" and "Suspiria".
Some acts indulged in pure showmanship. Jethro Tull frontman Ian Anderson was noted for his Pan-like persona and energetic performances in which he played the flute while standing on one leg. Grobschnitt displayed a cabaret-style show with pyrotechnics and slapstick acts. Rick Wakeman concerts in support of his "The Myths and Legends of King Arthur and the Knights of the Round Table" album featured ice skaters in Arthurian costumes. Keith Emerson, while with the Nice, was noted for holding organ notes by stabbing his keyboard with a pair of Hitler youth daggers provided by road crew member Lemmy. With ELP, he is known to have played his Moog modular synthesizer using his buttocks. ELP frequently used dangerous props and gimmicks such as flying pianos and exploding synthesizers in their stage act, and drummer Carl Palmer once cracked several ribs when he jumped over his drum set and landed on a trap door.
Progressive rock visual styles sometimes extended to the stage sets. Roger Dean designed stage sets for Yes that continued the visual themes used his album cover designs. Props included giant mushrooms and a drum set encased in a seashell, which nearly suffocated drummer Alan White when it failed to open during one performance. Tangerine Dream had a preference for performing in Gothic cathedrals and used light shows ranging from the minimal to full laser shows. Jean-Michel Jarre integrated projections and fireworks into his performances.
This enthusiasm for showmanship was not shared by all progressive rock bands. King Crimson initially employed a dramatic light show, but guitarist Robert Fripp became concerned that it distracted from the music. Fripp and Genesis guitarist Steve Hackett notably engaged in no stage movement at all and, instead, stayed seated throughout performances.
Album art.
Album covers prior to the Beatles' "Sgt. Pepper's Lonely Hearts Club Band" usually consisted of a photograph of the group, but the trend toward concept albums was accompanied by a move toward artwork that depicted the album's concept. This artwork often contains science fiction and fantasy motifs executed in a surrealist style. "Fragile", by Yes, has cover art that depicts the Earth splitting into pieces, which reflects the ecological focus of their lyrics. "Tarkus", by ELP, has a William Neal-designed LP gatefold that symbolically illustrates the titular suite's concept through a series of drawings of fantastic, cybernetic creatures who battle one another.
A number of artists became closely associated with the genre. Roger Dean, who designed album jackets for numerous bands and worked extensively with Yes, created imaginary worlds with a sense of imagination and grandeur that matched the music. Paul Whitehead illustrated early Genesis and Van der Graaf Generator albums with nightmarish art based on the songs' lyrics, and he encouraged the bands to develop a visual identity. Hipgnosis, a London design firm with close personal ties to members of Pink Floyd, used the music as inspiration for surrealistic designs that incorporated photographs and visual puns. Dean and Hipgnosis have influenced later visual artists and advertising designers.
Artwork was sometimes commissioned from artists who were famous in their own right, such as the H. R. Giger design for ELP's "Brain Salad Surgery" and caricaturist Gerald Scarfe's illustrations for Pink Floyd's "The Wall". This combination of music and artwork is intended to function as a total work of art, which is a further use of concepts borrowed from high culture. The practice of connecting an album's artwork to its concept still exists, but its effectiveness is limited by the smaller display area used by compact discs and mobile devices.
History.
Precursors.
When it emerged, progressive rock music was called progressive pop. Upon release, the Beach Boys' "Pet Sounds" (1966) was called "the most progressive pop album ever" by British newspapers. Later, some would retroactively characterize the term as a milder form of progressive rock. Bob Dylan's poetry, the Mothers of Invention's album "Freak Out!" (1966), and the Beatles' album "Sgt. Pepper's Lonely Hearts Club Band" (1967), have all been mentioned as important in progressive rock's development. The productions of Phil Spector were key influences, as they introduced the possibility of using the recording studio to create music that otherwise could never be achieved. The same is said for "Pet Sounds", which itself influenced "Sgt. Pepper's".
"Pet Sounds" and "Sgt. Pepper's".
"Pet Sounds" and "Sgt. Pepper's," with their lyrical unity, extended structure, complexity, eclecticism, experimentalism, and influences derived from classical music forms, is largely viewed as beginnings in the progressive rock genre and as turning points wherein rock, which previously had been considered dance music, became music that was made for listening to. Bill Bruford, a veteran of several progressive rock bands, said that "Sgt. Pepper" transformed both musicians' ideas of what was possible and audiences' ideas of what was acceptable in music. He believed that: "Without the Beatles, or someone else who had done what the Beatles did, it is fair to assume that there would have been no progressive rock." It also marked the point at which the LP record emerged as a creative format whose importance was equal to or greater than that of the single, an opinion which Brian Wilson began to share after hearing the US version of the Beatles' "Rubber Soul" (1965) with its deliberately reconfigured track listing intended to angle the album as a work of the emergent folk rock genre. LP sales first overtook those of singles in 1969.
Between "Pet Sounds" and "Sgt. Pepper", the Beach Boys released the single "Good Vibrations" (1966), dubbed a "pocket symphony" by Derek Taylor, who worked as a publicist for both groups. The song contained an eclectic array of exotic instruments and several discordant key and modal shifts. Scott Interrante of "Popmatters" wrote that its influence on progressive rock and the psychedelic movement "can't be overstated". Beatles biographer Jonathon Gould writes that "of the many ambitious pop singles released during the fall of 1966, none had a stronger influence on the Beatles". Martin likened the song to the Beatles' "A Day in the Life" from "Sgt. Pepper", elaborating that they showcase "the same reasons why much progressive rock is difficult to dance to".
Other works and influences.
Bob Dylan introduced a literary element to rock through his fascination with the Surrealists and the French Symbolists and his immersion in the New York City art scene of the early 1960s. The trend of bands with names drawn from literature, such as the Doors, Steppenwolf and the Ides of March, were a further sign of rock music aligning itself with high culture. Literary concepts such as Nietzsche and the Apollonian and Dionysian dichotomy were referenced by Doors singer Jim Morrison. Dylan also led the way in blending rock with folk music styles. This was followed by folk rock groups such as the Byrds, who based their initial sound on the work of Brian Wilson. In turn, the Byrds' vocal harmonies inspired those of Yes, and British electric folk bands such Fairport Convention, who emphasized instrumental virtuosity. Some of these artists, such as the Incredible String Band and Shirley and Dolly Collins, would prove influential through their use of instruments borrowed from world music and early music. Jimi Hendrix, who rose to prominence in the London scene and recorded with a band of English musicians, also initiated the trend toward virtuosity in rock music.
"Freak Out!", a Dadaist mixture of progressive rock, garage rock and avant-garde layered sounds is often considered to be the first concept album. The band 1-2-3, later renamed Clouds, began to experiment with song structure, improvisation, and multi-layered arrangements that same year. In March 1966, the Byrds released "Eight Miles High", a pioneering psychedelic rock single with a guitar lead inspired by the "sheets of sound" soloing style of jazz saxophonist John Coltrane. the Who later that year recorded "A Quick One While He's Away", a miniature rock opera considered to be the first example of the form. The rock opera was more fully realized in "S.F. Sorrow", an influential 1968 album by the Pretty Things. Love's "Revelation" from album "Da Capo" is also a notable early side long rock epic. "Going Home" by the Rolling Stones was the first long "jam" recorded expressly for an album with its over 10 minutes length.
The availability of newly affordable recording equipment coincided with the rise of a London underground scene at which LSD was commonly used. Pink Floyd and Soft Machine functioned as house bands at all-night events at locations such as Middle Earth and the UFO Club, where they experimented with sound textures and long-form songs. Beatles member John Lennon is known to have attended at least one such event, a happening called the 14 Hour Technicolor Dream. Paul McCartney was deeply connected to the underground through his involvement with the Indica Gallery. Many psychedelic, electric folk and early progressive bands were aided by exposure from BBC Radio 1 DJ John Peel.
Classical and jazz.
Harpsichords, orchestral wind instruments and string sections were used in mid-1960s recordings by groups such as the Beach Boys within their album "Today!" (1965). This created the form of Baroque rock heard in the Bach-inspired "A Whiter Shade of Pale" (1967), by Procol Harum. The use of instruments traditionally associated with classical music in rock music is difficult to trace in its beginnings, although it is evident in the early 1960s work of Burt Bacharach and Phil Spector. The Moody Blues established the popularity of symphonic rock when they recorded "Days of Future Passed" together with the London Festival Orchestra, and Procol Harum began to use a greater variety of acoustic instruments, particularly on their 1969 "A Salty Dog" album. Classical influences sometimes took the form of pieces adapted from or inspired by classical works, such as Jeff Beck's "Beck's Bolero" and parts of the Nice's "Ars Longa Vita Brevis". The latter, along with such Nice tracks as "Rondo" and "America", reflect a greater interest in music that is entirely instrumental. "Sgt. Pepper's" and "Days" both represent a growing tendency toward song cycles and suites made up of multiple movements.
Several bands that included jazz-style horn sections appeared, including Blood, Sweat & Tears and Chicago. Of these, Chicago in particular experimented with suites and extended compositions, such as the "Ballet for a Girl in Buchannon" on "Chicago II". Jazz influences appeared in the music of British bands such as Traffic, Colosseum and Canterbury scene bands such as Soft Machine. Canterbury scene bands emphasized the use of wind instruments, complex chord changes and long improvisations. Jethro Tull began as a heavy blues band fronted by Ian Anderson, a flautist deeply influenced by jazz musician Rahsaan Roland Kirk.
Early 1970s classic era.
The Nice, the Moody Blues, Procol Harum and Pink Floyd all contained elements of what is now called progressive rock, but none represented as complete an example of the genre as several bands that formed soon after. Almost all of the genre's major bands, including Jethro Tull, King Crimson, Yes, Genesis, Van der Graaf Generator, ELP, Gentle Giant and Curved Air, released their debut albums during the years 1968–1970. Most of these were folk-rock albums that gave little indication of what the band's mature sound would become, but King Crimson's "In the Court of the Crimson King" (1969) was a fully formed example of the genre. The term "progressive rock," which appeared in the liner notes of Caravan's 1968 self-titled debut LP, came to be applied to these bands that used classical music techniques to expand the styles and concepts available to rock music.
Most of the genre's major bands released their most critically acclaimed albums during the years 1971–1976. These include "Pawn Hearts", by Van der Graaf Generator; and "The Lamb Lies Down on Broadway", by Genesis; Yes' "The Yes Album", "Fragile" and "Close to the Edge"; "Aqualung" and "Thick as a Brick" by Jethro Tull, Gentle Giant's "Free Hand", ELP's "Brain Salad Surgery", Rush's "2112", and Pink Floyd's "The Dark Side of the Moon" and "Wish You Were Here".
Progressive rock experienced a high degree of commercial success during the early 1970s. Jethro Tull, ELP, Yes and Pink Floyd combined for four albums that reached number one in the US charts, and sixteen of their albums reached the top ten. Tull alone scored 11 gold albums and 5 platinum albums. Pink Floyd's 1970 album "Atom Heart Mother" reached the top spot on the UK charts. Their 1973 album "The Dark Side of the Moon", which united their extended compositions with the more structured kind of composing employed when Syd Barrett was their songwriter, spent more than two years at the top of the charts and remained on the "Billboard" 200 album chart for fifteen years. Mike Oldfield's "Tubular Bells", an excerpt of which was used as the theme for the film "The Exorcist", sold 16 million copies. A number of progressive bands released singles that became pop hits, including Kraftwerk ("Autobahn"), Yes ("Roundabout"), Jethro Tull ("Living in the Past"), Focus ("Hocus Pocus"), Curved Air ("Back Street Luv"), Strawbs ("Part of the Union"), and Genesis ("I Know What I Like").
The genre has always had its greatest appeal for white males. Most of the musicians involved were male, as was the case for most rock music of the time, although Curved Air vocalist Sonja Kristina and Renaissance singer Annie Haslam were prominent exceptions. Renaissance's lyricist also was female, and their feminine storytelling perspective is particularly prominent in their album art and in the songs "Ocean Gypsy" and "The Song of Scheherazade," both from "Scheherazade and Other Stories". Female singers were better represented in the progressive folk bands, who displayed a broader range of vocal styles than the progressive rock bands with whom they frequently toured and shared band members.
British and European audiences typically followed concert hall behavior protocols associated with classical music performances, and they were more reserved in their behavior than were audiences of other forms of rock. This confused musicians during US tours, as they found that American audiences were less attentive and more prone to outbursts during quiet passages.
North America.
Progressive rock came to be appreciated overseas, but it mostly remained a European, and especially British, phenomenon. Few American bands engaged in it, and the purest representatives of the genre, such as Starcastle and Happy the Man, remained limited to their own geographic regions. This is at least in part due to music industry differences between the US and Great Britain. Radio airplay was less important in the UK, where popular music recordings had never been played on official radio (as opposed to on pirate radio) until the 1967 launch of BBC Radio 1. Cultural factors were also involved, as US musicians tended to come from a blues background, while Europeans tended to have a foundation in classical music.
North American progressive rock bands often represented hybrid styles such as the complex metal of Rush, the psychedelic-driven hard rock of Captain Beyond, the Southern rock-tinged prog of Kansas, the jazz fusion of Mahavishnu Orchestra and Return to Forever, and the eclectic fusion of the all-instrumental Dixie Dregs. British progressive rock acts had their greatest US success in the same geographic areas in which British heavy metal bands experienced their greatest popularity. The overlap in audiences led to the success of arena rock bands, such as Boston, Kansas and Styx, who combined elements of the two styles.
Europe.
Progressive rock achieved popularity in Continental Europe more quickly than it did in the US. Italy remained generally uninterested in rock music until the strong Italian progressive rock scene developed in the early 1970s, and Van der Graaf Generator were much more popular there than in their own country. Genesis were hugely successful in Continental Europe at a time when they were still limited to a cult following in Britain and the US. Few of the European groups were successful outside of their own countries, with the exceptions of bands like Focus, who wrote English-language lyrics, and Le Orme and PFM, whose English lyrics were written by Peter Hammill and Peter Sinfield, respectively.
Some European bands played in a style derivative of English bands. This can be heard in Triumvirat, an organ trio in the style of ELP; Ange and Celeste who have had a strong King Crimson influence. Others brought national elements to their style: Spain's Triana introduced flamenco elements, groups such as the Swedish Samla Mammas Manna drew from the folk music styles of their respective nations, and Italian bands such as Il Balletto di Bronzo, Rustichelli & Bordini, leaned toward an approach that was more overtly emotional than that of their British counterparts.
Some progressive rock subgenres are tied to national scenes. Zeuhl was a name given to the style of the French band Magma. A number of bands were strongly influenced by Magma and are considered to be part of that subgenre. The "Kosmische music" scene in Germany came to be labeled as "krautrock" internationally. Bands such as Can, which included two members who had studied under Karlheinz Stockhausen, tended to be more strongly influenced by 20th century classical music than the British bands, whose musical vocabulary leaned more toward the Romantic era. Many of these groups were very influential even among bands that had little enthusiasm for the symphonic variety of progressive rock.
Late 1970s decline.
Political and social trends of the late 1970s shifted away from the early 1970s hippie attitudes that had led to the genre's development and popularity. The rise in punk cynicism made the utopian ideals expressed in progressive rock lyrics unfashionable. Virtuosity was rejected, as the expense of purchasing quality instruments and the time investment of learning to play them were seen as barriers to rock's energy and immediacy. There were also changes in the music industry, as record companies disappeared and merged into large media conglomerates. Promoting and developing experimental music was not part of the marketing strategy for these large corporations, who focused their attention on identifying and targeting profitable market niches.
Four of the biggest bands in progressive rock ceased performing or experienced major personnel changes during the mid-1970s. Robert Fripp disbanded King Crimson in 1974 and said later that the genre had gone "tragically off course." ELP went on hiatus the following year. Genesis moved in a more mainstream direction after the 1975 departure of Peter Gabriel and especially after the 1977 departure of Steve Hackett. Yes experienced lineup changes throughout the 1970s before fragmenting in 1980. A number of the major bands, including Van der Graaf Generator, Gentle Giant and U.K., dissolved between 1978 and 1980. Some decided that it was time to move on because they, as Caravan leader Pye Hastings admitted, had "got quite stale."
Many bands had by the mid-1970s reached the limit of how far they could experiment in a rock context, and fans had wearied of the extended, epic compositions. The sounds of the Hammond, Minimoog and Mellotron had been thoroughly explored, and their use became clichéd. Those bands who continued to record often simplified their sound, and the genre fragmented from the late 1970s onward. Corporate artists and repertoire staff exerted an increasing amount of control over the creative process that had previously belonged to the artists, and established acts were pressured to create music with simpler harmony and song structures and fewer changes in meter. This simplification can be heard as a softer, pop orientation in such albums as Genesis' "...And Then There Were Three...", Renaissance's "A Song for All Seasons", and the Moody Blues' "Octave". A number of symphonic pop bands, such as Supertramp, 10cc, the Alan Parsons Project and the Electric Light Orchestra, brought the orchestral-style arrangements into a context that emphasized pop singles while allowing for occasional instances of exploration. Jethro Tull, Gentle Giant and Pink Floyd opted for a harder sound in the style of arena rock.
Few new progressive rock bands formed during this era, and those who did found that record labels were not interested in signing them. Roth studied classical music with the intent of using the guitar in the way that classical composers used the violin. Finally, the Dutch-born and classically trained Alex and Eddie Van Halen formed Van Halen, who redefined the standard for rock virtuosity and paved the way for the "shred" music of the 1980s.
1980s.
Neo-progressive rock.
A second wave of progressive rock bands appeared in the early 1980s and have since been categorized as a separate "neo-progressive rock" subgenre. These largely keyboard-based bands played extended compositions with complex musical and lyrical structures. Most of the genre's major acts released debut albums between 1983 and 1985 and shared the same manager, Keith Goodwin, a publicist who had been instrumental in promoting progressive rock during the 1970s. The previous decade's bands had the advantage of appearing during a large countercultural movement that provided them with a large potential audience, but the neo-progressive bands were limited to a niche audience and found it difficult to attract a following. Only Marillion and Saga experienced international success.
Neo-prog bands tended to derive their sound and visual style from the symphonic prog bands of a decade earlier. The genre's most successful band, Marillion, suffered particularly from accusations of similarity to Genesis, although they used a different vocal style and a sound with more of a hard rock element. Authors Paul Hegarty and Martin Halliwell have pointed out that the neo-progressive bands were not so much plagiarizing progressive rock as they were creating a new style from progressive rock elements, just as the bands of a decade before had created a new style from jazz and classical elements. Author Edward Macan counters by pointing out that these bands were at least partially motivated by a nostalgic desire to preserve a past style rather than a drive to innovate.
A predecessor to this genre was the Enid, who fused rock with classical but were more heavily influenced by Ralph Vaughan Williams than by more modern composers. The change of approach can be heard in the shift toward shorter compositions and a keyboard-based sound in Rush albums such as "Grace Under Pressure". Neo-progressive bands emphasized individual solos instead of group improvisation, and they included more world music elements. Lyrics became more personal and less esoteric. Concept albums were still created, but not as frequently and on a smaller scale. Digital synthesizers took over many of the roles formerly filled by bulkier keyboards such as Mellotrons and organs, and their modern sound tended to minimize the folk influences that had been typical of 1970s progressive rock. Heavy metal bands such as Iron Maiden and Queensrÿche began to explore the mythological themes and extended concepts that had previously been the territory of progressive rock.
Commercialisation.
Some established bands moved toward music that was simpler and more commercially viable. Asia, a supergroup composed of veterans of several of the 1970s' major progressive rock acts, debuted in 1982 with an album that featured progressive rock-style Roger Dean artwork, some jazz influence, and advanced vocal arrangements. It however abandoned the complex song structures and interplay between music and vocals that had characterized progressive rock. The songs were based on pop hooks and repetitive choruses, were of a length appropriate for radio airplay, and featured slick production that pushed the vocals and snare drum to the front of the mix.
Echoes of progressive rock complexity could be heard in arena rock bands like Journey, Kansas, Styx, GTR, ELO and Foreigner, all of which either had begun as progressive rock bands or included members with strong ties to the genre. These bands retained some elements of the orchestral-style arrangements, but they moved away from lyrical mysticism in favor of teen-oriented songs about relationships. Genesis transformed into a successful pop act, and a reformed Yes released the relatively mainstream "90125", which yielded their only US number-one single, "Owner of a Lonely Heart". These radio-friendly groups have been called "prog lite".
One band who did experience great 1980s success while maintaining a progressive approach was Pink Floyd, who released "The Wall" late in 1979. The album, which brought punk anger into progressive rock, was a huge success and was later filmed as "Pink Floyd – The Wall". Pink Floyd were unable to repeat that combination of commercial and critical success, as their sole follow-up, "The Final Cut", was several years in coming and was essentially a Roger Waters solo project The band later reunited without Waters and restored many of the progressive elements that had been downplayed in the band's late-1970s work. This version of the band was very popular, Rush also incorporated new wave influences in their music by 1980, starting with "Permanent Waves", which blended progressive rock with a heavy use of synthesizers, and capitalized on more radio-friendly songs like "The Spirit of Radio", "Tom Sawyer", and "Subdivisions". Jethro Tull were able to capitalize on a 1980s interest in sword and sorcery with their 1982 "The Broadsword and the Beast", but they drifted toward a more mainstream style later in the decade, as did Rush.
Crossover with post-punk styles.
Progressive rock's influence was felt in the form of the post-punk bands, although these bands tended not to draw on classical rock or Canterbury bands as influences but rather Roxy Music and krautrock bands, particularly Can. Groups such as Public Image Ltd, Wire, and Simple Minds showed some influence of prog along with their more usually recognized punk influences. Julian Cope of the Teardrop Explodes wrote a history of the krautrock genre, "Krautrocksampler". New wave bands tended to be less hostile toward progressive rock than were the punks, and there were crossovers, such as Robert Fripp's and Brian Eno's involvement with Talking Heads, and Yes' replacement of Rick Wakeman and Jon Anderson with the pop duo the Buggles. A number of bands in New York's no wave scene were impressed with punk's energy but not with its primitivism. This led to experiments that combined that energy with greater musical sophistication, such as the guitar orchestras of Glenn Branca and the noise experiments of Sonic Youth.
Punk and prog were not necessarily as opposed as is commonly believed. Both genres reject commercialism, and punk bands did see a need for musical advancement, as evidenced by the albums "London Calling", by the Clash, and "My War", by Black Flag. Sex Pistols frontman Johnny Rotten famously wore a T-shirt that read "I hate Pink Floyd," but he expressed admiration for Van der Graaf Generator, and Pink Floyd themselves. Brian Eno expressed a preference for the approach of the punk and new wave bands in New York, as he found them to be more experimental and less personality-based than the English bands.
One progressive rock artist who was very supportive of the punk and new wave movements was former King Crimson leader Robert Fripp, who relocated to New York after a three-year retirement and collaborated with the new wave groups Blondie and Talking Heads. He formed a new band that experimented with gamelan music in a similar way to Talking Heads' approach on their "Remain in Light" album. The band was to be called "Discipline" but instead became a revived King Crimson. This edition featured new instrumentation that included Bill Bruford's electronic drums, Tony Levin's Chapman Stick, and guitar synthesizers played by Fripp and Adrian Belew, who was familiar to Fripp from the "Remain in Light" sessions. Their sound was highly percussive, featured tightly interconnected minimalist instrumentals with industrial noise influences, and often had a metallic edge. It was a new form of progressive rock that de-emphasized solos and overt virtuosity, but the music was nevertheless very complex and difficult.
Gamelan and minimalism also influenced Brian Eno, who after departing Roxy Music had collaborated with Fripp. Rush borrowed elements from world music and new wave, as on the reggae-tinged "The Spirit of Radio" and "Vital Signs."
1990s and 2000s.
Third wave.
A third wave of progressive rock bands, who might more properly be described as a second generation of neo-progressive bands, emerged in the 1990s. The use of the term "progressive" to describe groups that follow in the style of bands from ten to twenty years earlier is somewhat controversial, as it has been seen as a contradiction of the spirit of experimentation and progress. These new bands were aided in part by the availability of personal computer-based recording studios, which reduced album production expenses, and the Internet, which made it easier for bands outside of the mainstream to reach widely spread audiences. Record stores specializing in progressive rock appeared in large cities.
The shred music of the 1980s was a major influence on the progressive rock groups of the 1990s. Some of the newer bands, such as the Flower Kings, Spock's Beard, and Glass Hammer, played a 1970s-style symphonic prog but with an updated sound. A number of them began to explore the limits of the CD in the way that earlier groups had stretched the limits of the vinyl LP. "The Garden of Dreams," from the Flower Kings' "Flower Power" album, is nearly 60 minutes in length and is composed of 18 separate sections, and Transatlantic's "The Whirlwind" consists of a single track of 77 minutes in length.
Folk influences resurface on "The Garden of Dreams," a trend that also appears in Mostly Autumn's 2008 album "Glass Shadows". The Decemberists use folk themes and influences as a means of connecting with the past, while Midlake use them to express pastoralism and Shanghai's Cold Fairyland use them for nationalist purposes.
On their 2001 album "Origin of Symmetry", Muse included the new prog song "Citizen Erased," a 7-minute track with unusual structure, hard rock, classical, and electronic elements. With their return to space rock in 2006's Black Holes and Revelations, featuring the epic, satirical "Knights of Cydonia," they began to experiment more in the subsequent album; 2009's "The Resistance" was their most progressive to date, featuring the space rock opera "" and progressive arena rock anthem "United States of Eurasia." Frontman Matthew Bellamy confirmed that their 2015 album, "Drones", "does indeed include the sequel to fan favourite 'Citizen Erased' – and that the track in question is a crazy, ten minute prog nightmare."
Progressive metal.
Progressive rock and heavy metal have similar timelines. Both emerged from late-1960s psychedelia to achieve great early-1970s success despite a lack of radio airplay and support from critics, then faded in the late 1970s and experienced revivals in the early 1980s. Each genre experienced a fragmentation of styles at this time, and many metal bands from the New Wave of British Heavy Metal onward displayed progressive rock influences. Progressive metal reached a point of maturity with Queensrÿche's 1988 concept album "" and Voivod's 1989 "Nothingface", which featured abstract lyrics and a King Crimson-like texture.
Progressive metal drew attention when the US band Dream Theater's 1994 album "Awake" debuted at #32 on the album charts. King Crimson themselves returned in 1994 with a more metallic sound, as did Van der Graaf Generator in the following decade. Arjen Anthony Lucassen's Ayreon project, backed by an array of talent from the progressive rock genre, produced a series of innovative prog-metal concept albums from 1995 onward.
Several bands in the prog-metal genre, including the US bands Queensrÿche, Fates Warning and Dream Theater as well as Sweden's Opeth, name Rush as a primary influence. These bands also exhibit influences from more traditional metal and rock bands, such as Black Sabbath and Deep Purple. Tool have toured together with King Crimson and named them as an influence on their work, although Robert Fripp feels that the reverse is true and that there is a strong Tool influence on latter-day King Crimson.
Progressive rock elements appear in other metal subgenres. Black metal is conceptual by definition, due to its prominent theme of questioning the values of Christianity. Its guttural vocals are sometimes used by bands who can be classified as progressive, such as Mastodon, Mudvayne and Opeth, whose "In Live Concert at the Royal Albert Hall" DVD featured packaging that referenced vintage progressive rock albums such as Deep Purple's "Concerto for Group and Orchestra". Symphonic metal is an extension of the tendency toward orchestral passages in early progressive rock. Progressive rock has also served as a key inspiration for genres such as post-rock, math rock, power metal, and neo-classical metal.
New prog.
New prog, also known as alt-prog or post-prog, is a term that appeared in the mid-2000s to describe the new wave of progressive bands incorporating elements of post-hardcore and Alternative into their music. Although these bands incorporate elements of metal into their music such as Good Tiger, who at times employ death metal vocals, these bands are separate from the progressive metal movement although new-prog bands and progressive metal bands do commonly tour together.
Good Tiger, the new-prog band formed by the remaining members of The Safety Fire incorporate virtuoso musicianship, odd time signatures, and death metal vocals to fuse the framework of prog metal into the sound of the new-prog movement. The Mars Volta, who incorporated jazz, funk, punk rock, Latin music, and ambient noise into songs that range in length from a few minutes to over a half-hour, was formed by Cedric Bixler-Zavala and Omar Rodriguez-Lopez, former members of the post-hardcore band At the Drive-In. Their 2005 album "Frances the Mute" reached number 4 on the "Billboard" 200 chart after the single "The Widow" became a hit on modern rock radio. Coheed and Cambria are known for lengthy solos and a conceptual approach in which each album corresponds to an installment in lead singer/guitarist Claudio Sanchez's graphic novel series, "The Amory Wars". The Dear Hunter also follow a similar frame as coheed with continuing concept albums but instead fuse their new-prog sound with a sound akin to Jethro Tull's mix of folk and rock. Although leaning sometimes more in the direction of alternative than the previously mentioned bands, Circa Survive were also one of the earliest and most influential bands of The New-Prog movement.
2010s.
Progressive rock continues to appeal to its longtime fans and is also able to attract new audiences. The Progressive Music Awards were launched in 2012 by "Prog Magazine" to honor the genre's innovators and to promote its newer bands. Honorees, however, are not invited to perform at the awards ceremony, as the promoters want an event "that doesn't last three weeks."
Festivals.
Many prominent progressive rock bands got their initial exposure at large rock festivals that were held in Great Britain during the late 1960s and early 1970s. King Crimson made their first major appearance at the 1969 Hyde Park free concert, before a crowd estimated to be as large as 650,000, in support of the Rolling Stones. Emerson, Lake & Palmer debuted at the 1970 Isle of Wight Festival, at which Supertramp, Family and Jethro Tull also appeared. Jethro Tull were also present at the 1969 Newport Jazz Festival, the first year in which that festival invited rock bands to perform. Hawkwind appeared at many British festivals throughout the 1970s, although they sometimes showed up uninvited, set up a stage on the periphery of the event, and played for free.
Renewed interest in the genre in the 1990s led to the development of progressive rock festivals. ProgFest, organized by Greg Walker and David Overstreet in 1993, was first held in UCLA's Royce Hall, and featured Sweden's Änglagård, the UK's IQ, Quill and Citadel. CalProg was held annually in Whittier, California during the 2000s. The North East Art Rock Festival, or NEARfest, held its first event in 1999 in Bethlehem, Pennsylvania and held annual sold-out concerts until 2012's NEARfest Apocalypse, which featured headliners U.K. and Renaissance. Other festivals include the annual ProgDay (the longest-running and only outdoor prog festival) in Chapel Hill, North Carolina, the annual Rites of Spring Festival (RoSfest) in Gettysburg, Pennsylvania, The Rogue Independent Music Festival in Atlanta, Georgia, Baja Prog in Mexicali, Mexico, ProgPower USA in Atlanta, Georgia and ProgPower Europe in Baarlo, Netherlands. Progressive Nation tours were held in 2008 and 2009 with Dream Theater as the headline act.
Reception.
The genre has received both a great amount of critical acclaim and criticism throughout the years. Progressive rock has been described as parallel to the classical music of Igor Stravinsky and Béla Bartók. This desire to expand the boundaries of rock, combined with some musicians' dismissiveness toward mainstream rock and pop music, insulted critics and led to accusations of elitism. Its intellectual, fantastic and apolitical lyrics and its shunning of rock's blues roots were abandonments of the very things that many critics valued in rock music. Progressive rock also represented the maturation of rock as a genre, but there was an opinion among critics that rock was and should remain fundamentally tied to adolescence, so that rock and maturity were mutually exclusive.
Criticisms over the complexity of their music provoked some bands to create music that was even more complex. Yes' "Tales from Topographic Oceans" and "The Gates of Delirium" were both responses to such criticisms. Jethro Tull's "Thick As a Brick", a self-satirising concept album that consisted of a single 45-minute track, arose from the band's disagreement with the labeling of their previous "Aqualung" as a concept album.
These aspirations toward high culture reflect progressive rock's origins as a music created largely by upper- and middle-class, white-collar, college-educated males from Southern England. The music never reflected the concerns of or was embraced by working-class listeners, except in the US, where listeners appreciated the musicians' virtuosity. Progressive rock's exotic, literary topics were considered particularly irrelevant to British youth during the late 1970s, when the nation suffered from a poor economy and frequent strikes and shortages. Even King Crimson leader Robert Fripp dismissed progressive rock lyrics as "the philosophical meanderings of some English half-wit who is circumnavigating some inessential point of experience in his life." Bands whose darker lyrics avoided utopianism, such as King Crimson, Pink Floyd and Van der Graaf Generator, experienced less critical disfavor. Critics similarly came to regard krautrock as a genre separate from progressive rock. The simplicity of punk was in part a reaction against the elaborate nature of progressive rock, though, ironically, by the 21st century, some bands rooted in the hardcore punk-derived genre of metalcore turned towards progressive metal. Between the Buried and Me and Protest the Hero are examples of this movement.

</doc>
<doc id="51504" url="https://en.wikipedia.org/wiki?curid=51504" title="Dell Hymes">
Dell Hymes

Dell Hathaway Hymes (June 7, 1927 in Portland, OregonNovember 13, 2009 in Charlottesville, Virginia) was a linguist, sociolinguist, anthropologist, and folklorist who established disciplinary foundations for the comparative, ethnographic study of language use. His research focused upon the languages of the Pacific Northwest. He was one of the first to call the fourth subfield of anthropology "linguistic anthropology" instead of "anthropological linguistics". The terminological shift draws attention to the field's grounding in anthropology rather than in what, by that time, had already become an autonomous discipline (linguistics). In 1972 Hymes founded the journal "Language in Society" and served as its editor for 22 years.
Early life and education.
He was educated at Reed College, studying under David H. French, and graduated in 1950 after a stint in prewar Korea. His work in the Army as a decoder is part of what influenced him to become a linguist. Hymes earned his Ph.D. from Indiana University in 1955, and took a job at Harvard University.
Even at that young age, Hymes had a reputation as a strong linguist; his dissertation, completed in one year, was a grammar of the Kathlamet language spoken near the mouth of the Columbia and known primarily from Franz Boas’s work at the end of the 19th century.
Hymes remained at Harvard for five years, leaving in 1960 to join the faculty of the University of California, Berkeley. He spent five years at Berkeley as well, and then joined the Department of Anthropology at the University of Pennsylvania in 1965 (where he succeeded A. Irving Hallowell). In 1972 he joined the Department of Folklore and Folklife and became Dean of the University of Pennsylvania Graduate School of Education in 1975.
He served as president of the Linguistic Society of America in 1982, of the American Anthropological Association in 1983, and of the American Folklore Society - the last person to have held all three positions. He was a member of the Guild of Scholars of The Episcopal Church. While at Penn, Hymes was a founder of the journal "Language in Society". He won in 2001 the highest award for a scholar in Linguistics, the Gold Medal of Philology (http://insop.org/index.php?p=1_8_Ancient-Medal-Winners.). Hymes later joined the Departments of Anthropology and English at the University of Virginia, where he became the Commonwealth Professor of Anthropology and English, and from which he retired in 2000, continuing as emeritus professor until his death from complications of Alzheimer's disease on November 13, 2009.
His spouse, Virginia Hymes, is also a sociolinguist and folklorist.
Influences on his work.
Hymes was influenced by a number of linguists, anthropologists and sociologists, notably Franz Boas, Edward Sapir and Harry Hoijer of the Americanist Tradition; Roman Jakobson and others of the Prague Linguistic Circle; sociologist Erving Goffman, anthropologist Ray L. Birdwhistell, and ethnomethodologists Harold Garfinkel, Harvey Sacks, Emanuel Schegloff and Gail Jefferson.
Hymes' career can be divided into at least two phases. In his early career Hymes adapted Prague School Functionalism to American Linguistic Anthropology, pioneering the study of the relationship between language and social context. Together with John Gumperz, Erving Goffman and William Labov, Hymes defined a broad multidisciplinary concern with language in society.
Hymes' later work focuses on poetics, particularly the poetic organization of Native American oral narratives. He and Dennis Tedlock defined ethnopoetics as a field of study within linguistic anthropology and folkloristics. Hymes considers literary critic Kenneth Burke his biggest influence on this latter work, saying, “My sense of what I do probably owes more to KB than to anyone else”. Hymes studied with Burke in the 1950s. Burke's work was theoretically and topically diverse, but the idea that seems most influential on Hymes is the application of rhetorical criticism to poetry.
Hymes has included many other literary figures and critics among his influences, including Robert Alter, C. S. Lewis, A. L. Kroeber, Claude Lévi-Strauss.
Significance of his work.
As one of the first sociolinguists, Hymes helped to pioneer the connection between speech and social relations placing linguistic anthropology at the center of the performative turn within anthropology and the social sciences more generally.
Hymes formulated a response to Noam Chomsky's influential distinction between competence (knowledge of grammatical rules necessary to decoding and producing language) and performance (actual language use in context). Hymes objected to the marginalization of performance from the center of linguistic inquiry and proposed the notion of communicative competence, or knowledge necessary to use language in social context, as an object of linguistic inquiry. Since appropriate language use is conventionally defined, and varies across different communities, much of Hymes early work frames a project for ethnographic investigation into contrasting patterns of language use across speech communities. Hymes termed this approach "the ethnography of speaking." The SPEAKING acronym, described below, was presented as a lighthearted heuristic to aid fieldworkers in their attempt to document and analyze instances of language in use, which he termed "speech events." Embedded in the acronym is an application and extension of Jakobson's arguments concerning the multifunctionality of language. He articulated other, more technical, often typologically oriented approaches to variation in patterns of language use across speech communities in a series of articles.
As a result of discussions primarily with Ray Birdwhistell at the University of Pennsylvania, in his later work, Hymes renamed the "ethnography of speaking" the "ethnography of communication" to reflect the broadening of focus from instances of language production to the ways in which communication (including oral, written, broadcast, acts of receiving/listening) is conventionalized in a given community of users, and to include nonverbal as well as verbal behavior.
Hymes promoted what he and others call “ethnopoetics,” an anthropological method of transcribing and analyzing folklore and oral narrative that pays attention to poetic structures within speech. In reading the transcriptions of Indian myths, for example, which were generally recorded as prose by the anthropologists who came before, Hymes noticed that there are commonly poetic structures in the wording and structuring of the tale. Patterns of words and word use follow patterned, artistic forms.
Hymes’ goal, in his own mind, is to understand the artistry and “the competence… that underlies and informs such narratives” (Hymes 2003:vii). He created the Dell Hymes Model of Speaking and coined the term communicative competence within language education.
In addition to being entertaining stories or important myths about the nature of the world, narratives also convey the importance of aboriginal environmental management knowledge such as fish spawning cycles in local rivers or the disappearance of grizzly bears from Oregon. Hymes believes that all narratives in the world are organized around implicit principles of form which convey important knowledge and ways of thinking and of viewing the world. He argues that understanding narratives will lead to a fuller understanding of the language itself and those fields informed by storytelling, in which he includes ethnopoetics, sociolinguistics, psycholinguistics, rhetoric, semiotics, pragmatics, narrative inquiry and literary criticism.
Hymes clearly considers folklore and narrative a vital part of the fields of linguistics, anthropology and literature, and has bemoaned the fact that so few scholars in those fields are willing and able to adequately include folklore in its original language in their considerations (Hymes 1981:6-7). He feels that the translated versions of the stories are inadequate for understanding their role in the social or mental system in which they existed. He provides an example that in Navajo, the particles (utterances such as "uh," "So," "Well," etc. that have linguistic if not semantic meaning), omitted in the English translation, are essential to understanding how the story is shaped and how repetition defines the structure that the text embodies.
Hymes was the founding editor for the journal "Language in Society", which he edited for 22 years.
The "S-P-E-A-K-I-N-G" model.
Hymes developed a valuable model to assist the identification and labeling of components of linguistic interaction that was driven by his view that, in order to speak a language correctly, one needs not only to learn its vocabulary and grammar, but also the context in which words are used.
The model had sixteen components that can be applied to many sorts of discourse: message form; message content; setting; scene; speaker/sender; addressor; hearer/receiver/audience; addressee; purposes (outcomes); purposes (goals); key; channels; forms of speech; norms of interaction; norms of interpretation; and genres.
Hymes constructed the acronym SPEAKING, under which he grouped the sixteen components within eight divisions:
Setting and scene.
"Setting refers to the time and place of a speech act and, in general, to the physical circumstances". The living room in the grandparents' home might be a setting for a family story. Scene is the "psychological setting" or "cultural definition" of a setting, including characteristics such as range of formality and sense of play or seriousness. The family story may be told at a reunion celebrating the grandparents' anniversary. At times, the family would be festive and playful; at other times, serious and commemorative.
Participants.
Speaker and audience. Linguists will make distinctions within these categories; for example, the audience can be distinguished as addressees and other hearers. At the family reunion, an aunt might tell a story to the young female relatives, but males, although not addressed, might also hear the narrative.
Ends.
Purposes, goals, and outcomes. The aunt may tell a story about the grandmother to entertain the audience, teach the young women, and honor the grandmother.
Act sequence.
Form and order of the event. The aunt's story might begin as a response to a toast to the grandmother. The story's plot and development would have a sequence structured by the aunt. Possibly there would be a collaborative interruption during the telling. Finally, the group might applaud the tale and move onto another subject or activity.
Key.
Clues that establish the "tone, manner, or spirit" of the speech act. The aunt might imitate the grandmother's voice and gestures in a playful way, or she might address the group in a serious voice emphasizing the sincerity and respect of the praise the story expresses.
Instrumentalities.
Forms and styles of speech. The aunt might speak in a casual register with many dialect features or might use a more formal register and careful grammatically "standard" forms.
Norms.
Social rules governing the event and the participants' actions and reaction. In a playful story by the aunt, the norms might allow many audience interruptions and collaboration, or possibly those interruptions might be limited to participation by older females. A serious, formal story by the aunt might call for attention to her and no interruptions as norms.
Genre.
The kind of speech act or event; for the example used here, the kind of story. The aunt might tell a character anecdote about the grandmother for entertainment, or an exemplum as moral instruction. Different disciplines develop terms for kinds of speech acts, and speech communities sometimes have their own terms for types.
Personal life.
Credible accusations have been made of harassment of female students by Dell Hymes, though no formal action was taken as these claims were made posthumously.

</doc>
<doc id="51509" url="https://en.wikipedia.org/wiki?curid=51509" title="County seat">
County seat

A county seat is an administrative center, or seat of government, for a county or civil parish. The term is used in the United States, Canada, Taiwan and Romania. In the United Kingdom and Ireland, county towns have a similar function.
Function.
In the United States, counties are the administrative subdivisions of a state. Counties administer state law at the local level as part of the decentralization of state authority. In many states, state government is further decentralized below the county level by dividing counties into incorporated cities and towns and/or unincorporated civil townships, in order to provide local government services. The city, town, or populated place that houses county government is known as the seat of its respective county. Generally, the county legislature, county courthouse, sheriff's department headquarters, and hall of records, are located in the county seat, though some functions may also be conducted in other parts of the county, especially if it is geographically large.
A county seat is usually, but not always, an incorporated municipality. The exceptions include the county seats of counties that have no incorporated municipalities within their borders, such as Arlington County, Virginia, and Howard County, Maryland. (Ellicott City, the county seat of Howard County, is the largest unincorporated county seat in the United States, followed by Towson, the county seat of Baltimore County, Maryland.) Likewise, some county seats may not be incorporated in their own right, but are located within incorporated municipalities. For example, Cape May Court House, New Jersey, though unincorporated, is a section of Middle Township, an incorporated municipality. In some of the colonial states, county seats include or formerly included "Court House" as part of their name, (e.g. Spotsylvania Courthouse, Virginia).
U.S. counties with more than one county seat.
Most counties have only one county seat. However, some counties in Alabama, Arkansas, Iowa, Kentucky, Massachusetts, Mississippi, Missouri, New Hampshire, and Vermont have two or more county seats, usually located on opposite sides of the county. An example is Harrison County, Mississippi, which lists both Biloxi and Gulfport as county seats. The practice of multiple county seat towns dates from the days when travel was difficult. There have been few efforts to eliminate the two-seat arrangement, since a county seat is a source of pride (and jobs) for the towns involved.
There are 35 counties with multiple county seats (no more than two each) in 10 states:
Guilford County, North Carolina, in some ways effectively has two county seats. For example, the official county seat is Greensboro, but an additional has been located in nearby High Point since 1938.
Other counties in the United States effectively have two or more county seats by establishing one or more branch courthouses at which county business, including the recordation of documents affecting real estate, may be transacted. For example, Clearwater is the county seat of Pinellas County, Florida, but there is a branch courthouse in St. Petersburg.
Other variations.
In New England, the town, not the county, is the primary division of local government. Historically, counties in this region have served mainly as dividing lines for the states' judicial systems. Connecticut (since 1960) and Rhode Island have no county level of government and thus no county seats. In Vermont, Massachusetts, and Maine the county seats are called "shire towns". County government consists only of a Superior Court and Sheriff (as an officer of the court), both located in the respective shire town. Bennington County has two shire towns (Manchester for the "North Shire", Bennington for the "South Shire"), but the Sheriff is located in Bennington. In Massachusetts, most government functions which would otherwise be performed by county governments in other states are performed by town governments (there are no unincorporated areas in the state, that is, all land area in the state is within a town). As such, Massachusetts has dissolved many of its county governments, and the state government now operates the registries of deeds and sheriff's offices in those former counties. 
In Virginia, a county seat may be an independent city surrounded by, but not part of, the county of which it is the administrative center; for example, Fairfax City is both the county seat of Fairfax County and is completely surrounded by Fairfax County, but the city is politically independent of the county.
Two counties in South Dakota (Oglala Lakota and Todd) have their county seat and government services centered in a neighboring county. Their county-level services are provided by Fall River County and Tripp County, respectively.
In Louisiana, which is divided into parishes rather than counties, county seats are referred to as "parish seats".
Alaska is divided into boroughs rather than counties; the county seat in these case is referred to as the "borough seat"; this includes six consolidated city-borough governments and one municipality. The Unorganized Borough, which covers 49% of Alaska's area, has no county seat or equivalent. 
Canada.
In the Canadian Provinces of Prince Edward Island, New Brunswick and Nova Scotia, the term "shire town" is used in place of county seat.
Lists of U.S. county seats by state.
The state with the greatest number of counties is Texas, with 254, and the state with the least number of counties is Delaware, with 3.

</doc>
<doc id="51510" url="https://en.wikipedia.org/wiki?curid=51510" title="Silk">
Silk

Silk is a natural protein fiber, some forms of which can be woven into textiles. The protein fiber of silk is composed mainly of fibroin and is produced by certain insect larvae to form cocoons. The best-known silk is obtained from the cocoons of the larvae of the mulberry silkworm "Bombyx mori" reared in captivity (sericulture). The shimmering appearance of silk is due to the triangular prism-like structure of the silk fibre, which allows silk cloth to refract incoming light at different angles, thus producing different colors.
Silk is produced by several insects, but generally only the silk of moth caterpillars has been used for textile manufacturing. There has been some research into other types of silk, which differ at the molecular level. Silk is mainly produced by the larvae of insects undergoing complete metamorphosis, but some adult insects such as webspinners also produce silk, and some insects such as raspy crickets produce silk throughout their lives. Silk production also occurs in Hymenoptera (bees, wasps, and ants), silverfish, mayflies, thrips, leafhoppers, beetles, lacewings, fleas, flies, and midges. Other types of arthropod produce silk, most notably various arachnids such as spiders (see spider silk).
Etymology.
The word silk comes from Old English "sioloc", from Greek σηρικός "serikos", "silken", ultimately from an Asian source (cf. Chinese "si" "silk", Manchurian "sirghe", Mongolian "sirkek").
History.
Wild silk.
Several kinds of wild silk, which are produced by caterpillars other than the mulberry silkworm, have been known and used in China, South Asia, and Europe since ancient times. However, the scale of production was always far smaller than for cultivated silks. There are several reasons for this: first, they differ from the domesticated varieties in colour and texture and are therefore less uniform; second, cocoons gathered in the wild have usually had the pupa emerge from them before being discovered so the silk thread that makes up the cocoon has been torn into shorter lengths; and third, many wild cocoons are covered in a mineral layer that stymies attempts to reel from them long strands of silk. Thus, previously, the only way to obtain silk suitable for spinning into textiles in areas where commercial silks are not cultivated was by tedious and labor-intensive carding.
Commercial silks originate from reared silkworm pupae, which are bred to produce a white-colored silk thread with no mineral on the surface. The pupae are killed by either dipping them in boiling water before the adult moths emerge or by piercing them with a needle. These factors all contribute to the ability of the whole cocoon to be unravelled as one continuous thread, permitting a much stronger cloth to be woven from the silk.
Wild silks also tend to be more difficult to dye than silk from the cultivated silkworm. A technique known as demineralizing allows the mineral layer around the cocoon of wild silk moths to be removed, leaving only variability in color as a barrier to creating a commercial silk industry based on wild silks in the parts of the world where wild silk moths thrive, such as in Africa and South America.
Genetic modification of domesticated silkworms is used to facilitate the production of more useful types of silk.
China.
Silk fabric was first developed in ancient China. The earliest example of silk fabric is from 3630 BC, and it was used as wrapping for the body of a child from a Yangshao site in Qingtaicun at Xingyang, Henan.
Legend gives credit for developing silk to a Chinese empress, Leizu (Hsi-Ling-Shih, Lei-Tzu). Silks were originally reserved for the Emperors of China for their own use and gifts to others, but spread gradually through Chinese culture and trade both geographically and socially, and then to many regions of Asia. Because of its texture and lustre, silk rapidly became a popular luxury fabric in the many areas accessible to Chinese merchants. Silk was in great demand, and became a staple of pre-industrial international trade. In July 2007, archaeologists discovered intricately woven and dyed silk textiles in a tomb in Jiangxi province, dated to the Eastern Zhou Dynasty roughly 2,500 years ago. Although historians have suspected a long history of a formative textile industry in ancient China, this find of silk textiles employing "complicated techniques" of weaving and dyeing provides direct evidence for silks dating before the Mawangdui-discovery and other silks dating to the Han Dynasty (202 BC-220 AD).
Silk is described in a chapter on mulberry planting by Si Shengzhi of the Western Han (206 BC – 9 AD). There is a surviving calendar for silk production in an Eastern Han (25–220 AD) document. The two other known works on silk from the Han period are lost. The first evidence of the long distance silk trade is the finding of silk in the hair of an Egyptian mummy of the 21st dynasty, c.1070 BC. The silk trade reached as far as the Indian subcontinent, the Middle East, Europe, and North Africa. This trade was so extensive that the major set of trade routes between Europe and Asia came to be known as the Silk Road.
The Emperors of China strove to keep knowledge of sericulture secret to maintain the Chinese monopoly. Nonetheless sericulture reached Korea with technological aid from China around 200 BC, the ancient Kingdom of Khotan by AD 50, and India by AD 140.
In the ancient era, silk from China was the most lucrative and sought-after luxury item traded across the Eurasian continent, and many civilizations, such as the ancient Persians, benefited economically from trade.
India.
Silk has a long history in India. It is known as "Resham" in eastern and north India, and "Pattu" in southern parts of India. Recent archaeological discoveries in Harappa and Chanhu-daro suggest that sericulture, employing wild silk threads from native silkworm species, existed in South Asia during the time of the Indus Valley Civilization dating between 2450 BC and 2000 BC, while "hard and fast evidence" for silk production in China dates back to around 2570 BC. Shelagh Vainker, a silk expert at the Ashmolean Museum in Oxford, who sees evidence for silk production in China "significantly earlier" than 2500–2000 BC, suggests, "people of the Indus civilization either harvested silkworm cocoons or traded with people who did, and that they knew a considerable amount about silk."
India is the second largest producer of silk in the world after China. About 97% of the raw silk comes from five Indian states, namely, Andhra Pradesh, Karnataka, Jammu and Kashmir, Tamil Nadu and West Bengal. North Bangalore, the upcoming site of a $20 million "Silk City" Ramanagara and Mysore, contribute to a majority of silk production in Karnataka. 
In Tamil Nadu, mulberry cultivation is concentrated in the Coimbatore, Erode, Tiruppur, Salem and Dharmapuri districts. Hyderabad, Andhra Pradesh, and Gobichettipalayam, Tamil Nadu, were the first locations to have automated silk reeling units in India.
India is also the largest consumer of silk in the world. The tradition of wearing silk sarees for marriages and other auspicious ceremonies is a custom in Assam and southern parts of India. Silk is considered to be a symbol of royalty, and, historically, silk was used primarily by the upper classes. Silk garments and sarees produced in Kanchipuram, Pochampally, Dharmavaram, Mysore, Arani in the south, Banaras in the north, and Murshidabad in the east are well recognized. In the northeastern state of Assam, three different types of silk are produced, collectively called Assam silk: Muga, Eri and Pat silk. Muga, the golden silk, and Eri are produced by silkworms that are native only to Assam.
Thailand.
Silk is produced year round in Thailand by two types of silkworms, the cultured Bombycidae and wild Saturniidae. Most production is after the rice harvest in the southern and northeastern parts of the country. Women traditionally weave silk on hand looms and pass the skill on to their daughters, as weaving is considered to be a sign of maturity and eligibility for marriage. Thai silk textiles often use complicated patterns in various colours and styles. Most regions of Thailand have their own typical silks. A single thread filament is too thin to use on its own so women combine many threads to produce a thicker, usable fiber. They do this by hand-reeling the threads onto a wooden spindle to produce a uniform strand of raw silk. The process takes around 40 hours to produce a half kilogram of silk. Many local operations use a reeling machine for this task, but some silk threads are still hand-reeled. The difference is that hand-reeled threads produce three grades of silk: two fine grades that are ideal for lightweight fabrics, and a thick grade for heavier material.
The silk fabric is soaked in extremely cold water and bleached before dyeing to remove the natural yellow coloring of Thai silk yarn. To do this, skeins of silk thread are immersed in large tubs of hydrogen peroxide. Once washed and dried, the silk is woven on a traditional hand-operated loom.
Ancient Mediterranean.
In the "Odyssey", 19.233, when Odysseus, while pretending to be someone else, is questioned by Penelope about her husband's clothing, he says that he wore a shirt "gleaming like the skin of a dried onion" (varies with translations, literal translation here) which could refer to the lustrous quality of silk fabric. The Roman Empire knew of and traded in silk, and Chinese silk was the most highly priced luxury good imported by them. During the reign of emperor Tiberius, sumptuary laws were passed that forbade men from wearing silk garments, but these proved ineffectual. Despite the popularity of silk, the secret of silk-making only reached Europe around AD 550, via the Byzantine Empire. Legend has it that monks working for the emperor Justinian I smuggled silkworm eggs to Constantinople in hollow canes from China. All top-quality looms and weavers were located inside the Great Palace complex in Constantinople, and the cloth produced was used in imperial robes or in diplomacy, as gifts to foreign dignitaries. The remainder was sold at very high prices.
Middle East.
In the Torah, a scarlet cloth item called in Hebrew "sheni tola'at" שני תולעת - literally "crimson of the worm" - is described as being used in purification ceremonies, such as those following a leprosy outbreak (Leviticus 14), alongside cedar wood and hyssop (za'atar). Eminent scholar and leading medieval translator of Jewish sources and books of the Bible into Arabic, Rabbi Saadia Gaon, translates this phrase explicitly as "crimson silk" - חריר קרמז حرير قرمز.
In Islamic teachings, Muslim men are forbidden to wear silk. Many religious jurists believe the reasoning behind the prohibition lies in avoiding clothing for men that can be considered feminine or extravagant. There are disputes regarding the amount of silk a fabric can consist of (e.g., whether a small decorative silk piece on a cotton caftan is permissible or not) for it to be lawful for men to wear, but the dominant opinion of most Muslim scholars is that the wearing of silk by men is forbidden. Modern attire has raised a number of issues, including, for instance, the permissibility of wearing silk neckties, which are masculine articles of clothing.
Despite injunctions against silk for men, silk has retained its popularity in the Islamic world because of its permissibility for women, and due to the presence of non-Muslim communities. The Muslim Moors brought silk with them to Spain during their conquest of the Iberian Peninsula.
Medieval and modern Europe.
Italy was the most important producer of silk during the Medieval age. The first center to introduce silk production to Italy was the city of Catanzaro during the 11th century in the region of Calabria. The silk of Catanzaro supplied almost all of Europe and was sold in a large market fair in the port of Reggio Calabria, to Spanish, Venetian, Genovese and Dutch merchants. Catanzaro became the lace capital of the world with a large silkworm breeding facility that produced all the laces and linens used in the Vatican. The city was world-famous for its fine fabrication of silks, velvets, damasks and brocades.
Another notable center was the Italian city-state of Lucca which largely financed itself through silk-production and silk-trading, beginning in the 12th century. Other Italian cities involved in silk production were Genoa, Venice and Florence.
The Silk Exchange in Valencia from the 15th century—where previously in 1348 also "perxal" (percale) was traded as some kind of silk—illustrates the power and wealth of one of the great Mediterranean mercantile cities.
Silk was produced in and exported from the province of Granada, Spain, especially the Alpujarras region, until the Moriscos, whose industry it was, were expelled from Granada in 1571.
Since the 15th century, silk production in France has been centered around the city of Lyon where many mechanic tools for mass production were first introduced in the 17th century.
James I attempted to establish silk production in England, purchasing and planting 100,000 mulberry trees, some on land adjacent to Hampton Court Palace, but they were of a species unsuited to the silk worms, and the attempt failed. In 1732 John Guardivaglio set up a silk throwing enterprise at Logwood mill in Stockport; in 1744, Burton Mill was erected in Macclesfield; and in 1753 Old Mill was built in Congleton. These three towns remained the centre of the English silk throwing industry until silk throwing was replaced by silk waste spinning. British enterprise also established silk filature in Cyprus in 1928. In England in the mid-20th century, raw silk was produced at Lullingstone Castle in Kent. Silkworms were raised and reeled under the direction of Zoe Lady Hart Dyke. Production started elsewhere later.
North America.
King James I introduced silk-growing to the American colonies around 1619, ostensibly to discourage tobacco planting. The Shakers in Kentucky adopted the practice. In the 19th century a new attempt at a silk industry began with European-born workers in Paterson, New Jersey, and the city became a silk center in the United States. Manchester, Connecticut emerged as center of the silk industry in America from the late 19th through the mid-20th century. The Cheney Brothers Historic District showcases mills refurbished as apartments and includes nearby museums.
World War II interrupted the silk trade from Asia, and silk prices increased dramatically. U.S. industry began to look for substitutes, which led to the use of synthetics such as nylon. Synthetic silks have also been made from lyocell, a type of cellulose fiber, and are often difficult to distinguish from real silk (see spider silk for more on synthetic silks).
Malaysia.
In Terengganu, which is now part of Malaysia, a second generation of silkworm was being imported as early as 1764 for the country's silk textile industry, especially songket. However, since the 1980s, Malaysia is no longer engaged in sericulture but does plant mulberry trees.
Vietnam.
In Vietnamese legend, silk appeared in the sixth dynasty of Hùng Vương.
Production process.
The entire production process of silk can be divided into several steps which are typically handled by different entities. Extracting raw silk starts by cultivating the silkworms on mulberry leaves. Once the worms start pupating in their cocoons, these are dissolved in boiling water in order for individual long fibres to be extracted and fed into the spinning reel.
Properties.
Physical properties.
Silk fibers from the "Bombyx mori" silkworm have a triangular cross section with rounded corners, 5–10 μm wide. The fibroin-heavy chain is composed mostly of beta-sheets, due to a 59-mer amino acid repeat sequence with some variations. The flat surfaces of the fibrils reflect light at many angles, giving silk a natural sheen. The cross-section from other silkworms can vary in shape and diameter: crescent-like for "Anaphe" and elongated wedge for "tussah". Silkworm fibers are naturally extruded from two silkworm glands as a pair of primary filaments (brin), which are stuck together, with sericin proteins that act like glue, to form a bave. Bave diameters for tussah silk can reach 65 μm. See cited reference for cross-sectional SEM photographs.
Silk has a smooth, soft texture that is not slippery, unlike many synthetic fibers.
Silk is one of the strongest natural fibers, but it loses up to 20% of its strength when wet. It has a good moisture regain of 11%. Its elasticity is moderate to poor: if elongated even a small amount, it remains stretched. It can be weakened if exposed to too much sunlight. It may also be attacked by insects, especially if left dirty.
One example of the durable nature of silk over other fabrics is demonstrated by the recovery in 1840 of silk garments from a wreck of 1782: 'The most durable article found has been silk; for besides pieces of cloaks and lace, a pair of black satin breeches, and a large satin waistcoat with flaps, were got up, of which the silk was perfect, but the lining entirely gone ... from the thread giving way ... No articles of dress of woollen cloth have yet been found.'
Silk is a poor conductor of electricity and thus susceptible to static cling.
Unwashed silk chiffon may shrink up to 8% due to a relaxation of the fiber macrostructure, so silk should either be washed prior to garment construction, or dry cleaned. Dry cleaning may still shrink the chiffon up to 4%. Occasionally, this shrinkage can be reversed by a gentle steaming with a press cloth. There is almost no gradual shrinkage nor shrinkage due to molecular-level deformation.
Natural and synthetic silk is known to manifest piezoelectric properties in proteins, probably due to its molecular structure.
Silkworm silk was used as the standard for the denier, a measurement of linear density in fibers. Silkworm silk therefore has a linear density of approximately 1 den, or 1.1 dtex.
Chemical properties.
Silk emitted by the silkworm consists of two main proteins, sericin and fibroin, fibroin being the structural center of the silk, and serecin being the sticky material surrounding it. Fibroin is made up of the amino acids Gly-Ser-Gly-Ala-Gly-Ala and forms beta pleated sheets. Hydrogen bonds form between chains, and side chains form above and below the plane of the hydrogen bond network.
The high proportion (50%) of glycine allows tight packing. This is because glycine's R group is only a hydrogen and so is not as sterically constrained. The addition of alanine and serine makes the fibres strong and resistant to breaking. This tensile strength is due to the many interceded hydrogen bonds, and when stretched the force is applied to these numerous bonds and they do not break.
Silk is resistant to most mineral acids, except for sulfuric acid, which dissolves it. It is yellowed by perspiration. Chlorine bleach will also destroy silk fabrics.
Uses.
Silk's absorbency makes it comfortable to wear in warm weather and while active. Its low conductivity keeps warm air close to the skin during cold weather. It is often used for clothing such as shirts, ties, blouses, formal dresses, high fashion clothes, lining, lingerie, pajamas, robes, dress suits, sun dresses and Eastern folk costumes. For practical use, silk is excellent as clothing that protects from many biting insects that would ordinarily pierce clothing, such as mosquitoes and horseflies. Silk's attractive lustre and drape makes it suitable for many furnishing applications. It is used for upholstery, wall coverings, window treatments (if blended with another fiber), rugs, bedding and wall hangings. While on the decline now, due to artificial fibers, silk has had many industrial and commercial uses, such as in parachutes, bicycle tires, comforter filling and artillery gunpowder bags.
Fabrics that are often made from silk include charmeuse, habutai, chiffon, taffeta, crepe de chine, dupioni, noil, tussah, and shantung, among others.
A special manufacturing process removes the outer irritant sericin coating of the silk, which makes it suitable as non-absorbable surgical sutures. This process has also recently led to the introduction of specialist silk underclothing for people with eczema where it can significantly reduce it. New uses and manufacturing techniques have been found for silk for making everything from disposable cups to drug delivery systems and holograms. To produce 1 kg of silk, 104 kg of mulberry leaves must be eaten by 3000 silkworms. It takes about 5000 silkworms to make a pure silk kimono. The production of silk is called sericulture. The major silk producers are China (54%) and India (14%). Other statistics:
Cultivation.
Silk moths lay eggs on specially prepared paper. The eggs hatch and the caterpillars (silkworms) are fed fresh mulberry leaves. After about 35 days and 4 moltings, the caterpillars are 10,000 times heavier than when hatched and are ready to begin spinning a cocoon. A straw frame is placed over the tray of caterpillars, and each caterpillar begins spinning a cocoon by moving its head in a pattern. Two glands produce liquid silk and force it through openings in the head called spinnerets. Liquid silk is coated in sericin, a water-soluble protective gum, and solidifies on contact with the air. Within 2–3 days, the caterpillar spins about 1 mile of filament and is completely encased in a cocoon. The silk farmers then kill most caterpillars by heat, leaving some to metamorphose into moths to breed the next generation of caterpillars. Harvested cocoons are then soaked in boiling water to soften the sericin holding the silk fibers together in a cocoon shape. The fibers are then unwound to produce a continuous thread. Since a single thread is too fine and fragile for commercial use, anywhere from three to ten strands are spun together to form a single thread of silk.
Animal rights.
As the process of harvesting the silk from the cocoon kills the larvae, sericulture has been criticized by animal welfare and rights activists. Mohandas Gandhi was critical of silk production based on the Ahimsa philosophy which led to promotion of cotton and Ahimsa silk, a type of wild silk made from the cocoons of wild and semi-wild silk moths. Due to the fact that silk cultivation inherently kills the silkworms, People for the Ethical Treatment of Animals (PETA) urges people not to buy silk items.

</doc>
<doc id="51511" url="https://en.wikipedia.org/wiki?curid=51511" title="Prince">
Prince

A prince is a male ruler, monarch, or member of a monarch's or former monarch's family. "Prince" is also a hereditary title in the nobility of some European states. The feminine equivalent is a princess. The English word derives, via the French word "prince", from the Latin noun "princeps", from "primus" (first) + "capio" (to seize), meaning "the chief, most distinguished, ruler, prince".
Historical background.
(older Latin *prīsmo-kaps, literally "the one who takes the first [place/position]"), became the usual title of the informal leader of the Roman senate some centuries before the transition to empire, the "princeps senatus".
Emperor Augustus established the formal position of monarch on the basis of "principate", not "dominion". He also tasked his grandsons as summer rulers of the city when most of the government were on holiday in the country or attending religious rituals, and, for that task, granted them the title of "princeps".
The title has generic and substantive meanings:
Prince as generic for ruler.
The original, but now less common use of the word, originated in the application of the Latin word "princeps", from late Roman law, and the classical system of government that eventually gave way to the European feudal society. In this sense, a prince is a ruler of a territory which is sovereign, or quasi-sovereign, i.e., exercising substantial (though not all) prerogatives associated with monarchs of independent nations, as was common, for instance, within the historical boundaries of the Holy Roman Empire. In medieval and Early Modern Europe, there were as many as two hundred such territories, especially in Italy, Germany, and Gaelic Ireland. In this sense, "prince" is used of any and all rulers, regardless of actual title or precise rank. This is the Renaissance use of the term found in Niccolò Machiavelli's famous work, "Il Principe".
As a title, by the end of the medieval era, "prince" was borne by rulers of territories that were either substantially smaller than or exercised fewer of the rights of sovereignty than did emperors and kings. A lord of even a quite small territory might come to be referred to as a "prince" before the 13th century, either from translations of a native title into the Latin "princeps" (as for the hereditary ruler of Wales), or when the lord's territory was allodial. The lord of an allodium owned his lands and exercised prerogatives over the subjects in his territory absolutely, owing no feudal homage or duty as a vassal to a liege lord, nor being subject to any higher jurisdiction. Most small territories designated as principalities during feudal eras were allodial, e.g. the Princedom of Dombes.
Lords who exercised lawful authority over territories and people within a feudal hierarchy were also sometimes regarded as "princes" in the general sense, especially if they held the rank of count or higher. This is attested in some surviving styles for e.g., British earls, marquesses, and dukes are still addressed by the Crown on ceremonial occasions as "high and noble princes" (cf. Royal and noble styles).
In parts of the Holy Roman Empire in which primogeniture did not prevail (i.e. Germany), all legitimate agnates had an equal right to the family's hereditary titles. While this meant that offices, such as emperor, king, and elector could only be legally occupied by one dynast at a time, holders of such other titles as duke, margrave, landgrave, count palatine, and prince could only differentiate themselves by adding the name of their appanage to the family's original title. Not only did this tend to proliferate unwieldy titles (e.g. Princess Katherine of Anhalt-Zerbst, Karl, Count Palatine of Zweibrücken-Neukastell-Kleeburg and Prince Christian Charles of Schleswig-Holstein-Sonderburg-Plön-Norburg), but as agnatic primogeniture gradually became the norm in the Holy Roman Empire by the end of the 18th century, another means of distinguishing the monarch from other members of his dynasty became necessary. Gradual substitution of the title of "Prinz" for the monarch's title of "Fürst" occurred, and became customary for cadets in all German dynasties except in the grand duchies of Mecklenburg and Oldenburg. Both "Prinz" and "Fürst" are translated into English as "prince", but they reflect not only different but mutually exclusive concepts.
This distinction had evolved before the 18th century (Liechtenstein long remained an exception, cadets and females using "Fürst/Fürstin" into the 19th century) for dynasties headed by a "Fürst" in Germany. The custom spread through the Continent to such an extent that a renowned imperial general who belonged to a cadet branch of a reigning ducal family, remains best known to history by the generic dynastic title, "Prince Eugene of Savoy". Note that the princely title was used as a prefix to his Christian name, which also became customary.
Cadets of France's other "princes étrangers" affected similar usage under the Bourbon kings. Always facing the scepticism of Saint-Simon and like-minded courtiers, these quasi-royal aristocrats' assumption of the princely title as a personal, rather than territorial, designation encountered some resistance. In writing "Histoire Genealogique et Chonologique", Père Anselme accepts that, by the end of the 17th century, the heir apparent to the House of La Tour d'Auvergne's sovereign duchy bears the title, "prince de Bouillon", but he would record in 1728 that the heir's "La Tour" cousin, the Count of Oliergues, is ""known as" the Prince Frederick" (""dit" le prince Frédéric").
The post-medieval rank of "gefürsteter Graf" (princely count) embraced but elevated the German equivalent of the intermediate French, English and Spanish nobles. In the Holy Roman Empire, these nobles rose to dynastic status by preserving from the Imperial crown ("de jure" after the Peace of Westphalia in 1648) the exercise of such sovereign prerogatives as the minting of money; the muster of military troops and the right to wage war and contract treaties; local judicial authority and constabular enforcement; and the habit of inter-marrying with sovereign dynasties. By the 19th century, cadets of a "Fürst" would become known as "Prinzen".
Prince of the blood.
The husband of a queen regnant is usually titled "prince consort" or simply "prince", whereas the wives of male monarchs take the female equivalent (e.g., empress, queen) of their husband's title. In Brazil, Portugal and Spain, however, the husband of a female monarch was accorded the masculine equivalent of her title (e.g., emperor, king), at least after he fathered her heir. In previous epochs, husbands of queens regnant were often deemed entitled to the crown matrimonial, sharing their consorts' regnal title and rank "jure uxoris",
However, in cultures which allow the ruler to have several wives (e.g., four in Islam) and/or official concubines (e.g., Imperial China, Ottoman Empire, Thailand, KwaZulu-Natal) these women, sometimes collectively referred to as a harem there are often specific rules determining their relative hierarchy and a variety of titles, which may distinguish between those whose offspring can be in line for the succession or not, or specifically who is mother to the heir to the throne. 
To complicate matters, the style "His/Her (Imperial/Royal) Highness", a prefix often accompanying the title of a dynastic prince, may be awarded/withheld separately (as a compromise or consolation prize, in some sense, e.g., Duke of Cádiz, Duchess of Windsor, Princessse de Réthy, Prince d'Orléans-Braganza).
Although the arrangement set out above is the one that is most commonly understood, there are also different systems. Depending on country, epoch, and translation, other usages of "prince" are possible.
Foreign-language titles such as Italian "principe", French "prince", German "Fürst" and "Prinz" (non-reigning descendants of a reigning monarch), Russian "knyaz", etc., are usually translated as "prince" in English.
Some princely titles are derived from those of national rulers, such as tsarevich from tsar. Other examples are (e)mirza(da), khanzada, nawabzada, sahibzada, shahzada, sultanzada (all using the Persian patronymic suffix "-zada", meaning "son, descendant"). However, some princely titles develop in unusual ways, such as adoption of a style for dynasts which is not pegged to the ruler's title, but rather continues an old tradition (e.g., "grand duke" in Romanov Russia or "archduke" in Habsburg Austria), claims dynastic succession to a lost monarchy (e.g. "prince de Tarente" for the La Trémoïlle heirs to the Neapolitan throne, or descends from a ruler whose princely title or sovereign status was not, "de jure", hereditary, but attributed to descendants as an international courtesy, (e.g., Bibesco, Poniatowski, Ypsilanti). 
Specific titles.
In some dynasties, a specific style other than prince has become customary for dynasts, such as "fils de France" in the House of Capet, and "Infante". "Infante" was borne by children of the monarch other than the heir apparent in all of the Iberian monarchies. Some monarchies used a specific princely title for their heirs, such as Prince of Asturias in Spain and Prince of Brazil in Portugal.
Sometimes a specific title is commonly used by various dynasties in a region, e.g. Mian in various of the Punjabi princely Hill States (lower Himalayan region in British India).
European dynasties usually awarded appanages to princes of the blood, typically attached to a feudal noble title, such as Britain's royal dukes, the "Dauphin" in France, the Count of Flanders in Belgium, and the Count of Syracuse in Sicily. Sometimes appanage titles were princely, e.g. Prince of Achaia (Courtenay), "prince de Condé" (Bourbon), Prince of Carignan (Savoy), but it was the fact that their owners were of princely "rank" rather than that they held a princely "title" which was the source of their pre-eminence.
For the often specific terminology concerning an heir apparent, see Crown Prince.
Prince as a substantive title.
Other princes derive their title not from dynastic membership as such, but from inheritance of a title named for a specific and historical territory, although the family's possession of prerogatives or properties in that territory might be long past. Such were most of the "princedoms" of France's "ancien régime", so resented for their pretentiousness in the memoirs of Saint-Simon. These included the princedoms of Arches-Charleville, Boisbelle-Henrichemont, Chalais, Château-Regnault, Guéménée, Martigues, Mercœur, Sedan, Talmond, Tingrey, and the "kingship" of Yvetot, among others.
Prince as a reigning monarch.
A prince or princess who is the head of state of a territory that has a monarchy as a form of government is a reigning prince.
Extant principalities.
The current princely monarchies include:
Micronations.
In the same tradition, some self-proclaimed monarchs of so-called micronations style themselves as princes:
Prince exercising head of state's authority.
Various monarchies provide for different modes in which princes of the dynasty can temporarily or permanently share in the style and / or office of the monarch, e.g. as regent or viceroy.
Though these offices may not be reserved legally for members of the ruling dynasty, in some traditions they are filled by dynasts, a fact which may be reflected in the style of the office, e.g. "prince-president" for Napoleon III as French head of state but not yet emperor, or "prince-lieutenant" in Luxembourg, repeatedly filled by the crown prince before the grand duke's abdication, or in form of "consortium imperii".
Some monarchies even have a practice in which the monarch can formally abdicate in favor of his heir, and yet retain a kingly title with executive power, e.g. "Maha Upayuvaraja" (Sanskrit for "Great Joint King" in Cambodia), though sometimes also conferred on powerful regents who exercised executive powers.
Non-dynastic princes.
In several countries of the European continent, such as France, prince can be an aristocratic title of someone having a high rank of nobility or as lord of a significant fief, but not ruling any actual territory and without any necessary link to the royal family, which makes it difficult to compare with the British system of royal princes.
France and the Holy Roman Empire
The kings of France started to bestow the style of prince, as a title among the nobility, from the 16th century onwards. These titles were created by elevating a "seigneurie" to the nominal status of a principality—although prerogatives of sovereignty were never conceded in the letters patent. Princely titles self-assumed by the "princes du sang and by the princes étrangers" were generally tolerated by the king and used at the royal court, outside the Parlement of Paris. These titles held no official place in the hierarchy of the nobility, but were often treated as ranking just below ducal peerages, since they were often inherited (or assumed) by ducal heirs:
This can even occur in a monarchy within which an identical but real and substantive feudal title exists, such as "Fürst" in German. An example of this is:
Spain, France and Netherlands
In other cases, such titular princedoms are created in chief of an event, such as a treaty of a victory. An example of this is:
Poland and Russia
In Poland specifically, the titles of prince dated either to the times before the Union of Lublin or were granted to Polish nobles by foreign kings, as the law in Poland forbade the king from dividing nobility by granting them hereditary titles: see The Princely Houses of Poland.
In the Russian system, "knyaz", translated as "prince", is the highest degree of official nobility. Members of older dynasties, whose realms were eventually annexed to the Russian Empire, were also accorded the title of "knyaz" — sometimes after first being allowed to use the higher title of tsarevich (e.g. the Princes Gruzinsky and Sibirsky). The many surviving branches of the Rurik dynasty used the "knyaz" title before and after they yielded sovereignty to their kinsmen, the Grand Princes of Muscovy, who became Tsars and, under the House of Romanov, Emperors of Russia.
The title of prince in various Western traditions and languages.
In each case, the title is followed (when available) by the female form and then (not always available, and obviously rarely applicable to a prince of the blood without a principality) the name of the territory associated with it, each separated by a slash. If a second title (or set) is also given, then that one is for a Prince of the blood, the first for a principality. Be aware that the absence of a separate title for a prince of the blood may not always mean no such title exists; alternatively, the existence of a word does not imply there is also a reality in the linguistic territory concerned; it may very well be used exclusively to render titles in other languages, regardless whether there is a historical link with any (which often means that linguistic tradition is adopted)
Etymologically, we can discern the following traditions (some languages followed a historical link, e.g. within the Holy Roman Empire, not their language family; some even fail to follow the same logic for certain other aristocratic titles):
The title of prince in other traditions and languages.
In Belgium, France, Italy, Japan, Portugal, Russia, Spain and Hungary the title of "prince" has also been used as the highest title of nobility (without membership in a ruling dynasty), above the title of "duke", while the same usage (then as "fürst") has occurred in Germany and Austria but then one rank below the title of "duke" and above "count".
The above is essentially the story of European, Christian dynasties and other nobility, also 'exported' to their colonial and other overseas territories and otherwise adopted by rather westernized societies elsewhere (e.g. Haiti).
Applying these essentially western concepts, and terminology, to other cultures even when they don't do so, is common but in many respects rather dubious. Different (historical, religious...) backgrounds have also begot significantly different dynastic and nobiliary systems, which are poorly represented by the 'closest' western analogy.
It therefore makes sense to treat these per civilization.
Non-Islamic Asian traditions.
China.
In ancient China, the title of prince developed from being the highest title of nobility (synonymous with duke) in the Zhou Dynasty, to five grades of princes (not counting the sons and grandsons of the emperor) by the time of the fall of the Qing Dynasty.The Chinese word for prince "Wang" (, literally, King) as Chinese believe the emperor "Huang Di" () is the ruler of all kings. The most accurate translations of the English word "prince" are "Huang Zi" (, lit. Son of the Emperor) or "Wang Zi" (, lit. Son of the King).
Japan.
In Japan, the title "Kōshaku" () was used as the highest title of "Kazoku" ( Japanese modern nobility) before the present constitution. "Kōshaku", however, is more commonly translated as "Duke" to avoid confusion with the following royal ranks in the Imperial Household: "Shinnō" ( literally, Prince of the Blood); "Naishinnō" ( lit., Princess of the Blood in her own right); and "Shinnōhi" lit., Princess Consort); or "Ō" ( lit., Prince); "Jyo-Ō" ( lit., Princess (in her own right)); and "Ōhi" ( lit., Princess Consort). The former is the higher title of a male member of the Imperial family while the latter is the lower.
Korea.
In Joseon Dynasty, the title "Prince" was used for the king's male-line descendant. Prince translated generally into three divisions. The king's legitimate son used title "daegun" (대군, 大君, literally Grand Prince). A son born of a concubine and king's great-great-grand son used title "gun" (군, 君, lit. Prince). But the title of "gun" wasn't limited to royal family. Instead, it was often granted as an honorary and non-hereditory title.
Presently, as noble titles are no more granted or even recognized by the people, the English word "Prince" is usually translated as "wangja" (왕자, 王子, lit. king's son), only rendering the usage in foreign royal families. Princes and principalities in continental Europe are almost always confused with dukes and duchies, both being translated as "gong" (공, 公, lit. duke) and "gongguk" (공국, 公國, lit. duchy).
Sri Lanka.
The title 'Prince' was used for the King's son in Sinhalese generation in Sri Lanka.
India.
"See" princely states for the often particular, mainly Hindu titles in former British India, including modern Pakistan, Bangladesh, Burma, and Nepal.
Indochina.
"See" Cambodia, Vietnam, and Laos
Philippines.
"See" Principalia, the sultanates of Maguindanao and Sulu.
Thailand.
In Thailand (formerly Siam), the title of Prince was divided into three classes depending on the rank of their mothers. Those who were born of a king and had a royal mother (a queen or princess consort) are titled "Chaofa Chai" (: literally, "Male Celestial Lord"). Those born of a king and a commoner, or children of Chaofas, are tilted "Phra Ong Chao" (พระองค์เจ้า). The children of Phra Ong Chaos are titled "Mom Chao" (หม่อมเจ้า), abbreviated as M.C. (or ม.จ.).
African traditions.
A Western model was sometimes copied by emancipated colonial regimes (e.g. Bokassa I's short-lived Central-African Empire in Napoleonic fashion). Otherwise, most of the styles for members of ruling families do not lend themselves well to English translation. Nonetheless, in general the princely style has gradually replaced the colonialist title of "chief", which does not particularly connote dynastic rank to Westerners, e.g. Swazi Royal Family and Zulu Royal Family. Due to this, the nominally ministerial chiefly titles that still exist (e.g.: the Yoruba "Oloye") are usually viewed as little more than the equivalents of the British knighthood, of little dynastic consequence except as a means of passively honouring the supporters of a monarch who is himself probably more contemporary in his styling.
The title of prince in religion.
In states with an element of theocracy, this can affect princehood in several ways, such as the style of the ruler (e.g. with a secondary title meaning son or servant of a named divinity), but also the mode of succession (even reincarnation and recognition).
Furthermore, certain religious offices may be considered of princely rank, and/or imply comparable temporal rights.
The Prince-Popes, Pope, Hereditary Prince-Cardinals, Cardinals,Prince-Lord Bishops, Prince Bishops, Lord Bishops, Prince-Provost, and Prince-abbots are referred to as Princes of the Church. Also in Christianity, Jesus Christ is sometimes referred to as the "Prince of Peace". Other titles for Jesus Christ are "Prince of Princes", "Prince of the Covenant", and "Prince of the Kings of the Earth". Further, Satan is often titled the "Prince of Darkness"; and in the Christian faith he is also referred to as the "Prince of this World" and the "Prince of the Power of the Air". Another title for Satan, not as common today but apparently so in approximately 30 A.D. by the Pharisees of the day, was the title "Prince of the Devils". "Prince of Israel", "Prince of the Angels", and "Prince of Light" are titles given to the Archangel Michael. Some Christian churches also believe that since all Christians, like Jesus Christ, are children of God, then they too are princes and princesses of Heaven. Saint Peter, a disciple of Jesus, is also known as the Prince of the Apostles.

</doc>
<doc id="51512" url="https://en.wikipedia.org/wiki?curid=51512" title="Etiquette">
Etiquette

Etiquette ( or , ) is a code of behavior that delineates expectations for social behavior according to contemporary conventional norms within a society, social class, or group.
The French word "étiquette", literally signifying a tag or label, was used in a modern sense in English around 1750. Etiquette has changed and evolved over the years.
History.
In the 3rd millennium BC, Ptahhotep wrote "The Maxims of Ptahhotep". The Maxims were conformist precepts extolling such civil virtues as truthfulness, self-control and kindness towards one's fellow beings. Learning by listening to everybody and knowing that human knowledge is never perfect are a leitmotif. Avoiding open conflict wherever possible should not be considered weakness. Stress is placed on the pursuit of justice, although it is conceded that it is a god's command that prevails in the end. Some of the maxims refer to one's behaviour when in the presence of the great, how to choose the right master and how to serve him. Others teach the correct way to lead through openness and kindness. Greed is the base of all evil and should be guarded against, while generosity towards family and friends is deemed praiseworthy.
Confucius (551–479 BC) was a Chinese teacher, editor, politician, and philosopher whose philosophy emphasized personal and governmental morality, correctness of social relationships, justice and sincerity.
Louis XIV (1638–1718) "transformed a royal hunting lodge in Versailles, a village 25 miles southwest of the capital, into one of the largest palaces in the world, officially moving his court and government there in 1682. It was against this awe-inspiring backdrop that Louis tamed the nobility and impressed foreign dignitaries, using entertainment, ceremony and a highly codified system of etiquette to assert his supremacy.”
Politeness.
During the Enlightenment era, a self-conscious process of the imposition of polite norms and behaviours became a symbol of being a genteel member of the upper class. Upwardly mobile middle class bourgeoisie increasingly tried to identify themselves with the elite through their adopted artistic preferences and their standards of behaviour. They became preoccupied with precise rules of etiquette, such as when to show emotion, the art of elegant dress and graceful conversation and how to act courteously, especially with women. Influential in this new discourse was a series of essays on the nature of politeness in a commercial society, penned by the philosopher Lord Shaftesbury in the early 18th century. Shaftesbury defined politeness as the art of being pleasing in company:
Periodicals, such as "The Spectator", founded as a daily publication by Joseph Addison and Richard Steele in 1711, gave regular advice to its readers on how to conform to the etiquette required of a polite gentleman. Its stated goal was "to enliven morality with wit, and to temper wit with morality...to bring philosophy out of the closets and libraries, schools and colleges, to dwell in clubs and assemblies, at tea-tables and coffeehouses" It provided its readers with educated, topical talking points, and advice in how to carry on conversations and social interactions in a polite manner.
The allied notion of 'civility' - referring to a desired social interaction which valued sober and reasoned debate on matters of interest - also became an important quality for the 'polite classes'. Established rules and procedures for proper behaviour as well as etiquette conventions, were outlined by gentlemen's clubs, such as Harrington's Rota Club. Periodicals, including "The Tatler" and "The Spectator", infused politeness into English coffeehouse conversation, as their explicit purpose lay in the reformation of English manners and morals.
It was Philip Stanhope, 4th Earl of Chesterfield who first used the word 'etiquette' in its modern meaning, in his "Letters to His Son on the Art of Becoming a Man of the World and a Gentleman". This work comprised over 400 letters written from 1737 or 1738 and continuing until his son's death in 1768, and were mostly instructive letters on various subjects. The letters were first published by his son's widow Eugenia Stanhope in 1774. Chesterfield endeavoured to decouple the issue of manners from conventional morality, arguing that mastery of etiquette was an important weapon for social advancement. The Letters were full of elegant wisdom and perceptive observation and deduction. Chesterfield epitomised the restraint of polite 18th-century society, writing, for instance, in 1748:
By the Victorian era, etiquette had developed into an exceptionally complicated system of rules, governing everything from the proper method for writing letters and using cutlery to the minutely regulated interactions between different classes and gender.
Manners.
Manners is a term usually preceded by the word good or bad to indicate whether or not a behavior is socially acceptable. Every culture adheres to a different set of manners, although a lot of manners are cross‐culturally common. Manners are a subset of social norms which are informally enforced through self-regulation and social policing and publicly performed. They enable human ‘ultrasociality’ by imposing self-restraint and compromise on regular, everyday actions.
Sociology perspectives.
In his book The Civilizing Process, Norbert Elias argued that manners arose as a product of group living and persist as a way of maintaining social order. He theorized that manners proliferated during the Renaissance in response to the development of the ‘absolute state’ – the progression from small group living to the centralization of power by the state. Elias believed that the rituals associated with manners in the Court Society of England during this period were closely bound with social status. To him, manners demonstrate an individual’s position within a social network and act as a means by which the individual can negotiate that position.
Petersen and Lupton argue that manners helped reduce the boundaries between the public sphere and the private sphere and gave rise to "“a highly reflective self, a self who monitors his or her behavior with due regard for others with whom he or she interacts socially.”" They explain that that; "“The public behavior of "individuals came to signify their social standing, a means of presenting the self and of evaluating others and thus the control of the outward self was"
"vital.”" From this perspective, manners are seen not just as a means of displaying one’s social status, but also as a
means of maintaining social
boundaries relative to class and identity.
Pierre Bourdieu’s notion of "‘habitus’" can also contribute to the understanding of manners. The habitus, he explains, is a set of "‘dispositions’" that are
neither self‐determined, nor pre‐determined, by external environmental factors. They tend to operate at a subconscious level and are "“inculcated through experience and explicit teaching”" and produced and reproduced by social interactions. Manners, in this view, are likely to be a central part of the "‘dispositions’" which guide an individual’s ability to make socially compliant behavioral decisions.
Anthropology perspectives.
Anthropologists concern themselves primarily with detailing cultural variances and differences in "‘ways of seeing’." Theorists such as Mary Douglas have
claimed that each culture’s unique set of manners, behaviors and rituals enable the local cosmology to remain ordered and free from those things that may pollute or
defile it. In particular, she suggests that ideas of pollution and disgust are attached to the margins of socially acceptable behavior to curtail such actions and
maintain"“ the assumptions by which experience is controlled.”"
Evolutionary biology perspectives.
Evolutionary biology looks at the origin of behavior and the motivation behind it. Charles Darwin analyzed the remarkable universality of facial responses to disgust, shame and other complex emotions. Having identified the same behavior in young infants and blind individuals he concluded that these responses are not learned but innate. According to Val Curtis, the development of these responses was concomitant with the development of manners behavior. For Curtis, manners play an evolutionary role in the prevention of disease. This assumes that those who were hygienic, polite to others and most able to benefit from their membership within a cultural group, stand the best
chance of survival and reproduction.
Catherine Cottrell and Steven Neuberg explore how our behavioral responses to ‘otherness’ may enable the preservation of manners and norms. They suggest that the foreignness or unfamiliarity we experience when interacting with different cultural groups for the first time, may partly serve an evolutionary function: "“Group living surrounds one with individuals able to physically harm fellow group members, to spread contagious disease, or to “free ride” on their efforts. A commitment to sociality thus carries a risk: If threats such as these are left unchecked, the costs of sociality will quickly exceed its benefits. Thus, to maximize the returns on group "living, individual group members should be attuned to others’ features or behaviors.”"
Thus, people who possess similar traits, common to the group, are to be trusted, whereas those who do not are to be considered as ‘others’ and treated with suspicion or even exclusion. Curtis argues that selective pressure borne out of a shift towards communal living would have resulted in individuals being shunned from the group for hygiene lapses or uncooperative behavior. This would have led to people avoiding actions that might result in embarrassment or others being disgusted. Joseph Henrich and Robert Boyd developed a
model to demonstrate this process at work. They explain natural selection has favored the acquisition of genetically transmitted learning mechanisms that increase an individual’s chance of acquiring locally adaptive behavior. They hypothesize that: "“Humans possess a reliably developing neural encoding that compels them both to punish individuals who violate group norms (common beliefs or practices) and punish individuals who do not punish norm violators.”" From this approach, manners are a means of mitigating undesirable behavior and fostering the benefits of in‐group cooperation.
Types.
Curtis also specifically outlines three manner categories; hygiene, courtesy and cultural norms, each of which help to account for the multifaceted role manners play in society.
These categories are based on the outcome rather than the motivation of manners behavior and individual manner behaviors may fit in to 2 or more categories.
Hygiene Manners – are any manners which affect disease transmission. They are likely to be taught at an early age, primarily through parental discipline, positive behavioral
enforcement of continence with bodily fluids (such as toilet training), and the avoidance or removal of items that pose a disease risk for children. It is expected that,
by adulthood, hygiene manners are so entrenched in one’s behavior that they become second nature. Violations are likely to elicit disgust responses.
Courtesy Manners – demonstrate one’s ability to put the interests of others before oneself; to display self‐control and good intent for the purposes of being trusted
in social interactions. Courtesy manners help to maximize the benefits of group living by regulating social interaction. Disease avoidance behavior can sometimes be compromised in the performance of courtesy manners. They may be taught in the same way as hygiene manners but are likely to also be learned through direct,
indirect (i.e. observing the interactions of others) or imagined (i.e. through the executive functions of the brain) social interactions. The learning of courtesy manners may take place
at an older age than hygiene manners, because individuals must have at least some means of communication and some awareness of self and social positioning.
The violation of courtesy manners most commonly results in social disapproval from peers.
Cultural Norm Manners – typically demonstrate one’s identity within a specific socio‐cultural group. Adherence to cultural norm manners allows for the demarcation of socio‐cultural identities and the creation of boundaries which inform who is to be trusted or who is to be deemed as ‘other’. Cultural norm manners are learnt through the enculturation and routinisation of ‘the familiar’ and through exposure to ‘otherness’ or those who are identified as foreign or different. Transgressions and non‐adherence to cultural norm manners commonly result in alienation. Cultural norms, by their very nature, have a high level of between‐group variability but are likely to be common to all those who identify with a given group identity.
Rules of etiquette encompass most aspects of social interaction in any society, though the term itself is not commonly used. A rule of etiquette may reflect an underlying ethical code, or it may reflect a person's fashion or status. Rules of etiquette are usually unwritten, but aspects of etiquette have been codified from time to time.
Books.
Erasmus of Rotterdam published his book "On Good Manners for Boys" in 1530. Amid his advice for young children on fidgeting, yawning, bickering and scratching he highlights that a core tenet of manners is the ability to “"readily ignore the faults of others but avoid falling short yourself"”.
In centuries since then, many authors have tried to collate manners or etiquette guide books. One of the most famous of these was Emily Post who began to document etiquette in 1922. She described her work as detailing the “"trivialities"” of desirable everyday conduct but also provided descriptions of appropriate conduct for key life events such as baptisms, weddings and funerals. She later established an institute which continues to provide updated advice on how to negotiate modern day society with good manners and decorum. The most recent edition of her book provides advice on such topics as when it is acceptable to ‘unfriend’ someone on Facebook and who is entitled to which armrest when flying. Etiquette books such as these as well as those by Amy Vanderbilt, Hartley, Judith Martin, and Sandi Toksvig outline suggested behaviours for a range of social interactions. However, all note that to be a well-mannered person one must not merely read their books but be able to employ good manners fluidly in any situation that may arise.
Western office and business.
The etiquette of business is the set of written and unwritten rules of conduct that make social interactions run more smoothly. Office etiquette in particular applies to coworker interaction, excluding interactions with external contacts such as customers and suppliers. When conducting group meetings in the United States, the assembly might follow "Robert's Rules of Order", if there are no other company policies to control a meeting.
These rules are often echoed throughout an industry or economy. For instance, 49% of employers surveyed in 2005 by the American National Association of Colleges and Employers found that non-traditional attire would be a "strong influence" on their opinion of a potential job candidate. Business Etiquette at companies such as IBM influence global business etiquette and professional standards.
Both office and business etiquette overlap considerably with basic tenets of netiquette, the social conventions for using computer networks.
Business etiquette can vary significantly in different countries, which is invariably related to their culture. For example: A notable difference between Chinese and Western business etiquette is conflict handling. Chinese businesses prefer to look upon relationship management to avoid conflicts - stemming from a culture that heavily relies on "guanxi" (personal connections) - while the west leaves resolution of conflict to the interpretations of law through contracts and lawyers.
Adjusting to foreign etiquettes is a major complement of culture shock, providing a market for manuals. Other resources include business and diplomacy institutions, available only in certain countries such as the UK.
In 2011, a group of etiquette experts and international business group formed a non-profit organization called IITTI (pronounced as ""ET"") to help human resource (HR) departments of multinationals in measuring the etiquette skills of prospective new employees during the recruitment process by standardizing image and etiquette examination, similar to what ISO does for industrial process measurements.
Etiquette in retail is sometimes summarized as "The customer is always right." 
International.
Europe.
European etiquette is not uniform. Even within the regions of Europe, etiquette may not be uniform: within a single country there may be differences in customs, especially where there are different linguistic groups, as in Switzerland where there are French, German, and Italian speakers.
Japan.
The Japanese are very formal. Moments of silence are far from awkward. Smiling does not always mean that the individual is expressing pleasure. Business cards are to be handed out formally following this procedure: Hand card with writing facing upwards; bow when giving and receiving the card; grasp it with both hands; read it carefully; and put it in a prominent place. The Japanese feel a “Giri,” an obligation to reciprocate a gesture of kindness. They also rely on an innate sense of right and wrong.
Kenya.
Kenyans believe that their tribal identity is very important. Kenyans are also very nationalistic. Kenyans rarely prefer to be alone, and are usually very friendly and welcoming of guests. Kenyans are very family-oriented.
Cultural differences.
Etiquette is dependent on culture; what is excellent etiquette in one society may shock another. Etiquette evolves within culture. The Dutch painter Andries Both shows that the hunt for head lice ("illustration, right"), which had been a civilized grooming occupation in the early Middle Ages, a bonding experience that reinforced the comparative rank of two people, one groomed the other, one was the subject of the groomer, had become a peasant occupation by 1630. The painter portrays the familiar operation matter-of-factly, without the disdain this subject would have received in a 19th-century representation.
Etiquette can vary widely between different cultures and nations. For example, in Hausa culture, eating while standing may be seen as offensively casual and ill-omened behavior, insulting the host and showing a lack of respect for the scarcity of food—the offense is known as "eating with the devil" or "committing "santi"." In China, a person who takes the last item of food from a common plate or bowl without first offering it to others at the table may be seen as a glutton who is insulting the host's generosity. Traditionally, if guests do not have leftover food in front of them at the end of a meal, it is to the dishonour of the host. In the United States of America, a guest is expected to eat all of the food given to them, as a compliment to the quality of the cooking. However, it is still considered polite to offer food from a common plate or bowl to others at the table.
In such rigid hierarchal cultures as Korea and Japan, alcohol helps to break down the strict social barrier between classes. It allows for a hint of informality to creep in. It is traditional for host and guest to take turns filling each other's cups and encouraging each other to gulp it down. For someone who does not consume alcohol (except for religious reasons), it can be difficult escaping the ritual of the social drink.
Etiquette is a topic that has occupied writers and thinkers in all sophisticated societies for millennia, beginning with a behavior code by Ptahhotep, a vizier in ancient Egypt's Old Kingdom during the reign of the Fifth Dynasty king Djedkare Isesi (c. 2414–2375 BC). All known literate civilizations, including ancient Greece and Rome, developed rules for proper social conduct. Confucius included rules for eating and speaking along with his more philosophical sayings.
Early modern conceptions of what behavior identifies a "gentleman" were codified in the 16th century, in a book by Baldassare Castiglione, "Il Cortegiano" ("The Courtier"); its codification of expectations at the court of Urbino remained in force in its essentials until World War I. Louis XIV established an elaborate and rigid court ceremony, but distinguished himself from the high bourgeoisie by continuing to eat, stylishly and fastidiously, with his fingers. An important book about etiquette is "Il Galateo" by Giovanni della Casa; in fact, in Italian, etiquette is generally called "galateo" (or "etichetta" or "protocollo").
In the American colonies, Benjamin Franklin and George Washington wrote codes of conduct for young gentlemen. The immense popularity of advice columns and books by Letitia Baldrige and Miss Manners shows the currency of this topic. Even more recently, the rise of the Internet has necessitated the adaptation of existing rules of conduct to create Netiquette, which governs the drafting of e-mail, rules for participating in an online forum, and so on.
In Germany, many books dealing with etiquette, especially dining, dressing etc., are called "the Knigge", named after Adolph Freiherr Knigge who wrote the book "Über den Umgang mit Menschen" ("On Human Relations") in the late 18th century. However, this book is about good manners and also about the social state of its time, but not about etiquette.
Etiquette may be wielded as a social weapon. The outward adoption of the superficial mannerisms of an in-group, in the interests of social advancement rather than a concern for others, is considered by many a form of snobbery, lacking in virtue.
See also.
Etiquette and language
Etiquette and society
Worldwide etiquette

</doc>
<doc id="51513" url="https://en.wikipedia.org/wiki?curid=51513" title="Arrow">
Arrow

An arrow is a shafted projectile that is shot with a bow. It predates recorded history and is common to most cultures.
An arrow usually consists of a shaft with an arrowhead attached to the front end, with fletchings and a nock at the other.
History.
The oldest evidence of stone-tipped projectiles, which may or may not have been propelled by a bow (c.f. atlatl), dating to c. 64,000 years ago, were found in Sibudu Cave, South Africa.
The oldest evidence of the use of bows to shoot arrows dates to about 10,000 years ago; it is based on pinewood arrows found in the Ahrensburg valley north of Hamburg. They had shallow grooves on the base, indicating that they were shot from a bow. The oldest bow so far recovered is about 8,000 years old, found in the Holmegård swamp in Denmark.
Archery seems to have arrived in the Americas with the Arctic small tool tradition, about 4,500 years ago.
Size.
Arrow sizes vary greatly across cultures, ranging from eighteen inches to five feet (45 cm to 150 cm). However, most modern arrows are to ; most war arrows from an English ship sunk in 1545 were . Very short arrows have been used, shot through a guide attached either to the bow (an "overdraw") or to the archer's wrist (the Turkish "siper"). These may fly farther than heavier arrows, and an enemy without suitable equipment may find himself unable to return them.
Shaft.
The shaft is the primary structural element of the arrow, to which the other components are attached. Traditional arrow shafts are made from lightweight wood, bamboo or reeds, while modern shafts may be made from aluminium, carbon fibre reinforced plastic, or a combination of materials. Such shafts are typically made from an aluminium core wrapped with a carbon fibre outer.
The stiffness of the shaft is known as its spine, referring to how little the shaft bends when compressed. Hence, an arrow which bends less is said to have more spine. In order to strike consistently, a group of arrows must be similarly spined. "Center-shot" bows, in which the arrow passes through the central vertical axis of the bow riser, may obtain consistent results from arrows with a wide range of spines. However, most traditional bows are not center-shot and the arrow has to deflect around the handle in the archer's paradox; such bows tend to give most consistent results with a narrower range of arrow spine that allows the arrow to deflect correctly around the bow. Higher draw-weight bows will generally require stiffer arrows, with more spine (less flexibility) to give the correct amount of flex when shot.
GPI rating.
The weight of an arrow shaft can be expressed in GPI (Grains Per Inch). The length of a shaft in inches multiplied by its GPI rating gives the weight of the shaft in grains. For example, a shaft that is 30 inches long and has a GPI of 9.5 weighs 285 grains, or about 18 grams. This does not include the other elements of a finished arrow, so a complete arrow will be heavier than the shaft alone.
Footed arrows.
Sometimes a shaft will be made of two different types of wood fastened together, resulting in what is known as a footed arrow. Known by some as the finest of wood arrows, footed arrows were used both by early Europeans and Native Americans. Footed arrows will typically consist of a short length of hardwood near the head of the arrow, with the remainder of the shaft consisting of softwood. By reinforcing the area most likely to break, the arrow is more likely to survive impact, while maintaining overall flexibility and lighter weight.
Arrowhead.
The arrowhead or projectile point is the primary functional part of the arrow, and plays the largest role in determining its purpose. Some arrows may simply use a sharpened tip of the solid shaft, but it is far more common for separate arrowheads to be made, usually from metal, horn, or some other hard material. Arrowheads are usually separated by function:
There are two main types of broadheads used by hunters: The fixed-blade and the mechanical types. While the fixed-blade broadhead keeps its blades rigid and unmovable on the broadhead at all times, the mechanical broadhead deploys its blades upon contact with the target, its blades swinging out to wound the target. The mechanical head flies better because it is more streamlined, but has less penetration as it uses some of the kinetic energy in the arrow to deploy its blades.
Arrowheads may be attached to the shaft with a cap, a socketed tang, or inserted into a split in the shaft and held by a process called hafting. Points attached with caps are simply slid snugly over the end of the shaft, or may be held on with hot glue. Split-shaft construction involves splitting the arrow shaft lengthwise, inserting the arrowhead, and securing it using a ferrule, sinew, or wire.
Fletchings.
Fletchings are found at the back of the arrow and act as airfoils to provide a small amount of force used to stabilize the flight of the arrow. They are designed to keep the arrow pointed in the direction of travel by strongly damping down any tendency to pitch or yaw. Some cultures, for example most in New Guinea, did not use fletching on their arrows.
Fletchings are traditionally made from feathers (often from a goose or turkey) bound to the arrow's shaft, but are now often made of plastic (known as "vanes"). Historically, some arrows used for the proofing of armour used copper vanes. Flight archers may use razor blades for fletching, in order to reduce air resistance. With conventional three-feather fletching, one feather, called the "cock" feather, is at a right angle to the nock, and is normally nocked so that it will not contact the bow when the arrow is shot. Four-feather fletching is usually symmetrical and there is no preferred orientation for the nock; this makes nocking the arrow slightly easier.
Artisans who make arrows by hand are known as "fletchers," a word related to the French word for arrow, "flèche". This is the same derivation as the verb "fletch", meaning to provide an arrow with its feathers. Glue and/or thread are the main traditional methods of attaching fletchings. A "fletching jig" is often used in modern times, to hold the fletchings in exactly the right orientation on the shaft while the glue hardens.
Whenever natural fletching is used, the feathers on any one arrow must come from the same wing of the bird. The most common being the right-wing flight feathers of turkeys. The slight cupping of natural feathers requires them to be fletched with a right-twist for right wing, a left-twist for left wing. This rotation, through a combination of gyroscopic stabilization and increased drag on the rear of the arrow, helps the arrow to fly straight away. Artificial "helical" fletchings have the same effect. Most arrows will have three fletches, but some have four or even more. Fletchings generally range from two to six inches (152 mm) in length; flight arrows intended to travel the maximum possible distance typically have very low fletching, while hunting arrows with broadheads require long and high fletching to stabilize them against the aerodynamic effect of the head. Fletchings may also be cut in different ways, the two most common being "parabolic" (i.e. a smooth curved shape) and "shield" (i.e. shaped as one-half of a very narrow shield) cut.
In modern archery with screw-in points, right-hand rotation is generally preferred as it makes the points self-tighten. In traditional archery, some archers prefer a left rotation because it gets the hard (and sharp) quill of the feather farther away from the arrow-shelf and the shooter's hand.
A flu-flu is a form of fletching, normally made by using long sections of full length feathers taken from a turkey, in most cases six or more sections are used rather than the traditional three. Alternatively two long feathers can be spiraled around the end of the arrow shaft. The extra fletching generates more drag and slows the arrow down rapidly after a short distance, about 30 m or so .
Flu-Flu arrows are often used for hunting birds, or for children's archery, and can also be used to play Flu-Flu Golf.
Nocks.
The nock is a notch in the rearmost end of the arrow. It serves to keep the arrow in place on the string as the bow is being drawn. Nocks may be simple slots cut in the back of the arrow, or separate pieces made from wood, plastic, or horn that are then attached to the end of the arrow.
Modern nocks, and traditional Turkish nocks, are often constructed so as to curve around the string or even pinch it slightly, so that the arrow is unlikely to slip off. In English it is common to say "nock an arrow" when one readies a shot.
In Arab archery, there was the description of the use of "nockless arrows". In shooting at the enemies, either the Turks or the Persians, it was seen that the enemies would pick up the expended arrows and shoot them back at the Arabs. So they developed "nockless arrows", which would be useless to their foes. The bowstring would have a small ring that is tied onto the string at the proper point where the nock would normally be placed. The end of the arrow, rather than being slit for a nock, would be sharpened like an arrowhead, then the rear end of the arrow would be slipped into this ring and drawn and released as usual. Then the enemy could collect all the arrows they wanted, but they would be useless to them in shooting back. A piece of advice was in battle, to have several rings tied onto the bowstring in case one broke.

</doc>
<doc id="51518" url="https://en.wikipedia.org/wiki?curid=51518" title="Dam">
Dam

A dam is a barrier that impounds water or underground streams. Reservoirs created by dams not only suppress floods but also provide water for such activities as irrigation, human consumption, industrial use, aquaculture, and navigability. Hydropower is often used in conjunction with dams to generate electricity. A dam can also be used to collect water or for storage of water which can be evenly distributed between locations. Dams generally serve the primary purpose of retaining water, while other structures such as floodgates or levees (also known as dikes) are used to manage or prevent water flow into specific land regions.
The word "dam" can be traced back to Middle English, and before that, from Middle Dutch, as seen in the names of many old cities.
History.
Ancient dams.
Early dam building took place in Mesopotamia and the Middle East. Dams were used to control the water level, for Mesopotamia's weather affected the Tigris and Euphrates rivers.
The earliest known dam is the Jawa Dam in Jordan, northeast of the capital Amman. This gravity dam featured an originally and stone wall, supported by a earth rampart. The structure is dated to 3000 BC.
The Ancient Egyptian Sadd-el-Kafara Dam at Wadi Al-Garawi, located about south of Cairo, was long at its base and wide. The structure was built around 2800 or 2600 BC as a diversion dam for flood control, but was destroyed by heavy rain during construction or shortly afterwards. During the XIIth dynasty in the 19th century BC, the Pharaohs Senosert III, Amenemhat III and Amenmehat IV dug a canal long linking the Fayum Depression to the Nile in Middle Egypt. Two dams called Ha-Uar running east-west were built to retain water during the annual flood and then release it to surrounding lands. The lake called "Mer-wer" or Lake Moeris covered and is known today as Berkat Qaroun.
One of the engineering wonders of the ancient world was the Great Dam of Marib in Yemen. Initiated somewhere between 1750 and 1700 BC, it was made of packed earth - triangular in cross section, in length and originally high - running between two groups of rocks on either side, to which it was linked by substantial stonework. Repairs were carried out during various periods, most important around 750 BC, and 250 years later the dam height was increased to . After the end of the Kingdom of Saba, the dam fell under the control of the Ḥimyarites (~115 BC) who undertook further improvements, creating a structure high, with five spillway channels, two masonry-reinforced sluices, a settling pond, and a canal to a distribution tank. These extensive works were not actually finalized until 325 AD and allowed the irrigation of .
By the mid-late 3rd century BC, an intricate water-management system within Dholavira in modern-day India was built. The system included 16 reservoirs, dams and various channels for collecting water and storing it.
Eflatun Pınar is a Hittite dam and spring temple near Konya, Turkey. It is thought to be from the time of the Hittite empire between the 15th and 13th century BC.
The Kallanai is constructed of unhewn stone, over long, high and wide, across the main stream of the Kaveri river in Tamil Nadu, South India. The basic structure dates to the 2nd century AD and is considered one of the oldest water-diversion or water-regulator structures in the world which is still in use. The purpose of the dam was to divert the waters of the Kaveri across the fertile delta region for irrigation via canals.
Du Jiang Yan is the oldest surviving irrigation system in China that included a dam that directed waterflow. It was finished in 251 BC. A large earthen dam, made by Sunshu Ao, the prime minister of Chu (state), flooded a valley in modern-day northern Anhui province that created an enormous irrigation reservoir ( in circumference), a reservoir that is still present today.
Roman engineering.
Roman dam construction was characterized by "the Romans' ability to plan and organize engineering construction on a grand scale." Roman planners introduced the then-novel concept of large reservoir dams which could secure a permanent water supply for urban settlements over the dry season. Their pioneering use of water-proof hydraulic mortar and particularly Roman concrete allowed for much larger dam structures than previously built, such as the Lake Homs Dam, possibly the largest water barrier to that date, and the Harbaqa Dam, both in Roman Syria. The highest Roman dam was the Subiaco Dam near Rome; its record height of remained unsurpassed until its accidental destruction in 1305.
Roman engineers made routine use of ancient standard designs like embankment dams and masonry gravity dams. Apart from that, they displayed a high degree of inventiveness, introducing most of the other basic dam designs which had been unknown until then. These include arch-gravity dams, arch dams, buttress dams and multiple arch buttress dams, all of which were known and employed by the 2nd century AD (see List of Roman dams). Roman workforces also were the first to build dam bridges, such as the Bridge of Valerian in Iran.
In Iran, bridge dams such as the Band-e Kaisar were used to provide hydropower through water wheels, which often powered water-raising mechanisms. One of the first was the Roman-built dam bridge in Dezful, which could raise water 50 cubits in height for the water supply to all houses in the town. Also diversion dams were known. Milling dams were introduced which the Muslim engineers called the "Pul-i-Bulaiti". The first was built at Shustar on the River Karun, Iran, and many of these were later built in other parts of the Islamic world. Water was conducted from the back of the dam through a large pipe to drive a water wheel and watermill. In the 10th century, Al-Muqaddasi described several dams in Persia. He reported that one in Ahwaz was more than long, and that it had many water-wheels raising the water into aqueducts through which it flowed into reservoirs of the city. Another one, the Band-i-Amir dam, provided irrigation for 300 villages.
Middle Ages.
In the Netherlands, a low-lying country, dams were often applied to block rivers in order to regulate the water level and to prevent the sea from entering the marsh lands. Such dams often marked the beginning of a town or city because it was easy to cross the river at such a place, and often gave rise to the respective place's names in Dutch.
For instance the Dutch capital Amsterdam (old name "Amstelredam") started with a dam through the river Amstel in the late 12th century, and Rotterdam started with a dam through the river Rotte, a minor tributary of the Nieuwe Maas. The central square of Amsterdam, covering the original place of the 800-year-old dam, still carries the name "Dam Square" or simply "the Dam".
Industrial era.
The Romans were the first to build arch dams, where the reaction forces from the abutment stabilizes the structure from the external hydrostatic pressure, but it was only in the 19th century that the engineering skills and construction materials available were capable of building the first large-scale arch dams.
Three pioneering arch dams were built around the British Empire in the early 19th century. Henry Russel of the Royal Engineers oversaw the construction of the Mir Alam dam in 1804 to supply water to the city of Hyderabad (it is still in use today). It had a height of and consisted of 21 arches of variable span.
In the 1820s and 30s, Lieutenant-Colonel John By supervised the construction of the Rideau Canal in Canada near modern-day Ottawa and built a series of curved masonry dams as part of the waterway system. In particular, the Jones Falls Dam, built by John Redpath, was completed in 1832 as the largest dam in North America and an engineering marvel. In order to keep the water in control during construction, two sluices, artificial channels for conducting water, were kept open in the dam. The first was near the base of the dam on its east side. A second sluice was put in on the west side of the dam, about above the base. To make the switch from the lower to upper sluice, the outlet of Sand Lake was blocked off.
Hunts Creek near the city of Parramatta, Australia, was dammed in the 1850s, to cater for the demand for water from the growing population of the city. The masonry arch dam wall was designed by Lieutenant Percy Simpson who was influenced by the advances in dam engineering techniques made by the Royal Engineers in India. The dam cost £17,000 and was completed in 1856 as the first engineered dam built in Australia, and the second arch dam in the world built to mathematical specifications.
The first such dam was opened two years earlier in France. It was the first French arch dam of the industrial era, and it was built by François Zola in the municipality of Aix-en-Provence to improve the supply of water after the 1832 cholera outbreak devastated the area. After royal approval was granted in 1844, the dam was constructed over the following decade. Its construction was carried out on the basis of the mathematical results of scientific stress analysis.
The 75-miles dam near Warwick, Australia, was possibly the world's first concrete arch dam. Designed by Henry Charles Stanley in 1880 with an overflow spillway and a special water outlet, it was eventually heightened to .
In the latter half of the nineteenth century, significant advances in the scientific theory of masonry dam design were made. This transformed dam design from an art based on empirical methodology to a profession based on a rigorously applied scientific theoretical framework. This new emphasis was centered around the engineering faculties of universities in France and in the United Kingdom. William John Macquorn Rankine at the University of Glasgow pioneered the theoretical understanding of dam structures in his 1857 paper "On the Stability of Loose Earth". Rankine theory provided a good understanding of the principles behind dam design. In France, J. Augustin Tortene de Sazilly explained the mechanics of vertically faced masonry gravity dams, and Zola's dam was the first to be built on the basis of these principles.
Large dams.
The era of large dams was initiated with the construction of the Aswan Low Dam in Egypt in 1902, a gravity masonry buttress dam on the Nile River. Following their 1882 invasion and occupation of Egypt, the British began construction in 1898. The project was designed by Sir William Willcocks and involved several eminent engineers of the time, including Sir Benjamin Baker and Sir John Aird, whose firm, John Aird & Co., was the main contractor. Capital and financing were furnished by Ernest Cassel. When initially constructed between 1899 and 1902, nothing of its scale had ever been attempted; on completion, it was the largest masonry dam in the world.
The Hoover Dam is a massive concrete arch-gravity dam, constructed in the Black Canyon of the Colorado River, on the border between the US states of Arizona and Nevada between 1931 and 1936 during the Great Depression. In 1928, Congress authorized the project to build a dam that would control floods, provide irrigation water and produce hydroelectric power. The winning bid to build the dam was submitted by a consortium called Six Companies, Inc. Such a large concrete structure had never been built before, and some of the techniques were unproven. The torrid summer weather and the lack of facilities near the site also presented difficulties. Nevertheless, Six Companies turned over the dam to the federal government on 1 March 1936, more than two years ahead of schedule.
By 1997, there were an estimated 800,000 dams worldwide, some 40,000 of them over high. In 2014, scholars from the University of Oxford published a study of the cost of large dams – based on the largest existing dataset – documenting significant cost overruns for a majority of dams and questioning whether benefits typically offset costs for such dams.
Types of dams.
Dams can be formed by human agency, natural causes, or even by the intervention of wildlife such as beavers. Man-made dams are typically classified according to their size (height), intended purpose or structure.
By structure.
Based on structure and material used, dams are classified as easily created without materials, arch-gravity dams, embankment dams or masonry dams, with several subtypes.
Arch dams.
In the arch dam, stability is obtained by a combination of arch and gravity action. If the upstream face is vertical the entire weight of the dam must be carried to the foundation by gravity, while the distribution of the normal hydrostatic pressure between vertical cantilever and arch action will depend upon the stiffness of the dam in a vertical and horizontal direction. When the upstream face is sloped the distribution is more complicated. The normal component of the weight of the arch ring may be taken by the arch action, while the normal hydrostatic pressure will be distributed as described above. For this type of dam, firm reliable supports at the abutments (either buttress or canyon side wall) are more important. The most desirable place for an arch dam is a narrow canyon with steep side walls composed of sound rock. The safety of an arch dam is dependent on the strength of the side wall abutments, hence not only should the arch be well seated on the side walls but also the character of the rock should be carefully inspected.
Two types of single-arch dams are in use, namely the constant-angle and the constant-radius dam. The constant-radius type employs the same face radius at all elevations of the dam, which means that as the channel grows narrower towards the bottom of the dam the central angle subtended by the face of the dam becomes smaller. Jones Falls Dam, in Canada, is a constant radius dam. In a constant-angle dam, also known as a variable radius dam, this subtended angle is kept a constant and the variation in distance between the abutments at various levels are taken care of by varying the radii. Constant-radius dams are much less common than constant-angle dams. Parker Dam on the Colorado River is a constant-angle arch dam.
A similar type is the double-curvature or thin-shell dam. Wildhorse Dam near Mountain City, Nevada, in the United States is an example of the type. This method of construction minimizes the amount of concrete necessary for construction but transmits large loads to the foundation and abutments. The appearance is similar to a single-arch dam but with a distinct vertical curvature to it as well lending it the vague appearance of a concave lens as viewed from downstream.
The multiple-arch dam consists of a number of single-arch dams with concrete buttresses as the supporting abutments, as for example the Daniel-Johnson Dam, Québec, Canada. The multiple-arch dam does not require as many buttresses as the hollow gravity type, but requires good rock foundation because the buttress loads are heavy.
Gravity dams.
In a gravity dam, the force that holds the dam in place against the push from the water is Earth's gravity pulling down on the mass of the dam. The water presses laterally (downstream) on the dam, tending to overturn the dam by rotating about its toe (a point at the bottom downstream side of the dam). The dam's weight counteracts that force, tending to rotate the dam the other way about its toe. The designer ensures that the dam is heavy enough that the dam's weight wins that contest. In engineering terms, that is true whenever the resultant of the forces of gravity acting on the dam and water pressure on the dam acts in a line that passes upstream of the toe of the dam.
Furthermore, the designer tries to shape the dam so if one were to consider the part of dam above any particular height to be a whole dam itself, that dam also would be held in place by gravity. i.e. there is no tension in the upstream face of the dam holding the top of the dam down. The designer does this because it is usually more practical to make a dam of material essentially just piled up than to make the material stick together against vertical tension.
Note that the shape that prevents tension in the upstream face also eliminates a balancing compression stress in the downstream face, providing additional economy.
For this type of dam, it is essential to have an impervious foundation with high bearing strength.
When situated on a suitable site, a gravity dam can prove to be a better alternative to other types of dams. When built on a carefully studied foundation, the gravity dam probably represents the best developed example of dam building. Since the fear of flood is a strong motivator in many regions, gravity dams are being built in some instances where an arch dam would have been more economical.
Gravity dams are classified as "solid" or "hollow" and are generally made of either concrete or masonry. The solid form is the more widely used of the two, though the hollow dam is frequently more economical to construct. Grand Coulee Dam is a solid gravity dam and Braddock Locks & Dam is a hollow gravity dam.
Arch-gravity dams.
A gravity dam can be combined with an arch dam into an arch-gravity dam for areas with massive amounts of water flow but less material available for a purely gravity dam. The inward compression of the dam by the water reduces the lateral (horizontal) force acting on the dam. Thus, the gravitation force required by the dam is lessened, i.e. the dam does not need to be so massive. This enables thinner dams and saves resources.
Barrages.
A barrage dam is a special kind of dam which consists of a line of large gates that can be opened or closed to control the amount of water passing the dam. The gates are set between flanking piers which are responsible for supporting the water load, and are often used to control and stabilize water flow for irrigation systems.
Barrages that are built at the mouths of rivers or lagoons to prevent tidal incursions or utilize the tidal flow for tidal power are known as tidal barrages.
Embankment dams.
Embankment dams are made from compacted earth, and have two main types, rock-fill and earth-fill dams. Embankment dams rely on their weight to hold back the force of water, like gravity dams made from concrete.
Rock-fill dams.
Rock-fill dams are embankments of compacted free-draining granular earth with an impervious zone. The earth utilized often contains a high percentage of large particles, hence the term "rock-fill". The impervious zone may be on the upstream face and made of masonry, concrete, plastic membrane, steel sheet piles, timber or other material. The impervious zone may also be within the embankment in which case it is referred to as a "core". In the instances where clay is utilized as the impervious material the dam is referred to as a "composite" dam. To prevent internal erosion of clay into the rock fill due to seepage forces, the core is separated using a filter. Filters are specifically graded soil designed to prevent the migration of fine grain soil particles. When suitable material is at hand, transportation is minimized leading to cost savings during construction. Rock-fill dams are resistant to damage from earthquakes. However, inadequate quality control during construction can lead to poor compaction and sand in the embankment which can lead to liquefaction of the rock-fill during an earthquake. Liquefaction potential can be reduced by keeping susceptible material from being saturated, and by providing adequate compaction during construction. An example of a rock-fill dam is New Melones Dam in California or the Fierza Dam in Albania.
A core that is growing in popularity is asphalt concrete. The majority of such dams are built with rock and/or gravel as the main fill material. Almost 100 dams of this design have now been built worldwide since the first such dam was completed in 1962. All asphalt-concrete core dams built so far have an excellent performance record. The type of asphalt used is a viscoelastic-plastic material that can adjust to the movements and deformations imposed on the embankment as a whole, and to settlements in the foundation. The flexible properties of the asphalt make such dams especially suited in earthquake regions.
For the Moglicë Hydro Power Plant in Albania the Norwegian power company Statkraft is currently building an asphalt-core rock-fill dam. Upon completion in 2018 the 320 m long, 150 m high and 460 m wide dam is anticipated to be the world's highest of its kind.
Concrete-face rock-fill dams.
A concrete-face rock-fill dam (CFRD) is a rock-fill dam with concrete slabs on its upstream face. This design provides the concrete slab as an impervious wall to prevent leakage and also a structure without concern for uplift pressure. In addition, the CFRD design is flexible for topography, faster to construct and less costly than earth-fill dams. The CFRD concept originated during the California Gold Rush in the 1860s when miners constructed rock-fill timber-face dams for sluice operations. The timber was later replaced by concrete as the design was applied to irrigation and power schemes. As CFRD designs grew in height during the 1960s, the fill was compacted and the slab's horizontal and vertical joints were replaced with improved vertical joints. In the last few decades, the design has become popular.
Currently, the tallest CFRD in the world is the Shuibuya Dam in China which was completed in 2008.
Earth-fill dams.
Earth-fill dams, also called earthen dams, rolled-earth dams or simply earth dams, are constructed as a simple embankment of well compacted earth. A homogeneous rolled-earth dam is entirely constructed of one type of material but may contain a drain layer to collect seep water. A zoned-earth dam has distinct parts or zones of dissimilar material, typically a locally plentiful shell with a watertight clay core. Modern zoned-earth embankments employ filter and drain zones to collect and remove seep water and preserve the integrity of the downstream shell zone. An outdated method of zoned earth dam construction utilized a hydraulic fill to produce a watertight core. Rolled-earth dams may also employ a watertight facing or core in the manner of a rock-fill dam. An interesting type of temporary earth dam occasionally used in high latitudes is the frozen-core dam, in which a coolant is circulated through pipes inside the dam to maintain a watertight region of permafrost within it.
Tarbela Dam is a large dam on the Indus River in Pakistan. It is located about northwest of Islamabad, and a height of above the river bed and a reservoir size of makes it the largest earth-filled dam in the world. The principal element of the project is an embankment long with a maximum height of . The total volume of earth and rock used for the project is approximately 200 million cubic yards (152.8 million cu. meters) which makes it one of the largest man-made structures in the world.
Because earthen dams can be constructed from materials found on-site or nearby, they can be very cost-effective in regions where the cost of producing or bringing in concrete would be prohibitive.
By size.
International standards (including the International Commission on Large Dams, ICOLD) define "large dams" as higher than and "major dams" as over in height. The "Report of the World Commission on Dams" also includes in the "large" category, dams, such as barrages, which are between high with a reservoir capacity of more than .
The tallest dam in the world is the Nurek Dam in Tajikistan.
By use.
Saddle dam.
A saddle dam is an auxiliary dam constructed to confine the reservoir created by a primary dam either to permit a higher water elevation and storage or to limit the extent of a reservoir for increased efficiency. An auxiliary dam is constructed in a low spot or "saddle" through which the reservoir would otherwise escape. On occasion, a reservoir is contained by a similar structure called a dike to prevent inundation of nearby land. Dikes are commonly used for reclamation of arable land from a shallow lake. This is similar to a levee, which is a wall or embankment built along a river or stream to protect adjacent land from flooding.
Weir.
A weir (also sometimes called an "overflow dam") is a type of small overflow dam that is often used within a river channel to create an impoundment lake for water abstraction purposes and which can also be used for flow measurement or retardation.
Check dam.
A check dam is a small dam designed to reduce flow velocity and control soil erosion. Conversely, a wing dam is a structure that only partly restricts a waterway, creating a faster channel that resists the accumulation of sediment.
Dry dam.
A dry dam, also known as a flood retarding structure, is a dam designed to control flooding. It normally holds back no water and allows the channel to flow freely, except during periods of intense flow that would otherwise cause flooding downstream.
Diversionary dam.
A diversionary dam is a structure designed to divert all or a portion of the flow of a river from its natural course. The water may be redirected into a canal or tunnel for irrigation and/or hydroelectric power production.
Underground dam.
Underground dams are used to trap groundwater and store all or most of it below the surface for extended use in a localized area. In some cases they are also built to prevent saltwater from intruding into a freshwater aquifer. Underground dams are typically constructed in areas where water resources are minimal and need to be efficiently stored, such as in deserts and on islands like the Fukuzato Dam in Okinawa, Japan. They are most common in northeastern Africa and the arid areas of Brazil while also being used in the southwestern United States, Mexico, India, Germany, Italy, Greece, France and Japan.
There are two types of underground dams: a "sub-surface" and a "sand-storage" dam. A sub-surface dam is built across an aquifer or drainage route from an impervious layer (such as solid bedrock) up to just below the surface. They can be constructed of a variety of materials to include bricks, stones, concrete, steel or PVC. Once built, the water stored behind the dam raises the water table and is then extracted with wells. A sand-storage dam is a weir built in stages across a stream or wadi. It must be strong, as floods will wash over its crest. Over time, sand accumulates in layers behind the dam, which helps store water and, most importantly, prevent evaporation. The stored water can be extracted with a well, through the dam body, or by means of a drain pipe.
Tailings dam.
A tailings dam is typically an earth-fill embankment dam used to store tailings, which are produced during mining operations after separating the valuable fraction from the uneconomic fraction of an ore. Conventional water retention dams can serve this purpose, but due to cost, a tailings dam is more viable. Unlike water retention dams, a tailings dam is raised in succession throughout the life of the particular mine. Typically, a base or starter dam is constructed, and as it fills with a mixture of tailings and water, it is raised. Material used to raise the dam can include the tailings (depending on their size) along with dirt.
There are three raised tailings dam designs, the "upstream", "downstream" and "centerline", named according to the movement of the crest during raising. The specific design used is dependent upon topography, geology, climate, the type of tailings, and cost. An upstream tailings dam consists of trapezoidal embankments being constructed on top but toe to crest of another, moving the crest further upstream. This creates a relatively flat downstream side and a jagged upstream side which is supported by tailings slurry in the impoundment. The downstream design refers to the successive raising of the embankment that positions the fill and crest further downstream. A centerlined dam has sequential embankment dams constructed directly on top of another while fill is placed on the downstream side for support and slurry supports the upstream side.
Because tailings dams often store toxic chemicals from the mining process, they have an impervious liner to prevent seepage. Water/slurry levels in the tailings pond must be managed for stability and environmental purposes as well.
By material.
Steel dams.
A steel dam is a type of dam briefly experimented with around the start of the 20th century which uses steel plating (at an angle) and load-bearing beams as the structure. Intended as permanent structures, steel dams were an (arguably failed) experiment to determine if a construction technique could be devised that was cheaper than masonry, concrete or earthworks, but sturdier than timber crib dams.
Timber dams.
Timber dams were widely used in the early part of the industrial revolution and in frontier areas due to ease and speed of construction. Rarely built in modern times because of their relatively short lifespan and the limited height to which they can be built, timber dams must be kept constantly wet in order to maintain their water retention properties and limit deterioration by rot, similar to a barrel. The locations where timber dams are most economical to build are those where timber is plentiful, cement is costly or difficult to transport, and either a low head diversion dam is required or longevity is not an issue. Timber dams were once numerous, especially in the North American West, but most have failed, been hidden under earth embankments, or been replaced with entirely new structures. Two common variations of timber dams were the "crib" and the "plank".
"Timber crib dams" were erected of heavy timbers or dressed logs in the manner of a log house and the interior filled with earth or rubble. The heavy crib structure supported the dam's face and the weight of the water. Splash dams were timber crib dams used to help float logs downstream in the late 19th and early 20th centuries.
"Timber plank dams" were more elegant structures that employed a variety of construction methods utilizing heavy timbers to support a water retaining arrangement of planks.
Other types.
Cofferdams.
A cofferdam is a barrier, usually temporary, constructed to exclude water from an area that is normally submerged. Made commonly of wood, concrete, or steel sheet piling, cofferdams are used to allow construction on the foundation of permanent dams, bridges, and similar structures. When the project is completed, the cofferdam will usually be demolished or removed unless the area requires continuous maintenance. (See also causeway and retaining wall.)
Common uses for cofferdams include construction and repair of offshore oil platforms. In such cases the cofferdam is fabricated from sheet steel and welded into place under water. Air is pumped into the space, displacing the water and allowing a dry work environment below the surface.
Natural dams.
Dams can also be created by natural geological forces. Volcanic dams are formed when lava flows, often basaltic, intercept the path of a stream or lake outlet, resulting in the creation of a natural impoundment. An example would be the eruptions of the Uinkaret volcanic field about 1.8 million–10,000 years ago, which created lava dams on the Colorado River in northern Arizona in the United States. The largest such lake grew to about in length before the failure of its dam. Glacial activity can also form natural dams, such as the damming of the Clark Fork in Montana by the Cordilleran Ice Sheet, which formed the Glacial Lake Missoula near the end of the last Ice Age. Moraine deposits left behind by glaciers can also dam rivers to form lakes, such as at Flathead Lake, also in Montana (see Moraine-dammed lake).
Natural disasters such as earthquakes and landslides frequently create landslide dams in mountainous regions with unstable local geology. Historical examples include the Usoi Dam in Tajikistan, which blocks the Murghab River to create Sarez Lake. At high, it is the tallest dam in the world, including both natural and man-made dams. A more recent example would be the creation of Attabad Lake by a landslide on Pakistan's Hunza River.
Natural dams often pose significant hazards to human settlements and infrastructure. The resulting lakes often flood inhabited areas, while a catastrophic failure of the dam could cause even greater damage, such as the failure of western Wyoming's Gros Ventre landslide dam in 1927, which wiped out the town of Kelly and resulted in the deaths of six people.
Beaver dams.
Beavers create dams primarily out of mud and sticks to flood a particular habitable area. By flooding a parcel of land, beavers can navigate below or near the surface and remain relatively well hidden or protected from predators. The flooded region also allows beavers access to food, especially during the winter.
Construction elements.
Power generation plant.
, hydroelectric power, mostly from dams, supplies some 19% of the world's electricity, and over 63% of renewable energy. Much of this is generated by large dams, although China uses small-scale hydro generation on a wide scale and is responsible for about 50% of world use of this type of power.
Most hydroelectric power comes from the potential energy of dammed water driving a water turbine and generator; to boost the power generation capabilities of a dam, the water may be run through a large pipe called a penstock before the turbine. A variant on this simple model uses pumped-storage hydroelectricity to produce electricity to match periods of high and low demand, by moving water between reservoirs at different elevations. At times of low electrical demand, excess generation capacity is used to pump water into the higher reservoir. When there is higher demand, water is released back into the lower reservoir through a turbine. (For example, see Dinorwig Power Station.)
Spillways.
A spillway is a section of a dam designed to pass water from the upstream side of a dam to the downstream side. Many spillways have floodgates designed to control the flow through the spillway. There are several types of spillway. A "service spillway" or "primary spillway" passes normal flow. An "auxiliary spillway" releases flow in excess of the capacity of the service spillway. An "emergency spillway" is designed for extreme conditions, such as a serious malfunction of the service spillway. A "fuse plug spillway" is a low embankment designed to be overtopped and washed away in the event of a large flood. The elements of a fuse plug are independent free-standing blocks, set side by side which work without any remote control. They allow increasing the normal pool of the dam without compromising the security of the dam because they are designed to be gradually evacuated for exceptional events. They work as fixed weirs at times by allowing over-flow for common floods.
The spillway can be gradually eroded by water flow, including cavitation or turbulence of the water flowing over the spillway, leading to its failure. It was the inadequate design of the spillway which led to the 1889 over-topping of the South Fork Dam in Johnstown, Pennsylvania, resulting in the infamous Johnstown Flood (the "great flood of 1889").
Erosion rates are often monitored, and the risk is ordinarily minimized, by shaping the downstream face of the spillway into a curve that minimizes turbulent flow, such as an ogee curve.
Dam creation.
Common purposes.
Some of these purposes are conflicting, and the dam operator needs to make dynamic tradeoffs. For example, power generation and water supply would keep the reservoir high, whereas flood prevention would keep it low. Many dams in areas where precipitation fluctuates in an annual cycle will also see the reservoir fluctuate annually in an attempt to balance these difference purposes. Dam management becomes a complex exercise amongst competing stakeholders.
Location.
One of the best places for building a dam is a narrow part of a deep river valley; the valley sides then can act as natural walls. The primary function of the dam's structure is to fill the gap in the natural reservoir line left by the stream channel. The sites are usually those where the gap becomes a minimum for the required storage capacity. The most economical arrangement is often a composite structure such as a masonry dam flanked by earth embankments. The current use of the land to be flooded should be dispensable.
Significant other engineering and engineering geology considerations when building a dam include:
Impact assessment.
Impact is assessed in several ways: the benefits to human society arising from the dam (agriculture, water, damage prevention and power), harm or benefit to nature and wildlife, impact on the geology of an area (whether the change to water flow and levels will increase or decrease stability), and the disruption to human lives (relocation, loss of archeological or cultural matters underwater).
Environmental impact.
Reservoirs held behind dams affect many ecological aspects of a river. Rivers topography and dynamics depend on a wide range of flows, whilst rivers below dams often experience long periods of very stable flow conditions or sawtooth flow patterns caused by releases followed by no releases. Water releases from a reservoir including that exiting a turbine usually contain very little suspended sediment, and this in turn can lead to scouring of river beds and loss of riverbanks; for example, the daily cyclic flow variation caused by the Glen Canyon Dam was a contributor to sand bar erosion.
Older dams often lack a fish ladder, which keeps many fish from moving upstream to their natural breeding grounds, causing failure of breeding cycles or blocking of migration paths. Even the presence of a fish ladder does not always prevent a reduction in fish reaching the spawning grounds upstream. In some areas, young fish ("smolt") are transported downstream by barge during parts of the year. Turbine and power-plant designs that have a lower impact upon aquatic life are an active area of research.
A large dam can cause the loss of entire ecospheres, including endangered and undiscovered species in the area, and the replacement of the original environment by a new inland lake.
Large reservoirs formed behind dams have been indicated in the contribution of seismic activity, due to changes in water load and/or the height of the water table.
Dams are also found to have a role in the increase/decrease of global warming. The changing water levels in reservoirs are a source for greenhouse gases like methane. While dams and the water behind them cover only a small portion of earth's surface, they harbour biological activity that can produce large amounts of greenhouse gases.
Human social impact.
The impact on human society is also significant. Nick Cullather argues in "Hungry World: America's Cold War Battle Against Poverty in Asia" that dam construction requires the state to displace individual people in the name of the common good, and that it often leads to abuses of the masses by planners. He cites Morarji Desai, Interior Minister of India, in 1960 speaking to villagers upset about the Pong Dam, who threatened to "release the waters" and drown the villagers if they did not cooperate.
For example, the Three Gorges Dam on the Yangtze River in China is more than five times the size of the Hoover Dam (U.S.), and creates a reservoir long to be used for flood control and hydro-power generation. Its construction required the loss of over a million people's homes and their mass relocation, the loss of many valuable archaeological and cultural sites, as well as significant ecological change. During the 2010 China floods, the dam held back a what would have been a disastrous flood and the huge reservoir rose by 4m (13ft) overnight.
It is estimated that to date, 40–80 million people worldwide have been physically displaced from their homes as a result of dam construction.
Economics.
Construction of a hydroelectric plant requires a long lead time for site studies, hydrological studies, and environmental impact assessments, and are large-scale projects by comparison to traditional power generation based upon fossil fuels. The number of sites that can be economically developed for hydroelectric production is limited; new sites tend to be far from population centers and usually require extensive power transmission lines. Hydroelectric generation can be vulnerable to major changes in the climate, including variations in rainfall, ground and surface water levels, and glacial melt, causing additional expenditure for the extra capacity to ensure sufficient power is available in low-water years.
Once completed, if it is well designed and maintained, a hydroelectric power source is usually comparatively cheap and reliable. It has no fuel and low escape risk, and as an alternative energy source it is cheaper than both nuclear and wind power. It is more easily regulated to store water as needed and generate high power levels on demand compared to wind power.
Dam failure.
Dam failures are generally catastrophic if the structure is breached or significantly damaged. Routine deformation monitoring and monitoring of seepage from drains in and around larger dams is useful to anticipate any problems and permit remedial action to be taken before structural failure occurs. Most dams incorporate mechanisms to permit the reservoir to be lowered or even drained in the event of such problems. Another solution can be rock grouting – pressure pumping portland cement slurry into weak fractured rock.
During an armed conflict, a dam is to be considered as an "installation containing dangerous forces" due to the massive impact of a possible destruction on the civilian population and the environment. As such, it is protected by the rules of international humanitarian law (IHL) and shall not be made the object of attack if that may cause severe losses among the civilian population. To facilitate the identification, a protective sign consisting of three bright orange circles placed on the same axis is defined by the rules of IHL.
The main causes of dam failure include inadequate spillway capacity, piping through the embankment, foundation or abutments, spillway design error (South Fork Dam), geological instability caused by changes to water levels during filling or poor surveying (Vajont, Malpasset, Testalinden Creek dams), poor maintenance, especially of outlet pipes (Lawn Lake Dam, Val di Stava Dam collapse), extreme rainfall (Shakidor Dam), earthquakes, and human, computer or design error (Buffalo Creek Flood, Dale Dike Reservoir, Taum Sauk pumped storage plant).
A notable case of deliberate dam failure (prior to the above ruling) was the Royal Air Force 'Dambusters' raid on Germany in World War II (codenamed "Operation Chastise"), in which three German dams were selected to be breached in order to have an impact on German infrastructure and manufacturing and power capabilities deriving from the Ruhr and Eder rivers. This raid later became the basis for several films.
Since 2007, the Dutch IJkdijk foundation is developing, with an open innovation model and early warning system for levee/dike failures. As a part of the development effort, full-scale dikes are destroyed in the IJkdijk fieldlab. The destruction process is monitored by sensor networks from an international group of companies and scientific institutions.

</doc>
