<doc id="55578" url="https://en.wikipedia.org/wiki?curid=55578" title="Full employment">
Full employment

Full employment, in macroeconomics, is the level of employment rates where there is no cyclical or deficient-demand unemployment. It is defined by the majority of mainstream economists as being an acceptable level of unemployment somewhere above 0%. The discrepancy from 0% arises due to non-cyclical types of unemployment, such as frictional unemployment (there will always be people who have quit or have lost a seasonal job and are in the process of getting a new job) and structural unemployment (mismatch between worker skills and job requirements). Unemployment above 0% is seen as necessary to control inflation in capitalist economies, to keep inflation from "accelerating", i.e., from rising from year to year. This view is based on a theory centering on the concept of the Non-Accelerating Inflation Rate of Unemployment (NAIRU); in the current era, the majority of mainstream economists mean NAIRU when speaking of "full" employment. The NAIRU has also been described by Milton Friedman, among others, as the "natural" rate of unemployment. Having many names, it has also been called the structural unemployment rate.
The 20th century British economist William Beveridge stated that an unemployment rate of 3% was full employment. For the United States, economist William T. Dickens found that full-employment unemployment rate varied a lot over time but equaled about 5.5 percent of the civilian labor force during the 2000s. Recently, economists have emphasized the idea that full employment represents a "range" of possible unemployment rates. For example, in 1999, in the United States, the Organisation for Economic Co-operation and Development (OECD) gives an estimate of the "full-employment unemployment rate" of 4 to 6.4%. This is the estimated unemployment rate at full employment, plus & minus the standard error of the estimate.
The concept of full employment of labor corresponds to the concept of potential output or potential real GDP and the long run aggregate supply (LRAS) curve. In neoclassical macroeconomics, the highest sustainable level of aggregate real GDP or "potential" is seen as corresponding to a vertical LRAS curve: any increase in the demand for real GDP can only lead to rising prices in the long run, while any increase in output is temporary.
Economic concept.
What most neoclassical economists mean by "full" employment is a rate somewhat less than 100% employment. Others, such as the late James Tobin, have been accused of disagreeing, considering full employment as 0% unemployment. However, this was not Tobin's perspective in his later work.
Some see John Maynard Keynes as attacking the existence of rates of unemployment substantially above 0%:
Most readers would interpret this statement as referring to only cyclical, deficient-demand, or "involuntary" unemployment (discussed below) but not to unemployment existing as "full employment" (mismatch and frictional unemployment). This is because, writing in 1929, Keynes was discussing a period in which the unemployment rate had been persistently above most conceptions of what corresponds to full employment. That is, a situation where a tenth of the population (and thus a larger percentage of the labor force) is unemployed involves a disaster.
One major difference between Keynes and the Classical economists was that while the latter saw "full employment" as the normal state of affairs with a free-market economy (except for short periods of adjustment), Keynes saw the possibility of persistent aggregate-demand failure causing unemployment rates to exceed those corresponding to full employment. Put differently, while Classical economists saw all unemployment as "voluntary," Keynes saw the possibility that involuntary unemployment can exist when the demand for final products is low compared to potential output.
This can be seen in his later and more serious work. In his "General Theory of Employment, Interest, and Money," In chapter 2, he used a definition that should be familiar to modern macroeconomics:
The only difference from the usual definitions is that, as discussed below, most economists would add skill/location mismatch or structural unemployment as existing at full employment. More theoretically,Keynes had two main definitions of full employment, which he saw as equivalent. His first main definition of full employment involves the absence of "involuntary" unemployment.
Put another way, the full employment and the absence of involuntary unemployment correspond to the case where the real wage equals the marginal cost to workers of supplying labor for hire on the market (the "marginal disutility of employment"). That is, the real wage rate and the amount of employment correspond to a point on the aggregate supply curve of labor that is assumed to exist. In contrast, a situation with less than full employment and thus involuntary unemployment would have the real wage above the supply price of labor. That is, the employment situation corresponds to a point above and to the left of the aggregate supply curve of labor: the real wage would be above the point on the aggregate supply curve of labor at the current level of employment; alternatively, the level of employment would be below the point on that supply curve at the current real wage.
Second, in chapter 3, Keynes saw full employment as a situation where "a further increase in the value of the effective demand will no longer be accompanied by any increase in output."
This means that at and above full employment, any increase in aggregate demand and employment corresponds primarily to increases in prices rather than output. Thus, full employment of labor corresponds to potential output.
Whilst full employment is often an aim for an economy, most economists see it as more beneficial to have some level of unemployment, especially of the frictional sort. In theory, this keeps the labor market flexible, allowing room for new innovations and investment. As in the NAIRU theory, the existence of some unemployment is required to avoid accelerating inflation.
Historical measurement and discussion.
For the United Kingdom, the OECD estimated the NAIRU (or structural unemployment) rate as being equal to 8.5% on average between 1988 and 1997, 5.9% between 1998 and 2007, 6.2%, 6.6%, and 6.7 in 2008, 2009, and 2010, then staying at 6.9% in 2011-2013. For the United States, they estimate it as being 5.8% on average between 1988 and 1997, 5.5% between 1998 and 2007, 5.8% in 2008, 6.0% in 2009, and then staying at 6.1% from 2010 to 2013. They also estimate the NAIRU for other countries.
The era after the 2007-2009 Great Recession shows the relevance of this concept, for example as seen in the United States. On the one hand, in 2013 Keynesian economists such as Paul Krugman of Princeton University see unemployment rates as too high relative to full employment and the NAIRU and thus favor increasing the aggregate demand for goods and services and thus labor in order reduce unemployment. On the other hand, pointing to shortages of some skilled workers, some businesspeople and Classical economists suggest that the U.S. economy is already at full employment, so that any demand stimulus will lead to nothing but rising inflation rates. One example was Narayana Kocherlakota, President of the Minneapolis Federal Reserve Bank, who has since changed his mind.
Unemployment and Inflation.
"Ideal" unemployment.
An alternative, more normative, definition (used by some labor economists) would see "full employment" as the attainment of the "ideal" unemployment rate, where the types of unemployment that reflect labor-market inefficiency (such as mismatch or structural unemployment) do not exist. That is, only some "frictional" or voluntary unemployment would exist, where workers are temporarily searching for new jobs and are thus voluntarily unemployed. This type of unemployment involves workers "shopping" for the best jobs at the same time that employers "shop" for the best possible employees to serve their needs. Its existence can allow the best possible correspondence between workers and jobs from the points of view of both employees and employers and thus promotes the economy's efficiency.
Unemployment at Beveridge Full Employment.
William Beveridge defined "full employment" as where the number of unemployed workers equaled the number of job vacancies available (while preferring that the economy be kept above that full employment level in order to allow maximum economic production). But the point is that this definition allows for "some" unemployment. To see this, assume that frictional and mismatch unemployment can be separated. At Beveridge full employment, in the case of "frictional" unemployment the number of job-seekers corresponds to an equal number of job openings: as discussed above, the unemployed are "shopping" for the best possible jobs (as long as the cost of job-search is less than the expected benefit) at the same time that employers are "shopping" for the best possible employees to fill the vacancies. Similarly,at Beveridge full employment, the number of people suffering from mismatch or structural unemployment equals the number of vacancies. The problem here is that the skills and geographical locations of the unemployed workers does not correspond to the skill requirements and locations of the vacancies. In theory, Beveridge's concept full employment corresponds to that of Keynes (discussed above).
The situation with less than full employment in Beveridge's sense results either from "Classical" unemployment or "neoclassical" unemployment or from Keynesian deficient-demand unemployment. In terms of supply and demand, Classical or neoclassical unemployment results from the actual real wage exceeding the equilibrium real wage, so that the quantity of labor demanded (and the number of vacancies) is less than the quantity of labor supplied (and the number of unemployed workers). In the "Classical" theory, the problem is that real wages are rigid, i.e., do not fall due to an excess supply of labor. In theory, this might happen because of minimum wage laws and other interference with "free markets" that prevent the attainment of market perfection. Classical economists favor making labor markets more like the ideal competitive market—and so making real wages more flexible—in order to deal with this kind of unemployment.
The neoclassical theory, in contrast, follows John Maynard Keynes and more importantly, Milton Friedman to blame inflexible "money" or "nominal" wages for low employment relative to full employment. If the money wage is fixed, the real wage is fixed for any given average price level, so that rigid money wages have the same effect as rigid real wages when the price level is given. In this case, however, real wages can be depressed (and Beveridge full employment restored) if prices rise relative to nominal wages. Alternatively, people could wait for the persistence of high unemployment to eventually cause money wages to fall. This would have the same effect, reducing real wages and increasing the quantity of labor demanded. One of the big debates in macroeconomics is whether it is better to deal with neoclassical unemployment using a small amount of inflation or by waiting for markets to adjust.
In contrast, Keynesian deficient-demand unemployment (as explained by Don Patinkin) sees a situation with less than full employment (following Beveridge's definition) as possibly prevailing "even if" the actual real wage is equal to the equilibrium real wage at full employment. The problem is that the demand for final products is limited by aggregate demand failure. Low demand for products (below potential output) implies that there is a sales constraint on the labor market to the left of equilibrium so that the quantity of labor demanded is below the amount that would be demanded if the aggregate demand for products was sufficient (what Robert Clower called the "notional" demand for labor). In terms of neoclassical theory, the prevailing real wage is less than the marginal physical product of labor in this situation. In the absence of the sales constraint, profit-maximizing employers would hire unemployed workers as long as this inequality is true, moving the labor markets toward full employment. However, the sales constraint means that the extra product of these workers "could not be sold". Thus, employers would not hire the unemployed until aggregate demand role, which would shift the sales constraint to the right, allowing more employment of labor. In this situation, Keynesians recommend policies that raise the aggregate demand for final products and thus the aggregate demand for workers.
The economic literature concerning the Phillips Curve and the NAIRU moved away from the direct examination of labor market to focus instead on the behavior of inflation rates at different unemployment rates. That is, while Beveridge and Keynes saw full-employment unemployment as where the supply of and the demand for labor were in balance, later views saw it as a threshold which should not be crossed, since low unemployment causes serious inflation.
The Phillips curves.
The theories behind the Phillips curve pointed to the inflationary costs of lowering the unemployment rate. That is, as unemployment rates fell and the economy approached full employment, the inflation rate would rise. But this theory also says that there is no single unemployment number that one can point to as the "full employment" rate. Instead, there is a trade-off between unemployment and inflation: a government might choose to attain a lower unemployment rate but would pay for it with higher inflation rates. In essence, in this view, the meaning of “full employment” is really nothing but a matter of opinion based on how the benefits of lowering the unemployment rate compare to the costs of raising the inflation rate.
Though their theory had been proposed by the Keynesian economist Abba Lerner several years before , it was the work of Milton Friedman, leader of the monetarist school of economics, and Edmund Phelps that ended the popularity of this concept of full employment. In 1968, Friedman posited the theory that full employment rate of unemployment was ‘’’unique’’’ at any given time. He called it the "natural" rate of unemployment. Instead of being a matter of opinion and normative judgment, it is something we are stuck with, even if it is unknown. As discussed further, below, inflation/unemployment trade-offs cannot be relied upon. Further, rather than trying to attain full employment, Friedman argues that policy-makers should try to keep prices stable (meaning a low or even a zero inflation rate). If this policy is sustained, he suggests that a free-market economy will gravitate to the "natural" rate of unemployment automatically.
The NAIRU.
In an effort to avoid the normative connotations of the word "natural," James Tobin (following the lead of Franco Modigliani), introduced the term the “Non-Accelerating Inflation Rate of Unemployment” (NAIRU), which corresponds to the situation where the real gross domestic product equals potential output. It has been called the "inflation threshold" unemployment rate or the inflation barrier. This concept is identical to Milton Friedman’s concept of the "natural" rate but reflects the fact that there is nothing "natural" about an economy. The level of the NAIRU depends on the degree of "supply side" unemployment, i.e., joblessness that can't be abolished by high demand. This includes frictional, mismatch, and Classical unemployment. When the actual unemployment rate equals the NAIRU, there is no cyclical or deficient-demand unemployment. That is, Keynes’ involuntary unemployment does not exist.
To understand this concept, start with the actual unemployment equal to the NAIRU. Then, assume that a country’s government and its central bank use demand-side policy to reduce the unemployment rate and then attempt to keep the rate at a specific low level: rising budget deficits or falling interest rates increase aggregate demand and raise employment of labor. Thus, the actual unemployment rate falls, as going from point A to B in the nearby graph. Unemployment then stays below the NAIRU for years or more, as at point B. In this situation, the theory behind the NAIRU posits that inflation will accelerate, i.e. get worse and worse (in the absence of wage and price controls). As the short-run Phillips curve theory indicates, higher inflation rate results from low unemployment. That is, in terms of the "trade-off" theory, low unemployment can be "bought," paid for by suffering from higher inflation. But the NAIRU theory says that this is not the whole story, so that the trade-off breaks down: a persistently higher inflation rate is eventually incorporated as higher "inflationary expectations". Then, if workers and employers expect higher inflation, it results in higher inflation, as higher money wages are passed on to consumers as higher prices. This causes the short run Phillips curve to shift to the right and upward, worsening the trade-off between inflation and unemployment. At a given unemployment rate, inflation accelerates. But if the unemployment rate rises to equal the NAIRU, we see higher inflation than before the expansionary policies, as at point C in the nearby diagram. The fall of the unemployment rate was temporary because it could not be sustained. In sum, the trade-off between inflation and unemployment cannot be relied upon to be stable: taking advantage of it causes it to disappear. This story fits the experience of the United States during the late 1960s, during which unemployment rates stayed low (below 4% of the civilian labor force) and inflation rates rose significantly.
Second, examine the other main case. Again start with the unemployment rate equal to the NAIRU. Then, either shrinking government budget deficits (or rising government surpluses) or rising real interest rates encourage higher unemployment. In this situation, the NAIRU theory says that inflation will get better (decelerate) if unemployment rates exceed the NAIRU for a long time. High unemployment leads to lower inflation, which in turn causes lower inflationary expectations and a further round of lower inflation. High unemployment causes the short-run inflation/unemployment trade-off to improve. This story fits the experience of the United States during the early 1980s (Paul Volcker's war against inflation), during which unemployment rates stayed high (at about 10% of the civilian labor force) and inflation rates fell significantly.
Finally, the NAIRU theory says that the inflation rate does not rise or fall when the unemployment equals the "natural" rate. This is where the term NAIRU is derived. In macroeconomics, the case where the actual unemployment rate equals the NAIRU is seen as the long-run equilibrium because there are no forces inside the normal workings of the economy that cause the inflation rate to rise or fall. The NAIRU corresponds to the "long-run Phillips curve". While the short-run Phillips curve is based on a constant rate of inflationary expectations, the long-run Phillips curve reflects full adjustment of inflationary expectations to the actual experience of inflation in the economy.
As mentioned above, Abba Lerner had developed a version of the NAIRU before the modern "natural" rate or NAIRU theories were developed. Unlike the currently dominant view, Lerner saw a range of "full employment" unemployment rates. Crucially, the unemployment rate depended on the economy's institution. Lerner distinguished between "high" full employment, which was the lowest sustainable unemployment under incomes policies, and "low" full employment, i.e., the lowest sustainable unemployment rate without these policies.
Further, it is possible that the value of the NAIRU depends on government policy, rather than being "natural" and unvarying. A government can attempt to make people "employable" by both positive means (e.g. using training courses) and negative means (e.g. cuts in unemployment insurance benefits). These policies do not necessarily create full employment. Instead, the point is to reduces the amount of mismatch unemployment by facilitating the linking of unemployed workers with the available jobs by training them and or subsidizing their moving to the geographic location of the jobs.
In addition, the hysteresis hypothesis says that the NAIRU does not stay the same over time—and can change due to economic policy. A persistently low unemployment rate makes it easier for those workers who are unemployed for "mismatch" reasons to move to where the jobs are and/or to attain the training necessary for the available vacancies (often by getting those jobs and receiving on-the-job training). On the other hand, high unemployment makes it more difficult for those workers to adjust, while hurting their morale, job-seeking skills, and the value of their work skills. Thus, some economists argue that British Prime Minister Margaret Thatcher's anti-inflation policies using persistently high unemployment led to higher mismatch or structural unemployment and a higher NAIRU.
Uncertainty.
Whatever the definition of full employment, it is difficult to discover exactly what unemployment rate it corresponds to. In the United States, for example, the economy saw stable inflation "despite" low unemployment during the late 1990s, contradicting most economists' estimates of the NAIRU.
The idea that the full-employment unemployment rate (NAIRU) is not a unique number has been seen in recent empirical research. Staiger, Stock, and Watson found that the range of possible values of the NAIRU (from 4.3 to 7.3% unemployment) was too large to be useful to macroeconomic policy-makers. Robert Eisner suggested that for 1956-95 there was a zone from about 5% to about 10% unemployment between the low-unemployment realm of accelerating inflation and the high-unemployment realm of disinflation. In between, he found that inflation falls with falling unemployment.
Policy.
The active pursuit of national full employment through interventionist government policies is associated with Keynesian economics and marked the postwar agenda of many Western nations, until the stagflation of the 1970s.
Australia.
Australia was the first country in the world in which full employment in a capitalist society was made official policy by its government. On May 30, 1945, The Australian Labor Party Prime Minister John Curtin and his Employment Minister John Dedman proposed a white paper in the Australian House of Representatives titled "Full Employment In Australia", the first time any government apart from totalitarian regimes had unequivocally committed itself to providing work for any person who was willing and able to work. Conditions of full employment lasted in Australia from 1941 to 1975. This had been preceded by the Harvester Judgment (1907), establishing the basic wage (a living wage); while this earlier case was overturned, it remained influential.
United States.
The United States is, as a statutory matter, committed to full employment (defined as 3% unemployment for persons aged 20 and older, 4% for persons aged 16 and over); the government is empowered to effect this goal. The relevant legislation is the Employment Act (1946), initially the "Full Employment Act," later amended in the Full Employment and Balanced Growth Act (1978). The 1946 act was passed in the aftermath of World War II, when it was feared that demobilization would result in a depression, as it had following World War I in the Depression of 1920–21, while the 1978 act was passed following the 1973–75 recession and in the midst of continuing high inflation.
The law states that full employment is one of four economic goals, in concert with growth in production, price stability, balance of trade, and budget, and that the US shall rely primarily on private enterprise to achieve these goals. Specifically, the Act is committed to an unemployment rate of no more than 3% for persons aged 20 or over and not more than 4% for persons aged 16 or over (from 1983 onwards), and the Act expressly "allows" (but does not "require") the government to create a "reservoir of public employment" to affect this level of employment. These jobs are required to be in the lower ranges of skill and pay so as to not draw the workforce away from the private sector.
However, since the passage of this Act in 1978, the US has, never achieved this level of employment on the national level, though some states have neared it or met it, nor has such a reservoir of public employment been created.
Job guarantee.
Some, particularly Post-Keynesian economists have suggested ensuring full employment via a job guarantee program, where those who are unable to find work in the private sector are employed by the government, the stock of thus employed public sector workers fulfilling the same function as the unemployed do in controlling inflation, without the human costs of unemployment.

</doc>
<doc id="55579" url="https://en.wikipedia.org/wiki?curid=55579" title="MAD (programming language)">
MAD (programming language)

MAD (Michigan Algorithm Decoder) is a programming language and compiler for the IBM 704 and later the IBM 709, IBM 7090, IBM 7040, UNIVAC 1107, UNIVAC 1108, Philco 210-211, and eventually the IBM S/370 mainframe computers. Developed in 1959 at the University of Michigan by Bernard Galler, Bruce Arden and Robert M. Graham, MAD is a variant of the ALGOL language. It was widely used to teach programming at colleges and universities during the 1960s and played a minor role in the development of CTSS, Multics, and the Michigan Terminal System computer operating systems.
The archives at the Bentley Historical Library of the University of Michigan contain reference materials on the development of MAD and MAD/I, including three linear feet of printouts with hand-written notations and original printed manuals.
MAD, MAD/I, and GOM.
There are three MAD compilers:
History.
While MAD was motivated by ALGOL 58, it does not resemble ALGOL 58 in any significant way.
Programs written in MAD included MAIL, RUNOFF, one of the first text processing systems, and several other utilities all under Compatible Time-Sharing System(CTSS). Work was done on a design for a MAD compiler for Multics, but it was never implemented.
The following is an interesting quote from "An Interview with Brian Kernighan" when he was asked "What hooked you on programming?":
MAD was quite fast compared to some of the other compilers of its day. Because a number of people were interested in using the FORTRAN language and yet wanted to obtain the speed of the MAD compiler, a system called MADTRAN (written in MAD) was developed. MADTRAN was simply a translator from FORTRAN to MAD, which then produced machine code. MADTRAN was distributed through SHARE.
MAD/I has a syntactic structure similar to ALGOL 60 together with important features from the original MAD and from PL/I. MAD/I was designed as an extensible language. It was available for use under MTS and provided many new ideas which made their way into other languages, but MAD/I compilations were slow and MAD/I never extended itself into widespread use when compared to the original 7090 MAD.
GOM is essentially the 7090 MAD language modified and extended for the 360/370 architecture with some judicious tailoring to better fit current programming practices and problems. The MTS Message System was written in GOM.
MAD, MAD Magazine, and Alfred E. Neuman.
In a pre-release version of the original MAD, as a reference to MAD's namesake, MAD magazine, when a program contained too many compile time errors the compiler would print a full-page picture of Alfred E. Neuman using ASCII art. The caption read, "See this man about your program--He might want to publish it. He never worries--but from the looks of your program, you should." This feature was not included in the final official version. However, it was included in the production version for the IBM 7040.
And Bernie Galler remembers:
"Hello, world" example.
The "hello, world" example program prints the string "Hello, world" to a terminal or screen display.
The first character of the line is treated as logical carriage control, in this example the character "0" which causes a double-spaced line to be printed.
Or, if entering all of the keywords at your keypunch is too much work, you can use contractions and the compiler will expand them in the listing:
Language elements.
MAD and GOM, but not MAD/I, are composed of the following elements:
Input format.
MAD programs are a series of statements written on punched cards, generally one statement per card, although a statement can be continued to multiple cards. Columns 1-10 contains an optional statement label, comments or remarks are flagged using the letter "R" in column 11, and columns 73-80 are unused and could contain a sequence identifier. Spaces are not significant anywhere other than within character constants. For GOM input is free form with no sequence field and lines may be up to 255 characters long; lines that start with an asterisk (*) are comments; and lines that start with a plus-sign (+) are continuation lines.
Names.
Variable names, function names, and statement labels have the same form, a letter followed by zero to five letters or digits. Function names end with a period. All names can be subscripted (the name followed by parentheses, with multiple subscripts separated by commas). For GOM names may be up to 24 characters long and may include the underscore (_) character.
Few keywords in the language are reserved words since most are longer than six letters or are surrounded by periods. There is a standard set of abbreviations which can be used to replace the longer words. These consist of the first and last letters of the keywords with an apostrophe between them, such as W'R for WHENEVER and D'N for DIMENSION.
Data types.
MAD uses the term "mode" for its data types. Five basic modes are supported:
The mode of a constant can be redefined by adding the character M followed by a single digit at the end of the constant, where 0 indicates floating point, 1 integer, 2 boolean, 3 function name, and 4 statement label.
For GOM six additional modes are added: CHARACTER, SHORT INTEGER, BYTE INTEGER, LONG INTEGER, POINTER, and DYNAMIC RECORD.
Alphabetic or character constants are stored as integers and written using the dollar sign as a delimiter ($ABCDEF$) with double dollar-signs used to enter a true dollar sign ($$$.$56 is 56 cents). Strings longer than six characters are represented using arrays.
Declaration statements.
Variables may be implicitly or explicitly declared. By default all implicitly declared variables are assumed to be floating point. The NORMAL MODE IS statement may be used to change this default.
Functions.
Function names end with a period. Internal and external functions are supported. Internal functions are compiled as part of the program in which they are used and share declarations and variables with the main program. External functions are compiled separately and do not share declarations and variables. A one statement definition of internal functions is permitted. Recursive functions are permitted, although the function must do some of the required saving and restoring work itself.
Operator definition and redefinition.
One of the most interesting features in MAD is the ability to extend the language by redefining existing operators, defining new operators, or defining new data types (modes). The definitions are made using MAD declaration statements and assembly language mnemonics included following the declaration up to the END pseudo-instruction that implement the operation.
where:
Three pre-defined packages of definitions (MATRIX, DOUBLE PRECISION, and COMPLEX) are available for inclusion in MAD source programs using the INCLUDE statement.

</doc>
<doc id="55584" url="https://en.wikipedia.org/wiki?curid=55584" title="Gout">
Gout

Gout (also known as podagra when it involves the big toe) is usually characterized by recurrent attacks of inflammatory arthritis—a red, tender, hot, and swollen joint. Pain typically comes on rapidly in less than twelve hours. The joint at the base of the big toe is affected in about half of cases. It may also result in tophi, kidney stones, or urate nephropathy.
The cause is a combination of diet and genetic factors. --> It occurs more commonly in those who eat a lot of meat, drink a lot of beer, or are overweight. --> The underlying mechanisms involves elevated levels of uric acid in the blood. --> When the uric acid crystallizes and the crystals deposit in joints, tendons, and surrounding tissues, an attack of gout occurs. --> Diagnosis may be confirmed by seeing the characteristic crystals in joint fluid or tophus. --> Blood uric acid levels may be normal during an attack.
Treatment with nonsteroidal anti-inflammatory drugs (NSAIDs), steroids, or colchicine improves symptoms. --> Once the acute attack subsides, levels of uric acid are usually lowered via lifestyle changes, and in those with frequent attacks, allopurinol or probenecid provides long-term prevention. Taking vitamin C and eating a diet high in low fat dairy products may be preventative.
Gout affects about 1 to 2% of the Western population at some point in their lives. --> It has become more common in recent decades which is believed to be due to increasing risk factors in the population, such as metabolic syndrome, longer life expectancy, and changes in diet. --> Older males are most commonly affected. Gout was historically known as "the disease of kings" or "rich man's disease". It has been recognized at least since the time of the ancient Egyptians.
Signs and symptoms.
Gout can present in a number of ways, although the most usual is a recurrent attack of acute inflammatory arthritis (a red, tender, hot, swollen joint). The metatarsal-phalangeal joint at the base of the big toe is affected most often, accounting for half of cases. Other joints, such as the heels, knees, wrists, and fingers, may also be affected. Joint pain usually begins over 2–4 hours and during the night. This is mainly due to lower body temperature. Other symptoms may rarely occur along with the joint pain, including fatigue and a high fever.
Long-standing elevated uric acid levels (hyperuricemia) may result in other symptomatology, including hard, painless deposits of uric acid crystals known as tophi. Extensive tophi may lead to chronic arthritis due to bone erosion. Elevated levels of uric acid may also lead to crystals precipitating in the kidneys, resulting in stone formation and subsequent urate nephropathy.
Cause.
The crystallization of uric acid, often related to relatively high levels in the blood, is the underlying cause of gout. This can occur for a number of reasons, including diet, genetic predisposition, or underexcretion of urate, the salts of uric acid. Underexcretion of uric acid by the kidney is the primary cause of hyperuricemia in about 90% of cases, while overproduction is the cause in less than 10%. About 10% of people with hyperuricemia develop gout at some point in their lifetimes. The risk, however, varies depending on the degree of hyperuricemia. When levels are between 415 and 530 μmol/l (7 and 8.9 mg/dl), the risk is 0.5% per year, while in those with a level greater than 535 μmol/l (9 mg/dL), the risk is 4.5% per year.
Lifestyle.
Dietary causes account for about 12% of gout, and include a strong association with the consumption of alcohol, fructose-sweetened drinks, meat, and seafood. Other triggers include physical trauma and surgery.
Studies in the early 2000s have found that other dietary factors once believed associated are, in fact, not. Specifically, moderate consumption of purine-rich vegetables (e.g. beans, peas, lentils, and spinach) are not associated with the development of gout. Neither is total consumption of protein. Alcohol consumption is strongly associated with an increased risk of gout, with wine presenting somewhat less of a risk than beer and spirits.
The consumption of coffee, vitamin C, and dairy products, as well as physical fitness, appear to decrease the risk. This is believed to be partly due to their effect in reducing insulin resistance.
Genetics.
The occurrence of gout is partly genetic, contributing to about 60% of variability in uric acid level. Three genes called "SLC2A9", "SLC22A12", and "ABCG2" have been found to be commonly associated with gout, and variations in them can approximately double the risk. Loss-of-function mutations in "SLC2A9" and "SLC22A12" cause hereditary hypouricaemia by reducing urate absorption and unopposed urate secretion. A few rare genetic disorders, including familial juvenile hyperuricemic nephropathy, medullary cystic kidney disease, phosphoribosylpyrophosphate synthetase superactivity, and hypoxanthine-guanine phosphoribosyltransferase deficiency as seen in Lesch-Nyhan syndrome, are complicated by gout.
Medical conditions.
Gout frequently occurs in combination with other medical problems. Metabolic syndrome, a combination of abdominal obesity, hypertension, insulin resistance, and abnormal lipid levels, occurs in nearly 75% of cases. Other conditions commonly complicated by gout include: polycythemia, lead poisoning, kidney failure, hemolytic anemia, psoriasis, and solid organ transplants. A body mass index greater than or equal to 35 increases a male's risk of gout threefold. Chronic lead exposure and lead-contaminated alcohol are risk factors for gout due to the harmful effect of lead on kidney function. Lesch-Nyhan syndrome is often associated with gouty arthritis.
Medication.
Diuretics have been associated with attacks of gout. However, a low dose of hydrochlorothiazide does not seem to increase the risk. Other medicines that increase the risk include niacin and aspirin (acetylsalicylic acid). The immunosuppressive drugs ciclosporin and tacrolimus are also associated with gout, the former more so when used in combination with hydrochlorothiazide.
Pathophysiology.
Gout is a disorder of purine metabolism, and occurs when its final metabolite, uric acid, crystallizes in the form of monosodium urate, precipitating and forming deposits (tophi) in joints, on tendons, and in the surrounding tissues. Microscopic tophi may be walled off by a ring of proteins, which blocks interaction of the crystals with cells, and therefore avoids inflammation. Naked crystals may break out of walled-off tophi due to minor physical trauma to the joint, medical or surgical stress, or rapid changes in uric acid levels. When they breach the tophi, they trigger a local immune-mediated inflammatory reaction, with one of the key proteins in the inflammatory cascade being interleukin 1β. An evolutionary loss of urate oxidase (uricase), which breaks down uric acid, in humans and higher primates has made this condition common.
The triggers for precipitation of uric acid are not well understood. While it may crystallize at normal levels, it is more likely to do so as levels increase. Other factors believed to be important in triggering an acute episode of arthritis include cool temperatures, rapid changes in uric acid levels, acidosis, articular hydration, and extracellular matrix proteins, such as proteoglycans, collagens, and chondroitin sulfate. The increased precipitation at low temperatures partly explains why the joints in the feet are most commonly affected. Rapid changes in uric acid may occur due to a number of factors, including trauma, surgery, chemotherapy, diuretics, and stopping or starting allopurinol. Calcium channel blockers and losartan are associated with a lower risk of gout compared to other medications for hypertension.
Diagnosis.
Gout may be diagnosed and treated without further investigations in someone with hyperuricemia and the classic podagra. Synovial fluid analysis should be done, however, if the diagnosis is in doubt. X-rays, while useful for identifying chronic gout, have little utility in acute attacks.
Synovial fluid.
A definitive diagnosis of gout is based upon the identification of monosodium urate crystals in synovial fluid or a tophus. All synovial fluid samples obtained from undiagnosed inflamed joints by arthrocentesis should be examined for these crystals. Under polarized light microscopy, they have a needle-like morphology and strong negative birefringence. This test is difficult to perform, and often requires a trained observer. The fluid must also be examined relatively quickly after aspiration, as temperature and pH affect their solubility.
Blood tests.
Hyperuricemia is a classic feature of gout, but it occurs nearly half of the time without hyperuricemia, and most people with raised uric acid levels never develop gout. Thus, the diagnostic utility of measuring uric acid level is limited. Hyperuricemia is defined as a plasma urate level greater than 420 μmol/l (7.0 mg/dl) in males and 360 μmol/l (6.0 mg/dl) in females. Other blood tests commonly performed are white blood cell count, electrolytes, kidney function, and erythrocyte sedimentation rate (ESR). However, both the white blood cells and ESR may be elevated due to gout in the absence of infection. A white blood cell count as high as 40.0×109/l (40,000/mm3) has been documented.
Differential diagnosis.
The most important differential diagnosis in gout is septic arthritis. This should be considered in those with signs of infection or those who do not improve with treatment. To help with diagnosis, a synovial fluid Gram stain and culture may be performed. Other conditions that look similar include pseudogout and rheumatoid arthritis. Gouty tophi, in particular when not located in a joint, can be mistaken for basal cell carcinoma, or other neoplasms.
Prevention.
Both lifestyle changes and medications can decrease uric acid levels. Dietary and lifestyle choices that are effective include reducing intake of food such as meat and seafood, consuming adequate vitamin C, limiting alcohol and fructose consumption, and avoiding obesity. A low-calorie diet in obese men decreased uric acid levels by 100 µmol/l (1.7 mg/dl). Vitamin C intake of 1,500 mg per day decreases the risk of gout by 45%. Coffee, but not tea, consumption is associated with a lower risk of gout. Gout may be secondary to sleep apnea via the release of purines from oxygen-starved cells. Treatment of apnea can lessen the occurrence of attacks.
Treatment.
The initial aim of treatment is to settle the symptoms of an acute attack. Repeated attacks can be prevented by different drugs used to reduce the serum uric acid levels. Tentative evidence supports the application of ice for 20 to 30 minutes several times a day to decrease pain. Options for acute treatment include nonsteroidal anti-inflammatory drugs (NSAIDs), colchicine, and steroids, while options for prevention include allopurinol, febuxostat, and probenecid. Lowering uric acid levels can cure the disease. Treatment of associated health problems is also important. Lifestyle interventions have been poorly studied. It is unclear whether dietary supplements have an effect in people with gout.
NSAIDs.
NSAIDs are the usual first-line treatment for gout, and no specific agent is significantly more or less effective than any other. Improvement may be seen within four hours, and treatment is recommended for one to two weeks. They are not recommended, however, in those with certain other health problems, such as gastrointestinal bleeding, kidney failure, or heart failure. While indometacin has historically been the most commonly used NSAID, an alternative, such as ibuprofen, may be preferred due to its better side effect profile in the absence of superior effectiveness. For those at risk of gastric side effects from NSAIDs, an additional proton pump inhibitor may be given. There is some evidence that COX-2 inhibitors may work as well as nonselective NSAIDs for acute gout attack with fewer side effects.
Colchicine.
Colchicine is an alternative for those unable to tolerate NSAIDs. At high doses, side effects (primarily gastrointestinal upset) limit its usage. At lower doses, which are still effective, it is well tolerated. Colchicine may interact with other commonly prescribed drugs, such as atorvastatin and erythromycin, among others.
Steroids.
Glucocorticoids have been found to be as effective as NSAIDs and may be used if contraindications exist for NSAIDs. --> They also lead to improvement when injected into the joint. --> A joint infection must be excluded, however, as steroids worsens this condition.
Pegloticase.
Pegloticase was approved in the USA to treat gout in 2010. It is an option for the 3% of people who are intolerant to other medications. Pegloticase is administered as an intravenous infusion every two weeks, and has been found to reduce uric acid levels in this population. It is likely useful for tophi but has a high rate of side effects.
Prophylaxis.
A number of medications are useful for preventing further episodes of gout, including xanthine oxidase inhibitor (including allopurinol and febuxostat) and uricosurics (including probenecid and sulfinpyrazone). They are not usually started until one to two weeks after an acute flare has resolved, due to theoretical concerns of worsening the attack, and are often used in combination with either an NSAID or colchicine for the first three to six months. They are not recommended until a person has had two attacks of gout, unless destructive joint changes, tophi, or urate nephropathy exist, as medications have not been found to be cost-effective until this point. Urate-lowering measures should be increased until serum uric acid levels are below 300–360 µmol/l (5.0–6.0 mg/dl), and are continued indefinitely. If these medications are being used chronically at the time of an attack, discontinuation is recommended. If levels cannot be brought below 6.0 mg/dl and there are recurrent attacks, this is deemed treatment failure or refractory gout. Overall, probenecid appears to be less effective than allopurinol.
Uricosuric medications are typically preferred if undersecretion of uric acid, as indicated by a 24-hour collection of urine results in a uric acid amount of less than 800 mg, is found. They are, however, not recommended if a person has a history of kidney stones. In a 24-hour urine excretion of more than 800 mg, which indicates overproduction, a xanthine oxidase inhibitor is preferred.
Xanthine oxidase inhibitors (including allopurinol and febuxostat) block uric acid production, and long-term therapy is safe and well tolerated, and can be used in people with decreased kidney function or urate stones, although allopurinol has caused hypersensitivity in a small number of individuals. In such cases, the alternative drug, febuxostat, has been recommended.
Prognosis.
Without treatment, an acute attack of gout usually resolves in five to seven days; however, 60% of people have a second attack within one year. Those with gout are at increased risk of hypertension, diabetes mellitus, metabolic syndrome, and kidney and cardiovascular disease, and thus are at increased risk of death. This may be partly due to its association with insulin resistance and obesity, but some of the increased risk appears to be independent.
Without treatment, episodes of acute gout may develop into chronic gout with destruction of joint surfaces, joint deformity, and painless tophi. These tophi occur in 30% of those who are untreated for five years, often in the helix of the ear, over the olecranon processes, or on the Achilles tendons. With aggressive treatment, they may dissolve. Kidney stones also frequently complicate gout, affecting between 10 and 40% of people, and occur due to low urine pH promoting the precipitation of uric acid. Other forms of chronic kidney dysfunction may occur.
Epidemiology.
Gout affects around 1–2% of the Western population at some point in their lifetimes, and is becoming more common. There were about 5.8 million people affected in 2013. Rates of gout have approximately doubled between 1990 and 2010. This rise is believed to be due to increasing life expectancy, changes in diet, and an increase in diseases associated with gout, such as metabolic syndrome and high blood pressure. A number of factors have been found to influence rates of gout, including age, race, and the season of the year. In men over the age of 30 and women over the age of 50, prevalence is 2%.
In the United States, gout is twice as likely in African American males as it is in European Americans. Rates are high among the peoples of the Pacific Islands and the Māori of New Zealand, but rare in Australian aborigines, despite a higher mean concentration of serum uric acid in the latter group. It has become common in China, Polynesia, and urban sub-Saharan Africa. Some studies have found that attacks of gout occur more frequently in the spring. This has been attributed to seasonal changes in diet, alcohol consumption, physical activity, and temperature.
History.
The word "gout" was initially used by Randolphus of Bocking, around 1200 AD. It is derived from the Latin word "gutta", meaning "a drop" (of liquid). According to the Oxford English Dictionary, this is derived from humorism and "the notion of the 'dropping' of a morbid material from the blood in and around the joints".
Gout has, however, been known since antiquity. Historically, it has been referred to as "the king of diseases and the disease of kings" or "rich man's disease". The first documentation of the disease is from Egypt in 2,600 BC in a description of arthritis of the big toe. The Greek physician Hippocrates around 400 BC commented on it in his "Aphorisms", noting its absence in eunuchs and premenopausal women. Aulus Cornelius Celsus (30 AD) described the linkage with alcohol, later onset in women, and associated kidney problems:
Again thick urine, the sediment from which is white, indicates that pain and disease are to be apprehended in the region of joints or viscera... Joint troubles in the hands and feet are very frequent and persistent, such as occur in cases of podagra and cheiragra. These seldom attack eunuchs or boys before coition with a woman, or women except those in whom the menses have become suppressed... some have obtained lifelong security by refraining from wine, mead and venery.
In 1683, Thomas Sydenham, an English physician, described its occurrence in the early hours of the morning, and its predilection for older males:
Gouty patients are, generally, either old men, or men who have so worn themselves out in youth as to have brought on a premature old age—of such dissolute habits none being more common than the premature and excessive indulgence in venery, and the like exhausting passions. The victim goes to bed and sleeps in good health. About two o'clock in the morning he is awakened by a severe pain in the great toe; more rarely in the heel, ankle or instep. The pain is like that of a dislocation, and yet parts feel as if cold water were poured over them. Then follows chills and shivers, and a little fever... The night is passed in torture, sleeplessness, turning the part affected, and perpetual change of posture; the tossing about of body being as incessant as the pain of the tortured joint, and being worse as the fit comes on.
The Dutch scientist Antonie van Leeuwenhoek first described the microscopic appearance of urate crystals in 1679. In 1848, English physician Alfred Baring Garrod identified excess uric acid in the blood as the cause of gout.
Other animals.
Gout is rare in most other animals due to their ability to produce uricase, which breaks down uric acid. Humans and other great apes do not have this ability, thus gout is common. Other animals with uricase include fish, amphibians, and most non primate mammals. The "Tyrannosaurus rex" specimen known as "Sue", however, is believed to have suffered from gout.
Research.
A number of new medications are under study for treating gout, including anakinra, canakinumab, and rilonacept. Canakinumab may result in better outcomes than a low dose of a steroid but costs five thousand times more. A recombinant uricase enzyme (rasburicase) is available; its use, however, is limited, as it triggers an autoimmune response. Less antigenic versions are in development.

</doc>
<doc id="55585" url="https://en.wikipedia.org/wiki?curid=55585" title="Walker tariff">
Walker tariff

The Walker Tariff was a set of tariff rates adopted by the United States in 1845. The Walker Tariff was enacted by the Democrats, and made substantial cuts in the high rates of the "Black Tariff" of 1842, enacted by the Whigs. It was based on a report by Secretary of the Treasury Robert J. Walker. The Walker Tariff reduced tariff rates from 32% to 25%; it coincided with Britain's repeal of the Corn Laws and led to an increase in trade. It was one of the lowest tariffs in American history.
Adoption.
Democrat James K. Polk was elected President in 1844 over Whig Party (United States) Henry Clay, a high tariff advocate.
President Polk declared that reduction of the "Black Tariff" would be the first of the "four great measures" that would define his administration. He directed Walker to work out the details. In 1846, Polk delivered Walker's tariff proposal to Congress. Walker urged its adoption in order to increase commerce between the U.S. and Britain. He also predicted that a reduction in tariff rates would stimulate trade, including imports. The result, asserted Walker, would be a net increase in customs revenue despite the reduced rates.
Congress, then controlled by Democrats, acted quickly on Walker's recommendations. Southern Democrats, who had little industry in their states, were especially supportive.
The Walker Tariff produced the nation's first standardized tariff: rather than setting fixed rates for specific items on a case-by-case basis, it established general schedules into which all goods could be classified, subject to defined ad valorem rates. The bill reduced rates across the board on most major import items save luxury goods such as tobacco and alcoholic beverages.
Impact.
The bill made moderate reductions in many tariff rates. As Walker had predicted, trade increased substantially, and net revenue collected also increased, from $30 million annually under the Black Tariff in 1845 to almost $45 million annually by 1850. It also improved relations with Britain that had soured over the Oregon boundary dispute.
It was passed along with a series of financial reforms proposed by Walker including the Warehousing Act of 1846. The 1846 tariff rates initiated a fourteen-year period of relative free trade by nineteenth century standards lasting until the high Morrill Tariff of 1861.
The Walker Tariff remained in effect until the Tariff of 1857, which used it as a base and reduced rates further.
The 1861 Morril Tariff raised the effective rate collected on dutiable imports by approximately 70%. Customs revenue from tariffs totaled $345 million from 1861 through 1865.

</doc>
<doc id="55586" url="https://en.wikipedia.org/wiki?curid=55586" title="Seneca Falls Convention">
Seneca Falls Convention

The Seneca Falls Convention was the first women's rights convention. It advertised itself as "a convention to discuss the social, civil, and religious condition and rights of woman".
Held in Seneca Falls, New York, it spanned two days over July 19–20, 1848. Attracting widespread attention, it was soon followed by other women's rights conventions, including one in Rochester, New York, two weeks later. In 1850 the first in a series of annual 
National Women's Rights Conventions met in Worcester, Massachusetts.
Female Quakers local to the area organized the meeting along with Elizabeth Cady Stanton, who was not a Quaker. They planned the event during a visit to the area by Philadelphia-based Lucretia Mott. Mott, a Quaker, was famous for her oratorical ability, which was rare during an era which women were often not allowed to speak in public.
The meeting had six sessions, included a lecture on law, a humorous presentation, and multiple discussions about the role of women in society. Stanton and the Quaker women presented two prepared documents, the Declaration of Sentiments and an accompanying list of resolutions, to be debated and modified before being put forward for signatures. A heated debate sprang up regarding women's right to vote, with many including Mott urging the removal of this concept, but Frederick Douglass argued eloquently for its inclusion, and the suffrage resolution was retained. Exactly 100 of approximately 300 attendees signed the document, mostly women.
The convention was seen by some of its contemporaries, including featured speaker Mott, as one important step among many others in the continuing effort by women to gain for themselves a greater proportion of social, civil and moral rights,
while it was viewed by others as a revolutionary beginning to the struggle by women for complete equality with men. Stanton considered the Seneca Falls Convention to be the beginning of the women's rights movement, an opinion that was echoed in the "History of Woman Suffrage", which Stanton co-wrote.
The convention's Declaration of Sentiments became "the single most important factor in spreading news of the women's rights movement around the country in 1848 and into the future", according to Judith Wellman, a historian of the convention.
By the time of the National Women's Rights Convention of 1851, the issue of women's right to vote had become a central tenet of the United States women's rights movement. These conventions became annual events until the outbreak of the American Civil War in 1861.
Background.
Reform movement.
In the decades leading up to 1848, a small number of women began to push against restrictions imposed upon them by society. A few men aided in this effort. In 1831, Reverend Charles Grandison Finney began allowing women to pray aloud in gatherings of men and women. The Second Great Awakening was challenging women's traditional roles in religion. Recalling the era in 1870, Paulina Wright Davis set Finney's decision as the beginning of the American women's reform movement.
Abolitionism.
Starting in 1832, abolitionist and journalist William Lloyd Garrison organized anti-slavery associations which encouraged the full participation of women. Garrison's ideas were not welcomed by a majority of other abolitionists, and those unwilling to include women split from him to form other abolitionist societies.
A few women began to gain fame as writers and speakers on the subject of abolition. In the 1830s, Lydia Maria Child wrote to encourage women to write a will, and Frances Wright wrote books on women's rights and social reform. The Grimké sisters published their views against slavery in the late 1830s, and they began speaking to mixed gatherings of men and women for Garrison's American Anti-Slavery Society, as did Abby Kelley. Although these women lectured primarily on the evils of slavery, the fact that a woman was speaking in public was itself a noteworthy stand for the cause of women's rights. Ernestine Rose began lecturing in 1836 to groups of women on the subject of the "Science of Government" which included the enfranchisement of women.
In 1840, at the urging of Garrison and Wendell Phillips, Lucretia Coffin Mott and Elizabeth Cady Stanton traveled with their husbands and a dozen other American male and female abolitionists to London for the first World's Anti-Slavery Convention, with the expectation that a motion put forward by Phillips to include women's participation in the convention would be controversial. In London, the proposal was rebuffed after a full day of debate; the women were allowed to listen from the gallery but not allowed to speak or vote. Mott and Stanton became friends in London and on the return voyage, and together planned to organize their own convention to further the cause of women's rights, separate from abolition concerns.
In 1842 Thomas M'Clintock and his wife Mary Ann became founding members of the Western New York Anti-Slavery Society and helped write its constitution. When he moved to Rochester in 1847, Frederick Douglass joined Amy and Isaac Post and the M'Clintocks in this Rochester-based chapter of the American Anti-Slavery Society.
Women's rights.
In 1839 in Boston, Margaret Fuller began hosting conversations, akin to French "salons", among women interested in discussing the "great questions" facing their sex. Sophia Ripley was one of the participants. In 1845, Fuller published "The Great Lawsuit", asking women to claim themselves as self-dependent.
In the 1840s, women in America were reaching out for greater control of their lives. Husbands and fathers directed the lives of women, and many doors were closed to female participation. State statutes and common law prohibited women from inheriting property, signing contracts, serving on juries and voting in elections. Women's prospects in employment were dim: they could expect only to gain a very few service-related jobs, and were paid about half of what men were paid for the same work. In Massachusetts, Brook Farm was founded by Sophia Ripley and her husband George Ripley in 1841 as an attempt to find a way in which men and women could work together, with women receiving the same compensation as men. The experiment failed.
In the fall of 1841, Elizabeth Cady Stanton gave her first public speech, on the subject of the Temperance movement, in front of 100 women in Seneca Falls. She wrote to her friend Elizabeth J. Neal that she moved both the audience and herself to tears, saying "I infused into my speech an Homeopathic dose of woman's rights, as I take good care to do in many private conversations."
Lucretia Mott met with Elizabeth Cady Stanton in Boston in 1842, and discussed again the possibility of a woman's rights convention. They talked once more in 1847, prior to Stanton moving from Boston to Seneca Falls.
Women's groups led by Lucretia Mott and Paulina Wright Davis held public meetings in Philadelphia beginning in 1846. A wide circle of abolitionists friendly to women's rights began in 1847 to discuss the possibility of holding a convention wholly devoted to women's rights. In October 1847, Lucy Stone gave her first public speech on the subject of women's rights, entitled "The Province of Women", at her brother Bowman Stone's church in Gardner, Massachusetts.
In March 1848, Garrison, the Motts, Abby Kelley Foster, Stephen Symonds Foster and others hosted an Anti-Sabbath meeting in Boston, to work toward the elimination of laws that apply only to Sunday, and to gain for the laborer more time away from toil than just one day of rest per week. Lucretia Mott and two other women were active within the executive committee, and Mott spoke to the assemblage. Lucretia Mott raised questions about the validity of blindly following religious and social tradition.
Political gains.
On April 7, 1848, in response to a citizen's petition, the New York State Assembly passed the Married Woman's Property Act, giving women the right to retain property they brought into a marriage, as well as property they acquired during the marriage. Creditors could not seize a wife's property to pay a husband's debts. Leading up to the passage of this law, in 1846, supporters issued a pamphlet, probably authored by Judge John Fine, which relied on its readers' familiarity with the United States Declaration of Independence to demand "That all are created free and equal ...", and that this idea should apply equally to the sexes. "Women, as well as men, are entitled to the full enjoyment of its practical blessings". A group of 44 married women of western New York wrote to the Assembly in March 1848, saying "your Declaration of Independence declares, that governments derive their just powers from the consent of the governed. And as women have never consented to, been represented in, or recognized by this government, it is evident that in justice no allegiance can be claimed from them ... Our numerous and yearly petitions for this most desirable object having been disregarded, we now ask your august body, to abolish all laws which hold married women more accountable for their acts than infants, idiots, and lunatics."
The General Assembly in Pennsylvania passed a similar married woman's property law a few weeks later, one which Lucretia Mott and others had championed. These progressive state laws were seen by American women as a sign of new hope for women's rights.
On June 2, 1848 in Rochester, New York, Gerrit Smith was nominated as the Liberty Party's presidential candidate. Smith was Elizabeth Cady Stanton's first cousin, and the two enjoyed debating and discussing political and social issues with each other whenever he came to visit. At the National Liberty Convention, held June 14–15 in Buffalo, New York, Smith gave a major address, including in his speech a demand for "universal suffrage in its broadest sense, females as well as males being entitled to vote." The delegates approved a passage in their party platform addressing votes for women: "Neither here, nor in any other part of the world, is the right of suffrage allowed to extend beyond one of the sexes. This universal exclusion of woman ... argues, conclusively, that, not as yet, is there one nation so far emerged from barbarism, and so far practically Christian, as to permit woman to rise up to the one level of the human family." At this convention, five votes were placed calling for Lucretia Mott to be Smith's vice-president—the first time in the United States that a woman was suggested for federal executive office.
Quaker influence.
Many members of the Religious Society of Friends, known as Quakers, made their homes in western New York state, near Seneca Falls. A particularly progressive branch lived in and around Waterloo in Seneca County, New York. These Quakers strove for marital relationships in which men and women worked and lived in equality.
The M'Clintocks came to Waterloo from a Quaker community in Philadelphia. They rented property from Richard P. Hunt, a wealthy Quaker and businessman. The M'Clintock and Hunt families opposed slavery; both participated in the free produce movement, and their houses served as stations on the Underground Railroad.
Traditional Quaker tenets held that men and women should meet separately when making religious decisions. By the 1840s, some Hicksite Quakers determined to bring women and men together in the faith as an expression of their spiritual equality. In June 1848, approximately 200 Hicksites, including the Hunts and the M'Clintocks, formed an even more radical Quaker group, known as the Yearly Meeting of Congregational Friends, or Progressive Friends. The Progressive Friends intended to further elevate the influence of women in affairs of the faith. They introduced joint meetings of men and women, giving women an equal voice.
Planning.
Lucretia and James Mott visited central and western New York in the summer of 1848 for a number of reasons, including visiting the Cattaraugus Reservation of the Seneca Nation and former slaves living in the province of Ontario, Canada. Mott was present at the meeting in which the Progressive Friends left the Hicksite Quakers. They also visited Lucretia's sister Martha Coffin Wright in Auburn, NY, where Mott also preached to prisoners at the Auburn State Penitentiary. Lucretia Mott's skill and fame as an orator drew crowds wherever she went.
Announcement.
After Quaker service on Sunday July 9, 1848, Lucretia Coffin Mott joined Mary Ann M'Clintock, Martha Coffin Wright (Mott's witty sister, several months pregnant), Elizabeth Cady Stanton and Jane Hunt for tea at the Hunt home in Waterloo. The two eldest M'Clintock daughters, Elizabeth and Mary Ann, Jr. may have accompanied their mother. Jane Hunt had given birth two weeks earlier, and was tending the baby at home. Over tea, Stanton, the only non-Quaker present, vented a lifetime's worth of pent-up frustration, her "long-accumulating discontent" about women's subservient place in society. The five women decided to hold a women's rights convention in the immediate future, while the Motts were still in the area, and drew up an announcement to run in the "Seneca County Courier". The announcement began with these words: "WOMAN'S RIGHTS CONVENTION.—A Convention to discuss the social, civil, and religious condition and rights of woman".
The notice specified that only women were invited to the first day's meetings on July 19, but both women and men could attend on the second day to hear Lucretia Mott speak, among others. On July 11, the announcement first appeared, giving readers just eight days' notice until the first day of convention. Other papers such as Douglass's "North Star" picked up the notice, printing it on July 14. The meeting place was to be the Wesleyan Methodist Chapel in Seneca Falls. Built by a congregation of abolitionists and financed in part by Richard Hunt, the chapel had been the scene of many reform lectures, and was considered the only large building in the area that would open its doors to a women's rights convention.
Declaration, grievances, resolutions.
At their home in Waterloo on Sunday, July 16, the M'Clintocks hosted a smaller planning session for the convention. Mary Ann M'Clintock and her eldest daughters, Elizabeth and Mary Ann, Jr., discussed with Stanton the makeup of the resolutions that would be presented to the convention for approval. Each woman made certain her concerns were appropriately represented among the ten resolutions that they composed. Taken together, the resolutions demanded that women should have equality in the family, education, jobs, religion, and morals. One of the M'Clintock women selected the Declaration of Independence from 1776 as a model for the declaration they wanted to make at their convention. The Declaration of Sentiments was then drafted in the parlor on a round, three-legged, mahogany tea table. Stanton changed a few words of the Declaration of Independence to make it appropriate for a statement by women, replacing "The history of the present King of Great Britain" with "The history of mankind" as the basis for "usurpations on the part of man toward woman." The women added the phrase "and women" to make "... all men "and women" are created equal ..." A list of grievances was composed to form the second part of the Declaration.
Between July 16 and July 19, at home on her own writing desk, Stanton edited the grievances and resolutions. Henry Brewster Stanton, a lawyer, politician and Stanton's husband, helped substantiate the document by locating "extracts from laws bearing unjustly against woman's property interests." On her own, Stanton added a more radical point to the list of grievances and to the resolutions: the issue of women's voting rights. To the grievances, she added "He has never permitted her to exercise her inalienable right to the elective franchise", and to the Sentiments, she added a line about man depriving woman of "the elective franchise, thereby leaving her without representation in the halls of legislation..." Stanton then copied the Declaration and resolutions into final draft form for presentation at the meeting. When he saw the addition of woman suffrage, Henry Stanton warned his wife "you will turn the proceedings into a farce." He, like most men of his day, was not in favor of women gaining voting rights. Because he intended to run for elective office, he left Seneca Falls to avoid being connected with a convention promoting such an unpopular cause. Elizabeth Cady Stanton asked her sister Harriet Cady Eaton to accompany her; Eaton brought her young son Daniel.
On July 16, Lucretia Mott sent a note to Stanton apologizing in advance for James Mott not being able to attend the first day, as he was feeling "quite unwell". Lucretia Mott wrote to say she would bring her sister, Martha Wright, and that the two women would participate in both days of the convention.
First day.
On July 19, 1848, the morning of the first day of convention, the organizing committee arrived at the Wesleyan Methodist Chapel shortly before ten o'clock on a hot, sunny day to find a crowd gathered outside and the church doors locked—an overlooked detail. Stanton's young nephew Daniel was lifted through an open window so that he could unbar the doors from the inside. Even though the first session had been announced as being exclusively for women, some young children of both sexes had been brought by their mothers, and about 40 men were there expecting to attend. The men were not turned away, but were asked to remain silent. Mary Ann M'Clintock, Jr., 26 years old, was appointed secretary, to take notes.
Morning session.
Starting at 11 o'clock, Elizabeth Cady Stanton spoke first, exhorting each woman in the audience to accept responsibility for her own life, and to "understand the height, the depth, the length, and the breadth of her own degradation." Lucretia Mott then spoke, encouraging all to take up the cause. Stanton read the Declaration of Sentiments in its entirety, then re-read each paragraph so that it could be discussed at length, and changes incorporated. The question of whether men's signatures would be sought for the Declaration was discussed, with the vote looking favorable for including men, but the motion was tabled until the following day when men themselves could participate. The first session adjourned at 2:30 p.m.
Afternoon session.
After a pause for refreshment in the 90° heat, an afternoon session began with Stanton and then Mott addressing the audience. The Declaration of Sentiments was read again and more changes were made to it. The resolutions, now numbering eleven with Stanton's addition of women's suffrage, were read aloud and discussed. Lucretia Mott read a humorous newspaper piece written by her sister Martha Wright in which Wright questioned why, after an overworked mother completed the myriad daily tasks that were required of her but not of her husband, "she" was the one upon whom written advice was "so lavishly bestowed." Twenty-seven-year-old Elizabeth W. M'Clintock then delivered a speech, and the first day's business was called to a close.
Evening speech.
In the evening, the meeting was opened to all persons, and Lucretia Mott addressed a large audience. She spoke of the progress of other reform movements and so framed for her listeners the social and moral context for the struggle for women's rights. She asked the men present to help women gain the equality they deserved. The editor of the "National Reformer", a paper in Auburn, New York, reported that Mott's extemporaneous evening speech was "one of the most eloquent, logical, and philosophical discourses which we ever listened to."
Second day.
A larger crowd attended on the second day, including more men. Amelia Bloomer arrived late and took a seat in the upstairs gallery, there being none left in the main seating area. Quaker James Mott was well enough to attend, and he chaired the morning meeting; it was still too radical a concept that a woman serve as chair in front of both men and women.
Morning session, day two.
After Mott opened the meeting, the minutes of the previous day were read, and Stanton presented the Declaration of Sentiments. In regard to the grievance "He has taken from her all right in property, even to the wages she earns," Assemblyman Ansel Bascom stood to say that he had recently been at the New York State Assembly which passed the Married Woman's Property Act. Bascom spoke at length about the property rights it secured for married women, including property acquired after marriage. Further discussion of the Declaration ensued, including comments by Frederick Douglass, Thomas and Mary Ann M'Clintock, and Amy Post; the document was adopted unanimously. The question of men's signatures was solved by having two sections of signatures, one for women followed by one for men. One hundred of the 300 present signed the Declaration of Sentiments, including 68 women and 32 men. Amelia Bloomer was one of the participants who did "not" endorse the Declaration; she was focused at that time on the temperance movement. Ansel Bascom was the most conspicuous attendee who chose not to sign the Declaration. The "National Reformer" reported that those in the audience who evidently regarded the Declaration as "too bold and ultra", including the lawyers known to be opposed to the equal rights of women, "failed to call out any opposition, except in a neighboring BAR-ROOM."
Afternoon session, day two.
At the afternoon session, the eleven resolutions were read again, and each one was voted on individually. The only one that was materially questioned was the ninth, the one Stanton had added regarding women's right to vote. It read: 
Those who opposed this resolution argued that its presence would cause the other, more rational resolutions to lose support. Others argued that only the social, civil and religious rights of women should be addressed, not the political rights. James and Lucretia Mott were against the resolution; Lucretia said to Stanton, "Why Lizzie, thee will make us ridiculous." Stanton defended the concept of woman suffrage, saying women would then be able to affect future legislation and gain further rights. Frederick Douglass, the only African American at the meeting, stood and spoke eloquently in favor; he said that he could not accept the right to vote himself as a black man if woman could not also claim that right. Douglass projected that the world would be a better place if women were involved in the political sphere. "In this denial of the right to participate in government, not merely the degradation of woman and the perpetuation of a great injustice happens, but the maiming and repudiation of one-half of the moral and intellectual power of the government of the world." Douglass's powerful words rang true with many in attendance, and the resolution passed by a large majority. Lucretia Mott spoke to end the session.
Evening session, day two.
Quaker Thomas M'Clintock served as chair for the evening session, opening it at half-past seven. The minutes were read, then Stanton spoke in defense of the many severe accusations brought against the much-abused "Lords of Creation." Following Stanton, Thomas M'Clintock read several passages from Sir William Blackstone's laws, to expose for the audience the basis of woman's current legal condition of servitude to man. Lucretia Mott stood to offer another resolution: "Resolved, That the speedy success of our cause depends upon the zealous and untiring efforts of both men and women, for the overthrow of the monopoly of the pulpit, and for the securing to woman an equal participation with men in the various trades, professions and commerce." This, the twelfth resolution, passed.
Mary Ann M'Clintock, Jr. spoke briefly, calling upon woman to arouse from her lethargy and be true to herself and her God. Douglass again rose to speak in support of the cause of woman. Lucretia Mott spoke for an hour with one of her "most beautiful and spiritual appeals". Although Lucretia Mott's reputation as a speaker drew the audience, Mott recognized Elizabeth Cady Stanton and Mary Ann M'Clintock as the "chief planners and architects" of the convention. To close the meeting, a committee was appointed to edit and publish the convention proceedings, with Amy Post, Eunice Newton Foote, Mary Ann M'Clintock, Jr., Elizabeth W. M'Clintock and Stanton serving.
Afterward.
News reports.
Local newspapers printed reports of the convention, some positive, others not. The "National Reformer" reported that the convention "forms an era in the progress of the age; it being the first convention of the kind ever held, and one whose influence shall not cease until woman is guaranteed all the rights now enjoyed by the other half of creation—Social, Civil and POLITICAL." The "Oneida Whig" did not approve of the convention, writing of the Declaration: "This bolt is the most shocking and unnatural incident ever recorded in the history of womanity. If our ladies will insist on voting and legislating, where, gentleman, will be our dinners and our elbows? Where our domestic firesides and the holes in our stockings?"
Soon, newspapers across the country picked up the story. Reactions varied widely. In Massachusetts, the "Lowell Courier" published its opinion that, with women's equality, "the lords must wash the dishes, scour up, be put to the tub, handle the broom, darn stockings." In St. Louis, Missouri, the "Daily Reveille" trumpeted that "the flag of independence has been hoisted for the second time on this side of the Atlantic." Horace Greeley in the "New York Tribune" wrote "When a sincere republican is asked to say in sober earnest what adequate reason he can give, for refusing the demand of women to an equal participation with men in political rights, he must answer, None at all. However unwise and mistaken the demand, it is but the assertion of a natural right, and such must be conceded."
Religious reaction.
Some of the ministers heading congregations in the area attended the Seneca Falls Convention, but none spoke out during the sessions, not even when comments from the floor were invited. On Sunday, July 23, many who had attended, and more who had not, attacked the Convention, the Declaration of Sentiments, and the resolutions. Women in the congregations reported to Stanton, who saw the actions of the ministers as cowardly; in their congregations, no one would be allowed to reply.
Further conventions.
Signers of the Declaration of Sentiments hoped for "a series of Conventions, embracing every part of the country" to follow their own meeting. Because of the fame and drawing power of Lucretia Mott, who would not be staying in the Upstate New York area for much longer, a regional Woman's Rights Convention was held two weeks later in Rochester, New York with Abigail Bush serving as president, and Lucretia Mott as featured speaker. In the next two years, "the infancy ... of the movement", local and state women's rights conventions were called in Ohio, Indiana, and Pennsylvania.
Charlotte Woodward, alone among all 100 signers, was the only one still alive in 1920 when the Nineteenth Amendment passed. Woodward was not well enough to vote herself.
Remembrances.
A stamp was issued in 1948 in remembrance of the Seneca Falls Convention, featuring Elizabeth Cady Stanton, Carrie Chapman Catt, and Lucretia Mott as part of a Centennial Celebration in Seneca Falls.
The Women's Rights National Historical Park was established in 1980, and covers a total of 6.83 acres (27,600 m²) of land in Seneca Falls and nearby Waterloo, New York, USA.
The park consists of four major historical properties, including the Wesleyan Methodist Church, which was the site of the Seneca Falls Convention, Elizabeth Cady Stanton's home, and the M'Clintock House, which was where the Declaration of Sentiments, resolutions, and speeches were drawn up for the Seneca Falls Convention. The Wesleyan Methodist Church and the M'Clintock House were listed on the National Register of Historic Places in 1980.
In 1998 First Lady Hillary Clinton gave a speech on the occasion of the 150th anniversary of the Seneca Falls Convention.
Historiography.
In 1870, Paulina Wright Davis authored a history of the antebellum women's rights movement, "The History of the National Woman's Rights Movement", and received approval of her account from many of the involved suffragists including Lucretia Mott and Elizabeth Cady Stanton. Davis' version gave the Seneca Falls meeting in 1848 a minor role, equivalent to other local meetings that had been held by women's groups in the late 1840s. Davis set the beginning of the national and international women's rights movement at Worcester, Massachusetts in 1850, at the National Women's Rights Convention when women from many states were invited, the influence of which was felt across the continent and in Great Britain. Stanton seemed to agree; in an address to the National Woman Suffrage Association (NWSA) convention in 1870, on the subject of the women's rights movement, she said "The movement in England, as in America, may be dated from the first National Convention, held at Worcester, Mass., October, 1850."
In 1876, in the spirit of the nation's centennial celebrations, Stanton and Susan B. Anthony decided to write a more expansive history of the women's rights movement. They invited Lucy Stone to help, but Stone declined to be part of the project; she was of the opinion that Stanton and Anthony would not fairly portray the divisive split between NWSA and American Woman Suffrage Association (AWSA). Stanton and Anthony wrote without her and, in 1881, they published the first volume of the "History of Woman Suffrage", and placed themselves at each of its most important events, marginalizing Stone's contribution.
According to Lisa Tetrault, a professor of women's history, the Seneca Falls Convention was central to their rendition of the movement's history. Neither Stanton nor Anthony had been at the 1850 convention, which was associated with their rivals. Stanton, however, had played a key role at the Seneca Falls Convention in 1848, at which Stone had not been present. In the early 1870s, Stanton and Anthony began to present Seneca Falls as the beginning of the women's rights movement, an origin story that downplayed Stone's role. Pointing out that the women's rights movement could be said to have begun even earlier than Seneca Falls, Tetrault said the "History of Woman Suffrage" dealt with these earlier events relatively briefly in its first three chapters, the first of which is titled "Preceding Causes."
In the volume, Stanton did not mention the Liberty Party's plank on woman suffrage pre-dating the Seneca Falls Convention by a month, and she did not describe the Worcester National Women's Rights Convention, organized by Stone and Davis in 1850, as the beginning of the women's rights movement. Rather, Stanton named the 1840 Anti-Slavery Convention in London as the birth of the "movement for woman's suffrage, in both England and America". She positioned the Seneca Falls meeting as her own political debut, and characterized it as the beginning of the women's rights movement, calling it "the greatest movement for human liberty recorded on the pages of history—a demand for freedom to one-half the entire race." Stanton worked to enshrine the Declaration of Sentiments as a foundational treatise in a number of ways, not the least of which was by imbuing the small, three-legged tea table upon which the first draft of it was composed an importance similar to that of Thomas Jefferson's desk upon which he wrote the Declaration of Independence. The M'Clintocks gave Stanton the table, then Stanton gave it to Susan B. Anthony on the occasion of her 80th birthday, though Anthony had no part in the Seneca Falls meeting. In keeping with Stanton's promotion of the table as an iconic relic, women's rights activists put it in a place of honor at the head of the casket at the funeral of Susan B. Anthony on March 14, 1906. Subsequently, it was displayed prominently on the stage at each of the most important suffrage meetings until 1920, even though the grievance and resolution about woman suffrage was not written on it. The table is kept at the Smithsonian Institution's National Museum of American History in Washington, D.C.
Lucretia Mott reflected in August 1848 upon the two women's rights conventions in which she had participated that summer, and assessed them no greater than other projects and missions she was involved with. She wrote that the two gatherings were "greatly encouraging; and give hope that this long neglected subject will soon begin to receive the attention that its importance demands."
Historian Gerda Lerner has pointed out that religious ideas provided a fundamental source for the "Declaration of Sentiments". Most of the women attending the convention were active in Quaker or evangelical Methodist movements. The document itself drew from writings by the evangelical Quaker Sarah Grimké to make biblical claims that God had created woman equal to man and that man had usurped God's authority by establishing "absolute tyranny" over woman. According to author Jami Carlacio, Grimké's writings opened the public's eyes to ideas like women's rights, and for the first time they were willing to question conventional notions about the subordination of women.

</doc>
<doc id="55588" url="https://en.wikipedia.org/wiki?curid=55588" title="Morrill Tariff">
Morrill Tariff

The Morrill Tariff of 1861 was an increased import tariff in the United States, adopted on March 2, 1861, during the administration of President James Buchanan, a Democrat. It was the twelfth of seventeen planks in the platform of the incoming Republican Party, which had not yet been inaugurated, and it appealed to industrialists and factory workers as a way to foster rapid industrial growth.
It was named for its sponsor, Representative Justin Smith Morrill of Vermont, who drafted it with the advice of Pennsylvania economist Henry Charles Carey. The passage of the tariff was possible because many tariff-averse Southerners had resigned from Congress after their states declared their secession. The Morrill Tariff raised rates to encourage industry and to foster high wages for industrial workers. It replaced the low Tariff of 1857, which was written to benefit the South. Two additional tariffs sponsored by Morrill, each one higher, were passed during Abraham Lincoln's administration to raise urgently needed revenue during the Civil War.
The Morrill tariff inaugurated a period of continuous trade protection in the United States, a policy that remained until the adoption of the Revenue Act of 1913 (the Underwood tariff). The schedule of the Morrill Tariff and its two successor bills were retained long after the end of the Civil War.
History.
Origins.
A high tariff to encourage the development of domestic industry had been advocated for many years, especially by the Whig Party and its long-time leader Henry Clay. They enacted such a tariff in 1842, but in 1846 the Democrats enacted the Walker Tariff, cutting tariff rates substantially. The Democrats cut rates even further in the Tariff of 1857, which was highly favorable to the South.
Meanwhile, the Whig Party broke up, and this element of the Whig program was taken up by the new Republican Party, which ran its first national ticket in 1856. Some former Whigs from the Border States and upper South remained in Congress as "Opposition", "Unionist", or "American" (Know Nothing) members; they also supported higher tariffs.
The Panic of 1857 led to calls for protectionist tariff revision. Well-known economist Henry C. Carey blamed the Panic on the Tariff of 1857. His opinion was widely circulated in the high tariff (or "protectionist") media.
Efforts to revise the tariff schedules upward began in earnest in the 35th Congress of 1857–1859. Two proposals were submitted in the House. House Ways and Means Committee chairman John S. Phelps (D-Missouri wrote the Democrats' plan, which retained most of the low rates of the 1857 Tariff, with minor revisions to stimulate revenue.
Minority Ways and Means members Morrill and Henry Winter Davis (a Maryland "American") produced the Republican proposal, an upward revision of the tariff schedule. It replaced the existing "ad valorem" tariff schedule with specific duties and drastically increased tariff rates on goods produced by popular "protected" industries, such as iron, textiles, and other manufactured goods. Economic historian Frank Taussig argued that in many cases, the substitution of specific duties was used to disguise the extent of the rate increases. Supporters of the specific rates argued that they were necessary, though, because European exporters were routinely providing their American customers with phony invoices showing lower prices for goods than were actually paid. Specific rates made such subterfuge pointless.
However, the House took no action on either tariff bill during the 35th Congress.
House actions.
When the 36th Congress met in 1859, action remained blocked by a wrangle over the Speaker of the House until 1860, when Republican William Pennington of New Jersey was elected. A pro-tariff Republican majority was appointed to Ways and Means, and John Sherman of Ohio became chairman.
The Morrill bill was passed out of committee and brought up for a floor vote near the end of first session of the Congress (December 1859 – June 1860).
The vote was on May 10, 1860; the bill passed by a vote of 105 to 64.
The vote was largely but not entirely sectional. Republicans, all from the northern states, voted 89–2 for the bill. They were joined by 7 northern Democrats from New York, New Jersey, and Pennsylvania. Five of these were "anti-Lecompton Democrats" (dissident Democrats who opposed the pro-slavery Lecompton constitution for Kansas).
14 northern Democrats voted against the bill.
In the Border States, 4 "Opposition" Representatives from Kentucky voted for it, as did its co-sponsor Winter of Maryland, a Maryland "Unionist", and a Democrat from Delaware. 8 Border state Democrats and an "American" from Missouri voted no.
35 southern Democrats and 3 Oppositionists voted against it; one Oppositionist from Tennessee voted for it.
Thus the sectional breakdown was 96–15 in the north, 7–9 in the Border, and 1–39 in the south.
There were 55 abstentions, including 13 Republicans, 12 northern Democrats, 13 southern Democrats, and 8 southern "Oppositionists" and "Americans". (The remaining Representatives were mostly "paired" with opposing Representatives who could not be present.
Senate action.
The Morrill bill was sent on to the Senate. However, the Senate was controlled by Democrats, and so the bill was bottled up in the Finance Committee, chaired by Robert M. T. Hunter of Virginia.
This insured that the Senate vote would be put off till the second session in December. It also meant that the tariff would be a prominent issue in the 1860 election.
1860 election.
The Republican party included a strong pro-tariff plank in its 1860 platform. They also sent prominent tariff advocates such as Morrill and Sherman to campaign in Pennsylvania and New Jersey, where the tariff was popular, by touting the Morrill bill. Both Democratic candidates, John C. Breckinridge and Stephen Douglas, opposed all high tariffs and protectionism in general.
Historian Reinhard H. Luthin documents the importance of the Morrill Tariff to the Republicans in the 1860 presidential election. Abraham Lincoln's record as a protectionist and support for the Morrill Tariff bill, he notes, helped him to secure support in the important electoral college state of Pennsylvania, as well as neighboring New Jersey. Lincoln carried Pennsylvania handily in November, as part of his sweep of the North.
On February 14, 1861, President-elect Lincoln told an audience in Pittsburgh that he would make a new tariff his priority in the next session if the bill did not pass by inauguration day on March 4.
Renewed Senate action.
The second session of the 36th Congress began in December 1860. At first it appeared that Hunter would keep the Morrill bill tabled until the end of the term in March.
However, in December 1860 and January 1861, seven southern states declared secession, and their low-tariff Senators withdrew. Republicans took control of the Senate in February, and Hunter lost his hold on the Finance Committee.
Meanwhile, the Treasury was in financial crisis, with less than $500,000 on hand and millions in unpaid bills. The Union urgently needed new revenue. A recent historian concludes, "the impetus for revising the tariff arose as an attempt to augment revenue, stave off 'ruin,' and address the accumulating debt."
The Morrill bill was brought to the Senate floor for a vote on February 20, and passed 25 to 14. The vote was split almost completely down party lines. It was supported by 24 Republicans and Democrat William Bigler of Pennsylvania. It was opposed by 10 Southern Democrats, 2 Northern Democrats, and 2 Far West Democrats. 12 Senators abstained, including 3 Northern Democrats, 1 California Democrat, 5 Southern Democrats, 2 Republicans, and 1 Unionist from Maryland.
There were some minor amendments related to the tariffs on tea and coffee, which required a conference committee with the House, but these were resolved and the final bill was approved by unanimous consent on March 2.
Though a Democrat himself, outgoing President James Buchanan favored the bill because of the interests of his home state, Pennsylvania. He signed the bill into law as one of his last acts in office.
Adoption and amendments.
The Morrill Tariff took effect one month after it was signed into law. Besides setting tariff rates, the bill altered and restricted the Warehousing Act of 1846.
The Morrill Tariff was drafted and passed the House before the Civil War began or was even expected, and was passed by the Senate almost unchanged. Thus it should not be considered "Civil War" legislation.
In fact, the Tariff proved to be too low for the revenue needs of the Civil War, and was quickly supplanted by the Second Morrill Tariff, or Revenue Act of 1861, later that fall.
Impact.
In its first year of operation, the Morrill Tariff increased the effective rate collected on dutiable imports by approximately 70%. In 1860 American tariff rates were among the lowest in the world and also at historical lows by 19th century standards, the average rate for 1857 through 1860 being around 17% overall ("ad valorem"), or 21% on dutiable items only. The Morrill Tariff immediately raised these averages to about 26% overall or 36% on dutiable items, and further increases by 1865 left the comparable rates at 38% and 48%. Although higher than in the immediate antebellum period, these rates were still significantly lower than between 1825 and 1830, when rates had sometimes been over 50%.
The United States needed $3 billion to pay for the immense armies and fleets raised to fight the Civil War — over $400 million just in 1862. The chief source of Federal revenue had been the tariff revenues. Therefore, Secretary of the Treasury Salmon P. Chase, though a long-time free-trader, worked with Morrill to pass a second tariff bill in summer 1861, raising rates another 10 points in order to generate more revenues. These subsequent bills were primarily revenue driven to meet the war's needs, though they enjoyed the support of protectionists such as Carey, who again assisted Morrill in the bill's drafting.
However, the tariff played only a modest role in financing the war. It was far less important than other measures, such as $2.8 billion in bond sales and some printing of Greenbacks. Customs revenue from tariffs totaled $345 million from 1861 through 1865, or 43% of all federal tax revenue, while military spending totalled $3,065 million.
Reception abroad.
The Morrill Tariff was met with intense hostility in Britain, where the free trade movement dominated public opinion. Southern diplomats and agents sought to use British ire towards the Morrill Tariff in order to garner sympathy, with the aim of obtaining British recognition for the Confederacy. The new tariff schedule heavily penalized British iron, clothing, and manufactured exports with new taxes and sparked public outcry from many British politicians. The expectation of high tax rates probably caused British shippers to hasten their deliveries before the new rates took effect in the early summer of 1861. When complaints were heard from London, Congress counterattacked. The Senate Finance Committee chairman snapped, "What right has a foreign country to make any question about what we choose to do?"
When the American Civil War broke out in 1861, British public opinion was sympathetic to the Confederacy, in part because of lingering agitation over the tariff. As one diplomatic historian has explained, the Morrill Tariff:
"Not unnaturally gave great displeasure to England. It greatly lessened the profits of the American markets to English manufacturers and merchants, to a degree which caused serious mercantile distress in that country. Moreover, the British nation was then in the first flush of enthusiasm over free trade, and, under the lead of extremists like Cobden and Gladstone, was inclined to regard a protective tariff as essentially and intrinsically immoral, scarcely less so than larceny or murder. Indeed, the tariff was seriously regarded as comparable in offensiveness with slavery itself, and Englishmen were inclined to condemn the North for the one as much as the South for the other. "We do not like slavery," said Palmerston to Adams, "but we want cotton, and we dislike very much your Morrill tariff."
Many prominent British writers condemned the Morrill Tariff in the strongest terms. Economist William Stanley Jevons denounced it as a "retrograde" law. The well known novelist Charles Dickens used his magazine, "All the Year Round," to attack the new tariff. On December 28, 1861 Dickens published a lengthy article, believed to be written by Henry Morley,
which blamed the American Civil War on the Morrill Tariff:
If it be not slavery, where lies the partition of the interests that has led at last to actual separation of the Southern from the Northern States? …Every year, for some years back, this or that Southern state had declared that it would submit to this extortion only while it had not the strength for resistance. With the election of Lincoln and an exclusive Northern party taking over the federal government, the time for withdrawal had arrived … The conflict is between semi-independent communities which every feeling and interest the South calls for political partition, and every pocket interest the North calls for union … So the case stands, and under all the passion of the parties and the cries of battle lie the two chief moving causes of the struggle. Union means so many millions a year lost to the South; secession means the loss of the same millions to the North. The love of money is the root of this, as of many other evils... he quarrel between the North and South is, as it stands, solely a fiscal quarrel.
Communist philosopher Karl Marx was among the few writers in Britain who saw slavery as the major cause of the war. Marx wrote extensively in the British press and served as a London correspondent for several North American newspapers including Horace Greeley's "New York Tribune." Marx reacted to those who blamed the war on Morrill's bill, arguing instead that slavery had induced secession and that the tariff was just a pretext. Marx wrote, in October 1861:
Naturally, in America everyone knew that from 1846 to 1861 a free trade system prevailed, and that Representative Morrill carried his protectionist tariff through Congress only in 1861, after the rebellion had already broken out. Secession, therefore, did not take place because the Morrill tariff had gone through Congress, but, at most, the Morrill tariff went through Congress because secession had taken place.
Legacy.
According to historian Heather Cox Richardson, Morrill intended to offer protection to both the usual manufacturing recipients and a broad group of agricultural interests. The purpose was to appease interests beyond the northeast, which traditionally supported protection. For the first time protection was extended to every major farm product. 
Planning to distribute the benefits of a tariff to all sectors of the economy, and also hoping to broaden support for his party, Morrill rejected the traditional system of protection by proposing tariff duties on agricultural, mining, and fishing products, as well as on manufactures. Sugar, wool, flaxseed, hides, beef, pork, corn, grain, hemp, wool, and minerals would all be protected by the Morrill Tariff. The duty on sugar might well be expected to appease Southerners opposed to tariffs, and, notably, wool and flaxseed production were growing industries in the West. The new tariff bill also would protect coal, lead, copper, zinc, and other minerals, all of which the new northwestern states were beginning to produce. The Eastern fishing industry would receive a duty on dried, pickled, and salted fish. "In adjusting the details of a tariff," Morrill explained with a rhetorical flourish in his introduction of the bill, "I would treat agriculture, manufactures, mining, and commerce, as I would our whole people—as members of one family, all entitled to equal favor, and no one to be made the beast of burden to carry the packs of others."
According to Taussig, "Morrill and the other supporters of the act of 1861 declared that their intention was simply to restore the rates of 1846." However, he also gives reason to suspect that the bill's motives were intended to put high rates of protection on iron and wool to attract states in the West and in Pennsylvania:
"The important change which they (the sponsors) proposed to make from the provisions of the tariff of 1846 was to substitute specific for ad-valorem duties. Such a change from ad-valorem to specific duties is in itself by no means objectionable; but it has usually been made a pretext on the part of protectionists for a considerable increase in the actual duties paid. When protectionists make a change of this kind, they almost invariably make the specific duties higher than the ad-valorem duties for which they are supposed to be an equivalent...The Morrill tariff formed no exception to the usual course of things in this respect. The specific duties which it established were in many cases considerably above the ad-valorem duties of 1846. The most important direct changes made by the act of 1861 were in the increased duties on iron and on wool, by which it was hoped to attach to the Republican party Pennsylvania and some of the Western States"
Henry Carey, who assisted Morrill while drafting the bill and was one of its most vocal supporters, strongly emphasized its importance to the Republican Party in his January 2, 1861 letter to Lincoln. Carey told the President-Elect "the success of your administration is wholly dependent upon the passage of the Morrill bill at the present session." According to Carey:
"With it, the people will be relieved — your term will commence with a rising wave of prosperity — the Treasury will be filled and the party that elected you will be increased and strengthened. Without it, there will be much suffering among the people — much dissatisfaction with their duties — much borrowing on the part of the Government — & very much trouble among the Republican Party when the people shall come to vote two years hence. There is but one way to make the Party a permanent one, & that is, by the prompt repudiation to the free trade system."
Congressman John Sherman later wrote: 
The Morrill tariff bill came nearer than any other to meeting the double requirement of providing ample revenue for the support of the government and of rendering the proper protection to home industries. No national taxes, except duties on imported goods, were imposed at the time of its passage. The Civil War changed all this, reducing importations and adding tenfold to the revenue required. The government was justified in increasing existing rates of duty, and in adding to the dutiable list all articles imported, thus including articles of prime necessity and of universal use. In addition to these duties, it was compelled to add taxes on all articles of home production, on incomes not required for the supply of actual wants, and, especially, on articles of doubtful necessity, such as spirits, tobacco and beer. These taxes were absolutely required to meet expenditures for the army and navy, for the interest on the war debts and just pensions to those who were disabled by the war, and to their widows and orphans.
Secession and tariffs.
The Morrill Tariff and the secession movement.
The Morrill tariff was adopted against the backdrop of the secession movement, and provided an issue for secessionist agitation in some southern states. The law's critics compared it to the 1828 Tariff of Abominations that sparked the Nullification Crisis, although its average rate was significantly lower.
Slavery dominated the secession debate in the southern states, but the Morrill Tariff was addressed in the conventions of Georgia and South Carolina.
Robert Barnwell Rhett similarly railed against the then-pending Morrill Tariff before the South Carolina convention. Rhett included a lengthy attack on tariffs in the "Address of South Carolina to Slaveholding State19s", which the convention adopted on December 25, 1860 to accompany its secession ordinance.
And so with the Southern States, towards the Northern States, in the vital matter of taxation. They are in a minority in Congress. Their representation in Congress, is useless to protect them against unjust taxation; and they are taxed by the people of the North for their benefit, exactly as the people of Great Britain taxed our ancestors in the British parliament for their benefit. For the last forty years, the taxes laid by the Congress of the United States have been laid with a view of subserving the interests of the North. The people of the South have been taxed by duties on imports, not for revenue, but for an object inconsistent with revenue— to promote, by prohibitions, Northern interests in the productions of their mines and manufactures.
The Morrill Tariff played less prominently elsewhere in the South. In some portions of Virginia, secessionists promised a new protective tariff to assist the state's fledgling industries.
In the North, enforcement of the Morrill Tariff contributed to support for the Union cause among industrialists and merchant interests. Speaking of this class, the abolitionist Orestes Brownson derisively remarked that "the Morrill Tariff moved them more than the fall of Sumter." In one such example the New York Times, which had previously opposed Morrill's bill on free trade grounds, editorialized that the tariff imbalance would bring commercial ruin to the North and urged its suspension until the secession crisis passed. "We have imposed high duties on our commerce at the very moment the seceding states are inviting commerce to their ports by low duties." As secession became more evident and the fledgling Confederacy adopted a much lower tariff of its own, the paper urged military action to enforce the Morrill Tariff in the Southern states.
Historiography.
Historians, James Huston notes, have been baffled by the role of high tariffs in general and have offered multiple conflicting interpretations over the years. (Low tariffs, all historians agree, were noncontroversial and were needed to fund the federal government.) One school of thought says the Republicans were the willing tools of would-be monopolists. A second schools says the Republicans truly believed tariffs would promote nationalism and prosperity for everyone along with balanced growth in every region (as opposed to growth only in the cotton South). A third school emphasizes the undeniable importance of the tariff in cementing party loyalty, especially in industrial states. Another approach emphasizes that factory workers were eager for high tariffs because it protected their high wages from European competition.
Charles A. Beard argued in the 1920s that very long-term economic issues were critical, with the pro-tariff industrial Northeast forming a coalition with the anti-tariff agrarian Midwest against the plantation South. According to Luthin in the 1940s, "Historians are not unanimous as to the relative importance which Southern fear and hatred of a high tariff had in causing the secession of the slave states." However, none of the statesmen seeking a compromise in 1860-61 that would avert the war ever suggested the tariff might be the key to a solution, or might be a cause of the secession. Beginning in the 1950s, historians moved away from the Beard thesis of economic causality. In its place, historians led by Richard Hofstadter began to emphasize the social causes of the war, centered around the issue of slavery. The Beard thesis has enjoyed a recent revival among economists, pro-Confederate historians, and neo-Beardian scholars. A 2002 study by economists Robert McGuire and T. Norman Van Cott concluded:
A de facto constitutional mandate that tariffs lie on the lower end of the Laffer relationship means that the Confederacy went beyond simply observing that a given tax revenue is obtainable with a "high" and "low" tax rate, a la Alexander Hamilton and others. Indeed, the constitutional action suggests that the tariff issue may in fact have been even more important in the North–South tensions that led to the Civil War than many economists and historians currently believe."
Rather than contributing to secession, Marc-William Palen notes how the tariff was only able to pass through Congress following the secession of Southern states. Thus, secession itself allowed for the bill's passage, rather than the other way around. Allan Nevins and James M. McPherson downplay the significance of the tariff, arguing that it was peripheral to the issue of slavery. They note that slavery dominated the secessionist declarations, speeches, and pamphlets. Nevins also points to the argument of Alexander Stephens, who disputed Toombs' claims about the severity of the Morrill tariff. Though initially a unionist, Stephens would later cite slavery as the "cornerstone" reason behind his support of the secessionist cause.

</doc>
<doc id="55589" url="https://en.wikipedia.org/wiki?curid=55589" title="Homestead Acts">
Homestead Acts

The Homestead Acts were several United States federal laws that gave an applicant ownership of land, typically called a "homestead", at little or no cost. It gave settlers 160 acres (65 hectares). An extension of the "Homestead Principle" in law, the Homestead Acts were an expression of the "Free Soil" policy of Northerners who wanted individual farmers to own and operate their own farms, as opposed to Southern slave-owners who wanted to buy up large tracts of land and use slave labor, thereby shutting out free white men.
The first of the acts, the Homestead Act of 1862, opened up millions of acres. Any adult who had never taken up arms against the U.S. government could apply. Women, blacks, and immigrants were eligible.
Several additional laws were enacted in the latter half of the 19th and early 20th centuries. The Southern Homestead Act of 1866 sought to address land ownership inequalities in the south during Reconstruction. The Timber Culture Act of 1873 granted land to a claimant who was required to plant trees—the tract could be added to an existing homestead claim and had no residency requirement. The Kinkaid Amendment of 1904 granted a full section (640 acres) to new homesteaders settling in western Nebraska. An amendment to the Homestead Act of 1862, the Enlarged Homestead Act, was passed in 1909 and doubled the allotted acreage from 160 to 320 acres. Another amended act, the national Stock-Raising Homestead Act, was passed in 1916 and again increased the land involved, this time to 640 acres. In all, more than 270 million acres of public land, or nearly 10% of the total area of the U.S., was given away free to homesteaders.
Background.
Land-grant laws similar to the Homestead Acts had been proposed by northern Republicans before the Civil War, but had been repeatedly blocked in Congress by southern Democrats who wanted western lands open for purchase by slave-owners. The Homestead Act of 1860 did pass in Congress, but it was vetoed by President James Buchanan, a Democrat. After the Southern states seceded from the Union in 1861 (and their representatives had left Congress), the bill passed and was signed into law by President Abraham Lincoln (May 20, 1862). Daniel Freeman became the first person to file a claim under the new act.
Between 1862 and 1934, the federal government granted 1.6 million homesteads and distributed of federal land for private ownership. This was a total of 10% of all land in the United States. Homesteading was discontinued in 1976, except in Alaska, where it continued until 1986. About 40 percent of the applicants who started the process were able to complete it and obtain title to their homesteaded land.
History.
Donation Land Claim Act of 1850.
The Donation Land Claim Act allowed settlers to claim land in the Oregon Territory, then including the modern states of Washington, Oregon, Idaho and parts of Wyoming. Settlers were able to claim 320 or 640 acres of land for free between 1850 and 1854, and then at a cost of $1.25 per acres until the law expired in 1855.
Homestead Act of 1862.
The "yeoman farmer" ideal of Jeffersonian democracy was still a powerful influence in American politics during the 1840–1850s, with many politicians believing a homestead act would help increase the number of "virtuous yeomen". The Free Soil Party of 1848–52, and the new Republican Party after 1854, demanded that the new lands opening up in the west be made available to independent farmers, rather than wealthy planters who would develop it with the use of slaves forcing the yeomen farmers onto marginal lands. Southern Democrats had continually fought (and defeated) previous homestead law proposals, as they feared free land would attract European immigrants and poor Southern whites to the west. After the South seceded and their delegates left Congress in 1861, the Republicans and other supporters from the upper South passed a homestead act.
The intent of the first Homestead Act, passed in 1862, was to liberalize the homesteading requirements of the Preemption Act of 1841. Its leading advocates were Andrew Johnson, George Henry Evans and Horace Greeley.
The homestead was an area of public land in the West (usually ) granted to any US citizen willing to settle on and farm the land for at least five years. The law (and those following it) required a three-step procedure: file an application, improve the land, and file for deed of title. Anyone who had never taken up arms against the U.S. government (including freed slaves) and was at least 21 years old or the head of a household, could file an application to claim a federal land grant. The occupant had to reside on the land for five years, and show evidence of having made improvements.
Southern Homestead Act of 1866.
Enacted to allow poor tenant farmers and sharecroppers in the south become land owners in the southern United States during Reconstruction. It was not very successful, as even the low prices and fees were often too much for the applicants to afford.
The Timber Culture Act of 1873.
The Timber Culture Act granted up to 160 acres of land to a homesteader who would plant at least 40 acres of trees over a period of several years. This quarter-section could be added to an existing homestead claim, offering a total of 320 acres to a settler.
Kinkaid Amendment of 1904.
Recognizing that arid lands west of the 100th meridian, which passes through central Nebraska, required more than 160 acres for a claimant to support a family, Congress passed the Kinkaid Act which granted larger homestead tracts, up to 640 acres, to homesteaders in western Nebraska.
Enlarged Homestead Act of 1909.
Because by the early 1900s much of the prime low-lying alluvial land along rivers had been homesteaded, the "Enlarged Homestead Act" was passed in 1909. To enable dryland farming, it increased the number of acres for a homestead to given to farmers who accepted more marginal lands (especially in the Great Plains), which could not be easily irrigated.
A massive influx of these new farmers, combined with inappropriate cultivation techniques and misunderstanding of the ecology, led to immense land erosion and eventually the Dust Bowl of the 1930s.
The Stock-Raising Homestead Act of 1916.
In 1916, the "Stock-Raising Homestead Act" was passed for settlers seeking of public land for ranching purposes.
Subsistence Homesteads provisions under the New Deal – 1930.
Renewed interest in homesteading was brought about by U.S. President Franklin D. Roosevelt's program of Subsistence Homesteading implemented in the 1930s under the New Deal.
Homesteading requirements.
The Homestead Acts had few qualifying requirements. A "homesteader" had to be the head of the household or at least twenty-one years old. They had to live on the designated land, build a home, make improvements, and farm it for a minimum of five years. The filing fee was eighteen dollars (or ten to temporarily hold a claim to the land).
Immigrants, farmers without their own land, single women, and former slaves could all qualify. The fundamental racial qualification was that one had to be a citizen, or have filed a declaration of intention to become a citizen, and so the qualification changed over the years with the varying legal qualifications for citizenship. African-Americans became qualified with the passage of the Fourteenth Amendment in 1868. South Asians and East Asians who had been born in the United States became qualified with the decision of United States v. Wong Kim Ark in 1898, but little high-quality land remained available by that time. For immigrants the fundamental qualification was that they had to be permitted to enter the country (which was usually co-extensive with being allowed to file a declaration of intention to become a citizen). During the 1800s, the bulk of immigrants were from Europe, with immigrants from South Asia and East Asia being largely excluded, and (voluntary) immigrants from Africa were permitted but uncommon.
The act in practice.
Settlers found land and staked their claims, usually in individual family units, although others formed closer knit communities. Often, the homestead consisted of several buildings or structures besides the main house.
The Homestead Act of 1862 gave rise later to a new phenomenon, large land rushes, such as the Oklahoma Land Runs of the 1880s and 90s.
End of homesteading.
The Federal Land Policy and Management Act of 1976 ended homesteading; by that time, federal government policy had shifted to retaining control of western public lands. The only exception to this new policy was in Alaska, for which the law allowed homesteading until 1986.
The last claim under this Act was made by Ken Deardorff for of land on the Stony River in southwestern Alaska. He fulfilled all requirements of the homestead act in 1979 but did not receive his deed until May 1988. He is the last person to receive title to land claimed under the Homestead Acts.
Criticism.
The homestead acts were much abused. Although the intent was to grant land for agriculture, in the arid areas east of the Rocky Mountains, was generally too little land for a viable farm (at least prior to major federal public investments in irrigation projects). In these areas, people manipulated the provisions of the act to gain control of resources, especially water. A common scheme was for an individual, acting as a front for a large cattle operation, to file for a homestead surrounding a water source, under the pretense that the land was to be used as a farm. Once the land was granted, other cattle ranchers would be denied the use of that water source, effectively closing off the adjacent public land to competition. That method was also used by large businesses and speculators to gain ownership of timber and oil-producing land. The federal government charged royalties for extraction of these resources from public lands. On the other hand, homesteading schemes were generally pointless for land containing "locatable minerals," such as gold and silver, which could be controlled through mining claims under the Mining Act of 1872, for which the federal government did not charge royalties.
The government developed no systematic method to evaluate claims under the homestead acts. Land offices relied on affidavits from witnesses that the claimant had lived on the land for the required period of time and made the required improvements. In practice, some of these witnesses were bribed or otherwise colluded with the claimant.
Although not necessarily fraud, it was common practice for the eligible children of a large family to claim nearby land as soon as possible. After a few generations, a family could build up a sizable estate.
The homesteads were criticized as too small for the environmental conditions on the Great Plains; a homesteader using 19th-century animal-powered tilling and harvesting could not have cultivated the 1500 acres later recommended for dry land farming. Some scholars believe the acreage limits were reasonable when the act was written, but reveal that no one understood the physical conditions of the plains.
According to Hugh Nibley, much of the rain forest west of Portland, Oregon was acquired by the Oregon Lumber Company by illegal claims under the Act.
The Homestead Act also brought settlers into conflict with indigenous Americans, displacing them, and accelerating the decline in their population.
Related acts in other countries.
Canada.
Similar laws were passed in Canada:
The Legislative Assembly of Ontario passed "The Free Grants and Homestead Act" in 1868, which introduced a conditional scheme to an existing free grant plan previously authorized by the Province of Canada in "The Public Lands Act" of 1860. It was extended to include settlement in the Rainy River District under "The Rainy River Free Grants and Homestead Act, 1886", These Acts were consolidated in 1913 in "The Public Lands Act", which was further extended in 1948 to provide for free grants to former members of the Canadian Forces. The original free grant provisions for settlers were repealed in 1951, and the remaining provisions were repealed in 1961.
The Parliament of Canada passed the "Dominion Lands Act" in 1872 in order to encourage settlement in the Northwest Territories. Its application was restricted after the passage of the Natural Resources Acts in 1930, and it was finally repealed in 1950.
The Legislative Assembly of Quebec did not expand the scope of the 1860 Province of Canada Act (which modern day Quebec was part of in 1860), but did provide in 1868 that such lands were exempt from seizure, and chattels thereon were also exempt for the first ten years of occupation. Later known as the "Settlers Protection Act", it was repealed in 1984.
Newfoundland and Labrador provided for free grants of land upon proof of possession for twenty years prior to 1977, with continuous use for agricultural, business or residential purposes during that time. Similar programs continued to operate in Alberta and British Columbia until 1970. In the early 21st century, some land is still being granted in the Yukon Territory under its Agricultural Lands Program.
New Zealand.
Despite the 1840 Treaty of Waitangi provisions for sale of land, the Māori Land Court decided that all land not cultivated by Māori was 'waste land' and belonged to the Crown without purchase. Most New Zealand provinces had Waste Lands Acts enacted between 1854 and 1877. The 1877 Land Act in Auckland Province used the term Homestead, with allocation administered by a Crown Lands Board. There was similar legislation in Westland. It gave up to , with settlers just paying the cost of a survey. They had to live there for five years, build a house and cultivate a third of the land, if already open, or a fifth if bush had to be cleared. The land was forfeited if they didn't clear enough bush. This contributed to rapid deforestation.
Elsewhere in the British Empire.
Similar in intent, the British Crown Lands Acts were extended to several of the Empire's territories, and many are still in effect, to some extent, today. For instance, the Australian selection acts were passed in the various Australian colonies following the first, in 1861, in New South Wales.

</doc>
<doc id="55592" url="https://en.wikipedia.org/wiki?curid=55592" title="Bob Black">
Bob Black

Robert Charles "Bob" Black, Jr. (born January 4, 1951) is an American anarchist. He is the author of the books "The Abolition of Work and Other Essays", "Beneath the Underground", "Friendly Fire", "Anarchy After Leftism", "Defacing the Currency", and numerous political essays.
Biography.
Black graduated from the University of Michigan and Georgetown Law School. He later took M.A. degrees in jurisprudence and social policy from the University of California (Berkeley), criminal justice from the State University of New York (SUNY) at Albany, and an LL.M in criminal law from the SUNY Buffalo School of Law. During his college days (1969-1973) he became disillusioned with the New Left of the 1970s and undertook extensive readings in anarchism, utopian socialism, council communism, and other left tendencies critical of both Marxism–Leninism and social democracy. He found some of these sources at the Labadie Collection at the University of Michigan, a major collection of radical, labor, socialist, and anarchist materials which is now the repository for Black's papers and correspondence. He was soon drawn to Situationist thought, egoist communism, and the anti-authoritarian analyses of John Zerzan and the Detroit magazine Fifth Estate. He produced a series of ironic political posters signed "The Last International", first in Ann Arbor, Michigan, then in San Francisco where he moved in 1978. In the Bay Area he became involved with the publishing and cultural underground, writing reviews and critiques of what he called the "marginals milieu." Since 1988 he has lived in upstate New York.
Black is best known for a 1985 essay, "The Abolition of Work," which has been widely reprinted and translated into at least thirteen languages (most recently, Urdu). In it he argued that work is a fundamental source of domination, comparable to capitalism and the state, which should be transformed into voluntary "productive play." Black acknowledged among his inspirations the French utopian socialist Charles Fourier, the British utopian socialist William Morris, the Russian anarcho-communist Peter Kropotkin, and the Situationists. "The Abolition of Work and Other Essays", published by Loompanics in 1986, included, along with the title essay, some of his short Last International texts, and some essays and reviews reprinted from his column in "San Francisco's Appeal to Reason," a leftist and counter-cultural tabloid published from 1980 to 1984.
Two more essay collections were later published as books, "Friendly Fire" (Autonomedia, 1992) and "Beneath the Underground" (Feral House, 1994), the latter devoted to the do-it-yourself/fanzine subculture of the '80s and '90s which he called "the marginals milieu" and in which he had been heavily involved. "Anarchy after Leftism" (C.A.L. Press, 1996) is a more or less point-by-point rebuttal of Murray Bookchin's "Social Anarchism or Lifestyle Anarchism: An Unbridgeable Chasm" (A.K. Press, 1996), which had criticized as "lifestyle anarchism" various nontraditional tendencies in contemporary anarchism. Black's short book ("about an even shorter book," as he put it) was succeeded—as an E-book published in 2011 at the online Anarchist Library—by "Nightmares of Reason", a longer and more wide-ranging critique of Bookchin's anthropological and historical arguments, especially Bookchin's espousal of "libertarian municipalism" which Black ridiculed as "mini-statism."
In 1996 Black cooperated with the Seattle police Narcotics Division against Seattle author Jim Hogshire, leading to a police raid on Hogshire's home and the subsequent arrest of Hogshire and his wife.
Since 2000, Black has focused on topics reflecting his education and reading in the sociology and the ethnography of law, resulting in writings often published in "Anarchy: A Journal of Desire Armed." His recent interests have included the anarchist implications of dispute resolution institutions in stateless primitive societies (arguing that mediation, arbitration, etc., cannot feasibly be annexed to the U.S. criminal justice system, because they presuppose anarchism and a relative social equality not found in state/class societies). At the 2011 annual B.A.S.T.A.R.D. anarchist conference in Berkeley, California, Black presented a workshop where he argued that, in society as it is, crime can be an anarchist method of social control, especially for people systematically disserved by the legal system. An article based on this presentation appeared in "Anarchy" magazine and in his 2013 book, "Defacing the Currency: Selected Writings, 1992-2012".
Black has expressed an interest, which grew out of his polemics with Bookchin, in the relation of democracy to anarchism. For Bookchin, democracy—the "direct democracy" of face-to-face assemblies of citizens—is anarchism. Some contemporary anarchists agree, including the academics Cindy Milstein, David Graeber, and Peter Staudenmeier. Black, however, has always rejected the idea that democracy (direct or representative) is anarchist. He made this argument at a presentation at the Long Haul Bookshop (in Berkeley) in 2008. In 2011, C.A.L. Press published as a pamphlet "Debunking Democracy", elaborating on the speech and providing citation support. This too is reprinted in "Defacing the Currency".
Writing.
Some of his work from the early 1980s includes (anthologized in "The Abolition of Work and Other Essays") highlights his critiques of the nuclear freeze movement ("Anti-Nuclear Terror"), the editors of "Processed World" ("Circle A Deceit: A Review of "Processed World""), radical feminists ("Feminism as Fascism"), and right wing libertarians ("The Libertarian As Conservative"). Some of these essays previously appeared in "San Francisco's Appeal to Reason" (1981-1984), a leftist and counter-cultural tabloid newspaper for which Black wrote a column.
"The Abolition of Work".
"The Abolition of Work and Other Essays" (1986), draws upon some ideas of the Situationist International, the utopian socialists Charles Fourier and William Morris, anarchists such as Paul Goodman, and anthropologists such as Richard Borshay Lee and Marshall Sahlins. Black criticizes work for its compulsion, and, in industrial society, for taking the form of "jobs"—the restriction of the worker to a single limited task, usually one which involves no creativity and often no skill. Black's alternative is the elimination of what William Morris called "useless toil" and the transformation of useful work into "productive play," with opportunities to participate in a variety of useful yet intrinsically enjoyable activities, as proposed by Charles Fourier. "Beneath the Underground" (1992) is a collection of texts relating to what Black calls the "marginals milieu"—the do-it-yourself zine subculture which flourished in the 80s and early 90s. "Friendly Fire" (1992) is, like Black's first book, an eclectic collection touching on many topics including the Art Strike, Nietzsche, the first Gulf War and the Dial-a-Rumor telephone project he conducted with Zack Replica (1981-1983).
"Defacing the Currency: Selected Writings, 1992-2012" was published by Little Black Cart Press in 2013. It includes a lengthy (113 pages), previously unpublished critique of Noam Chomsky, "Chomsky on the Nod." A similar collection has been published, in Russian translation, by Hylaea Books in Moscow. Black's most recent book, also from LBC Books, is Instead of Work, which collects "The Abolition of Work" and seven other previously published texts, with a lengthy new update, "Afterthoughts on the Abolition of Work." The introduction is by science fiction writer Bruce Sterling.

</doc>
<doc id="55593" url="https://en.wikipedia.org/wiki?curid=55593" title="Morrill Land-Grant Acts">
Morrill Land-Grant Acts

The Morrill Land-Grant Acts are United States statutes that allowed for the creation of land-grant colleges, including the Morrill Act of 1862 ( et seq.) and the Morrill Act of 1890 (the Agricultural College Act of 1890, (, et seq.)).
Passage of original bill.
For 20 years prior to the first introduction of the bill in 1857, there was a political movement calling for the creation of agriculture colleges. The movement was led by Professor Jonathan Baldwin Turner of Illinois College. For example, the Michigan Constitution of 1850 called for the creation of an "agricultural school", though it was not until February 12, 1855, that Michigan Governor Kinsley S. Bingham signed a bill establishing the United States' first agriculture college, the Agricultural College of the State of Michigan, known today as Michigan State University, which served as a model for the Morrill Act.
On February 8, 1853, the Illinois Legislature adopted a resolution, drafted by Turner, calling for the Illinois congressional delegation to work to enact a land-grant bill to fund a system of industrial colleges, one in each state. Senator Lyman Trumbull of Illinois believed it was advisable that the bill should be introduced by an eastern congressman, and two months later Representative Justin Smith Morrill of Vermont introduced his bill.
Unlike the Turner Plan, which provided an equal grant to each state, the Morrill bill allocated land based on the number of senators and representatives each state had in Congress. This was more advantageous to the more populous eastern states.
The Morrill Act was first proposed in 1857, and was passed by Congress in 1859, but it was vetoed by President James Buchanan. In 1861, Morrill resubmitted the act with the amendment that the proposed institutions would teach military tactics as well as engineering and agriculture. Aided by the secession of many states that did not support the plans, this reconfigured Morrill Act was signed into law by President Abraham Lincoln on July 2, 1862. The previous day Lincoln signed a bill financing the transcontinental railroad with land grants. Less than two months earlier he signed the Homestead Act encouraging western settlement. Together these actions, taken at a time when the Union Army was poorly performing, did much to define post-Civil War America.
Land-grant colleges.
The purpose of the land-grant colleges was:
Under the act, each eligible state received a total of of federal land, either within or contiguous to its boundaries, for each member of congress the state had as of the census of 1860. This land, or the proceeds from its sale, was to be used toward establishing and funding the educational institutions described above. Under provision six of the Act, "No State while in a condition of rebellion or insurrection against the government of the United States shall be entitled to the benefit of this act," in reference to the recent secession of several Southern states and the contemporaneously raging American Civil War.
After the war, however, the 1862 Act was extended to the former Confederate states; it was eventually extended to every state and territory, including those created after 1862. If the federal land within a state was insufficient to meet that state's land grant, the state was issued "scrip" which authorized the state to select federal lands in other states to fund its institution. For example, New York carefully selected valuable timber land in Wisconsin to fund Cornell University.p. 9 The resulting management of this scrip by the university yielded one third of the total grant revenues generated by all the states, even though New York received only one-tenth of the 1862 land grant.p. 10 Overall, the 1862 Morrill Act allocated of land, which when sold yielded a collective endowment of $7.55 million.p. 8
On September 11, 1862, the state of Iowa was the first to accept the terms of the Morrill Act which provided the funding boost needed for the fledgling State Agricultural College and Model Farm (eventually renamed Iowa State University of Science and Technology). The first land-grant institution actually created under the Act was Kansas State University, which was established on February 16, 1863, and opened on September 2, 1863.
Before the Civil War American engineers were mostly educated at West Point. While the Congressional debate associated with the Morrill Act was largely focused on benefits to agriculture, the mechanic arts were specifically included. After the Civil War, as the German University model began to replace the English College, with the encouragement of the Morrill Act the engineering discipline was gradually defined. Because the Morrill Act excluded spending on buildings, engineering specific infrastructure such as textbooks and laboratories were developed. In 1866 there were around 300 American men with engineering degrees and six reputable colleges granting them. By 1911 the United States was graduating 3000 engineers a year, and had a total of 38,000 degreed engineers. The Morrill Act coincided with the establishment of engineering in the American university.
With a few exceptions (including Cornell University, University of Delaware and the Massachusetts Institute of Technology), nearly all of the Land-Grant Colleges are public. (Cornell University, while private, administers several state-supported contract colleges that fulfill its public land-grant mission to the state of New York.)
To maintain their status as land-grant colleges, a number of programs are required to be maintained by the college. These include programs in agriculture and engineering, as well as a Reserve Officers' Training Corps program.
Expansion.
A second Morrill Act in 1890 was also aimed at the former Confederate states. This act required each state to show that race was not an admissions criterion, or else to designate a separate land-grant institution for persons of color. Among the seventy colleges and universities which eventually evolved from the Morrill Acts are several of today's historically Black colleges and universities. Though the 1890 Act granted cash instead of land, it granted colleges under that act the same legal standing as the 1862 Act colleges; hence the term "land-grant college" properly applies to both groups.
Later on, other colleges such as the University of the District of Columbia and the "1994 land-grant colleges" for Native Americans were also awarded cash by Congress in lieu of land to achieve "land-grant" status.
In imitation of the land-grant colleges' focus on agricultural and mechanical research, Congress later established programs of sea grant colleges (aquatic research, in 1966), urban grant colleges (urban research, in 1985), space grant colleges (space research, in 1988), and sun grant colleges (sustainable energy research, in 2003).
Agricultural experiment stations and cooperative extension service.
Starting in 1887, Congress also funded agricultural experiment stations and various categories of agricultural and veterinary research "under direction of" the land-grant universities. Congress later recognized the need to disseminate the knowledge gained at the land-grant colleges to farmers and homemakers. The Smith-Lever Act of 1914 started federal funding of cooperative extension, with the land-grant universities' agents being sent to virtually every county of every state. In some states, the annual federal appropriations to the land-grant college under these laws exceed the current income from the original land grants. In the fiscal year 2006 USDA budget, $1.033 billion went to research and cooperative extension activities nationwide. For this purpose, former president Bush proposed a $1.035 billion appropriation for fiscal year 2008.

</doc>
<doc id="55595" url="https://en.wikipedia.org/wiki?curid=55595" title="Wade–Davis Bill">
Wade–Davis Bill

The Wade–Davis Bill of 1864 was a bill proposed for the Reconstruction of the South written by two Radical Republicans, Senator Benjamin Wade of Ohio and Representative Henry Winter Davis of Maryland. In contrast to President Abraham Lincoln's more lenient Ten Percent Plan, the bill made re-admittance to the Union for former Confederate states contingent on a majority in each Southern state to take the Ironclad oath to the effect they had never in the past supported the Confederacy. The bill passed both houses of Congress on July 2, 1864, but was pocket vetoed by Lincoln and never took effect. The Radical Republicans were outraged that Lincoln did not sign the bill. Lincoln wanted to mend the Union by carrying out the Ten percent plan. He believed it would be too difficult to repair all of the ties within the Union if the Wade–Davis bill passed.
Background.
The Wade–Davis Bill emerged from a plan introduced in the Senate by Ira Harris of New York in February, 1863.
It proposed to base the Reconstruction of the South on the government's power to guarantee a republican form of government. The Wade–Davis Bill was also important for national and congressional power. Although federally imposed conditions of reconstruction retrospectively seem logical, there was a widespread belief that southern Unionism would return the seceded states to the Union after the South's military power was broken. This belief was not fully abandoned until later in 1863. The provisions, critics complained, were virtually impossible to meet, thus making it likely there would be permanent national control over the southern states.
Senate voting.
Those voting for passage in the Senate were (18): 
Messrs. Anthony (R), Chandler (R), Clark (R), Conness (R), Foot (R), Harlan (R), Harris (R), Howe, Lane of Kansas (R), Morgan (R), Pomeroy (R), Ramsey (R), Sherman (R), Sprague (R), Sumner (R), Wade (R), Wilkinson (R), Wilson(R).
Those voting against passage were (14):
Messrs. Buckalew (D), Carlile (U), Davis (UU), Doolittle (R), Henderson (UU), Hendricks (D), Lane of Indiana (R), McDougall (D), Powell (D), Riddle (D), Saulsbury (D), Ten Eyck (R), Trumbull (R), Van Winkle (UU).
Lincoln's veto.
One of Lincoln's objections was to the idea that Southern states needed to "re-join" the Union (an idea that permeated the whole bill). The philosophy of the war from Lincoln's point of view was that the Southern states were not constitutionally allowed to secede in the first place and therefore were still part of the Union, even though their return to a full participation in the Union would require the fulfillment of some conditions. But he didn't think the war was being waged against "treasonous" States as such (since the refusal of the Union to recognize their right to secede made the ordinances of secession null) but merely to "compel the obedience of rebellious individuals". The problem was that the language of the bill was at times undermining the Northern rationale for the war by plainly asserting for instance that the Southern states were not part of the Union anymore.
Moreover, the bill compelled those states to draft new Constitutions banning slavery, which was plainly unconstitutional at the time since, in the then-absence of a Constitutional amendment on the issue (which would soon pass on its own right), Congress had no power to deal with slavery within each state.
On a more pragmatic level, Lincoln also feared the bill would sabotage his own reconstruction activities in states like Louisiana, Arkansas, and Tennessee, all of which had seceded but were under Federal occupation and control of Union governments. He believed that Wade–Davis would jeopardize state-level emancipation movements in loyal border states like Missouri and, especially, Maryland. The bill threatened to destroy the delicate political coalitions which Lincoln had begun to construct between northern and southern moderates. More broadly, it underscored how differently Lincoln and Radical Republicans viewed the Confederates. The President thought they needed to be coaxed back into peaceful coexistence while Wade–Davis treated them as traitors that needed to be punished. Lincoln ended up killing the bill with a "pocket veto" and it was not resurrected.
The aftermath.
Davis was a bitter enemy of Lincoln because he believed that Lincoln was too lenient in terms of his policies for the South. Davis and Wade issued a manifesto "To the Supporters of the Government" on August 4, 1864, that accused Lincoln of using reconstruction to secure electors in the South who would "be at the dictation of his personal ambition," and condemned what they saw as his efforts to usurp power from Congress ("the authority of Congress is "paramount" and must be respected"). The Manifesto backfired, however, and while it initially caused much debate on the nature of the Reconstruction to come, Winter Davis was not renominated for his Congressional seat. Its ideas, particularly on the fact Congress should be the main driver of the post-war process and its conception of the Presidency as a weaker office (the President "must confine himself to his executive duties – "to obey and execute", not to make the laws –, to suppress by arms armed rebellion, and leave political reorganization to Congress" ), did influence Congressional Republicans during the following years, leading to Andrew Johnson's impeachment trial.
Lincoln survived their attacks and greatly strengthened his position with a landslide victory in the 1864 election, and national passage of the 13th Amendment in February, 1865. He momentarily marginalized the Radicals in terms of shaping Reconstruction policy. After Lincoln's death, Radical Republicans battled President Andrew Johnson, who tried to continue a version of Lincoln's plan. The midterm elections of 1866 turned into a referendum on the 14th amendment and the trajectory of Reconstruction policy. With the Republicans' victory, Congress took control of Reconstruction. The radicals wanted a much harsher plan, but they did not try to reimpose the terms of Wade-Davis. Instead they took control of the southern states with the Army, which registered black men as voters and refused to allow former Confederates to run for office.

</doc>
<doc id="55596" url="https://en.wikipedia.org/wiki?curid=55596" title="National Bank Act">
National Bank Act

The National Banking Acts of 1863 and 1864 were two United States federal banking acts that established a system of national banks for banks, and created the United States National Banking System. They encouraged development of a national currency backed by bank holdings of U.S. Treasury securities and established the Office of the Comptroller of the Currency as part of the United States Department of the Treasury and authorized the Comptroller to examine and regulate nationally chartered banks. The Act shaped today's national banking system and its support of a uniform U.S. banking policy.
Background.
After the expiration of the Second Bank of the United States in 1836, the control of banking regimes devolved mostly to the states. Different states adopted policies including a total ban on banking (as in Wisconsin), a single state-chartered bank (as in Indiana and Illinois), limited chartering of banks (as in Ohio), and free entry (as in New York). While the relative success of New York's "free banking" laws led a number of states to also adopt a free-entry banking regime, the system remained poorly integrated across state lines. Though all banknotes were uniformly denominated in dollars, notes would often circulate at a steep discount in states beyond their issue.
In addition, there were well-publicized frauds arising in states like Michigan, which had adopted free entry regimes but did not require redeemability of bank issues for specie. The perception of dangerous "wildcat" banking, along with the poor integration of the U.S. banking system, led to increasing public support for a uniform national banking regime.
The United States Government, on the other hand, still had limited taxation capabilities, and so had an interest in the seigniorage potential of a national bank. In 1846, the Polk Administration created a United States Treasury system that moved public funds from private banks to Treasury branches in order to fund the Mexican–American War. However, without a national currency, the revenue generated this way was limited.
One of the first attempts to issue a national currency came in the early days of the Civil War when Congress approved the Legal Tender Act of 1862, allowing the issue of $150 million in national notes known as greenbacks and mandating that paper money be issued and accepted in lieu of gold and silver coins. The bills were backed only by the national government's promise to redeem them and their value was dependent on public confidence in the government as well as the ability of the government to give out specie in exchange for the bills in the future. Many thought this promise backing the bills was about as good as the green ink printed on one side, hence the name "greenbacks."
In 1863, the Second Legal Tender Act, enacted July 11, 1862, a Joint Resolution of Congress, and the Third Legal Tender Act, enacted March 3, 1863, expanded the limit to $450 million. The largest amount of greenbacks outstanding at any one time was calculated as $447,300,203.10.
The National Banking Act (ch. 58, 12 Stat. 665; February 25, 1863), originally known as the National Currency Act, was passed in the Senate by a narrow 23–21 vote. The main goal of this act was to create a single national currency and to eradicate the problem of notes from multiple banks circulating all at once. The Act established national banks that could issue notes which were backed by the United States Treasury and printed by the government itself. The quantity of notes that a bank was allowed to issue was proportional to the bank's level of capital deposited with the Comptroller of the Currency at the Treasury. To further control the currency, the Act taxed notes issued by state and local banks, essentially pushing non-federally issued paper out of circulation.
The National Banking Act of 1863 was superseded by the National Banking Act of 1864 (ch. 106, 13 Stat. 99; June 3, 1864) just one year later. The new act also established federally issued bank charters, which took banking out of the hands of state governments. Before the act, charters were granted by state legislatures who were under an immense amount of political pressure and could be influenced by bribes. This problem was resolved to some degree by free banking laws in some states but it was not until this act was passed that free banking was established on a uniform, national level and charter issuance was taken out of the hands of discriminating and corrupt state legislatures. The first bank to receive a national charter was the First National Bank of Philadelphia, Pennsylvania (Charter #1). The first new national bank to open was The First National Bank of Davenport, Iowa (Charter #15). Additionally, the new Act converted more than 1,500 state banks to national banks.
National Bank Acts.
National Bank Act of 1863.
The National Bank Act of 1863 was designed to create a national banking system, float federal war loans, and establish a national currency. Congress passed the act to help resolve the financial crisis that emerged during the early days of the American Civil War (1861–1865). The fight with the South was expensive and no effective tax program had been drawn up to finance it. In December 1861 banks suspended specie payments (payments in gold or silver coins for paper currency called notes or bills). People could no longer convert bank notes into coins. Government responded by passing the Legal Tender Act (1862), issuing $150 million in national notes called greenbacks. However, bank notes (paper bills issued by state banks) accounted for most of the currency in circulation.
National Bank Act of 1864.
The 1864 act, based on a New York State law, brought the federal government into active supervision of commercial banks. It established the Office of the Comptroller of the Currency with the responsibility of chartering, examining and supervising all national banks.
National Bank Act of 1865–66.
On July 13, 1866, the banking Act of 1865 was extended beyond requiring every national banking association, state bank, or state banking association to pay a 10% tax on any notes paid out by them. The act of 1866 added that any persons, as well as of state banks and state banking associations used for circulation to also be taxed 10%. This act was considered enforced by the court case "Veazie Bank v. Fenno", supra. The official enactment by congress read, "That every national banking association, state bank, or state banking association shall pay a tax of ten percentum on the amount of notes of any person, state bank, or state banking association used for circulation and paid out by them after the 1st day of August, 1866, and such tax shall be assessed and paid in such manner as shall be prescribed by the Commissioner of Internal Revenue."
Most state banks flipped charters to avoid the tax. It was not until the 1870s and 80s when the growing popularity of checks developed by state banks and the declining importance and profitability of bank notes issued by national banks caused a resurgence in chartering of state banks.
"Veazie Bank v. Fenno" was a case between a state chartered Maine bank and the collector of internal revenue. The bank declined to pay the new 10% tax on notes when Fenno tried to collect, saying the tax imposed was unconstitutional. The question posed to the court was, "Whether the second clause of the 9th section of the Act of Congress of the 13th of July, 1866, under which the tax in his case was levied and collected, is a valid and constitutional law." The Chief Justices ruling over the case sided with congress, in effect ending any resistance by state banks to the acts of 1865 or 1866.
Resurgence of state banks.
The granting of charters led to the creation of many national banks and a national banking system which grew at a fast pace. The number of national banks rose from 66 immediately after the Act to 7473 in 1913. Initially, this rise in national banking came at the expense of state banking—the number of state banks dwindled from 1466 in 1863 to 247 in 1868. Though state banks were no longer allowed to issue notes, local bankers took advantage of less strict capital requirements ($10,000 for state banks vs. $50,000–200,000 for national banks) and opened new branches en masse. These new state banks then served as competition for national banks, growing to 15,526 in number by 1913.
The years leading up to the passing of the 10% tax on bank notes consisted of events surrounding the National Banking Act of 1864. During this time period, Hugh McCulloch was determined to "fight against the national banking legislation, which he rightly perceived as a threat to state-chartered banking. Although he tried to block the system's creation, he was not determined to be its champion." Part of his plans to revamp this portion of the banking system included hiring a new staff, being hands-on with several aspects such as "personally evaluating applications for bank charters and consoled prospective bankers", and "assisting in the design of the new national bank notes, and arranged for their engraving, printing, and distribution." As an end result of McCulloch's efforts, many banks were just not willing to conform to his system of operations. This prompted Congress to pass "a 10 percent tax on the notes of state banks, signaling its determination that national banks would triumph and the state banks would fade away."
A later act, passed on March 3, 1865, imposed a tax of 10 percent on the notes of state banks to take effect on July 1, 1866. Similar to previous taxes, this effectively forced all non-federal currency from circulation. It also resulted in the creation of demand deposit accounts, and encouraged banks to join the national system, increasing the number of national banks substantially.
Legacy.
The National Banking Acts served to create the (federal-state) dual structure that is now a defining characteristic of the U.S. banking system and economy. The Comptroller of the Currency continues to have significance in the U.S. economy and is responsible for administration and supervision of national banks as well as certain activities of bank subsidiaries (per the Gramm-Leach-Bliley Act of 1999). In 2004 the Act was used by John D. Hawke, Jr., Comptroller of the Currency, to effectively bar state attorneys general from national bank oversight and regulatory roles. Many blame the resulting lack of oversight and regulation for the late-2000s recession, the bailout of the U.S. financial system and the subprime mortgage crisis.

</doc>
<doc id="55601" url="https://en.wikipedia.org/wiki?curid=55601" title="Sleepy Hollow (film)">
Sleepy Hollow (film)

Sleepy Hollow is a 1999 American horror film directed by Tim Burton. It is a film adaptation loosely inspired by the 1820 short story "The Legend of Sleepy Hollow" by Washington Irving and stars Johnny Depp and Christina Ricci, with Miranda Richardson, Michael Gambon, Casper Van Dien and Jeffrey Jones in supporting roles. The plot follows police constable Ichabod Crane (Depp) sent from New York City to investigate a series of murders in the village of Sleepy Hollow by a mysterious Headless Horseman.
Development began in 1993 at Paramount Pictures with Kevin Yagher originally set to direct Andrew Kevin Walker's script as a low-budget slasher film. Disagreements with Paramount resulted in Yagher being demoted to prosthetic makeup designer, and Burton was hired to direct in June 1998. Filming took place from November 1998 to May 1999, and "Sleepy Hollow" was released to generally favorable reviews from critics, and grossed approximately $206 million worldwide. The film won the Academy Award for Best Art Direction and is considered groundbreaking in terms of visual style and cinematography.
Plot.
In 1799, New York City police constable Ichabod Crane (Johnny Depp) is facing imprisonment for going against traditional methods. Ichabod submits to deployment to the Westchester County hamlet of Sleepy Hollow, New York, which has been plagued by a series of brutal slayings in which the victims are found decapitated: Peter Van Garrett (Martin Landau), a wealthy farmer; his son Dirk; and the widow Emily Winship. Crane is informed that the killer is an undead headless Hessian mercenary from the American Revolutionary War who rides on a black steed in search of his missing head.
Crane begins his investigation, remaining skeptical about the supernatural elements until he actually encounters the Headless Horseman, who kills the town magistrate, Samuel Phillipse (Richard Griffiths). Boarding at the home of the town's richest family, the Van Tassels, Crane is taken with their daughter Katrina (Christina Ricci). Crane and Young Masbath, the son of one of the Horseman's victims, go to the cave dwelling of a reclusive sorceress. She reveals the location of the Tree of the Dead, which marks the Horseman's grave, as well as his portal into the natural world.
Crane discovers that the ground is freshly disturbed and the Horseman's skeleton has the skull missing. He realizes that whoever dug up and stole the skull is the person controlling the Horseman. The Killian family are taken by the Horseman and Katrina's suitor Brom van Brunt (Casper Van Dien) is killed trying to stop the Horseman.
Crane starts to believe that a conspiracy links all the deaths together, so he looks into Van Garrett's Last Will. Van Garrett had made a new will just before he died, leaving all his possessions to his new bride, Emily Winship. Crane deduces that all who knew about the new will were the victims of Horseman and that Katrina's father Baltus Van Tassel (Michael Gambon), who would have inherited the fortune, is the person holding the skull. Katrina, finding out that Crane suspects her father, burns the evidence that Crane has accumulated.
A council is held in the church. The Horseman seemingly kills Katrina's stepmother, Lady Van Tassel, and heads off to the church to get Baltus. Crane realizes the Horseman can't enter the church due to it being holy. A fight breaks out in the church and the chaos ends only when the Horseman harpoons Baltus through a window, dragging him out and acquiring his head. The next day, Crane believes Katrina to be the one who controls the Headless Horsemen.
Crane becomes suspicious when the corpse of Lady Van Tassel has a wound that seems to have been caused post-mortem. The real Lady Van Tassel (Miranda Richardson) then emerges, alive. Lady Van Tassel tells Katrina that her family was driven from their ancestral home by the Van Garretts, and that she became a witch and summoned the Horseman to kill them off and make herself sole heir to the family fortune. She then sends the killer after Katrina to solidify her hold on what she considers her rightful property.
Following a fight and a stagecoach chase, Crane eventually thwarts Lady Van Tassel by throwing the skull to the Horseman, which causes his head to become reattached to his body and the curse broken. The Horseman, no longer under Lady Van Tassel's control, simultaneously kisses and bites her, and hoists her up on his horse. He then rides to Hell, taking her with him, fulfilling her end of the deal with the Devil. Crane returns home to New York with Katrina and Young Masbath, just in time for the new century.
Production.
Development.
In 1993, Kevin Yagher, a make-up effects designer who had turned to directing with "Tales from the Crypt", had the notion to adapt Washington Irving's short story "The Legend of Sleepy Hollow" into a feature film. Through his agent, Yagher was introduced to Andrew Kevin Walker; they spent a few months working on a film treatment that transformed Ichabod Crane as a schoolmaster from Connecticut to a banished New York City detective. Yagher and Walker subsequently pitched "Sleepy Hollow" to various studios and production companies, eventually securing a deal with producer Scott Rudin, who had been impressed with Walker's unproduced spec script for "Seven". Rudin optioned the project to Paramount Pictures in a deal that had Yagher set to direct, with Walker scripting; the pair would share story credit. Following the completion of "", Yahger had planned "Sleepy Hollow" as a low-budget production—"a pretentious slasher film with a spectacular murder every five minutes or so." Paramount disagreed on the concept and demoted Yagher's involvement to prosthetic makeup designer. "They never really saw it as a commercial movie," producer Adam Schroeder noted. "The studio thinks 'old literary classic' and they think "The Crucible". We started developing it before horror movies came back."
Paramount CEO Sherry Lansing revived studio interest in 1998. Schroeder, who shepherded Tim Burton's "Edward Scissorhands" as a studio executive at 20th Century Fox in 1990, suggested that Burton direct the film. Francis Ford Coppola's minimal production duties came from American Zoetrope; Burton only became aware of Coppola's involvement during the editing process when he was sent a copy of "Sleepy Hollow"s trailer and saw Coppola's name on it. Burton, coming off the troubled production of "Superman Lives", was hired to direct in June 1998. "I had never really done something that was more of a horror film," he explained, "and it's funny, because those are the kind of movies that I like probably more than any other genre." His interest in directing a horror film influenced by his love for Hammer Film Productions and "Black Sunday"—particularly the supernatural feel they evoked as a result of being filmed primarily on sound stages. As a result, "Sleepy Hollow" is a homage to various Hammer Film Productions, including "Dr. Jekyll and Sister Hyde", and other films such as "Frankenstein", "Bride of Frankenstein", various Roger Corman horror films, "Jason and the Argonauts", and "Scream Blacula Scream". The image of the Headless Horseman had fascinated Burton during his apprenticeship as a Disney animator at CalArts in the early 1980s. "One of my teachers had worked on the Disney version as one of the layout artists on the chase, and he brought in some layouts from it, so that was exciting. It was one of the things that maybe shaped what I like to do." Burton worked with Walker on rewrites, but Rudin suggested that Tom Stoppard rewrite the script to add to the comical aspects of Ichabod's bumbling mannerisms, and emphasize the character's romance with Katrina. His work went uncredited through the WGA screenwriting credit system.
While Johnny Depp was Burton's first choice for the role of Ichabod Crane, Paramount required him to consider Brad Pitt, Liam Neeson and Daniel Day-Lewis. Depp was cast in July 1998 for his third collaboration with Burton. The actor wanted Ichabod to parallel Irving's description of the character in the short story. This included a long prosthetic snipe nose, huge ears, and elongated fingers. Paramount turned down his suggestions, and after Depp read Tom Stoppard's rewrite of the script, he was inspired to take the character even further. "I always thought of Ichabod as a very delicate, fragile person who was maybe a little too in touch with his feminine side, like a frightened little girl," Depp explained. He did not wish to portray the character as a typical action star would have, and instead took inspiration by Angela Lansbury's performance in "Death on the Nile". "It's good," Burton reasoned, "because I'm not the greatest action director in the world, and he's not the greatest action star." Depp modeled Ichabod's detective personality from Basil Rathbone in the 1939 "Sherlock Holmes" film series. He also studied Roddy McDowall's acting for additional influence. Burton added that "the idea was to try and find an elegance in action of the kind that Christopher Lee or Peter Cushing or Vincent Price had." Christina Ricci, who worked with producer Scott Rudin on "The Addams Family", was cast as Katrina Van Tassel. "Sleepy Hollow" also reunited Burton with Jeffrey Jones (from "Beetlejuice" and "Ed Wood") as Reverent Steenwyck, Christopher Walken (Max Schreck in "Batman Returns") as the Hessian Horseman, Martin Landau ("Ed Wood") in a cameo role, and Hammer veteran Michael Gough (Alfred in Burton's "Batman" films), whom Burton tempted out of retirement. The Hammer influence was further confirmed by the casting of Christopher Lee in a small role as the Burgomaster who sends Crane to Sleepy Hollow.
Filming.
The original intention had been to shoot "Sleepy Hollow" predominantly on location with a $30 million budget. Towns were scouted throughout Upstate New York along the Hudson Valley, and the filmmakers decided on Tarrytown for an October 1998 start date. The Historic Hudson Valley organization assisted in scouting locations, which included the Philipsburg Manor House and forests in the Rockefeller State Park Preserve. "They had a wonderful quality to them," production designer Rick Heinrichs reflected on the locations, "but it wasn't quite lending itself to the sort of expressionism that we were going for, which wanted to express the feeling of foreboding." Disappointed, the filmmakers scouted locations in Sturbridge, Massachusetts, and considered using Dutch colonial villages and period town recreations in the Northeastern United States. When no suitable existing location could be found, coupled with a lack of readily available studio space in the New York area needed to house the production's large number of sets, producer Scott Rudin suggested the UK.
Rudin believed England offered the level of craftsmanship in period detail, painting and costuming that was suitable for the film's design. Having directed "Batman" entirely in Britain, Burton agreed, and designers from "Batman"s art department were employed by Paramount for "Sleepy Hollow". As a result, principal photography was pushed back to November 20, 1998 at Leavesden Film Studios, which had been recently vacated by "". The majority of filming took place at Leavesden, with studio other work at Shepperton Studios, where the massive Tree of the Dead set was built using Stage H. Production then moved to the Hambleden estate at Lime Tree Valley for a month-long shoot in March, where the town of Sleepy Hollow was constructed. "We came to England figuring we would find a perfect little town," producer Adam Schroeder recalled, "and then we had to build it anyway." Filming in Britain continued through April, and a few last minute scenes were shot using a sound stage in Yonkers, New York the following May.
Design.
Responsible for the film's production design was Rick Heinrichs, who Burton intended to use on "Superman Lives". While the production crew was always going to build a substantial number of sets, the decision was taken early on that to fulfill Burton's vision best would necessitate shooting "Sleepy Hollow" in a totally controlled environment at Leavesden Film Studios. The production design was influenced by Burton's love for Hammer Film Productions and "Black Sunday"—particularly the supernatural feel they evoked as a result of being filmed primarily on sound stages. Heinrichs was also influenced by American colonial architecture, German Expressionism, Dr. Seuss illustrations, and Hammer's "Dracula Has Risen from the Grave". One sound stage at Leavesden was dedicated to the "Forest to Field" set, for the scene in which the Headless Horseman races out of the woods and into a field. This stage was then transformed into, variously, a graveyard, a corn field, a field of harvested wheat, a churchyard, and a snowy battlefield. In addition, a small backlot area was devoted to a New York City street and waterfront tank.
Cinematography.
Burton was impressed by the cinematography in "Great Expectations", and hired Emmanuel Lubezki as "Sleepy Hollow"s director of photography. Initially, Lubezki and Burton contemplated shooting the film in black and white and in old square Academy ratio. When that proved unfeasible, they opted for an almost monochromatic effect which would enhance the fantasy aspect. Burton and Lubezki intentionally planned the over-dependency of smoke and soft lighting to accompany the film's sole wide-angle lens strategy. Lubezki also used Hammer horror and Mexican lucha films from the 1960s, such as "Santo Contra los Zombis" and "Santo vs. las Mujeres Vampiro". Lighting effects increased the dynamic energy of the Headless Horseman, while the contrast of the film stock was increased in post-production to add to the monochromatic feel.
Leavesden Studios, a converted airplane factory, presented problems because of its relatively low ceilings. This was less of an issue for "The Phantom Menace", in which set height was generally achieved by digital means. "Our visual choices get channeled and violent," Heinrichs elaborated, "so you end up with liabilities that you tend to exploit as virtues. When you've got a certain ceiling height, and you're dealing with painted backings, you need to push atmosphere and diffusion." This was particularly the case in several exteriors that were built on sound stages. "We would mitigate the disadvantages by hiding lights with teasers and smoke."
Visual effects.
The majority of "Sleepy Hollow"s 150 visual effects shots were handled by Industrial Light & Magic (ILM), while Kevin Yagher supervised the human and creature effects. Framestore also assisted on digital effects, and The Mill handled motion control photography. In part a reaction to the computer-generated effects in "Mars Attacks!", Burton opted to use as limited an amount of digital effects as possible. Ray Park, who served as the Headless Horseman stunt double, wore a blue ski mask for the chroma key effect, digitally removed by ILM. Burton and Heinrichs applied to "Sleepy Hollow" many of the techniques they had used in stop motion animation on "Vincent"—such as forced perspective sets.
The windmill was a 60-foot-tall forced-perspective exterior (visible to highway travellers miles away), a base and rooftop set and a quarter-scale miniature. The interior of the mill, which was about 30-feet high and 25-feet wide, featured wooden gears equipped with mechanisms for grinding flour. A wider view of the windmill was rendered on a Leavesden soundstage set with a quarter-scale windmill, complete with rotating vanes, painted sky backdrop and special-effects fire. "It was scary for the actors who were having burning wood explode at them," Heinrichs recalled. "There were controls in place and people standing by with hoses, of course, but there's always a chance of something going wrong." For the final shot of the burning mill exploding, the quarter-scale windmill and painted backdrop were erected against the outside wall of the "flight shed", a spacious hangar on the far side of Leavesden Studios. The hangar's interior walls were knocked down to create a 450-foot run, with a 40-foot width still allowing for coach and cameras. Heinrichs tailored the sets so cinematographer Emmanuel Lubezki could shoot from above without seeing the end of the stage.
Actor Ian McDiarmid, who portrayed Dr. Lancaster, had just finished another Leavesden production with "". He compared the aesthetics of the two films, stating that physical sets helped the actors get into a natural frame of mind. "Having come from the blue-screen world of "Star Wars" it was wonderful to see gigantic, beautifully made perspective sets and wonderful clothes, and also people recreating a world. It's like the way movies used to be done."
Musical score.
The film score was written and produced by Danny Elfman. It won the Golden Satellite Award and was also nominated by the Las Vegas Film Critics.
Track listing.
Tracks marked with ♦ are only available as a bonus track on disc 8 of the Danny Elfman / Tim Burton 25th Anniversary Music Box.
The track numbers listed here do not therefore correspond to the original 1999 album.
Release.
To promote "Sleepy Hollow", Paramount Pictures featured the film's trailer at San Diego Comic-Con International in August 1999. The following October, the studio launched a website, which "Variety" described as being the "most ambitious online launch of a motion picture to date." The site (sleepyhollowmovie.com) offered visitors live video chats with several of the filmmakers hosted by Yahoo! Movies and enabled them to send postcards, view photos, trailers and a six-minute behind-the-scenes featurette edited from a broadcast that aired on "Entertainment Tonight". Extensive tours of 10 sets where offered, where visitors were able to roam around photographs, including the sets for the entire town of Sleepy Hollow, forest, church, graveyard and covered bridge. Arthur Cohen, president of worldwide marketing for Paramount, explained that the "Web-friendly" pre-release reports from websites such as Ain't It Cool News and Dark Horizons encouraged the studio to create the site. In the weeks pre-dating the release of "Sleepy Hollow", a toy line was marketed by McFarlane Toys. Simon & Schuster also published "The Art of Sleepy Hollow" (ISBN 0671036572), which included the film's screenplay and an introduction by Tim Burton. A novelization, also published by Simon & Schuster, was written by Peter Lerangis.
"Sleepy Hollow" was released in the United States on November 19, 1999 in 3,069 theaters, grossing $30,060,467 in its opening weekend at the #2 spot behind "The World Is Not Enough". "Sleepy Hollow" eventually earned $101,071,502 in domestic gross, and $105 million in foreign sales, coming to a worldwide total of $206,071,502. David Walsh of the National Institute on Media and the Family criticized the film's financial success from the exaggeration of gore. "The real impact is not so much that violent images create violent behavior," Walsh explained, "but that they create an atmosphere of disrespect." Burton addressed the concerns as a matter of opinion. "Everyone has a different perception of things. When I was a kid," Burton continued, "I was probably more scared by seeing John Wayne or Barbra Streisand on the big screen than by seeing violence."
Paramount Home Video first released "Sleepy Hollow" on DVD in the United States on May 23, 2000. The HD DVD release came in July 2006, while the film was released on Blu-ray Disc two years later, in June 2008.
Reception.
Film review aggregator Rotten Tomatoes reports that 67% of critics gave the film a "Certified Fresh" rating, based on 126 reviews with an average score of 6.2/10, with the site's consensus stating, ""Sleepy Hollow" entertains with its stunning visuals and creepy atmosphere." Metacritic, another review aggregator, assigned the film a weighted average score of 65 (out of 100) based on 35 reviews from mainstream critics, considered to be "generally favorable".
Roger Ebert praised Johnny Depp's performance and Tim Burton's methods of visual design. "Johnny Depp is an actor able to disappear into characters," Ebert continued, "never more readily than in one of Burton's films." Richard Corliss wrote, in his review for "TIME Magazine", "Burton's richest, prettiest, weirdest since "Batman Returns". The simple story bends to his twists, freeing him for an exercise in high style."
David Sterritt of "The Christian Science Monitor" highly praised Burton's filmmaking and the high-spirited acting of cast, but believed Andrew Kevin Walker's writing was too repetitious and formulaic for the third act. "You go into a Tim Burton film wanting to be transported, but "Sleepy Hollow" is little more than a lavish, art-directed slasher movie."
Owen Gleiberman from "Entertainment Weekly" wrote "Sleepy Hollow" is "a choppily plotted crowd-pleaser that lacks the seductive, freakazoid alchemy of Burton's best work." Gleiberman compared the film to "The Mummy", and said "it feels like every high-powered action climax of the last 10 years. Personally, I'd rather see Burton so intoxicated by a movie that he lost his head."
Andrew Johnston of "Time Out New York" wrote: "Like the best of Burton's films, "Sleepy Hollow" takes place in a world so richly imagined that, despite its abundant terrors, you can't help wanting to step through the screen."
Mick LaSalle, writing in the "San Francisco Chronicle", criticized Burton's perceived image as a creative artist. "All "Sleepy Hollow" has going for it is art direction, and even in that it falls back on cliché." Doug Walker linked the film to the Hammer Films style of horror cinematography, considering it an homage to those movies, comparing the usage of dignified British actors, choices in color and movie sets and character relations. Walker gave it the merit of recreating the "very specific genre" of Hammer Films, citing the skill and "clever casting" Burton used to manage this.
Jonathan Rosenbaum from the "Chicago Reader" called "Sleepy Hollow" "a ravishing visual experience, a pretty good vehicle for some talented American and English actors," but concluded that the film was a missed opportunity to depict an actual representation of the short story. "Burton's fidelity is exclusively to the period feeling he gets from disreputable Hammer horror films and a few images culled from "Ichabod and Mr. Toad". When it comes to one of America's great stories, Burton obviously couldn't care less."
Accolades.
American Film Institute recognition:

</doc>
<doc id="55602" url="https://en.wikipedia.org/wiki?curid=55602" title="Peanut">
Peanut

Peanut, also known as groundnut ("Arachis hypogaea"), is a crop of global importance. It is widely grown in the tropics and subtropics, being important to both smallholder and large commercial producers. It is classified as both a grain legume, and, because of its high oil content, an oil crop. World annual production is about 46 million tonnes per year. Very unusual among crop plants, peanut pods develop under the ground. 
As a legume, peanut belongs to the botanical family Fabaceae (also known as Leguminosae, and commonly known as the bean or pea family). Like most other legumes, peanuts harbor symbiotic nitrogen-fixing bacteria in root nodules. This capacity to fix nitrogen means peanuts require less nitrogen-containing fertilizer and improve soil fertility, making them valuable in crop rotations.
Peanuts are similar in taste and nutritional profile to tree nuts such as walnuts and almonds, and are often served in similar ways in Western cuisines. The botanical definition of a "nut" is a fruit whose ovary wall becomes very hard at maturity. Using this criterion, the peanut is not a nut, but rather a legume. However, for culinary purposes and in common English language usage, peanuts are usually referred to as nuts.
History.
Cultivated peanut ("A. hypogaea") has two sets of chromosomes from two different species, thought to be "A. duranensis" and "A. ipaensis". The two species' chromosomes combined by hybridization and doubling, to form what is termed an amphidiploid or allotetraploid. Genetic analysis suggests this hybridization event probably occurred only once and gave rise to "A. monticola", a wild form of peanut that occurs in a few restricted locations in northwestern Argentina, and by artificial selection to "A. hypogaea". The process of domestication through artificial selection made "A. hypogaea" dramatically different from its wild relatives. The domesticated plants are more bushy and compact, and have a different pod structure and larger seeds. The initial domestication may have taken place in northwestern Argentina, or in southeastern Bolivia, where the peanut landraces with the most wild-like features are grown today. From this primary center of origin, cultivation spread and formed secondary and tertiary centers of diversity in Peru, Ecuador, Brazil, Paraguay, and Uruguay. Over time, thousands of peanut landraces evolved; these are classified into six botanical varieties and two subspecies (as listed in the peanut scientific classification table). Subspecies "A. h. fastigiata" types are more upright in their growth habit and have a shorter crop cycles. Subspecies "A. h. hypogaea" types spread more on the ground and have longer crop cycles.
The oldest known archeological remains of pods have been dated at about 7,600 years old. These may be pods from a wild species that was in cultivation, or "A. hypogaea" in the early phase of domestication. They were found in Peru, where dry climatic conditions are favorable to the preservation of organic material. Almost certainly, peanut cultivation antedated this at the center of origin where the climate is moister. Many pre-Columbian cultures, such as the Moche, depicted peanuts in their art. Cultivation was well established in Mesoamerica before the Spanish arrived. There, the conquistadors found the "tlalcacahuatl" (the plant's Nahuatl name, whence Mexican Spanish "cacahuate", Castillian Spanish "cacahuete", and French "cacahuète") being offered for sale in the marketplace of Tenochtitlan. The peanut was later spread worldwide by European traders, and cultivation is now very widespread in tropical and subtropical regions. In West Africa, it substantially replaced a crop plant from the same family, the Bambara groundnut, whose seed pods also develop underground. In Asia, it became an agricultural mainstay and this region is now the largest producer in the world.
In the English-speaking world, peanut growing is most important in the United States. Although it was mainly a garden crop for much of the colonial period, it was mostly used as animal feed stock until the 1930s. The US Department of Agriculture initiated a program to encourage agricultural production and human consumption of peanuts in the late 19th and early 20th centuries. George Washington Carver developed hundreds of recipes for peanuts during his tenure in the program.
Botany.
Peanut is an annual herbaceous plant growing tall. As a legume, it belongs to the botanical family Fabaceae (also known as Leguminosae, and commonly known as the bean or pea family). Like most other legumes, peanuts harbor symbiotic nitrogen-fixing bacteria in their root nodules. The leaves are opposite and pinnate with four leaflets (two opposite pairs; no terminal leaflet); each leaflet is 1 to 7 cm (⅜ to 2¾ in) long and 1 to 3 cm (⅜ to 1 in) across. Like many other legumes, the leaves are nyctinastic, that is, they have "sleep" movements, closing at night.
The specific name, "hypogaea" means "under the earth", because peanut pods develop underground, a feature known as geocarpy. The flowers are 1.0 to 1.5 cm (0.4 to 0.6 in) across, and yellowish orange with reddish veining. They are borne in axillary clusters on the stems above ground and last for just one day. In structure, they appear superficially similar to the flowers of peas and beans. However, intriguing differences are seen. The ovary is not positioned where expected, but is at the base of what appears to be the flower stem (in fact, this "stem" is a highly elongated floral cup). After fertilization, a short stalk at the base of the ovary (termed a pedicel) elongates to form a thread-like structure known as a "peg". This pushes the ovary down into the soil, where it develops into a mature peanut pod. Pods are long, normally containing one to four seeds.
Cultivation.
Peanuts grow best in light, sandy loam soil with a pH of 5.9–7. Their capacity to fix nitrogen means that, providing they nodulate properly, peanuts benefit little or not at all from nitrogen-containing fertilizer, and they improve soil fertility. Therefore, they are valuable in crop rotations. Also, the yield of the peanut crop itself is increased in rotations, through reduced diseases, pests and weeds. For instance, in Texas, peanuts in a three-year rotation with corn yield 50% more than nonrotated peanuts. Adequate levels of phosphorus, potassium, calcium, magnesium, and micronutrients are also necessary for good yields. To develop well, peanuts need warm weather throughout the growing season. They can be grown with as little as 350 mm of water, but for best yields need at least 500 mm. Depending on growing conditions and the cultivar of peanut, harvest is usually 90 to 130 days after planting for subspecies "A. h. fastigiata" types, and 120 to 150 days after planting for subspecies "A. h. hypogaea" types. Subspecies "A. h. hypogaea" types yield more, and are usually preferred where the growing seasons are long enough.
Peanut plants continue to produce flowers when pods are developing, therefore even when they are ready for harvest, some pods are immature. The timing of harvest is an important decision to maximize yield. If it is too early, too many pods will be unripe. If too late, the pods will snap off at the stalk, and will remain in the soil. For harvesting, the entire plant, including most of the roots, is removed from the soil. The fruits have wrinkled shells that are constricted between pairs of the one to four (usually two) seeds per pod.
Harvesting occurs in two stages: In mechanized systems, a machine is used to cut off the main root of the peanut plant by cutting through the soil just below the level of the peanut pods. The machine lifts the "bush" from the ground and shakes it, then inverts the bush, leaving the plant upside down on the ground to keep the peanuts out of the soil. This allows the peanuts to dry slowly to a little less than a third of their original moisture level over a period of three to four days. Traditionally, peanuts were pulled and inverted by hand.
After the peanuts have dried sufficiently, they are threshed, removing the peanut pods from the rest of the bush. It is particularly important that peanuts are dried properly and stored in dry conditions. If they are too high in moisture, or if storage conditions are poor, they may become infected by the mold fungus "Aspergillus flavus". The fungus releases a toxic and highly carcinogenic substance aflatoxin.
Cultivars in the United States.
Thousands of peanut cultivars are grown, with four major cultivar groups being the most popular: Spanish, Runner, Virginia, and Valencia. Certain cultivar groups are preferred for particular characteristics, such as differences in flavor, oil content, size, shape, and disease resistance. Most peanuts marketed in the shell are of the Virginia type, along with some Valencias selected for large size and the attractive appearance of the shell. Spanish peanuts are used mostly for peanut candy, salted nuts, and peanut butter.
Each year, new cultivars of peanuts are bred and introduced, creating changes in the planting rate, adjusting the planter, harvester, dryer, cleaner, sheller, and the method of marketing.
Spanish group.
The small Spanish types are grown in South Africa, and in the southwestern and southeastern US. Prior to 1940, 90% of the peanuts grown in Georgia, USA, were Spanish types, but the trend since then has been larger-seeded, higher-yielding, more disease-resistant cultivars. Spanish peanuts have a higher oil content than other types of peanuts, and in the US are now primarily grown in New Mexico, Oklahoma, and Texas.
Cultivars of the Spanish group include 'Dixie Spanish', 'Improved Spanish 2B', 'GFA Spanish', 'Argentine', 'Spantex', 'Spanette', 'Shaffers Spanish', 'Natal Common (Spanish)', "White Kernel Varieties', 'Starr', 'Comet', 'Florispan', 'Spanhoma', 'Spancross', 'OLin', 'Tamspan 90', 'AT 9899–14', 'Spanco', 'Wilco I', 'GG 2', 'GG 4', 'TMV 2', and 'Tamnut 06'.
Runner group.
Since 1940, the southeastern US region has seen a shift to production of Runner group peanuts. This shift is due to good flavor, better roasting characteristics and higher yields when compared to Spanish types, leading to food manufacturers' preference for the use in peanut butter and salted nuts. Georgia's production is now almost 100% Runner type.
Cultivars of Runners include 'Southeastern Runner 56-15', 'Dixie Runner', 'Early Runner', 'Virginia Bunch 67', 'Bradford Runner', 'Egyptian Giant' (also known as 'Virginia Bunch' and 'Giant'), 'Rhodesian Spanish Bunch' (Valencia and Virginia Bunch), 'North Carolina Runner 56-15', 'Florunner', 'Virugard', 'Georgia Green', 'Tamrun 96', 'Flavor Runner 458', 'Tamrun OL01', 'Tamrun OL02' 'AT-120', 'Andru-93', 'Southern Runner', 'AT1-1', 'Georgia Brown', 'GK-7',and 'AT-108'.
Virginia group.
The large-seeded Virginia group peanuts are grown in the US states of Virginia, North Carolina, Tennessee, Texas, New Mexico, Oklahoma, and parts of Georgia. They are increasing in popularity due to demand for large peanuts for processing, particularly for salting, confections, and roasting in the shells.
Virginia group peanuts are either bunch or running in growth habit. The bunch type is upright to spreading. It attains a height of , and a spread of , with rows that seldom cover the ground. The pods are borne within 5 to 10 cm of the base of the plant.
Cultivars of Virginia type peanuts include 'NC 7', 'NC 9', 'NC 10C', 'NC-V 11', 'VA 93B', 'NC 12C', 'VA-C 92R', 'Gregory', 'VA 98R', 'Perry', 'Wilson, 'Hull', 'AT VC-2' and' Shulamit'.
Valencia group.
Valencia group peanuts are coarse, and they have heavy reddish stems and large foliage. In the United States, large commercial production is primarily in the South Plains of West Texas and eastern New Mexico near and south of Portales, New Mexico, but they are grown on a small scale elsewhere in the South as the best-flavored and preferred type for boiled peanuts. They are comparatively tall, having a height of and a spread of . Peanut pods are borne on pegs arising from the main stem and the side branches. Most of the pods are clustered around the base of the plant, and only a few are found several inches away. Valencia types are three- to five-seeded and smooth, with no constriction of the shell between the seeds. Seeds are oval and tightly crowded into the pods. Typical seed weight is 0.4 to 0.5 g. This type is used heavily for sale roasted and salted in-shell peanuts and peanut butter. Varieties include 'Valencia A' and 'Valencia C'.
Tennessee Red and Tennessee White groups.
These are alike, except for the color of the seed. Sometimes known also as Texas Red or White, the plants are similar to Valencia types, except the stems are green to greenish brown, and the pods are rough, irregular, and have a smaller proportion of kernels.
Production and trade.
Peanut is widely produced in tropical and subtropical regions of the world. China accounts for 37% of world production, Africa for 25%, India for 21%, the Americas for 8% and Oceania for 6% (for major producing countries see table). Major exporters are India, which accounts for 37% of world exports, Argentina for 13%, the United States for 10%, China for 8% and Malawi for 5%. Major importers are the Netherlands, which accounts for 17% of world imports (most being shipped on to other countries in the European Union), Indonesia that accounts for 10%, Mexico for 7%, Germany for 6% and Russia for 5%.
In the United States, Georgia is the leading peanut-producing state, followed by Texas and Alabama, respectively. About half of all peanuts produced in the US are grown within a radius of Dothan, Alabama. Dothan is home to the National Peanut Festival established in 1938 and held each fall to honor peanut growers and celebrate the harvest.
Food.
Peanut oil.
Peanut oil is often used in cooking, because it has a mild flavor and a relatively high smoke point. Due to its high monounsaturated content, it is considered healthier than saturated oils, and is resistant to rancidity. The several types of peanut oil include: aromatic roasted peanut oil, refined peanut oil, extra virgin or cold-pressed peanut oil, and peanut extract. In the United States, refined peanut oil is exempt from allergen labeling laws.
Peanut flour.
Peanut flour is lower in fat than peanut butter, and is popular with chefs because its high protein content makes it suitable as a flavor enhancer. Peanut flour is used as a gluten-free solution.
Boiled peanuts.
Boiled peanuts are a popular snack in the southern United States, as well as in India, China, and West Africa. In the US South, boiled peanuts are often prepared in briney water, and sold in streetside stands.
Dry-roasted peanuts.
Dry peanuts can be roasted in the shell or shelled in a home oven if spread out one layer deep in a pan and baked at a temperature of 350 °F or 177 °C for 15 to 20 min (shelled) and 20 to 25 min (in shell).
Cuisine.
Latin America.
Peanuts are particularly common in Peruvian and Mexican cuisine, both of which marry indigenous and European ingredients. For instance, in Peru, a popular traditional dish is "picante de cuy", a roasted guinea pig served in a sauce of ground peanuts (ingredients native to South America) with roasted onions and garlic (ingredients from European cuisine). Also, in the city of Arequipa in Peru, a well known dish called "ocopa" consists of a smooth sauce of roasted peanuts and hot peppers (both native to the region) with roasted onions, garlic, and oil, poured over meat or potatoes. Another example is a fricassee combining a similar mixture with sautéed seafood or boiled and shredded chicken. These dishes are generally known as "ajíes", meaning "hot peppers", such as "ají de pollo" and "ají de mariscos" (seafood "ajíes" may omit peanuts).
Likewise, during colonial times, the Spanish in Peru used peanuts to replace nuts unavailable in Peru, but used extensively in Spanish cuisine, such as almonds, pine nuts, and other nuts, typically ground or as paste and mixed with rice, meats, and vegetables for dishes such as rice pilaf.
Throughout the region, many candies and snacks are made using peanuts as a base.
Middle East.
Crunchy coated peanuts, called "kabukim" in Hebrew, are a popular snack in Israel. "Kabukim" are commonly sold by weight at corner stores where fresh nuts and seeds are sold, though they are also available packaged. The coating typically consists of flour, salt, starch, lecithin, and sometimes sesame seeds. The origin of the name is obscure (it may be derived from "kabuk" which means nutshell or husk in Turkish). An additional variety of crunchy coated peanuts popular in Israel is "American peanuts". The coating of this variety is thinner, but harder to crack.
Another popular Israeli peanut snack, Bamba puffs, is similar in shape to Cheez Doodles, but are made of corn and flavored with peanut butter.
Southeast Asia.
Peanuts are also widely used in Southeast Asian cuisine, such as in Malaysia, Vietnam, and Indonesia, where they are typically made into a spicy sauce. Peanuts originally came to Indonesia from the Philippines, where the legume came from Mexico in times of Spanish colonization. Some of the most famous Philippine dish using peanuts is the "kare-kare", a mixture of meat and peanut butter.
Common Indonesian peanut-based dishes include "gado-gado", "pecel", "karedok", and "ketoprak", all vegetable salads mixed with peanut sauce, and the peanut-based sauce for "satay".
South Asia.
In the Indian subcontinent, peanuts are known as a light snack by themselves, usually roasted and salted (sometimes with the addition of chilli powder), and often sold roasted in pod, or boiled with salt. They are also made into little dessert or sweet snack pieces by processing with refined sugar and jaggery. Indian cuisine uses roasted, crushed peanuts to give a crunchy body to salads; they are added whole (without pods) to leafy vegetable stews for the same reason. Another use of peanut oil as cooking oil. Most Indians use mustard, sunflower, and peanut oil for cooking.
In South india, groundnut 'chutney' is a popular combination, usually partaken with "dosa" and "idli" at breakfast.
West Africa.
Peanuts grow well in southern Mali and adjacent regions of the Ivory Coast, Burkina Faso, Ghana, Nigeria, and Senegal; peanuts are similar in both agricultural and culinary qualities to the Bambara groundnut native to the region, and West Africans have adopted the crop as a staple. Peanut sauce, prepared with onions, garlic, peanut butter/paste, and vegetables such as carrots, cabbage, and cauliflower, can be vegetarian (the peanuts supplying ample protein) or prepared with meat, usually chicken.
Peanuts are used in the Malian meat stew "maafe". In Ghana, peanut butter is used for peanut butter soup "nkate nkwan". Crushed peanuts may also be used for peanut candies "nkate cake" and "kuli-kuli", as well as other local foods such as "oto". Peanut butter is also an ingredient in Nigeria's "African salad".
Peanut powder is an important ingredient in the spicy coating for kebabs in Nigeria and Ghana.
East Africa.
Peanuts are a common ingredient of several types of relishes (dishes which accompany "nshima") eaten by the tribes in Malawi and in the eastern part of Zambia, and these dishes are now common throughout both countries. Thick peanut butter sauces are also made in Uganda to go with rice and other starchy foods. Groundnut stew, locally called "ebinyebwa" in Uganda, is made by boiling ground peanut flour with other ingredients, such as cabbage, mushrooms, dried fish, meat or other vegetables. Across East Africa, roasted peanuts, often in cones of newspaper, are a popular snack sold in the street.
North America.
In Canada and the US, peanuts are used in candies, cakes, cookies, and other sweets. They are also enjoyed roasted and salted. Peanut butter is one of the most popular peanut-based foods in the US, and for 400 years, recipes for peanut soup have been present in the South, Virginia in particular. In some southern portions of the US, peanuts are boiled for several hours until soft and moist. Peanuts are also deep-fried, shell and all.
Malnutrition.
Peanuts are used to help fight malnutrition. Plumpy Nut, MANA Nutrition, and Medika Mamba are high-protein, high-energy, and high-nutrient peanut-based pastes developed to be used as a therapeutic food to aid in famine relief. The World Health Organization, UNICEF, Project Peanut Butter, and Doctors Without Borders have used these products to help save malnourished children in developing countries.
Peanuts can be used like other legumes and grains to make a lactose-free, milk-like beverage, peanut milk, which is promoted in Africa as a way to reduce malnutrition among children.
Animal feed.
Peanut plant tops are used for hay.
The protein cake (oilcake meal) residue from oil processing is used as an animal feed and as a soil fertilizer. Groundnut cake is a livestock feed, mostly used by cattle as protein supplements. It is one of the most important and valuable feed for all types of livestocks and one of the most active ingredient for poultry rations. Poor storage of the cake may sometimes results in its contamination by aflatoxin, a naturally occurring Mycotoxins that are produced by Aspergillus flavus and Aspergillus parasiticus. The major constituents of the cake are essential amino acids such as lysine and glutamine. Other components are crude fiber, crude protein, and fat.
Industrial use.
Peanuts have a variety of industrial end uses. Paint, varnish, lubricating oil, leather dressings, furniture polish, insecticides, and nitroglycerin are made from peanut oil. Soap is made from saponified oil, and many cosmetics contain peanut oil and its derivatives. The protein portion is used in the manufacture of some textile fibers. Peanut shells are used in the manufacture of plastic, wallboard, abrasives, fuel, cellulose (used in rayon and paper), and mucilage (glue).
Nutritional value.
Peanuts are rich in essential nutrients (right table, USDA nutrient data). In a 100 g serving, peanuts provide 570 calories and are an excellent source (defined as more than 20% of the Daily Value, DV) of several B vitamins, vitamin E, several dietary minerals, such as manganese (95% DV), magnesium (52% DV) and phosphorus (48% DV), and dietary fiber (right table). They also contain about 25 g protein per 100 g serving, a higher proportion than in many tree nuts.
Some studies show that regular consumption of peanuts is associated with a lower risk of mortality specifically from certain diseases. However, the study designs do not allow cause and effect to be inferred. According to the US Food and Drug Administration, "Scientific evidence suggests but does not prove that eating 1.5 ounces per day of most nuts (such as peanuts) as part of a diet low in saturated fat and cholesterol may reduce the risk of heart disease."
Phytochemicals.
Recent research on peanuts has found polyphenols and other phytochemicals that are under basic research for their potential to provide health benefits. New research shows peanuts, especially the skins, to have comparable polyphenol content of many fruits.
Peanut skins are a significant source of resveratrol, a phenolic under research for a variety of potential effects in humans.
Oil composition.
A common cooking and salad oil, peanut oil is 46% monounsaturated fats (primarily oleic acid), 32% polyunsaturated fats (primarily linoleic acid), and 17% saturated fats (primarily palmitic acid). Extractable from whole peanuts using a simple water and centrifugation method, the oil is being considered by NASA's Advanced Life Support program for future long-duration human space missions.
Health concerns.
Allergies.
Some people (0.6% of the United States population) report that they experience mild to severe allergic reactions to peanut exposure; symptoms can range from watery eyes to anaphylactic shock, which can be fatal if untreated. For these individuals, eating a small amount of peanuts can cause a reaction. Because of their widespread use in prepared and packaged foods, the avoidance of peanuts is difficult. Some foods processed in facilities which also handle peanuts may carry such warnings on their labels.
The hygiene hypothesis of allergy states that a lack of early childhood exposure to infectious agents like germs and parasites could be causing the increase of food allergies.
Studies comparing age of peanut introduction in Great Britain with introduction in Israel showed that delaying exposure to peanuts can dramatically increase the risk of developing peanut allergies.
Peanut allergy has been associated with the use of skin preparations containing peanut oil among children, but the evidence is not regarded as conclusive. Peanut allergies have also been associated with family history and intake of soy products.
Though the allergy can last a lifetime, one study indicates that 23.3% of children will outgrow a peanut allergy.
Some school districts in the United States have banned peanuts. There are experimental techniques which appear to have desensitized some allergic individuals. The most popular technique, oral immunotherapy, works to create desensitization in those allergic by feeding them small amounts of peanuts until their bodies become desensitized.
Refined peanut oil will not cause allergic reactions in most people with peanut allergies. However, crude (unrefined) peanut oils have been shown to contain protein, which may cause allergic reactions. In a randomized, double-blind crossover study, 60 people with proven peanut allergy were challenged with both crude peanut oil and refined peanut oil. The authors concluded, "Crude peanut oil caused allergic reactions in 10% of allergic subjects studied and should continue to be avoided." They also stated, "Refined peanut oil does not seem to pose a risk to most people with peanut allergy." However, they point out that refined peanut oil can still pose a risk to peanut-allergic individuals if oil that has previously been used to cook foods containing peanuts is reused.
Contamination with aflatoxin.
Peanuts may be contaminated with the mold "Aspergillus flavus" which produces a carcinogenic substance called aflatoxin. Lower-quality specimens, particularly where mold is evident, are more likely to be contaminated. The United States Department of Agriculture tests every truckload of raw peanuts for aflatoxin; any containing aflatoxin levels of more than 15 parts per billion are destroyed. The peanut industry has manufacturing steps in place to ensure all peanuts are inspected for aflatoxin.
United States Department of Agriculture program.
George Washington Carver is often credited with inventing 300 different uses for peanuts (which, contrary to popular belief, did not include peanut butter, but did include salted peanuts). Carver was one of many United States Department of Agriculture researchers who encouraged cotton farmers in the South to grow peanuts instead of, or in addition to, cotton, because cotton had depleted so much nitrogen from the soil, and one of the peanut's properties as a legume is to put nitrogen back into the soil (nitrogen fixation).
Rising demand for peanuts in the early 20th century was due to a shortage of plant oils during World War I and the growing popularity of peanut butter, roasted peanuts, and peanut candies. Peanut products originating around the early 20th century include many brands still sold today such as Cracker Jack (1893), Planters peanuts (1906), Oh Henry! candy bar (1920), Baby Ruth candy bar (1920), Butterfinger candy bar (1923), Mr. Goodbar candy bar (1925), Reese's Peanut Butter Cup (1925), and Peter Pan (peanut butter) (1928).

</doc>
<doc id="55605" url="https://en.wikipedia.org/wiki?curid=55605" title="Stimulus">
Stimulus

A stimulus is something that causes a physiological response:
It may also refer to:

</doc>
<doc id="55606" url="https://en.wikipedia.org/wiki?curid=55606" title="Haifa">
Haifa

Haifa ( ', , colloquial "Hayfa"; ') is the third largest city in the State of Israel, with a population of over 277,082. Another 300,000 people live in towns directly adjacent to the city including Daliyat al-Karmel, the Krayot, Nesher, Tirat Carmel, and some Kibbutzim. Together these areas form a contiguous urban area home to nearly 600,000 residents which makes up the inner core of the Haifa metropolitan area, the second or third most populous metropolitan area in Israel. It is also home to the Bahá'í World Centre, a UNESCO World Heritage Site and destination for Baha'i pilgrims.
Built on the slopes of Mount Carmel, the history of settlement at the site spans more than 3,000 years. The earliest known settlement in the vicinity was Tell Abu Hawam, a small port city established in the Late Bronze Age (14th century BCE). In the 3rd century CE, Haifa was known as a dye-making center. Over the centuries, the city has changed hands: It has been conquered and ruled by the Phoenicians, Persians, Hasmoneans, Romans, Byzantines, Arabs, Crusaders, Ottomans, British, and the Israelis. Since the establishment of the State of Israel in 1948, the city has been governed by the Haifa Municipality.
Today, the city is a major seaport located on Israel's Mediterranean coastline in the Bay of Haifa covering . It is located about north of Tel Aviv and is the major regional center of northern Israel. Two respected academic institutions, the University of Haifa and the Technion, are located in Haifa in addition to the largest k-12 school in Israel, 
The Hebrew Reali School. The city plays an important role in Israel's economy. It is home to Matam, one of the oldest and largest high-tech parks in the country. Haifa Bay is a center of heavy industry, petroleum refining and chemical processing. Haifa was formerly the western terminus of an oil pipeline from Iraq via Jordan.
Etymology.
The earliest named settlement within the domain of modern-day Haifa was a city known as "Sycaminum". Tel Shikmona Hebrew meaning "mound of the Ficus sycomorus" (Arabic "Tell el-Semak" or "Tell es-Samak", meaning "mound of the fish") preserved and transformed this ancient name and is mentioned once in the Mishnah (composed c. 200 CE) for the wild fruits that grow around it., with locals using it to refer to a coastal tell at the foot of the Carmel Mountains that contains its remains.
The name "Efa" first appears during Roman rule, some time after the end of the 1st century, when a Roman fortress and small Jewish settlement were established not far from Tell es-Samak. Haifa is also mentioned more than 100 times in the Talmud, a book central to Judaism.
"Hefa" or "Hepha" in Eusebius of Caesarea's 4th-century work, "Onomasticon" ("Onom." 108, 31), is said to be another name for "Sycaminus". This synonymizing of the names is explained by Moshe Sharon who writes that the twin ancient settlements, which he calls "Haifa-Sycaminon", gradually expanded into one another, becoming a twin city known by the Greek names "Sycaminon" or "Sycaminos Polis". References to this city end with the Byzantine period.
Around the 6th century, "Porphyreon" or "Porphyrea" is mentioned in the writings of William of Tyre, and while it lies within the area covered by modern Haifa, it was a settlement situated south of Haifa-Sycaminon.
Following the Arab conquest in the 7th century, "Haifa" was used to refer to a site established on Tell es-Samak upon what were already the ruins of "Sycaminon" ("Shiqmona"). "Haifa" (or "Haifah") is mentioned by the mid-11th-century Persian chronicler Nasir Khusraw, and the 12th- and 13th-century Arab chroniclers, Muhammad al-Idrisi and Yaqut al-Hamawi.
The Crusaders, who captured Haifa briefly in the 12th century, call it "Caiphas", and believe its name related to "Cephas", the Aramaic name of Simon Peter. Eusebius is also said to have referred to "Hefa" as "Caiaphas civitas", and Benjamin of Tudela, the 12th-century Jewish traveller and chronicler, is said to have attributed the city's founding to Caiaphas, the Jewish high priest at the time of Jesus.
Other spellings in English have included "Caipha", "Kaipha", "Caiffa", "Kaiffa" and "Khaifa".
"Haifa al-'Atiqa" (Arabic: "Ancient Haifa") is another name used by locals to refer to "Tell es-Samak", as it was the site of Haifa when it was a hamlet of 250 residents, before it was moved in 1764-5 to a new fortified site founded by Zahir al-Umar to the east. The new village, the nucleus of modern Haifa, was originally named "al-imara al-jadida" (Arabic: "the new construction"), but locals called it "Haifa al-Jadida" (Arabic: "New Haifa") at first, and then simply "Haifa". In the early 20th century, "Haifa al 'Atiqa" was repopulated as a predominantly Arab Christian neighborhood of Haifa as it expanded outward from its new location.
The ultimate origin of the name "Haifa" remains unclear. One theory holds it derives from the name of the high priest Caiaphas. Some Christians believe it was named for Saint Peter, whose Aramaic name was "Keiphah". Another theory holds it could be derived from the Hebrew verb root חפה ("hafa"), meaning to cover or shield, i.e. Mount Carmel covers Haifa; others point to a possible origin in the Hebrew word חוֹף ("hof"), meaning "shore", or חוֹף יָפֶה ("hof yafe"), meaning "beautiful shore".
History.
Early history.
A small port city known today as Tell Abu Hawam was established Late Bronze Age (14th century BCE). During the 6th century BCE, Greek geographer Scylax told of a city "between the bay and the Promontory of Zeus" (i.e., the Carmel) which may be a reference to Shikmona, a locality in the Haifa area, during the Persian period. By Hellenistic times, the city had moved to a new site south of what is now Bat Galim because the port's harbour had become blocked with sand. About the 3rd century CE, the city was first mentioned in Talmudic literature, as a Jewish fishing village and the home of Rabbi Avdimi and other Jewish scholars. A Greek-speaking population living along the coast at this time was engaged in commerce.
Haifa was located near the town of Shikmona, a center for making the traditional Tekhelet dye used in the garments of the high priests in the Temple. The archaeological site of Shikmona is southwest of Bat Galim. Mount Carmel and the Kishon River are also mentioned in the Bible. A grotto on the top of Mount Carmel is known as the "Cave of Elijah", traditionally linked to the Prophet Elijah and his apprentice, Elisha. In Arabic, the highest peak of the Carmel range is called the "Muhraka", or "place of burning," harking back to the burnt offerings and sacrifices there in Canaanite and early Israelite times
Early Haifa is believed to have occupied the area which extends from the present-day Rambam Hospital to the Jewish Cemetery on Yafo Street. The inhabitants engaged in fishing and agriculture.
Under Byzantine rule, Haifa continued to grow but did not assume major importance. Following the Arab conquest of Palestine in the 630s-40s, Haifa was largely overlooked in favor of the port city of 'Akka. Under the Rashidun Caliphate, Haifa began to develop. In the 9th century under the Umayyad and Abbasid Caliphates, Haifa established trading relations with Egyptian ports and the city featured several shipyards. The inhabitants, Arabs and Jews, engaged in trade and maritime commerce. Glass production and dye-making from marine snails were the city's most lucrative industries.
Crusader, Ayyubid and Mamluk rule.
Prosperity ended in 1100 or 1101, when Haifa was besieged and blockaded by the Crusaders and then conquered after a fierce battle with its Jewish inhabitants and Fatimid garrison. Under the Crusaders, Haifa was reduced to a small fortified coastal stronghold. It was a part of the Principality of Galilee within the Kingdom of Jerusalem. Following their victory at the Battle of Hattin, Saladin's Ayyubid army captured Haifa in mid-July 1187 and the city's Crusader fortress was destroyed. The Crusaders under Richard the Lionheart retook Haifa in 1191.
In the 12th century religious hermits started inhabiting the caves on Mount Carmel, and in the 13th century they formed a new Catholic monastic order, the Carmelites. Under Muslim rule, the church which they had built on Mount Carmel was turned into a mosque, later becoming a hospital. In the 19th century, it was restored as a Carmelite monastery. The altar of the church as we see it today, stands over a cave associated with Prophet Elijah.
In 1265, the army of Baibars the Mamluk captured Haifa, destroying its fortifications, which had been rebuilt by King Louis IX of France, as well as the majority of the city's homes to prevent the European Crusaders from returning. For much of their rule, the city was desolate in the Mamluk period between the 13th and 16th centuries. Information from this period is scarce. During Mamluk rule in the 14th century, al-Idrisi wrote that Haifa served as the port for Tiberias and featured a "fine harbor for the anchorage of galleys and other vessels.
Ottoman era.
In 1596, Haifa appeared in Ottoman tax registers as being in the "Nahiya" of Sahil Atlit of the "Liwa" of Lajjun. It had a population of 32 Muslim households and paid taxes on wheat, barley, summercrops, olives, and goats or beehives.
Haifa was a hamlet of 250 inhabitants in 1764-5. It was located at "Tell el-Semak", the site of ancient Sycaminum. In 1765 Zahir al-Umar, the Arab ruler of Acre and the Galilee, moved the population to a new fortified site to the east and laid waste to the old site. According to historian Moshe Sharon, the new Haifa was established by Zahir in 1769. This event marked the beginning of the town's life at its modern location. After al-Umar's death in 1775, the town remained under Ottoman rule until 1918, with the exception of two brief periods.
In 1799, Napoleon Bonaparte conquered Haifa during his unsuccessful campaign to conquer Palestine and Syria, but soon had to withdraw; in the campaign's final proclamation, Napoleon took credit for having razed the fortifications of "Kaïffa" (as the name was spelled at the time) along with those of Gaza, Jaffa and Acre.
Between 1831 and 1840, the Egyptian viceroy Muhammad Ali governed Haifa, after his son Ibrahim Pasha had wrested its control from the Ottomans. When the Egyptian occupation ended and Acre declined, the importance of Haifa rose.
The arrival of the German Templers in 1868, who settled in what is now known as the German Colony of Haifa, was a turning point in Haifa's development. The Templers built and operated a steam-based power station, opened factories and inaugurated carriage services to Acre, Nazareth and Tiberias, playing a key role in modernizing the city.
The first major Jewish Immigration took place at the middle 19th century from Morocco, with small immigration from Turkey few years later. A wave of European Jews arrived at the end of the 19th century from Romania. The Central Jewish Colonisation Society in Romania purchased over near Haifa. As the Jewish settlers had been city dwellers, they hired the former fellahin tenants to instruct them in agriculture.
In 1909, Haifa became important to the Bahá'í Faith when the remains of the Báb, founder of the Bábí Faith and forerunner of Bahá'u'lláh in the Bahá'í Faith, were moved from Acre to Haifa and interred in the shrine built on Mount Carmel. Bahá'ís consider the shrine to be their second holiest place on Earth after the Shrine of Bahá'u'lláh in Acre. Its precise location on Mount Carmel was shown by Bahá'u'lláh himself to his eldest son, `Abdu'l-Bahá, in 1891. `Abdu'l-Bahá planned the structure, which was designed and completed several years later by his grandson, Shoghi Effendi. In a separate room, the remains of `Abdu'l-Bahá were buried in November 1921.
A branch of the Hejaz railway, known as the Jezreel Valley railway, was built between 1903 and 1905. This event accelerated the growth of Haifa, which became a township (nahiya) centre in Akka in the sanjak of Beyrut Eyalet before the end of Ottoman rule. The Technion Institute of Technology was established around this time, that is, in 1912.
British Mandate.
Haifa was captured from the Ottomans in September 1918 by Indian horsemen of the British Army after overrunning Ottoman positions armed with spears and swords. On 22 September, British troops were heading to Nazareth when a reconnaissance report was received indicating that the Turks were leaving Haifa. The British made preparations to enter the city and came under fire in the Balad al-Sheikh district (today Nesher). After the British regrouped, an elite unit of Indian horsemen were sent to attack the Turkish positions on the flanks and overrun their artillery guns on Mount Carmel.
Under the British Mandate, Haifa became an industrial port city. The Bahá'í Faith in 1918 and today has its administrative and spiritual centre in the environs of Haifa. Over the next few decades the number of Jews increased steadily, due to immigration, especially from Europe. The Arab immigration on the other hand swelled by influx of Arabs, coming mainly from surrounding villages as well as Syrian Hauran. The Arab immigration mainly came as a result of prices and salary drop. Between the censuses of 1922 and 1931, the Muslim, Jewish, and Christian populations rose by 217%, 256%, and 156%, respectively.
Haifa's development owed much to British plans to make it a central port and hub for Middle-East crude oil. The British Government of Palestine developed the port and built refineries, thereby facilitating the rapid development of the city as a center for the country's heavy industries. Haifa was also among the first towns to be fully electrified. The Palestine Electric Company inaugurated the Haifa Electrical Power Station already in 1925, opening the door to considerable industrialization. The State-run Palestine Railways also built its main workshops in Haifa.
By 1945 the population had shifted to 33 percent Muslim, 20 percent Christian and 47 percent Jewish. In 1947, about 70,910 Arabs (41,000 Muslims, 29,910 Christians) and 74,230 Jews were living there. The Christian community were mostly Greek-Melkite Catholics. 
The 1947 UN Partition Plan in late November 1947 designated Haifa as part of the proposed Jewish state. Arab protests over that decision evolved into violence between Jews and Arabs that left several dozen people dead during December. On 30 December 1947, members of the Irgun, a Jewish underground militia, threw bombs into a crowd of Arabs outside the gates of the Consolidated Refineries in Haifa, killing six and injuring 42. In response Arab employees of the company killed 39 Jewish employees in what became known as the Haifa Oil Refinery massacre. The Jewish Haganah militia retaliated with a raid on the Arab village of Balad al-Shaykh, where many of the Arab refinery workers lived, in what became known as the Balad al-Shaykh massacre. Control of Haifa was critical in the ensuing civil war, since it was the major industrial and oil refinery port in British Palestine.
British forces in Haifa redeployed on 21 April 1948, withdrawing from most of the city while still maintaining control over the port facilities. Two days later the downtown, controlled by a combination of local and foreign (ALA) Arab irregulars was assaulted by Jewish forces in Operation Bi'ur Hametz, by the Carmeli Brigade of the Haganah, commanded by Moshe Carmel. The operation led to a massive displacement of Haifa's Arab population. According to "The Economist" at the time, only 5,000–6,000 of the city's 62,000 Arabs remained there by 2 October 1948.
Contemporaneous sources emphasized the Jewish leadership's attempt to stop the Arab exodus from the city and the Arab leadership as a motivating factor in the refugees' flight. According to the British district superintendent of police, "Every effort is being made by the Jews to persuade the Arab populace to stay and carry on with their normal lives, to get their shops and business open and to be assured that their lives and interests will be safe." "Time Magazine" wrote on 3 May 1948:
Benny Morris said Haifa's Arabs left due to of a combination of Zionist threats and encouragement to do so by Arab leaders. Ilan Pappé writes that the shelling culminated in an attack on a Palestinian crowd in the old marketplace using three-inch (76 mm) mortars on 22 April 1948. Shabtai Levy, the Mayor of the city, and some other Jewish leaders urged Arabs not to leave. According to Ilan Pappé, Jewish loudspeakers could be heard in the city ordering Arab residents to leave "before it's too late."
Morris quotes British sources as stating that during the battles between 22 and 23 April 100 Arabs were killed and 100 wounded, but he adds that the total may have been higher.
State of Israel.
After the Declaration of the Establishment of the State of Israel on 14 May 1948 Haifa became the gateway for Jewish immigration into Israel. During the 1948 Arab–Israeli War, the neighborhoods of Haifa were sometimes contested. After the war, Jewish immigrants were settled in new neighborhoods, among them Kiryat Hayim, Ramot Remez, Ramat Shaul, Kiryat Sprinzak, and Kiryat Eliezer. Bnei Zion Hospital (formerly Rothschild Hospital) and the Central Synagogue in Hadar Hacarmel date from this period. In 1953, a master plan was created for transportation and the future architectural layout.
In 1959, a group of Sephardi and Mizrahi Jews, mostly Moroccan Jews, rioted in Wadi Salib, claiming the state was discriminating against them. Their demand for “bread and work” was directed at the state institutions and what they viewed as an Ashkenazi elite in the Labor Party and the Histadrut.
Tel Aviv gained in status, while Haifa suffered a decline in the role as regional capital. The opening of Ashdod as a port exacerbated this. Tourism shrank when the Israeli Ministry of Tourism placed emphasis on developing Tiberias as a tourist centre.
Nevertheless, Haifa's population had reached 200,000 by the early 1970s, and mass immigration from the former Soviet Union boosted the population by a further 35,000.
Many of Wadi Salib's historic Ottoman buildings have now been demolished, and in the 1990s a major section of the Old City was razed to make way for a new municipal center.
From 1999 to 2003, several Palestinian suicide attacks took place in Haifa (in Maxim and Matza restaurants, bus 37, and others), killing 68 civilians.
In 2006, Haifa was hit by 93 Hezbollah rockets during the Second Lebanon War, killing 11 civilians and leading to half of the city's population fleeing at the end of the first week of the war. Among the places hit by rockets were a train depot and the oil refinery complex.
Demographics.
Haifa is Israel's third-largest city, consisting of 103,000 households, or a population of 266,300. Immigrants from the former Soviet Union constitute 25% of Haifa's population. According to the Israeli Central Bureau of Statistics, Israeli Arabs constitute 10% of Haifa's population, the majority living in Wadi Nisnas, Abbas and Halissa neighborhoods.
Haifa is commonly portrayed as a model of co-existence between Arabs and Jews, although tensions and hostility do still exist.
Between 1994 and 2009, the city had a declining and aging population compared to Tel Aviv and Jerusalem, as young people moved to the center of the country for education and jobs, while young families migrated to bedroom communities in the suburbs. However, as a result of new projects and improving infrastructure, the city managed to reverse its population decline, reducing emigration while attracting more internal migration into the city. In 2009, positive net immigration into the city was shown for the first time in 15 years.
Religious and ethnic communities.
The population is heterogeneous. Jews comprise some 82% of the population, almost 14% are Christians (the majority of whom are Arab Christians) and, some 4% are Muslims (of which many are Ahmadis). Haifa also includes Druze and Bahá'í communities. In 2006, 27% of the Arab population was aged 14 and under, compared to 17% of the Jewish and other population groups. The trend continues in the age 15-29 group, in which 27% of the Arab population is found, and the age 30-44 group (23%). The population of Jews and others in these age groups are 22% and 18% respectively. Nineteen percent of the city's Jewish and other population is between 45 and 59, compared to 14% of the Arab population. This continues with 14% of Jews and others aged 60–74 and 10% over age 75, in comparison to 7% and just 2% respectively in the Arab population.
In 2006, 2.9% of the Jews in the city were Haredi, compared to 7.5% on a national scale. However, the Haredi community in Haifa is growing fast due to a high fertility rate. 66.6% were secular, compared to a national average of 43.7%. A significant portion of the immigrants from the former Soviet Union either lack official religious-ethnic classification or are Non-Jews as they are from mixed-marriage families of some Jewish origin.
There is also a Scandinavian Seamen Protestant church, established by Norwegian Righteous Among the Nations pastor Per Faye-Hansen.
Haifa is the center of liberal Palestinian culture, as it was under British colonial rule. The Palestinian neighborhoods, which are mixed Muslim and Christian, are in the lowlands near the sea while Jewish neighborhoods are at higher elevation. An active Palestinian cultural life has developed in the 21st century.
Geography.
Haifa is situated on the Israeli Mediterranean Coastal Plain, the historic land bridge between Europe, Africa, and Asia, and the mouth of the Kishon River. Located on the northern slopes of Mount Carmel and around Haifa Bay, the city is split over three tiers. The lowest is the center of commerce and industry including the Port of Haifa. The middle level is on the slopes of Mount Carmel and consists of older residential neighborhoods, while the upper level consists of modern neighborhoods looking over the lower tiers. From here views can be had across the Western Galilee region of Israel towards Rosh HaNikra and the Lebanese border. Haifa is about north of the city of Tel Aviv, and has a large number of beaches on the Mediterranean.
Flora and fauna.
The Carmel Mountain has three main wadis: Lotem, Amik and Si’ach. For the most part these valleys are undeveloped natural corridors that run up through the city from the coast to the top of the mountain. Marked hiking paths traverse these areas and they provide habitat for wildlife such as wild boar, golden jackal, hyrax, Egyptian mongoose, owls and chameleons.
Climate.
Haifa has a hot-summer Mediterranean climate with hot, dry summers and cool, rainy winters (Köppen climate classification "Csa"). Spring arrives in March when temperatures begin to increase. By late May, the temperature has warmed up considerably to herald warm summer days. The average temperature in summer is and in winter, . Snow is rare in Haifa, but temperatures around can sometimes occur, usually in the early morning. Humidity tends to be high all year round, and rain usually occurs between September and May. Annual precipitation is approximately .
Neighborhoods.
Haifa has developed in tiers, from the lower to the upper city on the Carmel. The oldest neighborhood in the modern Haifa is Wadi Salib, the Old City center near the port, which has been bisected by a major road and razed in part to make way for government buildings. Wadi Salib stretches across to Wadi Nisnas, the center of Arab life in Haifa today. In the 19th century, under Ottoman rule, the German Colony was built, providing the first model of urban planning in Haifa. Some of the buildings have been restored and the colony has turned into a center of Haifa nightlife.
The first buildings in Hadar were constructed at the start of the 20th century. Hadar was Haifa's cultural center and marketplace throughout the 1920s and into the 1980s, nestled above and around the Haifa's Arab neighborhoods. Today Hadar stretches from the port area near the bay, approximately halfway up Mount Carmel, around the German Colony, Wadi Nisnas and Wadi Salib. Hadar houses two commercial centers (one in the port area, and one midway up the mountain) surrounded by some of the city's older neighborhoods.
Neve Sha'anan, a neighborhood located on the second tier of Mount Carmel, was founded in the 1920s. West of the port are the neighborhoods of Bat Galim, Shikmona Beach, and Kiryat Eliezer. To the west and east of Hadar are the Arab neighborhoods of Abbas and Khalisa, built in the 1960s and 70s. To the south of Mount Carmel's headland, along the road to Tel Aviv, are the neighborhoods of Ein HaYam, Shaar HaAliya, Kiryat Sprinzak and Neve David.
Above Hadar are affluent neighborhoods such as the Carmel Tzarfati (French Carmel), Merkaz Ha'Carmel, Romema, Ahuzat Ha'Carmel (Ahuza), Carmeliya, Vardiya, Ramat Golda, Ramat Alon and Hod Ha'Carmel (Denya). While there are general divisions between Arab and Jewish neighborhoods, there is an increasing trend for wealthy Arabs to move into affluent Jewish neighborhoods. Another of the Carmel neighborhoods is Kababir, home to the National Headquarters of Israel's Ahmadiyya Muslim Community; located near Merkaz HaCarmel and overlooking the coast.
Urban development.
Recently, residential construction has been concentrated around Kiryat Haim and Kiryat Shmuel, with of new residential construction between 2002–2004, the Carmel, with , and Ramot Neve Sha'anan with approximately Non-residential construction was highest in the Lower Town, (90,000 sq m), Haifa Bay (72,000 sq m) and Ramot Neve Sha'anan (54,000 sq m). In 2004, 80% of construction in the city was private.
Currently, the city has a modest number of skyscrapers and high-rise buildings, and many additional high-rise buildings are planned, have been approved, or are under construction. Though buildings rising up to 20 stories were built on Mount Carmel in the past, the Haifa municipality banned the construction of any new buildings taller than nine stories on Mount Carmel in July 2012.
The neighborhood of Wadi Salib, located in the heart of downtown Haifa, is being redeveloped. Most of its Jewish and Arab residents are considered squatters and have been gradually evicted over the years. The Haifa Economic Corporation Ltd is developing two 1,000 square meter lots for office and commercial use. Some historic buildings have been renovated and redeveloped, especially into nightclubs and theaters, such as the Palace of the Pasha, a Turkish bathhouse, and a Middle Eastern music and dance club, which has been converted into theaters and offices.
In 2012, a new, massive development plan was announced for Haifa's waterfront. According to the plan, the western section of the city's port will be torn down, and all port activity will be moved to the east. The west side of the port will be transformed into a tourism and nightlife center and a point of embarkation and arrival for sea travel through the construction of public spaces, a beach promenade, and the renovation of commercial buildings. The train tracks that currently bisect the city and separate the city's beach from the rest of Haifa will also be buried. A park will be developed on the border of the Kishon River, the refineries' cooling towers will be turned into a visitors' center, and bridges will lead from the port to the rest of the city. Massive renovations are also currently underway in Haifa's lower town, in the Turkish market and Paris Square, which will become the city's business center. In addition, the ammonia depository tank in the Haifa bay industrial zone will be dismantled, and a new one built in an alternative location.
Another plan seeks to turn the western section of Haifa Port into a major tourism and nightlife center, as well as a functioning point of embarkation and arrival for sea travel. All port activity will be moved to the western side, and the area will be redeveloped. Public spaces and a beach promenade will be developed, and commercial buildings will be renovated.
As part of the development plans, the Israeli Navy, which has a large presence in Haifa, will withdraw from the shoreline between Bat Galim and Hof Hashaket. A long esplanade which will encircle the shoreline will be constructed. It will include a bicycle path, and possibly also a small bridge under which navy vessels will pass on their way to the sea.
In addition, a 50,000 square-meter entertainment complex that will contain a Disney theme park, cinemas, shops, and a 25-screen Multiplex theater will be built at the Check Post exit from the Carmel Tunnels.
In 2014, a new major plan for the city was proposed, under which extensive development of residential, business, and leisure areas will take place with the target of increasing the city's population by 60,000 by 2025. Under the plan, five new neighborhoods will be built, along with new high-tech parks. In addition, existing employment centers will be renovated, and new leisure areas and a large park will be built.
Economy.
The common Israeli saying, "Haifa works, Jerusalem prays, and Tel Aviv plays" attests to Haifa's reputation as a city of workers and industry. The industrial region of Haifa is in the eastern part of the city, around the Kishon River. It is home to the Haifa oil refinery, one of the two oil refineries in Israel (the other refinery being located in Ashdod). The Haifa refinery processes 9 million tons (66 million barrels) of crude oil a year. Its nowadays unused twin 80-meter high cooling towers, built in the 1930s, were the tallest buildings built in the British Mandate period.
"Matam" (short for "Merkaz Ta'asiyot Mada" - Scientific Industries Center), the largest and oldest business park in Israel, is at the southern entrance to the city, hosting manufacturing and R&D facilities for a large number of Israeli and international hi-tech companies, such as Intel, IBM, Microsoft, Motorola, Google, Yahoo!, Elbit, CSR, Philips, and Amdocs. The campus of the University of Haifa is also home to IBM Haifa Labs.
The Port of Haifa is the leader in passenger traffic among Israeli ports, and is also a major cargo harbor, although deregulation has seen its dominance challenged by the Port of Ashdod.
Haifa malls and shopping centers include Hutsot Hamifratz, Horev Center Mall, Panorama Center, Castra Center, Colony Center (Lev HaMoshava), Hanevi'im Tower Mall, Kanyon Haifa, Lev Hamifratz Mall and Grand Kanyon.
In 2010, "Monocle" magazine identified Haifa as the city with the most promising business potential, with the greatest investment opportunities in the world. The magazine noted that "a massive head-to-toe regeneration is starting to have an impact; from scaffolding and cranes around town, to renovated façades and new smart places to eat". The Haifa municipality had spent more than $350 million on roads and infrastructure, and the number of building permits had risen 83% in the previous two years.
In 2014, it was announced that a technology-focused stock exchange would be established to compete with the Tel Aviv Stock Exchange.
Currently, some 40 hotels, mostly boutique hotels, are planned, have been approved, or are under construction. The Haifa Municipality is seeking to turn the city into Northern Israel's tourist center, from where travelers can embark on day trips into Acre, Nazareth, Tiberias, and the Galilee.
A new life sciences industrial park containing five buildings with 85,000 square meters of space on a 31-duman (7.75 acre) site is being built adjacent to the Matam industrial park.
Tourism.
In 2005, Haifa has 13 hotels with a total of 1,462 rooms. The city has a shoreline, of which are beaches. Haifa's main tourist attraction is the Bahá'í World Centre, with the golden-domed Shrine of the Báb and the surrounding gardens. Between 2005 and 2006, 86,037 visited the shrine. In 2008, the Bahá'í gardens were designated a UNESCO World Heritage Site. The restored German Colony, founded by the Templers, Stella Maris and Elijah's Cave also draw many tourists.
Located in the Haifa district are the Ein Hod artists' colony, where over 90 artists and craftsmen have studios and exhibitions, and the Mount Carmel national park, with caves where Neanderthal and early Homo Sapiens remains were found.
A 2007 report commissioned by the Haifa Municipality calls for the construction of more hotels, a ferry line between Haifa, Acre and Caesarea, development of the western anchorage of the port as a recreation and entertainment area, and an expansion of the local airport and port to accommodate international travel and cruise ships.
Arts and culture.
Despite its image as a port and industrial city, Haifa is the cultural hub of northern Israel. During the 1950s, mayor Abba Hushi made a special effort to encourage authors and poets to move to the city, and founded the Haifa Theatre, a repertory theater, the first municipal theater founded in the country. The principal Arabic theater servicing the northern Arab population is the al-Midan Theater. Other theaters in the city include the Krieger Centre for the Performing Arts and the Rappaport Art and Culture Center. The Congress Center hosts exhibitions, concerts and special events.
The New Haifa Symphony Orchestra, established in 1950, has more than 5,000 subscribers. In 2004, 49,000 people attended its concerts. The Haifa Cinematheque, founded in 1975, hosts the annual Haifa International Film Festival during the intermediate days of the Sukkot holiday. Haifa has 29 movie theaters. The city publishes a local newspaper, Yediot Haifa, and has its own radio station, Radio Haifa.
During the 1990s, Haifa hosted the Haifa Rock & Blues Festival featuring Bob Dylan, Nick Cave, Blur and PJ Harvey. The last festival was held in 1995 with Sheryl Crow, Suede and Faith No More as headliners.
Museums.
Haifa has over a dozen museums. The most popular museum is the Israel National Museum of Science, Technology, and Space, which recorded almost 150,000 visitors in 2004. The museum is located in the historic Technion building in the Hadar neighborhood. The Haifa Museum of Art houses a collection of modern and classical art, as well as displays on the history of Haifa. The Tikotin Museum of Japanese Art is the only museum in the Middle East dedicated solely to Japanese art. Other museums in Haifa include the Museum of Prehistory, the National Maritime Museum and Haifa City Museum, the Hecht Museum, the Dagon Archaeological Museum of Grain Handling, the Railway Museum, the Clandestine Immigration and Navy Museum, the Israeli Oil Industry Museum, and Chagall Artists' House. As part of his campaign to bring culture to Haifa, Mayor Abba Hushi provided the artist Mane-Katz with a building on Mount Carmel to house his collection of Judaica, which is now a museum.
The Haifa Educational Zoo at Gan HaEm park houses a small animal collection including Syrian brown bears, now extinct from Israel. Wןthin the zoo is the Pinhas House biology institute. In the close vicinity of Haifa, on the Carmel, the Northern "Hai-Bar" ("wild life") operated by Israel's Parks and Reserves Authority for the purpose of breeding and reintroduction of species now extinct from Israel, such as Persian Fallow Deer.
Government.
As an industrial port city, Haifa has traditionally been a Labor party stronghold. The strong presence of dock workers and trade unions earned it the nickname 'Red Haifa.' In addition, many prominent Arabs in the Israeli Communist Party, among them Tawfik Toubi, Emile Habibi, Zahi Karkabi, Bulus Farah and Emile Toma, were from Haifa. recent years, there has been a drift toward the center. This was best signified by, in the 2006 legislative elections, the Kadima party receiving about 28.9% of the votes in Haifa, and Labor lagging behind with 16.9%.
Before 1948, Haifa's Municipality was fairly unusual as it developed cooperation between the mixed Arab and Jewish community in the city, with representatives of both groups involved in the city's management. Under mayor al-Haj, between 1920 and 1927, the city council had six Arab and two Jewish representatives, with the city run as a mixed municipality with overall Arab control. Greater cooperation was introduced under Hasan Bey Shukri, who adopted a positive and conciliatory attitude toward the city's Jews and gave them senior posts in the municipality. In 1940, the first Jewish mayor, Shabtai Levy, was elected. Levy's two deputies were Arab (one Muslim, the other Christian), with the remainder of the council made up of four Jews and six Arabs.
Today, Haifa is governed by its 12th city council, headed by the mayor Yona Yahav. The results of municipal elections decide on the makeup of the council, similarly to the Knesset elections. The city council is the legislative council in the city, and has the authority to pass auxiliary laws. The 12th council, which was elected in 2003, has 31 members, with the liberal Shinui-Greens ticket holding the most seats (6), and Likud coming second with 5. Many of the decisions passed by the city council are results of recommendation made by the various municipal committees, which are committees where non-municipal organs meet with representatives from the city council. Some committees are spontaneous, but some are mandatory, such as the security committee, tender committee and financial committee.
Medical facilities.
Haifa medical facilities have a total of 4,000 hospital beds. The largest hospital is the government-operated Rambam Hospital with 900 beds and 78,000 admissions in 2004. Bnai Zion Hospital and Carmel Hospital each have 400 beds. Other hospitals in the city include the Italian Hospital, Elisha Hospital (100 beds), Horev Medical Center (36 beds) and Ramat Marpe (18 beds). Haifa has 20 family health centers. In 2004, there were a total of 177,478 hospital admissions.
Rambam Medical Center was in the direct line of fire during the Second Lebanon War in 2006 and was forced to take special precautions to protect its patients. Whole wings of the hospital were moved to large underground shelters.
Education.
Haifa is home to two internationally acclaimed universities and several colleges The University of Haifa, founded in 1963, is at the top of Mt. Carmel. The campus was designed by the architect of Brasília and United Nations Headquarters in New York, Oscar Niemeyer. The top floor of the 30-story Eshkol Tower provides a panoramic view of northern Israel. The Hecht Museum, with important archeology and art collections, is on the campus of Haifa University.
The Technion - Israel Institute of Technology, described as Israel's MIT, was founded in 1912. It has 18 faculties and 42 research institutes. The original building now houses Haifa's science museum. The Hebrew Reali School was founded in 1913. It is the largest k-12 school in Israel, with 4,000 students in 7 branches, all over the city. The first technological high school in Israel, Bosmat, was established in Haifa in 1933.
Other academic institutions in Haifa are the Gordon College of Education and Sha'anan Religious Teachers' College, the WIZO Haifa Academy of Design and Education, and Tiltan College of Design. The Michlala Leminhal College of Management and the Open University of Israel have branches in Haifa. The city also has a nursing college and the P.E.T Practical Engineering School.
–07, Haifa had 70 elementary schools, 23 middle schools, 28 academic high schools and 8 vocational high schools. There were 5,133 pupils in municipal kindergartens, 20,081 in elementary schools, 7,911 in middle schools, 8,072 in academic high schools, 2,646 in vocational high schools, and 2,068 in comprehensive district high schools. 86% of the students attended Hebrew-speaking schools and 14% attended Arab schools. 5% were in special education. In 2004, Haifa had 16 municipal libraries stocking 367,323 books.
Two prestigious Arab schools in Haifa are the Orthodox School, run by the Greek Orthodox church, and the Nazareth Nuns' School, a Catholic institution.
Transportation.
Haifa is served by six railway stations and the Carmelit, currently Israel's only subway system (another is under construction in Tel Aviv). The Nahariya–Tel Aviv Coastal Railway main line of Israel Railways runs along the coast of the Gulf of Haifa and has six stations within the city. From south-west to north-east, these stations are: Haifa Hof HaCarmel, Haifa Bat Galim, Haifa Merkaz HaShmona, Lev HaMifratz, Hutzot HaMifratz and Kiryat Haim. Together with the Kiryat Motzkin Railway Station in the northern suburb Kiryat Motzkin, they form the Haifa - Krayot suburban line ("Parvarit"). There are direct trains from Haifa to Tel Aviv, Ben Gurion International Airport, Nahariya, Akko, Kiryat Motzkin, Binyamina, Lod, Ramla, Beit Shemesh, Jerusalem and other locations, but all trains to Beersheba skips all Haifa stations
Haifa's intercity bus connections are operated almost exclusively by the Egged bus company, which operates two terminals:
Lines to the North of the country use HaMifratz Central Bus Station and their coverage includes most towns in the North of Israel. Lines heading south use Haifa Hof HaCarmel Central Bus Station. Destinations directly reachable from Hof HaCarmel CBS include Tel Aviv, Jerusalem, Eilat, Raanana, Netanya, Hadera, Zikhron Ya'akov, Atlit, Tirat Carmel, Ben Gurion International Airport and intermediate communities. There are also three Egged lines that have their terminus in the Ramat Vizhnitz neighborhood and run to Jerusalem, Bnei Brak and Ashdod. These used to be ""mehadrin"" (i.e. gender segregated) lines.
All urban lines are run by Egged. There are also share taxis that run along some bus routes but do not have an official schedule. In 2006, Haifa implemented a trial network of neighborhood mini-buses – named "Shkhunatit" and run by Egged. In December 2012, GetTaxi, an app and taxi service which allows users to hail a cab using their smartphone without contacting the taxi station by identifying and summoning the closest taxi. In the current initial phase, 50 taxis from the service are operating in Haifa.
Haifa and the Krayot suburbs also have a new Phileas concept bus rapid transit system called the Metronit. These buses, operating with hybrid engines, follow optical strips embedded in designated lanes of roads, providing tram-like public transportation services. The Metronit consists of 100 18-meter buses, each with the capacity for 150 passengers, operating along of designated roadways. The new system officially opened on 16 August 2013 serving three lines.
Haifa is one of the few cities in Israel where buses operate on Shabbat. Bus lines operate throughout the city on a reduced schedule from late Saturday morning onwards, and also connect Haifa with Nesher, Tirat Karmel, Yokneam, Nazareth, Nazareth Illit and intermediate communities. Since the summer of 2008, night buses are operated by Egged in Haifa (line 200) and the Krayot suburbs (line 210). During the summer of 2008 these lines operated 7 nights a week. During the winter their schedule is limited to Thursday, Friday and Saturday nights, making them the only buses in Israel to operate on Friday night. Haifa is also the only city in Israel to operate a Saturday bus service to the beaches during summer time. Egged lines run during Saturday mornings from many neighborhoods to the Dado and Bat Galim beaches, and back in the afternoon.
The Haifa underground railway system is called Carmelit. It is a subterranean funicular on rails, running from downtown Paris Square to Gan HaEm (Mother's Park) on Mount Carmel. With a single track, six stations and two trains, it is listed in "Guinness World Records" as the world's shortest metro line. The Carmelit accommodates bicycles.
Haifa also has a cable car. The Haifa Cable Car gondola lift consists of six cabins and connects Bat Galim on the coast to the Stella Maris observation deck and monastery atop Mount Carmel. It serves mainly tourists.
There are currently plans to add a 4.4 kilometre commuter cable car service to Haifa's public transport system, running from HaMifratz Central Bus Station at the foot of Mount Carmel to the Technion, and then to the University of Haifa.
Air and sea transport.
Haifa Airport serves domestic flights to Tel Aviv and Eilat as well as international charters to Cyprus, Greece and Jordan. The airliners that operates flights from Haifa are Arkia and Israir. There are currently plans to expand services from Haifa. Cruise ships operate from Haifa port primarily to destinations in the Eastern Mediterranean, Southern Europe and Black Sea.
Roads.
Travel between Haifa and the center of the country is possible by road with Highway 2, the main highway along the coastal plain, beginning at Tel Aviv and ending at Haifa. Furthermore, Highway 4 runs along the coast to the north of Haifa, as well as south, inland from Highway 2. In the past, traffic along Highway 2 to the north of Haifa had to pass through the downtown area of the city; the Carmel Tunnels, opened for traffic 1 December 2010, now route this traffic under Mount Carmel, reducing congestion in the downtown area.
Sports.
The main stadiums in Haifa are the 14,002-seat Kiryat Eliezer Stadium and Thomas D'Alesandro Stadium. Neve Sha'anan Athletic Stadium seats 1,000. Construction of the Sammy Ofer Stadium, a UEFA-approved 30,820 seat stadium was completed in 2014.
The city's two main football clubs are Maccabi Haifa and Hapoel Haifa who both currently play in the Israeli Premier League and share the Sammy Ofer Stadium as their home pitch. Maccabi has won twelve Israeli titles, while Hapoel has won one.
The city also has an American football club, the Haifa Underdogs, that are a part of the Israeli Football League and play in Yoqneam Stadium. The team lost in the championship game of the league's inaugural season, but won one title as part of American Football Israel, which merged with the Israeli Football League in 2005.
The city has several clubs in the regional leagues, including Beitar Haifa in Liga Bet (the fourth tier) and Hapoel Ahva Haifa, F.C. Haifa Ruby Shapira and Maccabi Neve Sha'anan Eldad in Liga Gimel (the fifth tier).
Haifa has a professional basketball club, Maccabi Haifa. Maccabi Haifa was recently promoted to Israeli Basketball Super League, the top division. The team plays at Romema Arena, which seats 5,000.
The Haifa Hawks are an ice hockey team based out of the city of Haifa. They participate in the Israeli League, the top level of Israeli ice hockey.
In 1996, the city hosted the World Windsurfing Championship. The Haifa Tennis Club, near the southwest entrance to the city, is one of the largest in Israel.
John Shecter, Olympic horse breeder and owner of triple cup champion Shergar was born here.
Twin towns - sister cities.
Haifa is twinned with the following cities:

</doc>
<doc id="55607" url="https://en.wikipedia.org/wiki?curid=55607" title="Discriminant">
Discriminant

In algebra, the discriminant of a polynomial is a function of its coefficients, typically denoted by a capital 'D' or the capital Greek letter Delta (Δ). It gives information about the nature of its roots. Typically, the discriminant is zero if and only if the polynomial has a multiple root. For example, the discriminant of the quadratic polynomial
is
Here for real a, b and c, if Δ > 0, the polynomial has two real roots, if Δ = 0, the polynomial has one real double root, and if Δ < 0, the two roots of the polynomial are complex conjugates.
The discriminant of the cubic polynomial
is
For higher degrees, the discriminant is always a polynomial function of the coefficients. It becomes significantly longer for the higher degrees. The discriminant of a "general" quartic has 16 terms, that of a quintic has 59 terms, that of a 6th degree polynomial has 246 terms,
and the number of terms increases exponentially with the degree.
A polynomial has a multiple root (i.e. a root with multiplicity greater than one) in the complex numbers if and only if its discriminant is zero.
The concept also applies if the polynomial has coefficients in a field which is not contained in the complex numbers. In this case, the discriminant vanishes if and only if the polynomial has a multiple root in any algebraically closed field containing the coefficients.
As the discriminant is a polynomial function of the coefficients, it is defined as long as the coefficients belong to an integral domain "R" and, in this case, the discriminant is in "R". In particular, the discriminant of a polynomial with integer coefficients is always an integer. This property is widely used in number theory.
The term "discriminant" was coined in 1851 by the British mathematician James Joseph Sylvester.
Definition.
In terms of the roots, the discriminant is given by
where formula_6 is the leading coefficient and formula_7 are the roots (counting multiplicity) of the polynomial in some splitting field. It is the square of the Vandermonde polynomial times formula_8.
As the discriminant is a symmetric function in the roots, it can also be expressed in terms of the coefficients of the polynomial, since the coefficients are the elementary symmetric polynomials in the roots; such a formula is given below.
Expressing the discriminant in terms of the roots makes its key property clear, namely that it vanishes if and only if there is a repeated root, but does not allow it to be calculated without factoring a polynomial, after which the information it provides is redundant (if one has the roots, one can tell if there are any duplicates). Hence the formula in terms of the coefficients allows the nature of the roots to be determined without factoring the polynomial.
Formulas for low degrees.
The discriminant of a linear polynomial (degree 1) is rarely considered. If needed, it is commonly defined to be equal to 1 (this is compatible with the usual conventions for the empty product and the determinant of the empty matrix). There is no common convention for the discriminant of a constant polynomial (degree 0).
The quadratic polynomial
has discriminant
The cubic polynomial
has discriminant
The quartic polynomial
has discriminant
These are homogeneous polynomials in the coefficients, respectively of degree 2, 4 and 6. They are also homogeneous in terms of the roots, of respective degrees 2, 6 and 12.
Simpler polynomials have simpler expressions for their discriminants. For example, the monic quadratic polynomial "x"2 + "bx" + "c" has discriminant Δ = "b"2 − 4"c".
The monic cubic polynomial without quadratic term "x"3 + "px" + "q" has discriminant Δ = −4"p"3 − 27"q"2.
In terms of the roots, these discriminants are homogeneous polynomials of respective degree 2 and 6.
Homogeneity.
The discriminant is a homogeneous polynomial in the coefficients; it is also a homogeneous polynomial in the roots.
In the coefficients, the discriminant is homogeneous of degree 2"n"−2; this can be seen two ways. In terms of the roots-and-leading-term formula, multiplying all the coefficients by λ does not change the roots, but multiplies the leading term by λ. In terms of the formula as a determinant of a (2"n"−1) ×(2"n"−1) matrix divided by "an", the determinant of the matrix is homogeneous of degree 2"n"−1 in the entries, and dividing by "an" makes the degree 2"n"−2; explicitly, multiplying the coefficients by λ multiplies all entries of the matrix by λ, hence multiplies the determinant by λ2"n"−1.
For a monic polynomial, the discriminant is a polynomial in the roots alone (as the "an" term is one), and is of degree "n"("n"−1) in the roots, as there are formula_15 terms in the product, each squared.
Let us consider the polynomial
It follows from what precedes that its discriminant is homogeneous of degree 2"n"−2 in the formula_17 and quasi-homogeneous of weight "n"("n"−1) if each formula_18 is given the weight "i". In other words, every monomial formula_19 appearing in the discriminant satisfies the two equations
and
These thus correspond to the partitions of "n"("n"−1) into at 2"n"−2 (non negative) parts of size at most n
This restricts the possible terms in the discriminant. For the quadratic polynomial formula_22 there are only two possibilities for formula_23, these are the partitions of 6 into 4 parts of size at most 3:
All these five monomials occur effectively in the discriminant.
While this approach gives the possible terms, it does not determine the coefficients. Moreover, in general not all possible terms will occur in the discriminant. The first example is for the quartic polynomial formula_25, in which case formula_26 satisfies formula_27 and formula_28, even though the corresponding discriminant does not involve the monomial formula_29.
Quadratic formula.
The quadratic polynomial formula_30 has discriminant
which is the quantity under the square root sign in the quadratic formula. For real numbers "a", "b", "c", one has:
and its graph crosses the "x"-axis twice.
and its graph is tangent to the "x"-axis.
An alternative way to understand the discriminant of a quadratic is to use the characterization as "zero if and only if the polynomial has a repeated root".
In that case the polynomial is formula_35
The coefficients then satisfy formula_36 so formula_37
and a monic quadratic has a repeated root if and only if this is the case, in which case the root is formula_38 Putting both terms on one side and including a leading coefficient yields formula_39
Discriminant of a polynomial.
To find the formula for the discriminant of a polynomial in terms of its coefficients, it is easiest to introduce the resultant. Just as the discriminant of a single polynomial is the product of the square of the differences between distinct roots, the resultant of two polynomials is the product of the differences between their roots, and just as the discriminant vanishes if and only if the polynomial has a repeated root, the resultant vanishes if and only if the two polynomials share a root.
Since a polynomial formula_40 has a repeated root if and only if it shares a root with its derivative formula_41 the discriminant formula_42 and the resultant formula_43 both have the property that they vanish if and only if "p" has a repeated root, and they have almost the same degree (the degree of the resultant is one greater than the degree of the discriminant) and thus are equal up to a factor of degree one, which is, up to the sign, the leading coefficient of "p". 
The benefit of the resultant is that it can be computed as a determinant, namely as the determinant of the Sylvester matrix, a matrix, whose first rows contain the coefficients of "p" and the "n" last ones the coefficients of its derivative.
The resultant formula_43 of the general polynomial
is equal to the determinant of the Sylvester matrix:
The discriminant formula_42 of formula_40 is now given by the formula
For example, in the case , the above determinant is
The discriminant of the degree 4 polynomial is then obtained from this determinant upon dividing by formula_51.
In terms of the roots, the discriminant is equal to
where "r"1, ..., "r""n" are the complex roots (counting multiplicity) of the polynomial:
This second expression makes it clear that "p" has a multiple root if and only if the discriminant is zero. (This multiple root can be complex.)
The discriminant can be defined for polynomials over arbitrary fields, in exactly the same fashion as above. The product formula involving the roots "r""i" remains valid; the roots have to be taken in some splitting field of the polynomial. The discriminant can even be defined for polynomials over any commutative ring. However, if the ring is not an integral domain, above division of the resultant by formula_6 should be replaced by substituting formula_6 by 1 in the first column of the matrix.
Nature of the roots.
The discriminant gives additional information on the nature of the roots beyond simply whether there are any repeated roots: for polynomials with real coefficients, it also gives information on whether the roots are real or complex. This is most transparent and easily stated for quadratic and cubic polynomials; for polynomials of degree 4 or higher this is more difficult to state.
Quadratic.
Because the quadratic formula expressed the roots of a quadratic polynomial as a rational function in terms of the "square root" of the discriminant, the roots of a quadratic polynomial are in the same field as the coefficients if and only if the discriminant is a square in the field of coefficients: in other words, the polynomial factors over the field of coefficients if and only if the discriminant is a square.
As a real number has real square roots if and only if it is nonnegative, and these roots are distinct if and only if it is positive (not zero), the sign of the discriminant allows a complete description of the nature of the roots of a quadratic polynomial with real coefficients:
Further, for a quadratic polynomial with rational coefficients, it factors over the rationals if and only if the discriminant – which is necessarily a rational number, being a polynomial in the coefficients – is in fact a square.
Cubic.
For a cubic polynomial with real coefficients, the discriminant reflects the nature of the roots as follows:
If a cubic polynomial has a triple root, it is a root of its derivative and of its second derivative, which is linear. Thus to decide if a cubic polynomial has a triple root or not, one may compute the root of the second derivative and look if it is a root of the cubic and of its derivative.
Higher degrees.
More generally, for a polynomial of degree "n" with real coefficients, we have
Discriminant of a polynomial over a commutative ring.
The definition of the discriminant of a polynomial in terms of the resultant may easily be extended to polynomials whose coefficients belong to any commutative ring. However, as the division is not always defined in such a ring, instead of dividing the determinant by the leading coefficient, one substitutes the leading coefficient by 1 in the first column of the determinant. This generalized discriminant has the following property which is fundamental in algebraic geometry.
Let "f" be a polynomial with coefficients in a commutative ring "A" and "D" its discriminant. Let φ be a ring homomorphism of "A" into a field "K" and φ("f") be the polynomial over "K" obtained by replacing the coefficients of "f" by their images by φ. Then φ("D") = 0 if and only if either the difference of the degrees of "f" and φ("f") is at least 2 or φ("f") has a multiple root in an algebraic closure of "K". The first case may be interpreted by saying that φ("f") has a multiple root at infinity.
The typical situation where this property is applied is when "A" is a (univariate or multivariate) polynomial ring over a field "k" and φ is the substitution of the indeterminates in "A" by elements of a field extension "K" of "k".
For example, let "f" be a bivariate polynomial in "X" and "Y" with real coefficients, such that "f" = 0 is the implicit equation of a plane algebraic curve. Viewing "f" as a univariate polynomial in "Y" with coefficients depending on "X", then the discriminant is a polynomial in "X" whose roots are the "X"-coordinates of the singular points, of the points with a tangent parallel to the "Y"-axis and of some of the asymptotes parallel to the "Y"-axis. In other words the computation of the roots of the "Y"-discriminant and the "X"-discriminant allows one to compute all of the remarkable points of the curve, except the inflection points.
Generalizations.
The concept of discriminant has been generalized to other algebraic structures besides polynomials of one variable, including conic sections, quadratic forms, and algebraic number fields. Discriminants in algebraic number theory are closely related, and contain information about ramification. In fact, the more geometric types of ramification are also related to more abstract types of discriminant, making this a central algebraic idea in many applications.
Discriminant of a conic section.
For a conic section defined in plane geometry by the real polynomial
the discriminant is equal to
and determines the shape of the conic section. If the discriminant is less than 0, the equation is of an ellipse or a circle. If the discriminant equals 0, the equation is that of a parabola. If the discriminant is greater than 0, the equation is that of a hyperbola. This formula will not work for degenerate cases (when the polynomial factors).
Discriminant of a quadratic form.
There is a substantive generalization to quadratic forms "Q" over any field "K" of . For characteristic 2, the corresponding invariant is the Arf invariant.
Given a quadratic form "Q", the discriminant or determinant is the determinant of a symmetric matrix "S" for "Q".
Change of variables by a matrix "A" changes the matrix of the symmetric form by "A"T"SA", which has determinant , so under change of variables, the discriminant changes by a non-zero square, and thus the class of the discriminant is well-defined in "K"/("K"×)2, i.e., up to non-zero squares. See also Quadratic residue.
Less intrinsically, by a theorem of Jacobi, quadratic forms on formula_60 can be expressed, after a linear change of variables, in diagonal form as
More precisely, a quadratic forms on "V" may be expressed as a sum
where the "L""i" are independent linear forms and "n" is the number of the variables (some of the "a""i" may be zero). Then the discriminant is the product of the "a""i", which is well-defined as a class in "K"/("K"×)2.
For , the real numbers, (R×)2 is the positive real numbers (any positive number is a square of a non-zero number), and thus the quotient R/(R×)2 has three elements: positive, zero, and negative. This is a cruder invariant than signature , where "n"0 is the number of 0s and "n"± is the number of ±1s in diagonal form. The discriminant is then zero if the form is degenerate (), and otherwise it is the parity of the number of negative coefficients, (−1)n−.
For , the complex numbers, (C×)2 is the non-zero complex numbers (any complex number is a square), and thus the quotient C/(C×)2 has two elements: non-zero and zero.
This definition generalizes the discriminant of a quadratic polynomial, as the polynomial formula_63 homogenizes to the quadratic form formula_64 which has symmetric matrix
whose determinant is formula_66. Up to a factor of −4, this is formula_67.
The invariance of the class of the discriminant of a real form (positive, zero, or negative) corresponds to the corresponding conic section being an ellipse, parabola, or hyperbola.
Alternating polynomials.
The discriminant is a symmetric polynomial in the roots; if one adjoins a square root of it (halves each of the powers: the Vandermonde polynomial) to the ring of symmetric polynomials in "n" variables formula_68, one obtains the ring of alternating polynomials, which is thus a quadratic extension of formula_68.

</doc>
<doc id="55608" url="https://en.wikipedia.org/wiki?curid=55608" title="Gwen Verdon">
Gwen Verdon

Gwyneth Evelyn “Gwen” Verdon (January 13, 1925 – October 18, 2000) was an American actress and dancer who won four Tony awards for her musical comedy performances and served as uncredited choreographers assistant and specialty dance coach for both theater and film. With flaming red hair and a quaver in her voice, Verdon was a critically acclaimed performer on Broadway in the 1950s, 1960s, and 1970's. Having originated many roles in musicals she is also strongly identified with her second husband, director–choreographer Bob Fosse, remembered as the dancer–collaborator–muse for whom he choreographed much of his work and as the guardian of his legacy after his death.
Early life.
Verdon was born in Culver City, California, the second child of Gertrude Lilian (née Standring; October 24, 1896 – October 16, 1956) and Joseph William Verdon (December 31, 1896 – June 23, 1978), British immigrants to the United States by way of Canada. Her brother was William Farrell Verdon (August 1, 1923–June 10, 1991). The Verdon family could be described as "showpeople." Her father was an electrician at MGM Studios, and her mother was a former vaudevillian of the Denishawn dance troupe, as well as a dance teacher.
As a toddler, she had rickets, which left her legs so badly misshapen she was called "Gimpy" by other children and spent her early years in orthopedic boots and rigid leg braces. Her mother put the three-year-old in dance classes. Further ballet training strengthened her legs and improved her carriage.
By the time she was six, she was already dancing on stage. She went on to study multiple dance forms, ranging from tap, jazz, ballroom and flamenco to Balinese. She even added juggling to her repertoire. At age 11, she appeared as a solo ballerina in the musical romance film "The King Steps Out" (1936), directed by Josef von Sternberg and starring Grace Moore and Franchot Tone. She attended Hamilton High School in Los Angeles and studied under famed balletomane Ernest Belcher. While in high school, she was cast in a revival of "Show Boat".
Verdon shocked her parents and instructors when she abandoned her budding career aged 17 to elope with reporter James Henaghan in 1942. In 1945, she appeared as a dancer in the movie musical "The Blonde From Brooklyn". After her divorce, she entrusted her son Jimmy to the care of her parents.
Career.
Early on, Verdon found a job as assistant to choreographer Jack Cole, whose work was respected by both Broadway and Hollywood movie studios. During her five-year employment with Cole, she took small roles in movie musicals as a "specialty dancer". She also taught dance to stars such as Jane Russell, Fernando Lamas, Lana Turner, Rita Hayworth, Betty Grable and Marilyn Monroe.
Verdon started out on Broadway as a "gypsy", going from one chorus line to another. Her breakthrough role finally came when choreographer Michael Kidd cast her as the second female lead in Cole Porter's musical "Can-Can" (1953), starring French prima donna Lilo. Out-of-town reviewers hailed Verdon's interpretation of Eve in the "Garden of Eden" ballet as a performance that upstaged the show's star, who jealously demanded Verdon's role be cut to only two featured dance numbers. With her role reduced to little more than an ensemble part, Verdon formally announced her intention to quit by the time the show premiered on Broadway. But her opening-night "Garden of Eden" performance was so well received that the audience screamed her name until the startled actress was brought from her dressing room in her bathrobe to take a curtain call. Verdon received a pay increase and her first Tony Award for her triumphant performance.
With her short shock of flaming red hair, exquisite body of a pin-up girl and a guileless vulnerability on stage and off, Verdon was considered the best dancer on Broadway in the 1950s and 1960s. That reputation solidified during her next show, George Abbott's "Damn Yankees" (1955), based on the novel "The Year the Yankees Lost the Pennant." She would forever be identified with her role as the vampish Lola, and it was on this show that she first worked with Bob Fosse as her choreographer. In the story, Verdon's Lola is a woman who was once "the ugliest woman in Providence, Rhode Island" but sold herself to the Devil to be the beauty we see in the play. The Devil (played by a wryly comic Ray Walston) convinces a baseball fan to sell his soul so he can play for the Washington Senators and win the league pennant in the playoffs. The Devil then employs the seductive Lola to keep the guy ("Joe") from escaping his grasp. The hitch is that Lola falls for the guy and has to choose between her love for him and her beauty pact with the Devil. The musical ran for 1019 performances. Vernon won another Tony and went to Hollywood to repeat her role in the 1958 movie version "Damn Yankees", memorably singing "Whatever Lola wants, Lola gets". (Fosse can be seen partnered deliciously with her in the original mambo duet "Who's Got the Pain".)
Another Tony came when Verdon memorably played a role associated with Greta Garbo, Eugene O'Neill's Anna Christie, the hard-luck girl fleeing from her past as a prostitute, in the musical "New Girl in Town". When Fosse directed as well as choreographed his first Broadway musical, it was "Redhead", for which Verdon won her fourth Tony. In 1960, Fosse and Verdon wed.
In 1966, Verdon returned to the stage in the role of Charity in "Sweet Charity", which like many of her earlier Broadway triumphs was choreographed and directed by husband Fosse. The show is based on Federico Fellini's screenplay for "Nights of Cabiria". But whereas Fellini's black-and-white Italian film concerns the romantic ups and downs of an ever-hopeful prostitute, the musical makes the central character a hoofer-for-hire at a Times Square dance hall. The trademark Fosse showmanship, a dynamite musical score and theatregoers' affection for the exuberant, 41-year-old Verdon put the show over, despite Fellini's source material straining against the sanitized, Broadway-ized storyline. It was followed by a movie version starring Shirley MacLaine as Charity, featuring Ricardo Montalban, Sammy Davis, Jr. and Chita Rivera, with Fosse at the helm of his very first film as director and choreographer. Characteristically generous, Verdon helped with the choreography. The numbers include the famed "Big Spender", the fast-paced "Rhythm of Life", the witty "If My Friends Could See Me Now" and "I'm a Brass Band", in which MacLaine's Charity marched down the middle of Manhattan's Wall Street district. Verdon would also travel to Berlin to help Fosse with "Cabaret", the musical film for which he won an Academy Award for Best Director.
Although estranged as a couple, Verdon and Fosse continued to collaborate on projects such as "Chicago" (1975) (in which she originated the role of murderess Roxie Hart) and the musical "Dancin"' (1978), as well as Fosse's autobiographical movie "All That Jazz" (1979). The helpmeet/peer played by Leland Palmer in that film is based on the role Verdon played in Fosse's real life. She also developed a close working relationship with Fosse's partner, Broadway dancer Ann Reinking, and she instructed for Reinking's musical theatre classes. Reinking can be seen in "All That Jazz" playing the protagonist's partner, as she was in Fosse's real life. She, as much as Verdon, would become responsible for keeping Fosse's trademark choreography alive after Fosse's death. Reinking played Roxie Hart in the highly successful Broadway revival of "Chicago" that opened in 1996. She choreographed the dances "in the style of Bob Fosse" for that revival.
After originating the role of Roxie opposite Chita Rivera in "Chicago", Verdon focused on film acting, playing character roles in movies such as "The Cotton Club" (1984), "Cocoon" (1985) and ' (1988). She continued to teach dance and musical theater and to act. She received three Emmy Award nominations for appearances on "Magnum, P.I." (1988), "Dream On" (1993) and ' (1993). Verdon appeared as Alice's mother in the Woody Allen movie "Alice" (1990) and as Ruth in "Marvin's Room" (1996), co-starring Meryl Streep, Diane Keaton, and Hume Cronyn. In 1999, Verdon served as artistic consultant on a plotless Broadway musical designed to showcase examples of classic Fosse choreography. Called simply Fosse, the revue was conceived and directed by Richard Maltby Jr and Ann Reinking and choreographed by Reinking and Chet Walker. Verdon's daughter Nicole received a "special thanks" credit. The show received a Tony for best musical.
In 1997 Verdon appeared in an episode of Walker Texas Ranger as Maisie Whitman. She later reprised the role in 1999.
Verdon played Alora in the movie "Walking Across Egypt" (1999) and appeared in the film "Bruno", released in 2000. Verdon received a total of four Tonys, for best supporting actress for "Can-Can" (1953) and best leading actress for "Damn Yankees" (1955), "New Girl in Town" (1957) and "Redhead" (1959), a murder-mystery musical. She also won a Grammy Award for the cast recording of "Redhead".
Gwen Verdon was inducted into the American Theatre Hall of Fame in 1981. In 1998, she was awarded the National Medal of Arts.
Personal life.
Verdon had two husbands, tabloid reporter James Henaghan (married 1942, divorced 1947) and Bob Fosse (married 1960, his death 1987). She and Henaghan had one son, Jim Henaghan (born 1943); she and Fosse had a daughter, Nicole Fosse (born 1963).
Fosse's extramarital affairs put a strain on their marriage and by 1971 they were separated. They never divorced. She held him in her arms as he suffered a fatal heart attack in his room at the Willard Hotel as the show Sweet Charity was beginning nearby. He was taken to George Washington University Hospital, where he was pronounced dead. 
She was a cat fancier, and had up to six cats at one time, with names such as "Feets Fosse", "Junie Moon", and "Tidbits Tumbler Fosse".
Verdon died in her sleep in 2000 of a heart attack at the home of her daughter, Nicole, in Woodstock, Vermont, at the age of 75. At 8 p.m. on the night she died, all marquee lights on Broadway were dimmed in a tribute to the actress. Her remains were cremated.
Awards & nominations.
STAGE:
FILM & TV:

</doc>
<doc id="55610" url="https://en.wikipedia.org/wiki?curid=55610" title="Interior (topology)">
Interior (topology)

In mathematics, specifically in topology, the interior of a subset "S" of points of a topological space "X" consists of all points of "S" that do not belong to the boundary of "S". A point that is in the interior of "S" is an interior point of "S". 
The interior of "S" is the complement of the closure of the complement of "S". In this sense interior and closure are dual notions. 
The exterior of a set is the interior of its complement, equivalently the complement of its closure; it consists of the points that are in neither the set nor its boundary. The interior, boundary, and exterior of a subset together partition the whole space into three blocks (or fewer when one or more of these is empty). The interior and exterior are always open while the boundary is always closed. Sets with empty interior have been called boundary sets.
Definitions.
Interior point.
If "S" is a subset of a Euclidean space, then "x" is an interior point of "S" if there exists an open ball centered at "x" which is completely contained in "S". (This is illustrated in the introductory section to this article.)
This definition generalizes to any subset "S" of a metric space "X" with metric "d": "x" is an interior point of "S" if there exists "r" > 0, such that "y" is in "S" whenever the distance "d"("x", "y") < "r".
This definition generalises to topological spaces by replacing "open ball" with "open set". Let "S" be a subset of a topological space "X". Then "x" is an interior point of "S" if "x" is contained in an open subset of "S". (Equivalently, "x" is an interior point of "S" if there exists a neighbourhood of "x" which is contained in "S".)
Interior of a set.
The interior of a set "S" is the set of all interior points of "S". The interior of "S" is denoted int("S"), Int("S") or "S"o. The interior of a set has the following properties.
Sometimes the second or third property above is taken as the "definition" of the topological interior.
Note that these properties are also satisfied if "interior", "subset", "union", "contained in", "largest" and "open" are replaced by "closure", "superset", "intersection", "which contains", "smallest", and "closed", respectively. For more on this matter, see interior operator below.
Examples.
On the set of real numbers one can put other topologies rather than the standard one.
These examples show that the interior of a set depends upon the topology of the underlying space. The last two examples are special cases of the following.
Interior operator.
The interior operator o is dual to the closure operator —, in the sense that
and also
where "X" is the topological space containing "S", and the backslash refers to the set-theoretic difference.
Therefore, the abstract theory of closure operators and the Kuratowski closure axioms can be easily translated into the language of interior operators, by replacing sets with their complements. 
Exterior of a set.
The exterior of a subset "S" of a topological space "X", denoted ext("S") or Ext("S"), is the interior int("X" \ "S") of its relative complement. Alternatively, it can be defined as "X" \ "S"—, the complement of the closure of "S". Many properties follow in a straightforward way from those of the interior operator, such as the following.
Unlike the interior operator, ext is not idempotent, but the following holds:
Interior-disjoint shapes.
Two shapes "a" and "b" are called "interior-disjoint" if the intersection of their interiors is empty. Interior-disjoint shapes may or may not intersect in their boundary. 

</doc>
<doc id="55611" url="https://en.wikipedia.org/wiki?curid=55611" title="Alexandroff extension">
Alexandroff extension

In the mathematical field of topology, the Alexandroff extension is a way to extend a noncompact topological space by adjoining a single point in such a way that the resulting space is compact. It is named for the Russian mathematician Pavel Alexandrov.
More precisely, let "X" be a topological space. Then the Alexandroff extension of "X" is a certain compact space "X"* together with an open embedding "c" : "X" → "X"* such that the complement of "X" in "X"* consists of a single point, typically denoted ∞. The map "c" is a Hausdorff compactification if and only if "X" is a locally compact, noncompact Hausdorff space. For such spaces the Alexandroff extension is called the one-point compactification or Alexandroff compactification. The advantages of the Alexandroff compactification lie in its simple, often geometrically meaningful structure and the fact that it is in a precise sense minimal among all compactifications; the disadvantage lies in the fact that it only gives a Hausdorff compactification on the class of locally compact, noncompact Hausdorff spaces, unlike the Stone–Čech compactification which exists for any Tychonoff space, a much larger class of spaces.
Example: inverse stereographic projection.
A geometrically appealing example of one-point compactification is given by the inverse stereographic projection. Recall that the stereographic projection "S" gives an explicit homeomorphism from the unit sphere minus the north pole (0,0,1) to the Euclidean plane. The inverse stereographic projection formula_1 is an open, dense embedding into a compact Hausdorff space obtained by adjoining the additional point formula_2. Under the stereographic projection latitudinal circles formula_3 get mapped to planar circles formula_4. It follows that the deleted neighborhood basis of formula_5 given by the punctured spherical caps formula_6 corresponds to the complements of closed planar disks formula_7. More qualitatively, a neighborhood basis at formula_8 is furnished by the sets formula_9 as "K" ranges through the compact subsets of formula_10. This example already contains the key concepts of the general case.
Motivation.
Let formula_11 be an embedding from a topological space "X" to a compact Hausdorff topological space "Y", with dense image and one-point remainder formula_12. Then "c"("X") is open in a compact Hausdorff space so is locally compact Hausdorff, hence its homeomorphic preimage "X" is also locally compact Hausdorff. Moreover, if "X" were compact then "c"("X") would be closed in "Y" and hence not dense. Thus a space can only admit a one-point compactification if it is locally compact, noncompact and Hausdorff. Moreover, in such a one-point compactification the image of a neighborhood basis for "x" in "X" gives a neighborhood basis for "c"("x") in "c"("X"), and—because a subset of a compact Hausdorff space is compact if and only if it is closed—the open neighborhoods of formula_8 must be all sets obtained by adjoining formula_8 to the image under "c" of a subset of "X" with compact complement.
The Alexandroff extension.
Let "X" be any topological space, and let formula_8 be any object which is not already an element of "X". Put formula_16, and topologize formula_17 by taking as open sets all the open subsets "U" of "X" together with all subsets "V" which contain formula_8 and such that formula_19 is closed and compact, .
The inclusion map formula_20 is called the Alexandroff extension of "X" (Willard, 19A).
The above properties all follow from the above discussion:
The one-point compactification.
In particular, the Alexandroff extension formula_20 is a compactification of "X" if and only if "X" is Hausdorff, noncompact and locally compact. In this case it is called the one-point compactification or Alexandroff compactification of "X". Recall from the above discussion that any compactification 
with one point remainder is necessarily (isomorphic to) the Alexandroff compactification.
Let "X" be any noncompact Tychonoff space. Under the natural partial ordering on the set formula_26 of equivalence classes of compactifications, any minimal element is equivalent to the Alexandroff extension (Engelking, Theorem 3.5.12). It follows that a noncompact Tychonoff space admits a minimal compactification if and only if it is locally compact.

</doc>
<doc id="55614" url="https://en.wikipedia.org/wiki?curid=55614" title="Nurse uniform">
Nurse uniform

A nurse uniform is attire worn by nurses for hygiene and identification. The traditional nurse uniform consists of a dress, apron and cap. It has existed in many variants, but the basic style has remained recognizable.
History.
The first nurse uniforms were derived from the nun's habit. Before the 19th century, nuns took care of sick and injured people so it was obvious that trained lay nurses might copy the nun's habit as they have adopted ranks like "Sister". One of Florence Nightingale's first students (Miss van Rensselaer) designed the original uniform for the students at Miss Nightingale's school of nursing. Before the 1940s minor changes occurred in the uniform. The clothing consisted of a mainly blue outfit. Hospitals were free to determine the style of the nurse uniform, including the nurse's cap which exists in many variants.
In Britain, the national uniform (or simply "national") was designed with the advent of the National Health Service (NHS) in 1948, and the Newcastle dress. From the 1960s open necks began to appear. In the 1970s, white disposable paper caps replaced cotton ones; in the 1980s, plastic aprons displaced the traditional ones and outerwear began to disappear. From the 1990s, scrubs became popular in Britain, having first appeared in the USA; however some nurses in Britain continue to wear dresses, although some NHS trusts have removed them in favour of scrubs as in many other countries.
Standard nurse's uniform.
Historically, a typical nurse uniform consisted of a dress, pinafore apron and nurse's cap. In some hospitals, however, student nurses also wore a nursing pin, or the pinafore apron may have been replaced by a cobbler style apron. This type of nurse's dress continues to be worn in many countries.
Alternative nurse uniforms.
Since the late 1980s, there has been a move towards alternative designs of nursing uniforms in some countries. Newer style nurse's uniform in the United Kingdom consists of either:
Male Nursing uniform.
Male nurses generally wear a different unform to their female counterparts. Male Nurses wear a white tunic with epaulettes in a colour or quantity that represents their year of training or grade.
There is some suggestion that Male Nurses should wear the same uniform as their female counterparts to assist patients in recognizing who is a nurse and assisting in establishing male nurses on equal terms as their counterparts. 
Traditional uniforms remain common in many countries, but in Western Europe and North America, the so-called "scrubs" or tunics have become more popular. "Scrub dress" is a simpler type of uniform, and is almost always worn in operating rooms and emergency rooms.
Nurse uniforms vs scrubs.
Beginning in the 1990s, and until the present time, the traditional nurse uniforms have been replaced with the "new" scrub dress in some countries. Most hospitals in the USA and Europe argue that the scrub uniform is easier to clean than the old nurse uniforms. The nurses who wear the uniforms are divided into two camps:
In many parts of the world, nurses continue to wear a white uniform consisting of a dress and cap.
The traditional white uniform for male nursing staff is now going out of fashion, excepting for student nurses.
A tunic of either the dental surgeon style or a v neck with a collar is very often used.
The colours vary with grade, area of work, and hospital; however, the male equivalent of sister (that is, charge nurse) tend to be shades of blue or dark green: often, this is the only colour to be recognised by the public as signifying a person in authority.
Nursing Jewellery.
Nurses were actively discouraged from wearing jewellery which might distract from their purpose and get caught on patient skin during care activity. A fob watch or "pendant" Watch was once considered synonymous with nursing. The fob watch freed the nurses hands for client care and prevented the wrist watch becoming a vector for disease. Watches were sometimes given as a token rite-of-passage gift from parents to young nurses, who were making the transition into nurses quarters and lived away from home for the first time.

</doc>
<doc id="55615" url="https://en.wikipedia.org/wiki?curid=55615" title="School uniform">
School uniform

A school uniform is a uniform worn by students primarily for a school or otherwise educational institution. They are common in primary and secondary schools in various countries. Although often used interchangeably, there is an important distinction between dress codes and school uniforms: according to scholars such as Nathan Joseph, clothing can only be considered a uniform when it "(a) serves as a group emblem, (b) certifies an institution's legitimacy by revealing individual’s relative positions and (c) suppresses individuality." An example of a uniform would be requiring white button-downs and ties for boys and pleated skirts for girls, with both wearing blazers. A uniform can even be as simple as requiring collared shirts, or restricting colour choices and limiting items students are allowed to wear. A dress code, on the other hand, is much less restrictive, and focuses "on promoting modesty and discouraging anti-social fashion statements," according to Marian Wilde. Examples of a dress code would be not allowing ripped clothing, no logos or limiting the amount of skin that can be shown.
History.
It is difficult to trace the origins of the uniform as there is no comprehensive written history but rather a variety of known influences. Although uniforms can often be considered conservative and old-fashioned, uniforms in recent years have changed as societal dress codes have changed. Little is known prior to the nineteenth century about uniforms, but there certainly are influences dating back to the 13th century. In 1222, the Archbishop of Canterbury ordered monks to wear a specific monastic form of dress. Despite this example, the roots of the modern day uniform come mostly from the collegiate uniforms in England. Universities, primary schools and secondary schools used uniforms as a marker of class and status, which in turn served as a boundary. As early as the sixteenth century, uniforms were utilised and became more specific as various fashion trends became undesirable to the university.
Contemporary.
In the United States, a movement toward using uniforms in state schools began when Bill Clinton addressed it in the 1996 State of the Union, saying: "If it means that teenagers will stop killing each other over designer jackets, then our public schools should be able to require their students to wear uniforms." As of 1998 approximately 25% of all U.S. public elementary, middle and junior high schools had adopted a uniform policy or were considering a policy, and two thirds were implemented between 1995 and 1997.
There are an abundance of theories and empirical studies looking at school uniforms, making statements about their effectiveness. These theories and studies elaborate on the benefits and also the shortcomings of uniform policies. The issue of nature vs. nurture comes into play, as uniforms affect the perceptions of masculinity and femininity, complicate the issue of gender classification and also subdue the sexuality of girls. With uniforms also comes a variety of controversies, pros, cons and major legal implications.
There are two main empirical findings that are most often cited in the political rhetoric surrounding the uniform debate. One of these, the case study of the Long Beach Unified School District, is most often cited in support of school uniforms and their effectiveness whereas "Effects of Student Uniforms on Attendance, Behavior Problems, Substance Use, and Academic Achievement" is the most frequently cited research in opposition to the implementation of school uniform policies.
Long Beach Unified School District study.
The case study of the Long Beach Unified School District was the study of the first large, urban school in the United States to implement a uniform policy. In 1994, mandatory school uniforms were implemented for the districts elementary and middle schools as a strategy to address the students' behaviour issues. The district simultaneously implemented a longitudinal study to research the effects of the uniforms on student behavior. The study attributed favourable student behavioral changes and a significant drop in school discipline issues to the mandatory uniform policy. This case study attributed the following noticeable outcomes to the use of uniforms throughout the district:
Effects of Student Uniforms on Students.
The Brunsma et al. research, on the other hand, empirically examined how a school uniform affects attendance, behavior problems, substance abuse, and academic achievement. In this very well known study, researchers tested the following hypotheses:
Researchers in this study expected that the direct of uniforms on these outcomes would disappear once the moderating variables were introduced to the equation. If this were to be the case, then arguments proclaiming uniform policies' direct effect on said outcomes would be proven false. This study used a nationally representative sample of students. From the results, researchers were able to conclude that students wearing uniforms did not have any significant difference in academic preparedness or proschool attitudes than other students. Researchers also found that student uniforms were not significantly correlated with school commitment variables such as truancy, behaviour, or drug abuse. In terms of the original four hypotheses researchers found that:
Because the four hypotheses were not supported, researchers were able to conclude that implementing uniform policies at high school level does not create the desired outcomes, as all four of the original hypotheses were derived from public discourse surrounding the uniform debate. In fact, Brunsma et al., 1998 found that uniforms had a significant negative effect on achievement, as students who wore uniforms and had high proschool attitudes actually had worse behavior problems than all other students. Researchers in this study suggested that "instead of directly affecting specific outcomes, uniforms act as a catalyst for change and provide a highly visible opportunity for additional programs" within schools. In fact, Brunsma et al., 1998 found that this was the case with the Long Beach Unified School District case study, as several additional reform efforts were implemented simultaneously with the mandatory uniform policy.
Laws and rulings.
As uniforms have become more normalised, there have also been an increasing number of lawsuits brought against school districts. According to David Brunsma, one in four public elementary schools and one in eight public middle and high schools in the USA have policies dictating what a student wears to school. The school code within states’ constitutions typically asserts that it allows the board of school directors to make reasonable rules and regulations as they see fit in managing the school’s affairs. As of 2008, there are currently 23 states that allow school districts to mandate school uniforms. The constitutional objections usually brought upon school districts tend to fall into one of the following two categories: (1) a violation of the students’ First Amendment right to free expression (2) a violation of parents to raise their children without government interference. Although up until this point, The Supreme Court has not ruled on a case involving school uniforms directly, in the 1968 Tinker v. Des Moines Independent Community School District, the Court ruled that upon entering school, students do not shed their constitutional rights to freedom of speech.
Internationally, there are differing views of school uniforms. In the Australian state of Queensland, Ombudsman Fred Albietz ruled in 1998 that state schools may not require uniforms. In the Philippines, the Department of Education abolished the requirement of school uniforms in public schools. In England and Wales, technically a state school may not permanently exclude students for "breaching school uniform policy", under a policy promulgated by the Department for Children, Schools and Families but students not wearing the correct uniform are asked to go home and change. In Scotland, some local councils (that have responsibility for delivering state education) do not insist on students wearing a uniform as a precondition to attending and taking part in curricular activities. Turkey abolished mandatory uniforms in 2010.
Examples of lawsuits.
"Canady v. Bossier Parish School Board".
In the "Canady v. Bossier Parish School Board" lawsuit in 2000, a Louisiana district court ruled in favour of the school board because it did not see how the free speech rights of the students were being violated due to the school board's uniform policy. Even though the plaintiff appealed the decision, the Fifth Circuit Court also ruled in favour of the school board after implementing a four-step system that is still used today. Firstly, a school board has to have the right to set up a policy. Secondly, the policy must be determined to support a fundamental interest of the board as a whole. Thirdly, the guidelines cannot have been set for the purpose of censorship. Finally, the limits on student expression cannot be greater than the interest of the board. As long as these four policies are in place, then no constitutional violation can be claimed.
"Littlefield v. Forney Independent School District".
In the Forney Independent School District of Forney, Texas in 2001, the school board decided to implement a school uniform policy allowing the students to wear a polo shirt, oxford shirt or blouse in four possible colours, and blue or khaki pants or shirts, a skirt or jumper. While there was some flexibility with shoes, certain types were prohibited along with any sort of baggy clothes. The parents of the Littlefield family requested that their son be exempt from the policy, but were denied. In response, the Littlefields filed a lawsuit against the school district, under the pretenses that this uniform mandate infringed on their rights as parents to control how they brought up their children and their education. They even went as far as to cite an infringement on religious freedom, claiming that opting out of the uniforms on the grounds of religion allowed the school to rank the validity of certain religions. Before trial, the District Court dismissed the case, so the family appealed. Ultimately, the Fifth Circuit Court ruled that the students' rights were not being violated even though the claims presented were valid. They ruled that school rules derived from the education would override the parents' right to control their children's upbringing in this specific situation. As far as the religious freedom violation accusations, the court ruled that the policy did not have a religious goal, and thus did not infringe on religious freedom rights.
"Jacobs v. Clark County School District".
In 2003, Liberty High School, a school of the Clark County School District in Henderson, Nevada, implemented a uniform policy of khakis and red, white or blue polo shirts. A junior by the name of Kimberly Jacobs was suspended a total of five times because she wore a religious shirt to school and got cited for uniform violations. Her family sued the Clark County School District under the claims that her First Amendment rights were being infringed upon and that the uniform policy was causing students to be deprived of due process. The plaintiff's requests were for injunctive relief, the expunging of suspensions from Jacob's school record and awarding of damages. The injunction was granted to the family meaning that the school could no longer discipline her for breaking the uniform policy. At this ruling, the school district appealed. The next court ruled on the side of the school district as it determined that the uniform policy was in fact neutral and constitutional, and it dismissed the claims of the plaintiff.
"Frudden v. Washoe County School District".
In 2011, a Nevada public elementary school of the Washoe County School District decided to add the school's motto, "Tomorrow's Leaders" embroidered in small letters on the shirt. In response, Mary and John Frudden, parents of a student sued the school district on the basis of it violating the 1st Amendment. The court ultimately dismissed the case filed by the Fruddens over the uniforms. However, the family appealed, and two years later, a three-judge panel of the 9th U.S. Circuit Court of Appeals heard the case. The court ruled to reverse the previous decision of dismissing the case, and also questioned the apparent policy for students that were part of a nationally recognised group such as Boy Scouts and Girl Scouts who were able to wear the uniforms in place of the school ones on regular meeting days. The 9th circuit panel ruled that the school had not provided enough evidence for why it instituted this policy, and that the family was never given a chance to argue.
Social implications of school uniforms on gender.
There are several positive and negative social implications of uniforms on both the students wearing them and society as a whole.
Perceptions of masculinity and femininity.
One of the criticisms of uniforms is that it imposes standards of masculinity and femininity from a young age. Uniforms are considered a form of discipline that schools use to control student behavior and often promote conventional gendered dress. Boys often are required to wear trousers, belts, and closed-toe shoes and have their shirts tucked in at all times. They are also often required to have their hair cut short. Some critics allege that this uniform is associated with the dress of a professional business man, which, they claim, gives boys at a young age the impression that masculinity is gained through business success. For girls, some uniforms promote femininity by requiring girls to wear skirts. Skirts are seen by some critics as a symbol of femininity because they restrict movement and force certain ways of sitting and playing.
Enforcing gender binaries.
Another criticism of uniforms is that they promote a binary gender classification. There are generally uniforms for females and uniforms for males but no unisex options for students who do not necessarily identify as totally female or totally male. Therefore, students are not given the opportunity to explore their gender identity and must conform to the binary gender system. These students are punished for their exploration of gender expression because it violates uniform and dress codes (such as boys not having their hair longer than shoulder length or girls only being allowed to wear skirts and tights).
However, proponents of uniforms say that schools are a place for academic learning and that there must be some limitations on individual expression. They argue that if schools do not enforce rules, some focus on learning may be lost.
Sexualization of girls.
Uniforms often start to increase in popularity around middle school in the United States, when students begin going through puberty. Uniforms can be seen as a way to restrict the sexualization of girls (rules on hems of skirts, no shoulders). Uniforms take the focus away from sexuality and focus it on academics in a school setting for girls.
Miniskirts have been very popular in Japan, where they became part of school uniforms, and they came to be worn within the Kogal culture.
Controversies.
General.
In some cultures, the topic of school uniforms has sparked a multitude of controversies and debates over the years. Debates concerning the constitutionality and economic feasibility of uniforms also contribute to the controversy.
In the United States, the implementation of school uniforms began following ten years of research indicating the effectiveness of private schools. Some state-school reformers cited this research to support policies linked to private and Catholic school success. However, within the Catholic school literature, school uniforms have never been acknowledged as a primary factor in producing a Catholic school effect. Some public-school administrators began implementing uniform policies to improve the overall school environment and academic achievement of the students. This is based on the assumption that uniforms are the direct cause of behavioral and academic outcome changes.
Another area of controversy regarding school uniform and dress code policies revolve around the issue of gender. Nowadays, more teenagers are more frequently "dressing to articulate, or confound gender identity and sexual orientation", which brings about "responses from school officials that ranged from indifferences to applause to bans". In 2009, there were multiple conflicts across the United States arising from disparities between the students' perception of their own gender, and the school administrators' perception of the students' gender identity. Instances include the following:
Although not all schools in the United States are required to wear school uniforms. The United States is slowly adapting the use of school uniforms. "Almost one in five US public schools required students to wear uniforms during the 2011-2012 school year, up from one in eight in 2003-2004." The ideology of school uniform is that it will create a safer environment for students and help with equality. In some areas uniforms have become essential due to the poverty level that the schools reside in. "Mandatory uniform policies in public schools are found more commonly in high-poverty areas."
Positives.
Advocates of uniforms have proposed multiple reasons supporting their implementation and claiming their success in schools. A variety of these claims have research supporting them. Some of these pros include the following:
Advocates believe that uniforms affect student safety by:
For example, in the first year of the mandatory uniform policy in Long Beach, California, officials reported that fighting in schools decreased by more than 50%, assault and battery by 34%, sex offenses by 74%, and robbery by 66%. 
Advocates also believe that uniforms increase student learning and positive attitudes toward school through:
Wearing uniforms leads to decreased behavior problems by increasing attendance rates, lowering suspension rates, and decreasing substance use among the student body. Proponents also attribute positive psychological outcomes like increased self-esteem, increased spirit, and reinforced feelings of oneness among students to wearing uniforms. Additional proponent arguments include that school uniforms:
Currently pros of school uniforms center around how uniforms impact schools' environments. Proponents have found a significant positive impact on school climate, safety, and students’ self-perception from the implementation of uniforms.
Negatives.
The opposing side of uniforms have claimed their ineffectiveness using a variety of justifications, a variety of which have research supporting them. Some of the cons to school uniforms include the following legal, financial, and questionable effectiveness concerns: The primary concern with school uniforms or strict dress codes is that it limits the ability of the student to express themselves. Clothing is viewed as a mean of expression. By making all students wear the same clothes or limit them to what they can wear, can disrupt their sense of identity. One of the main controversies can lie within Dress Code Policies vs. Freedom of Speech. This establishes that students cannot wear the latest trends, mid-drift, or clothes that the school finds that interrupts the learning environment. Although, students can wear clothing artifacts that express their religion. " Both the Constitution and most state laws protect students’ rights to wear religious attire inool school, such as the wearing of a turban, yarmulke, or head scarf."
Another negative aspect of school uniforms is that it can be sexist. Boys and girls are not disciplined the same when it comes to dress codes. "Transgender students have been sent home for wearing clothing different than what’s expected of their legal sex, while others have been excluded from yearbooks." Some schools are not advocates of females and females dressing of the opposite sex. Research on how school uniforms and school dress codes influence the student can be inconclusive , but many people oppose to school uniforms and strict dress code policies. "In the U.S., over half of public schools have a dress code, which frequently outline gender-specific policies."
According to Marian Wilde, additional opponent arguments include that school uniforms:

</doc>
<doc id="55620" url="https://en.wikipedia.org/wiki?curid=55620" title="Catiline Orations">
Catiline Orations

The Catiline Orations or Catilinarian Orations were speeches given in 63 B.C. by Marcus Tullius Cicero, the consul of Rome, exposing to the Roman Senate the plot of Lucius Sergius Catilina and his allies to overthrow the Roman government.
The Catiline plot and the orations of Cicero.
Running for the consulship for a second time after having lost the first time, Catiline was an advocate for the poor, calling for the cancellation of debts, and land redistribution. However, there was substantial evidence Catiline had bribed numerous senators to vote for him, and engaged in other unethical conduct related to the election. Cicero, in indignation at this, issued a law prohibiting such machinations. It seemed obvious to all that the law was directed specifically at Catiline. Catiline, in turn, conspired to murder Cicero and the key men of the Senate on the day of the election in what became known as the second Catilinarian conspiracy. Cicero discovered the plan and postponed the election to give the Senate time to discuss the attempted coup d'état.
The day after the election was supposed to be held, Cicero addressed the Senate on the matter and Catiline's reaction was immediate and violent. In response to Catiline's behavior, the Senate issued a "senatus consultum ultimum", a kind of declaration of martial law invoked whenever the Senate and the Roman Republic were considered to be in imminent danger from treason or sedition. Ordinary law was suspended and Cicero, as consul, was invested with absolute power.
When the election was finally held, Catiline lost again. Anticipating the bad news, the conspirators had already begun to assemble an army, made up mostly of Sulla's veteran soldiers. The nucleus of conspirators was also joined by some senators. The plan was to initiate an insurrection in all of Italy, put Rome to the torch and to kill as many senators as they could.
Through his own investigations, Cicero was aware of the conspiracy. On November 8, Cicero called for a meeting of the Senate in the Temple of Jupiter Stator near the forum, which was used for this purpose only when great danger was imminent. Catiline attended as well. It was in this context that Cicero delivered one of his most famous orations.
"Oratio in Catilinam Prima in Senatu Habita".
As political orations go, this was relatively short—some 3,400 words—and to the point. The opening remarks are still widely remembered and used after 2,000 years:
Also remembered is the famous exasperated exclamation, "O tempora, o mores!" (Oh the times! Oh the customs!)
Catiline was present when this speech was delivered. When he arrived at the Temple of Jupiter Stator and took his seat, however, the other senators moved away from him leaving him alone in his bench. Catiline tried to reply after the speech, but senators repeatedly interrupted him, calling him a traitor. He ran from the temple, hurling threats at the Senate. Later he left the city and, though he claimed that he was placing himself in self-imposed exile at Marseilles, he in fact went to the camp of Manlius, who was in charge of the army of rebels. The next morning Cicero assembled the people, and gave a further oration.
"Oratio in Catilinam Secunda Habita ad Populum".
In this speech, Cicero informed the citizens of Rome that Catiline had left the city, not in exile (as it was rumored), but to join with his illegal army. He described the conspirators as rich men who were in debt, men eager for power and wealth, Sulla's veterans, ruined men who hoped for any change, criminals, profligates, and other men of Catiline's ilk. He assured the people of Rome that they had nothing to fear because he, the consul, and the gods would protect the state.
Meanwhile, Catiline joined up with Gaius Manlius, commander of the rebel force. When the Senate was informed of these developments, they declared the two of them public enemies. Antonius Hybrida (Cicero's fellow consul), with troops loyal to Rome, followed Catiline while Cicero remained at home to guard the city.
"Oratio in Catilinam Tertia ad Populum".
In this speech, Cicero claims that the city should rejoice because it has been saved from a bloody rebellion. He presents evidence that all of Catiline's accomplices confessed to their crimes. He asked for nothing for himself but the grateful remembrance of the city, and acknowledged that this victory was more difficult than one in foreign lands because the enemies were citizens of Rome.
"Oratio in Catilinam Quarta in Senatu Habita".
In his fourth and final argument, which took place in the Temple of Concordia, Cicero establishes a basis for other orators (primarily Cato) to argue for the execution of the conspirators. As consul, Cicero was formally not allowed to voice any opinion in the matter, but he circumvented the rule with subtle oratory. Although very little is known about the actual debate (except for Cicero's argument, which has probably been altered from its original), the Senate majority probably opposed the death sentence for various reasons, one of which was the nobility of the accused. For example, Julius Caesar argued that exile and disenfranchisement would be sufficient punishment for the conspirators, and one of the accused, Lentulus, was a praetor. However, after the combined efforts of Cicero and Cato, the vote shifted in favor of execution, and the sentence was carried out shortly afterwards.
While most historians agree that Cicero's actions, and in particular the final speeches before the Senate, saved the republic, they also reflect his self-aggrandisement—and to a certain extent envy—probably born out of the fact that he was considered a "novus homo", a Roman citizen without noble or ancient lineage.

</doc>
<doc id="55623" url="https://en.wikipedia.org/wiki?curid=55623" title="Cotyledon">
Cotyledon

A cotyledon (; "seed leaf" from Latin "cotyledon", from Greek: κοτυληδών "kotylēdōn", gen.: κοτυληδόνος "kotylēdonos", from κοτύλη "kotýlē" "cup, bowl") is a significant part of the embryo within the seed of a plant, and is defined by the Oxford English Dictionary as "The primary leaf in the embryo of the higher plants (Phanerogams); the seed-leaf." Upon germination, the cotyledon may become the embryonic first leaves of a seedling. The number of cotyledons present is one characteristic used by botanists to classify the flowering plants (angiosperms). Species with one cotyledon are called monocotyledonous ("monocots"). Plants with two embryonic leaves are termed dicotyledonous ("dicots") and placed in the class Magnoliopsida.
In the case of dicot seedlings whose cotyledons are photosynthetic, the cotyledons are functionally similar to leaves. However, true leaves and cotyledons are developmentally distinct. Cotyledons are formed during embryogenesis, along with the root and shoot meristems, and are therefore present in the seed prior to germination. True leaves, however, are formed post-embryonically (i.e. after germination) from the shoot apical meristem, which is responsible for generating subsequent aerial portions of the plant.
The cotyledon of grasses and many other monocotyledons is a highly modified leaf composed of a "scutellum" and a "coleoptile". The scutellum is a tissue within the seed that is specialized to absorb stored food from the adjacent endosperm. The coleoptile is a protective cap that covers the "plumule" (precursor to the stem and leaves of the plant).
Gymnosperm seedlings also have cotyledons, and these are often variable in number (multicotyledonous), with from 2 to 24 cotyledons forming a whorl at the top of the hypocotyl (the embryonic stem) surrounding the plumule. Within each species, there is often still some variation in cotyledon numbers, e.g. Monterey pine ("Pinus radiata") seedlings have 5–9, and Jeffrey pine ("Pinus jeffreyi") 7–13 (Mirov 1967), but other species are more fixed, with e.g. Mediterranean cypress always having just two cotyledons. The highest number reported is for big-cone pinyon ("Pinus maximartinezii"), with 24 (Farjon & Styles 1997).
The cotyledons may be ephemeral, lasting only days after emergence, or persistent, enduring at least a year on the plant. The cotyledons contain (or in the case of gymnosperms and monocotyledons, have access to) the stored food reserves of the seed. As these reserves are used up, the cotyledons may turn green and begin photosynthesis, or may wither as the first true leaves take over food production for the seedling.
Epigeal versus hypogeal development.
Cotyledons may be either epigeal, expanding on the germination of the seed, throwing off the seed shell, rising above the ground, and perhaps becoming photosynthetic; or hypogeal, not expanding, remaining below ground and not becoming photosynthetic. The latter is typically the case where the cotyledons act as a storage organ, as in many nuts and acorns.
Hypogeal plants have (on average) significantly larger seeds than epigeal ones. They are also capable of surviving if the seedling is clipped off, as meristem buds remain underground (with epigeal plants, the meristem is clipped off if the seedling is grazed). The tradeoff is whether the plant should produce a large number of small seeds, or a smaller number of seeds which are more likely to survive.
Related plants show a mixture of hypogeal and epigeal development, even within the same plant family. Groups which contain both hypogeal and epigeal species include, for example, the Araucariaceae family of Southern Hemisphere conifers, the Fabaceae (pea family), and the genus "Lilium" (see Lily seed germination types). The frequently garden grown common bean - Phaseolus vulgaris - is epigeal while the closely related runner bean - Phaseolus coccineus - is hypogeal.
History.
The term "cotyledon" was coined by Marcello Malpighi (1628–1694). John Ray was the first botanist to recognize that some plants have two and others only one, and eventually the first to recognize the immense importance of this fact to systematics, in "Methodus plantarum" (1682).
Theophrastus (3rd or 4th century BC) and Albertus Magnus (13th century) may also have recognized the distinction between the dicotyledons and monocotyledons.

</doc>
<doc id="55625" url="https://en.wikipedia.org/wiki?curid=55625" title="Monocotyledon">
Monocotyledon

Monocotyledons (), commonly referred to as monocots, (Lilianae "sensu" Chase & Reveal) are flowering plants (angiosperms) whose seeds typically contain only one embryonic leaf, or cotyledon. They constitute one of the major groups into which the flowering plants have traditionally been divided, the rest of the flowering plants having two cotyledons and therefore classified as dicotyledons, or dicots. However, molecular phylogenetic research has shown that while the monocots form a monophyletic group or clade (comprising all the descendants of a common ancestor), the dicots do not. Monocots have almost always been recognized as a group, but with various taxonomic ranks and under several different names. The APG III system of 2009 recognises a clade called "monocots" but does not assign it to a taxonomic rank.
The monocots include about 60,000 species. The largest family in this group (and in the flowering plants as a whole) by number of species are the orchids (family Orchidaceae), with more than 20,000 species. About half as many species belong to the true grasses (Poaceae), who are economically the most important family of monocots. In agriculture the majority of the biomass produced comes from monocots. These include not only major grains (rice, wheat, maize, etc.), but also forage grasses, sugar cane, and the bamboos. Other economically important monocot crops include various palms (Arecaceae), bananas (Musaceae), gingers and their relatives, turmeric and cardamom (Zingiberaceae), asparagus and the onions and garlic family (Amaryllidaceae). Additionally most of the horticultural bulbs, plants cultivated for their blooms, are monocots, such as lilies, daffodils, irises, amaryllis, cannas, bluebells and tulips.
Description.
The monocots or monocotyledons have a single cotyledon, or embryonic leaf, in their seeds. Historically, this feature was used to contrast the monocots with the dicotyledons or dicots which typically have two cotyledons; however modern research has shown that the dicots are not a natural group. From a diagnostic point of view the number of cotyledons is neither a particularly useful characteristic (as they are only present for a very short period in a plant's life), nor is it completely reliable.
Comparison with dicotyledons.
The traditionally listed differences between monocotyledons and dicotyledons are as follows. This is a broad sketch only, not invariably applicable, as there are a number of exceptions. The differences indicated are more true for monocots versus eudicots.
A number of these differences are not unique to the monocots, and while still useful no one single feature, will infallibly identify a plant as a monocot. For example, trimerous flowers and monosulcate pollen are also found in magnoliids, of which exclusively adventitious roots are found in some of the Piperaceae. Similarly, at least one of these traits, parallel leaf veins, is far from universal among the monocots. Monocots with broad leaves and reticulate leaf veins, typical of dicots, are found in a wide variety of monocot families: for example, "Trillium", "Smilax" (greenbriar), and "Pogonia" (an orchid), and the Dioscoreales (yams). "Potamogeton" are one of several monocots with tetramerous flowers. Other plants exhibit a mixture of characteristics. Nymphaeaceae (water lilies) have reticulate veins, a single cotyledon, adventitious roots and a monocot like vascular bundle. These examples reflect their shared ancestry. Nevertheless, this list of traits is a generally valid set of contrasts, especially when contrasting monocots with eudicots rather than non-monocot flowering plants in general.
Vascular system.
Monocots have a distinctive arrangement of vascular tissue known as an atactostele in which the vascular tissue is scattered rather than arranged in concentric rings. Collenchyma is absent in monocot stems, roots and leaves. Many monocots are herbaceous and do not have the ability to increase the width of a stem (secondary growth) via the same kind of vascular cambium found in non-monocot woody plants. However, some monocots do have secondary growth, and because it does not arise from a single vascular cambium producing xylem inwards and phloem outwards, it is termed "anomalous secondary growth". Examples of large monocots which either exhibit secondary growth, or can reach large sizes without it, are palms (Arecaceae), screwpines (Pandanaceae), bananas (Musaceae), "Yucca", "Aloe", "Dracaena", and "Cordyline".
Synapomorphies.
By contrast Soltis and others identify thirteen synapomorphies (shared characteristics that unite monophyletic groups of taxa);
Taxonomy.
The monocots form one of five major lineages of mesangiosperms, which in themselves form 99.95% of all angiosperms. The monocots and the eudicots, are the largest and most diversified angiosperm radiations accounting for 20% and 75% of all angiosperm species respectively.
Monocot diversity includes perennial geophytes including ornamental flowers (orchids, tulips and lilies) (Asparagales, Liliales respectively), rosette and succulent epiphytes (Asparagales), mycoheterotrophs (Liliales, Dioscoreales, Pandanales), all in the lilioid monocots, major grains (maize, rice and wheat) in the grass family (Poales) as well as woody tree-like palm trees (Arecales) and bamboo (Poales) in the commelinid monocots, as well as both emergent (Poales, Acorales) and floating or submerged aquatic plants (Alismatales).
Early history.
The monocots are one of the major divisions of the flowering plants or angiosperms. They have been recognized as a natural group since John Ray's studies of seed structure in the 17th century. Ray was the first botanical systematist, and in his examination of seeds, first observed the dichotomy of cotyledon structure. He reported his findings in a paper read to the Royal Society on 17 December 1674, entitled "A Discourse on the Seeds of Plants".
Although Linnaeus did not utilise Ray's discovery, basing his own classification solely on floral reproductive morphology, every taxonomist since then, starting with De Jussie and De Candolle, has used Ray's distinction as a major classification characteristic.
Modern era.
Modern research based on DNA has confirmed the status of the monocots as a monophyletic group or clade, in contrast to the other historical divisions of the flowering plants, which have had to be substantially reorganized. The monocots form about a quarter of all of the Angiosperms (flowering plants). Of some 60,000 species, by far the largest number (65%) are found in two families, the orchids and grasses. The orchids (Orchidaceae, Asparagales) contain about 25,000 species and the grasses (Poaceae, Poales) about 11,000. Other well known groups within the Poales order include the Cyperaceae (sedges) and Juncaceae (rushes), and the monocots also include familiar families such as the palms (Arecaceae, Arecales) and lilies (Liliaceae, Liliales).
Taxonomists had considerable latitude in naming this group, as the monocots are a group above the rank of family. Article 16 of the "ICBN" allows either a descriptive name or a name formed from the name of an included family.
Historically, the monocotyledons were named:
Until the rise of the phylogenetic APG systems, it was widely accepted that angiosperms were neatly split between monocots and dicots, a state reflected in virtually all the systems. It is now understood that various groups, notably the Magnoliids and ancient lineages known as the basal angiosperms fall outside of this dichotomy. Each of these systems uses its own internal taxonomy for the group. The monocotyledons are famous as a group that is extremely stable in its outer borders (it is a well-defined, coherent group), while in its internal taxonomy is extremely unstable (historically no two authoritative systems have agreed with each other on how the monocotyledons are related to each other).
Molecular studies have both confirmed the monophyly of the monocots and helped elucidate relationships within this group. The APG III system does not assign the monocots to a taxonomic rank, instead recognizing a monocots clade. However there has remained some uncertainty regarding the exact relationships between the major lineages, with a number of competing models (including APG).
Subdivisions.
Historically, Bentham (1877), considered the monocots to consist of four alliances, Epigynae, Coronariae, Nudiflorae and Glumales, based on floral characteristics. He describes the attempts to subdivide the group since the days of Lindley as largely unsuccessful. Like most subsequent classification systems it failed to distinguish between two major orders, Liliales and Asparagales, now recognised as quite separate. A major advance in this respect was the work of Rolf Dahlgren (1980), which would form the basis of the Angiosperm Phylogeny Group's (APG) subsequent modern classification of monocot families. Dahlgren who used the alternate name Lilliidae considered the monocots as a subclass of angiosperms characterised by a single cotyledon and the presence of triangular protein bodies in the sieve tube plastids. He divided the monocots into seven superorders, Alismatiflorae, Ariflorae, Triuridiflorae, Liliiflorae, Zingiberiflorae, Commeliniflorae and Areciflorae. With respect to the specific issue regarding Liliales and Asparagales, Dahlgren followed Huber (1969) in adopting a splitter approach, in contrast to the longstanding tendency to view Liliaceae as a very broad sensu lato family. Following Dahlgren's untimely death in 1987, his work was continued by his widow, Gertrud Dahlgren, who published a revised version of the classification in 1989. In this scheme the suffix "-florae" was replaced with "-anae" ("e.g." Alismatanae) and the number of superorders expanded to ten with the addition of Bromelianae, Cyclanthanae and Pandananae.
The APG system establishes ten orders of monocots and two families of monocots (Petrosaviaceae and Dasypogonaceae) not yet assigned to any order. More recently, the Petrosaviaceae has been included in the Petrosaviales, and placed near the lilioid orders. The family Hydatellaceae, assigned to order Poales in the APG II system, has since been recognized as being misplaced in the monocots, and instead proves to be most closely related to the water lilies, family Nymphaeaceae.
Evolution.
The monocots form a monophyletic group arising early in the history of the flowering plants, but the fossil record is meagre. The earliest fossils presumed to be monocot remains date from the early Cretaceous period. For a very long time, fossils of palm trees were believed to be the oldest monocots, first appearing 90 million years ago, but this estimate may not be entirely true. At least some putative monocot fossils have been found in strata as old as the eudicots. The oldest fossils that are unequivocally monocots are pollen from the Late Barremian–Aptian – Early Cretaceous period, about 120-110 million years ago, and are assignable to clade-Pothoideae-Monstereae Araceae; being Araceae, sister to other Alismatales. They have also found flower fossils of Triuridaceae (Pandanales) in Upper Cretaceous rocks in New Jersey, becoming the oldest known sighting of saprophytic/mycotrophic habits in angiosperm plants and among the oldest known fossils of monocotyledons.
Topology of the angiosperm phylogenetic tree could infer that the monocots would be among the oldest lineages of angiosperms, which would support the theory that they are just as old as the eudicots. The pollen of the eudicots dates back 125 million years, so the lineage of monocots should be that old too.
Molecular clock estimates.
Kåre Bremer, using rbcL sequences and the mean path length method ("mean-path lengths method"), estimated the age of the monocot crown group (i.e. the time at which the ancestor of today's "Acorus" diverged from the rest of the group) as 134 million years. Similarly, Wikström "et al.", using Sanderson's non-parametric rate smoothing approach ("nonparametric rate smoothing approach"), obtained ages of 158 or 141 million years for the crown group of monocots. All these estimates have large error ranges (usually 15-20%), and Wikström "et al." used only a single calibration point, namely the split between Fagales and Cucurbitales, which was set to 84 Ma, in the late Santonian period). Early molecular clock studies using strict clock models had estimated the monocot crown age to 200 ± 20 million years ago or 160 ± 16 million years, while studies using relaxed clocks have obtained 135-131 million years or 133.8 to 124 million years. Bremer's estimate of 134 million years has been used as a secondary calibration point in other analyses. Some estimates place the emergence of the monocots as far back as 150 mya in the Jurassic period.
Core group.
The age of the core group of so-called 'nuclear monocots' or 'core monocots', which correspond to all orders except Acorales and Alismatales, is about 131 million years to present, and crown group age is about 126 million years to the present. The subsequent branching in this part of the tree (i.e. Petrosaviaceae, Dioscoreales + Pandanales and Liliales clades appeared), including the crown Petrosaviaceae group may be in the period around 125–120 million years BC (about 111 million years so far), and stem groups of all other orders, including Commelinidae would have diverged about or shortly after 115 million years. These and many clades within these orders may have originated in southern Gondwana, i.e. Antarctica, Australasia, and southern South America.
Aquatic monocots.
The aquatic monocots of Alismatales have commonly been regarded as "primitive". They have also been considered to have the most primitive foliage, which were cross-linked as Dioscoreales and Melanthiales. Keep in mind that the "most primitive" monocot is not necessarily "the sister of everyone else". This is because the ancestral or primitive characters are inferred by means of the reconstruction of characteristic states, with the help of the phylogenetic tree. So primitive characters of monocots may be present in some derived groups. On the other hand, the basal taxa may exhibit many morphological autapomorphies. So although Acoraceae is the sister group to the remaining monocotyledons, the result does not imply that Acoraceae is "the most primitive monocot" in terms of its characteristics. In fact, Acoraceae is highly derived in most morphological characteristics, which is precisely why so many Alismatales Acoraceae occupied relatively imitative positions in trees produced by Chase "et al." and others.
Some authors support the idea of an aquatic phase as the origin of monocots. The phylogenetic position of Alismatales (many water), which occupy a relationship with the rest except the Acoraceae, do not rule out the idea, because it could be 'the most primitive monocots' but not 'the most basal'. The Atactostele stem, the long and linear leaves, the absence of secondary growth (see the biomechanics of living in the water), roots in groups instead of a single root branching (related to the nature of the substrate), including sympodial use, are consistent with a water source. However, while monocots were sisters of the aquatic Ceratophyllales, or their origin is related to the adoption of some form of aquatic habit, it would not help much to the understanding of how it evolved to develop their distinctive anatomical features: the monocots seem so different from the rest of angiosperms and it's difficult to relate their morphology, anatomy and development and those of broad-leaved angiosperms.
Other taxa.
In the past, taxa which had petiolate leaves with reticulate venation were considered "primitive" within the monocots, because of its superficial resemblance to the leaves of dicotyledons. Recent work suggests that these taxa are sparse in the phylogenetic tree of monocots, such as fleshy fruited taxa (excluding taxa with aril seeds dispersed by ants), the two features would be adapted to conditions that evolved together regardless. Among the taxa involved were "Smilax", "Trillium" (Liliales), "Dioscorea" (Dioscoreales), etc. A number of these plants are vines that tend to live in shaded habitats for at least part of their lives, and may also have a relationship with their shapeless stomata. Reticulate venation seems to have appeared at least 26 times in monocots, in fleshy fruits 21 times (sometimes lost later), and the two characteristics, though different, showed strong signs of a tendency to be good or bad in tandem, a phenomenon described as "concerted convergence" ("coordinated convergence").
Etymology.
The name monocotyledons is derived from the traditional botanical name "Monocotyledones", which refers to the fact that most members of this group have one cotyledon, or embryonic leaf, in their seeds.
Ecology.
Emergence.
Some monocots, such as grasses, have hypogeal emergence, where the mesocotyl elongates and pushes the coleoptile (which encloses and protects the shoot tip) toward the soil surface. Since elongation occurs above the cotyledon, it is left in place in the soil where it was planted. 
Many dicots have epigeal emergence, in which the hypocotyl elongates and becomes arched in the soil. As the hypocotyl continues to elongate, it pulls the cotyledons upward, above the soil surface.
Uses.
Of the monocots, the grasses are of enormous economic importance as a source of animal and human food, and form the largest component of agricultural species in terms of biomass produced.

</doc>
<doc id="55632" url="https://en.wikipedia.org/wiki?curid=55632" title="Linear combination">
Linear combination

In mathematics, a linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results (e.g. a linear combination of "x" and "y" would be any expression of the form "ax" + "by", where "a" and "b" are constants). The concept of linear combinations is central to linear algebra and related fields of mathematics.
Most of this article deals with linear combinations in the context of a vector space over a field, with some generalizations given at the end of the article.
Definition.
Suppose that "K" is a field (for example, the real numbers) and "V" is a vector space over "K". As usual, we call elements of "V" "vectors" and call elements of "K" "scalars".
If "v"1...,"v""n" are vectors and "a"1...,"a""n" are scalars, then the "linear combination of those vectors with those scalars as coefficients" is
There is some ambiguity in the use of the term "linear combination" as to whether it refers to the expression or to its value. In most cases the value is emphasized, like in the assertion "the set of all linear combinations of "v"1...,"v""n" always forms a subspace". However, one could also say "two different linear combinations can have the same value" in which case the expression must have been meant. The subtle difference between these uses is the essence of the notion of linear dependence: a family "F" of vectors is linearly independent precisely if any linear combination of the vectors in "F" (as value) is uniquely so (as expression). In any case, even when viewed as expressions, all that matters about a linear combination is the coefficient of each "v""i"; trivial modifications such as permuting the terms or adding terms with zero coefficient do not give distinct linear combinations.
In a given situation, "K" and "V" may be specified explicitly, or they may be obvious from context. In that case, we often speak of "a linear combination of the vectors" "v"1...,"v""n", with the coefficients unspecified (except that they must belong to "K"). Or, if "S" is a subset of "V", we may speak of "a linear combination of vectors in S", where both the coefficients and the vectors are unspecified, except that the vectors must belong to the set "S" (and the coefficients must belong to "K"). Finally, we may speak simply of "a linear combination", where nothing is specified (except that the vectors must belong to "V" and the coefficients must belong to "K"); in this case one is probably referring to the expression, since every vector in "V" is certainly the value of some linear combination.
Note that by definition, a linear combination involves only finitely many vectors (except as described in Generalizations below).
However, the set "S" that the vectors are taken from (if one is mentioned) can still be infinite; each individual linear combination will only involve finitely many vectors.
Also, there is no reason that "n" cannot be zero; in that case, we declare by convention that the result of the linear combination is the zero vector in "V".
Examples and counterexamples.
Euclidean vectors.
Let the field "K" be the set R of real numbers, and let the vector space "V" be the Euclidean space R3.
Consider the vectors "e"1 = (1,0,0), "e"2 = (0,1,0) and "e"3 = (0,0,1).
Then "any" vector in R3 is a linear combination of "e"1, "e"2 and "e"3.
To see that this is so, take an arbitrary vector ("a"1,"a"2,"a"3) in R3, and write:
Functions.
Let "K" be the set C of all complex numbers, and let "V" be the set CC("R") of all continuous functions from the real line R to the complex plane C.
Consider the vectors (functions) "f" and "g" defined by "f"("t") := "e""it" and "g"("t") := "e"−"it".
Some linear combinations of "f" and "g" are:
On the other hand, the constant function 3 is "not" a linear combination of "f" and "g". To see this, suppose that 3 could be written as a linear combination of "e""it" and "e"−"it". This means that there would exist complex scalars "a" and "b" such that "ae""it" + "be"−"it" = 3 for all real numbers "t". Setting "t" = 0 and "t" = π gives the equations "a" + "b" = 3 and "a" + "b" = −3, and clearly this cannot happen. See Euler's identity.
Polynomials.
Let "K" be R, C, or any field, and let "V" be the set "P" of all polynomials with coefficients taken from the field "K".
Consider the vectors (polynomials) "p"1 := 1, "p"2 := "x" + 1, and "p"3 := "x"2 + "x" + 1.
Is the polynomial "x"2 − 1 a linear combination of "p"1, "p"2, and "p"3?
To find out, consider an arbitrary linear combination of these vectors and try to see when it equals the desired vector "x"2 − 1.
Picking arbitrary coefficients "a"1, "a"2, and "a"3, we want
Multiplying the polynomials out, this means
and collecting like powers of "x", we get
Two polynomials are equal if and only if their corresponding coefficients are equal, so we can conclude
This system of linear equations can easily be solved.
First, the first equation simply says that "a"3 is 1.
Knowing that, we can solve the second equation for "a"2, which comes out to −1.
Finally, the last equation tells us that "a"1 is also −1.
Therefore, the only possible way to get a linear combination is with these coefficients.
Indeed,
so "x"2 − 1 "is" a linear combination of "p"1, "p"2, and "p"3.
On the other hand, what about the polynomial "x"3 − 1?
If we try to make this vector a linear combination of "p"1, "p"2, and "p"3, then following the same process as before, we’ll get the equation
However, when we set corresponding coefficients equal in this case, the equation for "x"3 is
which is always false.
Therefore, there is no way for this to work, and "x"3 − 1 is "not" a linear combination of "p"1, "p"2, and "p"3.
The linear span.
"Main article: linear span"
Take an arbitrary field "K", an arbitrary vector space "V", and let "v"1...,"v""n" be vectors (in "V").
It’s interesting to consider the set of "all" linear combinations of these vectors.
This set is called the "linear span" (or just "span") of the vectors, say S ={"v"1...,"v""n"}. We write the span of S as span(S) or sp(S):
Linear independence.
For some sets of vectors "v"1...,"v""n",
a single vector can be written in two different ways as a linear combination of them:
Equivalently, by subtracting these (formula_17) a non-trivial combination is zero:
If that is possible, then "v"1...,"v""n" are called "linearly dependent"; otherwise, they are "linearly independent".
Similarly, we can speak of linear dependence or independence of an arbitrary set "S" of vectors.
If "S" is linearly independent and the span of "S" equals "V", then "S" is a basis for "V".
Affine, conical, and convex combinations.
By restricting the coefficients used in linear combinations, one can define the related concepts of affine combination, conical combination, and convex combination, and the associated notions of sets closed under these operations.
Because these are more "restricted" operations, more subsets will be closed under them, so affine subsets, convex cones, and convex sets are "generalizations" of vector subspaces: a vector subspace is also an affine subspace, a convex cone, and a convex set, but a convex set need not be a vector subspace, affine, or a convex cone.
These concepts often arise when one can take certain linear combinations of objects, but not any: for example, probability distributions are closed under convex combination (they form a convex set), but not conical or affine combinations (or linear), and positive measures are closed under conical combination but not affine or linear – hence one defines signed measures as the linear closure.
Linear and affine combinations can be defined over any field (or ring), but conical and convex combination require a notion of "positive", and hence can only be defined over an ordered field (or ordered ring), generally the real numbers.
If one allows only scalar multiplication, not addition, one obtains a (not necessarily convex) cone; one often restricts the definition to only allowing multiplication by positive scalars.
All of these concepts are usually defined as subsets of an ambient vector space (except for affine spaces, which are also considered as "vector spaces forgetting the origin"), rather than being axiomatized independently.
Operad theory.
More abstractly, in the language of operad theory, one can consider vector spaces to be algebras over the operad formula_19 (the infinite direct sum, so only finitely many terms are non-zero; this corresponds to only taking finite sums), which parametrizes linear combinations: the vector formula_20 for instance corresponds to the linear combination formula_21. Similarly, one can consider affine combinations, conical combinations, and convex combinations to correspond to the sub-operads where the terms sum to 1, the terms are all non-negative, or both, respectively. Graphically, these are the infinite affine hyperplane, the infinite hyper-octant, and the infinite simplex. This formalizes what is meant by formula_22 being or the standard simplex being model spaces, and such observations as that every bounded convex polytope is the image of a simplex. Here suboperads correspond to more restricted operations and thus more general theories.
From this point of view, we can think of linear combinations as the most general sort of operation on a vector space – saying that a vector space is an algebra over the operad of linear combinations is precisely the statement that "all possible" algebraic operations in a vector space are linear combinations.
The basic operations of addition and scalar multiplication, together with the existence of an additive identity and additive inverses, cannot be combined in any more complicated way than the generic linear combination: the basic operations are a generating set for the operad of all linear combinations.
Ultimately, this fact lies at the heart of the usefulness of linear combinations in the study of vector spaces.
Generalizations.
If "V" is a topological vector space, then there may be a way to make sense of certain "infinite" linear combinations, using the topology of "V".
For example, we might be able to speak of "a"1"v"1 + "a"2"v"2 + "a"3"v"3 + ..., going on forever.
Such infinite linear combinations do not always make sense; we call them "convergent" when they do.
Allowing more linear combinations in this case can also lead to a different concept of span, linear independence, and basis.
The articles on the various flavours of topological vector spaces go into more detail about these.
If "K" is a commutative ring instead of a field, then everything that has been said above about linear combinations generalizes to this case without change.
The only difference is that we call spaces like this "V" modules instead of vector spaces.
If "K" is a noncommutative ring, then the concept still generalizes, with one caveat:
Since modules over noncommutative rings come in left and right versions, our linear combinations may also come in either of these versions, whatever is appropriate for the given module.
This is simply a matter of doing scalar multiplication on the correct side.
A more complicated twist comes when "V" is a bimodule over two rings, "K"L and "K"R.
In that case, the most general linear combination looks like
where "a"1...,"a""n" belong to "K"L, "b"1...,"b""n" belong to "K"R, and "v"1...,"v""n" belong to "V".

</doc>
<doc id="55633" url="https://en.wikipedia.org/wiki?curid=55633" title="Region">
Region

In geography, regions are areas broadly divided by physical characteristics (physical geography), human impact characteristics (human geography), and the interaction of humanity and the environment (environmental geography). Geographic regions and sub-regions are mostly described by their imprecisely defined, and sometimes transitory boundaries, except in human geography, where jurisdiction areas such as national borders are clearly defined in law.
Apart from the global continental regions, there are also hydrospheric and atmospheric regions that cover the oceans, and discrete climates above the land and water masses of the planet. The land and water global regions are divided into subregions geographically bounded by large geological features that influence large-scale ecologies, such as plains and features.
As a way of describing spatial areas, the concept of regions is important and widely used among the many branches of geography, each of which can describe areas in regional terms. For example, ecoregion is a term used in environmental geography, cultural region in cultural geography, bioregion in biogeography, and so on. The field of geography that studies regions themselves is called regional geography.
In the fields of physical geography, ecology, biogeography, zoogeography, and environmental geography, regions tend to be based on natural features such as ecosystems or biotopes, biomes, drainage basins, natural regions, mountain ranges, soil types. Where human geography is concerned, the regions and subregions are described by the discipline of ethnography.
A region has its own nature that could not be moved. The first nature is its natural environment (landform, climate, etc.). The second nature is its physical elements complex that were built by people in the past. The third nature is its socio-cultural context that could not be replaced by new immigrants.
Globalization.
Global regions distinguishable from space, and are therefore clearly distinguished by the two basic terrestrial environments, land and water. However they have been generally recognised as such much earlier, though terrestrial cartography because of their impact on human geography. They are divided into largest of land regions, known as continents, and the largest of water regions known as oceans. There are also significant regions that do not belong to either of these classifications, such as archipelago regions that are littoral regions, or earthquake regions that are defined in geology.
Continental regions.
Continental regions are usually based on broad experiences in human history and attempts to reduce very large areas to more manageable regionalisation for the purpose of study. As such they are conceptual constructs, usually lacking distinct boundaries. Oceanic division into maritime regions are used in conjunction with the relationship to the central area of the continent, using directions of the compass. Some continental regions are defined by the major continental feature of their identity, such as the Amazon basin, or the Sahara, which both occupy a significant percentage of their respective continental land area.
To a large extent, major continental regions are mental constructs created by considering an efficient way to define large areas of the continents. For the most part, the images of the World are derived as much from academic study s the media, or from personal experience of global exploration. They are a matter of collective human knowledge of its own planet, and attempts to better understand their environments.
Regional geography.
Regional geography is a branch of geography that studies regions of all sizes across the Earth. It has a prevailing descriptive character. The main aim is to understand or define the uniqueness or character of a particular region, which consists of natural as well as human elements. Attention is paid also to regionalization, which covers the proper techniques of space delimitation into regions.
Regional geography is also considered as a certain approach to study in geographical sciences (similar to quantitative or critical geographies, for more information see History of geography).
Geographical regions.
Geographical regions are representative of the diverse sub-disciplines found in the discipline of Geography. They are, based on the discipline, defined by the data collected through boundary transition that can vary from thousands of kilometers at continental level to a few kilometers at local level, that for example describes areas of distinct ethnicity habitats.
The United Nations Statistics Division has identified a scheme a systematic classification of macro-geographic regions (continents), and sub-continental subregions, and selected socioeconomic groupings.
Regions in physical geography.
Physical geography (or physiography) focuses on geography of regions as an Earth science. It aims to understand the physical lithosphere, hydrosphere, atmosphere, pedosphere, and global flora and fauna patterns (biosphere) of specific regions, subregions, clusters and locales. Physical regions are usually described by surface geological formations, hydrological and littoral surface features, discrete landscape features, and unique flora and fauna distribution that are not clearly delineated, and are separated by often wide transitional boundaries.
Palaeogeographic regions.
Palaeogeography is the study of ancient geologic environments. Since the physical structures of the Earth's surface have changed over geologic time, palaeogeographers have coined various names for ancient regions that no longer exist, from very large regions such as the supercontinents Rodinia, Pangaea, and Pannotia, to relatively small regions like Beringia. Other examples include the Tethys Ocean and Ancylus Lake. Palaeogeographic continental regions that include Laurentia, Proto-Laurasia, Laurasia, Euramerica (the "Old Red Continent"), and Gondwana.The Paleogeographic region is also where paleontologist find answers in history.
Regions in human geography.
Human geography is a branch of geography that focuses on the study of patterns and processes that shape human interaction with various discrete environments. It encompasses human, political, cultural, social, and economic aspects among others that are often clearly delineated. While the major focus of human geography is not the physical landscape of the Earth (see physical geography), it is hardly possible to discuss human geography without referring to the physical landscape on which human activities are being played out, and environmental geography is emerging as a link between the two. Regions of human geography can be divided into many broad categories, such as:
Historical regions.
The field of historical geography involves the study of human history as it relates to places and regions, or, inversely, the study of how places and regions have changed over time.
D. W. Meinig, a historical geographer of America, describes many historical regions in his book "The Shaping of America: A Geographical Perspective on 500 Years of History". For example, in identifying European "source regions" in early American colonization efforts, he defines and describes the "Northwest European Atlantic Protestant Region", which includes sub-regions such as the "Western Channel Community", which itself is made of sub-regions such as the "English West Country" of Cornwall, Devon, Somerset, and Dorset.
In describing historic regions of America, Meinig writes of "The Great Fishery" off the coast of Newfoundland and New England, an oceanic region that includes the Grand Banks. He rejects regions traditionally used in describing American history, like New France, "West Indies", the Middle Colonies, and the individual colonies themselves (Province of Maryland, for example). Instead he writes of "discrete colonization areas," which may be named after colonies, but rarely adhere strictly to political boundaries. Historic regions of this type Meinig writes about include "Greater New England" and its major sub-regions of "Plymouth," "New Haven shores" (including parts of Long Island), "Rhode Island" (or "Narragansett Bay"), "the Piscataqua," "Massachusetts Bay," "Connecticut Valley," and to a lesser degree, regions in the sphere of influence of Greater New England, "Acadia" (Nova Scotia), "Newfoundland and The Fishery/The Banks."
Other examples of historical regions include Iroquoia, Ohio Country, Illinois Country, and Rupert's Land.
Tourism region.
A tourism region is a geographical region that has been designated by a governmental organization or tourism bureau as having common cultural or environmental characteristics. These regions are often named after a geographical, former, or current administrative region or may have a name created for tourism purposes. The names often evoke certain positive qualities of the area and suggest a coherent tourism experience to visitors. Countries, states, provinces, and other administrative regions are often carved up into tourism regions to facilitate attracting visitors.
Some of the more famous tourism regions based on historical or current administrative regions include Tuscany in Italy and Yucatán in Mexico. Famous examples of regions created by a government or tourism bureau include the United Kingdom's Lake District and California's Wine Country.
great plains region
Natural resource regions.
Natural resources often occur in distinct regions. Natural resource regions can be a topic of physical geography or environmental geography, but also have a strong element of human geography and economic geography. A coal region, for example, is a physical or geomorphological region, but its development and exploitation can make it into an economic and a cultural region. Some examples of natural resource regions include the Rumaila Field, the oil field that lies along the border or Iraq and Kuwait and played a role in the Gulf War; the Coal Region of Pennsylvania, which is a historical region as well as a cultural, physical, and natural resource region; the South Wales Coalfield, which like Pennsylvania's coal region is a historical, cultural, and natural region; the Kuznetsk Basin, a similarly important coal mining region in Russia; Kryvbas, the economic and iron ore mining region of Ukraine; and the James Bay Project, a large region of Quebec where one of the largest hydroelectric systems in the world has been developed.
Religious regions.
Sometimes a region associated with a religion is given a name, like Christendom, a term with medieval and renaissance connotations of Christianity as a sort of social and political polity. The term Muslim world is sometimes used to refer to the region of the world where Islam is dominant. These broad terms are very vague when used to describe regions.
Within some religions there are clearly defined regions. The Roman Catholic Church, the Church of England, the Eastern Orthodox Church, and others, define ecclesiastical regions with names such as diocese, eparchy, ecclesiastical provinces, and parish.
For example, the United States is divided into 32 Roman Catholic ecclesiastical provinces. The Lutheran Church–Missouri Synod is organized into 33 geographic "districts", which are subdivided into "circuits" (the Atlantic District (LCMS), for example). The Church of Jesus Christ of Latter-day Saints uses regions similar to dioceses and parishes, but uses terms like ward and stake.
Political regions.
In the field of political geography regions tend to be based on political units such as sovereign states; subnational units such as provinces, counties, townships, territories, etc.; and multinational groupings, including formally defined units such as the European Union, the Association of Southeast Asian Nations, and NATO, as well as informally defined regions such as the Third World, Western Europe, and the Middle East.
Administrative regions.
The word "region" is taken from the Latin "regio" (derived from "regere", to rule), and a number of countries have borrowed the term as the formal name for a type of subnational entity (e.g., the "región", used in Chile). In English, the word is also used as the conventional translation for equivalent terms in other languages (e.g., the "область" ("oblast"), used in Russia alongside a broader term "регион").
The following countries use the term "region" (or its cognate) as the name of a type of subnational administrative unit:
The Canadian province of Québec also uses the "administrative region" ("région administrative").
Scotland had local government regions from 1975 to 1996.
In Spain the official name of the autonomous community of Murcia is "Región de Murcia". Also, some single-province autonomous communities such as Madrid use the term "región" interchangeably with "comunidad autónoma".
Two län (counties) in Sweden are officially called 'regions': Skåne and Västra Götaland, and there is currently a controversial proposal to divide the rest of Sweden into large regions, replacing the current counties.
The government of the Philippines uses the term "region" (in Filipino, "rehiyon") when it's necessary to group provinces, the primary administrative subdivision of the country. This is also the case in Brazil, which groups its primary administrative divisions ("estados"; "states") into "grandes regiões" (greater regions) for statistical purposes, while Russia uses "экономические районы" (economic regions) in a similar way, as does Romania and Venezuela.
The government of Singapore makes use of the term "region" for its own administrative purposes.
The following countries use an administrative subdivision conventionally referred to as a region in English:
China has five 自治区 ("zìzhìqū") and two 特別行政區 (or 特别行政区; "tèbiéxíngzhèngqū"), which are translated as "autonomous region" and "special administrative region", respectively.
Local administrative regions.
There are many relatively small regions based on local government agencies such as districts, agencies, or regions. In general, they are all regions in the general sense of being bounded spatial units. Examples include electoral districts such as Washington's 6th congressional district and Tennessee's 1st congressional district; school districts such as Granite School District and Los Angeles Unified School District; economic districts such as the Reedy Creek Improvement District; metropolitan areas such as the Seattle metropolitan area, and metropolitan districts such as the Metropolitan Water Reclamation District of Greater Chicago, the Las Vegas-Clark County Library District, the Metropolitan Police Service of Greater London, as well as other local districts like the York Rural Sanitary District, the Delaware River Port Authority, the Nassau County Soil and Water Conservation District, and C-TRAN.
Traditional or informal regions.
The traditional territorial divisions of some countries are also commonly rendered in English as "regions". These informal divisions do not form the basis of the modern administrative divisions of these countries, but still define and delimit local regional identity and sense of belonging. Examples include:
Functional regions.
Functional regions are usually understood to be the areas organised by the horizontal functional relations (flows, interactions) that are maximised within a region and minimised across its borders so that the principles of internal cohesiveness and external separation regarding spatial interactions are met (see, for instance, Farmer and Fotheringham, 2011; Klapka et al., 2013; Smart, 1974). A functional region is not an abstract spatial concept, but to a certain extent it can be regarded as a reflection of the spatial behaviour of individuals in a geographic space. 
The functional region is conceived as a general concept while its inner structure, inner spatial flows, and interactions need not necessarily show any regular pattern, only selfcontainment. The concept of self-containment remains the only crucial defining characteristic of a functional region. Nodal regions, functional urban regions, daily urban systems, local labour-market areas (LLMAs), or travel-to-work areas (TTWAs) are considered to be special instances of a general functional region that need to fulfil some specific conditions regarding, for instance, the character of the region-organising interaction or the presence of urban cores, (Halas et al., 2015).
Military regions.
In military usage, a region is shorthand for the name of a military formation larger than an Army Group and smaller than an Army Theater or simply Theater. The full name of the military formation is Army Region. The size of an Army Region can vary widely but is generally somewhere between about 1 million and 3 million soldiers. Two or more Army Regions could make up an Army Theater. An Army Region is typically commanded by a full General (US four stars), a Field Marshal, or General of the Army (US five stars), or Generalissimo (Soviet Union). Due to the large size of this formation, its use is rarely employed. Some of the very few examples of an Army Region are each of the Eastern, Western, and southern (mostly in Italy) fronts in Europe during World War II. The military map unit symbol for this echelon of formation (see Military organization and APP-6A) consists of six Xs.

</doc>
<doc id="55635" url="https://en.wikipedia.org/wiki?curid=55635" title="Region (Europe)">
Region (Europe)

The European Union created a Committee of the Regions to represent Regions of Europe as the layer of EU government administration directly below the nation-state level. The Committee has its headquarters in Brussels.
Reasons given for this include:
The term 'region' as used here includes England, Scotland, Wales and Northern Ireland which are non-sovereign countries, referred to as separate countries, even though collectively they form the country known as the United Kingdom they are recognised as countries by the UK Government and are not referred to as regions.
Some nation states which have historically had a strong centralized administration have transferred political power to the regions. Examples of this include the devolution of power in the UK (the Scotland Act 1998, the Government of Wales Act 1998) and the current negotiations in France concerning increased autonomy for Corsica. Some other states have traditionally had strong regions, such as the Federal Republic of Germany; yet others have been structured on the basis of national and municipal government with little in between.
Competence.
Regional and local authorities elect delegates to the Committee of the Regions. The Committee is a consultative body, and is asked for its opinion by the Council or the Commission on new policies and legislation in the following areas:
On certain issues it works in partnership with the Economic and Social Committee.
Political influence.
The politics of regionalism has also had an impact at the pan-European level. The regions of Europe had lobbied for an increased say in EU affairs, especially the German Länder. This resulted in the creation by the Maastricht Treaty of the Committee of the Regions, and provision for member states to be represented in the Council by ministers from their regional governments.
The Council of Europe also has a congress of local and regional authorities, similar to the EU's Committee of the Regions.
Strengthening economic competition between communities further supports the creation of authentic regions within the EU and almost all EU member states recently have or currently are re-organizing their administration to create competitive EU regions. 
Often these regions better reflect culture and identity and a sense of common interests.
Of the major organisations representing the regions of Europe, the Assembly of European Regions (AER) is the largest. Established in 1985, the organisation now brings together over 270 regions from 33 countries, along with 16 interregional associations, across wider Europe.
Apart from playing a key role as the regions' political voice on the European stage, AER is a forum for interregional cooperation in numerous areas of regional competence, including economic development, social policy, public health, culture, education and youth. The organisation is also a key defender of the subsidiarity principle in Europe, lobbying for its inclusion in the EU treaties and demanding recognition of the word in dictionaries via the worldwide "Subsidiarity is a word" movement.
Outside EU institutions, the Council of European Municipalities and Regions (CEMR-CCRE) is the largest organisation of local and regional government in Europe; its members are national associations of towns, municipalities and regions from over 35 countries. Together these associations represent some 100,000 local and regional authorities.
CEMR works to promote a united Europe that is based on local and regional self-government and democracy. To achieve this goal it endeavours to shape the future of Europe by enhancing local and regional contribution, to influence European law and policy, to exchange experience at local and regional level and to cooperate with partners in other parts of the world.

</doc>
<doc id="55639" url="https://en.wikipedia.org/wiki?curid=55639" title="Luton">
Luton

Luton ( , ) is a large town, borough and unitary authority area of Bedfordshire, England. Luton and its near neighbours, Dunstable and Houghton Regis, form the Luton/Dunstable Urban Area with a population of about 258,000. It is located east of Aylesbury, west of Stevenage, north-northwest of London, and southeast of Milton Keynes.
Luton is home to League Two team Luton Town Football Club, whose history includes several spells in the top flight of the English league as well as a Football League Cup triumph in 1988. They play at Kenilworth Road stadium, which has been their home since 1905.
London Luton Airport, opened in 1938, is one of England's major airports. During the Second World War it doubled as an RAF base.
The University of Bedfordshire is based in the town.
The Luton Carnival, which was traditionally been held on the Whitsun May bank holiday, is the largest one-day carnival in Europe. In 2012, it was moved to July to coincide with the Olympic Torch Relay and celebrations. Luton Carnival was transferred from Luton Borough Council to UK Centre for Carnival Arts in 2013, and since then has been held on the Bank Holiday Sunday instead in order to save the enhanced costs of operating on a bank holiday. 
The town was for many years famous for hat-making, and was also home to a large Vauxhall Motors factory; the head office of Vauxhall Motors is still situated in the town. Car production at the plant began in 1905 and continued until 2002, where commercial vehicle production remains.
History.
Early history.
The earliest settlements in the Luton area were at Round Green and Mixes Hill, where Paleolithic encampments (about 250,000 years old) have been found. Settlements re-appeared after the ice had retreated in the Mesolithic period around 8000 BC. Traces of these settlements have been found in the Leagrave area of the modern town. Remains from the Neolithic period (4500–2500 BC in this area) are much more common. A particular concentration of Neolithic burials occurs at Galley Hill. The most prominent Neolithic structure is Waulud's Bank – a henge dating from around 3000 BC. From the Neolithic onwards, the area seems to have been populated, but without any single large settlement.
The first urban settlement nearby was the small Roman town of "Durocobrivis" at Dunstable, but Roman remains in Luton itself consist only of scattered farmsteads.
The foundation of Luton is usually dated to the 6th century when a Saxon outpost was founded on the River Lea, Lea tun.
After the establishment of the Danelaw in the east of England and the unification of the remaining English kingdoms in the west, Luton stood on the border between Christendom and Heathenism which ran up the River Lea from London through to Bedford.
The Anglo Saxon Chronicle for the year 913 mentions Luton because locals fought off a Viking raiding band: "In this year the army from Northampton and Leicester rode out after Easter [28th March and broke the peace, and killed many men at Hook Norton and round about there. And then very soon after that, as the one force came home, they met another raiding band which rode out against Luton. And then the people of the district became aware of it and fought against them and reduced them to full flight and rescued all that they had captured and also a great part of their horses and their weapons".
Archaeological finds for this genesis of Lutonian history include 50 burials, 8 cremations, 16 spears, 22 knives (seax), a sword, 8 shield bosses, a pair of iron shears, a single bone comb, countless examples of brooches, pendants and other jewellery of bronze and amber and shards of pottery.
The Domesday Book records Luton as "Loitone" and also as "Lintone". Agriculture dominated the local economy at that time, and the town's population was around 700 to 800. But this number could represent a recently reduced population as a direct result of the Norman Invasion and the English resistance that followed. The Domesday Book records the value of King William's English possessions 20 years after his victory at Hastings, during which period, as the book would suggest, much destruction and death took place. Besides Luton, Biscot and Caddington also have entries in the Domesday Book for the surrounding area and in both these cases the value of the lands are much lower than their pre-invasion state, indicating a loss of households, livestock and crops.
In 1121 Robert, 1st Earl of Gloucester started work on St Mary's Church in the centre of the town. The work was completed by 1137. A motte-and-bailey castle which gives its name to the modern Castle Street was built in 1139. The castle was demolished in 1154 and the site is now home to a Matalan store. During the Middle Ages Luton is recorded as being home to six watermills. Mill Street, in the town centre, takes its name from one of them.
King John (1166–1216) had hired a mercenary soldier, Falkes de Breauté, to act on his behalf. (Breauté is a small town near Le Havre in France.) When he married, Falkes de Breauté acquired his wife's house which came to be known as "Fawkes Hall", subsequently corrupted over the years to "Foxhall", then to "Vauxhall". In return for his services, King John granted Falkes the manor of Luton, where he built a castle alongside St Mary's Church. He was also granted the right to bear his own coat of arms and chose the mythical griffin as his heraldic emblem. The griffin thus became associated with both Vauxhall and Luton in the early 13th century.
By 1240 the town is recorded as "Leueton". One "Simon of Luton" was Abbot of Bury St Edmunds from 1257 to 1279. The town had a market for surrounding villages in August each year, and with the growth of the town a second fair was granted each October from 1338.
In 1336 a large fire destroyed much of Luton; however, the town was soon rebuilt.
The agriculture base of the town changed in the 16th century with a brick-making industry developing around Luton; many of the older wooden houses were rebuilt in brick.
17th century.
During the English Civil War of the 17th century, in 1645, royalists entered the town and demanded money and goods. Parliamentary forces arrived and during the fighting four royalist soldiers were killed and a further twenty-two were captured. A second skirmish occurred three years later in 1648 when a royalist army passed through Luton. A number of royalists were attacked by parliamentary soldiers at an inn on the corner of the current Bridge Street. Most of the royalists escaped but nine were killed.
18th century.
The hat making industry began in the 17th century and became synonymous with the town. By the 18th century the industry dominated the town. Hats are still produced in the town but on a much smaller scale.
The first Luton Workhouse was constructed in the town in 1722.
Luton Hoo, a nearby large country house was built in 1767 and substantially rebuilt after a fire in 1843. It is now a luxury hotel.
19th century.
The town grew strongly in the 19th century. In 1801 the population was 3,095. By 1850 it was over 10,000 and by 1901 it was almost 39,000. Such rapid growth demanded a railway connection but the town had to wait a long time for one. The London and Birmingham Railway (L&BR) had been built through Tring in 1838, and the Great Northern Railway was built through Hitchin in 1852, both bypassing Luton, the largest town in the area. A branch line connecting with the L&BR at Leighton Buzzard was proposed, but because of objections to release of land, construction terminated at Dunstable in 1848. It was another ten years before the branch was extended to Bute Street Station, and the first train to Dunstable ran on 3 May 1858. The line was later extended to Welwyn and from 1860 direct trains to King's Cross ran. The Midland Railway was extended from Bedford to St Pancras through Leagrave and Midland Road station and opened on 9 September 1867.
Luton received a gas supply in 1834. Gas street lights were erected and the first town hall was opened in 1847.
Following a cholera epidemic in 1848 Luton formed a water company and had a complete water and sewerage system by the late 1860s. Newspaper printing arrived in the town in 1854. The first public cemetery was opened in the same year. The first covered market was built (the Plait Halls – now demolished) in 1869. Luton was made a borough in 1876. A professional football club – the first in the South of England – was founded in 1885 following a resolution at the town hall that a 'Luton Town Club be formed'.
The crest also includes a hand holding a bunch of wheat, either taken as a symbol of the straw-plaiting industry, or from the arms of John Whethamsteade, Abbott of St Albans, who rebuilt the chancel of St Mary's Church in the 15th century.
20th century.
In the 20th century, the hat trade severely declined and was replaced by other industries. In 1905, Vauxhall Motors opened the largest car plant in the United Kingdom in Luton. In 1914 Hewlett & Blondeau aviation entrepreneurs built a factory in Leagrave which began aircraft production built under licence for the war effort; the site was purchased in 1920 by new proprietors Electrolux domestic appliances, and this was followed by other light engineering businesses.
In 1901 the Bailey Water Tower was built on the edge of what was to become Luton Hoo memorial park. It is now a private residence.
In 1904 councillors Asher Hucklesby and Edwin Oakley purchased the estate at Wardown Park and donated it to the people of Luton. Hucklesby went on to become Mayor of Luton. The main house in the park became Wardown Park Museum.
The town had a tram system from 1908 until 1932, and the first cinema was opened in 1909. By 1914 the population had reached 50,000.
The original town hall was destroyed in 1919 during Peace Day celebrations at the end of the First World War. Local people, including many ex-servicemen, were unhappy with unemployment and had been refused the use of a local park to hold celebratory events. They stormed the town hall, setting it alight ("see Luton Town Hall"). A replacement building was completed in 1936. Luton Airport opened in 1938, owned and operated by the council.
In the Second World War, the Vauxhall Factory built Churchill tanks as part of the war effort. Despite heavy camouflage, the factory made Luton a target for the Luftwaffe and the town suffered a number of air raids. 107 died and there was extensive damage to the town (over 1,500 homes were damaged or destroyed). Other industry in the town, such as SKF, which produced ball bearings, made a vital contribution to the war effort. Although a bomb landed at the SKF Factory, no major damage was caused.
The pre-war years, even at the turn of the 1930s when a Great Depression saw unemployment reach record levels nationally, were something of an economic boom for Luton, as new industries grew and prospered. New private and council housing was built in the 1920s and 1930s, with Luton growing as a town to incorporate nearby villages Leagrave, Limbury and Stopsley between 1928 and 1933.
Post-war, the slum clearance continued, and a number of substantial estates of council housing were built, notably at Farley Hill, Stopsley, Limbury, Marsh Farm and Leagrave (Hockwell Ring). The M1 motorway passed just to the west of the town, opening in 1959 and giving it a direct motorway link with London and – eventually – the Midlands and the North. In 1962 a new library (to replace the cramped Carnegie Library) was opened by the Queen in the corner of St George's Square.
In the late 1960s a large part of the town centre was cleared to build a large covered shopping centre, the Arndale Centre, which was opened in 1972. It was refurbished and given a glass roof in the 1990s.
In 2000, Vauxhall announced the end of car production in Luton; the plant closed in March 2002. At its peak it had employed in excess of 30,000 people. Vauxhall's headquarters remain in the town, as does its van and light commercial vehicle factory.
21st century.
A major regeneration programme for the town centre is underway, which will include upgrades to the town's bus and railway stations as well as improvements to the town's urban environment. St George's Square has been rebuilt and reopened in 2007. The new design won a Gold Standard Award for the Town Centre Environment from the annual British Council of Shopping Centres awards.
Work was completed on an extension to the Mall Shopping Centre facing St George's Square, the largest of the new units to was taken by TK Maxx. Planning applications for a much larger extension to the Mall Arndale Shopping Centre (In the Northern Gateway area – Bute Street, Silver Street and Guildford Street) and also for a new centre in Power Court (close to St Mary's Church) have been submitted. On the edge of Luton at Putteridge Bury a high-technology office park, Butterfield Green, is under construction. The former Vauxhall site is also to be re-developed as a mixed use site called Napier Park. It will feature housing, retail and entertainment use, including a new casino.
Governance.
The town is situated within the historic county of Bedfordshire, but since 1997 Luton has been an administratively independent unitary authority. The town remains part of Bedfordshire for ceremonial purposes.
Luton Borough Council applied for city status at the Millennium in 2000, Golden Jubilee of Elizabeth II in 2002 and Diamond Jubilee in 2012. The latest bid was rejected in March 2012.
Parliamentary representation.
Luton is represented by two Members of Parliament. The constituency of Luton North has been held by Kelvin Hopkins (Labour) since 1997. Luton South has been held by Gavin Shuker (Labour) since 2010. Luton is within the East of England European Parliament constituency.
Police and crime commissioner.
Luton is served by the Bedfordshire police. The police commissioner is Olly Martins who lives in the High Town area of the town.
Local council.
Lutonians are governed by Luton Borough Council. The town is split into 19 wards, represented by 48 councillors. Elections are held for all seats every four years, with the most recent local elections held in May 2011 and the next due in May 2015. The Council is controlled by the Labour group, who have 36 Local Councillors (a majority of 24). The next largest party is the Liberal Democrats with 8 seats, followed by the Conservative Party with 4 seats.
Luton Council coat of arms.
In 1876 the town council was granted its own coat of arms. The wheatsheaf was used on the crest to represent agriculture and the supply of straw used in the local hatting industry (the straw-plaiting industry was brought to Luton by a group of Scots under the protection of Sir John Napier of Luton Hoo). The bee is traditionally the emblem of industry and the hive represents the straw-plaiting industry for which Luton was famous. The rose is from the arms of the Napier family, whereas the thistle is a symbol for Scotland. An alternative suggestion is that the rose was a national emblem, and the thistle represents the Marquess of Bute, who formerly owned the Manor of Luton Hoo.
Geography.
Luton is located in a break in the Eastern part of the Chiltern Hills. The Chilterns are a mixture of chalk from the Cretaceous period (about 66 – 145 million years ago) and deposits laid at the southernmost points of the ice sheet during the last ice age (the Warden Hills area can be seen from much of the town).
Bedfordshire had a reputation for brick making but the industry is now significantly reduced. The brickworks at Stopsley took advantage of the clay deposits in the east of the town.
The source of the River Lea, part of the Thames Valley drainage basin, is in the Leagrave area of the town. The Great Bramingham Wood surrounds this area. It is classified as ancient woodland; records mention the wood at least 400 years ago.
There are few routes through the hilly area for some miles, this has led to several major roads (including the M1 and the A6) and a major rail-link being constructed through the town.
Climate.
Luton has a temperate marine climate, like much of the British Isles, with generally light precipitation throughout the year. The weather is very changeable from day to day and the warming influence of the Gulf Stream makes the region mild for its latitude. The average total annual rainfall is with rain falling on 117 days of the year.
The local climate around Luton is differentiated somewhat from much of South East England due to its position in the Chiltern Hills, meaning it tends to be 1–2 degrees Celsius cooler than the surrounding towns – often flights at Luton airport, lying above sea level, will be suspended when marginal snow events occur, while airports at lower elevations, such as Heathrow, at above sea level, continue to function. An example of this is shown in the photograph to the right, the snowline being about above sea level. Absolute temperature extremes recorded at Rothamsted Research Station, south south east of Luton town centre and at a similar elevation range from in December 1981 and in January 1963 to in August 2003 and in August 1990 and July 2006. Records for Rothamsted date back to 1901.
Areas.
The Victorian expansion of Luton focused on areas close to the existing town centre and railways. In the 1920s and 1930s growth typically was though absorbing neighbouring villages and hamlets(an example being Leagrave) and infill construction between them and Luton. After the Second World War there were several estates and developments constructed both by the local council such as Farley Hill or Marsh Farm, or privately such as Bushmead.
"More about Places within Luton "
Demography.
The United Kingdom Census 2011 showed that the borough had a population of 203,201, a 10.2% increase from the previous census in 2001, when Luton was the 27th largest settlement in the United Kingdom. In 2011, 46,756 were aged under 16, 145,208 were 16 to 74, and 11,237 were 75 or over. The latest population figure for the borough is ().
Local inhabitants are known as "Lutonians".
Ethnicity.
Luton has seen several waves of immigration. In the early part of the 20th century Irish and Scottish people arrived in the town – these were followed by Afro-Caribbean and Asian immigrants. More recently immigrants from other European Union countries have made Luton their home. As a result of this Luton has a diverse ethnic mix, with a significant population of Asian descent, mainly Pakistani 29,353 (14.4%), Indian 10,625 (5.2%) and Bangladeshi 13,606 (6.7%).
Since the 2011 census, Luton has become one of three white British-minority towns in the United Kingdom. It was announced in a report based on the census figures that along with Leicester and Slough, Luton was one of three towns outside London where the white British were now a minority, making up only 45% of Luton's population. However, the town still has a white majority when non-British whites such as the Irish and Eastern Europeans are included, and 81% of the population of Luton still define themselves as British, despite the majority of its residents being from a foreign ethnic background.
Religion.
In the ten-year period since the United Kingdom Census 2001, the percentage of inhabitants in Luton reporting being Christian fell from 60 to 47%. Meanwhile, those reporting being Muslim increased from 15 to 25%.
Luton has been identified in the media as a home of people with extremist social and religious viewpoints. The Muslim group Al-Muhajiroun was based there before it was banned, and the founder of the English Defence League is from Luton. A Muslim protest in March 2009 against soldiers returning from the Iraq War was followed by a counter-demonstration opposing sharia law in the United Kingdom. However, many residents say that the numbers of extremists, both Muslims and far-right, are small. Inayat Bunglawala of the Muslim Council of Britain lives in Luton, and a local representative of Churches Together described "the reality of life in the town" as "a healthy interaction between people of different faiths".
Economic activity.
Of the town's working population (classified 16–74 years of age by the Office for National Statistics), 63% are employed. This figure includes students, the self-employed and those who are in part-time employment. 11% are retired, 8% look after the family or take care of the home and 5% are unemployed.
Economy.
Luton's economy has, traditionally been focused on several different areas of industry including Car Manufacture, engineering and millinery. However, today, Luton is moving towards a service based economy mainly in the retail and the airport sectors, although there is still a focus on light industry in the town.
Notable firms with headquarters in Luton include:
Notable firms with offices in Luton include:
Shopping.
The main shopping area in Luton is centred on the Mall Luton. Built in the 1960s/1970s and opened as an Arndale Centre, construction of the shopping centre led to the demolition of a number of the older buildings in the town centre including the Plait Halls (a Victorian covered market building with an iron and glass roof). Shops and businesses in the remaining streets, particularly in the roads around Cheapside and in High Town, have been in decline ever since. George Street, on the south side of the Arndale, was pedestrianised in the 1990s.
The shopping centre had some construction and re-design work done to it over the 2011/12 period and now has a new square used for leisure events, as well as numerous new food restaurants such as Toby's Carvery and Costa Coffee.
Contained within the main shopping centre is the market, which contains butchers, fishmongers, fruit and veg, hairdressers, tattoo parlours, ice cream, flower stall, T-shirt printing and the markets original sewing shop for clothes alterations and repairs as well as eating places.
Another major shopping area is Bury Park where there are shops catering to Luton's ethnic minorities.
Food and drink.
Luton has a diverse selection of restaurants – English, Italian, Chinese, Indian, Caribbean, Thai and Malaysian to name a few. No area of the town is specifically restaurant-orientated, but in some areas (such as Bury Park) there is a concentration of Asian restaurants.
There are pubs and clubs in the town centre. A number of these cater for the town's student population; however, there are still a number of traditional pubs in the town.
On 11 November 2015, it was announced that Spice King would be visiting Luton in February 2016 to sample and pass judgement on the local kebab selection.
Principal employers.
According to the Luton Borough Council, the principal employers in the town are:
Transport.
Luton is situated less than 30 miles north of the centre of London, giving it good links with the City and other parts of the country via the motorway network and the National Rail system. Luton is also home to London Luton Airport, one of the major feeder airports for London and the southeast. Luton is also served by bus services run by Arriva and Centrebus and a large taxi network. As a Unitary Authority, Luton Borough Council is responsible for the local highways and public transport in the Borough and licensing of Taxis.
Education.
Luton is one of the main locations of the University of Bedfordshire. A large campus of the university is in Luton town centre, with a smaller campus based on the edge of town in Putteridge Bury, an old Victorian manor house. The other main campus of the university is located in Bedford.
The town is home to Luton Sixth Form College and Barnfield College. Both have been awarded Learning & Skills Beacon Status by the Department for Children, Schools and Families.
Luton's schools and colleges had also been earmarked for major investment in the government scheme Building Schools for the Future programme, which intends to renew and refit buildings in institutes across the country. Luton is in the 3rd wave of this long term programme with work intending to start in 2009. Some schools were rebuilt before the programme was scrapped by the coalition government.
There are 98 educational institutes in Luton – seven nurseries, 70 primary schools (9 voluntary-aided, 2 Special Requirements), 13 secondary schools (1 voluntary-aided, 1 Special Requirements), four further educational institutes and four other educational institutes.
Culture and leisure.
Sport.
Luton is the home town of Luton Town Football Club who currently play in the Football League 2, Their nickname, "The Hatters", dates back to when Luton had a substantial millinery industry. The club began the 2008/09 season with a thirty-point deficit, and were consequently relegated from the Football League to the Conference Premier on 13 April 2009. However, Luton did win the Football League Trophy that year in front of 42,000 Luton fans at Wembley, despite being the lowest placed team in the competition for the whole season, Conference Premier after failing to win automatic promotion to Football League Two during the 2009–10, 2010–11 and 2011–12 seasons. Luton were beaten 2–0 on aggregate by York City in the semi finals of the playoffs, and therefore failed to progress to the final at Wembley Stadium. The following season Luton progressed to the final of the playoffs, losing to Wimbledon on penalties. In 2011–12 once again the team reached the final of the play-offs, only to lose 2–1 to York.
Luton were promoted back to the football league as champions of the Conference in 2014
Bedfordshire County Cricket Club is based at Wardown Park and is one of the county clubs which make up the Minor Counties in the English domestic cricket structure, representing the historic county of Bedfordshire and competing in the Minor Counties Championship and the MCCA Knockout Trophy.
Speedway racing was staged in Luton in the mid-1930s.
The town has three rugby union clubs – Stockwood Park Rugby Club who play in Midlands 3 SE, Luton Rugby Club who play in London 1 North, and Vauxhall Motors RFC who do not currently play in the RFU league structure.
Wardown Park.
Wardown Park is situated on the River Lea in Luton. The park has sporting facilities, is home to the Wardown Park Museum and contains formal gardens. The park is located between "Old Bedford Road" and the A6, "New Bedford Road" and is within walking distance of the town centre.
Stockwood Park.
Stockwood Park is a large municipal park near Junction 10 of the M1. Located in the park is Stockwood Discovery Centre a free museum that houses the Mossman Collection and Luton local social history, archaeology and geology. There is an athletics track, an 18-hole golf course, several rugby pitches and areas of open space.
The park was originally the estate and grounds to Stockwood house, which was demolished in 1964.
Carnival.
Luton International Carnival is the largest one-day carnival in Europe. It usually takes place on the late May Bank Holiday. Crowds can reach 150,000 on each occasion.
The procession starts at Wardown Park and makes its way down New Bedford Road, around the town centre via St George's Square, back down New Bedford Road and finishes back at Wardown Park. There are music stages and stalls around the town centre and at Wardown Park.
Luton is home to the UK Centre for Carnival Arts (UKCCA), the country's first purpose-built facility of its kind.
Due to budget cuts, the most recent carnival was run on a significantly smaller scale, with approximately one third of the typical attendance – most of the attendees were residents of the Luton area.
Luton St. Patrick's Festival.
The festival celebrating the patron saint of Ireland and organised by Luton Irish Forum, St Patrick, is held on the weekend nearest to 17 March. In its 15th year in 2014, the festival includes a parade, market stalls and music stands as well as Irish themed events.
Theatre.
Luton is home to the Library Theatre, a 238-seat theatre located on the 3rd floor of the town's Central Library. The theatre's programme consists of local amateur dramatic societies, pantomime, children's theatre (on Saturday mornings) and one night shows of touring theatre companies.
Luton is also home to the Hat Factory, originally as its name suggests, this arts centre was in fact a real hat factory. The Hat Factory is a combined arts venue in the centre of Luton. It opened in 2003 and since then has been the area’s main provider of contemporary theatre, dance and music. The venue provides live music, club nights, theatre, dance, films, children's activities, workshops, classes and gallery exhibitions.
Museums.
Luton Museum.
Wardown Park Museum previously known as Luton Museum and Art Gallery, is housed in a large Victorian mansion in Wardown Park on the outskirts of the town centre. The museum collection focusses on the traditional crafts and industry of Luton and Bedfordshire, notably lace-making and hat-making. There are samples of local lace from as early as the 17th century.
Stockwood Craft Museum.
Based in Stockwood Park, Luton, the collection of rural crafts and trades held at Stockwood Park Museum was amassed by Thomas Wyatt Bagshawe, who was a notable local historian and a leading authority on folk life. Bagshawe was born in Dunstable in 1901 and became a director of the family engineering firm.
The collection only contains examples from Bedfordshire and the borders of neighbouring counties, giving the collection a very strong regional identity.
Mossman Collection.
The Mossman Carriage collection is held at Stockwood Park, Luton and is the largest and most significant vehicle collection of its kind in the country, including originals from the 18th, 19th and 20th centuries.
The Mossman collection of horse-drawn vehicles was given to Luton Museum Service in 1991. It illustrates the development of horse-drawn road transport in Britain from Roman times up until the 1930s.
Twin towns.
Luton participates in international town twinning; its partners are:
Media.
Media references.
In the TV series "One Foot in the Grave" there are often references to places within Luton. The script-writer David Renwick was brought up in the town.
The town was mentioned several times in the seminal sketch show "Monty Python's Flying Circus". In one sketch a rather half-hearted hijacker demands that a plane headed for Cuba be diverted to Luton. Luton is one of the constituencies returning a "Silly Party" victory in the famous sketch "Election Night Special". In the Piranha Brothers sketch Spiny Norman lived in a hangar at Luton Airport. A 1976 episode of the sci-fi series ""Space: 1999"" was called The Rules of Luton, inspired by the town name. The well known comedian Eric Morecambe frequently made references to Luton Town FC, due to him being a former chairman of the club, as well as living in close proximity to Luton in Harpenden.
Lutonians.
People who were born in Luton or are associated with the town.

</doc>
<doc id="55641" url="https://en.wikipedia.org/wiki?curid=55641" title="Pope Sabinian">
Pope Sabinian

Pope Sabinian (, died 22 February 606) was Pope from 13 September 604 to his death in 606. Pope during the Byzantine Papacy, he was fourth former "apocrisiarius" to Constantinople elected pope.
Sabinian was born at Blera (Bieda) near Viterbo. He had been sent by Pope Gregory I as Apostolic "nuncio", to Constantinople, but he apparently was not entirely satisfactory in that office. He returned to Rome in 597.
He was probably consecrated pope on 13 September 604. He incurred unpopularity by his unseasonable economies, although the "Liber Pontificalis" states that he distributed grain during a famine at Rome under his pontificate. The erudite Italian Augustinian Onofrio Panvinio (1529–1568), in his "Epitome pontificum Romanorum" (Venice, 1557), attributes to him the introduction of the custom of ringing bells at the canonical hours and the celebration of the Eucharist. The first attribution was this was in Guillaume Durand's thirteenth-century "Rationale Divinorum Officiorum".
During his reign, Sabinian was seen as a counterfoil to his predecessor Pope Gregory I. Whereas Gregory distributed grain to the Roman populace as invasion loomed, Sabinian sold it for high prices (though this may be a later interpolation by Gregory's biographers). The "Liber Pontificalis" praises him for "filling the church with clergy," in contrast to Gregory, who rose rapidly from simple monk to bishop of Rome.

</doc>
<doc id="55649" url="https://en.wikipedia.org/wiki?curid=55649" title="1320s BC">
1320s BC


</doc>
<doc id="55654" url="https://en.wikipedia.org/wiki?curid=55654" title="Sun Quan">
Sun Quan

Sun Quan (182–252), courtesy name Zhongmou, formally known as Emperor Da of Wu (lit. "Great Emperor of Wu"), was the founder of the state of Eastern Wu during the Three Kingdoms period. He inherited control of the warlord state founded by his elder brother Sun Ce in 200. He declared formal independence and ruled from 222 to 229 as "King of Wu" and from 229 to 252 as the "Emperor of Wu". Unlike his rivals Cao Cao and Liu Bei, Quan governed his empire mostly separate of politics and ideology, he is sometimes portrayed as neutral considering he accommodated the wills of both his rivals but only when it benefited his state of Wu and never fully attempted to conquer his rivals, although most of historians would cite his lack of logistical resources to do so.
Sun Quan was born in Xiapi while his father Sun Jian served there. After Sun Jian's death in the early 190s, he and his family lived at various cities on the lower Yangtze River, until his older brother Sun Ce carved out a warlord state in the region of present-day Zhejiang, based on his own followers and a number of local clan allegiances. When Sun Ce was assassinated by the retainers of Xu Gong in 200, the eighteen-year-old Sun Quan inherited the lands southeast of the Yangtze River from his brother. His administration proved to be relatively stable in those early years as Sun Jian and Sun Ce's most senior officers, such as Zhou Yu, Zhang Zhao, Zhang Hong, and Cheng Pu supported the succession. Thus throughout the 200s, Sun Quan, under the tutelage of his able advisers, continued to build up his strength along the Yangtze River. In early 207, his forces finally won complete victory over Huang Zu, a military leader under Liu Biao, who dominated the middle Yangtze.
In winter of that year, the northern warlord Cao Cao led an army of some 830,000 to conquer south to complete the reunification of China. Two distinct factions emerged at his court on how to handle the situation. One, led by Zhang Zhao, urged surrender whilst the other, led by Zhou Yu and Lu Su, opposed capitulation. In the finality, Sun Quan decided to oppose Cao Cao in the middle Yangtze with his superior riverine forces. Allied with Liu Bei and employing the combined strategies of Zhou Yu and Huang Gai, they defeated Cao Cao decisively at the Battle of Red Cliffs.
In 220, Cao Pi, son of Cao Cao, seized the throne and proclaimed himself to be the Emperor of China, ending and succeeding the nominal rule of the Han dynasty. At first Sun Quan nominally served as a Wei vassal with the Wei-created title of King of Wu, but after Cao Pi demanded that he send his son Sun Deng as a hostage to the Wei capital Luoyang and he refused, in 222, he declared himself independent by changing his era name. It was not until the year 229 that he formally declared himself emperor.
Because of his skill in gathering important, honourable men to his cause, Sun Quan was able to delegate authority to capable figures. This primary strength served him well in gaining the support of the common people and surrounding himself with capable generals.
After the death of his original crown prince, Sun Deng, two opposing factions supporting different potential successors slowly emerged. When Sun He succeeded Sun Deng as the new crown prince, he was supported by Lu Xun and Zhuge Ke, while his rival Sun Ba was supported by Quan Cong and Bu Zhi and their clans. Over a prolonged internal power struggle, numerous officials were executed, and Sun Quan harshly settled the conflict between the two factions by exiling Sun He and forcing Sun Ba to commit suicide. Sun Quan died in 252 at the age of 70. He enjoyed the longest reign among all the founders of the Three Kingdoms and was succeeded by his son Sun Liang.
Early life.
Sun Quan was born in 182, while his father Sun Jian was still a general of the Han dynasty. After his father's death in 191, he became the charge of his brother Sun Ce. As he grew up, he served his brother during the conquests of the region south of the Yangtze River. He was made a county magistrate in 196, at the age of 14, and continued to rise through the ranks as his brother gave him more and more important tasks.
The "Records of the Three Kingdoms" mentioned that Sun Jian was a descendant of Sun Wu (better known as Sun Tzu), a militarist in the Spring and Autumn Period and the author of "The Art of War". According to later tradition, Sun Quan was born on Sunzhou ("Sun Island", later Wangzhou - "King's Island"), an islet at the intersection of the Fuchun River and one of its tributaries. Local folklore relates a story about how Sun Quan's grandfather, Sun Zhong, was originally a melon farmer on the islet.
Succeeding Sun Ce.
Sun Ce was assassinated in 200 during a hunt. On his deathbed, he knew that his son was still too young to be considered a realistic heir, so he entrusted the 18-year-old Sun Quan to his faithful subordinates. Initially, Sun Quan mourned his brother's death so much that he could do nothing, but at Zhang Zhao's behest, he dressed himself in military uniform and set out to visit the commanderies under his brother's control. Many of Sun Ce's subordinates thought that Sun Quan was too young to sustain Sun Ce's domain and wanted to leave, but Zhang Zhao and Zhou Yu saw special qualities in the young man and chose to stay to serve Sun Quan. Zhang Hong, whom Sun Ce had earlier sent as a liaison to the warlord Cao Cao, also returned from Cao's domain to assist Sun Quan. (At Zhang Hong's request, Cao Cao, in the name of Emperor Xian, commissioned Sun Quan as General Who Attacks Barbarians (討虜將軍), a title that he would be known for a long time.) He listened carefully to his mother Lady Wu's encouraging words, and greatly trusted Zhang Zhao and Zhang Hong with regard to civilian affairs and Zhou Yu, Cheng Pu, and Lü Fan with regard to military matters. Sun Quan also sought out talented young men to serve as his personal advisors, and it was around this time that he befriended Lu Su and Zhuge Jin, who would later play prominent roles in his administration. Throughout this period and decades to come, Sun Quan's leadership would be characterized by his ability to find men of character and entrust important matters to him, and his ability to react swiftly to events.
For the next several years, Sun Quan was largely interested in first defending his realm against potential enemies, but he gradually sought to harass and weaken Liu Biao's key subordinate, Huang Zu (who controlled the northeastern region of Liu Biao's domain) -- particularly because Huang Zu had killed his father in battle. In 208, he was finally able to defeat and kill Huang Zu in battle. Soon after, Liu Biao died while Cao Cao was preparing a major campaign to subjugate both Liu Biao and Sun Quan under his control, precipitating a major confrontation.
Battle of Red Cliffs.
After Liu Biao's death, a succession struggle for his domain came into being, between his sons Liu Qi and younger son Liu Cong, whom Liu Biao's second wife Lady Cai favored (because he had married her niece). After Huang Zu's death, Liu Qi was therefore given Huang's post as the governor of Jiangxia Commandery (in present-day Huanggang, Hubei). Liu Cong therefore succeeded Liu Biao after his death, and Liu Qi was displeased and considered, but did not carry out, an attack against his brother. Nevertheless, Liu Cong, in fear of having to fight Cao Cao and his brother on two fronts, surrendered to Cao Cao against the advice of Liu Biao's key ally Liu Bei. Liu Bei, unwilling to submit to Cao Cao, fled south. Cao caught up to him and crushed his forces, but Liu Bei escaped with his life; he fled to Dangyang (當陽, in present-day Yichang, Hubei). Cao Cao took over most of Jing Province, and appeared set on finally unifying the empire.
Sun Quan was well aware of Cao Cao's intentions, and he quickly entered into an alliance with Liu Bei and Liu Qi to prepare for an attack by Cao. Cao Cao wrote Sun Quan with a letter intending to intimidate, and in face of Cao's overwhelming force (estimated to be about 220,000 men, although Cao claimed 800,000, against Sun's 30,000 and the Lius' combined force of 10,000), many of Sun's subordinates, including Zhang Zhao, advocated surrender. Sun Quan refused, under advice from Zhou Yu and Lu Su (that Cao Cao would surely not tolerate him even if he surrendered).
Sun Quan put Zhou Yu in charge of his 30,000 men, largely stationed on naval ships, and Zhou set up in a defense position in conjunction with Liu Bei, whose army was stationed on land. About this time, there was a plague developing in Cao Cao's forces which significantly weakened it. Zhou Yu set up a trap where he pretended to be punishing his subordinate Huang Gai, and Huang pretended to surrender to Cao Cao in fear. Zhou Yu then sent ships under Huang Gai's command to pretend to surrender and, as Huang's ships approached Cao Cao's fleet, they were set aflame to assault Cao's fleet, and Cao's fleet was largely destroyed by fire. Cao Cao led his forces to escape on land, but much of the force was destroyed by Sun Quan and Liu Bei's land forces.
Uneasy alliance with Liu Bei.
Immediately, after Cao Cao withdrew, Sun Quan took over the northern half of Jing Province. Liu Bei marched south and took over the southern half. The Sun-Liu alliance was further cemented by a marriage of Sun Quan's younger sister, Lady Sun, to Liu Bei. Zhou Yu was suspicious of Liu Bei's intentions, however, and suggested to Sun Quan that Liu be seized and put under house arrest (albeit be very well-treated) and his forces be merged into Sun's; Sun Quan, believing that Liu Bei's forces would rebel if he did that, declined. Sun Quan did agree to Zhou Yu's plans to consider attacking Liu Zhang and Zhang Lu (who controlled the modern southern Shaanxi) to try to take over their territories, but after Zhou Yu died in 210, the plans were abandoned. However, Sun Quan was able to persuade the warlords in present-day Guangdong, Guangxi, and northern Vietnam to submit to him, and they became part of his domain. He then yielded northern Jing Province to Liu Bei as well, agreeing with Liu that the south was insufficient to supply his troops.
After Liu Bei's conquest of Yi Province, he was able to supply his troops on his own, so Sun Quan sent Lu Su as an emissary to demand for the return of Jing Province, but Liu Bei refused. Sun Quan then sent Lü Meng and Ling Tong to lead 20,000 men to attack southern Jing Province and they succeeded in capturing Changsha, Guiyang, and Lingling commanderies. Meantime, Lu Su and Gan Ning advanced to Yiyang (益陽) with 10,000 men (to block Guan Yu) and took over command of the army at Lukou (陸口). Liu Bei personally went to Gong'an and Guan Yu led 30,000 men to Yiyang. When an all-out war was about to break out, the news that Cao Cao planned to attack Hanzhong was received by Liu Bei, and he requested for a border treaty with Sun Quan as he became worried about Cao Cao seizing Hanzhong. Liu Bei asked Sun Quan to give him back Lingling commandery and create a diversion for Cao Cao by attacking Hefei; in return, Liu Bei ceded Changsha and Guiyang commanderies to Sun Quan, setting the new border along the Xiang River. Sun Quan's attack on Hefei was disastrous - he was nearly captured on a few occasions, if not saved by Ling Tong.
Breaking of alliance with Liu Bei.
In 219, Guan Yu advanced north, attacking Fancheng, scoring a major victory over Cao Ren. While Fancheng did not fall at this time, Guan Yu put it under siege, and the situation was severe enough that Cao Cao considered moving the capital away from Xu. However, Sun Quan, resentful of Guan Yu's prior constant instigation of hostilities (including seizing Sun's food supplies to use for his campaign north), took the opportunity to attack Guan from the rear, and Guan's forces collapsed. Guan Yu was captured by forces under general Lü Meng; Guan Yu was executed, Jing Province came under Sun's control, and the Sun-Liu alliance ended. Sun Quan nominally submitted to Cao Cao and urged him to take the throne but Cao refused.
After Cao Cao's death in 220, Cao Pi forced Emperor Xian to yield the throne to him, ending the Han dynasty and establishing the state of Cao Wei. Sun Quan did not immediately submit to Wei or declare independence after Cao Pi's enthronement, but took a wait-and-see attitude; by contrast, in early 221, Liu Bei declared himself emperor, establishing the state of Shu Han. Immediately, Liu Bei planned a campaign against Sun Quan to avenge Guan Yu. After attempting to negotiate peace and receiving no positive response from Liu Bei, fearing attack on both sides, Sun Quan became a vassal of Wei. Cao Pi's strategist Liu Ye suggested that Cao Pi decline — and in fact attack Sun Quan on a second front, effectively partitioning Sun's domain with Shu, and then eventually seek to destroy Shu as well. Cao Pi declined, in a fateful choice that most historians believe doomed his empire to ruling only the northern and central China — and this chance would not come again. Indeed, against Liu Ye's advice, he appointed Sun Quan the King of Wu and granted him the nine bestowments.
In 222, at the Battle of Xiaoting, Sun Quan's general Lu Xun dealt Liu Bei a major defeat, stopping the Shu offensive. Shu would not again pose a threat to Sun Quan from that point on. Later that year, when Cao Pi demanded that Sun Quan send his crown prince Sun Deng to the Wei capital Luoyang as a hostage (to guarantee his loyalty), Sun Quan refused and declared independence (by changing era name), thus establishing Eastern Wu as an independent state. Cao Pi launched a major attack on Wu, but after Wei defeats in early 223, it became clear that Wu was secure. After Liu Bei's death later that year, Zhuge Jin's brother Zhuge Liang, the regent for Liu Bei's son and successor Liu Shan, reestablished the alliance with Sun Quan, and the two states would remain allies until Shu's eventual destruction in 263.
Reign as the monarch of Eastern Wu.
Early reign.
Early in Sun Quan's reign, the Wu administration was known for its efficiency, as Sun showed a knack for listening to correct advice and for delegating authorities to the proper individuals. For example, he correctly trusted the faithful Lu Xun and Zhuge Jin, so much so that he made a duplicate imperial seal and left it with Lu Xun; whenever he would correspond with Shu's emperor Liu Shan or regent Zhuge Liang, he would deliver the letter to Lu Xun first (as Lu's post was near the Shu border), and then if, in Lu's opinion, changes were needed, he would revise the letter and then restamp it with Sun's imperial seal. Further, Lu Xun and Zhuge Jin were authorized to coordinate their actions with Shu without prior imperial approval. Sun Quan treated his high-level officials as friends and addressed them accordingly (with courtesy names), and in accordance they dedicated all effort to Wu's preservation. He also knew what were the proper roles for officials that he trusted; for example, in 225, when selecting a chancellor, while the key officials all respected Zhang Zhao greatly and wanted him to be chancellor, Sun Quan declined, reasoning that while he respected Zhang greatly, a chancellor needed to handle all affairs of state, and Zhang, while capable, had such strong opinions that he would surely be in conflict with Sun Quan and other officials at all times. He also repeatedly promoted his official Lü Fan even though, while he was young, Lü Fan had informed to Sun Ce about his improper spending habits, understanding that Lü did so only out of loyalty to Sun Ce.
In 224 and 225, Cao Pi again made attacks on Wu, but each time the Wu forces were able to repel Wei's with fair ease — so easily that Cao Pi made the comment, "Heaven created the Yangtze to divide the north and south." However, Sun Quan was himself equally unsuccessful in efforts to make major attacks on Wei. After Cao Pi's death in 226, for example, Sun Quan launched an attack on Wei's Jiangxia Commandery (in present-day Xiaogan, Hubei) but was forced to withdraw as soon as Wei reinforcements arrived. However, later that year, he was able to increase his effective control over Jiao Province (交州, present-day northern Vietnam) when his general Lü Dai was able to defeat the warlord Shi Hui (士徽) and end the effective independence that the Shi clan had. In addition, the several independent kingdoms in modern Cambodia, Laos, and southern Vietnam all became Wu vassals as well.
The one major victory that Wu would have over Wei during this period came in 228, when, with Sun Quan's approval, his general Zhou Fang pretended to be surrendering to Wei after pretending to have been punished repeatedly by Sun Quan. This tricked the Wei general Cao Xiu, who led a large army south to support Zhou Fang. He walked into the trap set by Zhou Fang and Lu Xun and suffered major losses, but was saved from total annihilation by Jia Kui.
In 229, Sun Quan declared himself emperor, which almost damaged the alliance with Shu, as many Shu officials saw this as a sign of betrayal of the Han dynasty — to which Shu claimed to be the legitimate successor. However, Zhuge Liang opposed ending the alliance and in fact confirmed it with a formal treaty later that year, in which the two states pledged to support each other and divide Wei equally if they could conquer it. Later that year, he moved his capital from Wuchang (武昌, in present-day Ezhou, Hubei) to Jianye, leaving his crown prince Sun Deng, assisted by Lu Xun, in charge of the western empire.
Middle reign.
In 230, however, the first sign of the deterioration of Sun Quan's reign occurred. That year, he sent his generals Wei Wen (衛溫) and Zhuge Zhi (諸葛直) with a navy of 10,000 into the East China Sea to seek the legendary islands of Yizhou (夷洲) and Danzhou (亶洲) to seek to conquer them, despite strenuous opposition of Lu Xun and Quan Cong. The navy was not able to locate Danzhou but located Yizhou, and returned in 231 after capturing several thousand men — but only after 80-90% of the navy had died from illness. Instead of seeing his own fault in this venture, Sun Quan simply executed Wei Wen and Zhuge Zhi. Perhaps concerned about this deterioration in Sun Quan's judgment, Sun Deng left the western empire in Lu Xun's hands in 232 and returned to Jianye, and would remain at Jianye until his own death in 241.
In 232, Sun Quan had another misadventure involving his navy — as he sent his generals Zhou He (周賀) and Pei Qian (裴濳) to the nominal Wei vassal Gongsun Yuan, in control of Liaodong Commandery (present-day central Liaoning), to purchase horses, against the advice of Yu Fan - and indeed, he exiled Yu Fan to the desolate Cangwu Commandery (roughly modern Wuzhou, Guangxi) as punishment. Just as Yu Fan predicted, however, the venture would end in failure — as Zhou He and Pei Qian, on their way back, were intercepted by Wei forces and killed. Regretting his actions, Sun Quan tried to recall Yu Fan back to Jianye, only to learn that Yu had died in exile.
The next year, however, Sun Quan would have yet another misadventure in his dealings with Gongsun Yuan, as Gongsun sent messengers to him, offering to be his subject. Sun Quan was ecstatic, and appointed Gongsun Yuan the Prince of Yan and granted him the nine bestowments, and further sent a detachment of 10,000 men by sea north to assist Gongsun Yuan in his campaign against Wei, against the advice of nearly every single one of his high-level officials, particularly Zhang Zhao. Once the army arrived, however, Gongsun Yuan betrayed them, killing Sun Quan's officials Zhang Mi (張彌) and Xu Yan (許晏), whom Sun had sent to grant the bestowments and seized their troops. Once that happened, the enraged Sun Quan wanted to personally head north with a fleet to attack Gongsun Yuan, and initially, not even Lu Xun's opposition was able to stop him, although he eventually calmed down and did not follow through. To his credit, he also personally went to Zhang Zhao's house and apologized to him. Further, despite the deterioration in his previous clear thinking, he was still capable of making proper decisions at times. For example, in 235, when, as a sign of contempt, Wei's emperor Cao Rui offered horses to him in exchange for pearls, jade, and tortoise shells, Sun Quan ignored the implicit insult and made the exchange, reasoning that his empire needed horses much more than pearls, jade, or tortoise shells.
In 234, in coordination with Zhuge Liang's final northern expedition against Wei, Sun Quan personally led a major attack against Wei's border city Hefei, while having Lu Xun and Zhuge Jin attack Xiangyang, with the strategy of trying to attract Wei relief forces and then attacking them. However, Wei generals correctly saw the situation and simply let Sun Quan siege Hefei. Only after Sun Quan's food supplies ran low did Cao Rui personally arrive with reinforcements, and Sun withdrew, as did Lu Xun and Zhuge Jin.
In 238, when Gongsun Yuan was under attack by Wei's general Sima Yi, Sun Quan, despite his prior rage against Gongsun, correctly judged the situation as one where he might be able to take advantage if Sima Yi were initially unsuccessful, so he did not immediately refuse Gongsun's request for help. However, as Sima Yi was able to conquer Gongsun Yuan quickly, Sun Quan never launched the major attack that he considered if Sima got stuck in a stalemate with Gongsun. That year, he also recognized how his head secretary Lü Yi (呂壹) had been falsely accusing his officials, and had Lü executed; he then further confirmed his trust in the high-level officials by personally writing an emotional letter to Zhuge Jin, Bu Zhi, Zhu Ran, and Lü Dai, blaming himself for the recent problems with his administration while urging them to speak out honestly whenever they saw faults in him.
In 241, Sun Quan would launch the last major assault against Wei of his reign, in light of Cao Rui's death in 239, but he rejected a strategy offered by Yin Zha (殷札) to attack Wei in coordinated effort with Shu on four different fronts, and the campaign ended in failure as well.
Late reign.
Later in 241, the crown prince Sun Deng died — an event that left open the issue of succession and appeared to mark the start of a precipitous decline in Sun Quan's mental health. In 242, he appointed his son Sun He, born to Consort Wang, crown prince. However, he also favored another son by Consort Wang, Sun Ba (孫霸) the Prince of Lu, and permitted Sun Ba to have the same staffing level as the crown prince — a move that was objected to by a number of officials as encouraging Sun Ba to compete with Sun He, but Sun Quan did not listen to them. After 245, when Sun He and Sun Ba began to have separate residences, their relationship detriorated further, and Sun Ba began to scheme at how to seize heir status from Sun He. Fanned by gossip from his daughter Sun Dahu (孫大虎), Sun Quan blamed the princes' mother Consort Wang for this — and she died in fear. He also cut off Sun He and Sun Ba's access to the officials who supported them in hopes of receiving future favors, but this could not stop Sun Ba's machinations. Indeed, when Lu Xun tried to intervene to protect Sun He, Sun Ba falsely accused him of many crimes, and Sun Quan became provoked so much that he repeatedly rebuked Lu, causing Lu to die in anger.
In 250, fed up with Sun Ba's constant attacks against Sun He, Sun Quan carried out an inexplicable combination of actions, He forced Sun Ba to commit suicide, while deposing Sun He (who had not been shown to have committed any crimes), and instead creating his youngest son, Sun Liang, crown prince to replace Sun He. This move was opposed by his son-in-law Zhu Ju (the husband of Sun Xiaohu), but Zhu's pleas not only did not help Sun He, but also resulted in his own death, as Sun Quan forced him to commit suicide. Many other officials who also opposed the move, as well as officials who had supported Sun Ba, were executed.
Around this time, Sun Quan also had his generals destroy a number of levees near the border with Wei, creating large areas of flooding, in order to obstruct potential attacks from Wei.
In 251, Sun Quan created the first empress of his reign — Sun Liang's mother Consort Pan. (Previously, he had a succession of wives, but never made any of them empress, except for his favorite, Lady Bu, who was created empress posthumously after her death in 238.) Later that year, however, he realized that Sun He was blameless and wanted to recall him from his exile, but was persuaded not to do so by his daughter Sun Dahu and Sun Jun, who had supported Sun Liang's ascension. He realized that he was getting very old (69 by this point) and, at Sun Jun's recommendation, commissioned Zhuge Jin's son Zhuge Ke as the future regent for Sun Liang, even though he correctly had misgivings about how Zhuge Ke was arrogant and had overly high opinion of his own abilities. At that time virtually the entire empire, awed by Zhuge's prior military victories, was convinced that Zhuge would be the correct choice for regent.
In 252, as Sun Quan neared death, Empress Pan was murdered, but how she was murdered remains a controversy. Wu officials claimed that her servants, unable to stand her temper, strangled her while she was asleep, while a number of historians, including Hu Sanxing, the commentator to Sima Guang's "Zizhi Tongjian", believed that top Wu officials were complicit, as they feared that she would seize power as empress dowager after Sun Quan's death. Later that year, Sun Quan died at the age of 70, and Sun Liang succeeded him. Sun Quan was buried in a mausoleum at Purple Mountain in present-day Nanjing.
Modern references.
Sun Quan appears as a playable character in Koei's "Dynasty Warriors" and "Warriors Orochi" video game series.
Sun Quan is portrayed by Chang Chen in John Woo's 2008 film "Red Cliff".
In the collectible card game "" there is a card named "Sun Quan, Lord of Wu", in the "Portal Three Kingdoms" set.
In the selection of hero cards in the Chinese card game "San Guo Sha" (三国杀), there is also a Sun Quan hero that players can select at the beginning of the game.
In the movie "The Weird Man" by the Shaw Brothers Studio, Sun Quan is shown at the end of the film and Sun Ce names him successor before he died from his injuries sustained by Xu Gong and Yu Ji's spirit.
Sun Quan also appears in the mobile video game Puzzle & Dragons as part of the Three Kingdoms Gods series.

</doc>
<doc id="55655" url="https://en.wikipedia.org/wiki?curid=55655" title="Tenure of Office Act (1867)">
Tenure of Office Act (1867)

The Tenure of Office Act was a United States federal law (in force from 1867 to 1887) that was intended to restrict the power of the President of the United States to remove certain office-holders without the approval of the Senate. The law was enacted on March 3, 1867, over the veto of President Andrew Johnson. It purported to deny the president the power to remove any executive officer who had been appointed by the president with the advice and consent of the Senate, unless the Senate approved the removal during the next full session of Congress. The act was significantly amended on April 5, 1869. Congress repealed the act in its entirety in 1887.
Background.
In the post-Civil War political environment, President Andrew Johnson endorsed the quick re-admission of the Southern secessionist states. The two-thirds Republican majorities of both houses of Congress, however, passed laws over Johnson's vetoes, establishing a series of five military districts overseeing newly created state governments. This "Congressional Reconstruction" was designed to create local civil rights laws to protect newly freed slaves; to protect and patrol the area; to ensure the secessionist states would show some good faith before being readmitted; to ensure Republican control of the states; and, arguably, to inflict some punishment on the secessionists. States would be readmitted gradually.
Overpowered politically, the sole check Johnson could apply to the Congressional Reconstruction plan was through his control, as commander-in-chief of the military, which would be the primary means by which to enforce the plan's provisions. However, even Johnson's control of the military was inhibited by the fact that his Secretary of War, Edwin Stanton, was a staunch Radical Republican who supported Congressional Reconstruction in full. This further set Johnson against the Republican-controlled Congress, with Johnson wanting to remove Stanton from office and Congress wanting to keep him in place.
Stanton and impeachment.
The Tenure of Office Act restricted the President to suspend an officer while the Senate was not in session. (At that time, Congress sat during a relatively small portion of the year.) If, when the Senate reconvened, it declined to ratify the removal, the President would be required to reinstate the official.
In August 1867, with the Senate out of session, Johnson made his move against Stanton, suspending him pending the next session of the Senate. However, when the Senate convened on January 4, 1868, it refused to ratify the removal by a vote of 35-16. Notwithstanding the vote, President Johnson attempted to appoint a new Secretary of War because he wanted, by such action, to create a case through which to challenge the legitimacy of the Act before the Supreme Court. Proceedings began within days, leading to Johnson's impeachment, the first impeachment of a United States President. After a three-month trial, Johnson avoided removal from office by the Senate by a single vote. Stanton resigned in May 1868.
It was actually unclear whether Johnson had violated the Tenure of Office Act. The act's phrasing was murky, and it was not clear whether his removal of Stanton (a holdover from the Lincoln administration whom Johnson had not appointed) violated the Act. While the Act, by its terms, applied to current office holders, it also limited the protection offered to Cabinet members to one month after a new president took office.
Later use.
The act was amended on April 5, 1869, one month and one day after Republican Ulysses S. Grant assumed the presidency. The revisions grew out of an attempt to completely repeal the 1867 act. The measure to repeal passed the House of Representatives with only 16 negative votes but failed in the Senate. The new provisions were significantly less onerous, allowing the President to suspend office holders "in his discretion" and designate replacements while the Senate was in recess, subject only to confirmation of the replacements at the next session. The President no longer had to report his reasons for suspension to the Senate, and the Senate could no longer force reinstatement of suspended office holders. 
Although Grant, in his first message to Congress, in December 1869, urged the repeal of even the revised act, it did not cause further problems until the election of Democrat Grover Cleveland in 1884. Under the spoils system it had long been accepted practice for the administration of a new party to replace current office holders with party faithful. However, Cleveland, a supporter of a civil service system, had promised to avoid wholesale replacements, vowing to replace incumbents only for cause. When he suspended several hundred office holders for cause, Senate committees requested information from cabinet members regarding the reasons for the suspensions, which Cleveland refused to provide. If he had simply said that the incumbents were being replaced for political reasons, the Senate would have complied, but Cleveland would not do so. When, in early 1886, the Senate as a whole demanded information regarding the conduct of the suspended U.S. Attorney for southern Alabama, Cleveland sent a message to Congress explaining his position opposing impingement of independence of the executive. Cleveland's replacement nominee was eventually confirmed when it was discovered that the suspended incumbent's term had expired in the meantime anyway. The Tenure of Office Act was finally repealed in 1887.
Constitutionality.
In 1926, a similar law (though not dealing with Cabinet secretaries) was ruled unconstitutional by the United States Supreme Court in the case of "Myers v. United States", which affirmed the ability of the President to remove a Postmaster without Congressional approval. In reaching that decision, the Supreme Court stated in its majority opinion (though in dicta), "that the Tenure of Office Act of 1867, insofar as it attempted to prevent the President from removing executive officers who had been appointed by him by and with the advice and consent of the Senate, was invalid".

</doc>
<doc id="55657" url="https://en.wikipedia.org/wiki?curid=55657" title="Amnesty Act">
Amnesty Act

The Amnesty Act of May 22, 1872 was a United States federal law that removed voting restrictions and office-holding disqualification against most of the secessionists who rebelled in the American Civil War, except for some 500 military leaders of the Confederacy. The act was passed by the 42nd United States Congress and the original restrictive Act was passed by the United States Congress in May 1866.
The 1872 Act affected over 150,000 former Confederate troops who had taken part in the American Civil War.

</doc>
<doc id="55658" url="https://en.wikipedia.org/wiki?curid=55658" title="Specie Payment Resumption Act">
Specie Payment Resumption Act

The Specie Payment Resumption Act of January 14, 1875, was a law in the United States which restored the nation to the gold standard through the redemption of previously unbacked United States Notes and reversed inflationary government policies promoted directly after the American Civil War. The decision further contracted the nation's money supply and was seen by critics as an exacerbating factor of the so-called "Long Depression", which struck in 1873.
History.
Late in 1861, seeking to raise revenue for the American Civil War effort without exhausting its reserves of gold and silver, the United States federal government suspended specie payments, or the payments made in gold and silver in redemption of currency notes. Early in 1862, the United States issued legal-tender notes, called greenbacks. By war's end, a total of $431 million in greenbacks had been issued, and authorization had been given for another $50 million in small denominations, known as fractional currency or "shin plasters." The issuance of greenbacks caused inflation during the period.
Immediately after the Civil War during Reconstruction, there were large capital inflows into the United States and a general improvement in the export-to-import ratio since the export-dominant South was reintegrated with the North. The United States Treasury, however, had increased its cash balance through the summer of 1873 by selling gold for $14 million. National banks also increased issuance of national bank notes by $44 million. The failure of several railroad companies including Jay Cooke & Company on their bond obligations encouraged capital outflows from the United States to Europe and weakened demand for dollars leading to the Panic of 1873. Increased Treasury cash balances, continued issuance of national bank notes, and capital outflows together depreciated the currency. These factors further caused a reduction in reserves held by monetary institutions because higher prices increased domestic demand for currency. Reserves held by banks were insufficient to be able to meet seasonal demands in autumn of 1873 as greenback reserves declined from $34 million in September 1873 to $5 million in October 1873. Tensions surrounding the Panic of 1873 between creditors and debtors revived the specie payment resumption debate.
Two views dominated this debate. Conservatives and the creditor class favored "hard money," that is they favored resumption as a method for making up losses incurred due to dollar depreciation during the past decade. The resumption of specie payments was perceived as a method to curb the rise in the price level and eventually equate currency with gold. For creditors that had issued debts in inflated greenbacks, resumption would increase the real interest rate that they received. Supporters of the Resumption Act argued that the Panic of 1873 might not have occurred had there been sufficient reserves of gold in the United States Treasury as would have been in case in specie payments were resumed.
Opposed to resumption, a new coalition of agrarian and labor interests found common cause during Reconstruction in advocating for "soft money" or the promotion of inflationary monetary policies. These groups viewed the Panic of 1873 as the result of insufficient currency that should have been used to fuel the growth in production that occurred in the South and the West. These regions relied on cheap money – that is low interest rates – to be able to continue to grow. Other soft money advocates included gold speculators and the railroad industry. Collis P. Huntington and other railroad leaders called for further greenback issuance in light of harsh business conditions that made honoring debt obligations difficult. Opponents of the Resumption Act also argued that many debt obligations were made in an environment in which there existed a premium on gold, that is that inflation in the currency in the previous decade made gold relatively more valuable than currency. Resumption therefore entailed up to a 50% increase in debt obligation if gold and currency equalized. For manufacturers, the rising price of gold made domestic prices cheaper relative to import prices since many European currencies including the English sterling were fixed to the price of gold. Hence continued inflationary measures such as further issuance of greenbacks artificially supported domestic industries. Hard and soft money interests often did cross party lines, although a larger portion of Democrats were hard money advocates.
Following a Democratic congressional victory in the elections of 1874, a lame-duck Republican congress passed The Resumption Act of January 14, 1875. It required the Secretary of the Treasury to redeem greenbacks in specie on demand on or after 1 January 1879. The Act, however, did not provide for a specific mechanism for redemption. The Act, though, did allow the Secretary of Treasury to acquire gold reserves either via any federal surpluses or the issuance of government bonds. An established gold reserve allowed for daily variations in specie flows and facilitated resumption. The act abolished the seigniorage fee on coining gold and substituted silver for any still existing fractional currency. The Resumption Act set no limit on the quantity of national bank notes that could be issued; this idea became known as "free banking." This provision led many conservatives to believe that the Act was inflationary in nature. However, the Resumption Act also required that greenbacks be retired in a proportion of 80% of new national bank note issue, which in theory aimed to contract the money supply and hence encourage dollar appreciation such that gold and currency might equate. However, in practice the effect was mild: the total quantity of greenbacks in circulation fell from $382 million at the end of 1874 to $300 million following the passage of the Resumption Act.
The Resumption Act was hotly debated during the 1880 presidential election, with most western politicians opposed to it. Specie payments finally resumed during the presidency of Rutherford B. Hayes. Aided by the return of prosperity in 1877, Secretary of the Treasury John Sherman accumulated a gold reserve to be redeemed for existing greenbacks mostly from transactions with Europe. Sherman earmarked a redemption fund by January 1, 1879 that amounted to $133 million acquired from the sale of bonds to Europe and Treasury surplus. However, when people found greenbacks to be on par with gold, they lost their desire for redemption.
Reaction and criticism.
Reactions as to the effects of the Resumption Act are mixed. Contemporaries did not consider it an outright victory for hard money. The legislation stood as a compromise engineered by Senators John Sherman and George Edmunds between hard and soft money advocates. Milton Friedman and Anna J. Schwartz argue that the Resumption Act had mixed effects on actual resumption of specie payments, saying that primary economic product of the Act was that it instilled confidence in the business community on the maintenance of specie payments. The Act served as a signal to businesses of an approaching exchange rate between gold and currency. Preparations among businesses for this exchange rate actually encourage parity between gold and currency.
The Act did not directly address the price level although successful resumption at par value required that the premium on gold to fall to zero, which in turn required a fall in the price level as the world price of gold was exogenous. In fact, the final date for Resumption was decided only after the premium on gold had fallen to a tenth of its peak level. The decline in the premium cannot entirely be attributed to the Resumption Act, as downward pressure on the overall price level also resulted from increased production in the South especially during 1877. The first four months of that year sold as much beef to England as had been sold all of the preceding year. The act has also been criticized for both failing to remove all greenbacks from circulation and failing to dictate what might be done with the greenbacks remaining in circulation.

</doc>
<doc id="55659" url="https://en.wikipedia.org/wiki?curid=55659" title="Bland–Allison Act">
Bland–Allison Act

The Bland–Allison Act, also referred to as the Grand Bland Plan of 1878, was an act of United States Congress requiring the U.S. Treasury to buy a certain amount of silver and put it into circulation as silver dollars. Though the bill was vetoed by President Rutherford B. Hayes, the Congress overrode Hayes' veto on February 23, 1878 to enact the law.
Background.
The five-year depression following the Panic of 1873 caused cheap-money advocates (led by Representative Richard P. Bland, a Democrat of Missouri), to join with silver-producing interests in urging a return to bimetallism, the use of both silver and gold as a standard.Coupled with Senator William B. Allison of Iowa, they agreed to a proposal that allowed silver to be purchased at market rates, metals to be minted into silver dollars, and required the US Treasury to purchase between $2 million to $4 million silver each month from western mines. President Rutherford B. Hayes, who held interests in industrials and banking, vetoed the measure, which was overturned by Congress. As a result, the Hayes administration purchased the limited amount of silver each month. This act helped restore bimetallism with gold and silver both supporting the currency. However, gold remained heavily favored over silver, paving way for the gold standard.
Free Silver Movement.
The free-silver movement of the late 19th century advocated the unlimited coinage of silver, which would have resulted in inflationary monetary policy. In 1873, Congress had removed the usage of silver dollar from the list of authorized coins under the Coinage Act of 1873 (referred to by opponents as 'the Crime of '73'"). Although the Bland-Allison Act of 1878 directed the Treasury to purchase silver from the "best-western" miners, President Grover Cleveland repealed the act in 1893. Advocates of free silver included owners of silver mines in the West, farmers who believed an inclusion of silver would increase crop prices, and debtors who believed would alleviate their debts. Although the free silver movement ended, the debate of inflation and monetary policy continues to this day.
Coinage Act of 1873.
The Fourth Coinage Act acknowledged the gold standard over silver. Those who advocated for silver labeled this act as the "Crime of '73". As a result of demonetized silver, gold became the only metallic standard in the United States and became the default standard. The price of gold was more stable than that of silver, largely due to silver discoveries in Nevada and other places in the West, and the price of silver to gold declined from 16-to-1 in 1873 to nearly 30-to-1 by 1893. The term limping bimetallism describes this problem. The U.S. government finally ceded to pressure from the western mining states and the Bland-Allison Act went into effect in 1878, which was replaced by the Sherman Silver Purchase Act of 1890. The law was replaced in 1890 by the similar Sherman Silver Purchase Act, which in turn was repealed by Congress in 1893. These were two instances were the United States attempted to establish bimetallic standards in the long run.
Reactions and economic impact.
Western miners and debtors regarded the Bland-Allison Act as an insufficient measure to enforce unlimited coinage of silver, but opponents repealed the act and advocated for the gold standard. The effect of the Bland-Allison act was also blunted by the minimal purchase of silver required by the Hayes administration. Although the act was a near turning point for bimetallism, gold continued to be favored over the bimetallism standard.
Throughout 1860 to 1871, several attempts were made by the Treasury to establish the bimetallic standard by having gold and silver franc. However, the discovery of silver led to an influx of supply, lowering the price of silver. The eventual removal of the bimetallic standard, including the Bland-Allison Act and the acceptance of the gold standard formed the monetary stability in the late 19th century.
The limitation placed on the supply of new notes and the Treasury control over the issue of new notes allowed for economic stability. Prior to the acceptance, the devaluation of silver forced local governments into a financial turmoil. In addition, there was a need for money supply to increase as the credit system expanded and large banks established themselves across states.

</doc>
<doc id="55661" url="https://en.wikipedia.org/wiki?curid=55661" title="List of Danish monarchs">
List of Danish monarchs

This is a list of Danish monarchs, that is, the Kings and Queens regnant of Denmark. This includes:
The house of Oldenburg held the Danish Crown between 1448 and 1863, when it passed to the house of Schleswig-Holstein-Sonderburg-Glücksburg, a cadet branch of the same house, descended from King Frederick V of Denmark. The kingdom had been elective (although the eldest son or brother of the previous king was usually elected) until 1660, when it became hereditary and absolutist. Until 1864 Denmark was also united in a personal union with the duchies of Holstein and Saxe-Lauenburg, and in a political and personal union with the Duchy of Schleswig.

</doc>
<doc id="55662" url="https://en.wikipedia.org/wiki?curid=55662" title="Hans Janmaat">
Hans Janmaat

Johannes Gerardus Hendrikus "Hans" Janmaat (November 3, 1934 – June 9, 2002) was a Dutch politician of the Centre Party (CP) and later his own formed Centre Democrats (CD). He was Parliamentary leader of the Centre Party in the House of Representatives from September 16, 1982 until October 15, 1984 when he was expelled from the party. He later served as Parliamentary leader of the Centre Democrats in the House of Representatives from September 8, 1989 until May 19, 1998 when his party lost all its seats. 
Although he was widely known, he was never a major force in the Dutch political landscape, partly because of a cordon sanitaire imposed by the Third Lubbers cabinet.
Biography.
Early life.
Johannes Gerardus Hendrikus Janmaat was born on November 3, 1934 in Nes aan de Amstel in North Holland, as the oldest of nine children in a traditional Roman Catholic family. His father was a salesman and insurance broker. When Janmaat was 4 years old, the family moved to Gouda, where it would endure the war years in relative peace.
After graduating in 1954, Janmaat started a study in aeronautical engineering, but had to drop out two years later after his father could no longer afford the tuition fees. Having to give up his studies to work was the first of many setbacks he would often refer to later in life.
In 1966 he married Belgian Evi Hock, having met her while working in Germany. In 1979, they divorced. In 1996, Janmaat married Wil Schuurman. No children were born in either marriage.
In the early 1960s, Janmaat ran a furniture factory with two of his brothers, but it burned down in 1966. He used the insurance payout to study politicology at the University of Amsterdam. Fellow-students found him ambitious, provocative and witty. In 1969, he participated in the occupation of the "Maagdenhuis", the university's administrative center, as part of a student protest. He completed his studies in 1972.
Politics.
After graduating, Janmaat held part-time positions as a teacher of civics. He also ran a one-man consultancy for small businesses.
In 1972, he joined the Catholic People's Party (KVP). He also worked in several commissions for the Democratic Socialists '70 (DS'70) party. Despite his efforts, he was not considered suitable for a front line position because of his capriciousness and tendency to go against the grain.
In the 1970s, he became more interested in the emerging issue of immigration as large numbers of foreign workers came to the Netherlands. His increasingly radical stance lead to a break with the KVP as well as DS'70.
In 1980, he read an article in Vrij Nederland which drew his attention to the recently founded extreme-right Centre Party (CP). After several interviews, he joined the party as its seventh member. Starting as a publicity worker, he would rapidly rise to be the party's top and was its "lijsttrekker" (top candidate) for the 1982 elections. The party won a single seat in the House of Representatives, which went to Janmaat. Other political parties largely ignored and ostracized him.
Centre Democrats.
After disagreements and a power struggle with other members of the Centrum Party, he was expelled from the CP in October 1984. However, he retained his seat in parliament, in accordance with Dutch law. Janmaat officially launched his own party, the Centre Democrats (CD) in November 1984. Politically, the party did not differ greatly from the CP, except that it was strongly centered around Janmaat, to prevent another power struggle. 
Several attempts were made to reconcile the differences between CP and CD. One such meeting in a hotel in Kedichem was disrupted by left-wing activists, who set fire to the building. Janmaat narrowly escaped with his life, CD secretary (and later wife of Janmaat) Wil Schuurman lost a leg because of injuries sustained jumping out of a window to escape the fire.
In the 1986 election, Janmaat lost his seat in parliament, however he regained his single seat in 1989.
His biggest political success would be in the 1994 elections, when he gained three seats. Major political parties changed their response to Janmaat and his views: rather than actively ignoring him they also started openly addressing the issue of immigration. In the 1998 election the CD lost all three of its seats. Janmaat had become increasingly paranoid and said that computers used for voting had been tampered with.
In 1999, Janmaat was in the process of starting another party, the "Conservative Democrats", however it did not get off the ground and did not participate in the 2002 election. 
His failing health forced him to withdraw from politics, however the changing political climate did prompt him to challenge his conviction for discrimination at the European Court of Justice. Janmaat's death in 2002 halted the case.
Political views.
Janmaat wanted to represent the indigenous Dutch workers and middle class. His views were based mostly on economic and materialistic arguments rather than an underlying ideology. Disappointing economic growth, unemployment and government cutbacks could not be addressed while large numbers of immigrants were flowing into the country. Janmaat was against a multicultural society: he argued that immigrants should either assimilate into Dutch culture, or return to their country of birth.
His best known slogans were "Holland is not a country of immigration," "full=full" and "we will abolish the multicultural society, as soon as we get the chance and power"; he was convicted for the last two statements. According to Jan van de Beek, Hans Janmaat often used economic arguments in his tirades against immigrants.
He was often accused of committing acts of hate speech, and received fines and a conditional prison sentence for incitement to hatred and discrimination against foreigners.
He often made controversial remarks about immigrants and other politicians. He argued that Ernst Hirsch Ballin should not be allowed to hold a high office because of his Jewish heritage and said he was not saddened by the sudden death of political opponent Ien Dales.
Legacy.
Other parties erected a cordon sanitaire around Janmaat, ignoring him while he spoke in parliament. A taboo on discussing negative aspects of immigration existed in the Dutch political climate in the 1980s.
Meindert Fennema, Emeritus Professor of Political Theory of Ethnic Relations at the University of Amsterdam, argued in 2006 that Janmaat was convicted for statements that are now commonplace due to changes in the political climate (caused in part by the September 11 attacks, and the assassinations of Pim Fortuyn and Theo van Gogh).

</doc>
<doc id="55663" url="https://en.wikipedia.org/wiki?curid=55663" title="Pendleton Civil Service Reform Act">
Pendleton Civil Service Reform Act

The Pendleton Civil Service Reform Act (ch. 27, ) is a United States federal law, enacted in 1883, which established that positions within the federal government should be awarded on the basis of merit instead of political affiliation. The act provided selection of government employees by competitive exams, rather than ties to politicians or political affiliation. It also made it illegal to fire or demote government officials for political reasons and prohibited soliciting campaign donations on Federal government property. To enforce the merit system and the judicial system, the law also created the United States Civil Service Commission. This board would be in charge of determining the rules and regulations of the act. The Act also allowed for the president, by executive order to decide which positions could be subject to the act and which would not. A crucial result was the shift of the parties to reliance on funding from business, since they could no longer depend on patronage hopefuls.
History.
In 1877, there was growing interest in the United States concerning the effects of the spoil system on the American political system. New York City established the Civil Service Reform Association to help address the issues, which would lead to several other organizations like it showing up in other cities. The presence of these organizations was one of the first steps in trying to up end the spoils system in America.
The assassination of President James A. Garfield moved the Civil Service Reform from city organizations to a leading topic in the political realm. President Garfield was shot in July 1881 by Charles Guiteau, because Guiteau believed the president owed him a patronage position for his "vital assistance" in securing Garfield's election the previous year. Garfield died two months later, and Vice President Chester A. Arthur acceded to the presidency. Once in office, President Arthur pushed through legislation for civil reform.
On January 16, 1883 Congress passed the Civil Service Act, which is sometimes referred to as the Pendleton Act after Senator George H. Pendleton of Ohio, one of the primary sponsors. The Act was written by Dorman Bridgman Eaton, a staunch opponent of the patronage system who was later first chairman of the United States Civil Service Commission. However, the law would also prove to be a major political liability for Arthur. The law offended machine politicians, or politicians who belong to a small clique that controls a political party. These politicians realized that with the Pendleton Act in place they would have to find a new means of income, since they could no longer count on donations from the wealthy hoping to receive jobs.
The Act initially covered only about 10% of the U.S. government's civilian employees. However, there was a provision that allowed outgoing presidents to lock in their own appointees by converting jobs to civil service. After a series of party reversals at the presidential level (1884, 1888, 1892, 1896), the result was that most federal jobs were under civil service.

</doc>
<doc id="55664" url="https://en.wikipedia.org/wiki?curid=55664" title="Millicent Fawcett">
Millicent Fawcett

Dame Millicent Garrett Fawcett, GBE (11 June 1847 – 5 August 1929) was an English feminist, intellectual, political and union leader, and writer. However, she is primarily known for her work as a suffragist (a campaigner for women to have the vote).
She was born Millicent Garrett in Aldeburgh, Suffolk. 
As a suffragist (as opposed to a suffragette), she took a moderate line, but was a tireless campaigner. 
She concentrated much of her energy on the struggle to improve women's opportunities for higher education and in 1871 co-founded Newnham College, Cambridge. She later became president of the National Union of Women's Suffrage Societies (the NUWSS), a position she held from 1897 until 1919. In July 1901 she was appointed to lead the British Government's commission to South Africa to investigate conditions in the concentration camps that had been created there in the wake of the Second Boer War. Her report corroborated what the campaigner Emily Hobhouse had said about conditions in the camps.
Early life.
Millicent Garrett was born on 11 June 1847 in Aldeburgh to Newson Garrett, a warehouse owner, and his wife Louise Dunnell.
The Garrett ancestors had been ironworkers in East Suffolk since the early seventeenth century. Newson Garrett was the youngest of three sons and not academically inclined, although he possessed the family’s entrepreneurial spirit. When he finished school, the town of Leiston offered little to Newson, so he left for London to make his fortune. There, he fell in love with his brother's sister-in-law, Louisa Dunnell, the daughter of an innkeeper of Suffolk origin. After their wedding, the couple went to live above a pawnbroker's shop at 1 Commercial Road, Whitechapel. The Garretts had their first three children in quick succession: Louie, Elizabeth and their brother (Newson Dunnell) who died at the age of six months. In 1839 the family moved to 142 Long Acre, where they were to live for 2 years, whilst two more children were born.
Her father moved up in the world, becoming not only the manager of a larger pawnbroker's shop, but also a silversmith. Garrett's grandfather, owner of the family engineering works, Richard Garrett & Sons, had died in 1837, leaving the business to his eldest son, Garrett's uncle. Despite his lack of capital, Newson was determined to be successful and in 1841, at the age of 29, he moved his family to Suffolk, where he bought a barley and coal merchants business in Snape, Suffolk. The Garretts lived in a square Georgian house opposite the church in Aldeburgh until 1852. Newson's malting business expanded and five more children were born, Alice (1842), Millicent (1847), Sam (1850), Josephine (1853) and George (1854). By 1850, Newson was a prosperous businessman and was able to build Alde House, a mansion on a hill behind Aldeburgh.
In 1858 when she was twelve, Millicent was sent to London, with her sister Elizabeth (Elizabeth Garrett Anderson, the first female doctor in Britain ) to study at a private boarding school in Blackheath. Her sister Louise took her to the sermons of Frederick Denison Maurice, who was a more socially aware and less traditional Church of England minister, and whose opinion influenced Millicent's view of religion. A key moment occurred when Millicent was 19 and went to hear a speech by the radical MP, John Stuart Mill. Mill was an early advocate of universal women’s suffrage. His speech on equal rights for women made a big impression on Millicent, and she became actively involved in his campaign. She was impressed by Mills practical support for women’s rights on the basis of utilitarianism – rather than abstract principles.
Married life.
These visits were the start of Millicent Fawcett's interest in women's rights. 
In 1865, Elizabeth took her to see a speech by John Stuart Mill on the subject; Millicent was impressed by this speech, and became an active supporter of his work. 
In 1866, at the age of 19, she became secretary of the London Society for Women's Suffrage. Mill introduced her to many other women's rights activists, including Henry Fawcett, a liberal Member of Parliament who had originally intended to marry Elizabeth before she decided to focus on her medical career. Millicent and the politician became close friends, and despite a fourteen-year age gap they married in 1867. Millicent took his last name, becoming Millicent Garrett Fawcett. The MP had been blinded in a shooting accident in 1858, and Millicent acted as his secretary. The marriage was described as one based on "perfect intellectual sympathy", and Millicent pursued a writing career of her own while caring for him. Their only child, Philippa Fawcett, was born in 1868. She was close to Phillipa as they shared skill in needlework, Phillipa also excelled in school, which fared well with her mother and with women's rights. Fawcett ran two households, one in Cambridge and one in London. "The Fawcetts were a radical couple, flirting even with republicanism, supporters of proportional representation and trade unionism, keen advocates of individualistic and free trade principles and the advancement of women". Henry and Millicent's close relationship was never doubted; they had a real, and loving, marriage.
In 1868, Millicent joined the London Suffrage Committee, and in 1869 she spoke at the first public pro-suffrage meeting to be held in London. In March 1870 she spoke in Brighton, her husband's constituency, and as a speaker was known for her clear speaking voice. In 1870 she published "Political Economy for Beginners", which although short was "wildly successful", and ran through 10 editions in 41 years. In 1872 she and her husband published "Essays and Lectures on Social and Political Subjects", which contained eight essays by Millicent. In 1875 she was a co-founder of Newnham Hall, and served on its Council.
Later years.
After the death of her husband on 6 November 1884, she temporarily withdraw from public life. She sold both family homes and moved with Philippa into the house of Agnes Garrett, her sister. 
She resumed work in 1885. Millicent began to concentrate on politics. Originally an active Liberal, she joined the Liberal Unionist party in 1886 in opposition to Irish Home Rule. In 1904, she resigned from the party on the issue of Free Trade when Joseph Chamberlain gained control in his campaign for Tariff Reform.
After the death of Lydia Becker, she became the leader of the National Union of Women's Suffrage Societies (NUWSS), the main suffragist organisation in Britain. She held this post until 1919, a year after the first women had been granted the vote. After that, she left the suffrage campaign for the most part, and devoted much of her time to writing books, including a biography of Josephine Butler.
She was granted an honorary LLD by the University of St Andrews in 1899, awarded a damehood (GBE) in 1925, and died four years later, in 1929. Her memory is preserved now in the name of the Fawcett Society, and in "Millicent Fawcett Hall", constructed in 1929 in Westminster as a place that women could use to debate and discuss the issues that affected them. The hall is currently owned by Westminster School and is the location of its drama department, incorporating a 150-seat studio theatre.
Millicent Fawcett died in London in 1929 and was cremated at Golders Green Crematorium.
Political activities.
Millicent began her career in the political platform at twenty-two years old at the first women's suffrage meeting. Millicent Fawcett (leader of NUWSS) was a moderate campaigner, distancing herself from the militant and violent activities of the Pankhursts and the Women's Social and Political Union (WSPU). 
She believed that their actions were in fact harming women's chances of gaining the vote, as they were alienating the MPs who were debating whether or not to give women the vote, as well as souring much of the general public towards the campaign. Despite the publicity given to the WSPU, the NUWSS (one of whose slogans was "Law-Abiding suffragists" ) retained the majority of the support of the women's movement. By 1905, Fawcett's NUWSS had reached 305 constituent societies and nearly fifty thousand members. In 1913 they had 50,000 members compared to 2,000 of the WSPU. Fawcett mainly fought for women's right to vote, and found home rule to be "a blow to the greatness and prosperity of England as well as disaster and… misery and pain and shame". 
Fawcett cut her liberal ties in 1884, her belief in women's suffrage was unchanged however her political views did change and began to resemble the views she had when she was younger. In 1883, Fawcett received the role of president of the Special Appeal Committee.
The South African War created an opportunity for Millicent to share female responsibilities in British culture. Millicent was nominated to be the leader of the commission of women who were sent to South Africa. In July 1901, she sailed to South Africa with other women "to investigate Emily Hobhouse's indictment of atrocious conditions in concentration camps where the families of the Boer soldiers were interned". In Britain a woman had never been trusted with such a responsibility during wartime. Millicent fought for the civil rights of the Uitlanders, "as the cause of revival of interest in women's suffrage".
Over many years, Millicent had backed countless campaigns; which were not all successful. A few campaigns Millicent supported were, "to curb child abuse by raising the age of consent, criminalizing incest, cruelty to children within the family, to end the practice of excluding women from courtrooms when sexual offences were under consideration, to stamp out the 'white slave trade', and to prevent child marriage and the introduction of regulated prostitution in India". Fawcett also campaigned for the repeal of the Contagious Diseases Acts, which reflected sexual double standards. The Acts required that prostitutes be examined for sexually transmitted diseases, and if they were found to have passed any on to their customers, they were imprisoned. Women could be arrested on suspicion of being a prostitute, and could also be imprisoned for refusing consent to the examination, which was invasive and could be painful. The prostitutes' infectious male customers were not subject to the Acts. The Acts were eventually repealed as a result of Fawcett's and others' campaigning. Millicent believed the double standard of morality would never become eradicated until women were represented in the public sphere of life.
Fawcett was also an author. She usually penned under her own name as Millicent Garrett Fawcett, however as a public figure she was styled Mrs. Henry Fawcett. Fawcett had three books, a co-authorized book with her husband Henry Fawcett, and many of her articles were published respectively. Fawcett published a textbook Political Economy for Beginners that had ten editions, sparked two novels and was produced in many languages. One of Fawcett's first articles on women's education was published in Macmillan's Magazine in 1875. In 1875, Fawcett's interest in women's education lead her to become one of the founders of the Newnham College for Women, located in Cambridge. Fawcett served on the school's council, she was also supportive when there was a controversial bid to all women to receive Cambridge degrees. Millicent was also a speaker and lecturer at girl's schools and women's colleges, she also spoke in adult education centres. For her services in education the University of St. Andrews awarded her an honorary LLD in 1899.
When the First World War broke out in 1914, while the WSPU ceased all of their activities to focus on the war effort, Fawcett's NUWSS did not. 
This was largely because as the organisation was significantly less militant than the WSPU, it contained many more pacifists, and general support for the war within the organisation was weaker. 
The WSPU, in comparison, was called jingoistic as a result of its leaders' strong support for the war. While Fawcett was not a pacifist, she risked dividing the organisation if she ordered a halt to the campaign, and the diverting of NUWSS funds from the government, as the WSPU had done. 
The NUWSS continued to campaign for the vote during the war, and used the situation to their advantage by pointing out the contribution women had made to the war effort in their campaigns.
Fawcett is considered instrumental in gaining the vote for six million British women over 30 years old in 1918.
“A memorial inscription added to the monument to Henry Fawcett in Westminster Abbey in 1932 asserts that she 'won citizenship for women'".
The archives of Millicent Garrett Fawcett are held at The Women's Library at the Library of the London School of Economics, ref 7MGF.
Archives.
The archives of Millicent Fawcett are held at The Women's Library at the Library of the London School of Economics ref 7MGF

</doc>
<doc id="55667" url="https://en.wikipedia.org/wiki?curid=55667" title="Spoils system">
Spoils system

In the politics of the United States, a spoils system (also known as a patronage system) is a practice in which a political party, after winning an election, gives government jobs to its supporters, friends and relatives as a reward for working toward victory, and as an incentive to keep working for the party—as opposed to a merit system, where offices are awarded on the basis of some measure of merit, independent of political activity.
The term was derived from the phrase "to the victor belong the spoils" by New York Senator William L. Marcy, referring to the victory of the Jackson Democrats in the election of 1828, with the term spoils meaning goods or benefits taken from the loser in a competition, election or military victory.
Similar spoils systems are common in other nations that traditionally have been based on tribal organization or other kinship groups and localism in general.
Origins.
Before March 8, 1829, moderation had prevailed in the transfer of political power from one U.S. presidency to another. President Andrew Jackson's inauguration signaled a sharp departure from past presidencies. An unruly mob of office seekers made something of a shambles of the March inauguration, and though some tried to explain this as democratic enthusiasm, the real truth was Jackson supporters had been lavished with promises of positions in return for political support. These promises were honored by an astonishing number of removals after Jackson assumed power. At the beginning of Jackson's administration, fully 919 officials were removed from government positions, amounting to nearly 10 percent of all government postings.
The Jackson administration attempted to explain this unprecedented purge as reform, or constructive turnover, aimed at creating a more efficient system where the chain of command of public employees all obeyed the higher entities of government. The hardest changed organization within the federal government proved to be the post office. The post office was the largest department in the federal government, and had even more personnel than the war department. In one year 423 postmasters were deprived of their positions, most with extensive records of good service. 
Reform.
By the late 1860s, citizens began demanding civil service reform. Running under the Liberal Republican Party in 1872, they were soundly defeated by Ulysses S. Grant.
After the assassination of James A. Garfield by a rejected office-seeker in 1881, the calls for civil service reform intensified. Moderation of the spoils system at the federal level came with the passage of the Pendleton Act in 1883, which created a bipartisan Civil Service Commission to evaluate job candidates on a nonpartisan merit basis. While few jobs were covered under the law initially, the law allowed the President to transfer jobs and their current holders into the system, thus giving the holder a permanent job. The Pendleton Act's reach was expanded as the two main political parties alternated control of the White House every election between 1884 and 1896. After each election the outgoing President applied the Pendleton Act to jobs held by his political supporters. By 1900, most federal jobs were handled through civil service and the spoils system was limited only to very senior positions.
The separation between the political activity and the civil service was made stronger with the Hatch Act of 1939 which prohibited federal employees from engaging in many political activities.
The spoils system survived much longer in many states, counties and municipalities, such as the Tammany Hall ring, which survived well into the 1930s when New York City reformed its own civil service. Illinois modernized its bureaucracy in 1917 under Frank Lowden, but Chicago held on to patronage in city government until the city agreed to end the practice in the Shakman Decrees of 1972 and 1983. 
Modern variations on the spoils system are often described as the political machine.

</doc>
<doc id="55668" url="https://en.wikipedia.org/wiki?curid=55668" title="Chinese Exclusion Act">
Chinese Exclusion Act

The Chinese Exclusion Act was a United States federal law signed by President Chester A. Arthur on May 6, 1882. It was one of the most significant restrictions on free immigration in US history, prohibiting all immigration of Chinese laborers. The act followed the Angell Treaty of 1880, a set of revisions to the US-China Burlingame Treaty of 1868 that allowed the US to suspend Chinese immigration. The act was initially intended to last for 10 years, but was renewed in 1892 with the Geary Act and made permanent in 1902. The Chinese Exclusion Act was the first law implemented to prevent a specific ethnic group from immigrating to the United States. It was repealed by the Magnuson Act on December 17, 1943.
Background.
The first significant Chinese immigration to North America began with the California Gold Rush of 1848–1855 and continued with subsequent large labor projects, such as the building of the First Transcontinental Railroad. During the early stages of the gold rush, when surface gold was plentiful, the Chinese were tolerated, if not well received. As gold became harder to find and competition increased, animosity toward the Chinese and other foreigners increased. After being forcibly driven from the mines, most Chinese settled in enclaves in cities, mainly San Francisco, and took up low-wage labor, such as restaurant and laundry work. With the post-Civil War economy in decline by the 1870s, anti-Chinese animosity became politicized by labor leader Denis Kearney and his Workingman's Party as well as by California Governor John Bigler, both of whom blamed Chinese "coolies" for depressed wage levels. Another significant anti-Chinese group organized in California during this same era was the Supreme Order of Caucasians, with some 60 chapters statewide.
In the early 1850s, there was resistance to the idea of excluding Chinese migrant workers from immigration, because they provided essential tax revenue which helped fill the fiscal gap of California. But toward the end of the decade, the financial situation improved and subsequently, attempts to legislate Chinese exclusion became successful on the state level. In 1858, the California Legislature passed a law that made it illegal for any person "of the Chinese or Mongolian races" to enter the state; however, this law was struck down by an unpublished opinion of the State Supreme Court in 1862.
The Chinese immigrant workers provided cheap labor and did not use any of the government infrastructure (schools, hospitals, etc.) because the Chinese migrant population was predominantly made up of healthy male adults. As time passed and more and more Chinese migrants arrived in California, violence would often break out in cities such as Los Angeles. By 1878 Congress decided to act and passed legislation excluding the Chinese, but this was vetoed by President Rutherford B. Hayes. In 1879, California adopted a new Constitution, which explicitly authorized the state government to determine which individuals were allowed to reside in the state, and banned the Chinese from employment by corporations and state, county or municipal governments. Once the Chinese Exclusion Act was finally passed in 1882, California went further by passing various laws that were later held to be unconstitutional. After the act was passed, most Chinese families were faced with a dilemma: stay in the United States alone or go back to China to reunite with their families. Although there was widespread dislike for the Chinese, some capitalists and entrepreneurs resisted their exclusion because they accepted lower wages.
The Act.
For the first time, Federal law proscribed entry of an ethnic working group on the premise that it endangered the good order of certain localities. (The earlier Page Act of 1875 had prohibited immigration of Asian forced laborers and prostitutes, and the Naturalization Act of 1790 prohibited naturalization of non-white subjects.) The Act excluded Chinese laborers, meaning "skilled and unskilled laborers and Chinese employed in mining," from entering the country for ten years under penalty of imprisonment and deportation.
The Chinese Exclusion Act required the few nonlaborers who sought entry to obtain certification from the Chinese government that they were qualified to emigrate. However, this group found it increasingly difficult to prove that they were not laborers because the 1882 act defined excludables as “skilled and unskilled laborers and Chinese employed in mining.” Thus very few Chinese could enter the country under the 1882 law. Diplomatic officials and other officers on business, along with their house servants, for the Chinese government were also allowed entry as long as they had the proper certification verifying their credentials.
The Act also affected the Chinese who had already settled in the United States. Any Chinese who left the United States had to obtain certifications for reentry, and the Act made Chinese immigrants permanent aliens by excluding them from U.S. citizenship. After the Act's passage, Chinese men in the U.S. had little chance of ever reuniting with their wives, or of starting families in their new abodes.
Amendments made in 1884 tightened the provisions that allowed previous immigrants to leave and return, and clarified that the law applied to ethnic Chinese regardless of their country of origin. The Scott Act (1888) expanded upon the Chinese Exclusion Act, prohibiting reentry after leaving the U.S. Constitutionality of the Chinese Exclusion Act and the Scott Act was upheld by the Supreme Court in "Chae Chan Ping v. United States" (1889); the Supreme Court declared that "the power of exclusion of foreigners an incident of sovereignty belonging to the government of the United States as a part of those sovereign powers delegated by the constitution." The Act was renewed for ten years by the 1892 Geary Act, and again with no terminal date in 1902. When the act was extended in 1902, it required "each Chinese resident to register and obtain a certificate of residence. Without a certificate, he or she faced deportation."
Between 1882 and 1905, about 10,000 Chinese appealed against negative immigration decisions to federal court, usually via a petition for habeas corpus. In most of these cases, the courts ruled in favor of the petitioner. Except in cases of bias or negligence, these petitions were barred by an act that passed Congress in 1894 and was upheld by the U.S. Supreme Court in "U.S." vs "Lem Moon Sing" (1895). In "U.S." vs "Ju Toy" (1905), the U.S. Supreme Court reaffirmed that the port inspectors and the Secretary of Commerce had final authority on who could be admitted. Ju Toy's petition was thus barred despite the fact that the district court found that he was an American citizen. The Supreme Court determined that refusing entry at a port does not require due process and is legally equivalent to refusing entry at a land crossing. All these developments, along with the extension of the act in 1902, triggered a boycott of U.S. goods in China between 1904 and 1906. There was one 1885 case in San Francisco, however, in which Treasury Department officials in Washington overturned a decision to deny entry to two Chinese Students.
One of the critics of the Chinese Exclusion Act was the anti-slavery/anti-imperialist Republican Senator George Frisbie Hoar of Massachusetts who described the Act as "nothing less than the legalization of racial discrimination."
The laws were driven largely by racial concerns; immigration of persons of other races was unlimited during this period.
On the other hand, many people strongly supported the Chinese Exclusion Act, including the Knights of Labor, a labor union, who supported it because it believed that industrialists were using Chinese workers as a wedge to keep wages low. Among labor and leftist organizations, the Industrial Workers of the World were the sole exception to this pattern. The IWW openly opposed the Chinese Exclusion Act from its inception in 1905.
For all practical purposes, the Exclusion Act, along with the restrictions that followed it, froze the Chinese community in place in 1882. Limited immigration from China continued until the repeal of the Chinese Exclusion Act in 1943. From 1910 to 1940, the Angel Island Immigration Station on what is now Angel Island State Park in San Francisco Bay served as the processing center for most of the 56,113 Chinese immigrants who are recorded as immigrating or returning from China; upwards of 30% more who showed up were returned to China. Furthermore, after the 1906 San Francisco earthquake, which destroyed City Hall and the Hall of Records, many immigrants (known as "paper sons") claimed that they had familial ties to resident Chinese-American citizens. Whether these were true or not cannot be proven.
The Chinese Exclusion Act gave rise to the first great wave of commercial human smuggling, an activity that later spread to include other national and ethnic groups.
The Chinese Exclusion Act also led to an expansion of the power of U.S. immigration law through its influence on Canada's policies on Chinese exclusion during this time because of the need for greater vigilance at the U.S.-Canadian border. Shortly after the U.S. Chinese Exclusion Act, Canada established the Chinese Immigration Act of 1885 which imposed a head tax on Chinese migrants entering Canada. After increasing pressure from the U.S. government, Canada finally established the Chinese Immigration Act, 1923 which banned most forms of immigration by the Chinese to Canada. There was also a need for this kind of border control along the U.S-Mexico border, however, efforts to control the border went along a different path because Mexico was fearful of expanding imperial power of the U.S. and did not want U.S. interference in Mexico. Not only this, but Chinese immigration to Mexico was welcomed because the Chinese immigrants filled Mexico's labor needs. The Chinese Exclusion Act actually led to heightened Chinese immigration to Mexico because of exclusion by the U.S. Therefore, the U.S. resorted to heavily policing the border along Mexico.
Later, the Immigration Act of 1924 restricted immigration even further, excluding all classes of Chinese immigrants and extending restrictions to other Asian immigrant groups. Until these restrictions were relaxed in the middle of the twentieth century, Chinese immigrants were forced to live a life separated from their families from, and to build ethnic enclaves in which they could survive on their own (Chinatown).
The Chinese Exclusion Act did not address the problems that whites were facing; in fact, the Chinese were quickly and eagerly replaced by the Japanese, who assumed the role of the Chinese in society. Unlike the Chinese, some Japanese were even able to climb the rungs of society by setting up businesses or becoming truck farmers. However, the Japanese were later targeted in the National Origins Act of 1924, which banned immigration from east Asia entirely.
In 1891 the Government of China refused to accept the U.S. Senator Mr. Henry W. Blair as U.S. Minister to China due to his abusive remarks regarding China during negotiation of the Chinese Exclusion Act.
The American Christian Rev. Dr. George F. Pentecost spoke out against western imperialism in China, saying: "I personally feel convinced that it would be a good thing for America if the embargo on Chinese immigration were removed. I think that the annual admission of 100,000 into this country would be a good thing for the country. And if the same thing were done in the Philippines those islands would be a veritable Garden of Eden in twenty-five years. The presence of Chinese workmen in this country would, in my opinion, do a very great deal toward solving our labor problems. There is no comparison between the Chinaman, even of the lowest coolie class, and the man who comes here from Southeastern Europe, form Russia, or from Southern Italy. The Chinese are thoroughly good workers. That is why the laborers here hate them. I think, too, that the emigration to America would help the Chinese. At least he would come into contact with some real Christian people in America. The Chinaman lives in squalor because he is poor. If he had some prosperity his squalor would cease."
Repeal and current status.
The Chinese Exclusion Act was repealed by the 1943 Magnuson Act, during a time when China had become an ally of the U.S. against Japan in World War II. This repeal also occurred in the context of World War II, during which the U.S. fought against German Nazism and needed to embody an image of fairness and justice, and also during a time when the Japanese were becoming vilified. The Magnuson Act permitted Chinese nationals already residing in the country to become naturalized citizens and stop hiding from the threat of deportation. While the Magnuson Act overturned the discriminatory Chinese Exclusion Act, it only allowed a national quota of 105 Chinese immigrants per year, and did not repeal the restrictions on immigration from the other Asian countries. Large scale Chinese immigration did not occur until the passage of the Immigration and Nationality Act of 1965. The crackdown on Chinese immigrants reached a new level in its last decade, from 1956–1965, with the Chinese Confession Program launched by the Immigration and Naturalization Service, that encouraged Chinese who had committed immigration fraud to confess, so as to be eligible for some leniency in treatment.
Despite the fact that the exclusion act was repealed in 1943, the law in California prohibiting non-whites from marrying whites was not repealed until 1948, in which the California Supreme Court ruled the ban of interracial marriage within the state unconstitutional in "Perez v. Sharp". Other states had such laws until 1967, when the United States Supreme Court unanimously ruled in "Loving v. Virginia" that anti-miscegenation laws across the nation are unconstitutional.
Even today, although all its constituent sections have long been repealed, Chapter 7 of Title 8 of the United States Code is headed "Exclusion of Chinese." It is the only chapter of the 15 chapters in Title 8 (Aliens and Nationality) that is completely focused on a specific nationality or ethnic group.
On June 18, 2012, the United States House of Representatives passed a resolution introduced by Congresswoman Judy Chu, that formally expresses the regret of the House of Representatives for the Chinese Exclusion Act, which imposed almost total restrictions on Chinese immigration and naturalization and denied Chinese-Americans basic freedoms because of their ethnicity. The resolution had been approved by the U.S. Senate in October 2011.
In 2014, the California Legislature took formal action to pass measures that formally recognize the many proud accomplishments of Chinese-Americans in California and to call upon Congress to formally apologize for the 1882 adoption of the Chinese Exclusion Act. Senate Republican Leader Bob Huff (R-Diamond Bar) and incoming Senate President pro-Tem Kevin de León (D-Los Angeles) served as Joint Authors for Senate Joint Resolution (SJR) 23 and Senate Concurrent Resolution (SCR) 122.
SJR 23 acknowledges and celebrates the history and contributions of Chinese Americans in California. The resolution also formally calls on Congress to apologize for laws which resulted in the persecution of Chinese Americans, such as the Chinese Exclusion Act.

</doc>
<doc id="55670" url="https://en.wikipedia.org/wiki?curid=55670" title="Warsaw Convention">
Warsaw Convention

The Convention for the Unification of certain rules relating to international carriage by air, commonly known as the Warsaw Convention, is an international convention which regulates liability for international carriage of persons, luggage, or goods performed by aircraft for reward.
Originally signed in 1929 in Warsaw (hence the name), it was amended in 1955 at The Hague, Netherlands, and in 1971 in Guatemala City, Guatemala. United States courts have held that, at least for some purposes, the Warsaw Convention is a different instrument from the Warsaw Convention as amended by the Hague Protocol.
History.
On 17 August 1923, the French government proposed the convening of a diplomatic conference in November 1923 for the purpose of concluding a convention relating to liability in international carriage by air. The conference was formally deferred on two occasions due to reluctant behavior of the governments of various nations to act on such a short notice without the knowledge of the proposed convention. Finally, between 27 October and 6 November, the first conference met in Paris to study the draft convention. Since most of the participants were diplomats accredited to the French government and not professionals, it was agreed unanimously that a body of technical, legal experts be set up to study the draft convention prior to its submission to the diplomatic conference for approval. Accordingly, in 1925, the "Committee International Technique of Experts Juridique Aeriens" (CITEJA) was formed. In 1927–28 CITEJA studied and developed the proposed draft convention and developed it into the present package of unification of law and presented it at the Warsaw Conference which was approved between 4 and 12 October 1929. It unified an important sector of private air law.
The Convention was written originally in French and the original documents were deposited in the archives of the Ministry for Foreign Affairs of Poland. After coming into force on 13 February 1933, it resolved some conflicts of law and jurisdiction.
Between 1948–51 it was further studied by a legal committee set up by the International Civil Aviation Organization (ICAO) and in 1952 a new draft was prepared to replace the convention. However it was rejected and it was decided that the convention be amended rather than replaced in 1953. The work done by the legal committee at the Ninth Session was presented to the International Conference on Air Law which was convened by the Council of the ICAO and met at The Hague from 6 to 28 September 1955. The Hague Conference adopted a Protocol (the Hague Protocol) for the amendment of the Warsaw Convention. Between the parties of the Protocol, it was agreed that the 1929 Warsaw Convention and the 1955 Hague Protocol were to be read and interpreted together as one single instrument to be known as the Warsaw Convention as amended at the Hague in 1955. This was not an amendment to the convention but rather a creation of a new and separate legal instrument that is only binding between the parties. If one nation is a party to the Warsaw Convention and another to the Hague Protocol, neither state has an instrument in common and therefore there is no mutual international ground for litigation.
The Montreal Convention, signed in 1999, replaced the Warsaw Convention system.
Content.
There are five chapters:
In the convention there is a provision of successive carriage and a combined carriage partly by air and partly by other modes of transport as well.
In particular, the Warsaw Convention:
The sums limiting liability were originally given in gold francs (defined in terms of a particular quantity of gold by article 22 paragraph 5 of the convention). These sums were amended by the Montreal Additional Protocol No. 2 to substitute an expression given in terms of SDR's. These sums are valid in the absence of a differing agreement (on a higher sum) with the carrier. Agreements on "lower" sums are null and void.
A court may also award a claiming party's costs, unless the carrier made an offer within 6 months of the loss (or at least 6 months before the beginning of any legal proceedings) which the claiming party has failed to beat.
The Warsaw Convention provides that a plaintiff can file a lawsuit at his or her discretion in one of the following forums:
According to Clauses 17 and 18 of the Warsaw Convention, airline companies are liable for any damage that occurs to passengers or their belongings during in-flight. However, airline companies will not be held responsible if the damage results from the passenger's own fault or one of their temporary servants such as doctors assisting ill passengers on their own initiative (Clause 20). To be covered by air carriers, doctors should respond to the captain's call when it comes to assisting ill passengers. In such cases, doctors are considered an airline's temporary servants who acted on the airline's instructions. Major airlines are all covered by insurance to meet such contingencies and to cover doctors who act as their temporary agents.
Ratifications.
As of 2015, the Warsaw Convention had been ratified by 152 states. The Protocol to the Convention had been ratified by 137 states.

</doc>
