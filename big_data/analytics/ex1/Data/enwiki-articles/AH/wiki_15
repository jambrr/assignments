<doc id="48977" url="https://en.wikipedia.org/wiki?curid=48977" title="Go strategy and tactics">
Go strategy and tactics

The game of Go has simple rules that can be learned very quickly but, as with chess and similar board games, complex strategies may be deployed by experienced players.
Go opening theory.
The whole board opening is called Fuseki. An important principle to follow in early play is "corner, side, center." In other words, the corners are the easiest places to take territory, because two sides of the board can be used as boundaries. Once the corners are occupied, the next most valuable points are along the side, aiming to use the edge as a territorial boundary. Capturing territory in the middle, where it must be surrounded on all four sides, is extremely difficult. The same is true for founding a living group: Easiest in the corner, most difficult in the center.
The first moves are usually played on or near the 4-4 star points in the corners, because in those places it is easiest to gain territory or influence. (In order to be totally secure alone, a corner stone must be placed on the 3-3 point. However, if a stone is placed at a 4-4 point and the opponent invades, the first player can build a surrounding wall as the second (invader) is forming a live group, thus exerting strong influence on a large area.) After that, standard sequences (Joseki) can be used to develop corner positions, and extensions along the side can be made. Usually, the center area is kept empty the longest. Plays are usually on the third or fourth line—the second makes too little territory, while the fifth is too easily undermined by a play on the third. A play on the fourth line is directed more towards influence to the center, a play on the third line more towards making territory along the side.
Connection and separation.
A fundamental Go strategy involves keeping stones connected. Connecting a group with one eye to another one-eyed group makes them live together. Connecting individual stones into a single group results in an increase of liberties; for instance, a single stone played in the center of the board has four liberties, while two adjacent stones in the center of the board form a unit with six; to capture the unit, an opponent would have to play stones on all of its liberties. Thus connected stones are stronger because they share their liberties. (While two separate stones have a total of up to eight liberties, they can be captured separately from one another.)
Since connecting stones keeps them secure, an important offensive tactic is to prevent the opponent from connecting his stones, while at the same time keeping one's own stones connected. This act of dividing the opponent's stones into separate groups is called "cutting."
While one should generally try to keep one's own stones connected, situations exist where doing so would be a wasted move. Stones are considered "tactically connected" if no move by the opposing player could prevent them from being connected.
In a handicap game, Black starts with two or more handicap stones played before White's first move. If played in the traditional places on the "star points", these stones will be useful for the purpose of connection and separation of stones played closer to the edge ("lower"), as well as in many other ways. The White player's stones are threatened immediately with separation, while Black has many potential connections to begin with.
An example of inefficiency or poor coordination of stones in the context of connection is the empty triangle, where the stones are arranged so that they share fewer liberties than if they were deployed in a straight line.
Life and death.
A key concept in the tactics of Go, though not part of the rules, is the classification of groups of stones into "alive", "dead" or "unsettled".
At the end of the game, groups that cannot avoid being captured during normal play are removed as captures. These stones are "dead". Groups can reach this state much earlier during play; a group of stones can quickly run out of options so that further play to save them is fruitless, or even detrimental. Similarly, further play to capture such a group is often of no benefit (except when securing liberties for an adjacent group), since if it remains on the board at the end of the game it is captured anyway. Thus groups can be considered "dead as they stand", or just "dead", by both sides during the course of the game.
Groups enclosing an area completely can be harder to kill. Normally, when a play causes an area completely enclosed by the opponent to become filled, the group filling the area is captured since it has no remaining liberties (such a play is called "suicide"). Only if the last play inside the area would kill the enclosing group, thus freeing one or more liberties for the group that filled the space, can the play be considered. This can only be achieved if the liberties on the outside of the enclosing group have been covered first. Thus, enclosing an area of one or more liberties (called an "eye") can make the group harder to kill, since the opponent must cover all of its external liberties before covering the final, internal liberty.
From this, it is possible to create groups that cannot be killed at all. If a group encloses two or more separate areas (two or more "eyes"), the opponent cannot simultaneously fill both of them with a single play, and thus can never play on the last liberty of the group. Such a group, or a group that cannot be prevented from forming such an enclosure, is called "alive".
Groups which are not definitely alive nor definitely dead are sometimes called "unsettled" groups. Much of the tactical fighting in Go focuses on making one's own groups live, by ensuring they can make two eyes, and on making the opponent's groups die, by denying them two eyes.
Reading.
Determining ahead of time whether a group is currently alive, dead, or unsettled, requires the ability to extrapolate from the current position and imagine possible plays by both sides, the best responses to those plays, the best responses to those responses, and so on. This is called "reading ahead", or just "reading", and it is a skill that grows with experience. Many players study books of life and death problems to increase their skill at reading more and more complicated positions.
In general, go players refer to analysis of positions as "reading". One major purpose of reading is to be sure that a local position can be neglected for a while. For instance, a player may be able to make gains by playing for a certain patch of territory. Yet, this play may be worth only a few points, and thus deemed unnecessary, depending on the state of the game. With confidence in one's reading, it becomes much easier to set priorities and switch around the board (see sente). Not changing gears at the correct time can be a loss of opportunity.
High and low.
In order to build a harmonious position, usually in the opening, one does not place all stones on the third line (for territory), nor all stones on the fourth line (for center influence).
"Harmonious" as used here is not just an aesthetic quality; rather, it stands for a balance in the overall ("strategic") connection of the stones.
Thickness and lightness.
An outward-facing position that cannot be attacked, that is, one which can easily get two eyes or connect to a friendly live group and so does not need to answer enemy moves close by, is called "thick". Thick positions are important as they radiate influence across the board. An error that is often made by weaker players is to make territory in front of their thick position; this is inefficient because the player is likely to get that territory anyway. Doing so is also inflexible strategically, so invites enemy forcing moves at the border of the incomplete territory. Thickness is better used from a distance, as support for other actions. For example, if Black has a thick group and a weak group nearby, and White attacks the weak group, Black can have its weak group run towards its thick group. If successful, the strength of the thick group will protect the weak group. Or, if White tries to invade near a thick group, Black can try to push White towards its thick group. If Black is successful, the strength of the thick group may help destroy the invasion. Even if the invaders are not killed, the pressure exerted by the thick position can allow Black to profit from the attack, for example gaining territory or thickness in a neighbouring area whilst chasing the weak stones. A thick group can also support invasion of enemy spheres of influence.
A "light" group is also one that is hard to attack, but for a different reason. If a group has a large number of options, often including the sacrifice of part of it, it is called light. Because it is usually impossible to take away all or almost all options, attacking such a group is very hard for the opponent and brings little advantage. A weak group which is too important to sacrifice is called "heavy".
Attack and defense.
A large part of the middle game of a game of Go may be spent by one player attacking the other player's weak group(s). What is important to remember is that in most cases the goal of an attack is not to kill the attacked group, but to gain territory or influence. The attack is more or less used to restrict the opponent's options and make it impossible for her/him to make territory or influence.
Territory and influence.
See Go concepts
Ko fighting.
Suppose that Black begins a "ko" by taking a stone of White's. White cannot immediately recapture; the rules state that white must, for the moment, play elsewhere. White may believe that good strategy requires he/she eventually recapture, but Black, on his/her next turn, will have the option of ending the "ko", for example by filling in the spot White would use to recapture. To prevent Black from doing this, White can play a "ko threat".
A "ko" threat is a move that forces one's opponent to respond, or risk a punishing blow. A "ko" threat by White will force Black to choose between responding to the threat, and allowing White to recapture (thereby continuing the "ko"), or ending the ko, but having a damaged, poor position elsewhere on the board.
A good "ko" threat should threaten more damage to one's opponent's position than the value of losing the ko. If the ko is then "lost", the trade was still beneficial.
"Sente" and "gote".
Sente and 'gote' are complementary terms. Sente loosely corresponds to taking the initiative, and gote loosely corresponds to the responsibility of defense. If a move forces the other player to respond, that player "has gote"; if not, he/she can play elsewhere, "taking sente." The player who holds "sente" more often in effect controls the flow of the game.
'Taking gote unnecessarily' would mean that one had defended for oneself a smaller area of the board than one could have threatened to take from the opponent, elsewhere. Very few plays in a game are really forcing — the opponent may well ignore you. If your play was 'really' "sente", you expect to gain by following it up, as soon as possible.
The act of playing elsewhere (in other words, breaking off from a local exchange of plays in one area of the board) is called "tenuki". It may indicate either a natural pause in the sequence, or a disagreement as to the importance of an area of the board. Between strong players "tenuki" may be used as a kind of gambit. Because the Go board is so spacious, the balance between attack and defense, and amongst different areas, holds great importance for strategy.
Direction of Play.
The direction of play is a higher level concept in the opening, relating to the efficiency of stones played on the board. This involves the important trade-off between overconcentration ("korigatachi") and vagueness - between playing a move that accomplishes too little and a move that tries to accomplish too much. Additionally, the stones already played are taken into consideration. The players aim not only at making efficient new moves, but also at playing moves that heighten the value of their previous moves, and at achieving maximum harmony in a global sense. One must strive to make each stone played important and valuable, that is, utilize its aji to the fullest extent possible. Thus, choosing the correct direction of play requires not only a deep understanding of the value of thickness, but also demands a good sense of positional judgment from the players.
Endgame (yose).
The endgame begins when large-scale contention over life and death, invasion, reduction etc. ends. Players then set about maximizing the boundaries of their territories while minimizing the opponent's territory. One must choose which of these moves is more urgent to play based not only on the points it may gain, but on whether that move is sente. Generally, in the endgame, all the major areas are staked out— however, there are still points to be made, as well as possible ways to reduce the opponent's territory. A simple example would be a move that is dame (neutral point for you), but when filled in, it is sente, requiring white to fill a stone in his territory to answer. It would be thus said this is 'a one-point reduction in sente.'

</doc>
<doc id="48980" url="https://en.wikipedia.org/wiki?curid=48980" title="Basidiomycota">
Basidiomycota

Basidiomycota () is one of two large phyla that, together with the Ascomycota, constitute the subkingdom Dikarya (often referred to as the "higher fungi") within the kingdom Fungi.
More specifically the Basidiomycota include these groups: mushrooms, puffballs, stinkhorns, bracket fungi, other polypores, jelly fungi, boletes, chanterelles, earth stars, smuts, bunts, rusts, mirror yeasts, and the human pathogenic yeast "Cryptococcus".
Basidiomycota are filamentous fungi composed of hyphae (except for yeasts), and reproduce sexually via the formation of specialized club-shaped end cells called basidia that normally bear external meiospores (usually four). These specialized spores are called basidiospores. However, some Basidiomycota reproduce asexually in addition or exclusively. Basidiomycota that reproduce asexually (discussed below) can be recognized as members of this phylum by gross similarity to others, by the formation of a distinctive anatomical feature (the clamp connection - see below), cell wall components, and definitively by phylogenetic molecular analysis of DNA sequence data.
Classification.
The most recent classification adopted by a coalition of 67 mycologists recognizes three subphyla (Pucciniomycotina, Ustilaginomycotina, Agaricomycotina) and two other class level taxa (Wallemiomycetes, Entorrhizomycetes) outside of these, among the Basidiomycota. As now classified, the subphyla join and also cut across various obsolete taxonomic groups (see below) previously commonly used to describe Basidiomycota. According to a 2008 estimate, Basidiomycota comprise three subphyla (including six unassigned classes) 16 classes, 52 orders, 177 families, 1,589 genera, and 31,515 species.
Traditionally, the Basidiomycota were divided into two classes, now obsolete:
Previously the entire Basidiomycota were called Basidiomycetes, an invalid class level name coined in 1959 as a counterpart to the Ascomycetes, when neither of these taxa were recognized as phyla. The terms basidiomycetes and ascomycetes are frequently used loosely to refer to Basidiomycota and Ascomycota. They are often abbreviated to "basidios" and "ascos" as mycological slang.
Agaricomycotina.
The Agaricomycotina include what had previously been called the Hymenomycetes (an obsolete morphological based class of Basidiomycota that formed hymenial layers on their fruitbodies), the Gasteromycetes (another obsolete class that included species mostly lacking hymenia and mostly forming spores in enclosed fruitbodies), as well as most of the jelly fungi. The three classes in the Agaricomycotina are the Agaricomycetes, the Dacrymycetes, and the Tremellomycetes.
The class Wallemiomycetes is not yet placed in a subphylum, but recent genomic evidence suggests that it is a sister group of Agaricomycotina.
Pucciniomycotina.
The Pucciniomycotina include the rust fungi, the insect parasitic/symbiotic genus "Septobasidium", a former group of smut fungi (in the Microbotryomycetes, which includes mirror yeasts), and a mixture of odd, infrequently seen, or seldom recognized fungi, often parasitic on plants. The eight classes in the Pucciniomycotina are Agaricostilbomycetes, Atractiellomycetes, Classiculomycetes, Cryptomycocolacomycetes, Cystobasidiomycetes, Microbotryomycetes, Mixiomycetes, and Pucciniomycetes.
Ustilaginomycotina.
The Ustilaginomycotina are most (but not all) of the former smut fungi and the Exobasidiales. The classes of the Ustilaginomycotina are the Exobasidiomycetes, the Entorrhizomycetes, and the Ustilaginomycetes.
Typical life-cycle.
Unlike higher animals and plants which have readily recognizable male and female counterparts, Basidiomycota (except for the Rust (Pucciniales)) tend to have mutually indistinguishable, compatible haploids which are usually mycelia being composed of filamentous hyphae. Typically haploid Basidiomycota mycelia fuse via plasmogamy and then the compatible nuclei migrate into each other's mycelia and pair up with the resident nuclei. Karyogamy is delayed, so that the compatible nuclei remain in pairs, called a dikaryon. The hyphae are then said to be dikaryotic. Conversely, the haploid mycelia are called monokaryons. Often, the dikaryotic mycelium is more vigorous than the individual monokaryotic mycelia, and proceeds to take over the substrate in which they are growing. The dikaryons can be long-lived, lasting years, decades, or centuries. "The monokaryons are neither male nor female". They have either a bipolar (unifactorial) or a tetrapolar (bifactorial) mating system. This results in the fact that following meiosis, the resulting haploid basidiospores and resultant monokaryons, have nuclei that are compatible with 50% (if bipolar) or 25% (if tetrapolar) of their sister basidiospores (and their resultant monokaryons) because the mating genes must differ for them to be compatible. However, there are many variations of these genes in the population, and therefore, over 90% of (monokaryons) are compatible with each other. It is as if there were multiple sexes.
The maintenance of the dikaryotic status in dikaryons in many Basidiomycota is facilitated by the formation of clamp connections that physically appear to help coordinate and re-establish pairs of compatible nuclei following synchronous mitotic nuclear divisions. Variations are frequent and multiple. In a typical Basidiomycota lifecycle the long lasting dikaryons periodically (seasonally or occasionally) produce basidia, the specialized usually club-shaped end cells, in which a pair of compatible nuclei fuse (karyogamy) to form a diploid cell. Meiosis follows shortly with the production of 4 haploid nuclei that migrate into 4 external, usually apical basidiospores. Variations occur, however. Typically the basidiospores are ballistic, hence they are sometimes also called ballistospores. In most species, the basidiospores disperse and each can start a new haploid mycelium, continuing the lifecycle. Basidia are microscopic but they are often produced on or in multicelled large fructifications called basidiocarps or basidiomes, or fruitbodies), variously called mushrooms, puffballs, etc. Ballistic basidiospores are formed on sterigmata which are tapered spine-like projections on basidia, and are typically curved, like the horns of a bull. In some Basidiomycota the spores are not ballistic, and the sterigmata may be straight, reduced to stubbs, or absent. The basidiospores of these non-ballistosporic basidia may either bud off, or be released via dissolution or disintegration of the basidia.
In summary, meiosis takes place in a diploid basidium. Each one of the four haploid nuclei migrates into its own basidiospore. The basidiospores are ballistically discharged and start new haploid mycelia called monokaryons. There are no males or females, rather there are compatible thalli with multiple compatibility factors. Plasmogamy between compatible individuals leads to delayed karyogamy leading to establishment of a dikaryon. The dikaryon is long lasting but ultimately gives rise to either fruitbodies with basidia or directly to basidia without fruitbodies. The paired dikaryon in the basidium fuse (i.e. karyogamy takes place). The diploid basidium begins the cycle again.
Meiosis.
"Coprinopsis cinerea" is a multicellular basidiomycete mushroom. It is particularly suited to the study of meiosis because meiosis progresses synchronously in about 10 million cells within the mushroom cap, and the meiotic prophase stage is prolonged. Burns et al. studied the expression of genes involved in the 15-hour meiotic process, and found that the pattern of gene expression of "C. cinerea" was similar to two other fungal species, the yeasts "Saccharomyces cerevisiae" and "Schizosaccharomyces pombe". These similarities in the patterns of expression led to the conclusion that the core expression program of meiosis has been conserved in these fungi for over half a billion years of evolution since these species diverged.
"Cryptococcus neoformans" and "Ustilago maydis" are examples of pathogenic basidiomycota. Such pathogens must be able to overcome the oxidative defenses of their respective hosts in order to produce a successful infection. The ability to undergo meiosis may provide a survival benefit for these fungi by promoting successful infection. A characteristic central feature of meiosis is recombination between homologous chromosomes. This process is associated with repair of DNA damages, particularly double-strand breaks. The ability of "C. neoformans" and "U. maydis" to undergo meiosis may contribute to their virulence by removing the oxidative DNA damages caused by their host’s release of reactive oxygen species.
Variations in lifecycles.
Many variations occur. Some are self-compatible and spontaneously form dikaryons without a separate compatible thallus being involved. These fungi are said to be homothallic, versus the normal heterothallic species with mating types. Others are secondarily homothallic, in that two compatible nuclei following meiosis migrate into each basidiospore, which is then dispersed as a pre-existing dikaryon. Often such species form only two spores per basidium, but that too varies. Following meiosis, mitotic divisions can occur in the basidium. Multiple numbers of basidiospores can result, including odd numbers via degeneration of nuclei, or pairing up of nuclei, or lack of migration of nuclei. For example, the chanterelle genus "Craterellus" often has six-spored basidia, while some corticioid "Sistotrema" species can have two-, four-, six-, or eight-spored basidia, and the cultivated button mushroom, "Agaricus bisporus". can have one-, two-, three- or four-spored basidia under some circumstances. Occasionally, monokaryons of some taxa can form morphologically fully formed basidiomes and anatomically correct basidia and ballistic basidiospores in the absence of dikaryon formation, diploid nuclei, and meiosis. A rare few number of taxa have extended diploid lifecycles, but can be common species. Examples exist in the mushroom genera "Armillaria" and "Xerula", both in the Physalacriaceae. Occasionally, basidiospores are not formed and parts of the "basidia" act as the dispersal agents, e.g. the peculiar mycoparasitic jelly fungus, "Tetragoniomyces" or the entire "basidium" acts as a "spore", e.g. in some false puffballs ("Scleroderma"). In the human pathogenic genus "Cryptococcus", four nuclei following meiosis remain in the basidium, but continually divide mitotically, each nucleus migrating into synchronously forming nonballistic basidiospores that are then pushed upwards by another set forming below them, resulting in four parallel chains of dry "basidiospores".
Other variations occur, some as standard lifecycles (that themselves have variations within variations) within specific orders.
Rusts.
Rusts (Pucciniales, previously known as Uredinales) at their greatest complexity, produce five different types of spores on two different host plants in two unrelated host families. Such rusts are heteroecious (requiring two hosts) and macrocyclic (producing all five spores types). Wheat stem rust is an example. By convention, the stages and spore states are numbered by Roman numerals. Typically, basidiospores infect host one, also known as the alternate or sexual host, the mycelium forms pycnidia, which are miniature, flask-shaped, hollow, submicroscopic bodies embedded in host tissue (such as a leaf). This stage, numbered "0", produces single-celled spores that ooze out in a sweet liquid and that act as nonmotile spermatia, and also protruding receptive hyphae. Insects and probably other vectors such as rain carry the spermatia from spermagonia to spermagonia, cross inoculating the mating types. Neither thallus is male or female. Once crossed, the dikaryons are established and a second spore stage is formed, numbered "I" and called aecia, which form dikaryotic aeciospores in dry chains in inverted cup-shaped bodies embedded in host tissue. These aeciospores then infect the second host, known as the primary or asexual host (in macrocyclic rusts). On the primary host a repeating spore stage is formed, numbered "II", the urediospores in dry pustules called uredinia. Urediospores are dikaryotic and can infect the same host that produced them. They repeatedly infect this host over the growing season. At the end of the season, a fourth spore type, the teliospore, is formed. It is thicker-walled and serves to overwinter or to survive other harsh conditions. It does not continue the infection process, rather it remains dormant for a period and then germinates to form basidia (stage "IV"), sometimes called a promycelium. In the Pucciniales, the basidia are cylindrical and become 3-septate after meiosis, with each of the 4 cells bearing one basidiospore each. The basidospores disperse and start the infection process on host 1 again. Autoecious rusts complete their life-cycles on one host instead of two, and microcyclic rusts cut out one or more stages.
Smuts.
The characteristic part of the life-cycle of smuts is the thick-walled, often darkly pigmented, ornate, teliospore that serves to survive harsh conditions such as overwintering and also serves to help disperse the fungus as dry diaspores. The teliospores are initially dikaryotic but become diploid via karyogamy. Meiosis takes place at the time of germination. A promycelium is formed that consists of a short hypha (equated to a basidium). In some smuts such as "Ustilago maydis" the nuclei migrate into the promycelium that becomes septate (i.e., divided into cellular compartments separated by cell walls called "septa"), and haploid yeast-like conidia/basidiospores sometimes called sporidia, bud off laterally from each cell. In various smuts, the yeast phase may proliferate, or they may fuse, or they may infect plant tissue and become hyphal. In other smuts, such as "Tilletia caries", the elongated haploid basidiospores form apically, often in compatible pairs that fuse centrally resulting in "H"-shaped diaspores which are by then dikaryotic. Dikaryotic conidia may then form. Eventually the host is infected by infectious hyphae. Teliospores form in host tissue. Many variations on these general themes occur.
Smuts with both a yeast phase and an infectious hyphal state are examples of dimorphic Basidiomycota. In plant parasitic taxa, the saprotrophic phase is normally the yeast while the infectious stage is hyphal. However, there are examples of animal and human parasites where the species are dimorphic but it is the yeast-like state that is infectious. The genus "Filobasidiella" forms basidia on hyphae but the main infectious stage is more commonly known by the anamorphic yeast name "Cryptococcus", e.g. "Cryptococcus neoformans" and "Cryptococcus gattii".
The dimorphic Basidiomycota with yeast stages and the pleiomorphic rusts are examples of fungi with anamorphs, which are the asexual stages. Some Basidiomycota are only known as anamorphs. Many are yeasts, collectively called basidiomycetous yeasts to differentiate them from ascomycetous yeasts in the Ascomycota. Aside from yeast anamorphs, and uredinia, aecia and pycnidia, some Basidiomycota form other distinctive anamorphs as parts of their life-cycles. Examples are "Collybia tuberosa" with its apple-seed-shaped and coloured sclerotium, "Dendrocollybia racemosa" with its sclerotium and its "Tilachlidiopsis racemosa" conidia, "Armillaria" with their rhizomorphs, "Hohenbuehelia" with their "Nematoctonus" nematode infectious, state and the coffee leaf parasite, "Mycena citricolor" and its "Decapitatus flavidus" propagules called gemmae.

</doc>
<doc id="48981" url="https://en.wikipedia.org/wiki?curid=48981" title="Ascomycota">
Ascomycota

Ascomycota is a division or phylum of the kingdom Fungi that, together with the Basidiomycota, form the subkingdom Dikarya. Its members are commonly known as the sac fungi or ascomycetes. They are the largest phylum of Fungi, with over 64,000 species. The defining feature of this fungal group is the "ascus" (from Greek: ("askos"), meaning "sac" or "wineskin"), a microscopic sexual structure in which nonmotile spores, called ascospores, are formed. However, some species of the Ascomycota are asexual, meaning that they do not have a sexual cycle and thus do not form asci or ascospores. Previously placed in the Deuteromycota along with asexual species from other fungal taxa, asexual (or anamorphic) ascomycetes are now identified and classified based on morphological or physiological similarities to ascus-bearing taxa, and by phylogenetic analyses of DNA sequences.
The ascomycetes are a monophyletic group, i.e. it contains all descendants of one common ancestor. This group is of particular relevance to humans as sources for medicinally important compounds, such as antibiotics and for making bread, alcoholic beverages, and cheese, but also as pathogens of humans and plants. Familiar examples of sac fungi include morels, truffles, brewer's yeast and baker's yeast, dead man's fingers, and cup fungi. The fungal symbionts in the majority of lichens (loosely termed "ascolichens") such as "Cladonia" belong to the Ascomycota. There are many plant-pathogenic ascomycetes, including apple scab, rice blast, the ergot fungi, black knot, and the powdery mildews. Several species of ascomycetes are biological model organisms in laboratory research. Most famously, "Neurospora crassa", several species of yeasts, and "Aspergillus" species are used in many genetics and cell biology studies. "Penicillium" species on cheeses and those producing antibiotics for treating bacterial infectious diseases are examples of taxa that belong to the Ascomycota.
Asexual Reproduction in Ascomycetes and their Characteristics.
Ascomycetes:
Ascomycetes are 'spore shooters'. They are fungi which produce microscopic spores inside special, elongated cells or sacs, known as 'asci', which give the group its name.
Asexual Reproduction:
Asexual reproduction is the dominant form of propagation in the Ascomycota, and is responsible for the rapid spread of these fungi into new areas. Asexual reproduction of ascomycetes is very diverse from both structural and functional points of view. The most important and general is production of conidia, but chlamydospores are also frequently produced. Furthermore, Ascomycota also reproduce asexually through budding.
1) Conidia Formation:
Asexual reproduction may occur through vegetative reproductive spores, the conidia. Asexual, non-motile haploid spore of a fungus, which is named after the Greek word for dust; conia and hence also known as conidiospores and mitospores. The conidiospores commonly contain one nucleus and are products of mitotic cell divisions and thus are sometimes call mitospores, which are genetically identical to the mycelium from which they originate. They are typically formed at the ends of specialized hyphae, the conidiophores. Depending on the species they may be dispersed by wind or water, or by animals. Conidiophores may simply branch off from the mycelia or they may be formed in fruiting bodies.
The hypha that creates the sporing (conidiating) tip can be very similar to the normal hyphal tip, or it can be differentiated. The most common differentiation is the formation of a bottle shaped cell called a phialide, from which the spores are produced. As all of these asexual structures are not single hyphae. In some groups, the conidiophores (the structures that bear the conidia) are aggregated to form a thick structure.
E.g. In the order "Moniliales," all of them are single hyphae with the exception of the aggregations, termed as coremia or synnema. These produce structures rather like corn-stokes, with many conidia being produced in a mass from the aggregated conidiophores.
The diverse conidia and conidiophores sometimes develop in asexual sporocarps with different characteristics (e.g. aecervulus, pycnidium, sporodochium). Some species of "Ascomycetes" form their structures within plant tissue, either as parasite or saprophytes. These fungi have evolved more complex asexual sporing structures, probably influenced by the cultural conditions of plant tissue as a substrate. These structures are called the sporodochium. This is a cushion of conidiophores created from a pseudoparenchymatous stoma in plant tissue. The pycnidium is a globose to flask-shaped parenchymatous structure, lined on its inner wall with conidiophores. The acervulus is a flat saucer shaped bed of conidiophores produced under a plant cuticle, which eventually erupt through the cuticle for dispersal.
2) Budding:
Asexual reproduction process in ascomycetes also involves the budding which we clearly observe in yeast. This is termed a “blastic process”. It involves the blowing out or blebbing of the hyphal tip wall. The blastic process can involve all wall layers, or there can be a new cell wall synthesized which is extruded from within the old wall.
The initial events of budding can be seen as the development of a ring of chitin around the point where the bud is about to appear. This reinforces and stabilizes the cell wall. Enzymatic activity and turgor pressure act to weaken and extrude the cell wall. New cell wall material is incorporated during this phase. Cell contents are forced into the progeny cell, and as the final phase of mitosis ends a cell plate, the point at which a new cell wall will grow inwards from, forms.
Characteristics Of Ascomycetes:
· Ascomycota are morphologically diverse. The group includes organisms from unicellular yeasts to complex cup fungi.
· There are 2000 identified genera and 30,000 species of Ascomycota.
· The unifying characteristic among these diverse groups is the presence of a reproductive structure known as the ascus, though in some cases it has a reduced role in the life cycle.
· Many ascomycetes are of commercial importance. Some play a beneficial role, such as the yeasts used in baking, brewing, and wine fermentation, plus truffles and morels, which are held as gourmet delicacies.
· Many of them cause tree diseases, such a
s Dutch elm disease and apple blights.
· Some of the plant pathogenic ascomycetes are apple scab, rice blast, the ergot fungi, black knot, and the powdery mildews.
· The yeasts are used to produce alcoholic beverages and breads. The mold Penicillium is used to produce the anti-biotic penicillin.
· Almost half of all members of the phylum Ascomycota form symbiotic associations with algae to form lichens.
· Others, such as morels (a highly prized edible fungi), form important mychorrhizal relationships with plants, thereby providing enhanced water and nutrient uptake and, in some cases, protection from insects.
· Almost all ascomycetes are terrestrial or parasitic. However, a few have adapted to marine or freshwater environments.
· The cell walls of the hyphae are variably composed of chitin and β-glucans, just as in Basidiomycota. However, these fibers are set in a matrix of glycoprotein containing the sugars galactose and mannose.
· The mycelium of ascomycetes is usually made up of septate hyphae. However, there is not necessarily any fixed number of nuclei in each of the divisions.
· The septal walls have septal pores which provide cytoplasmic continuity throughout the individual hyphae. Under appropriate conditions, nuclei may also migrate between septal compartments through the septal pores.
· A unique character of the Ascomycota (but not present in all ascomycetes) is the presence of Woronin bodies on each side of the septa separating the hyphal segments which control the septal pores. If an adjoining hypha is ruptured, the Woronin bodies block the pores to prevent loss of cytoplasm into the ruptured compartment. The Woronin bodies are spherical, hexagonal, or rectangular membrane bound structures with a crystalline protein matrix.
Modern classification of Ascomycota.
There are three subphyla that are described and accepted:
Outdated taxon names.
Several outdated taxon names—based on morphological features—are still occasionally used for species of the Ascomycota. These include the following sexual (teleomorphic) groups, defined by the structures of their sexual fruiting bodies: the Discomycetes, which included all species forming apothecia; the Pyrenomycetes, which included all sac fungi that formed perithecia or pseudothecia, or any structure resembling these morphological structures; and the Plectomycetes, which included those species that form cleistothecia. Hemiascomycetes included the yeasts and yeast-like fungi that have now been placed into the Saccharomycotina or Taphrinomycotina, while the Euascomycetes included the remaining species of the Ascomycota, which are now in the Pezizomycotina, and the Neolecta, which are in the Taphrinomycotina.
Some ascomycetes do not reproduce sexually or are not known to produce asci and are therefore anamorphic species. Those anamorphs that produce conidia (mitospores) were previously described as Mitosporic Ascomycota. Some taxonomists placed this group into a separate artificial phylum, the Deuteromycota (or "Fungi Imperfecti"). Where recent molecular analyses have identified close relationships with ascus-bearing taxa, anamorphic species have been grouped into the Ascomycota, despite the absence of the defining ascus. Sexual and asexual isolates of the same species commonly carry different binomial species names, as, for example, "Aspergillus nidulans" and "Emericella nidulans", for asexual and sexual isolates, respectively, of the same species.
Species of the Deuteromycota were classified as Coelomycetes if they produced their conidia in minute flask- or saucer-shaped conidiomata, known technically as "pycnidia" and "acervuli". The Hyphomycetes were those species where the conidiophores ("i.e.", the hyphal structures that carry conidia-forming cells at the end) are free or loosely organized. They are mostly isolated but sometimes also appear as bundles of cells aligned in parallel (described as "synnematal") or as cushion-shaped masses (described as "sporodochial").
Morphology.
Most species grow as filamentous, microscopic structures called hyphae. Many interconnected hyphae form a mycelium, which—when visible to the naked eye (macroscopic)—is commonly called mold (or, in botanical terminology, thallus). During sexual reproduction, many Ascomycota typically produce large numbers of asci. The asci is often contained in a multicellular, occasionally readily visible fruiting structure, the ascocarp (also called an "ascoma"). Ascocarps come in a very large variety of shapes: cup-shaped, club-shaped, potato-like, spongy, seed-like, oozing and pimple-like, coral-like, nit-like, golf-ball-shaped, perforated tennis ball-like, cushion-shaped, plated and feathered in miniature (Laboulbeniales), microscopic classic Greek shield-shaped, stalked or sessile. They can appear solitary or clustered. Their texture can likewise be very variable, including fleshy, like charcoal (carbonaceous), leathery, rubbery, gelatinous, slimy, powdery, or cob-web-like. Ascocarps come in multiple colors such as red, orange, yellow, brown, black, or, more rarely, green or blue. Some ascomyceous fungi, such as "Saccharomyces cerevisiae", grow as single-celled yeasts, which—during sexual reproduction—develop into an ascus, and do not form fruiting bodies.
In lichenized species, the thallus of the fungus defines the shape of the symbiotic colony. Some dimorphic species, such as "Candida albicans", can switch between growth as single cells and as filamentous, multicellular hyphae. Other species are pleomorphic, exhibiting asexual (anamorphic) as well as a sexual (teleomorphic) growth forms.
Except for lichens, the non-reproductive (vegetative) mycelium of most ascomycetes is usually inconspicuous because it is commonly embedded in the substrate, such as soil, or grows on or inside a living host, and only the ascoma may be seen when fruiting. Pigmentation, such as melanin in hyphal walls, along with prolific growth on surfaces can result in visible mold colonies; examples include "Cladosporium" species, which form black spots on bathroom caulking and other moist areas. Many ascomycetes cause food spoilage, and, therefore, the pellicles or moldy layers that develop on jams, juices, and other foods are the mycelia of these species or occasionally Mucoromycotina and almost never Basidiomycota. Sooty molds that develop on plants, especially in the tropics are the thalli of many species.
Large masses of yeast cells, asci or ascus-like cells, or conidia can also form macroscopic structures. For example. "Pneumocystis" species can colonize lung cavities (visible in x-rays), causing a form of pneumonia. Asci of "Ascosphaera" fill honey bee larvae and pupae causing mummification with a chalk-like appearance, hence the name "chalkbrood". Yeasts for small colonies in vitro and in vivo, and excessive growth of "Candida" species in the mouth or vagina causes "thrush", a form of candidiasis.
The cell walls of the ascomycetes almost always contain chitin and β-glucans, and divisions within the hyphae, called "septa", are the internal boundaries of individual cells (or compartments). The cell wall and septa give stability and rigidity to the hyphae and may prevent loss of cytoplasm in case of local damage to cell wall and cell membrane. The septa commonly have a small opening in the center, which functions as a cytoplasmic connection between adjacent cells, also sometimes allowing cell-to-cell movement of nuclei within a hypha. Vegetative hyphae of most ascomycetes contain only one nucleus per cell ("uninucleate" hyphae), but multinucleate cells—especially in the apical regions of growing hyphae—can also be present.
Metabolism.
In common with other fungal phyla, the Ascomycota are heterotrophic organisms that require organic compounds as energy sources. These are obtained by feeding on a variety of organic substrates including dead matter, foodstuffs, or as symbionts in or on other living organisms. To obtain these nutrients from their surroundings, ascomycetous fungi secrete powerful digestive enzymes that break down organic substances into smaller molecules, which are then taken up into the cell. Many species live on dead plant material such as leaves, twigs, or logs. Several species colonize plants, animals, or other fungi as parasites or mutualistic symbionts and derive all their metabolic energy in form of nutrients from the tissues of their hosts.
Owing to their long evolutionary history, the Ascomycota have evolved the capacity to break down almost every organic substance. Unlike most organisms, they are able to use their own enzymes to digest plant biopolymers such as cellulose or lignin. Collagen, an abundant structural protein in animals, and keratin—a protein that forms hair and nails—, can also serve as food sources. Unusual examples include "Aureobasidium pullulans", which feeds on wall paint, and the kerosene fungus "Amorphotheca resinae", which feeds on aircraft fuel (causing occasional problems for the airline industry), and may sometimes block fuel pipes. Other species can resist high osmotic stress and grow, for example, on salted fish, and a few ascomycetes are aquatic.
The Ascomycota is characterized by a high degree of specialization; for instance, certain species of Laboulbeniales attack only one particular leg of one particular insect species. Many Ascomycota engage in symbiotic relationships such as in lichens—symbiotic associations with green algae or cyanobacteria—in which the fungal symbiont directly obtains products of photosynthesis. In common with many basidiomycetes and Glomeromycota, some ascomycetes form symbioses with plants by colonizing the roots to form mycorrhizal associations. The Ascomycota also represents several carnivorous fungi, which have developed hyphal traps to capture small protists such as amoebae, as well as roundworms ("Nematoda"), rotifers, tardigrades, and small arthropods such as springtails ("Collembola").
Distribution and living environment.
The Ascomycota are represented in all land ecosystems worldwide, occurring on all continents including Antarctica. Spores and hyphal fragments are dispersed through the atmosphere and freshwater environments, as well as ocean beaches and tidal zones. The distribution of species is variable; while some are found on all continents, others, as for example the white truffle "Tuber magnatum", only occur in isolated locations in Italy and Eastern Europe. The distribution of plant-parasitic species is often restricted by host distributions; for example, "Cyttaria" is only found on "Nothofagus" (Southern Beech) in the Southern Hemisphere.
Reproduction.
Asexual reproduction.
Asexual reproduction is the dominant form of propagation in the Ascomycota, and is responsible for the rapid spread of these fungi into new areas. It occurs through vegetative reproductive spores, the conidia. The conidiospores commonly contain one nucleus and are products of mitotic cell divisions and thus are sometimes called mitospores, which are genetically identical to the mycelium from which they originate. They are typically formed at the ends of specialized hyphae, the "conidiophores". Depending on the species they may be dispersed by wind or water, or by animals.
Asexual spores.
Different types of asexual spores can be identified by colour, shape, and how they are released as individual spores. Spore types can be used as taxonomic characters in the classification within the Ascomycota. The most frequent types are the single-celled spores, which are designated "amerospores". If the spore is divided into two by a cross-wall (septum), it is called a "didymospore".
When there are two or more cross-walls, the classification depends on spore shape. If the septae are "transversal", like the rungs of a ladder, it is a "phragmospore", and if they possess a net-like structure it is a "dictyospore". In "staurospores" ray-like arms radiate from a central body; in others ("helicospores") the entire spore is wound up in a spiral like a spring. Very long worm-like spores with a length-to-diameter ratio of more than 15:1, are called "scolecospores".
Conidiogenesis and dehiscence.
Important characteristics of the anamorphs of the Ascomycota are "conidiogenesis", which includes spore formation and dehiscence (separation from the parent structure). Conidiogenesis corresponds to Embryology in animals and plants and can be divided into two fundamental forms of development: "blastic" conidiogenesis, where the spore is already evident before it separates from the conidiogenic hypha, and "thallic" conidiogenesis, during which a cross-wall forms and the newly created cell develops into a spore. The spores may or may not be generated in a large-scale specialized structure that helps to spread them.
These two basic types can be further classified as follows:
Sometimes the conidia are produced in structures visible to the naked eye, which help to distribute the spores. These structures are called "conidiomata" (singular: conidioma), and may take the form of "pycnidia" (which are flask-shaped and arise in the fungal tissue) or "acervuli" (which are cushion-shaped and arise in host tissue).
Dehiscence happens in two ways. In "schizolytic" dehiscence, a double-dividing wall with a central lamella (layer) forms between the cells; the central layer then breaks down thereby releasing the spores. In "rhexolytic" dehiscence, the cell wall that joins the spores on the outside degenerates and releases the conidia.
Heterokaryosis and parasexuality.
Several Ascomycota species are not known to have a sexual cycle. Such asexual species may be able to undergo genetic recombination between individuals by processes involving "heterokaryosis" and "parasexual" events.
Parasexuality refers to the process of heterokaryosis, caused by merging of two hyphae belonging to different individuals, by a process called "anastomosis", followed by a series of events resulting in genetically different cell nuclei in the mycelium.
The merging of nuclei is not followed by meiotic events, such as gamete formation and results in an increased number of chromosomes per nuclei. "Mitotic crossover" may enable recombination, i.e., an exchange of genetic material between homologous chromosomes. The chromosome number may then be restored to its haploid state by nuclear division, with each daughter nuclei being genetically different from the original parent nuclei. Alternatively, nuclei may lose some chromosomes, resulting in aneuploid cells. "Candida albicans" (class Saccharomycetes) is an example of a fungus that has a parasexual cycle (see Candida albicans and Parasexual cycle).
Sexual reproduction.
Sexual reproduction in the Ascomycota leads to the formation of the "ascus", the structure that defines this fungal group and distinguishes it from other fungal phyla. The ascus is a tube-shaped vessel, a "meiosporangium", which contains the sexual spores produced by meiosis and which are called "ascospores".
Apart from a few exceptions, such as "Candida albicans", most ascomycetes are haploid, i.e., they contain one set of chromosomes per nuclei. During sexual reproduction there is a diploid phase, which commonly is very short, and meiosis restores the haploid state. The sexual cycle of one well-studied representative species of Ascomycota is described in greater detail in Neurospora crassa.
Formation of sexual spores.
The sexual part of the life cycle commences when two hyphal structures mate. In the case of "homothallic" species, mating is enabled between hyphae of the same fungal clone, whereas in "heterothallic" species, the two hyphae must originate from fungal clones that differ genetically, i.e., those that are of a different mating type. Mating types are typical of the fungi and correspond roughly to the sexes in plants and animals; however one species may have more than two mating types, resulting in sometimes complex vegetative incompatibility systems. The adaptive function of mating type is discussed in Neurospora crassa.
Gametangia are sexual structures formed from hyphae, and are the generative cells. A very fine hypha, called trichogyne emerges from one gametangium, the "ascogonium", and merges with a gametangium (the "antheridium") of the other fungal isolate. The nuclei in the antheridium then migrate into the ascogonium, and plasmogamy—the mixing of the cytoplasm—occurs. Unlike in animals and plants, plasmogamy is not immediately followed by the merging of the nuclei (called "karyogamy"). Instead, the nuclei from the two hyphae form pairs, initiating the "dikaryophase" of the sexual cycle, during which time the pairs of nuclei synchronously divide. Fusion of the paired nuclei leads to mixing of the genetic material and recombination and is followed by meiosis. A similar sexual cycle is present in the red algae (Rhodophyta).
From the fertilized ascogonium, "dinucleate" hyphae emerge in which each cell contains two nuclei. These hyphae are called "ascogenous" or fertile hyphae. They are supported by the vegetative mycelium containing uni– (or mono–) nucleate hyphae, which are sterile. The mycelium containing both sterile and fertile hyphae may grow into fruiting body, the "ascocarp", which may contain millions of fertile hyphae.
The sexual structures are formed in the fruiting layer of the ascocarp, the hymenium. At one end of ascogenous hyphae, characteristic U-shaped hooks develop, which curve back opposite to the growth direction of the hyphae. The two nuclei contained in the apical part of each hypha divide in such a way that the threads of their mitotic spindles run parallel, creating two pairs of genetically different nuclei. One daughter nucleus migrates close to the hook, while the other daughter nucleus locates to the basal part of the hypha. The formation of two parallel cross-walls then divides the hypha into three sections: one at the hook with one nucleus, one at the basal of the original hypha that contains one nucleus, and one that separates the U-shaped part, which contains the other two nuclei.
Fusion of the nuclei (karyogamy) takes place in the U-shaped cells in the hymenium, and results in the formation of a diploid zygote. The zygote grows into the ascus, an elongated tube-shaped or cylinder-shaped capsule. Meiosis then gives rise to four haploid nuclei, usually followed by a further mitotic division that results in eight nuclei in each ascus. The nuclei along with some cytoplasma become enclosed within membranes and a cell wall to give rise to ascospores that are aligned inside the ascus like peas in a pod. (For a general description of meiosis and its adaptive function see Meiosis and Bernstein and Bernstein).
Upon opening of the ascus, ascospores may be dispersed by the wind, while in some cases the spores are forcibly ejected form the ascus; certain species have evolved spore cannons, which can eject ascospores up to 30 cm. away. When the spores reach a suitable substrate, they germinate, form new hyphae, which restarts the fungal life cycle.
The form of the ascus is important for classification and is divided into four basic types: unitunicate-operculate, unitunicate-inoperculate, bitunicate, or prototunicate. See the article on asci for further details.
Ecology.
The Ascomycota fulfil a central role in most land-based ecosystems. They are important decomposers, breaking down organic materials, such as dead leaves and animals, and helping the detritivores (animals that feed on decomposing material) to obtain their nutrients. Ascomycetes along with other fungi can break down large molecules such as cellulose or lignin, and thus have important roles in nutrient cycling such as the carbon cycle.
The fruiting bodies of the Ascomycota provide food for many animals ranging from insects and slugs and snails ("Gastropoda") to rodents and larger mammals such as deer and wild boars.
Many ascomycetes also form symbiotic relationships with other organisms, including plants and animals.
Lichens.
Probably since early in their evolutionary history, the Ascomycota have formed symbiotic associations with green algae ("Chlorophyta"), and other types of algae and cyanobacteria. These mutualistic associations are commonly known as lichens, and can grow and persist in terrestrial regions of the earth that are inhospitable to other organisms and characterized by extremes in temperature and humidity, including the Arctic, the Antarctic, deserts, and mountaintops. While the photoautotrophic algal partner generates metabolic energy through photosynthesis, the fungus offers a stable, supportive matrix and protects cells from radiation and dehydration. Around 42% of the Ascomycota (about 18,000 species) form lichens, and almost all the fungal partners of lichens belong to the Ascomycota.
Mycorrhizal fungi and endophytes.
Members of the Ascomycota form two important types of relationship with plants: as mycorrhizal fungi and as endophytes. Mycorrhiza are symbiotic associations of fungi with the root systems of the plants, which can be of vital importance for growth and persistence for the plant. The fine mycelial network of the fungus enables the increased uptake of mineral salts that occur at low levels in the soil. In return, the plant provides the fungus with metabolic energy in the form of photosynthetic products.
Endophytic fungi live inside plants, and those that form mutualistic or commensal associations with their host, do not damage their hosts. The exact nature of the relationship between endophytic fungus and host depends on the species involved, and in some cases fungal colonization of plants can bestow a higher resistance against insects, roundworms (nematodes), and bacteria; in the case of grass endophytes the fungal symbiont produces poisonous alkaloids, which can affect the health of plant-eating (herbivorous) mammals and deter or kill insect herbivores.
Symbiotic relationships with animals.
Several ascomycetes of the genus "Xylaria" colonize the nests of leafcutter ants and other fungus-growing ants of the tribe Attini, and the fungal gardens of termites (Isoptera). Since they do not generate fruiting bodies until the insects have left the nests, it is suspected that, as confirmed in several cases of Basidiomycota species, they may be cultivated.
Bark beetles (family Scolytidae) are important symbiotic partners of ascomycetes. The female beetles transport fungal spores to new hosts in characteristic tucks in their skin, the "mycetangia". The beetle tunnels into the wood and into large chambers in which they lay their eggs. Spores released from the mycetangia germinate into hyphae, which can break down the wood. The beetle larvae then feed on the fungal mycelium, and, on reaching maturity, carry new spores with them to renew the cycle of infection. A well-known example of this is Dutch elm disease, caused by "Ophiostoma ulmi", which is carried by the European elm bark beetle, "Scolytus multistriatus".
Importance for humans.
Ascomycetes make many contributions to the good of humanity, and also have many ill effects.
Harmful interactions.
One of their most harmful roles is as the agent of many plant diseases. For instance:
Positive effects.
On the other hand, ascus fungi have brought some important benefits to humanity.

</doc>
<doc id="48985" url="https://en.wikipedia.org/wiki?curid=48985" title="History of anatomy">
History of anatomy

The history of anatomy extends from the earliest examinations of sacrificial victims to the sophisticated analyses of the body performed by modern scientists. It has been characterized, over time, by a continually developing understanding of the functions of organs and structures in the body. Human anatomy was the most prominent of the biological sciences of the 19th and early 20th centuries. Methods have also improved dramatically.
Ancient anatomy.
Egypt.
The study of anatomy begins at least as early as 1600 BC, the date of the Edwin Smith Surgical Papyrus. This treatise shows that the heart, its vessels, liver, spleen, kidneys, hypothalamus, uterus and bladder were recognized, and that the blood vessels were known to emanate from the heart. Other vessels are described, some carrying air, some mucus, and two to the right ear are said to carry the "breath of life", while two to the left ear the "breath of death".The Ebers Papyrus (c. 1550 BC) features a treatise on the heart. It notes that the heart is the center of blood supply, and attached to it are vessels for every member of the body. The Egyptians seem to have known little about the function of the kidneys and made the heart the meeting point of a number of vessels which carried all the fluids of the body – blood, tears, urine and semen. However, they did not have a theory as to where saliva and sweat came from.
Greek advances in anatomy.
Nomenclature, methods and applications for the study of anatomy all date back to the Greeks. The early scientist Alcmaeon began to construct a background for medical and anatomical science with the dissection of animals. He identified the optic nerves and the tubes later termed the Eustachius. Others such as Acron (480 BC), Pausanias (480 BC), and Philistion of Locri made investigations into anatomy. One important figure during this time was Empedocles (480B.C.) who viewed the blood as the "innate heat" which he acquired from previous folklore. He also argued that the heart was the chief organ of both the vascular system and the pneuma (this could refer to either breath or soul; it was considered to be distributed by the blood vessels).
Many medical texts by various authors are collected in the "Hippocratic Corpus", none of which can definitely be ascribed to Hippocrates himself. The texts show an understanding of musculoskeletal structure, and the beginnings of understanding of the function of certain organs, such as the kidneys. The tricuspid valve of the heart and its function is documented in the treatise "On the Heart".
In the 4th century BCE, Aristotle and several contemporaries produced a more empirically founded system, based on animal dissection. Through his work with animal dissections and evolutionary biology, Aristotle founded comparative anatomy. Around this time, Praxagoras is credited as the first to identify the difference between arteries and veins, and the relations between organs are described more accurately than in previous works.
The first recorded school of anatomy was in Alexandria from about 300 to the 2nd century BC. Ptolemy I Soter was the first to allow for medical officials to cut open and examine dead bodies for the purposes of learning how human bodies operated. On some occasions King Ptolemy even took part in these dissections. Most of the early dissections were done on executed criminals. The first use of human cadavers for anatomical research occurred later in the 4th century BCE when Herophilos and Erasistratus gained permission to perform live dissections, or vivisection, on criminals in Alexandria under the auspices of the Ptolemaic dynasty. Herophilos in particular developed a body of anatomical knowledge much more informed by the actual structure of the human body than previous works had been. Herophilos was the first physician to dissect human bodies and is considered to be the founder of Anatomy. He reversed the longstanding notion made by Aristotle that the heart was the "seat of intelligence." He argued instead that this seat was the brain. However, Herophilos was eventually accused by his contemporaries of dissecting live criminals. The number of victims is said to be around 600 prisoners.
From ancient to medieval.
Galen.
The final major anatomist of ancient times was Galen, active in the 2nd century. He compiled much of the knowledge obtained by previous writers, and furthered the inquiry into the function of organs by performing vivisection on animals. Due to a lack of readily available human specimens, discoveries through animal dissection were broadly applied to human anatomy as well. Galen served as chief physician to the gladiators in Pergamum (AD 158). Through his position with the gladiators, Galen was able to study all kinds of wounds without performing any actual human dissection. By default, Galen was able to view much of the abdominal cavity. His study on pigs and apes, however, gave him more detailed information about the organs and provided the basis for his medical tracts. Around 100 of these tracts survive and fill 22 volumes of modern text. His two great anatomical works are "On anatomical procedure" and "On the uses of the parts of the body of man". The information in these tracts became the foundation of authority for all medical writers and physicians for the next 1300 years until they were challenged by Vesalius and Harvey in the 16th century.
It was through his experiments that Galen was able to overturn many long-held beliefs, such as the theory that the arteries contained air which carried it to all parts of the body from the heart and the lungs. This belief was based originally on the arteries of dead animals, which appeared to be empty. Galen was able to demonstrate that living arteries contain blood, but in his error, which became the established medical orthodoxy for centuries, was to assume that the blood goes back and forth from the heart in an ebb-and-flow motion.
Early modern anatomy.
From the third century B.C.E until the twelfth century C.E. human anatomy was mainly learned through books and animal dissection. Human dissection became restricted after Boniface viii past a bull that forbade the dismemberment and boiling of corpses for funerary purposes. For many decades human dissection was thought unnecessary when all the knowledge about a human body could be read about from early authors such as Galen. In The twelfth century as universities were being established in Italy, Emperor Frederick ii made it mandatory for students of medicine to take courses on human anatomy and surgery. In the universities the lectern would sit elevated before the audience and instruct someone else in the dissection of the body, but in his early years Mondino de Luzzi performed the dissection himself making him one of the first and few to use a hands on approach to teaching human anatomy.
Mondino de Luzzi "Mundinus" was born around 1276 and died in 1326,from 1314 to 1324 he presented many lectures on human anatomy at Bologna university. Mondino de'Luzzi put together a book called "Anathomia" in 1316 that consisted of detailed dissections that he had performed, this book was used as a text book in universities for 250 years. "Mundinus" carried out the first systematic human dissections since Herophilus of Chalcedon and Erasistratus of Ceos 1500 years earlier. The first major development in anatomy in Christian Europe since the fall of Rome occurred at Bologna, where anatomists dissected cadavers and contributed to the accurate description of organs and the identification of their functions. Following de Liuzzi's early studies, fifteenth century anatomists included Alessandro Achillini and Antonio Benivieni Pathological anatomy
Leonardo da Vinci.
Leonardo da Vinci (1452–1519) was trained in anatomy by Andrea del Verrocchio. In 1489 Leonardo began a series of anatomical drawings depicting the ideal human form. This work was carried out intermittently for over 2 decades. During this time he made use of his anatomical knowledge in his artwork, making many sketches of skeletal structures, muscles and organs of humans and other vertebrates that he dissected. Initially adopting an Aristotlean understanding of anatomy, he later studied Galen and adopted a more empirical approach, eventually abandoning Galen altogether and relying entirely on his own direct observation. His surviving 750 drawings represent groundbreaking studies in anatomy. Leonardo dissected around thirty human specimens until he was forced to stop under order of Pope Leo X.
As an artist-anatomist, Leonardo made many important discoveries, and had intended to publish a comprehensive treatise on human anatomy. For instance, he produced the first accurate depiction of the human spine, while his notes documenting his dissection of the Florentine centenarian contain the earliest known description of cirrhosis of the liver and arteriosclerosis. He was the first to develop drawing techniques in anatomy to convey information using cross-sections and multiple angles, although centuries would pass before anatomical drawings became accepted as crucial for learning anatomy. None of Leonardo's Notebooks were published during his lifetime, many being lost after his death, with the result that his anatomical discoveries remained unknown until they were later found and published centuries after his death.
Vesalius.
The Galenic doctrine in Europe was first seriously challenged in the 16th century. Thanks to the printing press, all over Europe a collective effort proceeded to circulate the works of Galen and later publish criticisms on their works. Andreas Vesalius, born and educated in Belgium, contributed the most to human anatomy. Vesalius was the first to publish a treatise, "De humani corporis fabrica", that challenged Galen "drawing for drawing." These drawings were a detailed series of explanations and vivid drawings of the anatomical parts of human bodies. Vesalius traveled all the way from Leuven to Padua for permission to dissect victims from the gallows without fear of persecution. His superbly executed drawings are triumphant descriptions of the differences between dogs and humans, but it took a century for Galen's influence to fade. His work led to anatomy marked a new era in the study of anatomy and its relation to medicine. Under Vesalius, anatomy became an actual discipline. “His skill in and attention to dissection featured prominently in his publications as well as his demonstrations, in his research as well as his teaching." 
In 1540, Vesalius gave a public demonstration of the inaccuracies of Galen's anatomical theories, which are still the orthodoxy of the medical profession. Vesalius now has on display, for comparison purposes, the skeletons of a human being alongside that of an ape of which he was able to show, that in many cases, Galen's observations were indeed correct for the ape, but bear little relation to man. Clearly what was needed was a new account of human anatomy. While the lecturer explained human anatomy, as revealed by Galen more than 1000 years earlier, an assistant pointed to the equivalent details on a dissected corpse. At times, the assistant was unable to find the organ as described, but invariably the corpse rather than Galen was held to be in error. Vesalius then decided that he will dissect corpses himself and trust to the evidence of what he found. His approach was highly controversial, but his evident skill led to his appointment as professor of surgery and anatomy at the University of Padua.
A succession of researchers proceeded to refine the body of anatomical knowledge, giving their names to a number of anatomical structures along the way. The 16th and 17th centuries also witnessed significant advances in the understanding of the circulatory system, as the purpose of valves in veins was identified, the left-to-right ventricle flow of blood through the circulatory system was described, and the hepatic veins were identified as a separate portion of the circulatory system. The lymphatic system was also identified as a separate system at this time.
17th and 18th centuries.
The study of anatomy flourished in the 17th and 18th centuries. The advent of the printing press facilitated the exchange of ideas. Because the study of anatomy concerned observation and drawings, the popularity of the anatomist was equal to the quality of his drawing talents, and one need not be an expert in Latin to take part. [http://www.nlm.nih.gov/dreamanatomy/da_info.html] Many famous artists studied anatomy, attended dissections, and published drawings for money, from Michelangelo to Rembrandt. For the first time, prominent universities could teach something about anatomy through drawings, rather than relying on knowledge of Latin. Contrary to popular belief, the Church neither objected to nor obstructed anatomical research.
Only certified anatomists were allowed to perform dissections, and sometimes then only yearly. These dissections were sponsored by the city councilors and often charged an admission fee, rather like a circus act for scholars. Many European cities, such as Amsterdam, London, Copenhagen, Padua, and Paris, all had Royal anatomists (or some such office) tied to local government. Indeed, Nicolaes Tulp was Mayor of Amsterdam for three terms. Though it was a risky business to perform dissections, and unpredictable depending on the availability of fresh bodies, "attending" dissections was legal.
To cope with shortages of cadavers and the rise in medical students during the 17th and 18th centuries, body-snatching and even anatomy murder were practiced to obtain cadavers. 'Body snatching' was the act of sneaking into a graveyard, digging up a corpse and using it for study. Men known as 'resurrectionists' emerged as outside parties, who would steal corpses for a living and sell the bodies to anatomy schools. The leading London anatomist John Hunter paid for a regular supply of corpses for his anatomy school. The British Parliament passed the Anatomy Act 1832, which finally provided for an adequate and legitimate supply of corpses by allowing legal dissection of executed murderers. The view of anatomist at the time, however, became similar to that of an executioner. Having one's body dissected was seen as a punishment worse than death, "if you stole a pig, you were hung. If you killed a man, you were hung and then dissected." Demand grew so great that some anatomist resorted to dissecting their own family members (William Harvey dissected his own father and sister) as well as robbing bodies from their graves.
Many Europeans interested in the study of anatomy traveled to Italy, then the centre of anatomy. Only in Italy could certain important research methods be used, such as dissections on women. Realdo Colombo (also known as Realdus Columbus) and Gabriele Falloppio were pupils of Vesalius. Columbus, as Vesalius's immediate successor in Padua, and afterwards professor at Rome, distinguished himself by describing the shape and cavities of the heart, the structure of the pulmonary artery and aorta and their valves, and tracing the course of the blood from the right to the left side of the heart.
Anatomical theatres.
Anatomical theatres became a popular form for anatomical teaching in the early 16th century. The University of Padua was the first and most widely known theatre, founded in 1594. As a result, Italy became the center for human dissection. People came from all over to watch as professors taught lectures on the human physiology and anatomy, as anyone was welcome to witness the spectacle. Participants “were fascinated by corporeal display, by the body undergoing dissection.”. Most professors did not do the dissections themselves. Instead they sat in seats above the bodies while hired hands did the cutting. Students and observers would be placed around the table in a circular, stadium like arena and listen as professors explained the various anatomical parts. The 19th century eventually saw a move from anatomical theatres to classrooms, reducing "the number of people who could benefit from each cadaver." 
19th century anatomy.
During the 19th century, anatomical research was extended with histology and developmental biology of both humans and animals. Women, who were not allowed to attend medical school, could attend the anatomy theatres. From 1822 the Royal College of Surgeons forced unregulated schools to close. Medical museums provided examples in comparative anatomy, and were often used in teaching.
Modern anatomy.
Anatomical research in the past hundred years has taken advantage of technological developments and growing understanding of sciences such as evolutionary and molecular biology to create a thorough understanding of the body's organs and structures. Disciplines such as endocrinology have explained the purpose of glands that anatomists previously could not explain; medical devices such as MRI machines and CAT scanners have enabled researchers to study organs, living or dead, in unprecedented detail. Progress today in anatomy is centered in the development, evolution, and function of anatomical features, as the macroscopic aspects of human anatomy have largely been catalogued. Non-human anatomy is particularly active as researchers use techniques ranging from finite element analysis to molecular biology.
To save time, some medical schools such as Birmingham, England have adopted prosection, where a demonstrator dissects and explains to an audience, in place of dissection by students. This enables students to observe more than one body. Improvements in colour images and photography means that an anatomy text is no longer an aid to dissection but rather a central material to learn from. Plastic models are regularly used in anatomy teaching, offering a good substitute to the real thing. Use of living models for anatomy demonstration is once again becoming popular within teaching of anatomy. Surface landmarks that can be palpated on another individual provide practice for future clinical situations. It is possible to do this on oneself; in the Integrated Biology course at the University of Berkeley, students are encouraged to "introspect" on themselves and link what they are being taught to their own body.
Donations of bodies have declined with public confidence in the medical profession. In Britain, the Human Tissue Act 2004 has tightened up the availability of resources to anatomy departments. The outbreaks of Bovine Spongiform Encephalitis (BSE) in the late 80s and early 90s further restricted the handling of brain tissue.
The controversy with Gunther von Hagens and public displays of dissections, preserved by plastination, may divide opinions on what is ethical or legal.

</doc>
<doc id="48987" url="https://en.wikipedia.org/wiki?curid=48987" title="Central City, Colorado">
Central City, Colorado

The City of Central, commonly known as Central City, is the Home Rule Municipality in Gilpin and Clear Creek counties that is the county seat and the most populous municipality of Gilpin County, Colorado, United States. The city population was 663 at the 2010 United States Census. The city is a historic mining settlement founded in 1859 during the Pike's Peak Gold Rush and came to be known as the "Richest Square Mile on Earth". Central City and the adjacent city of Black Hawk form the federally designated Central City/Black Hawk Historic District. The city is now a part of the Denver-Aurora-Lakewood, CO Metropolitan Statistical Area.
History.
On May 6, 1859, during the Pike's Peak Gold Rush, John H. Gregory found a gold-bearing vein (the Gregory Lode) in Gregory Gulch between Black Hawk and Central City. Within two months many other veins were discovered, including the Bates, Gunnell, Kansas, and Burroughs. By 1860, as many as 10,000 prospectors had flocked to the town, then known as Mountain City, and surrounding prospects, but most soon left, many returning east. The 1900 census showed 3,114 people.
The year 1863 brought the first attempt by hard rock miners to form a hard rock miners' union. Of 125 miners signing a union resolution in Central City, about fifty broke windows and doors at the Bob Tail mine, forcing other workers out. After a night of shooting and fighting, the union effort among Central City miners failed.
Many Chinese lived in Central City during the early days working the placer deposits of Gregory Gulch. They were forbidden work in the underground mines. Most of them are believed to have returned to China after making their stake.
The frontier gambler Poker Alice lived for a time in Central City and several other Colorado mining communities.
Gold mining in the Central City district decreased rapidly between 1900 to 1920, as the veins were exhausted. Mining revived in the early 1930s in response to the increase in the price of gold from $20 to $35 per ounce, but then virtually shut down during World War II when gold mining was declared nonessential to the war effort. The district was enlivened in the 1950s by efforts to locate uranium deposits, but these proved unsuccessful.
The population of Central City and its sister city Black Hawk fell to a few hundred by the 1950s. Casino gambling was introduced in both towns in the early 1990s, but had more success in Black Hawk (which has 18 casinos) than in Central City (which has 6 casinos), partly because the main road to Central City passed through Black Hawk, tempting gamblers to stop in Black Hawk instead. In an effort to compete, Central City completed a four-lane, parkway from Interstate 70 to Central City, without going through Black Hawk. The highway was completed in 2004, but Black Hawk, which prior to the introduction of gambling was much smaller than Central City, continues to generate more than seven times the gambling revenue that Central City does. To compete, Central City has recently eliminated height restrictions for building on undeveloped land. Buildings were previously limited to heights of , so as not to overshadow the town's historic buildings.
Tax from the gambling revenue provides funding for the State Historical Fund, administered by the Colorado Office of Archaeology and Historic Preservation.
Geography.
Central City is located in southern Gilpin County. The city limits extend south along the Central City Parkway into Clear Creek County, as far as Interstate 70. The city is bordered by Black Hawk to the east and Idaho Springs to the south.
According to the United States Census Bureau, the city has a total area of . None of the area is covered with water.
Demographics.
As of the census of 2000, there were 515 people, 261 households, and 101 families residing in the city. The population density was . There were 394 housing units at an average density of . The racial makeup of the city was 91.84% White, 0.19% Black or African American, 1.55% Native American, 1.17% Asian, 1.17% Pacific Islander, 2.52% from other races, and 1.55% from two or more races. 9.32% of the population were Hispanic or Latino of any race.
There were 261 households out of which 17.6% had children under the age of 18 living with them, 26.8% were married couples living together, 8.8% had a female householder with no husband present, and 61.3% were non-families. 43.3% of all households were made up of individuals and 6.1% had someone living alone who was 65 years of age or older. The average household size was 1.97 and the average family size was 2.76.
In the city the population was spread out with 16.5% under the age of 18, 10.1% from 18 to 24, 34.0% from 25 to 44, 30.9% from 45 to 64, and 8.5% who were 65 years of age or older. The median age was 39 years. For every 100 females there were 115.5 males. For every 100 females age 18 and over, there were 115.0 males.
The median income for a household in the city was $30,921, and the median income for a family was $31,667. Males had a median income of $32,917 versus $25,446 for females. The per capita income for the city was $26,465. About 7.4% of families and 12.3% of the population were below the poverty line, including 8.3% of those under age 18 and 5.2% of those age 65 or over.
Education.
Central City Public Schools are part of the Gilpin County School District RE-1. The district has one elementary school and one high school, Gilpin County Elementary School and Gilpin County Undivided High School.
Tina Goar is the Superintendent of Schools.
There are approximately 380 students enrolled in the district.

</doc>
<doc id="48988" url="https://en.wikipedia.org/wiki?curid=48988" title="Idaho Springs, Colorado">
Idaho Springs, Colorado

The City of Idaho Springs is a Statutory City which is the most populous municipality in Clear Creek County, Colorado, United States. As of the 2010 census it had a population of 1,717. Idaho Springs is located in Clear Creek Canyon, in the mountains upstream from Golden, some west of Denver. Local legend is that the name of the city derived from annual visits to the radium hot springs made by a Native American chief and his tribe who journeyed there each year from Idaho to bathe in the magic healing waters.
Founded in 1859 by prospectors during the early days of the Pike's Peak Gold Rush, the town was at the center of the region's mining district throughout the late nineteenth century. The Argo Tunnel drained and provided access to many lodes of ore between Idaho Springs and Central City. During the late twentieth century, the town evolved into a tourist center along U.S. Highway 6 and U.S. Highway 40, which ascend Clear Creek Canyon through the historic mining district.
The town today is squeezed along the north side of Interstate 70, with a historical downtown in the central portion, a strip of tourist-related businesses on its eastern end, and mostly residences on its western end. It also serves as a bedroom community for workers at the Loveland Ski Area farther up the canyon. The town today is the largest community in Clear Creek County, but, for historical reasons, the county seat has remained at Georgetown.
History.
On January 5, 1859, during the Colorado gold rush, prospector George A. Jackson discovered placer gold at the present site of Idaho Springs, where Chicago Creek empties into Clear Creek. It was the first substantial gold discovery in Colorado. Jackson, a Missouri native with experience in the California gold fields, was drawn to the area by clouds of steam rising from some nearby hot springs. Jackson kept his find secret for several months, but after he paid for some supplies with gold dust, others rushed to Jackson's diggings. The location was originally known as "Jackson's Diggings". Once the location became a permanent settlement, it was variously called "Sacramento City", "Idahoe", "Idaho City", and finally "Idaho Springs".
The first placer discoveries were soon followed by discoveries of gold veins in the rocks of the canyon walls on both sides of Clear Creek. Hard rock mining became the mainstay of the town long after the gold-bearing gravels were exhausted.
A strike by Idaho Springs miners demanding the eight-hour day in May 1903 erupted into violence. This was a local struggle in a much broader fight called the Colorado Labor Wars.
The 1969 film "Downhill Racer" portrayed an alpine ski racer from Idaho Springs, played by Robert Redford; a brief scene was shot on location in Idaho Springs. Several scenes from the comedy film "The Overbrook Brothers" were filmed here in the spring of 2008.
Geography.
Idaho Springs is located in northeastern Clear Creek County at (39.742456, -105.514391), along Clear Creek near the confluence of its tributary, Chicago Creek.
According to the United States Census Bureau, the city has a total area of , of which , or 1.53%, is water.
Demographics.
As of the census of 2010, there were 1,717 people and 934 housing units. As of the census of 2000, there were 485 families residing in the city. The population density was . There were 904 housing units at an average density of . The racial makeup of the city was 94.71% White, 0.74% Black or African American, 1.06% Native American, 0.48% Asian, 1.54% from other races, and 1.48% from two or more races. 5.03% of the population were Hispanic or Latino of any race.
There were 841 households out of which 27.1% had children under the age of 18 living with them, 42.8% were married couples living together, 10.9% had a female householder with no husband present, and 42.3% were non-families. 33.2% of all households were made up of individuals and 8.4% had someone living alone who was 65 years of age or older. The average household size was 2.25 and the average family size was 2.87.
In the city the population was spread out with 23.1% under the age of 18, 9.1% from 18 to 24, 30.3% from 25 to 44, 28.0% from 45 to 64, and 9.5% who were 65 years of age or older. The median age was 39 years. For every 100 females there were 105.5 males. For every 100 females age 18 and over, there were 103.9 males.
The median income for a household in the city was $39,643, and the median income for a family was $48,790. Males had a median income of $35,446 versus $22,688 for females. The per capita income for the city was $20,789. About 2.2% of families and 6.7% of the population were below the poverty line, including 5.4% of those under age 18 and 13.4% of those age 65 or over.
Education.
Idaho Springs Public Schools are part of the Clear Creek School District RE-1. There are two elementary schools, one middle school, one high school, and one charter school. Students attend Clear Creek High School.
Carlson Elementary School is located in Idaho Springs.
Roslin Marshall is the Interim Superintendent of Schools.

</doc>
<doc id="48990" url="https://en.wikipedia.org/wiki?curid=48990" title="Social justice">
Social justice

Social justice is the fair and just relation between the individual and society. This is measured by the explicit and tacit terms for the distribution of wealth, opportunities for personal activity and social privileges. In Western as well as in older Asian cultures, the concept of social justice has often referred to the process of ensuring that individuals fulfill their societal roles and receive what was their due from society. In the current global grassroots movements for social justice, the emphasis has been on the breaking of unspoken barriers for social mobility, the creation of safety nets and economic justice.
Social justice assigns rights and duties in the institutions of society, which enables people to receive the basic benefits and burdens of cooperation. The relevant institutions often include taxation, social insurance, public health, public school, public services, labour law and regulation of markets, to ensure fair distribution of wealth, equal opportunity and equality of outcome.
Interpretations that relate justice to a reciprocal relationship to society are mediated by differences in cultural traditions, some of which emphasize the individual responsibility toward society and others the equilibrium between access to power and its responsible use. Hence, social justice is invoked today while reinterpreting historical figures such as Bartolomé de las Casas, in philosophical debates about differences among human beings, in efforts for gender, racial and social equality, for advocating justice for migrants, prisoners, the environment, and the physically and mentally disabled.
While the concept of social justice can be traced through the theology of Augustine of Hippo and the philosophy of Thomas Paine, the term "social justice" became used explicitly from the 1840s. A Jesuit priest named Luigi Taparelli is typically credited with coining the term, and it spread during the revolutions of 1848 with the work of Antonio Rosmini-Serbati. In the late industrial revolution, progressive American legal scholars began to use the term more, particularly Louis Brandeis and Roscoe Pound. From the early 20th century it was also embedded in international law and institutions; the preamble to establish the International Labour Organization recalled that "universal and lasting peace can be established only if it is based upon social justice." In the later 20th century, social justice was made central to the philosophy of the social contract, primarily by John Rawls in "A Theory of Justice" (1971). In 1993, the Vienna Declaration and Programme of Action treats social justice as a purpose of the human rights education.
History.
The different concepts of justice, as discussed in ancient Western philosophy, were typically centered upon the community. Plato wrote in "The Republic" that it would be an ideal state that "every member of the community must be assigned to the class for which he finds himself best fitted." Aristotle believed rights existed only between free people, and the law should take "account in the first instance of relations of inequality in which individuals are treated in proportion to their worth and only secondarily of relations of equality." Reflecting this time when slavery and subjugation of women was typical, ancient views of justice tended to reflect the rigid class systems that still prevailed. On the other hand, for the privileged groups, strong concepts of fairness and the community existed. Distributive justice was said by Aristotle to require that people were distributed goods and assets according to their merit. Socrates (through Plato's dialogue "Crito") is attributed with developing the idea of a social contract, whereby people ought to follow the rules of a society, and accept its burdens because they have accepted its benefits. During the Middle Ages, religious scholars particularly, such as Thomas Aquinas continued discussion of justice in various ways, but ultimately connected being a good citizen to the purpose of serving God.
After the Renaissance and Reformation, the modern concept of social justice, as developing human potential, began to emerge through the work of a series of authors. Baruch Spinoza in "On the Improvement of the Understanding" (1677) contended that the one true aim of life should be to acquire "a human character much more stable than [one's] own", and to achieve this "pitch of perfection... The chief good is that he should arrive, together with other individuals if possible, at the possession of the aforesaid character." During the enlightenment and responding to the French and American Revolutions, Thomas Paine similarly wrote in "The Rights of Man" (1792) society should give "genius a fair and universal chance" and so "the construction of government ought to be such as to bring forward... all that extent of capacity which never fails to appear in revolutions."
The first modern usage of the specific term "social justice" is typically attributed to Catholic thinkers from the 1840s, including the Jesuit Luigi Taparelli in "Civiltà Cattolica", based on the work of St. Thomas Aquinas. He argued that rival capitalist and socialist theories, based on subjective Cartesian thinking, undermined the unity of society present in Thomistic metaphysics as neither were sufficiently concerned with moral philosophy. Writing in 1861, the influential British philosopher and economist, John Stuart Mill stated in "Utilitarianism" his view that "Society should treat all equally well who have deserved equally well of it, that is, who have deserved equally well absolutely. This is the highest abstract standard of social and distributive justice; towards which all institutions, and the efforts of all virtuous citizens, should be made in the utmost degree to converge."
In the later 19th and early 20th century, social justice became an important theme in American political and legal philosophy, particularly in the work of John Dewey, Roscoe Pound and Louis Brandeis. One of the prime concerns was the "Lochner era" decisions of the US Supreme Court to strike down legislation passed by state governments and the Federal government for social and economic improvement, such as the eight-hour day or the right to join a trade union. After the First World War, the founding document of the International Labour Organization took up the same terminology in its preamble, stating that "peace can be established only if it is based on social justice". From this point, the discussion of social justice entered into mainstream legal and academic discourse. In the late 20th century, a number of liberal and conservative thinkers, notably Friedrich von Hayek rejected the concept by stating that it did not mean anything, or meant too many things. However the concept remained highly influential, particularly with its promotion by philosophers such as John Rawls.
Contemporary theory.
Philosophical perspectives.
Cosmic values.
Hunter Lewis' work promoting natural healthcare and sustainable economies advocates for conservation as a key premise in social justice. His manifesto on sustainability ties the continued thriving of human life to real conditions, the environment supporting that life, and associates injustice with the detrimental effects of unintended consequences of human actions. Quoting classical Greek thinkers like Epicurus on the good of pursuing happiness, Hunter also cites ornithologist, naturalist, and philosopher Alexander Skutch in his book Moral Foundations:
Pope Benedict XVI cites Teilhard de Chardin in a vision of the cosmos as a 'living host' embracing an understanding of ecology that includes humanity's relationship to others, that pollution affects not just the natural world but interpersonal relations as well. Cosmic harmony, justice and peace are closely interrelated:
John Rawls.
Political philosopher John Rawls draws on the utilitarian insights of Bentham and Mill, the social contract ideas of John Locke, and the categorical imperative ideas of Kant. His first statement of principle was made in "A Theory of Justice" where he proposed that, "Each person possesses an inviolability founded on justice that even the welfare of society as a whole cannot override. For this reason justice denies that the loss of freedom for some is made right by a greater good shared by others." A deontological proposition that echoes Kant in framing the moral good of justice in absolutist terms. His views are definitively restated in "Political Liberalism" where society is seen "as a fair system of co-operation over time, from one generation to the next".
All societies have a basic structure of social, economic, and political institutions, both formal and informal. In testing how well these elements fit and work together, Rawls based a key test of legitimacy on the theories of social contract. To determine whether any particular system of collectively enforced social arrangements is legitimate, he argued that one must look for agreement by the people who are subject to it, but not necessarily to an objective notion of justice based on coherent ideological grounding. Obviously, not every citizen can be asked to participate in a poll to determine his or her consent to every proposal in which some degree of coercion is involved, so one has to assume that all citizens are reasonable. Rawls constructed an argument for a two-stage process to determine a citizen's hypothetical agreement:
This applies to one person who represents a small group (e.g., the organiser of a social event setting a dress code) as equally as it does to national governments, which are ultimate trustees, holding representative powers for the benefit of all citizens within their territorial boundaries. Governments that fail to provide for welfare of their citizens according to the principles of justice are not legitimate. To emphasise the general principle that justice should rise from the people and not be dictated by the law-making powers of governments, Rawls asserted that, "There is ... a general presumption against imposing legal and other restrictions on conduct without sufficient reason. But this presumption creates no special priority for any particular liberty." This is support for an unranked set of liberties that reasonable citizens in all states should respect and uphold — to some extent, the list proposed by Rawls matches the normative human rights that have international recognition and direct enforcement in some nation states where the citizens need encouragement to act in a way that fixes a greater degree of equality of outcome. According to Rawls, the basic liberties that every good society should guarantee are,
Thomas Pogge.
Thomas Pogge's arguments pertain to a standard of social justice that creates human rights deficits. He assigns responsibility to those who actively cooperate in designing or imposing the social institution, that the order is foreseeable as harming the global poor and is reasonably avoidable. Thomas argues that social institutions have a negative duty, that means that their duty is to not harm the poor.
Pogge speaks of Institutional Cosmopolitanism and assigns responsibility to institutional schemes for deficits of human rights. An example given is slavery and third parties. A third party should not recognize or enforce slavery. The institutional order should be held responsible only for deprivations of human rights that it establishes or authorizes. The current institutional design systematically harms developing economies by enabling corporate tax evasion, illicit financial flows, corruption, trafficking of people and weapons as a few examples. Joshua Cohen disputes his claims based on the fact that some poor countries have done well in spite of the current institutional design. Elizabeth Kahn argues that some of these responsibilities should apply globally.
United Nations.
The United Nations’ 2006 document "Social Justice in an Open World: The Role of the United Nations", states that "Social justice may be broadly understood as the fair and compassionate distribution of the fruits of economic growth..."
The term "social justice" was seen by the U.N. "as a substitute for the protection of human rights first appeared in United Nations texts during the second half of the 1960s. At the initiative of the Soviet Union, and with the support of developing countries, the term was used in the Declaration on Social Progress and Development, adopted in 1969."
The same document reports, "From the comprehensive global perspective shaped by the United Nations Charter and the Universal Declaration of Human Rights, neglect of the pursuit of social justice in all its dimensions translates into de facto acceptance of a future marred by violence, repression and chaos." The report concludes, "Social justice is not possible without strong and coherent redistributive policies conceived and implemented by public agencies."
The same UN document offers a concise history: "he notion of social justice is relatively new. None of history’s great philosophers—not Plato or Aristotle, or Confucius or Averroes, or even Rousseau or Kant—saw the need to consider justice or the redress of injustices from a social perspective. The concept first surfaced in Western thought and political language in the wake of the industrial revolution and the parallel development of the socialist doctrine. It emerged as an expression of protest against what was perceived as the capitalist exploitation of labour and as a focal point for the development of measures to improve the human condition. It was born as a revolutionary slogan embodying the ideals of progress and fraternity. Following the revolutions that shook Europe in the mid-1800s, social justice became a rallying cry for progressive thinkers and political activists... By the mid-twentieth century, the concept of social justice had become central to the ideologies and programmes of virtually all the leftist and centrist political parties around the world..."
Religious perspectives.
Hinduism.
Present day jati hierarchy is undergoing changes for variety of reasons including 'social justice', which is a politically popular stance in democratic India. Institutionalized affirmative action has promoted this. The disparity and wide inequalities in social behaviour of the jatis has led to various reform movements in hinduism. While legally outlawed, the caste system remains strong in practice.
Islam.
The Quran contains numerous references to elements of social justice. For example, one of Islam's Five Pillars is Zakāt, or alms-giving. Charity and assistance to the poor – concepts central to social justice – are and have historically been important parts of the Islamic faith.
In Muslim history, Islamic governance has often been associated with social justice. Establishment of social justice was one of the motivating factors of the Abbasid revolt against the Umayyads. The Shi'a believe that the return of the "Mahdi" will herald in "the messianic age of justice" and the Mahdi along with the Messiah (Jesus) will end plunder, torture, oppression and discrimination.
For the Muslim Brotherhood the implementation of social justice would require the rejection of consumerism and communism. The Brotherhood strongly affirmed the right to private property as well as differences in personal wealth due to factors such as hard work. However, the Brotherhood held Muslims had an obligation to assist those Muslims in need. It held that "zakat" (alms-giving) was not voluntary charity, but rather the poor had the right to assistance from the more fortunate. Most Islamic governments therefore enforce the "zakat" through taxes.
Though monetary donations are the most practiced way of zakat, Islam is deeply rooted in the tenets of volunteerism and social activism. Areas of one's communities which require assistance and beneficiaries must be a Muslim's foci if need be, rather than strictly her or his personal or superficial wants. For example, the ecological well-being of the planet (i.e.: animal rights, global warming, natural resources degradation); locally, nationally, globally, is a campaign to which every Muslim must adhere. Many Muslims practice this today by ensuring that they produce minimal waste, give to charity what they no longer need, and spend time in prayer and meditation upon the bounties of nature so as to more mindfully approach all that is provided by nature,and ultimately, Allah. Other areas of society in need may be the safety and security of minority populations, i.e.: women or persons of color, children, the elderly, the developmentally or physically disabled, animals, et al.
Judaism.
In "To Heal a Fractured World: The Ethics of Responsibility", Rabbi Jonathan Sacks states that social justice has a central place in Judaism. One of Judaism’s most distinctive and challenging ideas is its ethics of responsibility reflected in the concepts of simcha ("gladness" or "joy"), tzedakah ("the religious obligation to perform charity and philanthropic acts"), chesed ("deeds of kindness"), and tikkun olam ("repairing the world"). 
Christianity.
Methodism.
From its founding, Methodism was a Christian social justice movement. Under John Wesley's direction, Methodists became leaders in many social justice issues of the day, including the prison reform and abolition movements. Wesley himself was among the first to preach for slaves rights attracting significant opposition.
Today, social justice plays a major role in the United Methodist Church. The "Book of Discipline of the United Methodist Church" says, "We hold governments responsible for the protection of the rights of the people to free and fair elections and to the freedoms of speech, religion, assembly, communications media, and petition for redress of grievances without fear of reprisal; to the right to privacy; and to the guarantee of the rights to adequate food, clothing, shelter, education, and health care." The United Methodist Church also teaches population control as part of its doctrine.
Catholicism.
Catholic social teaching consists of those aspects of Roman Catholic doctrine which relate to matters dealing with the respect of the individual human life. A distinctive feature of Catholic social doctrine is its concern for the poorest and most vulnerable members of society. Two of the seven key areas of "Catholic social teaching" are pertinent to social justice:
Even before it was propounded in the Catholic social doctrine, social justice appeared regularly in the history of the Catholic Church:
The Catechism (§1928–1948) contain more detail of the Church's view of social justice.
Criticism.
Many authors criticize the idea that there exists an objective standard of social justice. Moral relativists deny that there is any kind of objective standard for justice in general. Non-cognitivists, moral skeptics, moral nihilists, and most logical positivists deny the epistemic possibility of objective notions of justice. Political realists believe that any ideal of social justice is ultimately a mere justification for the status quo.
Many other people accept some of the basic principles of social justice, such as the idea that all human beings have a basic level of value, but disagree with the elaborate conclusions that may or may not follow from this. One example is the statement by H. G. Wells that all people are "equally entitled to the respect of their fellowmen."
On the other hand, some scholars reject the very idea of social justice as meaningless, religious, self-contradictory, and ideological, believing that to realize any degree of social justice is unfeasible, and that the attempt to do so must destroy all liberty. Perhaps the most complete rejection of the concept of social justice comes from Friedrich Hayek of the Austrian School of economics:
Ben O'Neill of the University of New South Wales argues that, for proponents of "social justice":
Sociologist Carl L. Bankston has argued that a secular, leftist view of social justice entails viewing the redistribution of goods and resources as based on the rights of disadvantaged categories of people, rather than on compassion or national interest. Bankston maintains that this secular version of social justice became widely accepted due to the rise of demand-side economics and to the moral influence of the civil rights movement.
Social justice movements.
Social justice is also a concept that is used to describe the movement towards a socially just world, e.g., the Global Justice Movement. In this context, social justice is based on the concepts of human rights and equality, and can be defined as ""the way in which human rights are manifested in the everyday lives of people at every level of society"".
A number of movements are working to achieve social justice in society. These movements are working towards the realization of a world where all members of a society, regardless of background or procedural justice, have basic human rights and equal access to the benefits of their society.
Liberation theology.
Liberation theology is a movement in Christian theology which conveys the teachings of Jesus Christ in terms of a liberation from unjust economic, political, or social conditions. It has been described by proponents as "an interpretation of Christian faith through the poor's suffering, their struggle and hope, and a critique of society and the Catholic faith and Christianity through the eyes of the poor", and by detractors as Christianity perverted by Marxism and Communism.
Although liberation theology has grown into an international and inter-denominational movement, it began as a movement within the Catholic Church in Latin America in the 1950s–1960s. It arose principally as a moral reaction to the poverty caused by social injustice in that region. It achieved prominence in the 1970s and 1980s. The term was coined by the Peruvian priest, Gustavo Gutiérrez, who wrote one of the movement's most famous books, "A Theology of Liberation" (1971). According to Sarah Kleeb, "Marx would surely take issue," she writes, "with the appropriation of his works in a religious context...there is no way to reconcile Marx's views of religion with those of Gutierrez, they are simply incompatible. Despite this, in terms of their understanding of the necessity of a just and righteous world, and the nearly inevitable obstructions along such a path, the two have much in common; and, particularly in the first edition of Theology of Liberation, the use of Marxian theory is quite evident."
Other noted exponents are Leonardo Boff of Brazil, Carlos Mugica of Argentina, Jon Sobrino of El Salvador, and Juan Luis Segundo of Uruguay.
Health care.
Social justice has more recently made its way into the field of bioethics. Discussion involves topics such as affordable access to health care, especially for low income households and families. The discussion also raises questions such as whether society should bear healthcare costs for low income families, and whether the global marketplace is the best way to distribute healthcare. Ruth Faden of the Johns Hopkins Berman Institute of Bioethics and Madison Powers of Georgetown University focus their analysis of social justice on which inequalities matter the most. They develop a social justice theory that answers some of these questions in concrete settings.
Social injustices occur when there is a preventable difference in health states among a population of people. These social injustices take the form of health inequities when negative health states such as malnourishment, and infectious diseases are more prevalent in impoverished nations. These negative health states can often be prevented by providing social and economic structures such as primary healthcare which ensures the general population has equal access to health care services regardless of income level, gender, education or any other stratifying factors. Integrating social justice with health inherently reflects the social determinants of health model without discounting the role of the bio-medical model.
Human rights education.
The Vienna Declaration and Programme of Action affirm that "Human rights education should include peace, democracy, development and social justice, as set forth in international and regional human rights instruments, in order to achieve common understanding and awareness with a view to strengthening universal commitment to human rights."

</doc>
<doc id="48991" url="https://en.wikipedia.org/wiki?curid=48991" title="Arts and Crafts movement">
Arts and Crafts movement

The Arts and Crafts movement was an international movement in the decorative and fine arts that flourished in Europe and North America between 1880 and 1910, emerging in Japan in the 1920s. It stood for traditional craftsmanship using simple forms, and often used medieval, romantic or folk styles of decoration. It advocated economic and social reform and was essentially anti-industrial. It had a strong influence on the arts in Europe until it was displaced by Modernism in the 1930s, and its influence continued among craft makers, designers and town planners long afterwards.
The term was first used by T. J. Cobden-Sanderson at a meeting of the Arts and Crafts Exhibition Society in 1887, although the principles and style on which it was based had been developing in England for at least twenty years. It was inspired by the writings of the architect Augustus Pugin (1812–1852), the writer John Ruskin (1819–1900), and the artist William Morris (1834–1896).
The movement developed earliest and most fully in the British Isles, and spread across the British Empire and to the rest of Europe and North America. It was largely a reaction against the perceived impoverished state of the decorative arts at the time and the conditions in which they were produced.
Social and design principles.
The Arts and Crafts style emerged from the attempt to reform design and decoration in mid 19th century Britain. It was a reaction against a decline in standards that the reformers associated with machinery and factory production, and was in part a response to items shown in the Great Exhibition of 1851 that were ornate, artificial and ignored the qualities of the materials used. 
The art historian Nikolaus Pevsner has said that exhibits in the Great Exhibition showed "ignorance of that basic need in creating patterns, the integrity of the surface" and "vulgarity in detail". Design reform began with the organisers of the Exhibition itself, Henry Cole (1808–1882), Owen Jones (1809–1874), Matthew Digby Wyatt (1820–1877) and Richard Redgrave (1804–1888), and the dislike of excessive ornament and badly made things was not exclusive to the Arts and Crafts movement. Owen Jones, for example, declared that "Ornament ... must be secondary to the thing decorated", that there must be "fitness in the ornament to the thing ornamented", and that wallpapers and carpets must not have any patterns "suggestive of anything but a level or plain". Where a fabric or wallpaper in the Great Exhibition might be decorated with a natural motif made to look as real as possible, an Arts and Crafts, like the Artichoke design illustrated above, would use a flat and simplified natural motif.
William Morris, the major figure in 19th century design reform, whose ideas inspired the Arts and Crafts Movement, was inconsistent in his view of the proper place of machinery in manufacture. At one point he said that production by machinery was "altogether an evil", but he was willing to use manufacturers able to work to his standards with the aid of machinery; and he said that, in a "true society", where neither luxuries nor cheap trash were made, machinery could be improved and used to reduce the hours of labour. Fiona MacCarthy says that "unlike later zealots like Gandhi, William Morris had no practical objections to the use of machinery "per se" so long as the machines produced the quality he needed." Morris's followers also had subtly differing views or changed their minds over time. C.R.Ashbee, for example, a central figure in the Arts and Crafts Movement, shared Morris's ambivalence. At the time of his Guild of Handicraft, initiated in 1888, he said, "We do not reject the machine, we welcome it. But we would desire to see it mastered." After unsuccessfully pitting his Guild and School of Handicraft guild against modern methods of manufacture, he acknowledged that "Modern civilization rests on machinery", but he continued to criticize the deleterious effects of what he called "mechanism", saying that "the production of certain mechanical commodities is as bad for the national health as is the production of slave-grown cane or child-sweated wares."
Morris insisted that the artist should be a craftsman-designer working by hand and advocated a society of free craftspeople, which he believed had existed during the Middle Ages. "Because craftsmen took pleasure in their work", he wrote, "the Middle Ages was a period of greatness in the art of the common people. ... The treasures in our museums now are only the common utensils used in households of that age, when hundreds of medieval churches - each one a masterpiece - were built by unsophisticated peasants." Medieval art was the model for much Arts and Crafts design and medieval life, before capitalism and the factory system, was idealised by the movement.
Morris and his followers believed the division of labour on which modern industry depended was undesirable, but not all Arts and Crafts artists carried out every stage in the making of goods themselves, and it was only in the twentieth century that that became an essential part of the definition of craftsmanship. The founders of the Arts and Crafts Exhibition Society did not insist that the designer should also be the maker. Peter Floud, writing in the 1950s, said that "The founders of the Society ... never executed their own designs, but invariably turned them over to commercial firms." The idea that the designer should be the maker and the maker the designer derived "not from Morris or early Arts and Crafts teaching, but rather from the second-generation elaboration doctrine worked out in the first decade of twentieth century by men such as W. R. Lethaby".
The Arts and Crafts Movement was associated with socialist ideas in the persons of Morris, T. J. Cobden Sanderson, Walter Crane, Ashbee and others. In the early 1880s Morris was spending more of his time on socialist propaganda than on designing and making. Ashbee established a community of craftsmen, the Guild of Handicraft, in east London, later moving to Chipping Campden. Those adherents who were not socialists, for example, Alfred Hoare Powell, advocated a more humane and personal relationship between employer and employee. Lewis Foreman Day, a very successful and influential Arts and Crafts designer, was not a socialist either, despite his long friendship with Crane.
In Britain the movement was associated with dress reform, ruralism, the garden city movement and the folk-song revival, and in continental Europe with the preservation of national traditions in building, the applied arts, domestic design and costume.
Origins.
John Ruskin.
The Arts and Crafts philosophy derived partly from Ruskin's social criticism, which related the moral and social health of a nation to the qualities of its architecture and to the nature of work. Ruskin considered the sort of mechanized production and division of labour that had been created in the industrial revolution to be "servile labour" and he thought that a healthy and moral society required independent workers who designed the things they made. His followers in the Arts and Crafts movement favoured craft production over industrial manufacture and were concerned about the loss of traditional skills, but they were arguably more troubled by effects of the factory system than by machinery itself and William Morris's idea of "handicraft" was essentially work without any division of labour rather than work without any sort of machinery.
Gothic revival.
A.W.N. Pugin (1812–1852) was a leader in the Gothic revival in architecture and influenced the designers of the Arts and Crafts movement. He advocated truth to material, structure and function. 
William Morris.
Morris, the towering figure of 19th century design, was a major influence on the Arts and Crafts movement. The aesthetic and social vision of the Arts and Crafts movement derived from ideas he developed in the 1850s with a group of students at the University of Oxford, who combined a love of Romantic literature with a commitment to social reform. By 1855 they had discovered Ruskin and, conscious of the contrast between the barbarity of contemporary art and the painters preceding Raphael (1483-1530), they formed themselves into the Pre-Raphaelite Brotherhood to pursue their artistic aims. The medievalism of Mallory's "Morte d'Arthur" set the standard for their early style.In Edward Burne-Jones' words, they intended to "wage Holy warfare against the age". 
Morris began experimenting with various crafts and designing furniture and interiors. He was personally involved in manufacture as well as design, which was to be the hallmark of the Arts and Crafts movement. Ruskin had argued that the separation of the intellectual act of design from the manual act of physical creation was both socially and aesthetically damaging; Morris further developed this idea, insisting that no work should be carried out in his workshops before he had personally mastered the appropriate techniques and materials, arguing that "without dignified, creative human occupation people became disconnected from life".
In 1861 Morris began making furniture and decorative objects commercially, modeling his designs on medieval styles and using bold forms and strong colors. His patterns were based on flora and fauna and his products were inspired by the vernacular or domestic traditions of the British countryside. In order to display the beauty of the materials and the work of the craftsman, some were deliberately left unfinished, creating a rustic appearance. Truth to materials, structure and function became characteristic of the Arts and Crafts movement.
Development.
Morris's designs quickly became popular, attracting interest when his company's work was exhibited at the 1862 International Exhibition. Much of Morris & Co's early work was for churches and Morris won important interior design commissions at St James's Palace and the South Kensington Museum (now the Victoria and Albert Museum). Later his work became popular with the middle and upper classes, despite his wish to create a democratic art, and by the end of the 19th century, Arts and Crafts design in houses and domestic interiors was the dominant style in Britain, copied in products made by conventional industrial methods.
The spread of Arts and Crafts ideas during the late 19th and early 20th centuries resulted in the establishment of many associations and craft communities, although Morris was not involved with them because of his preoccupation with socialism in the 1880s. A hundred and thirty Arts and Crafts organisations were formed in Britain, most between 1895 and 1905.
In 1881, Eglantyne Louisa Jebb, Mary Fraser Tytler and others initiated the Home Arts and Industries Association to encourage the working classes, especially those in rural areas, to take up handicrafts under supervision, not for profit, but in order to provide them with useful occupations and to improve their taste.
In 1882, architect A.H.Mackmurdo formed the Century Guild, a partnership of designers including Selwyn Image, Herbert Horne, Clement Heaton and Benjamin Creswick.
In 1884, the Art Workers Guild was initiated by five young architects, William Lethaby, Edward Prior, Ernest Newton, Mervyn Macartney and Gerald C. Horsley, with the goal of bringing together fine and applied arts and raising the status of the latter. It was directed originally by George Blackall Simonds. By 1890 the Guild had 150 members, representing the increasing number of practitioners of the Arts and Crafts style. It still exists.
The London department store Liberty & Co., founded in 1875, was a prominent retailer of goods in the style and of the "artistic dress" favoured by followers of the Arts and Crafts movement.
In 1887 the Arts and Crafts Exhibition Society, which gave its name to the movement, was formed with Walter Crane as president, holding its first exhibition in the New Gallery, London, in November 1888. It was the first show of contemporary decorative arts in London since the Grosvenor Gallery's Winter Exhibition of 1881. Morris & Co. was well represented in the exhibition with furniture, fabrics, carpets and embroideries. Edward Burne-Jones observed, "here for the first time one can measure a bit the change that has happened in the last twenty years". The society still exists as the Society of Designer Craftsmen.
In 1888, C.R.Ashbee, a major late practitioner of the style in England, founded the Guild and School of Handicraft in the East End of London. The guild was a craft co-operative modelled on the medieval guilds and intended to give working men satisfaction in their craftsmanship. Skilled craftsmen, working on the principles of Ruskin and Morris, were to produce hand-crafted goods and manage a school for apprentices. The idea was greeted with enthusiasm by almost everyone except Morris, who was by now involved with promoting socialism and thought Ashbee's scheme trivial. From 1888 to 1902 the guild prospered, employing about 50 men. In 1902 Ashbee relocated the guild out of London to begin an experimental community in Chipping Campden in the Cotswolds. The guild's work is characterized by plain surfaces of hammered silver, flowing wirework and colored stones in simple settings. Ashbee designed jewellery and silver tableware. The guild flourished at Chipping Camden but did not prosper and was liquidated in 1908. Some craftsmen stayed, contributing to the tradition of modern craftsmanship in the area.
Charles Francis Annesley Voysey (1857–1941) was an Arts and Crafts architect who also designed fabrics, tiles, ceramics, furniture and metalwork. His style combined simplicity with sophistication. His wallpapers and textiles, featuring stylised bird and plant forms in bold outlines with flat colors, were used widely.
Morris's thought influenced the distributism of G. K. Chesterton and Hilaire Belloc.
By the end of the nineteenth century, Arts and Crafts ideals had influenced architecture, painting, sculpture, graphics, illustration, book making and photography, domestic design and the decorative arts, including furniture and woodwork, stained glass, leatherwork, lacemaking, embroidery, rug making and weaving, jewelry and metalwork, enameling and ceramics. By 1910, there was a fashion for "Arts and Crafts" and all things hand-made and a proliferation of amateur handicrafts of variable quality.
Later influences.
The British artist potter Bernard Leach brought to England many ideas he had developed in Japan with the social critic Yanagi Soetsu about the moral and social value of simple crafts. Leach was an active propagandist for these ideas, which struck a chord with practitioners of the crafts in the inter-war years, and he expounded them in his book "The Art of the Potter", published in 1940, which denounced industrial society in terms as vehement as those of Ruskin and Morris. Thus the Arts and Crafts philosophy was perpetuated in the small world of the crafts in the 1950s and 1960s, long after the demise of the Arts and Crafts movement and at the high tide of Modernism. British Utility furniture of the 1940s was simple in design and also derived from Arts and Crafts principles. Gordon Russell, chairman of the Utility Furniture Design Panel, manufactured in the Cotswold Hills, which had become a region of Arts and Crafts furniture making when Ashbee relocated there. William Morris's biographer, Fiona MacCarthy, detected the Arts and Crafts philosophy behind the Festival of Britain (1951), the work of the designer Terence Conran and the founding of the British Crafts Council in the 1970s.
Outside England.
Ireland.
The movement spread to Ireland, representing an important time for the nation's cultural development, a visual counterpart to the literary revival of the same time and was a publication of Irish nationalism. The Arts and Crafts use of stained glass was popular in Ireland, with Harry Clarke the best-known artist and also with Evie Hone. The architecture of the style is represented by the Honan Chapel (1916) in Cork in the grounds of University College Cork. Other architects practicing in Ireland included Sir Edwin Lutyens (Heywood House in Co. Laois, Lambay Island and the Irish National War Memorial Gardens in Dublin) and Frederick 'Pa' Hicks (Malahide Castle estate buildings and round tower). Irish Celtic motifs were popular with the movement in silvercraft, carpet design, book illustrations and hand-carved furniture.
Scotland
The beginnings of the Arts and Crafts movement in Scotland were in the stained glass revival of the 1850s, pioneered by James Ballantine (1808–77). His major works included the great west window of Dunfermline Abbey and the scheme for St. Giles Cathedral, Edinburgh. In Glasgow it was pioneered by Daniel Cottier (1838–91), who had probably studied with Ballantine, and was directly influenced by William Morris, Ford Madox Brown and John Ruskin. His key works included the "Baptism of Christ" in Paisley Abbey, (c. 1880). His followers included Stephen Adam and his son of the same name. The Glasgow-born designer and theorist Christopher Dresser (1834–1904) was one of the first, and most important, independent designers, a pivotal figure in the Aesthetic Movement and a major contributor to the allied Anglo-Japanese movement. The movement had an "extraordinary flowering" in Scotland where it was represented by the development of the 'Glasgow Style' which was based on the talent of the Glasgow School of Art. Celtic revival took hold here, and motifs such as the Glasgow rose became popularised. Charles Rennie Mackintosh and the Glasgow School of Art were to influence others worldwide.
Continental Europe.
In continental Europe, the revival and preservation of national styles was an important motive of Arts and Crafts designers; for example, in Germany, after unification in 1871 under the encouragement of the "Bund für Heimatschutz" (1897) and the "Vereinigte Werkstätten für Kunst im Handwerk" founded in 1898 by Karl Schmidt; and in Hungary Károly Kós revived the vernacular style of Transylvanian building. In central Europe, where several diverse nationalities lived under powerful empires (Germany, Austria-Hungary and Russia), the discovery of the vernacular was associated with the assertion of national pride and the striving for independence, and, whereas for Arts and Crafts practitioners in Britain the ideal style was to be found in the medieval, in central Europe it was sought in remote peasant villages.
Widely exhibited in Europe, the Arts and Crafts style's simplicity inspired designers like Henry van de Velde and styles such as Art Nouveau, the Dutch De Stijl group, Vienna Secession, and eventually the Bauhaus style. Pevsner regarded the style as a prelude to Modernism, which used simple forms without ornamentation.
The earliest Arts and Crafts activity in continental Europe was in Belgium in about 1890, where the English style inspired artists and architects including can de Velde, Gabriel Van Dievoet, Gustave Serrurier-Bovy and a group known as "La Libre Esthétique" (Free Aesthetic).
The Deutscher Werkbund, formed in 1899, was an arts and crafts association influenced by William Morris. Its leading members, van de Velde and Hermann Muthesius, had opposing opinions about the place of machinery in production: van de Velde thought mass production threatened creativity and individuality; Muthesius championed mass production, standardisation and an affordable, democratic art.
In Austria, the style became popular in Vienna, inspired by an exhibition of the works of Charles Rennie Mackintosh and Charles Robert Ashbee. The Wiener Werkstätte, founded in 1903 by Josef Hoffmann and Koloman Moser, was influenced by the Arts and Crafts principles of the "unity of the arts" and the hand-made. It had an independent role in the development of Modernism, with its Wiener Werkstätte Style.
In Finland, an idealistic artists' colony in Helsinki was designed by Herman Gesellius, Armas Lindgren and Eliel Saarinen, who worked in the National Romantic style, akin to the British Gothic Revival.
In Hungary, under the influence of Ruskin and Morris, a group of artists and architects, including Károly Kós, Aladár Körösfői-Kriesch and Ede Toroczkai Wigand, discovered the folk art and vernacular architecture of Transylvania. Many of Kós's buildings, including those in the Budapest zoo and the Wekerle estate in the same city, show this influence.
In Russia, Viktor Hartmann, Viktor Vasnetsov, Yelena Polenova and other artists associated with Abramtsevo Colony sought to revive the quality of medieval Russian decorative arts quite independently from the movement in Great Britain.
In Iceland, Sölvi Helgason's work shows Arts and Crafts influence.
North America.
In the United States, the Arts and Crafts style initiated a variety of attempts to reinterpret European Arts and Crafts ideals for Americans. These included the "Craftsman"-style architecture, furniture, and other decorative arts such as designs promoted by Gustav Stickley in his magazine, "The Craftsman" and designs produced on the Roycroft campus as publicized in Elbert Hubbard's "The Fra". Both men used their magazines as a vehicle to promote the goods produced with the Craftsman workshop in Eastwood, NY and Elbert Hubbard's Roycroft campus in East Aurora, NY. A host of imitators of Stickley's furniture (the designs of which are often mislabelled the "Mission Style") included three companies established by his brothers.
The terms "American Craftsman" or "Craftsman style" are often used to denote the style of architecture, interior design, and decorative arts that prevailed between the dominant eras of Art Nouveau and Art Deco in the USA, or approximately the period from 1910 to 1925. The movement was particularly notable for the professional opportunities it opened up for women as artisans, designers and entrepreneurs who founded and ran, or were employed by, such successful enterprises as the Kalo Shops, Rookwood Pottery, and Tiffany Studios. In Canada, the term "Arts and Crafts" predominates, but "Craftsman" is also recognized.
While the Europeans tried to recreate the virtuous crafts being replaced by industrialisation, Americans tried to establish a new type of virtue to replace heroic craft production: well-decorated middle-class homes. They claimed that the simple but refined aesthetics of Arts and Crafts decorative arts would ennoble the new experience of industrial consumerism, making individuals more rational and society more harmonious. The American Arts and Crafts movement was the aesthetic counterpart of its contemporary political philosophy, progressivism. Characteristically, when the Arts and Crafts Society began in October 1897 in Chicago, it was at Hull House, one of the first American settlement houses for social reform.
Arts and Crafts ideals disseminated in America through journal and newspaper writing were supplemented by societies that sponsored lectures. The first was organized in Boston in the late 1890s, when a group of influential architects, designers, and educators determined to bring to America the design reforms begun in Britain by William Morris; they met to organize an exhibition of contemporary craft objects. The first meeting was held on January 4, 1897, at the Museum of Fine Arts (MFA) in Boston to organize an exhibition of contemporary crafts. When craftsmen, consumers, and manufacturers realised the aesthetic and technical potential of the applied arts, the process of design reform in Boston started. Present at this meeting were General Charles Loring, Chairman of the Trustees of the MFA; William Sturgis Bigelow and Denman Ross, collectors, writers and MFA trustees; Ross Turner, painter; Sylvester Baxter, art critic for the "Boston Transcript"; Howard Baker, A.W. Longfellow Jr.; and Ralph Clipson Sturgis, architect.
The first American Arts and Crafts Exhibition began on April 5, 1897, at Copley Hall, Boston featuring more than 1000 objects made by 160 craftsmen, half of whom were women. Some of the advocates of the exhibit were Langford Warren, founder of Harvard's School of Architecture; Mrs. Richard Morris Hunt; Arthur Astor Carey and Edwin Mead, social reformers; and Will H. Bradley, graphic designer. The success of this exhibition resulted in the incorporation of The Society of Arts and Crafts (SAC), on June 28, 1897, with a mandate to "develop and encourage higher standards in the handicrafts." The 21 founders claimed to be interested in more than sales, and emphasized encouragement of artists to produce work with the best quality of workmanship and design. This mandate was soon expanded into a credo, possibly written by the SAC's first president, Charles Eliot Norton, which read:
This Society was incorporated for the purpose of promoting artistic work in all branches of handicraft. It hopes to bring Designers and Workmen into mutually helpful relations, and to encourage workmen to execute designs of their own. It endeavors to stimulate in workmen an appreciation of the dignity and value of good design; to counteract the popular impatience of Law and Form, and the desire for over-ornamentation and specious originality. It will insist upon the necessity of sobriety and restraint, or ordered arrangement, of due regard for the relation between the form of an object and its use, and of harmony and fitness in the decoration put upon it.
Also influential were the Roycroft community initiated by Elbert Hubbard in Buffalo and East Aurora, New York, Joseph Marbella, utopian communities like Byrdcliffe Colony in Woodstock, New York, and Rose Valley, Pennsylvania, developments such as Mountain Lakes, New Jersey, featuring clusters of bungalow and chateau homes built by Herbert J. Hapgood, and the contemporary studio craft style. Studio pottery—exemplified by the Grueby Faience Company, Newcomb Pottery in New Orleans, Marblehead Pottery, Teco pottery, Overbeck and Rookwood pottery and Mary Chase Perry Stratton's Pewabic Pottery in Detroit, as well as the art tiles made by Ernest A. Batchelder in Pasadena, California, and idiosyncratic furniture of Charles Rohlfs all demonstrate the influence of Arts and Crafts.
Architecture.
The "Prairie School" of Frank Lloyd Wright, George Washington Maher and other architects in Chicago, the Country Day School movement, the bungalow and ultimate bungalow style of houses popularized by Greene and Greene, Julia Morgan, and Bernard Maybeck are some examples of the American Arts and Crafts and American Craftsman style of architecture. Restored and landmark-protected examples are still present in America, especially in California in Berkeley and Pasadena, and the sections of other towns originally developed during the era and not experiencing post-war urban renewal. Mission Revival, Prairie School, and the 'California bungalow' styles of residential building remain popular in the United States today.
Museums.
The Museum of the American Arts and Crafts Movement is under construction in St. Petersburg, Florida, scheduled to open in 2017.
Asia.
In Japan, Yanagi Sōetsu, creator of the Mingei movement which promoted folk art from the 1920s onwards, was influenced by the writings of Morris and Ruskin. Like the Arts and Crafts movement in Europe, Mingei sought to preserve traditional crafts in the face of modernising industry.
Architecture.
Many of the leading of the Arts and Crafts movement were trained as architects (e.g. William Morris, A. H. Mackmurdo, C. R. Ashbee, W. R. Lethaby) and it was on building that the movement had its most visible and lasting influence.
Red House, in Bexleyheath, London, designed for Morris in 1859 by architect Philip Webb, exemplifies the early Arts and Crafts style, with its well-proportioned solid forms, wide porches, steep roof, pointed window arches, brick fireplaces and wooden fittings. Webb rejected classical and other revivals of historical styles based on grand buildings, and based his design on British vernacular architecture, expressing the texture of ordinary materials, such as stone and tiles, with an asymmetrical and picturesque building composition.
The London suburb of Bedford Park, built mainly in the 1880s and 1890s, has about 360 Arts and Crafts style houses and was once famous for its Aesthetic residents. Several Almshouses were built in the Arts and Crafts style, for example, Whiteley Village, Surrey, built between 1914 and 1917, with over 280 buildings, and the Dyers Almshouses, Sussex, built between 1939 and 1971. Letchworth Garden City, the first garden city, was inspired by Arts and Crafts ideals. The first houses were designed by Barry Parker and Raymond Unwin in the vernacular style popularized by the movement and the town became associated with high-mindedness and simple living. The sandal-making workshop set up by Edward Carpenter moved from Yorkshire to Letchworth Garden City and George Orwell's jibe about "every fruit-juice drinker, nudist, sandal-wearer, sex-maniac, Quaker, ‘Nature Cure’ quack, pacifist, and feminist in England" going to a socialist conference in Letchworth has become famous.
Garden design.
Gertrude Jekyll applied Arts and Crafts principles to garden design. She worked with the English architect, Sir Edwin Lutyens, for whose projects she created numerous landscapes, and who designed her home "Munstead Wood", near Godalming in Surrey.
Art education.
Morris's ideas were adopted by the New Education philosophy in the late 1880s, which incorporated handicraft teaching in schools at Abbotsholme (1889) and Bedales (1892), and his influence has been noted in the social experiments of Dartington Hall during the mid-20th century.
Arts and Crafts practitioners in Britain were critical of the government system of art education which was based on design in the abstract with little teaching of practical craft. This lack of craft training also caused concern in industrial and official circles, and in 1884 a Royal Commission (accepting the advice of William Morris) recommended that art education should pay more attention to the suitability of design to the material in which it was to be executed. The first school to make this change was the Birmingham School of Arts and Crafts, which "led the way in introducing executed design to the teaching of art and design nationally (working in the material for which the design was intended rather than designing on paper). In his external examiner's report of 1889, Walter Crane praised Birmingham School of Art in that it 'considered design in relationship to materials and usage.'" Under the direction of Edward Taylor, its headmaster from 1877 to 1903, and with the help of Henry Payne and Joseph Southall, the Birmingham School became a leading Arts-and-Crafts centre.
Other local authority schools also began to introduce more practical teaching of crafts, and by the 1890s Arts and Crafts ideals were being disseminated by members of the Art Workers Guild into art schools throughout the country. Members of the Guild held influential positions: Walter Crane was director of the Manchester School of Art and subsequently the Royal College of Art; F.M. Simpson, Robert Anning Bell and C.J.Allen were respectively professor of architecture, instructor in painting and design, and instructor in sculpture at Liverpool School of Art; Robert Catterson-Smith, the headmaster of the Birmingham Art School from 1902-1920, was also an AWG member; W. R. Lethaby and George Frampton were inspectors and advisors to the London County Council's (LCC) education board and in 1896, largely as a result of their work, the LCC set up the Central School of Arts and Crafts and made them joint principals. Shortly after, the Camberwell School of Arts and Crafts was set up on Arts and Crafts lines by the local borough council.
As head of the Royal College of Art in 1898, Crane tried to reform it along more practical lines, but resigned after a year, defeated by the bureaucracy of the Board of Education, who then appointed Augustus Spencer to implement his plan. Spencer brought in Lethaby to head its school of design and several members of the Art Workers' Guild as teachers. Ten years after reform, a committee of inquiry reviewed the RCA and found that it was still not adequately training students for industry. In the debate that followed the publication of the committee's report, C.R.Ashbee published a highly critical essay, "Should We Stop Teaching Art", in which he called for the system of art education to be completely dismantled and for the crafts to be learned in state-subsidised workshops instead. Lewis Foreman Day, an important figure in the Arts and Crafts movement, took a different view in his dissenting report to the committee of inquiry, arguing for greater emphasis on principles of design against the growing orthodoxy of teaching design by direct working in materials. Nevertheless, the Arts and Crafts ethos thoroughly pervaded British art schools and persisted, in the view of the historian of art education, Stuart MacDonald, until after the Second World War.
Bibliography and further reading.
Kreisman, Lawrence. The Arts & Craft Movement in the Pacific Northwest. Timber Press.

</doc>
<doc id="48993" url="https://en.wikipedia.org/wiki?curid=48993" title="Quiz bowl">
Quiz bowl

Quiz Bowl (quizbowl, scholar bowl, scholastic bowl, academic bowl, etc.) is a quiz game that tests players on a wide variety of academic subjects. Standardized quiz bowl formats are played by lower school, middle school, high school, and university students throughout the United States, Canada, and the United Kingdom.
The game is typically played with a lockout buzzer system between at least two teams, usually consisting of four or five players each. Players are read questions and try to score points for their team by buzzing first and responding with the correct answer.
Quiz bowl is most commonly played in a tossup/bonus format, which consists of a series of two different types of questions. Other formats, particularly in local competitions, may deviate from the above rules.
History.
Most forms of modern quiz bowls are modeled after game shows. "College Bowl", which was created by Don Reid as a USO activity for US service men during World War II was an early influential quiz bowl program. "College Bowl", also known as "The College Quiz Bowl", started on radio in 1953 and then aired on national television from 1959 to 1970. 
In the first half of the 20th century, many other quiz bowl-like competitions were also created. Delco Hi-Q began in 1948 as a radio quiz competition sponsored by the Scott Paper Company for high school students in Delaware County, Pennsylvania. It claims to be the oldest continuously running student quiz contest in the United States. The "It's Academic" televised student quiz show program has been run for high school teams in the Washington, D.C. metropolitan area since 1961 and is recognized by the Guinness Book of World Records as the longest-running quiz program in television history. "It's Academic" has been spun off in many other US media markets and has inspired many other televised high school competitions.
In 1977, College Bowl was revived as an activity on college campuses by College Bowl Company Inc. (CBCI). In September 1990, the Academic Competition Federation (ACF) was founded as the first major alternative to The College Bowl Company. National Academic Quiz Tournaments (NAQT) was founded in 1996 and currently organizes national competitions at all levels in the United States and supplies tournament questions for grade school and college teams across North America and other parts of the world. In 2008, the College Bowl program abruptly ended, although the company itself continues to operate the Honda Campus All-Star Challenge (HCASC) for historically black colleges and universities.
Gameplay.
During a quiz bowl game, two or more teams of usually up to four or five players are read questions by a moderator. In most forms of quiz bowl, there are two basic types of questions: toss-ups and bonuses. Toss-ups are questions that any individual player can attempt to answer, and players are generally not allowed to confer with each other. Each player usually has a signaling device, also called a buzzer, to signal in at any time during the question to give an answer. If the answer given is incorrect, then no other member of that team may give an answer. If a toss-up is successfully answered, the correctly answering team is given an opportunity to answer a bonus. Bonuses are usually worth a total of 30 points, and consist of three individual questions worth ten points each. Team members are generally permitted to confer with each other on these questions.
Regional or local tournaments may dispose of any number of standard rules entirely. Some may only have toss-ups and not use bonuses at all. Some formats include a lightning round during which a team attempts to answer multiple questions as fast as possible under a given time limit, usually sixty seconds.
Match length is determined by either a game clock or the number of questions in a packet. In most formats, a game ends once the moderator has finished reading every question in a packet. Tie-breaking procedures may include reading extra toss-ups until the tie is broken or sudden-death toss-ups.
Quiz bowl tests players in a variety of academic subjects including literature, science, history, and fine arts. Additionally, some quiz bowl events may feature small amounts of popular culture content like sports, popular music, and other non-academic general knowledge subjects, although their inclusion is generally kept to a minimum.
In most quiz bowl competitions, players and coaches may protest the moderator's decision if they believe their answer was incorrectly rejected, or an opponent's answer was incorrectly accepted.
Toss-ups.
Two common types of toss-ups include buzzer-beaters and pyramidal tossups. Buzzer-beaters (also known as speed check or quick-recall questions) are relatively short, rarely more than two sentences long, and contain few clues. This type of question is written specifically to test quick recall skills of players, and does not discriminate the different levels of knowledge that the players possess.
Pyramidal or pyramid-style tossups include multiple clues and are written so that each question starts with more difficult clues and moves toward easier clues. This way the player with the most knowledge of the subject being asked about has the most opportunity to answer first. Pyramidal toss-ups are a common standard for college quiz bowl.
In most formats, correctly answering a toss-up earns a team 10 points. Extra points, usually for a total of 15 or 20 points, may be awarded if a question is answered prior to a certain clue-providing keyword in the question, an action known as "powering". Answering a tossup incorrectly is called "negging", and may incur a 5-point penalty for a team. After a neg occurs, the moderator continues reading the rest of the question for the other team. There are usually no further penalties after one team has already negged.
Bonuses.
Bonuses usually have multiple parts that are related by some common thread and may or may not be related to corresponding tossup. A team is usually rewarded with 10 points upon correctly answering each bonus part. Usually, only the team that answered the tossup correctly can answer the bonus questions, though some formats allow the opposing team to answer certain parts of the bonus not correctly answered by the team in control of the bonus, a gameplay element known as a "bounceback". Less-used types of bonus questions include list bonuses, which require players to give their answers from a requested list, and "30-20-10" bonuses, which give a number of discrete clues for a single answer in order of decreasing difficulty, with more points being awarded for giving the correct answer on an earlier clue. The 30-20-10 bonus was officially banned from ACF in 2008 and NAQT in 2009.
Variations.
Several variations on the game of quiz bowl exist that affect question structure and content, rules of play, and round format. One standardized format is the pyramidal tossup/bonus format, which is used in ACF (or mACF) and NAQT competitions. ACF/mACF tossups are written in pyramidal style and are generally much longer than College Bowl and NAQT questions. It has a rigorous emphasis on academics, with very little popular culture. Games are usually untimed and last until a total of 20 tossups are read. NAQT is another common variation on the tossup/bonus format that balances academic rigor with a wider variety of subjects, including popular culture and an increased amount of current events and geography content. Unlike many mACF events, most questions used in this format are written and sold by NAQT themselves. NAQT also uses powers their in tossups, which reward players with 15 points instead of 10 for a tossup answered before a predetermined point. Games played on NAQT rules consist of two nine-minute halves and a total of 24 tossups. NAQT tossups are typically shorter than most other pyramidal tossups because of a character limit enforced on the questions. The format used for the now-defunct College Bowl tournament uses comparatively shorter questions. Gameplay is relatively quick as it is played in eight-minute halves, to a usual total of 22–24 tossups read. The Honda Campus All Star Challenge and "University Challenge" use similar formats.
Matches played at the National Academic Championship and its affiliated tournaments are split into four quarters, with differing styles of gameplay in each phase. Individual tournaments may use worksheet quizzes, lightning rounds, or tossups without accompanying bonuses.
Preparation.
Since questions are generally derived from an unofficial canon of topics, players commonly review question content from older competitions to prepare for upcoming tournaments. In this vein, NAQT also sells lists of topics that are frequently asked about in their questions. Players also research and write questions to prepare for quiz bowls. Active participation in academic coursework can also serve as means of preparing for quiz bowls. Blind memorization of out-of-context facts is often discouraged. Team members often specialize in a few subjects. Players benefit from exposure to a broad range of school and cultural subjects, memorization, and study skills, and an improved ability to cooperate and work in teams.
Competitions.
Quiz bowl is primarily played at single-day tournaments. Some events have eligibility rules that dictate who may participate, such as allowing only freshman and sophomore players or excluding graduate students from play. Additionally, most tournaments allow multiple teams from a single school to compete.
Some schools hold intramural tournaments where any team formed from students can play. High school-level quiz bowl is occasionally played over an extended period of time schools within a league or preexisting athletic conference or even in single matches against other schools.
Some regional variants organized for grade school students include Knowledge Bowl, Ohio Academic Competition (OAC), Florida's Commissioner's Academic Challenge (CAC), and various television quiz competitions such as "It's Academic". Athletic and activities associations in some US states also organize quiz bowl competitions, including Missouri's MSHSAA, Illinois's IHSA, and Virginia's VHSL.
Additionally, various formats have been developed to test knowledge in specific areas like the Bible, classics, science, and agricultural science. DECA runs quiz bowl events at their competitions that tests knowledge on business and market topics. Gallaudet University sponsors a National Academic Bowl for deaf university students. Tournaments designated as "trash" focus on pop culture and sports trivia questions.
National tournaments.
There are several collegiate-level national championship tournaments for which teams usually qualify through regional competitions. These tournaments include:
Several national competitions are conducted in the United States every year for high school students. Compared to at the college level, there are usually many more tournaments at which teams can qualify. These include:
The following high school tournaments are for single all-star teams from each US state or other political subdivision:
Educational value.
Some proponents of reform seek to increase the educational value and fairness of quiz bowl, primarily by using pyramidal questions. Many competitions at grade school levels are criticized for their use of speed-check questions, which encourage participants to rely more on their ability to buzz in quickly than on knowledge of the subjects tested. Some tournaments such as College Bowl are criticized for being insufficiently academic, including superfluous clues in their questions, and for recycling questions from previous years. The use of "hoses", misleading clues that discourage players from buzzing in too early, is also considered a mark of "bad" quiz bowl. The use of mathematical computation problems in tossups is criticized by some for rewarding fast problem solving skills over conceptual knowledge and being non-pyramidal. Pyramidal questions are sometimes criticized for containing obscure information and being unsuitable for television.
Broadcasting.
Quiz bowl shows have been on television for many years in some areas and usually feature competitors from local high schools. Many of these competitions may have rules and formats that differ slightly from standardized quiz bowl.
"College Bowl" was broadcast on NBC radio from 1953 to 1955. The program moved to television as "General Electric College Bowl" and was broadcast from 1959 to 1970, first on CBS and later on NBC. "College Bowl" would return to CBS radio from 1979 to 1982, and HCASC was broadcast on BET from 1990 to 1995. The "Texaco Star National Academic Championship" ran from 1989 to 1991 on Discovery Channel and was hosted by Chip Beall and Mark L. Walberg. In 1994, it was syndicated as the "Star Challenge" and hosted by Mark Wahlberg. "University Challenge" is licensed from CBCI by Granada TV Ltd. and broadcast in the United Kingdom. Reach for the Top, a Canadian competition with a quiz bowl-like format, has been broadcast on the CBC in the past.
Game show contestants.
Quiz bowl has received media coverage due to the number of highly successful game show contestants with backgrounds in the activity. Despite this, most game shows have little resemblance to quiz bowl in both question content and gameplay. NAQT maintains a list of current and former quiz bowl players at any level who have appeared on TV game shows. Several of the top dollar winners in the history of "Jeopardy!" include former players such as Ken Jennings, David Madden, and Brad Rutter.

</doc>
<doc id="48994" url="https://en.wikipedia.org/wiki?curid=48994" title="Limerick (poetry)">
Limerick (poetry)

A limerick is a form of poetry, especially one in five-line, predominantly Anapaest meter with a strict rhyme scheme (AABBA), which is sometimes obscene with humorous intent. The third and fourth lines are usually shorter than the other three. The following example is a limerick of unknown origin:
The form appeared in England in the early years of the 18th century. It was popularized by Edward Lear in the 19th century, although he did not use the term. Gershon Legman, who compiled the largest and most scholarly anthology, held that the true limerick as a folk form is always obscene, and cites similar opinions by Arnold Bennett and George Bernard Shaw, describing the clean limerick as a "periodic fad and object of magazine contests, rarely rising above mediocrity". From a folkloric point of view, the form is essentially transgressive; violation of taboo is part of its function.
Form.
The standard form of a limerick is a stanza of five lines, with the first, second and fifth rhyming with one another and having three feet of three syllables each; and the shorter third and fourth lines also rhyming with each other, but having only two feet of three syllables. The defining "foot" of a limerick's meter is usually the anapaest, ("ta-ta-TUM"), but catalexis (missing a weak syllable at the beginning of a line) and extra-syllable rhyme (which adds an extra unstressed syllable) can make limericks appear amphibrachic ("ta-TUM-ta").
The first line traditionally introduces a person and a place, with the place appearing at the end of the first line and establishing the rhyme scheme for the second and fifth lines. In early limericks, the last line was often essentially a repeat of the first line, although this is no longer customary.
Within the genre, ordinary speech stress is often distorted in the first line, and may be regarded as a feature of the form: "There "was" a young "man" from the "coast";" "There "once" was a "girl" from De"troit"…" Legman takes this as a convention whereby prosody is violated simultaneously with propriety. Exploitation of geographical names, especially exotic ones, is also common, and has been seen as invoking memories of geography lessons in order to subvert the decorum taught in the schoolroom; Legman finds that the exchange of limericks is almost exclusive to comparatively well-educated males, women figuring in limericks almost exclusively as "villains or victims". The most prized limericks incorporate a kind of twist, which may be revealed in the final line or lie in the way the rhymes are often intentionally tortured, or both. Many limericks show some form of internal rhyme, alliteration or assonance, or some element of word play. Verses in limerick form are sometimes combined with a refrain to form a limerick song, a traditional humorous drinking song often with obscene verses.
David Abercrombie, a phonetician, takes a different view of the limerick, and one which seems to accord better with the form. It is this: Lines one, two, and five have three feet, that is to say three stressed syllables, while lines three and four have two stressed syllables. The number and placement of the unstressed syllables is rather flexible. There is at least one unstressed syllable between the stresses but there may be more – as long as there are not so many as to make it impossible to keep the equal spacing of the stresses. (See “the young man of Japan”, below.) The following limerick is an example:
In the first line of this limerick, there are three unstressed syllables between the first and second stresses, two between the second and third, but only one unstressed syllable before the first stress. There may or may not be an unstressed syllable (or, rarely, two) after the final stress of the line. In the example above there are unstressed syllables at the end of lines three and four but not at the end of the remaining lines. Moreover, it is intrinsic to the limerick that there be a silent stress at the end of lines one, three, and five. A silent stress occurs when the reader undergoes the physiological changes associated with a stress but without any sound. To understand this, imagine that a drum is struck each time there is a stress. Then, in English verse, the drum beats would be equally spaced, regardless of the number of unstressed syllables that separate them. However, in reading a limerick, after the third beat of the first line, the next beat falls at the end of the line, not on the first stress of the second line. Thus, it is perhaps better to think of the limerick as having four stresses (the final one silent) in lines one, two, and five – and two stresses, of course, in lines three and four.
Origin of the name.
The origin of the name "limerick" for this type of poem is debated. As of several years ago, its usage was first documented in England in 1898 ("New English Dictionary") and in the United States in 1902, but in recent years several earlier uses have been documented. The name is generally taken to be a reference to the City or County of Limerick in Ireland sometimes particularly to the Maigue Poets, and may derive from an earlier form of nonsense verse parlour game that traditionally included a refrain that included "Will won't you come (up) to Limerick?"
The earliest known use of the term "limerick" for this type of poem is an 1880 reference, in a Saint John, New Brunswick newspaper, to an apparently well-known tune,
Edward Lear.
The limerick form was popularized by Edward Lear in his first "Book of Nonsense" (1846) and a later work, "More Nonsense, Pictures, Rhymes, Botany, etc". (1872). Lear wrote 212 limericks, mostly considered nonsense literature. It was customary at the time for limericks to accompany an absurd illustration of the same subject, and for the final line of the limerick to be a variant of the first line ending in the same word, but with slight differences that create a nonsensical, circular effect. The humor is not in the "punch line" ending but rather in the tension between meaning and its lack.
The following is an example of one of Edward Lear's limericks.
Lear's limericks were often typeset in three or four lines, according to the space available under the accompanying picture.
Variations.
The idiosyncratic link between spelling and pronunciation in the English language is explored in this Scottish example (where the name "Menzies" is pronounced ).
The limerick form is so well known that it can be parodied in fairly subtle ways. These parodies are sometimes called anti-limericks. The following example is of unknown origin:
Other anti-limericks deliberately break the rhyme scheme, like the following example, attributed to W.S. Gilbert, in a parody of a limerick by Lear:
Comedian John Clarke has also parodied Lear's style:
Web Cartoonist Zach Weiner, author of SMBC-Comics, wrote a reversed limerick that makes sense read top-to-bottom, and vice versa.
The British wordplay and recreational mathematics expert Leigh Mercer (1893–1977) devised the following mathematical limerick:
This is read as follows:
References.
Notes
Bibliography
External links.
Limerick bibliographies:

</doc>
<doc id="48997" url="https://en.wikipedia.org/wiki?curid=48997" title="Overseas Chinese">
Overseas Chinese

Overseas Chinese () are people of Chinese birth or descent who live outside the People's Republic of China and Republic of China (Taiwan) or Hong Kong and Macau. People of partial Chinese ancestry living outside the Greater China Area may also consider themselves overseas Chinese. Overseas Chinese can be of the Han Chinese ethnic majority, or from any of the other ethnic groups in China.
Terminology.
The Chinese language has various terms equivalent to the English "overseas Chinese" which refers to Chinese citizens residing in countries other than China: Huáqiáo () or "Hoan-kheh" in Hokkien (). The term "haigui" () refers to returned overseas Chinese and "guīqiáo qiáojuàn" () to their returning relatives.
Huáyì () refers to ethnic Chinese residing outside of China. Another often-used term is 海外华人 (Hǎiwài Huárén), a more literal translation of "overseas Chinese"; it is often used by the PRC government to refer to people of Chinese ethnicities who live outside the PRC, regardless of citizenship.
Overseas Chinese who are ethnically Han Chinese, such as Cantonese, Hoochew, Hokkien, or Hakka refer to overseas Chinese as 唐人 (Tángrén), pronounced "tòhng yàn" in Cantonese, "toung ning" in Hoochew, "Tn̂g-lâng" in Hokkien, and "tong nyin" in Hakka. Literally, it means "Tang people", a reference to Tang dynasty China when it was ruling China proper. It should be noted that this term is commonly used by the Cantonese, Hoochew, Hakka and Hokkien as a colloquial reference to the Chinese people, and has little relevance to the ancient dynasty.
The term "shǎoshù mínzú" () is added to the various terms for overseas Chinese to indicate those in the diaspora who would be considered ethnic minorities in China. The terms "shǎoshù mínzú huáqiáo huárén"; "shǎoshù mínzú huáqiáo huárén"; and "shǎoshù mínzú hǎiwài qiáobāo" () are all in usage. The Overseas Chinese Affairs Office of the PRC does not distinguish between Han and ethnic minority populations for official policy purposes. For example, members of the Tibetan diaspora may travel to China on passes granted to certain overseas Chinese. Various estimates of the overseas Chinese minority population include 3.1 million (1993), 3.4 million (2004), 5.7 million (2001, 2010), or approximately one tenth of all overseas Chinese (2006, 2011). Cross-border ethnic groups (, "kuàjìng mínzú") are not considered overseas Chinese minorities unless they left China "after" the establishment of an independent state on China's border.
History.
The Chinese people have a long history of migrating overseas. One of the migrations dates back to the Ming dynasty when Zheng He (1371–1435) became the envoy of Ming. He sent people - many of them Cantonese and Hokkien - to explore and trade in the South China Sea and in the Indian Ocean.
Chinese Civil War.
When China was under the imperial rule of the Qing Dynasty, subjects who left the Qing Empire without the Administrator's consent were considered to be traitors and were executed. Their family members faced consequences as well. However, the establishment of the Lanfang Republic () in West Kalimantan, Indonesia, as a tributary state of Qing China, attests that it was possible to attain permission. The republic lasted until 1884, when it fell under Dutch occupation as Qing influence waned.
Under the administration of the Republic of China from 1911 to 1949, these rules were abolished and many migrated outside of the Republic of China, mostly through the coastal regions via the ports of Fujian, Guangdong, Hainan and Shanghai. These migrations are considered to be among the largest in China's history. Many nationals of the Republic of China fled and settled down in South East Asia mainly between the years 1911-1949, after the Nationalist government led by Kuomintang lost to the Communist Party of China in the Chinese Civil War in 1949. Most of the nationalist and neutral refugees fled Mainland China to South East Asia(Singapore, Malaysia, Philippines, Brunei and Indonesia) as well as Taiwan, Republic of China. Many nationalists who stayed behind were persecuted or even executed.
Most of the Chinese who fled during 1911 – 1949 under the Republic of China settled down in Singapore, Malaysia and automatically gain citizenship in 1957 and 1963 as these countries gained independence. Kuomintang members who settled down in Malaysia and Singapore played a major role in the establishment of Malaysian Chinese Association. There is some evidence that they intend to reclaim mainland China from the Communists by funding the Kuomintang in China.
Waves of immigration.
Different waves of immigration led to subgroups among overseas Chinese such as the new and old immigrants in Southeast Asia, North America, Oceania, the Caribbean, Latin America, South Africa, and Russia.
In the 19th century, the age of colonialism was at its height and the great Chinese diaspora began. Many colonies lacked a large pool of laborers. Meanwhile, in the provinces of Fujian and Guangdong in China, there was a surge in emigration as a result of the poverty and ruin caused by the Taiping rebellion. The Qing Empire was forced to allow its subjects to work overseas under colonial powers. Many Hokkien chose to work in Southeast Asia (where they had earlier links starting from the Ming era), as did the Cantonese. The city of Taishan in Guangdong province was the source for many of the economic migrants. For the countries in North America and Australasia, great numbers of laborers were needed in the dangerous tasks of gold mining and railway construction. Widespread famine in Guangdong impelled many Cantonese to work in these countries to improve the living conditions of their relatives. Some overseas Chinese were sold to South America during the Punti-Hakka Clan Wars (1855–1867) in the Pearl River Delta in Guangdong. After World War II many people from the New Territories in Hong Kong emigrated to the UK (mainly England) and to the Netherlands to earn a better living.
From the mid-19th century onward, emigration has been directed primarily to Western countries such as the United States, Canada, Australia, New Zealand, Brazil, and the nations of Western Europe; as well as to Peru where they are called "tusán", Panama, and to a lesser extent to Mexico. Many of these emigrants who entered Western countries were themselves overseas Chinese, particularly from the 1950s to the 1980s, a period during which the PRC placed severe restrictions on the movement of its citizens. In 1984, Britain agreed to transfer the sovereignty of Hong Kong to the PRC; this triggered another wave of migration to the United Kingdom (mainly England), Australia, Canada, USA, Latin America and other parts of the world. The Tiananmen Square protests of 1989 further accelerated the migration. The wave calmed after Hong Kong's transfer of sovereignty in 1997. In addition, many citizens of Hong Kong hold citizenships or have current visas in other countries so if the need arises, they can leave Hong Kong at short notice. In fact, after the Tiananmen Square incident, the lines for immigration visas increased at every consulate in Hong Kong.
In recent years, the People's Republic of China has built increasingly stronger ties with African nations. Author Howard French estimates that over one million Chinese have moved in the past 20 years to Africa.
More recent Chinese presences have developed in Europe, where they number nearly a million, and in Russia, they number over 600,000, concentrated in Russian Far East. Russia’s main Pacific port and naval base of Vladivostok, once closed to foreigners and belonged to China until the late 19th century, bristles with Chinese markets, restaurants and trade houses. Experts predict that the Chinese diaspora in Russia will increase to at least 10 million by 2010 and Chinese may become the dominant ethnic group in the Russian Far East region 20 to 30 years from now.
Other experts discount such stories estimating the numbers of Chinese in Russia at less than half a million, most of whom are temporary traders. A growing Chinese community in Germany consists of around 76,000 people . An estimated 15,000 to 30,000 Chinese live in Austria.
Occupations.
The Chinese in Southeast Asian countries have established themselves in commerce and finance. In North America, Europe and Oceania, occupations are diverse and impossible to generalize; ranging from catering to significant ranks in medicine, the arts, and academia.
Overseas Chinese experience.
The Chinese usually identify a person by ethnic origin instead of nationality. As long as the person is of Chinese descent, that person is considered Chinese, and if that person lives outside of China, that person is overseas Chinese. The majority of PRC Chinese do not understand the overseas Chinese experience of being a minority, as ethnic Han Chinese comprise approximately 91% of the population.
Discrimination.
Overseas Chinese have often experienced hostility and discrimination.
In countries with small Chinese minorities, the economic disparity can be remarkable. For example, in 1998, ethnic Chinese made up just 1% of the population of the Philippines and 4% of the population in Indonesia, but have wide influence in Philippines and Indonesian private economy. The book "", describing the Chinese as a "market-dominant minority", notes that "Chinese market dominance and intense resentment amongst the indigenous majority is characteristic of virtually every country in Southeast Asia except Thailand and Singapore". (Chinese market dominance is present in Thailand, which is noted for its lack of resentment, and Singapore is majority ethnic Chinese.)
This asymmetrical economic position has incited anti-Chinese sentiment among the poorer majorities. Sometimes the anti-Chinese attitudes turn violent, such as the May 13 Incident in Malaysia in 1969 and the Jakarta riots of May 1998 in Indonesia, in which more than 2,000 people died, mostly rioters burned to death in a shopping mall. During the colonial era, some genocides killed tens of thousands of Chinese.
During the Indonesian killings of 1965–66, in which more than 500,000 people died, ethnic Chinese were killed and their properties looted and burned as a result of anti-Chinese racism on the excuse that Dipa "Amat" Aidit had brought the PKI closer to China. The anti-Chinese legislation was in the Indonesian constitution until 1998.
It is commonly held that a major point of friction is the apparent tendency of overseas Chinese to segregate themselves into a subculture. For example, the anti-Chinese Kuala Lumpur Racial Riots of 13 May 1969 and Jakarta Riots of May 1998 were believed to have been motivated by these racially biased perceptions. This analysis has been questioned by some historians, most notably Dr. Kua Kia Soong, the principal of New Era College, who has put forward the controversial argument that the May 13 Incident was a pre-meditated attempt by sections of the ruling Malay elite to incite racial hostility in preparation for a coup. In 2006, rioters damaged shops owned by Chinese-Tongans in Nukualofa. Chinese migrants were evacuated from the riot-torn Solomon Islands.
Ethnic politics can be found to motivate both sides of the debate. In Malaysia, overseas Chinese tend to support equal and meritocratic treatment on the expectation that they would not be discriminated against in the resulting competition for government contracts, university places, etc., whereas many "Bumiputra" ("native sons") Malays oppose this on the grounds that their group needs such protections in order to retain their patrimony. The question of to what extent ethnic Malays, Chinese, or others are "native" to Malaysia is a sensitive political one. It is currently a taboo for Chinese politicians to raise the issue of Bumiputra protections in parliament, as this would be deemed ethnic incitement.
Many of the overseas Chinese who worked on railways in North America in the 19th century suffered from racial discrimination in Canada and the United States. Although discriminatory laws have been repealed or are no longer enforced today, both countries had at one time introduced statutes that barred Chinese from entering the country, for example the United States Chinese Exclusion Act of 1882 (repealed 1943) or the Canadian Chinese Immigration Act, 1923 (repealed 1947).
In Australia, Chinese were targeted by a system of discriminatory laws known as the 'White Australia Policy' which was enshrined in the Immigration Restriction Act of 1901. The policy was formally abolished in 1973, and in recent years Australians of Chinese background have publicly called for an apology from the Australian Federal Government similar to that given to the 'stolen generations' of indigenous people in 2007 by the then Prime Minister Kevin Rudd.
Assimilation.
Overseas Chinese vary widely as to their degree of assimilation, their interactions with the surrounding communities (see Chinatown), and their relationship with China.
Thailand has the largest overseas Chinese community and is also the most successful case of assimilation, with many claiming Thai identity. For over 400 years, Thai-Chinese have largely intermarried and/or assimilated with their compatriots. The present Thai monarch, Chakri Dynasty, is founded by King Rama I who himself is partly Chinese. His predecessor, King Taksin of the Thonburi Kingdom, is the son of a Chinese immigrant from Guangdong Province and was born with a Chinese name. His mother, Lady Nok-iang (Thai: นกเอี้ยง), was Thai (and was later awarded the feudal title of Somdet Krom Phra Phithak Thephamat).
In the Philippines, Chinese from Guangdong were already migrating to the islands from the 9th century, and have largely intermarried with either indigenous Filipinos or Spanish colonisers. Their descendants would eventually form the bulk of the elite and ruling classes in a sovereign Philippines. Since the 1860s, most Chinese immigrants have come from Fujian; unlike earlier migrants, Fujianese settlers rarely intermarried, and thus form the bulk of the "unmixed" Chinese Filipinos. Older generations have retained Chinese traditions and the use of Minnan (Hokkien), while the majority of younger generations largely communicate in English, Filipino, and other Philippine languages, and have largely layered facets of both Western and Filipino culture onto their Chinese cultural background.
In Myanmar, the Chinese rarely intermarry (even amongst different Chinese linguistic groups), but have largely adopted the Burmese culture whilst maintaining Chinese cultural affinities.
In Cambodia, between 1965 and 1993, people with Chinese names were prevented from finding governmental employment, leading to a large number of people changing their names to a local, Cambodian name. Indonesia, and Myanmar were among the countries that do not allow birth names to be registered in foreign languages, including Chinese. But since 2003, the Indonesian government has allowed overseas Chinese to use their Chinese name or using their Chinese family name on their birth certificate.
In Vietnam, Chinese names are pronounced with Sino-Vietnamese readings. For example, the name of the previous Chinese president, (pinyin: Hú Jǐntāo), would be transcribed as "Hồ Cẩm Đào". In Western countries, the overseas Chinese generally use romanised versions of their Chinese names, and the use of local first names is also common. Vietnamese people have adopted some Chinese traditions, ancient Chinese characters, philosophy such as Confucianism, Taoism after centuries of the rule of China until the establishment of Ngo dynasty (Han-Nom: 吳朝); some Hoa people adopt the Vietnamese culture due to their similarities, however many Hoa still prefer maintaining Chinese cultural background (See Sinic world or Adoption of Chinese literary culture). The official census from 2009 accounted the Hoa population at some 823,000 individuals and ranked 6th in terms of its population size. 70% of the Hoa live in cities and towns, mostly in Ho Chi Minh city while the remainder live in the countryside in the southern provinces.
On the other hand, in Malaysia, Singapore, Indonesia and Brunei, overseas Chinese have maintained a distinct communal identity.
In East Timor, a large fraction of Chinese are of Hakka descent.
Language.
The usage of Chinese by overseas Chinese has been determined by a large number of factors, including their ancestry, their migrant ancestors' "regime of origin", assimilation through generational changes, and official policies of their country of residence. The general trend is that more established Chinese populations in the Western world and in many regions of Asia have Cantonese as either the dominant variety or as a common community vernacular, while Mandarin is much more prevalent among new arrivals, making it increasingly common in many Chinatowns.
Southeast Asia.
Within Southeast Asia, Hokkien and Cantonese has traditionally served as the "lingua franca" among overseas Chinese across most of the region and within many of its countries. However, the language situation of overseas Chinese can vary greatly amongst neighboring nations or even within.
Singapore.
Singapore has an ethnic Chinese majority population, with Mandarin recognized as one of its official languages. Furthermore, simplified Chinese characters are used in contrast to other overseas Chinese communities, which almost exclusively use traditional Chinese characters. Although the majority of ethnic Chinese in Singapore are predominantly of Hokkien descent and Hokkien has historically been the most spoken Chinese variety, the government of Singapore discourages the usage of non-Mandarin Chinese varieties through the Speak Mandarin Campaign (SMC). The Singaporean government also actively promotes English as the common language of the multiracial society of Singapore, with younger Chinese Singaporeans being mostly bilingual in Mandarin and English, while the older generations speak other Chinese varieties.
Under the SMC policy, all nationally produced non-Mandarin Chinese TV and radio programs were stopped after 1979. Additionally, Hong Kong (Cantonese) and Taiwanese dramas are unavailable in their original languages on non-cable TV. Nevertheless, since the government restriction on non-Mandarin media was relaxed in the mid-1990s, these media have become available once on again on cable TV and sold in stores. However, only Cantonese seems to have benefited from this uplift, thanks to a large following of Hong Kong popular culture, such as television dramas, cinema and Cantopop. Consequently, there has been a substantial of number of non-Cantonese Chinese Singaporeans being able to understand or speak the language, with a number of educational institutes offering Cantonese as an elective language course. Meanwhile, the number of speakers for other non-Mandarin Chinese varieties continues to decline.
Malaysia.
Malaysia is the only country besides Mainland China and Taiwan that has a complete Chinese education system, from primary school to university. Malaysian Chinese speak a wide variety of dialects, which are concentrated around particular population centers. Hokkien, the largest Chinese group, is concentrated in Penang, Klang, Kelantan and Malacca, with Penang having its own Hokkien variety. Cantonese is centered on Kuala Lumpur, Seremban, Kuantan and Ipoh, with Hakka minorities also found. Meanwhile, in East Malaysia (Malaysian Borneo), Hokkien, Teochew, Hakka and Mandarin are found except in Sibu, where the Fuzhou dialect is predominant, and in Sandakan, where Cantonese and Hakka are widely spoken.
Regardless of location, however, younger generations are educated in the Malaysian standard of Mandarin at Chinese-language schools. Also, most Chinese Malaysians can speak both Malay, the national language, and English, which is widely used in business and at tertiary level. Furthermore, Cantonese is understood by most Malaysian Chinese as it is the prevalent language used in Chinese-language media, although many are unable to speak it.
Indonesia.
Ethnic Chinese in Indonesia had been subjected for decades to official, and at times discriminatory, assimilation policies. As a result, a large number are no longer proficient in Chinese. Originally, the majority of the population emigrated from Fujian and Guangdong provinces in South China, with the first wave of arrivals preceding the Dutch colonial period in the 1700s. The four recognized varieties of Chinese spoken by the Chinese Indonesian community are, ordered by number of speakers: Hokkien, Hakka, Mandarin, and Cantonese. Additionally, Teochew and Puxian Min are also found.,Indonesia's 2010 census reported more than 2.8 million self-identified ethnic Chinese: 1.20 percent of the country's population. However other source stated that there are about 10 to 12 million Chinese living in the country, making up 5-6% of Indonesia population. Most of the Chinese resided in big cities and towns around east coast of Sumatra, north coast of Java and west coast of Kalimantan.
The distribution of Chinese varieties are scattered throughout the archipelago. On North Sumatra, Riau, Riau Islands, Jambi, two varieties of Hokkien exist, Medan Hokkien and Riau Hokkien, which incorporate local and Indonesian vocabulary. Hakka Chinese is concentrated in Bangka-Belitung, South Sumatra, Jakarta and West Kalimantan where they form a significant part of the local population. Meanwhile, Pontianak to Ketapang, Kendawangan on the southern tip of West Kalimantan is populated by Teochew speakers. Cantonese and, more recently, Mandarin have been used in Chinese-language schools and both variants are found in major cities such as Jakarta, Medan, Batam and Surabaya, with Mandarin usage increasing with recent Chinese arrivals from China and Taiwan and Cantonese speaker from Hong Kong, Guangdong, and Macau. Younger generations of Indonesian Chinese are generally fluent in Indonesian, some are fluent in English and Mandarin meanwhile the older generations are fluent in their local Chinese dialects aside Indonesian.
Thailand.
Although Thailand is home to the largest Overseas Chinese community, the level of integration is also the highest and most Thai Chinese today speak Thai as their native or main language. Most ethnic Chinese live in major cities such as Bangkok, Chiang Mai, Phuket, Chumphon , Ratchaburi , Chonburi , Hat Yai and Nakhon Sawan, and Chinatowns in these cities still feature signage in both Chinese and Thai. As of the 2000s, only a little over 200,000 Thai Chinese still speak a variant of Chinese at home. A little over half speak Teochew, the largest dialect group, followed by Hakka, Hainanese, Cantonese, and Hokkien. In commerce, Teochew, Cantonese and Thai are used as common languages and Chinese-language schools often use Cantonese as the medium of instruction due to its "lingua franca" status among most Chinese in Southeast Asia.
Vietnam.
Ethnic Chinese in Vietnam are categorized into three groups that are based on migrant history, location and level of integration. The largest group is the Hoa, numbering almost a million individuals and have historically been influential in Vietnamese society and economy. They are largely concentrated in major cities of the former South Vietnam (especially in Ho Chi Minh City) and largely speak Cantonese, with Teochew found among a significant minority.
The smaller two Chinese groups consist of the San Diu and Ngái. The San Diu number over 100,000 and are concentrated in the mountains of northern Vietnam. They actually trace their origins to Yao people rather than Han Chinese, but nevertheless have been heavily influenced by Chinese culture and speak a variant of Cantonese. Meanwhile, the Ngái are concentrated in rural areas of Central Vietnam and number around 1,000. They speak Hakka natively and use Cantonese to communicate with Hoa communities.
Cambodia.
A 2013 census estimated there to be 15,000 ethnic Chinese in Cambodia. However, Chinese community organizations have estimated that up to around 7% of the population may have Chinese ancestry. Chinese Cambodians have historically played important economic and political roles in the country and are still often overrepresented in Cambodian commerce.
As a vast majority of the group emigrated from Cambodia following the Khmer Rouge, the community has assimilated greatly into Cambodian society and many now speak Khmer as their main language. Over three-fourths of Chinese Cambodians belong to the Teochew group, which is also the most commonly spoken Chinese variety. The other two largest groups include Hokkien and Hainanese. Cantonese formed the largest group from the 17th to the mid-20th century, but form only a minority today and are concentrated in major urban centers. Nevertheless, Cantonese continues to serve as the common community language among most Chinese Cambodians. In Chinese-language schools, Mandarin is taught.
Laos.
Among the small ethnic Chinese community of Laos, Teochew and Cantonese are the two most spoken Chinese varieties. Ethnic Chinese living on the border with China speak Southwest varieties of Mandarin.
Myanmar.
Although the Burmese Chinese (or Chinese Burmese) officially make up three percent of the population, the actual figure is believed to be much higher. Among the under-counted Chinese populations are: those of mixed background; those that have registered themselves as ethnic Bamar to escape discrimination; illegal Chinese immigrants that have flooded Upper Burma since the 1990s (up to 2 million by some estimates) but are not counted due to the lack of reliable census taking. The Burmese Chinese dominate the Burmese economy today. They also have a very large presence in Burmese higher education, and make up a high percentage of the educated class in Burma. Most Burmese Chinese speak Burmese as their mother tongue. Those with higher education also speak Mandarin and/or English. The use of Chinese dialects still prevails. Hokkien (a dialect of Min Nan) is mostly used in Yangon as well as in Lower Burma, while Taishanese (a Yue dialect akin to Cantonese) and Yunnanese Mandarin are well preserved in Upper Burma.
Brunei.
A number of Chinese dialects are spoken in Brunei. Mandarin and Hokkien are the most commonly spoken varieties in the country.
Philippines.
Chinese Filipinos officially comprise 1.5% of the country's population, although demographic surveys from third parties find that 18-27% of the Philippine population have at least some Chinese ancestry, totalling up to 27 million people.
Most Chinese Filipinos are trilingual, speaking Chinese, English, and a Philippine language (most often Tagalog or Cebuano). Older Chinese Filipinos generally prefer to use Chinese, whereas younger generations prefer to use either English or a Philippine language, a result of the prohibition of Chinese-language education enacted during the dictatorship of President Marcos (1972–1986).
The most widely spoken Chinese variety is Hokkien, specifically a Filipino variant of it called Lan-nang. Other Hokkien dialects and Chinese varieties such as Cantonese, Shanghainese, and Teochew are also spoken, albeit by a very tiny population. In contrast to much of Southeast Asia, the Chinese community in the Philippines does not use Cantonese as its preferred community language, but rather Philippine Hokkien, which is also spoken informally at schools and in business among Chinese Filipinos.
In Chinese-language schools, Mandarin is taught as "Standard Chinese", although most Chinese Filipinos do not speak it at home and do not attain the same level of fluency as those of Chinese descent in China, Taiwan, and Singapore.
Due to extensive albeit informal contacts with the Ministry of Education of the Taiwan (ROC) during 1950-1990, the traditional Chinese script as well as the bopomofo are still used, although these are gradually being eased out in favor of simplified Chinese characters and pinyin starting 2005, with Chinese-language textbooks increasingly imported from both China and Singapore.
As part of a recent trend, partly due to increased contacts with other overseas Chinese in Hong Kong and Singapore, more Chinese Filipino families are now opting to use English as their first language at home. There is also a trend among some young Chinese Filipinos to relearn Hokkien, a result of increasing pride in being "ethnic Chinese" and the popularity of Taiwanese films and shows, which is associated with the rise of China in the 21st century.
Despite the perceived widespread assimilation of the Chinese Filipinos into the general Philippine population, most still form part of a "Tsinoy" community where Chinese culture is celebrated and practiced. Despite the fact that not all Chinese Filipinos can fluently speak Hokkien or any other Chinese variant, most can still understand at least some Hokkien.
On the other hand, most Chinese Mestizos (called "chhut-si-ia" in Hokkien), or those who are of mixed Chinese and Filipino, Spanish, and/or American ancestry, tend to downplay their Chinese roots and invariably consider themselves Filipino. Most Chinese Mestizos speak Tagalog or English.
North America.
Many overseas Chinese populations in North America speak some variety of Chinese. In the United States and Canada, Chinese is the third most spoken language. Yue dialects have historically been the most prevalent forms of Chinese due to immigrants being mostly from southern China from the 19th century up through the 1980s. However, Mandarin is becoming increasingly more prevalent due to the opening up of the PRC.
In New York City at least, although Mandarin is spoken as a native language among only 10% of Chinese speakers, it is used as a secondary dialect among the greatest number of them and is on its way to replace Cantonese as their lingua franca. Although Min Chinese or Hoochew, the majority of Min Chinese, is spoken natively by a third of the Chinese population there, it is not used as a lingua franca because speakers of other dialect groups do not learn Min.
In Richmond (part of the Greater Vancouver metropolitan area in Canada), 55% of the population is Chinese. Chinese words can be seen everywhere from local banks to grocery stores. In the broader Vancouver Census Metropolitan Area, 18% of the population is Chinese. Similarly in Toronto, which is the largest city in Canada, Chinese people make up 11.4% of the local population with the higher percentages of between 20-50% in the suburbs of Markham, Richmond Hill and within the city's east end, Scarborough. Cantonese and Mandarin are the most popular forms of Chinese spoken in the area.
Economic growth in the People's Republic of China has given mainland Chinese more opportunities to emigrate. A 2011 survey showed that 60% of Chinese millionaires plan to emigrate, mostly to the USA or Canada. The EB-5 Investment Visa allows many powerful Chinese to seek U.S. citizenship, and recent reports show that 75% of applicants for this visa in 2011 were Chinese. Chinese multimillionaires benefited most from the EB-5 Immigrant Investor Program in the U.S. Now, as long as one has at least US$500,000 to invest in projects listed by United States Citizenship and Immigration Services (USCIS), where it is possible to get an EB-5 green card that comes with permanent U.S. residency rights, but only in states specified by the pilot project.
Relationship with China.
Both the People's Republic of China and Taiwan maintain highly complex relationships with overseas Chinese populations. Both maintain cabinet level ministries to deal with overseas Chinese affairs, and many local governments within the PRC have overseas Chinese bureaus. Both the PRC and ROC have some legislative representation for overseas Chinese. In the case of the PRC, some seats in the National People's Congress are allocated for returned overseas Chinese. In the ROC's Legislative Yuan, there used to be eight seats allocated for overseas Chinese. These seats were apportioned to the political parties based on their vote totals in the ROC, and then the parties assigned the seats to overseas Chinese party loyalists. Now, political parties in the ROC are still allowed to assign overseas Chinese into the Legislative Yuan, but they are not required to. Most of these members elected to the Legislative Yuan hold dual citizenship, but must renounce their foreign citizenship before being sworn in.
Overseas Chinese have sometimes played an important role in Chinese politics. Most of the funding for the Chinese revolution of 1911 came from overseas Chinese.
During the 1950s and 1960s, the ROC tended to seek the support of overseas Chinese communities through branches of the Kuomintang based on Sun Yat-sen's use of expatriate Chinese communities to raise money for his revolution. During this period, the People's Republic of China tended to view overseas Chinese with suspicion as possible capitalist infiltrators and tended to value relationships with southeast Asian nations as more important than gaining support of overseas Chinese, and in the Bandung declaration explicitly stated that overseas Chinese owed primary loyalty to their home nation. On the other hand, overseas Chinese in their home nations were often persecuted for suspected or fabricated ties to "Communist China". This was used as a pretext for the massacres of ethnic Chinese in Indonesia and other Southeast Asian countries.
After the Deng Xiaoping reforms, the attitude of the PRC toward overseas Chinese changed dramatically. Rather than being seen with suspicion, they were seen as people who could aid PRC development via their skills and capital. During the 1980s, the PRC actively attempted to court the support of overseas Chinese by among other things, returning properties that had been confiscated after the 1949 revolution. More recently PRC policy has attempted to maintain the support of recently emigrated Chinese, who consist largely of Chinese seeking graduate education in the West. Many overseas Chinese are now investing in People's Republic of China providing financial resources, social and cultural networks, contacts and opportunities.
The Nationality Law of the People's Republic of China, which does not recognise dual citizenship, provides for automatic loss of PRC citizenship when a former PRC citizen both settles in another country "and" acquires foreign citizenship. For children born overseas of a PRC citizen, whether the child receives PRC citizenship at birth depends on whether the PRC parent has settled overseas: ""Any person born abroad whose parents are both Chinese nationals or one of whose parents is a Chinese national shall have Chinese nationality. But a person whose parents are both Chinese nationals and have both settled abroad, or one of whose parents is a Chinese national and has settled abroad, and who has acquired foreign nationality at birth shall not have Chinese nationality"" (Art 5).
By contrast, the Nationality Law of the Republic of China, which both permits and recognises dual citizenship, considers such persons to be citizens of the ROC (if their parents have household registration in Taiwan).
Returning and re-emigration.
With People's Republic of China's growing economic strength and the influence on the world, many overseas Chinese have begun to migrate back to China even though many mainland Chinese millionaires are considering emigrating out of the nation for better opportunities.
With China being the second largest economy in the world, this trend is expected to rise even more in the future as China's vigorous economy is poised to surpass the United States in the upcoming decade. For instance, in the case of Indonesia and Burma, political and ethnic strife has cause a significant number of people of Chinese origins to re-emigrate. Other Southeast Asian countries with large Chinese communities such as Malaysia, the economic rise of People's Republic of China has made it an attractive destination for many Malaysian Chinese to re-emigrate. As the Chinese economy opens up, Malaysian Chinese act as a bridge because many Malaysian Chinese are educated in the United States or Britain but can also understand the Chinese language and culture making it easier for potential entrepreneurial and business to be done between the people among the two countries.
Economic impact.
Overseas Chinese are estimated to control US$1.5 to 2 trillion in liquid assets and have considerable amounts of wealth to stimulate economic power in China. Overseas Chinese often send remittances back home to family members to help better them financially and socioeconomically. China ranks second after India of top remittance receiving countries in 2010 with over US$51 billion sent. The overseas Chinese business community of Southeast Asia, known as the bamboo network, has a prominent role in the region's private sectors.
Country statistics.
There are over 50 million overseas Chinese. Most overseas Chinese are living in Southeast Asia where they make up a majority of the population of Singapore and significant minority populations in Thailand, Malaysia, Indonesia, Brunei, the Philippines, and Vietnam.

</doc>
<doc id="48998" url="https://en.wikipedia.org/wiki?curid=48998" title="591">
591

__NOTOC__
Year 591 (DXCI) was a common year starting on Monday (link will display the full calendar) of the Julian calendar. The denomination 591 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="48999" url="https://en.wikipedia.org/wiki?curid=48999" title="Chinese Americans">
Chinese Americans

Chinese Americans, also known as American Chinese or Sino-Americans, comprise Americans who have full or partial Chinese – particularly Han Chinese – heritage. Chinese Americans constitute one group of overseas Chinese and also a subgroup of East Asian Americans, which is further a subgroup of Asian Americans. Many Chinese Americans are immigrants along with their descendants from China, Hong Kong, Macau, Taiwan, as well as from other countries that include large populations of the Chinese diaspora.
Demographic research tends to categorize overseas Chinese who have immigrated from South East Asia and South America and immigrants from China, Hong Kong, Macau, and Taiwan as Chinese Americans; however, both the governments of the Republic of China and the United States refer to Taiwanese Americans as a separate subgroup of Chinese Americans.
The Chinese American community is the largest overseas Chinese community outside of Asia. It is also the third largest in the Chinese diaspora, behind the Chinese communities in Thailand and Malaysia. The Chinese American community comprises the largest ethnic group of Asian Americans, comprising 25.9% of the Asian American population as of 2010. Americans of Chinese descent, including those with partial Chinese ancestry constitute 1.2% of the total U.S. population as of 2010. According to the 2010 census, the Chinese American population numbered approximately 3.8 million. In 2010, half of Chinese-born people living in the United States resided in the states of California and New York.
History.
The first Chinese immigrants arrived in 1820, according to U.S. government records. 325 men are known to have arrived before the 1849 California Gold Rush, which drew the first significant number of laborers from China who mined for gold and performed menial labor. There were 25,000 immigrants by 1852, and 105,465 by 1880, most of whom lived on the West Coast. They formed over a tenth of California's population. Nearly all of the early immigrants were young males with low educational levels from six districts in Guangdong Province.
The Chinese came to California in large numbers during the California Gold Rush, with 40,400 being recorded as arriving from 1851–1860, and again in the 1860s, when the Central Pacific Railroad recruited large labor gangs, many on five-year contracts, to build its portion of the Transcontinental Railroad. The Chinese laborers worked out well and thousands more were recruited until the railroad's completion in 1869. Chinese labor provided the massive workforce needed to build the majority of the Central Pacific's difficult route through the Sierra Nevada mountains and across Nevada.
The Chinese population rose from 2,716 in 1851 to 63,000 by 1871. In the decade 1861-70, 64,301 were recorded as arriving, followed by 123,201 in 1871-80 and 61,711 in 1881-1890. 77% were located in California, with the rest scattered across the West, the South, and New England. Most came from Southern China looking for a better life, escaping a high rate of poverty left after the Taiping Rebellion.
The initial immigration group may have been as high as 90% male, because most immigrants came with the thought of earning money, and then returning to China to start a family. Those that stayed in America faced the lack of suitable Chinese brides, because Chinese women were not allowed to immigrate to the US in significant numbers after 1872. As a result, many isolated mostly-bachelor communities slowly aged in place with very low Chinese birth rates. Later, as a result of the Fourteenth Amendment and the 1898 "United States v. Wong Kim Ark" Supreme Court decision, ethnic Chinese born in the United States became American citizens. However, Chinese immigration had remained severely restricted by the Chinese Exclusion Act (1882) until 1943 when the Magnuson Act allowed a national quota of 105 Chinese immigrants per year. 
In the mid 1850s, 70 to 150 Chinese were living in New York City and 11 of them married Irish women. In 1906, "The New York Times" (6 August) reported that 300 white women (Irish American) were married to Chinese men in New York, with many more cohabited. In 1900, based on Liang research, of the 120,000 men in more than 20 Chinese communities in the United States, he estimated that one out of every twenty Chinese men (Cantonese) was married to white women. In the 1960s census showed 3500 Chinese men married to white women and 2900 Chinese women married to white men. Originally at the start of the 20th century there was a 55% rate of Chinese men in New York engaging in interracial marriage which was maintained in the 1920s but in the 1930s it slid to 20%.
During and after World War II, severe immigration restrictions were eased as the United States allied with China against Japanese expansionism. Later reforms in the 1960s placed increasing value on family unification, allowing relatives of U.S. citizens to receive preference in immigration.
The Chinese American experience has been documented at the Museum of Chinese in America in Manhattan's Chinatown since 1980.
Demographics.
Population (1840–present).
The chart on the right shows the total number of ethnic Chinese in the United States since 1850.
According to the 2012 Census estimates, the three metropolitan areas with the largest Chinese American populations were the Greater New York Combined Statistical Area at 735,019 people, the San Jose-San Francisco-Oakland Combined Statistical Area at 629,243 people, and the Greater Los Angeles Combined Statistical Area at about 566,968 people. New York City is home to the highest Chinese American population of any city proper (522,619), while the Los Angeles County city of Monterey Park has the highest percentage of Chinese Americans of any municipality, at 43.7% of its population, or 24,758 people.
The states with the largest estimated Chinese American populations, according to both the 2010 Census, were California (1,253,100; 3.4%), New York (577,000; 3.0%), Texas (157,000; 0.6%), New Jersey (134,500; 1.5%), Massachusetts (123,000; 1.9%), Illinois (104,200; 0.8%), Washington (94,200; 1.4%), Pennsylvania (85,000; 0.7%), Maryland (69,400; 1.2%), Virginia (59,800; 0.7%), and Ohio (51,033; 0.5%). The state of Hawaii has the highest concentration of Chinese Americans at 4.0%, or 55,000 people.
The New York City Metropolitan Area, consisting of New York City, Long Island, and nearby areas within the states of New York, New Jersey, Connecticut, and Pennsylvania, is home to the largest Chinese American population of any metropolitan area within the United States and the largest Chinese population outside of China, enumerating 682,265 individuals as of the 2010 United States Census, and including at least nine Chinatowns. Continuing significant immigration from Mainland China, both legal and illegal in origin, has spurred the ongoing rise of the Chinese American population in the New York metropolitan area; this immigration continues to be fueled by New York's status as an alpha global city, its high population density, its extensive mass transit system, and the New York metropolitan area's enormous economic marketplace.
Also on the East Coast, the Washington, Boston, and Philadelphia metropolitan areas have significant Chinese American communities. The Washington, D.C. suburbs of Montgomery County, Maryland, and Fairfax County, Virginia, are 3.9% and 2.4% Chinese American, respectively. Boston's Chinatown is the only historical Chinese neighborhood within New England. The Boston suburb of Quincy also has a prominent Chinese American population, especially within the North Quincy area.
San Francisco, California has the highest per capita concentration of Chinese Americans of any major city in the United States, at an estimated 21.4%, or 172,181 people, and contains the second-largest total number of Chinese Americans of any U.S. city. San Francisco's Chinatown was established in the 1840s, making it the oldest Chinatown in North America and one of the largest neighborhoods of Chinese people outside of Asia, composed in large part by immigrants hailing from Guangdong province and also many from Hong Kong. The San Francisco neighborhoods of Sunset District and Richmond District also contain significant Chinese populations.
In addition to the big cities, smaller pockets of Chinese Americans are also dispersed in rural towns, often university-college towns, throughout the United States. For example, the number of Chinese Americans, including college professors, doctors, professionals, and students, has increased over 200% from 2005 to 2010 in Providence, Rhode Island, a small city with a large number of colleges.
Income and social status of these Chinese-American locations vary widely. Although many Chinese Americans in Chinatowns of large cities are often members of an impoverished working class, others are well-educated upper-class people living in affluent suburbs. The upper and lower-class Chinese are also widely separated by social status and class discrimination. In California's San Gabriel Valley, for example, the cities of Monterey Park and San Marino are both Chinese American communities lying geographically close to each other but they are separated by a large socioeconomic gap.
A third of a million Chinese Americans are not United States citizens.
Cultural centers.
A list of large cities (250,000+ residents) with a Chinese-American population in excess of 1% of the general population in 2010.
Social status and assimilation.
Some noteworthy historical Chinese contributions include building the western half of the Transcontinental Railroad, and levees in the Sacramento River Delta; the popularization of Chinese American food; and the introduction of Chinese and East Asian culture to America, such as Buddhism, Taoism, and Kung fu.
Chinese immigrants to the United States brought many of their ideas and values with them. Some of these have continued to influence later generations. Among them are Confucian respect for elders. Similarly, education and the civil service were the most important path for upward social mobility in China. The first Broadway show about Asian Americans was "Flower Drum Song" which premiered on Broadway in 1958; the hit "Chinglish" premiered on Broadway in 2011.
In most American cities with significant Chinese populations, the new year is celebrated with cultural festivals and parties. In Seattle, the is held every year. Other important festivals include the Dragon Boat Festival and the Mid-Autumn Festival.
Stereotypes.
Analysis indicated that most non-Asian Americans do not differentiate between Chinese Americans and East Asian Americans generally, and perceptions of both groups are nearly identical. A 2001 survey of Americans' attitudes toward Asian Americans and Chinese Americans indicated that one fourth of the respondents had somewhat or very negative attitude toward Chinese Americans in general. The study did find several positive perceptions of Chinese Americans: strong family values (91%); honesty as entrepreneurs (77%); high value on education (67%).
Language.
Chinese is the third most-spoken language in the United States, almost completely spoken within Chinese American populations and by immigrants or the descendants of immigrants, especially in California. Over 2 million Americans speak some variety of Chinese, with Mandarin Chinese becoming increasingly common due to immigration from mainland China and Taiwan.
In New York City at least, although Mandarin is spoken as a native language among only 10% of American born Chinese speakers, it is used as a secondary dialect to English. In addition, immigration from Fujian is bringing an increasingly large number of Min speakers. Wu dialects, previously unheard of in the United States, are now spoken by a minority of recent Chinese immigrants hailing from Jiangsu, Zhejiang, and Shanghai.
Although Chinese Americans grow up learning English, some teach their children Chinese for a variety of reasons: preservation of an ancient civilization, preservation of a group identity, preservation of their cultural ancestry, desire for easy communication with each other and their relatives, and the perception that Chinese is a very useful language, regardless of China's economic strength. Although Simplified Chinese is the most oft-written language in China, United States public notices and signage in Chinese are generally in Traditional Chinese.
Religion.
The Chinese American community differs from the rest of the population in that the majority of Chinese Americans do not report a religious affiliation. 43% of Chinese Americans switched to a different religion and 54% stayed within their childhood religion within their lifetime. According to the Pew Research Center's 2012 Asian-American Survey, 52% of Chinese Americans aged 15 and over said that they didn't have any religious affiliation. This is also compared with the religious affiliation of Asian American average of 26% and a national average of 19%.
Of Chinese Americans who were religious, 15% were Buddhist, 8% were Catholic, and 22% belonged to a Protestant denomination. Fully half of Chinese Americans (52%)—including 55% of those born in the U.S. and 51% of those born overseas—describe themselves as religiously unaffiliated. Because Chinese Americans are the largest subgroup of Asian Americans, nearly half of all religiously unaffiliated Asians in the U.S. are of Chinese descent (49%).
Politics.
Chinese Americans are divided among many subgroups based on factors such as age, nativity, and socioeconomic status and do not have uniform attitudes about the People's Republic of China (Communist China) or the Republic of China (Taiwan Kuomintang), about the United States, or about Chinese nationalism. Different subgroups of Chinese Americans also have radically different and sometimes very conflicting political priorities and goals.
In 2013, Chinese Americans were the least likely Asian American ethnicity to be affiliated with a political party.
Nonetheless, Chinese Americans are clustered in majority-Democratic states and have increasingly voted Democratic in recent presidential elections, following the trend for Asian Americans in general. Polling just before the 2004 U.S. Presidential Election found John Kerry was favored by 58% of Chinese Americans and George W. Bush by only 23%, as compared with a 54/44 split in California, a 58/40 split in New York, and a 48/51 split in America as a whole on Election Day itself. In the 2012 presidential election, 81% of Chinese American voters selected Barack Obama over Mitt Romney.
Chinese Americans were an important source of funds for Han revolutionaries during the later Qing dynasty, and Sun Yat-sen was raising money in America at the time of the Xinhai Revolution, which established the Republic of China. During the Cultural Revolution, Chinese Americans, as overseas Chinese in general, were viewed as capitalist traitors by the PRC government. This attitude changed dramatically in the late 1970s with the reforms of Deng Xiaoping. Increasingly, Chinese Americans were seen as sources of business and technical expertise and capital who could aid in China's economic and other development.
Immigration.
Economic growth in the People's Republic of China has given mainland Chinese more opportunities to emigrate. A 2011 survey showed that 60% of Chinese millionaires plan to emigrate and 40% of Chinese millionaires selecting the United States as the top destination for immigration. The EB-5 Investment Visa allows many powerful Chinese to seek U.S. citizenship, and recent reports show that 75% of applicants for this visa in 2011 were Chinese. Chinese multimillionaires benefited most from the EB-5 Immigrant Investor Program in the U.S. Now, as long as one has at least US$500,000 to invest in projects listed by United States Citizenship and Immigration Services (USCIS), where it is possible to get an EB-5 green card that comes with permanent U.S. residency rights, but only in states specified by the pilot project.
Socioeconomics.
Education.
Overall, as a demographic group, Chinese Americans are highly educated and earn higher incomes when compared to other demographic groups in the United States. Educational achievements of Chinese in the United States are one of the highest among Asian Americans and also among all ethnic groups in the United States. Chinese Americans often have some of the highest averages in tests such as SAT, GRE, etc. in the United States. Although verbal scores lag somewhat due to the influx of new immigrants, combined SAT scores have also been higher than for most Americans. Chinese Americans are the largest racial group on all but one of the nine fully established University of California campuses.
They are the largest group among US National Merit Scholarship awardees in California, They are more likely to apply to competitively elite higher education institutions. They also constitute 24% of all Olympic Seattle Scholarship winners, 33% of USA Math Olympiad winners, 15.5% of Putnam Math Competition winners, and 36% of Duke Talent Identification Grand Recognition Ceremony attendees from the Dallas Metropolitan area.
International students studying at various higher education institutions around the United States account for a significant percentage of the international student body. International undergraduates, who make up 8% of Duke's undergraduate body, come from China more than any other country. International Chinese students also comprise 11% of the nearly 5,800 freshmen at the University of Washington. Mainland China is the top sending country of international students to the United States. As a result of its growing economy and large population, more middle-class families from China are able to afford American college tuition, bringing an influx of Chinese students to study abroad in the United States. With a more diverse educational background and higher level of English proficiency, international Chinese students also value American degrees, as it gives them a notable advantage over their college-educated counterparts in China by the time they return to their native country to seek employment.
Due to cultural factors, many Chinese international students are brand name conscious, choosing nationally ranked elite higher education institutes throughout the United States as their target schools. International Chinese students are also widely found at many elite liberal arts colleges such as Barnard College and Mount Holyoke. Students from China gravitate towards Americans colleges and universities for their high quality and the style of education which stresses interdisciplinary approaches, creativity, student participation and critical thinking.
Chinese students comprise 18% of the international student population in America, and make up 32.2% of the undergraduate students and 48.8% of the graduate students. Chinese international students tend to gravitate towards technical majors that involve heavy use of mathematics and the natural sciences. 27.5% of international Chinese students study business management, finance, or economics, 19.2% study engineering, 11.5% study the life sciences and 10.6% study math or computer science.
Largely driven by educational immigration, among American PhD recipients in fields related to science and engineering, 25% of the recipients are ethnic Chinese. According to the 2010 U.S. Census Bureau of Labor Statistics, 51.8% of all Chinese Americans have attained at least a bachelor's degree, compared with 28.2% nationally and 49.9% for all Asian American groups. The Census reports that 54.7% of Chinese American men attained a bachelor's degree and 49.3% of Chinese American women attained a bachelor's degree. In addition, 26.6% of all Chinese Americans in the United States possess a master's, doctorate or other professional degree, compared to 20.3% for all Asian Americans, and is roughly two and a half times above the national average, with high educational attainment largely driven by educational immigration.
Employment.
There has been a significant change in the perceptions about Chinese Americans. In as little as 100 years of American history, stereotypes of Chinese Americans have changed to portraying a hard working and educated minority. Thus, most Chinese Americans work as white collar professionals, many of whom are highly educated, salaried professionals whose work is largely self-directed in management, professional, and related occupations such as engineering, medicine, investment banking, law, and academia. 53.1% of Chinese Americans work in many white collar professions compared with 48.1% for all Asian Americans and a national average of 35.1%. They make up 2% of working physicians in the United States. Chinese Americans also make up a third of the Asian American high tech professional workforce and a tenth of the entire Silicon Valley workforce. Chinese Americans also hold lower unemployment rates than the population average with a figure of 4.7% compared to a national rate of 5.9% in 2010.
Many Chinese Americans have turned to the high tech center to jump-start potential computer and internet startups to capitalize on the regions wealth of venture capital, business expertise, and cultural and financial incentives for innovation. Ethnic Chinese have been successful in starting new firms in technology centers across the United States, including California's Silicon Valley. Chinese Americans have been disproportionately successful in high technology sectors, as evidenced by the Goldsea 100 Compilation of America's Most Successful Asian Entrepreneurs. Chinese Americans accounted for 4% of people listed in the 1998 Forbes Hi Tech 100 List.
Annalee Saxenian, a UC Berkeley professor, whose research interests include the contribution of Chinese immigrants on America's technology concludes that in Silicon Valley, carried out a study that showed that since 1998, one out of five high tech start-ups in Silicon Valley were led by Chinese Americans. During the same year, 5 of the 8 fastest growing companies had Chinese American CEO's except for Yahoo, whose Jerry Yang was a founder but not a CEO. In Silicon Valley there are at least 2 to 3 dozen Chinese American organizations according to professional interests each with at least 100 members. One prominent organization of which is the Committee of 100. Immigrants from China and Taiwan were key founders in 12.8% of all Silicon Valley start-ups between 1995 to 2005. Almost 6% of the immigrants who founded companies in the innovation/manufacturing-related services field are from Mainland China and Taiwan.
Research funded by the Public Policy Institute of California indicates that in 1996, 1,786 Silicon Valley technology companies with $12.5 billion in sales and 46,000 employees were run by Indian or Chinese executives. Moreover, the pace of entrepreneurship among local immigrants is increasing rapidly. While Chinese or Indian executives are at the helm of 13% of the Silicon Valley technology businesses started between 1980 and 1985, they are running 27% of the more than 4,000 businesses started between 1991 and 1996. Start-up firms remain a primary source for new ideas and innovation for Chinese American internet entrepreneurs. Many of them are employed or directly engaged in new start-up activities. The proportional share of start-up firms by ethnic Chinese in Silicon Valley skyrocketed from 9% in 1980-1984 to about 20% between 1995-1998. By 2006, Chinese American internet entrepreneurs continued to start 20% of all Silicon Valley start-up firms, leading 2000 Silicon Valley companies, and employing 58,000 workers. They still continue to own about 20% of all information technology companies that were founded in Silicon Valley since 1980.
Numerous professional organizations in perspective in the 1990s as a support network for fellow Chinese American high tech start-ups in the valley. Between 1980 and 1999, 17% of the 11,443 high-tech firms in Silicon Valley - including some 40 publicly traded firms were controlled by ethnic Chinese. In 1990, Chinese Americans made up a third of the Asian American high tech professional workforce or 11% of the entire Silicon Valley professional workforce. In 1998, Chinese Americans managed 2001 firms, employing 41,684 workers, and ran up 13.2 billion in sales. They also account for 17% of all Silicon Valley firm owners, 10% of the professional workforce in the Valley, and 13.5% of the total sales accounting for less than 1% of the U.S. population at the time.
Though Chinese Americans are also noted for their high rates of self-employment, as they have an extensive history of self-employment dating back to the California Gold Rush in the 1880s, However, as more Chinese Americans seek higher education to elevate themselves socioeconomically, rates of self-employment are generally lower than population average. In 2007, there were over 109,614 Chinese-owned employer firms, employing more than 780,000 workers, and generating more than $128 billion in revenue.
Among Chinese-owned U.S. firms, 40% were in the professional, scientific, and technical services sector; the accommodation and food services sector; and the repair, maintenance, personal, and laundry services sector. Chinese-owned U.S. firms comprised 2% of all U.S. businesses in these sectors. Wholesale trade and accommodation and food services accounted for 50.4% of Chinese-owned business revenue. 66,505 or 15.7% of Chinese-owned firms had receipts of $250,000 or more compared with 2% for all U.S. businesses.
Economics.
With their above average educational attainment rates, Chinese Americans from all socioeconomic backgrounds have achieved significant advances in their educational levels, income, life expectancy, and other social indicators as the financial and socioeconomic opportunities offered by the United States have lifted many Chinese Americans out of poverty, bringing them into the ranks of America's middle class, upper middle class, as well as the enjoyment of substantial well being.
Chinese Americans are more likely to own homes than the general American population. According to the 2000 U.S. Census, 65% of Chinese Americans owned a home, higher than the total population's rate of 54%. In 2003, real estate economist Gary Painter of the University of Southern California Lusk Center for Real Estate Research found out that when comparing homeowners with similar income levels Los Angeles, the Chinese-American home-ownership rate is 20% higher than Whites; in San Francisco, 23% higher; and in the New York metropolitan area, 18% higher. A 2008 Asian Real Estate Association of America report released on behalf of the American community survey, Chinese Americans living in the states of Texas, New York, and California all had high home ownership rates that were significantly near or above the general population average.
According to the 2010 U.S. Census, Chinese American men had a full-time median income of $57,061 and Chinese American women had a median income of $47,224. Chinese Americans also have one of the highest median household incomes among most demographic groups in the United States, which is 30% higher than the national average but is slightly lower compared with the Asian American population.
Despite positive economic indicators, a number of economic deterrents have been noted to afflict the Chinese American community. While median income remains above some ethnic groups in the United States, studies in the wake of the 2008 financial crisis revealed that Asian men have the highest rate of persistent long-term unemployment.

</doc>
<doc id="49001" url="https://en.wikipedia.org/wiki?curid=49001" title="African-American Civil Rights Movement (1954–68)">
African-American Civil Rights Movement (1954–68)

The Civil Rights Movement or 1960s Civil Rights Movement (sometimes referred to as the "African-American Civil Rights Movement" although the term "African American" was not widely used in the 1950s and '60s) encompasses social movements in the United States whose goals were to end racial segregation and discrimination against African Americans and to secure legal recognition and federal protection of the citizenship rights enumerated in the Constitution and federal law. This article covers the phase of the movement between 1954 and 1968, particularly in the South. The leadership was African-American, and much of the political and financial support came from labor unions (led by Walter Reuther), major religious denominations, and prominent white Democratic Party politicians such as Hubert Humphrey and Lyndon B. Johnson.
The movement was characterized by major campaigns of civil resistance. Between 1955 and 1968, acts of nonviolent protest and civil disobedience produced crisis situations and productive dialogues between activists and government authorities. Federal, state, and local governments, businesses, and communities often had to respond immediately to these situations, which highlighted the inequities faced by African Americans. Forms of protest and/or civil disobedience included boycotts such as the successful Montgomery Bus Boycott (1955–56) in Alabama; "sit-ins" such as the influential Greensboro sit-ins (1960) in North Carolina; marches, such as the Selma to Montgomery marches (1965) in Alabama; and a wide range of other nonviolent activities.
Noted federal legislative achievements during this phase of the Civil Rights Movement were passage of the Civil Rights Act of 1964, which banned discrimination based on race, color, religion, sex, or national origin in employment practices and ended unequal application of voter registration requirements and racial segregation in schools, at the workplace, and by public accommodations; the Voting Rights Act of 1965, which restored and protected voting rights; the Immigration and Nationality Services Act of 1965, which dramatically opened entry to the U.S. to immigrants other than traditional Northern European and Germanic groups; and the Fair Housing Act of 1968, which banned discrimination in the sale or rental of housing. African Americans re-entered politics in the South, and across the country young people were inspired to take action.
A wave of inner city riots in black communities from 1964 through 1970 undercut support from the white community. The emergence of the Black Power movement, which lasted from about 1966 to 1975, challenged the established black leadership for its cooperative attitude and its nonviolence, and instead demanded political and economic self-sufficiency.
Many popular representations of the movement are centered on the leadership and philosophy of Martin Luther King, Jr., who won the 1964 Nobel Peace Prize for his role in the movement. But, some scholars note that the movement was too diverse to be credited to one person, organization, or strategy.
Background.
Before the American Civil War, almost four million blacks were enslaved in the South, only white men of property could vote, and the Naturalization Act of 1790 limited U.S. citizenship to whites only. Following the Civil War, three constitutional amendments were passed, including the 13th Amendment (1865) that ended slavery; the 14th Amendment (1868) that gave African Americans citizenship, adding their total population of four million to the official population of southern states for Congressional apportionment; and the 15th Amendment (1870) that gave African-American males the right to vote (only males could vote in the U.S. at the time). From 1865 to 1877, the United States underwent a turbulent Reconstruction Era trying to establish free labor and civil rights of freedmen in the South after the end of slavery. Many whites resisted the social changes, leading to insurgent movements such as the Ku Klux Klan, whose members attacked black and white Republicans to maintain white supremacy. In 1871, President Ulysses S. Grant, the U.S. Army, and U.S. Attorney General Amos T. Akerman, initiated a campaign to repress the KKK under the Enforcement Acts. Some states were reluctant to enforce the federal measures of the act; by the early 1870s, other white supremacist and paramilitary groups arose that violently opposed African-American legal equality and suffrage.
After the disputed election of 1876 resulted in the end of Reconstruction and federal troops were withdrawn, whites in the South regained political control of the region's state legislatures by the end of the century, after having intimidated and violently attacked blacks before and during elections.
From 1890 to 1908, southern states passed new constitutions and laws to disenfranchise African Americans and many poor whites by creating barriers to voter registration; voting rolls were dramatically reduced as blacks and poor whites were forced out of electoral politics. While progress was made in some areas, this status of excluding African Americans from the political system lasted in most southern states until national civil rights legislation was passed in the mid-1960s to provide federal enforcement of constitutional voting rights. For more than 60 years, blacks in the South were not able to elect anyone to represent their interests in Congress or local government. Since they could not vote, they could not serve on local juries.
During this period, the white-dominated Democratic Party maintained political control of the South. With whites controlling all the seats representing the total population of the South, they had a powerful voting block in Congress. The Republican Party—the "party of Lincoln"—which had been the party that most blacks belonged to, shrank to insignificance as black voter registration was suppressed. Until 1965, the "solid South" was a one-party system under the Democrats. Outside a few areas (usually in remote Appalachia), the Democratic Party nomination was tantamount to election for state and local office. In 1901, President Theodore Roosevelt invited Booker T. Washington to dine at the White House, making him the first African American to attend an official dinner there. "The invitation was roundly criticized by southern politicians and newspapers." Washington persuaded the president to appoint more blacks to federal posts in the South and to try to boost African-American leadership in state Republican organizations. However, this was resisted by both white Democrats and white Republicans as an unwanted federal intrusion into state politics.
During the same time as African Americans were being disenfranchised, white Democrats imposed racial segregation by law. Violence against blacks increased, with numerous lynchings through the turn of the century. The system of "de jure" state-sanctioned racial discrimination and oppression that emerged from the post-Reconstruction South became known as the "Jim Crow" system. The United States Supreme Court, made up almost entirely of Northerners, upheld the constitutionality of those state laws that required racial segregation in public facilities in its 1896 decision "Plessy v. Ferguson", legitimizing them through the "separate but equal" doctrine.
Segregation remained intact into the mid-1950s, when many states began to gradually integrate their schools following the unanimous Supreme Court decision in "Brown v. Board of Education" that overturned "Plessy v. Ferguson". The early 20th century is a period often referred to as the "nadir of American race relations". While tensions and civil rights violations were most intense in the South, social discrimination affected African Americans in other regions as well. At the national level, the Southern bloc controlled important committees in Congress, defeated passage of laws against lynching, and exercised considerable power beyond the number of whites in the South.
Characteristics of the post-Reconstruction period:
African Americans and other ethnic minorities rejected this regime. They resisted it in numerous ways and sought better opportunities through lawsuits, new organizations, political redress, and labor organizing (see the African-American Civil Rights Movement (1896–1954)). The National Association for the Advancement of Colored People (NAACP) was founded in 1909. It fought to end race discrimination through litigation, education, and lobbying efforts. Its crowning achievement was its legal victory in the Supreme Court decision "Brown v. Board of Education" in 1954 when the Court rejected separate white and colored school systems and, by implication, overturned the "separate but equal" doctrine established in "Plessy v. Ferguson" of 1896.
The integration of Southern public libraries involved many of the same characteristics seen in the larger Civil Rights Movement. This includes sit-ins, beatings, and white resistance. For example, in 1963 in the city of Anniston, Alabama, two black ministers were brutally beaten for attempting to integrate the public library. Though there was resistance and violence, the integration of libraries was generally quicker than integration of other public institutions.
Black veterans of the military after both World Wars pressed for full civil rights and often led activist movements. In 1948 they gained integration in the military under President Harry Truman, who issued Executive Order 9981 to accomplish it. The situation for blacks outside the South was somewhat better (in most states they could vote and have their children educated, though they still faced discrimination in housing and jobs). From 1910 to 1970, African Americans sought better lives by migrating north and west out of the South. Nearly seven million blacks left the South in what was known as the Great Migration. So many people migrated that the demographics of some previously black-majority states changed to white majority (in combination with other developments).
Housing segregation was a nationwide problem, persistent well outside the South. Although the federal government had become increasingly involved in mortgage lending and development in the 1930s and 1940s, it did not reject the use of race-restrictive covenants until 1950. Suburbanization was already connected with white flight by this time, a situation perpetuated by real estate agents' continuing discrimination. In particular, from the 1930s to the 1960s the National Association of Real Estate Boards (NAREB) issued guidelines that specified that a realtor ""should never be instrumental in introducing to a neighborhood a character or property or occupancy, members of any race or nationality, or any individual whose presence will be clearly detrimental to property values in a neighborhood.""
Invigorated by the victory of "Brown" and frustrated by the lack of immediate practical effect, private citizens increasingly rejected gradualist, legalistic approaches as the primary tool to bring about desegregation. They were faced with "massive resistance" in the South by proponents of racial segregation and voter suppression. In defiance, African-American activists adopted a combined strategy of direct action, nonviolence, nonviolent resistance, and many events described as civil disobedience, giving rise to the African-American Civil Rights Movement of 1954–1968.
Mass action replacing litigation.
The strategy of public education, legislative lobbying, and litigation that had typified the civil rights movement during the first half of the 20th century broadened after "Brown" to a strategy that emphasized "direct action": boycotts, sit-ins, Freedom Rides, marches, and similar tactics that relied on mass mobilization, nonviolent resistance, and civil disobedience. This mass action approach typified the movement from 1960 to 1968.
Churches, local grassroots organizations, fraternal societies, and black-owned businesses mobilized volunteers to participate in broad-based actions. This was a more direct and potentially more rapid means of creating change than the traditional approach of mounting court challenges used by the NAACP and others.
In 1952, the Regional Council of Negro Leadership (RCNL), led by T. R. M. Howard, a black surgeon, entrepreneur, and planter, organized a successful boycott of gas stations in Mississippi that refused to provide restrooms for blacks. Through the RCNL, Howard led campaigns to expose brutality by the Mississippi state highway patrol and to encourage blacks to make deposits in the black-owned Tri-State Bank of Nashville which, in turn, gave loans to civil rights activists who were victims of a "credit squeeze" by the White Citizens' Councils.
Although considered and rejected after Claudette Colvin's arrest for not giving up her seat on a Montgomery, Alabama, bus in March, 1955, after Rosa Parks' arrest in December Jo Ann Gibson-Robinson of the Montgomery Women's Political Council put a long-considered Bus Boycott protest in motion. Late that night, she, two students, and John Cannon, chairman of the Business Department at Alabama State University, mimeographed and distributed approximately 52,500 leaflets calling for a boycott of the buses.
The first day of the boycott having been successful, King, E.D. Nixon, and other civic and religious leaders created the Montgomery Improvement Association—so as to continue the Montgomery Bus Boycott. The MIA managed to keep the boycott going for over a year until a federal court order required Montgomery to desegregate its buses. The success in Montgomery made its leader Dr. Martin Luther King, Jr. a nationally known figure. It also inspired other bus boycotts, such as the successful Tallahassee, Florida, boycott of 1956–57.
In 1957 Dr. King and Rev. Ralph Abernathy, the leaders of the Montgomery Improvement Association, joined with other church leaders who had led similar boycott efforts, such as Rev. C. K. Steele of Tallahassee and Rev. T. J. Jemison of Baton Rouge; and other activists such as Rev. Fred Shuttlesworth, Ella Baker, A. Philip Randolph, Bayard Rustin and Stanley Levison, to form the Southern Christian Leadership Conference. The SCLC, with its headquarters in Atlanta, Georgia, did not attempt to create a network of chapters as the NAACP did. It offered training and leadership assistance for local efforts to fight segregation. The headquarters organization raised funds, mostly from Northern sources, to support such campaigns. It made nonviolence both its central tenet and its primary method of confronting racism.
In 1959, Septima Clarke, Bernice Robinson, and Esau Jenkins, with the help of Myles Horton's Highlander Folk School in Tennessee, began the first Citizenship Schools in South Carolina's Sea Islands. They taught literacy to enable blacks to pass voting tests. The program was an enormous success and tripled the number of black voters on Johns Island. SCLC took over the program and duplicated its results elsewhere.
Key events.
"Brown v. Board of Education", 1954.
In the spring of 1951, black students in Virginia protested their unequal status in the state's segregated educational system. Students at Moton High School protested the overcrowded conditions and failing facility. Some local leaders of the NAACP had tried to persuade the students to back down from their protest against the Jim Crow laws of school segregation. When the students did not budge, the NAACP joined their battle against school segregation. The NAACP proceeded with five cases challenging the school systems; these were later combined under what is known today as "Brown v. Board of Education".
On May 17, 1954, the U.S. Supreme Court ruled unanimously in "Brown v. Board of Education of Topeka, Kansas", that mandating, or even permitting, public schools to be segregated by race was unconstitutional. The Court stated that the
"segregation of white and colored children in public schools has a detrimental effect upon the colored children. The impact is greater when it has the sanction of the law; for the policy of separating the races is usually interpreted as denoting the inferiority of the Negro group."
The lawyers from the NAACP had to gather plausible evidence in order to win the case of "Brown vs. Board of Education". Their method of addressing the issue of school segregation was to enumerate several arguments. One pertained to having exposure to interracial contact in a school environment. It was argued that interracial contact would, in turn, help prepare children to live with the pressures that society exerts in regards to race and thereby afford them a better chance of living in democracy. In addition, another argument emphasized how "'education' comprehends the entire process of developing and training the mental, physical and moral powers and capabilities of human beings".
Risa Goluboff wrote that the NAACP's intention was to show the Courts that African American children were the victims of school segregation and their futures were at risk. The Court ruled that both "Plessy v. Ferguson" (1896), which had established the "separate but equal" standard in general, and "Cumming v. Richmond County Board of Education" (1899), which had applied that standard to schools, were unconstitutional.
The federal government filed a friend of the court brief in the case urging the justices to consider the effect that segregation had on America's image in the Cold War. Secretary of State Dean Acheson was quoted in the brief stating that ""The United States is under constant attack in the foreign press, over the foreign radio, and in such international bodies as the United Nations because of various practices of discrimination in this country."
"
The following year, in the case known as "Brown II", the Court ordered segregation to be phased out over time, "with all deliberate speed". "Brown v. Board of Education of Topeka, Kansas" (1954) did not overturn "Plessy v. Ferguson" (1896). "Plessy v. Ferguson" was segregation in transportation modes. "Brown v. Board of Education" dealt with segregation in education. "Brown v. Board of Education" did set in motion the future overturning of 'separate but equal'.
On May 18, 1954 Greensboro, North Carolina became the first city in the South to publicly announce that it would abide by the Supreme Court's "Brown v. Board of Education" ruling. "It is unthinkable,' remarked School Board Superintendent Benjamin Smith, 'that we will try to the laws of the United States." This positive reception for Brown, together with the appointment of African American Dr. David Jones to the school board in 1953, convinced numerous white and black citizens that Greensboro was heading in a progressive direction. Integration in Greensboro occurred rather peacefully compared to the process in Southern states such as Alabama, Arkansas, and Virginia where "massive resistance" was practiced by top officials and throughout the states. In Virginia, some counties closed their public schools rather than integrate, and many white Christian private schools were founded to accommodate students who used to go to public schools. Even in Greensboro, much local resistance to desegregation continued, and in 1969, the federal government found the city was not in compliance with the 1964 Civil Rights Act. Transition to a fully integrated school system did not begin until 1971.
Many Northern cities also had de facto segregation policies, which resulted in a vast gulf in educational resources between black and white communities. In Harlem, New York for example, neither a single new school was built since the turn of the century, nor did a single nursery school exist – even as the Second Great Migration was causing overcrowding. Existing schools tended to be dilapidated and staffed with inexperienced teachers. "Brown" helped stimulate activism among New York City parents like Mae Mallory who, with support of the NAACP, initiated a successful lawsuit against the city and state on "Brown"'s principles. Mallory and thousands of other parents bolstered the pressure of the lawsuit with a school boycott in 1959. During the boycott, some of the first freedom schools of the period were established. The city responded to the campaign by permitting more open transfers to high-quality, historically-white schools. (New York's African-American community, and Northern desegregation activists generally, now found themselves contending with the problem of white flight, however.)
Rosa Parks and the Montgomery Bus Boycott, 1955–1956.
Civil rights leaders began focusing on Montgomery, Alabama, highlighting extreme forms of segregation occurring there. On December 1, 1955, local black leader Rosa Parks refused to give up her seat on a public bus to make room for a white passenger. She was arrested and received national publicity, hailed as the "mother of the civil rights movement." Parks was secretary of the Montgomery NAACP chapter and had recently returned from a meeting at the Highlander Center in Tennessee where nonviolent civil disobedience as a strategy was taught. African-Americans gathered and organized the Montgomery Bus Boycott to demand a bus system in which passengers would be treated equally. After the city rejected many of their suggested reforms, the NAACP, led by E.D. Nixon, pushed for full desegregation of public buses. With the support of most of Montgomery's 50,000 African Americans, the boycott lasted for 381 days, until the local ordinance segregating African Americans and whites on public buses was repealed. Ninety percent of African Americans in Montgomery partook in the boycotts, which reduced bus revenue significantly, as they comprised the majority of the riders. In November 1956, a federal court ordered Montgomery's buses desegregated and the boycott ended.
Local leaders established the Montgomery Improvement Association to focus their efforts. Martin Luther King, Jr., was elected President of this organization. The lengthy protest attracted national attention for him and the city. His eloquent appeals to Christian brotherhood and American idealism created a positive impression on people both inside and outside the South.
Desegregating Little Rock Central High School, 1957.
A crisis erupted in Little Rock, Arkansas when Governor of Arkansas Orval Faubus called out the National Guard on September 4 to prevent entry to the nine African-American students who had sued for the right to attend an integrated school, Little Rock Central High School. The nine students had been chosen to attend Central High because of their excellent grades.
On the first day of school, only one of the nine students showed up because she did not receive the phone call about the danger of going to school. She was harassed by white protesters outside the school, and the police had to take her away in a patrol car to protect her. Afterward, the nine students had to carpool to school and be escorted by military personnel in jeeps.
Faubus was not a proclaimed segregationist. The Arkansas Democratic Party, which then controlled politics in the state, put significant pressure on Faubus after he had indicated he would investigate bringing Arkansas into compliance with the "Brown" decision. Faubus then took his stand against integration and against the Federal court ruling.
Faubus' resistance received the attention of President Dwight D. Eisenhower, who was determined to enforce the orders of the Federal courts. Critics had charged he was lukewarm, at best, on the goal of desegregation of public schools. But, Eisenhower federalized the National Guard in Arkansas and ordered them to return to their barracks. Eisenhower deployed elements of the 101st Airborne Division to Little Rock to protect the students.
The students attended high school under harsh conditions. They had to pass through a gauntlet of spitting, jeering whites to arrive at school on their first day, and to put up with harassment from other students for the rest of the year. Although federal troops escorted the students between classes, the students were teased and even attacked by white students when the soldiers were not around. One of the Little Rock Nine, Minnijean Brown, was suspended for spilling a bowl of chili on the head of a white student who was harassing her in the school lunch line. Later, she was expelled for verbally abusing a white female student.
Only Ernest Green of the Little Rock Nine graduated from Central High School. After the 1957–58 school year was over, Little Rock closed its public school system completely rather than continue to integrate. Other school systems across the South followed suit.
The method of Nonviolence and Nonviolence Training.
During the time period considered to be the "African-American Civil Rights" era, the predominant use of protest was nonviolent, or peaceful. Often referred to as pacifism, the method of nonviolence is considered to be an attempt to impact society positively. Although acts of racial discrimination have occurred historically throughout the United States, perhaps the most violent regions have been in the former Confederate states. During the 1950s and 1960s, the nonviolent protesting of the Civil Rights Movement caused definite tension, which gained national attention.
In order to prepare for protests physically and psychologically, demonstrators received training in nonviolence. According to former Civil Rights activist Bruce Hartford, there are two main branches of nonviolence training. There is the philosophical method, which involves understanding the method of nonviolence and why it is considered useful, and there is the tactical method, which ultimately teaches demonstrators "how to be a protestor—how to sit-in, how to picket, how to defend yourself against attack, giving training on how to remain cool when people are screaming racist insults into your face and pouring stuff on you and hitting you" (Veterans of the Civil Rights Movement). The philosophical method of nonviolence, in the American Civil Rights Movement, was largely inspired by Mahatma Gandhi's "non-cooperation" with the British colonists in India, which was intended to gain attention so that the public would either "intervene in advance," or "provide public pressure in support of the action to be taken" (Erikson, 415). As Hartford explains it, philosophical nonviolence training aims to "shape the individual person's attitude and mental response to crises and violence" (Veterans of the Civil Rights Movement). Hartford and activists like him, who trained in tactical nonviolence, considered it necessary in order to ensure physical safety, instill discipline, teach demonstrators how to demonstrate, and form mutual confidence among demonstrators (Veterans of the Civil Rights Movement).
For many, the concept of nonviolent protest was a way of life, a culture. However, not everyone agreed with this notion. James Forman, former SNCC (and later Black Panther) member and nonviolence trainer, was among those who did not. In his autobiography, "The Making of Black Revolutionaries", Forman revealed his perspective on the method of nonviolence as "strictly a tactic, not a way of life without limitations." Similarly, Robert Moses, who was also an active member of SNCC, felt that the method of nonviolence was practical. When interviewed by author Robert Penn Warren, Moses said "There's no question that he Luther King, Jr. had a great deal of influence with the masses. But I don't think it's in the direction of love. It's in a practical direction . . ." (Who Speaks for the Negro? Warren).
Robert F. Williams and the debate on nonviolence, 1959–1964.
The Jim Crow system employed "terror as a means of social control," with the most organized manifestations being the Ku Klux Klan and their collaborators in local police departments. This violence played a key role in blocking the progress of the Civil Rights Movement in the late 1950s. Some black organizations in the South began practicing armed self-defense. The first to do so openly was the Monroe, North Carolina chapter of the NAACP led by Robert F. Williams. Williams had rebuilt the chapter after its membership was terrorized out of public life by the Klan. He did so by encouraging a new, more working-class membership to arm itself thoroughly and defend against attack. When Klan nightriders attacked the home of NAACP member Dr. Albert Perry in October 1957, Williams' militia exchanged gunfire with the stunned Klansmen, who quickly retreated. The following day, the city council held an emergency session and passed an ordinance banning KKK motorcades. One year later, Lumbee Indians in North Carolina would have a similarly successful armed stand-off with the Klan (known as the Battle of Hayes Pond) which resulted in KKK leader James W. "Catfish" Cole being convicted of incitement to riot.
After the acquittal of several white men charged with sexually assaulting black women in Monroe, Williams announced to United Press International reporters that he would "meet violence with violence" as a policy. Williams' declaration was quoted on the front page of "The New York Times", and "The Carolina Times" considered it "the biggest civil rights story of 1959." NAACP National chairman Roy Wilkins immediately suspended Williams from his position, but the Monroe organizer won support from numerous NAACP chapters across the country. Ultimately, Wilkins resorted to bribing influential organizer Daisy Bates to campaign against Williams at the NAACP national convention and the suspension was upheld. The convention nonetheless passed a resolution which stated: "We do not deny, but reaffirm the right of individual and collective self-defense against unlawful assaults." Martin Luther King Jr. argued for Williams' removal, but Ella Baker and WEB Dubois both publicly praised the Monroe leader's position.
Williams – along with his wife, Mabel Williams – continued to play a leadership role in the Monroe movement, and to some degree, in the national movement. The Williamses published "The Crusader", a nationally circulated newsletter, beginning in 1960, and the influential book "Negroes With Guns" in 1962. Williams did not call for full militarization in this period, but "flexibility in the freedom struggle." Williams was well-versed in legal tactics and publicity, which he had used successfully in the internationally known "Kissing Case" of 1958, as well as nonviolent methods, which he used at lunch counter sit-ins in Monroe – all with armed self-defense as a complementary tactic.
Williams led the Monroe movement in another armed stand-off with white supremacists during an August 1961 Freedom Ride; he had been invited to participate in the campaign by Ella Baker and James Forman of the Student Nonviolent Coordinating Committee (SNCC). The incident (along with his campaigns for peace with Cuba) resulted in him being targeted by the FBI and prosecuted for kidnapping; he was cleared of all charges in 1976. Meanwhile, armed self-defense continued discreetly in the Southern movement with such figures as SNCC's Amzie Moore, Hartman Turnbow, and Fannie Lou Hamer all willing to use arms to defend their lives from nightrides. Taking refuge from the FBI in Cuba, the Willamses broadcast the radio show "Radio Free Dixie" throughout the eastern United States via Radio Progresso beginning in 1962. In this period, Williams advocated guerilla warfare against racist institutions, and saw the large ghetto riots of the era as a manifestation of his strategy.
University of North Carolina historian Walter Rucker has written that "the emergence of Robert F Williams contributed to the marked decline in anti-black racial violence in the US…After centuries of anti-black violence, African-Americans across the country began to defend their communities aggressively – employing overt force when necessary. This in turn evoked in whites real fear of black vengeance…" This opened up space for African-Americans to use nonviolent demonstration with less fear of deadly reprisal. Of the many civil rights activists who share this view, the most prominent was Rosa Parks. Parks gave the eulogy at Williams' funeral in 1996, praising him for "his courage and for his commitment to freedom," and concluding that "The sacrifices he made, and what he did, should go down in history and never be forgotten."
Sit-ins, 1958–1960.
In July 1958, the NAACP Youth Council sponsored sit-ins at the lunch counter of a Dockum Drug Store in downtown Wichita, Kansas. After three weeks, the movement successfully got the store to change its policy of segregated seating, and soon afterward all Dockum stores in Kansas were desegregated. This movement was quickly followed in the same year by a student sit-in at a Katz Drug Store in Oklahoma City led by Clara Luper, which also was successful.
Mostly black students from area colleges led a sit-in at a Woolworth's store in Greensboro, North Carolina. On February 1, 1960, four students, Ezell A. Blair, Jr., David Richmond, Joseph McNeil, and Franklin McCain from North Carolina Agricultural & Technical College, an all-black college, sat down at the segregated lunch counter to protest Woolworth's policy of excluding African Americans from being served there. The four students purchased small items in other parts of the store and kept their receipts, then sat down at the lunch counter and asked to be served. After being denied service, they produced their receipts and asked why their money was good everywhere else at the store, but not at the lunch counter.
The protesters had been encouraged to dress professionally, to sit quietly, and to occupy every other stool so that potential white sympathizers could join in. The Greensboro sit-in was quickly followed by other sit-ins in Richmond, Virginia; Nashville, Tennessee; and Atlanta, Georgia. The most immediately effective of these was in Nashville, where hundreds of well organized and highly disciplined college students conducted sit-ins in coordination with a boycott campaign.
As students across the south began to "sit-in" at the lunch counters of local stores, police and other officials sometimes used brutal force to physically escort the demonstrators from the lunch facilities.
The "sit-in" technique was not new—as far back as 1939, African-American attorney Samuel Wilbert Tucker organized a sit-in at the then-segregated Alexandria, Virginia library. In 1960 the technique succeeded in bringing national attention to the movement.
On March 9, 1960 an Atlanta University Center group of students released An Appeal for Human Rights as a full page advertisement in newspapers, including the "Atlanta Constitution", "Atlanta Journal", and "Atlanta Daily World". Known as the Committee on Appeal for Human Rights (COAHR), the group initiated the Atlanta Student Movement and began to lead sit-ins starting on March 15, 1960. By the end of 1960, the process of sit-ins had spread to every southern and border state, and even to facilities in Nevada, Illinois, and Ohio that discriminated against blacks.
Demonstrators focused not only on lunch counters but also on parks, beaches, libraries, theaters, museums, and other public facilities. In April 1960 activists who had led these sit-ins were invited by SCLC activist Ella Baker to hold a conference at Shaw University, a historically black university in Raleigh, North Carolina. This conference led to the formation of the Student Nonviolent Coordinating Committee (SNCC). SNCC took these tactics of nonviolent confrontation further, and organized the freedom rides. As the constitution protected interstate commerce, they decided to challenge segregation on interstate buses and in public bus facilities by putting interracial teams on them, to travel from the North through the segregated South.
Freedom Rides, 1961.
Freedom Rides were journeys by Civil Rights activists on interstate buses into the segregated southern United States to test the United States Supreme Court decision "Boynton v. Virginia," (1960) 364 U.S., which ruled that segregation was unconstitutional for passengers engaged in interstate travel. Organized by CORE, the first Freedom Ride of the 1960s left Washington D.C. on May 4, 1961, and was scheduled to arrive in New Orleans on May 17.
During the first and subsequent Freedom Rides, activists traveled through the Deep South to integrate seating patterns on buses and desegregate bus terminals, including restrooms and water fountains. That proved to be a dangerous mission. In Anniston, Alabama, one bus was firebombed, forcing its passengers to flee for their lives.
In Birmingham, Alabama, an FBI informant reported that Public Safety Commissioner Eugene "Bull" Connor gave Ku Klux Klan members fifteen minutes to attack an incoming group of freedom riders before having police "protect" them. The riders were severely beaten "until it looked like a bulldog had got a hold of them." James Peck, a white activist, was beaten so badly that he required fifty stitches to his head.
In a similar occurrence in Montgomery, Alabama, the Freedom Riders followed in the footsteps of Rosa Parks and rode an integrated Greyhound bus from Birmingham. Although they were protesting interstate bus segregation in peace, they were met with violence in Montgomery as a large, white mob attacked them for their activism. They caused an enormous, 2-hour long riot which resulted in 22 injuries, five of whom were hospitalized.
Mob violence in Anniston and Birmingham temporarily halted the rides. SNCC activists from Nashville brought in new riders to continue the journey from Birmingham to New Orleans. In Montgomery, Alabama, at the Greyhound Bus Station, a mob charged another bus load of riders, knocking John Lewis unconscious with a crate and smashing "Life" photographer Don Urbrock in the face with his own camera. A dozen men surrounded James Zwerg, a white student from Fisk University, and beat him in the face with a suitcase, knocking out his teeth.
On May 24, 1961, the freedom riders continued their rides into Jackson, Mississippi, where they were arrested for "breaching the peace" by using "white only" facilities. New freedom rides were organized by many different organizations and continued to flow into the South. As riders arrived in Jackson, they were arrested. By the end of summer, more than 300 had been jailed in Mississippi.
...When the weary Riders arrive in Jackson and attempt to use "white only" restrooms and lunch counters they are immediately arrested for Breach of Peace and Refusal to Obey an Officer. Says Mississippi Governor Ross Barnett in defense of segregation: "The Negro is different because God made him different to punish him." From lockup, the Riders announce "Jail No Bail"—they will not pay fines for unconstitutional arrests and illegal convictions—and by staying in jail they keep the issue alive. Each prisoner will remain in jail for 39 days, the maximum time they can serve without loosing their right to appeal the unconstitutionality of their arrests, trials, and convictions. After 39 days, they file an appeal and post bond...
The jailed freedom riders were treated harshly, crammed into tiny, filthy cells and sporadically beaten. In Jackson, some male prisoners were forced to do hard labor in 100 °F heat. Others were transferred to the Mississippi State Penitentiary at Parchman, where they were treated to harsh conditions. Sometimes the men were suspended by "wrist breakers" from the walls. Typically, the windows of their cells were shut tight on hot days, making it hard for them to breathe.
Public sympathy and support for the freedom riders led John F. Kennedy's administration to order the Interstate Commerce Commission (ICC) to issue a new desegregation order. When the new ICC rule took effect on November 1, 1961, passengers were permitted to sit wherever they chose on the bus; "white" and "colored" signs came down in the terminals; separate drinking fountains, toilets, and waiting rooms were consolidated; and lunch counters began serving people regardless of skin color.
The student movement involved such celebrated figures as John Lewis, a single-minded activist; James Lawson, the revered "guru" of nonviolent theory and tactics; Diane Nash, an articulate and intrepid public champion of justice; Bob Moses, pioneer of voting registration in Mississippi; and James Bevel, a fiery preacher and charismatic organizer, strategist, and facilitator. Other prominent student activists included Charles McDew, Bernard Lafayette, Charles Jones, Lonnie King, Julian Bond, Hosea Williams, and Stokely Carmichael.
Voter registration organizing.
After the Freedom Rides, local black leaders in Mississippi such as Amzie Moore, Aaron Henry, Medgar Evers, and others asked SNCC to help register black voters and to build community organizations that could win a share of political power in the state. Since Mississippi ratified its new constitution in 1890 with provisions such as poll taxes, residency requirements, and literacy tests, it made registration more complicated and stripped blacks from voter rolls and voting. In addition, violence at the time of elections had earlier suppressed black voting.
By the mid-20th century, preventing blacks from voting had become an essential part of the culture of white supremacy. In the fall of 1961, SNCC organizer Robert Moses began the first voter registration project in McComb and the surrounding counties in the Southwest corner of the state. Their efforts were met with violent repression from state and local lawmen, the White Citizens' Council, and the Ku Klux Klan. Activists were beaten, there were hundreds of arrests of local citizens, and the voting activist Herbert Lee was murdered.
White opposition to black voter registration was so intense in Mississippi that Freedom Movement activists concluded that all of the state's civil rights organizations had to unite in a coordinated effort to have any chance of success. In February 1962, representatives of SNCC, CORE, and the NAACP formed the Council of Federated Organizations (COFO). At a subsequent meeting in August, SCLC became part of COFO.
In the Spring of 1962, with funds from the Voter Education Project, SNCC/COFO began voter registration organizing in the Mississippi Delta area around Greenwood, and the areas surrounding Hattiesburg, Laurel, and Holly Springs. As in McComb, their efforts were met with fierce opposition—arrests, beatings, shootings, arson, and murder. Registrars used the literacy test to keep blacks off the voting roles by creating standards that even highly educated people could not meet. In addition, employers fired blacks who tried to register, and landlords evicted them from their rental homes. Despite these actions, over the following years, the black voter registration campaign spread across the state.
Similar voter registration campaigns—with similar responses—were begun by SNCC, CORE, and SCLC in Louisiana, Alabama, southwest Georgia, and South Carolina. By 1963, voter registration campaigns in the South were as integral to the Freedom Movement as desegregation efforts. After passage of the Civil Rights Act of 1964, protecting and facilitating voter registration despite state barriers became the main effort of the movement. It resulted in passage of the Voting Rights Act of 1965, which had provisions to enforce the constitutional right to vote for all citizens.
Integration of Mississippi universities, 1956–65.
Beginning in 1956, Clyde Kennard, a black Korean War-veteran, wanted to enroll at Mississippi Southern College (now the University of Southern Mississippi) under the G.I. Bill at Hattiesburg. Dr. William David McCain, the college president, used the Mississippi State Sovereignty Commission, in order to prevent his enrollment by appealing to local black leaders and the segregationist state political establishment.
The state-funded organization tried to counter the Civil Rights Movement by positively portraying segregationist policies. More significantly, it collected data on activists, harassed them legally, and used economic boycotts against them by threatening their jobs (or causing them to lose their jobs) to try to suppress their work.
Kennard was twice arrested on trumped-up charges, and eventually convicted and sentenced to seven years in the state prison. After three years at hard labor, Kennard was paroled by Mississippi Governor Ross Barnett. Journalists had investigated his case and publicized the state's mistreatment of his colon cancer.
McCain's role in Kennard's arrests and convictions is unknown. While trying to prevent Kennard's enrollment, McCain made a speech in Chicago, with his travel sponsored by the Mississippi State Sovereignty Commission. He described the blacks' seeking to desegregate Southern schools as "imports" from the North. (Kennard was a native and resident of Hattiesburg.) McCain said:
"We insist that educationally and socially, we maintain a segregated society...In all fairness, I admit that we are not encouraging Negro voting...The Negroes prefer that control of the government remain in the white man's hands."
Note: Mississippi had passed a new constitution in 1890 that effectively disfranchised most blacks by changing electoral and voter registration requirements; although it deprived them of constitutional rights authorized under post-Civil War amendments, it survived US Supreme Court challenges at the time. It was not until after passage of the 1965 Voting Rights Act that most blacks in Mississippi and other southern states gained federal protection to enforce the constitutional right of citizens to vote.
In September 1962, James Meredith won a lawsuit to secure admission to the previously segregated University of Mississippi. He attempted to enter campus on September 20, on September 25, and again on September 26. He was blocked by Mississippi Governor Ross Barnett, who said, "o school will be integrated in Mississippi while I am your Governor." The Fifth U.S. Circuit Court of Appeals held Barnett and Lieutenant Governor Paul B. Johnson, Jr. in contempt, ordering them arrested and fined more than $10,000 for each day they refused to allow Meredith to enroll.
Attorney General Robert Kennedy sent in a force of U.S. Marshals. On September 30, 1962, Meredith entered the campus under their escort. Students and other whites began rioting that evening, throwing rocks and firing on the U.S. Marshals guarding Meredith at Lyceum Hall. Two people, including a French journalist, were killed; 28 marshals suffered gunshot wounds; and 160 others were injured. President John F. Kennedy sent regular US Army forces to the campus to quell the riot. Meredith began classes the day after the troops arrived.
Kennard and other activists continued to work on public university desegregation. In 1965 Raylawni Branch and Gwendolyn Elaine Armstrong became the first African-American students to attend the University of Southern Mississippi. By that time, McCain helped ensure they had a peaceful entry. In 2006, Judge Robert Helfrich ruled that Kennard was factually innocent of all charges for which he had been convicted in the 1950s.
Albany Movement, 1961–62.
The SCLC, which had been criticized by some student activists for its failure to participate more fully in the freedom rides, committed much of its prestige and resources to a desegregation campaign in Albany, Georgia, in November 1961. King, who had been criticized personally by some SNCC activists for his distance from the dangers that local organizers faced—and given the derisive nickname "De Lawd" as a result—intervened personally to assist the campaign led by both SNCC organizers and local leaders.
The campaign was a failure because of the canny tactics of Laurie Pritchett, the local police chief, and divisions within the black community. The goals may not have been specific enough. Pritchett contained the marchers without violent attacks on demonstrators that inflamed national opinion. He also arranged for arrested demonstrators to be taken to jails in surrounding communities, allowing plenty of room to remain in his jail. Prichett also foresaw King's presence as a danger and forced his release to avoid King's rallying the black community. King left in 1962 without having achieved any dramatic victories. The local movement, however, continued the struggle, and it obtained significant gains in the next few years.
Birmingham Campaign, 1963.
The Albany movement was shown to be an important education for the SCLC, however, when it undertook the Birmingham campaign in 1963. Executive Director Wyatt Tee Walker carefully planned the early strategy and tactics for the campaign. It focused on one goal—the desegregation of Birmingham's downtown merchants, rather than total desegregation, as in Albany.
The movement's efforts were helped by the brutal response of local authorities, in particular Eugene "Bull" Connor, the Commissioner of Public Safety. He had long held much political power, but had lost a recent election for mayor to a less rabidly segregationist candidate. Refusing to accept the new mayor's authority, Connor intended to stay in office.
The campaign used a variety of nonviolent methods of confrontation, including sit-ins, kneel-ins at local churches, and a march to the county building to mark the beginning of a drive to register voters. The city, however, obtained an injunction barring all such protests. Convinced that the order was unconstitutional, the campaign defied it and prepared for mass arrests of its supporters. King elected to be among those arrested on April 12, 1963.
While in jail, King wrote his famous "Letter from Birmingham Jail" on the margins of a newspaper, since he had not been allowed any writing paper while held in solitary confinement. Supporters appealed to the Kennedy administration, which intervened to obtain King's release. King was allowed to call his wife, who was recuperating at home after the birth of their fourth child, and was released early on April 19.
The campaign, however, faltered as it ran out of demonstrators willing to risk arrest. James Bevel, SCLC's Director of Direct Action and Director of Nonviolent Education, then came up with a bold and controversial alternative: to train high school students to take part in the demonstrations. As a result, in what would be called the Children's Crusade, more than one thousand students skipped school on May 2 to meet at the 16th Street Baptist Church to join the demonstrations. More than six hundred marched out of the church fifty at a time in an attempt to walk to City Hall to speak to Birmingham's mayor about segregation. They were arrested and put into jail.
In this first encounter the police acted with restraint. On the next day, however, another one thousand students gathered at the church. When Bevel started them marching fifty at a time, Bull Connor finally unleashed police dogs on them and then turned the city's fire hoses water streams on the children. National television networks broadcast the scenes of the dogs attacking demonstrators and the water from the fire hoses knocking down the schoolchildren.
Widespread public outrage led the Kennedy administration to intervene more forcefully in negotiations between the white business community and the SCLC. On May 10, the parties announced an agreement to desegregate the lunch counters and other public accommodations downtown, to create a committee to eliminate discriminatory hiring practices, to arrange for the release of jailed protesters, and to establish regular means of communication between black and white leaders.
Not everyone in the black community approved of the agreement— the Rev. Fred Shuttlesworth was particularly critical, since he was skeptical about the good faith of Birmingham's power structure from his experience in dealing with them. Parts of the white community reacted violently. They bombed the Gaston Motel, which housed the SCLC's unofficial headquarters, and the home of King's brother, the Reverend A. D. King. In response, thousands of blacks rioted, burning numerous buildings and one of them stabbed and wounded a police officer.
Kennedy prepared to federalize the Alabama National Guard if the need arose. Four months later, on September 15, a conspiracy of Ku Klux Klan members bombed the Sixteenth Street Baptist Church in Birmingham, killing four young girls.
"Rising tide of discontent" and Kennedy's Response, 1963.
Birmingham was only one of over a hundred cities rocked by chaotic protest that spring and summer, some of them in the North. During the March on Washington, Martin Luther King would refer to such protests as "the whirlwinds of revolt." In Chicago, blacks rioted through the South Side in late May after a white police officer shot a fourteen-year-old black boy who was fleeing the scene of a robbery. Violent clashes between black activists and white workers took place in both Philadelphia and Harlem in successful efforts to integrate state construction projects. On June 6, over a thousand whites attacked a sit-in in Lexington, North Carolina; blacks fought back and one white man was killed. Edwin C. Berry of the National Urban League warned of a complete breakdown in race relations: "My message from the beer gardens and the barbershops all indicate the fact that the Negro is ready for war."
In Cambridge, Maryland, a working‐class city on the Eastern Shore, Gloria Richardson of SNCC led a movement that pressed for desegregation but also demanded low‐rent public housing, job‐training, public and private jobs, and an end to police brutality. On June 14, struggles between blacks and whites escalated to the point where local authorities declared martial law, and Attorney General Robert F. Kennedy directly intervened to negotiate a desegregation agreement. Richardson felt that the increasing participation of poor and working-class blacks was expanding both the power and parameters of the movement, asserting that "The people as a whole really do have more intelligence than a few of their leaders.ʺ
In their deliberations during this wave of protests, the Kennedy administration privately felt that militant demonstrations were ʺbad for the countryʺ and that "Negroes are going to push this thing too far." On May 24, Robert Kennedy had a meeting with prominent black intellectuals to discuss the racial situation. The blacks criticized Kennedy harshly for vacillating on civil rights, and said that the African-American community's thoughts were increasingly turning to violence. The meeting ended with ill will on all sides. Nonetheless, the Kennedys ultimately decided that new legislation for equal public accommodations was essential to drive activists "into the courts and out of the streets."
On June 11, 1963, George Wallace, Governor of Alabama, tried to block the integration of the University of Alabama. President John F. Kennedy sent a military force to make Governor Wallace step aside, allowing the enrollment of Vivian Malone Jones and James Hood. That evening, President Kennedy addressed the nation on TV and radio with his historic civil rights speech, where he lamented "a rising tide of discontent that threatens the public safety." He called on Congress to pass new civil rights legislation, and urged the country to embrace civil rights as "a moral issue...in our daily lives." In the early hours of June 12, Medgar Evers, field secretary of the Mississippi NAACP, was assassinated by a member of the Klan. The next week, as promised, on June 19, 1963, President Kennedy submitted his Civil Rights bill to Congress.
March on Washington, 1963.
A. Philip Randolph had planned a march on Washington, D.C. in 1941 to support demands for elimination of employment discrimination in defense industries; he called off the march when the Roosevelt administration met the demand by issuing Executive Order 8802 barring racial discrimination and creating an agency to oversee compliance with the order.
Randolph and Bayard Rustin were the chief planners of the second march, which they proposed in 1962. In 1963, the Kennedy administration initially opposed the march out of concern it would negatively impact the drive for passage of civil rights legislation. However, Randolph and King were firm that the march would proceed. With the march going forward, the Kennedys decided it was important to work to ensure its success. Concerned about the turnout, President Kennedy enlisted the aid of additional church leaders and the UAW union to help mobilize demonstrators for the cause.
The march was held on August 28, 1963. Unlike the planned 1941 march, for which Randolph included only black-led organizations in the planning, the 1963 march was a collaborative effort of all of the major civil rights organizations, the more progressive wing of the labor movement, and other liberal organizations. The march had six official goals:
Of these, the march's major focus was on passage of the civil rights law that the Kennedy administration had proposed after the upheavals in Birmingham.
National media attention also greatly contributed to the march's national exposure and probable impact. In his section "The March on Washington and Television News," William Thomas notes: "Over five hundred cameramen, technicians, and correspondents from the major networks were set to cover the event. More cameras would be set up than had filmed the last presidential inauguration. One camera was positioned high in the Washington Monument, to give dramatic vistas of the marchers". By carrying the organizers' speeches and offering their own commentary, television stations framed the way their local audiences saw and understood the event.
The march was a success, although not without controversy. An estimated 200,000 to 300,000 demonstrators gathered in front of the Lincoln Memorial, where King delivered his famous "I Have a Dream" speech. While many speakers applauded the Kennedy administration for the efforts it had made toward obtaining new, more effective civil rights legislation protecting the right to vote and outlawing segregation, John Lewis of SNCC took the administration to task for not doing more to protect southern blacks and civil rights workers under attack in the Deep South.
After the march, King and other civil rights leaders met with President Kennedy at the White House. While the Kennedy administration appeared sincerely committed to passing the bill, it was not clear that it had the votes in Congress to do it. However when President Kennedy was assassinated on November 22, 1963, the new President Lyndon Johnson decided to use his influence in Congress to bring about much of Kennedy's legislative agenda.
Malcolm X joins the movement, 1964–1965.
In March 1964, Malcolm X (el-Hajj Malik el-Shabazz), national representative of the Nation of Islam, formally broke with that organization, and made a public offer to collaborate with any civil rights organization that accepted the right to self-defense and the philosophy of Black nationalism (which Malcolm said no longer required Black separatism). Gloria Richardson–head of the Cambridge, Maryland chapter of SNCC, leader of the Cambridge rebellion and an honored guest at The March on Washington – immediately embraced Malcolm's offer. Mrs. Richardson, "the nation's most prominent woman rights leader," told "The Baltimore Afro-American" that "Malcolm is being very practical…The federal government has moved into conflict situations only when matters approach the level of insurrection. Self-defense may force Washington to intervene sooner." Earlier, in May 1963, James Baldwin had stated publicly that "the Black Muslim movement is the only one in the country we can call grassroots, I hate to say it…Malcolm articulates for Negroes, their suffering…he corroborates their reality..." On the local level, Malcolm and the NOI had been allied with the Harlem chapter of the Congress of Racial Equality (CORE) since at least 1962.
On March 26, 1964, as the Civil Rights Act was facing stiff opposition in Congress, Malcolm had a public meeting with Martin Luther King, Jr. at the Capitol building. Malcolm had attempted to begin a dialog with Dr. King as early as 1957, but King had rebuffed him. Malcolm had responded by calling King an "Uncle Tom" who turned his back on black militancy in order to appease the white power structure. However, the two men were on good terms at their face-to-face meeting. There is evidence that King was preparing to support Malcolm's plan to formally bring the US government before the United Nations on charges of human rights violations against African-Americans. Malcolm now encouraged Black nationalists to get involved in voter registration drives and other forms of community organizing to redefine and expand the movement.
Civil rights activists became increasingly combative in the 1963 to 1964 period, owing to events such as the thwarting of the Albany campaign, police repression and Ku Klux Klan terrorism in Birmingham, and the assassination of Medgar Evers. Mississippi NAACP Field Director Charles Evers–Medgar Evers' brother–told a public NAACP conference on February 15, 1964 that "non-violence won't work in Mississippi…we made up our minds…that if a white man shoots at a Negro in Mississippi, we will shoot back." The repression of sit-ins in Jacksonville, Florida provoked a riot that saw black youth throwing Molotov cocktails at police on March 24, 1964. Malcolm X gave extensive speeches in this period warning that such militant activity would escalate further if African-Americans' rights were not fully recognized. In his landmark April 1964 speech "The Ballot or the Bullet", Malcolm presented an ultimatum to white America: "There's new strategy coming in. It'll be Molotov cocktails this month, hand grenades next month, and something else next month. It'll be ballots, or it'll be bullets."
As noted in "Eyes on the Prize", "Malcolm X had a far reaching effect on the civil rights movement. In the South, there had been a long tradition of self reliance. Malcolm X's ideas now touched that tradition". Self-reliance was becoming paramount in light of the 1964 Democratic National Convention's decision to refuse seating to the Mississippi Freedom Democratic Party (MFDP) and to seat the state delegation elected in violation of the party's rules through Jim Crow law instead. SNCC moved in an increasingly militant direction and worked with Malcolm X on two Harlem MFDP fundraisers in December 1964. When Fannie Lou Hamer spoke to Harlemites about the Jim Crow violence that she'd suffered in Mississippi, she linked it directly to the Northern police brutality against blacks that Malcolm protested against; When Malcolm asserted that African-Americans should emulate the Mau Mau army of Kenya in efforts to gain their independence, many in SNCC applauded. During the Selma campaign for voting rights in 1965, Malcolm made it known that he'd heard reports of increased threats of lynching around Selma, and responded in late January with an open telegram to George Lincoln Rockwell, the head of the American Nazi Party, stating: "if your present racist agitation against our people there in Alabama causes physical harm to Reverend King or any other black Americans…you and your KKK friends will be met with maximum physical retaliation from those of us who are not handcuffed by the disarming philosophy of nonviolence." The following month, the Selma chapter of SNCC invited Malcolm to speak to a mass meeting there. On the day of Malcolm's appearance, President Johnson made his first public statement in support of the Selma campaign. Paul Ryan Haygood, a co-director of the NAACP Legal Defense Fund, credits Malcolm with a role in stimulating the responsiveness of the federal government. Haygood noted that "shortly after Malcolm's visit to Selma, a federal judge, responding to a suit brought by the Department of Justice, required Dallas County, Alabama registrars to process at least 100 Black applications each day their offices were open."
St. Augustine, Florida, 1963–64.
St. Augustine, on the northeast coast of Florida was famous as the "Nation's Oldest City," founded by the Spanish in 1565. It became the stage for a great drama leading up to the passage of the landmark Civil Rights Act of 1964. A local movement, led by Dr. Robert B. Hayling, a black dentist and Air Force veteran, and affiliated with the NAACP, had been picketing segregated local institutions since 1963, as a result of which Dr. Hayling and three companions, James Jackson, Clyde Jenkins, and James Hauser, were brutally beaten at a Ku Klux Klan rally in the fall of that year.
Nightriders shot into black homes, and teenagers Audrey Nell Edwards, JoeAnn Anderson, Samuel White, and Willie Carl Singleton (who came to be known as "The St. Augustine Four") spent six months in jail and reform school after sitting in at the local Woolworth's lunch counter. It took a special action of the governor and cabinet of Florida to release them after national protests by the "Pittsburgh Courier", Jackie Robinson, and others.
In response to the repression, the St. Augustine movement practiced armed self-defense in addition to nonviolent direct action. In June 1963, Dr. Hayling publicly stated that "I and the others have armed. We will shoot first and answer questions later. We are not going to die like Medgar Evers." The comment made national headlines. When Klan nightriders terrorized black neighborhoods in St. Augustine, Hayling's NAACP members often drove them off with gunfire, and in October, a Klansman was killed.
In 1964, Dr. Hayling and other activists urged the Southern Christian Leadership Conference to come to St. Augustine. The first action came during spring break, when Hayling appealed to northern college students to come to the Ancient City, not to go to the beach, but to take part in demonstrations. Four prominent Massachusetts women—Mrs. Mary Parkman Peabody, Mrs. Esther Burgess, Mrs. Hester Campbell (all of whose husbands were Episcopal bishops), and Mrs. Florence Rowe (whose husband was vice president of John Hancock Insurance Company) came to lend their support. The arrest of Mrs. Peabody, the 72-year-old mother of the governor of Massachusetts, for attempting to eat at the segregated Ponce de Leon Motor Lodge in an integrated group, made front page news across the country, and brought the movement in St. Augustine to the attention of the world.
Widely publicized activities continued in the ensuing months, as Congress saw the longest filibuster against a civil rights bill in its history. Dr. Martin Luther King, Jr. was arrested at the Monson Motel in St. Augustine on June 11, 1964, the only place in Florida he was arrested. He sent a "Letter from the St. Augustine Jail" to a northern supporter, Rabbi Israel Dresner of New Jersey, urging him to recruit others to participate in the movement. This resulted, a week later, in the largest mass arrest of rabbis in American history—while conducting a pray-in at the Monson.
A well-known photograph taken in St. Augustine shows the manager of the Monson Motel pouring acid in the swimming pool while blacks and whites are swimming in it. The horrifying photograph was run on the front page of the Washington newspaper the day the senate went to vote on passing the Civil Rights Act of 1964.
Mississippi Freedom Summer, 1964.
In the summer of 1964, COFO brought nearly 1,000 activists to Mississippi—most of them white college students—to join with local black activists to register voters, teach in "Freedom Schools," and organize the Mississippi Freedom Democratic Party (MFDP).
Many of Mississippi's white residents deeply resented the outsiders and attempts to change their society. State and local governments, police, the White Citizens' Council and the Ku Klux Klan used arrests, beatings, arson, murder, spying, firing, evictions, and other forms of intimidation and harassment to oppose the project and prevent blacks from registering to vote or achieving social equality.
On June 21, 1964, three civil rights workers disappeared. James Chaney, a young black Mississippian and plasterer's apprentice; and two Jewish activists, Andrew Goodman, a Queens College anthropology student; and Michael Schwerner, a CORE organizer from Manhattan's Lower East Side, were found weeks later, murdered by conspirators who turned out to be local members of the Klan, some of them members of the Neshoba County sheriff's department. This outraged the public, leading the U.S. Justice Department along with the FBI (the latter which had previously avoided dealing with the issue of segregation and persecution of blacks) to take action. The outrage over these murders helped lead to the passage of the Civil Rights Act. (See Mississippi civil rights workers murders for details).
From June to August, Freedom Summer activists worked in 38 local projects scattered across the state, with the largest number concentrated in the Mississippi Delta region. At least 30 Freedom Schools, with close to 3,500 students were established, and 28 community centers set up.
Over the course of the Summer Project, some 17,000 Mississippi blacks attempted to become registered voters in defiance of the red tape and forces of white supremacy arrayed against them—only 1,600 (less than 10%) succeeded. But more than 80,000 joined the Mississippi Freedom Democratic Party (MFDP), founded as an alternative political organization, showing their desire to vote and participate in politics.
Though Freedom Summer failed to register many voters, it had a significant effect on the course of the Civil Rights Movement. It helped break down the decades of people's isolation and repression that were the foundation of the Jim Crow system. Before Freedom Summer, the national news media had paid little attention to the persecution of black voters in the Deep South and the dangers endured by black civil rights workers. The progression of events throughout the South increased media attention to Mississippi.
The deaths of affluent northern white students and threats to other northerners attracted the full attention of the media spotlight to the state. Many black activists became embittered, believing the media valued lives of whites and blacks differently. Perhaps the most significant effect of Freedom Summer was on the volunteers, almost all of whom—black and white—still consider it to have been one of the defining periods of their lives.
Civil Rights Act of 1964.
Although President Kennedy had proposed civil rights legislation and it had support from Northern Congressmen and Senators of both parties, Southern Senators blocked the bill by threatening filibusters. After considerable parliamentary maneuvering and 54 days of filibuster on the floor of the United States Senate, President Johnson got a bill through the Congress.
On July 2, 1964, Johnson signed the Civil Rights Act of 1964, that banned discrimination based on "race, color, religion, sex or national origin" in employment practices and public accommodations. The bill authorized the Attorney General to file lawsuits to enforce the new law. The law also nullified state and local laws that required such discrimination.
Mississippi Freedom Democratic Party, 1964.
Blacks in Mississippi had been disfranchised by statutory and constitutional changes since the late 19th century. In 1963 COFO held a Freedom Vote in Mississippi to demonstrate the desire of black Mississippians to vote. More than 80,000 people registered and voted in the mock election, which pitted an integrated slate of candidates from the "Freedom Party" against the official state Democratic Party candidates.
In 1964, organizers launched the Mississippi Freedom Democratic Party (MFDP) to challenge the all-white official party. When Mississippi voting registrars refused to recognize their candidates, they held their own primary. They selected Fannie Lou Hamer, Annie Devine, and Victoria Gray to run for Congress, and a slate of delegates to represent Mississippi at the 1964 Democratic National Convention.
The presence of the Mississippi Freedom Democratic Party in Atlantic City, New Jersey, was inconvenient, however, for the convention organizers. They had planned a triumphant celebration of the Johnson administration's achievements in civil rights, rather than a fight over racism within the Democratic Party. All-white delegations from other Southern states threatened to walk out if the official slate from Mississippi was not seated. Johnson was worried about the inroads that Republican Barry Goldwater's campaign was making in what previously had been the white Democratic stronghold of the "Solid South", as well as support that George Wallace had received in the North during the Democratic primaries.
Johnson could not, however, prevent the MFDP from taking its case to the Credentials Committee. There Fannie Lou Hamer testified eloquently about the beatings that she and others endured and the threats they faced for trying to register to vote. Turning to the television cameras, Hamer asked, "Is this America?"
Johnson offered the MFDP a "compromise" under which it would receive two non-voting, at-large seats, while the white delegation sent by the official Democratic Party would retain its seats. The MFDP angrily rejected the "compromise."
The MFDP kept up its agitation at the convention, after it was denied official recognition. When all but three of the "regular" Mississippi delegates left because they refused to pledge allegiance to the party, the MFDP delegates borrowed passes from sympathetic delegates and took the seats vacated by the official Mississippi delegates. National party organizers removed them. When they returned the next day, they found convention organizers had removed the empty seats that had been there the day before. They stayed and sang "freedom songs".
The 1964 Democratic Party convention disillusioned many within the MFDP and the Civil Rights Movement, but it did not destroy the MFDP. The MFDP became more radical after Atlantic City. It invited Malcolm X to speak at one of its conventions and opposed the war in Vietnam.
King awarded Nobel Peace Prize.
On December 10, 1964, Dr. Martin Luther King, Jr. was awarded the Nobel Peace Prize, the youngest man to receive the award; he was 35 years of age.
Boycott of New Orleans by American Football League players, January 1965.
After the 1964 professional American Football League season, the AFL All-Star Game had been scheduled for early 1965 in New Orleans' Tulane Stadium. After numerous black players were refused service by a number of New Orleans hotels and businesses, and white cabdrivers refused to carry black passengers, black and white players alike lobbied for a boycott of New Orleans. Under the leadership of Buffalo Bills' players, including Cookie Gilchrist, the players put up a unified front. The game was moved to Jeppesen Stadium in Houston.
The discriminatory practices that prompted the boycott were illegal under the Civil Rights Act of 1964, which had been signed in July 1964. This new law likely encouraged the AFL players in their cause. It was the first boycott by a professional sports event of an entire city.
Selma Voting Rights Movement and the Voting Rights Act, 1965.
SNCC had undertaken an ambitious voter registration program in Selma, Alabama, in 1963, but by 1965 had made little headway in the face of opposition from Selma's sheriff, Jim Clark. After local residents asked the SCLC for assistance, King came to Selma to lead several marches, at which he was arrested along with 250 other demonstrators. The marchers continued to meet violent resistance from police. Jimmie Lee Jackson, a resident of nearby Marion, was killed by police at a later march in February 17, 1965. Jackson's death prompted James Bevel, director of the Selma Movement, to initiate and organize a plan to march from Selma to Montgomery, the state capital.
On March 7, 1965, acting on Bevel's plan, Hosea Williams of the SCLC and John Lewis of SNCC led a march of 600 people to walk the 54 miles (87 km) from Selma to the state capital in Montgomery. Only six blocks into the march, at the Edmund Pettus Bridge, state troopers and local law enforcement, some mounted on horseback, attacked the peaceful demonstrators with billy clubs, tear gas, rubber tubes wrapped in barbed wire, and bull whips. They drove the marchers back into Selma. Lewis was knocked unconscious and dragged to safety. At least 16 other marchers were hospitalized. Among those gassed and beaten was Amelia Boynton Robinson, who was at the center of civil rights activity at the time.
The national broadcast of the news footage of lawmen attacking unresisting marchers' seeking to exercise their constitutional right to vote provoked a national response, as had scenes from Birmingham two years earlier. The marchers were able to obtain a court order permitting them to make the march without incident two weeks later.
The evening of a second march on March 9 to the site of Bloody Sunday, local whites attacked Rev. James Reeb, a voting rights supporter. He died of his injuries in a Birmingham hospital March 11. On March 25, four Klansmen shot and killed Detroit homemaker Viola Liuzzo as she drove marchers back to Selma at night after the successfully completed march to Montgomery.
Eight days after the first march, but before the final march, President Johnson delivered a televised address to support the voting rights bill he had sent to Congress. In it he stated:
Johnson signed the Voting Rights Act of 1965 on August 6. The 1965 act suspended poll taxes, literacy tests, and other subjective voter registration tests. It authorized Federal supervision of voter registration in states and individual voting districts where such tests were being used. African Americans who had been barred from registering to vote finally had an alternative to taking suits to local or state courts, which had seldom prosecuted their cases to success. If discrimination in voter registration occurred, the 1965 act authorized the Attorney General of the United States to send Federal examiners to replace local registrars. Johnson reportedly told associates of his concern that signing the bill had lost the white South as voters for the Democratic Party for the foreseeable future.
The act had an immediate and positive effect for African Americans. Within months of its passage, 250,000 new black voters had been registered, one third of them by federal examiners. Within four years, voter registration in the South had more than doubled. In 1965, Mississippi had the highest black voter turnout at 74% and led the nation in the number of black public officials elected. In 1969, Tennessee had a 92.1% turnout among black voters; Arkansas, 77.9%; and Texas, 73.1%.
Several whites who had opposed the Voting Rights Act paid a quick price. In 1966 Sheriff Jim Clark of Alabama, infamous for using cattle prods against civil rights marchers, was up for reelection. Although he took off the notorious "Never" pin on his uniform, he was defeated. At the election, Clark lost as blacks voted to get him out of office.
Blacks' regaining the power to vote changed the political landscape of the South. When Congress passed the Voting Rights Act, only about 100 African Americans held elective office, all in northern states. By 1989, there were more than 7,200 African Americans in office, including more than 4,800 in the South. Nearly every Black Belt county (where populations were majority black) in Alabama had a black sheriff. Southern blacks held top positions in city, county, and state governments.
Atlanta elected a black mayor, Andrew Young, as did Jackson, Mississippi, with Harvey Johnson, Jr., and New Orleans, with Ernest Morial. Black politicians on the national level included Barbara Jordan, elected as a Representative from Texas in Congress, and President Jimmy Carter appointed Andrew Young as United States Ambassador to the United Nations. Julian Bond was elected to the Georgia State Legislature in 1965, although political reaction to his public Opposition to the U.S. involvement in the Vietnam War prevented him from taking his seat until 1967. John Lewis represents Georgia's 5th congressional district in the United States House of Representatives, where he has served since 1987.
Fair housing movements, 1966–1968.
The first major blow against housing segregation in the era, the Rumford Fair Housing Act, was passed in California in 1963. It was overturned by white California voters and real estate lobbyists the following year with Proposition 14, a move which helped precipitate the Watts Riots. In 1966, the California Supreme Court invalidated Proposition 14 and reinstated the Fair Housing Act.
Struggles for fair housing laws became a major project of the movement over the next two years, with Martin Luther King, Jr., James Bevel, and Al Raby leading the Chicago Freedom Movement around the issue in 1966. In the following year, Father James Groppi and the NAACP Youth Council also attracted national attention with a fair housing campaign in Milwaukee. Both movements faced violent resistance from white homeowners and legal opposition from conservative politicians.
The Fair Housing Bill was the most contentious civil rights legislation of the era. Senator Walter Mondale, who advocated for the bill, noted that over successive years, it was the most filibustered legislation in US history. It was opposed by most Northern and Southern senators, as well as the National Association of Real Estate Boards. A proposed "Civil Rights Act of 1966" had collapsed completely because of its fair housing provision. Mondale commented that:
Memphis, King assassination and the Poor People's March 1968.
Rev. James Lawson invited King to Memphis, Tennessee, in March 1968 to support a sanitation workers' strike. These workers launched a campaign for union representation after two workers were accidentally killed on the job, and King considered their struggle to be a vital part of the Poor People's Campaign he was planning.
A day after delivering his stirring "I've Been to the Mountaintop" sermon, which has become famous for his vision of American society, King was assassinated on April 4, 1968. Riots broke out in black neighborhoods in more than 110 cities across the United States in the days that followed, notably in Chicago, Baltimore, and in Washington, D.C. The damage done in many cities destroyed black businesses and homes, and slowed economic development for a generation.
The day before King's funeral, April 8, Coretta Scott King and three of the King children led 20,000 marchers through the streets of Memphis, holding signs that read, "Honor King: End Racism" and "Union Justice Now". Armed National Guardsmen lined the streets, sitting on M-48 tanks, to protect the marchers, and helicopters circled overhead. On April 9, Mrs. King led another 150,000 people in a funeral procession through the streets of Atlanta. Her dignity revived courage and hope in many of the Movement's members, cementing her place as the new leader in the struggle for racial equality.
Coretta Scott King said,
Rev. Ralph Abernathy succeeded King as the head of the SCLC and attempted to carry forth King's plan for a Poor People's March. It was to unite blacks and whites to campaign for fundamental changes in American society and economic structure. The march went forward under Abernathy's plainspoken leadership but did not achieve its goals.
Civil Rights Act of 1968.
As 1968 began, the fair housing bill was being filibustered once again, but two developments revived it. The Kerner Commission report on the 1967 ghetto riots was delivered to Congress on March 1, and it strongly recommended "a comprehensive and enforceable federal open housing law" as a remedy to the civil disturbances. The Senate was moved to end their filibuster that week.
As the House of Representatives deliberated the bill in April, Dr. King was assassinated, and the largest wave of unrest since the Civil War swept the country. Senator Charles Mathias wrote that
The House passed the legislation on April 10, and President Johnson signed it the next day. The Civil Rights Act of 1968 prohibited discrimination concerning the sale, rental, and financing of housing based on race, religion, and national origin. It also made it a federal crime to "by force or by threat of force, injure, intimidate, or interfere with anyone…by reason of their race, color, religion, or national origin."
Other issues.
Competing ideas.
Despite the common notion that the ideas of Martin Luther King, Jr., Malcolm X and Black Power only conflicted with each other and were the only ideologies of the Civil Rights Movement, there were other sentiments felt by many blacks. Fearing the events during the movement were occurring too quickly, there were some blacks who felt that leaders should take their activism at a slower pace. Others had reservations on how focused blacks were on the movement and felt that such attention was better spent on reforming issues within the black community.
While most popular representations of the movement are centered on the leadership and philosophy of Martin Luther King, Jr., some scholars note that the movement was too diverse to be credited to one person, organization, or strategy. Sociologist Doug McAdam has stated that, "in King's case, it would be inaccurate to say that he was the leader of the modern civil rights movement...but more importantly, there was no singular civil rights movement. The movement was, in fact, a coalition of thousands of local efforts nationwide, spanning several decades, hundreds of discrete groups, and all manner of strategies and tactics—legal, illegal, institutional, non-institutional, violent, non-violent. Without discounting King's importance, it would be sheer fiction to call him the leader of what was fundamentally an amorphous, fluid, dispersed movement."
Those who blatantly rejected integration usually had a legitimate rationale for doing so, such as fearing a change in the status quo they had been used to for so long, or fearing for their safety if they found themselves in environments where whites were much more present. However, there were also those who defended segregation for the sake of keeping ties with the white power structure from which many relied on for social and economic mobility above other blacks. Based on her interpretation of a 1966 study made by Donald Matthews and James Prothro detailing the relative percentage of blacks for integration, against it or feeling something else, Lauren Winner asserts that:
"Black defenders of segregation look, at first blush, very much like black nationalists, especially in their preference for all-black institutions; but black defenders of segregation differ from nationalists in two key ways. First, while both groups criticize NAACP-style integration, nationalists articulate a third alternative to integration and Jim Crow, while segregationists preferred to stick with the status quo. Second, absent from black defenders of segregation's political vocabulary was the demand for self-determination. They called for all-black institutions, but not autonomous all-black institutions; indeed, some defenders of segregation asserted that black people needed white paternalism and oversight in order to thrive."
Oftentimes, African-American community leaders would be staunch defenders of segregation. Church ministers, businessmen and educators were among those who wished to keep segregation and segregationist ideals in order to retain the privileges they gained from patronage from whites, such as monetary gains. In addition, they relied on segregation to keep their jobs and economies in their communities thriving. It was feared that if integration became widespread in the South, black-owned businesses and other establishments would lose a large chunk of their customer base to white-owned businesses, and many blacks would lose opportunities for jobs that were presently exclusive to their interests. On the other hand, there were the everyday, average black people who criticized integration as well. For them, they took issue with different parts of the Civil Rights Movement and the potential for blacks to exercise consumerism and economic liberty without hindrance from whites.
For Martin Luther King, Jr., Malcolm X and other leading activists and groups during the movement, these opposing viewpoints acted as an obstacle against their ideas. These different views made such leaders' work much harder to accomplish, but they were nonetheless important in the overall scope of the movement. For the most part, the black individuals who had reservations on various aspects of the movement and ideologies of the activists were not able to make a game-changing dent in their efforts, but the existence of these alternate ideas gave some blacks an outlet to express their concerns about the changing social structure.
Avoiding the "Communist" label.
On December 17, 1951, the Communist Party–affiliated Civil Rights Congress delivered the petition "We Charge Genocide: "The Crime of Government Against the Negro People"", often shortened to "We Charge Genocide", to the United Nations in 1951, arguing that the U.S. federal government, by its failure to act against lynching in the United States, was guilty of genocide under Article II of the UN Genocide Convention. The petition was presented to the United Nations at two separate venues: Paul Robeson, concert singer and activist, to a UN official in New York City, while William L. Patterson, executive director of the CRC, delivered copies of the drafted petition to a UN delegation in Paris.
Patterson, the editor of the petition, was a leader in the Communist Party USA and head of the International Labor Defense, a group that offered legal representation to communists, trade unionists, and African-Americans in cases involving issues of political or racial persecution. The ILD was known for leading the defense of the Scottsboro boys in Alabama in 1931, where the Communist Party had considerable influence among African Americans in the 1930s. This had largely declined by the late 1950s, although they could command international attention. As earlier Civil Rights figures such as Robeson, Du Bois and Patterson became more politically radical (and therefore targets of Cold War anti-Communism by the US. Government), they lost favor with both mainstream Black America and the NAACP.
In order to secure a place in the mainstream and gain the broadest base, the new generation of civil rights activists believed they had to openly distance themselves from anything and anyone associated with the Communist party. According to Ella Baker, the Southern Christian Leadership Conference adopted "Christian" into its name to deter charges of Communism. The FBI under J. Edgar Hoover had been concerned about communism since the early 20th century, and continued to label as "Communist" or "subversive" some of the civil rights activists, whom it kept under close surveillance. In the early 1960s, the practice of distancing the Civil Rights Movement from "Reds" was challenged by the Student Nonviolent Coordinating Committee who adopted a policy of accepting assistance and participation by anyone, regardless of political affiliation, who supported the SNCC program and was willing to "put their body on the line." At times this political openness put SNCC at odds with the NAACP.
Kennedy administration, 1961–63.
During the years preceding his election to the presidency, John F. Kennedy's record of voting on issues of racial discrimination had been minimal. Kennedy openly confessed to his closest advisors that during the first months of his presidency, his knowledge of the civil rights movement was "lacking".
For the first two years of the Kennedy administration, civil rights activists had mixed opinions of both the president and attorney general, Robert F. Kennedy. Many viewed the administration with suspicion. A well of historical cynicism toward white liberal politics had left African Americans with a sense of uneasy disdain for any white politician who claimed to share their concerns for freedom. Still, many had a strong sense that the Kennedys represented a new age of political dialogue.
Although observers frequently assert the phrases "The Kennedy administration" or "President Kennedy" when discussing the executive and legislative support of the Civil Rights movement between 1960 and 1963, many of the initiatives resulted from Robert Kennedy's passion. Through his rapid education in the realities of racism, Robert Kennedy underwent a thorough conversion of purpose as Attorney-General. The President came to share his brother's sense of urgency on the matters; the Attorney-General succeeded in urging the president to address the issue in a speech to the nation.
Robert Kennedy first became seriously concerned with civil rights in mid-May 1961 during the Freedom Rides, when photographs of the burning bus and savage beatings in Aniston and Birmingham were broadcast around the world. They came at an especially embarrassing time, as President Kennedy was about to have a summit with the Soviet premier in Vienna. The White House was concerned with its image among the populations of newly independent nations in Africa and Asia, and Robert Kennedy responded with an address for Voice of America stating that great progress had been made on the issue of race relations. Meanwhile, behind the scenes, the administration worked to resolve the crisis with a minimum of violence and prevent the Freedom Riders from generating a fresh crop of headlines that might divert attention from the President's international agenda. The "Freedom Riders" documentary notes that, "The back burner issue of civil rights had collided with the urgent demands of Cold War realpolitik."
On May 21, when a white mob attacked and burned the First Baptist Church in Montgomery, Alabama, where King was holding out with protesters, Robert Kennedy telephoned King to ask him to stay in the building until the U.S. Marshals and National Guard could secure the area. King proceeded to berate Kennedy for "allowing the situation to continue". King later publicly thanked Robert Kennedy's commanding the force to break up an attack, which might otherwise have ended King's life.
With a very small majority in Congress, the president's ability to press ahead with legislation relied considerably on a balancing game with the Senators and Congressmen of the South. Without the support of Vice-President Lyndon Johnson, a former Senator who had years of experience in Congress and longstanding relations there, many of the Attorney-General's programs would not have progressed.
By late 1962, frustration at the slow pace of political change was balanced by the movement's strong support for legislative initiatives: housing rights, administrative representation across all US Government departments, safe conditions at the ballot box, pressure on the courts to prosecute racist criminals. King remarked by the end of the year,
This administration has reached out more creatively than its predecessors to blaze new trails, in voting rights and government appointments. Its vigorous young men launched imaginative and bold forays displayed a certain "élan" in the attention they give to civil-rights issues.
From squaring off against Governor George Wallace, to "tearing into" Vice-President Johnson (for failing to desegregate areas of the administration), to threatening corrupt white Southern judges with disbarment, to desegregating interstate transport, Robert Kennedy came to be consumed by the Civil Rights movement. He continued to work on these social justice issues in his bid for the presidency in 1968.
On the night of Governor Wallace's capitulation to African-American enrollment at the University of Alabama, President Kennedy gave an address to the nation, which marked the changing tide, an address that was to become a landmark for the ensuing change in political policy as to civil rights. In it President Kennedy spoke of the need to act decisively and to act now:
Assassination cut short the life and careers of both the Kennedy brothers and Dr. Martin Luther King, Jr. The essential groundwork of the Civil Rights Act 1964 had been initiated before John F. Kennedy was assassinated. The dire need for political and administrative reform was driven home on Capitol Hill by the combined efforts of the Kennedy brothers, Dr. King (and other leaders) and President Lyndon Johnson.
In 1966, Robert Kennedy undertook a tour of South Africa in which he championed the cause of the anti-apartheid movement. His tour gained international praise at a time when few politicians dared to entangle themselves in the politics of South Africa. Kennedy spoke out against the oppression of the black population. He was welcomed by the black population as though a visiting head of state. In an interview with "LOOK" Magazine he said:
American Jewish community and the Civil Rights Movement.
Many in the Jewish community supported the Civil Rights Movement. In fact, statistically Jews were one of the most actively involved non-black groups in the Movement. Many Jewish students worked in concert with African Americans for CORE, SCLC, and SNCC as full-time organizers and summer volunteers during the Civil Rights era. Jews made up roughly half of the white northern volunteers involved in the 1964 Mississippi Freedom Summer project and approximately half of the civil rights attorneys active in the South during the 1960s.
Jewish leaders were arrested while heeding a call from Rev. Dr. Martin Luther King, Jr. in St. Augustine, Florida, in June 1964, where the largest mass arrest of rabbis in American history took place at the Monson Motor Lodge—a nationally important civil rights landmark that was demolished in 2003 so that a Hilton Hotel could be built on the site. Abraham Joshua Heschel, a writer, rabbi, and professor of theology at the Jewish Theological Seminary of America in New York, was outspoken on the subject of civil rights. He marched arm-in-arm with Dr. King in the 1965 Selma to Montgomery march. In the Mississippi civil rights workers' murders of 1964, the two white activists killed, Andrew Goodman and Michael Schwerner, were both Jewish.
Brandeis University, the only nonsectarian Jewish-sponsored college university in the world, created the Transitional Year Program (TYP) in 1968, in part response to Rev. Dr. Martin Luther King's assassination. The faculty created it to renew the University's commitment to social justice. Recognizing Brandeis as a university with a commitment to academic excellence, these faculty members created a chance to disadvantaged students to participate in an empowering educational experience.
The program began by admitting 20 black males. As it developed, two groups have been given chances. The first group consists of students whose secondary schooling experiences and/or home communities may have lacked the resources to foster adequate preparation for success at elite colleges like Brandeis. For example, their high schools do not offer AP or honors courses nor high quality laboratory experiences.
Students selected had to have excelled in the curricula offered by their schools. The second group of students includes those whose life circumstances have created formidable challenges that required focus, energy, and skills that otherwise would have been devoted to academic pursuits. Some have served as heads of their households, others have worked full-time while attending high school full-time, and others have shown leadership in other ways.
The American Jewish Committee, American Jewish Congress, and Anti-Defamation League (ADL) actively promoted civil rights.
While Jews were very active in the civil rights movement in the South, in the North, many had experienced a more strained relationship with African Americans. In communities experiencing white flight, racial rioting, and urban decay, Jewish Americans were more often the last remaining whites in the communities most affected. With Black militancy and the Black Power movements on the rise, Black Anti-Semitism increased leading to strained relations between Blacks and Jews in Northern communities. In New York City, most notably, there was a major socio-economic class difference in the perception of African Americans by Jews. Jews from better educated Upper Middle Class backgrounds were often very supportive of African American civil rights activities while the Jews in poorer urban communities that became increasingly minority were often less supportive largely in part due to more negative and violent interactions between the two groups.
Profile.
Despite large Jewish organisations such as the American Jewish Committee, American Jewish Congress and the ADL being actively involved in the Movement, many Jewish individuals in the Southern states who supported civil rights for African-Americans tended to keep a low profile on "the race issue", in order to avoid attracting the attention of the anti-Black and antisemitic Ku Klux Klan. However, Klan groups exploited the issue of African-American integration and Jewish involvement in the struggle to launch acts of violent antisemitism. As an example of this hatred, in one year alone, from November 1957 to October 1958, temples and other Jewish communal gatherings were bombed and desecrated in Atlanta, Nashville, Jacksonville, and Miami, and dynamite was found under synagogues in Birmingham, Charlotte, and Gastonia, North Carolina. Some rabbis received death threats, but there were no injuries following these outbursts of violence.
Fraying of alliances.
King reached the height of popular acclaim during his life in 1964, when he was awarded the Nobel Peace Prize. His career after that point was filled with frustrating challenges. The liberal coalition that had gained passage of the Civil Rights Act of 1964 and the Voting Rights Act of 1965 began to fray.
King was becoming more estranged from the Johnson administration. In 1965 he broke with it by calling for peace negotiations and a halt to the bombing of Vietnam. He moved further left in the following years, speaking of the need for economic justice and thoroughgoing changes in American society. He believed change was needed beyond the civil rights gained by the movement.
King's attempts to broaden the scope of the Civil Rights Movement were halting and largely unsuccessful, however. King made several efforts in 1965 to take the Movement north to address issues of employment and housing discrimination. SCLC's campaign in Chicago publicly failed, as Chicago Mayor Richard J. Daley marginalized SCLC's campaign by promising to "study" the city's problems. In 1966, white demonstrators holding "white power" signs in notoriously racist Cicero, a suburb of Chicago, threw stones at marchers demonstrating against housing segregation.
Johnson administration: 1963-1968.
Lyndon Johnson made civil rights one of his highest priorities, coupling it with a whites war on poverty. However in creasing the shrill opposition to the War in Vietnam, coupled with the cost of the war, undercut support for his domestic programs.
Under Kennedy, major civil rights legislation had been stalled in Congress his assassination changed everything. On one hand president Lyndon Johnson was a much more skillful negotiator than Kennedy but he had behind him a powerful national momentum demanding immediate action on moral and emotional grounds. Demands for immediate action originated from unexpected directions, especially white Protestant church groups. The Justice Department, led by Robert Kennedy, moved from a posture of defending Kennedy from the quagmire minefield of racial politics to acting to fulfill his legacy. The violent death and public reaction dramatically moved the moderate Republicans, led by Senator Everett McKinley Dirksen, whose support was the margin of victory for the Civil Rights Act of 1964. The act immediately ended de jure (legal) segregation and the era of Jim Crow.
With the civil rights movement at full blast, Lyndon Johnson coupled black entrepreneurship with his war on poverty, setting up special program in the Small Business Administration, the Office of Economic Opportunity, and other agencies. This time there was money for loans designed to boost minority business ownership. Richard Nixon greatly expanded the program, setting up the Office of Minority Business Enterprise (OMBE) in the expectation that black entrepreneurs would help defuze racial tensions and possibly support his reelection .
Race riots, 1963–70.
By the end of World War II, more than half of the country's black population lived in Northern and Western industrial cities rather than Southern rural areas. Migrating to those cities for better job opportunities, education and to escape legal segregation, African Americans often found segregation that existed in fact rather than in law.
Beginning in the 1950s, deindustrialization and restructuring of major industries: railroads and meatpacking, steel industry and car industry, markedly reduced working-class jobs, which had earlier provided middle-class incomes. As the last population to enter the industrial job market, blacks were disadvantaged by its collapse. At the same time, investment in highways and private development of suburbs in the postwar years had drawn many ethnic whites out of the cities to newer housing in expanding suburbs. Urban blacks who did not follow the middle class out of the cities became concentrated in the older housing of inner city neighborhoods, among the poorest in most major cities.
Because jobs in new service areas and parts of the economy were being created in suburbs, unemployment was much higher in many black than in white neighborhoods, and crime was frequent. African Americans rarely owned the stores or businesses where they lived. Many were limited to menial or blue-collar jobs, although union organizing in the 1930s and 1940s had opened up good working environments for some. African Americans often made only enough money to live in dilapidated tenements that were privately owned, or poorly maintained public housing. They also attended schools that were often the worst academically in the city and that had fewer white students than in the decades before WWII.
The racial makeup of most major city police departments, largely ethnic white (especially Irish), was a major factor in adding to racial tensions. Even a black neighborhood such as Harlem had a ratio of one black officer for every six white officers. The majority-black city of Newark, New Jersey had only 145 blacks among its 1322 police officers. Police forces in Northern cities were largely composed of white ethnics, descendants of 19th-century immigrants: mainly Irish, Italian, and Eastern European officers. They had established their own power bases in the police departments and in territories in cities. Some would routinely harass blacks with or without provocation.
Harlem riot of 1964.
When police shot an unarmed black teenager in Harlem in July 1964, tensions escalated out of control. Residents were frustrated with racial inequalities. Rioting broke out, and Bedford-Stuyvesant, a major black neighborhood in Brooklyn erupted next. That summer, rioting also broke out in Philadelphia, for similar reasons. The riots were on a much smaller scale than what would occur in 1965 and later.
Washington responded with a pilot program called Project Uplift. Thousands of young people in Harlem were given jobs during the summer of 1965. The project was inspired by a report generated by HARYOU called "Youth in the Ghetto." HARYOU was given a major role in organizing the project, together with the National Urban League and nearly 100 smaller community organizations. Permanent jobs at living wages were still out of reach of many young black men.
Watts riot (1965).
The new Voting Rights Act of 1965 had no immediate effect on living conditions for poor blacks. A few days after the act became law, a riot broke out in the South Central Los Angeles neighborhood of Watts. Like Harlem, Watts was an impoverished neighborhood with very high unemployment. Its residents confronted a largely white police department that had a history of abuse against blacks.
While arresting a young man for drunk driving, police officers argued with the suspect's mother before onlookers. The spark triggered a massive destruction of property through six days of rioting. Thirty-four people were killed and property valued at about $30 million was destroyed, making the Watts Riots among the most expensive in American history.
With black militancy on the rise, ghetto residents directed acts of anger at the police. Black residents growing tired of police brutality continued to riot. Some young people joined groups such as the Black Panthers, whose popularity was based in part on their reputation for confronting police officers. Riots among blacks occurred in 1966 and 1967 in cities such as Atlanta, San Francisco, Oakland, Baltimore, Seattle, Tacoma, Cleveland, Cincinnati, Columbus, Newark, Chicago, New York City (specifically in Brooklyn, Harlem and the Bronx), and worst of all in Detroit.
Detroit riot of 1967.
In Detroit, a large black middle class had begun to develop among those African-Americans who worked at unionized jobs in the automotive industry; these workers still complained of racist practices, concerns which the United Auto Workers channeled into bureaucratic and ineffective grievance procedures. White mobs enforced the segregation of housing up through the 1960s; upon learning that a new homebuyer was black, whites would congregate outside the home picketing, often breaking windows, committing arson, and attacking their new neighbors. Blacks who were not upwardly mobile were living in substandard conditions, subject to the same problems as African-Americans in Watts and Harlem.
When white police officers shut down an illegal bar and arrested a large group of patrons during the hot summer, furious residents rioted. Blacks looted and destroyed property for five days, and National Guardsmen and federal troops patrolled in tanks through the streets. Residents reported that police officers shot at black people before even determining if the suspects were armed or dangerous. After five days, 43 people had been killed, hundreds injured, and thousands left homeless. $40 to $45 million worth of damage was caused.
State and local governments responded to the riot with a dramatic increase in minority hiring. Mayor Cavanaugh in May 1968 appointed a Special Task Force on Police Recruitment and Hiring, and by July 1972, blacks made up 14 percent of the Detroit police, more than double their percentage in 1967. The Michigan government used its reviews of contracts issued by the state to secure a 21 percent increase in nonwhite employment. In the aftermath of the turmoil, the Greater Detroit Board of Commerce launched a campaign to find jobs for ten thousand "previously unemployable" persons, a preponderant number of whom were black.
Prior to the disorder, Detroit enacted no ordinances to end housing segregation, and few had been enacted in the state of Michigan at all. Governor George Romney immediately responded to the riot of 1967 with a special session of the Michigan legislature where he forwarded sweeping housing proposals that included not only fair housing, but "important relocation, tenants' rights and code enforcement legislation." Romney had supported such proposals in 1965, but abandoned them in the face of organized opposition. White conservative resistance was powerful in 1967 as well, but this time Romney did not relent and once again proposed the housing laws at the regular 1968 session of the legislature.
The governor publicly warned that if the housing measures were not passed, "it will accelerate the recruitment of revolutionary insurrectionists." The laws passed both houses of the legislature. Historian Sidney Fine writes that: ""The Michigan Fair Housing Act, which took effect on November 15, 1968, was stronger than the federal fair housing law…and than just about all the existing state fair housing acts. It is probably more than a coincidence that the state that had experienced the most severe racial disorder of the 1960s also adopted one of the strongest state fair housing acts.""
Detroit's decline had begun in the 1950s, during which the city lost almost a tenth of its population. It has been argued – including by Mayor Coleman Young – that the riot was the primary accelerator of "white flight", an ethnic succession by which white residents moved out of inner-city neighborhoods into the suburbs. In contrast, urban affairs experts largely blame a Supreme Court decision against NAACP lawsuits on school desegregation – 1974's "Milliken v. Bradley" case – which maintained the suburban schools as a lily-white refuge. In his dissenting opinion, Supreme Court Justice William O. Douglas wrote that the "Milliken" decision perpetuated "restrictive covenants" that "maintained...black ghettos." (Detroit lost 12.8% of its white population in the 1950s, 15.2% of its white population in the 1960s, and 21.2% of its white population in the 1970s.)
Nationwide riots of 1967.
In addition to Detroit, over 100 US cities experienced riots in 1967, including Newark, Cincinnati, Cleveland, and Washington D.C. President Johnson created the National Advisory Commission on Civil Disorders in 1967. The commission's final report called for major reforms in employment and public assistance for black communities. It warned that the United States was moving toward separate white and black societies.
King riots (1968).
In April 1968 after the assassination of Dr. Martin Luther King, Jr. in Memphis, Tennessee, rioting broke out in cities across the country from frustration and despair. These included Cleveland, Baltimore, Washington, D.C., Chicago, New York City and Louisville, Kentucky. As in previous riots, most of the damage was done in black neighborhoods. In some cities, it has taken more than a quarter of a century for these areas to recover from the damage of the riots; in others, little recovery has been achieved.
Programs in affirmative action resulted in the hiring of more black police officers in every major city. Today blacks make up a proportional majority of the police departments in cities such as Baltimore, Washington, New Orleans, Atlanta, Newark, and Detroit. Civil rights laws have reduced employment discrimination.
The conditions that led to frequent rioting in the late 1960s have receded, but not all the problems have been solved. With industrial and economic restructuring, hundreds of thousands of industrial jobs disappeared since the later 1950s from the old industrial cities. Some moved South, as has much population following new jobs, and others out of the U.S. altogether. Civil unrest broke out in Miami in 1980, in Los Angeles in 1992, and in Cincinnati in 2001.
Black power, 1966.
During the Freedom Summer campaign of 1964, numerous tensions within the Civil Rights Movement came to the forefront. Many blacks in SNCC developed concerns that white activists from the North were taking over the movement. The massive presence of white students was also not reducing the amount of violence that SNCC suffered, but seemed to be increasing it. Additionally, there was profound disillusionment at Lyndon Johnson's denial of voting status for the Mississippi Freedom Democratic Party. Meanwhile, during CORE's work in Louisiana that summer, that group found the federal government would not respond to requests to enforce the provisions of the Civil Rights Act of 1964, or to protect the lives of activists who challenged segregation. For the Louisiana campaign to survive it had to rely on a local African-American militia called the Deacons for Defense and Justice, who used arms to repel white supremacist violence and police repression. CORE's collaboration with the Deacons was effective against breaking Jim Crow in numerous Louisiana areas.
In 1965, SNCC helped organize an independent political party, the Lowndes County Freedom Organization (LCFO), in the heart of Alabama Klan territory, and permitted its black leaders to openly promote the use of armed self-defense. Meanwhile, the Deacons for Defense and Justice expanded into Mississippi and assisted Charles Evers' NAACP chapter with a successful campaign in Natchez. The same year, the Watts Rebellion took place in Los Angeles, and seemed to show that most black youth were now committed to the use of violence to protest inequality and oppression.
During the March Against Fear in 1966, SNCC and CORE fully embraced the slogan of "black power" to describe these trends towards militancy and self-reliance. In Mississippi, Stokely Carmichael declared, ""I'm not going to beg the white man for anything that I deserve, I'm going to take it. We need power.""
Several people engaging in the Black Power movement started to gain more of a sense in black pride and identity as well. In gaining more of a sense of a cultural identity, several blacks demanded that whites no longer refer to them as "Negroes" but as "Afro-Americans." Up until the mid-1960s, blacks had dressed similarly to whites and straightened their hair. As a part of gaining a unique identity, blacks started to wear loosely fit dashikis and had started to grow their hair out as a natural afro. The afro, sometimes nicknamed the "'fro," remained a popular black hairstyle until the late 1970s.
Black Power was made most public, however, by the Black Panther Party, which was founded by Huey Newton and Bobby Seale in Oakland, California, in 1966. This group followed the ideology of Malcolm X, a former member of the Nation of Islam, using a "by-any-means necessary" approach to stopping inequality. They sought to rid African American neighborhoods of police brutality and created a ten-point plan amongst other things.
Their dress code consisted of black leather jackets, berets, slacks, and light blue shirts. They wore an afro hairstyle. They are best remembered for setting up free breakfast programs, referring to police officers as "pigs", displaying shotguns and a raised fist, and often using the statement of "Power to the people".
Black Power was taken to another level inside prison walls. In 1966, George Jackson formed the Black Guerrilla Family in the California San Quentin State Prison. The goal of this group was to overthrow the white-run government in America and the prison system. In 1970, this group displayed their dedication after a white prison guard was found not guilty of shooting and killing three black prisoners from the prison tower. They retaliated by killing a white prison guard.
Numerous popular cultural expressions associated with black power appeared at this time. Released in August 1968, the number one Rhythm & Blues single for the Billboard Year-End list was James Brown's "Say It Loud – I'm Black and I'm Proud". In October 1968, Tommie Smith and John Carlos, while being awarded the gold and bronze medals, respectively, at the 1968 Summer Olympics, donned human rights badges and each raised a black-gloved Black Power salute during their podium ceremony.
King was not comfortable with the "Black Power" slogan, which sounded too much like black nationalism to him. When King was murdered in 1968, Stokely Carmichael stated that whites murdered the one person who would prevent rampant rioting and that blacks would burn every major city to the ground.
Prison reform.
"Gates v. Collier".
Conditions at the Mississippi State Penitentiary at Parchman, then known as Parchman Farm, became part of the public discussion of civil rights after activists were imprisoned there. In the spring of 1961, Freedom Riders came to the South to test the desegregation of public facilities. By the end of June 1963, Freedom Riders had been convicted in Jackson, Mississippi. Many were jailed in Mississippi State Penitentiary at Parchman. Mississippi employed the trusty system, a hierarchical order of inmates that used some inmates to control and enforce punishment of other inmates.
In 1970 the civil rights lawyer Roy Haber began taking statements from inmates. He collected 50 pages of details of murders, rapes, beatings and other abuses suffered by the inmates from 1969 to 1971 at Mississippi State Penitentiary. In a landmark case known as "Gates v. Collier" (1972), four inmates represented by Haber sued the superintendent of Parchman Farm for violating their rights under the United States Constitution.
Federal Judge William C. Keady found in favor of the inmates, writing that Parchman Farm violated the civil rights of the inmates by inflicting cruel and unusual punishment. He ordered an immediate end to all unconstitutional conditions and practices. Racial segregation of inmates was abolished, as was the trusty system, which allowed certain inmates to have power and control over others.
The prison was renovated in 1972 after the scathing ruling by Judge Keady, who wrote that the prison was an affront to "modern standards of decency." Among other reforms, the accommodations were made fit for human habitation. The system of trusties was abolished. (The prison had armed lifers with rifles and given them authority to oversee and guard other inmates, which led to many abuses and murders.)
In integrated correctional facilities in northern and western states, blacks represented a disproportionate number of the prisoners, in excess of their proportion of the general population. They were often treated as second-class citizens by white correctional officers. Blacks also represented a disproportionately high number of death row inmates. Eldridge Cleaver's book "Soul on Ice" was written from his experiences in the California correctional system; it contributed to black militancy.
Cold War.
There was an international context for the actions of the U.S. Federal government during these years. It had stature to maintain in Europe and a need to appeal to the people in the Third World. In "Cold War Civil Rights: Race and the Image of American Democracy", the historian Mary L. Dudziak wrote that Communists critical of the United States accused the nation for its hypocrisy in portraying itself as the "leader of the free world," when so many of its citizens were subjected to severe racial discrimination and violence; she argued that this was a major factor in moving the government to support civil rights legislation.
See also.
History preservation:
Post–Civil Rights Movement:

</doc>
<doc id="49005" url="https://en.wikipedia.org/wiki?curid=49005" title="Atlantic slave trade">
Atlantic slave trade

The Atlantic slave trade or transatlantic slave trade took place across the Atlantic Ocean from the 15th through 19th centuries. The vast majority of those who were enslaved and transported to the New World, mainly on the triangular trade route and its Middle Passage, were West Africans from the central and western parts of the continent who had been sold by other western Africans to western European slave traders, with a small minority being captured directly by the slave traders in coastal raids, and brought to the Americas. The numbers were so great that Africans who came by way of the slave trade became the most numerous Old World immigrants in both North and South America before the late 18th century. Far more slaves were taken to South America than to the north. The South Atlantic and Caribbean economic system centered on producing commodity crops, and making goods and clothing to sell in Europe, and increasing the numbers of African slaves brought to the New World. This was crucial to those western European countries which, in the late 17th and 18th centuries, were vying with each other to create overseas empires.
The Portuguese were the first to engage in the New World slave trade in the 16th century. Between 1418 and the 1470s, the Portuguese launched a series of exploratory expeditions that remapped the oceans south of Portugal, charting new territories that one explorer described as "oceans where none have ever sailed before." In 1526, the Portuguese completed the first transatlantic slave voyage from Africa to the Americas, and other countries soon followed. Ship owners considered the slaves as cargo to be transported to the Americas as quickly and cheaply as possible, there to be sold to labour in coffee, tobacco, cocoa, sugar and cotton plantations, gold and silver mines, rice fields, construction industry, cutting timber for ships, in skilled labour, and as domestic servants. The first Africans imported to the English colonies were classified as "indentured servants", like workers coming from England, and also as "apprentices for life". By the middle of the 17th century, slavery had hardened as a racial caste; they and their offspring were legally the property of their owners, and children born to slave mothers were slaves. As property, the people were considered merchandise or units of labour, and were sold at markets with other goods and services.
The Atlantic slave traders, ordered by trade volume, were: the Portuguese, the British, the French, the Spanish, and the Dutch Empire. Several had established outposts on the African coast where they purchased slaves from local African leaders. These slaves were managed by a factor who was established on or near the coast to expedite the shipping of slaves to the New World. These slaves were kept in a factory while awaiting shipment. Current estimates are that about 12 million Africans were shipped across the Atlantic, although the number purchased by the traders is considerably higher.
Background.
Atlantic travel.
The Atlantic slave trade arose after trade contacts were first made between the continents of the "Old World" (Europe, Africa, and Asia) and those of the "New World" (North America and South America). For centuries, tidal currents had made ocean travel particularly difficult and risky for the ships that were then available, and as such there had been very little, if any, naval contact between the peoples living in these continents. In the 15th century, however, new European developments in seafaring technologies meant that ships were better equipped to deal with the problem of tidal currents, and could begin traversing the Atlantic Ocean. Between 1600 and 1800, approximately 300,000 sailors engaged in the slave trade visited West Africa. In doing so, they came into contact with societies living along the west African coast and in the Americas which they had never previously encountered. Historian Pierre Chaunu termed the consequences of European navigation "disenclavement", with it marking an end of isolation for some societies and an increase in inter-societal contact for most others.
Historian John Thornton noted, "A number of technical and geographical factors combined to make Europeans the most likely people to explore the Atlantic and develop its commerce". He identified these as being the drive to find new and profitable commercial opportunities outside Europe as well as the desire to create an alternative trade network to that controlled by the Muslim Empire of the Middle East, which was viewed as a commercial, political and religious threat to European Christendom. In particular, European traders wanted to trade for gold, which could be found in western Africa, and also to find a naval route to "the Indies" (India), where they could trade for luxury goods such as spices without having to obtain these items from Middle Eastern Islamic traders.
Although the initial Atlantic naval explorations were performed purely by Europeans, members of many European nationalities were involved, including sailors from Portugal, Spain, the Italian kingdoms, England, France and the Netherlands. This diversity led Thornton to describe the initial "exploration of the Atlantic" as "a truly international exercise, even if many of the dramatic discoveries as those by Christopher Columbus and Ferdinand Magellan were made under the sponsorship of the Iberian monarchs." That leadership later gave rise to the myth that "the Iberians were the sole leaders of the exploration".
African slavery.
Slavery was practiced in some parts of Africa, Europe, Asia and the Americas for many centuries before the beginning of the Atlantic slave trade. There is evidence that enslaved people from some African states were exported to other states in Africa, Europe and Asia prior to the European colonization of the Americas. The African slave trade provided a large number of slaves to Europeans and many more to people in Muslim countries.
The Atlantic slave trade was not the only slave trade from Africa, although it was the largest in volume and intensity. As Elikia M’bokolo wrote in "Le Monde diplomatique":
According to John K. Thornton, Europeans usually bought enslaved people who were captured in endemic warfare between African states. Some Africans had made a business out of capturing Africans from neighboring ethnic groups or war captives and selling them. A reminder of this practice is documented in the Slave Trade Debates of England in the early 19th century: "All the old writers... concur in stating not only that wars are entered into for the sole purpose of making slaves, but that they are fomented by Europeans, with a view to that object." People living around the Niger River were transported from these markets to the coast and sold at European trading ports in exchange for muskets and manufactured goods such as cloth or alcohol. However, the European demand for slaves provided a large new market for the already existing trade. While those held in slavery in their own region of Africa might hope to escape, those shipped away had little chance of returning to Africa.
European colonization and slavery in West Africa.
Upon discovering new lands through their naval explorations, European colonisers soon began to migrate to and settle in lands outside their native continent. Off the coast of Africa, European migrants, under the directions of the Kingdom of Castile, invaded and colonised the Canary Islands during the 15th century, where they converted much of the land to the production of wine and sugar. Along with this, they also captured native Canary Islanders, the Guanches, to use as slaves both on the Islands and across the Christian Mediterranean.
As historian John Thornton remarked, "the actual motivation for European expansion and for navigational breakthroughs was little more than to exploit the opportunity for immediate profits made by raiding and the seizure or purchase of trade commodities". Using the Canary Islands as a naval base, European, at the time primarily Portuguese traders, began to move their activities down the western coast of Africa, performing raids in which slaves would be captured to be later sold in the Mediterranean. Although initially successful in this venture, "it was not long before African naval forces were alerted to the new dangers, and the Portuguese ships began to meet strong and effective resistance", with the crews of several of them being killed by African sailors, whose boats were better equipped at traversing the west African coasts and river systems.
By 1494, the Portuguese king had entered agreements with the rulers of several West African states that would allow trade between their respective peoples, enabling the Portuguese to "tap into" the "well-developed commercial economy in Africa... without engaging in hostilities". "Peaceful trade became the rule all along the African coast", although there were some rare exceptions when acts of aggression led to violence. For instance Portuguese traders attempted to conquer the Bissagos Islands in 1535. In 1571 Portugal, supported by the Kingdom of Kongo, took control of the south-western region of Angola in order to secure its threatened economic interest in the area. Although Kongo later joined a coalition in 1591 to force the Portuguese out, Portugal had secured a foothold on the continent that it continued to occupy until the 20th century. Despite these incidences of occasional violence between African and European forces, many African states ensured that any trade went on in their own terms, for instance, imposing custom duties on foreign ships. In 1525, the Kongolese king, Afonso I, seized a French vessel and its crew for illegally trading on his coast.
Historians have widely debated the nature of the relationship between these African kingdoms and the European traders. The Guyanese historian Walter Rodney (1972) has argued that it was an unequal relationship, with Africans being forced into a "colonial" trade with the more economically developed Europeans, exchanging raw materials and human resources (i.e. slaves) for manufactured goods. He argued that it was this economic trade agreement dating back to the 16th century that led to Africa being underdeveloped in his own time. These ideas were supported by other historians, including Ralph Austen (1987). This idea of an unequal relationship was contested by John Thornton (1998), who argued that "the Atlantic slave trade was not nearly as critical to the African economy as these scholars believed" and that "African manufacturing this period was more than capable of handling competition from preindustrial Europe". However, Anne Bailey, commenting on Thornton's suggestion that Africans and Europeans were equal partners in the Atlantic slave trade, wrote:
16th, 17th and 18th centuries.
The Atlantic slave trade is customarily divided into two eras, known as the First and Second Atlantic Systems.
The First Atlantic system was the trade of enslaved Africans to, primarily, South American colonies of the Portuguese and Spanish empires; it accounted for slightly more than 3% of all Atlantic slave trade. It started (on a significant scale) in about 1502 and lasted until 1580 when Portugal was temporarily united with Spain. While the Portuguese were directly involved in trading enslaved peoples, the Spanish empire relied on the asiento system, awarding merchants (mostly from other countries) the license to trade enslaved people to their colonies. During the first Atlantic system most of these traders were Portuguese, giving them a near-monopoly during the era. Some Dutch, English, and French traders also participated in the slave trade. After the union, Portugal came under Spanish legislation that prohibited it from directly engaging in the slave trade as a carrier. It became a target for the traditional enemies of Spain, losing a large share of the trade to the Dutch, English and French.
The Second Atlantic system was the trade of enslaved Africans by mostly English, Portuguese, French and Dutch traders. The main destinations of this phase were the Caribbean colonies and Brazil, as European nations built up economically slave-dependent colonies in the New World. Slightly more than 3% of the enslaved people exported from Africa were traded between 1450 and 1600, and 16% in the 17th century.
It is estimated that more than half of the entire slave trade took place during the 18th century, with the British, Portuguese and French being the main carriers of nine out of ten slaves abducted in Africa. By the 1690s, the English were shipping the most slaves from West Africa. They maintained this position during the 18th century, becoming the biggest shippers of slaves across the Atlantic.
Following the British and United States' bans on the African slave trade in 1808, it declined, but the period still accounted for 28.5% of the total volume of the Atlantic slave trade.
A burial ground in Campeche, Mexico, suggests slaves had been brought there not long after Hernán Cortés completed the subjugation of Aztec and Mayan Mexico in the 16th century. The graveyard had been in use from approximately 1550 to the late 17th century.
Triangular trade.
The first side of the triangle was the export of goods from Europe to Africa. A number of African kings and merchants took part in the trading of enslaved people from 1440 to about 1833. For each captive, the African rulers would receive a variety of goods from Europe. These included guns, ammunition and other factory made goods. The second leg of the triangle exported enslaved Africans across the Atlantic Ocean to the Americas and the Caribbean Islands. The third and final part of the triangle was the return of goods to Europe from the Americas. The goods were the products of slave-labour plantations and included cotton, sugar, tobacco, molasses and rum. Sir John Hawkins, considered the pioneer of the British slave trade, was the first to run the Triangular trade, making a profit at every stop.
Labour and slavery.
The Atlantic Slave Trade was the result of, among other things, labour shortage, itself in turn created by the desire of European colonists to exploit New World land and resources for capital profits. Native peoples were at first utilized as slave labour by Europeans, until a large number died from overwork and Old World diseases. Alternative sources of labour, such as indentured servitude, failed to provide a sufficient workforce.
Many crops could not be sold for profit, or even grown, in Europe. Exporting crops and goods from the New World to Europe often proved to be more profitable than producing them on the European mainland. A vast amount of labour was needed to create and sustain plantations that required intensive labour to grow, harvest, and process prized tropical crops. Western Africa (part of which became known as "the Slave Coast"), and later Central Africa, became the source for enslaved people to meet the demand for labour.
The basic reason for the constant shortage of labour was that, with large amounts of cheap land available and lots of landowners searching for workers, free European immigrants were able to become landowners themselves after a relatively short time, thus increasing the need for workers.
Thomas Jefferson attributed the use of slave labour in part to the climate, and the consequent idle leisure afforded by slave labour: "For in a warm climate, no man will labour for himself who can make another labour for him. This is so true, that of the proprietors of slaves a very small proportion indeed are ever seen to labour."
African participation in the slave trade.
Africans played a direct role in the slave trade, selling their captives or prisoners of war to European buyers. The prisoners and captives who were sold were usually from neighbouring or enemy ethnic groups. These captive slaves were considered "other", not part of the people of the ethnic group or "tribe" ; African kings held no particular loyalty to them. Sometimes criminals would be sold so that they could no longer commit crimes in that area. Most other slaves were obtained from kidnappings, or through raids that occurred at gunpoint through joint ventures with the Europeans. But some African kings refused to sell any of their captives or criminals. King Jaja of Opobo, a former slave, refused to do business with the slavers completely. 
European participation in the slave trade.
Although Europeans were the market for slaves, Europeans rarely entered the interior of Africa, due to fear of disease and fierce African resistance. In Africa, convicted criminals could be punished by enslavement, a punishment which became more prevalent as slavery became more lucrative. Since most of these nations did not have a prison system, convicts were often sold or used in the scattered local domestic slave market.
As of 1778, Thomas Kitchin estimated that Europeans were bringing an estimated 52,000 slaves to the Caribbean yearly, with the French bringing the most Africans to the French West Indies (13,000 out of the yearly estimate). The Atlantic slave trade peaked in the last two decades of the 18th century, during and following the Kongo Civil War. Wars among tiny states along the Niger River's Igbo-inhabited region and the accompanying banditry also spiked in this period. Another reason for surplus supply of enslaved people was major warfare conducted by expanding states, such as the kingdom of Dahomey, the Oyo Empire, and the Asante Empire.
Slavery in Africa and the New World contrasted.
Forms of slavery varied both in Africa and in the New World. In general, slavery in Africa was not heritable – that is, the children of slaves were free – while in the Americas, children of slave mothers were considered born into slavery. This was connected to another distinction: slavery in West Africa was not reserved for racial or religious minorities, as it was in European colonies, although the case was otherwise in places such as Somalia, where Bantus were taken as slaves for the ethnic Somalis.
The treatment of slaves in Africa was more variable than in the Americas. At one extreme, the kings of Dahomey routinely slaughtered slaves in hundreds or thousands in sacrificial rituals, and slaves as human sacrifices was also known in Cameroon. On the other hand, slaves in other places were often treated as part of the family, "adopted children," with significant rights including the right to marry without their masters' permission. Scottish explorer Mungo Park wrote:
In the Americas, slaves were denied the right to marry freely and masters did not generally accept them as equal members of the family. New World slaves were considered the property of their owners, and slaves convicted of revolt or murder were executed.
Slave market regions and participation.
There were eight principal areas used by Europeans to buy and ship slaves to the Western Hemisphere. The number of enslaved people sold to the New World varied throughout the slave trade. As for the distribution of slaves from regions of activity, certain areas produced far more enslaved people than others. Between 1650 and 1900, 10.24 million enslaved Africans arrived in the Americas from the following regions in the following proportions:
African kingdoms of the era.
There were over 173 city-states and kingdoms in the African regions affected by the slave trade between 1502 and 1853, when Brazil became the last Atlantic import nation to outlaw the slave trade. Of those 173, no fewer than 68 could be deemed nation states with political and military infrastructures that enabled them to dominate their neighbours. Nearly every present-day nation had a pre-colonial predecessor, sometimes an African Empire with which European traders had to barter.
Ethnic groups.
The different ethnic groups brought to the Americas closely corresponds to the regions of heaviest activity in the slave trade. Over 45 distinct ethnic groups were taken to the Americas during the trade. Of the 45, the ten most prominent, according to slave documentation of the era are listed below.
Human toll.
The transatlantic slave trade resulted in a vast and as yet still unknown loss of life for African captives both in and outside America. Approximately 1.2 – 2.4 million Africans died during their transport to the New World. More died soon upon their arrival. The number of lives lost in the procurement of slaves remains a mystery but may equal or exceed the number who survived to be enslaved.
The savage nature of the trade led to the destruction of individuals and cultures. The following figures do not include deaths of enslaved Africans as a result of their labour, slave revolts, or diseases suffered while living among New World populations.
Historian Ana Lucia Araujo has noted that the process of enslavement did not end with arrival on the American shores; the different paths taken by the individuals and groups who were victims of the Atlantic slave trade were influenced by different factors—including the disembarking region, the kind of work performed, gender, age, religion, and language.
A database compiled in the late 1990s put the figure for the transatlantic slave trade at more than 11 million people. For a long time, an accepted figure was 15 million, although this has in recent years been revised down. Estimates by Patrick Manning are that about 12 million slaves entered the Atlantic trade between the 16th and 19th century, but about 1.5 million died on board ship. About 10.5 million slaves arrived in the Americas. Besides the slaves who died on the Middle Passage, more Africans likely died during the slave raids in Africa and forced marches to ports. Manning estimates that 4 million died inside Africa after capture, and many more died young. Manning's estimate covers the 12 million who were originally destined for the Atlantic, as well as the 6 million destined for Asian slave markets and the 8 million destined for African markets.
African conflicts.
According to Dr Kimani Nehusi, the presence of European slavers affected the way in which the legal code in African societies responded to offenders. Crimes traditionally punishable by some other form of punishment became punishable by enslavement and sale to slave traders. According to David Stannard's "American Holocaust", 50% of African deaths occurred in Africa as a result of wars between native kingdoms, which produced the majority of slaves. This includes not only those who died in battles, but also those who died as a result of forced marches from inland areas to slave ports on the various coasts. The practice of enslaving enemy combatants and their villages was widespread throughout Western and West Central Africa, although wars were rarely started to procure slaves. The slave trade was largely a by-product of tribal and state warfare as a way of removing potential dissidents after victory, or financing future wars. However, some African groups proved particularly adept and brutal at the practice of enslaving, such as Oyo, Benin, Igala, Kaabu, Asanteman, Dahomey, the Aro Confederacy and the Imbangala war bands.
In letters written by the Manikongo, Nzinga Mbemba Afonso, to the King João III of Portugal, he writes that Portuguese merchandise flowing in is what is fueling the trade in Africans. He requests the King of Portugal to stop sending merchandise but should only send missionaries. In one of his letters he writes:
Before the arrival of the Portuguese, slavery had already existed in Kongo. Afonso believed that the slave trade should be subject to Kongo law. When he suspected the Portuguese of receiving illegally enslaved persons to sell, he wrote to King João III in 1526 imploring him to put a stop to the practice.
The kings of Dahomey sold war captives into transatlantic slavery; they would otherwise have been killed in a ceremony known as the Annual Customs. As one of West Africa's principal slave states, Dahomey became extremely unpopular with neighbouring peoples. Like the Bambara Empire to the east, the Khasso kingdoms depended heavily on the slave trade for their economy. A family's status was indicated by the number of slaves it owned, leading to wars for the sole purpose of taking more captives. This trade led the Khasso into increasing contact with the European settlements of Africa's west coast, particularly the French. Benin grew increasingly rich during the 16th and 17th centuries on the slave trade with Europe; slaves from enemy states of the interior were sold, and carried to the Americas in Dutch and Portuguese ships. The Bight of Benin's shore soon came to be known as the "Slave Coast".
King Gezo of Dahomey said in the 1840s:
In 1807, the UK Parliament passed the Bill that abolished the trading of slaves. The King of Bonny (now in Nigeria) was horrified at the conclusion of the practice:
Port factories.
After being marched to the coast for sale, enslaved people waited in large forts called factories. The amount of time in factories varied, but Milton Meltzer's "Slavery: A World History" states this period resulted in or around 4.5% of deaths during the transatlantic slave trade. In other words, over 820,000 people would have died in African ports such as Benguela, Elmina and Bonny, reducing the number of those shipped to 17.5 million.
Atlantic shipment.
After being captured and held in the factories, slaves entered the infamous Middle Passage. Meltzer's research puts this phase of the slave trade's overall mortality at 12.5%. Their deaths were the result of brutal treatment and poor care from the time of their capture and throughout their voyage. Around 2.2 million Africans died during these voyages where they were packed into tight, unsanitary spaces on ships for months at a time. Measures were taken to stem the onboard mortality rate, such as enforced "dancing" (as exercise) above deck and the practice of force-feeding enslaved persons who tried to starve themselves. The conditions on board also resulted in the spread of fatal diseases. Other fatalities were suicides, slaves who escaped by jumping overboard. The slave traders would try to fit anywhere from 350 to 600 slaves on one ship. Before the African slave trade was completely banned by participating nations in 1853, 15.3 million enslaved people had arrived in the Americas.
Raymond L. Cohn, an economics professor whose research has focused on economic history and international migration, has researched the mortality rates among Africans during the voyages of the Atlantic slave trade. He found that mortality rates decreased over the history of the slave trade, primarily because the length of time necessary for the voyage was declining. "In the eighteenth century many slave voyages took at least 2½ months. In the nineteenth century, 2 months appears to have been the maximum length of the voyage, and many voyages were far shorter. Fewer slaves died in the Middle Passage over time mainly because the passage was shorter."
Seasoning camps.
Meltzer also states that 33% of Africans would have died in the first year at the seasoning camps found throughout the Caribbean. Jamaica held one of the most notorious of these camps. Dysentery was the leading cause of death. Around 5 million Africans died in these camps, reducing the number of survivors to about 10 million.
European competition.
The trade of enslaved Africans in the Atlantic has its origins in the explorations of Portuguese mariners down the coast of West Africa in the 15th century. Before that, contact with African slave markets was made to ransom Portuguese who had been captured by the intense North African Barbary pirate attacks on Portuguese ships and coastal villages, frequently leaving them depopulated. The first Europeans to use enslaved Africans in the New World were the Spaniards, who sought auxiliaries for their conquest expeditions and labourers on islands such as Cuba and Hispaniola. The alarming decline in the native population had spurred the first royal laws protecting them (Laws of Burgos, 1512–13). The first enslaved Africans arrived in Hispaniola in 1501. After Portugal had succeeded in establishing sugar plantations ("engenhos") in northern Brazil ca. 1545, Portuguese merchants on the West African coast began to supply enslaved Africans to the sugar planters. While at first these planters had relied almost exclusively on the native Tupani for slave labour, after 1570 they began importing Africans, as a series of epidemics had decimated the already destabilized Tupani communities. By 1630, Africans had replaced the Tupani as the largest contingent of labour on Brazilian sugar plantations. This ended the European medieval household tradition of slavery, resulted in Brazil's receiving the most enslaved Africans, and revealed sugar cultivation and processing as the reason that roughly 84% of these Africans were shipped to the New World.
As Britain rose in naval power and settled continental North America and some islands of the West Indies, they became the leading slave traders. At one stage the trade was the monopoly of the Royal Africa Company, operating out of London. But, following the loss of the company's monopoly in 1689, Bristol and Liverpool merchants became increasingly involved in the trade. By the late 17th century, one out of every four ships that left Liverpool harbour was a slave trading ship. Much of the wealth on which the city of Manchester, and surrounding towns, was built in the late 18th century, and for much of the 19th century, was based on the processing of slave-picked cotton and manufacture of cloth. Other British cities also profited from the slave trade. Birmingham, the largest gun-producing town in Britain at the time, supplied guns to be traded for slaves. 75% of all sugar produced in the plantations was sent to London, and much of it was consumed in the highly lucrative coffee houses there.
New World destinations.
The first slaves to arrive as part of a labour force in the New World reached the island of Hispaniola (now Haiti and the Dominican Republic) in 1502. Cuba received its first four slaves in 1513. Jamaica received its first shipment of 4000 slaves in 1518. Slave exports to Honduras and Guatemala started in 1526.
The first enslaved Africans to reach what would become the United States arrived in January 1526 as part of a Spanish attempt to colonize South Carolina near Jamestown. By November the 300 Spanish colonists were reduced to 100, and their slaves from 100 to 70. The enslaved people revolted and joined a nearby Native American tribe, while the Spanish abandoned the colony altogether. Colombia received its first enslaved people in 1533. El Salvador, Costa Rica and Florida began their stints in the slave trade in 1541, 1563 and 1581, respectively.
The 17th century saw an increase in shipments, with Africans arriving in the English colony of Jamestown, Virginia in 1619. These first kidnapped Africans were classed as indentured servants and freed after seven years. Chattel slavery was codified in Virginia law in 1656, and in 1662, the colony adopted the principle of "partus sequitur ventrem", by which children of slave mothers were slaves, regardless of paternity. Irish immigrants took slaves to Montserrat in 1651, and in 1655, slaves were shipped to Belize.
By 1802, Russian colonists noted that "Boston" (U.S.-based) skippers were trading African slaves for otter pelts with the Tlingit people in Southeast Alaska.
The number of the Africans arrived in each area can be calculated taking into consideration that the total number of slaves was close to 10,000,000.
Economics of slavery.
In France in the 18th century, returns for investors in plantations averaged around 6%; as compared to 5% for most domestic alternatives, this represented a 20% profit advantage. Risks—maritime and commercial—were important for individual voyages. Investors mitigated it by buying small shares of many ships at the same time. In that way, they were able to diversify a large part of the risk away. Between voyages, ship shares could be freely sold and bought.
By far the most financially profitable West Indian colonies in 1800 belonged to the United Kingdom. After entering the sugar colony business late, British naval supremacy and control over key islands such as Jamaica, Trinidad, the Leeward Islands and Barbados and the territory of British Guiana gave it an important edge over all competitors; while many British did not make gains, a handful of individuals made small fortunes. This advantage was reinforced when France lost its most important colony, St. Domingue (western Hispaniola, now Haiti), to a slave revolt in 1791 and supported revolts against its rival Britain, after the 1793 French revolution in the name of liberty. Before 1791, British sugar had to be protected to compete against cheaper French sugar.
After 1791, the British islands produced the most sugar, and the British people quickly became the largest consumers. West Indian sugar became ubiquitous as an additive to Indian tea. It has been estimated that the profits of the slave trade and of West Indian plantations created up to one-in-twenty of every pound circulating in the British economy at the time of the Industrial Revolution in the latter half of the 18th century.
Effects.
Historian Walter Rodney has argued that at the start of the slave trade in the 16th century, although there was a technological gap between Europe and Africa, it was not very substantial. Both continents were using Iron Age technology. The major advantage that Europe had was in ship building. During the period of slavery, the populations of Europe and the Americas grew exponentially, while the population of Africa remained stagnant. Rodney contended that the profits from slavery were used to fund economic growth and technological advancement in Europe and the Americas. Based on earlier theories by Eric Williams, he asserted that the industrial revolution was at least in part funded by agricultural profits from the Americas. He cited examples such as the invention of the steam engine by James Watt, which was funded by plantation owners from the Caribbean.
Other historians have attacked both Rodney's methodology and accuracy. Joseph C. Miller has argued that the social change and demographic stagnation (which he researched on the example of West Central Africa) was caused primarily by domestic factors. Joseph Inikori provided a new line of argument, estimating counterfactual demographic developments in case the Atlantic slave trade had not existed. Patrick Manning has shown that the slave trade did have profound impact on African demographics and social institutions, but criticized Inikori's approach for not taking other factors (such as famine and drought) into account, and thus being highly speculative.
Effect on the economy of West Africa.
No scholars dispute the harm done to the enslaved people but the effect of the trade on African societies is much debated, due to the apparent influx of goods to Africans. Proponents of the slave trade, such as Archibald Dalzel, argued that African societies were robust and not much affected by the trade. In the 19th century, European abolitionists, most prominently Dr. David Livingstone, took the opposite view, arguing that the fragile local economy and societies were being severely harmed by the trade.
Because the negative effects of slavery on the economies of Africa have been well documented, namely the significant decline in population, some African rulers likely saw an economic benefit from trading their subjects with European slave traders. With the exception of Portuguese controlled Angola, coastal African leaders "generally controlled access to their coasts, and were able to prevent direct enslavement of their subjects and citizens." Thus, as African scholar John Thornton argues, African leaders who allowed the continuation of the slave trade likely derived an economic benefit from selling their subjects to Europeans. The Kingdom of Benin, for instance, participated in the African slave trade, at will, from 1715 to 1735, surprising Dutch traders, who had not expected to buy slaves in Benin. The benefit derived from trading slaves for European goods was enough to make the Kingdom of Benin rejoin the trans-Atlantic slave trade after centuries of non-participation. Such benefits included military technology (specifically guns and gunpowder), gold, or simply maintaining amicable trade relationships with European nations. The slave trade was therefore a means for some African elite to gain economic advantages. Historian Walter Rodney estimates that by c.1770, the King of Dahomey was earning an estimated £250,000 per year by selling captive African soldiers and enslaved people to the European slave-traders.
Both Thornton and Fage contend that while African political elite may have ultimately benefited from the slave trade, their decision to participate may have been influenced more by what they could lose by not participating. In Fage's article "Slavery and the Slave Trade in the Context of West African History," he notes that for West Africans "... there were really few effective means of mobilizing labour for the economic and political needs of the state" without the slave trade.
Effects on the British economy.
Historian Eric Williams in 1944 argued that the profits that Britain received from its sugar colonies, or from the slave trade between Africa and the Caribbean, was a major factor in financing Britain's industrial revolution. However, he says that by the time of its abolition in 1833 it had lost its profitability and it was in Britain's economic interest to ban it.
Other researchers and historians have strongly contested what has come to be referred to as the “Williams thesis” in academia. David Richardson has concluded that the profits from the slave trade amounted to less than 1% of domestic investment in Britain. Economic historian Stanley Engerman finds that even without subtracting the associated costs of the slave trade (e.g., shipping costs, slave mortality, mortality of British people in Africa, defense costs) or reinvestment of profits back into the slave trade, the total profits from the slave trade and of West Indian plantations amounted to less than 5% of the British economy during any year of the Industrial Revolution. Engerman’s 5% figure gives as much as possible in terms of benefit of the doubt to the Williams argument, not solely because it does not take into account the associated costs of the slave trade to Britain, but also because it carries the full-employment assumption from economics and holds the gross value of slave trade profits as a direct contribution to Britain’s national income. Historian Richard Pares, in an article written before Williams’ book, dismisses the influence of wealth generated from the West Indian plantations upon the financing of the Industrial Revolution, stating that whatever substantial flow of investment from West Indian profits into industry there was occurred after emancipation, not before.
Seymour Drescher and Robert Anstey argue the slave trade remained profitable until the end, and that moralistic reform, not economic incentive, was primarily responsible for abolition. They say slavery remained profitable in the 1830s because of innovations in agriculture.
Karl Marx in his influential economic history of capitalism "Das Kapital" wrote that "...the turning of Africa into a warren for the commercial hunting of black-skins, signaled the rosy dawn of the era of capitalist production." He argued that the slave trade was part of what he termed the "primitive accumulation" of capital, the 'non-capitalist' accumulation of wealth that preceded and created the financial conditions for Britain's industrialisation.
Demographics.
The demographic effects of the slave trade is a controversial and highly debated issue.
Walter Rodney argued that the export of so many people had been a demographic disaster and had left Africa permanently disadvantaged when compared to other parts of the world, and largely explains the continent's continued poverty. He presented numbers showing that Africa's population stagnated during this period, while that of Europe and Asia grew dramatically. According to Rodney, all other areas of the economy were disrupted by the slave trade as the top merchants abandoned traditional industries to pursue slaving, and the lower levels of the population were disrupted by the slaving itself.
Others have challenged this view. J. D. Fage compared the number effect on the continent as a whole. David Eltis has compared the numbers to the rate of emigration from Europe during this period. In the 19th century alone over 50 million people left Europe for the Americas, a far higher rate than were ever taken from Africa.
Other scholars accused Rodney of mischaracterizing the trade between Africans and Europeans. They argue that Africans, or more accurately African elites, deliberately let European traders join in an already large trade in enslaved people and were not patronized.
As Joseph E. Inikori argues, the history of the region shows that the effects were still quite deleterious. He argues that the African economic model of the period was very different from the European, and could not sustain such population losses. Population reductions in certain areas also led to widespread problems. Inikori also notes that after the suppression of the slave trade Africa's population almost immediately began to rapidly increase, even prior to the introduction of modern medicines.
Legacy of racism.
Walter Rodney states, "The role of slavery in promoting racist prejudice and ideology has been carefully studied in certain situations, especially in the USA. The simple fact is that no people can enslave another for four centuries without coming out with a notion of superiority, and when the colour and other physical traits of those peoples were quite different it was inevitable that the prejudice should take a racist form."
Eric Williams argued that, "A racial twist given to what is basically an economic phenomenon. Slavery was not born of racism: rather, racism was the consequence of slavery."
End of the Atlantic slave trade.
In Britain, America, Portugal and in parts of Europe, opposition developed against the slave trade. Davis says that abolitionists assumed "that an end to slave imports would lead automatically to the amelioration and gradual abolition of slavery". Opposition to the trade was led by the Religious Society of Friends (Quakers) and establishment Evangelicals such as William Wilberforce. Many people joined the movement and they began to protest against the trade, but they were opposed by the owners of the colonial holdings. Following Lord Mansfield's decision in 1772, slaves became free upon entering the British isles. Under the leadership of Thomas Jefferson, the new state of Virginia in 1778 became the first state and one of the first jurisdictions anywhere to stop the importation of slaves for sale; it made it a crime for traders to bring in slaves from out of state or from overseas for sale; migrants from other states were allowed to bring their own slaves. The new law freed all slaves brought in illegally after its passage and imposed heavy fines on violators. Denmark, which had been active in the slave trade, was the first country to ban the trade through legislation in 1792, which took effect in 1803. Britain banned the slave trade in 1807, imposing stiff fines for any slave found aboard a British ship ("see Slave Trade Act 1807"). The Royal Navy moved to stop other nations from continuing the slave trade, and declared that slaving was equal to piracy and was punishable by death. The United States Congress passed the Slave Trade Act of 1794, which prohibited the building or outfitting of ships in the U.S. for use in the slave trade. In 1807 Congress outlawed the importation of slaves beginning on 1 January 1808, the earliest date permitted by the United States Constitution for such a ban.
On Sunday, 28 October 1787, William Wilberforce wrote in his diary: ""God Almighty has set before me two great objects, the suppression of the slave trade and the Reformation of society."" For the rest of his life, William Wilberforce dedicated his life as a Member of the British Parliament to opposing the slave trade and working for the abolition of slavery throughout the British Empire. On 22 February 1807, twenty years after he first began his crusade, and in the middle of Britain's war with France, Wilberforce and his team's labours were rewarded with victory. By an overwhelming 283 votes for to 16 against, the motion to abolish the Atlantic slave trade was carried in the House of Commons. The United States acted to abolish the slave trade the same year, but not its internal slave trade which became the dominant character in American slavery until the 1860s. In 1805 the British Order-in-Council had restricted the importation of slaves into colonies that had been captured from France and the Netherlands. Britain continued to press other nations to end its trade; in 1810 an Anglo-Portuguese treaty was signed whereby Portugal agreed to restrict its trade into its colonies; an 1813 Anglo-Swedish treaty whereby Sweden outlawed its slave trade; the Treaty of Paris 1814 where France agreed with Britain that the trade is "repugnant to the principles of natural justice" and agreed to abolish the slave trade in five years; the 1814 Anglo-Netherlands treaty where the Dutch outlawed its slave trade.
With peace in Europe from 1815, and British supremacy at sea secured, the Royal Navy turned its attention back to the challenge and established the West Africa Squadron in 1808, known as the "preventative squadron", which for the next 50 years operated against the slavers. By the 1850s, around 25 vessels and 2,000 officers and men were on the station, supported by some ships from the small United States Navy, and nearly 1,000 "Kroomen"—experienced fishermen recruited as sailors from what is now the coast of modern Liberia. Service on the West Africa Squadron was a thankless and overwhelming task, full of risk and posing a constant threat to the health of the crews involved. Contending with pestilential swamps and violent encounters, the mortality rate was 55 per 1,000 men, compared with 10 for fleets in the Mediterranean or in home waters. Between 1807 and 1860, the Royal Navy's Squadron seized approximately 1,600 ships involved in the slave trade and freed 150,000 Africans who were aboard these vessels. Several hundred slaves a year were transported by the navy to the British colony of Sierra Leone, where they were made to serve as "apprentices" in the colonial economy until the Slavery Abolition Act 1833. Action was taken against African leaders who refused to agree to British treaties to outlaw the trade, for example against "the usurping King of Lagos", deposed in 1851. Anti-slavery treaties were signed with over 50 African rulers.
The last recorded slave ship to land on American soil was the Clotilde, which in 1859 illegally smuggled a number of Africans into the town of Mobile, Alabama. The Africans on board were sold as slaves; however, slavery in the U.S. was abolished five years later following the end of the American Civil War in 1865. The last survivor of the voyage was Cudjoe Lewis, who died in 1935.
The last country to ban the Atlantic slave trade was Brazil in 1831. However, a vibrant illegal trade continued to ship large numbers of enslaved people to Brazil and also to Cuba until the 1860s, when British enforcement and further diplomacy finally ended the Atlantic slave trade. In 1870 Portugal ended the last trade route with the Americas where the last country to import slaves was Brazil. In Brazil, however, slavery itself was not ended until 1888, making it the last country in the Americas to end involuntary servitude.
The historian Walter Rodney contends that it was a decline in the profitability of the triangular trades that made it possible for certain basic human sentiments to be asserted at the decision-making level in a number of European countries- Britain being the most crucial because it was the greatest carrier of African captives across the Atlantic. Rodney states that changes in productivity, technology and patterns of exchange in Europe and the Americas informed the decision by the British to end their participation in the trade in 1807. In 1809 President James Madison outlawed the slave trade with the United States.
Nevertheless, Michael Hardt and Antonio Negri argue that it was neither a strictly economic nor moral matter. First, because slavery was (in practice) still beneficial to capitalism, providing not only influx of capital, but also disciplining hardship into workers (a form of "apprenticeship" to the capitalist industrial plant). The more "recent" argument of a "moral shift" (the basis of the previous lines of this article) is described by Hardt and Negri as an "ideological" apparatus in order to eliminate the sentiment of guilt in western society. Although moral arguments did play a secondary role, they usually had major resonance when used as a strategy to undercut competitors' profits. This argument holds that Eurocentric history has been blind to the most important element in this fight for emancipation, precisely, the constant revolt and the antagonism of slaves' revolts. The most important of those being the Haitian Revolution. The shock of this revolution in 1804, certainly introduces an essential political argument into the end of the slave trade, which happened only three years later.
Legacy.
African diaspora.
The African diaspora which was created via slavery has been a complex interwoven part of American history and culture. In the United States, the success of Alex Haley's book "", published in 1976, and the subsequent television miniseries based upon it "Roots", broadcast on the ABC network in January 1977, led to an increased interest and appreciation of African heritage amongst the African-American community. The influence of these led many African Americans to begin researching their family histories and making visits to West Africa. In turn, a tourist industry grew up to supply them. One notable example of this is through the Roots Homecoming Festival held annually in the Gambia, in which rituals are held through which African Americans can symbolically "come home" to Africa. Issues of dispute have however developed between African Americans and African authorities over how to display historic sites that were involved in the Atlantic slave trade, with prominent voices in the former criticising the latter for not displaying such sites sensitively, but instead treating them as a commercial enterprise.
"Back to Africa".
In 1816, a group of wealthy European-Americans, some of whom were abolitionists and others who were racial segregationists, founded the American Colonization Society with the express desire of returning African Americans who were in the United States to West Africa. In 1820, they sent their first ship to Liberia, and within a decade around two thousand African Americans had been settled in the west African country. Such re-settlement continued throughout the 19th century, increasing following the deterioration of race relations in the southern states of the US following Reconstruction in 1877.
Rastafari movement.
The Rastafari movement, which originated in Jamaica, where 98% of the population are descended from victims of the Atlantic slave trade, has made great efforts to publicize the slavery, and to ensure it is not forgotten, especially through reggae music.
Apologies.
Civil societies.
In 1998, UNESCO designated 23 August as International Day for the Remembrance of the Slave Trade and its Abolition. Since then there have been a number of events recognizing the effects of slavery.
On 9 December 1999 Liverpool City Council passed a formal motion apologizing for the City's part in the slave trade. It was unanimously agreed that Liverpool acknowledges its responsibility for its involvement in three centuries of the slave trade. The City Council has made an unreserved apology for Liverpool's involvement and the continual effect of slavery on Liverpool's Black communities.
Benin.
In 1999, President Mathieu Kerekou of Benin (formerly the Kingdom of Dahomey) issued a national apology for the role Africans played in the Atlantic slave trade. Luc Gnacadja, minister of environment and housing for Benin, later said: "The slave trade is a shame, and we do repent for it." Researchers estimate that 3 million slaves were exported out of the Slave Coast bordering the Bight of Benin.
Ghana.
President Jerry Rawlings of Ghana also apologized for his country's involvement in the slave trade.
World conference against racism.
At the 2001 World Conference Against Racism in Durban, South Africa, African nations demanded a clear apology for slavery from the former slave-trading countries. Some nations were ready to express an apology, but the opposition, mainly from the United Kingdom, Portugal, Spain, the Netherlands, and the United States blocked attempts to do so. A fear of monetary compensation might have been one of the reasons for the opposition. As of 2009, efforts are underway to create a UN Slavery Memorial as a permanent remembrance of the victims of the Atlantic slave trade.
France.
On 30 January 2006, Jacques Chirac (the then French President) said that 10 May would henceforth be a national day of remembrance for the victims of slavery in France, marking the day in 2001 when France passed a law recognising slavery as a crime against humanity.
United Kingdom.
On 27 November 2006, British Prime Minister Tony Blair made a partial apology for Britain's role in the African slavery trade. However African rights activists denounced it as "empty rhetoric" that failed to address the issue properly. They feel his apology stopped shy to prevent any legal retort. Blair again apologized on March 14, 2007.
On 24 August 2007, Ken Livingstone (Mayor of London) apologized publicly for London's role in the slave trade. "You can look across there to see the institutions that still have the benefit of the wealth they created from slavery", he said pointing towards the financial district, before breaking down in tears. He claimed that London was still tainted by the horrors of slavery. Jesse Jackson praised Mayor Livingstone, and added that reparations should be made.
United States of America.
On 24 February 2007 the Virginia General Assembly passed House Joint Resolution Number 728 acknowledging "with profound regret the involuntary servitude of Africans and the exploitation of Native Americans, and call for reconciliation among all Virginians." With the passing of that resolution, Virginia became the first of the 50 United States to acknowledge through the state's governing body their state's involvement in slavery. The passing of this resolution came on the heels of the 400th anniversary celebration of the city of Jamestown, Virginia, which was the first permanent English colony to survive in what would become the United States. Jamestown is also recognized as one of the first slave ports of the American colonies. On 31 May 2007, the Governor of Alabama, Bob Riley, signed a resolution expressing "profound regret" for Alabama's role in slavery and apologizing for slavery's wrongs and lingering effects. Alabama is the fourth state to pass a slavery apology, following votes by the legislatures in Maryland, Virginia, and North Carolina.
On 30 July 2008, the United States House of Representatives passed a resolution apologizing for American slavery and subsequent discriminatory laws. The language included a reference to the "fundamental injustice, cruelty, brutality and inhumanity of slavery and Jim Crow" segregation. On 18 June 2009, the United States Senate issued an apologetic statement decrying the "fundamental injustice, cruelty, brutality, and inhumanity of slavery". The news was welcomed by President Barack Obama.
Uganda.
In 1998, President Yoweri Museveni of Uganda, called tribal chieftains to apologize for their involvement in the slave trade: "African chiefs were the ones waging war on each other and capturing their own people and selling them. If anyone should apologise it should be the African chiefs. We still have those traitors here even today."
Nigeria.
In 2009, the Civil Rights Congress of Nigeria has written an open letter to all African chieftains who participated in trade calling for an apology for their role in the Atlantic slave trade: "We cannot continue to blame the white men, as Africans, particularly the traditional rulers, are not blameless. In view of the fact that the Americans and Europe have accepted the cruelty of their roles and have forcefully apologized, it would be logical, reasonable and humbling if African traditional rulers ... accept blame and formally apologize to the descendants of the victims of their collaborative and exploitative slave trade."

</doc>
<doc id="49006" url="https://en.wikipedia.org/wiki?curid=49006" title="Diosdado Cabello">
Diosdado Cabello

Diosdado Cabello Rondón (born 15 April 1963), more commonly known as Diosdado Cabello, is a Venezuelan politician, member of the National Assembly of Venezuela and a former Speaker of the country's legislature, and active member of the Venezuelan armed forces. He was involved in Hugo Chávez’s return to power after the 2002 coup d'état. He became a leading member of Chavez’s Movimiento V República (MVR), and remains a leading member of the United Socialist Party of Venezuela, into which MVR was merged in 2007. Governor of Miranda from 2004 to 2008, he lost the 2008 election to Henrique Capriles Radonski, and was subsequently appointed Public Works & Housing Minister. In November 2009 he was additionally appointed head of the National Commission of Telecom, a position traditionally independent from Ministry of Public Works and Housing. In 2010, he was elected a member of parliament by his home state of Monagas. In 2011, President Hugo Chávez named him Vice-President of Venezuela’s ruling party, the PSUV. In 2012, he was elected and sworn in as President of the National Assembly of Venezuela, the country’s parliament.
Early life and education.
Diosdado Cabello was born in El Furrial, Monagas State. His background is in engineering. He has an undergraduate degree in systems engineering from the Instituto Universitario Politécnico de las Fuerzas Armadas Nacionales and a graduate degree in engineering project management from the Andrés Bello Catholic University.
Military career.
While at Instituto Universitario Politécnico de las Fuerzas Armadas Nacionales, Cabello befriended Hugo Chávez and they played on the same baseball team.
During Chávez’s abortive coup d'état of February 1992, Cabello led a group of four tanks to attack Miraflores Palace. Cabello was jailed for his participation in the coup, though President Rafael Caldera, who had prior knowledge of the coup, later pardoned him with the rest of the coup participants and Cabello was released after only two years without any charges.
Political career.
Following Chávez’s 1998 electoral victory, he helped set up the pro-Chávez grassroots civil society organizations known as "Bolivarian Circles" which have been compared to Cuba's Committees for the Defence of the Revolution and are parent organizations for the Colectivos. He was head of telecoms regulator Conatel during the time the market was opened to competition. In May 2001 he became Chavez' chief of staff, and was appointed Vice President by President Hugo Chávez on January 13, 2002, replacing Adina Bastidas. As such, he was responsible to both the president and the National Assembly, and for the relations between the executive and legislative branches of the government.
On April 13, 2002, he took on the duties of the presidency on a temporary basis, replacing Pedro Carmona, head of the Venezuelan Chamber of Commerce, as interim president during the coup d'état attempt when Chávez was kept prisoner and was consequently absent from office. Upon taking office, Cabello said that "I, Diosdado Cabello, am assuming the presidency until such time as the president of the republic, Hugo Chávez Frías, appears." A few hours later, Chávez was back in office. This made Cabello’s presidency the world’s second briefest, after that of Mexican President Pedro Lascuráin.
On April 28, 2002, Cabello was replaced as Vice President by José Vicente Rangel. Cabello was named interior minister in May 2002, and then infrastructure minister in January 2003.
In October 2004, Cabello was elected to a four-year term as Governor of Miranda State. He lost the 2008 election to Henrique Capriles Radonski, and was subsequently appointed Public Works & Housing Minister. In November 2009 he was additionally appointed head of Conatel.
In 2011 Cabello was installed as the Vice-President of the United Socialist Party (PSUV), thus becoming the second most powerful figure in the party after Hugo Chávez.
Cabello was appointed president of the National Assembly in early 2012 and was re-elected to that post in January 2013.
Cabello’s status after the death of Hugo Chávez was disputed. Some argue that Cabello was constitutionally required to be the acting President but Nicolás Maduro held the position.
TV program.
Cabello has his own weekly program on state TV, Con el Mazo Dando (Hitting with the Sledge Hammer). In that program Cabello talks about the government's view on many political issues and presents accusations against the opposition. The Inter-American Commission on Human Rights (IACHR) has expressed concerns about how the program has intimidated people that went to the IACHR denouncing the government.
Some Venezuelan commentators have compared the use of illegally recorded private conversations on programs such as Cabello's to the practices in place in the German Democratic Republic as shown in the film The Life of Others.
Amnesty International has denounced the way in which Cabello has revealed details on the travel
arrangements of two human rights defenders in his program and how he routinely shows state monitoring of people that may disagree with the government.
Corruption allegations.
Cabello was nicknamed "the octopus" for having "tentacles everywhere". He is very influential in the Venezuelan government, using a network of patronage throughout the military, ministries and pro-government militias. 
Information presented to the United States State Department by Stratfor claimed that Cabello was "head of one of the major centers of corruption in Venezuela." A Wikileaked U.S. Embassy cable from 2009 characterized Cabello as a “major pole” of corruption within the regime, describing him as “amassing great power and control over the regime’s apparatus as well as a private fortune, often through intimidation behind the scenes.” The communiqué likewise created speculation that “Chavez himself might be concerned about Cabello's growing influence but unable to diminish it.” He is described by a contributor to "The Atlantic" as the "Frank Underwood" of Venezuela under whose watch the National Assembly of Venezuela has made a habit of ignoring constitutional hurdles entirely—at various times preventing opposition members from speaking in session, suspending their salaries, stripping particularly problematic legislators of parliamentary immunity, and, on one occasion, even presiding over the physical beating of unfriendly lawmakers while the assembly was meeting.
Nepotism.
Cabello has been accused on several occasions of nepotism.
His wife, Marlenys Contreras, served as a member of the National Assembly until she became minister of tourism in 2015. Cabello’s sister, Glenna, is a political scientist and current Counsellor of the Venezuelan Permanent Mission to the United Nations. His brother, José David, previously minister of infrastructure, was later in charge of the nation’s taxes as head of SENIAT, Venezuela’s revenue service. Now José David is minister of Industries.
Drug trafficking.
Allegations of corruption involving Cabello includes being head of an international drug trafficking organization, accepting bribes from Derwick Associates for public works projects in Venezuela, using nepotism to reward friends and family members and directing colectivos while paying them with funds from Petróleos de Venezuela. In 2013, there were at least 17 formal corruption allegations lodged against Cabello in Venezuela's prosecutors office.
On January 27, 2015, reports accusing Cabello of drug trafficking emerged. In a series of investigations by the United States government, it was stated that Cabello's alleged involvement in the drug trade as the "capo" (head) of the Cartel of the Suns (Spanish "Cartél de los soles"), had also involved high-ranking generals of Venezuelan military. Cabello has also been accused by the Human Rights Foundation president of corruption and drug trafficking.

</doc>
<doc id="49007" url="https://en.wikipedia.org/wiki?curid=49007" title="Stream cipher">
Stream cipher

[[Image:A5-1 GSM cipher.svg|280px|thumbnail|
The operation of the keystream generator in A5/1, an LFSR-based stream cipher used to encrypt mobile phone conversations.]]
A stream cipher is a symmetric key cipher where plaintext digits are combined with a pseudorandom cipher digit stream (keystream). In a stream cipher each plaintext digit is encrypted one at a time with the corresponding digit of the keystream, to give a digit of the ciphertext stream. Since encryption of each digit is dependent on the current state of the cipher, it is also known as state cipher. In practice, a digit is typically a bit and the combining operation an exclusive-or (XOR)(see cipher).
The pseudorandom keystream is typically generated serially from a random seed value using digital shift registers. The seed value serves as the cryptographic key for decrypting the ciphertext stream.
Stream ciphers represent a different approach to symmetric encryption from block ciphers. Block ciphers operate on large blocks of digits with a fixed, unvarying transformation. This distinction is not always clear-cut: in some modes of operation, a block cipher primitive is used in such a way that it acts effectively as a stream cipher. Stream ciphers typically execute at a higher speed than block ciphers and have lower hardware complexity. However, stream ciphers can be susceptible to serious security problems if used incorrectly (see stream cipher attacks); in particular, the same starting state (seed) must never be used twice.
Loose inspiration from the one-time pad.
Stream ciphers can be viewed as approximating the action of a proven unbreakable cipher, the one-time pad (OTP), sometimes known as the Vernam cipher. A one-time pad uses a keystream of completely random digits. The keystream is combined with the plaintext digits one at a time to form the ciphertext. This system was proved to be secure by Claude E. Shannon in 1949. However, the keystream must be generated completely at random with at least the same length as the plaintext and cannot be used more than once. This makes the system very cumbersome to implement in practice, and as a result the one-time pad has not been widely used, except for the most critical applications.
A stream cipher makes use of a much smaller and more convenient key such as 128 bits. Based on this key, it generates a pseudorandom keystream which can be combined with the plaintext digits in a similar fashion to the one-time pad. However, this comes at a cost. The keystream is now pseudorandom and so is not truly random. The proof of security associated with the one-time pad no longer holds. It is quite possible for a stream cipher to be completely unsecure.
Types of stream ciphers.
A stream cipher generates successive elements of the keystream based on an internal state. This state is updated in essentially two ways: if the state changes independently of the plaintext or ciphertext messages, the cipher is classified as a "synchronous" stream cipher. By contrast, "self-synchronising" stream ciphers update their state based on previous ciphertext digits.
Synchronous stream ciphers.
In a synchronous stream cipher a stream of pseudo-random digits is generated independently of the plaintext and ciphertext messages, and then combined with the plaintext (to encrypt) or the ciphertext (to decrypt). In the most common form, binary digits are used (bits), and the keystream is combined with the plaintext using the exclusive or operation (XOR). This is termed a binary additive stream cipher.
In a synchronous stream cipher, the sender and receiver must be exactly in step for decryption to be successful. If digits are added or removed from the message during transmission, synchronisation is lost. To restore synchronisation, various offsets can be tried systematically to obtain the correct decryption. Another approach is to tag the ciphertext with markers at regular points in the output.
If, however, a digit is corrupted in transmission, rather than added or lost, only a single digit in the plaintext is affected and the error does not propagate to other parts of the message. This property is useful when the transmission error rate is high; however, it makes it less likely the error would be detected without further mechanisms. Moreover, because of this property, synchronous stream ciphers are very susceptible to active attacks: if an attacker can change a digit in the ciphertext, he might be able to make predictable changes to the corresponding plaintext bit; for example, flipping a bit in the ciphertext causes the same bit to be flipped in the plaintext.
Self-synchronizing stream ciphers.
Another approach uses several of the previous "N" ciphertext digits to compute the keystream. Such schemes are known as self-synchronizing stream ciphers, asynchronous stream ciphers or ciphertext autokey (CTAK). The idea of self-synchronization was patented in 1946, and has the advantage that the receiver will automatically synchronise with the keystream generator after receiving "N" ciphertext digits, making it easier to recover if digits are dropped or added to the message stream. Single-digit errors are limited in their effect, affecting only up to "N" plaintext digits.
An example of a self-synchronising stream cipher is a block cipher in cipher feedback (CFB) mode.
Linear feedback shift register-based stream ciphers.
Binary stream ciphers are often constructed using linear feedback shift registers (LFSRs) because they can be easily implemented in hardware and can be readily analysed mathematically. The use of LFSRs on their own, however, is insufficient to provide good security. Various schemes have been proposed to increase the security of LFSRs.
Non-linear combining functions.
Because LFSRs are inherently linear, one technique for removing the linearity is to feed the outputs of several parallel LFSRs into a non-linear Boolean function to form a "combination generator". Various properties of such a "combining function" are critical for ensuring the security of the resultant scheme, for example, in order to avoid correlation attacks. 
Clock-controlled generators.
Normally LFSRs are stepped regularly. One approach to introducing non-linearity is to have the LFSR clocked irregularly, controlled by the output of a second LFSR. Such generators include the stop-and-go generator, the alternating step generator and the shrinking generator.
An alternating step generator comprises three linear feedback shift registers, which we will call LFSR0, LFSR1 and LFSR2 for convenience. The output of one of the registers decides which of the other two is to be used; for instance if LFSR2 outputs a 0, LFSR0 is clocked, and if it outputs a 1, LFSR1 is clocked instead. The output is the exclusive OR of the last bit produced by LFSR0 and LFSR1. The initial state of the three LFSRs is the key.
The stop-and-go generator (Beth and Piper, 1984) consists of two LFSRs. One LFSR is clocked if the output of a second is a "1", otherwise it repeats its previous output. This output is then (in some versions) combined with the output of a third LFSR clocked at a regular rate.
The shrinking generator takes a different approach. Two LFSRs are used, both clocked regularly. If the output of the first LFSR is "1", the output of the second LFSR becomes the output of the generator. If the first LFSR outputs "0", however, the output of the second is discarded, and no bit is output by the generator. This mechanism suffers from timing attacks on the second generator, since the speed of the output is variable in a manner that depends on the second generator's state. This can be alleviated by buffering the output.
Filter generator.
Another approach to improving the security of an LFSR is to pass the entire state of a single LFSR into a non-linear "filtering function".
Other designs.
Instead of a linear driving device, one may use a nonlinear update function. For example, Klimov and Shamir proposed triangular functions (T-functions) with a single cycle on n bit words.
Security.
For a stream cipher to be secure, its keystream must have a large period and it must be impossible to "recover the cipher's key" or internal state from the keystream. Cryptographers also demand that the keystream be free of even subtle biases that would let attackers "distinguish" a stream from random noise, and free of detectable relationships between keystreams that correspond to "related keys" or related cryptographic nonces. This should be true for all keys (there should be no "weak keys"), and true even if the attacker can "know" or "choose" some "plaintext" or "ciphertext".
As with other attacks in cryptography, stream cipher attacks can be "certificational", meaning they are not necessarily practical ways to break the cipher but indicate that the cipher might have other weaknesses.
Securely using a secure synchronous stream cipher requires that one never reuse the same keystream twice; that generally means a different nonce or key must be supplied to each invocation of the cipher. Application designers must also recognize that most stream ciphers don't provide "authenticity", only "privacy": encrypted messages may still have been modified in transit.
Short periods for stream ciphers have been a practical concern. For example, 64-bit block ciphers like DES can be used to generate a keystream in output feedback (OFB) mode. However, when not using full feedback, the resulting stream has a period of around 232 blocks on average; for many applications, this period is far too low. For example, if encryption is being performed at a rate of 8 megabytes per second, a stream of period 232 blocks will repeat after about a half an hour.
Some applications using the stream cipher RC4 are attackable because of weaknesses in RC4's key setup routine; new applications should either avoid RC4 or make sure all keys are unique and ideally unrelated (such as generated by a well-seeded CSPRNG or a cryptographic hash function) and that the first bytes of the keystream are discarded.
Usage.
Stream ciphers are often used for their speed and simplicity of implementation in hardware, and in applications where plaintext comes in quantities of unknowable length like a secure wireless connection. If a block cipher (not operating in a stream cipher mode) were to be used in this type of application, the designer would need to choose either transmission efficiency or implementation complexity, since block ciphers cannot directly work on blocks shorter than their block size. For example, if a 128-bit block cipher received separate 32-bit bursts of plaintext, three quarters of the data transmitted would be padding. Block ciphers must be used in ciphertext stealing or residual block termination mode to avoid padding, while stream ciphers eliminate this issue by naturally operating on the smallest unit that can be transmitted (usually bytes).
Another advantage of stream ciphers in military cryptography is that the cipher stream can be generated in a separate box that is subject to strict security measures and fed to other devices such as a radio set, which will perform the xor operation as part of their function. The latter device can then be designed and used in less stringent environments.
RC4 is the most widely used stream cipher in software; others include: 
A5/1,
A5/2,
Chameleon, 
FISH, 
Helix,
ISAAC, 
MUGI,
Panama,
Phelix, 
Pike,
SEAL, 
SOBER,
SOBER-128 and
WAKE.

</doc>
<doc id="49008" url="https://en.wikipedia.org/wiki?curid=49008" title="Robert Andrews Millikan">
Robert Andrews Millikan

Robert A. Millikan (March 22, 1868 – December 19, 1953) was an American experimental physicist honored with the Nobel Prize for Physics in 1923 for his measurement of the elementary electronic charge and for his work on the photoelectric effect.
Millikan graduated from Oberlin College in 1891 and obtained his doctorate at Columbia University in 1895. In 1896 he became an assistant at the University of Chicago, where he became a full professor in 1910. In 1909 Millikan began a series of experiments to determine the electric charge carried by a single electron. He began by measuring the course of charged water droplets in an electric field. The results suggested that the charge on the droplets is a multiple of the elementary electric charge, but the experiment was not accurate enough to be convincing. He obtained more precise results in 1910 with his famous oil-drop experiment in which he replaced water (which tended to evaporate too quickly) with oil.
In 1914 Millikan took up with similar skill the experimental verification of the equation introduced by Albert Einstein in 1905 to describe the photoelectric effect. He used this same research to obtain an accurate value of Planck’s constant. In 1921 Millikan left the University of Chicago to become director of the Norman Bridge Laboratory of Physics at the California Institute of Technology (Caltech) in Pasadena, California. There he undertook a major study of the radiation that the physicist Victor Hess had detected coming from outer space. Millikan proved that this radiation is indeed of extraterrestrial origin, and he named it "cosmic rays." As chairman of the Executive Council of Caltech (the school's governing body at the time) from 1921 until his retirement in 1945, Millikan helped to turn the school into one of the leading research institutions in the United States. He also served on the board of trustees for Science Service, now known as Society for Science & the Public, from 1921 to 1953.
Biography.
Education.
Robert Andrews Millikan was born on March 22, 1868, in Morrison, Illinois. Millikan went to high school in Maquoketa, Iowa. Millikan received a bachelor's degree in the classics from Oberlin College in 1891 and his doctorate in physics from Columbia University in 1895 – he was the first to earn a Ph.D. from that department.
At the close of my sophomore year [...] my Greek professor [...] asked me to teach the course in elementary physics in the preparatory department during the next year. To my reply that I did not know any physics at all, his answer was, "Anyone who can do well in my Greek can teach physics." "All right," said I, "you will have to take the consequences, but I will try and see what I can do with it." I at once purchased an Avery's "Elements of Physics", and spent the greater part of my summer vacation of 1889 at home – trying to master the subject. [...] I doubt if I have ever taught better in my life than in my first course in physics in 1889. I was so intensely interested in keeping my knowledge ahead of that of the class that they may have caught some of my own interest and enthusiasm.
Millikan's enthusiasm for education continued throughout his career, and he was the coauthor of a popular and influential series of introductory textbooks, which were ahead of their time in many ways. Compared to other books of the time, they treated the subject more in the way in which it was thought about by physicists. They also included many homework problems that asked conceptual questions, rather than simply requiring the student to plug numbers into a formula.
In 1902 he married Greta Ervin Blanchard. They had three sons: Clark Blanchard, Glenn Allan, and Max Franklin.
Charge of the electron.
Starting in 1908, while a professor at the University of Chicago, Millikan worked on an oil-drop experiment in which he measured the charge on a single electron. J.J. Thomson had already discovered the charge-to-mass ratio of the electron. However, the actual charge and mass values were unknown. Therefore, if one of these two values were to be discovered, the other could easily be calculated. Millikan and his then graduate student Harvey Fletcher used the oil-drop experiment to measure the charge of the electron (as well as the electron mass, and Avogadro's number, since their relation to the electron charge was known).
Professor Millikan took sole credit, in return for Harvey Fletcher claiming full authorship on a related result for his dissertation. Millikan went on to win the 1923 Nobel Prize for Physics, in part for this work, and Fletcher kept the agreement a secret until his death. After a publication on his first results in 1910, contradictory observations by Felix Ehrenhaft started a controversy between the two physicists. After improving his setup, Millikan published his seminal study in 1913.
The elementary charge is one of the fundamental physical constants and accurate knowledge of its value is of great importance. His experiment measured the force on tiny charged droplets of oil suspended against gravity between two metal electrodes. Knowing the electric field, the charge on the droplet could be determined. Repeating the experiment for many droplets, Millikan showed that the results could be explained as integer multiples of a common value (1.592 × 10−19 coulomb), the charge on a single electron. That this is somewhat lower than the modern value of 1.602 176 53(14) x 10−19 coulomb is probably due to Millikan's use of an inaccurate value for the viscosity of air.
Although at the time of Millikan's oil-drop experiments it was becoming clear that there exist such things as subatomic particles, not everyone was convinced. Experimenting with cathode rays in 1897, J.J. Thomson had discovered negatively charged 'corpuscles', as he called them, with a charge to mass ratio 1840 times that of a hydrogen ion. Similar results had been found by George FitzGerald and Walter Kaufmann. Most of what was then known about electricity and magnetism, however, could be explained on the basis that charge is a continuous variable; in much the same way that many of the properties of light can be explained by treating it as a continuous wave rather than as a stream of photons.
The beauty of the oil-drop experiment is that as well as allowing quite accurate determination of the fundamental unit of charge, Millikan's apparatus also provided a 'hands on' demonstration that charge is actually quantized. The General Electric Company's Charles Steinmetz, who had previously thought that charge is a continuous variable, became convinced otherwise after working with Millikan's apparatus.
Data selection controversy.
There is some controversy over selectivity in Millikan's use of results from his second experiment measuring the electron charge. This has been discussed by Allan Franklin, a former high-energy experimentalist and current philosopher of science at the University of Colorado. Franklin contends that Millikan's exclusions of data do not affect the final value of the charge obtained, but that Millikan's substantial "cosmetic surgery" reduced the statistical error. This enabled Millikan to give the charge of the electron to better than one half of one percent; in fact, if Millikan had included all of the data he discarded, the error would have been within 2%. While this would still have resulted in Millikan's having measured the charge of "e−" better than anyone else at the time, the slightly larger uncertainty might have allowed more disagreement with his results within the physics community, which Millikan likely tried to avoid. David Goodstein argues that Millikan's statement, that all drops observed over a sixty-day period were used in the paper, was clarified in a subsequent sentence which specified all "drops upon which complete series of observations were made". Goodstein attests that this is indeed the case and notes that five pages of tables separate the two sentences.
Photoelectric effect.
When Einstein published his seminal 1905 paper on the particle theory of light, Millikan was convinced that it had to be wrong, because of the vast body of evidence that had already shown that light was a wave. He undertook a decade-long experimental program to test Einstein's theory, which required building what he described as "a machine shop "in vacuo"" in order to prepare the very clean metal surface of the photo electrode. His results published in 1914 confirmed Einstein's predictions in every detail, but Millikan was not convinced of Einstein's interpretation, and as late as 1916 he wrote, "Einstein's photoelectric equation... cannot in my judgment be looked upon at present as resting upon any sort of a satisfactory theoretical foundation," even though "it actually represents very accurately the behavior" of the photoelectric effect. In his 1950 autobiography, however, he simply declared that his work "scarcely permits of any other interpretation than that which Einstein had originally suggested, namely that of the semi-corpuscular or photon theory of light itself".
Since Millikan's work formed some of the basis for modern particle physics, it is ironic that he was rather conservative in his opinions about 20th century developments in physics, as in the case of the photon theory. Another example is that his textbook, as late as the 1927 version, unambiguously states the existence of the ether, and mentions Einstein's theory of relativity only in a noncommittal note at the end of the caption under Einstein's portrait, stating as the last in a list of accomplishments that he was "author of the special theory of relativity in 1905 and of the general theory of relativity in 1914, both of which have had great success in explaining otherwise unexplained phenomena and in predicting new ones."
Millikan is also credited with measuring the value of Planck's constant by using photoelectric emission graphs of various metals.
Later life.
In 1917, solar astronomer George Ellery Hale convinced Millikan to begin spending several months each year at the Throop College of Technology, a small academic institution in Pasadena, California that Hale wished to transform into a major center for scientific research and education. A few years later Throop College became the California Institute of Technology (Caltech), and Millikan left the University of Chicago in order to become Caltech's "chairman of the executive council" (effectively its president). Millikan would serve in that position from 1921 to 1945. At Caltech most of his scientific research focused on the study of "cosmic rays" (a term which he coined). In the 1930s he entered into a debate with Arthur Compton over whether cosmic rays were composed of high-energy photons (Millikan's view) or charged particles (Compton's view). Millikan thought his cosmic ray photons were the "birth cries" of new atoms continually being created to counteract entropy and prevent the heat death of the universe. Compton would eventually be proven right by the observation that cosmic rays are deflected by the Earth's magnetic field (and so must be charged particles).
Robert Millikan was Vice Chairman of the National Research Council during World War I. During that time, he helped to develop anti-submarine and meteorological devices. He received the Chinese Order of Jade. In his private life, Millikan was an enthusiastic tennis player. He was married and had three sons, the eldest of whom, Clark B. Millikan, became a prominent aerodynamic engineer. Another son, Glenn, also a physicist, married the daughter (Clare) of George Leigh Mallory of "Because it's there" Mount Everest fame. Glenn was killed in a climbing accident in Cumberland Mountains in 1947.
A religious man and the son of a minister, in his later life Millikan argued strongly for a complementary relationship between Christian faith and science. He dealt with this in his Terry Lectures at Yale in 1926–7, published as "Evolution in Science and Religion". A more controversial belief of his was eugenics. This led to his association with the Human Betterment Foundation and his praising of San Marino, California for being "the westernmost outpost of Nordic civilization . . . a population which is twice as Anglo-Saxon as that existing in New York, Chicago or any of the great cities of this country." 
Westinghouse time capsule.
In 1938, he wrote a short passage to be placed in the Westinghouse Time Capsules.
At this moment, August 22, 1938, the principles of representative ballot government, such as are represented by the governments of the Anglo-Saxon, French, and Scandinavian countries, are in deadly conflict with the principles of despotism, which up to two centuries ago had controlled the destiny of man throughout practically the whole of recorded history. If the rational, scientific,
progressive principles win out in this struggle there is a possibility of a warless, golden age ahead for mankind. If the reactionary principles of despotism triumph now and in the future, the future history of mankind will repeat the sad story of war and oppression as in the past.
Death and legacy.
Millikan died of a heart attack at his home in San Marino, California in 1953 at age 85, and was interred in the "Court of Honor" at Forest Lawn Memorial Park Cemetery in Glendale, California.
Millikan Middle School (formerly Millikan Junior High School) in the suburban Los Angeles neighborhood of Sherman Oaks is named in his honor, as is Robert A. Millikan High School in Long Beach, California. The Millikan Library, the tallest building on the Caltech campus is also named after him. The building however, is now used as office space while the actual library has been moved to another building on the Caltech campus. Additionally, a major street through the Tektronix campus in Portland, Oregon, is named after him, with the Millikan Way (MAX station), a station on Portland, Oregon's MAX Blue Line named after the street. One of four suites at the Athenaeum Hotel on the Caltech campus is named after him; Room #50, The Millikan Suite.
On January 26, 1982, he was honored by the United States Postal Service with a 37¢ Great Americans series (1980–2000) postage stamp.
Famous statements.
""If Einstein's equation and Aston's curve are even roughly correct, as I'm sure they are, for Dr. Cameron and I have computed with their aid the maximum energy evolved in radioactive change and found it to check well with observation, then this supposition of an energy evolution through the disintegration of the common elements is from the one point of view a childish Utopian dream, and from the other a foolish bugaboo."" 

</doc>
<doc id="49013" url="https://en.wikipedia.org/wiki?curid=49013" title="Grimm's law">
Grimm's law

Grimm's Law (also known as the First Germanic Sound Shift or Rask's rule) is a set of statements named after Jacob Grimm describing the inherited Proto-Indo-European (PIE) stop consonants as they developed in Proto-Germanic (the common ancestor of the Germanic branch of the Indo-European family) in the 1st millennium BC. It establishes a set of regular correspondences between early Germanic stops and fricatives and the stop consonants of certain other centum Indo-European languages (Grimm used mostly Latin and Greek for illustration).
History.
Grimm's law describes the first non-trivial systematic sound change to be discovered in philology; its formulation was a turning point in the development of linguistics, enabling the introduction of a rigorous methodology to historical linguistic research. The correspondence between Latin p and Germanic f was first noted by Friedrich von Schlegel in 1806. In 1818, Rasmus Christian Rask elaborated the set of correspondences to include other Indo-European languages, such as Sanskrit and Greek, and the full range of consonants involved. In 1822, Jacob Grimm formulated the law as a general rule in his book "Deutsche Grammatik", and extended it to include standard German. (Jacob was the elder of the Brothers Grimm.)
Grimm himself already noticed that there were many words that had different consonants from what his law predicted. These exceptions defied linguists for a few decades, but eventually received explanation from Danish linguist Karl Verner in the form of Verner's law.
Overview.
Grimm's law consists of three parts which form consecutive phases in the sense of a chain shift. The phases are usually constructed as follows:
This chain shift can be abstractly represented as:
Here each sound moves one position to the right to take on its new sound value. Note that within Proto-Germanic, the sounds denoted by , , and were stops in some environments and fricatives in others, so > should be understood here as > , and likewise for the others. The voiceless fricatives are customarily spelled , , and in the context of Germanic.
The exact details of the shift are unknown, and it may have progressed in a variety of ways before arriving at the final situation. The three stages listed above show the progression of a "pull chain", in which each change leaves a "gap" in the phonological system that "pulls" other phonemes into it to fill the gap. But it is also conceivable that the shift happened as a push chain, where the changes happened in reverse order, with each change "pushing" the next forward to avoid merging the phonemes.
The steps could also have occurred somewhat differently. Another possible sequence of events could have been:
This sequence would lead to the same end result. This variety of Grimm's law is often suggested in the context of the glottalic theory of Proto-Indo-European, which is followed by a minority of linguists. This theoretical framework assumes that "voiced stops" in PIE were actually voiceless to begin with, so that the second phase did not actually exist as such, or was not actually devoicing but a loss of some other articulatory feature such as glottalization. This alternative sequence also accounts for the phonetics of Verner's law (see below), which are easier to explain within the glottalic theory framework when Grimm's law is formulated in this manner.
Further changes.
Once the changes described by Grimm's law had taken place, there was only one type of voiced consonant, with no distinction between voiced stops and voiced fricatives. They eventually became stops at the beginning of a word (for the most part), as well as after a nasal consonant, but fricatives elsewhere. Whether they were plosives or fricatives at first is therefore not clear. The voiced aspirated stops may have first become voiced fricatives, before hardening to stops under certain conditions. But they may also have become stops at first, softening to fricatives in most positions later.
Around the same time as the Grimm's law adjustments took place, another change occurred known as Verner's law. Verner's law caused, under certain conditions, the voicing of the voiceless fricatives that resulted from the Grimm's law changes, creating apparent exceptions to the rule. For example:
Here, the same sound "*t" appears as "*þ" in one word (following Grimm's law), but as "*d" in another (apparently violating Grimm's law). See the Verner's law article for a more detailed explanation of this discrepancy.
The early Germanic "*gw" that had arisen from Proto-Indo-European (and from through Verner's law) underwent further changes of various sorts:
Perhaps the usual reflex was "*b" (as suggested by the connection of "bid" < "*bidjaną" and Old Irish "guidid"), but "*w" appears in certain cases (possibly through dissimilation when another labial consonant followed?), such as in "warm" and "wife" (provided that the proposed explanations are correct). Proto-Germanic "*hw" voiced by Verner's law fell together with this sound and developed identically, compare the words for 'she-wolf': from Middle High German "wülbe" and Old Norse "ylgr", one can reconstruct Proto-Germanic nominative singular "*wulbī", genitive singular "*wulgijōz", from earlier "*wulgwī", "*wulgwijōz".
Examples.
Further changes following Grimm's law, as well as sound changes in other Indo-European languages, can sometimes obscure its effects. The most illustrative examples are used here.
This is strikingly regular. Each phase involves one single change which applies equally to the labials () and their equivalent dentals (), velars () and rounded velars (). The first phase left the phoneme repertoire of the language without voiceless stops, the second phase filled this gap, but created a new one, and so on until the chain had run its course.
Behaviour in consonant clusters.
When two obstruents occurred in a pair, the first was changed according to Grimm's law, if possible, while the second was not. If either of the two was voiceless, the whole cluster was devoiced, and the first obstruent also lost its labialisation, if it was present.
Most examples of this occurred with obstruents preceded by *s (resulting in *sp, *st, *sk, *skʷ), or obstruents followed by *t (giving *ft, *ss, *ht, *ht) or *s (giving *fs, *ss, *hs, *hs). The latter change was frequent in suffixes, and became a phonotactic restriction known as the Germanic spirant law. This rule remained productive throughout the Proto-Germanic period. The cluster *tt became *ss, but this was often restored to *st later on.
Examples with preceding *s:
Examples with following *t:
Correspondences to PIE.
The Germanic "sound laws", combined with regular changes reconstructed for other Indo-European languages, allow one to define the expected sound correspondences between different branches of the family. For example, Germanic (word-initial) *b- corresponds regularly to Latin "*f-", Greek ', Sanskrit ', Slavic, Baltic or Celtic "b-", etc., while Germanic "*f-" corresponds to Latin, Greek, Sanskrit, Slavic and Baltic "p-" and to zero (no initial consonant) in Celtic. The former set goes back to PIE * (faithfully reflected in Sanskrit and modified in various ways elsewhere), and the latter set to PIE *p- (shifted in Germanic, lost in Celtic, but preserved in the other groups mentioned here).
One of the more conspicuous present surface correspondences is the English digraph "wh" and the corresponding Latin and Romance digraph "qu," notably found in interrogative words ("wh"-words) such as the five Ws. These both come from , which is echoed in the phonemic spelling "kw" (as in "kwik") for "qu." The present pronunciations have undergone further sound changes, such as "wh"-cluster reductions in English, though the spellings reflect the history more; see Interrogative word: Etymology for details.

</doc>
<doc id="49016" url="https://en.wikipedia.org/wiki?curid=49016" title="Verner's law">
Verner's law

Verner's law, stated by Karl Verner in 1875, describes a historical sound change in the Proto-Germanic language whereby voiceless fricatives *"f", *"þ", *"s", *"h", *"hʷ", when immediately following an unstressed syllable in the same word, underwent voicing and became respectively the fricatives *"b", *"d", *"z", *"g", *"gʷ".
The problem.
When Grimm's law was discovered, a strange irregularity was spotted in its operation. The Proto-Indo-European (PIE) voiceless stops *"p", *"t" and *"k" should have – according to Grimm's law – changed into Proto-Germanic (PGmc) *"f" (bilabial fricative ), *"þ" (dental fricative ) and *"h" (velar fricative ). Indeed, that was known to be the usual development. However, there appeared to be a large set of words in which the agreement of Latin, Greek, Sanskrit, Baltic, Slavic etc. guaranteed PIE *"p", *"t" or *"k", and yet the Germanic reflex was voiced (*"b", *"d" or *"g").
At first, irregularities did not cause concern for scholars since there were many examples of the regular outcome. Increasingly, however, it became the ambition of linguists like the Neogrammarians to formulate general and "exceptionless" rules of sound change that would account for all the data (or as close to all the data as possible), not merely for a well-behaved subset of it.
One classic example of PIE *"t" → PGmc *"d" is the word for 'father'. PIE *"" (here, the macron marks vowel length) → PGmc *"fadēr" (instead of expected *"faþēr"). The structurally similar family term *"bʰréh₂tēr" 'brother' did indeed develop as predicted by Grimm's Law (Gmc. *"brōþēr"). Even more curiously, they often found "both" *"þ" and *"d" as reflexes of PIE *"t" in different forms of one and the same root, e.g. *"werþaną" 'to turn', preterite third-person singular *"warþ" 'he turned', but preterite third-person plural *"wurdun" and past participle *"wurdanaz".
Solution.
Karl Verner was the first scholar to note the factor governing the distribution of the two outcomes. He observed that the apparently unexpected voicing of voiceless stops occurred if they were non-word-initial and if the vowel preceding them carried no stress in PIE. The original location of stress was often retained in Greek and early Sanskrit; in Germanic, though, stress eventually became fixed on the initial (root) syllable of all words. The crucial difference between ' and ' was therefore one of second-syllable versus first-syllable stress (cf. Sanskrit "pitā́" versus "bhrā́tā").
The *"werþaną" : *"wurdun" contrast is likewise explained as due to stress on the root versus stress on the inflectional suffix (leaving the first syllable unstressed). There are also other Vernerian alternations, as illustrated by modern German "ziehen" 'to draw, pull' : Old High "zogōn" 'to tug, drag' ← PGmc. *"teuhaną" : *"tugōną" ← Pre-Germanic *' : *' 'lead'.
There is a spinoff from Verner's Law: the rule accounts also for PGmc *"z" as the development of PIE *"s" in some words. Since this *"z" changed to *"r" in the Scandinavian languages and in West Germanic (German, Dutch, English, Frisian), Verner's Law resulted in alternation of *"s" and *"r" in some inflectional paradigms, known as grammatischer Wechsel. For example, the Old English verb "ceosan" 'choose' had the past plural form "curon" and the past participle "(ge)coren" ← *"keusaną" : *"kuzun" ~ *"kuzanaz" ← Pre-Germanic *' : *' ~ *"" 'taste, try'. We would have "chorn" for "chosen" in Modern English if the consonantal shell of "choose" and "chose" had not been morphologically levelled (cf. obs. German †"kiesen" 'to choose' : "gekoren" 'chosen'). On the other hand, Vernerian *"r" has not been levelled out in En "were" ← PGmc *"wēzun", related to En "was". Similarly, En "lose", though it has the weak form "lost", also has the archaic form "lorn" (now seen in the compound "forlorn") (cf. Dutch "verliezen" : "verloren"); in German, on the other hand, the *"s" has been levelled out both in "war" 'was' (pl. "waren" 'were') and "verlieren" 'lose' (part. "verloren" 'lost').
The following table illustrates the sound changes according to Verner. In the bottom row, for each pair, the sound on the right represents the sound changed according to Verner's Law.
Significance.
Karl Verner published his discovery in the article "Eine Ausnahme der ersten Lautverschiebung" (an exception to the first sound shift) in Kuhn's "Journal of Comparative Linguistic Research" in 1876, but he had presented his theory already on 1 May, 1875 in a comprehensive personal letter to his friend and mentor, Vilhelm Thomsen.
It was received with great enthusiasm by the young generation of comparative philologists, the so-called "Junggrammatiker", because it was an important argument in favour of the Neogrammarian dogma that the sound laws were without exceptions ("die Ausnahmslosigkeit der Lautgesetze").
Dating the change described by Verner's law.
It is worth noting that the change in the pronunciation of the consonant, described by Verner's Law, must have occurred before the shift of stress to the first syllable. The voicing of the new consonant in Proto-Germanic is conditioned by which syllable is stressed in Proto-Indo-European, yet this syllabic stress has disappeared in Proto-Germanic, so the change in the consonant must have occurred at a time when the syllabic stress in earlier Proto-Germanic still conformed to the Indo-European pattern. However, the syllabic stress shift erased the conditioning environment, and made the variation between voiceless fricatives and their voiced alternants look mysteriously haphazard. 
Until recently it was assumed that Verner's law was productive "after" Grimm's Law. Now it has been pointed out (Vennemann 1984:21, Kortlandt 1988:5-6) that, even if the sequence is reversed, the result can be just the same given certain conditions. Noske (2012) argues that Grimm's Law and Verner's Law must have been part of a single bifurcating chain shift.
Newer considerations regarding the dating.
Some scholars today—e.g. Wolfram Euler and Konrad Badenheuer (2009), pp. 54 f. and 61–64, see below—are inclined towards preferring a new theory in which the sequence of the two changes is the opposite of what was previously assumed. This chronological reordering, however, has far-reaching implications on the shape and development of the Proto-Germanic language. The traditionally assumed order has been gradually put into question since ca. 1998 based on the following two main arguments:
Moreover, the combination of the above-mentioned traditional order (Grimm's before Verner's) and the dating of Grimm's law to the 1st century BC requires an unusually fast change of the late Common Germanic at the turn of the millennium: within only a few decades, the three dramatic changes mentioned below would have had to happen in quick succession. This would be the only way to explain that all Germanic languages show these changes. Such a rapid language change seems implausible.
Against this background, the thesis that Verner's Law might have been valid before Grimm's Law—maybe long before it—has been finding more and more acceptance. Accordingly this order now would have to be assumed:
If Kluge's law is valid, it also requires Verner's law to precede Grimm's.
Here is a table with an alternative view of Verner's law, occurring before the shift of Grimm's law.
It is required to postulate aspiration in the voiceless stops, because the results of Verner's law merge with the descendants of the voiced aspirate stops, not of the plain voiced stops. (This can however be bypassed in the glottalic theory framework, where the voiced aspirate stops are replaced with plain voiced stops, and plain voiced stops with glottalized stops.)
There is, however, a phonologic argument against this dating: The traditional order makes it possible to narrow down the effect of Verner's law to the voiceless fricatives. If on the other hand one wants to apply the First Sound Shift after Verner's law, one has to suppose that Verner's law applies both to voiceless plosives *"p", *"t", *"k" and *"kʷ" and to the voiceless fricative *"s". In other words, in this scenario, Verner's law affected all obstruents, not just fricatives. As for the names "Cimbri" and "Vacalus", it could simply be that the presence of in these two words was due to Roman scribes hearing the early Germanic *"h" () sound as a rather than an , since their own did not often occur between vowels and was at any rate already in the process of going silent.

</doc>
<doc id="49020" url="https://en.wikipedia.org/wiki?curid=49020" title="Road transport">
Road transport

Road transport (British English) or road transportation (American English) is the transport of passengers or goods on roads.
History.
Early roads.
The first methods of road transport were horses, oxen or even humans carrying goods over dirt tracks that often followed game trail. The Persians later built a network of Royal Roads across their empire.
With the advent of the Roman Empire, there was a need for armies to be able to travel quickly from one area to another, and the roads that existed were often muddy, which greatly delayed the movement of large masses of troops. To resolve this issue, the Romans built great roads. The Roman roads used deep roadbeds of crushed stone as an underlying layer to ensure that they kept dry, as the water would flow out from the crushed stone, instead of becoming mud in clay soils. The Islamic Caliphate later built tar-paved roads in Baghdad.
New road networks.
As states developed and became richer, especially with the Renaissance, new roads and bridges began to be built, often based on Roman designs. Although there were attempts to rediscover Roman methods, there was little useful innovation in road building before the 18th century.
Starting in the early 18th century, the British Parliament began to pass a series of acts that gave the local justices powers to erect toll-gates on the roads, in exchange for professional upkeep. The toll-gate erected at Wadesmill became the first effective toll-gate in England. The first scheme that had trustees who were not justices was established through a Turnpike Act in 1707, for a section of the London-Chester road between Fornhill and Stony Stratford. The basic principle was that the trustees would manage resources from the several parishes through which the highway passed, augment this with tolls from users from outside the parishes and apply the whole to the maintenance of the main highway. This became the pattern for the turnpiking of a growing number of highways, sought by those who wished to improve flow of commerce through their part of a county.
The quality of early turnpike roads was varied. Although turnpiking did result in some improvement to each highway, the technologies used to deal with geological features, drainage, and the effects of weather, were all in their infancy. Road construction improved slowly, initially through the efforts of individual surveyors such as John Metcalf in Yorkshire in the 1760s. British turnpike builders began to realise the importance of selecting clean stones for surfacing, and excluding vegetable material and clay to make better lasting roads.
Industrial civil engineering.
By the late 18th and early 19th centuries, new methods of highway construction had been pioneered by the work of three British engineers, John Metcalf, Thomas Telford and John Loudon McAdam, and by the French road engineer Pierre-Marie-Jérôme Trésaguet.
The first professional road builder to emerge during the Industrial Revolution was John Metcalf, who constructed about of turnpike road, mainly in the north of England, from 1765. He believed a good road should have good foundations, be well drained and have a smooth convex surface to allow rainwater to drain quickly into ditches at the side. He understood the importance of good drainage, knowing it was rain that caused most problems on the roads.
Pierre-Marie-Jérôme Trésaguet established the first scientific approach to road building in France at the same time. He wrote a memorandum on his method in 1775, which became general practice in France. It involved a layer of large rocks, covered by a layer of smaller gravel. The lower layer improved on Roman practice in that it was based on the understanding that the purpose of this layer (the sub-base or base course) is to transfer the weight of the road and its traffic to the ground, while protecting the ground from deformation by spreading the weight evenly. Therefore, the sub-base did not have to be a self-supporting structure. The upper running surface provided a smooth surface for vehicles, while protecting the large stones of the sub-base.
The surveyor and engineer Thomas Telford also made substantial advances in the engineering of new roads and the construction of bridges. His method of road building involved the digging of a large trench in which a foundation of heavy rock was set. He also designed his roads so that they sloped downwards from the centre, allowing drainage to take place, a major improvement on the work of Trésaguet. The surface of his roads consisted of broken stone. He also improved on methods for the building of roads by improving the selection of stone based on thickness, taking into account traffic, alignment and slopes. During his later years, Telford was responsible for rebuilding sections of the London to Holyhead road, a task completed by his assistant of ten years, John MacNeill.
It was another Scottish engineer, John Loudon McAdam, who designed the first modern roads. He developed an inexpensive paving material of soil and stone aggregate (known as macadam). His road building method was simpler than Telford's, yet more effective at protecting roadways: he discovered that massive foundations of rock upon rock were unnecessary, and asserted that native soil alone would support the road and traffic upon it, as long as it was covered by a road crust that would protect the soil underneath from water and wear.
Also unlike Telford and other road builders, McAdam laid his roads as level as possible. His road required only a rise of three inches from the edges to the center. Cambering and elevation of the road above the water table enabled rain water to run off into ditches on either side. Size of stones was central to the McAdam's road building theory. The lower road thickness was restricted to stones no larger than . The upper layer of stones was limited to size and stones were checked by supervisors who carried scales. A workman could check the stone size himself by seeing if the stone would fit into his mouth. The importance of the 20 mm stone size was that the stones needed to be much smaller than the 100 mm width of the iron carriage tyres that traveled on the road. Macadam roads were being built widely in the United States and Australia in the 1820s and in Europe in the 1830s and 1840s.
20th Century.
Macadam roads were adequate for use by horses and carriages or coaches, but they were very dusty and subject to erosion with heavy rain. The Good Roads Movement occurred in the United States between the late 1870s and the 1920s. Advocates for improved roads led by bicyclists turned local agitation into a national political movement.
Outside cities, roads were dirt or gravel; mud in the winter and dust in the summer. Early organizers cited Europe where road construction and maintenance was supported by national and local governments. In its early years, the main goal of the movement was education for road building in rural areas between cities and to help rural populations gain the social and economic benefits enjoyed by cities where citizens benefited from railroads, trolleys and paved streets. Even more than traditional vehicles, the newly invented bicycles could benefit from good country roads.Later on, they did not hold up to higher-speed motor vehicle use. Methods to stabilise macadam roads with tar date back to at least 1834 when John Henry Cassell, operating from "Cassell's Patent Lava Stone Works" in Millwall, patented "Pitch Macadam".
This method involved spreading tar on the subgrade, placing a typical macadam layer, and finally sealing the macadam with a mixture of tar and sand. Tar-grouted macadam was in use well before 1900, and involved scarifying the surface of an existing macadam pavement, spreading tar, and re-compacting. Although the use of tar in road construction was known in the 19th century, it was little used and was not introduced on a large scale until the motorcar arrived on the scene in the early 20th century.
Modern tarmac was patented by British civil engineer Edgar Purnell Hooley, who noticed that spilled tar on the roadway kept the tar down and created a smooth surface. He took out a patent in 1901 for tarmac.
Transportation.
Transport on roads can be roughly grouped into the transportation of goods and transportation of people. In many countries licensing requirements and safety regulations ensure a separation of the two industries.
The nature of road transportation of goods depends, apart from the degree of development of the local infrastructure, on the distance the goods are transported by road, the weight and volume of the individual shipment, and the type of goods transported. For short distances and light, small shipments a van or pickup truck may be used. For large shipments even if less than a full truckload a truck is more appropriate. (Also see Trucking and Hauling below). In some countries cargo is transported by road in horse-drawn carriages, donkey carts or other non-motorized mode. Delivery services are sometimes considered a separate category from cargo transport. In many places fast food is transported on roads by various types of vehicles. For inner city delivery of small packages and documents bike couriers are quite common.
People are transported on roads either in individual cars or in mass transit by bus or coach. Special modes of individual transport by road such as cycle rickshaws may also be locally available. There are also specialist modes of road transport for particular situations, such as ambulances.
Trucking and haulage.
Trucking companies (AE) or haulage companies / hauliers (BE) accept cargo for road transport. Truck drivers operate either independently – working directly for the client – or through freight carriers or shipping agents. Some big companies (e.g. grocery store chains) operate their own internal trucking operations. The market size for general freight trucking was nearly $125 billion in 2010. Since 2005, the trucking industry has decreased by 1%.
In the U.S. many truckers own their truck (rig), and are known as owner-operators. Some road transportation is done on regular routes or for only one consignee per run, while others transport goods from many different loading stations/shippers to various consignees. On some long runs only cargo for one leg of the route (to) is known when the cargo is loaded. Truckers may have to wait at the destination for the return cargo (from).
A bill of lading issued by the shipper provides the basic document for road freight. On cross-border transportation the trucker will present the cargo and documentation provided by the shipper to customs for inspection (for EC see also Schengen Agreement). This also applies to shipments that are transported out of a free port.
To avoid accidents caused by fatigue, truckers have to keep to strict rules for drivetime and required rest periods. In the United States and Canada, these regulations are known as hours of service, and in the European Union as drivers working hours. One such regulation is the Hours of Work and Rest Periods (Road Transport) Convention, 1979. Tachographs record the times the vehicle is in motion and stopped. Some companies use two drivers per truck to ensure uninterrupted transportation; with one driver resting or sleeping in a bunk in the back of the cab while the other is driving.
Truck drivers often need special licences to drive, known in the U.S. as a commercial driver's license. In the U.K. a Large Goods Vehicle licence is required.
For transport of hazardous materials (see dangerous goods) truckers need a licence, which usually requires them to pass an exam (e.g. in the EU). They have to make sure they affix proper labels for the respective hazard(s) to their vehicle. Liquid goods are transported by road in tank trucks (AE) or tanker lorries (BE) (also road-tankers) or special tankcontainers for intermodal transport. For unpackaged goods and liquids weigh stations confirm weight after loading and before delivery. For transportation of live animals special requirements have to be met in many countries to prevent cruelty to animals (see animal rights). For fresh and frozen goods refrigerator trucks or reefer (container)s are used.
In Australia road trains replace rail transport for goods on routes throughout the center of the country. B-doubles and semi-trailers are used in urban areas because of their smaller size. Low-loader or flat-bed trailers are used to haul containers, see containerization, in intermodal transport.
Modern roads.
Today, roadways are primarily asphalt or concrete. Both are based on McAdam's concept of stone aggregate in a binder, asphalt cement or Portland cement respectively. Asphalt is known as a flexible pavement, one which slowly will "flow" under the pounding of traffic. Concrete is a rigid pavement, which can take heavier loads but is more expensive and requires more carefully prepared subbase. So, generally, major roads are concrete and local roads are asphalt. Concrete roads are often covered with a thin layer of asphalt to create a wearing surface.
Modern pavements are designed for heavier vehicle loads and faster speeds, requiring thicker slabs and deeper subbase. Subbase is the layer or successive layers of stone, gravel and sand supporting the pavement. It is needed to spread out the slab load bearing on the underlying soil and to conduct away any water getting under the slabs. Water will undermine a pavement over time, so much of pavement and pavement joint design are meant to minimize the amount of water getting and staying under the slabs.
Shoulders are also an integral part of highway design. They are multipurpose; they can provide a margin of side clearance, a refuge for incapacitated vehicles, an emergency lane, and parking space. They also serve a design purpose, and that is to prevent water from percolating into the soil near the main pavement's edge. Shoulder pavement is designed to a lower standard than the pavement in the traveled way and won't hold up as well to traffic, so driving on the shoulder is generally prohibited.
Pavement technology is still evolving, albeit in not easily noticed increments. For instance, chemical additives in the pavement mix make the pavement more weather resistant, grooving and other surface treatments improve resistance to skidding and hydroplaning, and joint seals which were once tar are now made of low maintenance neoprene.
Traffic control.
Nearly all roadways are built with devices meant to control traffic. Most notable to the motorist are those meant to communicate directly with the driver. Broadly, these fall into three categories: signs, signals or pavement markings. They help the driver navigate; they assign the right-of-way at intersections; they indicate laws such as speed limits and parking regulations; they advise of potential hazards; they indicate passing and no passing zones; and otherwise deliver information and to assure traffic is orderly and safe.
Two hundred years ago these devices were signs, nearly all informal. In the late 19th century signals began to appear in the biggest cities at a few highly congested intersections. They were manually operated, and consisted of semaphores, flags or paddles, or in some cases colored electric lights, all modeled on railroad signals. In the 20th century signals were automated, at first with electromechanical devices and later with computers. Signals can be quite sophisticated: with vehicle sensors embedded in the pavement, the signal can control and choreograph the turning movements of heavy traffic in the most complex of intersections. In the 1920s traffic engineers learned how to coordinate signals along a thoroughfare to increase its speeds and volumes. In the 1980s, with computers, similar coordination of whole networks became possible.
In the 1920s pavement markings were introduced. Initially they were used to indicate the road's centerline. Soon after they were coded with information to aid motorists in passing safely. Later, with multi-lane roads they were used to define lanes. Other uses, such as indicating permitted turning movements and pedestrian crossings soon followed.
In the 20th century traffic control devices were standardized. Before then every locality decided on what its devices would look like and where they would be applied. This could be confusing, especially to traffic from outside the locality. In the United States standardization was first taken at the state level, and late in the century at the federal level. Each country has a Manual of Uniform Traffic Control Devices (MUTCD) and there are efforts to blend them into a worldwide standard.
Besides signals, signs, and markings, other forms of traffic control are designed and built into the roadway. For instance, curbs and rumble strips can be used to keep traffic in a given lane and median barriers can prevent left turns and even U-turns.
Toll roads.
Early toll roads were usually built by private companies under a government franchise. They typically paralleled or replaced routes already with some volume of commerce, hoping the improved road would divert enough traffic to make the enterprise profitable. Plank roads were particularly attractive as they greatly reduced rolling resistance and mitigated the problem of getting mired in mud. Another improvement, better grading to lessen the steepness of the worst stretches, allowed draft animals to haul heavier loads.
A "toll road" in the United States is often called a "turnpike". The term "turnpike" probably originated from the gate, often a simple pike, which blocked passage until the fare was paid at a "toll house" (or "toll booth" in current terminology). When the toll was paid the pike, which was mounted on a swivel, was turned to allow the vehicle to pass. Tolls were usually based on the type of cargo being transported, not the type of vehicle. The practice of selecting routes so as to avoid tolls is called shunpiking. This may be simply to avoid the expense, as a form of economic protest (or boycott), or simply to seek a road less traveled as a bucolic interlude.
Companies were formed to build, improve, and maintain a particular section of roadway, and tolls were collected from users to finance the enterprise. The enterprise was usually named to indicate the locale of its roadway, often including the name of one of both of the termini. The word "turnpike" came into common use in the names of these roadways and companies, and is essentially used interchangeably with "toll road" in current terminology.
In the United States, toll roads began with the Lancaster Turnpike in the 1790s, within Pennsylvania, connecting Philadelphia and Lancaster. In the state of New York, the Great Western Turnpike was started in Albany in 1799 and eventually extended, by several alternate routes, to near what is now Syracuse, New York.
Toll roads peaked in the mid 19th century, and by the turn of the twentieth century most toll roads were taken over by state highway departments. The demise of this early toll road era was due to the rise of canals and railroads, which were more efficient (and thus cheaper) in moving freight over long distances. Roads wouldn't again be competitive with rails and barges until the first half of the 20th century when the internal combustion engine replaces draft animals as the source of motive power.
With the development, mass production, and popular embrace of the automobile, faster and higher capacity roads were needed. In the 1920s limited access highways appeared. Their main characteristics were dual roadways with access points limited to (but not always) grade-separated interchanges. Their dual roadways allowed high volumes of traffic, the need for no or few traffic lights along with relatively gentle grades and curves allowed higher speeds.
The first limited access highways were "Parkways", so called because of their often park-like landscaping and, in the metropolitan New York City area, they connected the region's system of parks. When the German autobahns built in the 1930s introduced higher design standards and speeds, road planners and road-builders in the United States started developing and building toll roads to similar high standards. The Pennsylvania Turnpike, which largely followed the path of a partially built railroad, was the first, opening in 1940.
After 1940 with the Pennsylvania Turnpike, toll roads saw a resurgence, this time to fund limited access highways. In the late 1940s and early 1950s, after World War II interrupted the evolution of the highway, the US resumed building toll roads. They were to still higher standards and one road, the New York State Thruway, had standards that became the prototype for the U.S. Interstate Highway System. Several other major toll-roads which connected with the Pennsylvania Turnpike were established before the creation of the Interstate Highway System. These were the Indiana Toll Road, Ohio Turnpike, and New Jersey Turnpike.
Interstate Highway System.
In the United States, beginning in 1956, Dwight D. Eisenhower National System of Interstate and Defense Highways, commonly called the Interstate Highway System was built. It uses 12 foot (3.65m) lanes, wide medians, a maximum of 4% grade, and full access control, though many sections don't meet these standards due to older construction or constraints. This system created a continental-sized network meant to connect every population center of 50,000 people or more.
By 1956, most limited access highways in the eastern United States were toll roads. In that year, the federal Interstate highway program was established, funding non-toll roads with 90% federal dollars and 10% state match, giving little incentive for states to expand their turnpike system. Funding rules initially restricted collections of tolls on newly funded roadways, bridges, and tunnels. In some situations, expansion or rebuilding of a toll facility using Interstate Highway Program funding resulted in the removal of existing tolls. This occurred in Virginia on Interstate 64 at the Hampton Roads Bridge-Tunnel when a second parallel roadway to the regional 1958 bridge-tunnel was completed in 1976.
Since the completion of the initial portion of the interstate highway system, regulations were changed, and portions of toll facilities have been added to the system. Some states are again looking at toll financing for new roads and maintenance, to supplement limited federal funding. In some areas, new road projects have been completed with public-private partnerships funded by tolls, such as the Pocahontas Parkway (I-895) near Richmond, Virginia.
The newest policy passed by Congress and the Obama Administration regarding highways is the Surface and Air Transportation Programs Extension Act of 2011.
Pneumatic tires.
As the horse-drawn carriage was replaced by the car and lorry or truck, and speeds increased, the need for smoother roads and less vertical displacement became more apparent, and pneumatic tires were developed to decrease the apparent roughness. Wagon and carriage wheels, made of wood, had a tire in the form of an iron strip that kept the wheel from wearing out quickly. Pneumatic tires, which had a larger footprint than iron tires, also were less likely to get bogged down in the mud on unpaved roads.
See also.
Other topics:

</doc>
<doc id="49021" url="https://en.wikipedia.org/wiki?curid=49021" title="Coen de Koning">
Coen de Koning

Coen de Koning (30 March 1879 – 29 July 1954) was the second Dutch speed skater to win a world title, which he had done in 1905. He finished second in 500 m, and won the 1500, 5000 and 10,000 m events. De Koning won the national all-around title in 1903, 1905 and 1912, and set national records in the 500 m and 10,000 m in 1905; these records stood until 1926 and 1929. De Koning also set a world record in one-hour skating, at 32,370 m in 1906, and won the Elfstedentocht in 1912 and 1917.
De Koning came from a speed skating family. His brother Jacobus "Sjaak" Petrus de Koning won the national all-around title in 1914. His son Jacobus Petrus Coenradus de Koning (born 1907) competed at the 1942 national championships, and his cousin Aad de Koning took part in the 1948 Winter Olympics. His more distant relatives on the brother's side, Truus Dijkstra and Jacques de Koning were also prominent Dutch speed skaters.

</doc>
