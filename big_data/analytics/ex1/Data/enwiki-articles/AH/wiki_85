<doc id="54695" url="https://en.wikipedia.org/wiki?curid=54695" title="RAID">
RAID

RAID (originally redundant array of inexpensive disks, now commonly redundant array of independent disks) is a data storage virtualization technology that combines multiple physical disk drive components into a single logical unit for the purposes of data redundancy, performance improvement, or both.
Data is distributed across the drives in one of several ways, referred to as RAID levels, depending on the required level of redundancy and performance. The different schemas, or data distribution layouts, are named by the word RAID followed by a number, for example RAID 0 or RAID 1. Each schema, or a RAID level, provides a different balance among the key goals: reliability, availability, performance, and capacity. RAID levels greater than RAID 0 provide protection against unrecoverable sector read errors, as well as against failures of whole physical drives.
History.
The term "RAID" was invented by David Patterson, Garth A. Gibson, and Randy Katz at the University of California, Berkeley in 1987. In their June 1988 paper ""A Case for Redundant Arrays of Inexpensive Disks (RAID)"", presented at the SIGMOD conference, they argued that the top performing mainframe disk drives of the time could be beaten on performance by an array of the inexpensive drives that had been developed for the growing personal computer market. Although failures would rise in proportion to the number of drives, by configuring for redundancy, the reliability of an array could far exceed that of any large single drive.
Although not yet using that terminology, the technologies of the five levels of RAID named in the June 1988 paper were used in various products prior to the paper's publication, including the following:
Industry RAID manufacturers later tended to interpret the acronym as standing for "redundant array of "independent" disks".
Overview.
Many RAID levels employ an error protection scheme called "parity", a widely used method in information technology to provide fault tolerance in a given set of data. Most use simple XOR, but RAID 6 uses two separate parities based respectively on addition and multiplication in a particular Galois field or Reed–Solomon error correction.
RAID can also provide data security with solid-state drives (SSDs) without the expense of an all-SSD system. For example, a fast SSD can be mirrored with a mechanical drive. For this configuration to provide a significant speed advantage an appropriate controller is needed that uses the fast SSD for all read operations. Adaptec calls this "hybrid RAID".
Standard levels.
A number of standard schemes have evolved. These are called "levels". Originally, there were five RAID levels, but many variations have evolved, notably several nested levels and many non-standard levels (mostly proprietary). RAID levels and their associated data formats are standardized by the Storage Networking Industry Association (SNIA) in the Common RAID Disk Drive Format (DDF) standard:
Nested (hybrid) RAID.
In what was originally termed "hybrid RAID", many storage controllers allow RAID levels to be nested. The elements of a "RAID" may be either individual drives or arrays themselves. Arrays are rarely nested more than one level deep.
The final array is known as the top array. When the top array is RAID 0 (such as in RAID 1+0 and RAID 5+0), most vendors omit the "+" (yielding RAID 10 and RAID 50, respectively).
Non-standard levels.
Many configurations other than the basic numbered RAID levels are possible, and many companies, organizations, and groups have created their own non-standard configurations, in many cases designed to meet the specialized needs of a small niche group. Such configurations include the following:
Implementations.
The distribution of data across multiple drives can be managed either by dedicated computer hardware or by software. A software solution may be part of the operating system, part of the firmware and drivers supplied with a standard drive controller (so-called "hardware-assisted software RAID"), or it may reside entirely within the hardware RAID controller.
Software-based.
Software RAID implementations are provided by many modern operating systems. Software RAID can be implemented as:
Some advanced file systems are designed to organize data across multiple storage devices directly, without needing the help of a third-party logical volume manager:
Many operating systems provide RAID implementations, including the following:
If a boot drive fails, the system has to be sophisticated enough to be able to boot off the remaining drive or drives. For instance, consider a computer whose disk is configured as RAID 1 (mirrored drives); if the first drive in the array fails, then a first-stage boot loader might not be sophisticated enough to attempt loading the second-stage boot loader from the second drive as a fallback. The second-stage boot loader for FreeBSD is capable of loading a kernel from such an array.
Firmware- and driver-based.
Software-implemented RAID is not always compatible with the system's boot process, and it is generally impractical for desktop versions of Windows. However, hardware RAID controllers are expensive and proprietary. To fill this gap, inexpensive "RAID controllers" were introduced that do not contain a dedicated RAID controller chip, but simply a standard drive controller chip with proprietary firmware and drivers. During early bootup, the RAID is implemented by the firmware and, once the operating system has been more completely loaded, the drivers take over control. Consequently, such controllers may not work when driver support is not available for the host operating system. An example is Intel Matrix RAID, implemented on many consumer-level motherboards.
Because some minimal hardware support is involved, this implementation approach is also called "hardware-assisted software RAID", "hybrid model" RAID, or even "fake RAID". If RAID 5 is supported, the hardware may provide a hardware XOR accelerator. An advantage of this model over the pure software RAID is that—if using a redundancy mode—the boot drive is protected from failure (due to the firmware) during the boot process even before the operating systems drivers take over.
Integrity.
Data scrubbing (referred to in some environments as "patrol read") involves periodic reading and checking by the RAID controller of all the blocks in an array, including those not otherwise accessed. This detects bad blocks before use. Data scrubbing checks for bad blocks on each storage device in an array, but also uses the redundancy of the array to recover bad blocks on a single drive and to reassign the recovered data to spare blocks elsewhere on the drive.
Frequently, a RAID controller is configured to "drop" a component drive (that is, to assume a component drive has failed) if the drive has been unresponsive for eight seconds or so; this might cause the array controller to drop a good drive because that drive has not been given enough time to complete its internal error recovery procedure. Consequently, using RAID for consumer-marketed drives can be risky, and so-called "enterprise class" drives limit this error recovery time to reduce risk. Western Digital's desktop drives used to have a specific fix. A utility called WDTLER.exe limited a drive's error recovery time. The utility enabled TLER (time limited error recovery), which limits the error recovery time to seven seconds. Around September 2009, Western Digital disabled this feature in their desktop drives (e.g. the Caviar Black line), making such drives unsuitable for use in RAID configurations. However, Western Digital enterprise class drives are shipped from the factory with TLER enabled. Similar technologies are used by Seagate, Samsung, and Hitachi. Of course, for non-RAID usage, an enterprise class drive with a short error recovery timeout that cannot be changed is therefore less suitable than a desktop drive. In late 2010, the Smartmontools program began supporting the configuration of ATA Error Recovery Control, allowing the tool to configure many desktop class hard drives for use in RAID setups.
While RAID may protect against physical drive failure, the data is still exposed to operator, software, hardware, and virus destruction. Many studies cite operator fault as the most common source of malfunction, such as a server operator replacing the incorrect drive in a faulty RAID, and disabling the system (even temporarily) in the process.
An array can be overwhelmed by catastrophic failure that exceeds its recovery capacity and, of course, the entire array is at risk of physical damage by fire, natural disaster, and human forces, while backups can be stored off site. An array is also vulnerable to controller failure because it is not always possible to migrate it to a new, different controller without data loss.
Weaknesses.
Correlated failures.
In practice, the drives are often the same age (with similar wear) and subject to the same environment. Since many drive failures are due to mechanical issues (which are more likely on older drives), this violates the assumptions of independent, identical rate of failure amongst drives; failures are in fact statistically correlated. In practice, the chances for a second failure before the first has been recovered (causing data loss) are higher than the chances for random failures. In a study of about 100,000 drives, the probability of two drives in the same cluster failing within one hour was four times larger than predicted by the exponential statistical distribution—which characterizes processes in which events occur continuously and independently at a constant average rate. The probability of two failures in the same 10-hour period was twice as large as predicted by an exponential distribution.
Unrecoverable read errors during rebuild.
"Unrecoverable read errors" (URE) present as sector read failures, also known as "latent sector errors" (LSE). The associated media assessment measure, "unrecoverable bit error" (UBE) rate, is typically guaranteed to be less than one bit in 1015 for enterprise-class drives (SCSI, FC, SAS or SATA), and less than one bit in 1014 for desktop-class drives (IDE/ATA/PATA or SATA). Rebuilding a RAID set after a drive failure fails if such an error occurs on the remaining drives, and increasing drive capacities and large RAID 5 instances have led to the maximum error rates being insufficient to guarantee a successful recovery. When rebuilding, parity-based schemes such as RAID 5 are particularly prone to the effects of UREs as they affect not only the sector where they occur, but also reconstructed blocks using that sector for parity computation. Thus, an URE during a RAID 5 rebuild typically leads to a complete rebuild failure.
Double-protection parity-based schemes, such as RAID 6, attempt to address this issue by providing redundancy that allows double-drive failures; as a downside, such schemes suffer from elevated write penalty. Schemes that duplicate (mirror) data in a drive-to-drive manner, such as RAID 1 and RAID 10, have a lower risk from UREs than those using parity computation or mirroring between striped sets. Data scrubbing, as a background process, can be used to detect and recover from UREs, effectively reducing the risk of them happening during RAID rebuilds and causing double-drive failures. The recovery of UREs involves remapping of affected underlying disk sectors, utilizing the drive's sector remapping pool; in case of UREs detected during background scrubbing, data redundancy provided by a fully operational RAID set allows the missing data to be reconstructed and rewritten to a remapped sector.
Increasing rebuild time and failure probability.
Drive capacity has grown at a much faster rate than transfer speed, and error rates have only fallen a little in comparison. Therefore, larger-capacity drives may take hours if not days to rebuild, during which time other drives may fail. The rebuild time is also limited if the entire array is still in operation at reduced capacity. Given an array with only one redundant drive (which applies to RAID levels 3, 4 and 5, and to "classic" two-drive RAID 1), a second failure would cause complete failure of the array. Even though individual drives' mean time between failure (MTBF) have increased over time, this increase has not kept pace with the increased storage capacity of the drives. The time to rebuild the array after a single drive failure, as well as the chance of a second failure during a rebuild, have increased over time.
Some commentators have declared that RAID 6 is only a "band aid" in this respect, because it only kicks the problem a little further down the road. However, according to a 2006 NetApp study of Berriman et al., the chance of failure decreases by a factor of about 3,800 (relative to RAID 5) for a proper implementation of RAID 6, even when using commodity drives. Nevertheless, if the currently observed technology trends remain unchanged, in 2019 a RAID 6 array will have the same chance of failure as its RAID 5 counterpart had in 2010.
Mirroring schemes such as RAID 10 have a bounded recovery time as they require the copy of a single failed drive, compared with parity schemes such as RAID 6, which require the copy of all blocks of the drives in an array set. Triple parity schemes, or triple mirroring, have been suggested as one approach to improve resilience to an additional drive failure during this large rebuild time.
Atomicity: including parity inconsistency due to system crashes.
A system crash or other interruption of a write operation can result in states where the parity is inconsistent with the data due to non-atomicity of the write process, such that the parity cannot be used for recovery in the case of a disk failure (the so-called RAID 5 write hole). The RAID write hole is a known data corruption issue in older and low-end RAIDs, caused by interrupted destaging of writes to disk.
This is a little understood and rarely mentioned failure mode for redundant storage systems that do not utilize transactional features. Database researcher Jim Gray wrote "Update in Place is a Poison Apple" during the early days of relational database commercialization.
Write-cache reliability.
There are concerns about write-cache reliability, specifically regarding devices equipped with a write-back cache, which is a caching system that reports the data as written as soon as it is written to cache, as opposed to being written the non-volatile medium. If the system experiences a power loss or other major failure, the data may be irrevocably lost from the cache before reaching the non-volatile storage.

</doc>
<doc id="54702" url="https://en.wikipedia.org/wiki?curid=54702" title="Mani">
Mani

Mani may refer to: 

</doc>
<doc id="54703" url="https://en.wikipedia.org/wiki?curid=54703" title="EXPSPACE">
EXPSPACE

In complexity theory, EXPSPACE is the set of all decision problems solvable by a deterministic Turing machine in O(2"p"("n")) space, where "p"("n") is a polynomial function of "n". (Some authors restrict "p"("n") to be a linear function, but most authors instead call the resulting class "ESPACE".) If we use a nondeterministic machine instead, we get the class "NEXPSPACE", which is equal to "EXPSPACE" by Savitch's theorem.
In terms of "DSPACE" and "NSPACE",
A decision problem is "EXPSPACE-complete" if it is in "EXPSPACE", and every problem in "EXPSPACE" has a polynomial-time many-one reduction to it. In other words, there is a polynomial-time algorithm that transforms instances of one to instances of the other with the same answer. "EXPSPACE-complete" problems might be thought of as the hardest problems in "EXPSPACE".
"EXPSPACE" is a strict superset of "PSPACE", "NP", and "P" and is believed to be a strict superset of "EXPTIME".
An example of an "EXPSPACE-complete" problem is the problem of recognizing whether two regular expressions represent different languages, where the expressions are limited to four operators: union, concatenation, the Kleene star (zero or more copies of an expression), and squaring (two copies of an expression).
If the Kleene star is left out, then that problem becomes "NEXPTIME-complete", which is like "EXPTIME-complete", except it is defined in terms of non-deterministic Turing machines rather than deterministic.
It has also been shown by L. Berman in 1980 that the problem of verifying/falsifying any first-order statement about real numbers that involves only addition and comparison (but no multiplication) is in "EXPSPACE".

</doc>
<doc id="54705" url="https://en.wikipedia.org/wiki?curid=54705" title="Willie Rushton">
Willie Rushton

William George Rushton (18 August 1937 – 11 December 1996) was an English cartoonist, satirist, comedian, actor and performer who co-founded the satirical magazine "Private Eye".
Early life.
Rushton was born 18 August 1937 in 3 Wilbraham Place, Chelsea, London, the only son of John and Veronica Rushton, and attended Shrewsbury School, where he was not academically successful but met his future "Private Eye" colleagues Richard Ingrams, Paul Foot and Christopher Booker. He also contributed to the satirical magazine "The Wallopian", (a play on the school magazine name "The Salopian") mocking school spirit, traditions and the masters.
After school Rushton had to do his two years of national service in the army where he failed officer selection. He later commented "The Army is, God bless it, one of the funniest institutions on earth and also a sort of microcosm of the world. It's split almost perfectly into our class system. Through serving in the ranks I discovered the basic wit of my fellow man – whom basically, to tell the truth, I'd never met before." On leaving the army, he worked in a solicitor's office for a short period.
"Private Eye" and the satire boom.
He was still in contact with his Shrewsbury friends, who had added John Wells to their number and were now running their own humour magazines at Oxford, "Parsons Pleasure" and "Mesopotamia", to which Rushton made many contributions during his frequent visits. A cartoon of a giraffe in a bar saying "The high balls are on me" was not met with approval by everyone in the university administrative quarters. It was Rushton who suggested that "Mesopotamia" could continue after they left university. During his time as a clerk he had been sending his cartoons out to "Punch" but none had been accepted. After being knocked over by a bus he quit his clerking, determined not to waste another day.
After almost but not quite being accepted by "Tribune" (a Labour-supporting journal edited by Michael Foot, Paul's uncle), Rushton found a place at the "Liberal News", which was also employing Christopher Booker as a journalist. From June 1960 until March 1961 he contributed a weekly strip, "Brimstone Belcher", following the exploits of the titular journo (a fore-runner of "Private Eye"'s Lunchtime O'Booze), from bizarre skulduggery in the British colonies (where the squaddies holding back the politicised rabble bear a strong resemblance to privates Rushton and Ingrams), travelogues through the US, and the hazards of by-electioneering as the independent candidate for the constituency of Gumboot North. After the strip folded, Rushton still contributed a weekly political cartoon to the "Liberal News" until mid-1962.
The Salopians finally found a financier and the first issue of "Private Eye" was published on 25 October 1961. Rushton put it together in his bedroom in Scarsdale Villas using Letraset and cow-gumming illustrations onto cards which were taken away to be photo-lithographed. He also contributed all the illustrations and the mast-head figure of Little Nitty (who still appears on the cover, a blended caricature of John Wells and the "Daily Express" standard-head). One critic described the original lay-out of the magazine as owing much to "Neo-Brechtian Nihilism", although Rushton thought it resembled a betting shop floor. One feature in the early issues was the "Aesop Revisited", a full-page comic strip which let him work in a wealth of puns and background jokes. With "Private Eye" riding the satire boom, Peter Cook soon took an interest and contributed two serials recounting the bizarre adventures of Sir Basil Nardly-Stoads and the Rhandi Phurr, both of which were illustrated by Rushton, as was "Mrs Wilson's Diary". In the early days the team also worked on two books, "Private Eye on London" and "Private Eye's Romantic England" that make heavy use of his cartooning talents. One of the first "Private Eye"-published books was Rushton's first collection of cartoons, "Willie Rushton's Dirty Weekend Book" (banned in Ireland).
Reuniting with his Salopian chums had also reawakened Rushton's taste for acting. After they had finished university he had accompanied his friends in a well-received revue at the Edinburgh Fringe (Richard Burton even appeared one night in their parody of "Luther"). In 1961, Richard Ingrams directed a production of Spike Milligan's surreal post-nuclear apocalypse farce "The Bed-Sitting Room", in which Rushton was hailed by Kenneth Tynan as "brilliant". But it was a cabaret at the Room at the Top, a chicken-in-the-basket nightclub at the top a department store in Ilford, that really launched his career. Rushton recalled meeting the Kray twins in the audience one night and that fellow performer Barbara Windsor "wouldn't come out for a drink that night". The revue also starred John Wells. Rushton’s impersonation of Prime Minister Harold Macmillan caught the attention of Ned Sherrin, a young BBC producer searching for talent to appear in a forthcoming TV satire series.
"That Was the Week That Was" (aka "TW3") ran from November 1962 until December 1963. It drew audiences of up to 13 million, making stars of its cast, particularly David Frost. Rushton became known for his impersonation of the Prime Minister, a daring novelty in those respectful days. "It's the only impersonation that people have ever actually recognised – so I'm very grateful to the old bugger ... But then I had voted for him, so he owed me something." Rushton also appeared on the original flexi-discs of skits, squibs and invective that "Private Eye" gave away, having success with two self-penned songs: "Neasden" ("you won't be sorry that you breezed in ... where the rissoles are deep-freezed-en") and the "Bum Song" ("if you’re feeling glum / stick a finger up your bum / and the world is a happier place"). He also wrote songs for "TW3", many of which were revisited on later solo albums like "Now in Bottles" and "The Complete Works".
In the autumn of 1963 a health scare led Macmillan to resign and Sir Alec Douglas-Home became Prime Minister. It was necessary that Douglas-Home resign his peerage to find a safe Parliamentary seat. The "Private Eye" team were so disgusted by the Conservative Party’s machinations that they decided to run their own protest candidate in the Kinross and West Perthshire by-election. Since he was the most famous member of the team Rushton was the obvious choice to run. Rushton garnered much attention from journalists, since he ran under the slogan "Death to the Tories". He polled only 45 votes, having at the last minute advised his supporters to vote Liberal, the Conservatives' only credible challenger. Douglas-Home won.
Films, TV and radio.
When "TW3" was cancelled in anticipation of the 1963 election, Rushton and some of the cast as well as some of the members of the Cambridge University revue "Cambridge Circus" (including future Goodies Tim Brooke-Taylor and Bill Oddie) went on tour in America as "David Frost Presents TW3". Rushton and Barry Fantoni (another "Private Eye" contributor) entered a painting titled "Nude Reclining", a satirical portrait of three establishment types, for the 1963 Royal Academy Summer Exhibition under the name of Stuart Harris, which excited much controversy. He also began a career as a character actor for films in 1963. In late 1964 Rushton was involved as one of the hosts in the early episodes of another satirical programme, "Not So Much A Programme", but drifted away as it became the vehicle that launched David Frost as a chat show host. In 1964 he appeared as Richard Burbage in Sherrin and Caryl Brahms' musical of "No Bed for Bacon", while his early stature as a personality was confirmed by a cartoon advert he devised for the Brewer’s Society proclaiming the charms of the local pub. Rushton did his own host duties for "New Stars and Garters", a variety entertainment show in 1965, where he first met Arlene Dorgan. He also appeared as a guest in programmes including "Not Only... But Also" with Peter Cook and Dudley Moore.
During the later 1960s Rushton spent much of his time in Australia, following Dorgan back to her homeland. He married her in 1968. He also had several series of his own on Australian television, "Don’t Adjust Your Set – The Programme is at Fault" and "From Rushton with Love". He said of Australia, "They've got their priorities right, they're dedicated to lying in the sun, knocking back ice-cold beer". During this period he found time to model for "She" magazine and also appear in a 1967 stage production of "Treasure Island" as Squire Trelawney, alongside Spike Milligan and Barry Humphries, at the Mermaid Theatre in London. It was on one of his return visits to the UK in 1968 that he also brought back the late Tony Hancock's ashes to the UK in an Air France bag – "My session with the Customs was a Hancock Half Hour in itself."
He appeared in cameo roles in films, including "Those Magnificent Men in Their Flying Machines" (1965), "Monte Carlo or Bust" (1969), "The Best House in London" (1969) and "The Adventures of Barry McKenzie" (1972). He played Tim Brooke-Taylor's gay husband in Sharon Tate’s last film before her murder, "The Thirteen Chairs" (1969), and Tobias Cromwell in "Flight of the Doves" (1971), as well as appearing in sex comedies such as "Keep It Up Downstairs" (1976), "Adventures of a Private Eye" (1977) and "Adventures of a Plumber's Mate" (1978). His final film appearance was as Big Teddy in "Consuming Passions" in 1988. As a TV actor in the 1970s he appeared in episodes of popular programmes as different as "The Persuaders!", "Colditz" (episode: "The Guests" – Major Trumpington in a kilt) and "Up Pompeii!" as the narrator Plautus. He was Dr Watson to John Cleese's Sherlock Holmes in N. F. Simpson's surreal comedy "Elementary, My Dear Watson". In 1975 and 1976 he appeared in well-received pantomimes of "Gulliver’s Travels"; in 1981 in Eric Idle’s "Pass the Butler"; and in 1988 as Peter Tinniswood’s irascible Brigadier in "Tales from a Long Room". Rushton also wrote two musicals:
His last major solo TV project was "Rushton's Illustrated" (1981). By now he was an established guest on quiz shows and celebrity panel games: "Celebrity Squares", "Blankety Blank", "Countdown" and "Through the Keyhole". When asked why he appeared on these "ludicrous programmes", his answer was simple: "Because I meet everybody there".
For 22 years until his death, he was a panellist in the long-running BBC Radio 4 panel comedy game show "I'm Sorry I Haven't a Clue", which he joined as a regular team member in the third series in 1974. In its later years, the show's wealth of silliness, smut and punning was drawing audiences of up to a thousand people for its recordings. No permanent replacement has been found for Rushton; instead his seat has been filled by a series of guests. Producer Jon Naismith has said that "As befits such an irreplaceable character, he has never been formally replaced." In 1990 he teamed up with his co-panellist Barry Cryer in their own show "Two old Farts in the Night", performing to full audiences at the Edinburgh festival, the Royal Albert Hall and the Festival Hall, touring the country irregularly until Rushton's death.
He played a recurring character as a policeman in Southern Television's 1970-73 children's show Little Big Time with Freddie Garrity; his policeman's helmet bore a blue flashing light. His manner and voice meant Rushton was in constant demand for adverts, voice-overs and presenting jobs. In the mid-1970s, reading "Winnie the Pooh" for the BBC's "Jackanory", he won over a whole new audience of young children. He also provided all the voices for the claymation animated series "The Trap Door" in the late 1980s. He was a popular choice for narrating audio books, especially those for children. In particular he recorded 18 of the books by the Rev. W. Awdry for "The Railway Stories" series. He also recorded adaptations of Asterix books and "Alice in Wonderland", and provided the voice of the King in the early animated Muzzy films. In the early 1980s he wrote and illustrated a series of children’s books about "The Incredible Cottage", and provided illustrations for many children‘s books.
Rushton had not been involved in "Private Eye" since the latter part of the 1960s, other than a brief stint illustrating "Mrs Wilson's Diary" when the Labour Party came back into power in the mid-1970s. He returned to "Private Eye" in 1978 to take over the task of illustrating "Auberon Waugh's Diary". The cartoons perfectly complemented Auberon Waugh's scabrous and surreal flights of invective, and when Waugh moved his column to "The Daily Telegraph" as the "Way of the World" in the mid-1980s, Rushton followed. The Victoria and Albert Museum, recognising his accomplishments, commissioned 24 large colour illustrations that were collected as "Willie Rushton's Great Moments of History". (Rushton had previous experience with the V&A when he had pulled a prank on the institution by labelling an electric plug socket in one of the galleries: "Plug hole designed by Hans Plug (b. 1908)", which remained for a full year – to the great annoyance of a cleaner who had to use a hefty extension lead for 12 months so as not to damage the exhibit.) This large scale excursion into the use of colour was good practice for the monthly colour covers he created for the "Literary Review" when Waugh became its editor in the late 1980s. Rushton drew these covers along with the fortnightly caricatures for "Private Eye'"s literary review page until his death.
Rushton had always been conscious of his weight, listing his recreations in "Who's Who" as "gaining weight, losing weight and parking", and in 1973 he had been the host of a slimming programme, "Don't Just Sit There". His first major health scare had been the onset of diabetes (the cause of his father's death in 1958). Having to give up beer, Rushton became, according to Ingrams, "quite grumpy as a result, but his grumpiness had an admirable and jaunty quality to it." A sudden loss of three stone had prevented him from playing in Prince Rainier's XI at Monte Carlo, Monaco. Rushton was always passionate about cricket. His father had sent him for coaching at Lord's before he went to Shrewsbury. His cricket and general knowledge were called upon in his role as a regular team captain on BBC Radio 4's quiz show "Trivia Test Match" with Tim Rice and Brian Johnston, which ran from 1986 to 1993. Rushton was always an enthusiastic cricketer, playing in the Lord's Taverners, a charity celebrity cricket team.
In 1989 he performed in "The Secret Policeman's Biggest Ball". His act consisted of singing "Top Hat, White Tie and Tails" and acting out the lyrics, which left him standing in top hat, white tie, and tails – but no trousers. In his later years he was part of an exhibition at the National Portrait Gallery, and even had the "privilege" of being invited back to Shrewsbury to open a new wing at his old school, the entirety of his speech being "The bugger’s open."
He went into the Cromwell Hospital, Kensington, for heart surgery, and died there from complications on 11 December 1996. According to Rushton's widow, his last words included a message to his long-time friend and comedy partner, Barry Cryer: "Tell Bazza he's too old to do pantomime."
Memorials.
He is honoured by a Comic Heritage blue plaque at Mornington Crescent tube station, a reference to the game Mornington Crescent on "I'm Sorry I Haven't A Clue".
BBC7 showcased his contribution to "I'm Sorry I Haven't A Clue" – in the week of the 10th anniversary of his death – by rebroadcasting five episodes of the show, one on each weekday night (11–15 December 2006). The broadcasts chosen included the last shows he recorded for the programme.
According to the autobiography of Nicholas Parsons, Rushton's ashes were buried by the boundary line at The Oval Cricket Ground.

</doc>
<doc id="54707" url="https://en.wikipedia.org/wiki?curid=54707" title="Kru languages">
Kru languages

The Kru languages belong to the Niger–Congo language family and are spoken by the Kru people from the southeast of Liberia to the east of Ivory Coast.
The term "Kru" is of unknown origin. According to Westermann (1952) it was used by Europeans to denote a number of tribes speaking related dialects. Marchese (1989) notes the fact that many of these peoples were recruited as “crew” by European seafarers; “the homonymy with crew is obvious, and is at least one source of the confusion among Europeans that there was a Kru/crew tribe” 
Andrew Dalby noted the historical importance of the Kru languages for their position at the crossroads of African-European interaction. He wrote that “Kru and associated languages were among the first to be encountered by European voyagers on what was then known as the Pepper Coast, a center of the production and export of Guinea and melegueta pepper; a once staple African seaborne trade”. The Kru languages are known for some of the most complex tone systems in Africa, rivaled perhaps only by the Omotic languages.
Current status.
Recent documentation has noted “Kru societies can now be found along the coast of Monrovia, Liberia to Bandama River in Cote d'Ivoire” “Villages maintain their ties based on presumed common descent, reinforced by ceremonial exchanges and gifts”. The Kru people and their languages, although now many speak English (in Liberia) or French (in Côte d'Ivoire) as a second language, are said to be “dominant in the southwest region where the forest zone reaches the coastal lagoons”. The Kru people rely on the forest for farming, supplemented by hunting for their livelihood. In 2010, Kru and associated languages were spoken by 95 percent of the approximately 3.5 million people in Liberia.
Subgroups and associated languages.
The Kru languages include many subgroups such as Kuwaa, Grebo, Belle, Belleh, Kwaa and many others. According to Breitbonde, categorization of communities based on cultural distinctiveness, historical or ethnic identity, and socio-political autonomy “may have brought about the large number of distinct Kru dialects; "Although the natives were in many respects similar in type and tribe, every village was an independent state; there was also very little intercommunication". Breitbonde notes the Kru people were categorized based on their cultural distinctiveness, separate historical or ethnic identities, and social and political autonomy. This is the possible reason for so many subgroups of the Kru language. As noted by Fisiak, there is very little documentation on the Kru and associated languages.
The Marchese (1989) classification of Kru languages is as follows. Many of these languages are dialect clusters and are sometimes considered more than a single language.
"Ethnologue" adds Neyo, which may be closest to Dida or Godie.

</doc>
<doc id="54708" url="https://en.wikipedia.org/wiki?curid=54708" title="Nyabwa language">
Nyabwa language

The Nyabwa (or Nyaboa) language is a Kru language spoken in Ivory Coast. It is part of the Wee dialect continuum.

</doc>
<doc id="54711" url="https://en.wikipedia.org/wiki?curid=54711" title="GPG">
GPG

GPG may refer to:

</doc>
<doc id="54712" url="https://en.wikipedia.org/wiki?curid=54712" title="Abdominal obesity">
Abdominal obesity

Abdominal obesity, also known as central obesity, is when excessive abdominal fat around the stomach and abdomen has built up to the extent that it is likely to have a negative impact on health. There is a strong correlation between central obesity and cardiovascular disease. Abdominal obesity is not confined only to the elderly and obese subjects. Abdominal obesity has been linked to Alzheimer's disease as well as other metabolic and vascular diseases.
Visceral and central abdominal fat and waist circumference show a strong association with type 2 diabetes.
Visceral fat, also known as organ fat or "intra-abdominal fat", is located inside the peritoneal cavity, packed in between internal organs and torso, as opposed to subcutaneous fat‚ which is found underneath the skin, and intramuscular fat‚ which is found interspersed in skeletal muscle. Visceral fat is composed of several adipose depots including mesenteric, epididymal white adipose tissue (EWAT) and perirenal fat. An excess of visceral fat is known as central obesity, the "pot belly" or "beer belly" effect, in which the abdomen protrudes excessively. This body type is also known as "apple shaped‚" as opposed to "pear shaped‚" in which fat is deposited on the hips and buttocks.
Researchers first started to focus on abdominal obesity in the 1980s when they realized that it had an important connection to cardiovascular disease, diabetes, and dyslipidemia. Abdominal obesity was more closely related with metabolic dysfunctions connected with cardiovascular disease than was general obesity. In the late 1980s and early 1990s insightful and powerful imaging techniques were discovered that would further help advance the understanding of the health risks associated with body fat accumulation. Techniques such as computed tomography and magnetic resonance imaging made it possible to categorize mass of adipose tissue located at the abdominal level into intra-abdominal fat and subcutaneous fat.
Health risks.
Central obesity is associated with a statistically higher risk of heart disease, hypertension, insulin resistance, and Diabetes Mellitus Type 2 (see below). With an increase in the waist to hip ratio and overall waist circumference the risk of death increases as well. Metabolic syndrome is associated with abdominal obesity, blood lipid disorders, inflammation, insulin resistance, full-blown diabetes, and increased risk of developing cardiovascular disease. It is now generally believed that intra-abdominal fat is the depot that conveys the biggest health risk.
Central obesity can be a feature of lipodystrophies, a group of diseases that is either inherited, or due to secondary causes (often protease inhibitors, a group of medications against AIDS). Central obesity is a symptom of Cushing's syndrome and is also common in patients with polycystic ovary syndrome (PCOS). Central obesity is associated with glucose intolerance and dyslipidemia. Once dyslipidemia becomes a severe problem, an individual's abdominal cavity would generate elevated free fatty acid flux to the liver. The effect of abdominal adiposity occurs not just in those who are obese, but also affects people who are non-obese and it also contributes to insulin sensitivity.
Diabetes.
There are numerous theories as to the exact cause and mechanism in Type 2 Diabetes. Central obesity is known to predispose individuals for insulin resistance. Abdominal fat is especially active hormonally, secreting a group of hormones called adipokines that may possibly impair glucose tolerance. But adiponectin which is found in lower concentration in obese and diabetic individuals has shown to be beneficial and protective in Type 2 diabetes mellitus.
Insulin resistance is a major feature of Diabetes Mellitus Type 2 (T2DM), and central obesity is correlated with both insulin resistance and T2DM itself. Increased adiposity (obesity) raises serum resistin levels, which in turn directly correlate to insulin resistance. Studies have also confirmed a direct correlation between resistin levels and T2DM. And it is waistline adipose tissue (central obesity) which seems to be the foremost type of fat deposits contributing to rising levels of serum resistin. Conversely, serum resistin levels have been found to "decline" with decreased adiposity following medical treatment.
Asthma.
Developing asthma due to abdominal obesity is also a main concern. As a result of breathing at low lung volume, the muscles are tighter and the airway is narrower. It is commonly seen that people who are obese breathe quickly and often, while inhaling small volumes of air. People with obesity are also more likely to be hospitalized for asthma. A study has stated that 75% of patients treated for asthma in the emergency room were either overweight or obese.
Alzheimer's disease.
Based on studies, it is evident that obesity has a strong association with vascular and metabolic disease which could potentially be linked to Alzheimer's disease. Recent studies have also shown an association between mid-life obesity and dementia, but the relationship between later life obesity and dementia is less clear. A study by Debette et al. (2010) examining over 700 adults found evidence to suggest higher volumes of visceral fat, regardless of overall weight, were associated with smaller brain volumes and increased risk of dementia. Alzheimer's disease and abdominal obesity has a strong correlation and with metabolic factors added in, the risk of developing Alzheimer's disease was even higher. Based on logistic regression analyses, it was found that obesity was associated with an almost 10-fold increase risk of Alzheimer's disease.
Causes.
The currently prevalent belief is that the immediate cause of obesity is net energy imbalance—the organism consumes more usable calories than it expends, wastes‚ or discards through elimination. Some studies indicate that visceral adiposity, together with lipid dysregulation and decreased insulin sensitivity, is related to the excessive consumption of fructose. Greater meat consumption has also been positively associated with greater weight gain, and specifically abdominal obesity, even when accounting for calories. Other environmental factors, such as maternal smoking, estrogenic compounds in the diet‚ and endocrine-disrupting chemicals may be important also. Obesity plays an important role in the impairment of lipid and carbohydrate metabolism shown in high-carbohydrate diets. It has also been shown that quality protein intake during a 24-hour period and the number of times the essential amino acid threshold of approximately 10 g has been achieved is inversely related to the percentage of central abdominal fat. Quality protein uptake is defined as the ratio of essential amino acids to daily dietary protein.
Visceral fat cells will release their metabolic by-products in the portal circulation, where the blood leads straight to the liver. Thus, the excess of triglycerides and fatty acids created by the visceral fat cells will go into the liver and accumulate there. In the liver, most of it will be stored as fat. This concept is known as 'lipotoxicity'.
Hypercortisolism, such as in Cushing's syndrome, also leads to central obesity. Many prescription drugs, such as dexamethasone and other steroids, can also have side effects resulting in central obesity, especially in the presence of elevated insulin levels.
The prevalence of abdominal obesity is increasing in western populations, possibly due to a combination of low physical activity and high-calorie diets, and also in developing countries, where it is associated with the urbanization of populations.
Waist measurement is more prone to errors than measuring height and weight. It is recommended to use both standards. BMI will illustrate the best estimate of your total body fatness, while waist measurement gives an estimate of visceral fat and risk of obesity-related disease.
Alcohol consumption.
A study has shown that alcohol consumption is directly associated with waist circumference and with a higher risk of abdominal obesity in men, but not in women, in the present population. Excluding energy under-reporters slightly attenuated these associations. After controlling for energy under-reporting, it was observed that increasing alcohol consumption significantly increased the risk of exceeding recommended energy intakes in male participants – but not in the small number of female participants (2.13%) with elevated alcohol consumption, even after establishing a lower number of drinks per day to characterize women as consuming a high quantity of alcohol. Further study is needed to determine whether a significant relationship between alcohol consumption and abdominal obesity exists among women who consume higher amounts of alcohol.
[http://redheracles.net/media/upload/research/pdf/17885722.pdf]
Diagnosis.
There are various ways of measuring abdominal obesity including:
In those with a BMI under 35, intra-abdominal body fat is related to negative health outcomes independent of total body fat. Intra-abdominal or visceral fat has a particularly strong correlation with cardiovascular disease.
Men are considered to be at high risk from abdominal obesity if their waist measurements are or higher, while women are considered to be at high risk if their waist measurements are > or higher. BMI and waist measurements are well recognized ways to characterize obesity. However, waist measurements are not as accurate as BMI measurements. For this reason, it is recommended to use both methods of measurements.
While central obesity can be obvious just by looking at the naked body (see the picture), the severity of central obesity is determined by taking waist and hip measurements. The absolute waist circumference in men and in women) and the waist-hip ratio (>0.9 for men and >0.85 for women) are both used as measures of central obesity. A differential diagnosis includes distinguishing central obesity from ascites and intestinal bloating. In the cohort of 15,000 people participating in the National Health and Nutrition Examination Survey (NHANES III), waist circumference explained obesity-related health risk better than the body mass index (or BMI) when metabolic syndrome was taken as an outcome measure and this difference was statistically significant. In other words, excessive waist circumference appears to be more of a risk factor for metabolic syndrome than BMI. Another measure of central obesity which has shown superiority to BMI in predicting cardiovascular disease risk is the Index of Central Obesity (waist-to-height ratio - WHtR), where a ratio of >=0.5 (i.e. a waist circumference at least half of the individual's height) is predictive of increased risk.
Another diagnosis of obesity is the analysis of intra-abdominal fat having the most risk to one's personal health. The increased amount of fat in this region relates to the higher levels of plasma lipid and lipoproteins as per studies mentioned by Eric Poehlman (1998) review.
An increasing acceptance of the importance of central obesity within the medical profession as an indicator of health risk has led to new developments in obesity diagnosis such as the Body Volume Index, which measures central obesity by measuring a person's body shape and their weight distribution. The effect of abdominal adiposity occurs not just in those who are obese, but also affects people who are non-obese and it also contributes to insulin sensitivity
Index of central obesity.
Index of Central Obesity (ICO) is the ratio of waist circumference and height first proposed by a Parikh "et al." in 2007 as a better substitute to the widely used waist circumference in defining metabolic syndrome. The National Cholesterol Education Program Adult Treatment Panel III suggested cut off of and for males and females as a marker of central obesity. The same was used in defining metabolic syndrome. Misra et al. suggested that these cutoffs are not applicable among Indians and the cutoffs be lowered to and for males and females. Various race specific cutoffs were suggested by different groups. The International Diabetes Federation defined central obesity based on these various race and gender specific cutoffs. The other limitation of waist circumference is that it can not be applied in children.
Parikh et al. looked at the average heights of various races and suggested that by using ICO various race- and gender-specific cutoffs of waist circumference can be discarded. An ICO cutoff of 0.5 was suggested as a criterion to define central obesity. Parikh "et al." further tested a modified definition of metabolic syndrome in which waist circumference was replaced with ICO in the National Health and Nutrition Examination Survey (NHANES) database and found the modified definition to be more specific and sensitive.
This parameter has been used in the study of metabolic syndrome and cardiovascular disease.
Body volume index.
BVI is based upon the principle that excess abdominal weight, measured by part volume as a percentage of total volume, constitutes a greater health risk. Recent validation has concluded that total and regional body volume estimates correlate positively and significantly with biomarkers of cardiovascular risk and BVI calculations correlate significantly with all biomarkers of cardio-vascular risk.
Ghroubi et al. (2007) examined whether abdominal circumference is a more reliable indicator than BMI of the presence of knee osteoarthritis in obese patients. They found that it actually appears to be a factor linked with the presence of knee pain as well as osteoarthritis in obese study subjects. Ghroubi et al. (2007) concluded that a high abdominal circumference is associated with great functional repercussion.
Sex differences.
50% of men and 70% of women in the United States between the ages of 50 and 79 years now exceed the waist circumference threshold for central obesity.
When comparing the body fat of men and women it is seen that men have close to twice the visceral fat as that of pre-menopausal women.
Central obesity is positively associated with coronary heart disease risk in women and men. It has been hypothesized that the sex differences in fat distribution may explain the sex difference in coronary heart disease risk.
There are sex-dependent differences in regional fat distribution. In women, estrogen is believed to cause fat to be stored in the buttocks, thighs, and hips. When women reach menopause and the estrogen produced by ovaries declines, fat migrates from their buttocks, hips‚ and thighs to their belly.
Males are more susceptible to upper-body fat accumulation, most likely in the belly, due to sex hormone differences.
Abdominal obesity in males is correlated with comparatively low testosterone levels. Testosterone administration significantly increased thigh muscle area, reduced subcutaneous fat deposition at all levels measured, but slightly increased the visceral fat area.
Even with the differences, at any given level of central obesity measured as waist circumference or waists to hip ratio, coronary artery disease rates are identical in men and women.
Prevention and treatments.
A permanent routine of exercise, eating healthier‚ and, during periods of overweight, consuming the same number or fewer calories than used will prevent and help fight obesity. A single pound of fat (0.454 kg) yields approximately 3500 calories (14644kJ) of energy, and weight loss is achieved by reducing energy intake. Adjunctive therapies which may be prescribed by a physician are orlistat or sibutramine, although the latter has been associated with increased cardiovascular events and strokes and has been withdrawn from the market in the United States, the UK, the EU, Australia, Canada, Hong Kong, Thailand, Egypt and Mexico.
A 2006 study published in the International Journal of Sport Nutrition and Exercise Metabolism, suggests that combining cardiovascular (aerobic) exercise with resistance training is more effective than cardiovascular training alone in getting rid of abdominal fat. An additional benefit to exercising is that it reduces stress and insulin levels, which reduces the presence of cortisol, a hormone that leads to more belly fat deposits.
Self-motivation by understanding the risks associated with abdominal obesity is widely regarded as being far more important than worries about cosmetics. In addition, understanding the health issues linked with abdominal obesity can help in the self-motivation process of losing the abdominal fat. As mentioned above, abdominal fat is linked with cardiovascular disease, diabetes, and cancer. Specifically it's the deepest layer of belly fat (the fat you cannot see or grab) that poses health risks, as these "visceral" fat cells produce hormones that can affect health (e.g. increased insulin resistance and/or breast cancer risk). The risk increases considering the fact that they are located in the proximity or in between organs in the abdominal cavity. For example, fat next to the liver drains into it, causing a fatty liver, which is a risk factor for insulin resistance, setting the stage for Type 2 diabetes.
In the presence of diabetes mellitus type 2, the physician might instead prescribe metformin and thiazolidinediones (rosiglitazone or pioglitazone) as antidiabetic drugs rather than sulfonylurea derivatives. Thiazolidinediones may cause slight weight gain but decrease "pathologic" abdominal fat (visceral fat), and therefore may be prescribed for diabetics with central obesity.
Thiazolidinedione has been associated with heart failure and increased cardiovascular risk; so it has been withdrawn from the market in Europe by EMA in 2010.
Low-fat diets may not be an effective long-term intervention for obesity: as Bacon and Aphramor wrote, "The majority of individuals regain virtually all of the weight that was lost during treatment." The Women's Health Initiative ("the largest and longest randomized, controlled dietary intervention clinical trial") found that long-term dietary intervention increased the waist circumference of both the intervention group and the control group, though the increase was smaller for the intervention group. The conclusion was that mean weight decreased significantly in the intervention group from baseline to year 1 by 2.2 kg (P<.001) and was 2.2 kg less than the control group change from baseline at year 1. This difference from baseline between control and intervention groups diminished over time, but a significant difference in weight was maintained through year 9, the end of the study.
Society and culture.
Myths.
There is a common misconception that spot exercise (that is, exercising a specific muscle or location of the body) most effectively burns fat at the desired location, but this is not the case. Spot exercise is beneficial for building specific muscles, but it has little effect, if any, on fat in that area of the body, or on the body's distribution of body fat. The same logic applies to sit-ups and belly fat. Sit-ups, crunches and other abdominal exercises are useful in building the abdominal muscles, but they have little effect, if any, on the adipose tissue located there.
Colloquialisms.
Several colloquial terms used to refer to central obesity, and to people who have it, refer to beer drinking. However, there is little scientific evidence that beer drinkers are more prone to central obesity, despite its being known colloquially as "beer belly‚" "beer gut‚" or "beer pot". One of the few studies conducted on the subject did not find that beer drinkers are more prone to central obesity than nondrinkers or drinkers of wine or spirits. Chronic alcoholism can lead to cirrhosis, symptoms of which include gynecomastia (enlarged breasts) and ascites (abdominal fluid). These symptoms can suggest the appearance of central obesity.
Deposits of excess fat at the sides of one's waistline are commonly referred to as "love handles."
Economics.
Researchers in Copenhagen examined the relationship between waist circumferences and costs among 31,840 subjects aged 50–64 years of age with different waist circumferences. Their study showed that an increase in just an additional centimetre above normal waistline caused a 1.25% and 2.08% rise in health care costs in women and men respectively. To put this in perspective, a woman with a waistline of 95 cm. and without underlying health problems or co-morbidities can incur economic costs that are 22%, of 397 USD, higher per year than a woman with a normal waist circumference.

</doc>
<doc id="54716" url="https://en.wikipedia.org/wiki?curid=54716" title="Natufian culture">
Natufian culture

The Natufian culture was an Epipaleolithic culture that existed from 12,500 to 9,500 BC in the Levant, a region in the Eastern Mediterranean. It was unusual in that it was sedentary, or semi-sedentary, before the introduction of agriculture. The Natufian communities may be the ancestors of the builders of the first Neolithic settlements of the region, which may have been the earliest in the world. Some evidence suggests deliberate cultivation of cereals, specifically rye, by the Natufian culture, at Tell Abu Hureyra, the site of earliest evidence of agriculture in the world. Generally, though, Natufians exploited wild cereals. Animals hunted included gazelles. According to Christy G. Turner II, there is an archaeological and physical anthropological reason for a relationship between the modern Semitic-speaking populations of the Levant and the Natufians.
The term "Natufian" was coined by Dorothy Garrod who studied the Shuqba cave in Wadi an-Natuf, in the western Judean Mountains, about halfway between Tel Aviv and Ramallah.
Dating.
Radiocarbon dating places this culture from the terminal Pleistocene to the very beginning of the Holocene, from 12,500 to 9,500 BC.
The period is commonly split into two subperiods: Early Natufian (12,500–10,800 BC) and Late Natufian (10,800–9,500 BC). The Late Natufian most likely occurred in tandem with the Younger Dryas (10,800 to 9,500 BC). The Levant hosts more than a hundred kinds of cereals, fruits, nuts, and other edible parts of plants, and the flora of the Levant during the Natufian period was not the dry, barren, and thorny landscape of today, but rather woodland.
Precursors and associated cultures.
The Natufian developed in the same region as the earlier Kebaran complex, and is generally seen as a successor which developed from at least elements within that earlier culture. There were also other cultures in the region, such as the Mushabian culture of the Negev and Sinai, which are sometimes distinguished from the Kebaran, and sometimes also seen as having played a role in the development of the Natufian.
More generally there has been discussion of the similarities of these cultures with those found in coastal North Africa. Graeme Barker notes there are: "similarities in the respective archaeological records of the Natufian culture of the Levant and of contemporary foragers in coastal North Africa across the late Pleistocene and early Holocene boundary".
Ofer Bar-Yosef has argued that there are signs of influences coming from North Africa to the Levant, citing the microburin technique and “microlithic forms such as arched backed bladelets and La Mouillah points.” But recent research has shown that the presence of arched backed bladelets, La Mouillah points, and the use of the microburin technique was already apparent in the Nebekian industry of the Eastern Levant. And Maher et al. state that, "Many technological nuances that have often been always highlighted as signiﬁcant during the Natuﬁan were already present during the Early and Middle EP and do not, in most cases, represent a radical departure in knowledge, tradition, or behavior."
Authors such as Christopher Ehret have built upon the little evidence available to develop scenarios of intensive usage of plants having built up first in North Africa, as a precursor to the development of true farming in the Fertile Crescent, but such suggestions are considered highly speculative until more North African archaeological evidence can be gathered. In fact, Weiss et al. have shown that the earliest known intensive usage of plants was in the Levant 23,000 years ago at the Ohalo II site. Anthropologist C. Loring Brace in a recent study on cranial metric traits however, was also able to identify a "clear link" to Sub-Saharan African populations for early Natufians based on his observation of gross anatomical similarity with extant populations found mostly in the Sahara. Brace believes that these populations later became assimilated into the broader continuum of Southwest Asian populations.
According to Bar-Yosef and Belfer-Cohen, "It seems that certain preadaptive traits, developed already by the Kebaran and Geometric Kebaran populations within the Mediterranean park forest, played an important role in the emergence of the new socioeconomic system known as the Natufian culture."
Settlements.
Settlements occur in the woodland belt where oak and "Pistacia" species dominated. The underbrush of this open woodland was grass with high frequencies of grain. The high mountains of Lebanon and the Anti-Lebanon, the steppe areas of the Negev desert in Israel and Sinai, and the Syro-Arabian desert in the east were much less favoured for Natufian settlement, presumably due to both their lower carrying capacity and the company of other groups of foragers who exploited this region.
The habitations of the Natufian are semi-subterranean, often with a dry-stone foundation. The superstructure was probably made of brushwood. No traces of mudbrick have been found, which became common in the following Pre-Pottery Neolithic A (PPNA). The round houses have a diameter between three and six meters, and they contain a central round or subrectangular fireplace. In Ain Mallaha traces of postholes have been identified. "Villages" can cover over 1,000 square meters. Smaller settlements have been interpreted by some researchers as camps. Traces of rebuilding in almost all excavated settlements seem to point to a frequent relocation, indicating a temporary abandonment of the settlement. Settlements have been estimated to house 100–150 people, but there are three categories: small, median, and large, ranging from 15 sq. m to 1,000 sq. m. There are no definite indications of storage facilities.
Lithics.
The Natufian had a microlithic industry, based on short blades and bladelets. The microburin technique was used. Geometric microliths include lunates, trapezes and triangles. There are backed blades as well. A special type of retouch (Helwan retouch) is characteristic for the early Natufian. In the late Natufian, the Harif-point, a typical arrowhead made from a regular blade, became common in the Negev. Some scholars use it to define a separate culture, the Harifian.
Sickle blades appear for the first time. The characteristic sickle-gloss shows that they have been used to cut the silica-rich stems of cereals and form an indirect proof for incipient agriculture. Shaft straighteners made of ground stone indicate the practice of archery. There are heavy ground-stone bowl mortars as well.
Other finds.
There was a rich bone industry, including harpoons and fish hooks. Stone and bone were worked into pendants and other ornaments. There are a few human figurines made of limestone (El-Wad, Ain Mallaha, Ain Sakhri), but the favourite subject of representative art seems to have been animals. Ostrich-shell containers have been found in the Negev.
Subsistence.
The Natufian people lived by hunting and gathering. The preservation of plant remains is poor because of the soil conditions, but wild cereals, legumes, almonds, acorns and pistachios may have been collected. Animal bones show that gazelle ("Gazella gazella" and "Gazella subgutturosa") were the main prey. Additionally deer, aurochs and wild boar were hunted in the steppe zone, as well as onagers and caprids (ibex). Water fowl and freshwater fish formed part of the diet in the Jordan River valley. Animal bones from Salibiya I (12,300 – 10,800 BP) have been interpreted as evidence for communal hunts with nets.
Development of agriculture.
According to one theory, it was a sudden change in climate, the Younger Dryas event (ca. 10,800 to 9500 BCE), that inspired the development of agriculture. The Younger Dryas was a 1,000-year-long interruption in the higher temperatures prevailing since the Last Glacial Maximum, which produced a sudden drought in the Levant. This would have endangered the wild cereals, which could no longer compete with dryland scrub, but upon which the population had become dependent to sustain a relatively large sedentary population. By artificially clearing scrub and planting seeds obtained from elsewhere, they began to practice agriculture. However, this theory of the origin of agriculture is controversial in the scientific community.
Domesticated dog.
It is at Natufian sites that some of the earliest archaeological evidence for the domestication of the dog is found. At the Natufian site of Ain Mallaha in Israel, dated to 12,000 BCE, the remains of an elderly human and a four-to-five-month-old puppy were found buried together. At another Natufian site at the cave of Hayonim, humans were found buried with two canids.
Art.
The "Ain Sakhri lovers", a carved stone object held at the British Museum, is the oldest known depiction of a couple having sex. It was found in the Ain Sakhri cave in the Judean desert.
Burials.
Burials made of shell, teeth (of red deer), bones, and stone. There are pendants, bracelets, necklaces, earrings, and belt-ornaments as well.
In 2008, the grave of a Natufian 'priestess' was discovered (in most media reports referred to as a shaman or witch doctor). The burial contained complete shells of 50 tortoises, which are thought to have been brought to the site and eaten during the funeral feast.
Long distance exchange.
At Ain Mallaha (in Northern Israel), Anatolian obsidian and shellfish from the Nile valley have been found. The source of malachite beads is still unknown.
Language.
While the period involved makes it difficult to speculate on any language associated with the Natufian culture, linguists who believe it is possible to speculate this far back in time have written on this subject. As with other Natufian subjects, opinions tend to either emphasize North African connections or Eurasian connections. The view that the Natufians spoke an Afro-Asiatic language is accepted by Vitaly Shevoroshkin. Alexander Militarev and others have argued that the Natufian may represent the culture which spoke Proto-Afroasiatic, which he in turn believes has a Eurasian origin associated with the concept of Nostratic languages. The possibility of Natufians speaking proto-Afro-Asiatic, and that the language was introduced into Africa from the Levant, is approved by Colin Renfrew with caution, as a possible hypothesis for proto-Afro-Asiatic dispersal.
Some scholars, for example Christopher Ehret, Roger Blench and others, contend that the Afroasiatic Urheimat is to be found in North or North East Africa, probably in the area of Egypt, the Sahara, Horn of Africa or Sudan. Within this group, Ehret, who like Militarev believes Afroasiatic may already have been in existence in the Natufian period, would associate Natufians only with the Near Eastern pre-Proto-Semitic branch of Afroasiatic.
Craniofacial research.
The Epipalaeolithic Natufian of Israel from whom the Neolithic realm was assumed to arise is described as having a clear link to Sub-Saharan Africa. The Sub-Saharan element in the remains is also said to be of almost equal importance to that of the Eurasian element. The authors, however, remain cautious because of the small sample size. The authors further speculate that the admixture process between Neolithic people and in situ foragers diluted any discoverable trace of Sub-Saharan ancestry that may have been present.
Sites.
Natufian sites include:

</doc>
<doc id="54717" url="https://en.wikipedia.org/wiki?curid=54717" title="De Broglie–Bohm theory">
De Broglie–Bohm theory

The de Broglie–Bohm theory, also known as the pilot-wave theory, Bohmian mechanics, the Bohm or Bohm's interpretation, and the causal interpretation, is an interpretation of quantum theory. In addition to a wavefunction on the space of all possible configurations, it also postulates an actual configuration that exists even when unobserved. The evolution over time of the configuration (that is, of the positions of all particles or the configuration of all fields) is defined by the wave function via a guiding equation. The evolution of the wave function over time is given by Schrödinger's equation. The theory is named after Louis de Broglie (1892–1987), and David Bohm (1917–1992).
The theory is deterministic and explicitly nonlocal: the velocity of any one particle depends on the value of the guiding equation, which depends on the configuration of the system given by its wavefunction; the latter depends on the boundary conditions of the system, which in principle may be the entire universe.
The theory results in a measurement formalism, analogous to thermodynamics for classical mechanics, that yields the standard quantum formalism generally associated with the Copenhagen interpretation. The theory's explicit non-locality resolves the "measurement problem", which is conventionally delegated to the topic of interpretations of quantum mechanics in the Copenhagen interpretation.
The Born rule in Broglie–Bohm theory is not a basic law. Rather, in this theory the link between the probability density and the wave function has the status of a hypothesis, called the quantum equilibrium hypothesis, which is additional to the basic principles governing the wave function.
The theory was historically developed by de Broglie in the 1920s, who in 1927 was persuaded to abandon it in favour of the then-mainstream Copenhagen interpretation. David Bohm, dissatisfied with the prevailing orthodoxy, rediscovered de Broglie's pilot wave theory in 1952. Bohm's suggestions were not widely received then, partly due to reasons unrelated to their content, connected to Bohm's youthful communist affiliations. De Broglie–Bohm theory was widely deemed unacceptable by mainstream theorists, mostly because of its explicit non-locality. Bell's theorem (1964) was inspired by Bell's discovery of the work of David Bohm and his subsequent wondering if the obvious nonlocality of the theory could be eliminated. Since the 1990s, there has been renewed interest in formulating extensions to de Broglie–Bohm theory, attempting to reconcile it with special relativity and quantum field theory, besides other features such as spin or curved spatial geometries.
The "Stanford Encyclopedia of Philosophy" article on Quantum decoherence (Guido Bacciagaluppi, 2012) groups "approaches to quantum mechanics" into five groups, of which "pilot-wave theories" are one (the others being the Copenhagen interpretation, objective collapse theories, many-world interpretations and modal interpretations).
There are several equivalent mathematical formulations of the theory and it is known by a number of different names. The de Broglie wave has a macroscopic analogy termed Faraday wave.
Overview.
De Broglie–Bohm theory is based on the following postulates:
Where formula_8 is the probability current or probability flux and formula_9 is the momentum operator. Here, formula_10 is the standard complex-valued wavefunction known from quantum theory, which evolves according to Schrödinger's equation
This already completes the specification of the theory for any quantum theory with Hamilton operator of type formula_12.
Notably, even if this latter relation is frequently presented as an axiom of the theory, in Bohm's original papers of 1952 it was presented as derivable from statistical-mechanical arguments. This argument was further supported by the work of Bohm in 1953 and was substantiated by Vigier and Bohm's paper of 1954 in which they introduced stochastic "fluid fluctuations" that drive a process of asymptotic relaxation from quantum non-equilibrium to quantum equilibrium (ρ → |ψ|2).
Double-slit experiment.
The double-slit experiment is an illustration of wave-particle duality. In it, a beam of particles (such as electrons) travels through a barrier that has two slits. If one puts a detector screen on the side beyond the barrier, the pattern of detected particles shows interference fringes characteristic of waves arriving at the screen from two sources (the two slits); however, the interference pattern is made up of individual dots corresponding to particles that had arrived on the screen. The system seems to exhibit the behaviour of both waves (interference patterns) and particles (dots on the screen).
If we modify this experiment so that one slit is closed, no interference pattern is observed. Thus, the state of both slits affects the final results. We can also arrange to have a minimally invasive detector at one of the slits to detect which slit the particle went through. When we do that, the interference pattern disappears.
The Copenhagen interpretation states that the particles are not localised in space until they are detected, so that, if there is not any detector on the slits, there is no information about which slit the particle has passed through. If one slit has a detector on it, then the wavefunction collapses due to that detection.
In de Broglie–Bohm theory, the wavefunction is defined at both slits, but each particle has a well-defined trajectory that passes through exactly one of the slits. The final position of the particle on the detector screen and the slit through which the particle passes is determined by the initial position of the particle. Such initial position is not knowable or controllable by the experimenter, so there is an appearance of randomness in the pattern of detection. In Bohm's 1952 papers he used the wavefunction to construct a quantum potential that, when included in Newton's equations, gave the trajectories of the particles streaming through the two slits. In effect the wave function interferes with itself and guides the particles via the quantum potential in such a way that the particles avoid the regions in which the interference is destructive and are attracted to the regions in which the interference is constructive, resulting in the interference pattern on the detector screen.
To explain the behavior when the particle is detected to go through one slit, one needs to appreciate the role of the conditional wavefunction and how it results in the collapse of the wavefunction; this is explained below. The basic idea is that the environment registering the detection effectively separates the two wave packets in configuration space.
The theory.
The ontology.
The ontology of de Broglie-Bohm theory consists of a configuration formula_15 of the universe and a pilot wave formula_16. The configuration space formula_3 can be chosen differently, as in classical mechanics and standard quantum mechanics.
Thus, the ontology of pilot wave theory contains as the trajectory formula_15 we know from classical mechanics, as the wave function formula_16 of quantum theory. So, at every moment of time there exists not only a wave function, but also a well-defined configuration of the whole universe (i.e., the system as defined by the boundary conditions used in solving the Schrödinger equation). The correspondence to our experiences is made by the identification of the configuration of our brain with some part of the configuration of the whole universe formula_15, as in classical mechanics.
While the ontology of classical mechanics is part of the ontology of de Broglie–Bohm theory, the dynamics are very different. In classical mechanics, the accelerations of the particles are imparted directly by forces, which exist in physical three-dimensional space. In de Broglie–Bohm theory, the velocities of the particles are given by the wavefunction, which exists in a 3N-dimensional configuration space, where N corresponds to the number of particles in the system; Bohm hypothesized that each particle has a "complex and subtle inner structure" that provides the capacity to react to the information provided by the wavefunction via the quantum potential. Also, unlike in classical mechanics, physical properties (e.g., mass, charge) are spread out over the wavefunction in de Broglie-Bohm theory, not localized at the position of the particle.
The wavefunction itself, and not the particles, determines the dynamical evolution of the system: the particles do not act back onto the wave function. As Bohm and Hiley worded it, "the Schrödinger equation for the quantum field does not have sources, nor does it have any other way by which the field could be directly affected by the condition of the particles [...] the quantum theory can be understood completely in terms of the assumption that the quantum field has no sources or other forms of dependence on the particles." P. Holland considers this lack of reciprocal action of particles and wave function to be one "the many nonclassical properties exhibited by this theory". It should be noted however that Holland has later called this a merely "apparent" lack of back reaction, due to the incompleteness of the description.
In what follows below, we will give the setup for one particle moving in formula_21 followed by the setup for formula_5 particles moving in 3 dimensions. In the first instance, configuration space and real space are the same while in the second, real space is still formula_21, but configuration space becomes formula_24. While the particle positions themselves are in real space, the velocity field and wavefunction are on configuration space, which is how particles are entangled with each other in this theory.
Extensions to this theory include spin and more complicated configuration spaces.
We use variations of formula_25 for particle positions while formula_26 represents the complex-valued wavefunction on configuration space.
Guiding equation.
For a spinless single particle moving in formula_21, the particle's velocity is given
For many particles, we label them as formula_4 for the formula_30th particle and their velocities are given by
The main fact to notice is that this velocity field depends on the actual positions of all of the formula_5 particles in the universe. As explained below, in most experimental situations, the influence of all of those particles can be encapsulated into an effective wavefunction for a subsystem of the universe.
Schrödinger's equation.
The one particle Schrödinger equation governs the time evolution of a complex-valued wavefunction on formula_21. The equation represents a quantized version of the total energy of a classical system evolving under a real-valued potential function formula_34 on formula_21:
For many particles, the equation is the same except that formula_26 and formula_34 are now on configuration space, formula_24.
This is the same wavefunction of conventional quantum mechanics.
Relation to the Born Rule.
In Bohm's original papers 1952, he discusses how de Broglie–Bohm theory results in the usual measurement results of quantum mechanics. The main idea is that this is true if the positions of the particles satisfy the statistical distribution given by formula_41. And that distribution is guaranteed to be true for all time by the guiding equation if the initial distribution of the particles satisfies formula_41.
For a given experiment, we can postulate this as being true and verify experimentally that it does indeed hold true, as it does. But, as argued in Dürr et al., one needs to argue that this distribution for subsystems is typical. They argue that formula_41 by virtue of its equivariance under the dynamical evolution of the system, is the appropriate measure of typicality for initial conditions of the positions of the particles. They then prove that the vast majority of possible initial configurations will give rise to statistics obeying the Born rule (i.e., formula_41) for measurement outcomes. In summary, in a universe governed by the de Broglie–Bohm dynamics, Born rule behavior is typical.
The situation is thus analogous to the situation in classical statistical physics. A low entropy initial condition will, with overwhelmingly high probability, evolve into a higher entropy state: behavior consistent with the second law of thermodynamics is typical. There are, of course, anomalous initial conditions that would give rise to violations of the second law. However, in the absence of some very detailed evidence supporting the actual realization of one of those special initial conditions, it would be quite unreasonable to expect anything but the actually observed uniform increase of entropy. Similarly, in the de Broglie–Bohm theory, there are anomalous initial conditions that would produce measurement statistics in violation of the Born rule (i.e., in conflict with the predictions of standard quantum theory). But the typicality theorem shows that, in the absence of some specific reason to believe that one of those special initial conditions was in fact realized, the Born rule behavior is what one should expect.
It is in that qualified sense that the Born rule is, for the de Broglie–Bohm theory, a theorem rather than (as in ordinary quantum theory) an additional postulate.
It can also be shown that a distribution of particles that is "not" distributed according to the Born rule (that is, a distribution 'out of quantum equilibrium') and evolving under the de Broglie-Bohm dynamics is overwhelmingly likely to evolve dynamically into a state distributed as formula_41. See, for example Ref.
. A video of the electron density in a 2D box evolving under this process is available here.
The conditional wave function of a subsystem.
In the formulation of the De Broglie–Bohm theory, there is only a wave function for the entire universe (which always evolves by the Schrödinger equation). It should however be noted that the "universe" is simply the system limited by the same boundary conditions used to solve the Schrödinger equation. However, once the theory is formulated, it is convenient to introduce a notion of wave function also for subsystems of the universe. Let us write the wave function of the universe as formula_46, where formula_47 denotes the configuration variables associated to some subsystem (I) of the universe and formula_48 denotes the remaining configuration variables. Denote, respectively, by formula_49 and by formula_50 the actual configuration of subsystem (I) and of the rest of the universe. For simplicity, we consider here only the spinless case. The "conditional wave function" of subsystem (I) is defined by:
It follows immediately from the fact that formula_52 satisfies the guiding equation that also the configuration formula_49 satisfies a guiding equation identical to the one presented in the formulation of the theory, with the universal wave function formula_26 replaced with the conditional wave function formula_55. Also, the fact that formula_56 is random with probability density given by the square modulus of formula_57 implies that the conditional probability density of formula_49 given formula_50 is given by the square modulus of the (normalized) conditional wave function formula_60 (in the terminology of Dürr et al. this fact is called the "fundamental conditional probability formula").
Unlike the universal wave function, the conditional wave function of a subsystem does not always evolve by the Schrödinger equation, but in many situations it does. For instance, if the universal wave function factors as:
then the conditional wave function of subsystem (I) is (up to an irrelevant scalar factor) equal to formula_55 (this is what Standard Quantum Theory would regard as the wave function of subsystem (I)). If, in addition, the Hamiltonian does not contain an interaction term between subsystems (I) and (II) then formula_55 does satisfy a Schrödinger equation. More generally, assume that the universal wave function formula_26 can be written in the form:
where formula_66 solves Schrödinger equation and formula_67 for all formula_14 and formula_47. Then, again, the conditional wave function of subsystem (I) is (up to an irrelevant scalar factor) equal to formula_55 and if the Hamiltonian does not contain an interaction term between subsystems (I) and (II), formula_55 satisfies a Schrödinger equation.
The fact that the conditional wave function of a subsystem does not always evolve by the Schrödinger equation is related to the fact that the usual collapse rule of Standard Quantum Theory emerges from the Bohmian formalism when one considers conditional wave functions of subsystems.
Extensions.
Relativity.
Pilot wave theory is explicitly nonlocal, which is in ostensible conflict with special relativity. Various extensions of "Bohm-like" mechanics exist that attempt to resolve this problem. Bohm himself in 1953 presented an extension of the theory satisfying the Dirac equation for a single particle. However, this was not extensible to the many-particles case because it used an absolute time.
A renewed interest in constructing Lorentz-invariant extensions of Bohmian theory arose in the 1990s; see Bohm and Hiley: The Undivided Universe, and [http://xxx.lanl.gov/abs/quant-ph/0302152, and references therein. Another approach is given in the work of Dürr et al. in which they use Bohm-Dirac models and a Lorentz-invariant foliation of space-time.
Thus, Dürr et al. (1999) showed that it is possible to formally restore Lorentz invariance for the Bohm-Dirac theory by introducing additional structure. This approach still requires a foliation of space-time. While this is in conflict with the standard interpretation of relativity, the preferred foliation, if unobservable, does not lead to any empirical conflicts with relativity. In 2013, Dürr et al. suggested that the required foliation could be covariantly determined by the wave function.
The relation between nonlocality and preferred foliation can be better understood as follows. In de Broglie–Bohm theory, nonlocality manifests as the fact that the velocity and acceleration of one particle depends on the instantaneous positions of all other particles. On the other hand, in the theory of relativity the concept of instantaneousness does not have an invariant meaning. Thus, to define particle trajectories, one needs an additional rule that defines which space-time points should be considered instantaneous. The simplest way to achieve this is to introduce a preferred foliation of space-time by hand, such that each hypersurface of the foliation defines a hypersurface of equal time.
Initially, it had been considered impossible to set out a description of photon trajectories in the de Broglie–Bohm theory in view of the difficulties of describing bosons relativistically. In 1996, Partha Ghose had presented a relativistic quantum mechanical description of spin-0 and spin-1 bosons starting from the Duffin–Kemmer–Petiau equation, setting out Bohmian trajectories for massive bosons and for massless bosons (and therefore photons). In 2001, Jean-Pierre Vigier emphasized the importance of deriving a well-defined description of light in terms of particle trajectories in the framework of either the Bohmian mechanics or the Nelson stochastic mechanics. The same year, Ghose worked out Bohmian photon trajectories for specific cases. Subsequent weak measurement experiments yielded trajectories that coincide with the predicted trajectories.
Chris Dewdney and G. Horton have proposed a relativistically covariant, wave-functional formulation of Bohm's quantum field theory and has extended it to a form that allows the inclusion of gravity.
Nikolić has proposed a Lorentz-covariant formulation of the Bohmian interpretation of many-particle wave functions. He has developed a generalized relativistic-invariant probabilistic interpretation of quantum theory, in which formula_41 is no longer a probability density in space, but a probability density in space-time. He uses this generalized probabilistic interpretation to formulate a relativistic-covariant version of de Broglie–Bohm theory without introducing a preferred foliation of space-time. His work also covers the extension of the Bohmian interpretation to a quantization of fields and strings.
Roderick I. Sutherland at the University in Sydney has a Lagrangian formalism for the pilot wave and its beables. I draws on Yakir Aharonov's retrocasual weak measurements to explain many-particle entanglement in a special relativistic way without the need for configuration space. The basic idea was already published by Costa de Beauregard in the 1950s and is also used by John Cramer in his transactional interpretation sans the beables that exist between the von Neumann strong projection operator measurements. Sutherland's Lagrangian includes two-way action-reaction between pilot wave and beables. Therefore, it is a post-quantum non-statistical theory with final boundary conditions that violate the no-signal theorems of quantum theory. Just as special relativity is a limiting case of general relativity when the spacetime curvature vanishes, so, too is statistical no-entanglement signaling quantum theory with the Born rule a limiting case of the post-quantum action-reaction Lagrangian when the reaction is set to zero and the final boundary condition is integrated out.
Spin.
To incorporate spin, the wavefunction becomes complex-vector valued. The value space is called spin space; for a spin-½ particle, spin space can be taken to be formula_73. The guiding equation is modified by taking inner products in spin space to reduce the complex vectors to complex numbers. The Schrödinger equation is modified by adding a Pauli spin term.
where formula_75 is the magnetic moment of the formula_30th particle, formula_77 is the appropriate spin operator acting in the formula_30th particle's spin space, formula_79 is spin of the particle (formula_80 for electron),
formula_82 and formula_83 are, respectively, the magnetic field and the vector potential in formula_84 (all other functions are fully on configuration space), formula_85 is the charge of the formula_30th particle, and formula_87 is the inner product in spin space formula_88,
For an example of a spin space, a system consisting of two spin 1/2 particle and one spin 1 particle has a wavefunction of the form
That is, its spin space is a 12 dimensional space.
Quantum field theory.
In Dürr et al., the authors describe an extension of de Broglie–Bohm theory for handling creation and annihilation operators, which they refer to as "Bell-type quantum field theories". The basic idea is that configuration space becomes the (disjoint) space of all possible configurations of any number of particles. For part of the time, the system evolves deterministically under the guiding equation with a fixed number of particles. But under a stochastic process, particles may be created and annihilated. The distribution of creation events is dictated by the wavefunction. The wavefunction itself is evolving at all times over the full multi-particle configuration space.
Hrvoje Nikolić introduces a purely deterministic de Broglie–Bohm theory of particle creation and destruction, according to which particle trajectories are continuous, but particle detectors behave as if particles have been created or destroyed even when a true creation or destruction of particles does not take place.
Curved space.
To extend de Broglie–Bohm theory to curved space (Riemannian manifolds in mathematical parlance), one simply notes that all of the elements of these equations make sense, such as gradients and Laplacians. Thus, we use equations that have the same form as above. Topological and boundary conditions may apply in supplementing the evolution of Schrödinger's equation.
For a de Broglie–Bohm theory on curved space with spin, the spin space becomes a vector bundle over configuration space and the potential in Schrödinger's equation becomes a local self-adjoint operator acting on that space.
Exploiting nonlocality.
Antony Valentini has extended the de Broglie–Bohm theory to include signal nonlocality that would allow entanglement to be used as a stand-alone communication channel without a secondary classical "key" signal to "unlock" the message encoded in the entanglement. This violates orthodox quantum theory but it has the virtue that it makes the parallel universes of the chaotic inflation theory observable in principle.
Unlike de Broglie–Bohm theory, Valentini's theory has the wavefunction evolution also depend on the ontological variables. This introduces an instability, a feedback loop that pushes the hidden variables out of "sub-quantal heat death". The resulting theory becomes nonlinear and non-unitary.
Results.
Below are some highlights of the results that arise out of an analysis of de Broglie–Bohm theory. Experimental results agree with all of the standard predictions of quantum mechanics in so far as the latter has predictions. However, while standard quantum mechanics is limited to discussing the results of 'measurements', de Broglie–Bohm theory is a theory that governs the dynamics of a system without the intervention of outside observers (p. 117 in Bell).
The basis for agreement with standard quantum mechanics is that the particles are distributed according to formula_41. This is a statement of observer ignorance, but it can be proven that for a universe governed by this theory, this will typically be the case. There is apparent collapse of the wave function governing subsystems of the universe, but there is no collapse of the universal wavefunction.
Measuring spin and polarization.
According to ordinary quantum theory, it is not possible to measure the spin or polarization of a particle directly; instead, the component in one direction is measured; the outcome from a single particle may be 1, meaning that the particle is aligned with the measuring apparatus, or −1, meaning that it is aligned the opposite way. For an ensemble of particles, if we expect the particles to be aligned, the results are all 1. If we expect them to be aligned oppositely, the results are all −1. For other alignments, we expect some results to be 1 and some to be −1 with a probability that depends on the expected alignment. For a full explanation of this, see the Stern-Gerlach Experiment.
In de Broglie–Bohm theory, the results of a spin experiment cannot be analyzed without some knowledge of the experimental setup. It is possible to modify the setup so that the trajectory of the particle is unaffected, but that the particle with one setup registers as spin up while in the other setup it registers as spin down. Thus, for the de Broglie–Bohm theory, the particle's spin is not an intrinsic property of the particle—instead spin is, so to speak, in the wave function of the particle in relation to the particular device being used to measure the spin. This is an illustration of what is sometimes referred to as contextuality, and is related to naive realism about operators.
Measurements, the quantum formalism, and observer independence.
De Broglie–Bohm theory gives the same results as quantum mechanics. It treats the wavefunction as a fundamental object in the theory as the wavefunction describes how the particles move. This means that no experiment can distinguish between the two theories. This section outlines the ideas as to how the standard quantum formalism arises out of quantum mechanics. References include Bohm's original 1952 paper and Dürr et al.
Collapse of the wavefunction.
De Broglie–Bohm theory is a theory that applies primarily to the whole universe. That is, there is a single wavefunction governing the motion of all of the particles in the universe according to the guiding equation. Theoretically, the motion of one particle depends on the positions of all of the other particles in the universe. In some situations, such as in experimental systems, we can represent the system itself in terms of a de Broglie–Bohm theory in which the wavefunction of the system is obtained by conditioning on the environment of the system. Thus, the system can be analyzed with Schrödinger's equation and the guiding equation, with an initial formula_41 distribution for the particles in the system (see the section on the conditional wave function of a subsystem for details).
It requires a special setup for the conditional wavefunction of a system to obey a quantum evolution. When a system interacts with its environment, such as through a measurement, the conditional wavefunction of the system evolves in a different way. The evolution of the universal wavefunction can become such that the wavefunction of the system appears to be in a superposition of distinct states. But if the environment has recorded the results of the experiment, then using the actual Bohmian configuration of the environment to condition on, the conditional wavefunction collapses to just one alternative, the one corresponding with the measurement results.
Collapse of the universal wavefunction never occurs in de Broglie–Bohm theory. Its entire evolution is governed by Schrödinger's equation and the particles' evolutions are governed by the guiding equation. Collapse only occurs in a phenomenological way for systems that seem to follow their own Schrödinger's equation. As this is an effective description of the system, it is a matter of choice as to what to define the experimental system to include and this will affect when "collapse" occurs.
Operators as observables.
In the standard quantum formalism, measuring observables is generally thought of as measuring operators on the Hilbert space. For example, measuring position is considered to be a measurement of the position operator. This relationship between physical measurements and Hilbert space operators is, for standard quantum mechanics, an additional axiom of the theory. The de Broglie–Bohm theory, by contrast, requires no such measurement axioms (and measurement as such is not a dynamically distinct or special sub-category of physical processes in the theory). In particular, the usual operators-as-observables formalism is, for de Broglie–Bohm theory, a theorem. A major point of the analysis is that many of the measurements of the observables do not correspond to properties of the particles; they are (as in the case of spin discussed above) measurements of the wavefunction.
In the history of de Broglie–Bohm theory, the proponents have often had to deal with claims that this theory is impossible. Such arguments are generally based on inappropriate analysis of operators as observables. If one believes that spin measurements are indeed measuring the spin of a particle that existed prior to the measurement, then one does reach contradictions. De Broglie–Bohm theory deals with this by noting that spin is not a feature of the particle, but rather that of the wavefunction. As such, it only has a definite outcome once the experimental apparatus is chosen. Once that is taken into account, the impossibility theorems become irrelevant.
There have also been claims that experiments reject the Bohm trajectories in favor of the standard QM lines. But as shown in [http://arxiv.org/abs/quant-ph/0108038 and [http://arxiv.org/abs/quant-ph/0305131], such experiments cited above only disprove a misinterpretation of the de Broglie–Bohm theory, not the theory itself.
There are also objections to this theory based on what it says about particular situations usually involving eigenstates of an operator. For example, the ground state of hydrogen is a real wavefunction. According to the guiding equation, this means that the electron is at rest when in this state. Nevertheless, it is distributed according to formula_41 and no contradiction to experimental results is possible to detect.
Operators as observables leads many to believe that many operators are equivalent. De Broglie–Bohm theory, from this perspective, chooses the position observable as a favored observable rather than, say, the momentum observable. Again, the link to the position observable is a consequence of the dynamics. The motivation for de Broglie–Bohm theory is to describe a system of particles. This implies that the goal of the theory is to describe the positions of those particles at all times. Other observables do not have this compelling ontological status. Having definite positions explains having definite results such as flashes on a detector screen. Other observables would not lead to that conclusion, but there need not be any problem in defining a mathematical theory for other observables; see Hyman et al. for an exploration of the fact that a probability density and probability current can be defined for any set of commuting operators.
Hidden variables.
De Broglie–Bohm theory is often referred to as a "hidden variable" theory. Bohm used this description in his original papers on the subject, writing, "From the point of view of the usual interpretation, these additional elements or parameters a detailed causal and continuous description of all processes could be called 'hidden' variables." Bohm and Hiley later stated that they found Bohm's choice of the term "hidden variables" to be too restrictive. In particular, they argued that a particle is not actually hidden but rather "is what is most directly manifested in an observation its properties cannot be observed with arbitrary precision (within the limits set by uncertainty principle)". However, others nevertheless treat the term "hidden variable" as a suitable description.
Generalized particle trajectories can be extrapolated from numerous weak measurements on an ensemble of equally prepared systems, and such trajectories coincide with the de Broglie–Bohm trajectories. In particular, experiment with two entangled photons, in which a set of Bohmian trajectories for one of the photons was determined using weak measurements and postselection, can be understood in terms of a nonlocal connection between that photon's trajectory and the other photon's polarisation. However, not only the De Broglie-Bohm interpretation, but also many other interpretation of quantum mechanics that do not include such trajectories are consistent with such experimental evidence.
Heisenberg's uncertainty principle.
The Heisenberg uncertainty principle states that when two complementary measurements are made, there is a limit to the product of their accuracy. As an example, if one measures the position with an accuracy of formula_94, and the momentum with an accuracy of formula_95, then formula_96 If we make further measurements in order to get more information, we disturb the system and change the trajectory into a new one depending on the measurement setup; therefore, the measurement results are still subject to Heisenberg's uncertainty relation.
In de Broglie–Bohm theory, there is always a matter of fact about the position and momentum of a particle. Each particle has a well-defined trajectory, as well as a wave function. Observers have limited knowledge as to what this trajectory is (and thus of the position and momentum). It is the lack of knowledge of the particle's trajectory that accounts for the uncertainty relation. What one can know about a particle at any given time is described by the wavefunction. Since the uncertainty relation can be derived from the wavefunction in other interpretations of quantum mechanics, it can be likewise derived (in the epistemic sense mentioned above), on the de Broglie–Bohm theory.
To put the statement differently, the particles' positions are only known statistically. As in classical mechanics, successive observations of the particles' positions refine the experimenter's knowledge of the particles' initial conditions. Thus, with succeeding observations, the initial conditions become more and more restricted. This formalism is consistent with the normal use of the Schrödinger equation.
For the derivation of the uncertainty relation, see Heisenberg uncertainty principle, noting that it describes it from the viewpoint of the Copenhagen interpretation.
Quantum entanglement, Einstein-Podolsky-Rosen paradox, Bell's theorem, and nonlocality.
De Broglie–Bohm theory highlighted the issue of nonlocality: it inspired John Stewart Bell to prove his now-famous theorem, which in turn led to the Bell test experiments.
In the Einstein–Podolsky–Rosen paradox, the authors describe a thought-experiment one could perform on a pair of particles that have interacted, the results of which they interpreted as indicating that quantum mechanics is an incomplete theory.
Decades later John Bell proved Bell's theorem (see p. 14 in Bell), in which he showed that, if they are to agree with the empirical predictions of quantum mechanics, all such "hidden-variable" completions of quantum mechanics must either be nonlocal (as the Bohm interpretation is) or give up the assumption that experiments produce unique results (see counterfactual definiteness and many-worlds interpretation). In particular, Bell proved that any local theory with unique results must make empirical predictions satisfying a statistical constraint called "Bell's inequality".
Alain Aspect performed a series of Bell test experiments that test Bell's inequality using an EPR-type setup. Aspect's results show experimentally that Bell's inequality is in fact violated—meaning that the relevant quantum mechanical predictions are correct. In these Bell test experiments, entangled pairs of particles are created; the particles are separated, traveling to remote measuring apparatus. The orientation of the measuring apparatus can be changed while the particles are in flight, demonstrating the apparent nonlocality of the effect.
The de Broglie–Bohm theory makes the same (empirically correct) predictions for the Bell test experiments as ordinary quantum mechanics. It is able to do this because it is manifestly nonlocal. It is often criticized or rejected based on this; Bell's attitude was: "It is a merit of the de Broglie–Bohm version to bring this out so explicitly that it cannot be ignored."
The de Broglie–Bohm theory describes the physics in the Bell test experiments as follows: to understand the evolution of the particles, we need to set up a wave equation for both particles; the orientation of the apparatus affects the wavefunction. The particles in the experiment follow the guidance of the wavefunction. It is the wavefunction that carries the faster-than-light effect of changing the orientation of the apparatus. An analysis of exactly what kind of nonlocality is present and how it is compatible with relativity can be found in Maudlin. Note that in Bell's work, and in more detail in Maudlin's work, it is shown that the nonlocality does not allow for signaling at speeds faster than light.
Classical limit.
Bohm's formulation of de Broglie–Bohm theory in terms of a classical-looking version has the merits that the emergence of classical behavior seems to follow immediately for any situation in which the quantum potential is negligible, as noted by Bohm in 1952. Modern methods of decoherence are relevant to an analysis of this limit. See Allori et al. for steps towards a rigorous analysis.
Quantum trajectory method.
Work by Robert E. Wyatt in the early 2000s attempted to use the Bohm "particles" as an adaptive mesh that follows the actual trajectory of a quantum state in time and space. In the "quantum trajectory" method, one samples the quantum wavefunction with a mesh of quadrature points. One then evolves the quadrature points in time according to the Bohm equations of motion. At each time-step, one then re-synthesizes the wavefunction from the points, recomputes the quantum forces, and continues the calculation. (QuickTime movies of this for H+H2 reactive scattering can be found on the Wyatt group web-site at UT Austin.)
This approach has been adapted, extended, and used by a number of researchers in the Chemical Physics community as a way to compute semi-classical and quasi-classical molecular dynamics. A recent (2007) issue of the Journal of Physical Chemistry A was dedicated to Prof. Wyatt and his work on "Computational Bohmian Dynamics".
Eric R. Bittner's group at the University of Houston has advanced a statistical variant of this approach that uses Bayesian sampling technique to sample the quantum density and compute the quantum potential on a structureless mesh of points. This technique was recently used to estimate quantum effects in the heat-capacity of small clusters Nen for n~100.
There remain difficulties using the Bohmian approach, mostly associated with the formation of singularities in the quantum potential due to nodes in the
quantum wavefunction. In general, nodes forming due to interference effects lead to the case where
formula_97
This results in an infinite force on the sample particles forcing them to move away from the node and often crossing the path of other sample points (which violates single-valuedness). Various schemes have been developed to overcome this; however, no general solution has yet emerged.
These methods, as does Bohm's Hamilton-Jacobi formulation, do not apply to situations in which the full dynamics of spin need to be taken into account.
Occam's razor criticism.
Both Hugh Everett III and Bohm treated the wavefunction as a physically real field. Everett's many-worlds interpretation is an attempt to demonstrate that the wavefunction alone is sufficient to account for all our observations. When we see the particle detectors flash or hear the click of a Geiger counter then Everett's theory interprets this as our "wavefunction" responding to changes in the detector's "wavefunction", which is responding in turn to the passage of another "wavefunction" (which we think of as a "particle", but is actually just another wave-packet). No particle (in the Bohm sense of having a defined position and velocity) exists, according to that theory. For this reason Everett sometimes referred to his own many-worlds approach as the "pure wave theory". Talking of Bohm's 1952 approach, Everett says: 
In the Everettian view, then, the Bohm particles are superfluous entities, similar to, and equally as unnecessary as, for example, the luminiferous ether, which was found to be unnecessary in special relativity. This argument of Everett's is sometimes called the "redundancy argument", since the superfluous particles are redundant in the sense of Occam's razor.
Many authors have expressed critical views of the de Broglie-Bohm theory, by comparing it to Everett's many worlds approach. Many (but not all) proponents of the de Broglie-Bohm theory (such as Bohm and Bell) interpret the universal wave function as physically real. According to some supporters of Everett's theory, if the (never collapsing) wave function is taken to be physically real, then it is natural to interpret the theory as having the same many worlds as Everett's theory. In the Everettian view the role of the Bohm particle is to act as a "pointer", tagging, or selecting, just one branch of the universal wavefunction (the assumption that this branch indicates which "wave packet" determines the observed result of a given experiment is called the "result assumption"); the other branches are designated "empty" and implicitly assumed by Bohm to be devoid of conscious observers. H. Dieter Zeh comments on these "empty" branches:
David Deutsch has expressed the same point more "acerbically":
According to Brown & Wallace the de Broglie-Bohm particles play no role in the solution of the measurement problem. These authors claim that the "result assumption" (see above) is inconsistent with the view that there is no measurement problem in the predictable outcome (i.e. single-outcome) case. These authors also claim that a standard tacit assumption of the de Broglie-Bohm theory (that an observer becomes aware of configurations of particles of ordinary objects by means of correlations between such configurations and the configuration of the particles in the observer's brain) is unreasonable. This conclusion has been challenged by Valentini who argues that the entirety of such objections arises from a failure to interpret de Broglie-Bohm theory on its own terms.
According to Peter R. Holland, in a wider Hamiltonian framework, theories can be formulated in which particles "do" act back on the wave function.
Derivations.
De Broglie–Bohm theory has been derived many times and in many ways. Below are six derivations all of which are very different and lead to different ways of understanding and extending this theory.
History.
De Broglie–Bohm theory has a history of different formulations and names. In this section, each stage is given a name and a main reference.
Pilot-wave theory.
Dr. de Broglie presented his pilot wave theory at the 1927 Solvay Conference, after close collaboration with Schrödinger, who developed his wave equation for de Broglie's theory. At the end of the presentation, Wolfgang Pauli pointed out that it was not compatible with a semi-classical technique Fermi had previously adopted in the case of inelastic scattering. Contrary to a popular legend, de Broglie actually gave the correct rebuttal that the particular technique could not be generalized for Pauli's purpose, although the audience might have been lost in the technical details and de Broglie's mild manner left the impression that Pauli's objection was valid. He was eventually persuaded to abandon this theory nonetheless because he was "discouraged by criticisms which roused". De Broglie's theory already applies to multiple spin-less particles, but lacks an adequate theory of measurement as no one understood quantum decoherence at the time. An analysis of de Broglie's presentation is given in Bacciagaluppi et al. Also, in 1932 John von Neumann published a paper, that was widely (and erroneously, as shown by Jeffrey Bub) believed to prove that all hidden-variable theories are impossible. This sealed the fate of de Broglie's theory for the next two decades.
In 1926, Erwin Madelung had developed a hydrodynamic version of Schrödinger's equation, which is incorrectly considered as a basis for the density current derivation of the de Broglie–Bohm theory. The Madelung equations, being quantum Euler equations (fluid dynamics), differ philosophically from the de Broglie–Bohm mechanics and are the basis of the stochastic interpretation of quantum mechanics.
Peter R. Holland has pointed out that, earlier in 1927, Einstein had actually submitted a preprint with a similar proposal but, not convinced, had withdrawn it before publication. According to Holland, failure to appreciate key points of the de Broglie–Bohm theory has led to confusion, the key point being "that the trajectories of a many-body quantum system are correlated not because the particles exert a direct force on one another ("à la" Coulomb) but because all are acted upon by an entity – mathematically described by the wavefunction or functions of it – that lies beyond them". This entity is the quantum potential.
After publishing a popular textbook on Quantum Mechanics that adhered entirely to the Copenhagen orthodoxy, Bohm was persuaded by Einstein to take a critical look at von Neumann's theorem. The result was 'A Suggested Interpretation of the Quantum Theory in Terms of "Hidden Variables" I and II' 1952. It was an independent origination of the pilot wave theory, and extended it to incorporate a consistent theory of measurement, and to address a criticism of Pauli that de Broglie did not properly respond to; it is taken to be deterministic (though Bohm hinted in the original papers that there should be disturbances to this, in the way Brownian motion disturbs Newtonian mechanics). This stage is known as the "de Broglie–Bohm Theory" in Bell's work 1987 and is the basis for 'The Quantum Theory of Motion' 1993.
This stage applies to multiple particles, and is deterministic.
The de Broglie–Bohm theory is an example of a hidden variables theory. Bohm originally hoped that hidden variables could provide a local, causal, objective description that would resolve or eliminate many of the paradoxes of quantum mechanics, such as Schrödinger's cat, the measurement problem and the collapse of the wavefunction. However, Bell's theorem complicates this hope, as it demonstrates that there can be no local hidden variable theory that is compatible with the predictions of quantum mechanics. The Bohmian interpretation is causal but not local.
Bohm's paper was largely ignored or panned by other physicists. Albert Einstein, who had suggested that Bohm search for a realist alternative to the prevailing Copenhagen approach, did not consider Bohm's interpretation to be a satisfactory answer to the quantum nonlocality question, calling it "too cheap", while Werner Heisenberg considered it a "superfluous 'ideological superstructure' ". Wolfgang Pauli, who had been unconvinced by de Broglie in 1927, conceded to Bohm as follows:
I just received your long letter of 20th November, and I also have studied more thoroughly the details of your paper. I do not see any longer the possibility of any logical contradiction as long as your results agree completely with those of the usual wave mechanics and as long as no means is given to measure the values of your hidden parameters both in the measuring apparatus and in the observe system. As far as the whole matter stands now, your ‘extra wave-mechanical predictions’ are still a check, which cannot be cashed.
He subsequently described Bohm's theory as "artificial metaphysics".
According to physicist Max Dresden, when Bohm's theory was presented at the Institute for Advanced Study in Princeton, many of the objections were ad hominem, focusing on Bohm's sympathy with communists as exemplified by his refusal to give testimony to the House Un-American Activities Committee.
In 1979, Chris Philippidis, Chris Dewdney and Basil Hiley were the first to perform numeric computations on the basis of the quantum potential to deduce ensembles of particle trajectories. Their work renewed the interests of physicists in the Bohm interpretation of quantum physics.
Eventually John Bell began to defend the theory. In "Speakable and Unspeakable in Quantum Mechanics" 1987, several of the papers refer to hidden variables theories (which include Bohm's).
The trajectories of the Bohm model that would result for particular experimental arrangements were termed "surreal" by some. Still in 2016, mathematical physicist Sheldon Goldstein said about Bohm's theory: “There was a time when you couldn’t even talk about it because it was heretical. It probably still is the kiss of death for a physics career to be actually working on Bohm, but maybe that’s changing.”
Bohmian mechanics.
This term is used to describe the same theory, but with an emphasis on the notion of current flow, which is determined on the basis of the quantum equilibrium hypothesis that the probability follows the Born rule. The term "Bohmian mechanics" is also often used to include most of the further extensions past the spin-less version of Bohm. While de Broglie–Bohm theory has Lagrangians and Hamilton-Jacobi equations as a primary focus and backdrop, with the icon of the quantum potential, Bohmian mechanics considers the continuity equation as primary and has the guiding equation as its icon. They are mathematically equivalent in so far as the Hamilton-Jacobi formulation applies, i.e., spin-less particles. The papers of Dürr et al. popularized the term.
All of non-relativistic quantum mechanics can be fully accounted for in this theory.
Causal interpretation and ontological interpretation.
Bohm developed his original ideas, calling them the "Causal Interpretation". Later he felt that "causal" sounded too much like "deterministic" and preferred to call his theory the "Ontological Interpretation". The main reference is 'The Undivided Universe' Hiley 1993.
This stage covers work by Bohm and in collaboration with Jean-Pierre Vigier and Basil Hiley. Bohm is clear that this theory is non-deterministic (the work with Hiley includes a stochastic theory). As such, this theory is not, strictly speaking, a formulation of the de Broglie–Bohm theory. However, it deserves mention here because the term "Bohm Interpretation" is ambiguous between this theory and the de Broglie–Bohm theory.
An in-depth analysis of possible interpretations of Bohm's model of 1952 was given in 1996 by philosopher of science Arthur Fine.
R,v MA

</doc>
<doc id="54725" url="https://en.wikipedia.org/wiki?curid=54725" title="Scotland Yard (band)">
Scotland Yard (band)

Scotland Yard is a pop rock group from Los Angeles, California: Scotland Yard began when Chris Hill began demoing songs in 1989 for a female fronted new wave pop group. In 1991 Kim Cahill responded to an out of date ad for a singer in a local industry trade magazine. The edgy pop rock sound from the duo would catch an ear in the Los Angeles and Orange county music scene.
Career.
The addition of a live band and constant touring would break the band out of its Orange County home turf. Numerous compilations and college radio air play helped the band build a devoted following across the USA and overseas. Local live shows featured casino nights with play money, food and prizes, raffle giveaways and swag bags for the audiences. The band licensed merchandise for everything from T-shirts to candy bars.
After the second CD release, the band's revolving lineup and shifting influences lead to Hill and Cahill parting ways. The group was dissolved in January 2004 after the recording of their third and final studio collaboration.
The last single release, "Wake Up", released under Cahill's name, with no promotion met with limited success, and the group ended their label Interpol Records.
Notable bands that Scotland Yard opened for are <br>Lenny Kravitz, Lit, Berlin, Missing Persons, Dave Wakling (General Public),<br> Gene Loves Jezabel, John Easdale (Dramarama), and Martha Davis (The Motels).
The Band won one of the early contests on www.Garageband.com peaking at #1 in Pop out of 34,000 songs and were number #1 on Australian radio's "World Underground charts" for more than 365 weeks. The Band's music was also featured on MTV's "Undressed" episode 432 and a Friends episode in Italy.

</doc>
<doc id="54727" url="https://en.wikipedia.org/wiki?curid=54727" title="Scotland Yard (board game)">
Scotland Yard (board game)

Scotland Yard is a board game in which a team of players, as police, cooperate to track down a player controlling a criminal around a board representing the streets of London. It is named after Scotland Yard, the headquarters of London's Metropolitan Police Service. "Scotland Yard" is an asymmetric board game, with the detective players cooperatively solving a variant of the pursuit-evasion problem. The game is published by Ravensburger in most of Europe and Canada and by Milton Bradley in the United States. It received the "Spiel des Jahres" (Game of the Year) award in 1983. A sequel to Scotland Yard was released called "Mister X".
Gameplay.
One player controls "Mr. X", a criminal whose location is only revealed periodically, and the other players each control a detective, which is always present on the board.
All players start with a number of tokens allowing them to use the following methods of transport:
Each player (Mr. X and the detectives) draws one of 18 possible cards which show where a player has to start, with Mr. X always drawing first. The locations on these cards are spaced far enough apart to ensure that Mr. X cannot be caught in the first round of play. There are a total of 199 locations on the board.
Each detective begins with a total of 22 tokens. Once each transport token is used by a detective, it is turned over to Mr. X, effectively giving him unlimited transport. As he makes his moves, he writes them in a log book or any book and covers them with the tokens he uses, so that the detectives have clues as to his whereabouts. Mr. X also has a number of 'valid on any transport' black tokens equal to the number of detectives in play (in the Milton Bradley version this is always five), and two 'move twice this turn' cards. The water routes require a black token; when one of these is played, the detectives must consider whether or not it is being used to hide a river trip. The detectives must move in the same order each turn so their moves have to be well thought out.
At five specific times during the game, Mr. X has to reveal his current position. Detectives will take this opportunity to refine their search and, if possible, plan ways to encircle him. From each known position, the types of transport used by Mr. X limit the number of possible locations he may be standing in, which provides useful information to detectives (as well as preventing some types of cheating by the fugitive player).
The game is won by the detectives if any of them lands on Mr. X's current location or vice versa. Mr. X can win by avoiding capture until all detectives can no longer move, due to either exhausting their token supplies or reaching a space for which they have no more usable tokens.
Although the game says it is for 3-6 players many play this game with only 2 players. The police, when controlled by 1 person, are far more coordinated and have a better chance of catching Mr. X. When 3-5 people are playing as the police they have to work as a team and coordinate their moves which can be difficult, especially when one player wants to play a hunch.
The contents of the game contain:
There are two main board editions, one typically associated with Milton Bradley, and another typically associated with Ravensburger. The primary difference between these is in the numbering of the stations: five stations are numbered differently, with 108 missing from the Milton Bradley boards, and 200 missing from the Ravensburger boards. There are also minor differences in the routes, such as a bus line between stations 198 and 199 that is changed to a taxi line in later editions, and the removal of a taxi line between stations 13 and 14 sometime after the renumbering.
Alternative versions.
The game has been adapted to take place on maps of different cities. "Scotland Yard Tokyo", also distributed by Ravensburger, takes place on the streets of Tokyo, with the major difference being game aesthetics. "Scotland Yard: Swiss Edition" uses the same gameplay and is set in Switzerland, with the addition of more boat routes and ski areas available only to Mr. X.
"NY Chase" is a version based on New York City. In this version, detectives do not hand their used tokens over, and they have access to roadblocks and a helicopter, tilting the game more in favour of those playing as detectives.
A faster travel version called "Die Jagd Nach Mister X" exists that functions quite differently. In this version, Mr. X's location is only hidden when a black travel token is used, and the game is essentially an open chase around London. Evasion is accomplished with black tokens and using the fastest travel to distant locations. In this version, each player takes turns as Mr. X, and points collected (in the form of the detectives' used travel tokens) determine the overall winner.
Alternative rules.
Three years after the game's publication, Alain Munoz and Serge Laget posted an article in the French magazine "Jeux & Stratégie" suggesting alternative rules to balance and expand the game.

</doc>
<doc id="54728" url="https://en.wikipedia.org/wiki?curid=54728" title="888">
888

__NOTOC__
Year Eight hundred four score and eight (DCCCLXXXVIII) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="54732" url="https://en.wikipedia.org/wiki?curid=54732" title="Kensington">
Kensington

Kensington is a district within the Royal Borough of Kensington and Chelsea in west London. The north east is taken up by Kensington Gardens, once private as the name suggests, but today a public park which has Italian and Dutch gardens, public buildings such as the Albert Memorial, the Serpentine Gallery and Speke's monument.
Its commercial heart is Kensington High Street. This affluent and densely populated area contains the major museum district of South Kensington, which has the Royal Albert Hall for music and nearby Royal College of Music. The area is home to many of London's European embassies. Cementing Kensington's indicators of connections with France, the area has the Lycée Français Charles de Gaulle, French Consulate, French Embassy Cultural Department and the London Oratory on the borders with diminutive Knightsbridge.
Name.
The first mention of the area is in the Domesday Book of 1086, where it was written in Latin as "Chenesitone", which has been interpreted to have originally been "Kenesignetun" (Kenesigne's land or meadows) in Anglo-Saxon. A variation may be "Kesyngton", in 1396.
History.
The manor of Kensington, Middlesex, was granted by William I to Geoffrey de Montbray or Mowbray, bishop of Coutances, one of his inner circle of advisors and one of the wealthiest men in post-Conquest England. He in turn granted the tenancy of Kensington to his vassal Aubrey de Vere I, who was holding the manor in 1086, according to Domesday Book. The bishop's heir, Robert de Mowbray, rebelled against William Rufus and his vast barony was declared forfeit. Aubrey de Vere I had his tenure converted to a tenancy in-chief, holding Kensington after 1095 directly of the crown. He granted land and church there to Abingdon Abbey at the deathbed request of his young eldest son, Geoffrey. As the Veres became the earls of Oxford, their estate at Kensington came to be known as Earls Court, while the Abingdon lands were called Abbots Kensington and the church St Mary Abbots.
Geography.
The focus of the area is Kensington High Street, a busy commercial centre with many shops, typically upmarket. The street was declared London's second best shopping street in February 2005 thanks to its range and number of shops. However, since October 2008 the street has faced competition from the Westfield shopping centre in nearby White City.
Kensington's second group of non-residential buildings is at South Kensington, where several streets of small to medium-sized shops and service businesses are close to South Kensington tube station. This is also the southern end of Exhibition Road, the thoroughfare that serves the area's museums and educational institutions.
The edges of Kensington are not well-defined; in particular, the southern part of Kensington has conflicting and complex borders with Chelsea whether electoral or postal definitions are used, and has similar architecture. To the west, a border is kept along the line of the Counter Creek marked by the West London railway line and Earl's Court Road further south into other London districts. To the north, the only obvious dividing line is Holland Park Avenue, to the north of which is the district of Notting Hill which is part of the traditional definitions of Kensington and a subset of North Kensington.
In the north east, the large Royal Park of Kensington Gardens (contiguous with its eastern neighbour, Hyde Park) is a green buffer. The other main green area in Kensington is Holland Park, just north of Kensington High Street, a minority of roads have small residential garden squares.
South Kensington is of the same, largely private housing, use as central Kensington; the more economically and socially nationally reflective North Kensington and West Kensington are diverse and lack the tourism of the rest of Kensington.
Kensington is, in general, an extremely affluent area, a trait that it now shares with its neighbour to the south, Chelsea. The area has some of London's most expensive streets and garden squares, including Edwardes Square, most of the Holland Park neighbourhood and Wycombe Square, private redevelopments in Regency architecture. In early 2007, houses sold in Upper Phillimore Gardens for in excess of £20 million. Adjoining neighbourhoods have residential areas and have accordingly been subdivided or have overlapping district names all, unlike Kensington, without an ancient parish predecessor: Knightsbridge, Brompton, Belgravia, Holland Park and Notting Hill.
Kensington is also very densely populated; it forms part of the most densely populated local government district (the Royal Borough of Kensington and Chelsea) in the United Kingdom. This high density is not formed from high-rise buildings; instead, it has come about through the subdivision of large mid-rise Georgian and Victorian terraced houses (generally of some four to six floors) into flats. Unlike northern extremities of the Borough, Kensington lacks high-rise buildings except for the Holiday Inn's London Kensington Forum Hotel in Cromwell Road, which is a 27-storey building. Typical for the region are the iconic buildings with historical value, as most of them are located at Queen's Gate street. For their special architectural or historic interest some are listed as Grade II buildings.
Notable attractions and institutions in Kensington (or South Kensington) include: Kensington Palace in Kensington Gardens, the Royal Albert Hall opposite the Albert Memorial in Hyde Park, the Royal College of Music, the Natural History Museum, the Science Museum, the Victoria and Albert Museum, Heythrop College, Imperial College, London, the Royal College of Art and Kensington and Chelsea College. The Olympia exhibition hall is just over the western border in West Kensington.
Administration.
Kensington is part of the Royal Borough of Kensington and Chelsea, and lies within the Kensington parliamentary constituency.
Newspapers and TV channel.
The head office of newspaper group DMGT is located in Northcliffe House in Kensington, which is the office part of the large Barkers building. In addition to housing the offices for the DMGT newspapers "Daily Mail", "Mail on Sunday" and "Metro", Northcliffe House also accommodates the offices of the four newspapers owned by Evgeny Lebedev: "The Independent", "The Independent on Sunday", the "I", and the "Evening Standard".
The building also houses Lebedev's TV channel London Live, with its news studio situated in part of the former department store, using St Mary Abbots church and Kensington Church Street as live backdrop.
Transport.
Kensington is crossed east-west by three main roads, the most important of which is the A4 or Cromwell Road which connects it to both Central London and Heathrow Airport. To the north is the mostly parallel Kensington Road (of which Kensington High Street forms a large part), linking central London and Hammersmith to the area. To the south is Fulham Road, which connects South Kensington with Fulham to the southwest. North-south connections are not as well-developed and there is no obvious single north-south route through the area.
Kensington is well served by public transport. Most of Kensington is served by three stations in the Travelcard Zone 1: High Street Kensington, Gloucester Road and South Kensington. All three are served by the Circle line which connects them to London's railway terminals. The District line also serves all three stations, albeit on different branches; it links the latter two to Westminster and the City. The Piccadilly line also links South Kensington and Gloucester Road to the West End in about 10 minutes, and in the other direction to Heathrow Airport in about 40 minutes. In addition Kensington (Olympia) in Travelcard Zone 2 serves the western part of Kensington, with District line trains to Earl's Court and High Street Kensington. Nearby West Kensington station takes its name from the former boundaries with Hammersmith and is not in the Borough.
A number of local bus services link Kensington into the surrounding districts, and key hubs are Kensington High Street and South Kensington station. These bus services were improved in frequency and spread from 2007 until 2010 when the western extension of the London congestion charge area existed (which required drivers of cars and vans during the charging hours Monday-Friday to pay a daily fee of £8).

</doc>
<doc id="54737" url="https://en.wikipedia.org/wiki?curid=54737" title="Letter of marque">
Letter of marque

In the days of fighting sail, a letter of marque and reprisal was a government license authorizing a person (known as a "privateer") to attack and capture enemy vessels and bring them before admiralty courts for condemnation and sale. Cruising for prizes with a letter of marque was considered an honorable calling combining patriotism and profit, in contrast to unlicensed piracy, which was universally reviled. In addition to the term lettre de marque, the French sometimes used the term lettre de course for their letters of marque. "Letter of marque" was sometimes used to describe the vessel used: a "letter of marque" generally refers to a lumbering square-rigged cargo carrier that might pick up a prize if the opportunity arose. A "privateer" was a fast and weatherly fore-and-aft-rigged vessel heavily armed and heavily crewed, intended exclusively for fighting.
A "letter of marque and reprisal" would include permission to cross an international border to effect a reprisal (take some action against an attack or injury) authorized by an issuing jurisdiction to conduct reprisal operations outside its borders.
Etymology.
Old English "mearc", from Germanic "*mark-" ‘boundary; boundary marker’, from Proto-Indo-European "*merǵ-" ‘boundary, border’.
French, from Provençal "marca", from "marcar" ‘seize as a pledge‘
Nomenclature history.
According to the "Oxford English Dictionary", the first recorded use of "letters of marque and reprisal" was in an English statute in 1354 during the reign of Edward III. The phrase referred to "a licene granted by a sovereign to a subject, authorizing him to make reprisals on the subjects of a hostile state for injuries alleged to have been done to him by the enemy's army."
Early history.
During the Middle Ages, armed private vessels enjoying their sovereign's tacit consent, if not always an explicit formal commission, regularly raided shipping of other nations, as in the case of Francis Drake's attacks on Spanish shipping, of which Elizabeth I (despite protestations of innocence) took a share. Grotius's 1604 seminal work on international law, "De Iure Praedae" (Of The Law of Prize and Booty), was an advocate's brief defending Dutch raids on Spanish and Portuguese shipping.
King Henry III of England first issued what later became known as privateering commissions in 1243. These early licences were granted to specific individuals to seize the king’s enemies at sea in return for splitting the proceeds between the privateers and the crown.
The letter of marque and reprisal first arose in 1295, 50 years after wartime privateer licences were first issued. According to Grotius, letters of marque and reprisal were akin to a "private war", a concept alien to modern sensibilities but related to an age when the ocean was lawless and all merchant vessels sailed armed for self-defense. A reprisal involved seeking the sovereign's permission to exact private retribution against some foreign prince or subject. The earliest instance of a licensed reprisal recorded in England was in the year 1295 under the reign of Edward I. The notion of reprisal, and behind it that just war involved avenging a wrong, clung to the letter of marque until 1620 in England, in that to apply for one a shipowner had to submit to the Admiralty Court an estimate of actual losses.
Licensing privateers during wartime became widespread in Europe by the 16th Century, when most countries began to enact laws regulating the granting of letters of marque and reprisal. Business could be very profitable, during the eight years of the American Revolutionary War ships from the tiny island of Guernsey carrying letter of marque captured French and American vessels to the value of £900,000 and continued to operate during the Napoleonic Wars.
Although privateering commissions and letters of marque were originally distinct legal concepts, such distinctions became purely technical by the eighteenth century. The United States Constitution, for instance, states that "The Congress shall have Power To ... grant Letters of marque and reprisal ...”, without separately addressing privateer commissions.
During the Napoleonic Wars and the War of 1812, and the American Independence War it was common to distinguish verbally between privateers (also known as private ships of war) on the one hand, and armed merchantmen, which were referred to as "letters of marque", on the other, though both received the same commission. The "Sir John Sherbrooke" (Halifax) was a privateer; the "Sir John Sherbrooke" (Saint John) was an armed merchantman. The East India Company arranged for letters of marque for its East Indiamen such as the "Lord Nelson", not so that they could carry cannons to fend off warships, privateers, and pirates on their voyages to India and China—that they could do without permission—but so that, should they have the opportunity to take a prize, they could do so without being guilty of piracy. Similarly, the "Earl of Mornington", an East India Company packet ship of only six guns, too carried a letter of marque.
In July 1793, the East Indiamen "Royal Charlotte", "Triton", and "Warley" participated in the capture of Pondichéry by maintaining a blockade of the port. Afterwards, as they were on their way to China, the same three East Indiamen participated in an action in the Straits of Malacca. They came upon a French frigate, with some six or seven of her prizes, replenishing her water casks ashore. The three British vessels immediately gave chase. The frigate fled towards the Sunda Strait. The Indiamen were able to catch up with a number of the prizes, and, after a few cannon shots, were able to retake them. Had they not carried letters of marque, such behaviour might well have qualified as piracy. Similarly, on 10 November 1800 the East Indiaman "Phoenix" captured the French privateer "General Malartic", under Jean-Marie Dutertre, an action made legal by a letter of marque.
Applying for, and legal effect of, letter of marque.
The procedure for issuing letters of marque and the issuing authority varied by time and circumstance. In colonial America, for instance, colonial governors issued them in the name of the king. During the American Revolution, first the state legislatures, then both the states and the Continental Congress, then, after ratification of the Constitution, Congress authorized and the President signed letters of marque. A shipowner would send in an application stating the name, description, tonnage, and force (armaments) of the vessel, the name and residence of the owner, and the intended number of crew, and tendered a bond promising strict observance of the country's laws and treaties and of international laws and customs. The commission was granted to the vessel, not to its captain, often for a limited time or specified area, and stated the enemy upon whom attacks were permitted. For instance, during the Second Barbary War President James Madison authorized the Salem, Mass., brig "Grand Turk" to cruise against "Algerine vessels, public or private, goods and effects, of or belonging to the Dey of Algiers". (Interestingly, this particular commission was never put to use, as it was issued the same day the treaty was signed ending the U.S. involvement in the war—July 3, 1815.)
A letter of marque and reprisal in effect converted a private merchant vessel into a naval auxiliary. A commissioned privateer enjoyed the protection and was subject to the obligations of the laws of war. If captured, the crew was entitled to honorable treatment as prisoners of war, while without the licence they were deemed mere pirates "at war with all the world," criminals who were properly hanged.
For this reason, enterprising maritime raiders commonly took advantage of "flag of convenience" letters of marque, shopping for cooperative governments to license and legitimize their depredations. French/Irishman Luke Ryan and his lieutenants in just over two years commanded six vessels under the flags of three different nations and on opposite sides in the same war. Likewise the notorious Lafitte brothers in New Orleans cruised under letters of marque secured by bribery from corrupt officials of tenuous Central American governments, to cloak plunder with a thin veil of legality.
Adjudicating captures, invalid letter of marque, or illegal cruelty.
The letter of marque by its terms required privateers to bring captured vessels and their cargoes before admiralty courts of their own or allied countries for condemnation. Applying the rules and customs of prize law, the courts decided whether the letter of marque was valid and current, and whether the captured vessel or its cargo in fact belonged to the enemy (not always easy, when flying false flags was common practice), and if so the prize and its cargo were "condemned," to be sold at auction with the proceeds divided among the privateer's owner and crew. A prize court's formal condemnation was required to transfer title; otherwise the vessel's previous owners might well reclaim her on her next voyage, and seek damages for the confiscated cargo.
Often questions arose as to the legitimacy of the letter of marque in the case of divided sovereignty during civil wars. An English court, for instance, refused to recognize the letters of marque issued by rebellious Ireland under James II, and hanged eight privateer captains as pirates. Seventy-nine years later during the American Civil War, the Union charged officers and crew of the Confederate privateer "Savannah" with piracy, calling their letter of marque invalid since the Union refused to acknowledge the breakaway Confederacy as a sovereign nation. The case resulted in a hung jury, and after Confederate President Jefferson Davis threatened to retaliate by hanging one Union officer for each executed Confederate privateer, the Union relented and thereafter treated Confederate privateersmen honorably as prisoners of war.
Privateers were also required by the terms of their letters of marque to obey the laws of war, honour treaty obligations (avoid attacking neutrals), and in particular to treat captives as courteously and kindly as they safely could. If they failed to live up to their obligations, the Admiralty courts could — and did — revoke the letter of marque, refuse to award prize money, forfeit bonds, or even award tort (personal injury) damages against the privateer's officers and crew.
Abolition of privateering.
Nations often agreed by treaty to forgo privateering, as England and France repeatedly did starting with the diplomatic overtures of Edward III in 1324; privateering nonetheless recurred in every war between them for the next 500 years. 
Benjamin Franklin had attempted to persuade the French to lead by example and stop issuing letters of marque to their corsairs, but the effort foundered when war loomed with Britain once again. The French Convention did forbid the practice, but it was reinstated after the Thermidorian Reaction, in August 1795; on 26 September 1797, the Ministry of the Navy was authorized to sell small ships to private parties for this purpose.
Finally, after the Congress of Paris at the end of the Crimean War, seven European nations signed the Paris Declaration of 1856 renouncing privateering, and forty-five more eventually joined them, which in effect abolished privateering worldwide. The United States was not a signatory to that declaration. Despite the attempt to end privateering around the world, nations continued issuing letters of marque. In 1879 at the beginning of the War of the Pacific, Bolivia issued letters of marque to any vessels willing to fight for them. At the time Bolivia was under threat from Chile's fleet but had no navy.
20th century.
In December 1941 and the first months of 1942, Goodyear commercial L class blimp "Resolute" operating out of Moffett Field in Sunnyvale, California, flew anti-submarine patrols. As the civilian crew was armed with a rifle, many thought this made the ship a privateer, and that she and sister commercial blimps were operated under letter of marque until the Navy took over operation. Without congressional authorization, the Navy would not have been able to legally issue any letters of marque.
21st-century American reconsideration of letters of marque.
Article 1 of the United States Constitution lists issuing letters of marque and reprisal in Section 8 as one of the enumerated powers of Congress, alongside the power to tax and to declare War. However, since the American Civil War, the United States as a matter of policy has consistently followed the terms of the 1856 Paris Declaration forbidding the practice. The United States has not legally commissioned any privateers since 1815, although the status of submarine-hunting Goodyear airships in the early days of World War II created significant confusion. Various accounts refer to airships "Resolute" and "Volunteer" as operating under a "privateer status", but Congress never authorized a commission, nor did the President sign one. 
The issue of marque and reprisal was raised before Congress after the September 11 attacks and again on July 21, 2007, by Congressman Ron Paul. The attacks were defined as acts of "air piracy" and the "Marque and Reprisal Act of 2001" was introduced, which would have granted the president the authority to use letters of marque and reprisal against the specific terrorists, instead of warring against a foreign state. The terrorists were compared to pirates in that they are difficult to fight by traditional military means. Paul on April 15, 2009, also advocated the use of letters of marque to address the issue of Somali pirates operating in the Gulf of Aden. However, the bills Paul introduced were not enacted into law.
Former New Mexico Governor and 2012 Libertarian Presidential candidate Gary Johnson quipped in his reddit AMA that letters of marque and reprisal would be an effective way to apprehend Lord's Resistance Army leader Joseph Kony instead of using US troops.

</doc>
<doc id="54738" url="https://en.wikipedia.org/wiki?curid=54738" title="Interpretations of quantum mechanics">
Interpretations of quantum mechanics

Interpretations of quantum mechanics deal with two problems: how to relate the mathematical formalism of quantum mechanics to empirical observations; and how to understand that relation in physical and metaphysical terms and in ordinary language. The mathematico-empirical relation, though needing experience for its understanding, is relatively well agreed upon, and is supported by rigorous and thorough experimental testing. But the physical and metaphysical questions give rise to doubt and contention over what is the place and character of causality in atomic and sub-atomic physics, how far quantum mechanics is limited to what can be known with certainty, what is physical reality, and other questions.
History of interpretations.
The definition of quantum theorists' terms, such as "wavefunctions" and "matrix mechanics", progressed through many stages. For instance, Erwin Schrödinger originally viewed the electron's wavefunction as its charge density smeared across the field, whereas Max Born reinterpreted it as the electron's probability density distributed across the field. There was detailed and vigorous debate about this and many other related questions at the fifth Solvay Conference in 1927. Debate has continued right to present times.
The eponym "Copenhagen interpretation" is often used; it seems to have been invented by Heisenberg in 1955. Its exact meaning is not agreed upon. It is sometimes said to refer to the "orthodox" or "standard" interpretation. One may look cautiously for its meaning in the opinions of Bohr, Heisenberg, Born, and perhaps Dirac, especially those offered before 1928.
Amongst more recent interpretational concepts are quantum decoherence and many worlds.
During most of the 20th century, the questions of "interpretation" of quantum mechanics mostly revolved around how to interpret "reduction of the wave function", a concept also known as "collapse". Proponents of either "pilot-wave" (de Broglie-Bohm-like) or "many-worlds" (Everettian) interpretations tend to emphasize how their respective camps were intellectually marginalized throughout the 1950s to 1980s.
Since the 1990s, however, there has been a resurgence of interest in theories that avoid the concept of reduction of the wave function. The "Stanford Encyclopedia" as of 2015 groups interpretations of quantum mechanics into
"Bohmian mechanics" (pilot-wave theories),
"collapse theories",
"many-worlds interpretations",
"modal interpretation"
and "relational interpretations"
as classes of into which most suggestions may be grouped.
A major metaphysical interpretational movement in the last one-third of the twentieth century was initiated by John Stewart Bell with a 1964 paper. It examines the status of physical causality in the light of quantum mechanics. It draws metaphysical conclusions from experiments on quantum entanglement.
As a rough guide development of the mainstream view during the 1990s to 2000s, consider the "snapshot" of opinions collected in a poll by Schlosshauer et al. at the 2011 "Quantum Physics and the Nature of Reality" conference of July 2011.
The authors reference a similarly informal poll carried out by Max Tegmark at the "Fundamental Problems
in Quantum Theory" conference in August 1997.
The main conclusion of the authors is that "the Copenhagen interpretation still reigns supreme", receiving the most votes in their poll (42%), besides the rise to mainstream notability of the 
many-worlds interpretations:
Nature of interpretation.
An interpretation of quantum mechanics is a conceptual or argumentative way of relating between:
Quantum mechanics is concerned with physical phenomena that require observation of the very smallest physical things and processes, especially involving atoms and quantum jumps. Large physical things and processes, such as chairs and tables, and actions by persons, are usually observed through millions of quantum jumps, jointly perceived, in ordinary sensations such as sight, touch, and hearing. In contrast, quantum mechanical phenomena are characteristically investigated through single quantum jumps, which, in general, are not perceptible except by use of special laboratory devices. Ordinary thinking is not immediately adapted to such stringent specialization. Special reasoning is needed for quantum mechanics, but eventually such reasoning is exposed to the demands of ordinary thinking. Interpretations of quantum mechanics concern the structure of such special reasoning, and its exposure to ordinary thinking.
An example of a question of interpretation is "are bras and kets different, and if so, how and why?" Another example of a question of interpretation is "the Schrödinger equation is deterministic but quantum mechanical observation is not; how can this be?"
Instrumentalism is a view that the proper province of science is only the directly physical questions, namely the relations between (1) the mathematical formalism and (2) the experimental facts. It vigorously discards attempts at (3) metaphysical understanding of the world, and does not recognize the problems of (4) abductive reasoning. The instrumentalist view is carried by the famous quote of David Mermin, "Shut up and calculate". One might say that instrumentalism is a reaction to extravagances of interpretation.
Opinions vary: J.S. Bell was interested in debate along Bohr–Einstein lines, but recognized that important contributors to quantum mechanics were not. He wrote of "why bother?'ers", for example Paul Dirac. Some writers differ from Bell's opinion on the leanings of Dirac. Bell himself was responsible for a vigorous area of metaphysical interpretation.
Although Dirac wrote mainly on the physical aspects of orthodox interpretation, he also wrote about the metaphysical debate:
Dirac's final written words on the debate were:
According to Dirac, one of those "more fundamental things" was the issue of renormalization. More explicitly, he refers to "the renormalized kind of quantum theory with which physicists are working today":
Other physicists who might be regarded as "why bother?'ers" are John Ward, Nico van Kampen, Willis Lamb, Richard Dalitz, and even Richard Feynman.
Some approaches to resolve conceptual problems introduce new mathematical formalism, and so propose additional theories with their interpretations. An example is Bohmian mechanics, which is empirically equivalent with the standard formalisms, but requires extra equations to describe the precise trajectories that it postulates.
Challenges for interpretations.
Interpretations of quantum mechanics attempt to provide a conceptual framework for understanding the many aspects of quantum mechanics which are not easily handled by the conceptual framework used for classical physics:
The mathematical structure of quantum mechanics is based on rather abstract mathematics, like Hilbert spaces. In classical field theory, a physical property at a given location in the field is readily derived. In Heisenberg's formalism, on the other hand, to derive physical information about a location in the field, one must apply a quantum operation to a quantum state, an elaborate mathematical process.
Schrödinger's formalism describes a waveform governing the probability of outcomes across a field. Yet how do we find in a specific location a particle whose wavefunction, a mere probability distribution of existence, spans a vast region of space?
The act of measurement can interact with the system state in peculiar ways, as found in double-slit experiments. The Copenhagen interpretation holds that the myriad probabilities across a quantum field are unreal, yet that the act of observation/measurement collapses the wavefunction and sets a single possibility to become real. Yet quantum decoherence grants that all the possibilities can be real, and that the act of observation/measurement sets up new subsystems.
A key interpretational question is posed by Dirac's famous sentences about quantum interference: "Each photon then interferes only with itself. Interference between two different photons never occurs." Dirac stops short of repeating this statement for objects other than photons, such as electrons, contenting himself with saying "... for particles even as light as electrons the associated wave frequency is so high that it is not easy to demonstrate interference." Dirac was of course well familiar with the concept of electron diffraction by crystals, which is usually regarded as an interference phenomenon. The comments of Dirac surrounding these sentences indicate that he considers them to be interpretive. No experiment can directly test them, an actual particular photon being detectable only once.
Quantum entanglement, as illustrated in the EPR paradox, seemingly violates principles of local causality.
Complementarity holds that no set of classical physical concepts can simultaneously refer to all properties of a quantum system. For instance, wave description "A" and particulate description "B" can each describe a quantum system "S", but not simultaneously. Still, complementarity does not usually imply that classical logic is at fault (although Hilary Putnam took such view in "Is logic empirical?"); rather, the composition of physical properties of "S" does not obey the rules of classical propositional logic when using propositional connectives (see "Quantum logic"). As now well known, the "origin of complementarity lies in the non-commutativity of operators" that describe quantum objects (Omnès 1999).
Instrumentalist description.
Any modern scientific theory requires at the very least a description that relates the mathematical formalism to experimental practice and prediction. 
In the case of quantum mechanics, the most common description is an assertion of statistical regularity between state preparation processes and measurement processes. That is, if a measurement of a real-value quantity is performed many times, each time starting with the same initial conditions, the outcome is a well-defined probability distribution agreeing with the real numbers; moreover, quantum mechanics provides a computational instrument to determine statistical properties of this distribution, such as its expectation value.
Calculations for measurements performed on a system S postulate a Hilbert space "H" over the complex numbers. When the system S is prepared in a pure state, it is associated with a vector in "H". Measurable quantities are associated with Hermitian operators acting on "H": these are referred to as observables.
Repeated measurement of an observable "A" where S is prepared in state ψ yields a distribution of values. The expectation value of this distribution is given by the expression
This mathematical machinery gives a simple, direct way to compute a statistical property of the outcome of an experiment, once it is understood how to associate the initial state with a Hilbert space vector, and the measured quantity with an observable (that is, a specific Hermitian operator).
As an example of such a computation, the probability of finding the system in a given state formula_2 is given by computing the expectation value of a (rank-1) projection operator.
The probability is then the non-negative real number given by
In the context of quantum mechanics, the "instrumentalist interpretation" refers to the position that there can be no meaningful "interpretation" beyond the mere instrumentalist description, a position often equated with eschewing all interpretation. It is summarized by the sentence "Shut up and calculate!".
Depending on the understanding of the term instrumentalism, the "instrumentalist" or "shut up and calculate"-interpretation may either express pragmatism (the formalism of quantum mechanics has no application outside of the description of predicting experimental outcomes) or radical empiricism or phenomenalism (there is no valid concept of reality outside of perceptual phenomena).
Concerns of Einstein.
An interpretation of the mathematical formalism of quantum mechanics can be characterized by its treatment of some physical or micro-cosmological problems that Einstein saw in Copenhagenism, such as:
To explain these problems, we need to be more explicit about the kind of picture an interpretation provides. To that end we will regard an interpretation as a correspondence between the elements of the mathematical formalism M and the elements of an interpreting structure I, where:
One way of assessing an interpretation is whether the elements of I are regarded as physically real. Hence the bare instrumentalist view of quantum mechanics outlined in the previous section is not an interpretation at all, for it makes no claims about elements of physical reality.
The current usage of realism and completeness originated in the 1935 paper in which Einstein and others proposed the EPR paradox. In that paper the authors proposed the concepts of "element of reality" and of "completeness of a physical theory". They characterised element of reality as a quantity whose value can be predicted with certainty before measuring or otherwise disturbing it, and defined a complete physical theory as one in which every element of physical reality is accounted for by the theory. The paper proposed that an interpretation is complete if every element of the interpreting structure is present in the mathematics. Realism is also a property of each of the elements of the maths; an element is real if it corresponds to something physical in the interpreting structure. For example, in some interpretations of quantum mechanics (such as the many-worlds interpretation) the ket vector associated to the system state is said to correspond to an element of physical reality, while in other interpretations it is not. Einstein was not the active author of the EPR paper, and it did not quite focus on his principal concern, which was about causality.
Determinism is a property characterizing state changes due to the passage of time, namely that the state at a future instant is a uniquely defined mathematical function of the state in the present (see time evolution). It may not always be clear whether a particular interpretation is deterministic or not, as there may not be a clear choice of a time parameter. Moreover, a given theory may have two interpretations, one of which is deterministic and the other not.
Local realism is an attempt to formulate in relevant mathematical terms the subtle physical, micro-cosmological, or metaphysical concept of causality. It has two aspects:
A formulation of local realism in terms of a local hidden variable theory was proposed by John Bell. This is expressed in what is widely called Bell's theorem. This, combined with experimental testing, is regarded by some physicists as restricting the kinds of properties that a quantum theory can have, the primary proposed implication being that quantum mechanics cannot satisfy both the principle of locality and counterfactual definiteness.
Summary of interpretations.
"Reduction of the wave packet".
The Copenhagen interpretation.
The Copenhagen interpretation is the interpretation of quantum mechanics formulated by Niels Bohr and Werner Heisenberg while collaborating in Copenhagen around 1927. For most of the twentieth century it was regarded as the "standard" interpretation. Bohr and Heisenberg extended the probabilistic interpretation of the wavefunction proposed originally by Max Born. The Copenhagen interpretation rejects questions like "where was the particle before I measured its position?" as meaningless. The measurement process picks out exactly one of the many possibilities allowed for by the state's wave function in a manner consistent with the well-defined probabilities that are assigned to each possible state. According to the interpretation, the interaction of an observer or apparatus that is external to the quantum system is the explanation of reduction of the wave function.
Consciousness causes "collapse".
In his treatise "The Mathematical Foundations of Quantum Mechanics", John von Neumann deeply analyzed the so-called measurement problem. He concluded that the entire physical universe could be made subject to the Schrödinger equation (the universal wave function). He also examined the change in the wave function in a process of measurement. Eugene Wigner argued that human experimenter consciousness (or maybe even dog consciousness) was critical for what was called "collapse", but he later abandoned this interpretation.
Variations of such ideas include:
Other physicists have elaborated their own variations of the von Neumann interpretation; including:
Objective collapse theories.
Objective collapse theories differ from the Copenhagen interpretation in regarding both the wavefunction and the process of collapse as ontologically objective. In objective theories, collapse occurs randomly ("spontaneous localization"), or when some physical threshold is reached, with observers having no special role. Thus, they are realistic, indeterministic, no-hidden-variables theories. The mechanism of collapse is not specified by standard quantum mechanics, which needs to be extended if this approach is correct, meaning that Objective Collapse is more of a theory than an interpretation. Examples include the Ghirardi-Rimini-Weber theory and the Penrose interpretation.
Many worlds theories.
The many-worlds interpretation is an interpretation of quantum mechanics in which a universal wavefunction obeys the same deterministic, reversible laws at all times; in particular there is no (indeterministic and irreversible) wavefunction collapse associated with measurement. The phenomena associated with measurement are claimed to be explained by decoherence, which occurs when states interact with the environment producing entanglement, repeatedly splitting the universe into mutually unobservable alternate histories—distinct universes within a greater multiverse. In this interpretation the wavefunction has objective reality.
Many minds.
The many-minds interpretation of quantum mechanics extends the many-worlds interpretation by proposing that the distinction between worlds should be made at the level of the mind of an individual observer.
Hidden variables.
Pilot-wave theories.
The de Broglie–Bohm or "pilot wave" theories form a class of interpretations of quantum mechanics based on a theory of Louis de Broglie later extended by David Bohm. 
Particles, which always have positions, are guided by the wavefunction. The wavefunction evolves according to the Schrödinger wave equation, and the wavefunction never collapses. The theory takes place in a single space-time, is non-local, and is deterministic. The simultaneous determination of a particle's position and velocity is subject to the usual uncertainty principle constraint. The theory is considered to be a hidden variable theory, and by embracing non-locality it satisfies Bell's inequality. The measurement problem is resolved, since the particles have definite positions at all times. The appearance of collapse is explained as phenomenological.
Time-symmetric theories.
Several theories have been proposed which modify the equations of quantum mechanics to be symmetric with respect to time reversal. (E.g. see Wheeler–Feynman time-symmetric theory). This creates retrocausality: events in the future can affect ones in the past, exactly as events in the past can affect ones in the future. In these theories, a single measurement cannot fully determine the state of a system (making them a type of hidden variables theory), but given two measurements performed at different times, it is possible to calculate the exact state of the system at all intermediate times. The collapse of the wavefunction is therefore not a physical change to the system, just a change in our knowledge of it due to the second measurement. Similarly, they explain entanglement as not being a true physical state but just an illusion created by ignoring retrocausality. The point where two particles appear to "become entangled" is simply a point where each particle is being influenced by events that occur to the other particle in the future.
Not all advocates of time-symmetric causality favour modifying the unitary dynamics of standard quantum mechanics. Thus a leading exponent of the two-state vector formalism, Lev Vaidman, highlights how well the two-state vector formalism dovetails with Hugh Everett's many-worlds interpretation.
Transactional Interpretation.
The transactional interpretation of quantum mechanics (TIQM) by John G. Cramer is an interpretation of quantum mechanics inspired by the Wheeler–Feynman absorber theory. It describes a quantum interaction in terms of a standing wave formed by the sum of a retarded (forward-in-time) and an advanced (backward-in-time) wave. The author argues that it avoids the philosophical problems with the Copenhagen interpretation and the role of the observer, and resolves various quantum paradoxes.
Stochastic mechanics.
An entirely classical derivation and interpretation of Schrödinger's wave equation by analogy with Brownian motion was suggested by Princeton University professor Edward Nelson in 1966. Similar considerations had previously been published, for example by R. Fürth (1933), I. Fényes (1952), and Walter Weizel (1953), and are referenced in Nelson's paper. More recent work on the stochastic interpretation has been done by M. Pavon. An alternative stochastic interpretation was developed by Roumen Tsekov.
Scale relativity.
An approach closely related to stochastic mechanics is scale relativity developed by Laurent Nottale. The main difference with stochastic mechanics is that the stochastic fluctuations which transform classical mechanics into quantum mechanics are the consequence of the scale relativistic nature of the law of motion. This is a generalisation of Einstein's theory of relativity to include scale transformations. It is scale covariance that makes spacetime fractal and hence particle paths become non-differentiable fractal paths just like quantum paths.
Scale relativity is also more general than Nelson stochastic mechanics since it not only derives the Schrödinger equation from quantum mechanics but also the equations of quantum field theory. Depending of the form of the scale covariance law one gets different theories ranging from standard non-relativistic quantum mechanics, over non-linear non-relativistic Schrödinger equations, to relativistic quantum mechanics (Klein-Gordon & Dirac equation) and various quantum field theories.
Popper's experiment.
Karl Popper took part in the "EPR controversy", by exchanging letters with Einstein, Bell etc. about the issue, and by proposing his own Bell test experiment.
Popper first proposed an experiment that would test indeterminacy in Quantum Mechanics in two works of 1934. However, Einstein wrote a letter to Popper about the experiment in which he raised some crucial objections, causing Popper to admit that his initial idea was "based on a mistake". In the 1950s he returned to the subject and formulated this later experiment, which was finally published in 1982.
Since Popper holds both counterfactual definiteness and locality to be true, it is under dispute whether his view is an interpretation (which is what he claimed) or a modification of Quantum Mechanics (which is what many physicists claim), and, in case of the latter, if it has been empirically refuted or not by Bell test experiments.
Other than that, Popper's theory is a variant of the de Broglie–Bohm theory that interprets the probabilities as a stochastic element in the particle movement instead of uncertainties in their initial position. In this sense it is a position between de Broglie–Bohm and stochastic mechanics, accepting the reality of the wave function like the former (in Popper's view, it is a propensity field), and the stochastic element like the latter (see also note 138 in Popper's autobiography "Unended Quest", where he expresses some sympathy for Nelson's stochastic interpretation).
Information-based interpretations.
Quantum informational approaches have attracted growing support during the 2000s. 
Hagar and Hemmo (2008) even refer to it (critically) as "a new
orthodoxy in the foundations of quantum mechanics."
J. A. Wheeler (1990) with his "It from Bit" ("It": physical entity, "Bit": unit of information) has been described as "the cheerleader of this sort of view".
These approaches have been described as a revival of immaterialism.
Relational quantum mechanics.
The essential idea behind relational quantum mechanics, following the precedent of special relativity, is that different observers may give different accounts of the same series of events: for example, to one observer at a given point in time, a system may be in a single, "collapsed" eigenstate, while to another observer at the same time, it may be in a superposition of two or more states. 
Consequently, if quantum mechanics is to be a complete theory, relational quantum mechanics argues that the notion of "state" describes not the observed system itself, but the relationship, or correlation, between the system and its observer(s). The state vector of conventional quantum mechanics becomes a description of the correlation of some "degrees of freedom" in the observer, with respect to the observed system. However, it is held by relational quantum mechanics that this applies to all physical objects, whether or not they are conscious or macroscopic. Any "measurement event" is seen simply as an ordinary physical interaction, an establishment of the sort of correlation discussed above. Thus the physical content of the theory has to do not with objects themselves, but the relations between them.
An independent relational approach to quantum mechanics was developed in analogy with David Bohm's elucidation of special relativity, in which a detection event is regarded as establishing a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg's uncertainty principle is subsequently avoided.
Quantum Bayesianism.
Interpretations where quantum mechanics is said to describe an observer's knowledge of the world, rather than the world itself. 
Quantum Bayesianism ("QBism") aims at giving a "subjective Bayesian account of quantum probability", to derive quantum mechanics from informational considerations.
This approach has some similarity with Bohr's thinking. Collapse (also known as reduction) is often interpreted as an observer acquiring information from a measurement, rather than as an objective event. These approaches have been appraised as similar to instrumentalism.
Ensemble interpretation.
The ensemble interpretation, or statistical interpretation can be viewed as a minimalist approach; it is a quantum mechanical interpretation that claims to make the fewest assumptions associated with the standard mathematical formalization. At its heart, it takes to the fullest extent the statistical Born rule.
It does not attempt to justify, or otherwise derive, or explain quantum mechanics from any deterministic process, or make any other statement about the real nature of quantum phenomena; it is simply a statement as to the manner of wave function interpretation.
The wave function in this interpretation is not a property of any individual system, it is by its nature a statistical description of a hypothetical "ensemble" of similar systems. The probabilistic nature of quantum mechanical predictions thus follow directly from the construction or scope of the theory rather than from any intrinsic property of nature.
Probably the most notable supporter of such an interpretation was Albert Einstein:
A prominent advocate of the ensemble interpretation is Leslie E. Ballentine, Professor at Simon Fraser University, and writer of the graduate-level textbook "Quantum Mechanics, a Modern Development".
Modal interpretations.
Modal interpretations of quantum mechanics were first conceived of in 1972 by B. van Fraassen, in his paper "A formal approach to the philosophy of science." However, this term now is used to describe a class of models that grew out of this approach.
The "Stanford Encyclopedia of Philosophy" has an article on "Modal Interpretations of Quantum Mechanics"
which presents "modal interpretations" as a class contrasting with objective collapse theories, pilot-wave theories and many-worlds interpretations.
Van Fraassen's proposal distinguished a "dynamical state" from a "value state". The dynamical state corresponds to the ordinary quantum state, which however never collapses.
The value state is the feature which replaces the idea of "collapse".
An observable of a system is taken to have a sharp value even if the dynamical state is not an eigenstate of that same observable.
Van Fraassen's proposal is "modal" because it leads to a modal logic of quantum propositions. 
Since the 1980s, a number of authors have developed other "realist" proposals which can in retrospect be classed with van Fraassen's "modal" proposal.
Consistent histories.
The consistent histories interpretation is based on a consistency criterion that allows the history of a system to be described so that the probabilities for each history obey the additive rules of classical probability.
According to this interpretation, the purpose of a quantum-mechanical theory is to predict the relative probabilities of various alternative histories (for example, of a particle). It is claimed to be consistent with the Schrödinger equation. It attempts to provide a natural interpretation of quantum cosmology.
According to Robert E. Griffiths "It is in fact not necessary to interpret quantum mechanics in terms of measurements."
Nevertheless, Griffiths also says "A quantum theory of measurements is a necessary part of any consistent way of understanding quantum theory for a fairly obvious reason." Griffiths' explanation of this is that quantum measurement theory is derived from the principles of quantum mechanics, which, however, do not themselves explicitly postulate a primary ontological category of measurement in its own right, and which can be interpreted without explicit talk of measurement. Griffiths writes "Thus quantum measurements can, at least in principle, be analyzed using quantum theory." This contradicts the postulate of the orthodox interpretation, that the wave function changes in two ways, (1) according to the Schrödinger equation, which does not involve measurement, and (2) in the so-called 'collapse' or 'reduction' that occurs upon particle detection in the process of measurement.
Tabular comparison.
The most common interpretations are summarized in the table below. The values shown in the cells of the table are not without controversy, for the precise meanings of some of the concepts involved are unclear and, in fact, are themselves at the center of the controversy surrounding the given interpretation.
No empirical evidence exists that distinguishes among these interpretations. To that extent, the physical theory stands, and is consistent within itself and with observation and experiment; difficulties arise only when one attempts to "interpret" the theory. Nevertheless, designing experiments which would test the various interpretations is the subject of active research.
Most of these interpretations have variants. For example, it is difficult to get a precise definition of the Copenhagen interpretation as it was developed and argued about by many people.
Further reading.
Almost all authors below are professional physicists.

</doc>
<doc id="54741" url="https://en.wikipedia.org/wiki?curid=54741" title="Tony Award">
Tony Award

The Antoinette Perry Award for Excellence in Theatre, more commonly known informally as the Tony Award, recognizes achievement in live Broadway theatre. The awards are presented by the American Theatre Wing and The Broadway League at an annual ceremony in New York City. The awards are given for Broadway productions and performances, and an award is given for regional theatre. Several discretionary non-competitive awards are also given, including a Special Tony Award, the Tony Honors for Excellence in Theatre, and the Isabelle Stevenson Award. The awards are named after Antoinette Perry, co-founder of the American Theatre Wing.
The rules for the Tony Awards are set forth in the official document "Rules and Regulations of The American Theatre Wing's Tony Awards", which applies for that season only. The Tony Awards are considered the highest U.S. theatre honor, the New York theatre industry's equivalent to the Academy Awards (Oscars) for motion pictures, the Grammy Awards for music and the Emmy Awards for television, and the Laurence Olivier Award for theatre in the United Kingdom and the Molière Award of France.
From 1997 to 2010, the Tony Awards ceremony was held at Radio City Music Hall in New York City in June and broadcast live on CBS television, except in 1999, when it was held at the Gershwin Theatre. In 2011 and 2012, the ceremony was held at the Beacon Theatre. The 67th Tony Awards returned to Radio City Music Hall on June 9, 2013, as did the 68th Tony Awards on June 8, 2014 and the 69th Tony Awards on June 7, 2015.
Award categories.
, there are 24 categories of awards, plus several special awards. Starting with 11 awards in 1947, the names and number of categories have changed over the years. Some examples: the category Best Book of a Musical was originally called "Best Author (Musical)". The category of Best Costume Design was one of the original awards. For two years, in 1960 and 1961, this category was split into Best Costume Designer (Dramatic) and Best Costume Designer (Musical). It then went to a single category, but in 2005 it was divided again. For the category of Best Director of a Play, a single category was for directors of plays and musicals prior to 1960.
A newly established non-competitive award, The Isabelle Stevenson Award, was given for the first time at the awards ceremony in 2009. The award is for an individual who has made a "substantial contribution of volunteered time and effort on behalf of one or more humanitarian, social service or charitable organizations".
The category of Best Special Theatrical Event was retired as of the 2009–2010 season. The categories of Best Sound Design of a Play and Best Sound Design of a Musical were retired as of the 2014-2015 season.
Performance categories
Show and technical categories
Special awards
Retired awards
History.
The award was founded in 1947 by a committee of the American Theatre Wing headed by Brock Pemberton. The award is named after Antoinette Perry, nicknamed Tony, an actress, director, producer and co-founder of the American Theatre Wing, who died in 1946. As her official biography at the Tony Awards website states, "At [Warner Bros. story editor] Jacob Wilk's suggestion, proposed an award in her honor for distinguished stage acting and technical achievement. At the initial event in 1947, as he handed out an award, he called it a Tony. The name stuck."
The first awards ceremony was held on April 6, 1947, at the Waldorf Astoria hotel in New York City. The first prizes were "a scroll, cigarette lighter and articles of jewelry such as 14-carat gold compacts and bracelets for the women, and money clips for the men." It was not until the third awards ceremony in 1949 that the first Tony medallion was given to award winners.
Awarded by a panel of approximately 868 voters (as of 2014) from various areas of the entertainment industry and press, the Tony Award is generally regarded as the theatre's equivalent to the Academy Award, for excellence in film; the Grammy Award, for the music industry; and the Emmy Award, for excellence in television. It also forms the fourth spoke in the "EGOT", that is, someone who has won all four awards. In British theatre, the equivalent of the Tony Award is the Laurence Olivier Award. A number of the world's longest-running and most successful shows, as well as some actors, directors, choreographers and designers, have received both Tony Awards and Olivier Awards.
Since 1967, the award ceremony has been broadcast on U.S. national television and includes songs from the nominated musicals, and occasionally has included video clips of, or presentations about, nominated plays. The American Theatre Wing and The Broadway League jointly present and administer the awards. Audience size for the telecast is generally well below that of the Academy Awards shows, but the program reaches an affluent audience, which is prized by advertisers. According to a June 2003 article in "The New York Times": "What the Tony broadcast does have, say CBS officials, is an all-important demographic: rich and smart. Jack Sussman, CBS's senior vice president in charge of specials, said the Tony show sold almost all its advertising slots shortly after CBS announced it would present the three hours. 'It draws upscale premium viewers who are attractive to upscale premium advertisers,' Mr. Sussman said..." The viewership has declined from the early years of its broadcast history (for example, the number of viewers in 1974 was 20,026,000, in 1999 9,155,000) but has settled into between six and eight million viewers for most of the decade of the 2000s. In contrast, the 2009 Oscar telecast had 36.3 million viewers.
The medallion.
The Tony Award medallion was designed by art director Herman Rosse and is a mix of mostly brass and a little bronze, with a nickel plating on the outside; a black acrylic glass base, and the nickel-plated pewter swivel. The face of the medallion portrays an adaptation of the comedy and tragedy masks. Originally, the reverse side had a relief profile of Antoinette Perry; this later was changed to contain the winner's name, award category, production and year. The medallion has been mounted on a black base since 1967.
A larger base was introduced in time for the 2010 award ceremony. The new base is slightly taller , up from and heavier , up from . This change was implemented to make the award "feel more substantial" and easier to handle at the moment the award is presented to the winners. According to Howard Sherman, the executive director of the American Theatre Wing:
For the specific Tony Awards presented to a Broadway production, awards are given to the author and up to two of the producers free of charge. All other members of the above-the-title producing team are eligible to purchase the physical award. Sums collected are designed to help defray the cost of the Tony Awards ceremony itself. An award cost $400 as of at least 2000, $750 as of at least 2009, and, as of 2013, had been $2,500 "for several years", according to Tony Award Productions.
Details of the Tony Awards.
"Source: Tony Awards Official Site, Rules"
Rules for a new play or musical.
For the purposes of the award, a new play or musical is one that has not previously been produced on Broadway and is not "determined to be 'classic' or in the historical or popular repertoire", as determined by the Administration Committee (per Section (2g) of the Rules and Regulations). The rule about "classic" productions was instituted by the Tony Award Administration Committee in 2002, and stated (in summary) "A play or musical that is determined ... to be a 'classic' or in the historical or popular repertoire shall not be eligible for an Award in the Best Play or Best Musical Category but may be eligible in that appropriate Best Revival category." Shows transferred from Off-Broadway or the West End are eligible as "new", as are productions based closely on films.
This rule has been the subject of some controversy, as some shows have been ruled ineligible for the "new" category, meaning that their authors did not have a chance to win the important awards of Best Play or Best Musical (or Best Score or Best Book for musicals). On the other hand, some people feel that allowing plays and musicals that have been frequently produced to be eligible as "new" gives them an unfair advantage, because they will have benefited from additional development time as well as additional familiarity with the Tony voters.
Committees and voters.
The Tony Awards Administration Committee has twenty-four members: ten designated by the American Theatre Wing, ten by The Broadway League, and one each by the Dramatists Guild, Actors' Equity Association, United Scenic Artists and the Society of Stage Directors and Choreographers. This committee, among other duties, determines eligibility for nominations in all awards categories.
The Tony Awards Nominating Committee makes the nominations for the various categories. This rotating group of theatre professionals is selected by the Tony Awards Administration Committee. Nominators serve three-year terms and are asked to see every new Broadway production. The Nominating Committee for the 2012-13 Broadway season (named in June 2012) had 42 members; the Nominating Committee for the 2014-2015 season has 50 members and was appointed in June 2014.
There are approximately 868 eligible Tony Award voters (as of 2014), a number that changes slightly from year to year. The number was decreased in 2009 when the first-night critics were excluded as voters. That decision was changed, and members of the New York Drama Critics' Circle were invited to be Tony voters beginning in the 2010-2011 season.
The eligible Tony voters include the board of directors and designated members of the advisory committee of the American Theatre Wing, members of the governing boards of Actors' Equity Association, the Dramatists Guild, the Society of Stage Directors and Choreographers, United Scenic Artists, and the Association of Theatrical Press Agents and Managers, members of the Theatrical Council of the Casting Society of America and voting members of The Broadway League (in 2000, what was then The League of American Theaters and Producers changed membership eligibility and Tony voting status from a life-time honor to all above-the-title producers, to ones who had been active in the previous 10 years. This action dis-enfranchised scores of Tony voters, including Gail Berman, Harve Brosten, Dick Button, Tony Lo Bianco, and Raymond Serra).
Eligibility date (Season).
To be eligible for Tony Award consideration, a production must have officially opened on Broadway by the eligibility date that the Management Committee establishes each year. For example, the cut-off date for eligibility the 2013–2014 season was April 24, 2014. The season for Tony Award eligibility is defined in the Rules and Regulations.
Broadway theatre.
A Broadway theatre is defined as having 500 or more seats, among other requirements. While the rules define a Broadway theatre in terms of its size, not its geographical location, the list of Broadway theatres is determined solely by the Tony Awards Administration Committee. As of the 2010–2011 season, the list consisted solely of the 40 theaters located in the vicinity of Times Square in New York City and Lincoln Center's Vivian Beaumont Theater.
Criticism.
While the theatre-going public may consider the Tony Awards to be the Oscars of live theatre, critics have suggested that the Tony Awards are primarily a promotional vehicle for a small number of large production companies and theatre owners in New York City. In a 2014 "Playbill" article, Robert Simonson wrote that "Who gets to perform on the Tony Awards broadcast, what they get to perform, and for how long, have long been politically charged questions in the Broadway theatre community..." The producers "accept the situation ... because just as much as actually winning a Tony, a performance that lands well with the viewing public can translate into big box-office sales." Producer Robyn Goodman noted that, if the presentation at the ceremony shows well and the show wins a Tony, "you’re going to spike at the box office".
The awards met further criticism when they eliminated the sound design awards in 2014.
Award milestones.
Some notable records and facts about the Tony Awards include the following:

</doc>
<doc id="54743" url="https://en.wikipedia.org/wiki?curid=54743" title="Inbreeding">
Inbreeding

Inbreeding is the production of offspring from the mating or breeding of individuals or organisms that are closely related genetically. By analogy, the term is used in human reproduction, but more commonly refers to the genetic disorders and other consequences that may arise from incestuous sexual relationships and consanguinity.
Inbreeding results in homozygosity, which can increase the chances of offspring being affected by recessive or deleterious traits. This generally leads to a decreased biological fitness of a population (called inbreeding depression), which is its ability to survive and reproduce. An individual who inherits such deleterious traits is referred to as "inbred". The avoidance of expression of such deleterious recessive alleles caused by inbreeding, via inbreeding avoidance mechanisms, is the main selective reason for outcrossing. Crossbreeding between populations also often has positive effects on fitness-related traits.
Inbreeding is a technique used in selective breeding. In livestock breeding, breeders may use inbreeding when, for example, trying to establish a new and desirable trait in the stock, but will need to watch for undesirable characteristics in offspring, which can then be eliminated through further selective breeding or culling. Inbreeding is used to reveal deleterious recessive alleles, which can then be eliminated through assortative breeding or through culling. In plant breeding, inbred lines are used as stocks for the creation of hybrid lines to make use of the effects of heterosis. Inbreeding in plants also occurs naturally in the form of self-pollination.
Overview.
Offspring of biologically related persons are subject to the possible effect of inbreeding, such as congenital birth defects. The chances of such disorders are increased when the biological parents are more closely related. (See coefficient of inbreeding.) This is because such pairings have a 25% probability of producing homozygous zygotes, resulting in offspring with two recessive alleles, which can produce disorders when these alleles are deleterious. (See inbreeding depression.) Because most recessive alleles are rare in populations, it is unlikely that two unrelated marriage partners will both be carriers of the same deleterious allele. However, because close relatives share a large fraction of their alleles, the probability that any such deleterious allele is inherited from the common ancestor through both parents is increased dramatically. It should also be noted that, in terms of probability, for each homozygous recessive individual formed, there is an equal chance of producing a homozygous dominant individual, one completely devoid of the harmful allele. Contrary to common belief, inbreeding does not in itself alter allele frequencies, but rather increases the relative proportion of homozygotes to heterozygotes. However, because the increased proportion of deleterious homozygotes exposes the allele to natural selection, in the long run its frequency decreases more rapidly in inbred populations. In the short term, incestuous reproduction is expected to increase the number of spontaneous abortions of zygotes, perinatal deaths, and postnatal offspring with birth defects. The advantages of inbreeding may be the result of a tendency to preserve the structures of alleles interacting at different loci that have been adapted together by a common selective history.
Malformations or harmful traits can stay within a population due to a high homozygosity rate, and this will cause a population to become fixed for certain traits, like having too many bones in an area, like the vertebral column of wolves on Isle Royale or having cranial abnormalities, such as in Northern elephant seals, where their cranial bone length in the lower mandibular tooth row has changed. Having a high homozygosity rate is problematic for a population because it will unmask recessive deleterious alleles generated by mutations, reduce heterozygote advantage, and it is detrimental to the survival of small, endangered animal populations. When deleterious recessive alleles are unmasked due to the increased homozygosity generated by inbreeding, this can cause inbreeding depression. The authors think that it is possible that the severity of inbreeding depression can be diminished if natural selection can purge such alleles from populations during inbreeding. If inbreeding depression can be diminished by natural selection than some traits, harmful or not, can be reduced and change the future outlook on a small, endangered populations.
There may also be other deleterious effects besides those caused by recessive diseases. Thus, similar immune systems may be more vulnerable to infectious diseases (see Major histocompatibility complex and sexual selection).
Inbreeding history of the population should also be considered when discussing the variation in the severity of inbreeding depression between and within species. With persistent inbreeding, there is evidence that shows that inbreeding depression becoming less severe. This is associated with the unmasking and elimination of severely deleterious recessive alleles. However, inbreeding depression is not a temporary phenomenon because this elimination of deleterious recessive alleles will never be complete. Eliminating slightly deleterious mutations through inbreeding under moderate selection is not as effective. Fixation of alleles most likely occurs through Muller’s ratchet, when an asexual population’s genome accumulates deleterious mutations that are irreversible.
Despite all its disadvantages, inbreeding can also have a variety of advantages, such as reducing the cost of sex, reducing the recombination load, and allowing the expression of recessive advantageous phenotypes. It has been proposed that under circumstances when the advantages of inbreeding outweigh the disadvantages, preferential breeding within small groups could be promoted, potentially leading to speciation.
Genetic disorders.
Autosomal recessive disorders occur in individuals who have two copies of an allele for a particular recessive genetic mutation. Except in certain rare circumstances, such as new mutations or uniparental disomy, both parents of an individual with such a disorder will be carriers of the gene. These carriers do not display any signs of the mutation and may be unaware that they carry the mutated gene. Since relatives share a higher proportion of their genes than do unrelated people, it is more likely that related parents will both be carriers of the same recessive allele, and therefore their children are at a higher risk of inheriting an autosomal recessive genetic disorder. The extent to which the risk increases depends on the degree of genetic relationship between the parents; the risk is greater when the parents are close relatives and lower for relationships between more distant relatives, such as second cousins, though still greater than for the general population. A study has provided evidence for the relationship between inbreeding depression and cognitive abilities among children, showing that the frequency of developmental disability among offspring is proportional to their inbreeding coefficients.
Children of parent-child or sibling-sibling unions are at an increased risk compared to cousin-cousin unions.
Inbreeding may result in a greater than expected phenotypic expression of deleterious recessive alleles within a population. As a result, first-generation inbred individuals are more likely to show physical and health defects, including:
The isolation of a small population for a period of time can lead to inbreeding within that population, resulting in increased genetic relatedness between breeding individuals. Inbreeding depression can also occur in a large population if individuals tend to mate with their relatives, instead of mating randomly.
Many individuals in the first generation of inbreeding will never live to reproduce. Over time, with isolation, such as a population bottleneck caused by purposeful (assortative) breeding or natural environmental factors, the deleterious inherited traits are culled.
Island species are often very inbred, as their isolation from the larger group on a mainland allows natural selection to work on their population. This type of isolation may result in the formation of race or even speciation, as the inbreeding first removes many deleterious genes, and permits the expression of genes that allow a population to adapt to an ecosystem. As the adaptation becomes more pronounced, the new species or race radiates from its entrance into the new space, or dies out if it cannot adapt and, most importantly, reproduce.
The reduced genetic diversity, for example due to a bottleneck will unavoidably increase inbreeding for the entire population. This may mean that a species may not be able to adapt to changes in environmental conditions. Each individual will have similar immune systems, as immune systems are genetically based. When a species becomes endangered, the population may fall below a minimum whereby the forced interbreeding between the remaining animals will result in extinction.
Natural breedings include inbreeding by necessity, and most animals only migrate when necessary. In many cases, the closest available mate is a mother, sister, grandmother, father, brother, or grandfather. In all cases, the environment presents stresses to remove from the population those individuals who cannot survive because of illness.
There was an assumption that wild populations do not inbreed; this is not what is observed in some cases in the wild. However, in species such as horses, animals in wild or feral conditions often drive off the young of both sexes, thought to be a mechanism by which the species instinctively avoids some of the genetic consequences of inbreeding. In general, many mammal species, including humanity's closest primate relatives, avoid close inbreeding possibly due to the deleterious effects.
Examples.
Although there are several examples of inbred populations of wild animals, the negative consequences of this inbreeding are poorly documented.
In the South American sea lion, there was concern that recent population crashes would reduce genetic diversity. Historical analysis indicated that a population expansion from just two matrilineal lines was responsible for most of the individuals within the population. Even so, the diversity within the lines allowed great variation in the gene pool that may help to protect the South American sea lion from extinction.
In lions, prides are often followed by related males in bachelor groups. When the dominant male is killed or driven off by one of these bachelors, a father may be replaced by his son. There is no mechanism for preventing inbreeding or to ensure outcrossing. In the prides, most lionesses are related to one another. If there is more than one dominant male, the group of alpha males are usually related. Two lines are then being "line bred". Also, in some populations, such as the Crater lions, it is known that a population bottleneck has occurred. Researchers found far greater genetic heterozygosity than expected. In fact, predators are known for low genetic variance, along with most of the top portion of the trophic levels of an ecosystem. Additionally, the alpha males of two neighboring prides can be from the same litter; one brother may come to acquire leadership over another's pride, and subsequently mate with his 'nieces' or cousins. However, killing another male's cubs, upon the takeover, allows the new selected gene complement of the incoming alpha male to prevail over the previous male. There are genetic assays being scheduled for lions to determine their genetic diversity. The preliminary studies show results inconsistent with the outcrossing paradigm based on individual environments of the studied groups.
In Central California, Sea Otters were thought to have been driven to extinction due to over hunting, until a colony of about 30 breeding pairs was discovered in the Big Sur region in the 1930s. Since then, the population has grown and spread along the central Californian coast to around 2,000 individuals, a level that has remained stable for over a decade. Population growth is limited by the fact that all Californian Sea Otters are descended from the isolated colony, resulting in inbreeding.
Cheetahs are another example of inbreeding. Thousands of years ago the cheetah went through a population bottleneck that reduced its population dramatically so the animals that are alive today are all related to one another. A consequence from inbreeding for this species has been high juvenile mortality, low fecundity, and poor breeding success.
In a study on an island population of song sparrows, individuals that were inbred showed significantly lower survival rates than outbred individuals during a severe winter weather related population crash. These studies show that inbreeding depression and ecological factors have an influence on survival.
Measures of inbreeding.
A measure of inbreeding of an individual A is the probability "F"(A) that both alleles in one locus are derived from the same allele in an ancestor. These two identical alleles that are both derived from a common ancestor are said to be identical by descent. This probability F(A) is called the "coefficient of inbreeding".
Another useful measure that describes the extent to which two individuals are related (say individuals A and B) is their coancestry coefficient f(A,B), which gives the probability that one randomly selected allele from A and another randomly selected allele from B are identical by descent. This is also denoted as the kinship coefficient between A and B.
A particular case is the self-coancestry of individual A with itself, f(A,A), which is the probability that taking one random allele from A and then, independently and with replacement, another random allele also from A, both are identical by descent. Since they can be identical by descent by sampling the same allele or by sampling both alleles that happen to be identical by descent, we have f(A,A) = 1/2 + F(A)/2.
Both the inbreeding and the coancestry coefficients can be defined for specific individuals or as average population values. They can be computed from genealogies or estimated from the population size and its breeding properties, but all methods assume no selection and are limited to neutral alleles.
There are several methods to compute this percentage. The two main ways are the path method and the tabular method.
Typical coancestries between relatives are as follows:
Domestic animals.
Breeding in domestic animals is primarily assortative breeding (see selective breeding). Without the sorting of individuals by trait, a breed could not be established, nor could poor genetic material be removed.
Homozygosity is the case where similar or identical alleles combine to express a trait that is not otherwise expressed (recessiveness). Inbreeding exposes recessive alleles through increasing homozygosity.
Breeders must avoid breeding from individuals that demonstrate either homozygosity or heterozygosity for disease causing alleles. The goal of preventing the transfer of deleterious alleles may be achieved by reproductive isolation, sterilization, or, in the extreme case, culling. Culling is not strictly necessary if genetics are the only issue in hand. Small animals such as cats and dogs may be sterilized, but in the case of large agricultural animals, such as cattle, culling is usually the only economic option.
The issue of casual breeders who inbreed irresponsibly is discussed in the following quotation on cattle:
"Meanwhile, milk production per cow per lactation increased from 17,444 lbs to 25,013 lbs from 1978 to 1998 for the Holstein breed. Mean breeding values for milk of Holstein cows increased by 4,829 lbs during this period. High producing cows are increasingly difficult to breed and are subject to higher health costs than cows of lower genetic merit for production" (Cassell, 2001).
"Intensive selection for higher yield has increased relationships among animals within breed and increased the rate of casual inbreeding."
"Many of the traits that affect profitability in crosses of modern dairy breeds have not been studied in designed experiments. Indeed, all crossbreeding research involving North American breeds and strains is very dated (McAllister, 2001) if it exists at all".
The BBC produced two documentaries on dog inbreeding titled Pedigree Dogs Exposed and Pedigree Dogs Exposed - Three Years On that document the negative health consequences of excessive inbreeding.
Linebreeding is a form of inbreeding. There is no clear distinction between the two terms, but linebreeding may encompass crosses between individuals and their descendants or two cousins. This method can be used to increase a particular animal's contribution to the population. While linebreeding is less likely to cause problems in the first generation than does inbreeding, over time, linebreeding can reduce the genetic diversity of a population and cause problems related to a too-small genepool that may include an increased prevalence of genetic disorders and inbreeding depression.
Outcrossing is where two unrelated individuals are crossed to produce progeny. In outcrossing, unless there is verifiable genetic information, one may find that all individuals are distantly related to an ancient progenitor. If the trait carries throughout a population, all individuals can have this trait. This is called the founder effect. In the well established breeds, that are commonly bred, a large gene pool is present. For example, in 2004, over 18,000 Persian cats were registered. A possibility exists for a complete outcross, if no barriers exist between the individuals to breed. However, it is not always the case, and a form of distant linebreeding occurs. Again it is up to the assortative breeder to know what sort of traits, both positive and negative, exist within the diversity of one breeding. This diversity of genetic expression, within even close relatives, increases the variability and diversity of viable stock.
Laboratory animals.
Systematic inbreeding and maintenance of inbred strains of laboratory mice and rats is of great importance for biomedical research. The inbreeding guarantees a consistent and uniform animal model for experimental purposes and enables genetic studies in congenic and knock-out animals. The use of inbred strains is also important for genetic studies in animal models, for example to distinguish genetic from environmental effects. The mice that are inbred typically show considerably lower survival rates.
Humans.
Effects.
Inbreeding increases the chances of the expression of deleterious recessive alleles by increasing homozygosity and therefore has the potential to decrease the fitness of the offspring. With continuous inbreeding, genetic variation is lost and homozygosity is increased, enabling the expression of recessive deleterious alleles in homozygotes. The inbreeding coefficient, a term used to describe the degree of inbreeding in an individual, is an estimate of the percent of homozygous alleles in the overall genome. The more biologically related the parents are, the greater the inbreeding coefficient (See Coefficient of Inbreeding), since their genomes have many similarities already. This overall homozygosity becomes an issue when there are deleterious recessive alleles in the gene pool of the family. By pairing chromosomes of similar genomes, the chance for these recessive alleles to pair and become homozygous greatly increases, leading to offspring with autosomal recessive disorders.
Inbreeding is especially problematic in small populations where the genetic variation is already limited. By inbreeding, individuals are further decreasing genetic variation by increasing homozygosity in the genomes of their offspring. Thus, the likelihood of deleterious recessive alleles to pair is significantly higher in an small inbreeding population than in a larger inbreeding population. By consistently increasing the homozygosity of alleles, the population is permitting the expression of these harmful alleles, resulting in genetic disorders. Due to the inflicted individuals decreased fitness and reproductive success, the recessive alleles will eventually become culled by natural selection. Thus in small inbreeding populations, deleterious recessive alleles are more likely to be prevalent in the short term, but will decrease more swiftly in the long term after consistent expression followed by natural selection.
The fitness consequences of consanguineous mating have been studied since their scientific recognition by Charles Darwin in 1839. Some of the most harmful effects known from such breeding includes its effects on the mortality rate as well as on the general health of the offspring. Within the past several decades, there have been many studies to support such debilitating effects on the human organism. Specifically, inbreeding has been found to decrease fertility as a direct result of increasing homozygosity of deleterious recessive alleles. Fetuses produced by inbreeding also face a greater risk of spontaneous abortions due to inherent complications in development. Among mothers who experience stillbirths and early infant deaths, those that are inbreeding have a significantly higher chance of reaching repeated results with future offspring. Additionally, consanguineous parents possess a high risk of premature birth and producing underweight and undersized infants. Viable inbred offspring are also likely to be inflicted with physical deformities and genetically inherited diseases. Studies have confirmed an increase in several genetic disorders due to inbreeding such as blindness, hearing loss, neonatal diabetes, limb malformations, Schizophrenia and several others. Moreover, there is an increased risk for congenital heart disease depending on the inbreeding coefficient (See Coefficient of Inbreeding) of the offspring, with significant risk accompanied by an F =.125 or higher. A study in 2009 revealed that inbred individuals have a greater susceptibility to TB and hepatitis. Being that resistance to these diseases is dependent on one's immune system, it is thereby suggested that consanguinity increases individual's susceptibility to infectious diseases. While inbreeding may seem to mainly implicate physical disparities, it also strongly affects cognitive ability. A recent behavioral study discovered that parental consanguinity followed by inbreeding is significantly correlated with the depression in intelligence among children. Furthermore, such offspring are known to have a lower IQ and higher risk for developmental disabilities.
Prevalence.
The general negative outlook and eschewal of inbreeding that is prevalent in the western world today holds roots from over 1500 years ago. Specifically, written documents such as the Bible illustrate that there have been laws and social customs that have called for the abstention from inbreeding. Along with cultural taboos, parental education and awareness of inbreeding consequences have played large roles in minimizing inbreeding frequencies in areas like Europe. That being so, there are less urbanized and less populated regions across the world that have shown continuity in the practice of inbreeding. This continuity is often either by choice or unavoidably due to the limitations of the geographical area. When by choice, the rate of consanguinity is highly dependent on religion and culture. Of the practicing regions, Middle Eastern and northern Africa territories show the greatest frequencies of consanguinity. The link between the high frequency and the region is primarily due to the dominance of Islamic populations, who have historically engaged in familyline relations.
Aside from the Middle East, north African territories such as Tunisia and Egypt experience similar consanguinity frequencies and correlations.
Among these populations with high levels of inbreeding, researchers have found several disorders prevalent among inbred offspring. Specifically, in Lebanon, Saudi Arabia, Egypt, and Arabs in Israel, it has been discovered that offspring of consanguineous relationships have an increased risk of congenital malformations, congenital heart defects, congenital hydrocephalus and neural tube defects. Furthermore, among inbred children in Palestine and Lebanon, there is a positive association between consanguinity and reported cleft lip/palate cases. 
For further information on inbreeding effects, explore Tadmouri's review, "Consanguinity and reproductive health among Arabs," Bittle's article, "Consanguinity, human evolution, and complex diseases," and Hamamy's review, "Consanguineous marriages."
Possible increase of fertility.
A study in Iceland by the deCODE genetics company, published by the journal "Science", found that third cousins produced more children and grandchildren than more distant marriages, suggesting that "in spite of the fact that bringing together two alleles of a recessive trait may be bad, there may be some biological wisdom in the union of relatively closely related people". For hundreds of years, inbreeding was historically unavoidable in Iceland due to its then tiny and isolated population.
Royalty and nobility.
Inter-nobility marriage was used as a method of forming political alliances among elites. These ties were often sealed only upon the birth of progeny within the arranged marriage. Thus marriage was seen as a union of lines of nobility, not of a contract between individuals as it is seen today.
Royal intermarriage was often practiced among European royal families, usually for interests of state. Over time, due to the relatively limited number of potential consorts, the gene pool of many ruling families grew progressively smaller, until all European royalty was related. This also resulted in many being descended from a certain person through many lines of descent, such as the numerous European royalty and nobility descended from the British Queen Victoria or King Christian IX of Denmark. The House of Habsburg was infamous for its inbreeding, with the Habsburg lip cited as an ill-effect, although no genetic evidence has proved the allegation. The closely related houses of Habsburg, Bourbon, Braganza and Wittelsbach also frequently engaged in first-cousin unions as well as the occasional double-cousin and uncle-niece marriages. Examples of incestuous marriages and the impact of inbreeding on royal families include:
Today, royal intermarriage within European royal families has declined compared to past practice along with the power and prevalence of noble families and their importance in international affairs.

</doc>
<doc id="54746" url="https://en.wikipedia.org/wiki?curid=54746" title="Incest taboo">
Incest taboo

An incest taboo is any cultural rule or norm that prohibits sexual relations between closely related persons. All human cultures have norms that exclude certain close relatives from those considered suitable or permissible sexual or marriage partners, making such relationships taboo. However, different norms exist among cultures as to which blood relations are permissible as sexual partners and which are not. The sexual relations between related persons which are subject to the taboo are called incestuous relationships.
Some cultures proscribed sexual relations between clan-members, even when no traceable biological relationship exists, while members of other clans are permissible irrespective of the existence of a biological relationship. In many cultures, certain types of cousin relations are preferred as sexual and marital partners, whereas in others these are taboo. Some cultures permit sexual and marital relations between aunts/uncles and nephews/nieces. In some instances, brother–sister marriages have been practiced by the elites with some regularity. Parent–child and sibling–sibling unions are almost universally taboo.
Source of the taboo.
Debate about the origin of the incest taboo has often been framed as a question of whether it is based in nature or nurture.
One explanation sees the incest taboo as a cultural implementation of a biologically evolved preference for sexual partners with whom one is unlikely to share genes, since inbreeding may have detrimental outcomes. The most widely held hypothesis proposes that the so-called Westermarck effect discourages adults from engaging in sexual relations with individuals with whom they grew up. The existence of the Westermarck effect has achieved some empirical support.
Another school argues that the incest prohibition is a cultural construct which arises as a side effect of a general human preference for group exogamy, which arises because intermarriage between groups construct valuable alliances that improve the ability for both groups to thrive. According to this view, the incest taboo is not necessarily universal, but is likely to arise and become more strict under cultural circumstances that favour exogamy over endogamy, and likely to become more lax under circumstances that favor endogamy. This hypothesis has also achieved some empirical support.
Limits to biological evolution of taboo.
While it is theoretically possible that natural selection may, under certain genetic circumstances, select for individuals that instinctively avoid mating with (close) relatives, biological evolution cannot select for punishing others for incest, since even genetically weakened, inbred individuals are better watchposts against predators than none at all, and weak individuals are useful for the stronger individuals in the group as looking out for predators without being able to seriously compete with the stronger individuals. Punishing both parties in an incestous relation cannot even be beneficial for the genes of individuals punishing a somewhat more distant relative for mating with a closer relative, since punishing the closer relative as well is counterproductive to any function of protecting the closer relative and the health of its offspring (in a context where predation and starvation are significant factors, as opposed to a rich welfare state). Genetic sexual attraction theory is also incompatible with the theory of smell being a significant factor in avoiding inbreeding.
Research on the taboo.
Modern anthropology developed at a time when a great many human societies were non-literate, and much of the research on incest taboos has taken place in societies without legal codes, and, therefore, without written laws concerning marriage and incest. Nevertheless, anthropologists have found that the institution of marriage, and rules concerning appropriate and inappropriate sexual behavior, exist in every society. The following excerpt from "Notes and Queries on Anthropology" (1951), a well-established field manual for ethnographic research, illustrates the scope of ethnographic investigation into the matter:
As this excerpt suggests, anthropologists distinguish between social norms and actual social behavior; much social theory explores the difference and relationship between the two. For example, what is the purpose of prohibitions that are routinely violated (as for example when people claim that incest is taboo yet engage in incestuous behavior)?
It should be further noted that in these theories anthropologists are generally concerned solely with brother–sister incest, and are not claiming that all sexual relations among family members are taboo or even necessarily considered incestuous by that society. These theories are further complicated by the fact that in many societies people related to one another in different ways, and sometimes distantly, are classified together as siblings, and others who are just as closely related genetically are not considered family members.
Moreover, the definition restricts itself to sexual intercourse; this does not mean that other forms of sexual contact do not occur, or are proscribed, or prescribed. For example, in some Inuit societies in the Arctic, and traditionally in Bali, mothers would routinely stroke the penises of their infant sons; such behavior was considered no more sexual than breast-feeding.
It should also be noted that, in these theories, anthropologists are primarily concerned with marriage rules and not actual sexual behavior. In short, anthropologists were not studying "incest" per se; they were asking informants what they meant by "incest", and what the consequences of "incest" were, in order to map out social relationships within the community.
This excerpt also suggests that the relationship between sexual and marriage practices is complex, and that societies distinguish between different sorts of prohibitions. In other words, although an individual may be prohibited from marrying or having sexual relations with many people, different sexual relations may be prohibited for different reasons, and with different penalties.
For example, Trobriand Islanders prohibit both sexual relations between a woman and her brother, and between a woman and her father, but they describe these prohibitions in very different ways: relations between a woman and her brother fall within the category of forbidden relations among members of the same clan; relations between a woman and her father do not. This is because the Trobrianders are matrilineal; children belong to the clan of their mother and not of their father. Thus, sexual relations between a man and his mother's sister (and mother's sister's daughter) are also considered incestuous, but relations between a man and his father's sister are not. A man and his father's sister will often have a flirtatious relationship, and, far from being taboo, Trobriand society encourages a man and his father's sister or the daughter of his father's sister to have sexual relations or marry.
Instinctual and genetic explanations.
An explanation for the taboo is that it is due to an instinctual, inborn aversion that would lower the adverse genetic effects of inbreeding such as a higher incidence of congenital birth defects (see article Inbreeding depression). Since the rise of modern genetics, belief in this theory has grown. 
The increase in frequency of birth defects often attributed to inbreeding results directly from an increase in the frequency of homozygous alleles inherited by the offspring of inbred couples. This leads to an increase in homozygous allele frequency within a population, and results in diverging effects. Should a child inherit the version of homozygous alleles responsible for a birth defect from its parents, the birth defect will be expressed; on the other hand, should the child inherit the version of homozygous alleles not responsible for a birth defect, it would actually decrease the ratio of the allele version responsible for the birth defect in that population. The overall consequences of these diverging effects depends in part on the size of the population. In small populations, as long as children born with inheritable birth defects die (or are killed) before they reproduce, the ultimate effect of inbreeding will be to "decrease" the frequency of defective genes in the population; over time, the gene pool will be healthier. However, in larger populations, it is more likely that large numbers of carriers will survive and mate, leading to more constant rates of birth defects. Besides recessive genes, there are also other reasons why inbreeding may be harmful, such as a narrow range of certain immune systems genes in a population increasing vulnerability to infectious diseases (see Major histocompatibility complex and sexual selection). The biological costs of incest also depend largely on the degree of genetic proximity between the two relatives engaging in incest. This fact may explain why the cultural taboo generally includes prohibitions against sex between close relatives but less often includes prohibitions against sex between more distal relatives. Children born of close relatives have decreased survival. Melford Spiro argued that his observations that unrelated children reared together on Israeli Kibbutzim nevertheless avoided one another as sexual partners confirmed the Westermarck effect. Joseph Shepher in a study examined the second generation in a kibbutz and found no marriages and no sexual activity between the adolescents in the same peer group. This was not enforced but voluntary. Looking at the second generation adults in all kibbutzim, out of a total of 2769 marriages, none were between those of the same peer group. However, according to a book review by John Hartung of a book by Shepher, out of 2516 marriages documented in Israel, 200 were between couples reared in the same kibbutz. These marriages occurred after young adults reared on kibbutzim had served in the military and encountered tens of thousands of other potential mates, and 200 marriages is higher than what would be expected by chance. Of these 200 marriages, five were between men and women who had been reared together for the first six years of their lives, which would argue against the Westermarck effect. A study in Taiwan of marriages where the future bride is adopted in the groom's family as an infant or small child found that these marriages have higher infidelity and divorce and lower fertility than ordinary marriages; it has been argued that this observation is consistent with the Westermarck effect.
Another approach is looking at moral objections to third-party incest. This increases the longer a child has grown up together with another child of the opposite sex. This occurs even if the other child is genetically unrelated. Humans have been argued to have a special kin detection system that besides the incest taboo also regulates a tendency towards altruism towards kin.
One objection against an instinctive and genetic basis for the incest taboo is that incest does occur. Anthropologists have also argued that the social construct "incest" (and the incest taboo) is not the same thing as the biological phenomenon of "inbreeding". For example, there is equal genetic relation between a man and the daughter of his father's sister and between a man and the daughter of his mother's sister, such that biologists would consider mating incestuous in both instances, but Trobrianders consider mating incestuous in one case and not in the other. Anthropologists have documented a great number of societies where marriages between some first cousins are prohibited as incestuous, while marriages between other first cousins are encouraged. Therefore, it is argued that the prohibition against incestuous relations in most societies is not based on or motivated by concerns over biological closeness. Other studies on cousin marriages have found support for a biological basis for the taboo. Also, current supporters of genetic influences on behavior do not argue that genes determine behavior absolutely, but that genes may create predispositions that are affected in various ways by the environment (including culture).
Sociological explanations.
Psychoanalytic theory—in particular, the claimed existence of an Oedipus complex, which is not an instinctual aversion against incest but an instinctual desire—has influenced many theorists seeking to explain the incest taboo using sociological theories.
The incest taboo and exogamy.
The anthropologist Claude Lévi-Strauss developed a general argument for the universality of the incest taboo in human societies. His argument begins with the claim that the incest taboo is in effect a prohibition against endogamy, and the effect is to encourage exogamy. Through exogamy, otherwise unrelated households or lineages will form relationships through marriage, thus strengthening social solidarity. That is, Lévi-Strauss views marriage as an exchange of women between two social groups. This theory is based in part on Marcel Mauss's theory of "The Gift", which (in Lévi-Strauss' words) argued:
It is also based on Lévi-Strauss's analysis of data on different kinship systems and marriage practices documented by anthropologists and historians. Lévi-Strauss called attention specifically to data collected by Margaret Mead during her research among the Arapesh. When she asked if a man ever sleeps with his sister, Arapesh replied: "No we don't sleep with our sisters. We give our sisters to other men, and other men give us their sisters." Mead pressed the question repeatedly, asking what would happen if a brother and sister did have sex with one another. Lévi-Strauss quotes the Arapesh response:
By applying Mauss's theory to data such as Mead's, Lévi-Strauss proposed what he called alliance theory. He argued that, in "primitive" societies, marriage is not fundamentally a relationship between a man and a woman, but a transaction involving a woman that forges a relationship—an alliance—between two men. His "Elementary Structures of Kinship" takes this as a starting point and uses it to analyze kinship systems of increasing complexity found in so-called primitive societies (that is, those not based on agriculture, class inequalities, and centralized government).
This theory was debated intensely by anthropologists in the 1950s. It appealed to many because it used the study of incest taboos and marriage to answer more fundamental research interests of anthropologists at the time: how can an anthropologist map out the social relationships within a given community, and how do these relationships promote or endanger social solidarity? Nevertheless, anthropologists never reached a consensus, and with the Vietnam War and the process of decolonization in Africa, Asia, and Oceania, anthropological interests shifted away from mapping local social relationships.
Some anthropologists argue that nuclear family incest avoidance can be explained in terms of the ecological, demographic, and economic benefits of exogamy.
While Lévi-Strauss generally discounted the relevance of alliance theory in Africa, a particularly strong concern for incest is a fundamental issue among the age systems of East Africa. Here, the avoidance between men of an age-set and their daughters is altogether more intense than in any other sexual avoidance. Paraphrasing Lévi-Strauss's argument, without this avoidance, the rivalries for power between age-sets, coupled with the close bonds of sharing between age-mates, could lead to a sharing of daughters as spouses. Young men entering the age system would then find a dire shortage of marriageable girls, and extended families would be in danger of dying out. Thus, by parading this avoidance of their daughters, senior men make these girls available for younger age-sets and their marriages form alliances that mitigate the rivalries for power.
Incest and endogamy.
Exogamy between households or descent groups is typically prescribed in classless societies. Societies that are stratified—that is, divided into unequal classes—often prescribe different degrees of endogamy. Endogamy is the opposite of exogamy; it refers to the practice of marriage between members of the same social group. A classic example is India's caste system, in which unequal castes are endogamous. Inequality between ethnic groups and races also correlates with endogamy. Class, caste, ethnic, and racial endogamy typically coexists with family exogamy and prohibitions against incest.
An extreme example of this principle, and an exception to the incest taboo, is found among members of the ruling class in certain ancient states, such as the Inca, Egypt, China, and Hawaii; brother–sister marriage (usually between half-siblings) was a means of maintaining wealth and political power within one family. Some scholars have argued that in Roman-governed Egypt this practice was also found among commoners, but others have argued that this was in fact not the norm.

</doc>
<doc id="54749" url="https://en.wikipedia.org/wiki?curid=54749" title="Chromatic aberration">
Chromatic aberration

In optics, chromatic aberration (CA, also called achromatism, chromatic distortion, and spherochromatism) is an effect resulting from dispersion in which there is a failure of a lens to focus all colors to the same convergence point. It occurs because lenses have different refractive indices for different wavelengths of light. The refractive index of transparent materials decreases with increasing wavelength in degrees unique to each.
Chromatic aberration manifests itself as "fringes" of color along boundaries that separate dark and bright parts of the image, because each color in the optical spectrum cannot be focused at a single common point. Since the focal length "f" of a lens is dependent on the refractive index "n", different wavelengths of light will be focused on different positions.
Types.
There are two types of chromatic aberration: "axial" ("longitudinal"), and "transverse" ("lateral"). Axial aberration occurs when different wavelengths of light are focused at different distances from the lens, i.e., different points on the optical "axis" (focus "shift"). Transverse aberration occurs when different wavelengths are focused at different positions in the focal plane (because the magnification and/or distortion of the lens also varies with wavelength; indicated in graphs as (change in) focus "length"). The acronym LCA is used, but ambiguous, and may refer to either longitudinal or lateral CA; for clarity, this article uses "axial" (shift in the direction of the optical axis) and "transverse" (shift perpendicular to the optical axis, in the plane of the sensor or film).
These two types have different characteristics, and may occur together. Axial CA occurs throughout the image and is specified by optical engineers, optometrists, and vision scientists in the unit of focus known widely as diopters, and is reduced by stopping down. (This increases depth of field, so though the different wavelengths focus at different distances, they are still in acceptable focus.) Transverse CA does not occur in the center, and increases towards the edge, but is not affected by stopping down.
In digital sensors, axial CA results in the red and blue planes being defocused (assuming that the green plane is in focus), which is relatively difficult to remedy in post-processing, while transverse CA results in the red, green, and blue planes being at different magnifications (magnification changing along radii, as in geometric distortion), and can be corrected by radially scaling the planes appropriately so they line up.
Minimization.
In the earliest uses of lenses, chromatic aberration was reduced by increasing the focal length of the lens where possible. For example, this could result in extremely long telescopes such as the very long aerial telescopes of the 17th century. Isaac Newton's theories about white light being composed of a spectrum of colors led him to the conclusion that uneven refraction of light caused chromatic aberration (leading him to build the first reflecting telescope, his Newtonian telescope, in 1668).
There exists a point called the "circle of least confusion", where chromatic aberration can be minimized. It can be further minimized by using an achromatic lens or "achromat", in which materials with differing dispersion are assembled together to form a compound lens. The most common type is an achromatic doublet, with elements made of crown and flint glass. This reduces the amount of chromatic aberration over a certain range of wavelengths, though it does not produce perfect correction. By combining more than two lenses of different composition, the degree of correction can be further increased, as seen in an apochromatic lens or "apochromat". Note that "achromat" and "apochromat" refer to the "type" of correction (2 or 3 wavelengths correctly focused), not the "degree" (how defocused the other wavelengths are), and an achromat made with sufficiently low dispersion glass can yield significantly better correction than an achromat made with more conventional glass. Similarly, the benefit of apochromats is not simply that they focus 3 wavelengths sharply, but that their error on other wavelength is also quite small.
Many types of glass have been developed to reduce chromatic aberration. These are low dispersion glass, most notably, glasses containing fluorite. These hybridized glasses have a very low level of optical dispersion; only two compiled lenses made of these substances can yield a high level of correction.
The use of achromats was an important step in the development of the optical microscope and in telescopes.
An alternative to achromatic doublets is the use of diffractive optical elements. Diffractive optical elements are able to generate arbitrary complex wave fronts from a sample of optical material which is essentially flat. Diffractive optical elements have complementary dispersion characteristics to that of optical glasses and plastics. In the visible part of the spectrum, diffractives have an Abbe number of −3.5. Diffractive optical elements can be fabricated using diamond turning techniques.
Mathematics of chromatic aberration minimization.
For a doublet consisting of two thin lenses in contact, the Abbe number of the lens materials is used to calculate the correct focal length of the lenses to ensure correction of chromatic aberration. If the focal lengths of the two lenses for light at the yellow Fraunhofer D-line (589.2 nm) are "f"1 and "f"2, then best correction occurs for the condition:
where "V"1 and "V"2 are the Abbe numbers of the materials of the first and second lenses, respectively. Since Abbe numbers are positive, one of the focal lengths must be negative, i.e., a diverging lens, for the condition to be met.
The overall focal length of the doublet "f" is given by the standard formula for thin lenses in contact:
and the above condition ensures this will be the focal length of the doublet for light at the blue and red Fraunhofer F and C lines (486.1 nm and 656.3 nm respectively). The focal length for light at other visible wavelengths will be similar but not exactly equal to this.
Chromatic aberration is used during a duochrome eye test to ensure that a correct lens power has been selected. The patient is confronted with red and green images and asked which is sharper. If the prescription is right, then the cornea, lens and prescribed lens will focus the red and green wavelengths just in front, and behind the retina, appearing of equal sharpness. If the lens is too powerful or weak, then one will focus on the retina, and the other will be much more blurred in comparison.
Image processing to reduce the appearance of lateral chromatic aberration.
In some circumstances it is possible to correct some of the effects of chromatic aberration in digital post-processing. However, in real-world circumstances, chromatic aberration results in permanent loss of some image detail. Detailed knowledge of the optical system used to produce the image can allow for some useful correction. In an ideal situation, post-processing to remove or correct lateral chromatic aberration would involve scaling the fringed color channels, or subtracting some of a scaled versions of the fringed channels, so that all channels spatially overlap each other correctly in the final image.
As chromatic aberration is complex (due to its relationship to focal length, etc.) some camera manufacturers employ lens-specific chromatic aberration appearance minimization techniques. Almost every major camera manufacturer enables some form of chromatic aberration correction, both in-camera and via their proprietary software. Third party software tools such as PTLens are also capable of performing complex chromatic aberration appearance minimization with their large database of cameras and lens.
In reality, even a theoretically perfect post-processing based chromatic aberration reduction-removal-correction systems do not increase image detail as a lens that is optically well corrected for chromatic aberration would for the following reasons:
The above are closely related to the specific scene that is captured so no amount of programming and knowledge of the capturing equipment (e.g., camera and lens data) can overcome these limitations.
Photography.
The term "purple fringing" is commonly used in photography, although not all purple fringing can be attributed to chromatic aberration.
Similar colored fringing around highlights may also be caused by lens flare. Colored fringing around highlights or dark regions may be due to the receptors for different colors having differing dynamic range or sensitivity – therefore preserving detail in one or two color channels, while "blowing out" or failing to register, in the other channel or channels. On digital cameras, the particular demosaicing algorithm is likely to affect the apparent degree of this problem. Another cause of this fringing is chromatic aberration in the very small microlenses used to collect more light for each CCD pixel; since these lenses are tuned to correctly focus green light, the incorrect focusing of red and blue results in purple fringing around highlights. This is a uniform problem across the frame, and is more of a problem in CCDs with a very small pixel pitch such as those used in compact cameras. Some cameras, such as the Panasonic Lumix series and newer Nikon and Sony DSLRs, feature a processing step specifically designed to remove it.
On photographs taken using a digital camera, very small highlights may frequently appear to have chromatic aberration where in fact the effect is because the highlight image is too small to stimulate all three color pixels, and so is recorded with an incorrect color. This may not occur with all types of digital camera sensor. Again, the de-mosaicing algorithm may affect the apparent degree of the problem.
Black-and-white photography.
Chromatic aberration also affects black-and-white photography. Although there are no colors in the photograph, chromatic aberration will blur the image. It can be reduced by using a narrow-band color filter, or by converting a single color channel to black and white. This will, however, require longer exposure (and change the resulting image). (This is only true with panchromatic black-and-white film, since orthochromatic film is already sensitive to only a limited spectrum.)
Electron microscopy.
Chromatic aberration also affects electron microscopy, although instead of different colors having different focal points, different electron energies may have different focal points.

</doc>
<doc id="54751" url="https://en.wikipedia.org/wiki?curid=54751" title="Central Tibetan Administration">
Central Tibetan Administration

The Central Tibetan Administration (CTA; Tibetan: "Tsänjöl Bhömī Drikdzuk", , literally Exile Tibetan People's Organisation), is an organisation based in India with the stated goals of "rehabilitating Tibetan refugees and restoring freedom and happiness in Tibet". It is also referred to as the Tibetan Government in Exile, but while its internal structure is government-like, it has stated that it is "not designed to take power in Tibet"; rather, it will be dissolved "as soon as freedom is restored in Tibet" in favor of a government formed by Tibetans inside Tibet. In addition to political advocacy, it administers a network of schools and other cultural activities for Tibetans in India. On 11 February 1991, the CTA became a founding member of the Unrepresented Nations and Peoples Organization (UNPO) at a ceremony held at the Peace Palace in The Hague, Netherlands.
Position on Tibet.
The territory of Tibet is administered by the People's Republic of China, a situation that the Central Tibetan Administration considers an illegitimate military occupation. The position of the CTA is that Tibet is a distinct nation with a long history of independence. The position of the People's Republic of China holds that China is multi-ethnic and that Tibetans are among the recognised nations, that the central government of China (throughout its incarnations) has continuously exercised sovereignty over Tibet for over 700 years, that Tibet has not been independent but its "de facto" independence between 1912 and 1951 was "nothing but a fiction of the imperialists who committed aggression against China in modern history".
Headquarters.
The CTA is headquartered in McLeod Ganj, Dharamsala, India. It claims to represent the people of the entire Tibet Autonomous Region and Qinghai province, as well as two Tibetan Autonomous Prefectures and one Tibetan Autonomous County in Sichuan Province, one Tibetan Autonomous Prefecture and one Tibetan Autonomous County in Gansu Province and one Tibetan Autonomous Prefecture in Yunnan Province — all of which is termed "Historic Tibet" by the CTA.
The CTA attends to the welfare of the Tibetan exile community in India, who number around 100,000. It runs schools, health services, cultural activities and economic development projects for the Tibetan community. More than 1,000 refugees still arrive each year from China, usually via Nepal.
Green Book.
Tibetans living outside Tibet can apply at the CTA office in their country of residence for a personal document called the "Green Book", which serves as a receipt book for the person's "voluntary contributions" to the CTA and the evidence of their claims for "Tibetan citizenship."
For this purpose, CTA defines a Tibetan as "any person born in Tibet, or any person with one parent who was born in Tibet." As Tibetan refugees often lack documents attesting to their place of birth, the eligibility is usually established by an interview.
Blue Book.
The Blue Book or Tibetan Solidarity Partnership is a project by Central Tibetan Administration, in which the Tibetan Government in exile issues any supporter of Tibet who is of age 18 years or more a Blue Book. This initiative enables supporters of Tibet worldwide to make financial contributions to help the administration in supporting educational, cultural, developmental and humanitarian activities related to Tibetan children and refugees. The book is issued at various Tibet offices worldwide.
Internal structure.
The CTA operates under the "Charter of the Tibetans In-Exile", adopted in 1991. Executive authority is vested in the Kalon Tripa (chairperson of the cabinet, often translated as prime minister), an office currently held by Lobsang Sangay, who was elected in 2011. The Kalon Tripa is supported by a cabinet of ministers responsible for specific portfolios. Legislative authority is vested in the Parliament of the Central Tibetan Administration.
The Central Tibetan Administration's Department of Finance is made of seven departments and several special offices. Until 2003, it operated 24 businesses, including publishing, hotels, and handicrafts distribution companies. Officially, its annual revenue is US$22 million, with the biggest shares going to political activity ($7 million), and administration ($4.5 million). However, according to Michael Backman, these sums are "remarkably low" for what the organisation claims to do, and it probably receives millions more in donations. The CTA does not acknowledge such donations or their sources.
At the time of its founding, the Dalai Lama was head of the Central Tibetan Administration. Over the ensuing decades, a gradual transition to democratic governance was effected. The first elections for an exile parliament took place on September 2, 1960. The position of kalon tripa was later empowered to share executive authority with the Dalai Lama. The kalon tripa was initially appointed by the Dalai Lama, but, beginning in 2001, this position was democratically elected by the Tibetan exile voters. The first elected Kalon Tripa was a 62-year-old Buddhist monk, Lobsang Tenzin (better known as Samdhong Rinpoche), to the position of Prime Minister of the CTA. On 10 March 2011, the Dalai Lama proposed changes to the exile charter which would remove his position of authority within the organisation. These changes were ratified on 29 May 2011, resulting in the Kalon Tripa becoming the highest-ranking office holder.
Cabinet.
Notable past members of the Cabinet include Gyalo Thondup, the Dalai Lama's eldest brother, who served as Chairman of the Cabinet and as Minister of Security, and Jetsun Pema, the Dalai Lama's younger sister, who served variously as Minister of Health and of Education.
Politics.
Lynn Pulman, in her 1983 text on Tibetans living in India, argues that the broad goals of the CTA are to develop an intense cultural and political nationalism among Tibetans, to expand the charisma and structure of the Dalai Lama, and to establish and maintain "social, political, and economic boundaries" between the Tibetan diaspora and their host countries. To increase nationalism, the CTA has created the Tibetan Uprising Day holiday, and a Tibetan National Anthem which is sung daily in CTA-run schools. The CTA controls much of the Tibetan-language media which, according to Pulman, promote the idea that the Chinese are endeavouring to "eradicate the Tibetan race" and how it is the duty of the refugees to "maintain the greatness and vitality of Tibetan race and national culture." However, Lynn Pulman's findings are not the product of systematic research, for which Lynn had insufficient time, but of information gained from informal conversations with Tibetans, observations Lynn made, supplemented with the little published material available at the time.
Activities with other organisations.
The CTA is not recognised as a sovereign government by any country, but it receives financial aid from governments and international organisations for its welfare work among the Tibetan exile community in India. In October 1998, the Dalai Lama's administration acknowledged that it received US$1.7 million a year in the 1960s from the US Government through the Central Intelligence Agency, which had also trained a guerrilla force at Camp Hale in Colorado. On 11 February 1991, the CTA became a founding member of the Unrepresented Nations and Peoples Organization at a ceremony held at the Peace Palace in The Hague, Netherlands.

</doc>
<doc id="54752" url="https://en.wikipedia.org/wiki?curid=54752" title="Liu">
Liu

劉 / 刘 (Lao, Lau, Low, Lauv, Lieh, Lieu, Liew, Liu, Loo, Lowe, Liou or Yu) is a common Chinese family name. The Liu () as transcribed in English can represent several different surnames written in different Chinese characters:
In Cantonese transliteration, 刘/劉 (Liú) is Lau, Lao is also transliteration of 刘/劉 in Min Nan and Taiwanese Minnan Chinese language, whilst Liu is a different surname, 廖, pinyin: Liào, (Liêu in Vietnamese).
In Shanghainese, 刘/劉 is romanized as Lieu and rarely Lieh; 柳 is similarly romanized; whereas 廖 is transliterated as Lioh
In Teochew, 刘/劉 is usually romanized as Lau, Low or Lao; 柳 is written as Lew; and 廖 is romanized as Leow or Liau.
In Hakka, 刘/劉 is most commonly transliterated as Liew while 廖 is written as Liau or Liaw. The other variants of the romanised surname Liu, i.e. 柳, 留 and 六, are uncommon among speakers of Hakka.
The Indonesian-Chinese descent Latinise it according to Dutch pronunciation as Lauw.
In Vietnamese, the name can either take the form Liễu (in northern regions), or Lưu (in central or southern regions), or Lục. A few having Vietnamese-Chinese descent use the family name Lao.
Prominent people with the family name 刘 / 劉.
Commonly transcribed Liú, Lao or Lau.

</doc>
<doc id="54753" url="https://en.wikipedia.org/wiki?curid=54753" title="Tibet Autonomous Region">
Tibet Autonomous Region

The Tibet Autonomous Region (TAR) or Xizang Autonomous Region, called Tibet or Xizang (; ) for short, is a province-level autonomous region of the People's Republic of China (PRC). It was created in 1965 on the basis of Tibet's incorporation by the PRC in 1951.
Within China, Tibet is identified as an Autonomous Region. The current borders of Tibet were generally established in the 18th century and include about half of ethno-cultural Tibet. The Tibet Autonomous Region is the second-largest province-level division of China by area, spanning over , after Xinjiang, and mostly due to its harsh and rugged terrain, is the least densely populated provincial-level division of the PRC.
History.
Modern scholars still debate on the exact nature of relations between Tibet and the Chinese Ming dynasty (1368–1644) and whether the Ming had sovereignty over Tibet after the Mongol conquest of Tibet and Yuan administrative rule in the 13th and 14th centuries. While Tibet has formally been a protectorate of China and under administrative rule of the Qing dynasty (1644–1912) since 1720, from 1912 to 1950 Tibet was dissolved of suzerainty under China proper as a result of the Xinhai Revolution and concentration of the central government fighting against the Japanese invasion during World War II. Other parts of ethno-cultural Tibet (eastern Kham and Amdo) have also been under the administration of the Chinese dynastic government since the mid-eighteenth century; today they are distributed among the provinces of Qinghai, Gansu, Sichuan and Yunnan. (See also: Xikang province)
In 1950, the People's Liberation Army defeated the Tibetan army in a battle fought near the city of Chamdo. In 1951, the Tibetan representatives signed a 17-point agreement with the Chinese Central People's Government affirming China's sovereignty over Tibet and the incorporation of Tibet. The agreement was ratified in Lhasa a few months later. Although the 17-point agreement had provided for an autonomous administration led by the Dalai Lama, a "Preparatory Committee for the Autonomous Region of Tibet" (PCART) was established in 1955 to create a parallel system of administration along Communist lines. The Dalai Lama fled to India in 1959 and renounced the 17-point agreement. Tibet Autonomous Region was established in 1965, thus making Tibet an administrative division on the same legal footing as a Chinese province.
Geography.
The Tibet Autonomous Region is located on the Tibetan Plateau, the highest region on earth. In northern Tibet elevations reach an average of over . Mount Everest is located on Tibet's border with Nepal.
China's provincial-level areas of Xinjiang, Qinghai and Sichuan lie to the north, northeast, and east, respectively, of the Tibet AR. There is also a short border with Yunnan province to the southeast. The PRC has border disputes with the Republic of India over the McMahon Line of Arunachal Pradesh, known to the Chinese as "South Tibet". The disputed territory of Aksai Chin is to the west, and its boundary with that region is not defined. The other countries to the south are Myanmar, Bhutan and Nepal.
Physically, the Tibet AR may be divided into two parts, the "lakes region" in the west and north-west, and the "river region", which spreads out on three sides of the former on the east, south, and west. Both regions receive limited amounts of rainfall as they lie in the rain shadow of the Himalayas, however the region names are useful in contrasting their hydrological structures, and also in contrasting their different cultural uses which is nomadic in the lake region and agricultural in the river region. On the south the Tibet AR is bounded by the Himalayas, and on the north by a broad mountain system. The system at no point narrows to a single range; generally there are three or four across its breadth. As a whole the system forms the watershed between rivers flowing to the Indian Ocean − the Indus, Brahmaputra and Salween and its tributaries − and the streams flowing into the undrained salt lakes to the north.
The lake region extends from the Pangong Tso Lake in Ladakh, Lake Rakshastal, Yamdrok Lake and Lake Manasarovar near the source of the Indus River, to the sources of the Salween, the Mekong and the Yangtze. Other lakes include Dagze Co, Namtso, and Pagsum Co. The lake region is a wind-swept Alpine grassland. This region is called the Chang Tang (Byang sang) or 'Northern Plateau' by the people of Tibet. It is some broad, and covers an area about equal to that of France. Due to its great distance from the ocean it is extremely arid and possesses no river outlet. The mountain ranges are spread out, rounded, disconnected, separated by relatively flat valleys.
The Tibet AR is dotted over with large and small lakes, generally salt or alkaline, and intersected by streams. Due to the presence of discontinuous permafrost over the Chang Tang, the soil is boggy and covered with tussocks of grass, thus resembling the Siberian tundra. Salt and fresh-water lakes are intermingled. The lakes are generally without outlet, or have only a small effluent. The deposits consist of soda, potash, borax and common salt. The lake region is noted for a vast number of hot springs, which are widely distributed between the Himalaya and 34° N, but are most numerous to the west of Tengri Nor (north-west of Lhasa). So intense is the cold in this part of Tibet that these springs are sometimes represented by columns of ice, the nearly boiling water having frozen in the act of ejection.
The river region is characterised by fertile mountain valleys and includes the Yarlung Tsangpo River (the upper courses of the Brahmaputra) and its major tributary, the Nyang River, the Salween, the Yangtze, the Mekong, and the Yellow River. The Yarlung Tsangpo Canyon, formed by a horseshoe bend in the river where it flows around Namcha Barwa, is the deepest, and possibly longest canyon in the world. Among the mountains there are many narrow valleys. The valleys of Lhasa, Xigazê, Gyantse and the Brahmaputra are free from permafrost, covered with good soil and groves of trees, well irrigated, and richly cultivated.
The South Tibet Valley is formed by the Yarlung Tsangpo River during its middle reaches, where it travels from west to east. The valley is approximately 1200 kilometres long and 300 kilometres wide. The valley descends from 4500 metres above sea level to 2800 metres. The mountains on either side of the valley are usually around 5000 metres high. Lakes here include Lake Paiku and Lake Puma Yumco.
Government.
The Tibet Autonomous Region is a province-level entity of the People's Republic of China. It is governed by a People's Government, led by a Chairman. In practice, however, the Chairman is subordinate to the branch secretary of the Communist Party of China. As a matter of convention, the Chairman has almost always been an ethnic Tibetan, while the party secretary has almost always been a non-Tibetan. The current Chairman is Losang Jamcan and the current party secretary is Chen Quanguo.
India’s request, to open a consulate in Lhasa, capital of Tibet has been rejected by Beijing. Beijing, instead has offered Chengdu, the capital of Sichuan province. According to diplomatic sources familiar with the developments, the Chinese don’t want more consulates in Lhasa, where only Nepal has one.
Administrative divisions.
Tibet Autonomous Region is divided into seven prefecture-level divisions: five prefecture-level cities and two prefectures.
These in turn are subdivided into a total of 68 counties and five districts (Chengguan, Doilungdêqên, Samzhubzê, Karub, Bayi, and Nêdong).
Demography.
With an average of only two people per square kilometer, Tibet has the lowest population density among any of the Chinese province-level administrative regions, mostly due to its harsh and rugged terrain.
In 2011 the Tibetan population was three million. 
The ethnic Tibetans, comprising 90.48% of the population, mainly adhere to Tibetan Buddhism and Bön, although there is an ethnic Tibetan Muslim community. Other Muslim ethnic groups such as the Hui and the Salar have inhabited the Region. There is also a tiny Tibetan Christian community in eastern Tibet. Smaller tribal groups such as the Monpa and Lhoba, who follow a combination of Tibetan Buddhism and spirit worship, are found mainly in the southeastern parts of the region.
Historically, the population of Tibet consisted of primarily ethnic Tibetans. According to tradition the original ancestors of the Tibetan people, as represented by the six red bands in the Tibetan flag, are: the Se, Mu, Dong, Tong, Dru and Ra. Other traditional ethnic groups with significant population or with the majority of the ethnic group reside in Tibet include Bai people, Blang, Bonan, Dongxiang, Han, Hui people, Lhoba, Lisu people, Miao, Mongols, Monguor (Tu people), Menba (Monpa), Mosuo, Nakhi, Qiang, Nu people, Pumi, Salar, and Yi people.
According to Encyclopædia Britannica Eleventh Edition published between 1910–1911, total population of Tibetan capital of Lhasa, including the lamas in the city and vicinity, was about 30,000, and the permanent population also included Chinese families (about 2,000).
Most Han people in the TAR (8.17% of the total population) are recent migrants, because all of the Han were expelled from "Outer Tibet" (Central Tibet) following the British expedition until the establishment of the PRC.
Real population of Han people is almost one-third of the TAR population (more than 50% in Lhasa). Only 8% of Han people have household registration in TAR, other keep their household registration in place of origin.
Some ethnic Tibetans claim that, with the 2006 completion of the Qingzang Railway connecting the TAR to Qinghai Province, there has been an "acceleration" of Han migration into the region. The Central Tibetan Administration of the Dalai Lama claims that the PRC has actively swamped Tibet with migrants in order to alter Tibet's demographic makeup.
Religion.
The main religion in Tibet has been Buddhism since its outspread in the 8th century AD. Before the arrival of Buddhism, the main religion among Tibetans was an indigenous shamanic and animistic religion, Bon, which now comprises a sizeable minority and which would later influence the formation of Tibetan Buddhism.
According to estimates from the International Religious Freedom Report of 2012, most of Tibetans (who comprise 91% of the population of the Tibet Autonomous Region) are bound by Tibetan Buddhism, while a minority of 400,000 people (12.5% of the total population of the TAR) are bound to the native Bon or folk religions which share the image of Confucius (Tibetan: "Kongtse Trulgyi Gyalpo") with Chinese religion, though in a different light. According to some reports, the government of China has been promoting the Bon religion linking it with Confucianism.
Most of the Han Chinese who reside in Tibet practice their native Chinese folk religion ("Shendao" 神道, "Way of the Gods"). There is a Guandi Temple of Lhasa (拉萨关帝庙) where the Chinese god of war Guandi is identified with the cross-ethnic Chinese, Tibetan, Mongol and Manchu deity Gesar. The temple is built according to both Chinese and Tibetan architecture. It was first erected in 1792 under the Qing dynasty and renovated around 2013 after decades of disrepair.
Built or rebuilt between 2014 and 2015 is the Guandi Temple of Qomolangma (Mount Everest), on Ganggar Mount, in Tingri County.
There are four mosques in the Tibet Autonomous Region with approximately 4,000 to 5,000 Muslim adherents, although a 2010 Chinese survey found a higher proportion of 0.4%. There is a Catholic church with 700 parishioners, which is located in the traditionally Catholic community of Yanjing in the east of the region.
Towns and villages in Tibet.
"Comfortable Housing".
Beginning in 2006, 280,000 Tibetans who lived in traditional villages and as nomadic herdsmen have been forcefully relocated into villages and towns. In those areas new housing was built and existing houses were remodelled to serve a total of 2 million people. 
Those living in substandard housing were required to dismantle their houses and remodel them to government standards. Much of the expense was borne by the residents themselves often through bank loans. The population transfer program, which was first implemented in Qinghai where 300,000 nomads were resettled, is called "Comfortable Housing". which is part of the “Build a New Socialist Countryside” program. Its effect on Tibetan culture has been criticized by exiles and human rights groups. Finding employment is difficult for relocated persons who have only agrarian skills. Income shortfalls are made up for by government support programs. It was announced in 2011 that 20,000 Communist Party cadre were to be placed in the new towns.
Economy.
The Tibetans traditionally depended upon agriculture for survival. Since the 1980s, however, other jobs such as taxi-driving and hotel retail work have become available in the wake of Chinese economic reform. In 2011, Tibet's nominal GDP topped 60.5 billion yuan (US$9.60 billion), nearly more than seven times as big as the 11.78 billion yuan (US$1.47 billion) in 2000. In the past five years, Tibet's annual GDP growth has averaged 12%.
While traditional agriculture and animal husbandry continue to lead the area's economy, in 2005 the tertiary sector contributed more than half of its GDP growth, the first time it surpassed the area's primary industry. Rich reserves of natural resources and raw materials have yet to lead to the creation of a strong secondary sector, due in large part to the province's inhospitable terrain, low population density, an underdeveloped infrastructure and the high cost of extraction.
The collection of caterpillar fungus ("Cordyceps sinensis", known in Tibetan as "Yartsa Gunbu") in late spring / early summer is in many areas the most important source of cash for rural households. It contributes an average of 40% to rural cash income and 8.5% to the TAR's GDP.
The re-opening of the Nathu La pass (on southern Tibet's border with India) should facilitate Sino-Indian border trade and boost Tibet's economy.
In 2008, Chinese news media reported that the per capita disposable incomes of urban and rural residents in Tibet averaged 12,482 yuan (US$1,798) and 3,176 yuan (US$457) respectively.
The China Western Development policy was adopted in 2000 by the central government to boost economic development in western China, including the Tibet Autonomous Region.
Tourism.
Foreign tourists were first permitted to visit the Tibet Autonomous Region in the 1980s. While the main attraction is the Potala Palace in Lhasa, there are many other popular tourist destinations including the Jokhang Temple, Namtso Lake, and Tashilhunpo Monastery. Nonetheless, tourism in Tibet is still restricted for non-Chinese passport holders and Taiwan citizens, and presently the only way for foreigners to enter is via Tibet Entry Permit. The permit can only be obtained through a travel agency in Tibet, and travel in Tibet must be arranged in a group tour, in which the group must be accompanied by a licensed tour guide at all times. Those traveling into Tibet must specify every location they want to travel within the TAR, and thus cannot travel anywhere not specified in the application. Before entering on a train, plane, or road leading into Tibet, anyone without a Chinese passport must present the Tibet Entry Permit, or they will otherwise be denied entry. Even people coming to Tibet from Nepal must have arranged for the entry permit ahead of time. People barred from obtaining the permit are journalists, diplomats, professional media photographers, and government officials.
Transport.
Airports.
The civil airports in Tibet are Lhasa Gonggar Airport, Qamdo Bangda Airport, Nyingchi Airport, and the Gunsa Airport.
Gunsa Airport in Ngari Prefecture began operations on 1 July 2010, to become the fourth civil airport in China's Tibet Autonomous Region.
The Peace Airport for Xigazê was completed on 30 October 2010.
Nagqu Dagring Airport is expected to become the world's highest altitude airport by 2014 at 4,436 meters above sea level.
Railway.
The Qinghai–Tibet Railway from Golmud to Lhasa was completed on 12 October 2005. It opened to regular trial service on 1 July 2006. Five pairs of passenger trains run between Golmud and Lhasa, with connections onward to Beijing, Chengdu, Chongqing, Guangzhou, Shanghai, Xining and Lanzhou. The line includes the Tanggula Pass, which, at 5,072 m (16,640 ft) above sea level, is the world's highest railway.
The Lhasa–Xigazê Railway branch from Lhasa to Xigazê was completed in 2014. It opened to regular service on 15 August 2014.
The construction of first section of the Sichuan–Tibet Railway from Lhasa to Nyingchi. Construction work is expected to start in November 2014, and to take 7 years.

</doc>
<doc id="54754" url="https://en.wikipedia.org/wiki?curid=54754" title="Tibetan">
Tibetan

Tibetan can refer to:

</doc>
<doc id="54761" url="https://en.wikipedia.org/wiki?curid=54761" title="Chinese surname">
Chinese surname

Chinese surnames are used by Han Chinese and Sinicized ethnic groups in Mainland China, Hong Kong, Macau, Taiwan, Korea, Singapore, Vietnam and among overseas Chinese communities. In ancient times two types of surnames existed, namely "xing" () or clan names, and "shi" () or lineage names.
Chinese family names are patrilineal, passed from father to children. (In cases of adoption, the adoptee usually also takes the same surname.) Women do not normally change their surnames upon marriage, except in places with more Western influences such as Hong Kong. Traditionally Chinese surnames have been exogamous.
The colloquial expressions "laobaixing" (老百姓; lit. "old hundred surnames") and "bǎixìng" (, lit. "hundred surnames") are used in Chinese to mean "ordinary folks", "the people", or "commoners".
Origin of Chinese surnames.
Prior to the Warring States period (fifth century BC), only the ruling families and the aristocratic elite had surnames. Historically there was also a difference between clan names or xing (姓) and lineages names or shi (氏). Xing were surnames held by the noble clans. They generally are composed of a nü (女, "female") radical which has been taken by some as evidence they originated from matriarchal societies based on maternal lineages. Another hypothesis has been proposed by sinologist Léon Vandermeersch upon observation of the evolution of characters in oracular scripture from the Shang dynasty through the Zhou. The "female" radical seems to appear at the Zhou period next to Shang sinograms indicating an ethnic group or a tribe. This combination seems to designate specifically a female and could mean "lady of such or such clan". The structure of the "xing" sinogram could reflect the fact that in the royal court of Zhou, at least in the beginning, only females (wives married into the Zhou family from other clans) were called by their birth clan name, while the men were usually designated by their title or fief.
Prior to the Qin Dynasty (3rd century BC) China was largely a "fengjian" (feudal) society. As fiefdoms were divided and subdivided among descendants, so additional sub-surnames known as "shi" were created to distinguish between different seniority of lineages among the nobles though in theory they shared the same ancestor. In this way, a nobleman would hold a "shi" and a "xing". After the states of China were unified by Qin Shi Huang in 221 BC, surnames gradually spread to the lower classes and the difference between "xing" and "shi" blurred.
Many "shi" surnames survive to the present day. According to Kiang Kang-Hu, there are 18 sources from which Chinese surnames may be derived, while others suggested at least 24. These may be names associated with a ruling dynasty such as the various titles and names of rulers, nobility and dynasty, or they may be place names of various territories, districts, towns, villages, and specific locations, the title of official posts or occupations, or names of objects, or they may be derived from the names of family members or clans, and in a few cases, names of contempt given by a ruler. The following are some of the common sources:
Many also changed their surnames throughout history for a number of reasons. A ruler may bestow his own surname on those he considered to have given outstanding service to him, for example the surname Liu (劉) was granted by emperors in the Han Dynasty, Li (李) during the Tang Dynasty, and Zhao (趙) from the Song Dynasty. Others however may avoid using the name of a ruler, for example Shi (師) was changed to Shuai (帥) to avoid conflict with the name of Sima Shi. Others may modify their name in order to escape from their enemies at times of turmoil, for example Duanmu (端木) to Mu (木 and 沐), and Gong (共) to Gong (龔). The name may also be changed by simplification of the writing (e.g. Mu (幕) to Mo (莫)), or reducing from double or multiple character names to single character names (e.g. Duangan (段干) to Duan (段)). It may also have occurred through error, or changed due to a dissatisfaction with the name (e.g. 哀 meaning sorrow to 衷 meaning heartfelt feeling).
Distribution of surnames.
Surnames are not evenly distributed throughout China's geography. In northern China, Wang (王) is the most common surname, being shared by 9.9% of the population. Next are Li (李), Zhang (张/張) and Liu (刘/劉). In the south, Chen (陈/陳) is the most common, being shared by 10.6% of the population. Next are Li (李), Huang (黄), Lin (林) and Zhang (张/張). Around the major crossing points of the Yangtze River, the most common surname is Li (李), taking up 7.7%, followed by Wang (王), Zhang (张/張), Chen (陈/陳) and Liu (刘/劉).
A 1987 study showed over 450 family names in common use in Beijing, but there were fewer than 300 family names in Fujian.
A study by geneticist Yuan Yida has found that of all the people with a particular surname, there tends to be a population concentration in a certain province, as tabulated to the right. It does not show, however, the most common surnames in any one province.
The 55th most common family name "Xiao" () appears to be very rare in Hong Kong. This is explained by the fact Hong Kong uses traditional Chinese characters, not simplified Chinese characters. Originally, the surname 蕭 (Xiao) was rather common while the surname 肖 (Xiao) was extremely rare, if not non-existent (it is mentioned only sporadically in historical texts). The first round of simplification in 1956 simplified 蕭 into 萧, keeping 蕭/萧 and 肖 distinct. However the second-round in 1977, which has long been abolished, merged 萧 and 肖 into 肖. Despite the retraction of the second round, some people have kept 肖 as their surname, so that there are now two separate surnames, 萧 and 肖.
"Chén" (/) is perhaps the most common surname in Hong Kong and Macau, where it is romanized as Chan, and is also common in Taiwan, where it is romanized as Chen.
Fang (), which is only the 47th most common overall, is much more common in San Francisco's Chinatown in the United States, although the surname is more often than not romanized as Fong, as based on the Yue dialect. As with the concentration of family names, this can also be explained statistically, as a person with an uncommon name moving to an unsettled area and leaving his family name to large number of people.
After the Song Dynasty, surname distributions in China largely settled down. The Kuang family, for example, migrated from the capital in the north and settled in Guangdong after the revolts of the Song Dynasty. Villages are often made up of a single patrilineage, being individuals with the same surname, often with a common male ancestor. They usually intermarry with others from nearby villages, creating genetic clusters.
Surnames at present.
Of the thousands of surnames which have been identified from historical texts prior to the modern era, most have either been lost (see extinction of family names) or simplified. Historically there are close to 12,000 surnames recorded (including those from non-Han Chinese ethnic groups), of which only about 3,100 are in current use, a factor of almost 4:1 (about 75%) reduction. Surname extinction is due to various factors, such as people taking the names of their rulers, orthographic simplifications, taboos against using characters from an emperor's name, and others. A recent example of near surname extinction is the rare surname Shan .
The character is not able to be displayed on a computer and people born after the system change as well as people who didn't want a hassle had to change their name to another character such as Xian 冼.The name still exists for those who were grandfathered into it but some people from the village are concerned that future generations will forget their name origin.
While new names have arisen for various reasons, this has been outweighed by old names disappearing. The most significant factor affecting the surname frequency is other ethnic groups identifying as Han and adopting Han names. In recent centuries some two-character surnames have often dropped a character. Since the founding of the People's Republic of China, moreover, some surnames have been graphically simplified.
Although there are thousands of Chinese family names, the 100 most common, which together make up less than 5% of those in existence, are shared by 85% of the population. The three most common surnames in Mainland China are Li, Wang and Zhang, which make up 7.9%, 7.4% and 7.1% respectively. Together they number close to 300 million and are easily the most common surnames in the world. In Chinese, the phrase "three Zhang, four Li" () is used to say "just anybody".
In a 1990 study, the top 200 family names accounted for over 96% of a random sample of 174,900 persons, with over 500 other names accounting for the remaining 4%. In a different study (1987), which combined data from Taiwan and China (sample size of 570,000 persons), the top 19 names covered 55.6%, and the top 100 names covered 87% of the sample. Other data suggest that the top 50 names comprise 70% of the population.
Most commonly occurring Chinese family names have only one character; however, about twenty double-character family names have survived into modern times. These include Sima (, simp. ), Zhuge (, simp. ), Ouyang (, simp. ), occasionally romanized as "O'Young", suggesting an Irish origin to English-speakers, and Situ (or Sito ). Sima, Zhuge, and Ouyong also happen to be the surnames of four extremely famous premodern Chinese historical figures. There are family names with three or more characters, but those are not ethnically Han Chinese. For example, Aixinjueluo (, also romanized from the Manchu language as Aisin Gioro), was the family name of the Manchu royal family of the Qing dynasty.
Variations in romanization.
Transliteration of Chinese family names (see List of common Chinese surnames) into foreign languages poses a number of problems. Chinese surnames are shared by people speaking a number of dialects and languages which often have different pronunciations of their surnames. The spread of the Chinese diaspora into all parts of the world resulted in the Romanization of the surnames based on different languages. As a result, it is common for the same surname to be transliterated differently. In certain dialects, different surnames could be homonyms so it is common for family names to appear ambiguous when transliterated. Example: 鄭/郑 (pinyin: Zheng) can be romanized into Chang, Cheng, Chung, Teh, Tay, Tee, Tsang, Zeng or Zheng, (in pinyin, Chang, Cheng, Zheng and Zeng are all different names). Translating Chinese surnames from foreign transliteration often presents ambiguity. For example, the surname "Li" are all mandarin-based pinyin transliteration for the surnames 黎 (Lí); 李, 理 and 里 (Lǐ); 郦/酈, 栗, 厉/厲, and 利 (Lì) depending on the tone which are often omitted in foreign transliterations.
Due to the different pronunciation and romanizations, it is sometimes easy to tell whether a Chinese person has origins in China, Hong Kong, Indonesia, Malaysia, Philippines, Singapore, or Taiwan. In general people who are Mainland descent will have both their surnames and names in pinyin. Those who are Taiwanese descent use Wade-Giles romanization. People from Southeast Asia (mainly Thailand, Malaysia, Indonesia and the Philippines) and Hong Kong usually base their romanization of surnames and names on the Min, Hakka and Cantonese languages. The younger generation from Singapore predominantly have their surname in dialect and given name in English.
There are also people who use non-standard romanizations, e.g. the Hong Kong media mogul 邵逸夫 Run Run Shaw's surname 邵 is spelt as Shaw, pinyin: Shao. The use of different systems of romanization based on different Chinese language variants from 1900~1970 also contributed to the variations.
Some examples:
Malaysia/Singapore/Indonesia/Philippines: various spellings are used depending on name origin.
See List of common Chinese surnames for the different spellings and more examples.
Sociological use of surnames.
Throughout most of Chinese history, surnames have served sociological functions. Because of their association with the aristocratic elite in their early developments, surnames were often used as symbols of nobility. Thus nobles would use their surnames to be able to trace their ancestry and compete for seniority in terms of hereditary rank. Examples of early genealogies among the royalty can be found in Sima Qian's "Historical Records", which contain tables recording the descent lines of noble houses called "shibiao" ().
Later, during the Han Dynasty, these tables were used by prominent families to glorify themselves and sometimes even to legitimize their political power. For example, Cao Pi, who forced the abdication of the last Han emperor in his favor, claimed descent from the Yellow Emperor. Chinese emperors sometimes passed their own surnames to subjects as honors. Unlike European practice in which some surnames are obviously noble, Chinese emperors and members of the royal family had regular surnames except in cases where they came from non-Han ethnic groups. This was a result of Chinese imperial theory in which a commoner could receive the Mandate of Heaven and become emperor. Upon becoming emperor, the emperor would retain his original surname. Also as a consequence, many people also had the same surname as the emperor, but had no direct relation to the royal family.
The Tang Dynasty was the last period when the great aristocratic families, mostly descended from the nobility of pre-Qin states, held significant centralized and regional power. The surname was used as a source of prestige and common allegiance. During the period a large number of genealogical records called "pudie" () were compiled to trace the complex descent lines of clans and their marriage ties to other clans. A large number of these were collected by Ouyang Xiu in his "New History of Tang". To differentiate between different surnames, the Tang also choronyms before stating beforehand, for example Lǒngxī Lǐshì 隴西李氏, meaning Li of Longxi. These were generally the names of commanderies used prior to the reorganization during the Tang, so that they became exclusively associated to clans as their common use had died out. Cadet branches were also listed for further differentiation, such as Gūzāng Fáng 姑臧房, meaning Clan Li of Guzang. 
During the Song Dynasty, ordinary clans began to organize themselves into corporate units and produce genealogies. This trend was led by the poet Su Shi and his father. As competition for resources and positions in the bureaucracy intensified, individuals used their common ancestry and surname to promote solidarity. They established schools to educate their sons and held common lands to aid disadvantaged families. Ancestral temples were also erected to promote surname identity. Clan cohesion was usually encouraged by successive imperial governments since it aided in social stability. During the Qing Dynasty surname associations often undertook extrajudicial roles, providing primitive legal and social security functions. They played important roles in the Chinese diaspora to South-East Asia and elsewhere, providing the infrastructure for the establishment of trading networks. In southern China, however, clans sometimes engaged in armed conflict in competition for land. Of course, clans continued the tradition of tracing their ancestry to the distant past as a matter of prestige. Most of these origin myths, though well established, are spurious.
As a result of the importance of surnames, rules and traditions regarding family and marriage grew increasingly complex. For example, in Taiwan, there is a clan with the so-called "double Liao" surname. The story is that "Chang Yuan-zih of Liao’s in Siluo married the only daughter of Liao San-Jiou-Lang who had no son, and he took the oath that he should be in the name of Liao when alive and should be in the name of Chang after death." In some places, there are additional taboos against marriage between people of the same surname, considered to be closely related. Conversely, in some areas, there are different clans with the same surname which are not considered to be related, but even in these cases surname exogamy is generally practiced.
Surname identity and solidarity has declined markedly since the 1930s with the decline of Confucianism and later, the rise of Communism in Mainland China. During the Cultural Revolution, surname culture was actively persecuted by the government with the destruction of ancestral temples and genealogies. Moreover, the influx of Western culture and forces of globalization have also contributed to erode the previous sociological uses of the Chinese surnames.
Common Chinese surnames.
Mainland China.
According to a comprehensive survey of residential permits released by the Chinese Ministry of Public Security on April 24, 2007, the ten most common surnames in mainland China are Wang (王), Li (李), Zhang (张), Liu (刘), Chen (陈), Yang (杨), Huang (黄), Zhao (赵), Wu (吴), and Zhou (周). The same names were also found (in slightly different orders) by a fairly comprehensive survey of 296 million people in 2006, and by the 1982 census.
A commonly cited fact from the 1990 edition of the "Guinness Book of World Records" estimated that Zhang was the most common surname in the world, but no comprehensive information from China was available at the time and more recent editions have omitted the claim.
The MPS survey revealed that the top three surnames in China have a combined population larger than Indonesia, the world's fourth-most-populous country. The top 10 surnames each have a population greater than 20 million; the top 22 have populations of more than 10 million. The top 100 surnames cover 84.77% of China's population.
Taiwan.
Names in Taiwanboth among the immigrant ethnic Chinese and Taiwanese aboriginesare similar to those in southeast China but differ somewhat from the distribution of names among all Han Chinese. According to a comprehensive survey of residential permits released by the Taiwanese Ministry of the Interior's Department of Population in February, 2005, the ten most common surnames on Taiwan are Chen (陳), Lin (林), Huang (黃), Chang (張), Li (李), Wang (王), Wu (吳), Liu (劉), Tsai (蔡), and Yang (楊).
Taiwanese surnames include some archaic ones which have since become rare in China such as Ruan (阮), which became the common Vietnamese surname "Nguyen"and some local variants like Tu (塗) which do not even appear among the "Hundred Family Surnames". However, names on Taiwan show less diversity than China as a whole: the top ten comprise 52.63% of the Taiwanese population and the top hundred 96.11%. There were also only 1,989 surnames recorded by the Ministry's survey, against China's four or five thousand.
As is typical of China as a whole, these surnames conflate many different lineages and origins, although tradition may bind them to the same ancestral temples and rituals or ban intermarriage. For example, some aboriginal Taiwanese adopted the surname Pan (潘) from modification of their status as "barbarians" (番, "fan"). Some Taiwanese converts to Presbyterianism adopted the name Kai (偕, pin. "Xié") in honor of the Canadian missionary George Leslie Mackay (馬偕, POJ "Má-kai").

</doc>
<doc id="54762" url="https://en.wikipedia.org/wiki?curid=54762" title="Syndication">
Syndication

Syndication may refer to:

</doc>
<doc id="54764" url="https://en.wikipedia.org/wiki?curid=54764" title="Andrew Lloyd Webber">
Andrew Lloyd Webber

Andrew Lloyd Webber, Baron Lloyd-Webber Kt (born 22 March 1948) is an English composer and impresario of musical theatre.
Several of his musicals have run for more than a decade both in the West End and on Broadway. He has composed 13 musicals, a song cycle, a set of variations, two film scores, and a Latin Requiem Mass. Several of his songs have been widely recorded and were hits outside of their parent musicals, notably "The Music of the Night" from "The Phantom of the Opera", "I Don't Know How to Love Him" from "Jesus Christ Superstar", "Don't Cry for Me, Argentina" and "You Must Love Me" from "Evita", "Any Dream Will Do" from "Joseph and the Amazing Technicolor Dreamcoat" and "Memory" from "Cats".
He has received a number of awards, including a knighthood in 1992, followed by a peerage from Queen Elizabeth II for services to Music, seven Tonys, three Grammys (as well as the Grammy Legend Award), an Academy Award, fourteen Ivor Novello Awards, seven Olivier Awards, a Golden Globe, a Brit Award, the 2006 Kennedy Center Honors, and the 2008 Classic Brit Award for Outstanding Contribution to Music. He has a star on the Hollywood Walk of Fame, is an inductee into the Songwriter's Hall of Fame, and is a fellow of the British Academy of Songwriters, Composers and Authors.
His company, the Really Useful Group, is one of the largest theatre operators in London. Producers in several parts of the UK have staged productions, including national tours, of the Lloyd Webber musicals under licence from the Really Useful Group. Lloyd Webber is also the president of the Arts Educational Schools London, a performing arts school located in Chiswick, West London. He is involved in a number of charitable activities, including the Elton John AIDS Foundation, Nordoff Robbins, Prostate Cancer UK and War Child. In 1992 he set up the Andrew Lloyd Webber Foundation which supports the arts, culture and heritage in the UK.
Early life.
Andrew Lloyd Webber was born in Kensington, London, the elder son of William Lloyd Webber (1914–1982), a composer and organist, and Jean Hermione Johnstone (1921–1993), a violinist and pianist. His younger brother, Julian Lloyd Webber, is a renowned solo cellist.
Lloyd Webber started writing his own music at a young age, a suite of six pieces at the age of nine. He also put on "productions" with Julian and his Aunt Viola in his toy theatre (which he built at Viola's suggestion). Later, he would be the owner of a number of West End theatres, including the Palace. His aunt Viola, an actress, took him to see many of her shows and through the stage door into the world of the theatre. He also had originally set music to Old Possum's Book of Practical Cats at the age of 15.
In 1965, Lloyd Webber was a Queen's Scholar at Westminster School and studied history for a term at Magdalen College, Oxford, although he abandoned the course in Winter 1965 to study at the Royal College of Music and pursue his interest in musical theatre.
Professional career.
Early years.
Lloyd Webber's first collaboration with lyricist Tim Rice was "The Likes of Us", a musical based on the true story of Thomas John Barnardo. Although composed in 1965, it was not publicly performed until 2005, when a production was staged at Lloyd Webber's Sydmonton Festival. In 2008, amateur rights were released by the National Operatic and Dramatic Association (NODA) in association with the Really Useful Group. The first amateur performance was by a children's theatre group in Cornwall called "Kidz R Us". Stylistically, "The Likes of Us" is fashioned after the Broadway musical of the 1940s and 1950s; it opens with a traditional overture comprising a medley of tunes from the show, and the score reflects some of Lloyd Webber's early influences, particularly Richard Rodgers, Frederick Loewe, and Lionel Bart. In this respect, it is markedly different from the composer's later work, which tends to be either predominantly or wholly through-composed, and closer in form to opera than to the Broadway musical.
In 1968, Rice and Lloyd Webber were commissioned to write a piece for the Colet Court preparatory school, which resulted in "Joseph and the Amazing Technicolor Dreamcoat", a retelling of the biblical story of Joseph in which Lloyd Webber and Rice humorously pastiche a number of musical styles such as Elvis-style rock'n'roll, Calypso and country music. "Joseph" began life as a short cantata that gained some recognition on its second staging with a favourable review in "The Times". For its subsequent performances, Rice and Lloyd Webber revised the show and added new songs to expand it to a more substantial length. This culminated in a two-hour-long production being staged in the West End on the back of the success of "Jesus Christ Superstar".
In 1969 Rice and Lloyd Webber wrote a song for the Eurovision Song Contest called "Try It and See," which was not selected. With rewritten lyrics it became "King Herod's Song" in their third musical, "Jesus Christ Superstar" (1970).
The planned follow-up to "Jesus Christ Superstar" was a musical comedy based on the Jeeves and Wooster novels by P. G. Wodehouse. Tim Rice was uncertain about this venture, partly because of his concern that he might not be able to do justice to the novels that he and Lloyd Webber so admired. After doing some initial work on the lyrics, he pulled out of the project and Lloyd Webber subsequently wrote the musical with Alan Ayckbourn, who provided the book and lyrics. "Jeeves" failed to make any impact at the box office and closed after a short run of only three weeks. Many years later, Lloyd Webber and Ayckbourn revisited this project, producing a thoroughly reworked and more successful version entitled "By Jeeves" (1996). Only two of the songs from the original production remained ("Half a Moment" and "Banjo Boy").
Mid-1970s.
Lloyd Webber collaborated with Rice once again to write "Evita" (1978 in London/1979 in U.S.), a musical based on the life of Eva Perón. As with "Jesus Christ Superstar", Evita was released first as a concept album (1976) and featured Julie Covington singing the part of Eva Perón. The song "Don't Cry for Me Argentina" became a hit single and the musical was staged at the Prince Edward Theatre in a production directed by Harold Prince and starring Elaine Paige in the title role.
Patti LuPone created the role of Eva on Broadway for which she won a Tony. "Evita" was a highly successful show that ran for ten years in the West End. It transferred to Broadway in 1979. Rice and Lloyd Webber parted ways soon after "Evita". In an interview in 2011, LuPone commented "He writes crap music... "Evita" was his best score, "Evita" in its bizarreness - when I first heard it I thought 'I swear to God, he hated women' [...] There are some very romantic moments in his music, and there is some real...trash that he doesn't even think about parting with. He's not a very good editor of his own stuff."
In 1978, Lloyd Webber embarked on a solo project, the "Variations", with his cellist brother Julian based on the 24th Caprice by Paganini, which reached number two in the pop album chart in the United Kingdom. The main theme was used as the theme tune for ITV's long-running "South Bank Show" throughout its 32-year run.
1980s.
Lloyd Webber was the subject of "This Is Your Life" in November 1980 when he was surprised by Eamonn Andrews in the foyer of Thames Television's Euston Road Studios. He would be honoured a second time by the television programme in November 1994 when Michael Aspel surprised him at the Adelphi Theatre.
Lloyd Webber embarked on his next project without a lyricist, turning instead to the poetry of T. S. Eliot. "Cats" (1981) was to become the longest running musical in London, where it ran for 21 years before closing. On Broadway, "Cats" ran for 18 years, a record which would ultimately be broken by another Lloyd Webber musical, "The Phantom of the Opera".
"Starlight Express" (1984) was a commercial hit, but received negative reviews from the critics. It enjoyed a record run in the West End, but ran for less than two years on Broadway. The show has also seen two tours of the US, as well as an Australian/Japanese production, a three-year UK touring production, which transferred to New Zealand later in 2009. The show also runs full-time in a custom-built theatre in Bochum, Germany, where it has been running since 1988.
Lloyd Webber wrote a Requiem Mass dedicated to his father, William, who had died in 1982. It premiered at St. Thomas Church in New York on 24 February 1985. Church music had been a part of the composer's upbringing and the composition was inspired by an article he had read about the plight of Cambodian orphans. Lloyd Webber had on a number of occasions written sacred music for the annual Sydmonton Festival. Lloyd Webber received a Grammy Award in 1986 for "Requiem" in the category of best classical composition. "Pie Jesu" from Requiem achieved a high placing on the UK pop charts. Perhaps because of its large orchestration, live performances of the Requiem are rare.
"Cricket" (1986), also called "Cricket (Hearts and Wickets)", reunited Lloyd Webber with Tim Rice to create this short musical for Queen Elizabeth's 60th birthday, first performed at Windsor Castle. Several of the tunes were later used for "Aspects of Love" and "Sunset Boulevard".
Lloyd Webber also premiered "The Phantom of the Opera" in 1986, inspired by the 1911 Gaston Leroux novel. He wrote the part of Christine for his then-wife, Sarah Brightman, who played the role in the original London and Broadway productions alongside Michael Crawford as the Phantom. The production was directed by Harold Prince, who had also earlier directed "Evita." Charles Hart wrote the lyrics for "Phantom" with some additional material provided by Richard Stilgoe, with whom Lloyd-Webber co-wrote the book of the musical. It became a hit and is still running in both the West End and on Broadway; in January 2006 it overtook "Cats" as the longest-running musical on Broadway. On 11 February 2012, "Phantom of the Opera" played its 10,000th show on Broadway.
"Aspects of Love" followed in 1989, a musical based on the story by David Garnett. The lyrics were by Don Black and Charles Hart and the original production was directed by Trevor Nunn. "Aspects" had a run of four years in London, but closed after less than a year on Broadway. It has since gone on a tour of the UK.
1990s.
Lloyd Webber was asked to write a song for the 1992 Barcelona Olympics and composed "Amigos Para Siempre — Friends for Life" with Don Black providing the lyrics. This song was performed by Sarah Brightman and José Carreras.
Lloyd Webber had toyed with the idea of writing a musical based on Billy Wilder's critically acclaimed movie, "Sunset Boulevard", since the early 1970s when he saw the film, but the project didn't come to fruition until after the completion of "Aspects of Love" when the composer finally managed to secure the rights from Paramount Pictures, The composer worked with two collaborators, as he had done on "Aspects of Love"; this time Christopher Hampton and Don Black shared equal credit for the book and lyrics. The show opened at the Adelphi Theatre in London on 12 July 1993, and ran for 1,529 performances. In spite of the show's popularity and extensive run in London's West End, it lost money due to the sheer expense of the production.
In 1994, "Sunset Boulevard" became a successful Broadway show, opening with the largest advance in Broadway history, and winning seven Tony Awards that year. Even so, by its closing in 1997, "it had not recouped its reported $13 million investment."
From 1995-2000, Lloyd Webber wrote the Matters of Taste column in The Daily Telegraph where he reviewed restaurants and hotels, and these were illustrated by Lucinda Rogers.
In 1998, Lloyd Webber released a film version of "Cats", which was filmed at the Adelphi Theatre in London. David Mallet directed the film, and Gillian Lynne choreographed it. The cast consisted of performers who had been in the show before, including Ken Page (the original Old Deuteronomy on Broadway), Elaine Paige (original Grizabella in London) and Sir John Mills as Gus: the Theatre Cat.
In 1998 "Whistle Down the Wind" made its debut, a musical written with lyrics supplied by Jim Steinman. Originally opening in Washington, Lloyd Webber was reportedly not happy with the casting or Harold Prince's production and the show was subsequently revised for a London staging directed by Gale Edwards, the production is probably most notable for the number-one hit from Boyzone "No Matter What" which left only the UK charts when the price of the CD single was changed to drop it out of the official top ten. His "The Beautiful Game" opened in London and has never been seen on Broadway. The show had a respectable run at The Cambridge Theatre in London. The show has been re-worked into a new musical, "The Boys in the Photograph", which had its world première at The Liverpool Institute for Performing Arts in April 2008.
2000s.
Having achieved great popular success in musical theatre, Lloyd Webber was referred to by "The New York Times" in 2001 as "the most commercially successful composer in history."
On 16 September 2004, his production of "The Woman in White" opened at the Palace Theatre in London. It ran for 19 months and 500 performances. A revised production opened on Broadway at the Marquis Theatre on 17 November 2005. Garnering mixed reviews from critics, due in part to the frequent absences of the show's star Maria Friedman due to breast cancer treatment, it closed only a brief three months later on 19 February 2006.
Lloyd Webber produced a staging of "The Sound of Music", which débuted November 2006. He made the controversial decision to choose an unknown to play leading lady Maria, who was found through the BBC's reality television show "How Do You Solve a Problem like Maria?", in which he was a judge. The winner of the show was Connie Fisher.
It was announced on 25 August 2006, on his personal website, that his next project would be "The Master and Margarita"; however, it was announced in late March 2007 that he had abandoned the project.
In September 2006, Lloyd Webber was named to be a recipient of the prestigious Kennedy Center Honors with Zubin Mehta, Dolly Parton, Steven Spielberg, and Smokey Robinson. He was recognised for his outstanding contribution to American performing arts. He attended the ceremony on 3 December 2006; it aired on 26 December 2006. On 11 February 2007, Lloyd Webber was featured as a guest judge on the reality television show "" The contestants all sang "The Phantom of the Opera".
Between April and June 2007, he appeared in BBC One's "Any Dream Will Do!", which followed the same format as "How Do You Solve a Problem Like Maria?". Its aim was to find a new Joseph for his revival of "Joseph and the Amazing Technicolor Dreamcoat". Lee Mead won the contest after quitting his part in the ensemble – and as understudy in "The Phantom of the Opera" – to compete for the role. Viewers' telephone voting during the series raised more than £500,000 for the BBC's annual "Children in Need" charity appeal, according to host Graham Norton on air during the final. On 1 July 2007, Lloyd Webber presented excerpts from his musicals as part of the Concert for Diana organised to celebrate the life of Diana, Princess of Wales.
The BBC Radio 2 broadcast a concert of music from the Lloyd-Webber musicals on 24 August 2007. Denise Van Outen introduced songs from "Whistle Down the Wind", "The Beautiful Game", "Tell Me on a Sunday", "The Woman in White", "Evita" and "Joseph and the Amazing Technicolor Dreamcoat" – as well as Rodgers and Hammerstein's "The Sound of Music", which Webber revived in 2006 at the London Palladium and 2002's Lloyd Webber-produced "Bollywood"-style musical "Bombay Dreams" by A. R. Rahman and Don Black.
In April 2008, Lloyd Webber reprised his role as judge, this time in the BBC musical talent show "I'd Do Anything". The show followed a similar format to its "Maria" and "Joseph" predecessors, this time involving a search for an actress to play the role of Nancy in an upcoming West End production of the Lionel Bart musical "Oliver!" The show also featured a search for three young actors to play and share the title character's role, but the show's main focus was on the search for Nancy. The role was won by Jodie Prenger despite Lloyd Webber's stated preference for one of the other contestants; the winners of the Oliver role were Harry Stott, Gwion Wyn-Jones and Laurence Jeffcoate. Also in April 2008. Lloyd Webber was featured on the U.S. talent show "American Idol", acting as a mentor when the 6 finalists had to select one of Lloyd Webber's songs to perform for the judges that week.
Lloyd Webber accepted the challenge of managing the UK's entry for the 2009 Eurovision Song Contest, to be held in Moscow. In early 2009 a series, called "", was broadcast to find a performer for a song that he would compose for the competition. Jade Ewen won the right to represent Britain, winning with It's My Time, by Lloyd Webber and Diane Warren. At the contest, Lloyd Webber accompanied her on the piano during the performance. The United Kingdom finished 5th in the contest. The winner was Norway's Alexander Rybak with his world record composition "Fairytale".
On 8 October 2009, Lloyd Webber launched the musical "Love Never Dies" at a press conference held at Her Majesty's Theatre, where the original "Phantom" has been running since 1986. Also present were Sierra Boggess, who has been cast as Christine Daaé, and Ramin Karimloo, who portrayed Phantom, a role he most recently played in the West End.
2010s.
Following the opening of "Love Never Dies", Lloyd Webber again began a search for a new musical theatre performer in the BBC One series "Over the Rainbow". He cast the winner, Danielle Hope, in the role of Dorothy and a dog to play Toto in his forthcoming stage production of "The Wizard of Oz". He and lyricist and composer Tim Rice wrote a number of new songs for the production to supplement the songs from the film.
On 26 February 2010, he appeared on BBC's "Friday Night with Jonathan Ross" to promote "Love Never Dies".
On 1 March 2011, "The Wizard of Oz" opened at The Palladium Theatre, starring Danielle Hope as Dorothy and Michael Crawford as the Wizard.
In 2012 Lloyd Webber fronted a new ITV primetime show "Superstar" which gave the UK public the chance to decide who would play the starring role of Jesus in an upcoming arena tour of "Jesus Christ Superstar". The arena tour started in September 2012 and also starred comedian Tim Minchin as Judas Iscariot, former Spice Girl Melanie C as Mary Magdalene and BBC Radio 1 DJ Chris Moyles as King Herod. Tickets for most venues went on sale on 18 May 2012.
Webber caused controversy with a series of comments about Eurovision in a "Radio Times" interview. He said: "I don't think there's any point in beating around the bush. I saw no black faces on the programme Eurovision 2012. I was questioned by the press over Jade Ewen's race, and I think we would have placed second, but there is a problem when you go further east. If you're talking about Western Europe it's fine, but Ukraine, not so good." The EBU corrected Webber, telling him Ukraine's singer Gaitana was black, that year's winner Loreen for Sweden was of North African background and accompanied by a black backing dancer, and France's contestant Anggun was Indonesian. The contest organisers also told Webber that black singer Dave Benton won for Estonia in 2001. The EBU thoroughly denied racism in its show, and insisted it unites Europe for three nights in a year.
In 2013, Webber reunited with Christopher Hampton and Don Black on "Stephen Ward the Musical".
In 2014, it was announced that Webber's next project would be a musical adaptation of the 2003 film "School of Rock". On January 19, 2015, auditions opened for children aged nine to fifteen in cooperation with the School of Rock music education program, which predated the film by several years.
Accusations of plagiarism.
Lloyd Webber has been accused of plagiarism in his works. The Dutch composer Louis Andriessen commented that: "There are two sorts of stealing (in music) – taking something and doing nothing with it, or going to work on what you've stolen. The first is plagiarism. Andrew Lloyd Webber has yet to think up a single note; in fact, the poor guy's never invented one note by himself. That's rather poor".
However, Lloyd Webber's biographer, John Snelson, countered such accusations. He acknowledged a similarity between the Andante movement of Mendelssohn's Violin Concerto in E minor and the "Jesus Christ Superstar" song "I Don't Know How to Love Him", but wrote that Lloyd Webber:
...brings a new dramatic tension to Mendelssohn's original melody through the confused emotions of Mary Magdalene. The opening theme may be Mendelssohn, but the rhythmic and harmonic treatment along with new lines of highly effective melodic development are Lloyd Webber's. The song works in its own right as its many performers and audiences can witness.
In interviews promoting "Amused to Death", Roger Waters, formerly of Pink Floyd, claimed that Lloyd Webber had copied a short chromatic riff from the 1971 song "Echoes" for sections of "The Phantom of the Opera", released in 1986; nevertheless, he decided he did not want to file a lawsuit. The songwriter Ray Repp also claimed that Lloyd Webber stole a different melody from his own song "Till You". Unlike Roger Waters, Ray Repp did decide to sue, but the court ruled in Lloyd Webber's favour.
Politics.
Lloyd Webber was made a Conservative life peer in 1997.
Tax credits vote.
Lloyd Webber was involved in a controversial House of Lords vote about the Tory government's proposed cuts to Tax Credits in October 2015. He voted with the Government; he was in London having flown in from the US to attend the opening night of Cats at the London Palladium. Lloyd Webber had voted only 30 times in the past 14 years, and not voted at all in the previous 2 years.
Personal life.
Lloyd Webber has married three times. He married first Sarah Hugill on 24 July 1971 and they divorced on 14 November 1983. Together they had two children; a daughter and a son:
He then married singer Sarah Brightman on 22 March 1984 in Hampshire. He cast Brightman in the lead role in his musical "The Phantom of the Opera", among other notable roles. They divorced on 3 January 1990.
Thirdly, he married Madeleine Gurdon in Westminster on 9 February 1991. They have three children, two sons and one daughter, all of whom were born in Westminster:
The Sunday Times Rich List 2006 ranked him the 87th-richest man in Britain with an estimated fortune of £700 million. His wealth increased to £750 million in 2007, but the publication ranked him 101st in 2008. He lives at Sydmonton Court, Hampshire, and owns much of nearby Watership Down. Lloyd Webber is an art collector, with a passion for Victorian art. An exhibition of works from his collection was presented at the Royal Academy in 2003 under the title "Pre-Raphaelite and Other Masters – The Andrew Lloyd Webber Collection". He is also a devoted supporter of Leyton Orient Football Club.
Politically, Lloyd Webber has supported the UK's Conservative Party, allowing his song "Take That Look Off Your Face" to be used on a party promotional film seen by an estimated 1 million people in 80 cinemas before the 2005 UK General Election to accompany pictures of Prime Minister Tony Blair allegedly "smirking", the party said. In 2009, he publicly criticised the Labour government's introduction of a new 50% rate of income tax on Britain's top earners, claiming it would damage the country by encouraging talented people to leave. In August 2014, Lloyd Webber was one of 200 public figures who were signatories to a letter to "The Guardian" opposing Scottish independence in the run-up to September's referendum on that issue.
In late 2009, Lloyd Webber had surgery for early-stage prostate cancer, but had to be readmitted to hospital with post-operative infection in November. In January 2010, he declared he was cancer-free. He had his prostate completely removed as a preventative measure.
In 2006, Lloyd Webber planned to sell "Portrait of Angel Fernández de Soto" by Pablo Picasso to benefit the Andrew Lloyd Webber Foundation. In November 2006, he withdrew the painting from auction after a claim that the previous owner had been forced to sell it under duress in Nazi Germany. An out-of-court settlement was reached, where the foundation retained ownership rights. On 23 June 2010, the painting was sold at auction for £34.7 million to an anonymous telephone bidder.
Honours and styles of address.
Honours.
Andrew Lloyd Webber was knighted by the Queen in 1992. In 1997, he was created a life peer as Baron Lloyd-Webber, of Sydmonton in the County of Hampshire. He is properly styled as The Lord Lloyd-Webber; the title is hyphenated, although his surname is not. He sits as a Conservative member of the House of Lords.
Awards.
Academy Awards.
One nomination for Best Original Song Score and Adaptation: 1973 motion picture "Jesus Christ Superstar"
One nomination for Best Original Song: "Learn to Be Lonely" from the 2004 motion picture "The Phantom of the Opera" .
Golden Globes.
Plus one nomination for Best Original Song: "Learn to Be Lonely" from the 2004 motion picture "The Phantom of the Opera".
Film adaptations.
There have been a number of film adaptations of the Lloyd Webber musicals. "Jesus Christ Superstar" (1973) was directed by Norman Jewison; "Evita" (1996) was directed by Alan Parker; and "The Phantom of the Opera" (2004) was directed by Joel Schumacher and co-produced by Lloyd Webber. "Cats", "Joseph and the Amazing Technicolor Dreamcoat", "Jesus Christ Superstar" and "By Jeeves" have been adapted into made for television films that have been released on DVD and VHS and often air on BBC.
Lloyd Webber produced "Bombay Dreams" with Indian composer A. R. Rahman in 2002.
A special performance of "The Phantom of the Opera at the Royal Albert Hall" for the 25th anniversary was broadcast live to cinemas in early October 2011 and later released on DVD and Blu-ray in February 2012. The same was also done with a reworked version of "Love Never Dies". Filmed in Melbourne, Australia, it received a limited cinema release in the US and Canada in 2012, to see if it would be viable to bring the show to Broadway. It received positive reviews and was No.1 on DVD charts in the UK and Ireland, and did well in America.
In February 2014, it was announced that Elton John's production company had acquired the rights to "Joseph and the Amazing Technicolor Dreamcoat", and is planning to adapt it as a new theatrical animated musical film.

</doc>
<doc id="54765" url="https://en.wikipedia.org/wiki?curid=54765" title="War (card game)">
War (card game)

War is a card game typically involving two players. It uses a standard French playing card deck. Due to its simplicity, it is played most often by children.
Gameplay.
The objective of the game is to win all cards.
The deck is divided evenly among the players, giving each a down stack. In unison, each player reveals the top card of their deck – this is a "battle" – and the player with the higher card takes both of the cards played and moves them to their stack. Aces are high, and suits are ignored.
If the two cards played are of equal value, then there is a "war". Both players place the next card 
of their pile face down, depending on the variant, and then another card face-up. The owner of the higher face-up card wins the war and adds all six (or ten) cards on the table to the bottom of their deck. If the face-up cards are again equal then the battle repeats with another set of face-down/up cards. This repeats until one player's face-up card is higher than their opponent's.
Most descriptions of War are unclear about what happens if a player runs out of cards during a war. In some variants, that player immediately loses. In others, the player may play the last card in their deck as their face-up card for the remainder of the war.
Game designer Greg Costikyan has observed that since there are no choices in the game, and all outcomes are random, it cannot be considered a game by some definitions. However, the rules often do not specify in which order the cards should be returned to the deck. If they are returned in a non-random order, the decision of putting one card before another after a win can change the overall outcome of the game. The effects of such decisions are more visible with smaller size decks as it is easier for a player to card count, however the decisions can still affect gameplay if taken in standard decks.
Versions.
Being a widely known game, war has picked up many optional variations, some of which are listed below.
Mobile versions.
The game has been developed for multiple platforms including Android, Apple iOS and Windows Phones. The games have varying difficulties and number of players that can be played against. 

</doc>
<doc id="54768" url="https://en.wikipedia.org/wiki?curid=54768" title="95 BC">
95 BC

__NOTOC__
Year 95 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Crassus and Scaevola (or, less frequently, year 659 "Ab urbe condita"). The denomination 95 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia Minor.
</onlyinclude>

</doc>
<doc id="54770" url="https://en.wikipedia.org/wiki?curid=54770" title="Sima Guang">
Sima Guang

Sima Guang (17 November 1019 – 11 October 1086), courtesy name Junshi, was a high-ranking Song dynasty scholar-official and historian who authored the monumental history book "Zizhi Tongjian". Sima was a political conservative who opposed Wang Anshi's reforms.
Early life.
Sima Guang was named after his birthplace Guāng Prefecture, where his father Sima Chi (司馬池) served as a county magistrate in Guangshan County. The Simas were originally from Xia County in Shǎn Prefecture, and claimed descent from Sima Fu in the 3rd century. A famous anecdote relates how the young Sima Guang once saved a playmate who had fallen into an enormous vat full of water. As other children scattered in panic, Sima Guang calmly picked up a rock and smashed a hole in the base of the pot. Water leaked out, and his friend was saved from drowning.
At age 6, Sima Guang once heard a lecture on the 4th-century BC history book "Zuo Zhuan". Fascinated, he was able to retell the stories to his family when he returned home. He became an avid reader, "to the point of not recognizing hunger, thirst, coldness or heat".
Sima Guang obtained early success as a scholar and officer. When he was barely twenty, he passed the Imperial examination with the highest rank of "jìnshì" (進士 "metropolitan graduate"), and spent the next several years in official positions.
Professional life.
Sima Guang is best remembered for his masterwork, "Zizhi Tongjian", and Rafe de Crespigny describes him as "perhaps the greatest of all Chinese historians" .
In 1064, Sima presented to Emperor Yingzong of Song the five-volume () "Liniantu" (歷年圖 "Chart of Successive Years"). It chronologically summarized events in Chinese history from 403 BCE to 959 CE, and served as a prospectus for sponsorship of his ambitious project in historiography. These dates were chosen because 403 BCE was the beginning of the Warring States period, when the ancient State of Jin was subdivided, which eventually led to the establishment of the Qin Dynasty; and because 959 CE was the end of the Five Dynasties and Ten Kingdoms period and the beginning of the Song Dynasty.
In 1066, he presented a more detailed eight-volume "Tongzhi" (通志; "Comprehensive Records"), which chronicled Chinese history from 403 BCE to 207 BCE (the end of the Qin Dynasty). The emperor issued an edict for the compilation of a groundbreaking universal history of China, granting full access to imperial libraries, and allocating funds for the costs of compilation, including research assistance by experienced historians such as Liu Ban (劉攽, 1022–88), Liu Shu (劉恕, 1032-78), and Fan Zuyu (范祖禹, 1041–98). After Yingzong died in 1067, Sima was invited to the palace to introduce his work-in-progress to Emperor Shenzong of Song. The new emperor not only confirmed the interest his father had shown, but showed his favor by bestowing an imperial preface in which he changed the title from "Tongzhi" ("Comprehensive Records") to "Zizhi Tongjian" ("Comprehensive Mirror to Aid in Government"). Scholars interpret the "Mirror" of the title to denote a work of reference and guidance, indicating that Shenzong accepted Sima as his guide in the study of history and its application to government. The emperor maintained his support for the compilation of this comprehensive history until its completion in 1084.
From the late 1060s, Sima came to assume a role as leader of what has been identified as a conservative faction at court, resolutely opposed to the New Policies of Chancellor Wang Anshi. Sima presented increasingly critical memorials to the throne until 1070, when he refused further appointment and withdrew from court. In 1071, he took up residence in Luoyang, where he remained with an official sinecure, providing sufficient time and resources to continue the compilation of Zizhi Tongjian. Though the historian and the emperor continued to disagree on policies, Sima's enforced retirement proved essential for him to complete his chronological history over the following one and a half decades. Contemporary accounts relate that when Sima Guang was writing his great opus, the Zizhi Tongjian, he slept on a log to work more and sleep less. He called this Jingzhen 警枕 (Alert Pillow), and used it throughout the period of Zizhi Tongjian's compilation.
Death.
Emperor Shenzong died in 1085, shortly after Sima had submitted Zizhi Tongjian to the throne. Sima was recalled to court and appointed to lead the government under Emperor Zhezong of Song. He used this time in power to repeal many of the New Policies, but he died the following year, in 1086.
Achievement.
As well as his achievements as a statesman and historian, Sima Guang was also a lexicographer (who perhaps edited the "Jiyun"), and spent decades compiling his 1066 "Leipian" ("Classified Chapters", cf. the Yupian) dictionary. It was based on the Shuowen Jiezi, and included 31,319 Chinese characters, many of which were coined in the Song and Tang Dynasty.

</doc>
<doc id="54771" url="https://en.wikipedia.org/wiki?curid=54771" title="RP (complexity)">
RP (complexity)

In computational complexity theory, randomized polynomial time (RP) is the complexity class of problems for which a probabilistic Turing machine exists with these properties:
In other words, the algorithm is allowed to flip a truly random coin while it is running. The only case in which the algorithm can return YES is if the actual answer is YES; therefore if the algorithm terminates and produces YES, then the correct answer is definitely YES; however, the algorithm can terminate with NO "regardless" of the actual answer. That is, if the algorithm returns NO, it might be wrong. 
Some authors call this class R, although this name is more commonly used for the class of recursive languages.
If the correct answer is YES and the algorithm is run "n" times with the result of each run statistically independent of the others, then it will return YES at least once with probability at least . So if the algorithm is run 100 times, then the chance of it giving the wrong answer every time is lower than the chance that cosmic rays corrupted the memory of the computer running the algorithm. In this sense, if a source of random numbers is available, most algorithms in RP are highly practical.
The fraction 1/2 in the definition is arbitrary. The set RP will contain exactly the same problems, even if the 1/2 is replaced by any constant nonzero probability less than 1; here constant means independent of the input to the algorithm.
Related complexity classes.
The definition of RP says that a YES answer is always right and that a NO answer might be wrong (because a question with the YES answer can be sometimes answered NO). In other words, while NO questions are always answered NO, you cannot trust the NO answer, it may be a mistaken answer to a YES question. The complexity class co-RP is similarly defined, except that NO is always right and YES might be wrong. In other words, it accepts all YES instances but can either accept or reject NO instances. The class BPP describes algorithms that can give incorrect answers on both YES and NO instances, and thus contains both RP and co-RP. The intersection of the sets RP and co-RP is called ZPP. Just as RP may be called R, some authors use the name co-R rather than co-RP.
Connection to P and NP.
P is a subset of RP, which is a subset of NP. Similarly, P is a subset of co-RP which is a subset of co-NP. It is not known whether these inclusions are strict. However, if the commonly believed conjecture P = BPP is true, then RP, co-RP, and P collapse (are all equal). Assuming in addition that P ≠ NP, this then implies that RP is strictly contained in NP. It is not known whether RP = co-RP, or whether RP is a subset of the intersection of NP and co-NP, though this would be implied by P = BPP.
A natural example of a problem in co-RP currently not known to be in P is Polynomial Identity Testing, the problem of deciding whether a given multivariate arithmetic expression over the integers is the zero-polynomial. For instance, is the zero-polynomial while
An alternative characterization of RP that is sometimes easier to use is the set of problems recognizable by nondeterministic Turing machines where the machine accepts if and only if at least some constant fraction of the computation paths, independent of the input size, accept. NP on the other hand, needs only one accepting path, which could constitute an exponentially small fraction of the paths. This characterization makes the fact that RP is a subset of NP obvious.

</doc>
<doc id="54772" url="https://en.wikipedia.org/wiki?curid=54772" title="ZPP (complexity)">
ZPP (complexity)

In complexity theory, ZPP (zero-error probabilistic polynomial time) is the complexity class of problems for which a probabilistic Turing machine exists with these properties:
In other words, if the algorithm is allowed to flip a truly-random coin while it is running, it will always return the correct answer and, for a problem of size "n", there is some polynomial "p"("n") such that the average running time will be less than "p"("n"), even though it might occasionally be much longer. Such an algorithm is called a Las Vegas algorithm.
Alternatively, ZPP can be defined as the class of problems for which a probabilistic Turing machine exists with these properties:
The two definitions are equivalent.
The definition of ZPP is based on probabilistic Turing machines, but, for clarity, note that other complexity classes based on them include BPP and RP. The class BQP is based on another machine with randomness: the quantum computer.
Intersection definition.
The class ZPP is exactly equal to the intersection of the classes RP and co-RP. This is often taken to be the definition of ZPP. To show this, first note that every problem which is in "both" RP and co-RP has a Las Vegas algorithm as follows:
Note that only one machine can ever give a wrong answer, and the chance of that machine giving the wrong answer during each repetition is at most 50%. This means that the chance of reaching the "k"th round shrinks exponentially in "k", showing that the expected running time is polynomial. This shows that RP intersect co-RP is contained in ZPP.
To show that ZPP is contained in RP intersect co-RP, suppose we have a Las Vegas algorithm C to solve a problem. We can then construct the following RP algorithm:
By Markov's Inequality, the chance that it will yield an answer before we stop it is 1/2. This means the chance we'll give the wrong answer on a YES instance, by stopping and yielding NO, is only 1/2, fitting the definition of an RP algorithm. The co-RP algorithm is identical, except that it gives YES if C "times out".
Witness and proof.
The classes NP, RP and ZPP can be thought of in terms of proof of membership in a set.
Definition: A "verifier" V for a set X is a Turing machine such that:
The string "w" can be thought of as the proof of membership. In the case of short proofs (of length bounded by a polynomial in the size of the input) which can be efficiently verified ("V" is a polynomial-time deterministic Turing machine), the string "w" is called a "witness". 
Notes:
The classes NP, RP and ZPP are sets which have witnesses for membership. The class NP requires only that witnesses exist. They may be very rare. Of the 2"f"(|"x"|) possible strings, with "f" a polynomial, only one need cause the verifier to accept (if x is in X. If x is not in X, no string will cause the verifier to accept). 
For the classes RP and ZPP any string chosen at random will likely be a witness.
The corresponding co-classes have witness for non-membership. In particular, co-RP is the class of sets for which, if x is not in X, any randomly chosen string is likely to be a witness for non-membership. ZPP is the class of sets for which any random string is likely to be a witness of x in X, or x not in X, which ever the case may be.
Connecting this definition with other definitions of RP, co-RP and ZPP is easy. The probabilistic polynomial-time Turing Machine "V*w"("x") corresponds to the deterministic polynomial-time Turing Machine "V"("x", "w") by replacing the random tape of "V*" with a second input tape for V on which is written the sequence of coin flips. By selecting the witness as a random string, the verifier is a probabilistic polynomial-time Turing Machine whose probability of accepting x when x is in "X" is large (greater than 1/2, say), but zero if "x" ∉ "X" (for RP); of rejecting x when x is not in X is large but zero if "x" ∈ "X" (for co-RP); and of correctly accepting or rejecting "x" as a member of "X" is large, but zero of incorrectly accepting or rejecting x (for ZPP).
By repeated random selection of a possible witness, the large probability that a random string is a witness gives an expected polynomial time algorithm for accepting or rejecting an input. Conversely, if the Turing Machine is expected polynomial-time (for any given x), then a considerable fraction of the runs must be polynomial-time bounded, and the coin sequence used in such a run will be a witness.
ZPP should be contrasted with BPP. The class BPP does not require witnesses, although witnesses are sufficient (hence BPP contains RP, co-RP and ZPP). A BPP language has V(x,w) accept on a (clear) majority of strings w if x is in X, and conversely reject on a (clear) majority of strings w if x is not in "X". No single string w need be definitive, and therefore they cannot in general be considered proofs or witnesses.
Complexity-theoretic properties.
It is known that ZPP is closed under complement; that is, ZPP = co-ZPP. 
ZPP is low for itself, meaning that a ZPP machine with the power to solve ZPP problems instantly (a ZPP oracle machine) is not any more powerful than the machine without this extra power. In symbols, ZPPZPP = ZPP.
ZPPNPBPP = ZPPNP.
NPBPP is contained in ZPPNP.
Connection to other classes.
Since ZPP = RP ∩ coRP, ZPP is obviously contained in both RP and coRP.
The class P is contained in ZPP, and some computer scientists have conjectured that P = ZPP, i.e., every Las Vegas algorithm has a deterministic polynomial-time equivalent.
A proof for ZPP = EXPTIME would imply that P ≠ ZPP, as P ≠ EXPTIME (see time hierarchy theorem).

</doc>
<doc id="54773" url="https://en.wikipedia.org/wiki?curid=54773" title="Cherry">
Cherry

A cherry is the fruit of many plants of the genus "Prunus", and is a fleshy drupe (stone fruit).
The cherry fruits of commerce usually are obtained from a limited number of species such as cultivars of the sweet cherry, "Prunus avium". The name 'cherry' also refers to the cherry tree, and is sometimes applied to almonds and visually similar flowering trees in the genus "Prunus", as in "ornamental cherry", "cherry blossom", etc. Wild Cherry may refer to any of the cherry species growing outside of cultivation, although "Prunus avium" is often referred to specifically by the name "wild cherry" in the British Isles.
Botany.
Many cherries are members of the subgenus Cerasus, which is distinguished by having the flowers in small corymbs of several together (not singly, nor in racemes), and by having smooth fruit with only a weak groove along one side, or no groove. The subgenus is native to the temperate regions of the Northern Hemisphere, with two species in America, three in Europe, and the remainder in Asia. Other cherry fruits are members of subgenus "Padus". Cherry trees with low exposure to light tend to have a bigger leaf size so they can intercept all light possible. Cherry trees with high exposure to light tend to have thicker leaves to concentrate light and have a higher photosynthetic capacity.
Most eating cherries are derived from either "Prunus avium", the sweet cherry (also called the wild cherry), or from "Prunus cerasus", the sour cherry.
History.
Etymology and antiquity.
The indigenous range of the sweet cherry extends through most of Europe, western Asia and parts of northern Africa, and the fruit has been consumed throughout its range since prehistoric times. A cultivated cherry, as well as the apricot, is recorded as having been brought to Rome by Lucius Licinius Lucullus from northeastern Anatolia, also known as the Pontus region, in 72 BC.
A form of cherry was introduced into England at Teynham, near Sittingbourne in Kent by order of Henry VIII, who had tasted them in Flanders. Cherry trees also provide food for the caterpillars of several Lepidoptera (moths and butterflies).
The English word cherry, French "cerise", Spanish "cereza", and Turkish "kiraz" all derive from the classical Greek (κέρασος) through the Latin "cerasum", which referred to the ancient Greek place name "Cerasus", today the city of Giresun in northern Turkey in the ancient Pontus region, from which the cherry was first exported to Europe. The ancient Greek word κερασός "cherry" itself is thought to be derived from a pre-Greek Anatolian language.
Cultivation.
The cultivated forms are of the species sweet cherry ("P. avium") to which most cherry cultivars belong, and the sour cherry ("P. cerasus"), which is used mainly for cooking. Both species originate in Europe and western Asia; they do not cross-pollinate. Some other species, although having edible fruit, are not grown extensively for consumption, except in northern regions where the two main species will not grow. Irrigation, spraying, labor, and their propensity to damage from rain and hail make cherries relatively expensive. Nonetheless, demand is high for the fruit. In commercial production, cherries are harvested by using a mechanized 'shaker'. Hand picking is also widely used to harvest the fruit to avoid damage to both fruit and trees.
Common rootstocks include Mazzard, Mahaleb, Colt, and Gisela Series, a dwarfing rootstock that produces trees significantly smaller than others, only 8 to 10 feet (2.5 to 3 meters) tall. Sour cherries require no pollenizer while few sweet varieties are self-fertile.
Growing season.
Cherries have a very short growing season and can grow in most temperate latitudes. Cherries blossom in April (in England) and the peak season for cherries is in the summer: In southern Europe in June, in North America in June, in England in mid-July, and in south British Columbia (Canada) in July to mid-August. In many parts of North America, they are among the first tree fruits to ripen.
In the Southern Hemisphere, in Australia and New Zealand, cherries are usually at their peak in late December and are widely associated with Christmas. 'Kordia' is an early variety which ripens during the beginning of December, 'Lapins peak' ripens near the end of December, and 'Sweethearts' finish slightly later.
Like most temperate-latitude trees, cherry seeds require exposure to cold to germinate (a mechanism the tree evolved to prevent germination during the autumn, which would then result in the seedling being killed by winter temperatures). The pits are planted in the autumn (after first being chilled) and seedlings emerge in the spring. A cherry tree will take three to four years to produce its first crop of fruit, and seven years to attain full maturity. Because of the cold-weather requirement, none of the "Prunus" genus can grow in tropical climates.
Pests and diseases.
Generally, cherry trees are a difficult fruit tree to grow and keep alive. They do not tolerate wetness. In Europe, the first visible pest in the growing season soon after blossom (in April in western Europe) usually is the black cherry aphid ("cherry blackfly", "Myzus cerasi"), which causes leaves at the tips of branches to curl, with the blackfly colonies exuding a sticky secretion which promotes fungal growth on the leaves and fruit. At the fruiting stage in June/July (Europe), the cherry fruit fly ("Rhagoletis cingulata" and "Rhagoletis cerasi") lays its eggs in the immature fruit, whereafter its larvae feed on the cherry flesh and exit through a small hole (about 1mm diametre), which in turn is the entry point for fungal infection of the cherry fruit after rainfall. In addition cherry trees are susceptible to bacterial canker, cytospora canker, brown rot, root rot, crown rot, and to several viruses.
Cultivars.
The following cultivars have gained the Royal Horticultural Society's Award of Garden Merit:
See cherry blossom and "Prunus" for ornamental trees.
Commercial production.
Middle East.
Major commercial cherry orchards in West Asia are in Turkey (mainly Anatolia), Iran, Uzbekistan, Lebanon (Bekaa Valley), Syria (Golan Heights) and Israel (Golan Heights, Gush Eztion and Northern Galilee).
Europe.
Major commercial cherry orchards in Europe are in Italy, Spain and other mediterranean regions, and to a smaller extent in the Baltic States and southern Scandinavia.
In France since the 1920s, the first cherries of the season come in April/May from the region of Céret (Pyrénées-Orientales), where the local producers send, as a tradition since 1932, the first crate of cherries to the French president of the Republic.
North America.
In the United States, most sweet cherries are grown in Washington, California, Oregon, Wisconsin, and Michigan. Important sweet cherry cultivars include Bing, Ulster, Rainier, Brooks, Tulare, King, and Sweetheart. In addition, the 'Lambert' variety is grown on the eastern side of Flathead Lake in northwestern Montana. Both Oregon and Michigan provide light-colored 'Royal Ann' ('Napoleon'; alternately 'Queen Anne') cherries for the maraschino cherry process. Most sour (also called tart) cherries are grown in Michigan, followed by Utah, New York, and Washington. Sour cherries include 'Nanking' and 'Evans'. Traverse City, Michigan claims to be the "Cherry Capital of the World", hosting a National Cherry Festival and making the world's largest cherry pie. The specific region of northern Michigan known for tart cherry production is referred to as the "Traverse Bay" region.
Native and non-native sweet cherries grow well in Canada's provinces of Ontario and British Columbia where an annual cherry fiesta has been celebrated for 66 consecutive years (including 2014) in the Okanagan Valley town of Osoyoos. In addition to the Okanagan, other British Columbia cherry growing regions are the Similkameen Valley and Kootenay Valley, all three regions together producing 5.5 million kg annually or 60% of total Canadian output. Sweet cherry varieties in British Columbia include Rainier, Van, Chelan, Lapin, Sweetheart, Skeena, Staccato, Christalina and Bing.
Australia.
In Australia, cherries are grown in all the states except for the Northern Territory. The major producing regions are located in the temperate areas within New South Wales, Victoria, South Australia and Tasmania. Western Australia has limited production in the elevated parts in southwest of the state. Key production areas include Young, Orange and Bathurst in New South Wales, Wandin, the Goulburn and Murray valley areas in Victoria, the Adelaide Hills region in South Australia, and the Huon and Derwent Valleys in Tasmania.
Key commercial varieties in order of seasonality include 'Empress', 'Merchant', 'Supreme', 'Ron's seedling', 'Chelan', 'Ulster', 'Van', 'Bing', 'Stella', 'Nordwunder', 'Lapins', 'Simone', 'Regina', 'Kordia' and 'Sweetheart'. New varieties are being introduced, including the late season 'Staccato' and early season 'Sequoia'. The Australian Cherry Breeding program is developing a series of new varieties which are under testing evaluation.
The New South Wales town of Young is called the "Cherry Capital of Australia" and hosts the National Cherry Festival.
Nutritional value.
As raw fruit, sweet cherries provide little nutrient content per 100 g serving (nutrient table). Dietary fiber and vitamin C are present in moderate content while other vitamins and dietary minerals each supply less than 10% of the Daily Value (DV) per serving, respectively (table).
Compared to sweet cherries, raw sour cherries contain slightly higher content per 100 g of vitamin C (12% DV) and vitamin A (8% DV) (table).
Other uses.
The wood of some cherry species is especially esteemed for the manufacture of fine furniture.
Species.
The list below contains many "Prunus" species that bear the common name cherry, but they are not necessarily members of the subgenus "Cerasus", or bear edible fruit. For a complete list of species, see "Prunus". Some common names listed here have historically been used for more than one species, e.g. "rock cherry" is used as an alternative common name for both "P. prostrata" and "P. mahaleb" and "wild cherry" is used for several species.

</doc>
<doc id="54774" url="https://en.wikipedia.org/wiki?curid=54774" title="93 BC">
93 BC

__NOTOC__
Year 93 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Flaccus and Herennius (or, less frequently, year 661 "Ab urbe condita"). The denomination 93 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="54775" url="https://en.wikipedia.org/wiki?curid=54775" title="94 BC">
94 BC

__NOTOC__
Year 94 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Caldus and Ahenobarbus (or, less frequently, year 660 "Ab urbe condita"). The denomination 94 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="54776" url="https://en.wikipedia.org/wiki?curid=54776" title="90 BC">
90 BC

__NOTOC__
Year 90 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Caesar and Lupus (or, less frequently, year 664 "Ab urbe condita"). The denomination 90 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia Minor.
</onlyinclude>

</doc>
<doc id="54777" url="https://en.wikipedia.org/wiki?curid=54777" title="92 BC">
92 BC

__NOTOC__
Year 92 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Pulcher and Perperna (or, less frequently, year 662 "Ab urbe condita"). The denomination 92 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman republic.
</onlyinclude>

</doc>
<doc id="54778" url="https://en.wikipedia.org/wiki?curid=54778" title="91 BC">
91 BC

__NOTOC__
Year 91 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Philippus and Caesar (or, less frequently, year 663 "Ab urbe condita"). The denomination 91 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="54780" url="https://en.wikipedia.org/wiki?curid=54780" title="87 BC">
87 BC

__NOTOC__
Year 87 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Octavius and Cinna/Merula (or, less frequently, year 667 "Ab urbe condita"). The denomination 87 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By topic.
Technology.
</onlyinclude>

</doc>
<doc id="54781" url="https://en.wikipedia.org/wiki?curid=54781" title="89 BC">
89 BC

__NOTOC__
Year 89 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Strabo and Cato (or, less frequently, year 665 "Ab urbe condita"). The denomination 89 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia Minor.
</onlyinclude>

</doc>
<doc id="54782" url="https://en.wikipedia.org/wiki?curid=54782" title="88 BC">
88 BC

__NOTOC__
Year 88 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Sulla and Rufus (or, less frequently, year 666 "Ab urbe condita"). The denomination 88 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Greece.
</onlyinclude>

</doc>
<doc id="54783" url="https://en.wikipedia.org/wiki?curid=54783" title="Music theory">
Music theory

Music theory is the study of the practices and possibilities of music. It is derived from observation of, and involves hypothetical speculation about how musicians and composers make music. The term also describes the academic study and analysis of fundamental elements of music such as pitch, rhythm, harmony, and form, and refers to descriptions, concepts, or beliefs related to music. Because of the ever-expanding conception of what constitutes music (see Definition of music), a more inclusive definition could be that music theory is the consideration of any sonic phenomena, including silence, as it relates to music.
The present article is about music theory properly speaking, i.e. about theories, speculations and hypotheses made about the various aspects of music. It describes the elements of music only insofar as they give way to such theories; other information about these elements will be found in other articles such as Aspect of music and the specific parameters of music described there. Textbooks, especially in the United States of America, often also include under the term "theory" elements of musical acoustics, considerations of musical notation, techniques of (often tonal) composition (Harmony and Counterpoint), etc., which will not be dealt with as such here, but only as they were the subject of theories and hypotheses.
Music theory is a subfield of musicology, which is itself a subfield within the overarching field of the arts and humanities. Etymologically, "music theory" is an act of contemplation of music, from the Greek θεωρία, a looking at, viewing, contemplation, speculation, theory, also a sight, a spectacle. As such, it is often concerned with abstract musical aspects such as tuning and tonal systems, scales, consonance and dissonance, and rhythmic relationships, but there is also a body of theory concerning practical aspects, such as the creation or the performance of music, orchestration, ornamentation, improvisation, and electronic sound production. A person who researches, teaches, or writes articles about music theory is a music theorist. University study, typically to the M.A. or Ph.D level, is required to teach as a tenure-track music theorist in a US or Canadian university. Methods of analysis include mathematics, graphic analysis, and, especially, analysis enabled by Western music notation. Comparative, descriptive, statistical, and other methods are also used.
The development, preservation, and transmission of music theory may be found in oral and practical music-making traditions, musical instruments, and other artifacts. For example, ancient instruments from Mesopotamia, China, and prehistoric sites around the world reveal details about the music they produced and, potentially, something of the musical theory that might have been used by their makers (see History of music and Musical instrument). In ancient and living cultures around the world, the deep and long roots of music theory are clearly visible in instruments, oral traditions, and current music making. Many cultures, at least as far back as ancient Mesopotamia and ancient China have also considered music theory in more formal ways such as written treatises and music notation.
History.
Prehistory.
Preserved prehistoric instruments, artifacts, and later, depictions of performance in artworks give insight into early music-making and as such, might implicitly reveal something of a prehistoric theory of music. See for instance Paleolithic flutes, Gǔdí, Anasazi flute.
Antiquity.
Mesopotamia.
"See also Music of Mesopotamia"
Sumerian and Akkadian tablets do include musical information of a theoretical nature, mainly lists of intervals and tunings. Three tablets in particular, the first one known as the Philadelphia tablet (CBS 10996), the second one preserved in the British Museum (U. 3011), and the third one in the Berlin Museum (KAR 158), name intervals from a pentatonic or possibly a heptatonic scale. Another tablet from Ur (British Museum, U.7/80) similarly names the intervals for tuning a harp of seven strings, probably by a cycle of fifths. These texts are concerned mainly with terminology and with mathematical descriptions.
China.
"See also Music of China; Chinese musicology"
Much of Chinese music history and theory remains unclear.
The earliest texts about Chinese music theory are inscribed on the stone and bronze bells excavated in 1978 from the tomb of Marquis Yi (died 433 BCE) of the Zeng state. They include more than 2800 words describing theories and practices of music pitches of the time. The bells produce two intertwined pentatonic scales three tones apart, with additional pitches completing the chromatic scale.
Chinese theory starts from numbers, the main musical numbers being twelve, five and eight. Twelve refers to the number of pitches on which the scales can be constructed. The Lüshi chunqiu from about 239 BCE recalls the legend of Ling Lun. On order of the Yellow Emperor, Ling Lun collected twelve bamboo lengths with thick and even nodes. Blowing on one of these like a pipe, he found its sound agreeable and named it "huangzhong", the "Yellow Bell." He then heard phoenixes singing. The male and female phoenix each sang six tones. Ling Lun cut his bamboo pipes to match the pitches of the phoenixes, producing twelve pitch pipes in two sets: six from the male phoenix and six from the female: these were called the "lülü" or later the "shierlü".
The "lülü" formed the ritual scale to which many instruments were tuned. The name of the lowest sound, "huangzhong" also implyed 'musical correctness.' Its pitch formed a pitch standard, setting the base pitch of zithers, flutes and singers of imperial court orchestras. Straight-walled pitch pipes without finger holes were made of cast metal, their lengths specified by court regulations. The resulting chromatic scale provided twelve fundamental notes for the construction of the musical scales themselves. The "lülü" also has a cosmological value: its notes describe the energetic frequency of the twelve months of the year, the daily rhythm of the twelve bi-hours of the Chinese clock, the twelve main acupuncture meridians, etc.
The two sets of tones (male and female) dividing the twelve-tone scale were generated by the "Method of Subtracting and Adding Thirds," or "sanfen sunyi", which involved alternately rising a fifth and descending a fourth through the subtraction or addition of a third of the length of the preceding pitch pipe. The resulting pitches produced by adding a third (and descending a fourth) were referred to by Sima Qian in the Records of the Grand Historian (91 BCE) as pitches of "superior generation," that is, the pitches of Ling Lun’s male phoenix; the pitches produced by subtracting a third (and ascending a fifth) were referred to as pitches of "inferior generation," that is, the pitches of Ling Lun’s female phoenix.
"Apart from technical and structural aspects, ancient Chinese music theory also discusses topics such as the nature and functions of music. The "Yueji" ("Record of music", c1st and 2nd centuries BCE), for example, manifests Confucian moral theories of understanding music in its social context. Studied and implemented by Confucian scholar-officials [...], these theories helped form a musical Confucianism that overshadowed but did not erase rival approaches. These include the assertion of Mozi (c468–c376 BCE) that music wasted human and material resources, and Laozi’s claim that the greatest music had no sounds. [...] Even the music of the "qin" zither, a genre closely affiliated with Confucian scholar-officials, includes many works with Daoist references, such as "Tianfeng huanpei" ("Heavenly Breeze and Sounds of Jade Pendants")."
India.
The Samaveda and Yajurveda (c. 1200-1000 BCE) are among the earliest testimonies of Indian music, but they contain no theory properly speaking. The Natya Shastra, written between 200 BCE to 200 CE, discusses intervals ("Śrutis"), scales ("Grāmas"), consonances and dissonances, classes of melodic structure ("Mūrchanās", modes?), melodic types ("Jātis"), instruments, etc.
Greece.
"See also Musical system of ancient Greece; List of music theorists#Antiquity"
Early preserved Greek writings on music theory include two types of works:
Several names of theorists are known before these works, including Pythagoras (c. 570 – c. 495 BCE), Philolaus (c. 470 – c. 385 BCE), Archytas (428–347 BCE), and others.
Works of the first type (technical manuals) include
More philosophical treatises of the second type include
Middle Ages.
"See also List of music theorists#Middle Ages"
China.
Some imported early Chinese instruments became important components of the entertainment music of the Sui (581–618) and Tang (618–907) courts: the bent-neck pipa (quxiang pipa), the bili, the konghou and the jiegu. They generated not only new repertories and performing practices but also new music theories. The pipa, for example, carried with it a theory of musical modes that subsequently led to the Sui and Tang theory of 84 musical modes.
Arabic countries.
Medieval Arabic music theorists include:
Modern.
Europe.
Renaissance.
"See also List of music theorists#Renaissance"
Baroque.
"See also List of music theorists#17th century; List of music theorists#18th century"
1750-1900.
As Western musical influence spread throughout the world in the 1800s, musicians adopted Western theory as an international standard—but other theoretical traditions in both textual and oral traditions remain in use. For example, the long and rich musical traditions unique to ancient and current cultures of Africa are primarily oral, but describe specific forms, genres, performance practices, tunings, and other aspects of music theory.
"See also List of music theorists#19th century"
Contemporary.
"See also List of music theorists#20th century; List of music theorists#21st century"
Fundamentals of music.
Music is composed of aural phenomena; "music theory" considers how those phenomena apply in music. Music theory considers melody, rhythm, counterpoint, harmony, form, tonal systems, scales, tuning, intervals, consonance, dissonance, durational proportions, the acoustics of pitch systems, composition, performance, orchestration, ornamentation, improvisation, electronic sound production, etc.
Pitch.
Pitch is the lowness or highness of a tone, for example the difference between middle C and a higher C. The frequency of the sound waves producing a pitch can be measured precisely, but the perception of pitch is more complex because we rarely hear a single frequency or pure pitch. In music, tones, even those sounded by solo instruments or voices, are usually a complex combination of frequencies, and therefore a mix of pitches. Accordingly, theorists often describe pitch as a subjective sensation.
Most people appear to possess relative pitch, which means they perceive each note relative to some reference pitch, or as some interval from the previous pitch. Significantly fewer people demonstrate absolute pitch (or perfect pitch), the ability to identify pitches without comparison to another pitch. Human perception of pitch can be comprehensively fooled to create auditory illusions. Despite these perceptual oddities, perceived pitch is nearly always closely connected with the fundamental frequency of a note, with a lesser connection to sound pressure level, harmonic content (complexity) of the sound, and to the immediately preceding history of notes heard. In general, the higher the frequency of vibration, the higher the perceived pitch. The lower the frequency, the lower the pitch. However, even for tones of equal intensity, perceived pitch and measured frequency do not stand in a simple linear relationship.
Intensity (loudness) can change perception of pitch. Below about 1000 Hz, perceived pitch gets lower as intensity increases. Between 1000 and 2000 Hz, pitch remains fairly constant. Above 2000 Hz, pitch rises with intensity. This is due to the ear's natural sensitivity to higher pitched sound, as well as the ear's particular sensitivity to sound around the 2000–5000 Hz interval, the frequency range most of the human voice occupies.
The difference in frequency between two pitches is called an interval. The most basic interval is the unison, which is simply two notes of the same pitch, followed by the slightly more complex octave: pitches that are either double or half the frequency of the other. The unique characteristics of octaves gave rise to the concept of what is called pitch class, an important aspect of music theory. Pitches of the same letter name that occur in different octaves may be grouped into a single "class" by ignoring the difference in octave. For example, a high C and a low C are members of the same pitch class—the class that contains all C's. The concept of pitch class greatly aids aspects of analysis and composition.
Although pitch can be identified by specific frequency, the letter names assigned to pitches are somewhat arbitrary. For example, today most orchestras assign Concert A (the A above middle C on the piano) to the specific frequency of 440 Hz, rather than, for instance, 435 Hz as it was in France in 1859. In England, that A varied between 439 and 452. These differences can have a noticeable effect on the timbre of instruments and other phenomena. Many cultures do not attempt to standardize pitch, often considering that it should be allowed to vary depending on genre, style, mood, etc. In historically informed performance of older music, tuning is often set to match the tuning used in the period when it was written. A frequency of 440 Hz was recommended as the standard pitch for Concert A in 1939, and in 1955 the International Organization for Standardization affirmed the choice. A440 is now widely, though not exclusively, the standard for music around the world.
Pitch is also an important consideration in tuning systems, or temperament, used to determine the intervallic distance between tones, as within a scale. Tuning systems vary widely within and between world cultures. In Western culture, there have long been several competing tuning systems, all with different qualities. Internationally, the system known as equal temperament is most commonly used today because it is considered the most satisfactory compromise that allows instruments of fixed tuning (e.g. the piano) to sound acceptably in tune in all keys.
Scales and modes.
Notes can be arranged in a variety of scales and modes. Western music theory generally divides the octave into a series of twelve tones, called a chromatic scale, within which the interval between adjacent tones is called a half step or semitone. In equal temperament each semitone is equidistant from the next, but other tuning systems are also used. Selecting tones from this set of 12 and arranging them in patterns of semitones and whole tones creates other scales.
The most commonly encountered scales are the seven-toned major, the harmonic minor, the melodic minor, and the natural minor. Other examples of scales are the octatonic scale and the pentatonic or five-tone scale, which is common in folk music and blues. Non-Western cultures often use scales that do not correspond with an equally divided twelve-tone division of the octave. For example, classical Ottoman, Persian, Indian and Arabic musical systems often make use of multiples of quarter tones (half the size of a semitone, as the name indicates), for instance in 'neutral' seconds (three quarter tones) or 'neutral' thirds (seven quarter tones)—they do not normally use the quarter tone itself as a direct interval.
In traditional Western notation, the scale used for a composition is usually indicated by a key signature at the beginning to designate the pitches that make up that scale. As the music progresses, the pitches used may change and introduce a different scale. Music can be transposed from one scale to another for various purposes, often to accommodate the range of a vocalist. Such transposition raises or lowers the overall pitch range, but preserves the intervallic relationships of the original scale. For example, transposition from the key of C major to D major raises all pitches of the scale of C major equally by a whole tone. Since the interval relationships remain unchanged, transposition may be unnoticed by a listener, however other qualities may change noticeably because transposition changes the relationship of the overall pitch range compared to the range of the instruments or voices that perform the music. This often affects the music's overall sound, as well as having technical implications for the performers.
The interrelationship of the keys most commonly used in Western tonal music is conveniently shown by the circle of fifths. Unique key signatures are also sometimes devised for a particular composition. During the Baroque period, emotional associations with specific keys, known as the doctrine of the affections, were an important topic in music theory, but the unique tonal colorings of keys that gave rise to that doctrine were largely erased with the adoption of equal temperament. However, many musicians continue to feel that certain keys are more appropriate to certain emotions than others. Indian classical music theory continues to strongly associate keys with emotional states, times of day, and other extra-musical concepts and notably, does not employ equal temperament.
Consonance and dissonance.
Consonance and dissonance are subjective qualities of the sonority of intervals that vary widely in different cultures and over the ages.
Consonance (or concord) is the quality of an interval or chord that seems stable and complete in itself. Dissonance (or discord) is the opposite in that it feels incomplete and "wants to" resolve to a consonant interval. Dissonant intervals seem to clash. Consonant intervals seem to sound comfortable together. Commonly, perfect fourths, fifths, and octaves and all major and minor thirds and sixths are considered consonant. All others are dissonant to greater or lesser degree.
Context and many other aspects can affect apparent dissonance and consonance. For example, in a Debussy prelude, a major second may sound stable and consonant, while the same interval may sound dissonant in a Bach fugue. In the Common Practice era, the perfect fourth is considered dissonant when not supported by a lower third or fifth. Since the early 20th century, Arnold Schoenberg’s concept of "emancipated" dissonance, in which traditionally dissonant intervals can be treated as "higher," more remote consonances, has become more widely accepted.
Rhythm.
Rhythm is produced by the sequential arrangement of sounds and silences in time. Meter measures music in regular pulse groupings, called measures or bars. The time signature or meter signature specifies how many beats are in a measure, and which value of written note is counted or felt as a single beat.
Through increased stress, or variations in duration or articulation, particular tones may be accented. There are conventions in most musical traditions for regular and hierarchical accentuation of beats to reinforce a given meter. Syncopated rhythms contradict those conventions by accenting unexpected parts of the beat. Playing simultaneous rhythms in more than one time signature is called polymeter. See also polyrhythm.
In recent years, rhythm and meter have become an important area of research among music scholars. Recent work in these areas includes books by Bengt-Olov Palmqvist, Fred Lerdahl and Ray Jackendoff, and Jonathan Kramer..
A landmark study in rhythm theory as relates to pitch and meter was published by musicologist Maury Yeston, "The Stratification of Musical Rhythm" (Yale University Press 1976)
Melody.
A melody is a series of tones sounding in succession that typically move toward a climax of tension then resolve to a state of rest. Because melody is such a prominent aspect in so much music, its construction and other qualities are a primary interest of music theory.
The basic elements of melody are pitch, duration, rhythm, and tempo. The tones of a melody are usually drawn from pitch systems such as scales or modes. Melody may consist, to increasing degree, of the figure, motive, semi-phrase, antecedent and consequent phrase, and period or sentence. The period may be considered the complete melody, however some examples combine two periods, or use other combinations of constituents to create larger form melodies.
Chord.
A chord, in music, is any harmonic set of three or more notes that is heard as if sounding simultaneously. These need not actually be played together: arpeggios and broken chords may, for many practical and theoretical purposes, constitute chords. Chords and sequences of chords are frequently used in modern Western, West African, and Oceanian music, whereas they are absent from the music of many other parts of the world.
The most frequently encountered chords are triads, so called because they consist of three distinct notes: further notes may be added to give seventh chords, extended chords, or added tone chords. The most common chords are the "major" and "minor triads" and then the "augmented" and "diminished triads". The descriptions "major", "minor", "augmented", and "diminished" are sometimes referred to collectively as chordal "quality". Chords are also commonly classed by their root note—so, for instance, the chord C major may be described as a triad of major quality built on the note C. Chords may also be classified by inversion, the order in which the notes are stacked.
A series of chords is called a chord progression. Although any chord may in principle be followed by any other chord, certain patterns of chords have been accepted as establishing key in common-practice harmony. To describe this, chords are numbered, using Roman numerals (upward from the key-note), per its diatonic function. Common ways of notating or representing chords. in western music other than conventional staff notation include Roman numerals, figured bass (much used in the Baroque era), macro symbols (sometimes used in modern musicology), and various systems of chord charts typically found in the lead sheets used in popular music to lay out the sequence of chords so that the musician may play accompaniment chords or improvise a solo.
Harmony.
In music, harmony is the use of simultaneous pitches (tones, notes), or chords. The study of harmony involves chords and their construction and chord progressions and the principles of connection that govern them. Harmony is often said to refer to the "vertical" aspect of music, as distinguished from melodic line, or the "horizontal" aspect. Counterpoint, which refers to the interweaving of melodic lines, and polyphony, which refers to the relationship of separate independent voices, are thus sometimes distinguished from harmony.
In popular and jazz harmony, chords are named by their root plus various terms and characters indicating their qualities. For example, a lead sheet may indicate chords such as C major, D minor, and G dominant seventh. In many types of music, notably Baroque, Romantic, modern, and jazz, chords are often augmented with "tensions". A tension is an additional chord member that creates a relatively dissonant interval in relation to the bass. Typically, in the classical common practice period a dissonant chord (chord with tension) "resolves" to a consonant chord. Harmonization usually sounds pleasant to the ear when there is a balance between the consonant and dissonant sounds. In simple words, that occurs when there is a balance between "tense" and "relaxed" moments.
Timbre.
[[File:9577 Guitarz1970 Clean E9 Guitar Chord (Mike Tribulas).jpg|thumb|right|Spectrogram of the first second of an E9 chord played on a Fender Stratocaster guitar with noiseless pickups. Below is the E9 chord audio:
Timbre is principally determined by two things: (1) the relative balance of overtones produced by a given instrument due its construction (e.g. shape, material), and (2) the envelope of the sound (including changes in the overtone structure over time). Timbre varies widely between different instruments, voices, and to lesser degree, between instruments of the same type due to variations in their construction, and significantly, the performer's technique. The timbre of most instruments can be changed by employing different techniques while playing. For example, the timbre of a trumpet changes when a mute is inserted into the bell, the player changes their embouchure, or volume.
A voice can change its timbre by the way the performer manipulates their vocal apparatus, (e.g. the shape of the vocal cavity or mouth). Musical notation frequently specifies alteration in timbre by changes in sounding technique, volume, accent, and other means. These are indicated variously by symbolic and verbal instruction. For example, the word "dolce" (sweetly) indicates a non-specific, but commonly understood soft and "sweet" timbre. "Sul tasto" instructs a string player to bow near or over the fingerboard to produce a less brilliant sound. "Cuivre" instructs a brass player to produce a forced and stridently brassy sound. Accent symbols like "marcato" (^) and dynamic indications ("pp") can also indicate changes in timbre.
Dynamics.
In music, "dynamics" normally refers to variations of intensity or volume, as may be measured by physicists and audio engineers in decibels or phons. In music notation, however, dynamics are not treated as absolute values, but as relative ones. Because they are usually measured subjectively, there are factors besides amplitude that affect the performance or perception of intensity, such as timbre, vibrato, and articulation.
The conventional indications of dynamics are abbreviations for Italian words like "forte" (f) for loud and "piano" (p) for soft. These two basic notations are modified by indications including "mezzo piano" (mp) for moderately soft (literally "half soft") and "mezzo forte" (mf) for moderately loud, "sforzando" or "sforzato" (sfz) for a surging or "pushed" attack, or "fortepiano" (fp) for a loud attack with a sudden decrease to a soft level. The full span of these markings usually range from a nearly inaudible "pianissississimo" (pppp) to a loud-as-possible "fortissississimo" (ffff).
Greater extremes of pppppp and fffff and nuances such as p+ or "più piano" are sometimes found. Other systems of indicating volume are also used in both notation and analysis: dB (decibels), numerical scales, colored or different sized notes, words in languages other than Italian, and symbols such as those for progressively increasing volume ("crescendo") or decreasing volume ("decrescendo"), often called "hairpins" when indicated with diverging or converging lines as shown in the graphic above.
Articulation.
Articulation is the way the performer sounds notes. For example, "staccato" is the shortening of duration compared to the written note value, "legato" performs the notes in a smoothly joined sequence with no separation. Articulation is often described rather than quantified, therefore there is room to interpret how to execute precisely each articulation.
For example, "staccato" is often referred to as "separated" or "detached" rather than having a defined or numbered amount by which to reduce the notated duration. Violin players use a variety of techniques to perform different qualities of "staccato." The manner in which a performer decides to execute a given articulation is usually based on the context of the piece or phrase, but many articulation symbols and verbal instructions depend on the instrument and musical period (e.g. viol, wind; classical, baroque; etc.).
There are a set of articulations that most instruments and voices perform in common. They are—from long to short: "legato" (smooth, connected); "tenuto" (pressed or played to full notated duration); "marcato" (accented and detached); "staccato" ("separated", "detached"); "martelé" (heavily accented or "hammered"). Many of these can be combined to create certain "in-between" articulations. For example, "portato" is the combination of "tenuto" and "staccato". Some instruments have unique methods by which to produce sounds, such as "spicatto" for bowed strings, where the bow bounces off the string.
Texture.
In music, texture is how the melodic, rhythmic, and harmonic materials are combined in a composition, thus determining the overall quality of the sound in a piece. Texture is often described in regard to the density, or thickness, and range, or width, between lowest and highest pitches, in relative terms as well as more specifically distinguished according to the number of voices, or parts, and the relationship between these voices. For example, a thick texture contains many "layers" of instruments. One of these layers could be a string section, or another brass.
The thickness also is affected by the amount and the richness of the instruments playing the piece. The thickness varies from light to thick. A lightly textured piece will have light, sparse scoring. A thickly or heavily textured piece will be scored for many instruments. A piece's texture may be affected by the number and character of parts playing at once, the timbre of the instruments or voices playing these parts and the harmony, tempo, and rhythms used. The types categorized by number and relationship of parts are analyzed and determined through the labeling of primary textural elements: primary melody, secondary melody, parallel supporting melody, static support, harmonic support, rhythmic support, and harmonic and rhythmic support.
Common types included monophonic texture (a single melodic voice, such as a piece for solo soprano or solo flute), biphonic texture (two melodic voices, such as a duo for bassoon and flute in which the bassoon plays a drone note and the flute plays the melody), polyphonic texture and homophonic texture (chords accompanying a melody).
Form or structure.
The term musical form (or musical architecture) refers to the overall structure or plan of a piece of music, and it describes the layout of a composition as divided into sections. In the tenth edition of "The Oxford Companion to Music", Percy Scholes defines musical form as "a series of strategies designed to find a successful mean between the opposite extremes of unrelieved repetition and unrelieved alteration." According to Richard Middleton, musical form is "the shape or structure of the work." He describes it through difference: the distance moved from a repeat; the latter being the smallest difference. Difference is quantitative and qualitative: "how far", and "of what type", different. In many cases, form depends on statement and restatement, unity and variety, and contrast and connection.
Analysis.
[[File:Debussy Pelleas et Melisande prelude opening.PNG|400px|thumb|Right|Typically a given work is analyzed by more than one person and different or divergent analyses are created. For instance, the first two bars of the prelude to Claude Debussy's "Pelléas et Melisande"
are analyzed differently by Leibowitz, Laloy, van Appledorn, and Christ. Leibowitz analyses this succession harmonically as D minor:I-VII-V, ignoring melodic motion, Laloy analyses the succession as D:I-V, seeing the G in the second measure as an ornament, and both van Appledorn and Christ analyses the succession as D:I-VII.
Musical analysis is the attempt to answer the question "how does this music work?" The method employed to answer this question, and indeed exactly what is meant by the question, differs from analyst to analyst, and according to the purpose of the analysis. According to Ian Bent, "analysis, as a pursuit in its own right, came to be established only in the late 19th century; its emergence as an approach and method can be traced back to the 1750s. However, it existed as a scholarly tool, albeit an auxiliary one, from the Middle Ages onwards." Adolf Bernhard Marx was influential in formalising concepts about composition and music understanding towards the second half of the 19th century. The principle of analysis has been variously criticized, especially by composers, such as Edgard Varèse's claim that, "to explain by means of is to decompose, to mutilate the spirit of a work".
Schenkerian analysis is a method of musical analysis of tonal music based on the theories of Heinrich Schenker (1868–1935). The goal of a Schenkerian analysis is to interpret the underlying structure of a tonal work and to help reading the score according to that structure. The theory's basic tenets can be viewed as a way of defining tonality in music. A Schenkerian analysis of a passage of music shows hierarchical relationships among its pitches, and draws conclusions about the structure of the passage from this hierarchy. The analysis makes use of a specialized symbolic form of musical notation that Schenker devised to demonstrate various techniques of elaboration. The most fundamental concept of Schenker's theory of tonality may be that of "tonal space". The intervals between the notes of the tonic triad form a "tonal space" that is filled with passing and neighbour notes, producing new triads and new tonal spaces, open for further elaborations until the surface of the work (the score) is reached.
Although Schenker himself usually presents his analyses in the generative direction, starting from the fundamental structure ("Ursatz") to reach the score, the practice of Schenkerian analysis more often is reductive, starting from the score and showing how it can be reduced to its fundamental structure. The graph of the "Ursatz" is arrhythmic, as is a strict-counterpoint cantus firmus exercise. Even at intermediate levels of the reduction, rhythmic notation (open and closed noteheads, beams and flags) shows not rhythm but the hierarchical relationships between the pitch-events. Schenkerian analysis is "subjective". There is no mechanical procedure involved and the analysis reflects the musical intuitions of the analyst. The analysis represents a way of hearing (and reading) a piece of music.
Transformational theory is a branch of music theory developed by David Lewin in the 1980s, and formally introduced in his 1987 work, "Generalized Musical Intervals and Transformations". The theory, which models musical transformations as elements of a mathematical group, can be used to analyze both tonal and atonal music. The goal of transformational theory is to change the focus from musical objects—such as the "C major chord" or "G major chord"—to relations between objects. Thus, instead of saying that a C major chord is followed by G major, a transformational theorist might say that the first chord has been "transformed" into the second by the "Dominant operation." (Symbolically, one might write "Dominant(C major) = G major.") While traditional musical set theory focuses on the makeup of musical objects, transformational theory focuses on the intervals or types of musical motion that can occur. According to Lewin's description of this change in emphasis, "transformational attitude does not ask for some observed measure of extension between reified 'points'; rather it asks: 'If I am "at" s and wish to get to t, what characteristic "gesture" should I perform in order to arrive there?'"
Music perception and cognition.
Music psychology or the psychology of music may be regarded as a branch of both psychology and musicology. It aims to explain and understand musical behavior and experience, including the processes through which music is perceived, created, responded to, and incorporated into everyday life. Modern music psychology is primarily empirical; its knowledge tends to advance on the basis of interpretations of data collected by systematic observation of and interaction with human participants. Music psychology is a field of research with practical relevance for many areas, including music performance, composition, education, criticism, and therapy, as well as investigations of human aptitude, skill, intelligence, creativity, and social behavior.
Music psychology can shed light on non-psychological aspects of musicology and musical practice. For example, it contributes to music theory through investigations of the perception and computational modelling of musical structures such as melody, harmony, tonality, rhythm, meter, and form. Research in music history can benefit from systematic study of the history of musical syntax, or from psychological analyses of composers and compositions in relation to perceptual, affective, and social responses to their music. Ethnomusicology can benefit from psychological approaches to the study of music cognition in different cultures.
Expression.
Musical expression is the art of playing or singing music with emotional communication. The elements of music that comprise expression include dynamic indications, such as forte or piano, phrasing, differing qualities of timbre and articulation, color, intensity, energy and excitement. All of these devices can be incorporated by the performer. A performer aims to elicit responses of sympathetic feeling in the audience, and to excite, calm or otherwise sway the audience's physical and emotional responses.
Expression on instruments can be closely related to the role of the breath in singing, and the voice's natural ability to express feelings, sentiment and deep emotions. Whether these can somehow be categorized is perhaps the realm of academics, who view expression as an element of musical performance that embodies a consistently recognizable emotion, ideally causing a sympathetic emotional response in its listeners. The emotional content of musical expression is distinct from the emotional content of specific sounds (e.g., a startlingly-loud 'bang') and of learned associations (e.g., a national anthem), but can rarely be completely separated from its context.
The components of musical expression continue to be the subject of extensive and unresolved dispute.
Genre and technique.
A music genre is a conventional category that identifies some pieces of music as belonging to a shared tradition or set of conventions. It is to be distinguished from "musical form" and "musical style", although in practice these terms are sometimes used interchangeably.
Music can be divided into different genres in many different ways. The artistic nature of music means that these classifications are often subjective and controversial, and some genres may overlap. There are even varying academic definitions of the term "genre "itself. In his book "Form in Tonal Music", Douglass M. Green distinguishes between genre and form. He lists madrigal, motet, canzona, ricercar, and dance as examples of genres from the Renaissance period. To further clarify the meaning of "genre", Green writes, "Beethoven's Op. 61 and Mendelssohn's Op. 64 are identical in genre—both are violin concertos—but different in form. However, Mozart's Rondo for Piano, K. 511, and the "Agnus Dei" from his Mass, K. 317 are quite different in genre but happen to be similar in form." Some, like Peter van der Merwe, treat the terms "genre" and "style" as the same, saying that "genre" should be defined as pieces of music that came from the same style or "basic musical language."
Others, such as Allan F. Moore, state that "genre" and "style" are two separate terms, and that secondary characteristics such as subject matter can also differentiate between genres. A music genre or subgenre may also be defined by the musical techniques, the style, the cultural context, and the content and spirit of the themes. Geographical origin is sometimes used to identify a music genre, though a single geographical category will often include a wide variety of subgenres. Timothy Laurie argues that since the early 1980s, "genre has graduated from being a subset of popular music studies to being an almost ubiquitous framework for constituting and evaluating musical research objects".
Musical technique is the ability of instrumental and vocal musicians to exert optimal control of their instruments or vocal cords to produce precise musical effects. Improving technique generally entails practicing exercises that improve muscular sensitivity and agility. To improve technique, musicians often practice fundamental patterns of notes such as the natural, minor, major, and chromatic scales, minor and major triads, dominant and diminished sevenths, formula patterns and arpeggios. For example, triads and sevenths teach how to play chords with accuracy and speed. Scales teach how to move quickly and gracefully from one note to another (usually by step). Arpeggios teach how to play broken chords over larger intervals. Many of these components of music are found in compositions, for example, a scale is a very common element of classical and romantic era compositions.
Heinrich Schenker argued that musical technique's "most striking and distinctive characteristic" is repetition. Works known as études (meaning "study") are also frequently used for the improvement of technique.
Mathematics.
Music theorists sometimes use mathematics to understand music, and although music has no axiomatic foundation in modern mathematics, mathematics is "the basis of sound" and sound itself "in its musical aspects... exhibits a remarkable array of number properties", simply because nature itself "is amazingly mathematical". The attempt to structure and communicate new ways of composing and hearing music has led to musical applications of set theory, abstract algebra and number theory. Some composers have incorporated the golden ratio and Fibonacci numbers into their work. There is a long history of examining the relationships between music and mathematics. Though ancient Chinese, Egyptians and Mesopotamians are known to have studied the mathematical principles of sound, the Pythagoreans (in particular Philolaus and Archytas) of ancient Greece were the first researchers known to have investigated the expression of musical scales in terms of numerical ratios.
In the modern era, musical set theory uses the language of mathematical set theory in an elementary way to organize musical objects and describe their relationships. To analyze the structure of a piece of (typically atonal) music using musical set theory, one usually starts with a set of tones, which could form motives or chords. By applying simple operations such as transposition and inversion, one can discover deep structures in the music. Operations such as transposition and inversion are called isometries because they preserve the intervals between tones in a set. Expanding on the methods of musical set theory, some theorists have used abstract algebra to analyze music. For example, the pitch classes in an equally tempered octave form an abelian group with 12 elements. It is possible to describe just intonation in terms of a free abelian group.
Serial composition and set theory.
In music theory, serialism is a method or technique of composition that uses a series of values to manipulate different musical elements. Serialism began primarily with Arnold Schoenberg's twelve-tone technique, though his contemporaries were also working to establish serialism as one example of post-tonal thinking. Twelve-tone technique orders the twelve notes of the chromatic scale, forming a row or series and providing a unifying basis for a composition's melody, harmony, structural progressions, and variations. Other types of serialism also work with sets, collections of objects, but not necessarily with fixed-order series, and extend the technique to other musical dimensions (often called "parameters"), such as duration, dynamics, and timbre. The idea of serialism is also applied in various ways in the visual arts, design, and architecture
"Integral serialism" or "total serialism" is the use of series for aspects such as duration, dynamics, and register as well as pitch. Other terms, used especially in Europe to distinguish post–World War II serial music from twelve-tone music and its American extensions, are "general serialism" and "multiple serialism".
Musical set theory provides concepts for categorizing musical objects and describing their relationships. Many of the notions were first elaborated by Howard Hanson (1960) in connection with tonal music, and then mostly developed in connection with atonal music by theorists such as Allen Forte (1973), drawing on the work in twelve-tone theory of Milton Babbitt. The concepts of set theory are very general and can be applied to tonal and atonal styles in any equally tempered tuning system, and to some extent more generally than that.
One branch of musical set theory deals with collections (sets and permutations) of pitches and pitch classes (pitch-class set theory), which may be ordered or unordered, and can be related by musical operations such as transposition, inversion, and complementation. The methods of musical set theory are sometimes applied to the analysis of rhythm as well.
Musical semiotics.
Music semiology (semiotics) is the study of signs as they pertain to music on a variety of levels. Following Roman Jakobson, Kofi Agawu adopts the idea of musical semiosis being introversive or extroversive—that is, musical signs within a text and without. "Topics," or various musical conventions (such as horn calls, dance forms, and styles), have been treated suggestively by Agawu, among others. The notion of gesture is beginning to play a large role in musico-semiotic enquiry.
Writers on music semiology include Kofi Agawu (on topical theory, Schenkerian analysis), Robert Hatten (on topic, gesture), Raymond Monelle (on topic, musical meaning), Jean-Jacques Nattiez (on introversive taxonomic analysis and ethnomusicological applications), Anthony Newcomb (on narrativity), and Eero Tarasti (generally considered the founder of musical semiotics).
Roland Barthes, himself a semiotician and skilled amateur pianist, wrote about music in "Image-Music-Text," "The Responsibilities of Form," and "Eiffel Tower," though he did not consider music to be a semiotic system.
Signs, meanings in music, happen essentially through the connotations of sounds, and through the social construction, appropriation and amplification of certain meanings associated with these connotations. The work of Philip Tagg ("Ten Little Tunes", "Fernando the Flute", "Music’s Meanings") provides one of the most complete and systematic analysis of the relation between musical structures and connotations in western and especially popular, television and film music. The work of Leonard Meyer in "Style and Music" theorizes the relationship between ideologies and musical structures and the phenomena of style change, and focuses on romanticism as a case study.
Music subjects.
Notation.
Musical notation is the written or symbolized representation of music. This is most often achieved by the use of commonly understood graphic symbols and written verbal instructions and their abbreviations. There are many systems of music notation from different cultures and different ages. Traditional Western notation evolved during the Middle Ages and remains an area of experimentation and innovation.In the 2000s, computer file formats have become important as well. Spoken language and hand signs are also used to symbolically represent music, primarily in teaching.
In standard Western music notation, tones are represented graphically by symbols (notes) placed on a staff or staves, the vertical axis corresponding to pitch and the horizontal axis corresponding to time. Note head shapes, stems, flags, ties and dots are used to indicate duration. Additional symbols indicate keys, dynamics, accents, rests, etc. Verbal instructions from the conductor are often used to indicate tempo, technique, and other aspects.
In Western music, a range of different music notation systems are used. In Western Classical music, conductors use printed scores that show all of the instruments' parts and orchestra members read parts with their musical lines written out. In popular styles of music, much less of the music may be notated. A rock band may go into a recording session with just a handwritten chord chart indicating the song's chord progression using chord names (e.g., C major, D minor, G7, etc.). All of the chord voicings, rhythms and accompaniment figures are improvised by the band members.
Education and careers.
There are advertisements for university music theory professorships from the 2010s at the University at Buffalo in the U.S. and Western University in Canada that require a Ph.D in music theory. In the 1960s and 1970s, some music theorists obtained professor positions with an M.A. as their highest degree, but in the 2010s, the Ph.D is the standard minimum credential for tenure track professor positions. Adjunct and non-tenure-track instructor positions, such as the one advertised in 2011 at St Josephs University in Philadelphia, may require only a master's degree (while preferring a PhD). Other doctoral degrees are sometimes accepted, especially in the case of positions divided between theory and applied subjects (performance or composition), where a D.M.A. is the usual terminal degree. Sometimes, as in a position advertised at the College of William and Mary, "equivalent professional experience in composition" may be accepted in lieu of a doctorate. As part of their initial training, music theorists will typically complete a B.Mus or a B.A. in music (or a related field) and in many cases an M.A. in music theory. Some individuals apply directly from a bachelor's degree to a Ph.D, and in these cases, they may not receive an M.A. In the 2010s, given the increasingly interdisciplinary nature of university graduate programs, some applicants for music theory Ph.D programs may have academic training both in music and outside of music (e.g., a student may apply with a B.Mus and a Masters in Music Composition or Philosophy of Music).
Most music theorists work as instructors, lecturers or professors in colleges, universities or conservatories. The job market for tenure-track professor positions is very competitive. Applicants must hold a completed Ph.D or the equivalent degree (or expect to receive one within a year of being hired—called an "ABD", for "All But Dissertation" stage) and (for more senior positions) have a strong record of publishing in peer-reviewed journals. Some Ph.D-holding music theorists are only able to find insecure positions as sessional lecturers. The job tasks of a music theorist are the same as those of a professor in any other humanities discipline: teaching undergraduate and/or graduate classes in this area of specialization and, in many cases some general courses (such as Music Appreciation or Introduction to Music Theory), conducting research in this area of expertise, publishing research articles in peer-reviewed journals, authoring book chapters, books or textbooks, traveling to conferences to present papers and learn about research in the field, and, if the program includes a graduate school, supervising M.A. and Ph.D students and giving them guidance on the preparation of their theses and dissertations. Some music theory professors may take on senior administrative positions in their institution, such as Dean or Chair of the School of Music.

</doc>
<doc id="54789" url="https://en.wikipedia.org/wiki?curid=54789" title="Recursively enumerable language">
Recursively enumerable language

In mathematics, logic and computer science, a formal language is called recursively enumerable (also recognizable, partially decidable, semidecidable, Turing-acceptable or Turing-recognizable) if it is a recursively enumerable subset in the set of all possible words over the alphabet of the language, i.e., if there exists a Turing machine which will enumerate all valid strings of the language.
Recursively enumerable languages are known as type-0 languages in the Chomsky hierarchy of formal languages. All regular, context-free, context-sensitive and recursive languages are recursively enumerable.
The class of all recursively enumerable languages is called RE.
Definitions.
There exist three equivalent major definitions for the concept of a recursively enumerable language.
All regular, context-free, context-sensitive and recursive languages are recursively enumerable.
Post's theorem shows that RE, together with its complement co-RE, correspond to the first level of the arithmetical hierarchy.
Example.
The Halting problem is recursively enumerable but not recursive. Indeed one can run the Turing Machine and accept if the machine halts, hence it is recursively enumerable. On the other hand the problem is undecidable. 
Some other recursively enumerable languages that are not recursive:
Closure properties.
Recursively enumerable languages are closed under the following operations. That is, if "L" and "P" are two recursively enumerable languages, then the following languages are recursively enumerable as well:
Note that recursively enumerable languages are not closed under set difference or complementation. The set difference "L" - "P" may or may not be recursively enumerable. If "L" is recursively enumerable, then the complement of "L" is recursively enumerable if and only if "L" is also recursive.

</doc>
<doc id="54795" url="https://en.wikipedia.org/wiki?curid=54795" title="Undecidable">
Undecidable

Undecidable may refer to:
__NOTOC__

</doc>
<doc id="54800" url="https://en.wikipedia.org/wiki?curid=54800" title="Decidability">
Decidability

The word decidable may refer to:

</doc>
<doc id="54808" url="https://en.wikipedia.org/wiki?curid=54808" title="Termite">
Termite

Termites are eusocial insects that are classified at the taxonomic rank of infraorder Isoptera, or as epifamily Termitoidae within the cockroach order Blattodea. Termites were once classified in a separate order from cockroaches, but recent phylogenetic studies indicate that they evolved from close ancestors of cockroaches during the Jurassic or Triassic. It is possible, however, that the first termites emerged during the Permian or even the Carboniferous. Approximately 3,106 species are currently described, with a few hundred more left to be described. Although these insects are often called white ants, they are not ants.
Like ants and some bees and wasps from the separate order Hymenoptera, termites divide labour among castes consisting of sterile male and female "workers" and "soldiers". All colonies have fertile males called "kings" and one or more fertile females called "queens". Termites mostly feed on dead plant material and cellulose, generally in the form of wood, leaf litter, soil, or animal dung. Termites are major detritivores, particularly in the subtropical and tropical regions, and their recycling of wood and plant matter is of considerable ecological importance.
Termites are among the most successful groups of insects on Earth, colonising most landmasses except for Antarctica. Their colonies range in size from a couple of hundred individuals to enormous societies with several million individuals. Termite queens have the longest lifespan of any insect in the world, with some queens living up to 50 years. Unlike ants, which undergo a complete metamorphosis, each individual termite goes through an incomplete metamorphosis that proceeds through egg, nymph and adult stages. Colonies are described as superorganisms because the termites form part of a self-regulating entity: the colony itself.
Termites are a delicacy in the diet of some human cultures and are used in many traditional medicines. Several hundred species are economically significant as pests that can cause serious damage to buildings, crops or plantation forests. Some species, such as the West Indian drywood termite ("Cryptotermes brevis"), are regarded as invasive species.
Etymology.
The infraorder name Isoptera is derived from the Greek words "iso" (equal) and "ptera" (winged), which refers to the nearly equal size of the fore-wings and hind-wings. "Termite" derives from the Latin and Late Latin word "termes" ("woodworm, white ant"), altered by the influence of Latin "terere" ("to rub, wear, erode") from the earlier word "tarmes". Termite nests were commonly known as "terminarium" or "termitaria". In early English, termites were known as "wood ants" or "white ants". The modern term was first used in 1781.
Taxonomy and evolution.
DNA analysis from 16S rRNA sequences has supported a hypothesis, originally suggested by Cleveland and colleagues in 1934, that these insects are most closely related to wood-eating cockroaches (genus "Cryptocercus", the woodroach). This earlier conclusion had been based on the similarity of the symbiotic gut flagellates in the wood-eating cockroaches to those in certain species of termites regarded as living fossils. In the 1960s additional evidence supporting that hypothesis emerged when F. A. McKittrick noted similar morphological characteristics between some termites and "Cryptocercus" nymphs. These similarities have led some authors to propose that termites be reclassified as a single family, Termitidae, within the order Blattodea, which contains cockroaches. Other researchers advocate the more conservative measure of retaining the termites as Termitoidae, an epifamily within the cockroach order, which preserves the classification of termites at family level and below.
The oldest unambiguous termite fossils date to the early Cretaceous, but given the diversity of Cretaceous termites and early fossil records showing mutualism between microorganisms and these insects, it is likely that they originated earlier in the Jurassic or Triassic. Further evidence of a Jurassic origin is the assumption that the extinct "Fruitafossor" consumed termites, judging from its morphological similarity to modern termite-eating mammals. The oldest termite nest discovered is believed to be from the Upper Cretaceous in west Texas, where the oldest known faecal pellets were also discovered.
Claims that termites emerged earlier have faced controversy. For example, F. M. Weesner indicated that Mastotermitidae termites may go back to the Late Permian, 251 million years ago, and fossil wings that have a close resemblance to the wings of "Mastotermes" of the Mastotermitidae, the most primitive living termite, have been discovered in the Permian layers in Kansas. It is even possible that the first termites emerged during the Carboniferous. Termites are thought to be the descendants of the genus "Cryptocercus". The folded wings of the fossil wood roach "Pycnoblattina", arranged in a convex pattern between segments 1a and 2a, resemble those seen in "Mastotermes", the only living insect with the same pattern. On the other hand, Krishna "et al." consider that all of the Paleozoic and Triassic insects tentatively classified as termites are in fact unrelated to termites and should be excluded from the Isoptera. Termites were the first social insects to evolve a caste system, evolving more than 100 million years ago.
It has long been accepted that termites are closely related to cockroaches and mantids, and they are classified in the same superorder (Dictyoptera). There is strong evidence suggesting that termites are highly specialised wood-eating cockroaches. The cockroach genus "Cryptocercus" shares the strongest phylogenetical similarity with termites and is considered to be a sister-group to termites. Termites and "Cryptocercus" share similar morphological and social features: for example, most cockroaches do not exhibit social characteristics, but "Cryptocercus" takes care of its young and exhibits other social behaviour such as trophallaxis and allogrooming. The primitive giant northern termite ("Mastotermes darwiniensis") exhibits numerous cockroach-like characteristics that are not shared with other termites, such as laying its eggs in rafts and having anal lobes on the wings. Cryptocercidae and Isoptera are united in the clade Xylophagodea. Although termites are sometimes called "white ants", they are actually not ants. Ants belong to the family Formicidae within the order Hymenoptera. The similarity of their social structure to that of termites is attributed to convergent evolution.
As of 2013, about 3,106 living and fossil termite species are recognised, classified in 12 families. The infraorder Isoptera is divided into the following clade and family groups, showing the subfamilies in their respective classification:
Order Blattaria
Distribution and diversity.
Termites are found on all continents except Antarctica. The diversity of termite species is low in North America and Europe (10 species known in Europe and 50 in North America), but is high in South America, where over 400 species are known. Of the 3,000 termite species currently classified, 1,000 are found in Africa, where mounds are extremely abundant in certain regions. Approximately 1.1 million active termite mounds can be found in the northern Kruger National Park alone. In Asia, there are 435 species of termites, which are mainly distributed in China. Within China, termite species are restricted to mild tropical and subtropical habitats south of the Yangtze River. In Australia, all ecological groups of termites (dampwood, drywood, subterranean) are endemic to the country, with over 360 classified species.
Due to their soft cuticles, termites do not inhabit cool or cold habitats. There are three ecological groups of termites: dampwood, drywood and subterranean. Dampwood termites are found only in coniferous forests, and drywood termites are found in hardwood forests; subterranean termites live in widely diverse areas. One species in the drywood group is the West Indian drywood termite "(Cryptotermes brevis)", which is an invasive species in Australia.
Description.
Termites are usually small, measuring between in length. The largest of all extant termites are the queens of the species "Macrotermes bellicosus", measuring up to over 10 centimetres (4 in) in length. Another giant termite, the extinct "Gyatermes styriensis", flourished in Austria during the Miocene and had a wingspan of and a body length of .
Most worker and soldier termites are completely blind and do not have a pair of eyes. However, some species, such as "Hodotermes mossambicus", have compound eyes which they use for orientation and to distinguish sunlight from moonlight. The alates have eyes along with lateral ocelli. Lateral ocelli, however, are not found in all termites. Like other insects, termites have a small tongue-shaped labrum and a clypeus; the clypeus is divided into a postclypeus and anteclypeus. Termite antennae have a number of functions such as the sensing of touch, taste, odours (including pheromones), heat and vibration. The three basic segments of a termite antenna include a scape, a pedicel (typically shorter than the scape), and the flagellum (all segments beyond the scape and pedicel). The mouth parts contain a maxillae, a labium, and a set of mandibles. The maxillae and labium have palps that help termites sense food and handling.
Consistent with all insects, the anatomy of the termite thorax consists of three segments: the prothorax, the mesothorax and the metathorax. Each segment contains a pair of two legs. On alates, the wings are located at the mesothorax and metathorax. The mesothorax and metathorax have well-developed exoskeletal plates; the prothorax has smaller plates.
Termites have a ten-segmented abdomen with two plates, the tergites and the sternites. There are ten tergites, of which nine are wide and one is elongated. The reproductive organs are similar to those in cockroaches but are more simplified. For example, the intromittent organ is not present in male alates, and the sperm is either immotile or aflagellate. However, Mastotermitidae termites have multiflagellate sperm with limited motility. The genitals in females are also simplified. Unlike in other termites, Mastotermitidae females have an ovipositor, a feature strikingly similar to that in female cockroaches.
The non-reproductive castes of termites are wingless and rely exclusively on their six legs for locomotion. The alates fly only for a brief amount of time, so they also rely on their legs. The appearance of the legs is similar in each caste, but the soldiers have larger and heavier legs. The structure of the legs is consistent with other insects: the parts of a leg include a coxa, trochanter, femur, tibia and the tarsus. The number of tibial spurs on an individual's leg varies. Some species of termite have an arolium, located between the claws, which is present in species that climb on smooth surfaces but is absent in most termites.
Unlike in ants, the hind-wings and fore-wings are of equal length. Most of the time, the alates are poor flyers; their technique is to launch themselves in the air and fly in a random direction. Studies show that in comparison to larger termites, smaller termites cannot fly long distances. When a termite is in flight, its wings remain at a right angle, and when the termite is at rest, its wings remain parallel to the body.
Caste system.
Worker termites undertake the most labour within the colony, being responsible for foraging, food storage, and brood and nest maintenance. Workers are tasked with the digestion of cellulose in food and are thus the most likely caste to be found in infested wood. The process of worker termites feeding other nestmates is known as trophallaxis. Trophallaxis is an effective nutritional tactic to convert and recycle nitrogenous components. It frees the parents from feeding all but the first generation of offspring, allowing for the group to grow much larger and ensuring that the necessary gut symbionts are transferred from one generation to another. Some termite species do not have a true worker caste, instead relying on nymphs that perform the same work without differentiating as a separate caste.
The soldier caste has anatomical and behavioural specialisations, and their sole purpose is to defend the colony. Many soldiers have large heads with highly modified powerful jaws so enlarged they cannot feed themselves. Instead, like juveniles, they are fed by workers. Fontanelles, simple holes in the forehead that exude defensive secretions, are a feature of the family Rhinotermitidae. Many species are readily identified using the characteristics of the soldiers' larger and darker head and large mandibles. Among certain termites, soldiers may use their globular (phragmotic) heads to block their narrow tunnels. Different sorts of soldiers include minor and major soldiers, and nasutes, which have a horn-like nozzle frontal projection (a nasus). These unique soldiers are able to spray noxious, sticky secretions containing diterpenes at their enemies. Nitrogen fixation plays an important role in nasute nutrition.
The reproductive caste of a mature colony includes a fertile female and male, known as the queen and king. The queen of the colony is responsible for egg production for the colony. Unlike in ants, the king mates with her for life. In some species, the abdomen of the queen swells up dramatically to increase fecundity, a characteristic known as physogastrism. Depending on the species, the queen will start producing reproductive winged alates at a certain time of the year, and huge swarms emerge from the colony when nuptial flight begins. These swarms attract a wide variety of predators.
Life cycle.
Termites are often compared with the social Hymenoptera (ants and various species of bees and wasps), but their differing evolutionary origins result in major differences in life cycle. In the eusocial Hymenoptera, the workers are exclusively female: males (drones) are haploid and develop from unfertilised eggs, while females (both workers and the queen) are diploid and develop from fertilised eggs. In contrast, worker termites, which constitute the majority in a colony, are diploid individuals of both sexes and develop from fertilised eggs. Depending on species, male and female workers may have different roles in a termite colony.
The life cycle of a termite begins with an egg, but is different from that of a bee or ant in that it goes through a developmental process called incomplete metamorphosis, with egg, nymph and adult stages. After eggs hatch into nymphs, the nymphs will go through a series of moults until they become adults. In some species, eggs go through four moulting stages while nymphs go through three. Nymphs first moult into workers, and then some workers go through further moulting and become soldiers or alates; workers become alates only by moulting into alate nymphs.
The development of nymphs into adults can take months; the time period depends on food availability, temperature, and the general population of the colony. Since nymphs are unable to feed themselves, workers must feed them, but workers also take part in the social life of the colony and have certain other tasks to accomplish such as foraging, building or maintaining the nest or tending to the queen. Pheromones regulate the caste system in termite colonies, preventing all but a very few of the termites from becoming fertile queens.
Reproduction.
Termite alates only leave the colony when a nuptial flight takes place. Alate males and females will pair up together and then land in search of a suitable place for a colony. A termite king and queen will not mate until they find such a spot. When they do, they excavate a chamber big enough for both, close up the entrance and proceed to mate. After mating, the pair will never go outside and will spend the rest of their lives in the nest. Nuptial flight time varies in each species. For example, alates in certain species emerge during the day in summer while others emerge during the winter. The nuptial flight may also begin at dusk, when the alates swarm around areas with lots of lights. The time when nuptial flight begins depends on the environmental conditions, the time of day, moisture, wind speed and precipitation. The number of termites in a colony also varies, with the larger species typically having 100–1,000 individuals. However, some termite colonies, including those with large individuals, can number in the millions.
The queen will only lay 10–20 eggs in the very early stages of the colony, but will lay as many as 1,000 a day when the colony is several years old. At maturity, a primary queen has a great capacity to lay eggs. In some species, the mature queen has a greatly distended abdomen and may produce 40,000 eggs a day. The two mature ovaries may have some 2,000 ovarioles each. The abdomen increases the queen's body length to several times more than before mating and reduces her ability to move freely; attendant workers provide assistance.
The king grows only slightly larger after initial mating and continues to mate with the queen for life (a termite queen can live up to 50 years). This is very different from ant colonies, in which a queen mates once with the male(s) and stores the gametes for life, as the male ants die shortly after mating. If a queen is absent, a termite king will produce pheromones which encourage the development of replacement termite queens. As the queen and king are monogamous, sperm competition does not occur.
Termites going through incomplete metamorphosis on the path to becoming alates form a subcaste in certain species of termite, functioning as potential supplementary reproductives. These supplementary reproductives only mature into primary reproductives upon the death of a king or queen, or when the primary reproductives are separated from the colony. Supplementaries have the ability to replace a dead primary reproductive, and there may also be more than a single supplementary within a colony. Some queens have the ability to switch from sexual reproduction to asexual reproduction. Studies show that while termite queens mate with the king to produce colony workers, the queens reproduce their replacements (neotenic queens) parthenogenetically.
Behaviour and ecology.
Diet.
Termites are detritivores, consuming dead plants at any level of decomposition. They also play a vital role in the ecosystem by recycling waste material such as dead wood, faeces and plants. Many species eat cellulose, having a specialised midgut that breaks down the fibre. Termites are considered to be a major source (11%) of atmospheric methane produced from the breakdown of cellulose, one of the prime greenhouse gases. Termites rely primarily upon symbiotic protozoa (metamonads) and other microbes such as flagellate protists in their guts to digest the cellulose for them, allowing them to absorb the end products for their own use. Gut protozoa, such as "Trichonympha", in turn, rely on symbiotic bacteria embedded on their surfaces to produce some of the necessary digestive enzymes. Most higher termites, especially in the family Termitidae, can produce their own cellulase enzymes, but they rely primarily upon the bacteria. The flagellates have been lost in Termitidae. Scientists' understanding of the relationship between the termite digestive tract and the microbial endosymbionts is still rudimentary; what is true in all termite species, however, is that the workers feed the other members of the colony with substances derived from the digestion of plant material, either from the mouth or anus. Judging from closely related bacterial species, it is strongly presumed that the termites' and cockroach's gut microbiota derives from their dictyopteran ancestors.
Certain species such as "Gnathamitermes tubiformans" have seasonal food habits. For example, they may preferentially consume Red three-awn ("Aristida longiseta") during the summer, Buffalograss ("Buchloe dactyloides") from May to August, and blue grama "Bouteloua gracilis" during spring, summer and autumn. Colonies of "G. tubiformans" consume less food in spring than they do during autumn when their feeding activity is high.
Various woods differ in their susceptibility to termite attack; the differences are attributed to such factors as moisture content, hardness, and resin and lignin content. In one study, the drywood termite "Cryptotermes brevis" strongly preferred poplar and maple woods to other woods that were generally rejected by the termite colony. These preferences may in part have represented conditioned or learned behaviour.
Some species of termite practice fungiculture. They maintain a "garden" of specialised fungi of genus "Termitomyces", which are nourished by the excrement of the insects. When the fungi are eaten, their spores pass undamaged through the intestines of the termites to complete the cycle by germinating in the fresh faecal pellets.
Depending on their feeding habits, termites are placed into two groups: the lower termites and higher termites. The lower termites predominately feed on wood. As wood is difficult to digest, termites prefer to consume fungus-infected wood because it is easier to digest and the fungi are high in protein. Meanwhile, the higher termites consume a wide variety of materials, including faeces, humus, grass, leaves and roots. The gut in the lower termites contains many species of bacteria along with protozoa, while the higher termites only have a few species of bacteria with no protozoa.
Predators.
Termites are consumed by a wide variety of predators. One species alone, "Hodotermes mossambicus", was found in the stomach contents of 65 birds and 19 mammals. Arthropods and reptiles such as bees, centipedes, cockroaches, crickets, dragonflies, frogs, lizards, scorpions, spiders, and toads consume these insects, while two spiders in the family Ammoxenidae are specialist termite predators. Other predators include aardvarks, aardwolves, anteaters, bats, bears, bilbies, many birds, echidnas, foxes, galagos, numbats, mice and pangolins. The aardwolf is an insectivorous mammal that primarily feeds on termites; it locates its food by sound and also by detecting the scent secreted by the soldiers; a single aardwolf is capable of consuming thousands of termites in a single night by using its long, sticky tongue. Sloth bears break open mounds to consume the nestmates, while chimpanzees have developed tools to "fish" termites from their nest. Wear pattern analysis of bone tools used by the early hominin "Paranthropus robustus" suggests that they used these tools to dig into termite mounds.
Among all predators, ants are the greatest enemy to termites. Some ant genera are specialist predators of termites. For example, "Megaponera" is a strictly termite-eating (termitophagous) genus that perform raiding activities, some lasting several hours. "Paltothyreus tarsatus" is another termite-raiding species, with each individual stacking as many termites as possible in its mandibles before returning home, all the while recruiting additional nestmates to the raiding site through chemical trails. The Malaysian basicerotine ant "Eurhopalothrix heliscata" uses a different strategy of termite hunting by pressing themselves into tight spaces, as they hunt through rotting wood housing termite colonies. Once inside, the ants seize their prey by using their short but sharp mandibles. "Tetramorium uelense" is a specialised predator species that feeds on small termites. A scout will recruit 10–30 workers to an area where termites are present, killing them by immobilising them with their stinger. "Centromyrmex" and "Iridomyrmex" colonies sometimes nest in termite mounds, and so the termites are preyed on by these ants. No evidence for any kind of relationship (other than a predatory one) is known. Other ants, including "Acanthostichus", "Camponotus", "Crematogaster", "Cylindromyrmex", "Leptogenys", "Odontomachus", "Ophthalmopone", "Pachycondyla", "Rhytidoponera", "Solenopsis" and "Wasmannia", also prey on termites. In contrast to all these ant species, and despite their enormous diversity of prey, "Dorylus" ants rarely consume termites.
Ants are not the only invertebrates that perform raids. Many sphecoid wasps and several species including "Polybia Lepeletier" and "Angiopolybia Araujo" are known to raid termite mounds during the termites' nuptial flight.
Parasites, pathogens and viruses.
Termites are less likely to be attacked by parasites than bees, wasps and ants, as they are usually well protected in their mounds. Nevertheless, termites are infected by a variety of parasites. Some of these include dipteran flies, "Pyemotes" mites, and a large number of nematode parasites. Most nematode parasites are in the order Rhabditida; others are in the genus "Mermis", "Diplogaster aerivora" and "Harteria gallinarum". Under imminent threat of an attack by parasites, a colony may migrate to a new location. Fungi pathogens such as such as "Aspergillus nomius" and "Metarhizium anisopliae" are, however, major threats to a termite colony as they are not host-specific and may infect large portions of the colony; transmission usually occurs via direct physical contact. "M. anispliae" is known to weaken the termite immune system. Infection with "A. nomius" only occurs when a colony is under great stress. Inquilinism between two termite species does not occur in the termite world.
Termites are infected by viruses including Entomopoxvirinae and the Nuclear Polyhedrosis Virus.
Locomotion and foraging.
Because the worker and soldier castes lack wings and thus never fly, and the reproductives use their wings for just a brief amount of time, termites predominantly rely upon their legs to move about.
Foraging behaviour depends on the type of termite. For example, certain species feed on the wood structures they inhabit, and others harvest food that is near the nest. Most workers are rarely found out in the open, and do not forage unprotected; they rely on sheeting and runways to protect them from predators. Subterranean termites construct tunnels and galleries to look for food, and workers who manage to find food sources recruit additional nestmates by depositing a phagostimulant pheromone that attracts workers. Foraging workers use semiochemicals to communicate with each other, and workers who begin to forage outside of their nest release trail pheromones from their sternal glands. In one species, "Nasutitermes costalis", there are three phases in a foraging expedition: first, soldiers scout an area. When they find a food source, they communicate to other soldiers and a small force of workers starts to emerge. In the second phase, workers appear in large numbers at the site. The third phase is marked by a decrease in the number of soldiers present and an increase in the number of workers. Isolated termite workers may engage in Lévy flight behaviour as an optimised strategy for finding their nestmates or foraging for food.
Competition.
Competition between two colonies always results in agonistic behaviour towards each other, resulting in fights. These fights can cause mortality on both sides and, in some cases, the gain or loss of territory. "Cemetery pits" may be present, where the bodies of dead termites are buried.
Studies show that when termites encounter each other in foraging areas, some of the termites deliberately block passages to prevent other termites from entering. Dead termites from other colonies found in exploratory tunnels leads to the isolation of the area and thus the need to construct new tunnels. Conflict between two competitors does not always occur. For example, though they might block each other's passages, colonies of "Macrotermes bellicosus" and "Macrotermes subhyalinus" are not always aggressive towards each other. Suicide cramming is known in "Coptotermes formosanus". Since "C. formosanus" colonies may get into physical conflict, some termites will tightly squeeze into foraging tunnels and die, successfully blocking the tunnel and ending all agonistic activities.
Among the reproductive caste, neotenic queens may compete with each other to become the dominant queen when there are no primary reproductives. This struggle among the queens leads to the elimination of all but a single queen, which, with the king, will take over the colony.
Ants and termites may compete with each other for nesting space. In particular, ants that prey on termites usually have a negative impact on arboreal nesting species.
Communication.
Most termites are blind, so communication primarily occurs through chemical, mechanical and pheromonal cues. These methods of communication are used in a variety of activities, including foraging, locating reproductives, construction of nests, recognition of nestmates, nuptial flight, locating and fighting enemies, and defending the nests. The most common way of communicating is through antennation. A number of pheromones are known, including contact pheromones (which are transmitted when workers are engaged in trophallaxis or grooming) and alarm, trail and sex pheromones. The alarm pheromone and other defensive chemicals are secreted from the frontal gland. Trail pheromones are secreted from the sternal gland, and sex pheromones derive from two glandular sources: the sternal and tergal glands. When termites go out to look for food, they forage in columns along the ground through vegetation. A trail can be identified by the faecal deposits or runways that are covered by objects. Workers leave pheromones on these trails, which are detected by other nestmates through olfactory receptors. Termites can also communicate through mechanical cues, vibrations, and physical contact. These signals are frequently used for alarm communication or for evaluating a food source.
When termites construct their nests, they use predominantly indirect communication. No single termite would be in charge of any particular construction project. Individual termites react rather than think, but at a group level, they exhibit a sort of collective cognition. Specific structures or other objects such as pellets of soil or pillars cause termites to start building. The termite adds these objects onto existing structures, and such behaviour encourages building behaviour in other workers. The result is a self-organised process whereby the information that directs termite activity results from changes in the environment rather than from direct contact among individuals.
Termites can distinguish nestmates and non-nestmates through chemical communication and gut symbionts: chemicals consisting of hydrocarbons released from the cuticle allow the recognition of alien termite species. Each colony has its own distinct odour. This odour is a result of genetic and environmental factors such as the termites' diet and the composition of the bacteria within the termites' intestines.
Defence.
Termites rely on alarm communication to defend a colony. Alarm pheromones can be released when the nest has been breached or is being attacked by enemies or potential pathogens. Termites always avoid nestmates infected with "Metarhizium anisopliae" spores, through vibrational signals released by infected nestmates. Other methods of defence include intense jerking and secretion of fluids from the frontal gland and defecating faeces containing alarm pheromones.
In some species, some soldiers block tunnels to prevent their enemies from entering the nest, and they may deliberately rupture themselves as an act of defence. In cases where the intrusion is coming from a breach that is larger than the soldier's head, defence requires a special formations where soldiers form a phalanx-like formation around the breach and bite at intruders. If an invasion carried out by "Megaponera analis" is successful, an entire colony may be destroyed, although this scenario is rare.
To termites, any breach of their tunnels or nests is a cause for alarm. When termites detect a potential breach, the soldiers will usually bang their heads apparently to attract other soldiers for defence and to recruit additional workers to repair any breach. Additionally, an alarmed termite will bump into other termites which causes them to be alarmed and to leave pheromone trails to the disturbed area, which is also a way to recruit extra workers.
The pantropical subfamily Nasutitermitinae has a specialised caste of soldiers, known as nasutes, that have the ability to exude noxious liquids through a horn-like frontal projection that they use for defence. Nasutes have lost their mandibles through the course of evolution and must be fed by workers. A wide variety of monoterpene hydrocarbon solvents have been identified in the liquids that nasutes secrete.
Soldiers of the species "Globitermes sulphureus" commit suicide by autothysis – rupturing a large gland just beneath the surface of their cuticles. The thick, yellow fluid in the gland becomes very sticky on contact with the air, entangling ants or other insects which are trying to invade the nest. Another termite, "Neocapriterme taracua", also engages in suicidal defence. Workers physically unable to use their mandibles while in a fight form a pouch full of chemicals, then deliberately rupture themselves, releasing toxic chemicals that paralyse and kill their enemies. The soldiers of the neotropical termite family Serritermitidae have a defence strategy which involves front gland autothysis, with the body rupturing between the head and abdomen. When soldiers guarding nest entrances are attacked by intruders, they engage in autothysis, creating a block that denies entry to any attacker.
Workers use several different strategies to deal with their dead, including burying, cannibalism, and avoiding a corpse altogether. To avoid pathogens, termites occasionally engage in necrophoresis, in which a nestmate will carry away a corpse from the colony to dispose of it elsewhere. Which strategy is used depends on the nature of the corpse a worker is dealing with (i.e. the age of the carcass).
Relationship with other organisms.
A species of fungus is known to mimic termite eggs, successfully avoiding its natural predators. These small brown balls, known as "termite balls", rarely kill the eggs, and in some cases the workers will even tend to them. This fungus mimics these eggs by producing a cellulose-digesting enzyme known as glucosidases. A unique mimicking behaviour exists between various species of "Trichopsenius" beetles and certain termite species within "Reticulitermes". The beetles share the same cuticle hydrocarbons as the termites and even biosynthesize them. This chemical mimicry allows the beetles to integrate themselves within the termite colonies. The developed appendages on the physogastric abdomen of "Austrospirachtha mimetes" allows the beetle to mimic a termite worker.
Some species of ant are known to capture termites to use as a fresh food source later on, rather than killing them. For example, "Formica nigra" captures termites, and those who try to escape are immediately seized and driven underground. Certain species of ants in the subfamily Ponerinae conduct these raids although other ant species go in alone to steal the eggs or nymphs. Ants such as "Megaponera analis" attack the outside the mounds and Dorylinae ants attack underground. Despite this, some termites and ants can coexist peacefully. Some species of termite, including "Nasutitermes corniger", form associations with certain ant species to keep away predatory ant species. The earliest known association between "Azteca" ants and "Nasutitermes" termites date back to the Oligocene to Miocene period.
54 species of ants are known to inhabit "Nasutitermes" mounds, both occupied and abandoned ones. One reason many ants live in "Nasutitermes" mounds is due to the termites' frequent occurrence in their geographical range; another is to protect themselves from floods. "Iridomyrmex" also inhabits termite mounds although no evidence for any kind of relationship (other than a predatory one) is known. In rare cases, certain species of termites live inside active ant colonies. Some invertebrate organisms such as beetles, caterpillars, flies and millipedes are termitophiles and dwell inside termite colonies (they are unable to survive independently). As a result, certain beetles and flies have evolved with their hosts. They have developed a gland that secrete a substance that attracts the workers by licking them. Mounds may also provide shelter and warmth to birds, lizards, snakes and scorpions.
Termites are known to carry pollen and regularly visit flowers, so are regarded as potential pollinators for a number of flowering plants. One flower in particular, "Rhizanthella gardneri", is regularly pollinated by foraging workers, and it is perhaps the only Orchidaceae flower in the world to be pollinated by termites.
Many plants have developed effective defences against termites. However, seedlings are vulnerable to termite attacks and need additional protection, as their defence mechanisms only develop when they have passed the seedling stage. Defence is typically achieved by secreting antifeedant chemicals into the woody cell walls. This reduces the ability of termites to efficiently digest the cellulose. A commercial product, "Blockaid", has been developed in Australia that uses a range of plant extracts to create a paint-on nontoxic termite barrier for buildings. An extract of a species of Australian figwort, "Eremophila", has been shown to repel termites; tests have shown that termites are strongly repelled by the toxic material to the extent that they will starve rather than consume the food. When kept close to the extract, they become disoriented and eventually die.
Nests.
A termite nest can be considered as being composed of two parts, the inanimate and the animate. The animate is all of the termites living inside the colony, and the inanimate part is the structure itself, which is constructed by the termites. Nests can be broadly separated into three main categories: subterranean (completely below ground), epigeal (protruding above the soil surface), and arboreal (built above ground, but always connected to the ground via shelter tubes). Epigeal nests (mounds) protrude from the earth with ground contact and are made out of earth and mud. A nest has many functions such as providing a protected living space and providing shelter against predators. Most termites construct underground colonies rather than multifunctional nests and mounds. Primitive termites of today nest in wooden structures such as logs, stumps and the dead parts of trees, as did termites millions of years ago.
To build their nests, termites primarily use faeces, which have many desirable properties as a construction material. Other building materials include partly digested plant material, used in carton nests (arboreal nests built from faecal elements and wood), and soil, used in subterranean nest and mound construction. Not all nests are visible, as many nests in tropical forests are located underground. Species in the subfamily Apicotermitinae are good examples of subterranean nest builders, as they only dwell inside tunnels. Other termites live in wood, and tunnels are constructed as they feed on the wood. Nests and mounds protect the termites' soft bodies against desiccation, light, pathogens and parasites, as well as providing a fortification against predators. Nests made out of carton are particularly weak, and so the inhabitants use counter-attack strategies against invading predators.
Some species build complex nests called polycalic nests; this habitat is called polycalism. Polycalic species of termites form multiple nests, or calies, connected by subterranean chambers. The termite genera "Apicotermes" and "Trinervitermes" are known to have polycalic species. Polycalic nests appear to be less frequent in mound-building species although polycalic arboreal nests have been observed in a few species of "Nasutitermes".
Mounds.
Nests are considered mounds if they protrude from the earth's surface. A mound provides termites the same protection as a nest but is stronger. Mounds located in areas with torrential and continuous rainfall are at risk of mound erosion due to their clay-rich construction. Those made from carton can provide protection from the rain, and in fact can withstand high precipitation. Certain areas in mounds are used as strong points in case of a breach. For example, "Cubitermes" colonies build narrow tunnels used as strong points, as the diameter of the tunnels is small enough for soldiers to block. A highly protected chamber, known as the "queens cell", houses the queen and king and is used as a last line of defence.
Species in the genus "Macrotermes" arguably build the most complex structures in the insect world, constructing enormous mounds. These mounds are among the largest in the world, reaching a height of 8 to 9 metres (26 to 29 feet), and consist of chimneys, pinnacles and ridges. Another termite species, "Amitermes meridionalis", can build nests 3 to 4 metres (9 to 13 feet) high and 2.5 metres (8 feet) wide.
The sculptured mounds sometimes have elaborate and distinctive forms, such as those of the compass termite ("Amitermes meridionalis" and "A. laurensis"), which builds tall, wedge-shaped mounds with the long axis oriented approximately north–south, which gives them their common name. This orientation has been experimentally shown to assist thermoregulation. The north-south orientation causes the internal temperature of a mound to increase rapidly during the morning while avoiding overheating from the midday sun. The temperature then remains at a plateau for the rest of the day until the evening.
Shelter tubes.
Termites construct shelter tubes, also known as earthen tubes or mud tubes, that start from the ground. These shelter tubes can be found on walls and other structures. Constructed by termites during the night, a time of higher humidity, these tubes provide protection to termites from potential predators, especially ants. Shelter tubes also provide high humidity and darkness and allow workers to collect food sources that cannot be accessed in any other way. These passageways are made from soil and faeces and are normally brown in colour. The size of these shelter tubes depends on the amount of food sources that are available. They range from less than 1 cm to several cm in width, but may extend dozens of metres in length.
Relationship with humans.
As pests.
Owing to their wood-eating habits, many termite species can do great damage to unprotected buildings and other wooden structures. Their habit of remaining concealed often results in their presence being undetected until the timbers are severely damaged, leaving a thin layer of a wall that protects them from the environment. Of the 3,106 species known, only 183 species cause damage; 83 species cause significant damage to wooden structures. In North America, nine subterranean species are pests; in Australia, 16 species have an economic impact; in the Indian subcontinent 26 species are considered pests, and in tropical Africa, 24. In Central America and the West Indies, there are 17 pest species. Among the termite genera, "Coptotermes" has the highest number of pest species of any genus, with 28 species known to cause damage. Less than 10% of drywood termites are pests, but they infect wooden structures and furniture in tropical, subtropical and other regions. Dampwood termites only attack lumber material exposed to rainfall or soil.
Drywood termites thrive in warm climates, and human activities can enable them to invade homes since they can be transported through contaminated goods, containers and ships. Colonies of termites have been seen thriving in warm buildings located in cold regions. Some termites are considered invasive species. "Cryptotermes brevis", the most widely introduced invasive termite species in the world, has been introduced to all the islands in the West Indies and to Australia.
In addition to causing damage to buildings, termites can also damage food crops. Termites may attack trees whose resistance to damage is low but generally ignore fast-growing plants. Most attacks occur at harvest time; crops and trees are attacked during the dry season.
The damage caused by termites costs the southwestern United States approximately $1.5 billion each year in wood structure damage, but the true cost of damage worldwide cannot be determined. Drywood termites are responsible for a large proportion of the damage caused by termites.
To better control the population of termites, various methods have been developed to track termite movements. One early method involved distributing termite bait laced with immunoglobulin G (IgG) marker proteins from rabbits or chickens. Termites collected from the field could be tested for the rabbit-IgG markers using a rabbit-IgG-specific assay. More recently developed, less expensive alternatives include tracking the termites using egg white, cow milk, or soy milk proteins, which can be sprayed on termites in the field. Termites bearing these proteins can be traced using a protein-specific ELISA test.
As food.
43 termite species are used as food by humans or are fed to livestock. These insects are particularly important in less developed countries where malnutrition is common, as the protein from termites can help improve the human diet. Termites are consumed in many regions globally, but this practice has only become popular in developed nations in recent years.
Termites are consumed by people in many different cultures around the world. In Africa, the alates are an important factor in the diets of native populations. Tribes have different ways of collecting or cultivating insects; sometimes tribes will collect soldiers from several species. Though harder to acquire, queens are regarded as a delicacy. Termite alates are high in nutrition with adequate levels of fat and protein. They are regarded as pleasant in taste, having a nut-like flavour after they are cooked.
Alates are collected when the rainy season begins. During a nuptial flight, they are typically seen around lights to which they are attracted, and so nets are set up on lamps and captured alates are later collected. The wings are removed through a technique that is similar to winnowing. The best result comes when they are lightly roasted on a hot plate or fried until crisp. Oil is not required as their bodies usually contain sufficient amounts of oil. Termites are typically eaten when livestock is lean and tribal crops have not yet developed or produced any food, or if food stocks from a previous growing season are limited.
In addition to Africa, termites are consumed in local or tribal areas in Asia and North and South America. In Australia, Indigenous Australians are aware that termites are edible but do not consume them even in times of scarcity; there are few explanations as to why. Termite mounds are the main sources of soil consumption (geophagy) in many countries including Kenya, Tanzania, Zambia, Zimbabwe and South Africa. Researchers have suggested that termites are suitable candidates for human consumption and space agriculture, as they are high in protein and can be used to convert inedible waste to consumable products for humans.
In agriculture.
Termites can be major agricultural pests, particularly in East Africa and North Asia, where crop losses can be severe (3–100% in crop loss in Africa). Counterbalancing this is the greatly improved water infiltration where termite tunnels in the soil allow rainwater to soak in deeply, which helps reduce runoff and consequent soil erosion through bioturbation. In South America, cultivated plants such as eucalyptus, upland rice and sugarcane can be severely damaged by termite infestations, with attacks on leaves, roots and woody tissue. Termites can also attack other plants, including cassava, coffee, cotton, fruit trees, maize, peanuts, soybeans and vegetables. Mounds can disrupt farming activities, making it difficult for farmers to operate farming machinery; however, despite farmers' dislike of the mounds, it is often the case that no net loss of production occurs. Termites can be beneficial to agriculture, such as by boosting crop yields and enriching the soil. Termites and ants can re-colonise untilled land that contains crop stubble, which colonies use for nourishment when they establish their nests. The presence of nests in fields enables larger amounts of rainwater to soak into the ground and increases the amount of nitrogen in the soil, both essential for the growth of crops.
In science and technology.
The termite gut has inspired various research efforts aimed at replacing fossil fuels with cleaner, renewable energy sources. Termites are efficient bioreactors, capable of producing two litres of hydrogen from a single sheet of paper. Approximately 200 species of microbes live inside the termite hindgut, releasing the hydrogen that was trapped inside wood and plants that they digest. Through the action of unidentified enzymes in the termite gut, lignocellulose polymers are broken down into sugars and are transformed into hydrogen. The bacteria within the gut turns the sugar and hydrogen into cellulose acetate, an acetate ester of cellulose on which termites rely for energy. Community DNA sequencing of the microbes in the termite hindgut has been employed to provide a better understanding of the metabolic pathway. Genetic engineering may enable hydrogen to be generated in bioreactors from woody biomass.
The development of autonomous robots capable of constructing intricate structures without human assistance has been inspired by the complex mounds that termites build. These robots work independently and can move by themselves on a tracked grid, capable of climbing and lifting up bricks. Such robots may be useful for future projects on Mars, or for building levees to prevent flooding.
Termites use sophisticated means to control the temperatures of their mounds. As discussed above, the shape and orientation of the mounds of the Australian compass termite stabilises their internal temperatures during the day. As the towers heat up, the solar chimney effect (stack effect) creates an updraft of air within the mound. Wind blowing across the tops of the towers enhances the circulation of air through the mounds, which also include side vents in their construction. The solar chimney effect has been in use for centuries in the Middle East and Near East for passive cooling, as well as in Europe by the Romans. It is only relatively recently, however, that climate responsive construction techniques have become incorporated into modern architecture. Especially in Africa, the stack effect has become a popular means to achieve natural ventilation and passive cooling in modern buildings.
In culture.
The Eastgate Centre is a shopping centre and office block in central Harare, Zimbabwe, whose architect, Mick Pearce, used passive cooling inspired by that used by the local termites. It was the first major building exploiting termite-inspired cooling techniques to attract international attention. Other such buildings include the Learning Resource Center at the Catholic University of Eastern Africa and the Council House 2 building in Melbourne, Australia.
Few zoos hold termites, due to the difficulty in keeping them captive and to the reluctance of authorities to permit potential pests. One of the few that do, the Zoo Basel in Switzerland, has two thriving "Macrotermes bellicosus" populations – resulting in an event very rare in captivity: the mass migrations of young flying termites. This happened in September 2008, when thousands of male termites left their mound each night, died, and covered the floors and water pits of the house holding their exhibit.
African tribes in several countries have termites as totems, and for this reason tribe members are forbidden to eat the reproductive alates. Termites are widely used in traditional popular medicine; they are used as treatments for diseases and other conditions such as asthma, bronchitis, hoarseness, influenza, sinusitis, tonsillitis and whooping cough. In Nigeria, "Macrotermes nigeriensis" is used for spiritual protection and to treat wounds and sick pregnant women. In Southeast Asia, termites are used in ritual practices. In Malaysia, Singapore and Thailand, termite mounds are commonly worshiped among the populace. Abandoned mounds are viewed as structures created by spirits, believing a local guardian dwells within the mound; this is known as Keramat and Datok Kong. In urban areas, local residents construct red-painted shrines over mounds that have been abandoned, where they pray for good health, protection and luck.

</doc>
<doc id="54813" url="https://en.wikipedia.org/wiki?curid=54813" title="Shellac">
Shellac

Shellac is a resin secreted by the female lac bug, on trees in the forests of India and Thailand. It is processed and sold as dry flakes (pictured) and dissolved in ethanol to make liquid shellac, which is used as a brush-on colorant, food glaze and wood finish. Shellac functions as a tough natural primer, sanding sealant, tannin-blocker, odour-blocker, stain, and high-gloss varnish. Shellac was once used in electrical applications as it possesses good insulation qualities and it seals out moisture. Phonograph (gramophone) records were also made of it during the 78-rpm recording era which ended in Western countries during the 1950s.
From the time it replaced oil and wax finishes in the 19th century, shellac was one of the dominant wood finishes in the western world until it was largely replaced by nitrocellulose lacquer in the 1920s and 1930s.
Etymology.
"Shellac" comes from "shell" and "lac", a calque of French "laque en écailles", "lac in thin pieces", later "gomme-laque", "gum lac". Most European languages (except Romance ones and Greek) have borrowed the word for the substance from English or from the German equivalent "Schellack".
Production.
Shellac is scraped from the bark of the trees where the female lac bug, "Kerria lacca" (Order" Hemiptera", Family "Kerriidae"), also known as "Laccifer lacca", secretes it to form a tunnel-like tube as it traverses the branches of the tree. Though these tunnels are sometimes referred to as "cocoons", they are not literally cocoons in the entomological sense. This insect is in the same Superfamily as the insect from which cochineal is obtained. The insects suck the sap of the tree and excrete "sticklac" almost constantly. The least coloured shellac is produced when the insects feed on the kusum tree ("Schleichera").
The number of lac bugs required to produce of shellac has variously been estimated as , , or . The root word lakh is a South Asian unit for and presumably refers to the huge numbers of insects that swarm on host trees, up to 150 per square inch.
The raw shellac, which contains bark shavings and lac bugs removed during scraping, is placed in canvas tubes (much like long socks) and heated over a fire. This causes the shellac to liquify, and it seeps out of the canvas, leaving the bark and bugs behind. The thick, sticky shellac is then dried into a flat sheet and broken into flakes, or dried into "buttons" (pucks/cakes), then bagged and sold. The end-user then crushes it into a fine powder and mixes it with ethyl alcohol prior to use, to dissolve the flakes and make liquid shellac.
Liquid shellac has a limited shelf life (about 1 year), hence it is sold in dry form for dissolution prior to use. Liquid shellac sold in hardware stores is clearly marked with the production (mixing) date, so the consumer can know whether the shellac inside is still good. Alternatively, old shellac may be tested to see if it is still usable: a few drops on glass should quickly dry to a hard surface. Shellac that remains tacky for a long time is no longer usable. Storage life depends on peak temperature, so refrigeration extends shelf life.
The thickness (concentration) of shellac is measured by the unit "pound cut", referring to the amount (in pounds) of shellac flakes dissolved in a gallon of denatured alcohol. For example: a 1-lb. cut of shellac is the strength obtained by dissolving one pound of shellac flakes in a gallon of alcohol. Most pre-mixed commercial preparations come at a 3-lb. cut. Multiple thin layers of shellac produce a significantly better end result than a few thick layers. Thick layers of shellac do not adhere to the substrate or to each other well, and thus can peel off with relative ease; in addition, thick shellac will obscure fine details in carved designs in wood and other substrates.
Shellac naturally dries to a high-gloss sheen. For applications where a flatter (less shiny) sheen is desired, products containing amorphous silica, such as "Shellac Flat," may be added to the dissolved shellac.
Shellac naturally contains a small amount of wax (3%–5% by volume), which comes from the lac bug. In some preparations, this wax is removed (the resulting product being called "dewaxed shellac"). This is done for applications where the shellac will be coated with something else (such as paint or varnish), so the topcoat will adhere. Waxy (non-dewaxed) shellac appears milky in liquid form, but dries clear.
Colors and availability.
Shellac comes in many warm colors, ranging from a very light blond ("platina") to a very dark brown ("garnet"), with many varieties of brown, yellow, orange and red in between. The colour is influenced by the sap of the tree the lac bug is living on and by the time of harvest. Historically, the most commonly sold shellac is called "orange shellac", and was used extensively as a combination stain and protectant for wood paneling and cabinetry in the 20th century.
Shellac was once very common anywhere paints or varnishes were sold (such as hardware stores). However, cheaper and more abrasion- and chemical-resistant finishes, such as polyurethane, have almost completely replaced it in decorative residential wood finishing such as hardwood floors, wooden wainscoting plank paneling, and kitchen cabinets. These alternative products, however, must be applied over a stain if the user wants the wood coloured; clear or blond shellac may be applied over a stain without affecting the color of the finished piece, as a protective topcoat. "Wax over shellac" (an application of buffed-on paste wax over several coats of shellac) is often regarded as a beautiful, if fragile, finish for hardwood floors. Luthiers still use shellac to "French polish" fine acoustic stringed instruments, but it has been replaced by synthetic plastic lacquers and varnishes in many workshops.
Properties.
Shellac is a natural bioadhesive polymer and is chemically similar to synthetic polymers, and thus can be considered a natural form of plastic. It can be turned into a moulding compound when mixed with wood flour and moulded under heat and pressure methods, so it can also be classified as thermoplastic.
Shellac scratches more easily than most lacquers and varnishes, and application is more labor-intensive, which is why it has been replaced by plastic in most areas. But damaged shellac can easily be touched-up with another coat of shellac (unlike polyurethane) because the new coat merges with and bonds to the existing coat(s). Shellac is much softer than Urushi lacquer for instance, which is far superior in regards to both chemical and mechanical resistance.
Shellac is soluble in alkaline solutions such as ammonia, sodium borate, sodium carbonate, and sodium hydroxide, and also in various organic solvents. When dissolved in denatured alcohol or ethanol, shellac yields a coating of good durability and hardness.
Upon mild hydrolysis shellac gives a complex mix of aliphatic and alicyclic hydroxy acids and their polymers that varies in exact composition depending upon the source of the shellac and the season of collection. The major component of the aliphatic component is aleuritic acid, whereas the main alicyclic component is shellolic acid.
Shellac is UV-resistant, and does not darken as it ages (though the wood under it may do so, as in the case of pine).
History.
The earliest written evidence of shellac goes back years, but shellac is known to have been used earlier. According to the Mahabharata, an entire palace was built out of dried shellac.
Shellac was in rare use as a dyestuff for as long as there was a trade with the East Indies. Merrifield cites 1220 for the introduction of shellac as an artist's pigment in Spain. Lapis lazuli as ultramarine pigment from Afghanistan was already being imported long before this.
The use of overall paint or varnish decoration on large pieces of furniture was first popularised in Venice (then later throughout Italy). There are a number of 13th century references to painted or varnished cassone, often dowry cassone that were made deliberately impressive as part of dynastic marriages. The definition of varnish is not always clear, but it seems to have been a spirit varnish based on gum benjamin or mastic, both traded around the Mediterranean. At some time, shellac began to be used as well. An article from the "Journal of the American Institute of Conservation" describes the use of infrared spectroscopy to identify a shellac coating on a 16th-century cassone. This is also the period in history where "varnisher" was identified as a distinct trade, separate from both carpenter and artist.
Another use for shellac is sealing wax. Woods's "The Nature and Treatment of Wax and Shellac Seals" discusses the various formulations, and the period when shellac started to be added to the previous beeswax recipes.
The "period of widespread introduction" would seem to be around 1550 to 1650, when the substance moves from being a rarity on highly decorated pieces to being described in the standard texts of the day.
Uses.
Historical.
In the early- and mid-20th century, orange shellac was used as a one-product finish (combination stain and varnish-like topcoat) on decorative wood paneling used on walls and ceilings in homes, particularly in the US. In the American South, use of knotty pine plank paneling covered with orange shellac was once as common in new construction as drywall is today. It was also often used on kitchen cabinets and hardwood floors, prior to the advent of polyurethane.
Until the advent of vinyl in 1949, most gramophone records were pressed from shellac compounds. From 1921 to 1928, tons of shellac were used to create 260 million records for Europe. In the 1930s, it was estimated that half of all shellac was used for gramophone records. Use of shellac for records was common until the 1950s and continued into the 1970s in some non-Western countries.
Until recent advances in technology, shellac (French polish) was the only glue used in the making of ballet dancers' pointe shoes, to stiffen the box (toe area) to support the dancer en pointe. Many manufacturers of pointe shoes still use the traditional techniques, and many dancers use shellac to revive a softening pair of shoes.
Shellac was historically used as a protective coating on paintings.
Sheets of Braille were coated with shellac to help protect them from wear due to being read by hand.
Shellac was used from the mid-19th century to produce small moulded goods such as picture frames, boxes, toilet articles, jewelry, inkwells and even dentures. Advances in plastics have rendered shellac obsolete as a moulding compound.
Shellac (both orange and white varieties) were used both in the field and laboratory to glue and stabilize dinosaur bones until about the mid 1960s. While effective at the time, the long-term negative effects of shellac (being organic in nature) on dinosaur bones and other fossils is debated and shellac is very rarely used by professional conservators and fossil preparators today.
Shellac was once used for fixing inductor, motor, generator and transformer windings, where it was applied directly to single layer windings in an alcohol solution. For multilayer windings, the whole coil was submerged in shellac solution, then drained and placed in a warm place to allow the alcohol to evaporate. The shellac then locks the wire turns in place, provides extra insulation and prevents movement and vibration, reducing buzz and hum. In motors and generators it also helps transfer force generated by magnetic attraction and repulsion from the windings to the rotor or armature. In more recent times, synthetic resins, such as glyptol, (Glyptal), have been substituted for the shellac. Some applications use shellac mixed with other natural or synthetic resins, such as pine resin or phenol-formaldehyde resin, of which Bakelite is the best known, for electrical use. Mixed with other resins, barium sulfate, calcium carbonate, zinc sulfide, aluminium oxide and/or cuprous carbonate (malachite), shellac forms a component of heat-cured capping cement used to fasten the caps or bases to the bulbs of electric lamps.
Current.
It is the central element of the traditional "French polish" method of finishing furniture and fine violas, guitars and pianos.
Shellac, edible, is used as a glazing agent on pills (see excipients) and candies, in the form of "pharmaceutical glaze" (or, "confectioner's glaze"). Because of its acidic properties (resisting stomach acids), shellac-coated pills may be used for a timed enteric or colonic release. Shellac is used as a 'wax' coating on citrus fruit to prolong its shelf/storage life. It is also used to replace the natural wax of the apple, which is removed during the cleaning process. When used for this purpose, it has the food additive E number E904.
Shellac coating applied with either a standard or modified Huon-Stuehrer nozzle, can be economically micro-sprayed onto various smooth candies, such as chocolate coated peanuts. Irregularities on the surface of the product being sprayed typically result in the formation of unsightly aggregates ("lac-aggs") which precludes the use of this technique on foods such as walnuts or raisins (however, chocolate-coated raisins being smooth surfaced, are able to be sprayed successfully using a modified Huon-Stuehrer nozzle).
Because it is compatible with most other finishes, shellac is also used as a barrier or primer coat on wood to prevent the bleeding of resin or pigments into the final finish, or to prevent wood stain from blotching.
Shellac is an odour and stain blocker and so is often used as the base of "solves all problems" primers. Although its durability against abrasives and many common solvents is not very good, shellac provides an excellent barrier against water vapour penetration. Shellac-based primers are an effective sealant to control odours associated with fire damage.
Shellac has traditionally been used as a dye for cotton and, especially, silk cloth in Thailand, particularly in the northeastern region. It yields a range of warm colours from pale yellow through to dark orange-reds and dark ochre. Naturally dyed silk cloth, including that using shellac, is widely available in the rural northeast, especially in Ban Khwao District, Chaiyaphum province. The Thai name for the insect and the substance is "khrang" (Thai: ครั่ง).
Other.
Shellac is used:

</doc>
<doc id="54831" url="https://en.wikipedia.org/wiki?curid=54831" title="Copper Island">
Copper Island

Copper Island is a local name given to the northern part of the Keweenaw Peninsula (projecting northeastward into Lake Superior at the western end of the Upper Peninsula of Michigan, United States of America), separated from the rest of the Keweenaw Peninsula by Portage Lake and the Keweenaw Waterway.
Geography.
The area was "isolated" by dredging in 1859 and construction in the 1860s of a ship canal across an isthmus of the Keweenaw Peninsula from Portage Lake—on the east side of the Keweenaw Peninsula—to Lake Superior on the west. The ship canal is wide and deep. The resulting "island" was called "Kuparisaari" (meaning "Copper Island") by Finnish, Irish, and French/French Canadian settlers in the area. However, neither the United States Geological Survey nor the state of Michigan identify this area as an island or use this name. Isle Royale is the largest naturally isolated island in Lake Superior; considered as an island, Copper Island would be the largest, with an area of around 554 square miles. It has a population around 21,500.
History.
Historically, "Kuparisaari" ('Copper Island') was used to mean the Keweenaw north of Portage Lake, but more generically the copper country of the Upper Peninsula. Inhabitants of the area wryly claimed "that they were outside the American mainland. In practical usage, however, the term included towns such as Oskar, Atlantic, Baltic, South Range, Houghton, Dodgeville and Hurontown" all of which were south of Portage Lake. Nevertheless, "unquestionably" Finns in those locales considered themselves to be "Copper Islanders." As the foregoing source indicates, "Copper Island" has sometimes been used as a sobriquet for Michigan's "copper country."
But in a larger sense, "Kuparisaari" was an amalgam of geographic location and cultural identity, particularly for the Finns. As one scholarly source notes:
Finnish immigration to Michigan’s copper district grew to become the most populous ethnic group with an enduring cultural identity. Kuparisaari, “copper island,” went beyond the Finnish immigrant identification of the island that comprises the northern half of the Keweenaw Peninsula to a symbolic island of landing, an Ellis Island. Michigan’s Copper Country is recognized as focal to Finnish immigration to America, the birthplace of many Finnish-American institutions religious, political and educational. This “island” includes both settlements in growing industrial urban communities like the Quincy, Calumet & Hecla and Champion mining {"See," Copper Range Company} settlements, and cleared forestland for traditional Finnish agriculture as in Toivola, Tapiola, Elo, Pelkie, and Waasa; Finns settled north and south of the Portage Waterway that bisects the peninsula. Perhaps more than any other immigrant group, the Finnish communities in the district were bisected into divisions of politics and faith. The Finns who immigrated to the copper mining district held to a pietistic Laestadian (Apostolic) Lutheran belief, to the state-sanctioned Lutheranism of Finland (Suomi Synod) or rejected faith altogether. Within these divides of conscience of faith was a wide political spectrum: conservative to liberal adherents, resolute temperance advocates and active radical socialists. The social and economic conditions that emigrants left in northern Scandinavia and the Duchy of Finland influenced these allegiances and beliefs.
Communities and transportation.
The principal towns on the Copper Island end of Keweenaw Peninsula are Hancock and Calumet. The area is connected to the rest of the Upper Peninsula by the Portage Lake Lift Bridge, the latest in a series of bridges between Hancock and Houghton. The bridge crosses the Portage Canal.
US 41 crosses this bridge. It enters Michigan at Menominee and goes north to it terminus just east of Copper Harbor at the far eastern tip of the peninsula.
Modern usage of the name.
A newspaper named "Copper Island News" was formerly published in Hancock, at least in the 1880s. and an unrelated newspaper called the "Copper Island Sentinel" was published weekly in Calumet from 4 April 1978 to August 1986.
Copper island is the core that the Keweenaw Water Trail wraps around. It is a designated loop route (which eliminates any need to use a shuttle or spot two vehicles) around and through the Keweenaw Peninsula for canoes and sea kayaks. The Keweenaw Waterway is central to it, crossing the peninsula. Established in 1995, it was designated “A Superior Sports Port” by National Geographic Adventure Magazine. The trail "exemplifies the Keweenaw Peninsula in the most literal sense." The Lake Superior coast line—craggy, varied and forbidding—is claimed to be comparable to Isle Royale ("sans" the ferry). Uninhabited wilderness, parks, and nature preserves and parks offer counterpoint to sheltered harbors and towns, where paddlers find the option of civilization, including warm bed, hot meal and shower. The Copper Island grand tour takes an 'average paddler' six to eight days, but extra days should be planned "to compensate for being wind-bound." This circumnavigation is on its way to becoming "Michigan’s top paddling destination." Shorter trips are possible.
The 'Copper Island Classic' is an ice hockey tournament contested annually between Hancock Central High School and Calumet High School. Such local usage still persists, and there are many business in the area that use it.
"The Race for Copper Island" (New York: Benziger Bros., 1905) is a novel written by Henry Sanislaus Spaulding (1865–1934) that involves the area.
Alternate use.
The phrase "Copper Island" was also used, especially in the 18th century, to describe a possibly mythical island in Lake Superior where there is an abundance of copper sitting on the surface of the land. While some scholars believe this was a reference to Isle Royale, the "island," because of its abundance of copper, could also have been the northern Keweenaw Peninsula., especially given the presence of vast quantities of native copper in the region.

</doc>
