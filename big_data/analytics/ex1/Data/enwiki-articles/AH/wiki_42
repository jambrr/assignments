<doc id="51298" url="https://en.wikipedia.org/wiki?curid=51298" title="Aaron Copland">
Aaron Copland

Aaron Copland (; November 14, 1900December 2, 1990) was an American composer, composition teacher, writer, and later in his career a conductor of his own and other American music. Instrumental in forging a distinctly American style of composition, in his later years he was often referred to as "the Dean of American Composers" and is best known to the public for the works he wrote in the 1930s and 1940s in a deliberately accessible style often referred to as "populist" and which the composer labeled his "vernacular" style.
Works in this vein include the ballets "Appalachian Spring", "Billy the Kid" and "Rodeo", his "Fanfare for the Common Man" and Third Symphony. The open, slowly changing harmonies of many of his works are archetypical of what many people consider to be the sound of American music, evoking the vast American landscape and pioneer spirit. In addition to his ballets and orchestral works, he produced music in many other genres including chamber music, vocal works, opera and film scores.
After some initial studies with composer Rubin Goldmark, Copland traveled to Paris, where he studied at first with Isidor Philipp and Paul Vidal, then with noted pedagogue Nadia Boulanger. He studied three years with Boulanger, whose eclectic approach to music inspired his own broad taste. Determined upon his return to the U.S. to make his way as a full-time composer, Copland gave lecture-recitals, wrote works on commission and did some teaching and writing. He found composing orchestral music in the modernist style he had adapted abroad a financially contradictory approach, particularly in light of the Great Depression. He shifted in the mid-1930s to a more accessible musical style which mirrored the German idea of "Gebrauchsmusik" ("music for use"), music that could serve utilitarian and artistic purposes. During the Depression years, he traveled extensively to Europe, Africa, and Mexico, formed an important friendship with Mexican composer Carlos Chávez and began composing his signature works.
During the late 1940s, Copland felt a need to compose works of greater emotional substance than his utilitarian scores of the late 1930s and early 1940s. He was aware that Stravinsky, as well as many fellow composers, had begun to study Arnold Schoenberg's use of twelve-tone (serial) techniques. In his personal style, Copland began to make use of twelve-tone rows in several compositions. He incorporated serial techniques in some of his later works, including his Piano Quartet (1950), Piano Fantasy (1957), "Connotations" for orchestra (1961) and "Inscape" for orchestra (1967). From the 1960s onward, Copland's activities turned more from composing to conducting. He became a frequent guest conductor of orchestras in the U.S. and the UK and made a series of recordings of his music, primarily for Columbia Records.
Biography.
Early life.
Aaron Copland was born in Brooklyn into a Conservative Jewish family of Lithuanian origins, the last of five children, on November 14, 1900. While emigrating from Russia to the United States, Copland's father, Harris Morris Copland, Anglicized his surname "Kaplan" to "Copland" while living and working in Scotland for two to three years to pay for the boat fare to the US. Copland was however unaware until late in his life that the family name had been Kaplan, and his parents never told him this. Throughout his childhood, Copland and his family lived above his parents' Brooklyn shop, H.M. Copland's, at 628 Washington Avenue (which Aaron would later describe as "a kind of neighborhood Macy's"), on the corner of Dean Street and Washington Avenue, and most of the children helped out in the store. His father was a staunch Democrat. The family members were active in Congregation Baith Israel Anshei Emes, where Aaron celebrated his Bar Mitzvah. Not especially athletic, the sensitive young man became an avid reader and often read Horatio Alger stories on his front steps.
Copland's father had no musical interest at all, but his mother, Sarah Mittenthal Copland, sang and played the piano, and arranged for music lessons for her children. Of his siblings, oldest brother Ralph was the most advanced musically, proficient on the violin, while his sister Laurine had the strongest connection with Aaron, giving him his first piano lessons, promoting his musical education, and supporting him in his musical career. She attended the Metropolitan Opera School and was a frequent opera goer. She often brought home libretti for Aaron to study. Copland attended Boys' High School and in the summer went to various camps. Most of his early exposure to music was at Jewish weddings and ceremonies, and occasional family musicales.
At the age of eleven, Copland devised an opera scenario he called "Zenatello", which included seven bars of music, his first notated melody. From 1913 to 1917 he took music lessons with Leopold Wolfsohn, who taught him the standard classical fare. Copland's first public music performance was at a Wanamaker's recital.
By the age of 15, after attending a concert by composer-pianist Ignacy Jan Paderewski, Copland decided to become a composer. After attempts to further his music study from a correspondence course, Copland took formal lessons in harmony, theory, and composition from Rubin Goldmark, a noted teacher and composer of American music (who had given George Gershwin three lessons). Goldmark gave the young Copland a solid foundation, especially in the Germanic tradition, as he stated later: "This was a stroke of luck for me. I was spared the floundering that so many musicians have suffered through incompetent teaching." But Copland also commented that the maestro had "little sympathy for the advanced musical idioms of the day" and his "approved" composers ended with Richard Strauss.
Copland's graduation piece from his studies with Goldmark was a three-movement piano sonata in a Romantic style. But he had also composed more original and daring pieces which he did not share with his teacher. In addition to regularly attending the Metropolitan Opera and the New York Symphony, where he heard the standard classical repertory, Copland continued his musical development through an expanding circle of musical friends. After graduating from high school, Copland played in dance bands. Continuing his musical education, he received further piano lessons from Victor Wittgenstein, who found his student to be "quiet, shy, well-mannered, and gracious in accepting criticism." Copland's fascination with the Russian Revolution and its promise for freeing the lower classes drew a rebuke from his father and uncles. In spite of that, in his early adult life Copland would develop friendships with people with socialist and communist leanings.
Studying in Paris.
From 1917 to 1921, Copland composed juvenile works of short piano pieces and art songs. Copland's passion for the latest European music, plus glowing letters from his friend Aaron Schaffer, inspired him to go to Paris for further study. His father wanted him to go to college, but his mother's vote in the family conference allowed him to give Paris a try. On arriving in France, he studied at the Fontainebleau School of Music with noted pianist and pedagogue Isidor Philipp and with Paul Vidal. But finding Vidal too much like Goldmark, Copland switched to famed teacher Nadia Boulanger, then aged thirty-four. He had initial reservations: "No one to my knowledge had ever before thought of studying with a woman." She interviewed him, and recalled later: "One could tell his talent immediately."
Boulanger had as many as forty students at once and employed a formal regimen that Copland had to follow, too. Copland found her incisive mind much to his liking and stated: "This intellectual Amazon is not only professor at the Conservatoire, is not only familiar with all music from Bach to Stravinsky, but is prepared for anything worse in the way of dissonance. But make no mistake ... A more charming womanly woman never lived." Though he planned on only one year abroad, he studied with her for three years, finding her eclectic approach inspired his own broad musical taste.
Adding to the heady cultural atmosphere of the early 1920s in Paris was the presence of expatriate American writers Paul Bowles, Ernest Hemingway, Sinclair Lewis, Gertrude Stein, and Ezra Pound, as well as artists like Picasso, Chagall, and Modigliani. Also influential on the new music were the French intellectuals Marcel Proust, Paul Valéry, Sartre, and André Gide, the latter cited by Copland as being his personal favorite and most read. Travels to Italy, Austria, and Germany rounded out Copland's musical education. During his stay in Paris, Copland began writing musical critiques, the first on Gabriel Fauré, which helped spread his fame and stature in the music community. Instead of wallowing in self-pity and self-destruction like many of the expatriate members of the Lost Generation, Copland returned to America optimistic and enthusiastic about the future.
1925 to 1950.
Upon returning to the U.S., Copland was determined to make his way as a full-time composer. He rented a studio apartment on New York City's Upper West Side in the Empire Hotel, which kept him close to Carnegie Hall and other musical venues and publishers. He remained in that area for the next thirty years, later moving to Westchester County, New York. Copland lived frugally and survived financially with help from two $2,500 Guggenheim Fellowships—one in 1925 and one in 1926. Lecture-recitals, awards, appointments, and small commissions, plus some teaching, writing, and personal loans kept him afloat in the subsequent years through World War II. Also important were wealthy patrons who supported the arts community during the Depression, underwriting performances, publication, and promotion of musical events and composers.
Copland's compositions in the early 1920s reflected the prevailing modernist attitude among intellectuals: that they were a small vanguard leading the way for the masses, who would only come to appreciate their efforts over time. In this view, music and the other arts need be accessible to only a select cadre of the enlightened. Toward this end, Copland formed the Young Composer's Group, modeled after France's "Six", gathering together promising young composers, acting as their guiding spirit.
Soon after his return, Copland was introduced to the artistic circle of Alfred Stieglitz and met many of the leading artists of that time. Stieglitz's conviction that the American artist should reflect "the ideas of American Democracy" influenced Copland and a whole generation of artists and photographers, including Paul Strand, Edward Weston, Ansel Adams, Georgia O'Keeffe, and Walker Evans. Evans' photographs inspired portions of Copland's opera "The Tender Land".
In his quest to take up Stieglitz's challenge, Copland had few established American contemporaries to emulate apart from Carl Ruggles and the reclusive Charles Ives, although the 1920s were Golden Years for American popular music and jazz, with George Gershwin, Bessie Smith, and Louis Armstrong leading the way. Later, however, Copland joined up with his younger contemporaries and formed a group termed the "commando unit," which included Roger Sessions, Roy Harris, Virgil Thomson, and Walter Piston. They collaborated in joint concerts showcasing their work to new audiences.
Copland's relationship with the "commando unit" was one of both support and rivalry, and he played a key role in keeping them together. The five young American composers helped promote each other and their works but also had testy exchanges, inflamed by the assertion of the press that Copland was the "truly American" composer. Going beyond the five, Copland was generous with his time with nearly every American young composer he met during his life, later earning the title the "Dean of American Music."
Mounting troubles with the "Symphonic Ode" (1929) and "Short Symphony" (1933) caused him to rethink the paradigm of composing orchestral music for a select group, as it was a financially contradictory approach, particularly in the Depression. In many ways, this shift mirrored the German idea of Gebrauchsmusik ("music for use"), as composers sought to create music that could serve a utilitarian as well as artistic purpose. This approach encompassed two trends: first, music that students could easily learn, and second, music which would have wider appeal, such as incidental music for plays, movies, radio, etc. Copland undertook both goals, starting in the mid-1930s.
Perhaps motivated by the plight of children during the Depression, around 1935 Copland began to compose musical pieces for young audiences, in accordance with the first goal of American Gebrauchsmusik. These works included piano pieces ("The Young Pioneers") and an opera ("The Second Hurricane").
During the Depression years, Copland traveled extensively to Europe, Africa, and Mexico. He formed an important friendship with Mexican composer Carlos Chávez and would return often to Mexico for working vacations conducting engagements. During his initial visit to Mexico, Copland began composing the first of his signature works, "El Salón México", which he completed four years later in 1936. This and other incidental commissions fulfilled the second goal of American Gebrauchsmusik, creating music of wide appeal.
During this time, he composed (for radio broadcast) "Prairie Journal," one of his first pieces to convey the landscape of the American West. Branching out into theater, Copland also played an important role providing musical advice and inspiration to The Group Theater—Stella Adler's and Lee Strasberg's "method" acting school. The Group Theater followed Copland's musical agenda and focused on plays that illuminated the American experience. After Hitler and Mussolini's attacks on Spain in 1936, leftist parties had united in a Popular Front against Fascism. Many Group Theater members were influenced by Marxism and other progressive philosophies, and several had joined the Communist Party, including Elia Kazan and Clifford Odets. Copland also had contact later with other major American playwrights, including Thornton Wilder, William Inge, Arthur Miller, and Edward Albee, and considered projects with all of them. During the 1930s, Copland wrote incidental music for several plays, including Irwin Shaw's "Quiet City" (1939), considered one of his most personal and poignant scores.
In 1939, Copland completed his first two Hollywood film scores, for "Of Mice and Men" and "Our Town", and received sizable commissions. In the same year, he composed the radio score "John Henry", based on the folk ballad. But it wasn't until the worldwide market for classical recordings boomed after World War II that he achieved economic security. Even after securing a comfortable income, he continued to write, teach, lecture, and, eventually, conduct.
Demonstrating his broad range, Copland in the 1930s began composing music for ballet, including his highly successful "Billy the Kid" (1939), the second of four ballets he scored (after "Hear Ye! Hear Ye!" (1934)). In an interview with Vivian Perlis, Eugene Loring said of the ballet, "In our western states, there were still a few old-timers who remembered Billy. One came backstage in San Francisco to tell us that it was all fine, except that Billy really shot left-handed!" Copland's ballet music established him as an authentic composer of American music much as Stravinsky's ballet scores connected the composer with Russian music. Copland's timing was excellent; he helped fill a vacuum for the American choreographers who needed suitable music to score their own nationalistic dance repertory.
In keeping with the wartime period, Copland's Piano Sonata (1941) was a piece characterized as "grim, nervous, elegiac, with pervasive bell-like tolling of alarm and mourning." It was later adapted to "Day on Earth," a landmark American dance by Doris Humphrey.
Copland started to publish some of his lectures in the 1930s, "What to Listen for in Music" being one of the most notable of his writings. He also took a leading role in the American Composers Alliance, whose mission was "to regularize and collect all fees pertaining to performance of their copyrighted music" and "to stimulate interest in the performance of American music." Copland eventually moved over to rival ASCAP. Through royalties and with his great success from 1940 on, Copland amassed a multimillion-dollar fortune by the time of his death.
The decade of the 1940s was arguably Copland's most productive, and it firmly established his worldwide fame. His two ballet scores for "Rodeo" (1942) and "Appalachian Spring" (1944) were huge successes. His pieces "Lincoln Portrait" and "Fanfare for the Common Man" have become patriotic standards (See Popular works, below). Also important was the "Third Symphony". Composed in a two-year period from 1944 to 1946, it became Copland's best-known symphony.
In 1945, Copland contributed to "Jubilee Variation", a work commissioned by the Cincinnati Symphony in which ten American composers collaborated, but the piece is seldom heard in the concert hall. Copland's "In the Beginning" (1947) is a choral work using the first chapter and the first seven verses of the second chapter of Genesis from the King James Version of the Bible and is a masterpiece of the choral repertory.
Copland's Clarinet Concerto (1948), scored for solo clarinet, strings, harp, and piano, was a commission piece for bandleader and clarinetist Benny Goodman and a complement to Copland's earlier jazz-influenced work, the Piano Concerto (1926). His "Four Piano Blues" is an introspective composition with a jazz influence.
Copland finished the 1940s with two film scores, one for William Wyler's 1949 film "The Heiress" and one for the film adaptation of John Steinbeck's novel "The Red Pony".
In 1949, he returned to Europe to find Pierre Boulez dominating the group of post-war avant-garde composers. He also met with proponents of twelve-tone technique, based on the works of Arnold Schoenberg, and found himself interested in adapting serial methods to his own musical voice.
1950s and 1960s.
In 1950, Copland received a U.S.-Italy Fulbright Commission scholarship to study in Rome, which he did the following year. Around this time, he also composed his "Piano Quartet", adopting Schoenberg's twelve-tone method of composition, and "Old American Songs" (1950), the first set of which was premiered by Peter Pears and Benjamin Britten, the second by William Warfield.
Because of the political climate of that era, "A Lincoln Portrait" was withdrawn from the 1953 inaugural concert for President Eisenhower. That same year, Copland was called before Congress, where he testified that he was never a communist.
Despite the difficulties that his suspected Communist sympathies posed, Copland nonetheless traveled extensively during the 1950s and early 1960s, observing the avant-garde styles of Europe while experiencing the new school of Soviet music. In addition, he was rather taken with the work of Toru Takemitsu while in Japan and began a correspondence with him that would last over the next decade. Copland wrote of the Japanese composer: "He has the 'pure gold' touch, he chooses his notes carefully and meaningfully." Copland also gained exposure to the latest musical trends in Poland and Scandinavia. In observing these new musical forms, Copland revised his text "The New Music" with comments on the styles that he encountered. In particular, while Copland explained the importance of the work of John Cage and others (in his chapter titled "The Music of Chance"), he found that these radical trends in music which appealed to those "who enjoy teetering on the edge of chaos" were less likely to gain the appreciation of a wider audience "who envisage art as a bulwark against the irrationality of man's nature." As he summarized: "I've spent most of my life trying to get the right note in the right place. Just throwing it open to chance seems to go against my natural instincts."
In 1954, Copland received a commission from Richard Rodgers and Oscar Hammerstein to create music for the opera "The Tender Land", based on James Agee's "Let Us Now Praise Famous Men". Copland had been wary of writing an opera, being especially aware of the pitfalls of that form, including weak libretti and demanding production values. Nevertheless, Copland decided to try his hand at "la forme fatale," especially as the 1950s were boom times for American playwrights, with Arthur Miller, Clifford Odets and Thornton Wilder doing some of their best work. Originally two acts, "The Tender Land" was later expanded to three. As Copland feared, critics found the libretto to be the opera's weakness, and he later stated: "I admit that if I have one regret it is that I never did write a 'grand opera'." In spite of its flaws, the opera has established itself as one of the few American operas in the standard repertory.
In 1957, 1958, and 1976, Copland was the Music Director of the Ojai Music Festival, a classical and contemporary music festival in Ojai, California.
Copland exerted a major influence on the compositional style of an entire generation of American composers, including his friend and protégé Leonard Bernstein. Bernstein was considered the finest conductor of Copland's works and cites Copland's "aesthetic, simplicity with originality" as being his strongest and most influential traits.
For the occasion of the Metropolitan Museum of Art Centennial, Copland composed "Ceremonial Fanfare For Brass Ensemble" to accompany the exhibition "Masterpieces Of Fifty Centuries." Leonard Bernstein, Walter Piston, William Schuman, and Virgil Thomson also composed pieces for the Museum's Centennial exhibitions.
Later life.
From the 1960s onward, Copland's activities turned more from composing to conducting. Though not enamored with the prospect, he found himself without new ideas for composition, saying: "It was exactly as if someone had simply turned off a faucet." Copland was a frequent guest conductor of orchestras in the U.S. and the UK. He made a series of recordings of his music, primarily for Columbia Records. In 1960, RCA Victor released Copland's recordings with the Boston Symphony Orchestra of the orchestral suites from "Appalachian Spring" and "The Tender Land"; these recordings were later reissued on CD, as were most of Copland's Columbia recordings (by Sony).
From 1960 to his death, he resided at Cortlandt Manor, New York. His home, known as Rock Hill, was added to the National Register of Historic Places in 2003. It was further designated a National Historic Landmark in 2008. Copland's health deteriorated through the 1980s, and he died of Alzheimer's disease and respiratory failure on December 2, 1990, in North Tarrytown, New York (now Sleepy Hollow). Much of his large estate was bequeathed to the creation of the Aaron Copland Fund for Composers, which bestows over $600,000 per year to performing groups.
Personal life.
Deciding not to follow the example of his father, a solid Democrat, Copland never enrolled as a member of any political party, but he espoused a general progressive view and had strong ties with numerous colleagues and friends in the Popular Front, including Odets. Copland supported the Communist Party USA ticket during the 1936 presidential election, at the height of his involvement with The Group Theater, and remained a committed opponent of militarism and the Cold War, which he regarded as having been instigated by the United States. He condemned it as "almost worse for art than the real thing". Throw the artist "into a mood of suspicion, ill-will, and dread that typifies the cold war attitude and he'll create nothing". In keeping with these attitudes, Copland was a strong supporter of the Presidential candidacy of Henry A. Wallace on the Progressive Party ticket. As a result, he was later investigated by the FBI during the Red scare of the 1950s and found himself blacklisted.
Copland was included on an FBI list of 151 artists thought to have Communist associations. Joseph McCarthy and Roy Cohn questioned Copland about his lecturing abroad and his affiliations with various organizations and events, neglecting completely Copland's works which made a virtue of American values. Copland made several denials on record of any serious involvement with a list of political/cultural organizations identified as subversive by the House Un-American Activities Committee (HUAC). Copland has also been on record saying he does not think music has political importance despite having composed some of the most iconic American art music of the 20th century.
Given the nature of the hearings, Copland was asked to prepare explanations for his seemingly large involvement in explicitly communist and communist leaning organizations. The danger Copland potentially presented was not in belonging to communist organizations, but in the possibility of spreading those ideas in the Latin American countries he was paid by the state to lecture in. The U.S, at the time, still had an interest with overseeing the continuation of democracy in Latin America.
Outraged by the accusations, many members of the musical community held up Copland's music as a banner of his patriotism. The investigations ceased in 1955 and were closed in 1975. Though taxing of his time, energy, and emotional state, the McCarthy probes did not seriously affect Copland's career and international artistic reputation. In any case, beginning in 1950, Copland, who had been appalled at Stalin's persecution of Shostakovich and other artists, began resigning from participation in leftist groups. He decried the lack of artistic freedom in the Soviet Union, and in his 1954 Norton lecture he asserted that loss of freedom under Soviet Communism deprived artists of "the immemorial right of the artist to be wrong." He began to vote Democratic, first for Stevenson and then for Kennedy.
Copland was an agnostic. However, Copland had various encounters with organized religious thought, which influenced some of his early compositions. Copland was once close with the Zionist movement during the Popular Front movement, when it was endorsed by the left. In relation to his compositions one of his earliest musical interests was with klezmer music. The music of his childhood synagogue would be one of the early influences of his fresh musical aesthetic.
Copland is documented as gay in author Howard Pollack's biography, "Aaron Copland: The Life and Work of an Uncommon Man". Like many of his contemporaries he guarded his privacy, especially in regard to his homosexuality, providing very few written details about his private life. However, he was one of the few composers of his stature to live openly and travel with his intimates, most of whom were talented, much younger men. Among Copland's love affairs, most of which lasted for only a few years yet became enduring friendships, were ones with photographer Victor Kraft, artist Alvin Ross, pianist Paul Moor, dancer Erik Johns, composer John Brodbin Kennedy, and painter Prentiss Taylor.
Victor Kraft would prove to be the one constant romantic relationship in Copland's life. Originally a student of music under Copland, Kraft gave up music in pursuit of a career in photography on Copland's urging. Kraft would leave and re-enter Copland's life, often bringing much stress with him: their relationship would fluctuate from contentedness to erratically confrontational on Kraft's part. Kraft fathered a child to whom Copland later provided financial security, through a bequest from his estate.
Composer.
Influences.
Copland's earliest musical inclinations as a teenager ran toward Chopin, Debussy, Verdi and the Russian composers. Some of his preferences might also have been formed by the anti-German feelings during World War I, as later he studied German music. Copland's curiosity about the latest music from Debussy and Scriabin was frustrated by the fact that the scores of "avant-garde" works were expensive at that time and hard to come by. So he borrowed these works from a music library and studied them intensely. Some of his earliest compositions were songs and piano pieces inspired by these European influences.
Copland's teacher and mentor Nadia Boulanger became his most important influence. In gratitude for the immense support and promotion on his behalf, he told her in 1950, "I shall count our meeting the most important of my musical life ... Whatever I have accomplished is intimately associated in my mind with those early years, and with what you have since been as inspiration and example." Of all her students, she listed Copland first. Copland especially admired Boulanger's total grasp of all classical music, and he was encouraged to experiment and develop a "clarity of conception and elegance in proportion." Following her model, he studied all periods of classical music and all forms—from madrigals to symphonies. This breadth of vision led Copland to compose music for numerous settings—orchestra, opera, solo piano, small ensemble, art song, ballet, theater and film. Boulanger particularly emphasized "la grande ligne" (the long line), "a sense of forward motion ... the feeling for inevitability, for the creating of an entire piece that could be thought of as a functioning entity."
In discovering Johann Sebastian Bach, Copland pointed out: "has an inexhaustible wealth of musical riches, which no music lover can afford to ignore ... What strikes me most markedly about Bach's work is the marvelous rightness of it. It is the rightness not merely of a single individual, but a whole musical epoch." Copland stated that an ideal music might combine Mozart's "spontaneity and refinement" with Palestrina's "purity" and Bach's "profundity".
Copland was excited to be so close to the new post-Impressionistic French music of Ravel, Roussel, and Satie, as well as Les six, a group that included Milhaud, Poulenc, and Honegger. Webern, Berg, and Bartók also impressed him. Copland was "insatiable" in seeking out the newest European music, whether in concerts, score reading or heated debate. These "moderns" were discarding the old laws of composition and experimenting with new forms, harmonies and rhythms, and including the use of jazz and quarter-tone music. Serge Koussevitzky had just arrived in Paris and was adding to the ferment by conducting and promoting the new music of Russia and France. Later he would conduct many Copland premieres in New York. Among the first performances that Copland attended was Milhaud's "La création du monde", which caused riots in Paris. Milhaud was Copland's inspiration for some of his earlier "jazzy" works. He was also exposed to Schoenberg and admired his earlier atonal pieces, thinking Schoenberg's "Pierrot Lunaire" a landmark work comparable to Stravinsky's "The Rite of Spring." Copland even tried out Schoenberg's innovative twelve-tone system and adapted it to his style.
Above all others, Copland named Igor Stravinsky as his "hero" and his favorite 20th-century composer. Stravinsky was in many ways his premiere model. Stravinsky's rhythm and vitality is apparent in many of his works. Copland especially admired Stravinsky's "jagged and uncouth rhythmic effects," "bold use of dissonance," and "hard, dry, crackling sonority." In a 1950 radio interview, Copland is quoted saying that there is a "freshness of atmosphere; a freshness of personality—which looks very attractive to American composers. Europeans are not seeking freshness of music as much as American composers. The reason being that through their long tradition in music—they already know in advance what they are supposed to write." As a publicly identified composer of iconic American music, Copland's claim that American composers are still in search for a certain freshness to composition—found in Stravinsky—show they continuing uncertainty of the American art music scene in the 1950s. Despite using folk themes as a tool for signifying Americanness, Copland continued to find "freshness" in Stravinsky's work—especially in his usage of rhythm. Copland was similarly but not quite as strongly impressed by Sergei Prokofiev's "fresh, clean-cut, articulate style."
Another inspiration for much of Copland's music was jazz. Although familiar with jazz back in America—having listened to it and also played it in bands—he fully realized its potential while traveling in Austria: "The impression of jazz one receives in a foreign country is totally unlike the impression of such music heard in one's own country ... when I heard jazz played in Vienna, it was like hearing it for the first time." He also found that the distance from his native country helped him see the United States more clearly. Beginning in 1923, he employed "jazzy elements" in his classical music, but by the late 1930s, he moved on to Latin and American folk tunes in his more successful pieces. His earlier works especially demonstrate the influence of jazz rhythmic, timbral and harmonic practices. That influence is apparent in a few later works, such as the Clarinet Concerto commissioned by Benny Goodman. During the late 1920s and 1930s, Copland sought out jazz at the Cotton Club and heard Duke Ellington, Benny Carter and Bix Beiderbecke, among others. Of Duke Ellington among other jazz composers, Copland said he was "the master of them all."
Although Copland was intrigued by the idea of a "jazz concerto" and "symphonic jazz," his Concerto for Piano and Orchestra did not succeed in that form as had those of Maurice Ravel and George Gershwin, who was praised by such eminent musical exiles as Schoenberg, Bartók, and Stravinsky (Gershwin had recently died at 38 and so was no longer a potential rival). Copland would go on to write extensively and deliver the Norton lectures about jazz in America, especially the big band sound (1930s) and cool West Coast jazz (1950s). Yet, enthusiastic as he was about jazz throughout his life, Copland also recognized its limitations: With the Concerto I felt I had done all I could with the idiom, considering its limited emotional scope. True, it was an easy way to be American in musical terms, but all American music could not possibly be confined to two dominant jazz moods – the blues and the snappy number.
Jazz played an important role for some of Copland's compositions. What constituted as Jazz was contested by many musicians and scholars. Copland believed that the essence of Jazz was rooted in rhythm. Copland identified any sort of syncopation as metrical phenomenon. He called ragtime Jazz' closest ancestor, while also citing the foxtrot rhythm—and later the usage of poly-rhythms as the basis for modern jazz. By the 1950s,Copland had come to see the possibilities of Jazz less and less in his compositions, though the idea of syncopated rhythm would continue to feature prominently in many of his works.
Although his early focus of jazz gave way to other influences, Copland continued to make use of jazz in more subtle ways in later works. But it was the synthesizing of all his influences and inclinations which create the "Americanism" of his music. Copland pointed out in summarizing the American character of his music, "the optimistic tone", "his love of rather large canvases", "a certain directness in expression of sentiment", and "a certain songfulness". As he advanced in his career (by 1941), he said of himself and advised other composers: I no longer feel the need of seeking out conscious Americanisms and folk rhythms. Because we live here and work here, we can be certain that when our music is mature it will also be American in quality. In contradiction to this statement, however, he continued to look for and employ folk material for several more years.
Copland's work from the late 1940s onward included experimentation with Schönberg's twelve-tone system, resulting in two major works, the "Piano Quartet" (1950) and the "Piano Fantasy" (1957).
Early work.
Copland's earliest compositions before leaving for Paris were short works for piano and some art songs, inspired mostly by Liszt and Debussy. He experimented with ambiguous beginnings and endings, rapid key changes, and the frequent use of tritones. His first published work was "The Cat and the Mouse" (1920), a piano solo piece based on a fable by Jean de la Fontaine. In "Three Moods" (1921), Copland's final movement is entitled "Jazzy", which he noted "is based on two jazz melodies and ought to make the old professors sit up and take notice".
One of Copland's first significant works upon returning from his studies in Paris was the necromantic ballet "Grohg". This ballet, suggested to Copland by the film "Nosferatu", a free adaptation of the Dracula tale, provided the source material for his later "Dance Symphony". Originally intended as an orchestral exercise while he was studying in Paris, Copland completed it as a full orchestral score after returning to New York in 1925. It too had "jazz elements" as did many of Copland's works in the 1920s.
Copland's "Symphony for Organ and Orchestra" (1924) brought him into contact with Serge Koussevitzky, a conductor known as a champion of "new music", and another figure who would prove to be influential in Copland's life, perhaps the second most important after Boulanger. Koussevitzky performed twelve Copland works during his tenure as conductor of the Boston Symphony. Copland's relationship with Koussevitzky was apparently unique, as his interpretations of Copland's works reflected the particular admiration that the latter had for the young composer. Copland's "Music for the Theatre" (1925) and the Piano Concerto (1926) were both composed for Koussevitzky.
Visits to Europe in 1926 and 1927 brought him into contact with the most recent developments there, including Webern's "Five Pieces for Orchestra", which greatly impressed him. In August 1927, while staying in Königstein, Copland wrote "Poet's Song", a setting of a text by E. E. Cummings and his first composition using Schoenberg's twelve-tone technique. This was followed by the "Symphonic Ode" (1929) and the "Piano Variations" (1930), both of which rely on the exhaustive development of a single short motive. This procedure, which provided Copland with more formal flexibility and a greater emotional range than in his earlier music, is similar to Schoenberg's idea of "continuous variation" and, according to Copland's own admission, was influenced by the twelve-tone method, though neither work actually uses a twelve-tone row.
Other major works of his first period include the "Piano Variations" (1930), and the "Short Symphony" (1933). However, this jazz-inspired period was relatively brief, as his style evolved toward the goal of writing more accessible works using folk sources.
Popular works.
Impressed with the success of Virgil Thomson's "Four Saints in Three Acts", Copland wrote "El Salón México" between 1932 and 1936, which met with a popular acclaim that contrasted the relative obscurity of most of his previous works. It appears he intended it to be a popular favorite, as he wrote in 1955: "It seems a long long time since anyone has written an "España" or "Bolero"—the kind of brilliant orchestral piece that everyone loves." Inspiration for this work came from Copland's vivid recollection of visiting the "Salon Mexico" dancehall where he witnessed a more intimate view of Mexico's nightlife. For Copland, the biggest impact came, not from the music of the people dancing, but from the spirit of the environment. Copland said that he could literally feel the essence of the Mexican people in the dance hall. This prompted him to write a piece celebrating the spirit of Mexico using Mexican Themes. Copland derived freely from two collections of Mexican folk tunes, changing pitches and varying rhythms. The use of a folk tune with variations set in a symphonic context started a pattern he repeated in many of his most successful works right on through the 1940s. This work also marked the return of jazz patterns to Copland's compositional style, though they appeared in a more subdued form than before and were no longer the centerpiece. Chávez conducted the premiere, and "El Salón México" became an international hit, gaining Copland wide recognition.
Copland achieved his first major success in ballet music with his groundbreaking score "Billy the Kid" , based on a Walter Noble Burns novel, with choreography by Eugene Loring. The ballet was among the first to display an American music and dance vocabulary, adapting the "strong technique and intense charm of Astaire" and other American dancers. It was distinctive in its use of polyrhythm and polyharmony, particularly in the cowboy songs. The ballet premiered in New York in 1939, with Copland recalling "I cannot remember another work of mine that was so unanimously received." John Martin wrote, "Aaron Copland has furnished an admirable score, warm and human, and with not a wasted note about it anywhere." It became a staple work of the American Ballet Theatre, and Copland's twenty-minute suite from the ballet became part of the standard orchestral repertoire. When asked how a Jewish New Yorker managed so well to capture the Old West, Copland answered "It was just a feat of imagination."
In the early 1940s, Copland produced two important works intended as national morale boosters. "Fanfare for the Common Man", scored for brass and percussion, was written in 1942 at the request of the conductor Eugene Goossens, conductor of the Cincinnati Symphony Orchestra. It would later be used to open many Democratic National Conventions, and to add dignity to a wide range of other events. Even musical groups from Woody Herman's jazz band to the Rolling Stones adapted the opening theme. Emerson, Lake & Palmer recorded a "progressive rock" version of the composition in 1977. The fanfare was also used as the main theme of the fourth movement of Copland's "Third Symphony," where it first appears in a quiet, pastoral manner, then in the brassier form of the original. In the same year, Copland wrote "A Lincoln Portrait", a commission from conductor André Kostelanetz, leading to a further strengthening of his association with American patriotic music. The work is famous for the spoken recitation of Lincoln's words, though the idea had been previously employed by John Alden Carpenter's "Song of Faith" based on George Washington's quotations. "Lincoln Portrait" is often performed at national holiday celebrations. Many Americans have performed the recitation, including politicians, actors, and musicians and Copland himself, with Henry Fonda doing the most notable recording.
Continuing his string of successes, in 1942 Copland composed the ballet "Rodeo", a tale of a ranch wedding, written around the same time as "Lincoln Portrait". "Rodeo" is another enduring composition for Copland and contains many recognizable folk tunes, well-blended with Copland's original music. Notable in the final movement, is the striking "Hoedown". This was a recreation of Appalachian fiddler W. H. Stepp's version of the square-dance tune "Bonypart" ("Bonaparte's Retreat"), which had been transcribed for piano by Ruth Crawford Seeger and published in Alan Lomax and Seeger's book, "Our Singing Country" (1941). For the "Hoedown" in "Rodeo" Copland borrowed note for note from Seeger's piano transcription of Stepp's tune. This fragment (lifted from Ruth Crawford Seeger) is now one of the best-known compositions by any American composer, having been used numerous times in movies and on television, including commercials for the American beef industry. "Hoedown" was given a rock arrangement by Emerson, Lake & Palmer in 1972. The ballet, originally titled "The Courting at Burnt Ranch", was choreographed by Agnes de Mille, niece of film giant Cecil B. DeMille. It premiered at the Metropolitan Opera on October 16, 1942, with de Mille dancing the principal "cowgirl" role and the performance received a standing ovation. A reduced score is still popular as an orchestral piece, especially at "Pops" concerts.
Copland was commissioned to write another ballet, "Appalachian Spring", originally written using thirteen instruments, which he ultimately arranged as a popular orchestral suite. The commission for "Appalachian Spring" came from Martha Graham, who had requested of Copland merely "music for an American ballet". Copland titled the piece "Ballet for Martha", having no idea of how she would use it on stage but he had her in mind. "When I wrote 'Appalachian Spring' I was thinking primarily about Martha and her unique choreographic style, which I knew well ... And she's unquestionably very American: there's something prim and restrained, simple yet strong, about her which one tends to think of as American." Copland borrowed the flavor of Shaker songs and dances, and directly used the dance song Simple Gifts. Graham took the score and created a ballet she called "Appalachian Spring" (from a poem by Hart Crane which had no connection with Shakers). It was an instant success, and the music later acquired the same name. Copland was amused and delighted later in life when people would come up to him and say: "Mr. Copland, when I see that ballet and when I hear your music I can see the Appalachians and just feel spring." Copland had no particular setting in mind while writing the music, he just tried to give it an American flavor, and had no knowledge of the borrowed title, in which "spring" refers to a spring of water, not the season Spring.
Symphonic works.
Copland composed three numbered symphonies, but applied the word "symphony" to more than just symphonies of typical structure. He re-orchestrated his early three-movement Organ Symphony omitting the organ, calling the result his First Symphony. His fifteen-minute "Short Symphony" was the Second Symphony, though it also exists as the Sextet. His "Dance Symphony" was hurriedly extracted from the earlier unproduced ballet "Grohg" to meet an RCA Records commission deadline.
The Third Symphony is in the more traditional format (four movements; second movement, scherzo; third movement, adagio) and is his most famous symphony. At forty minutes, it is his longest orchestral composition. He composed it with Koussevitzky's unique character in mind, "I knew exactly the kind of music he enjoyed conducting and the sentiments he brought with it, and I knew the sound of his orchestra, so I had every reason to do my darnedest to write a symphony in the grand manner." Among the details of interest in the work is Copland's use of palindromic structure—whole movements as well as melodies end as they began. Completing the work after World War II was won by the Allies, he stated that the symphony was "intended to reflect the euphoric spirit of the country at the time." The work received generally strong acclaim. Koussevitzky "declared it simply the greatest American symphony ever written." Arthur Berger stated that it achieved "a kind of panorama of all the musical resources that have through the years formed his musical language." While Leonard Bernstein "deemed it the epitome of a decades-long search by many composers for a distinctly American music." It is the best known, most performed, and most recorded American symphony of the 20th Century.
Later work.
Copland's work in the late 1940s and 1950s included use of Schönberg's twelve-tone system, a development that he recognized as important, but which he did not fully embrace. His first result was his "Piano Quartet" (1950). However, he found the atonality of serialized music to run counter to his desire to reach a wide audience. So, in contrast to the Second Viennese School, Copland's use of the system emphasized the importance of the "classicalizing principles", in order to prevent the material from falling into "near-chaos". He began his first serial work, the "Piano Fantasy," in 1951 to fulfill a commission from the young virtuoso pianist William Kapell. The piece became one of his most challenging works, over which he labored until 1957. During the work's development, in 1953, Kapell died in an aircraft crash. Critics lauded the "Fantasy" when it was finally premiered, calling the piece "an outstanding addition to his own oeuvre and to contemporary piano literature" and "a tremendous achievement". Jay Rosenfield stated, "This is a new Copland to us, an artist advancing with strength and not building on the past alone."
Copland had approached dodecaphony with some initial skepticism. Although he had been "well aware that serial composition was the dominant method of composition" after World War II, he expressed doubts about it to Panamanian composer Roque Cordero at Tanglewood in 1946 and added in 1948, "Now they have come up with the Schoenberg twelve-tone system, 'discovering' it as if it were something quite new." While in Europe in 1949, he heard a number of serial works but did not admire much of it because "so often it seemed that individuality was sacrificed to the method." However, the music of French composer Pierre Boulez showed him that the technique could be separated from the "old Wagnerian" aesthetic with which he had associated it previously. The late music of Austrian composer Anton Webern and twelve-tone pieces by Swiss composer Frank Martin and Italian composer Luigi Dallapiccola only strengthened this opinion.
Eventually, Copland came to the conclusion that composing along serial lines was "like looking at a picture from a different point of view." He explained in 1957, "As I see it, twelve-tonism is nothing more than an angle of vision. Like fugal treatment, it is a stimulus that enlivens musical thinking, especially when applied to a series of tones that lend themselves to that treatment. It is a method, not a style; and therefore it solves no problems of musical expressivity." He began using dodecaphonism "with the hope that it would freshen and enrich my technique."
One attraction of serial composition, Copland said, "was that I began to hear chords that I wouldn't have heard otherwise. Heretofore, I had been thinking tonally, but this was a new way of moving tones about. It freshened up one's technique and one's approach." He liked the fact that the practice forced him to "unconventionalize his thinking with respect to chordal structure" and freshened "his melodic and figurational imagination." It fulfilled Copland's need for "more chords," as he phrased it to Bernstein and what Taruskin calls a renewal of his "technical or stylistic resources." As he later maintained in an interview, Copland attempted "to do my own thing using an extended language." Throughout his career, he had been open to incorporating different musical styles into his music, such as jazz and folk music. Serialism was one more style to adapt and use in his own way.
Serialism also allowed Copland a synthesis of serial and non-serial practices, a dichotomy that according to musicologist Joseph Straus had long concerned Copland and had been considered irreconcilable. Copland wrote that, to him, serialism pointed in two opposite directions, one "toward the extreme of total organization with electronic applications" and the other "a gradual absorption into what had become a very "freely interpreted tonalism" Copland." The path he said he chose was the latter one, which he said, when he described his "Piano Fantasy", allowed him to incorporate "elements able to be associated with the twelve-tone method and also with music tonally conceived."
Copland maintained that he used the technique in much the same manner as Schoenberg, in that he thought first of a musical theme rather than picking the theme from a series of notes. Nevertheless, his actual use of the tone row did not follow standard practice. Schoenberg generally structured his twelve-tone music around complete statements of the row in one of its four shapes—prime or original (forward), retrograde (reversed), inverted or retrograde and inverted. Copland rarely states a complete row. He includes the prime and reversed forms of the row at moments of structural importance comparable to the exposition of the two themes in sonata form. Otherwise, he used parts of the row of between two and six notes to form melodies and harmonies, a practice not far removed from how he composed his tonal works. By doing so, Copland took advantage of the potential for richly-dense harmonies, built on seconds and their inversions, that dodecaphony made available. In the Piano Quartet, this allowed him, according to Pollack, to combine "familiarly Coplandesque gestures and moods ... with fresh ideas and feelings."
Even after Copland started using 12-tone techniques, he did not stick to them exclusively but went back and forth between tonal and non-tonal compositions. Other late works include: "Dance Panels" (1959, ballet music), "Something Wild" (1961, his last film score, much of which would be later incorporated into his "Music for a Great City"), "Connotations" (1962, for the new Lincoln Center Philharmonic hall), "Emblems" (1964, for wind band), "Night Thoughts" (1972, for the Van Cliburn International Piano Competition), and "Proclamation'" (1982, his last work, started in 1973).
Film composer.
By the 1930s, Hollywood began to beckon "serious" composers with promises of better films and higher pay. The reality, however, was that few found good projects. Copland sought to enter that arena, as both a challenge for his abilities as a composer and an opportunity to expand his reputation and audience for his more serious works. Unlike the total attention he would hope to get from a concert-goer, Copland wrote that film music had to achieve a balance. It should be "secondary in importance to the story being told on the screen" while notably adding to the dramatic and emotional content of the film—but without diverting the viewer's attention from the action.
Upon arriving in Hollywood in 1937, he had high hopes: "It is just a matter of finding a feature film that needs my kind of music." What he found, however, was the ongoing tendency of studios to edit and cut movie scores, which often subverted a composer's intentions. No projects seemed suitable at first. But his patience paid off two years later when Copland found a kindred spirit in director Lewis Milestone, who allowed Copland to supervise his own orchestration and who refrained from interfering with his work. Copland composed three of his five film scores for Milestone.
This collaboration resulted in the notable film "Of Mice and Men" (1939), from the novel by John Steinbeck, that earned Copland his first nomination for an Academy Award ( he actually received two nominations, one for "best score" and another for "original score"). He considered himself lucky with his first film score: "Here was an American theme, by a great American writer, demanding appropriate music." Having accepted small sums for other projects in the past, especially to help out cash-strapped productions involving friends, this time Copland would capitalize on his efforts: "I thought if I was to sell myself to the movies, I ought to sell myself good." From then on, he became one of Hollywood's highest paid film composers, earning as much as $15,000 per film.
In a departure from other film scores of the time, Copland's work largely reflected his own style, instead of the usual borrowing from the late-Romantic period. Many silent and early talking films used classical music themes directly, both in the credit sequences and during the action. But with Copland, the film score's purpose was more comprehensive and subtle, setting the atmosphere of time and place, illustrating the thoughts of the actors, providing continuity and filler, and shaping the emotion and drama. He often avoided the full orchestra, and he rejected the common practice of using a leitmotiv to identify characters with their own personal themes. He instead matched a theme to the action, while avoiding the underlining of every action with exaggerated emphasis.
Another technique Copland employed was to keep silent during intimate screen moments and only begin the music as a confirming motive toward the end of a scene. Virgil Thompson wrote that the score for "Of Mice and Men" established "the most distinguished populist musical style yet created in America." Many composers who scored for western movies, particularly between 1940 and 1960, were influenced by Copland's style, though some also followed the "Max Steiner" approach, which was more bombastic and obvious. As a commentator on film scores, Copland singled out Bernard Herrmann, Miklós Rózsa, Alex North and Erich Wolfgang Korngold as innovative leaders in the field.
Copland's score for "The North Star" (1943) was nominated for an Academy Award, and his score for William Wyler's 1949 film, "The Heiress" won the award. Several themes from his scores are incorporated in the suite "Music for Movies." His score for the film adaptation of John Steinbeck's novel "The Red Pony" was arranged by commission of the Houston Symphony Orchestra as a suite for their performance in October 1948 and became widely popular. His score for the 1961 independent film "Something Wild" was released in 1964 as "Music For a Great City". Copland also composed scores for two documentary films, "The City" (1939) and "The Cummington Story" (1945).
When commenting on the effectiveness of film scores, Copland said: "I'd love to be able to have audiences see a film with the music, then see it a second time with the music turned off, and then see it a third time with the music turned on. Then, I think they'd get a much more specific idea of what the music does for a film.".
Critic, writer, and teacher.
Copland had a large following of pupils—often mixing his personal life with them. Of notable students, Leonard Bernstein and Victor Kraft were two with whom he continued having intimately personal relationships. Bernstein would go on to champion Copland as one of the greatest American composers of all time while being one of the few people Copland opened up to.
Copland also wrote prolifically on the subject of music. Across decades, Copland has published pieces on music criticism analysis on musical trends, and on his own compositions. Starting with his first critiques in 1924, Copland began a long career as music critic, teacher, and observer, mostly of contemporary classical music. He was an avid lecturer and lecturer-performer. He wrote reviews of specific works, trends, composers, festivals, books about music, and recordings. He took on a wide range of issues from the most general ("Creativity") to the most practical ("Composer Economics"). Copland also wrote three books, "What to Listen for in Music (1939)", "Our New Music (1941)", and "Music and Imagination" (1952). He had a long list of notable students (see below). Copland put a good deal of time and energy into supporting young musicians, especially through his association with the Berkshire Music Center at Tanglewood, both as a guest conductor and teacher. In working with young composers, Copland thought it more important to focus on expressive content than on technical points.
Conductor.
Copland studied conducting in Paris in 1921, but not until his involvement conducting his own Hollywood scores, did he undertake it except out of necessity. On his international travels in the 1940s, however, he began to make appearances as a guest conductor, performing his own works. By the 1950s, he was conducting the works of other composers as well. From the 1960s on, he conducted far more than he composed.
A self-taught conductor, Copland developed a very personal style. He occasionally asked friend Leonard Bernstein for advice. Copland took an understated and unpretentious approach to conducting and modeled his style after other composer/conductors such as Stravinsky and Hindemith. Observers of Copland noted that he had "none of the typical conductorial vanities". Though his friendly and modest persona, and his great enthusiasm, were appreciated by professional orchestra musicians, some criticized his beat as "unsteady" and his interpretations as "unexciting". Some of his peers, like Koussevitzky, went even further, advising him to "stay home and compose". Copland thoroughly enjoyed conducting but admitted that he did it in part because in the last seventeen years of his life he felt little inspiration to compose. He was offered "permanent" conducting posts but preferred to operate as a guest conductor. Nearly all of Copland's conducting appearances included his own works, which added to the intoxication of conducting. As he stated, "Conducting puts one in a very powerful position ... Best of all, it is a use of power for a good purpose." It also allowed him the freedom to travel which he always enjoyed.
Copland was a strong advocate for newer music and composers, and his programs always included heavy representation of 20th-century music and lesser-known composers. Performers and audiences generally greeted his conducting appearances as positive opportunities to hear his music as the composer intended, but sometimes found his efforts with other composers to be lacking. From Copland's point of view, he found both the New York Philharmonic and the Boston Symphony Orchestra to be "tough" groups, resistant to newer music. Newton Mansfield, violinist with the New York Philharmonic, stated, "The orchestra didn't take him too seriously. It was like going out to a nice lunch." Copland also found resistance from European orchestras; however, he was warmly received and respected in England. Copland recorded nearly all his orchestral works with himself conducting.

</doc>
<doc id="51299" url="https://en.wikipedia.org/wiki?curid=51299" title="Jammu and Kashmir">
Jammu and Kashmir

Jammu and Kashmir () is a state in northern India, often denoted by the acronym J&K. It is located mostly in the Himalayan mountains, and shares borders with the states of Himachal Pradesh and Punjab to the south. Jammu and Kashmir has an international border with China in the north and east, and the Line of Control separates it from the Pakistan-controlled territories of Azad Kashmir and Gilgit-Baltistan in the west and northwest respectively. The state has special autonomy under Article 370 of the Constitution of India.
A part of the erstwhile Princely State of Kashmir and Jammu, the region is the subject of a territorial conflict among China, India and Pakistan. The western districts of the former princely state known as Azad Kashmir and the northern territories known as Gilgit-Baltistan have been under Pakistani control since 1947. The Aksai Chin region in the east, bordering Tibet, has been under Chinese control since 1962.
Jammu and Kashmir consists of three regions: Jammu, the Kashmir Valley and Ladakh. Srinagar is the summer capital, and Jammu is the winter capital. The Kashmir valley is famous for its beautiful mountainous landscape, and Jammu's numerous shrines attract tens of thousands of Hindu pilgrims every year. Ladakh, also known as "Little Tibet", is renowned for its remote mountain beauty and Buddhist culture.
Jammu and Kashmir is the only state in India with a Muslim-majority population.
History.
Accession.
Maharaja Hari Singh became the ruler of the princely state of Jammu and Kashmir in 1925, and he was the reigning monarch at the conclusion of the British rule in the subcontinent in 1947. With the impending independence of India, the British announced that the British Paramountcy over the princely states would end, and the states were free to choose between the new Dominions of India and Pakistan or to remain independent. It was emphasized that independence was only a `theoretical possibility' because, during the long rule of the British in India, the states had come to depend on British Indian government for a variety of their needs including their internal and external security.
Jammu and Kashmir had a Muslim majority (77% Muslim by the 1941 census). Following the logic of Partition, many in Pakistan had expectations that Kashmir would join Pakistan. However, the predominant political movement in the Valley of Kashmir (Jammu and Kashmir National Conference) was secular, and was allied with the Indian National Congress since the 1930s. So many in India too had expectations that Kashmir would join India. The Maharaja was faced with indecision.
On 22 October 1947, rebellious citizens from the western districts of the State and Pushtoon tribesmen from the Northwest Frontier Province of Pakistan invaded the State, backed by Pakistan. The Maharaja initially fought back but appealed for assistance to the Governor-General Louis Mountbatten, who agreed on the condition that the ruler accede to India. Maharaja Hari Singh signed the Instrument of Accession on 26 October 1947 in return for military aid and assistance, which was accepted by the Governor General the next day. While the Government of India accepted the accession, it added the proviso that it would be submitted to a "reference to the people" after the state is cleared of the invaders, since "only the people, not the Maharaja, could decide where Kashmiris wanted to live." It was a provisional accession.
Once the Instrument of Accession was signed, Indian soldiers entered Kashmir with orders to evict the raiders. The resulting Indo-Pakistani War of 1947 lasted till the end of 1948. At the beginning of 1948, India took the matter to the United Nations Security Council. The Security Council passed a resolution asking Pakistan to withdraw its forces as well as the Pakistani nationals from the territory of Jammu and Kashmir, and India to withdraw the majority of its forces leaving only a sufficient number to maintain law and order, following which a Plebiscite would be held. A ceasefire was agreed on 1 January 1949, supervised by UN observers.
A special United Nations Commission for India and Pakistan (UNCIP) was set up to negotiate the withdrawal arrangements as per the Security Council resolution. The UNCIP made three visits to the subcontinent between 1948 and 1949, trying to find a solution agreeable to both India and Pakistan. It passed a resolution in August 1948 proposing a three-part process. It was accepted by India but effectively rejected by Pakistan.
In the end, no withdrawal was ever carried out, India insisting that Pakistan had to withdraw first, and Pakistan contending that there was no guarantee that India would withdraw afterwards. No agreement could be reached between the two countries on the process of demilitarisation.
India and Pakistan fought two further wars in 1965 and 1971. Following the latter war, the countries reached the Simla Agreement, agreeing on a Line of Control between their respective regions and committing to a peaceful resolution of the dispute through bilateral negotiations.
Debate over accession.
The primary argument for the continuing debate over the ownership of Kashmir is that India did not hold the promised plebiscite. In fact, neither side has adhered to the U.N. resolution of 13 August 1948; while India chose not to hold the plebiscite, Pakistan failed to withdraw its troops from Kashmir as was required under the resolution. 
India gives the following reasons for not holding the plebiscite:
In response Pakistan holds that: 
Diplomatic relations between India and Pakistan soured for many other reasons and eventually resulted in three further wars in Kashmir the Indo-Pakistani War of 1965, the Indo-Pakistan War of 1971 and the Kargil War in 1999. India has control of 60% of the area of the former Princely State of Jammu and Kashmir (Jammu, Kashmir Valley, Ladakh and Siachen Glacier); Pakistan controls 30% of the region (Gilgit–Baltistan and Azad Kashmir). China administers 10% (Aksai Chin and Trans-Karakoram Tract) of the state since 1962.
The Chenab formula was a compromise proposed in the 1960s, in which the Kashmir valley and other Muslim-dominated areas north of the Chenab river would go to Pakistan, and Jammu and other Hindu-dominated regions would go to India.
The eastern region of the erstwhile princely state of Kashmir has also been beset with a boundary dispute. In the late 19th- and early 20th centuries, although some boundary agreements were signed between Great Britain, Tibet, Afghanistan and Russia over the northern borders of Kashmir, China never accepted these agreements, and the official Chinese position did not change with the communist revolution in 1949. By the mid-1950s the Chinese army had entered the northeast portion of Ladakh.
By 1956–57 they had completed a military road through the Aksai Chin area to provide better communication between Xinjiang and western Tibet. India's belated discovery of this road led to border clashes between the two countries that culminated in the Sino-Indian war of October 1962. China has occupied Aksai Chin since 1962 and, in addition, an adjoining region, the Trans-Karakoram Tract was ceded by Pakistan to China in 1963.
For intermittent periods between 1957, when the state approved its own Constitution, and the death of Sheikh Abdullah in 1982, the state had alternating spells of stability and discontent. In the late 1980s, however, simmering discontent over the high-handed policies of the Union Government and allegations of the rigging of the 1987 assembly elections triggered a violent uprising which was backed by Pakistan.
Since then, the region has seen a prolonged, bloody conflict between separatists and the Indian Army, both of whom have been accused of widespread human rights abuses, including abductions, massacres, rapes and armed robbery. The army has officially denied these allegations.
However, violence in the state has been on the decline since 2004 with the peace process between India and Pakistan. The situation has become increasingly tense politically in recent years.
Geography and climate.
Jammu and Kashmir is home to several valleys such as the Kashmir Valley, Tawi Valley, Chenab Valley, Poonch Valley, Sind Valley and Lidder Valley. The main Kashmir valley is wide and in area. The Himalayas divide the Kashmir valley from Ladakh while the Pir Panjal range, which encloses the valley from the west and the south, separates it from the Great Plains of northern India. Along the northeastern flank of the Valley runs the main range of the Himalayas. This densely settled and beautiful valley has an average height of above sea-level but the surrounding Pir Panjal range has an average elevation of .
Because of Jammu and Kashmir's wide range of elevations, its biogeography is diverse. Northwestern thorn scrub forests and Himalayan subtropical pine forests are found in the low elevations of the far southwest. These give way to a broad band of western Himalayan broadleaf forests running from northwest-southeast across the Kashmir Valley. Rising into the mountains, the broadleaf forests grade into western Himalayan subalpine conifer forests. Above the tree line are found northwestern Himalayan alpine shrub and meadows. Much of the northeast of the state is covered by the Karakoram-West Tibetan Plateau alpine steppe. Around the highest elevations, there is no vegetation, simply rock and ice.
The Jhelum River is the only major Himalayan river which flows through the Kashmir valley. The Indus, Tawi, Ravi and Chenab are the major rivers flowing through the state. Jammu and Kashmir is home to several Himalayan glaciers. With an average altitude of above sea-level, the Siachen Glacier is long making it the longest Himalayan glacier.
The climate of Jammu and Kashmir varies greatly owing to its rugged topography. In the south around Jammu, the climate is typically monsoonal, though the region is sufficiently far west to average 40 to 50 mm (1.6 to 2 inches) of rain per month between January and March. In the hot season, Jammu city is very hot and can reach up to 40 °C (104 °F) whilst in July and August, very heavy though erratic rainfall occurs with monthly extremes of up to 650 millimetres (25.5 inches). In September, rainfall declines, and by October conditions are hot but extremely dry, with minimal rainfall and temperatures of around 29 °C (84 °F).
Across from the Pir Panjal range, the South Asian monsoon is no longer a factor and most precipitation falls in the spring from southwest cloudbands. Because of its closeness to the Arabian Sea, Srinagar receives as much as of rain from this source, with the wettest months being March to May with around 85 millimetres (3.3 inches) per month. Across from the main Himalaya Range, even the southwest cloudbands break up and the climate of Ladakh and Zanskar is extremely dry and cold. Annual precipitation is only around 100 mm (4 inches) per year and humidity is very low. In this region, almost all above 3,000 metres (9,750 ft) above sea level, winters are extremely cold. In Zanskar, the average January temperature is −20 °C (−4 °F) with extremes as low as −40 °C (−40 °F). All the rivers freeze over and locals make river crossings during this period because their high levels from glacier melt in summer inhibits crossing. In summer in Ladakh and Zanskar, days are typically a warm 20 °C (68 °F), but with the low humidity and thin air nights can still be cold.
Administrative divisions.
Jammu and Kashmir consists of three divisions: Jammu, Kashmir Valley and Ladakh, and is further divided into 22 districts. The Siachen Glacier, although under Indian military control, does not lie under the administration of the state of Jammu and Kashmir. Kishtwar, Ramban, Reasi, Samba, Bandipora, Ganderbal, Kulgam and Shopian are newly formed districts, and their areas are included with those of the districts from which they were formed.
Major cities.
Municipal corporations: 2 – Srinagar, Jammu
Municipal councils: 6 – Udhampur, Kathua, Poonch, Anantnag, Baramulla, Sopore
Municipal boards: 21 – Samba, Ranbirsinghpora, Akhnoor, Reasi, Ramban, Doda, Bhaderwah, Kishtwar, Kargil, Dooru-Verinag, Bijbehara, Pulwama, Tral, Badgam, Kulgam, Shopian, Ganderbal, Pattan, Sumbal, Kupwara, Handwara
Population of ten major cities:
Demographics.
The major ethnic groups living in Jammu & Kashmir include Kashmiris, Gujjars/Bakarwals, Paharis, Dogras and Ladakhis. The Kashmiris live mostly in the main valley of Kashmir and Chenab valley of Jammu division with a minority living in the Pir Panjal region. The Pahari-speaking people mostly live in and around the Pir Panjal region with some in the northern Kashmir valley. The nomadic Gujjars and Bakerwals practice transhumance and mostly live in the Pirpanjal region. The Dogras are ethnically, linguistically and culturally related to the neighboring Punjabi people and mostly live in the Udhampur and Jammu districts of the state. The Ladakhis are people of Mongoloid stock and resemble in their ethnic character to the neighboring Tibetan people. 
Jammu and Kashmir is the only Indian state with a Muslim majority population. According to the 2011 census, Islam is practiced by about 68.3% of the state population, while 28.4% follow Hinduism and small minorities follow Sikhism (1.9%), Buddhism (0.9%) and Christianity (0.3%). About 96.4% of the population of the Kashmir valley are Muslim followed by Hindus (2.45%) and Sikhs (0.98%) and others (0.17%) Shias live in the district of Badgam, with a majority population, and has been peaceful and has resisted separatism. Shia population is 15 lakhs of Indian state of Jammu and Kashmir, which is 14% of entire state population.
In Jammu, Hindus constitute 62.55% of the population, Muslims 33.45% and Sikhs, 3.3%; In Ladakh (comprises Buddhists-dominated Leh and Muslim-dominated Kargil), Muslims constitute about 46.4% of the population, the remaining being Buddhists (39.7%) and Hindus (12.1%). The people of Ladakh are of Indo-Tibetan origin, while the southern area of Jammu includes many communities tracing their ancestry to the nearby Indian states of Haryana and Punjab, as well as the city of Delhi.
Buddhists, Hindus, Sikhs and a few Christian, Jain, and Zoroastrian communities were once natives and made up a vast majority of the whole Kashmir province, as well as neighbouring states, and ancient and modern northern half of what is today India and Pakistan, but because of economic changes, political tension, military involvement, and foreign extremists resulted in vast majority of the followers of these religions to settle in the growing and advancing neighbouring regions and major cities in India over the years, often during no present borders or records. Hindu pandits were specifically affected in this region due to their status in the local society.
According to political scientist Alexander Evans, approximately 99% of the total population of 160,000–170,000 of Kashmiri Brahmins, also called Kashmiri Pandits, ("i.e." approximately 150,000 to 160,000) left the Kashmir Valley in 1990 as militancy engulfed the state. According to an estimate by the Central Intelligence Agency, about 300,000 Kashmiri Pandits from the entire state of Jammu and Kashmir have been internally displaced due to the ongoing violence.
In Jammu and Kashmir, the principal spoken languages are Kashmiri, Urdu, Dogri, Punjabi, Pahari, Balti, Ladakhi, Gojri, Shina and Pashto. However, Urdu written in the Persian script is the official language of the state. Hindustani is widely understood by peoples. Many speakers of these languages use Urdu or English as a second language.
The Kashmir Valley is dominated by ethnic Kashmiris, who have largely driven the Azadi campaign. Non-Kashmiri Muslim ethnic groups (Paharis, Gujjars and Bakarwalas), who dominate areas along the Line of Control, have remained indifferent to the separatist campaign. Jammu province region has a 70:30 Hindu-Muslim ratio. Parts of the region were hit by militants, but violence has ebbed there, along with the Valley, after India and Pakistan started a peace process in 2004.
Dogras (67%) are the single largest group in the multi-ethnic region of Jammu living with Punjabis, Kashmiris, Paharis, Bakerwals and Gujjars. Statehood is demanded in Hindu-dominated districts. Ladakh is the largest region in the state with over 200,000 people. Its two districts are Leh (68% Buddhist) and Kargil (91% Muslim population). Union territory status has been the key demand of Leh Buddhists for many years.
Politics and government.
Jammu and Kashmir is the only state in India which enjoys special autonomy under Article 370 of the Constitution of India, according to which no law enacted by the Parliament of India, except for those in the field of defence, communication and foreign policy, will be extendable in Jammu and Kashmir unless it is ratified by the state legislature of Jammu and Kashmir. Subsequently, jurisdiction of the Supreme Court of India over Jammu and Kashmir has been extended.
Jammu and Kashmir is the only Indian state to have its own official state flag along with national flag and constitution. Indians from other states cannot purchase land or property in the state. Designed by the then ruling National Conference, the flag of Jammu and Kashmir features a plough on a red background symbolising labour; it replaced the Maharaja's state flag. The three stripes represent the three distinct administrative divisions of the state, namely Jammu, Valley of Kashmir, and Ladakh.
In 1990, an Armed Forces Act, which gives special powers to the Indian security forces, has been enforced in Jammu and Kashmir. The decision to invoke this act was criticised by the Human Rights Watch.
Like all the states of India, Jammu and Kashmir has a multi-party democratic system of governance with a bicameral legislature. At the time of drafting the Constitution of Jammu and Kashmir, 100 seats were earmarked for direct elections from territorial constituencies. Of these, 25 seats were reserved for the areas of Jammu and Kashmir State that came under Pakistani occupation; this was reduced to 24 after the 12th amendment of the Constitution of Jammu and Kashmir:
After a delimitation in 1988, the total number of seats increased to 111, of which 87 were within Indian-administered territory. The Jammu & Kashmir Assembly is the only state in India to have a 6-year term, in contrast to the norm of a 5-year term followed in every other state's Assembly. There was indication from the previous INC Government to bring parity with the other states, but this does not seem to have received the required support to pass into law.
Influential political parties include the Jammu & Kashmir National Conference (NC), the Indian National Congress (INC), the Jammu and Kashmir People's Democratic Party (PDP), the Bharatiya Janata Party (BJP) and other smaller regional parties. After dominating Kashmir's politics for years, the National Conference's influence waned in 2002, when INC and PDP formed a political alliance and rose to power. Under the power-sharing agreement, INC leader Ghulam Nabi Azad replaced PDP's Mufti Mohammad Sayeed as the Chief Minister of Jammu and Kashmir in late 2005. However, in 2008, PDP withdrew its support from the government on the issue of temporary diversion of nearly of land to the Sri Amarnath Shrine Board. In the 2008 Kashmir Elections that were held from 17 November to 24 December, the National Conference party and the Congress party together won enough seats in the state assembly to form a ruling alliance. In the 2014 election, the voter turnout was recorded at 65% - the highest in the history of the state. The results gave a fractured mandate to either parties — the PDP won 28 seats, BJP 25, NC 15 and INC 12. After 2 months of deliberations and president's rule, the BJP and the PDP announced an agreement for a coalition government, and PDP patron Mufti Mohammad Sayeed was sworn-in as CM for a second term, with Nirmal Singh of the BJP sworn-in as deputy CM. This also marked the first time in 35 years that the BJP was a coalition partner in the state government.
Some Kashmiris, especially those residing in the Kashmir Valley, demand greater autonomy, sovereignty and even independence from India. Due to the economic integration of Jammu and Kashmir with the rest of India, separatist movements across the Kashmir Valley declined. However, following the unrest in 2008, which included more than 500,000 protesters at a rally on 18 August, secessionist movements gained a boost.
The 2009 edition of the Freedom in the World (report) by the U.S.-based NGO Freedom House rated Jammu and Kashmir as "Partly Free", while in comparison, the same report rated Pakistan-administered Kashmir as "Not Free" (both reports available on UNHCR refworld).
Economy.
Jammu and Kashmir's economy is predominantly dependent on agriculture and allied activities. The Kashmir valley is known for its sericulture and cold-water fisheries. Wood from Kashmir is used to make high-quality cricket bats, popularly known as "Kashmir Willow". Kashmiri saffron is very famous and brings the state a handsome amount of foreign exchange. Agricultural exports from Jammu and Kashmir include apples, barley, cherries, corn, millet, oranges, rice, peaches, pears, saffron, sorghum, vegetables, and wheat, while manufactured exports include handicrafts, rugs, and shawls.
Horticulture plays a vital role in the economic development of the state. With an annual turnover of over , apart from foreign exchange of over , this sector is the next biggest source of income in the state's economy. The region of Kashmir is known for its horticulture industry and is the wealthiest region in the state. Horticultural produce from the state includes apples, apricots, cherries, pears, plums, almonds and walnuts.
The Doda district has deposits of high-grade sapphire. Though small, the manufacturing and services sector is growing rapidly, especially in the Jammu division. In recent years, several consumer goods companies have opened manufacturing units in the region. The Associated Chambers of Commerce and Industry of India (ASSOCHAM) has identified several industrial sectors which can attract investment in the state, and accordingly, it is working with the union and the state government to set up industrial parks and special economic zones. In the fiscal year 2005–06, exports from the state amounted to . However, industrial development in the state faces several major constraints including extreme mountainous landscape and power shortage. The Jammu & Kashmir Bank, which is listed as a S&P CNX 500 conglomerate, is based in the state. It reported a net profit of in 2008.
The Government of India has been keen to economically integrate Jammu and Kashmir with the rest of India. The state is one of the largest recipients of grants from New Delhi, totalling US$812 million per year. It has a mere 4% incidence of poverty, one of the lowest in the country.
In an attempt to improve the infrastructure in the state, Indian Railways is constructing the ambitious Kashmir Railway project at a cost of more than US$2.5 billion. Trains run on the 130 km Baramula-Banihal section. The 17.5 km Qazigund-Banihal section through the 11 km long Pir Panjal Railway Tunnel was commissioned. Udhampur-Katra section of the track was commissioned early in July 2014. The Katra-Banihal section is under construction. The route crosses major earthquake zones, and is subjected to extreme temperatures of cold and heat, as well as inhospitable terrain, making it an extremely challenging engineering project. It is expected to increase tourism and travel to Kashmir. Three other railway lines, the Bilaspur–Mandi–Leh railway, Srinagar-Kargil-Leh railway and the Jammu-Poonch railway have been proposed.
Tourism.
Before the insurgency intensified in 1989, tourism formed an important part of the Kashmiri economy. The tourism economy in the Kashmir valley was worst hit. However, the holy shrines of Jammu and the Buddhist monasteries of Ladakh continue to remain popular pilgrimage and tourism destinations. Every year, thousands of Hindu pilgrims visit holy shrines of Vaishno Devi and Amarnath, which has had significant impact on the state's economy. It was estimated in 2007 that the Vaishno Devi yatra contributed to the local economy annually a few years ago. The contribution should be significantly greater now as the numbers of Indian visitors have increased considerably. Foreign tourists have been slower to return. The British government still advises against all travel to Jammu and Kashmir with the exception of the cities of Jammu and Srinagar, travel between these two cities on the Jammu-Srinagar highway, and the region of Ladakh, while Canada excludes the entire region excepting Leh.
Besides Kashmir, several areas in the Jammu region have a lot of tourist potential as well. Bhau Fort in Jammu city is the major attraction for the tourists visiting that city. Bage-e-Bahu is another tourist destination. The local aquarium, established by the fisheries department, is visited by many. Tourists from across India visit Jammu in a pilgrimage to Mata Vaishno Devi. Mata Vaishno Devi is located in the Trikuta Hills, about 40 to 45 km from Jammu City. Approximately 10 million Pilgrims visit this holy place every year.
Tourism in the Kashmir valley has rebounded in recent years, and in 2009, the state became one of the top tourist destinations of India. Gulmarg, one of the most popular ski resort destinations in India, is also home to the world's highest green golf course. The state's recent decrease in violence has boosted the economy and tourism. It was reported that more than a million tourists visited Kashmir in 2011.
Culture.
Ladakh is famous for its unique Indo-Tibetan culture. Chanting in Sanskrit and Tibetan language forms an integral part of Ladakh's Buddhist lifestyle. Annual masked dance festivals, weaving and archery are an important part of traditional life in Ladakh. Ladakhi food has much in common with Tibetan food, the most prominent foods being thukpa, noodle soup; and tsampa, known in Ladakhi as "Ngampe", roasted barley flour. Typical garb includes gonchas of velvet, elaborately embroidered waistcoats and boots, and gonads or hats. People adorned with gold and silver ornaments and turquoise headgears throng the streets during Ladakhi festivals.
The "Dumhal" is a famous dance in the Kashmir Valley, performed by men of the Wattal region. The women perform the Rouff, another traditional folk dance. Kashmir has been noted for its fine arts for centuries, including poetry and handicrafts. "Shikaras", traditional small wooden boats, and houseboats are a common feature in lakes and rivers across the Valley.
The Constitution of India does not allow people from regions other than Jammu and Kashmir to purchase land in the state. As a consequence, houseboats became popular among those who were unable to purchase land in the Valley and has now become an integral part of the Kashmiri lifestyle.
"Kawa", traditional green tea with spices and almond, is consumed all through the day in the chilly winter climate of Kashmir. Most of the buildings in the Valley and Ladakh are made from softwood and are influenced by Indian, Tibetan, and Islamic architecture.
Jammu's Dogra culture and tradition is very similar to that of neighbouring Punjab and Himachal Pradesh. Traditional Punjabi festivals such as Lohri and Vaisakhi are celebrated with great zeal and enthusiasm throughout the region, along with Accession Day, an annual holiday which commemorates the accession of Jammu & Kashmir to the Dominion of India. After "Dogras", "Gujjars" form the second-largest ethnic group in Jammu. Known for their semi-nomadic lifestyle, Gujjars are also found in large numbers in the Kashmir Valley. Similar to Gujjars, "Gaddis" are primarily herdsmen who hail from the Chamba region in Himachal Pradesh. Gaddis are generally associated with emotive music played on the flute. The "Bakkarwala"s found both in Jammu and the Kashmir valley are wholly nomadic pastoral people who move along the Himalayan slopes in search for pastures for their huge flocks of goats and sheep.
Education.
In 1970, the state government of Jammu and Kashmir established its own education board and university. Education in the state is divided into primary, middle, high secondary, college and university level. Jammu and Kashmir follows the 10+2 pattern for education of children. This is handled by Jammu and Kashmir State Board of School Education (abbreviated as JKBOSE). Private and public schools are recognised by the board to impart education to students. Board examinations are conducted for students in class VIII, X and XII. In addition, there are "Kendriya Vidyalayas" (run by the Government of India) and Indian Army schools that impart secondary school education. These schools follow the Central Board of Secondary Education pattern.
Notable higher education or research institutes in Jammu and Kashmir include the National Institute of Technology, Srinagar, Sher-i-Kashmir Institute of Medical Sciences, Srinagar, Government College of Engineering and Technology, Jammu, Government Medical College, Srinagar and Government Medical College, Jammu. University-level education is provided by University of Kashmir, University of Jammu, Sher-e-Kashmir University of Agricultural Sciences and Technology of Srinagar, Sher-e-Kashmir University of Agricultural Sciences and Technology of Jammu, Islamic University of Science & Technology, Baba Ghulam Shah Badhshah University, Shri Mata Vaishno Devi University, Institution of Technicians and Engineers (Kashmir), Government Degree College for Boys Anantnag, Central University of Kashmir located at Ganderbal and Central University of Jammu located at Raya Suchani in the Samba district of Jammu.
Sports.
Sports like cricket, football are famous along with sports like golf, skiing, water sports and adventure sports. Srinagar is home to the Sher-i-Kashmir Stadium, a stadium where international cricket matches have been played.
The first international match was played in 1983 in which West Indies defeated India and the last international match was played in 1986 in which Australia defeated India by six wickets. Since then no international match have taken place in the stadium due to the prevailing security situation.
Maulana Azad Stadium is a stadium in Jammu and is one of the home venues for the Jammu and Kashmir cricket team. Stadium has hosted home games for Jammu and Kashmir in domestic tournaments since 1966. It has also hosted one One Day International in 1988 between India and New Zealand, which was abandoned due to rain without a ball being bowled. The stadium has played host to one women's test match where India lost to West Indies and one Women's One Day International where India beat New Zealand in 1985.
Srinagar has an outdoor stadium namely Bakshi Stadium for hosting football matches. It is named after Bakshi Ghulam Mohammad.
The city has a golf course named Royal Springs Golf Course, Srinagar located on the banks of Dal lake, which is considered as one of the best golf courses of India.
Ladakh Marathon is held at Leh, is the marathon recognised by Association of International Marathons and Distance Races. Being held at height of 11,500 feet, it is known as the highest marathon in the world. In 2015, Ladakh Marathon was rated among "top ten nicest marathon" in the world.

</doc>
<doc id="51301" url="https://en.wikipedia.org/wiki?curid=51301" title="Pope Pontian">
Pope Pontian

Pope St. Pontian (; died October 235), was the Bishop of Rome from 21 July 230 to 28 September 235. In 235, during the persecution of Christians in the reign of the Emperor Maximinus the Thracian, Pontian was arrested and sent to the island of Sardinia. He resigned to make the election of a new pope possible.
Biography.
A little more is known of Pontian than his predecessors, apparently from a lost papal chronicle that was available to the compiler of the "Liberian Catalogue" of Bishops of Rome, written in the 4th century.
Pontian's pontificate was relatively peaceful under the reign of the Emperor Severus Alexander, and noted for the condemnation of Origen by a Roman synod, over which Pontian likely presided. According to early church historian Eusebius of Caesarea, the next emperor, Maximinus, overturned his predecessor's policy of tolerance towards Christianity. Both Pope Pontian and the Antipope Hippolytus of Rome were arrested and exiled to labor in the mines of Sardinia, generally regarded as a death sentence.
In light of his sentence, Pontian resigned as bishop on 28 September 235, so as to allow an orderly transition in the Church of Rome. This action ended a schism that had existed in the Roman Church for eighteen years. He was beaten to death with sticks. Neither Hippolytus nor Pontian survived, reconciling with one another there before their deaths. Pontian died in October 235.
Remembered.
Pope Fabian had the bodies of both Pontian and Hippolytus brought back to Rome in 236 or 237 and buried in the papal crypt in the Catacomb of Callixtus on the Appian Way. The slab covering his tomb was discovered in 1909. On it is inscribed in Greek: "Ποντιανός Επίσκ" ("Pontianus Episk"; in English "Pontianus Bish"). The inscription "MARTUR" had been added in another hand.
Pontian's feast day was previously celebrated on 19 November, but since 1969 both he and Hippolytus are commemorated jointly on 13 August.

</doc>
<doc id="51303" url="https://en.wikipedia.org/wiki?curid=51303" title="Cowpox">
Cowpox

Cowpox is an infectious disease caused by the cowpox virus. The virus, part of the orthopoxvirus family, is closely related to the "vaccinia" virus. The virus is zoonotic, meaning that it is transferable between species, such as from animal to human. The transferral of the disease was first observed in dairymaids who touched the udders of infected cows and consequently developed the signature pustules on their hands. Cowpox is more commonly found in animals other than bovines, such as rodents. Cowpox is similar to, but much milder than, the highly contagious and often deadly smallpox disease. Its close resemblance to the mild form of smallpox inspired the first smallpox vaccine, created and administered by English physician Edward Jenner.
The word “vaccination,” coined by Jenner in 1796, is derived from the Latin root "vaccinus", meaning of or from the cow. Once vaccinated, a patient develops antibodies that make him/her immune to cowpox, but they also develop immunity to the smallpox virus, or "Variola virus". The cowpox vaccinations and later incarnations proved so successful that in 1980, the World Health Organization announced that smallpox was the first disease to be eradicated by vaccination efforts worldwide. Other orthopox viruses remain prevalent in certain communities and continue to infect humans, such as the cowpox virus (CPXV) in Europe, vaccinia in Brazil, and monkeypox virus in Central and West Africa.
Origin.
Discovery.
In the years from 1770 to 1790, at least six people who had contact with a cow had independently tested the possibility of using the cowpox vaccine as an immunization for smallpox in humans. Amongst them were the English farmer Benjamin Jesty, in Dorset in 1774 and the German teacher Peter Plett in 1791. Jesty inoculated his wife and two young sons with cowpox, in a successful effort to immunize them to smallpox, an epidemic of which had arisen in their town. His patients who had contracted and recovered from the similar but milder cowpox (mainly milkmaids), seemed to be immune not only to further cases of cowpox, but also to smallpox. By scratching the fluid from cowpox lesions into the skin of healthy individuals, he was able to immunize those people against smallpox. Reportedly, farmers and people working regularly with cattle and horses were often spared during smallpox outbreaks. Investigations by the British Army in 1790 showed that horse-mounted troops were less infected by smallpox than infantry, due to probable exposure to the similar horse pox virus (Variola equina). By the early 19th century, more than 100,000 people in Great Britain had been vaccinated. The arm-to-arm method of transfer of the cowpox vaccine was also used to distribute Jenner's vaccine throughout the Spanish Empire. Spanish king Charles IV's daughter had been stricken with smallpox in 1798, and after she recovered, he arranged for the rest of his family to be vaccinated. In 1803, the king, convinced of the benefits of the vaccine, ordered his personal physician Francis Xavier de Balmis, to deliver it to the Spanish dominions in North and South America. To maintain the vaccine in an available state during the voyage, the physician recruited 22 young boys who had never had cowpox or smallpox before, aged three to nine years, from the orphanages of Spain. During the trip across the Atlantic, de Balmis vaccinated the orphans in a living chain. Two children were vaccinated immediately before departure, and when cowpox pustules had appeared on their arms, material from these lesions was used to vaccinate two more children.
Jesty did not publicize his findings, and Jenner, who performed his first inoculation 22 years later and publicized his findings, assumed credit. It is said that Jenner made this discovery by himself, possibly without knowing previous accounts 20 years earlier. Although Jesty may have been the first to discover it, Jenner made vaccination widely accessible and has therefore been credited for its invention.
Implementation.
Naturally occurring cases of cowpox were not common, but it was discovered that the vaccine could be “carried” in humans and reproduced and disseminated human-to-human. Jenner’s original vaccination used lymph from the cowpox pustule on a milkmaid, and subsequent “arm-to-arm” vaccinations applied the same principle. As this transfer of human fluids came with its own set of complications, a safer manner of producing the vaccine was first introduced in Italy, The new method used cows to manufacture the vaccine using a process called “retrovaccination,” in which a heifer was inoculated with humanized cowpox virus, and it was passed from calf to calf to produce massive quantities efficiently and safely. This then lead to the next incarnation, “true animal vaccine,” which used the same process but began with naturally-occurring cowpox virus, and not the humanized form.
This method of production proved to be lucrative and was taken advantage of by many entrepreneurs needing only calves and seed lymph from an infected cow to manufacture crude versions of the vaccine. W. F. Elgin of the National Vaccine Establishment presented his slightly refined technique to the Conference of State and Provincial Boards of Health of North America. A tuberculosis-free calf, stomach shaved, would be bound to an operating table, where incisions would be made on its lower body. Glycerinated lymph from a previously inoculated calf was spread along the cuts. After a few days, the cuts would have scabbed or crusted over. The crust was softened with sterilized water and mixed with glycerin, which disinfected it, then stored hermetically-sealed in capillary tubes for later use.
At some point, the virus in use was no longer cowpox, but vaccinia. Scientists have not determined exactly when the change or mutation occurred, but the effects of vaccinia and cowpox virus as vaccine are nearly the same.
The virus is found in Europe, and mainly in the UK. Human cases today are very rare and most often contracted from domestic cats. The virus is not commonly found in cattle; the reservoir hosts for the virus are woodland rodents, particularly voles. From these rodents, domestic cats contract the virus. Symptoms in cats include lesions on the face, neck, forelimbs, and paws, and less commonly upper respiratory tract infections. Symptoms of infection with cowpox virus in humans are localized, pustular lesions generally found on the hands and limited to the site of introduction. The incubation period is 9 to 10 days. The virus is prevalent in late summer and autumn.
Kinepox.
Kinepox is an alternate term for the smallpox vaccine used in early 19th-century America. Popularized by Jenner in the late 1790s, kinepox was a far safer method for inoculating people against smallpox than the previous method, variolation, which had a 3% fatality rate.
In a famous letter to Meriwether Lewis in 1803, Thomas Jefferson instructed the Lewis and Clark expedition to "carry with you some matter of the kine-pox; inform those of them with whom you may be, of its efficacy as a preservative from the smallpox; & encourage them in the use of it..." Jefferson had developed an interest in protecting Native Americans from smallpox, having been aware of epidemics along the Missouri River during the previous century. A year before his special instructions to Lewis, Jefferson had persuaded a visiting delegation of North American Indian Chieftains to be vaccinated with kinepox during the winter of 1801-2. Unfortunately, Lewis never got the opportunity to use kinepox during the pair's expedition, as it had become inadvertently inactive — a common occurrence in a time before vaccines were stabilized with preservatives such as glycerol or kept at refrigeration temperatures.
Historical use.
After inoculation, vaccination using the cowpox virus became the primary defense against smallpox. After infection by the cowpox virus, the body (usually) gains the ability to recognize the similar smallpox virus from its antigens and is able to fight the smallpox disease much more efficiently.
The cowpox virus contains 186 thousand base pairs of DNA, which contains the information for about 187 genes. This makes cowpox one of the most complicated viruses known. Some 100 of these genes give instructions for key parts of the human immune system, giving a clue as to why the closely related smallpox is so lethal. The vaccinia virus now used for smallpox vaccination is sufficiently different from the cowpox virus found in the wild as to be considered a separate virus.
Prevention.
Today, the virus is found in Europe, mainly in the UK. Human cases are very rare (though in 2010 a laboratory worker contracted cowpox.)
and most often contracted from domestic cats. Human infections usually remain localized and self-limiting, but can become fatal in immunosuppressed patients. The virus is not commonly found in cattle; the reservoir hosts for the virus are woodland rodents, particularly voles. Domestic cats contract the virus from these rodents. Symptoms in cats include lesions on the face, neck, forelimbs, and paws, and, less commonly, upper respiratory tract infections. Symptoms of infection with cowpox virus in humans are localized, pustular lesions generally found on the hands and limited to the site of introduction. The incubation period is 9 to 10 days. The virus is most prevalent in late summer and autumn.
Immunity to cowpox is gained when the smallpox vaccine is administered. Though the vaccine now uses vaccinia virus, the poxviruses are similar enough that the body becomes immune to both cow- and smallpox.

</doc>
<doc id="51304" url="https://en.wikipedia.org/wiki?curid=51304" title="White Rose">
White Rose

The White Rose () was a non-violent, intellectual resistance group in Nazi Germany led by a group of students and a professor at the University of Munich. The group conducted an anonymous leaflet and graffiti campaign which called for active opposition against the dictator Adolf Hitler's regime. Their activities started in June 1942 in Munich, and ended with their arrest by the Gestapo in February 1943. They faced trial by the Nazi "People's Court" under the infamous Roland Freisler, and were sentenced to death or imprisonment.
The group wrote, printed and initially distributed their leaflets in the greater Munich region. Later on, secret carriers distributed copies in other cities, mostly in the southern parts of the German Reich. In total, they authored six leaflets, which were distributed, in total, in about 15,000 copies. They branded the Nazi regime's crimes and oppression, and called for resistance. In their second leaflet, they openly denounced the Third Reich's persecution of the Jews. By the time of their arrest, members of the White Rose were just about to establish contacts with other German resistance groups like the Kreisau Circle or the Schulze-Boysen/Harnack group of the Red Orchestra. In today's Germany, the White Rose is the best-known example for the resistance of German students against the nazi regime, serving as an iconic role model for integrity, courage and readiness to make sacrifices for humanistic and democratic ideals from within a totalitarian dictatorship.
Historical background.
White Rose survivor Jürgen Wittenstein described what it was like to live in Hitler's Germany: 
The activities of the White Rose started at a time that was particularly critical for the German regime, as the German population became increasingly aware of the losses and damages of World War II. In Summer 1942, the German Wehrmacht was preparing a new military campaign in the southern part of the East front in order to regain the initiative after their earlier defeat close to Moscow. In February 1943, the advance of the German army had come to a halt at the Eastern front, and faced a major defeat in the Battle of Stalingrad. During this time, the authors of the leaflets could neither be discovered, nor could the campaign be stopped. When Hans and Sophie Scholl were discovered and arrested merely by chance, the regime reacted brutally.
Members and supporters.
Students from the University of Munich comprised the core of the White Rose: The siblings Hans Scholl and Sophie Scholl, Alexander Schmorell, Willi Graf, Christoph Probst, and Kurt Huber, a professor of philosophy and musicology.
They were supported by other persons, including Traute Lafrenz, Katharina Schüddekopf, Lieselotte (Lilo) Berndl, Jürgen Wittenstein, Marie-Luise Jahn, Falk Harnack, Hubert Furtwängler, Wilhelm Geyer, Manfred Eickemeyer, Josef Söhngen, Heinrich Guter, Heinrich Bollinger, Helmut Bauer, Harald Dohrn, Hans Conrad Leipelt, Gisela Schertling, Rudi Alt and Wolfgang Jaeger. Most were in their early twenties. Wilhelm Geyer taught Alexander Schmorell how to make the tin templates used in the graffiti campaign. Eugen Grimminger of Stuttgart funded their operations. Grimminger's secretary Tilly Hahn contributed her own funds to the cause, and acted as go-between for Grimminger and the group in Munich. She frequently carried supplies such as envelopes, paper, and an additional duplicating machine from Stuttgart to Munich. In addition, a group of students in the city of Ulm distributed a number of the group's leaflets. Among this group were Sophie Scholl's childhood friend and her teenage brother and .
Intellectual background and motivation.
Research into the intellectual background of the members of the White Rose has revealed various factors that may have influenced their thinking and the actions they have taken.
Social background.
The members of the core group all shared an academic background, being students at Munich university. The Scholl siblings, Christoph Probst, Willi Graf and Alexander Schmorell were all raised by liberal, independently thinking and wealthy parents. Some, but not all of them, in their early years had enthusiastically joined the youth organizations of the Nazi party: Hans Scholl had joined the Hitler Youth, Sophie Scholl was a member of the Bund Deutscher Mädel. Membership of both party youth organizations was compulsory for young Germans, although a few—such as Willi Graf, Otl Aicher, and Heinz Brenner—refused to join.
In a commemorative book about her siblings, Inge Scholl reported about the initial enthusiasm of the young people for the Nazi youth organization, to their parents' dismay:
Both the Scholl siblings became disenchanted with the Nazi organization only gradually, when they were subjected to increasing pressure into conformity, and realized that any signs of individualism were sanctioned.
German Youth Movement.
The ideas and thoughts of German Youth Movement, founded in 1896, had a major impact on the German youth at the beginning of the twentieth century. The movement aimed at providing free space to develop some healthy life. A common trait of the various organizations was a romantic longing for a pristine state of things, a return to older cultural traditions, with a strong emphasis on independent, non-conformist thinking. They propagated a return to nature, confraternity and shared adventures. The Deutsche Jungenschaft vom 1.11.1929 (abbreviated as "d.j.1.11.") was part of this youth movement, founded by Eberhard Koebel in 1929. Christoph Probst was a member of the German Youth Movement, Willi Graf was a member of "Neudeutschland", a Catholic youth association, and the "Grauer Orden". The Nazi party's youth organizations took over some of the elements of the youth movement, and engaged their members in activities similar to the adventures of Boy scouts – but at the same time demanded conformity to their ideology. Youth organizations other than those led by the Nazi party were dissolved in 1936, and both Hans Scholl and Willi Graf were arrested because of their membership in forbidden youth organizations in 1937–1938. Hans Scholl had joined the youth organization in 1934, when he and other Ulm Hitler Youth members considered membership in this group and the Hitler Youth to be compatible. Hans Scholl was also accused of transgressing Paragraph 175, the anti-homosexuality law, because of a same-sex teen relationship dating back to 1934-1935, when Hans was only 16 years old. The argument was built partially on the work of Eckard Holler, a sociologist specializing in the German Youth Movement, as well as on the Gestapo interrogation transcripts from the 1937-1938 arrest, and with reference to historian George Mosse's discussion of the homoerotic aspects of the German "bündische Jugend" Youth Movement. As Mosse indicated, idealized romantic attachments among male youths was not uncommon in Germany, especially among members of the "bündische Jugend" associations. It was argued that this experience led both Hans and Sophie to identify with the victims of the Nazi state, providing another explanation for why Hans and Sophie Scholl made the transformation from avid Hitler Youth leaders to passionate opponents of National Socialism.
Religion.
The group was motivated by ethical and moral considerations. They came from various religious backgrounds. Willi and Katharina were devout Catholics. Alexander Schmorell was Orthodox, the grandson of a priest and eventually glorified as an Orthodox Christian saint. Traute adhered to the concepts of anthroposophy, while Eugen Grimminger considered himself Buddhist. Christoph Probst was baptized a Catholic only shortly before his execution. His father Hermann was nominally a Catholic, but also a private scholar of Eastern thought and wisdom. Hans Scholl met Carl Muth, the founder of the catholic magazine Hochland. In his letters to Muth, he describes his growing attraction towards the Christian faith. In their diaries and letters to friends, both Scholl siblings write about their reading of Christian Scholars like Etienne Gilson, whose work on Medieval philosophy they discussed amongst other philosophical works within the network of friends, on which they relied when they started their resistance activities.
In 1941, Hans Scholl read a copy of a sermon by an outspoken critic of the Nazi regime, Bishop August von Galen, decrying the euthanasia policies expressed in Action T4 (and extended that same year to the Nazi concentration camps by Action 14f13) which the Nazis maintained would protect the German gene pool. Horrified by the Nazi policies, Sophie obtained permission to reprint the sermon and distribute it at the University of Munich as the group's first leaflet prior to their formal organization.
Experience at the World War II Eastern Front.
Hans Scholl, Alexander Schmorell, Christoph Probst, and Willi Graf were medical students. Their studies were regularly interrupted by terms of compulsory service as student soldiers in the Wehrmacht medical corps at the Eastern Front. Their experience during this time had a major impact on their thinking, and had motivated their resistance, as it led to disillusionment with the Nazi regime. Alexander Schmorell, who was born in Orenburg and raised by Russian nurses, spoke perfect Russian, which allowed him to have a direct contact and communication with the local Russian population and their plight. This Russian insight proved invaluable during their time there, and he could convey to his fellow White Rose members what was not understood or even heard by other Germans coming from the Eastern front.
In summer 1942, several members of the White Rose had to serve for three months on the Russian front alongside many other male medical students from the University of Munich. There, they observed the horrors of war, saw beatings and other mistreatment of Jews by the Germans, and heard about the persecution of the Jews from reliable sources. Some witnessed atrocities of the war on the battlefield and against civilian populations in the East. Willi Graf saw the Warsaw and Łódź Ghettos and could not get the images of brutality out of his mind.
By February 1943, the young friends sensed the implications of the reversal of fortune the Wehrmacht suffered at Stalingrad, which eventually led to Germany's defeat. As the brutality of the regime became more and more apparent, when deportations of Jews began, and the remaining few were forced to wear the yellow Star of David, when German atrocities in occupied Poland and Russia became known, and when the copies of Bishop Galen's sermon condemning the killing of inmates in insane asylums were circulated in secret, detachment gave way to the conviction something had to be done. It was not enough to keep to oneself one's beliefs, and ethical standards, but the time had come to act.
The members of the White Rose were fully aware of the risks they incurred by distributing their leaflets:
Origin of the name.
Under Gestapo interrogation, Hans Scholl gave several explanations for the origin of the name "The White Rose," and suggested he may have chosen it while he was under the emotional influence of a 19th-century poem with the same name by German poet Clemens Brentano. Earlier, before these Gestapo transcripts surfaced, Annette Dumbach and Jud Newborn speculated briefly that the origin might have come from a German novel "Die Weiße Rose" ("The White Rose"), published in Berlin in 1929 and written by B. Traven, the German author of "The Treasure of the Sierra Madre". Dumbach and Newborn said there was a chance that Hans Scholl and Alex Schmorell had read this. They also wrote that the symbol of the white rose was intended to represent purity and innocence in the face of evil.
In February 2006, however, Dr. Jud Newborn authored an essay entitled, "Solving Mysteries: The Secret of 'The White Rose'," originally intended as an afterword to his co-authored book. In this essay he argues that Hans Scholl's response to the Gestapo was intentionally misleading in order to protect Josef Söhngen, the anti-Nazi bookseller who had provided the White Rose members with a safe meeting place for the exchange of information and to receive occasional financial contributions. Söhngen kept a stash of banned books hidden in his store. Dr. Newborn also looked into the content of B. Traven's "The White Rose," arguing that the novel, banned by the Nazis in 1933, provided evidence of origin of the group's name.
Actions: The leaflets and graffiti.
After their experiences at the Eastern Front, having learned about mass murder in Poland and Russia, Hans Scholl and Alexander Schmorell felt compelled to take action. From end of June until mid of July 1942, they wrote four leaflets, which they sent by mail anonymously to intellectuals living in the greater Munich region. In Winter 1942, Willi Graf and Sophie Scholl joined the group. From 23 July to 30 October 1942, Graf, Scholl and Schmorell served again at the Russian front, and activities ceased until their return. The fifth leaflet, ""Aufruf an alle Deutsche!"" ("Appeal to all Germans!", in 6000–9000 copies) was distributed between 27 and 29 January 1943 in several south German and some Austrian cities. Sophie Scholl stated in her interrogation after her arrest on 18 February 1943 that from summer 1942 on, the aim of the White Rose was to address a broader range of the population. Thus, the fifth leaflet was written in a less intellectual and more popular style. The students had become convinced during their military service that the war was lost: ""Hitler kann den Krieg nicht gewinnen, nur noch verlängern." - Hitler cannot win the war, he can only prolong it." They appealed to renounce "national socialist subhumanism", imperialism and Prussian militarism "for all time". Their vision of future Germany was a federalist state in a unified Europe, after the war had ended.
By the end of January 1943, the Battle of Stalingrad ended with the capitulation and near-total loss of the Wehrmacht's Sixth Army. In Stalingrad, World War II had taken a decisive turn, inspiring resistance movements throughout European countries, then occupied by Germany. It also had a devastating effect on German morale. On 13 January 1943, a student riot broke out at Munich University, after the Nazi Gauleiter of Munich and Upper Bavaria had denounced in a speech male students not serving in the army as skulkers, and had made obscene remarks to female students. These events encouraged the members of the White Rose. When the defeat at Stalingrad was officially announced, they sent out their sixth – and last – leaflet. The tone of this writing, authored by Kurt Huber and revised by Hans Scholl and Alexander Schmorell, was more patriotic. On 3rd, 8th, and 15 February 1943, Alexander Schmorell, Hans Scholl, and Willi Graf used tin stencils to write paroles like "Down with Hitler" and "Freedom" on the walls of the university and other buildings in Munich.
Between June 1942 and February 1943, the group prepared and distributed six leaflets, in which they called for the active opposition of the German people to Nazi oppression and tyranny. Huber wrote the final leaflet. A draft of a seventh pamphlet, written by Christoph Probst, was found in the possession of Hans Scholl at the time of his arrest by the Gestapo. While Sophie Scholl got rid of incriminating evidence on her person before being taken into custody, Hans did try to destroy the draft of the last leaflet by ripping it into pieces and stuffing into his mouth and sallow it down. However, the Gestapo recovered enough to match with written, signed statements from Probst found later in Hans's apartment.
Quoting extensively from the Bible, Aristotle and Novalis, as well as Goethe and Schiller, the iconic poets of German bourgeoisie, they appealed to what they considered the German intelligentsia, believing that they would be intrinsically opposed to Nazism. These leaflets were left in telephone books in public phone booths, mailed to professors and students, and taken by courier to other universities for distribution. At first, the leaflets were sent out in mailings from cities in Bavaria and Austria, since the members believed that southern Germany would be more receptive to their anti-militarist message.
Alexander Schmorell, who penned the words the White Rose has become most famous for, became an Orthodox saint after his martyrdom. Most of the more practical material—calls to arms and statistics of murder—came from Alex's pen. Hans Scholl wrote in a characteristically high style, exhorting the German people to action on the grounds of philosophy and reason.
By the end of July 1942, some of the male students in the group were deployed to the Eastern Front for military service (acting as medics) during the academic break. In late autumn, the men returned, and the White Rose resumed its resistance activities. In January 1943, using a hand-operated duplicating machine, the group is thought to have produced between 6,000 and 9,000 copies of their fifth leaflet, "Appeal to all Germans!", which was distributed via courier runs to many cities (where they were mailed). Copies appeared in Stuttgart, Cologne, Vienna, Freiburg, Chemnitz, Hamburg, Innsbruck and Berlin. The fifth leaflet was composed by Hans Scholl with improvements by Huber. These leaflets warned that Hitler was leading Germany into the abyss; with the gathering might of the Allies, defeat was now certain. The reader was urged to "Support the resistance movement!" in the struggle for "freedom of speech, freedom of religion and protection of the individual citizen from the arbitrary action of criminal dictator-states". These were the principles that would form "the foundations of a new Europe".
The leaflets caused a sensation, and the Gestapo began an intensive search for the publishers. On the nights of the 3rd, 8th and 15 February 1943, the slogans "Freedom" and "Down with Hitler" appeared on the walls of the university and other buildings in Munich. Alexander Schmorell, Hans Scholl and Willi Graf had painted them with tar-based paint. (Similar graffiti that appeared in the surrounding area at this time were painted by imitators).
The shattering German defeat at Stalingrad at the beginning of February 1943 provided the occasion for the group's sixth leaflet, written by Huber. Headed "Fellow students!" (the now-iconic "Kommilitoninnen! Kommilitonen!"), it announced that the "day of reckoning" had come for "the most contemptible tyrant our people has ever endured." "The dead of Stalingrad adjure us!"
Capture and trial.
On 18 February 1943, the Scholls brought a suitcase full of leaflets to the university. They hurriedly dropped stacks of copies in the empty corridors for students to find when they left the lecture rooms. Leaving before the class break, the Scholls noticed that some copies remained in the suitcase and decided it would be a pity not to distribute them. They returned to the atrium and climbed the staircase to the top floor, and Sophie flung the last remaining leaflets into the air. This spontaneous action was observed by the university maintenance man, Jakob Schmied. The police were called, and Hans and Sophie Scholl were taken into Gestapo custody. Sophie and Hans were interrogated by the Gestapo interrogator Robert Mohr, who initially thought Sophie was innocent. However, after Hans had confessed, Sophie assumed full responsibility in an attempt to protect other members of the White Rose. Despite this, the other active members were soon arrested, and brought in for interrogation.
First "White Rose" trial.
The Scholls and Probst were to stand trial before the "Volksgerichtshof"— the Nazi "People's Court" infamous for its unfair political trials, which more often than not ended with a death sentence — on 22 February 1943. They were found guilty of treason and Roland Freisler, head judge of the court, sentenced them to death. The three were executed the same day by guillotine at Stadelheim Prison. All three were noted for the courage with which they faced their deaths, particularly Sophie, who remained firm despite intense interrogation. She said to Freisler during the trial, "You know as well as we do that the war is lost. Why are you so cowardly that you won't admit it?" Immediately before Hans was executed, he cried out ""Es lebe die Freiheit!" - Let freedom live!", as the blade fell.
Second trial.
The second White Rose trial took place on 19 April 1943. Only eleven had been indicted before this trial. At the last minute, the prosecutor added Traute Lafrenz (who was considered so dangerous that she was to have had a trial all to herself), Gisela Schertling and Katharina Schüddekopf. Others tried were Hans Hirzel, Susanne Hirzel, Franz Josef Müller, Heinrich Guter, Eugen Grimminger, Heinrich Bollinger, Helmut Bauer and Falk Harnack. None had an attorney. One was assigned after the women appeared in court with their friends. Prior to their deaths, several members of the White Rose believed that their execution would stir university students and other anti-war citizens into activism against Hitler and the war.
Huber had counted on the good services of his friend, attorney Justizrat Roder, a high-ranking Nazi. Roder had not bothered to visit Huber before the trial and had not read Huber's leaflet. Another attorney had carried out all the pre-trial paperwork. When Roder realized how damning the evidence was against Huber, he resigned. The junior attorney took over.
Grimminger initially was to receive the death sentence for funding their operations, but escaped with a sentence of ten years in a penitentiary.
Third trial.
The third White Rose trial was to have taken place on 20 April 1943 (Hitler's birthday), because Freisler anticipated death sentences for Wilhelm Geyer, Harald Dohrn, Josef Söhngen and Manfred Eickemeyer. He did not want too many death sentences at a single trial, and had scheduled those four for the next day. However, the evidence against them was lost, and the trial was postponed until 13 July 1943.
At that trial, Gisela Schertling—who had betrayed most of the friends, even fringe members like Gerhard Feuerle—changed her mind and recanted her testimony against all of them. Since Freisler did not preside over the third trial, the judge acquitted all but Söhngen (who got only six months in prison) for lack of evidence.
Alexander Schmorell and Kurt Huber were beheaded on 13 July 1943, and Willi Graf on 12 October 1943. Huber's widow was sent a bill for 600 marks (twice her husband's monthly salary) for "wear of the guillotine." Friends and colleagues of the White Rose, who had helped in the preparation and distribution of leaflets and in collecting money for the widow and young children of Probst, were sentenced to prison terms ranging from six months to ten years.
After her release for the sentence handed down on 19 April, Traute Lafrenz was rearrested. She spent the last year of the war in prison. Trials kept being postponed and moved to different locations because of Allied air raids. Her trial was finally set for April 1945, after which she probably would have been executed. Three days before the trial, however, the Allies liberated the town where she was held prisoner, thereby saving her life.
Initial reactions in World War II Germany and abroad.
The hopes of the White Rose members that the defeat at Stalingrad would incite the German opposition against the Nazi regime and its war did not come true. In contrary, the Nazi propaganda used the defeat to call on the German people to embrace "Total War". Coincidentally, on 18 February 1943, the same day that saw the arrests of Sophie and Hans Scholl and Christoph Probst, Nazi propaganda minister Joseph Goebbels delivered his Sportpalast speech, and was enthusiastically applauded by his audience.
Shortly after the arrest of the Scholl siblings and Christoph Probst, newspapers published all-points bulletins in search of Alexander Schmorell. On 22 February 1943, the students of Munich were assembled, and officially protested against the "traitors" who came from within their ranks. Gestapo and Nazi jurisdiction documented in their files their view of the White Rose members as "traitors and defeatists". On 23 February, the official newspaper of the Nazi party, ""Völkischer Beobachter"" and local newspapers in Munich briefly reported about the capture and execution of some "degenerate rogues". However, the network of friends and supporters proved to be too large, so that the rumors about the White Rose could not be suppressed any more by Nazi German officials. Until the end of World War II, further prosecutions took place, and German newspapers continued to report, mostly in brief notes, that further people had been arrested and punished. On 15 March 1943, a report by the "Sicherheitsdienst" of the "Schutzstaffel" stated that rumors about the leaflets spread "considerable unrest" amongst the German population. The report expressed particular concern about the fact that leaflets were not handed in to the Nazi authorities by their finders as promptly as they used to be in the past.
On 18 April 1943, the The New York Times issued an article about "Signs of strain seen in German populace", mentioning the student opposition in Munich. "The New York Times" published articles on the first White Rose trials on 29 March 1943 and 25 April 1943, entitled "Nazis Execute 3 Munich Students For Writing Anti-Hitler Pamphlets" and "Germans Clinging to Victory Hope in Fear of Reprisals," respectively. Though they did not correctly record all of the information about the resistance, the trials, and the execution, they were the first acknowledgement of the White Rose in the United States.
On 27 June 1943, the German author and Nobel prize winner Thomas Mann, in his monthly anti-Nazi broadcasts by the BBC called ""Deutsche Hörer!"" ("German Audience!") highly praised the White Rose members' courage. The Soviet Army propaganda issued a leaflet, wrongly attributed by later researchers to the National Committee for a Free Germany, in honour of the White Rose's fight for freedom.
The text of the sixth leaflet of the White Rose was smuggled out of Germany through Scandinavia to the United Kingdom by the German lawyer and member of the Kreisau Circle, Helmuth James Graf von Moltke. In July 1943, copies were dropped over Germany by Allied planes, retitled "The Manifesto of the Students of Munich". Thus, the activities of the White Rose became widely known in World War II Germany, but, like other attempts at resistance, did not provoke any active opposition against the totalitarian regime within the German population.
Commemoration.
With the fall of Nazi Germany, the White Rose came to represent opposition to tyranny in the German psyche and was lauded for acting without interest in personal power or self-aggrandizement. Their story became so well known that the composer Carl Orff claimed (falsely by some accounts) to his Allied interrogators that he was a founding member of the White Rose and was released. He was personally acquainted with Huber, but there is no evidence that Orff was ever involved in the movement.
On 5 February 2012 Alexander Schmorell was canonized as a New Martyr by the Orthodox Church.
The square where the central hall of Munich University is located has been named "Geschwister-Scholl-Platz" after Hans and Sophie Scholl; the square opposite to it is "Professor-Huber-Platz". Two large fountains are in front of the university, one on either side of Ludwigstraße. The fountain in front of the university is dedicated to Hans and Sophie Scholl. The other, across the street, is dedicated to Professor Huber. Many schools, streets, and other places across Germany are named in memory of the members of the White Rose.
One of Germany's leading literary prizes is called the Geschwister-Scholl-Preis (the "Scholl Siblings" prize). Likewise, the asteroid 7571 Weisse Rose is named after the group.
The White Rose has also received artistic treatments, including the acclaimed opera "Weiße Rose" by Udo Zimmermann, "In memoriam: die weisse Rose" by Hans Werner Henze and "Kommilitonen!", an opera by Peter Maxwell Davies.
In the media.
The following is a non-exhaustive chronological account of some of the more notable treatments of the White Rose in media, book and artistic form.
Beginning in the 1970s, three film accounts of the White Rose resistance were produced. The first was a film financed by the Bavarian state government entitled "Das Versprechen" ("The Promise") and released in the 1970s. The film is not well known outside Germany, and to some extent even within the country. It was particularly notable in that unlike most films, it showed the White Rose from its inception and how it progressed. In 1982, Percy Adlon's "Fünf letzte Tage" ("The Last Five Days") presented Lena Stolze as Sophie in her last days from the point of view of her cellmate Else Gebel. In the same year, Stolze repeated the role in Michael Verhoeven's "Die Weiße Rose" ("The White Rose").
A book, "Sophie Scholl and the White Rose", was published in English in February 2006. An account by Annette Dumbach and Dr. Jud Newborn tells the story behind the film "Sophie Scholl: The Final Days", focusing on the White Rose movement while setting the group's resistance in the broader context of German culture and politics and other forms of resistance during the Nazi era.
As mentioned earlier, Udo Zimmermann composed a chamber opera about the White Rose ("Weiße Rose") in 1986. Premiering in Hamburg, it went on to earn acclaim and a series of international performances.
Lillian Garrett-Groag's play, "The White Rose", premiered at the Old Globe Theatre in 1991. Several plays have also been written by teachers in the USA for performance by students.
In "Fatherland", an alternate history novel by Robert Harris, there is passing reference to the White Rose still remaining active in supposedly Nazi-ruled Germany in 1964.
In an extended German national TV competition held in the autumn of 2003 to choose "the ten greatest Germans of all time" (ZDF TV), Germans under the age of 40 placed Hans and Sophie Scholl in fourth place, selecting them over Bach, Goethe, Gutenberg, Willy Brandt, Bismarck, and Albert Einstein. Not long before, women readers of the mass-circulation magazine "Brigitte" had voted Sophie Scholl as "the greatest woman of the twentieth century".
In 2003, a group of students at the University of Texas at Austin, Texas established "The White Rose Society" dedicated to Holocaust remembrance and genocide awareness. Every April, the White Rose Society hands out 10,000 white roses on campus, representing the approximate number of people killed in a single day at Auschwitz. The date corresponds with Yom Hashoah, Holocaust Memorial Day. The group organizes performances of "The Rose of Treason", a play about the White Rose, and has rights to show the movie "Sophie Scholl – Die letzten Tage" ("Sophie Scholl: The Final Days"). The White Rose Society is affiliated with and the Anti-Defamation League.
In February 2005, a movie about Sophie Scholl's last days, "Sophie Scholl – Die letzten Tage" ("Sophie Scholl: The Final Days"), featuring actress Julia Jentsch as Sophie, was released. Drawing on interviews with survivors and transcripts that had remained hidden in East German archives until 1990, it was nominated for an Academy Award for Best Foreign Language Film in January 2006. An American film project about the White Rose continues to be under development by co-author Jud Newborn of the 2006 book "Sophie Scholl and the White Rose."
White Rose has inspired many people around the world, including many anti-war activists in recent years. Scattered throughout 2007–08, 5 hoax pipe bombs were placed at various military recruitment centers with the words "Die Weisse Rose" written upon them.
In February 2009, a biography of Sophie Scholl, "Sophie Scholl: The Real Story of the Woman Who Defied Hitler", was published in English by the History Press. The book, by the Oxford-educated British historian Frank McDonough.
The UK-based genocide prevention student network Aegis Students uses a white rose as their symbol in commemoration of the White Rose movement. There are numerous study guides to the White Rose, notably one available from the University of Minnesota's Holocaust Center.
In 2009, Dan Fesperman published a novel entitled "The Arms Maker of Berlin" in which activities by real and fictional White Rose characters play a significant role in the story.
In 2011, a documentary film by André Bossuroy addressing the memory of the victims of Nazism and of Stalinism "ICH BIN", with the support from the Fondation Hippocrène and from the EACEA Agency of the European Commission (programme Europe for Citizens – An active European remembrance), RTBF, VRT. Four young Europeans meet with historians and witnesses of our past… They investigate the events of the Second World War in Germany (the student movement of the White Rose in Munich), in France (the Vel' d'Hiv Roundup in Paris, the resistance in Vercors) and in Russia (Katyn Forest massacre). They examine the impact of these events; curious as to how the European peoples are creating their identities today.
Further reading.
Primary Source Materials in English Translation:

</doc>
<doc id="51306" url="https://en.wikipedia.org/wiki?curid=51306" title="Battle of Lake Benacus">
Battle of Lake Benacus

The Battle of Lake Benacus was fought along the banks of Lake Garda in northern Italy, which was known to the Romans as Benacus, in 268 or early 269 AD, between the army under the command of the Roman Emperor Claudius II and the Germanic tribes of the Alamanni and Juthungi.
Background.
Ιn 268, the Alamanni, who had been making incursions into Roman territory since the reign of Marcus Aurelius, had broken through the Roman frontier at the Danube and crossed the Alps. The power struggles in Mediolanum due to Aureolus' revolt, the murder of Emperor Gallienus and the resulting confrontation between Aureolus and Claudius, who had been nominated as emperor by Gallienus on his death bed, forced the Romans to denude the frontier of troops. Having defeated and killed Aureolus in the Siege of Mediolanum Claudius led his army, together with the remants of Aureolus' force, north to confront the Germans.
Battle and aftermath.
Details of the battle are unknown but future emperor Aurelian certainly played a part. After what was described as a complete victory, Claudius assumed the title Germanicus Maximus. Much of the German army was slaughtered on the field with the remainder retreating beyond the bounds of the empire. Claudius returned to Rome after the battle to attend to affairs of state. The Alemanni returned to Italy in 271 and won a victory against Emperor Aurelian at the Battle of Placentia before their ultimate defeat in the Battle of Fano.

</doc>
<doc id="51307" url="https://en.wikipedia.org/wiki?curid=51307" title="Miklós Horthy">
Miklós Horthy

Miklós Horthy de Nagybánya (; ; English: Nicholas Horthy; ; 18 June 18689 February 1957) was a Hungarian admiral and statesman, who served as Regent of the Kingdom of Hungary between World Wars I and II and throughout most of World War II, from 1 March 1920 to 15 October 1944. He was styled "His Serene Highness the Regent of the Kingdom of Hungary" (Hungarian: "Ő Főméltósága a Magyar Királyság Kormányzója").
Horthy started his career as a Frigate Lieutenant in the Austro-Hungarian Navy in 1896 and attained the rank of admiral in 1918. He saw action in the Otranto Raid and the Battle of the Strait of Otranto and became Commander-in-Chief of the Austro-Hungarian Navy in the last year of the First World War. In 1919, following a series of revolutions and external interventions in Hungary from Romania, Czechoslovakia, and Yugoslavia, Horthy returned to Budapest with the National Army and established a regency government.
Horthy led a national conservative
government through the interwar period, banning the Hungarian Communist Party as well as the fascist Arrow Cross party, and pursuing an irredentist foreign policy in the face of the Treaty of Trianon. King Charles IV of Hungary unsuccessfully attempted to re-gain his throne twice from Horthy until, in 1921, the Hungarian Parliament took formal steps to de-throne his dynasty, the House of Hapsburg.
In the late 1930s, Horthy's foreign policy led him into an alliance with Nazi Germany. With the support of Adolf Hitler, Horthy was able to recover certain Hungarian lands lost after World War I. Under Horthy's leadership, Hungary participated in the German invasion of the Soviet Union in 1941 and the German invasion of Yugoslavia the same year. However, Horthy's reluctance to contribute to the German war effort and the Holocaust in Hungary, coupled with attempts to strike a secret deal with the Allies of World War II, eventually led the Germans to invade and take control of the country in March 1944 in Operation Margarethe. In October 1944, Horthy announced that Hungary would surrender to the Allies and withdraw from the Axis. He was forced to resign, placed under arrest by the Germans and taken to Bavaria. At the end of the war, he came under the custody of American troops.
After appearing as a witness at the Nuremberg war-crimes trials in 1948, Horthy settled and lived out his remaining years in exile in Portugal. His memoirs, "Ein Leben für Ungarn" ("A Life for Hungary"), were first published in 1953. He is perceived as a controversial historical figure in contemporary Hungary.
Early life and naval career.
Miklós Horthy was born at Kenderes to an old Calvinist noble family descended from István Horti, ennobled by Ferdinand II of Hungary in 1635. His father, Miklós Horthy, Sr., was a member of the House of Magnates and lord of a 1,500 acre estate. He married Paula Halassy in 1857. Miklós was the fourth of their eight children.
Horthy entered the Austro-Hungarian naval academy at Fiume (now Rijeka, Croatia) at age 14. Because the official language of the naval academy was German, Horthy spoke Hungarian with a slight, but noticeable, Austro-German accent for the rest of his life. He also spoke Italian, Croatian, English, and French.
As a young man, Horthy travelled around the world and served as a diplomat for the Austro-Hungarian Empire in Turkey and other countries. Horthy married Magdolna Purgly in Arad in 1901. They had four children: Magdolna (1902), Paula (1903), István (1904) and Miklós (1907). From 1911 until 1914, he was a naval aide-de-camp to Emperor Franz Joseph I of Austria, for whom he had a great respect.
At the beginning of World War I, Horthy was commander of the the pre-dreadnought battleship . In 1915, he earned a reputation for boldness while commanding the new light cruiser . He planned the 1917 attack on the Otranto Barrage, which resulted in the Battle of the Strait of Otranto, the largest naval engagement of the war in the Adriatic Sea. A consolidated British, French and Italian fleet met the Austro-Hungarian force. Despite the numerical superiority of the Allied fleet, the Austrian force emerged from the battle victorious. The Austrian fleet remained relatively unscathed, however Horthy was wounded. After the Cattaro mutiny of February 1918, Emperor Charles I of Austria (Charles IV as King of Hungary) selected Horthy over many more senior commanders as the new Commander-in-Chief of the Imperial Fleet in March 1918. In June, Horthy planned another attack on Otranto, and in a departure from the cautious strategy of his predecessors, he committed the empire's battleships to the mission. While sailing through the night, the dreadnought met Italian MAS torpedo boats and was sunk, causing Horthy to abort the mission. He managed however to preserve the rest of the empire's fleet in being until he was ordered by Emperor Charles to surrender it to the new State of Slovenes, Croats and Serbs (the predecessor of Yugoslavia) on 31 October.
The end of the war saw Hungary turned into a landlocked nation, and hence the new government had little need for Horthy's services. He retired with his family to his private estate at Kenderes, but his role as a Hungarian leader was far from over.
Interwar period, 1919–1939.
Commander of the National Army.
Two national traumas that followed the First World War profoundly shaped the spirit and future of the Hungarian nation. The first was the loss, as dictated by the Allies of World War I, of large portions of Hungarian territory that had bordered other countries. These were lands that had belonged to Hungary as part of the Austro-Hungarian Empire but ceded to the nations of Czechoslovakia, Romania, Austria and Yugoslavia. The excisions, eventually ratified in the Treaty of Trianon of 1920, cost Hungary two-thirds of its territory and one-third of its native Hungarian speakers; this dealt the population a terrible psychological blow. The second trauma began in March 1919, when the Communist leader Béla Kun seized power in the capital of Budapest after the first proto-democratic government in Hungary faltered.
Kun and his colleagues proclaimed a Hungarian Soviet Republic and promised the restoration of Hungary's former grandeur. Instead, his efforts at reconquest failed, and Hungarians were treated to a Soviet-style repression in the form of armed gangs who intimidated or murdered enemies of the regime. This period of violence came to be known as the Red Terror.
Within weeks of his coup, Kun's popularity plummeted. On 30 May 1919, anti-Communist politicians formed a counter-revolutionary government in the southern city of Szeged, which occupied by French forces at the time. There, Gyula Károlyi, the prime minister of the counter-revolutionary government, asked former admiral Horthy, still considered a war hero, to be the Minister of War in the new government and take command of a counter-revolutionary force that would be named the National Army (). Horthy consented, and he arrived in Szeged on 6 June. Soon after, because of orders from the Allied powers, a cabinet was reformed, and Horthy was not given a seat in it. Undaunted, Horthy managed to retain control of the National Army by detaching the army command from the War Ministry.
On 6 August, French-supported Romanian forces entered Budapest. The Communist government collapsed, and its leaders fled. In retaliation for the Red Terror, reactionary crews now exacted revenge in a two-year wave of violent repression known today as the White Terror. These reprisals were organized and carried out by officers of Horthy's National Army, particularly Pál Prónay, Gyula Ostenburg-Moravek and Iván Héjjas. Their victims were primarily Communists, Social Democrats, and Jews. Most Hungarian Jews were not supporters of the Bolsheviks, but much of the leadership of the Hungarian Soviet Republic had been young Jewish intellectuals, and anger about the Communist revolution easily translated into anti-Semitic hostility.
In Budapest, Prónay installed his unit in the Hotel Britannia, where the group swelled to battalion size. Their program of vicious attacks continued; they planned a city-wide pogrom against the Jews until Horthy found out and put a stop to it. In his diary, Prónay reported that Horthy
The degree of Horthy's responsibility for the excesses of Prónay is disputed. On several occasions, Horthy reached out to stop Prónay from a particularly excessive burst of anti-Jewish cruelty, and the Jews of Pest went on record absolving Horthy of the White Terror as early as the fall of 1919, when they released a statement disavowing the Kun revolution and blaming the terror on a few units within the National Army. Horthy has never been found to have personally engaged in White Terror atrocities. But his American biographer, Thomas Sakmyster, concluded that he "tacitly supported the right wing officer detachments" who carried out the terror; Horthy called them "my best men". The admiral also had practical reasons for overlooking the terror his officers wrought, since he needed the dedicated officers to help stabilize the country. Nevertheless, it was at least another year before the terror died down. In the summer of 1920, Horthy's government took measures to rein in and eventually disperse the reactionary battalions. Prónay managed to undermine these measures, but only for a short time. Prónay was put on trial for extorting a wealthy Jewish politician, and for "insulting the President of the Parliament" by trying to cover up the extortion. Found guilty on both charges, Prónay was now a liability and an embarrassment. His command was revoked, and he was denounced as a common criminal on the floor of the Hungarian parliament.
After serving short jail sentences, Prónay tried to convince Horthy to restore his battalion command. The Prónay Battalion lingered for a few months more under the command of a junior officer, but the government officially dissolved the unit in January 1922 and expelled its members from the army. Prónay entered politics as a member of the government's right-wing opposition. In the 1930s, he sought and failed to emulate the Nazis by generating a Hungarian fascist mass movement. In 1932, he was charged with incitement, sentenced to six months in prison and stripped of his rank of lieutenant colonel. Prónay would support the pro-Nazi Arrow Cross and lead attacks on Jews before being killed by Soviet troops sometime during or after the Battle of Budapest of 1944-45.
Precisely how much Horthy knew about the excesses of the White Terror is not known. Horthy himself declined to apologize for the savagery of his officer detachments, writing later, "I have no reason to gloss over deeds of injustice and atrocities committed when an iron broom alone could sweep the country clean." He endorsed Edgar von Schmidt-Pauli's poetic justification of the White reprisals ("Hell let loose on earth cannot be subdued by the beating of angels' wings") remarking, "the Communists in Hungary, willing disciples of the Russian Bolshevists, had indeed let hell loose."
The International Committee of the Red Cross (ICRC) in an internal report by delegate George Burnier, stated the following in April 1920:
This deep hostility toward Communism would be the more lasting legacy of Kun's abortive revolution. It was a conviction shared by Horthy and his country's ruling class that would help drive Hungary into what might have been a fatal alliance with Adolf Hitler.
Following the orders of the Allied powers, Romanian troops finally evacuated Hungary on 25 February 1920.
Regent.
On 1 March 1920, the National Assembly of Hungary re-established the Kingdom of Hungary. However, it was apparent that the Allies of World War I would not accept any return of King Charles IV (the former Austro-Hungarian emperor) from exile. Instead, with National Army officers controlling the parliament building, the assembly voted to install Horthy as Regent; he defeated Count Albert Apponyi by a vote of 131 to 7.
Bishop Ottokár Prohászka then led a small delegation to meet Horthy, announcing, "Hungary's Parliament has elected you Regent! Would it please you to accept the office of Regent of Hungary?" To their astonishment, Horthy declined, unless the powers of the office were expanded. As Horthy stalled, the politicians gave in to his demands and granted him "the general prerogatives of the King, with the exception of the right to name titles of nobility and of the patronage of the Church." The prerogatives he was given included the power to appoint and dismiss prime ministers, to convene and dissolve parliament, and to command the armed forces. With those sweeping powers guaranteed, Horthy took the oath of office. (Charles I did try to regain his throne twice; see Charles I of Austria's attempts to retake the throne of Hungary for more details.)
The Hungarian state was legally a kingdom, but it had no king, as the Allied powers would not have tolerated any re-instatement of the Habsburg dynasty. The country retained its parliamentary system following the dissolution of Austria-Hungary, with a prime minister appointed as head of government. As head of state, Horthy retained significant influence through his constitutional powers and the loyalty of his ministers to the crown. Although his involvement in drafting legislation was minuscule, he nevertheless had the ability to ensure that laws passed by the Hungarian parliament conformed to his political preferences.
Seeking redress for the Treaty of Trianon.
The first decade of Horthy's reign was primarily consumed by stabilizing the Hungarian economy and political system. Horthy's chief partner in these efforts was his prime minister, István Bethlen. It was commonly known that Horthy was ananglophile, and British political and economic support played a significant role in the stabilization and consolidation of the early Horthy era in the Kingdom of Hungary.
Bethlen sought to stabilize the economy while building alliances with weaker nations that could advance Hungary's cause. That cause was, primarily, reversing the losses of the Treaty of Trianon. The humiliations of the Trianon treaty continued to occupy a central place in Hungarian foreign policy and the popular imagination. The indignant anti-Trianon slogan "Nem, nem soha!" ("No, no never!") became a ubiquitous motto of Hungarian outrage. When in 1927 the British newspaper magnate Lord Rothermere denounced, in the pages of his "Daily Mail", the partitions ratified at Trianon, an official letter of gratitude was eagerly signed by 1.2 million Hungarians.
But Hungary's stability was precarious, and the Great Depression derailed much of Bethlen's economic balance. Horthy replaced him with an old reactionary confederate from his Szeged days: Gyula Gömbös. Gömbös was an outspoken anti-Semite and a budding fascist. Although he agreed to Horthy's demands that he temper his anti-Jewish rhetoric and work amicably with Hungary's large Jewish professional class, Gömbös's tenure began swinging Hungary's political mood powerfully rightward. He strengthened Hungary's ties to Benito Mussolini's Italian fascist state. Fatefully, when Adolf Hitler took power in Germany in 1933, he found in Gömbös an admiring and obliging colleague. John Gunther in 1936 stated that Horthy,
Gömbös rescued the failing economy by securing trade guarantees from Germany – a strategy that positioned Germany as Hungary's primary trading partner and tied Hungary's future even more tightly to Hitler's. He also assured Hitler that Hungary would quickly become a one-party state modelled on the Nazi party control of Germany. Gömbös died in 1936, before he realized his most extreme goals, but he left his nation headed into firm partnership with the German dictator.
World War II and the Holocaust.
Uneasy alliance.
Hungary now entered into intricate political maneuvers with the regime of Adolf Hitler, and Horthy began to play a greater and more public role in navigating Hungary along this dangerous path.
For Horthy, Hitler served as a bulwark against Soviet encroachment or invasion. Horthy was, in the eyes of observers, obsessed with the Communist threat. One American diplomat remarked that Horthy's anti-Communist tirades were so common and ferocious that diplomats "discounted it as a phobia."
Horthy clearly saw his country as trapped between two stronger powers, both of them dangerous; evidently he considered Hitler to be the more manageable of the two, at least at first. Hitler was able to wield great influence over Hungary than the Soviet Union could not only as the country's major trading partner, but also because he could assist with two of Horthy's key ambitions: maintaining Hungarian sovereignty and satisfying the nationwide yearning to recover former Hungarian lands. Horthy's strategy was one of cautious, sometimes even grudging, alliance. The means by which the regent granted or resisted Hitler's demands, especially with regard to Hungarian military action and the treatment of Hungary's Jews, remain the central critera by which his career has been judged. Horthy's relationship with Hitler was, by his own account, a tense one – largely due, he said, to his unwillingness to bend his nation's policies to the German leader's desires. 
Horthy's attitude to Hitler was ambivalent. On one hand, Hungary was a revisionist state that refused to accept the frontiers imposed by the Treaty of Trianon. Furthermore, the three states with which Hungary had territorial disputes, namely Czechoslovakia, Yugoslavia, and Romania, were all allies of France, so a German-Hungarian alliance seemed logical. On the other hand, Admiral Horthy was a good navalist who believed that sea power was the most important factor in war. He felt that Britain, as the world's greatest sea power, would inevitably defeat Germany should another war begin. During a meeting with Hitler in 1935, Horthy was well pleased that Hitler informed him that he wanted Germany and Hungary to partition Czechoslovakia, but Horthy went on to tell Hitler that he must careful not to do anything that might cause an Anglo-German war, because British sea power would sooner or later caused the defeat of the "Reich". Horthy was always torn between his belief that an alliance with Germany was the only means that could enable him to revise Trianon and his belief that war against the international order could only end in defeat.
In August 1938, when Horthy, his wife and some Hungarian politicians took a special train from Budapest to Germany. SA and other National Socialist formations ceremonially welcomed the delegation at the Passau train station. Then, the train continued for the christening of the German war ship "Prinz Eugen".
During his ensuing state visit, Hitler asked Horthy for troops and matériel to participate in Germany's planned invasion of Czechoslovakia. In exchange, Horthy later reported, "He gave me to understand that as a reward we should be allowed to keep the territory we had invaded." Horthy said he declined, insisting to Hitler that Hungary's claims on the disputed lands should be settled by peaceful means.
Three months later, after the Munich Agreement put control of Czechoslovakia's Sudetenland in Hitler's hands, Hitler allowed Hungary to annex nearly one-fourth of Slovakia. Horthy enthusiastically rode into the re-acquired territories at the head of his troops, greeted by emotional ethnic Hungarians: "As I passed along the roads, people embraced one another, fell upon their knees, and wept with joy because liberation had come to them at last, without war, without bloodshed." But as "peaceful" as this annexation was, and as just as it may have seemed to many Hungarians, it was a dividend of Hitler's brinksmanship and threats of war, in which Hungary was now inextricably complicit.
Hungary was now committed to the Axis agenda: on 24 February 1939, it joined the Anti-Comintern pact, and on 11 April, it withdrew from the League of Nations. American journalists began to refer to Hungary as "the jackal of Europe."
This combination of menace and reward drifted Hungary closer to the status of a Nazi client state. In March 1939, when Hitler took what remained of Czechoslovakia by force, Hungary was allowed to annex Carpathian Ruthenia. After a conflict with the First Slovak Republic during the Slovak-Hungarian War of 1939, Hungary gained further territories. In August 1940, Hitler intervened on Hungary's behalf once again. Hungary was permitted to take Northern Transylvania away from Romania in the Second Vienna Award.
But in spite of their cooperation with the Nazi regime, Horthy and his government would be better described as "conservative authoritarian" than "fascist". Certainly Horthy was as hostile to the home-grown fascist and ultra-nationalist movements that emerged in Hungary between the wars (particularly the Arrow Cross Party) as he was to Communism. The Arrow Cross leader, Ferenc Szálasi, was repeatedly imprisoned at Horthy's command.
John F. Montgomery, who served in Budapest as U.S. ambassador from 1933 to 1941, openly admired this side of Horthy's character and reported the following incident in his memoir: in March 1939, Arrow Cross supporters disrupted a performance at the Budapest opera house by chanting "Justice for Szálasi!" loud enough for the regent to hear. A fight broke out, and when Montgomery went to take a closer look, he discovered that
And yet, by the time of this episode, Horthy had allowed his government to give in to Nazi demands that the Hungarians enact laws restricting the lives of the country's Jews. The first Hungarian anti-Jewish Law, in 1938, limited the number of Jews in the professions, the government and commerce to twenty percent, and the second reduced it to five percent the following year; 250,000 Hungarian Jews lost their jobs as a result. A "Third Jewish Law" of August 1941 prohibited Jews from marrying non-Jews and defined anyone having two Jewish grandparents as "racially Jewish." A Jewish man who had non-marital sex with a "decent non-Jewish woman resident in Hungary" could be sentenced to three years in prison.
Horthy's personal views on Jews and their role in Hungarian society are the subject of some debate. In an October 1940 letter to Prime Minister Pál Teleki, Horthy echoed a widespread national sentiment: that Jews enjoyed too much success in commerce, the professions, and industry – success that needed to be curtailed:
War.
The Kingdom of Hungary was gradually drawn into the war itself. In 1939 and 1940, volunteer units fought in Finland's Winter War. In April 1941, Hungary became, in effect, a member of the Axis. Hungary permitted Hitler to send troops across Hungarian territory for the invasion of Yugoslavia and ultimately sent its own troops to claim its share of the dismembered Kingdom of Yugoslavia. Prime Minister Pál Teleki, horrified that he had failed to prevent this collusion with the Nazis against a former ally, committed suicide.
In June 1941, the Hungarian government finally yielded to Hitler's demands that the nation contribute to the Axis war effort. On 27 June, Hungary became part of Operation Barbarossa and declared war on the Soviet Union. The Hungarians sent in troops and matériel only four days after Hitler began his invasion of the Soviet Union on 22 June 1941.
Eighteen months later, less well-equipped and less motivated than their German allies, 200,000 troops of the Hungarian Second Army ended up holding the front on the Don River west of Stalingrad.
The first massacre of Jewish people from Hungarian territory took place in August 1941, when government officials ordered the deportation of Jews without Hungarian citizenship (principally refugees from other Nazi-occupied countries) to Ukraine. Roughly 18,000–20,000 of these deportees were slaughtered by Friedrich Jeckeln and his SS troops; only 2,000–3,000 survived. These killings are known as the Kamianets-Podilskyi Massacre. This event, in which the slaughter of Jews for the first time numbered in the tens of thousands, is considered the first large-scale massacre of the Holocaust. Because of the objections of Hungary's leadership, the deportations were halted.
By early 1942, Horthy was already seeking to put some distance between himself and Hitler's regime. That March, he dismissed the pro-German prime minister László Bárdossy, and replaced him with Miklós Kállay, a moderate whom Horthy expected to loosen Hungary's ties to Germany.
In September 1942, personal tragedy struck the Hungarian Regent. 37-year-old István Horthy, Horthy's eldest son, was killed. István Horthy was the Deputy Regent of Hungary and a Flight Lieutenant in the reserves, 1/1 Fighter Squadron of the Royal Hungarian Air Force. He was killed when his Hawk ("Héja") fighter crashed at an air field near Ilovskoye.
In January 1943, Hungary's enthusiasm for the war effort, never especially high, suffered a tremendous blow. The Soviet army, in the full momentum of its triumphant turnaround after the Battle of Stalingrad, punched through Romanian troops at a bend in the Don River and virtually obliterated the Second Hungarian Army in a few days' fighting. In this single action, Hungarian combat fatalities jumped by 80,000. Jew and non-Jew suffered together in this defeat, as the Hungarian troops had been accompanied by some 40,000 Jews and political prisoners in forced-labour units whose job had been to clear minefields.
German officials blamed Hungary's Jews for the nation's "defeatist attitude." In the wake of the Don bend disaster, Hitler demanded at an April 1943 meeting that Horthy punish the 800,000 Jews still living in Hungary, who according to Hitler were responsible for this defeat. In response, Horthy and his government supplied 10,000 Jewish deportees for labour battalions. However, with the growing awareness that the Allies might well win the war, it became more expedient not to comply with further German requests. Cautiously, the Hungarian government began to explore contacts with the Allies in hopes of negotiating a surrender.
Occupation.
By 1944, the Axis was losing the war, and the Red Army was at Hungary's borders. Fearing that the Soviets would overrun the country, Kállay, with Horthy's approval, put out numerous feelers to the Allies. He even promised to surrender unconditionally to them once they reached Hungarian territory. An enraged Hitler summoned Horthy to a conference in Klessheim (today in Austria). He pressured Horthy to make greater contributions to the war effort and again commanded him to assist in the killing of more of Hungary's Jews. Horthy now permitted the deportation of a large number of Jews (the generally accepted figure is 100,000), but would not go further.
The conference was a ruse. As Horthy was returning home on 19 March, the Wehrmacht invaded and occupied Hungary. Horthy was told he could only stay in office if he dismissed Kállay and appointed a new government that would fully cooperate with Hitler and his plenipotentiary in Budapest, Edmund Veesenmayer. Knowing the likely alternative was a gauleiter who would treat Hungary in the same manner as the other countries under Nazi occupation, Horthy acquiesced and appointed his ambassador to Germany, General Döme Sztójay, as prime minister. The Germans originally wanted Horthy to reappoint Béla Imrédy (who had been prime minister from 1938 to 1939), but Horthy had enough influence to get Veesenmayer to accept Sztójay instead. Contrary to Horthy's hopes, Sztójay's government eagerly proceeded to participate in the Holocaust.
The chief agents of this collaboration were Andor Jaross, the Minister of the Interior, and his two rabidly anti-Semitic state secretaries, László Endre and László Baky (later to be known as the "Deportation Trio"). On 9 April, Prime Minister Sztójay and the Germans obligated Hungary to place 300,000 Jewish people at the "disposal" of the Reich, in effect, sentencing most of Hungary's remaining Jews to death. Five days later, on 14 April, Endre, Baky, and SS Colonel Adolf Eichmann commenced the deportation of the remaining Hungarian Jews. The Yellow Star, Ghettoization laws, and deportation were accomplished in less than 8 weeks with the help of the new Hungarian government and authorities. The deportation of Hungarian Jews to Auschwitz began on 15 May 1944 and continued at a rate of 12,000 a day until 9 July.
Upon learning about the deportations, Horthy wrote the following letter to the prime minister:
Just before the deportations began, two Slovakian Jewish prisoners, Rudolf Vrba and Alfréd Wetzler, escaped from Auschwitz and passed details of what was happening inside the camps to officials in Slovakia. This document, known as the Vrba-Wetzler Report, was quickly translated into German and passed among Jewish groups and then to Allied officials. Details from the report were broadcast by the BBC on 15 June and printed in "The New York Times" on 20 June. World leaders, including Pope Pius XII (25 June), President Franklin D. Roosevelt on 26 June, and King Gustaf V of Sweden on 30 June, subsequently pleaded with Horthy to use his influence to stop the deportations. Roosevelt specifically threatened military retaliation if the transports were not ceased. On 2 July, Allied bombers executed the heaviest bombings inflicted on Hungary during the war. Hungarian radio accused Jews of guiding the bombers to their targets with radio transmissions and light signals, but on 7 July Horthy at last ordered the transports halted. By that time, 437,000 Jews had been sent to Auschwitz, most of them to their deaths. Horthy was informed about the number of the deported Jews some days later: "approximately 400,000". By many estimates, one of every three people murdered at Auschwitz during its operation was a Hungarian Jew killed between May and July 1944.
There remains some uncertainty over how much Horthy could have known about the number of Hungarian Jews being deported, their destination, and their intended fate – and when he knew it as well as what he could have done about it. According to historian Péter Sipos, the Hungarian government had already known about the Jewish genocide since 1943. Some historians have argued that Horthy believed that the Jews were being sent to the camps to work, and that they would be returned to Hungary after the war. Horthy himself wrote in his memoirs: "Not before August," he wrote, "did secret information reach me of the horrible truth about the extermination camps." The Vrba-Wetzler statement is believed to have been passed to Hungarian Zionist leader Rudolf Kastner no later than 28 April 1944, however, Kastner did not make it public. He made an agreement with the SS to remain silent in order to save the Jews who escaped on the Kastner train. The "Kastner train", a convoy that enabled Hungarian Jews to escape to Switzerland, left Budapest on the 30 June 1944.
Deposition and arrest.
In August 1944, Romania withdrew from the Axis and turned on Hitler and his allies. This development, a sign of the failing German war effort, led Horthy in Budapest to reconsolidate his political position. He ousted Sztójay and the other Nazi-friendly ministers installed the preceding spring, replacing them with a new government under Géza Lakatos. He stopped the mass deportations of Jews and ordered the police to use deadly force if the Germans attempted to resume them. While some smaller groups continued to be deported by train, the Germans did not press Horthy to ramp the pace back up to pre-August levels. Indeed, when Horthy turned down Eichmann's request to re-start the deportations, Heinrich Himmler ordered Eichmann to return to Germany.
Horthy also renewed peace feelers to the Allies and began considering strategies for surrendering to the Allied force he distrusted the most: the Red Army. As bitterly anti-Communist as Horthy was, his dealings with the Nazis led him to conclude that the Communists were the far lesser evil. Working through his trustworthy General Béla Miklós, who was in contact with Soviet forces in eastern Hungary, Horthy sought to surrender to the Soviets while preserving the Hungarian government's autonomy. The Soviets willingly promised this, and on 11 October Horthy and the Soviets finally agreed to surrender terms. On 15 October 1944, Horthy told his government ministers that Hungary had signed an armistice with the Soviet Union. He said, "It is clear today that Germany has lost the war... Hungary has accordingly concluded a preliminary armistice with Russia, and will cease all hostilities against her." Horthy "...informed a representative of the German Reich that we were about to conclude a military armistice with our former enemies and to cease all hostilities against them."
The Nazis had anticipated Horthy's move. On 15 October, after Horthy announced the armistice in a nationwide radio address, Hitler initiated Operation Panzerfaust, sending commando Otto Skorzeny to Budapest with instructions to remove Horthy from power. Horthy's son Miklós Horthy, Jr., was meeting with Soviet representatives to finalize the surrender when Skorzeny and his troops forced their way into the meeting and kidnapped the younger Horthy at gunpoint. Trussed up in a carpet, Miklós Jr. was immediately driven to the airport and flown to Germany to serve as a hostage. Skorzeny then brazenly led a convoy of German troops and four Tiger II tanks to the Vienna Gates of Castle Hill, where the Hungarians had been ordered not to resist. Though one unit had not received the order, the Germans quickly captured Castle Hill with minimal bloodshed: only seven soldiers were killed and twenty-six wounded.
Horthy was captured by Veesenmayer and his staff later on the 15th and taken to the Waffen SS office, where he was held overnight. Vessenmayer told Horthy that unless he recanted the armistice and abdicated, his son would be killed the next morning. The fascist Arrow Cross Party swiftly took over Budapest. With his son's life in the balance, Horthy consented to sign a document officially abdicating his office and naming Ferenc Szálasi, leader of the Arrow Cross, as his successor. Horthy understood that the Germans merely wanted the stamp of his prestige on a Nazi-sponsored Arrow Cross coup, but he signed anyway. As he later explained his capitulation: "I neither resigned nor appointed Szálasi Premier. I merely exchanged my signature for my son's life. A signature wrung from a man at machine-gun point can have little legality."
Horthy met Skorzeny three days later at Pfeffer-Wildenbruch's apartment and was told he would be transported to Germany in his own special train. Skorzeny told Horthy that he would be a "guest of honour" in a secure Bavarian castle. On 17 October, Horthy was personally escorted by Skorzeny into captivity at Schloss Hirschberg in Bavaria, where he was guarded closely, but allowed to live in comfort.
With the help of the SS, the Arrow Cross leadership moved swiftly to take command of the Hungarian armed forces, and to prevent the surrender that Horthy had arranged, even though Soviet troops were now deep inside the country. Szálasi resumed persecution of Jews and other "undesirables". In the three months between November 1944 and January 1945, Arrow Cross death squads shot 10,000 to 15,000 Jews on the banks of the Danube. The Arrow Cross also welcomed Adolf Eichmann back to Budapest, where he began the deportation of the city's surviving Jews. Eichmann never successfully completed this phase of his plans, thwarted in large measure by the efforts of Swedish diplomat Raoul Wallenberg. Out of a pre-war Hungarian Jewish population estimated at 825,000, only 260,000 survived.
By December 1944, Budapest was under siege by Soviet forces. The Arrow Cross leadership retreated across the Danube into the hills of Buda in late January, and by February the city surrendered to the Soviet forces.
Horthy remained under house arrest in Bavaria until the war in Europe ended. On 29 April, his SS guardians fled in the face of the Allied advance. On 1 May, Horthy was first liberated, and then arrested, by elements of the U.S. 7th Army.
Exile.
After his arrest, Horthy was moved through a variety of detention locations before finally arriving at the prison facility at Nuremberg in late September 1945. There he was asked to provide evidence to the International Military Tribunal in preparation for the trial of the Nazi leadership. Although he was interviewed repeatedly about his contacts with some of the defendants, he did not testify in person. In Nuremberg he was reunited with his son, Miklós.
Horthy gradually came to believe that his arrest had been arranged and choreographed by the Americans in order to protect him from the Russians. Indeed, the former regent reported being told that Josip Broz Tito, the new ruler of Yugoslavia, asked that Horthy be charged with complicity with the 1942 massacre of Serbian and Jewish civilians by Hungarian troops in the Bačka region of Vojvodina. Serbian historian Zvonimir Golubović has claimed that not only was Horthy aware of these genocidal massacres, but had approved of them. However, American trial officials did not indict Horthy for war crimes. The former ambassador John Montgomery, who had some influence in Washington, also contributed to Horthy's release in Nuremberg.
According to the memoirs of Ferenc Nagy, who served for a year as prime minister in post-war Hungary, the Hungarian Communist leadership was also interested in extraditing Horthy for trial. Nagy said that Joseph Stalin was more forgiving: that Stalin told Nagy during a diplomatic meeting in April 1945 not to judge Horthy, because he was old and had offered an armistice in 1944.
On 17 December 1945, Horthy was released from Nuremberg prison and allowed to rejoin his family in the German town of Weilheim, Bavaria. The Horthys lived there for four years, supported financially by ambassador John Montgomery, his successor, Herbert Pell, and by Pope Pius XII, whom he knew personally.
In March 1948, Horthy returned to testify at the Ministries Trial, the last of the twelve U.S.-run Nuremberg Trials; he testified against Edmund Veesenmayer, the Nazi administrator who had controlled Hungary during the deportations to Auschwitz in the spring of 1944. Veesenmayer was sentenced to 20 years imprisonment, but was released in 1951.
For Horthy, returning to Hungary was impossible; it was now firmly in the hands of a Soviet-sponsored Communist government. In an extraordinary twist of fate, the chief of Hungary's post-war Communist apparatus was Mátyás Rákosi, one of Béla Kun's colleagues from the ill-fated Communist coup of 1919. Kun had been executed during Stalin's purges of the late 1930s, but Rákosi had survived in a Hungarian prison cell; in 1940 Horthy had permitted Rákosi to emigrate to the Soviet Union in exchange for a series of highly-symbolic Hungarian battle-flags from the 19th century that were in Russian hands.
In 1950, the Horthy family managed to find a home in Portugal, thanks to Miklós Jr.'s contacts with Portuguese diplomats in Switzerland. Horthy and members of his family were relocated to the seaside town of Estoril, in the house address Rua Dom Afonso Henriques, 1937 2765.573 Estoril.
His American supporter, John Montgomery, recruited a small group of wealthy Hungarians to raise funds for their upkeep in exile. According to Horthy's daughter-in-law, Countess Ilona Edelsheim Gyulai, Hungarian Jews also supported Horthy's family in exile, including industrialist Ferenc Chorin and lawyer László Pathy.
In exile, Horthy wrote his memoirs, "Ein Leben für Ungarn" (English: "A Life for Hungary"), published under the name of Nikolaus von Horthy, in which he narrated many personal experiences from his youth until the end of World War II. He claimed that he had distrusted Hitler for much of the time he knew him and tried to perform the best actions and appoint the best officials in his country. He also highlighted Hungary's mistreatment by many other countries since the end of World War I. Horthy was one of the few Axis heads of state to survive the war, and thus to write post-war memoirs.
Horthy never lost his deep contempt for communism, and in his memoirs he blamed Hungary's alliance with the Axis on the threat posed by the "Asiatic barbarians" of the Soviet Union. He railed against the influence that the Allies' victory had given to Stalin's totalitarian state. "I feel no urge to say 'I told you so,' " Horthy wrote, "nor to express bitterness at the experiences that have been forced upon me. Rather, I feel wonder and amazement at the vagaries of humanity."
He died in 1957 in Estoril.
Horthy married Magdolna Purgly de Jószáshely in 1901; they were married for just over 56 years, until his death. He had two sons, Miklós Horthy, Jr. (often rendered in English as "Nicholas" or "Nikolaus") and István Horthy, who served as his political assistants; and two daughters, Magda and Paula. Of his four children, only Miklós outlived him.
According to footnotes in his memoirs, Horthy was very distraught about the failure of the Hungarian Revolution of 1956. In his will, Horthy asked that his body not be returned to Hungary "until the last Russian soldier has left." His heirs honoured the request. In 1993, two years after the Soviet troops left Hungary, Horthy's body was returned to Hungary and he was buried in his home town of Kenderes. The reburial in Hungary was the subject of some controversy on part of the left.
Titles, styles, honours and arms.
Full title as Regent.
His Serene Highness Miklós Horthy, Regent of Hungary.
Legacy.
The interwar period dominated by Horthy's government is known in Hungarian as the "Horthy-kor" ("Horthy age") or "Horthy-rendszer" ("Horthy system"). Its legacy, and that of Horthy himself, remain among the most controversial political topics in Hungary today, tied inseparably to the Treaty of Trianon and the Holocaust. According to one school of thought, Horthy was a strong, conservative, but not undemocratic leader and patriot who only entered into an alliance with Hitler's Germany in order to restore lands Hungary lost after the First World War and was reluctant, or even defiant, in the face of Germany's demands to deport the Hungarian Jewry. Others see Horthy's alliance with Germany as foolhardy, or think that a positive view of Horthy serves a revisionist historical agenda, pointing to Horthy's passage of various anti-Jewish laws — the earliest in Europe, in 1920 — as a sign of his anti-Semitism and willing collaboration in the Holocaust.
The historiography and reception of Horthy has changed throughout the course of modern Hungarian history. He was officially denounced by the state during the communist era, while during his own time and in the 21st century, his reception has been more nuanced.
During the Horthy era.
During his own reign, Horthy's reception was fairly positive, though by no means monolithic. Opponents of the short-lived Soviet Republic saw him as a "national saviour," in contrast to the communist "losers of the nation." Because Horthy distanced himself from everyday politics, he was able to cultivate the image of the nationally governing admiral. The peaceful re-acquisition of Hungarian-majority lands lost after Trianon greatly bolstered this image. The regime's efforts at economic development and modernization also improved contemporaries' opinions, and although the Great Depression initially hurt his image, Horthy's wide-ranging social programs saved face for the most part.
On the other hand, Horthy's right-wing tendencies were not without their critics even in his time. Bourgeois liberals, among them Sándor Márai, criticized Horthy's authoritarian style as much as they disdained the violent tendencies of the far-left. He was also criticized by monarchists and elements of the aristocracy and clergy. While the harshest opposition to Horthy initially came from the communist parties he had overthrown and outlawed, the later 1930s saw him come under increasing criticism from the far-right. After the Arrow Cross took control of the country in 1945, Horthy was denounced as a "traitor" and "Jew-lover."
Horthy's reception in the West was positive until the outbreak of the Second World War, and while Hitler initially backed Horthy, relations between the two leaders were soured by Horthy's denial of involvement in the invasions of Poland and Czechoslovakia. Horthy likewise viewed the Nazis as "brigands and clowns." The Little Entente criticized Horthy, mainly for his irredentist policy goals.
During the communist era.
Under the Marxism mandated during the communist era, the Horthy era was depicted extremely negatively. Scholars agree that due to political pressure, Horthy's positive achievements were unmentioned while his negative aspects were exaggerated to the point of total distortion.
The communist takeover in 1945 saw the same powers that had denounced Horthy as an "executioner" and a "murdering monster" assume control of the state. The government systematically disseminated, through propaganda and state education, the idea that the Horthy era constituted the "lowest point in Hungarian history." Most of these views were supported by socialist or communist activists persecuted under the Horthy administration. Especially critical in this campaign was the 1950 publication of the textbook "The Story of the Hungarian People", which denounced Horthy's military as a "genocidal band" consisting of "sociopathic officers, kulaks, and the dregs of society." It further characterized Horthy himself as a "slave of the Habsburgs," a "red-handed dictator" who "spoke broken Hungarian" and was known for his "hatred of workers and soviets." "The Story of the Hungarian People" was required reading in middle schools throughout the 1950s.
The situation only slowly improved. While the professionalization of Hungarian history and historiography coupled with the loosening of state ideological controls inevitably led to a fairer assessment of Horthy's life, popular volumes still painted him negatively. Influential biographies openly leveled "ad hominem" attacks at Horthy, accusing him of bastardy, lechery, sadism, greed, nepotism, bloodthirst, warmongering, and cowardice, among other vices.
Reburial and contemporary politics.
The downfall of the communist regime and the rebirth of a free press and academia in Hungary vastly improved Hungarian understanding of the Horthy era. In 1993, only a few years after the first democratic elections, Horthy's body was returned from Portugal to his hometown of Kenderes. Tens of thousands of people, as well as almost the entirety of József Antall's MDF cabinet, attended the ceremony. Antall had prefaced the burial with a series of interviews praising Horthy as a "patriot." The reburial was broadcast on state television and was accompanied by large-scale protests in Budapest.
In contemporary Hungary, hagiography of Horthy is associated with the far-right Jobbik and its allies. Since 2012, Horthy statues, squares, or memorials have been erected in numerous villages and cities including Csókakő, Kereki, Gyömrő, and Debrecen. In November 2013, a Horthy statue's unveiling at a Calvinist church in Budapest drew international attention and criticism.
"Der Spiegel" has written about the resurgence of what its writers call "the Horthy cult," claiming that Horthy's popularity indicates returning irredentist, reactionary, and ultranationalistic elements. Critics have more specifically connected Horthy's popularity to the "Magyar Gárda", a paramilitary group that uses Árpád dynasty imagery and to recent incidents of antiziganist and antisemitic vandalism in Hungary. The ruling Fidesz party has, according to reporters, "hedged its bets" on the Horthy controversy, refusing to outright condemn Horthy statues and other commemorations for fear of losing far-right voters to Jobbik, although some Fidesz politicians have labeled Horthy memorials "provocative." This tension has led some to label Fidesz as "implicitly anti-semitic" and to accuse Prime Minister Viktor Orbán of a "revisionist" agenda.
Left-wing groups such as the Hungarian Socialist Party have condemned positive historiography of Horthy. Attila Mesterházy, the socialist leader, has condemned the Orbán government's position as "inexcusable," claiming that Fidesz was "“openly associating itself with the ideology of the regime that collaborated with the fascists." Words have led to actions in some instances, for example when leftist activist Péter Dániel vandalized a rural bust of Horthy by dousing it in red paint and hanging a sign that read "Mass Murderer — War Criminal" around its neck. Right-wing activists responded by vandalizing a Jewish cemetery in Székesfehérvár.
Film and television portrayals.
In the 1985 NBC TV film "", the role of Horthy was taken by Hungarian-born actor Guy Deghy, who appeared bearded although Horthy (as photographs bore out) appeared consistently clean-shaven throughout his life.
In the 2011 Spanish TV film series, "El ángel de Budapest" (The angel of Budapest), also set during Wallenberg's time in Hungary in 1944, he is portrayed by actor László Agárdi. In the 2014 American action drama film "Walking with the Enemy", Regent Horthy is portrayed by Ben Kingsley. The movie depicts a story of a young man during the Arrow Cross Party takeover in Hungary.

</doc>
<doc id="51310" url="https://en.wikipedia.org/wiki?curid=51310" title="Citroën">
Citroën

Citroën () is a major French automobile manufacturer, part of the PSA Peugeot Citroën group since 1976. Founded in 1919 by French industrialist André-Gustave Citroën (1878–1935).
In 1934, Citroën secured its reputation for innovation with the Traction Avant, not only the world's first mass-produced front-wheel drive car, but also one of the first cars to feature a unitary-type body, with no chassis frame holding the mechanical components.
In 1954 Citroën produced the world's first hydropneumatic self-levelling suspension system, then in 1955 the revolutionary DS, the first mass production car with modern disc brakes. In 1967, Citroën introduced swiveling headlights in several models, allowing for greater visibility on winding roads. Citroën cars have received various international and national-level awards, including three European Car of the Year.
Citroën has a successful history in motorsport, and is the only automobile manufacturer to have won three different official championships from the International Automobile Federation: the World Rally Raid Championship (five times), the World Rally Championship (eight times ), and the World Touring Car Championship.
Citroën has been selling vehicles in China since 1984, and it represents a major market for the brand today, largely via the Dongfeng Peugeot-Citroën joint venture. In 2014, when PSA Peugeot Citroën ran into severe financial difficulties, the Dongfeng Motor Corporation took an ownership stake.
The brand celebrated its 90th Anniversary in 2009. The Citroën's C_42 showroom, on the Champs Elysées in Paris, is a place where Citroën organises some exhibitions, shows its cars and concept cars.
History.
Early years.
André Citroën built armaments for France during World War I; after the war, however, unless he planned ahead he knew he would have a modern factory without a product. There was nothing automatic about the decision to become an automobile manufacturer once the war was finished, but the auto-business was one that Citroën knew well, thanks to a successful six-year stint working with Mors between 1908 and the outbreak of war. The decision to switch to automobile manufacturing was evidently taken as early as 1916 which is when Citroën asked the engineer Louis Dufresne, previously with Panhard, to design a technically sophisticated 18HP automobile for which he could use his factory once peace broke out. Long before that happened, however, he had modified his vision, and decided, (like Henry Ford), that the best post war opportunities in auto-making would involve a lighter car of good quality, but made in sufficient quantities to be priced enticingly. In February 1917 Citroën contacted another engineer, Jules Salomon, who already had a considerable reputation within the French automotive sector as the creator, in 1909, of a little car called Le Zèbre. André Citroën's mandate was characteristically demanding and characteristically simple: to produce an all-new design for a 10 HP car that would be better equipped, more robust and less costly to produce than any rival product at the time. The result was the Type A, announced to the press, just four months after the guns fell silent, in March 1919. The first "production" Type A emerged from the factory at the end of May, and in June it was exhibited at a show room in the Champs-Élysées which normally sold Alda cars. Citroën persuaded the owner of the Alda business, Fernand Charron, to lend him the show-room (just as a few years later Charron would be persuaded to become a major investor in Citroën business). On 7 July 1919 the first customer took delivery of a new Citroën 10HP "Type A".
That same year, André Citroën briefly negotiated with General Motors on a proposed sale of the Citroën company to GM. The deal nearly closed, but GM ultimately decided that its management and capital would be too overstretched by the takeover. thus Citroën remained independent till 1935.
Citroën was a keen marketer—he used the Eiffel Tower as the world's largest advertising sign, as recorded in "Guinness World Records". He also sponsored expeditions in Asia (Croisière Jaune), North America, (Croisière Blanche) and Africa (Croisière Noire) intended to demonstrate the potential for motor vehicles equipped with the Kégresse track system to cross inhospitable regions. The expeditions conveyed scientists and journalists.
Demonstrating extraordinary toughness, a 1923 Citroën that had already travelled was the first car to be driven around Australia. The car, a 1923 Citroën 5CV Type C Torpedo, was driven by Neville Westwood from Perth, Western Australia, on a round trip from August to December 1925. The car is now fully restored and in the collection of the National Museum of Australia.
In 1924, Citroën began a business relationship with American engineer Edward G. Budd. From 1899, Budd had worked to develop stainless steel bodies for railroad cars, for the Pullman in particular. Budd went on to manufacture steel bodies for many automakers, Dodge being his first big auto client. At the Paris Motor Show in October 1924, Citroën introduced the Citroën B10, the first all-steel body in Europe. The cars were initially successful in the marketplace, but soon competitors (who were still using a wooden structure for their bodies) introduced new body designs. Citroën did not redesign the bodies of his cars. Citroëns still sold in large quantities in spite of not changing the body design, but the car's low price was the main selling point and Citroën experienced heavy losses.
In 1927, the bank Lazard helped Citroën by bringing new, much-needed funds as well as by renegotiating its debt—for example, by buying out the SOVAC. It went even further by entering in its capital and being represented at the board. The three directors sent by Lazard were Raymond Philippe, Andre Meyer, and Paul Frantzen.
André Citroën perceived the need to differentiate his product, to avoid the low price competition surrounding his conventional rear drive models in the late 1920s/early 1930s. In 1933, Citroën introduced the Rosalie, the first commercially available passenger car with a diesel engine, developed with Harry Ricardo.
Traction Avant and Michelin ownership.
Traction Avant.
The Traction Avant is a car that pioneered mass production of three revolutionary features that are still in use today: a unitary body with no separate frame, four-wheel independent suspension, and front-wheel drive. The vast majority of cars for many decades were similar in conception to the Ford Model T – a body bolted onto a ladder frame which held all the mechanical elements of the car, a solid rear axle that rigidly connected the rear wheels, and rear wheel drive. The "Model T school" of automobile engineering proved popular, because it was thought to be cheap to build, but it did pose dynamic defects as cars became more capable, and resulted in a heavier car, which is why cars today are more like the Traction Avant than the Model T under the skin.
Citroën commissioned the American Budd Company to create a prototype, which evolved into the 7-horsepower (CV), Traction Avant of 1934.
Achieving quick development of the Traction Avant, tearing down and rebuilding the factory (in five months), and the extensive marketing efforts were investments that were too costly for Citroën to do all at once, causing the financial ruin of the company. In December 1934, despite the assistance of the Michelin company, Citroën filed for bankruptcy. Within the month, Michelin, already the car manufacturer's largest creditor, became its principal shareholder. Fortunately for Michelin, the technologically advanced Traction Avant met with market acceptance, and the basic philosophy of cutting edge technology used as a differentiator continued until the late 1990s. Pierre Michelin became the chairman of Citroën. Pierre-Jules Boulanger became the vice-president of Citroën and chief of the engineering and design department.
In 1935, founder André Citroën died from stomach cancer.
Research breakthroughs.
Pierre-Jules Boulanger had been a First World War air reconnaissance photography specialist with the French Air Force. He was capable and effective and finished the war having risen to the rank of captain. He was also courageous, having been decorated with the Military Cross and the Legion of Honour. He started working for Michelin in 1918, reporting directly to Édouard Michelin, co-director and founder of the business. Boulanger joined the Michelin board in 1922. He became president of Citroën in 1937 after the death of Édouard and kept his position until his death in 1950. In 1938, he also became Michelin's joint managing director.
During the German occupation of France in World War II Boulanger refused to meet Dr. Ferdinand Porsche or communicate with the German authorities except through intermediaries. He organized a "go slow" on production of trucks for the Wehrmacht, many of which were sabotaged at the factory, by putting the notch on the oil dipstick in the wrong place, resulting in engine seizure. In 1944 when the Gestapo headquarters in Paris was sacked by the French Resistance, his name was prominent on a Nazi blacklist of the most important "enemies of the Reich" to be arrested in the event of an allied invasion of France.
Citroën researchers, including Paul Magès, continued their work in secret, against the express orders of the Germans, and developed the concepts that were later brought to market in three remarkable vehicles – a small car (2CV), a delivery van (Type H), and a large, swift family car (DS). These were widely regarded by contemporary journalists as avant garde, even radical, solutions to automotive design.
This began a decades long period of unusual brand loyalty normally seen in the automobile industry only in niche brands, like Porsche and Ferrari.
The Deux Chevaux.
Citroën unveiled the 2CV—signifying two fiscal horsepower, initially only —at the Paris Salon in 1948. The car became a bestseller, achieving the designer's aim of providing rural French people with a motorized alternative to the horse. It was unusually inexpensive to purchase and with its tiny two cylinder engine, inexpensive to run as well. The 2CV pioneered a very soft, interconnected suspension, but did not have the more complex self-levelling feature. This car remained in production, with only minor changes, until 1990 and was a common sight on French roads until recently. 8.8 Million 2CV variants were produced in the period 1948–1990.
The Goddess.
1955 saw the introduction of the DS, the first full usage of Citroën's now legendary hydropneumatic self-levelling suspension system, tested on the rear suspension of the Traction in 1954.
The DS was the first production car with modern disc brakes.
A single high-pressure hydraulic system was used to activate the power steering, suspension, and brakes. Brakes were power assisted, to multiply force applied by the driver.
On the Citromatic (semi-automatic transmission) version, the system also operated the clutch, through a system of pistons in the gearbox cover to shift the gears in the transmission.
From 1968, the DS also introduced directional headlights, that moved with the steering, improving visibility at night.
The streamlined car was remarkable for its era, and had a remarkable sounding name – in French, "DS" is pronounced , the same as , which means "Goddess". The DS placed third in the 1999 Car of the Century competition.
High pressure hydraulics.
This high-pressure hydraulic system would form the basis of over 9 million Citroën cars, including the DS, SM, GS, CX, BX, XM, Xantia, C5, and C6. Self-levelling suspension is the principal user benefit – the car maintains a constant ride height above the road regardless of passenger and cargo load, despite the very soft suspension. This type of suspension is uniquely able to absorb road irregularities without disturbing the occupants. It is often compared to riding on a "magic carpet" for this reason.
These vehicles shared the distinguishing feature of rising to operating ride height when the engine was turned on, like a "mechanical camel" (per "Car & Driver" magazine). A lever beside the driver's seat allowed the driver to adjust the height of the car, later replaced with an electronic switch. The height adjustability of the suspension allows for clearing obstacles, fording shallow (slow-moving) streams, and changing tires.
Citroën was undercapitalised, so its vehicles had a tendency to be underdeveloped at launch, with limited distribution and service networks outside France. As a result, early DS models experienced teething issues with the novel suspension. The hydropneumatics were sorted out and became reliable.
Licensing such a technological leap forward was pursued to a limited extent – in 1965, the Rolls-Royce Silver Shadow included this suspension. The 1963 Mercedes-Benz 600 and Mercedes-Benz 300SEL 6.3 tried to replicate the advantages with a costly, complex, and expensive to maintain air suspension that avoided the Citroën patented technology. By 1975, the Mercedes-Benz 450SEL 6.9 could finally be produced with the proven hydropneumatic suspension, and Mercedes-Benz continues to offer variations on this technology today.
During Citroën's 1968–75 venture with Maserati, the Citroën high-pressure hydraulic system was used on several Maserati models, for power clutch operation (Bora), power pedal adjustment (Bora), pop-up headlights (Bora, Merak), brakes (Bora, Merak, Khamsin), steering (Khamsin), and the entire Quattroporte II prototype, which was a four-door Citroën SM under the skin.
Aerodynamic pioneer.
Citroën was one of the early pioneers of the now widespread trend of aerodynamic automobile design, which helps to reduce fuel consumption and improve high-speed performance by reducing wind resistance. The cruising speed was the same as the top speed because of these efforts – the DS could happily run at 100 mph without disturbing the occupants. The firm began using a wind tunnel in the 1950s, enabling them to create highly streamlined cars such as the DS that were years ahead of their time. So good were the aerodynamics of the CX that it took its name from the term used to measure drag coeffient: formula_1.
Expansion and financial challenges.
In the 1960s, Citroën undertook a series of financial and development maneuvers, aiming to build on its strength in the 1950s with the successful, 2CV, Type H, and DS models. Since Citroën went bankrupt in 1974, the effectiveness of these maneuvers is not clear.
The maneuvers were to address two key gaps facing the company.
The first was the lack of a midsize car between its own range of very small, cheap cars (2CV/Ami) and large, expensive cars (DS/ID). In today's terms, this would be similar to a brand consisting only of the Tata Nano and Mercedes-Benz E-Class. Because of the potential volume, the midsize segment was the most profitable part of the car market, and in 1965 the "Citroënesque" Renault 16 stepped in to address it.
The second large issue was the lack of a powerful engine suitable for export markets. The post-WW2 Tax horsepower system in France was steeply progressive, and vehicles over 2.0 (later 2.8) liters of engine displacement faced a heavy, annual tax. The result was that cars made in France were considered underpowered outside France.
For both the 1955 DS and 1974 CX models, development of the original engine around which the design was planned proved too expensive for the finances available, and the actual engine used in both cases was a modest and outdated four-cylinder design.
In 1963, Citroën negotiated with Peugeot to cooperate in the purchase of raw materials and equipment. Talks were broken off in 1965.
In 1964, Citroën partnered with NSU Motorenwerke to develop the Wankel engine via the Comobil (later Comotor) subsidiary. For Citroën, this represented the chance for a technological end run around the French Tax horsepower system – producing a more powerful car while maintaining a small engine. The first production car developed 106 hp with a 1-liter engine, while the standard GS delivered 55 hp with a 1-liter engine.
In 1965, Citroën took over the French carmaker Panhard in the hope of using Panhard's expertise in midsize cars. Cooperation between the two companies had begun 12 years earlier, and they had agreed to a partial merger of their sales networks in 1953. Panhard ceased making vehicles in 1967.
In 1967, Citroën purchased the truck manufacturer Berliet.
In 1968, Citroën purchased the Italian sports car maker Maserati, again with an eye to producing a more powerful car while maintaining a small engine in line with the French tax horsepower system. The first production car developed 170 hp with a 2.7 liter engine.
This was the 1970 SM, which featured a V6 Maserati engine, hydropneumatic suspension, and a fully powered, self-centering steering system called DIRAVI. The SM was engineered as if it were replacing the DS family car, a level of investment the small luxury Grand Touring car sector alone would never be able to support, even in the best of circumstances.
The year 1968 also saw a restructuring of Citroën's worldwide operations under a new holding company, Citroën SA. Michelin, Citroën's longtime controlling shareholder, sold a 49% stake to Fiat in what was referred to as the "PARDEVI" agreement (Participation et Développement Industriels).
From a model range perspective, the 1970s started well for Citroën, supported by the successful launch of the long-awaited midsize Citroën GS, finally filling the huge gap between the 2CV and the DS – with a 1-liter, hydropneumatically suspended car. The GS went on to sell 2.5 million units. 601,918 cars were produced just in 1972, up from 526,443 in 1971, and enough to lift the company past Peugeot into second place among French auto-makers when ranked by volume of units.
Circumstances became more unfavorable for Citroën as the 1970s progressed.
In 1973, Fiat sold its 49% stake in the "PARDEVI" holding company (that owned Citroën) back to Michelin. The Citroën and Fiat joint announcement indicated that benefits foreseen for their union in 1968 had failed to materialise. This was not in line with the tire company's long term strategy of ending involvements in the car manufacturing business, creating a very unstable ownership situation.
Citroën suffered a financial blow with the 1973 energy crisis. While some models sold well (peak production of the 2CV and its derivatives was 1974), the bets on Comotor and Maserati both had what was now clearly a flaw – high fuel consumption engines.
The teams of Charles Marchetti and Citroën began working together on the development of the helicopter.
In 1974, the carmaker withdrew from North America due to U.S. design regulations that outlawed core features of Citroën cars (see Citroën SM).
Huge losses at Citroën were caused by failure of the Comotor rotary engine venture, plus the strategic error of going the from 1955 to 1970 without a model in the profitable middle range of the European market, and the massive development costs for the GS, GS Birotor, CX, SM, Maserati Bora, Maserati Merak, Maserati Quattroporte II, and Maserati Khamsin models— each a technological marvel in its own right.
40 years after the bankruptcy related to the Traction Avant, Citroën went bankrupt again, losing its existence as an independent entity, selling Berliet and Maserati, and closing Comotor.
PSA Peugeot Citroën era.
The French government feared large job losses, due to the poor cash flow situation at Citroën and the unstable ownership structure. It arranged talks between Michelin and Citroën and decided to merge Automobiles Citroën and Automobiles Peugeot into a single company. A year after the break with Fiat, on 24 June 1974, Citroën announced their new partnership, this time with Peugeot. Michelin agreed to transfer control of the business to Peugeot.
In December 1974 Peugeot S.A. acquired a 38.2% share of Citroën. On 9 April 1976 they increased their stake of the then bankrupt company to 89.95%, thus creating the "PSA Group" (where PSA is short for Peugeot Société Anonyme), becoming PSA Peugeot Citroën.
Citroën sold off Maserati to De Tomaso in May 1975, and the Italian firm was able to exploit the sales potential of the models and technology developed by Citroën, as well as exploit the image of the Maserati brand in a downward brand extension to sell 40,000 of the newly designed Bi-Turbo models. Berliet was sold to Renault.
The PSA venture was a financial success from 1976 to 1979. Citroën had two successful new designs in the market at this time (the GS and CX), a resurgent Citroën 2CV, and the Citroën Dyane in the wake of the oil crisis, and Peugeot was typically prudent in its own finances, launching the Peugeot 104 based Citroën Visa and Citroën LNA.
PSA then purchased the ageing assets and substantial liabilities of Chrysler Europe for $1, leading to losses from 1980 to 1985. Since PSA needed a new brand for the Chrysler cars, it resurrected the Talbot name.
Trade union problems.
In the early 1980s Citroën was targeted by union action. Events led to a mass demonstration in the streets of Paris on 25 May 1982. Approximately 27,000 Citroën workers demonstrated in affirmation of their wish to work at a company which was being picketed by striking workers who had been blocking access to the factories for four weeks. The demonstrations were successful and six days later work at the plants resumed. Jacques Lombard, one of the company’s senior managers, had gone public with his concerns criticising the strikes.
Taming the innovative spirit.
PSA gradually diluted Citroën's ambitious attitude to engineering and styling in an effort to rebrand the marque to appeal to a wider market. In the 1980s, Citroën models became increasingly Peugeot-based, following the worldwide motor industry trend called "platform sharing." The 1982 BX used the hydropneumatic suspension system and still had a "Citroën-esque" appearance, while being powered by Peugeot-derived engines and using the floorpan later seen on the Peugeot 405. By the late 1980s, many of the distinctive features of the marque had been removed or diluted—conventional Peugeot's switchgear replaced Citroën's quirky but ergonomic "Lunule" designs, complete with self-cancelling indicators that Citroën had previously refused to adopt on ergonomic grounds. While the cars were more banal, sales continued steadily.
Geographical expansion.
Citroën expanded into many new geographic markets. In the late 1970s, the firm developed a small car for production in Romania known as the Oltcit, which it sold in Western Europe as the Citroën Axel. That joint venture has ended, but a new one between PSA and Toyota is now producing cars like the Citroën C1 in the Czech Republic.
In China, Citroën began selling cars in 1984 – today it is a major overseas market. The current range of family cars that includes the C3 and Xsara and locally designed cars like the Fukang and Elysée models. The Citroën brand recently increased its Chinese sales by 30% in an overall market growth of 11% and ranks highest in the 2014 customer satisfaction survey by JD Power in China.
Citroën is a global brand except in North America, where the company has not returned since the SM was effectively banned in 1974 for not meeting U.S. National Highway Traffic Safety Administration (NHTSA) bumper height regulations.
Recent decades.
From 2003–10, Citroën produced the C3 Pluriel, an unusual convertible with allusions to the 1948–90 2CV model, both in body style (such as the bonnet) and in its all-round practicality.
In 2001, Citroën acknowledged its history of innovation when it opened a museum of its many significant vehicles – the 'Conservatoire,' with 300 cars.
With the severe decline in European auto sales after 2009, worldwide sales of vehicles declined from 1,460,373 in 2010 to 1,435,688 in 2011, with 961,156 of these sold in Europe.
Dongfeng Peugeot-Citroën continues growing rapidly, and ranks highest in the 2014 customer satisfaction survey by JD Power in China, above luxury brands like Mercedes and BMW, and above mass market brands like Volkswagen ranking only thirteenth and seventeenth. On the 10 first months of 2014 in China, the sales of Donfeng Citroen cars increased by 30% in an overall market growth of 11%. The launch of the Citroën C3-XR in December 2014 will enable Citroën to continue its growth in 2015.
Despite the near death financial experience of PSA Peugeot Citroën in 2014, and financial rescue by Dongfeng Motors, the "Citroën" and "DS" brands are both planned to grow.
Since 2013, the model Carolina "Pampita" Ardohaín represents Citroën and its life style in some fashion films.
DS brand.
Citroën announced in early 2009 the development of a premium sub-brand DS, for Different Spirit or Distinctive Series (although the reference to the historical Citroën DS is evident), to run in parallel to its mainstream cars. The slogan of the DS car marque is "Spirit of avant-garde".
This new series of cars started with the Citroën DS3 in early 2010, a small car based on the floor plan of the new C3. The Citroën DS3 is based on the concept of the produced Citroën C3 Pluriel model and the Citroën DS Inside concept car. The Citroën DS3 is customisable with various roof colours that can contrast with the body panels. The Citroën DS3 was named 2010 "Car of the Year" by "Top Gear Magazine", awarded first supermini four times in a row by the JD Power Satisfaction Survey UK and second most efficient supermini (Citroën DS3 1.6 e-HDi 115 Airdream : True MPG 63.0mpg) by "What car ?" behind the Citroën C3. In 2013, the Citroën DS3 was again the most sold premium subcompact car with 40% of these market shares in Europe, validating the business model of this product development.
The DS series is deeply connected to Citroën, as the DS4, launched in 2010, is based on the 2008 Citroën Hypnos concept car and the DS5, following in 2011, is based on the 2005 Citroën C-SportLounge concept car.
Their rear badge is a new DS logo rather than the familiar Citroën double chevron, and all will have markedly different styling from their equivalent sister car. Citroën has produced several dramatic-looking concept sports cars of late with the fully working Citroën Survolt being badged as a DS. Indeed, the 2014 DS Divine concept car develops the Citroën Survolt prototype as the future sport coupé of the DS range.
In China, Citroën has "stand alone" DS sale rooms, including vehicle plants built for the production of these vehicles. Since 2014, Citroën sells the Chinese built DS 5LS and DS 6WR in China.
Awards.
Citroën was recognized in the 1999 Car of the Century competition as producing the third most influential car of the 20th century, the Citroën DS, which trailed only the Ford Model T and BMC Mini.
Citroën has produced three winners of the 50-year-old European Car of the Year award, and many rated second or third place.
Citroën has produced one winner of the United States Motor Trend Car of the Year award – the original Car of the Year designation, which began in 1949. This was especially significant because this award previously was only given to cars designed and built in the United States.
Citroën has produced eight "Auto Europa" winners in 28 years, since 1987. "Auto Europa" is the prize awarded by the jury of the Italian Union of Automotive Journalists (UIGA), which annually celebrates the best car produced at least at 10,000 units in the 27 countries of the European Union: Citroën XM(1990), Citroën ZX (1992), Citroën Xantia (1994), Citroën Xsara Picasso (2001), Citroën C5 (2002), Citroën C3 (2003), Citroën C4 (2005) and Citroën DS4 (2012).
Citroën Racing.
Citroën Racing, previously known as Citroën Sport and before that as Citroën Competitions, is the team responsible for Citroën's sporting activities. It is a successful winning competitor in the World Rally Championship and in the World Touring Car Championship.
Early rally wins for Citroën vehicles.
Citroën vehicles were entered in endurance rally driving events beginning in 1956, with the introduction of the DS. The brand was successful and won many key events over a decades long period, with what was essentially the same production car design.
Racing the 2CV.
Citroën discovered that while racing the uniquely slow 2CV against other cars made little sense, they could be interesting to watch racing against each other. Citroën Competitions sponsored three long distance competitions – Paris-Kaboul-Paris in 1970, Paris-Persepolis-Paris in 1972, and Raid Afrique in 1973.
Enthusiasts carried on the tradition with "2CV Cross" – a group of 2CV's racing around a dirt track – a sport that continues today.
Rebuilding the competition group.
The Citroën Competitions division was impacted negatively by the firm's 1974 bankruptcy.
Competitive rallying was also changing – away from standard production cars to specially developed low volume models. In response to the entry of the competitive short wheel base Group B 4 wheel drive Audi Quattro into rallying, Citroën developed the heavily modified Group B Citroën BX 4TC in 1986.
The team returned successfully with the Citroën ZX Rally Raid to win the Rally Raid Manufacturer's Championship five times (1993, 1994, 1995, 1996, and 1997) with Pierre Lartigue and Ari Vatanen. Citroën Racing won the Dakar Rally four times, in 1991, continuing the serial of four victories of Peugeot sport, and then again in 1994, 1995, and 1996.
From 2001, the Citroën Racing team returned successfully to the World Rally Championship, winning eight times the Manufacturer's Title, continuing the serial of three WRC Championships victories of Peugeot sport, in 2003, 2004, 2005, 2008, 2009, 2010, 2011 and 2012. The Citroën WRC Team pilot Sébastien Loeb also won nine Drivers' Championships. In 2004, 2005, and 2006, the French pilot won the Drivers' Championship, driving the Citroën Xsara WRC, in 2007, 2008, 2009 and 2010 with the Citroën C4 WRC, and in 2011 and 2012 with the new Citroën DS3 WRC.
The Citroën World Rally Team has a record of 97 victories in the World Rally Championship. In 2014, Citroën was the automaker that won the most world championship titles: 14 World Champion titles in 15 appearances. Citroën won the World Rally Raid Championship 5 times, the World Rally Championship 8 times, and the World Touring Car Championship in its first participation.
New competition division for touring cars.
In 2013, Citroën Racing created a new sub-division, the Citroën World Touring Car Team, in order to attempt the 2014 World Touring Car Championship. The name "Citroën C-Elysée WTCC" has been chosen for the race car running in this world competition. It was developed in a few months, thanks to the experience of the Citroën World Rally Team. Citroën revealed a thirty-minute film on its Internet channel, to show the different steps to the C-Elysée project development : Projet M43 WTCC, Citroën WTCC 2014.
The Citroën World Touring Car Team won fourteen victories out of the fifteen first races of the 2014 WTCC season, in spite of the handicap of the 60 kg Compensation Weight put to the leading cars. The Citroën/Total WTCC Team won the "Manufacturer's WTCC Championship", 5 races before the end of the season, after the 2014 Shanghai race, where Citroën won first, second, third and fourth place, and recorded the fastest lap time. The Citroën World Touring Car Team pilots also got the three first ranks of the Drivers' World Touring Car Championship.
Concept cars.
Citroen has produced numerous concept cars over the decades, previewing future design trends or technologies. Notable concepts include the Citroën Karin (1980), Citroën Activa (1988), Citroën C-Métisse (2006), GT by Citroën (2008) and Citroën Survolt (2010).
Logo.
The origin of the logo may be traced back to a trip made by the 22-year-old André Citroën to Łódź city, Poland, where he discovered an innovative design for a chevron-shaped gear used in milling. He bought the patent for its application in steel. Mechanically a gear with helical teeth produces an axial force. By adding a second helical gear in opposition, this force is cancelled. The two chevrons of the logo represent the intermeshing contact of the two.
The presentation of the logo has evolved over time. Before the war, it was rendered in yellow on a blue background. After the war, the chevrons became more subtle herringbones, usually on a white background. With the company searching for a new image during the 1980s, the logo became white on red to give an impression of dynamism, emphasized by publicity slogan.
In February 2009 Citroën launched a new brand identity to celebrate its 90th anniversary, replacing the 1977 design. The new logo was designed by Landor Associates — a 3D metallic variation of the double chevron logo accompanied by a new font for the Citroën name and the new slogan "Créative Technologie". A TV campaign reminiscing over of Citroën was commissioned to announce the new identity to the public. The new look is currently being rolled out to dealers globally and is expected to take three to five years.
Factories.
Some joint venture models are manufactured in third party or joint venture factories, including:

</doc>
<doc id="51314" url="https://en.wikipedia.org/wiki?curid=51314" title="History of Antarctica">
History of Antarctica

The history of Antarctica emerges from early Western theories of a vast continent, known as Terra Australis, believed to exist in the far south of the globe. The term "Antarctic", referring to the opposite of the Arctic Circle, was coined by Marinus of Tyre in the 2nd century AD.
The rounding of the Cape of Good Hope and Cape Horn in the 15th and 16th centuries proved that "Terra Australis Incognita" ("Unknown Southern Land"), if it existed, was a continent in its own right. In 1773 James Cook and his crew crossed the Antarctic Circle for the first time but although they discovered nearby islands, they did not catch sight of Antarctica itself. It is believed he was as close as 150 miles from the mainland.
In 1820, several expeditions claimed to have been the first to have sighted the ice shelf or the continent. A Russian expedition was led by Fabian Gottlieb von Bellingshausen and Mikhail Lazarev, a British expedition was captained by Edward Bransfield and an American sealer Nathaniel Palmer participated. The first landing was probably just over a year later when American Captain John Davis, a sealer, set foot on the ice.
Several expeditions attempted to reach the South Pole in the early 20th century, during the 'Heroic Age of Antarctic Exploration'. Many resulted in injury and death. Norwegian Roald Amundsen finally reached the Pole on December 14, 1911, following a dramatic race with the Englishman Robert Falcon Scott.
Early exploration.
The search for "Terra Australis Incognita".
In the Western world, belief in a Cold Land—a vast continent located in the far south of the globe to "balance" out the northern lands of Europe, Asia and North Africa—had existed for centuries. Aristotle had postulated a symmetry of the earth, which meant that there would be equally habitable lands south of the known world. The Greeks suggested that these two hemispheres, north and south, were divided by a "belt of fire", due to the general observation that the climate got warmer and warmer the further south someone travelled, and no Europeans had gone past the equator to see that this was not the case.
It was not until Prince Henry the Navigator began in 1418 to encourage the penetration of the torrid zone in the effort to reach India by circumnavigating Africa that European exploration of the southern hemisphere began. In 1473 Portuguese navigator Lopes Gonçalves proved that the equator could be crossed, and cartographers and sailors began to assume the existence of another, temperate continent to the south of the known world.
The doubling of the Cape of Good Hope in 1487 by Bartolomeu Dias first brought explorers within touch of the Antarctic cold, and proved that there was an ocean separating Africa from any Antarctic land that might exist.
Ferdinand Magellan, who passed through the Straits of Magellan in 1520, assumed that the islands of Tierra del Fuego to the south were an extension of this unknown southern land, and it appeared as such on a map by Ortelius: "Terra australis recenter inventa sed nondum plene cognita" ("Southern land recently discovered but not yet fully known").
European geographers connected the coast of Tierra del Fuego with the coast of New Guinea on their globes, and allowing their imaginations to run riot in the vast unknown spaces of the south Atlantic, south Indian and Pacific oceans they sketched the outlines of the "Terra Australis Incognita" ("Unknown Southern Land"), a vast continent stretching in parts into the tropics. The search for this great south land or Third World was a leading motive of explorers in the 16th and the early part of the 17th centuries.
In 1599, according to the account of Jacob le Maire, the Dutch Dirck Gerritsz Pomp observed mountainous land at latitude (64°). If so, these were the South Shetland Islands, and possibly the first European sighting of Antarctica (or offshore-lying islands belonging to it). Other accounts, however, do not note this observation, casting doubt on their accuracy. It has been argued that the Spaniard Gabriel de Castilla claimed to have sighted "snow-covered mountains" beyond the 64° S in 1603, but this claim is not generally recognized.
Quirós in 1606 took possession for the king of Spain all of the lands he had discovered in Australia del Espiritu Santo (the New Hebrides) and those he would discover "even to the Pole".
Francis Drake like Spanish explorers before him had speculated that there might be an open channel south of Tierra del Fuego. Indeed, when Schouten and Le Maire discovered the southern extremity of Tierra del Fuego and named it Cape Horn in 1615, they proved that the Tierra del Fuego archipelago was of small extent and not connected to the southern land.
Finally, in 1642 Tasman showed that even New Holland (Australia) was separated by sea from any continuous southern continent. Voyagers round the Horn frequently met with contrary winds and were driven southward into snowy skies and ice-encumbered seas; but so far as can be ascertained none of them before 1770 reached the Antarctic Circle, or knew it, if they did.
South of the Antarctic Convergence.
The visit to South Georgia by the English merchant Anthony de la Roché in 1675 was the first ever discovery of land south of the Antarctic Convergence . Soon after the voyage cartographers started to depict ‘Roché Island’, honouring the discoverer.
James Cook was aware of la Roché's discovery when surveying and mapping the island in 1775.
Edmond Halley's voyage in HMS "Paramour" for magnetic investigations in the South Atlantic met the pack ice in 52° S in January 1700, but that latitude (he reached 140 mi off the north coast of South Georgia) was his farthest south. A determined effort on the part of the French naval officer Jean-Baptiste Charles Bouvet de Lozier to discover the "South Land" – described by a half legendary "sieur de Gonneyville" – resulted in the discovery of Bouvet Island in 54°10′ S, and in the navigation of 48° of longitude of ice-cumbered sea nearly in 55° S in 1730 .
In 1771, Yves Joseph Kerguelen sailed from France with instructions to proceed south from Mauritius in search of "a very large continent." He lighted upon a land in 50° S which he called South France, and believed to be the central mass of the southern continent. He was sent out again to complete the exploration of the new land, and found it to be only an inhospitable island which he renamed the Isle of Desolation, but which was ultimately named after him.
The Antarctic Circle.
The obsession of the undiscovered continent culminated in the brain of Alexander Dalrymple, the brilliant and erratic hydrographer who was nominated by the Royal Society to command the Transit of Venus expedition to Tahiti in 1769. The command of the expedition was given by the admiralty to Captain James Cook. Sailing in 1772 with the "Resolution", a vessel of 462 tons under his own command and the "Adventure" of 336 tons under Captain Tobias Furneaux, Cook first searched in vain for Bouvet Island, then sailed for 20 degrees of longitude to the westward in latitude 58° S, and then 30° eastward for the most part south of 60° S, a higher southern latitude than had ever been voluntarily entered before by any vessel. On 17 January 1773 the Antarctic Circle was crossed for the first time in history and the two ships reached by , where their course was stopped by ice.
Cook then turned northward to look for French Southern and Antarctic Lands, of the discovery of which he had received news at Cape Town, but from the rough determination of his longitude by Kerguelen, Cook reached the assigned latitude 10° too far east and did not see it. He turned south again and was stopped by ice in by 95° E and continued eastward nearly on the parallel of 60° S to 147° E. On 16 March, the approaching winter drove him northward for rest to New Zealand and the tropical islands of the Pacific. In November 1773, Cook left New Zealand, having parted company with the "Adventure", and reached 60° S by 177° W, whence he sailed eastward keeping as far south as the floating ice allowed. The Antarctic Circle was crossed on 20 December and Cook remained south of it for three days, being compelled after reaching to stand north again in 135° W.
A long detour to served to show that there was no land connection between New Zealand and Tierra del Fuego. Turning south again, Cook crossed the Antarctic Circle for the third time at before his progress was once again blocked by ice four days later at by . This point, reached on 30 January 1774, was the farthest south attained in the 18th century. With a great detour to the east, almost to the coast of South America, the expedition regained Tahiti for refreshment. In November 1774, Cook started from New Zealand and crossed the South Pacific without sighting land between 53° and 57° S to Tierra del Fuego; then, passing Cape Horn on 29 December, he rediscovered Roché Island renaming it Isle of Georgia, and discovered the South Sandwich Islands (named "Sandwich Land" by him), the only ice-clad land he had seen, before crossing the South Atlantic to the Cape of Good Hope between 55° and 60°. He thereby laid open the way for future Antarctic exploration by exploding the myth of a habitable southern continent. Cook's most southerly discovery of land lay on the temperate side of the 60th parallel, and he convinced himself that if land lay farther south it was practically inaccessible and of no economic value.
First sighting.
The first land south of the parallel 60° south latitude was discovered by the Englishman William Smith, who sighted Livingston Island on 19 February 1819. A few months later Smith returned to explore the other islands of the South Shetlands archipelago, landed on King George Island, and claimed the new territories for Britain.
The first confirmed sighting of mainland Antarctica cannot be accurately attributed to one single person. It can, however, be narrowed down to three individuals. According to various sources, three men all sighted the ice shelf or the continent within days or months of each other: Fabian Gottlieb von Bellingshausen, a captain in the Russian Imperial Navy; Edward Bransfield, a captain in the Royal Navy; and Nathaniel Palmer, an American sealer out of Stonington, Connecticut.
It is certain that the expedition, led by von Bellingshausen and Lazarev on the ships "Vostok" and "Mirny", reached on 28 January 1820 a point within from Princess Martha Coast and recorded the sight of an ice shelf at that became known as the Fimbul ice shelf. On 30 January 1820, Bransfield sighted Trinity Peninsula, the northernmost point of the Antarctic mainland, while Palmer sighted the mainland in the area south of Trinity Peninsula in November 1820. Von Bellingshausen's expedition also discovered Peter I Island and Alexander I Island, the first islands to be discovered south of the circle.
Exploration.
Only slightly more than a year later, the first landing on the Antarctic mainland was arguably by the American Captain John Davis, a sealer, who claimed to have set foot there on 7 February 1821, though this is not accepted by all historians.
In December 1821, Nathaniel Palmer, an American sealer looking for seal breeding grounds, sighted what is now known as the Antarctic Peninsula, located between 55 and 80 degrees west. In 1823, James Weddell, a British sealer, sailed into what is now known as the Weddell Sea.
Charles Wilkes, as commander of a United States Navy expedition in 1840, discovered what is now known as Wilkes Land, a section of the continent around 120 degrees East.
After the North Magnetic Pole was located in 1831, explorers and scientists began looking for the South Magnetic Pole. One of the explorers, James Clark Ross, a British naval officer, identified its approximate location, but was unable to reach it on his trip in 1841. Commanding the British ships "Erebus" and "Terror", he braved the pack ice and approached what is now known as the Ross Ice Shelf, a massive floating ice shelf over high. His expedition sailed eastward along the southern Antarctic coast discovering mountains which were since named after his ships: Mount Erebus, the most active volcano on Antarctica, and Mount Terror.
The first documented landing on the mainland of East Antarctica was at Victoria Land by the American sealer Mercator Cooper on 26 January 1853.
These explorers, despite their impressive contributions to South Polar exploration, were unable to penetrate the interior of the continent and, rather, formed a broken line of discovered lands along the coastline of Antarctica. What followed this period of Antarctic interest is what historian H.R. Mill called 'the age of averted interest'. Following the expedition South by the ships "Erebus" and "Terror" under James Clark Ross (January, 1841), he suggested that there were no scientific discoveries, or 'problems', worth exploration in the far South.
It is considered that Ross' influence, as well as the loss of the Franklin expedition in the Arctic, lead to disinterest in polar interest, particularly by the Royal Society - the organization that helped oversee many Arctic explorations, including those that would be made by Shackleton and Scott. However, in the following twenty years after Ross' return, there was a general lull internationally in Antarctic exploration.
Heroic Age of Antarctic Exploration.
The Heroic Age of Antarctic Exploration began at the end of the 19th century and closed with Ernest Shackleton's Imperial Trans-Antarctic Expedition in 1917.
During this period the Antarctic continent became the focus of an international effort that resulted in intensive scientific and geographical exploration and in which 17 major Antarctic expeditions were launched from ten countries.
Origins.
The initial impetus for the Heroic Age of Antarctic Exploration was a lecture given by Dr. John Murray entitled "The Renewal of Antarctic Exploration", given to the Royal geographical Society in London, November 27, 1893. Murray advocated that research into the Antarctic should be organised to "resolve the outstanding geographical questions still posed in the south". Furthermore, the Royal Geographic Society instated an Antarctic Committee shortly prior to this, in 1887, which successfully encouraged many whalers to explore the Southern regions of the world and laid the groundwork for the lecture given by Murray.
In August 1895 the Sixth International Geographical Congress in London passed a general resolution calling on scientific societies throughout the world to promote the cause of Antarctic exploration "in whatever ways seem to them most effective". Such work would "bring additions to almost every branch of science". The Congress had been addressed by the Norwegian Carsten Borchgrevink, who had just returned from a whaling expedition during which he had become one of the first to set foot on the Antarctic mainland. During his address, Borchgrevink outlined plans for a full-scale pioneering Antarctic expedition, to be based at Cape Adare.
The Heroic Age was inaugurated by an expedition launched by the Belgian Geographical Society in 1897; Borchgrevink followed a year later with a privately sponsored British expedition. (Some histories consider the "Discovery" expedition, which departed in 1901, as the first proper expedition of the Heroic Age.)
The Belgian Antarctic Expedition was led by Belgian Adrian de Gerlache. In 1898, they became the first men to spend winter on Antarctica, when their ship Belgica became trapped in the ice. They became stuck on 28 February 1898, and only managed to get out of the ice on 14 March 1899.
During their forced stay, several men lost their sanity, not only because of the Antarctic winter night and the endured hardship, but also because of the language problems between the different nationalities. This was the first expedition to overwinter within the Antarctic Circle.
Early British expeditions.
The Southern Cross Expedition began in 1898 and lasted for two years. This was the first expedition to overwinter on the Antarctic mainland (Cape Adare) and was the first to make use of dogs and sledges. It made the first ascent of The Great Ice Barrier, (The Great Ice Barrier later became formally known as the Ross Ice Shelf). The expedition set a Farthest South record at 78°30'S. It also calculated the location of the South Magnetic Pole.
The Discovery Expedition was then launched, from 1901–04 and was led by Robert Falcon Scott. It made the first ascent of the Western Mountains in Victoria Land, and discovered the polar plateau. Its southern journey set a new Farthest South record, 82°17'S. Many other geographical features were discovered, mapped and named. This was the first of several expeditions based in McMurdo Sound.
A year later, the Scottish National Antarctic Expedition was launched, headed by William Speirs Bruce. 'Ormond House' was established as a meteorological observatory on Laurie Island in the South Orkneys and was the first permanent base in Antarctica. The Weddell Sea was penetrated to 74°01'S, and the coastline of Coats Land was discovered, defining the sea's eastern limits.
Ernest Shackleton, who had been a member of Scott's expedition, organized and led the Nimrod Expedition from 1907 to 1909. The expedition's primary objective was of reaching the South Pole. Based in McMurdo Sound, the expedition pioneered the Beardmore Glacier route to the South Pole, and the (limited) use of motorised transport. Its southern march reached 88°23'S, a new Farthest South record 97 geographical miles from the Pole before having to turn back. During the expedition, Shackleton was the first to reach the polar plateau. Parties led by T. W. Edgeworth David also became the first to climb Mount Erebus and to reach the South Magnetic Pole.
Expeditions from other countries.
The First German Antarctic Expedition was sent to investigate eastern Antarctica in 1901. It discovered the coast of Kaiser Wilhelm II Land, and Mount Gauss. The expedition's ship became trapped in ice, however, which prevented more extensive exploration.
The Swedish Antarctic Expedition, operating at the same time worked in the east coastal area of Graham Land, and was marooned on Snow Hill Island and Paulet Island in the Weddell Sea, after the sinking of its expedition ship. It was rescued by the Argentinian naval vessel "Uruguay".
The French organized their first expedition in 1903 under the leadership of Jean-Baptiste Charcot. Originally intended as a relief expedition for the stranded Nordenskiöld party, the main work of this expedition was the mapping and charting of islands and the western coasts of Graham Land, on the Antarctic peninsula. A section of the coast was explored, and named Loubet Land after the President of France.
A follow up trip was organized from 1908–10 which continued the earlier work of the French expedition with a general exploration of the Bellingshausen Sea, and the discovery of islands and other features, including Marguerite Bay, Charcot Island, Renaud Island, Mikkelsen Bay, Rothschild Island.
Race to the Pole.
The prize of the Heroic age was to reach the South Pole. Two expeditions set off in 1910 to attain this goal; a party led by Norwegian polar explorer Roald Amundsen from the ship "Fram" and Robert Falcon Scott's British group from the "Terra Nova".
Amundsen succeeded in reaching the Pole on 14 December 1911 using a route from the Bay of Whales to the polar plateau via the Axel Heiberg Glacier.
Scott and his four companions reached the South Pole via the Beardmore route on 17 January 1912, 33 days after Amundsen. All five died on the return journey from the Pole, through a combination of starvation and cold. The Amundsen–Scott South Pole Station was later named after these two men.
Further expeditions.
The Australasian Antarctic Expedition took place between 1911-1914 and was led by Sir Douglas Mawson. It concentrated on the stretch of Antarctic coastline between Cape Adare and Mount Gauss, carrying out mapping and survey work on coastal and inland territories.
Discoveries included Commonwealth Bay, Ninnis Glacier, Mertz Glacier, and Queen Mary Land. Major accomplishments were made in geology, glaciology and terrestrial biology.
The Imperial Trans-Antarctic Expedition of 1914-1917 was led by Ernest Shackleton and set out to cross the continent via the South pole. However, their ship, the "Endurance", was trapped and crushed by pack ice in the Weddell Sea before they were able to land. The expedition members survived after a journey on sledges over pack ice, a prolonged drift on an ice-floe, and a voyage in three small boats to Elephant Island. Then Shackleton and five others crossed the Southern Ocean in an open boat called "James Caird" and made the first crossing of South Georgia to raise the alarm at the whaling station Grytviken.
A related component of the Trans-Antarctic Expedition was the Ross Sea party, led by Aeneas Mackintosh. Its objective was to lay depots across the Great Ice Barrier, in order to supply Shackleton's party crossing from the Weddell Sea. All the required depots were laid, but in the process three men, including the leader Mackintosh, lost their lives.
Shackleton's last expedition and the one that brought the 'Heroic Age' to a close, was the Shackleton–Rowett Expedition from 1921-1922 on board the ship "Quest". Its vaguely defined objectives included coastal mapping, a possible continental circumnavigation, the investigation of sub-Antarctic islands, and oceanographic work. After Shackleton's death on 5 January 1922, "Quest" completed a shortened programme before returning home.| 
Further exploration.
By air.
After Shackleton's last expedition, there was a hiatus in Antarctic exploration for about seven years. From 1929, aircraft and mechanized transportation were increasingly used, earning this period the sobriquet of the 'Mechanical Age'. Hubert Wilkins first visited Antarctica in 1921-1922 as an ornithologist attached to the Shackleton-Rowett Expedition. From 1927, Wilkins and pilot Carl Ben Eielson began exploring the Arctic by aircraft.
On 15 April 1928, only a year after Charles Lindbergh's flight across the Atlantic, Wilkins and Eielson made a trans-Arctic crossing from Point Barrow, Alaska, to Spitsbergen, arriving about 20 hours later on 16 April, touching along the way at Grant Land on Ellesmere Island. For this feat and his prior work, Wilkins was knighted.
With financial backing from William Randolph Hearst, Wilkins returned to the South Pole and flew over Antarctica in the "San Francisco". He named the island of Hearst Land after his sponsor.
US Navy Rear Admiral Richard Evelyn Byrd led five expeditions to Antarctica during the 1930s, 1940s, and 1950s. He overflew the South Pole with pilot Bernt Balchen on November 28 and 29, 1929, to match his overflight of the North Pole in 1926. Byrd's explorations had science as a major objective and extensively used the aircraft to explore the continent.
Captain Finn Ronne, Byrd's executive officer, returned to Antarctica with his own expedition in 1947–1948, with Navy support, three planes, and dogs. Ronne disproved the notion that the continent was divided in two and established that East and West Antarctica was one single continent, i.e. that the Weddell Sea and the Ross Sea are not connected. The expedition explored and mapped large parts of Palmer Land and the Weddell Sea coastline, and identified the Ronne Ice Shelf, named by Ronne after his wife Edith Ronne. Ronne covered 3,600 miles by ski and dog sled—more than any other explorer in history.
Overland crossing.
The 1955–58 Commonwealth Trans-Antarctic Expedition successfully completed the first overland crossing of Antarctica, via the South Pole. Although supported by the British and other Commonwealth governments, most of the funding came from corporate and individual donations.
It was headed by British explorer Dr Vivian Fuchs, with New Zealander Sir Edmund Hillary leading the New Zealand Ross Sea Support team. After spending the winter of 1957 at Shackleton Base, Fuchs finally set out on the transcontinental journey in November 1957, with a twelve-man team travelling in six vehicles; three Sno-Cats, two Weasels and one specially adapted "Muskeg" tractor. On route, the team were also tasked with carrying out scientific research including seismic soundings and gravimetric readings.
In parallel Hillary's team had set up Scott Basewhich was to be Fuchs' final destinationon the opposite side of the continent at McMurdo Sound on the Ross Sea. Using three converted Massey Ferguson TE20 tractors and one Weasel (abandoned part-way), Hillary and his three men (Ron Balham, Peter Mulgrew and Murray Ellis), were responsible for route-finding and laying a line of supply depots up the Skelton Glacier and across the Polar Plateau on towards the South Pole, for the use of Fuchs on the final leg of his journey. Other members of Hillary's team carried out geological surveys around the Ross Sea and Victoria Land areas.
Hillary's party reached the South Pole on January 3, 1958, and was just the third (preceded by Amundsen in 1911 and Scott in 1912) to reach the Pole overland. Fuchs' team reached the Pole from the opposite direction on 19 January 1958, where they met up with Hillary. Fuchs then continued overland, following the route that Hillary had laid and on 2 March succeeded in reaching Scott Base, completing the first overland crossing of the continent by land via the South Pole.
Political history.
British claims.
The United Kingdom reasserted sovereignty over the Falkland Islands in the far South Atlantic in 1833 and maintained a continuous presence there. In 1908, the British government extended its territorial claim by declaring sovereignty over "South Georgia, the South Orkneys, the South Shetlands, and the Sandwich Islands, and Graham's Land, situated in the South Atlantic Ocean and on the Antarctic continent to the south of the 50th parallel of south latitude, and lying between the 20th and the 80th degrees of west longitude". All these territories were administered as Falkland Islands Dependencies from Stanley by the Governor of the Falkland Islands. The motivation for this declaration lay in the need for regulating and taxing the whaling industry effectively. Commercial operators would hunt whales in areas outside of the official boundaries of the Falkland Islands and its dependencies and there was a need to close this loophole.
In 1917, the wording of the claim was modified, so as to, among other things, unambiguously include all the territory in the sector stretching to the South Pole (thus encompassing all of the present-day British Antarctic Territory). The new claim covered "all islands and territories whatsoever between the 20th degree of west longitude and the 50th degree of west longitude which are situated south of the 50th parallel of south latitude; and all islands and territories whatsoever between the 50th degree of west longitude and the 80th degree of west longitude which are situated south of the 58th parallel of south latitude".
Under the ambition of Leopold Amery, the Under-Secretary of State for the Colonies, Britain attempted to incorporate the entire continent into the Empire. In a memorandum to the governor-generals for Australia and New Zealand, he wrote that 'with the exception of Chile and Argentina and some barren islands belonging to France... it is desirable that the whole of the Antarctic should ultimately be included in the British Empire.'
The first step was taken on 30 July 1923, when the British government passed an Order in Council under the British Settlements Act 1887, defining the new borders for the Ross Dependency - "that part of His Majesty's Dominions in the Antarctic Seas, which comprises all the islands and territories between the 160th degree of East Longitude and the 150th degree of West Longitude which are situated south of the 60th degree of South Latitude shall be named the Ross Dependency."
The Order in Council then went on to appoint the Governor-General and Commander-in Chief of New Zealand as the Governor of the territory.
In 1930, the United Kingdom claimed Enderby Land. In 1933, a British imperial order transferred territory south of 60° S and between meridians 160° E and 45° E to Australia as the Australian Antarctic Territory.
Following the passing of the Statute of Westminster in 1931, the government of the United Kingdom relinquished all control over the government of New Zealand and Australia. This however had no bearing on the obligations of the Governor-General of both countries in their capacity as Governor of the Antarctic territories.
Other European claims.
Meanwhile, alarmed by these unilateral declarations, the French government laid claim to a strip of the continent in 1924. The basis for their claim to Adélie Land lay on the discovery of the coastline in 1840 by the French explorer Jules Dumont d'Urville, who named it after his wife, Adèle. The British eventually decided to recognize this claim and the border between Adélie Land and Australian Antarctic Territory was fixed definitively in 1938.
These developments also concerned Norwegian whaling interests, who wished to avoid the British taxation of whaling stations in the Antarctic and were concerned that they would be commercially excluded from the continent. The whale-ship owner Lars Christensen financed several expeditions to the Antarctic with the view to claim land for Norway and establish stations on Norwegian territory to gain better privileges. The first expedition, led by Nils Larsen and Ola Olstad, landed on Peter I Island in 1929 and claimed the island for Norway. On 6 March 1931, a Norwegian royal proclamation declared the island under Norwegian sovereignty and on 23 March 1933 the island was declared a dependency.
The 1929 expedition led by Hjalmar Riiser-Larsen and Finn Lützow-Holm named the continental land mass near the island as Queen Maud Land, named after the Norwegian queen Maud of Wales. The territory was explored further during the "Norvegia" expedition of 1930–31. Negotiations with the British government in 1938 resulted in the western border of Queen Maud Land being set at 20°W.
Norway's claim was disputed by Nazi Germany, which in 1938 dispatched the German Antarctic Expedition, led by Alfred Ritscher, to fly over as much of it as possible. The ship "Schwabenland" reached the pack ice off Antarctica on 19 January 1939. During the expedition, an area of about was photographed from the air by Ritscher, who dropped darts inscribed with swastikas every . Germany eventually attempted to claim the territory surveyed by Ritscher under the name New Swabia, but lost any claim to the land following its defeat in the Second World War.
On 14 January 1939, five days prior to the German arrival, Queen Maud Land was annexed by Norway, after a royal decree announced that the land bordering the Falkland Islands Dependencies in the west and the Australian Antarctic Dependency in the east was to be brought under Norwegian sovereignty. The primary basis for the annexation was to secure the Norwegian whaling industry's access to the region. In 1948, Norway and the United Kingdom agreed to limit Queen Maud Land to from 20°W to 45°E, and that the Bruce Coast and Coats Land were to be incorporated into Norwegian territory.
South American involvement.
This encroachment of foreign powers was a matter of immense disquiet to the nearby South American countries, Argentina and Chile. Taking advantage of a European continent plunged into turmoil with the onset of the Second World War, Chile's president, Pedro Aguirre Cerda declared the establishment of a Chilean Antarctic Territory in areas already claimed by Britain.
Argentina had an even longer history of involvement in the Continent. Already in 1904 the Argentine government began a permanent occupation in the area with the purchase of a meteorological station on Laurie Island established in 1903 by Dr William S. Bruce's Scottish National Antarctic Expedition. Bruce offered to transfer the station and instruments for the sum of 5.000 pesos, on the condition that the government committed itself to the continuation of the scientific mission. British officer William Haggard also sent a note to the Argentine Foreign Minister, Jose Terry, ratifying the terms of Bruce proposition.
In 1906, Argentina communicated to the international community the establishment of a permanent base on South Orkney Islands. However, Haggard responded by reminding Argentina that the South Orkneys were British. The British position was that Argentine personnel was granted permission only for the period of one year. The Argentine government entered into negotiations with the British in 1913 over the possible transfer of the island. Although these talks were unsuccessful, Argentina attempted to unilaterally establish their sovereignty with the erection of markers, national flags and other symbols.
In response to this and earlier German explorations, the British Admiralty and Colonial Office launched Operation Tabarin in 1943 to reassert British territorial claims against Argentine and Chilean incursion and establish a permanent British presence in the Antarctic. The move was also motivated by concerns within the Foreign Office about the direction of United States post-war activity in the region.
A suitable cover story was the need to deny use of the area to the enemy. The "Kriegsmarine" was known to use remote islands as rendezvous points and as shelters for commerce raiders, U-boats and supply ships. Also, in 1941, there existed a fear that Japan might attempt to seize the Falkland Islands, either as a base or to hand them over to Argentina, thus gaining political advantage for the Axis and denying their use to Britain.
In 1943, British personnel from HMS "Carnarvon Castle" removed Argentine flags from Deception Island. The expedition was led by Lieutenant James Marr and left the Falkland Islands in two ships, HMS "William Scoresby" (a minesweeping trawler) and "Fitzroy", on Saturday January 29, 1944.
Bases were established during February near the abandoned Norwegian whaling station on Deception Island, where the Union Flag was hoisted in place of Argentine flags, and at Port Lockroy (on February 11) on the coast of Graham Land. A further base was founded at Hope Bay on February 13, 1945, after a failed attempt to unload stores on February 7, 1944. Symbols of British sovereignty, including post offices, signposts and plaques were also constructed and postage stamps were issued.
Operation Tabarin provoked Chile to organize its First Chilean Antarctic Expedition in 1947-48, where the Chilean president Gabriel González Videla personally inaugurated one of its bases.
Following the end of the war in 1945, the British bases were handed over to civilian members of the newly created Falkland Islands Dependencies Survey (subsequently the British Antarctic Survey) the first such national scientific body to be established in Antarctica.
Post war developments.
Friction between Britain and the Latin American states continued into the post war period. Royal Navy warships were despatched in 1948 to prevent naval incursions and in 1952, an Argentine shore party at Hope Bay (the British Base "D", established there in 1945, came up against the Argentine Esperanza Base, est. 1952) fired a machine gun over the heads of a British Antarctic Survey team unloading supplies from the "John Biscoe". The Argentines later extended a diplomatic apology, saying that there had been a misunderstanding and that the Argentine military commander on the ground had exceeded his authority.
The United States became politically interested in the Antarctic continent before and during WWII. The United States Antarctic Service Expedition, from 1939-1941, was sponsored by the government with additional support came from donations and gifts by private citizens, corporations and institutions. The objectives of the Expedition, outlined by President Franklin D. Roosevelt, was to establish two bases: East Base, in the vicinity of Charcot Island, and West Base, in the vicinity of King Edward VII Land. After operating successfully for two years, but with international tensions on the rise, it was considered wise to evacuate the two bases. However,
Immediately after the war, American interest was rekindled with an explicitly geopolitical motive. Operation Highjump, from 1946-1947 was organized by Rear Admiral Richard E. Byrd Jr. and included 4,700 men, 13 ships, and multiple aircraft. The primary mission of Operation Highjump was to establish the Antarctic research base Little America IV, for the purpose of training personnel and testing equipment in frigid conditions and amplifying existing stores of knowledge of hydrographic, geographic, geological, meteorological and electromagnetic propagation conditions in the area. The mission was also aimed at consolidating and extending United States sovereignty over the largest practicable area of the Antarctic continent, although this was publicly denied as a goal even before the expedition ended.
Towards an international treaty.
Meanwhile, in an attempt at ending the impasse, Britain submitted an application to the International Court of Justice in 1955 to adjudicate between the territorial claims of Britain, Argentina and Chile. This proposal failed, as both Latin American countries rejected submitting to an international arbitration procedure.
Negotiations towards the establishment of an international condominium over the continent first began in 1948, involving the 7 claimant powers (Britain, Australia, New Zealand, France, Norway, Chile and Argentina) and the US. This attempt was aimed at excluding the Soviet Union from the affairs of the continent and rapidly fell apart when the USSR declared an interest in the region, refused to recognize any claims of sovereignty and reserved the right to make its own claims in 1950.
An important impetus toward the formation of the Antarctic Treaty System in 1959, was the International Geophysical Year, 1957-1958. This year of international scientific cooperation triggered an 18-month period of intense Antarctic science. More than 70 existing national scientific organizations then formed IGY committees, and participated in the cooperative effort. The British established Halley Research Station in 1956 by an expedition from the Royal Society. Sir Vivian Fuchs headed the Commonwealth Trans-Antarctic Expedition, which completed the first overland crossing of Antarctica in 1958. In Japan, the Japan Maritime Safety Agency offered ice breaker Sōya as the South Pole observation ship and Showa Station was built as the first Japanese observation base on Antarctica.
France contributed with Dumont d'Urville Station and Charcot Station in Adélie Land. The ship "Commandant Charcot" of the French Navy spent nine months of 1949/50 at the coast of Adelie Land, performing ionospheric soundings. The US erected the Amundsen–Scott South Pole Station as the first permanent structure directly over the South Pole in January 1957.
Finally, to prevent the possibility of military conflict in the region, the United States, United Kingdom, the Soviet Union and 9 other countries with significant interests negotiated and signed the Antarctic Treaty in 1959. The treaty entered into force in 1961 and sets aside Antarctica as a scientific preserve, established freedom of scientific investigation and banned military activity on that continent. The treaty was the first arms control agreement established during the Cold War.
Recent history.
A baby, named Emilio Marcos de Palma, was born near Hope Bay on 7 January 1978, becoming the first baby born on the continent. He also was born farther south than anyone in history.
On 28 November 1979, an Air New Zealand DC-10 on a sightseeing trip crashed into Mount Erebus on Ross Island, killing all 257 people on board.
Børge Ousland, a Norwegian explorer, finished the first unassisted Antarctic solo crossing on January 18, 1997.
On 23 November 2007, the MS "Explorer" struck an iceberg and sank, but all on board were rescued by nearby ships, including a passing Norwegian cruise ship, the MS "Nordnorge".

</doc>
<doc id="51319" url="https://en.wikipedia.org/wiki?curid=51319" title="Intellectual history">
Intellectual history

Intellectual history refers to the historiography of ideas and thinkers. This history cannot be considered without the knowledge of the men and women who created, discussed, wrote about, and in other ways were concerned with ideas. Intellectual history as practiced by historians is parallel to the history of philosophy as done by philosophers, and is more akin to the history of ideas. Its central premise is that ideas do not develop in isolation from the people who create and use them, and that one must study ideas not as abstract propositions but in terms of the culture, lives, and historical contexts that produced them.
Intellectual history aims to understand ideas from the past by understanding them in context. The term "context" in the preceding sentence is ambiguous: it can be political, cultural, intellectual, and social. One can read a text both in terms of a chronological context (for example, as a contribution to a discipline or tradition as it extended over time) or in terms of a contemporary intellectual moment (for example, as participating in a debate particular to a certain time and place). Both of these acts of contextualization are typical of what intellectual historians do, nor are they exclusive. Generally speaking, intellectual historians seek to place concepts and texts from the past in multiple contexts.
It is important to realize that intellectual history is not just the history of intellectuals. It studies ideas as they are expressed in texts, and as such is different from other forms of cultural history which deal also with visual and other non-verbal forms of evidence. Any written trace from the past can be the object of intellectual history. The concept of the "intellectual" is relatively recent, and suggests someone professionally concerned with thought. Instead, anyone who has put pen to paper to explore his or her thoughts can be the object of intellectual history. A famous example of an intellectual history of a non-canonical thinker is Carlo Ginzburg's study of a 16th-century Italian miller, Menocchio, in his seminal work "The Cheese and the Worms".
Although the field emerged from European disciplines of "Kulturgeschichte" and "Geistesgeschichte", the historical study of ideas has engaged not only western intellectual traditions but others as well, including those in other parts of the world. Increasingly, historians are calling for a Global intellectual history that will show the parallels and interrelations in the history of thought of all human societies. Another important trend has been the history of the book and of reading, which has drawn attention to the material aspects of how books were designed, produced, distributed, and read.
Intellectual historiography.
Intellectual history as a self-conscious discipline is a relatively recent phenomenon. It has precedents, however, in the history of philosophy, the history of ideas, and in cultural history as practiced since Burckhardt or indeed since Voltaire. The history of the human mind, as it was called in the eighteenth century, was of great concern to scholars and philosophers, and their efforts can in part be traced to Francis Bacon’s call for what he termed a literary history in his The Advancement of Learning. However, the discipline of intellectual history as it is now understood emerged only in the immediate postwar period, in its earlier incarnation as "the history of ideas" under the leadership of Arthur Lovejoy, the founder of the Journal of the History of Ideas. Since that time, Lovejoy's formulation of "unit-ideas" has been discredited and replaced by more nuanced and more historically sensitive accounts of intellectual activity, and this shift is reflected in the replacement of the phrase history of ideas by "intellectual history".
In Britain the history of political thought has been a particular focus since the late 1960s and is associated especially with historians at Cambridge, such as John Dunn and Quentin Skinner. They studied European political thought in its historical context, emphasizing the emergence and development of such concepts as the state and freedom. Skinner in particular is renowned for his provocative methodological essays, which were and are widely read by philosophers and practitioners of other humanistic disciplines, and did much to give prominence to the practice of intellectual history. 
The University of Sussex has also achieved a reputation in this field of study, and the Sussex emphasis on broad interdisciplinary study has been particularly useful in relevant teaching and research.
In the United States, intellectual history is understood more broadly to encompass many different forms of intellectual output, not just the history of political ideas, and it includes such fields as the history of historical thought, associated especially with Anthony Grafton of Princeton University and J.G.A. Pocock of Johns Hopkins University. Formalized in 2010, the History and Culture Ph.D. at Drew University is one of a few graduate programs in the US currently specializing in intellectual history, both in its American and European contexts. Despite the prominence of early modern intellectual historians (those studying the age from the Renaissance to the Enlightenment), the intellectual history of the modern period has also been the locus of intense and creative output on both sides of the Atlantic. Prominent examples of such work include Louis Menand's "" and Martin Jay's "The Dialectical Imagination".
In continental Europe, equivalents of intellectual history can be found. An example is Reinhart Koselleck’s "Begriffsgeschichte" (history of concepts), though there are methodological differences between the work of Koselleck and his followers and the work of Anglo-American intellectual historians.

</doc>
<doc id="51320" url="https://en.wikipedia.org/wiki?curid=51320" title="Ancient history">
Ancient history

Ancient history is the aggregate of past events from the beginning of recorded human history and extending as far as the Early Middle Ages or the Postclassical Era. The span of recorded history is roughly 5,000 years, beginning with Sumerian Cuneiform script, the oldest discovered form of coherent writing from the protoliterate period around the 30th century BC.
The term classical antiquity is often used to refer to history in the Old World from the beginning of recorded Greek history in 776 BC (First Olympiad). This roughly coincides with the traditional date of the founding of Rome in 753 BC, the beginning of the history of ancient Rome, and the beginning of the Archaic period in Ancient Greece. Although the ending date of ancient history is disputed, some Western scholars use the fall of the Western Roman Empire in 476 AD (the most used), the closure of the Platonic Academy in 529 AD, the death of the emperor Justinian I in 565 AD, the coming of Islam or the rise of Charlemagne as the end of ancient and Classical European history.
In India, ancient history includes the early period of the Middle Kingdoms, and, in China, the time up to the Qin Dynasty.
Study.
Historians have two major avenues which they take to better understand the ancient world: archaeology and the study of source texts. Primary sources are those sources closest to the origin of the information or idea under study. Primary sources have been distinguished from secondary sources, which often cite, comment on, or build upon primary sources.
Archaeology.
Archaeology is the excavation and study of artefacts in an effort to interpret and reconstruct past human behavior. Archaeologists excavate the ruins of ancient cities looking for clues as to how the people of the time period lived. Some important discoveries by archaeologists studying ancient history include:
Source text.
Most of what is known of the ancient world comes from the accounts of antiquity's own historians. Although it is important to take into account the bias of each ancient author, their accounts are the basis for our understanding of the ancient past. Some of the more notable ancient writers include Herodotus, Thucydides, Arrian, Plutarch, Polybius, Sima Qian, Sallust, Livy, Josephus, Suetonius, and Tacitus.
A fundamental difficulty of studying ancient history is that recorded histories cannot document the entirety of human events, and only a fraction of those documents have survived into the present day. Furthermore, the reliability of the information obtained from these surviving records must be considered. Few people were capable of writing histories, as literacy was not widespread in almost any culture until long after the end of ancient history.
The earliest known systematic historical thought emerged in ancient Greece, beginning with Herodotus of Halicarnassus (484–c. 425 BC). Thucydides largely eliminated divine causality in his account of the war between Athens and Sparta, establishing a rationalistic element which set a precedent for subsequent Western historical writings. He was also the first to distinguish between cause and immediate origins of an event.
The Roman Empire was one of the ancient world's most literate cultures, but many works by its most widely read historians are lost. For example, Livy, a Roman historian who lived in the 1st century BC, wrote a history of Rome called "Ab Urbe Condita" ("From the Founding of the City") in 144 volumes; only 35 volumes still exist, although short summaries of most of the rest do exist. Indeed, only a minority of the work of any major Roman historian has survived.
Chronology.
Prehistory.
Prehistory is the period before written history. The early human migrations in the Lower Paleolithic saw Homo erectus spread across Eurasia 1.8 million years ago. The controlled use of fire occurred 800 thousand years ago in the Middle Paleolithic. 250 thousand years ago, Homo sapiens (modern humans) emerged in Africa. 60–70 thousand years ago, Homo sapiens migrated out of Africa along a coastal route to South and Southeast Asia and reached Australia. 50 thousand years ago, modern humans spread from Asia to the Near East. Europe was first reached by modern humans 40 thousand years ago. Humans migrated to the Americas about 15 thousand years ago in the Upper Paleolithic,
The 10th millennium BC is the earliest given date for the invention of agriculture and the beginning of the ancient era. Göbekli Tepe was erected by hunter-gatherers in the 10th millennium BC (c. 11,500 years ago), before the advent of sedentism. Together with Nevalı Çori, it has revolutionized understanding of the Eurasian Neolithic. In the 7th millennium BC, Jiahu culture began in China. By the 5th millennium BC, the late Neolithic civilizations saw the invention of the wheel and the spread of proto-writing. In the 4th millennium BC, the Cucuteni-Trypillian culture in the Ukraine-Moldova-Romania region develops. By 3400 BC, "proto-literate" cuneiform is spread in the Middle East. The 30th century BC, referred to as the Early Bronze Age II, saw the beginning of the literate period in Mesopotamia and Ancient Egypt. Around the 27th century BC, the Old Kingdom of Egypt and the First Dynasty of Uruk are founded, according to the earliest reliable regnal eras.
Timeline of ancient history.
Middle to Late Bronze Age.
The Bronze Age forms part of the three-age system. It follows the Neolithic Age in some areas of the world.
In the 24th century BC, the Akkadian Empire was founded in Mesopotamia.
The First Intermediate Period of Egypt of the 22nd century BC was followed by the Middle Kingdom of Egypt between the 21st to 17th centuries BC. The Sumerian Renaissance also developed c. the 21st century BC in Ur. Around the 18th century BC, the Second Intermediate Period of Egypt began.
By 1600 BC, Mycenaean Greece developed. The beginning of the Shang Dynasty emerged in China in this period, and there was evidence of a fully developed Chinese writing system. The beginning of Hittite dominance of the Eastern Mediterranean region is also seen in the 1600s BC. The time from the 16th to the 11th centuries BC around the Nile is called the New Kingdom of Egypt. Between 1550 BC and 1292 BC, the Amarna Period developed in Egypt.
Early Iron Age.
The Iron Age is the last principal period in the three-age system, preceded by the Bronze Age. Its date and context vary depending on the country or geographical region.
During the 13th to 12th centuries BC, the Ramesside Period occurred in Egypt. Around 1200 BC, the Trojan War was thought to have taken place. By around 1180 BC, the disintegration of the Hittite Empire was under way.
In 1046 BC, the Zhou force, led by King Wu of Zhou, overthrew the last king of the Shang Dynasty. The Zhou Dynasty was established in China shortly thereafter.
Pirak is an early iron-age site in Balochistan, Pakistan, going back to about 1200 BC. This period is believed to be the beginning of the Iron Age in India and the subcontinent.
In 1000 BC, the Mannaean Kingdom began in Western Asia. Around the 10th to 7th centuries BC, the Neo-Assyrian Empire developed in Mesopotamia. In 800 BC, the rise of Greek city-states began. In 776 BC, the first recorded Olympic Games were held.
Classical Antiquity.
"Classical antiquity" is a broad term for a long period of cultural history centered around the Mediterranean Sea, which begins roughly with the earliest-recorded Greek poetry of Homer (9th century BC), and continues through the rise of Christianity and the fall of the Western Roman Empire (5th century AD), ending in the dissolution of classical culture with the close of Late Antiquity.
Such a wide sampling of history and territory covers many rather disparate cultures and periods. "Classical antiquity" typically refers to an idealized vision of later people, of what was, in Edgar Allan Poe's words, "the glory that was Greece, the grandeur that was Rome!" In the 18th and 19th centuries AD, reverence for classical antiquity was much greater in Europe and the United States than it is today. Respect for the ancients of Greece and Rome affected politics, philosophy, sculpture, literature, theatre, education, and even architecture and sexuality.
In politics, the presence of a Roman Emperor was felt to be desirable long after the empire fell. This tendency reached its peak when Charlemagne was crowned "Roman Emperor" in the year 800, an act which led to the formation of the Holy Roman Empire. The notion that an emperor is a monarch who outranks a mere king dates from this period. In this political ideal, there would always be a Roman Empire, a state whose jurisdiction extended to the entire civilized world.
Epic poetry in Latin continued to be written and circulated well into the 19th century. John Milton and even Arthur Rimbaud received their first poetic educations in Latin. Genres like epic poetry, pastoral verse, and the endless use of characters and themes from Greek mythology left a deep mark on Western literature.
In architecture, there have been several Greek Revivals, (though while apparently more inspired in retrospect by Roman architecture than Greek). Still, one needs only to look at Washington, DC to see a city filled with large marble buildings with façades made out to look like Roman temples, with columns constructed in the classical orders of architecture.
In philosophy, the efforts of St Thomas Aquinas were derived largely from the thought of Aristotle, despite the intervening change in religion from paganism to Christianity. Greek and Roman authorities such as Hippocrates and Galen formed the foundation of the practice of medicine even longer than Greek thought prevailed in philosophy. In the French theatre, tragedians such as Molière and Racine wrote plays on mythological or classical historical subjects and subjected them to the strict rules of the classical unities derived from Aristotle's "Poetics". The desire to dance like a latter-day vision of how the ancient Greeks did it moved Isadora Duncan to create her brand of ballet. The Renaissance was partly caused by the rediscovery of classic antiquity.
Classical ancient history end.
The transition period from Classical Antiquity to the Early Middle Ages is known as Late Antiquity. Some key dates marking that transition are:
The beginning of the post-classical age (known generally as the Middle Ages) is a period in the history of Europe following the fall of the Western Roman Empire spanning roughly five centuries from AD 500 to 1000. Aspects of continuity with the earlier classical period are discussed in greater detail under the heading "Late Antiquity". Late Antiquity is the transitional centuries from Classical Antiquity to the Middle Ages in both mainland Europe and the Mediterranean world: generally from the end of the Roman Empire's Crisis of the 3rd century (c. 284) to the Islamic conquests and the re-organization of the Byzantine Empire under Heraclius.
Prominent civilizations.
Southwest Asia (Near East).
The Ancient Near East is considered the cradle of civilization. It was the first to practice intensive year-round agriculture; created the first coherent writing system, invented the potter's wheel and then the vehicular- and mill wheel, created the first centralized governments, law codes and empires, as well as introducing social stratification, slavery and organized warfare, and it laid the foundation for the fields of astronomy and mathematics.
Mesopotamia.
Mesopotamia is the site of some of the earliest known civilizations in the world. Early settlement of the alluvial plain lasted from the Ubaid period (late 6th millennium BC) through the Uruk period (4th millennium BC) and the Dynastic periods (3rd millennium BC) until the rise of Babylon in the early 2nd millennium BC. The surplus of storable foodstuffs created by this economy allowed the population to settle in one place instead of migrating after crops and herds. It also allowed for a much greater population density, and in turn required an extensive labor force and division of labor. This organization led to the necessity of record keeping and the development of writing (c. 3500 BC).
Babylonia was an Amorite state in lower Mesopotamia (modern southern Iraq), with Babylon as its capital. Babylonia emerged when Hammurabi (fl. c. 1728–1686 BC, according to the short chronology) created an empire out of the territories of the former kingdoms of Sumer and Akkad. The Amorites being a Semitic people, Babylonia adopted the written Semitic Akkadian language for official use; they retained the Sumerian language for religious use, which by that time was no longer a spoken language. The Akkadian and Sumerian cultures played a major role in later Babylonian culture, and the region would remain an important cultural center, even under outside rule. The earliest mention of the city of Babylon can be found in a tablet from the reign of Sargon of Akkad, dating back to the 23rd century BC.
The Neo-Babylonian Empire, or Chaldea, was Babylonia under the rule of the 11th ("Chaldean") dynasty, from the revolt of Nabopolassar in 626 BC until the invasion of Cyrus the Great in 539 BC. Notably, it included the reign of Nebuchadrezzar II who conquered Judah and Jerusalem.
Akkad was a city and its surrounding region in central Mesopotamia. Akkad also became the capital of the Akkadian Empire. The city was probably situated on the west bank of the Euphrates, between Sippar and Kish (in present-day Iraq, about southwest of the center of Baghdad). Despite an extensive search, the precise site has never been found. Akkad reached the height of its power between the 24th and 22nd centuries BC, following the conquests of king Sargon of Akkad. Because of the policies of the Akkadian Empire toward linguistic assimilation, Akkad also gave its name to the predominant Semitic dialect: the Akkadian language, reflecting use of "akkadû" ("in the language of Akkad") in the Old Babylonian period to denote the Semitic version of a Sumerian text.
Assyria was originally (in the Middle Bronze Age) a region on the Upper Tigris river, named for its original capital, the ancient city of Assur. Later, as a nation and empire that came to control all of the Fertile Crescent, Egypt and much of Anatolia, the term "Assyria proper" referred to roughly the northern half of Mesopotamia (the southern half being Babylonia), with Nineveh as its capital. The Assyrian kings controlled a large kingdom at three different times in history. These are called the "Old" (20th to 15th centuries BC), "Middle" (15th to 10th centuries BC), and "Neo-Assyrian" (911–612 BC) kingdoms, or periods, of which the last is the most well known and best documented. Assyrians invented excavation to undermine city walls, battering rams to knock down gates, as well as the concept of a corps of engineers, who bridged rivers with pontoons or provided soldiers with inflatable skins for swimming.
Mitanni was an Indo-Iranian empire in northern Mesopotamia from c. 1500 BC. At the height of Mitanni power, during the 14th century BC, it encompassed what is today southeastern Turkey, northern Syria and northern Iraq, centered around its capital, Washukanni, whose precise location has not been determined by archaeologists.
Ancient Persia.
Elam is the name of an ancient civilization located in what is now southwest Iran. Archaeological evidence associated with Elam has been dated to before 5000 BC. According to available written records, it is known to have existed from around 3200 BC – making it among the world's oldest historical civilizations – and to have endured up until 539 BC. Its culture played a crucial role in the Gutian Empire, especially during the Achaemenid dynasty that succeeded it, when the Elamite language remained among those in official use. The Elamite period is considered a starting point for the history of Iran.
The Medes were an ancient Iranian people. They had established their own empire by the 6th century BC, having defeated the Neo-Assyrian Empire with the Chaldeans. The Medes are credited with the foundation of the first Iranian empire, the largest of its day until Cyrus the Great established a unified Iranian empire of the Medes and Persian, often referred to as the Achaemenid Persian Empire, by defeating his grandfather and overlord, Astyages the king of Media.
The Achaemenid Empire was the first of the Persian Empires to rule over significant portions of Greater Persia, and followed the Median Empire as the second great empire of the Persian people. It is noted in western history as the foe of the Greek city states in the Greco-Persian Wars, for freeing the Israelites from their Babylonian captivity, and for instituting Aramaic as the empire's official language. Because of the Empire's vast extent and long endurance, Persian influence upon the language, religion, architecture, philosophy, law and government of nations around the world lasts to this day. At the height of its power, the Achaemenid dynasty encompassed approximately 8.0 million square kilometers, held the greatest percentage of world population to date, and was territorially the largest empire of classical antiquity.
Parthia was an Iranian civilization situated in the northeastern part of modern Iran. Their power was based on a combination of the guerrilla warfare of a mounted nomadic tribe, with organizational skills to build and administer a vast empire – even though it never matched in power and extent the Persian empires that preceded and followed it. The Parthian empire was led by the Arsacid dynasty, which reunited and ruled over the Iranian plateau, after defeating and disposing the Hellenistic Seleucid Empire, beginning in the late 3rd century BC, and intermittently controlled Mesopotamia between 150 BC and 224 AD. It was the third native dynasty of ancient Iran (after the Median and the Achaemenid dynasties). Parthia had many wars with the Roman Empire.
The Sassanid Empire, lasting the length of the Late Antiquity period, is considered to be one of Iran's most important and influential historical periods. In many ways the Sassanid period witnessed the highest achievements of Persian civilization and constituted the last great Iranian Empire before the Muslim conquest and the adoption of Islam. During Sassanid times, Persia influenced Roman civilization considerably, and the Romans reserved for the Sassanid Persians alone the status of equals. Sassanid cultural influence extended far beyond the empire's territorial borders, reaching as far as Western Europe, Africa, China, and India, playing a role, for example, in the formation of both European and Asiatic medieval art.
Armenia.
The early history of the Hittite empire is known through tablets that may first have been written in the 17th century BC but survived only as copies made in the 14th and 13th centuries BC. These tablets, known collectively as the Anitta text, begin by telling how Pithana the king of Kussara or Kussar (a small city-state yet to be identified by archaeologists) conquered the neighbouring city of Neša (Kanesh). However, the real subject of these tablets is Pithana's son Anitta, who conquered several neighbouring cities, including Hattusa and Zalpuwa (Zalpa).
Assyrian inscriptions of Shalmaneser I (c. 1270 BC) first mention "Uruartri" as one of the states of Nairi – a loose confederation of small kingdoms and tribal states in the Armenian Highland from the 13th to 11th centuries BC. Uruartri itself was in the region around Lake Van. The Nairi states were repeatedly subjected to attacks by the Assyrians, especially under Tukulti-Ninurta I (c. 1240 BC), Tiglath-Pileser I (c. 1100 BC), Ashur-bel-kala (c. 1070 BC), Adad-nirari II (c. 900), Tukulti-Ninurta II (c. 890), and Ashurnasirpal II (883-859 BC).
The Kingdom of Armenia was an independent kingdom from 190 BC to 387 АD, and a client state of the Roman and Persian empires until 428. Between 95 BC - 55 BC under the rule of King Tigranes the Great, the kingdom of Armenia became a large and powerful empire stretching from the Caspian to the Mediterranean Seas. During this short time it was considered to be the most powerful state in the Roman East.
Arabia.
The history of Pre-Islamic Arabia before the rise of Islam in the 630s is not known in great detail. Archaeological exploration in the Arabian peninsula has been sparse; indigenous written sources are limited to the many inscriptions and coins from southern Arabia. Existing material consists primarily of written sources from other traditions (such as Egyptians, Greeks, Persians, Romans, etc.) and oral traditions later recorded by Islamic scholars.
The first known inscriptions of the Kingdom of Hadhramaut are known from the 8th century BC. It was first referenced by an outside civilization in an Old Sabaic inscription of Karab'il Watar from the early 7th century BC, in which the King of Hadramaut, Yada`'il, is mentioned as being one of his allies.
Dilmun appears first in Sumerian cuneiform clay tablets dated to the end of 4th millennium BC, found in the temple of goddess Inanna, in the city of Uruk. The adjective "Dilmun" refers to a type of axe and one specific official; in addition, there are lists of rations of wool issued to people connected with Dilmun.
The Sabaeans were an ancient people speaking an Old South Arabian language who lived in what is today Yemen, in south west Arabian Peninsula; from 2000 BC to the 8th century BC. Some Sabaeans also lived in D'mt, located in northern Ethiopia and Eritrea, due to their hegemony over the Red Sea. They lasted from the early 2nd millennium to the 1st century BC. In the 1st century BC it was conquered by the Himyarites, but after the disintegration of the first Himyarite empire of the Kings of Saba' and dhu-Raydan the Middle Sabaean Kingdom reappeared in the early 2nd century. It was finally conquered by the Himyarites in the late 3rd century.
The ancient Kingdom of Awsan with a capital at Hagar Yahirr in the wadi Markha, to the south of the wadi Bayhan, is now marked by a tell or artificial mound, which is locally named Hagar Asfal. Once it was one of the most important small kingdoms of South Arabia. The city seems to have been destroyed in the 7th century BC by the king and mukarrib of Saba Karib'il Watar, according to a Sabaean text that reports the victory in terms that attest to its significance for the Sabaeans.
The Himyar was a state in ancient South Arabia dating from 110 BC. It conquered neighbouring Saba (Sheba) in c. 25 BC, Qataban in c. 200 AD and Hadramaut c. 300 AD. Its political fortunes relative to Saba changed frequently until it finally conquered the Sabaean Kingdom around 280 AD. It was the dominant state in Arabia until 525 AD. The economy was based on agriculture.
Foreign trade was based on the export of frankincense and myrrh. For many years it was also the major intermediary linking East Africa and the Mediterranean world. This trade largely consisted of exporting ivory from Africa to be sold in the Roman Empire. Ships from Himyar regularly traveled the East African coast, and the state also exerted a considerable amount of political control of the trading cities of East Africa.
The Nabataean origins remain obscure. On the similarity of sounds, Jerome suggested a connection with the tribe Nebaioth mentioned in "Genesis", but modern historians are cautious about an early Nabatean history. The Babylonian captivity that began in 586 BC opened a power vacuum in Judah, and as Edomites moved into Judaean grazing lands, Nabataean inscriptions began to be left in Edomite territory (earlier than 312 BC, when they were attacked at Petra without success by Antigonus I). The first definite appearance was in 312 BC, when Hieronymus of Cardia, a Seleucid officer, mentioned the Nabateans in a battle report. In 50 BC, the Greek historian Diodorus Siculus cited Hieronymus in his report, and added the following: "Just as the Seleucids had tried to subdue them, so the Romans made several attempts to get their hands on that lucrative trade."
Petra or Sela was the ancient capital of Edom; the Nabataeans must have occupied the old Edomite country, and succeeded to its commerce, after the Edomites took advantage of the Babylonian captivity to press forward into southern Judaea. This migration, the date of which cannot be determined, also made them masters of the shores of the Gulf of Aqaba and the important harbor of Elath. Here, according to Agatharchides, they were for a time very troublesome, as wreckers and pirates, to the reopened commerce between Egypt and the East, until they were chastised by the Ptolemaic rulers of Alexandria.
The Lakhmid Kingdom was founded by the Lakhum tribe that immigrated out of Yemen in the 2nd century and ruled by the Banu Lakhm, hence the name given it. It was formed of a group of Arab Christians who lived in Southern Iraq, and made al-Hirah their capital in (266). The founder of the dynasty was 'Amr and the son Imru' al-Qais converted to Christianity. Gradually the whole city converted to that faith. Imru' al-Qais dreamt of a unified and independent Arab kingdom and, following that dream, he seized many cities in Arabia.
The Ghassanids were a group of South Arabian Christian tribes that emigrated in the early 3rd century from Yemen to the Hauran in southern Syria, Jordan and the Holy Land where they intermarried with Hellenized Roman settlers and Greek-speaking Early Christian communities. The Ghassanid emigration has been passed down in the rich oral tradition of southern Syria. It is said that the Ghassanids came from the city of Ma'rib in Yemen. There was a dam in this city, however one year there was so much rain that the dam was carried away by the ensuing flood. Thus the people there had to leave. The inhabitants emigrated seeking to live in less arid lands and became scattered far and wide. The proverb "They were scattered like the people of Saba" refers to that exodus in history. The emigrants were from the southern Arab tribe of Azd of the Kahlan branch of Qahtani tribes.
Levant.
Though the Ugaritic site is thought to have been inhabited earlier, Neolithic Ugarit was already important enough to be fortified with a wall early on. The first written evidence mentioning the city comes from the nearby city of Ebla, c. 1800 BC. Ugarit passed into the sphere of influence of Egypt, which deeply influenced its art.
Israel.
Israel and Judah were related Iron Age kingdoms of the ancient Levant and had existed during the Iron Ages and the Neo-Babylonian, Persian and Hellenistic periods.
The name Israel first appears in the stele of the Egyptian pharaoh Merneptah c. 1209 BC, "Israel is laid waste and his seed is no more." This "Israel" was a cultural and probably political entity of the central highlands, well enough established to be perceived by the Egyptians as a possible challenge to their hegemony, but an ethnic group rather than an organised state; Archaeologist Paula McNutt says: "It is probably ... during Iron Age I a population began to identify itself as 'Israelite'," differentiating itself from its neighbours via prohibitions on intermarriage, an emphasis on family history and genealogy, and religion.
Israel had emerged by the middle of the 9th century BC, when the Assyrian king Shalmaneser III names "Ahab the Israelite" among his enemies at the battle of Qarqar (853). Judah emerged somewhat later than Israel, probably during the 9th century BC, but the subject is one of considerable controversy. Israel came into increasing conflict with the expanding neo-Assyrian empire, which first split its territory into several smaller units and then destroyed its capital, Samaria (722). A series of campaigns by the Neo-Babylonian Empire between 597 and 582 led to the destruction of Judah.
Followed by the fall of Babylon to the Persian empire, Jews were allowed, by Cyrus the Great, to return to Judea. The Hasmonean Kingdom (followed by the Maccabean revolt) had existed during the Hellenistic period and then the Herodian kingdom during the Roman period.
Phoenicians.
Phoenicia was an ancient civilization centered in the north of ancient Canaan, with its heartland along the coastal regions of modern-day Lebanon, Syria and Israel. Phoenician civilization was an enterprising maritime trading culture that spread across the Mediterranean between the period of 1550 to 300 BC.
A written reference, Herodotus's account (written c. 440 BC) refers to a memory from 800 years earlier, which may be subject to question in the fullness of genetic results. ("History," I:1). This is a legendary introduction to Herodotus' brief retelling of some mythical Hellene-Phoenician interactions. Though few modern archaeologists would confuse this myth with history, a grain of truth may yet lie therein.
Africa.
Egypt.
Ancient Egypt was a long-lived civilization geographically located in north-eastern Africa. It was concentrated along the middle to lower reaches of the Nile River reaching its greatest extension during the 2nd millennium BC, which is referred to as the New Kingdom period. It reached broadly from the Nile Delta in the north, as far south as Jebel Barkal at the Fourth Cataract of the Nile. Extensions to the geographical range of ancient Egyptian civilization included, at different times, areas of the southern Levant, the Eastern Desert and the Red Sea coastline, the Sinai Peninsula and the Western Desert (focused on the several oases).
Ancient Egypt developed over at least three and a half millennia. It began with the incipient unification of Nile Valley polities around 3500 BC and is conventionally thought to have ended in 30 BC when the early Roman Empire conquered and absorbed Ptolemaic Egypt as a province. (Though this last did not represent the first period of foreign domination, the Roman period was to witness a marked, if gradual transformation in the political and religious life of the Nile Valley, effectively marking the termination of independent civilisational development).
The civilization of ancient Egypt was based on a finely balanced control of natural and human resources, characterised primarily by controlled irrigation of the fertile Nile Valley; the mineral exploitation of the valley and surrounding desert regions; the early development of an independent writing system and literature; the organisation of collective projects; trade with surrounding regions in east / central Africa and the eastern Mediterranean; finally, military ventures that exhibited strong characteristics of imperial hegemony and territorial domination of neighbouring cultures at different periods. Motivating and organizing these activities were a socio-political and economic elite that achieved social consensus by means of an elaborate system of religious belief under the figure of a (semi)-divine ruler (usually male) from a succession of ruling dynasties and which related to the larger world by means of polytheistic beliefs.
Nubia.
The Kushite state was formed before a period of Egyptian incursion into the area. The Kushite civilization has also been referred to as Nubia. The first cultures arose in Sudan before the time of a unified Egypt, and the most widespread is known as the Kerma civilization. It is through Egyptian, Hebrew, Roman and Greek records that most of our knowledge of Kush (Cush) comes.
It is also referred to as Ethiopia in ancient Greek and Roman records. According to Josephus and other classical writers, the Kushite Empire covered all of Africa, and some parts of Asia and Europe at one time or another. The Kushites are also famous for having buried their monarchs along with all their courtiers in mass graves. The Kushites also built burial mounds and pyramids, and shared some of the same gods worshipped in Egypt, especially Amon and Isis.
Axum.
The Axumite Empire was an important trading nation in northeastern Africa, growing from the proto-Aksumite period c. 4th century BC to achieve prominence by the 1st century AD. Its ancient capital is found in northern Ethiopia, the Kingdom used the name "Ethiopia" as early as the 4th century. Aksum is mentioned in the 1st century AD "Periplus of the Erythraean Sea" as an important market place for ivory, which was exported throughout the ancient world, and states that the ruler of Aksum in the 1st century AD was Zoscales, who, besides ruling in Aksum also controlled two harbours on the Red Sea: Adulis (near Massawa) and Avalites (Assab). He is also said to have been familiar with Greek literature. It is also the alleged resting place of the Ark of the Covenant and the home of the Queen of Sheba. Aksum was also the first major empire to convert to Christianity.
Land of Punt.
The Land of Punt, also called Pwenet, or Pwene by the ancient Egyptians, was a trading partner known for producing and exporting gold, aromatic resins, African blackwood, ebony, ivory, slaves and wild animals. Information about Punt has been found in ancient Egyptian records of trade missions to this region.
The exact location of Punt remains a mystery. The mainstream view is that Punt was located to the south-east of Egypt, most likely on the coast of the Horn of Africa. The earliest recorded Egyptian expedition to Punt was organized by Pharaoh Sahure of the Fifth Dynasty (25th century BC) although gold from Punt is recorded as having been in Egypt in the time of king Khufu of the Fourth Dynasty of Egypt. Subsequently, there were more expeditions to Punt in the Sixth Dynasty of Egypt, the Eleventh dynasty of Egypt, the Twelfth dynasty of Egypt and the Eighteenth dynasty of Egypt. In the Twelfth dynasty of Egypt, trade with Punt was celebrated in popular literature in "Tale of the Shipwrecked Sailor".
Nok culture.
The Nok culture appeared in Nigeria around 1000 BC and mysteriously vanished around 200 AD. The civilization’s social system is thought to have been highly advanced. The Nok civilization was considered to be the earliest sub-Saharan producer of life-sized Terracotta which have been discovered by archaeologists. A Nok sculpture resident at the Minneapolis Institute of Arts, portrays a sitting dignitary wearing a "Shepherds Crook" on the right arm, and a "hinged flail" on the left. These are symbols of authority associated with ancient Egyptian pharaohs, and the god Osiris, which suggests that an ancient Egyptian style of social structure, and perhaps religion, existed in the area of modern Nigeria during the late Pharonic period. (Informational excerpt copied from Nigeria and Nok culture articles)
Carthage.
Carthage was founded in 814 BC by Phoenician settlers from the city of Tyre, bringing with them the city-god Melqart. Ancient Carthage was an informal hegemony of Phoenician city-states throughout North Africa and modern Spain from 575 BC until 146 BC. It was more or less under the control of the city-state of Carthage after the fall of Tyre to Babylonian forces. At the height of the city's influence, its empire included most of the western Mediterranean. The empire was in a constant state of struggle with the Roman Republic, which led to a series of conflicts known as the Punic Wars. After the third and final Punic War, Carthage was destroyed then occupied by Roman forces. Nearly all of the territory held by Carthage fell into Roman hands.
South Asia.
The earliest evidence of human civilization in South Asia is from the Mehrgarh region (7000 BC to 3200 BC) of Pakistan. Located near the Bolan Pass, to the west of the Indus River valley and between the present-day Pakistani cities of Quetta, Kalat and Sibi, Mehrgarh was discovered in 1974 by an archaeological team directed by French archaeologist Jean-François Jarrige, and was excavated continuously between 1974 and 1986. The earliest settlement at Mehrgarh—in the northeast corner of the site—was a small farming village dated between 7000 BC–5500 BC.
Early Mehrgarh residents lived in mud brick houses, stored their grain in granaries, fashioned tools with local copper ore, and lined their large basket containers with bitumen. They cultivated six-row barley, einkorn and emmer wheat, jujubes and dates, and herded sheep, goats and cattle. Residents of the later period (5500 BC to 2600 BC) put much effort into crafts, including flint knapping, tanning, bead production, and metal working. The site was occupied continuously until about 2600 BC.
In April 2006, it was announced in the scientific journal Nature that the oldest evidence in human history for the drilling of teeth in vivo (i.e. in a living person) was found in Mehrgarh. Mehrgarh is sometimes cited as the earliest known farming settlement in South Asia, based on archaeological excavations from 1974 (Jarrige et al.). The earliest evidence of settlement dates from 7000 BC. It is also cited for the earliest evidence of pottery in South Asia. Archaeologists divide the occupation at the site into several periods. Mehrgarh is now seen as a precursor to the Indus Valley Civilization.
Indus Valley Civilization.
The Indus Valley Civilization (c. 3300–1700 BC, flourished 2600–1900 BC), abbreviated IVC, was an ancient civilization that flourished in the Indus and Ghaggar-Hakra river valleys primarily in what is now Pakistan, although settlements linked to this ancient civilization have been found in eastern Afghanistan, and western India. Minor scattered sites have been found as far away as Turkmenistan. Another name for this civilization is the Harappan Civilization, after the first of its cities to be excavated, Harappa in the Pakistani province of Punjab. The IVC might have been known to the Sumerians as the Meluhha, and other trade contacts may have included Egypt, Africa, however the modern world discovered it only in the 1920s as a result of archaeological excavations and rail road building. Prominent historians of Ancient India would include Ram Sharan Sharma and Romila Thapar.
Mahajanapadas.
The births of Mahavira and Buddha in the 6th century BC mark the beginning of well-recorded history in the region. Around the 5th century BC, the ancient region of Pakistan was invaded by the Achaemenid Empire under Darius in 522 BC forming the easternmost satraps of the Persian Empire. The provinces of Sindh and Panjab were said to be the richest "satraps" of the Persian Empire and contributed many soldiers to various Persian expeditions. It is known that an "Indian" contingent fought in Xerxes' army on his expedition to Greece. Herodotus mentions that the Indus satrapy supplied cavalry and chariots to the Persian army. He also mentions that the Indus people were clad in armaments made of cotton, carried bows and arrows of cane covered with iron. Herodotus states that in 517 BC Darius sent an expedition under Scylax to explore the Indus. Under Persian rule, much irrigation and commerce flourished within the vast territory of the empire. The Persian empire was followed by the invasion of the Greeks under Alexander's army. Since Alexander was determined to reach the eastern-most limits of the Persian Empire he could not resist the temptation to conquer India (i.e. the Punjab region), which at this time was parcelled out into small chieftain-ships, who were feudatories of the Persian Empire. Alexander amalgamated the region into the expanding Hellenic empire. The "Rigveda", in Sanskrit, goes back to about 1500 BC. The Indian literary tradition has an oral history reaching down into the Vedic period of the later 2nd millennium BC.
"Ancient India" is usually taken to refer to the "golden age" of classical Indian culture, as reflected in Sanskrit literature, beginning around 500 BC with the sixteen monarchies and 'republics' known as the Mahajanapadas, stretched across the Indo-Gangetic plains from modern-day Afghanistan to Bangladesh. The largest of these nations were Magadha, Kosala, Kuru and Gandhara. Notably, the great epics of Ramayana and Mahabharata are rooted in this classical period.
Amongst the sixteen Mahajanapadas, the kingdom of Magadha rose to prominence under a number of dynasties that peaked in power under the reign of Ashoka Maurya, one of India's most legendary and famous emperors. During the reign of Ashoka, the four dynasties of Chola, Chera, and Pandya were ruling in the South, while the King Devanampiya Tissa was controlling the Anuradhapura Kingdom (now Sri Lanka). These kingdoms, while not part of Ashoka's empire, were in friendly terms with the Maurya Empire. There was a strong alliance existed between Devanampiya Tissa (250–210 BC) and Ashoka of India, who sent Arahat Mahinda, four monks, and a novice being sent to Sri Lanka. They encountered Devanampiya Tissa at Mihintale. After this meeting, Devanampiya Tissa embraced Buddhism the order of monks was established in the country. Devanampiya Tissa, guided by Arahat Mahinda, took steps to firmly establish Buddhism in the country.
The Satavahanas started out as feudatories to the Mauryan Empire, and declared independence soon after the death of Ashoka (232 BC). Other notable ancient South Indian dynasties include the Kadambas of Banavasi, western Ganga dynasty, Badami Chalukyas, Western Chalukyas, Hoysalas, Kakatiya dynasty, Pallavas, Rashtrakutas of Manyaketha and Satavahanas.
Middle kingdoms.
The period between AD 320–550 is known as the Classical Age, when most of North India was reunited under the Gupta Empire (c. AD 320–550). This was a period of relative peace, law and order, and extensive achievements in religion, education, mathematics, arts, Sanskrit literature and drama. Grammar, composition, logic, metaphysics, mathematics, medicine, and astronomy became increasingly specialized and reached an advanced level. The Gupta Empire was weakened and ultimately ruined by the raids of Hunas (a branch of the Hephthalites emanating from Central Asia). Under Harsha (r. 606–47), North India was reunited briefly.
The educated speech at that time was Sanskrit, while the dialects of the general population of northern India were referred to as Prakrits. The South Indian Malabar Coast and the Tamil people of the Sangam age traded with the Graeco-Roman world. They were in contact with the Phoenicians, Romans, Greeks, Arabs, Syrians, Jews, and the Chinese.
The regions of South Asia, primarily present-day Pakistan and India, were estimated to have had the largest economy of the world between the 1st and 15th centuries AD, controlling between one third and one quarter of the world's wealth up to the time of the Mughals, from whence it rapidly declined during British rule.
East Asia.
China.
Ancient era.
Written records of China's past dates from the Shang Dynasty (商朝) in perhaps the 13th century BC, and takes the form of inscriptions of divination records on the bones or shells of animals—the so-called "oracle bones" (甲骨文). Archaeological findings providing evidence for the existence of the Shang Dynasty, c. 1600–1046 BC is divided into two sets. The first, from the earlier Shang period (c. 1600–1300) comes from sources at Erligang (二里崗), Zhengzhou (鄭州) and Shangcheng. The second set, from the later Shang or Yin (殷) period, consists of a large body of oracle bone writings. Anyang (安陽) in modern-day Henan has been confirmed as the last of the nine capitals of the Shang (c. 1300–1046 BC).
By the end of the 2nd millennium BC, the Zhou Dynasty (周朝) began to emerge in the Yellow River valley, overrunning the Shang. The Zhou appeared to have begun their rule under a semi-feudal system. The ruler of the Zhou, King Wu, with the assistance of his brother, the Duke of Zhou, as regent managed to defeat the Shang at the Battle of Muye. The king of Zhou at this time invoked the concept of the Mandate of Heaven to legitimize his rule, a concept that would be influential for almost every successive dynasty. The Zhou initially moved their capital west to an area near modern Xi'an, near the Yellow River, but they would preside over a series of expansions into the Yangtze River valley. This would be the first of many population migrations from north to south in Chinese history.
Spring and Autumn.
In the 8th century BC, power became decentralized during the Spring and Autumn period (春秋時代), named after the influential Spring and Autumn Annals. In this period, local military leaders used by the Zhou began to assert their power and vie for hegemony. The situation was aggravated by the invasion of other peoples from the northwest, such as the Quanrong, forcing the Zhou to move their capital east to Luoyang. This marks the second large phase of the Zhou dynasty: the Eastern Zhou. In each of the hundreds of states that eventually arose, local strongmen held most of the political power and continued their subservience to the Zhou kings in name only. Local leaders for instance started using royal titles for themselves. The Hundred Schools of Thought (諸子百家) of Chinese philosophy blossomed during this period, and such influential intellectual movements as Confucianism (儒家), Taoism (道家), Legalism (法家) and Mohism (墨家) were founded, partly in response to the changing political world. The Spring and Autumn Period is marked by a falling apart of the central Zhou power. China now consists of hundreds of states, some only as large as a village with a fort.
Warring States.
After further political consolidation, seven prominent states remained by the end of 5th century BC, and the years in which these few states battled each other is known as the Warring States period (戰國時代). Though there remained a nominal Zhou king until 256 BC, he was largely a figurehead and held little power. As neighboring territories of these warring states, including areas of modern Sichuan (四川) and Liaoning (遼寧), were annexed, they were governed under the new local administrative system of commandery and prefecture (郡縣). This system had been in use since the Spring and Autumn Period and parts can still be seen in the modern system of Sheng & Xian (province and county, 省縣). The final expansion in this period began during the reign of Ying Zheng (嬴政), the king of Qin. His unification of the other six powers, and further annexations in the modern regions of Zhejiang (浙江), Fujian (福建), Guangdong (廣東) and Guangxi (廣西) in 214 BC enabled him to proclaim himself the First Emperor (Qin Shi Huangdi, 秦始皇帝).
Japan.
Japan first appeared in written records in AD 57 with the following mention in China's "Book of the Later Han": "Across the ocean from Luoyang are the people of Wa. Formed from more than one hundred tribes, they come and pay tribute frequently." According to the Kojiki, Emperor Jimmu, in 660 BC, unified the many peoples of the Japanese archipelago and established order. The "Book of Wei", written in the 3rd century, noted the country was the unification of some 30 small tribes or states and ruled by a shaman queen named Himiko of Yamataikoku.
During the Han Dynasty and Wei Dynasty, Chinese travelers to Kyūshū recorded its inhabitants and claimed that they were the descendants of the Grand Count (Tàibó) of the Wu. The inhabitants also show traits of the pre-sinicized Wu people with tattooing, teeth-pulling and baby-carrying. The "Book of Wei" records the physical descriptions which are similar to ones on "Haniwa" statues, such men with braided hair, tattooing and women wearing large, single-piece clothing.
Korea.
According to the Samguk Yusa and other Korean medieval-era Folklore collection, Gojoseon was the first Korean kingdom. Gojoseon was founded in 2333 BC by the legendary ruler Dangun, said to be descended from the Lord of Heaven. Then, Korea was governed for Jizi and the 40th generation descendant. According to Records of the Grand Historian, Korea was founded by Wiman from China in 197 BC. In 105 BC, Han Dynasty China ruined Korea and ruled for about 400 years.
The Three Kingdoms (Baekje, Goguryeo, and Silla) conquered other successor states of Gojoseon and came to dominate the peninsula and much of Manchuria. The three kingdoms competed with each other both economically and militarily; Goguryeo and Baekje were the more powerful states for much of the three kingdoms era. At times more powerful than the neighboring Sui Dynasty, Goguryeo was a regional power that defeated massive Chinese invasions multiple times. As one of the Three Kingdoms of Korea, Silla gradually extended across Korea and eventually became the first state since Gojoseon to cover most of Korean peninsula in 676. In 698, former Goguryeo general Dae Jo-yeong founded Balhae as the successor to Goguryeo.
Unified Silla itself fell apart in the late 9th century, giving way to the tumultuous Later Three Kingdoms period (892-936), which ended with the establishment of the Goryeo Dynasty. After the fall of Balhae in 926 to the Khitan, much of its people were absorbed into Goryeo Dynasty.
Vietnam.
Around 3000 BC, the 15 different Lạc Việt ethnic tribes lived together in many areas with other inhabitants. Due to increasing needs to control floods, fights against invaders, and culture and trade exchanges, these tribes living near each other tended to gather together and integrate into a larger mixed group. Among these Lac Viet tribes was the Van Lang, which was the most powerful tribe. The leader of this tribe later joined all the tribes together to found the Hồng Bàng Dynasty in 2897 BC. He became the first in a line of earliest Vietnamese kings, collectively known as the Hùng kings (Hùng Vương). The Hùng kings called the country, which was then located on the Red River delta in present-day northern Vietnam, Văn Lang. The people of Văn Lang were referred to as the Lạc Việt. The next generations followed in their father's footsteps and kept this appellation. Based on historical documents, researchers correlatively delineated the location of Văn Lang Nation to the present day regions of North and north of Central Vietnam, as well as the south of present-day Kwangsi (China).
The Đông Sơn culture was a prehistoric Bronze Age culture that was centered at the Red River Valley of northern Vietnam. Its influence flourished to other parts of Southeast Asia, including the Indo-Malayan Archipelago from about 2000 BC to 200 AD. The theory based on the assumption that bronze casting in eastern Asia originated in northern China; however, this idea has been discredited by archaeological discoveries in north-eastern Thailand in the 1970s. In the words of one scholar, "Bronze casting began in Southeast Asia and was later borrowed by the Chinese, not vice versa as the Chinese scholars have always claimed. Evidence of early kingdoms of Vietnam other than the Đông Sơn culture in Northern Vietnam was found in Cổ Loa, the ancient city situated within present-day Hà Nội.
Mongols.
North-western Mongolia was Turkic while south-western Mongolia had come under Indo-European (Tocharian and Scythian) influence. In antiquity, the eastern portions of both Inner and Outer Mongolia were inhabited by Mongolic peoples descended from the Donghu people, including the Xianbei, Wuhuan, Rouran, Tuoba, Murong, Shiwei, Kumo Xi and Khitan. These were Tengriist horse-riding pastoralist kingdoms that had close contact with the Chinese. The Donghu are first mentioned by Sima Qian as already existing in Inner Mongolia north of the state of Yan in 699-632 BC. The Mongolic-speaking Xianbei (208 BC-234 AD) originally formed a part of the Donghu confederation, but existed even before that time, as evidenced by a mention in the Guoyu "晉語八" section which states that during the reign of King Cheng of Zhou (reigned 1042-1021 BC) the Xianbei came to participate at a meeting of Zhou subject-lords at Qiyang (岐阳) (now Qishan County) but were only allowed to perform the fire ceremony under the supervision of Chu (楚), since they were not vassals by covenant (诸侯). As a nomadic confederation composed of the Xianbei and Wuhuan, the Donghu were prosperous in the 4th century BC, forcing surrounding tribes to pay tribute and constantly harassing the State of Zhao (325 BC, during the early years of the reign of Wuling) and the State of Yan (in 304 BC General Qin Kai was given as a hostage to the Donghu).
In 208 BC Xiongnu emperor Modu Chanyu, in his first major military campaign, defeated the formerly superior Donghu, who split into the Xianbei and Wuhuan. The Xianbei fled east all the way to Liaodong. In 49 AD the Xianbei ruler Bianhe attacked the Xiongnu and killed 2000 people after having received generous gifts from Emperor Guangwu of Han. In 54 AD the Xianbei rulers Yuchoupen and Mantu presented themselves to the Han emperor and received the titles of wang and gou. Until 93 AD the Xianbei were quietly protecting the Chinese border from Wuhuan and Xiongnu attacks and received ample rewards. From 93 AD the Xianbei began to occupy the lands of the Xiongnu. 100,000 Xiongnu families changed their name to Xianbei. In 97 AD Feijuxian in Liaodong was attacked by the Xianbei, and the governor Qi Sen was dismissed for inaction. Other Xianbei rulers who were active before the rise of the Xianbei emperor Tanshihuai (141-181) were Yanzhiyang, Lianxu and Cizhiqian. The Xianbei gave rise to different Mongolic branches, for example the Rouran (330-555), Khitan (388-1218) and Shiwei (444-present day). The Khitans developed the Khitan scripts in 920-925 AD. The Rouran king Shelun was the first major leader of the steppes to adopt (in 402 AD) the title of Khagan (可汗) or Qiudoufa Khan (丘豆伐可汗) (which was originally a title used by Xianbei nobles).
The Mongols of Genghis Khan were the Menggu sub-tribe of the Shiwei Xianbei. The first surviving Mongolian text is the Stele of Yisüngge, a report on sports in Mongolian script on stone, that is most often dated at the verge of 1224 and 1225. Other early sources are written in Mongolian, Phagspa (decrets), Chinese (the Secret history), Arabic (dictionaries) and a few other western scripts.
Huns.
The Huns left practically no written records. There is no record of what happened between the time they left Mongolian Plateau and arrived in Europe 150 years later. The last mention of the northern Xiongnu was their defeat by the Chinese in 151 at the lake of Barkol, after which they fled to the western steppe at Kangju (centered on the city of Turkistan in Kazakhstan). Chinese records between the 3rd and 4th centuries suggest that a small tribe called Yueban, remnants of northern Xiongnu, was distributed about the steppe of Kazakhstan.
Americas.
In pre-Columbian times, several large, centralized ancient civilizations developed in the Western Hemisphere, both in Mesoamerica and western South America.
Andean civilizations.
The Central Andes in South America has the largest ancient civilization register, spanning 4,500 years from Norte chico to the latest civilization, the Inca Empire.
Mesoamerica.
Mesoamerican ancient civilizations included the Olmecs and Mayans. Between 2000 and 300 BC, complex cultures began to form and many matured into advanced Mesoamerican civilizations such as the: Olmec, Izapa, Teotihuacan, Maya, Zapotec, Mixtec, Huastec, Purépecha, "Toltec" and Aztec, which flourished for nearly 4,000 years before the first contact with Europeans. These civilizations' progress included pyramid-temples, mathematics, astronomy, medicine, and theology.
The Zapotec emerged around 1500 years BC. They left behind the great city Monte Alban. Their writing system had been thought to have influenced the Olmecs but, with recent evidence, the Olmec may have been the first civilization in the area to develop a true writing system independently. At the present time, there is some debate as to whether or not Olmec symbols, dated to 650 BC, are actually a form of writing preceding the oldest Zapotec writing dated to about 500 BC.
Olmec symbols found in 2002 and 2006 date to 650 BC and 900 BC respectively, preceding the oldest Zapotec writing. The Olmec symbols found in 2006, dating to 900 BC, are known as the Cascajal Block.
The earliest Mayan inscriptions found which are identifiably Maya date to the 3rd century BC in San Bartolo, Guatemala.
Europe.
Etruria.
The history of the Etruscans can be traced relatively accurately, based on the examination of burial sites, artifacts, and writing. Etruscans culture that is identifiably and certainly Etruscan developed in Italy in earnest by 800 BC approximately over the range of the preceding Iron Age Villanovan culture. The latter gave way in the 7th century to a culture that was influenced by Greek traders and Greek neighbors in Magna Graecia, the Hellenic civilization of southern Italy.
From the descendants of the Villanovan people in Etruria in central Italy, a separate Etruscan culture emerged in the beginning of the 7th century BC, evidenced by around 7,000 inscriptions in an alphabet similar to that of Euboean Greek, in the non-Indo-European Etruscan language. The burial tombs, some of which had been fabulously decorated, promotes the idea of an aristocratic city-state, with centralized power structures maintaining order and constructing public works, such as irrigation networks, roads, and town defenses.
Greece.
Ancient Greece is the period in Greek history lasting for close to a millennium, until the rise of Christianity. It is considered by most historians to be the foundational culture of Western Civilization. Greek culture was a powerful influence in the Roman Empire, which carried a version of it to many parts of Europe.
The earliest known human settlements in Greece were on the island of Crete, more than 9,000 years ago, though there is evidence of tool use on the island going back over 100,000 years. The earliest evidence of a civilisation in ancient Greece is that of the Minoans on Crete, dating as far back as 3600 BC. On the mainland, the Mycenaean civilisation rose to prominence around 1600 BC, superseded the Minoan civilisation on Crete, and lasted until about 1100 BC, leading to a period known as the Greek Dark Ages
The Archaic Period in Greece is generally considered to have lasted from around the eighth century BC to the invasion by Xerxes in 480 BC. This period saw the expansion of the Greek world around the Mediterranean, with the founding of Greek city-states as far afield as Sicily in the West and the Black sea in the East. Politically, the Archaic period in Greece saw the collapse of the power of the old aristocracies, with democratic reforms in Athens and the development of Sparta's unique constitution. The end of the Archaic period also saw the rise of Athens, which would come to be a dominant power in the Classical period, after the reforms of Solon and the tyranny of Pisistratus.
The Classical Greek world was dominated throughout the fifth century BC by the major powers of Athens and Sparta. Through the Delian League, Athens was able to convert Pan-hellenist sentiment and fear of the Persian threat into a powerful empire, and this, along with the conflict between Sparta and Athens culminating in the Peloponnesian war, was the major political development of the first part of the Classical period.
The period in Greek history from the death of Alexander the Great until the rise of the Roman empire and its conquest of Egypt in 30 BC is known as the Hellenistic period. The name derives from the Greek word "Hellenistes" ("the Greek speaking ones"), and describes the spread of Greek culture into the non-Greek world following the conquests of Alexander and the rise of his successors.
Following the Battle of Corinth in 146 BC, Greece came under Roman rule, ruled from the province of Macedonia. In 27 BC, Augustus organised the Greek peninsula into the province of Achaea. Greece remained under Roman control until the break up of the Roman empire, in which it remained part of the Eastern Empire. Much of Greece remained under Byzantine control until the end of the Byzantine empire.
Rome.
Ancient Rome was a civilization that grew out of the city-state of Rome, originating as a small agricultural community founded on the Italian Peninsula in the 9th century BC. In its twelve centuries of existence, Roman civilization shifted from a monarchy to an oligarchic republic to an increasingly autocratic empire.
Roman civilization is often grouped into "classical antiquity" with ancient Greece, a civilization that inspired much of the culture of ancient Rome. Ancient Rome contributed greatly to the development of law, war, art, literature, architecture, and language in the Western world, and its history continues to have a major influence on the world today. The Roman civilization came to dominate Europe and the Mediterranean region through conquest and assimilation.
Throughout the territory under the control of ancient Rome, residential architecture ranged from very modest houses to country villas. A number of Roman founded cities had monumental structures. Many contained fountains with fresh drinking-water supplied by hundreds of miles of aqueducts, theatres, gymnasiums, bath complexes sometime with libraries and shops, marketplaces, and occasionally functional sewers.
However, a number of factors led to the eventual decline of the Roman Empire. The western half of the empire, including Hispania, Gaul, and Italy, eventually broke into independent kingdoms in the 5th century; the Eastern Roman Empire, governed from Constantinople, is referred to as the Byzantine Empire after AD 476, the traditional date for the "fall of Rome" and subsequent onset of the Middle Ages.
Late Antiquity.
The Roman Empire underwent considerable social, cultural and organizational change starting with reign of Diocletian, who began the custom of splitting the Empire into Eastern and Western halves ruled by multiple emperors. Beginning with Constantine the Great the Empire was Christianized, and a new capital founded at Constantinople. Migrations of Germanic tribes disrupted Roman rule from the late 4th century onwards, culminating in the eventual collapse of the Empire in the West in 476, replaced by the so-called barbarian kingdoms. The resultant cultural fusion of Greco-Roman, Germanic and Christian traditions formed the cultural foundations of Europe.
Germanic tribes.
Migration of Germanic peoples to Britain from what is now northern Germany and southern Scandinavia is attested from the 5th century (e.g. Undley bracteate). Based on Bede's "Historia ecclesiastica gentis Anglorum", the intruding population is traditionally divided into Angles, Saxons, and Jutes, but their composition was likely less clear-cut and may also have included ancient Frisians and Franks. The "Anglo-Saxon Chronicle" contains text that may be the first recorded indications of the movement of these Germanic Tribes to Britain. The Angles and Saxons and Jutes were noted to be a confederation in the Greek Geographia written by Ptolemy in around AD 150.
Anglo-Saxon is the term usually used to describe the peoples living in the south and east of Great Britain from the early 5th century AD. Benedictine monk Bede identified them as the descendants of three Germanic tribes: the Angles, the Saxons, and the Jutes, from the Jutland peninsula and Lower Saxony (, Germany). The Angles may have come from Angeln, and Bede wrote their nation came to Britain, leaving their land empty. They spoke closely related Germanic dialects. The Anglo-Saxons knew themselves as the "Englisc," from which the word "English" derives.
The Celts were a diverse group of tribal societies in Iron Age Europe. Proto-Celtic culture formed in the Early Iron Age in Central Europe (Hallstatt period, named for the site in present-day Austria). By the later Iron Age (La Tène period), Celts had expanded over wide range of lands: as far west as Ireland and the Iberian Peninsula, as far east as Galatia (central Anatolia), and as far north as Scotland. By the early centuries AD, following the expansion of the Roman Empire and the Great Migrations of Germanic peoples, Celtic culture had become
restricted to the British Isles (Insular Celtic), with the Continental Celtic languages extinct by the mid-1st millennium AD.
Viking refers to a member of the Norse (Scandinavian) peoples, famous as explorers, warriors, merchants, and pirates, who raided and colonized wide areas of Europe beginning in the late 8th. These Norsemen used their famed longships to travel. The Viking Age forms a major part of Scandinavian history, with a minor, yet significant part in European history.
Developments.
Religion and philosophy.
New philosophies and religions arose in both east and west, particularly about the 6th century BC. Over time, a great variety of religions developed around the world, with some of the earliest major ones being Hinduism, Buddhism, and Jainism in India, and Zoroastrianism in Persia. The Abrahamic religions trace their origin to Judaism, around 1800 BC.
The ancient Indian philosophy is a fusion of two ancient traditions: Sramana tradition and Vedic tradition. Indian philosophy begins with the "Vedas" where questions related to laws of nature, the origin of the universe and the place of man in it are asked. Jainism and Buddhism are continuation of the Sramana school of thought. The Sramanas cultivated a pessimistic world view of the samsara as full of suffering and advocated renunciation and austerities. They laid stress on philosophical concepts like Ahimsa, Karma, Jnana, Samsara and Moksa. While there are ancient relations between the Indian Vedas and the Iranian Avesta, the two main families of the Indo-Iranian philosophical traditions were characterized by fundamental differences in their implications for the human being's position in society and their view on the role of man in the universe.
In the east, three schools of thought were to dominate Chinese thinking until the modern day. These were Taoism, Legalism and Confucianism. The Confucian tradition, which would attain dominance, looked for political morality not to the force of law but to the power and example of tradition. Confucianism would later spread into the Korean peninsula and Goguryeo and toward Japan.
In the west, the Greek philosophical tradition, represented by Socrates, Plato, and Aristotle, was diffused throughout Europe and the Middle East in the 4th century BC by the conquests of Alexander III of Macedon, more commonly known as Alexander the Great. After the Bronze and Iron Age religions formed, the rise and spread of Christianity through the Roman world marked the end of Hellenistic philosophy and ushered in the beginnings of Medieval philosophy.
Science and technology.
In the history of technology and ancient science during the growth of the ancient civilizations, ancient technological advances were produced in engineering. These advances stimulated other societies to adopt new ways of living and governance.
The characteristics of Ancient Egyptian technology are indicated by a set of artifacts and customs that lasted for thousands of years. The Egyptians invented and used many basic machines, such as the ramp and the lever, to aid construction processes. The Egyptians also played an important role in developing Mediterranean maritime technology including ships and lighthouses.
The history of science and technology in India dates back to ancient times. The Indus Valley civilization yields evidence of hydrography, metrology and sewage collection and disposal being practiced by its inhabitants. Among the fields of science and technology pursued in India were Ayurveda, metallurgy, astronomy and mathematics. Some ancient inventions include plastic surgery, cataract surgery, Hindu-Arabic numeral system and Wootz steel.
The history of science and technology in China show significant advances in science, technology, mathematics, and astronomy. The first recorded observations of comets and supernovae were made in China. Traditional Chinese medicine, acupuncture and herbal medicine were also practiced.
Ancient Greek technology developed at an unprecedented speed during the 5th century BC, continuing up to and including the Roman period, and beyond. Inventions that are credited to the ancient Greeks such as the gear, screw, bronze casting techniques, water clock, water organ, torsion catapult and the use of steam to operate some experimental machines and toys. Many of these inventions occurred late in the Greek period, often inspired by the need to improve weapons and tactics in war. Roman technology is the engineering practice which supported Roman civilization and made the expansion of Roman commerce and Roman military possible over nearly a thousand years. The Roman Empire had the most advanced set of technology of their time, some of which may have been lost during the turbulent eras of Late Antiquity and the Early Middle Ages. Roman technological feats of many different areas, like civil engineering, construction materials, transport technology, and some inventions such as the mechanical reaper went unmatched until the 19th century.
Qanats which likely emerged on the Iranian plateau and possibly also in the Arabian peninsula sometime in the early 1st millennium BC spread from there slowly west- and eastward.
Maritime activity.
The history of ancient navigation began in earnest when men took to the sea in planked boats and ships propelled by sails hung on masts, like the Ancient Egyptian Khufu ship from the mid-3rd millennium BC. According to the Greek historian Herodotus, Necho II sent out an expedition of Phoenicians, which in three years sailed from the Red Sea around Africa to the mouth of the Nile. Many current historians tend to believe Herodotus on this point, even though Herodotus himself was in disbelief that the Phoenicians had accomplished the act.
Hannu was an ancient Egyptian explorer (around 2750 BC) and the first explorer of whom there is any knowledge. He made the first recorded exploring expedition, writing his account of his exploration in stone. Hannu travelled along the Red Sea to Punt, and sailed to what is now part of eastern Ethiopia and Somalia. He returned to Egypt with great treasures, including precious myrrh, metal and wood.
Warfare.
Ancient warfare is war as conducted from the beginnings of recorded history to the end of the ancient period. In Europe, the end of antiquity is often equated with the fall of Rome in 476. In China, it can also be seen as ending in the 5th century, with the growing role of mounted warriors needed to counter the ever-growing threat from the north.
The difference between prehistoric warfare and ancient warfare is less one of technology than of organization. The development of first city-states, and then empires, allowed warfare to change dramatically. Beginning in Mesopotamia, states produced sufficient agricultural surplus that full-time ruling elites and military commanders could emerge. While the bulk of military forces were still farmers, the society could support having them campaigning rather than working the land for a portion of each year. Thus, organized armies developed for the first time.
These new armies could help states grow in size and became increasingly centralized, and the first empire, that of the Sumerians, formed in Mesopotamia. Early ancient armies continued to primarily use bows and spears, the same weapons that had been developed in prehistoric times for hunting. Early armies in Egypt and China followed a similar pattern of using massed infantry armed with bows and spears.
Artwork and music.
[[File:Sassanid Music Plate 7thcentury.jpg|thumb|170px|right|Ancient Iranians attached great importance to music and poetry. 
7th-century silver plate. The British Museum.]]
Ancient music is music that developed in literate cultures, replacing prehistoric music. Ancient music refers to the various musical systems that were developed across various geographical regions such as Persia, India, China, Greece, Rome, Egypt and Mesopotamia (see music of Mesopotamia, music of ancient Greece, music of ancient Rome, Music of Iran). Ancient music is designated by the characterization of the basic audible tones and scales. It may have been transmitted through oral or written systems. Arts of the ancient world refers to the many types of art that were in the cultures of ancient societies, such as those of ancient China, Egypt, Greece, India, Persia, Mesopotamia and Rome.

</doc>
<doc id="51329" url="https://en.wikipedia.org/wiki?curid=51329" title="Famine">
Famine

A famine is a widespread scarcity of food, caused by several factors including crop failure, population imbalance, or government policies. This phenomenon is usually accompanied or followed by regional malnutrition, starvation, epidemic, and increased mortality. Nearly every continent in the world has experienced a period of famine throughout history. Some countries, particularly in sub-Sahara Africa, continue to have extreme cases of famine.
History.
The cyclical occurrence of famine has been a mainstay of societies engaged in subsistence agriculture since the dawn of agriculture itself. The frequency and intensity of famine has fluctuated throughout history, depending on changes in food demand, such as population growth, and supply-side shifts caused by changing climatic conditions. Famine was first eliminated in Holland and England during the 17th century, due to the commercialization of agriculture and the implementation of improved techniques to increase crop yields.
Decline of famine.
The feudal system of the Middle Ages, in which subsistence peasants worked on the land of a lord in return for protection, was not conducive to improvement or change, as neither the peasants nor the landlords had much economic incentive to increase the land's productivity.
In the 16th and 17th century, the feudal system began to break down, and more prosperous farmers began to enclose their own land and improve their yields to sell the surplus crops for a profit. These capitalist landowners paid their labourers with money, thereby increasing the commercialization of rural society. In the emerging competitive labour market, better techniques for the improvement of labour productivity were increasingly valued and rewarded. It was in the farmer's interest to produce as much as possible on their land in order to sell it to areas that demanded that product. They produced guaranteed surpluses of their crop every year if they could.
Subsistence peasants were also increasingly forced to commercialize their activities because of increasing taxes. Taxes that had to be paid to central governments in money forced the peasants to produce crops to sell. Sometimes they produced industrial crops, but they would find ways to increase their production in order to meet both their subsistence requirements as well as their tax obligations. Peasants also used the new money to purchase manufactured goods. The agricultural and social developments encouraging increased food production were gradually taking place throughout the 16th century, but took off in the early 17th century.
By the 1590s, these trends were sufficiently developed in the rich and commercialized province of Holland to allow its population to withstand a general outbreak of famine in Western Europe at that time. By that time, the Netherlands had one of the most commercialized agricultural systems in Europe. They grew many industrial crops such as flax, hemp and hops. Agriculture became increasingly specialized and efficient. The efficiency of Dutch agriculture allowed for much more rapid urbanization in the late sixteenth and early seventeenth centuries than anywhere else in Europe. As a result, productivity and wealth increased, allowing the Netherlands to maintain a steady food supply.
By 1650, English agriculture had also become commercialized on a much wider scale. The last peace-time famine in England was in 1623-24. There were still periods of hunger, as in the Netherlands, but no more famines ever occurred. Common areas for pasture were enclosed for private use and large scale, efficient farms were consolidated. Other technical developments included the draining of marshes, more efficient field use patterns, and the wider introduction of industrial crops. These agricultural developments led to wider prosperity in England and increasing urbanization. By the end of the 17th century, English agriculture was the most productive in Europe. In both England and the Netherlands, the population stabilized between 1650 and 1750, the same time period in which the sweeping changes to agriculture occurred. Famine still occurred in other parts of Europe, however. In East Europe, famines occurred as late as the twentieth century.
Attempts at famine alleviation.
Because of the severity of famine, it was a chief concern for governments and other authorities. In pre-industrial Europe, preventing famine, and ensuring timely food supplies, was one of the chief concerns of many governments, although they were severely limited in their options due to limited levels of external trade and an infrastructure and bureaucracy generally too rudimentary to affect real relief. Most governments were concerned by famine because it could lead to revolt and other forms of social disruption.
By the mid-19th century and the onset of the Industrial Revolution, it became possible for governments to alleviate the effects of famine through price controls, large scale importation of food products from foreign markets, stockpiling, rationing, regulation of production and charity. The Great Famine of 1845 in Ireland was one of the first famines to feature such intervention, although the government response was often lacklustre. The initial response of the British government to the early phase of the famine was "prompt and relatively successful," according to F. S. L. Lyons. Confronted by widespread crop failure in the autumn of 1845, Prime Minister Sir Robert Peel purchased £100,000 worth of maize and cornmeal secretly from America. Baring Brothers & Co initially acted as purchasing agents for the Prime Minister. The government hoped that they would not "stifle private enterprise" and that their actions would not act as a disincentive to local relief efforts. Due to weather conditions, the first shipment did not arrive in Ireland until the beginning of February 1846. The maize corn was then re-sold for a penny a pound.
In 1846, Peel moved to repeal the Corn Laws, tariffs on grain which kept the price of bread artificially high. The famine situation worsened during 1846 and the repeal of the Corn Laws in that year did little to help the starving Irish; the measure split the Conservative Party, leading to the fall of Peel's ministry. In March, Peel set up a programme of public works in Ireland.
Despite this promising start, the measures undertaken by Peel's successor, Lord John Russell, proved comparatively "inadequate" as the crisis deepened. Russell's ministry introduced public works projects, which by December 1846 employed some half million Irish and proved impossible to administer. The government was influenced by a laissez-faire belief that the market would provide the food needed. It halted government food and relief works, and turned to a mixture of "indoor" and "outdoor" direct relief; the former administered in workhouses through the Poor Law, the latter through soup kitchens.
A systematic attempt at creating the necessary regulatory framework for dealing with famine was developed by the British Raj in the 1880s. In order to comprehensively address the issue of famine, the British created an Indian Famine commission to recommend steps that the government would be required to take in the event of a famine. The Famine Commission issued a series of government guidelines and regulations on how to respond to famines and food shortages called the Famine Code. The famine code was also one of the first attempts to scientifically predict famine in order to mitigate its effects. These were finally passed into law in 1883 under Lord Ripon.
The Code introduced the first famine scale: three levels of food insecurity were defined: near-scarcity, scarcity, and famine. "Scarcity" was defined as three successive years of crop failure, crop yields of one-third or one-half normal, and large populations in distress. "Famine" further included a rise in food prices above 140% of "normal", the movement of people in search of food, and widespread mortality. The Commission identified that the loss of wages from lack of employment of agricultural labourers and artisans were the cause of famines. The Famine Code applied a strategy of generating employment for these sections of the population and relied on open-ended public works to do so.
20th century.
During the 20th century, an estimated 70 million people died from famines across the world, of whom an estimated 30 million died during the famine of 1958–61 in China. The other most notable famines of the century included the 1942–1945 disaster in Bengal, famines in China in 1928 and 1942, and a sequence of famines in the Soviet Union, including the Soviet famine of 1932-1933, caused by the policies of Stalin.
A few of the great famines of the late 20th century were: the Biafran famine in the 1960s, the Khmer Rouge-caused famine in Cambodia in the 1970s, the North Korean famine of the 1990s and the Ethiopian famine of 1984–85.
The latter event was reported on television reports around the world, carrying footage of starving Ethiopians whose plight was centered around a feeding station near the town of Korem. This stimulated the first mass movements to end famine across the world.
BBC newsreader Michael Buerk gave moving commentary of the tragedy on 23 October 1984, which he described as a "biblical famine". This prompted the Band Aid single, which was organized by Bob Geldof and featured more than 20 pop stars. The Live Aid concerts in London and Philadelphia raised even more funds for the cause. An estimated 900,000 people died within one year as a result of the famine, but the tens of millions of pounds raised by Band Aid and Live Aid are widely believed to have saved the lives of Ethiopians who were in danger of dying.
Regional history.
Africa.
In the mid-22nd century BC, a sudden and short-lived climatic change that caused reduced rainfall resulted in several decades of drought in Upper Egypt. The resulting famine and civil strife is believed to have been a major cause of the collapse of the Old Kingdom.
An account from the First Intermediate Period states, "All of Upper Egypt was dying of hunger and people were eating their children." In 1680s, famine extended across the entire Sahel, and in 1738 half the population of Timbuktu died of famine. In Egypt, between 1687 and 1731, there were six famines. The famine that afflicted Egypt in 1784 cost it roughly one-sixth of its population. The Maghreb experienced famine and plague in the late 18th century and early 19th century. There was famine in Tripoli in 1784, and in Tunis in 1785.
According to John Iliffe, "Portuguese records of Angola from the 16th century show that a great famine occurred on average every seventy years; accompanied by epidemic disease, it might kill one-third or one-half of the population, destroying the demographic growth of a generation and forcing colonists back into the river valleys."
The first documentation of weather in West-Central Africa occurs around the mid-16th to 17th centuries in areas such as Luanda Kongo, however, not much data was recorded on the issues of weather and disease except for a few notable documents. The only records obtained are of violence between Portuguese and Africans during the Battle of Mbilwa in 1665. In these documents the Portuguese wrote of African raids on Portuguese merchants solely for food, giving clear signs of famine. Additionally, instances of cannibalism by the African Jaga were also more prevalent during this time frame, indicating an extreme deprivation of a primary food source.
A notable period of famine occurred around the turn of the 20th century in the Congo Free State. In forming this state, Leopold used mass labor camps to finance his empire. This period resulted in the death of up to 10 million Congolese from brutality, disease and famine. Some colonial "pacification" efforts often caused severe famine, notably with the repression of the Maji Maji revolt in Tanganyika in 1906. The introduction of cash crops such as cotton, and forcible measures to impel farmers to grow these crops, sometimes impoverished the peasantry in many areas, such as northern Nigeria, contributing to greater vulnerability to famine when severe drought struck in 1913.
A large-scale famine occurred in Ethiopia in 1888 and succeeding years, as the rinderpest epizootic, introduced into Eritrea by infected cattle, spread southwards reaching ultimately as far as South Africa. In Ethiopia it was estimated that as much as 90 percent of the national herd died, rendering rich farmers and herders destitute overnight. This coincided with drought associated with an el Nino oscillation, human epidemics of smallpox, and in several countries, intense war. The Ethiopian Great famine that afflicted Ethiopia from 1888 to 1892 cost it roughly one-third of its population. In Sudan the year 1888 is remembered as the worst famine in history, on account of these factors and also the exactions imposed by the Mahdist state.
Records compiled for the Himba recall two droughts from 1910-1917. They were recorded by the Himba through a method of oral tradition. From 1910-1911 the Himba described the drought as "drought of the omutati seed" also called "omangowi", which means the fruit of an unidentified vine that people ate during the time period. From 1914-1916 droughts brought "katur' ombanda" or "kari' ombanda" which means "the time of eating clothing".
For the middle part of the 20th century, agriculturalists, economists and geographers did not consider Africa to be famine prone (they were much more concerned about Asia). There were notable counter-examples, such as the famine in Rwanda during World War II and the Malawi famine of 1949, but most famines were localized and brief food shortages. Although the drought was brief the main cause of death in Rwanda was due to Belgian prerogatives to acquisition grain from their colony (Rwanda). The increased grain acquisition was related to WW2. This and the drought caused 300,000 Rwandans to perish.
From 1967-1969 large scale famine occurred in Biafra and Nigeria due to a government blockade of the Breakaway territory. It is estimated that 1.5 million people died of starvation due to this famine. Additionally, drought and other government interference with the food supply caused 500 thousand Africans to perish in Central and West Africa.
Famine recurred in the early 1970s, when Ethiopia and the west African Sahel suffered drought and famine. The Ethiopian famine of that time was closely linked to the crisis of feudalism in that country, and in due course helped to bring about the downfall of the Emperor Haile Selassie. The Sahelian famine was associated with the slowly growing crisis of pastoralism in Africa, which has seen livestock herding decline as a viable way of life over the last two generations.
Famines occurred in Sudan in the late-1970s and again in 1990 and 1998. The 1980 famine in Karamoja, Uganda was, in terms of mortality rates, one of the worst in history. 21% of the population died, including 60% of the infants. In the 1980s, large scale multilayer drought occurred in the Sudan and Sahelian regions of Africa. This caused famine because even though the Sudanese Government believed there was a surplus of grain, there were local deficits across the region.
In October 1984, television reports describing the Ethiopian famine as "biblical", prompted the Live Aid concerts in London and Philadelphia, which raised large sums to alleviate the suffering. A primary cause of the famine (one of the largest seen in the country) is that Ethiopia (and the surrounding Horn) was still recovering from the droughts which occurred in the mid-late 1970s. Compounding this problem was the intermittent fighting due to civil war, the government's lack of organization in providing relief, and hoarding of supplies to control the population. Ultimately, over 1 million Ethiopians died and over 22 million people suffered due to the prolonged drought, which lasted roughly 2 years.
In 1992 Somalia became a war zone with no effective government, police, or basic services after the collapse of the dictatorship led by Siad Barre and the split of power between warlords. This coincided with a massive drought, causing over 300,000 Somalians to perish.
Since the start of the 21st century, more effective early warning and humanitarian response actions have reduced the number of deaths by famine markedly. That said, many African countries are not self-sufficient in food production, relying on income from cash crops to import food. Agriculture in Africa is susceptible to climatic fluctuations, especially droughts which can reduce the amount of food produced locally. Other agricultural problems include soil infertility, land degradation and erosion, swarms of desert locusts, which can destroy whole crops, and livestock diseases. Desertification is increasingly problematic: the Sahara reportedly spreads up to per year. The most serious famines have been caused by a combination of drought, misguided economic policies, and conflict. The 1983–85 famine in Ethiopia, for example, was the outcome of all these three factors, made worse by the Communist government's censorship of the emerging crisis. In Sudan at the same date, drought and economic crisis combined with denials of any food shortage by the then-government of President Gaafar Nimeiry, to create a crisis that killed perhaps 250,000 people—and helped bring about a popular uprising that overthrew Nimeiry.
Numerous factors make the food security situation in Africa tenuous, including political instability, armed conflict and civil war, corruption and mismanagement in handling food supplies, and trade policies that harm African agriculture. An example of a famine created by human rights abuses is the 1998 Sudan famine. AIDS is also having long-term economic effects on agriculture by reducing the available workforce, and is creating new vulnerabilities to famine by overburdening poor households. On the other hand, in the modern history of Africa on quite a few occasions famines acted as a major source of acute political instability. In Africa, if current trends of population growth and soil degradation continue, the continent might
be able to feed just 25% of its population by 2025, according to United Nations University (UNU)'s Ghana-based Institute for Natural Resources in Africa.
Recent famines in Africa include the 2005–06 Niger food crisis, the 2010 Sahel famine and the 2011 East Africa drought, where two consecutive missed rainy seasons precipitated the worst drought in East Africa in 60 years. An estimated 50,000 to 150,000 people are reported to have died during the period. In 2012, the Sahel drought put more than 10 million people in the western Sahel at risk of famine (according to a Methodist Relief & Development Fund (MRDF) aid expert), due to a month-long heat wave.
Today, famine is most widespread in Sub-Saharan Africa, but with exhaustion of food resources, overdrafting of groundwater, wars, internal struggles, and economic failure, famine continues to be a worldwide problem with hundreds of millions of people suffering. These famines cause widespread malnutrition and impoverishment. The famine in Ethiopia in the 1980s had an immense death toll, although Asian famines of the 20th century have also produced extensive death tolls. Modern African famines are characterized by widespread destitution and malnutrition, with heightened mortality confined to young children.
Against a backdrop of conventional interventions through the state or markets, alternative initiatives have been pioneered to address the problem of food security. An example is the "Community Area-Based Development Approach" to agricultural development ("CABDA"), an NGO programme with the objective of providing an alternative approach to increasing food security in Africa. CABDA proceeds through specific areas of intervention such as the introduction of drought-resistant crops and new methods of food production such as agro-forestry. Piloted in Ethiopia in the 1990s it has spread to Malawi, Uganda, Eritrea and Kenya. In an analysis of the programme by the Overseas Development Institute, CABDA's focus on individual and community capacity-building is highlighted. This enables farmers to influence and drive their own development through community-run institutions, bringing food security to their household and region.
Far East.
Chinese scholars had kept count of 1,828 instances of famine from 108 BC to 1911 in one province or another — an average of close to one famine per year. From 1333 to 1337 a terrible famine killed 6 million Chinese. The four famines of 1810, 1811, 1846, and 1849 are said to have killed no fewer than 45 million people.
Japan experienced more than 130 famines between 1603 and 1868.
The period from 1850 to 1873 saw, as a result of the Taiping Rebellion, drought, and famine, the population of China drop by over 30 million people. China's Qing Dynasty bureaucracy, which devoted extensive attention to minimizing famines, is credited with averting a series of famines following El Niño-Southern Oscillation-linked droughts and floods. These events are comparable, though somewhat smaller in scale, to the ecological trigger events of China's vast 19th-century famines. (Pierre-Etienne Will, "Bureaucracy and Famine") Qing China carried out its relief efforts, which included vast shipments of food, a requirement that the rich open their storehouses to the poor, and price regulation, as part of a state guarantee of subsistence to the peasantry (known as "ming-sheng").
When a stressed monarchy shifted from state management and direct shipments of grain to monetary charity in the mid-19th century, the system broke down. Thus the 1867–68 famine under the Tongzhi Restoration was successfully relieved but the Great North China Famine of 1877–78, caused by drought across northern China, was a catastrophe. The province of Shanxi was substantially depopulated as grains ran out, and desperately starving people stripped forests, fields, and their very houses for food. Estimated mortality is 9.5 to 13 million people. (Mike Davis,
The largest famine of the 20th century, and almost certainly of all time, was the 1958–61 Great Leap Forward famine in China. The immediate causes of this famine lay in Mao Zedong's ill-fated attempt to transform China from an agricultural nation to an industrial power in one huge leap. Communist Party cadres across China insisted that peasants abandon their farms for collective farms, and begin to produce steel in small foundries, often melting down their farm instruments in the process. Collectivisation undermined incentives for the investment of labor and resources in agriculture; unrealistic plans for decentralized metal production sapped needed labor; unfavorable weather conditions; and communal dining halls encouraged overconsumption of available food. Such was the centralized control of information and the intense pressure on party cadres to report only good news—such as production quotas met or exceeded—that information about the escalating disaster was effectively suppressed. When the leadership did become aware of the scale of the famine, it did little to respond, and continued to ban any discussion of the cataclysm. This blanket suppression of news was so effective that very few Chinese citizens were aware of the scale of the famine, and the greatest peacetime demographic disaster of the 20th century only became widely known twenty years later, when the veil of censorship began to lift.
The exact number of famine deaths during 1958–61 is difficult to determine, and estimates range from 18 to at least 42 million people, with a further 30 million cancelled or delayed births. It was only when the famine had wrought its worst that Mao was forced to reverse agricultural collectivisation policies, which were effectively dismantled in 1978. China has not experienced a famine of the proportions of the Great Leap Forward since 1961.
In 1975, the Khmer Rouge entered the capital of Phnom Penh and took control of Cambodia. The new government under Pol Pot drove all urban residents into the countryside to work on communal farm and civil work projects. No international relief would come until the Vietnamese army invaded in 1979 and liberated the country. While Pol Pot was in power, between one and three million people died out of a total population of eight million. Many were executed. Most died from malnourishment and exhaustion as a result of the famine caused by inept and negligent government officials.
Famine struck North Korea in the mid-1990s, set off by unprecedented floods. This Autarkic urban, industrial state depended on massive inputs of subsidised goods, including fossil fuels, primarily from the Soviet Union and the People's Republic of China. When the Soviet collapse and China's marketization switched trade to a hard currency, full price basis, North Korea's economy collapsed. The vulnerable agricultural sector experienced a massive failure in 1995–96, expanding to full-fledged famine by 1996–99.
Estimates based on the North Korean census suggest that 240,000 to 420,000 people died as a result of the famine and that there were 600,000 to 850,000 unnatural deaths in North Korea from 1993 to 2008. North Korea has not yet regained food self-sufficiency and relies on external food aid from China, Japan, South Korea, Russia and the United States. While Woo-Cumings have focused on the FAD side of the famine, Moon argues that FAD shifted the incentive structure of the authoritarian regime to react in a way that forced millions of disenfranchised people to starve to death (Moon, 2009).
Various famines have occurred in Vietnam. Japanese occupation during World War II caused the Vietnamese Famine of 1945, which caused 2 million deaths, or 10% of the population
then. Following the unification of the country after the Vietnam War, Vietnam experienced a food shortage in the 1980s, which prompted many people to flee the country.
India.
Owing to its almost entire dependence upon the monsoon rains, India is vulnerable to crop failures, which upon occasion deepen into famine. There were 14 famines in India between the 11th and 17th centuries (Bhatia, 1985). For example, during the 1022–1033 Great famines in India entire provinces were depopulated. Famine in Deccan killed at least two million people in 1702-1704. B.M. Bhatia believes that the earlier famines were localised, and it was only after 1860, during the British rule, that famine came to signify general shortage of foodgrains in the country. There were approximately 25 major famines spread through states such as Tamil Nadu in the south, and Bihar and Bengal in the east during the latter half of the 19th century.
Romesh Chunder Dutt argued as early as 1900, and present-day scholars such as Amartya Sen agree, that some historic famines were a product of both uneven rainfall and British economic and administrative policies, which since 1857 had led to the seizure and conversion of local farmland to foreign-owned plantations, restrictions on internal trade, heavy taxation of Indian citizens to support British expeditions in Afghanistan (see The Second Anglo-Afghan War), inflationary measures that increased the price of food, and substantial exports of staple crops from India to Britain. (Dutt, 1900 and 1902; Srivastava, 1968; Sen, 1982; Bhatia, 1985.) Some British citizens, such as William Digby, agitated for policy reforms and famine relief, but Lord Lytton, the governing British viceroy in India, opposed such changes in the belief that they would stimulate shirking by Indian workers. The first, the Bengal famine of 1770, is estimated to have taken around 10 million lives — one-third of Bengal's population at the time. Other notable famines include the Great Famine of 1876–78, in which 6.1 million to 10.3 million people died and the Indian famine of 1899–1900, in which 1.25 to 10 million people died. The famines were ended by the 20th century with the exception of the Bengal Famine of 1943–44 killing 1.5 million to 3 million Bengalis during World War II.
The observations of the Famine Commission of 1880 support the notion that food distribution is more to blame for famines than food scarcity. They observed that each province in British India, including Burma, had a surplus of foodgrains, and the annual surplus was 5.16 million tons (Bhatia, 1970). At that time, annual export of rice and other grains from India was approximately one million tons.
In 1966, there was a close call in Bihar, when the United States allocated 900,000 tons of grain to fight the famine. Three years of drought in India resulted in an estimated 1.5 million deaths from starvation and disease.
Middle East.
The Great Persian Famine of 1870–1871 is believed to have caused the death of 1.5 million persons (20–25 percent of the population) in Persia (present–day Iran).
In the early 20th century an Ottoman blockade of food being exported to Lebanon caused a famine which killed up to 450,000 Lebanese (about one-third of the population). The famine killed more people than the Lebanese Civil War. The blockade was caused by uprisings in the Syrian region of the Empire including one which occurred in the 1860s which lead to the massacre of thousands of Lebanese and Syrian by Ottoman Turks and local Druze 
Europe.
The Great Famine of 1315–1317 (or to 1322) was the first major food crisis to strike Europe in the 14th century. Millions in northern Europe died over an extended number of years, marking a clear end to the earlier period of growth and prosperity during the 11th and 12th centuries. Starting with bad weather in the spring of 1315, widespread crop failures lasted until the summer of 1317, from which Europe did not fully recover until 1322. It was a period marked by extreme levels of criminal activity, disease and mass death, infanticide, and cannibalism. It had consequences for Church, State, European society and future calamities to follow in the 14th century. There were 95 famines in medieval Britain, and 75 or more in medieval France. More than 10% of England's population, or at least 500,000 people, may have died during the famine of 1315–1316.
Famine was a very destabilizing and devastating occurrence. The prospect of starvation led people to take desperate measures. When scarcity of food became apparent to peasants, they would sacrifice long-term prosperity for short-term survival. They would kill their draught animals, leading to lowered production in subsequent years. They would eat their seed corn, sacrificing next year's crop in the hope that more seed could be found. Once those means had been exhausted, they would take to the road in search of food. They migrated to the cities where merchants from other areas would be more likely to sell their food, as cities had a stronger purchasing power than did rural areas. Cities also administered relief programs and bought grain for their populations so that they could keep order. With the confusion and desperation of the migrants, crime would often follow them. Many peasants resorted to banditry in order to acquire enough to eat.
One famine would often lead to difficulties in the following years because of lack of seed stock or disruption of routine, or perhaps because of less-available labour. Famines were often interpreted as signs of God's displeasure. They were seen as the removal, by God, of His gifts to the people of the Earth. Elaborate religious processions and rituals were made to prevent God's wrath in the form of famine.
During the Little Ice Age from the 15th century to the 18th century, famines in Europe became more frequent. This often led to a rise in conspiracy theories concerning the causes behind these famines, such as the Pacte de Famine in France.
The 1590s saw the worst famines in centuries across all of Europe. Famine had been relatively rare during the 16th century. The economy and population had grown steadily as subsistence populations tend to when there is an extended period of relative peace (most of the time). Subsistence peasant populations will almost always increase when possible since the peasants will try to spread the work to as many hands as possible. Although peasants in areas of high population density, such as northern Italy, had learned to increase the yields of their lands through techniques such as promiscuous culture, they were still quite vulnerable to famines, forcing them to work their land even more intensively.
The great famine of the 1590s began the period of famine and decline in the 17th century. The price of grain, all over Europe was high, as was the population. Various types of people were vulnerable to the succession of bad harvests that occurred throughout the 1590s in different regions. The increasing number of wage labourers in the countryside were vulnerable because they had no food of their own, and their meager living was not enough to purchase the expensive grain of a bad-crop year. Town labourers were also at risk because their wages would be insufficient to cover the cost of grain, and, to make matters worse, they often received less money in bad-crop years since the disposable income of the wealthy was spent on grain. Often, unemployment would be the result of the increase in grain prices, leading to ever-increasing numbers of urban poor.
All areas of Europe were badly affected by the famine in these periods, especially rural areas. The Netherlands was able to escape most of the damaging effects of the famine, though the 1590s were still difficult years there. Amsterdam's grain trade with the Baltic, guaranteed a food supply.
The years around 1620 saw another period of famine sweep across Europe. These famines were generally less severe than the famines of twenty-five years earlier, but they were nonetheless quite serious in many areas. Perhaps the worst famine since 1600, the great famine in Finland in 1696, killed one-third of the population.
Devastating harvest failures afflicted the northern Italian economy from 1618 to 1621, and it did not recover fully for centuries. There were serious famines in the late-1640s and less severe ones in the 1670s throughout northern Italy.
Over two million people died in two famines in France between 1693 and 1710. Both famines were made worse by ongoing wars.
As late as the 1690s, Scotland experienced famine which reduced the population of parts of Scotland by at least 15%.
The Great Famine of 1695–1697 may have killed a third of the Finnish population. and roughly 10% of Norway's population. Death rates rose in Scandinavia between 1740 and 1800 as the result of a series of crop failures. For instance, the Finnish famine of 1866–1868 killed 15% of the population.
The period of 1740–43 saw frigid winters and summer droughts, which led to famine across Europe and a major spike in mortality. The winter 1740-41 was unusually cold, possibly because of volcanic activity.
According to Scott and Duncan (2002), "Eastern Europe experienced more than 150 recorded famines between AD 1500 and 1700 and there were 100 hunger years and 121 famine years in Russia between AD 971 and 1974."
The Great Famine, which lasted from 1770 until 1771, killed about one tenth of Czech lands’ population, or 250,000 inhabitants, and radicalised countrysides leading to peasant uprisings.
There were sixteen good harvests and 111 famine years in northern Italy from 1451 to 1767. According to Stephen L. Dyson and Robert J. Rowland, "The Jesuits of Cagliari Sardinia recorded years during the late 1500s "of such hunger and so sterile that the majority of the people could sustain life only with wild ferns and other weeds" ... During the terrible famine of 1680, some 80,000 persons, out of a total population of 250,000, are said to have died, and entire villages were devastated..."
According to Bryson (1974), there were thirty-seven famine years in Iceland between 1500 and 1804. In 1783 the volcano Laki in south-central Iceland erupted. The lava caused little direct damage, but ash and sulphur dioxide spewed out over most of the country, causing three-quarters of the island's livestock to perish. In the following famine, around ten thousand people died, one-fifth of the population of Iceland. 1984, 152-153
Other areas of Europe have known famines much more recently. France saw famines as recently as the 19th century. The Great Famine in Ireland, 1846–1851, caused by the failure of the potato crop over a few years, resulted in 1,000,000 dead and another 2,000,000 refugees fleeing to Britain, Australia and the United States.
Famine still occurred in Eastern Europe during the 20th century. Droughts and famines in Imperial Russia are known to have happened every 10 to 13 years, with average droughts happening every 5 to 7 years. Russia experienced eleven major famines between 1845 and 1922, one of the worst being the famine of 1891–2.
Famines continued in the Soviet era, the most notorious being the "Holodomor" in various parts of the country, especially the Volga, and the Ukrainian and northern Kazakh SSR's during the winter of 1932–1933. The Soviet famine of 1932–1933 is nowadays reckoned to have cost an estimated 6 million lives. The last major famine in the USSR happened in 1947 due to the severe drought and the mismanagement of grain reserves by the Soviet government.
The Hunger Plan, i.e. the Nazi plan to starve large sections of the Soviet population, caused the deaths of many. The Russian Academy of Sciences in 1995 reported civilian victims in the USSR at German hands, including Jews, totalled 13.7 million dead, 20% of the 68 million persons in the occupied USSR. This included 4.1 million famine and disease deaths in occupied territory. There were an additional estimated 3 million famine deaths in areas of the USSR not under German occupation.
The 872 days of the Siege of Leningrad (1941–1944) caused unparalleled famine in the Leningrad region through disruption of utilities, water, energy and food supplies. This resulted in the deaths of about one million people.
Famine even struck in Western Europe during the Second World War. In the Netherlands, the "Hongerwinter" of 1944 killed approximately 30,000 people. Some other areas of Europe also experienced famine at the same time.
Latin America.
The pre-Columbian Americans often dealt with severe food shortages and famines. The persistent drought around 850 AD coincided with the collapse of Classic Maya civilization, and the famine of One Rabbit (AD 1454) was a major catastrophe in Mexico.
Brazil's 1877–78 "Grande Seca" (Great Drought), the worst in Brazil's history, caused approximately half a million deaths. The one from 1915 was devastating too.
Oceania.
Easter Island was hit by a great famine between the 15th and 18th centuries. Hunger and subsequent cannibalism was caused by overpopulation and depletion of natural resources as a result of deforestation, partly because work on megalithic monuments required a lot of wood.
There are other documented episodes of famine in various islands of Polynesia, such as occurred in Kau, Hawaii in 1868.
According to Daniel Lord Smail, "'Famine cannibalism' was until recently a regular feature of life in the islands of the Massim near New Guinea and of some other societies of Southeast Asia and the Pacific."
Relief technologies, including immunization, improved public health infrastructure, general food rations and supplementary feeding for vulnerable children, has provided temporary mitigation to the mortality impact of famines, while leaving their economic consequences unchanged, and not solving the underlying issue of too large a regional population relative to food production capability. Humanitarian crises may also arise from genocide campaigns, civil wars, refugee flows and episodes of extreme violence and state collapse, creating famine conditions among the affected populations.
Despite repeated stated intentions by the world's leaders to end hunger and famine, famine remains a chronic threat in much of Africa and Asia. In July 2005, the Famine Early Warning Systems Network labelled Niger with emergency status, as well as Chad, Ethiopia, South Sudan, Somalia and Zimbabwe. In January 2006, the United Nations Food and Agriculture Organization warned that 11 million people in Somalia, Kenya, Djibouti and Ethiopia were in danger of starvation due to the combination of severe drought and military conflicts. In 2006, the most serious humanitarian crisis in Africa was in Sudan's region Darfur.
Frances Moore Lappé, later co-founder of the Institute for Food and Development Policy (Food First) argued in "Diet for a Small Planet" (1971) that vegetarian diets can provide food for larger populations, with the same resources, compared to omnivorous diets.
Noting that modern famines are sometimes aggravated by misguided economic policies, political design to impoverish or marginalize certain populations, or acts of war, political economists have investigated the political conditions under which famine is prevented. Economist Amartya Sen states that the liberal institutions that exist in India, including competitive elections and a free press, have played a major role in preventing famine in that country since independence. Alex de Waal has developed this theory to focus on the "political contract" between rulers and people that ensures famine prevention, noting the rarity of such political contracts in Africa, and the danger that international relief agencies will undermine such contracts through removing the locus of accountability for famines from national governments.
The demographic impacts of famine are sharp. Mortality is concentrated among children and the elderly. A consistent demographic fact is that in all recorded famines, male mortality exceeds female, even in those populations (such as northern India and Pakistan) where there is a male longevity advantage during normal times. Reasons for this may include greater female resilience under the pressure of malnutrition, and possibly female's naturally higher percentage of body fat. Famine is also accompanied by lower fertility. Famines therefore leave the reproductive core of a population—adult women—lesser affected compared to other population categories, and post-famine periods are often characterized a "rebound" with increased births.
Even though the theories of Thomas Malthus would predict that famines reduce the size of the population commensurate with available food resources, in fact even the most severe famines have rarely dented population growth for more than a few years. The mortality in China in 1958–61, Bengal in 1943, and Ethiopia in 1983–85 was all made up by a growing population over just a few years. Of greater long-term demographic impact is emigration: Ireland was chiefly depopulated after the 1840s famines by waves of emigration.
Risk of future famine.
"The Guardian" reports that in 2007 approximately 40% of the world's agricultural land is seriously degraded. If current trends of soil degradation continue in Africa, the continent might be able to feed just 25% of its population by 2025, according to UNU's Ghana-based Institute for Natural Resources in Africa. As of late 2007, increased farming for use in biofuels, along with world oil prices at nearly $100 a barrel, has pushed up the price of grain used to feed poultry and dairy cows and other cattle, causing higher prices of wheat (up 58%), soybean (up 32%), and maize (up 11%) over the year. In 2007 Food riots have taken place in many countries across the world. An epidemic of stem rust, which is destructive to wheat and is caused by race Ug99, has in 2007 spread across Africa and into Asia.
Beginning in the 20th century, nitrogen fertilizers, new pesticides, desert farming, and other agricultural technologies began to be used to increase food production, in part to combat famine. Between 1950 and 1984, as the Green Revolution influenced agriculture, world grain production increased by 250%. However, as early as 1995, there were signs that these new developments may contribute to the decline of arable land (e.g. persistence of pesticides leading to soil contamination and decline of area available for farming). Developed nations have shared these technologies with developing nations with a famine problem.
David Pimentel, professor of ecology and agriculture at Cornell University, and Mario Giampietro, senior researcher at the National Research Institute on Food and Nutrition (INRAN), place in their study "Food, Land, Population and the U.S. Economy" the maximum U.S. population for a sustainable economy at 200 million.
According to geologist Dale Allen Pfeiffer, coming decades could see rising food prices without relief and massive starvation on a global level. Water deficits, which are already spurring heavy grain imports in numerous smaller countries, may soon do the same in larger countries, such as China or India. The water tables are falling in many countries (including Northern China, the US, and India) due to widespread overconsumption. Other countries affected include Pakistan, Iran, and Mexico. This will eventually lead to water scarcity and cutbacks in grain harvest. Even with the overpumping of its aquifers, China has developed a grain deficit, contributing to the upward pressure on grain prices. Most of the three billion people projected to be added worldwide by mid-century will be born in countries already experiencing water shortages.
After China and India, there is a second tier of smaller countries with large water deficits — Algeria, Egypt, Iran, Mexico, and Pakistan. Four of these already import a large share of their grain. Only Pakistan remains marginally self-sufficient. But with a population expanding by 4 million a year, it will also soon turn to the world market for grain. According to a UN climate report, the Himalayan glaciers that are the principal dry-season water sources of Asia's biggest rivers - Ganges, Indus, Brahmaputra, Yangtze, Mekong, Salween and Yellow - could disappear by 2350 as temperatures rise and human demand rises. Approximately 2.4 billion people live in the drainage basin of the Himalayan rivers. India, China, Pakistan, Afghanistan, Bangladesh, Nepal and Myanmar could experience floods followed by severe droughts in coming decades. In India alone, the Ganges provides water for drinking and farming for more than 500 million people.
Evan Fraser, a geographer at the University of Guelph in Ontario Canada, explores the ways in which climate change may affect future famines. To do this he draws on a range of historic cases where relatively small environmental problems triggered famines as a way of creating theoretical links between climate and famine in the future. Drawing on situations as diverse as the Great Irish Potato Famine, a series of weather induced famines in Asia during the late 19th century, and famines in Ethiopia during the 1980s, he concludes there are three “lines of defense” that protect a community’s food security from environmental change. The first line of defense is the agro-ecosystem on which food is produced: diverse ecosystems with well managed soils high in organic matter tend to be more resilient. The second line of defense is the wealth and skills of individual households: if those households affected by bad weather such as drought have savings or skills they may be able to do all right despite the bad weather. The final line of defense is created by the formal institutions present in a society. Governments, churches, or NGOs must be willing and able to mount effective relief efforts. Pulling this together, Evan Fraser argues that if an ecosystem is resilient enough, it may be able to withstand weather-related shocks. But if these shocks overwhelm the ecosystem’s line of defense, it is necessary for the household to adapt using its skills and savings. If a problem is too big for the family or household, then people must rely on the third line of defense, which is whether or not the formal institutions present in a society are able to provide help. Evan Fraser concludes that in almost every situation where an environmental problem triggered a famine you see a failure in each of these three lines of defense. Hence, understanding how climate change may cause famines in the future requires combining both an assessment of local socio-economic and environmental factors along with climate models that predict where bad weather may occur in the future
Causes.
Definitions of famines are based on three different categories – these include food supply-based, food consumption-based and mortality-based definitions. Some definitions of famines are:
Food shortages in a population are caused either by a lack of food or by difficulties in food distribution; it may be worsened by natural climate fluctuations and by extreme political conditions related to oppressive government or warfare. The conventional explanation until 1981 for the cause of famines was the Food availability decline (FAD) hypothesis. The assumption was that the central cause of all famines was a decline in food availability. However, FAD could not explain why only a certain section of the population such as the agricultural laborer was affected by famines while others were insulated from famines. Based on the studies of some recent famines, the decisive role of FAD has been questioned and it has been suggested that the causal mechanism for precipitating starvation includes many variables other than just decline of food availability. According to this view, famines are a result of entitlements, the theory being proposed is called the "failure of exchange entitlements" or FEE. A person may own various commodities that can be exchanged in a market economy for the other commodities he or she needs. The exchange can happen via trading or production or through a combination of the two. These entitlements are called trade-based or production-based entitlements. Per this proposed view, famines are precipitated due to a breakdown in the ability of the person to exchange his entitlements. An example of famines due to FEE is the inability of an agricultural laborer to exchange his primary entitlement, i.e., labor for rice when his employment became erratic or was completely eliminated.
According to the Physicians for Social Responsibility (PSR), global climate change is additionally challenging the Earth's ability to produce food, potentially leading to famine.
Some elements make a particular region more vulnerable to famine. These include poverty, population growth, an inappropriate social infrastructure, a suppressive political regime, and a weak or under-prepared government.
Climate and population pressure.
Many famines are caused by imbalance of food production compared to the large populations of countries whose population exceeds the regional carrying capacity . Historically, famines have occurred from agricultural problems such as drought, crop failure, or pestilence. Changing weather patterns, the ineffectiveness of medieval governments in dealing with crises, wars, and epidemic diseases such as the Black Death helped to cause hundreds of famines in Europe during the Middle Ages, including 95 in Britain and 75 in France. In France, the Hundred Years' War, crop failures and epidemics reduced the population by two-thirds.
The failure of a harvest or change in conditions, such as drought, can create a situation whereby large numbers of people continue to live where the carrying capacity of the land has temporarily dropped radically. Famine is often associated with subsistence agriculture. The total absence of agriculture in an economically strong area does not cause famine; Arizona and other wealthy regions import the vast majority of their food, since such regions produce sufficient economic goods for trade.
Famines have also been caused by volcanism. The 1815 eruption of the Mount Tambora volcano in Indonesia caused crop failures and famines worldwide and caused the worst famine of the 19th century. The current consensus of the scientific community is that the aerosols and dust released into the upper atmosphere causes cooler temperatures by preventing the sun's energy from reaching the ground. The same mechanism is theorized to be caused by very large meteorite impacts to the extent of causing mass extinctions.
State-sponsored famines.
In certain cases, such as the Great Leap Forward in China (which produced the largest famine in absolute numbers), North Korea in the mid-1990s, or Zimbabwe in the early-2000s, famine can occur because of government policy.
In 1932, under the rule of the USSR, Ukraine experienced one of its largest famines when between 2.4 and 7.5 million peasants died as a result of a state sponsored famine. It was termed the Holodomor, suggesting that it was a deliberate campaign of repression designed to eliminate resistance to collectivization. Forced grain quotas imposed upon the rural peasants and a brutal reign of terror contributed to the widespread famine. The Soviet government continued to deny the problem and it did not provide aid to the victims nor did it accept foreign aid.
In 1958 in China, Mao Zedong's Communist Government launched the Great Leap Forward campaign, aimed at rapidly industrializing the country. The government forcibly took control of agriculture. Barely enough grain was left for the peasants, and starvation occurred in many rural areas. Exportation of grain continued despite the famine and the government attempted to conceal it. While the famine is attributed to unintended consequences, it is believed that the government refused to acknowledge the problem, thereby further contributing to the deaths. In many instances, peasants were persecuted. Between 20 and 45 million people perished in this famine, making it one of the deadliest famines to date.
Malawi ended its famine by subsidizing farmers despite the strictures imposed by the World Bank. During the 1973 Wollo Famine in Ethiopia, food was shipped out of Wollo to the capital city of Addis Ababa, where it could command higher prices. In the late-1970s and early-1980s, residents of the dictatorships of Ethiopia and Sudan suffered massive famines, but the democracy of Botswana avoided them, despite also suffering a severe drop in national food production. In Somalia, famine occurred because of a failed state.
Famine prevention.
Food security.
Long term measures to improve food security, include investment in modern agriculture techniques, such as fertilizers and irrigation, but can also include strategic national food storage. In addition, recent work has indicated that the entire human population could be fed in the event of global catastrophe if alternative foods were used.
World Bank strictures restrict government subsidies for farmers, and increasing use of fertilizers is opposed by some environmental groups because of its unintended consequences: adverse effects on water supplies and habitat.
The effort to bring modern agricultural techniques found in the Western world, such as nitrogen fertilizers and pesticides, to Asia, called the Green Revolution, resulted in decreases in malnutrition similar to those seen earlier in Western nations. This was possible because of existing infrastructure and institutions that are in short supply in Africa, such as a system of roads or public seed companies that made seeds available. Supporting farmers in areas of food insecurity, through such measures as free or subsidized fertilizers and seeds, increases food harvest and reduces food prices.
The World Bank and some rich nations press nations that depend on them for aid to cut back or eliminate subsidized agricultural inputs such as fertilizer, in the name of privatization even as the United States and Europe extensively subsidized their own farmers.
Relief.
There is a growing realization among aid groups that giving cash or cash vouchers instead of food is a cheaper, faster, and more efficient way to deliver help to the hungry, particularly in areas where food is available but unaffordable. The United Nations' World Food Program (WFP), the biggest non-governmental distributor of food, announced that it will begin distributing cash and vouchers instead of food in some areas, which Josette Sheeran, the WFP's executive director, described as a "revolution" in food aid. The aid agency Concern Worldwide is piloting a method through a mobile phone operator, Safaricom, which runs a money transfer program that allows cash to be sent from one part of the country to another.
However, for people in a drought living a long way from and with limited access to markets, delivering food may be the most appropriate way to help. Fred Cuny stated that "the chances of saving lives at the outset of a relief operation are greatly reduced when food is imported. By the time it arrives in the country and gets to people, many will have died." US Law, which requires buying food at home rather than where the hungry live, is inefficient because approximately half of what is spent goes for transport. Fred Cuny further pointed out "studies of every recent famine have shown that food was available in-country — though not always in the immediate food deficit area" and "even though by local standards the prices are too high for the poor to purchase it, it would usually be cheaper for a donor to buy the hoarded food at the inflated price than to import it from abroad."
Deficient micronutrients can be provided through fortifying foods. Fortifying foods such as peanut butter sachets (see Plumpy'Nut) have revolutionized emergency feeding in humanitarian emergencies because they can be eaten directly from the packet, do not require refrigeration or mixing with scarce clean water, can be stored for years and, vitally, can be absorbed by extremely ill children.
WHO and other sources recommend that malnourished children - and adults who also have diarrhea - drink rehydration solution, and continue to eat, in addition to antibiotics, and zinc supplements. There is a special oral rehydration solution called ReSoMal which has less sodium and more potassium than standard solution. However, if the diarrhea is severe, the standard solution is preferable as the person needs the extra sodium. Obviously, this is a judgment call best made by a physician, and using either solution is better than doing nothing. Zinc supplements often can help reduce the duration and severity of diarrhea, and Vitamin A can also be helpful. The World Health Organization underlines the importance of a person with diarrhea continuing to eat, with a 2005 publication for physicians stating: “Food should "never" be withheld and the child's usual foods should "not" be diluted. Breastfeeding should "always" be continued."
Ethiopia has been pioneering a program that has now become part of the World Bank's prescribed recipe for coping with a food crisis and had been seen by aid organizations as a model of how to best help hungry nations. Through the country's main food assistance program, the Productive Safety Net Program, Ethiopia has been giving rural residents who are chronically short of food, a chance to work for food or cash. Foreign aid organizations like the World Food Program were then able to buy food locally from surplus areas to distribute in areas with a shortage of food.
The Green Revolution was widely viewed as an answer to famine in the 1970s and 1980s. Between 1950 and 1984, hybrid strains of high-yielding crops transformed agriculture around the globe and world grain production increased by 250%. Some criticize the process, stating that these new high-yielding crops require more chemical fertilizers and pesticides, which can harm the environment. Although these high-yielding crops make it technically possible to feed more people, there are indications that regional food production has peaked in many world sectors, due to certain strategies associated with intensive agriculture such as groundwater overdrafting and overuse of pesticides and other agricultural chemicals.
Levels of food insecurity.
In modern times, local and political governments and non-governmental organizations that deliver famine relief have limited resources with which to address the multiple situations of food insecurity that are occurring simultaneously. Various methods of categorizing the gradations of food security have thus been used in order to most efficiently allocate food relief. One of the earliest were the Indian Famine Codes devised by the British in the 1880s. The Codes listed three stages of food insecurity: near-scarcity, scarcity and famine, and were highly influential in the creation of subsequent famine warning or measurement systems. The early warning system developed to monitor the region inhabited by the Turkana people in northern Kenya also has three levels, but links each stage to a pre-planned response to mitigate the crisis and prevent its deterioration
The experiences of famine relief organizations throughout the world over the 1980s and 1990s resulted in at least two major developments: the "livelihoods approach" and the increased use of nutrition indicators to determine the severity of a crisis. Individuals and groups in food stressful situations will attempt to cope by rationing consumption, finding alternative means to supplement income, etc., before taking desperate measures, such as selling off plots of agricultural land. When all means of self-support are exhausted, the affected population begins to migrate in search of food or fall victim to outright mass starvation. Famine may thus be viewed partially as a social phenomenon, involving markets, the price of food, and social support structures. A second lesson drawn was the increased use of rapid nutrition assessments, in particular of children, to give a quantitative measure of the famine's severity.
Since 2003, many of the most important organizations in famine relief, such as the World Food Programme and the U.S. Agency for International Development, have adopted a five-level scale measuring intensity and magnitude. The intensity scale uses both livelihoods' measures and measurements of mortality and child malnutrition to categorize a situation as food secure, food insecure, food crisis, famine, severe famine, and extreme famine. The number of deaths determines the magnitude designation, with under 1000 fatalities defining a "minor famine" and a "catastrophic famine" resulting in over 1,000,000 deaths.
Society and culture.
Famine personified as an allegory is found in some cultures, e.g. one of the Four Horsemen of the Apocalypse in Christian tradition, the fear gorta of Irish folklore, or the Wendigo of Algonquian tradition.

</doc>
<doc id="51330" url="https://en.wikipedia.org/wiki?curid=51330" title="Babington Plot">
Babington Plot

The Babington Plot was a plot in 1586 to assassinate Queen Elizabeth, a Protestant, and put the rescued Mary, Queen of Scots, her Roman Catholic cousin, on the English throne. It led to the execution of Queen Mary Stuart of Scotland as a direct result of a letter sent by Mary (who had been imprisoned for 18 years since 1568 in England at the behest of Elizabeth) in which she consented directly to the assassination of Elizabeth.
The long-term goal of the plot was the invasion of England by the Spanish forces of King Philip II and the Catholic League in France, leading to the restoration of the old religion. The plot was discovered by Elizabeth's "spymaster" Sir Francis Walsingham and used to entrap Mary for the purpose of removing her as a claimant to the English throne.
The chief conspirators were Anthony Babington, a young recusant recruited by John Ballard, a Jesuit priest who desired to rescue the Scottish Queen; Robert Poley; Gilbert Gifford, and Thomas Phelippes, a Walsingham spy agent and cryptanalyst. The turbulent Catholic deacon Gifford had been in Walsingham's service since the end of 1585 or the beginning of 1586. Gifford obtained a letter of introduction to Queen Mary from a confidant and spy for her, Thomas Morgan. Walsingham then placed double agent Gifford and spy decipherer Phelippes inside Chartley Castle, where Queen Mary was imprisoned. Gifford organised the Walsingham plan to place Babington's and Queen Mary's encrypted communications into a beer barrel cork which were then intercepted by Phelippes, decoded and sent to Walsingham.
Ballard was attempting to recruit Babington in an undeveloped scheme to rescue Mary and place her on the throne of England by killing Elizabeth. Babington sent a coded letter to the imprisoned Mary, which gave his name to the complicated multiple-sided plot.
On 7 July 1586, the only Babington letter that was sent to Mary was decoded by Phelippes. Mary responded in code on 17 July ordering the would-be rescuers to assassinate Elizabeth. The response letter also included deciphered phrases indicating her desire to be rescued: "The affairs being thus prepared" and "I may suddenly be transported out of this place". At the Fotheringay trial in October 1586, Elizabeth's Secretary of State William Cecil and Walsingham used the letter against Mary who refused to admit that she was guilty. But she was betrayed by her secretaries Nau and Curle who confessed under pressure that the letter was mainly truthful.
Mary's imprisonment.
Mary, Queen of Scots, a Roman Catholic, was a legitimate heir to the throne of England. In 1568 she escaped imprisonment by Scottish rebels and sought the promised aid of her cousin, Queen Elizabeth I, a year after her forced abdication from the throne of Scotland. The issuance of the papal bull "Regnans in Excelsis" by Pope Pius V on 25 February 1570, granted English Catholics authority to overthrow the English queen. Queen Mary became the focal point of numerous plots and intrigues to restore England to its former religion, Catholicism, and to depose Elizabeth and even to take her life. Rather than the promised aid, Elizabeth imprisoned Mary for nineteen years in the charge of a succession of jailers, principally the Earl of Shrewsbury.
In 1584 Elizabeth's Privy Council signed a "Bond of Association" designed by Cecil and Walsingham which stated that anyone within the line of succession to the throne "on whose behalf" anyone plotted against the Queen, would be excluded from the line and executed. This was agreed upon by hundreds of Englishmen, who likewise signed the Bond. Mary also agreed to sign the Bond. The following year, Parliament passed the Act of Association, which provided for the execution of anyone who would benefit from the death of the Queen if a plot against her was discovered. However, due to the Bond, Mary could be executed if a plot was initiated by others that could lead to her accession to England's throne.
Elizabeth ordered Queen Mary transferred back to the ruined Tutbury Castle in the wintry weather of Christmas Eve 1584. Mary became ill due to the bad conditions of her captivity, imprisoned in a very damp cold room with closed windows and with no access to the sun.
In 1585, Elizabeth ordered Queen Mary to be transferred in a coach and under heavy guard and placed under the strictest confinement at Chartley Hall in Staffordshire, under the control of Sir Amias Paulet. She was prohibited any correspondence with the outside world. Puritan Paulet was chosen by Queen Elizabeth in part because he abhorred Queen Mary's Catholic faith.
Reacting to the growing threat posed by Catholics, urged on by the Pope and other Catholic monarchs in Europe, Sir Francis Walsingham, Elizabeth I's Secretary of State and spymaster, together with William Cecil, Elizabeth's chief advisor, realised that if Mary could be implicated in a plot to assassinate Elizabeth, she could be executed and the "Papist" threat diminished. As he wrote to the Earl of Leicester: "So long as that devilish woman lives, neither Her Majesty must make account to continue in quiet possession of her crown, nor her faithful servants assure themselves of safety of their lives."
Walsingham used the Babington plot to ensnare Queen Mary by sending his double agent, Gilbert Gifford to Paris to obtain the confidence of Morgan, then locked in the Bastille. Morgan previously worked for George Talbot, 6th Earl of Shrewsbury, an earlier jailor of Queen Mary. Through Shrewsbury, Queen Mary became acquainted with Morgan. Queen Mary sent Morgan to Paris to deliver letters to the French court. While in Paris Morgan became involved in a previous plot designed by William Parry, which resulted in Morgan's incarceration in the Bastille. In 1585 Gifford was arrested returning to England through Rye in Sussex with letters of introduction from Morgan to Queen Mary. Walsingham released Gifford to work as a double agent, in the Babington Plot. Gifford was assigned the alias "No. 4" and used many others in his espionage work, such as Colerdin, Pietro and Cornelys. Walsingham assigned Gifford to function as a courier in the entrapment plot against Queen Mary.
The plot.
The Babington plot was related to several separate plans:
At the behest of Mary's French supporters, John Ballard, a Jesuit priest and agent of the Roman Church, went to England on various occasions in 1585 to secure promises of aid from the northern Catholic gentry on behalf of Queen Mary. In March 1586, he met with John Savage, an ex-soldier who was involved in a separate plot against Elizabeth and who had sworn an oath to assassinate the queen. He was resolved in this plot after consulting with three friends, Dr. William Gifford, Christopher Hodgson (priest) and Gilbert Gifford. Gifford had been arrested by Walsingham and agreed to be a double agent. While it is certain that Gifford was already in Walsingham's employ by the time Savage was going ahead with the plot, according to Conyers Read () Later that same year, Gifford reported to Charles Paget and Don Bernardino de Mendoza, and told them that English Catholics were prepared to mount an insurrection against Elizabeth, provided that they would be assured of foreign support. While it was uncertain whether Ballard's report of the extent of Catholic opposition was accurate, what was certain that he was able to secure assurances that support would be forthcoming. He then returned to England, where he persuaded a member of the Catholic gentry, Anthony Babington to lead and organise the English Catholics against Elizabeth. Ballard informed Babington about all the plans that had been so far proposed. Babington's later confession made it clear that Ballard was sure of the support of the Catholic League:
Despite this assurance of foreign support, Babington was hesitant, as he thought that no foreign invasion would succeed for as long as Elizabeth remained, to which Ballard answered that the plans of John Savage would take care of that. After a lengthy discussion with friends and soon-to-be fellow conspirators, Babington consented to join and to lead the conspiracy.
Unfortunately for the conspirators, Walsingham was certainly aware of some of the aspects of the plot, based on reports by his spies, most notably Gilbert Gifford, who kept tabs on all the major participants. While he could have shut down some part of the plot and arrested some of those involved within reach, he still lacked any piece of evidence that would prove Queen Mary's active participation in the plot and he feared to commit any mistake which might cost Elizabeth her life.
Infiltration.
Walsingham and Cecil realised that the July 1584 decree by Queen Elizabeth after the Throckmorton plot that prevented all communication to and from Mary, also impaired their ability to entrap her in another plot. They needed evidence of another plot for which she could be executed based on their Bond of Association tenets. Thus Walsingham established a new line of communication, one which he could carefully control without incurring any suspicion from Mary. Gifford approached the French ambassador to England, Guillaume de l'Aubespine, Baron de Châteauneuf-sur-Cher, and described the new correspondence arrangement that had been designed by Walsingham. Gifford and jailer Paulet had arranged for a local brewer to facilitate the movement of messages between Queen Mary and her supporters by placing them in a watertight casing that could be placed inside the stopper of the barrel. Thomas Phelippes, a cipher and language expert in Walsingham's employ, was then quartered at Chartley Hall to receive the messages, decode them and send them to Walsingham. Gifford submitted a code table—that had been supplied by Walsingham—to Chateauneuf and then requested the first message be sent to Mary.
All subsequent messages to Mary would be sent via diplomatic packets to Chateauneuf, who then passed them on to Gifford. Gifford would pass them on to Walsingham, who would confide them to Phelippes. The cipher used was a nomenclator cipher. Phelippes would decode and make a copy of the letter. The letter was then resealed and given back to Gifford, who would pass it on to the brewer. The brewer would then "smuggle" the letter to Mary. If Mary sent a letter to her supporters, it would go through the reverse process. In short order, every message coming to and from Chartley Hall was intercepted and read by Walsingham.
The fatal correspondence.
This letter was received by Mary, who was in a dark mood at that period of time because she received the news that her son betrayed her in favour of Elizabeth, on 14 July 1586 — after being intercepted and deciphered — and on 17 July she replied to Babington in a long letter in which she outlined the components of a successful rescue and the need to assassinate Elizabeth if her rescue had any chance to be successful. She also stressed the necessity of foreign aid if the rescue attempt was to succeed:
Mary in her response letter, advised the would-be rescuers to confront the Puritans and to link her case to the Queen of England as her heir.
Mary was clear in her support for the murder of Elizabeth if that would have led to her liberty and Catholic domination of England. In addition, Queen Mary supported in that letter, and in another one to Ambassador Mendoza, a Spanish invasion of England.
The letter was again intercepted and deciphered by Phelippes. But this time, Phelippes, at the direction of Walsingham, kept the original and made a copy of the letter in which Mary supported the murder of Elizabeth, adding the names of the conspirators:
Arrests, trials and executions.
John Ballard was arrested on 4 August 1586, and under torture he confessed and implicated Babington. Although Babington was able to receive the letter with the postscript, he was not able to reply with the names of the conspirators, as he was arrested. Others were taken prisoner by 15 August 1586. Mary's two secretaries, Claude Nau de la Boisseliere (died 1605) and Gilbert Curle (died 1609), were likewise taken into custody and interrogated.
The conspirators were sentenced to death for treason and conspiracy against the crown, and were sentenced to be hanged, drawn, and quartered. This first group included Babington, Ballard, Chidiock Tichborne, Sir Thomas Salisbury, Robert Barnewell, John Savage and Henry Donn. A further group of seven men, Edward Havington, Charles Tilney, Edward Jones, John Charnock, John Travers, Jerome Bellamy, and Robert Gage, were tried and convicted shortly afterward. Ballard and Babington were executed on 20 September 1586 along with the other men who had been tried with them. Such was the public outcry at the horror of their execution that Elizabeth changed the order for the second group to be allowed to hang until dead before being disembowelled.
In October 1586 Mary was sent to be tried at Fotheringhay Castle in Northamptonshire by 46 English Lords, Bishops and Earls. She was not permitted legal counsel, not permitted to review the evidence against her or to provide witnesses. Portions of Phellipes' letter translations were read at the trial. As the Scottish Queen, Mary was convicted of treason against the foreign country of England. One English Lord voted not guilty. Elizabeth signed her cousin's death warrant, and on 8 February 1587, in front of 300 witnesses, Mary, Queen of Scots, was executed by beheading.
In literature.
"Mary Stuart" (), a dramatised version of the last days of Mary, Queen of Scots, including the Babington Plot, was written by Friedrich Schiller and performed in Weimar, Germany in 1800. This in turn formed the basis for "Maria Stuarda", an opera by Donizetti, in 1835: although the Babington Plot occurs before the events of the opera, and is only referenced twice during the opera, the second such occasion being Mary admitting her own part in it, in private, to her final confessor (a role taken by Lord Talbot in the opera, although not in real life).
The story of the Babington Plot is dramatised in the novel "Conies in the Hay" by Jane Lane(ISBN 0-7551-0835-3), and features prominently in Anthony Burgess's "A Dead Man in Deptford". Episode Four of the television series "Elizabeth R" (titled "Horrible Conspiracies") is devoted to the Babington Plot, and the movie "" deals substantially with the Plot as well. A more fictional account is given in the "My Story" book series, "The Queen's Spies" (retitled "To Kill A Queen" 2008) told in diary format by a fictional Elizabethan girl, Kitty.
The Babington plot is also the subject of the children's novel "A Traveller in Time" by Alison Uttley, who grew up near the Babington family home in Derbyshire.

</doc>
<doc id="51331" url="https://en.wikipedia.org/wiki?curid=51331" title="Dimensionless quantity">
Dimensionless quantity

In dimensional analysis, a dimensionless quantity is a quantity to which no physical dimension is applicable. It is thus a bare number, and is therefore also known as a quantity of dimension one. Dimensionless quantities are widely used in many fields, such as mathematics, physics, engineering, and economics. Numerous well-known quantities, such as pi, Euler's number, and Golden ratio, are dimensionless. By contrast, examples of quantities with dimensions are length, time, and speed, which are measured in dimensional units, such as meter, second and meter/second.
Dimensionless quantities are often obtained as products or ratios of quantities that are not dimensionless, but whose dimensions cancel out in the mathematical operation. This is the case, for instance, with the engineering strain, a measure of deformation. It is defined as change in length, divided by initial length, but because these quantities both have dimensions "L" (length), the result is a dimensionless quantity.
Properties.
All pure numbers are dimensionless quantities.
A dimensionless quantity may have dimensionless units, even though it has no physical dimension associated with it. For example, to show the quantity being measured (for example mass fraction or mole fraction), it is sometimes helpful to use the same units in both the numerator and denominator (kg/kg or mol/mol). The quantity may also be given as a ratio of two different units that have the same dimension (for instance, light years over meters). This may be the case when calculating slopes in graphs, or when making unit conversions. Such notation does not indicate the presence of physical dimensions, and is purely a notational convention. Other common dimensionless units are  % (= 0.01),  ‰ (= 0.001), ppm (= 10−6), ppb (= 10−9), ppt (= 10−12), angle units (degrees, radians, grad), dalton and mole. Units of number such as the dozen, gross, and googol are also dimensionless.
When a quantity is the ratio of two other quantities, each of the same dimension, the defined quantity is dimensionless and has the same value regardless of the units used to calculate the two composing quantities. For instance, if body A exerts a force of magnitude "F" on body B, and B exerts a force of magnitude "f" on A, then the ratio "F"/"f" is always equal to 1, regardless of the actual units used to measure "F" and "f". This is a fundamental property of dimensionless proportions and follows from the assumption that the laws of physics are independent of the system of units used in their expression. In this case, if the ratio "F"/"f" was not always equal to 1, but changed if one switched from SI to CGS, that would mean that Newton's Third Law's truth or falsity would depend on the system of units used, which would contradict this fundamental hypothesis. This assumption that the laws of physics are not contingent upon a specific unit system is the basis for the Buckingham π theorem, as discussed in a later section.
Examples.
There are many areas where dimensionless quantities are used. Some quantities are given here to illustrate the properties more concretely in their respective areas of application. The list of dimensionless quantities contains an extensive number of other important examples.
Mathematics.
Several examples from mathematics include proportions, angles, and special numbers. A simple problem illustrates how a proportion may be a dimensionless quantity. Sarah says, "Out of every 10 apples I gather, 1 is rotten." The rotten-to-gathered ratio is (1 apple) / (10 apples) = 0.1 = 10%, which is a dimensionless quantity. Similarly, angles may be defined as a proportion. The radian measure of angles is the ratio of the length of a circle's arc subtended by an angle whose vertex is the centre of the circle to some other length. The ratio—i.e., length divided by length—is dimensionless. When using radians as the unit, the length that is compared is the length of the radius of the circle. When using degree as the units, the arc's length is compared to 1/360 of the circumference of the circle. In the case of the dimensionless quantity π, being the ratio of a circle's circumference to its diameter, the number would be constant regardless of what unit is used to measure a circle's circumference and diameter (e.g., centimetres, miles, light-years, etc.), as long as the same unit is used for both. Additionally, the golden ratio, Golden ratio, is simply the ratio of length of the two sides of a golden rectangle. In statistics the coefficient of variation is the ratio of the standard deviation to the mean and is used to measure the dispersion in the data.
Dimensionless physical constants.
Certain fundamental physical constants, such as the speed of light in a vacuum, the universal gravitational constant, Planck's constant and Boltzmann's constant can be normalized to 1 if appropriate units for time, length, mass, charge, and temperature are chosen. The resulting system of units is known as the natural units. However, not all physical constants can be normalized in this fashion. For example, the values of the following constants are independent of the system of units and must be determined experimentally:
Origin and derivation.
History.
Dimensionless quantities are a special result of the field of dimensional analysis. In the nineteenth century, French mathematician Joseph Fourier and Scottish physicist James Clerk Maxwell lead significant developments in the modern concepts of dimension and unit. Later work by British physicists Osborne Reynolds and Lord Rayleigh contributed to the understanding of dimensionless numbers in physics. Building on Rayleigh's method of dimensional analysis, Edgar Buckingham proved the π theorem (independent of French mathematician Joseph Bertrand's previous work) to formalize the nature of dimensionless quantities. Numerous other dimensionless numbers were discovered in the early 1900s, particularly in the areas of fluid mechanics and heat transfer. In the 2000s, the International Committee for Weights and Measures contemplated defining the unit of 1 as the 'uno', but the idea was dropped.
Buckingham π theorem.
The Buckingham π theorem indicates that validity of the laws of physics does not depend on a specific unit system. A statement of this theorem is that any physical law can be expressed as an identity involving only dimensionless combinations (ratios or products) of the variables linked by the law (e. g., pressure and volume are linked by Boyle's Law – they are inversely proportional). If the dimensionless combinations' values changed with the systems of units, then the equation would not be an identity, and Buckingham's theorem would not hold.
Another consequence of the Buckingham π theorem of dimensional analysis is that the functional dependence between a certain number (say, "n") of variables can be reduced by the number (say, "k") of independent dimensions occurring in those variables to give a set of "p" = "n" − "k" independent, dimensionless quantities. For the purposes of the experimenter, different systems that share the same description by dimensionless quantity are equivalent.
Example.
To demonstrate the application of the π theorem, consider the power consumption of a stirrer with a given shape.
The power, "P", in dimensions · L2/T3, is a function of the density, "ρ" and the viscosity of the fluid to be stirred, "μ" [M/(L · T), as well as the size of the stirrer given by its diameter, "D" and the angular speed of the stirrer, "n" [1/T. Therefore, we have a total of "n" = 5 variables representing our example. Those "n" = 5 variables are built up from "k" = 3 fundamental dimensions, the length: L (SI units: m), time: T (s), and mass: M (kg).
According to the π-theorem, the "n" = 5 variables can be reduced by the "k" = 3 dimensions to form "p" = "n" − "k" = 5 − 3 = 2 independent dimensionless numbers. These quantities are formula_1, commonly named the Reynolds number which describes the fluid flow regime, and formula_2, the Power number, which is the dimensionless description of the stirrer.
Nondimensionalization of Differential Equations.
The process of nondimensionalization has significant applications in the analysis of differential equations.

</doc>
<doc id="51332" url="https://en.wikipedia.org/wiki?curid=51332" title="Power number">
Power number

The power number "N"p (also known as Newton number) is a commonly used dimensionless number relating the resistance force to the inertia force. 
The power-number has different specifications according to the field of application. E.g., for stirrers the power number is defined as:
with 

</doc>
<doc id="51333" url="https://en.wikipedia.org/wiki?curid=51333" title="Miloš Forman">
Miloš Forman

Jan Tomáš Forman (; born 18 February 1932), known as Miloš Forman (, ), is a Czech film director, screenwriter, actor, and professor, who until 1968 lived and worked primarily in the former Czechoslovakia.
Forman was one of the most important directors of the Czechoslovak New Wave. His 1967 film "The Fireman's Ball", on the surface a naturalistic representation of an ill-fated social event in a provincial town, was seen by both movie scholars and authorities in Czechoslovakia as a biting satire on Eastern European Communism, resulting in it being banned for many years in Forman's home country.
Since Forman left Czechoslovakia, two of his films, "One Flew Over the Cuckoo's Nest" and "Amadeus", have acquired particular renown, both gaining him an Academy Award for Best Director. "One Flew Over the Cuckoo's Nest" was the second to win all five major Academy Awards (Best Picture, Actor in Leading Role, Actress in Leading Role, Director, and Screenplay) following "It Happened One Night" in 1934, an accomplishment not repeated until 1991 by "The Silence of the Lambs". Forman was also nominated for a Best Director Oscar for "The People vs. Larry Flynt". He has also won Golden Globe, Cannes, Berlinale, BAFTA, Cesar, David di Donatello, European Film Academy, and Czech Lion awards.
Personal life.
Forman was born in Čáslav, Czechoslovakia (present-day Czech Republic), the son of Anna (née Švábová), who ran a summer hotel. When young, he believed his biological father to be Rudolf Forman, a professor. Both Anna and Rudolf Forman were Protestants. During the Nazi occupation, a member of the anti-Nazi Underground named Rudolf Forman as a member of the Underground while being interrogated by the Gestapo. Rudolf was arrested for distributing banned books and died in Buchenwald in 1944. Forman's mother died in Auschwitz in 1943. Forman has stated that he did not fully understand what had happened to them until he saw footage of the concentration camps when he was 16.
Forman lived with relatives during World War II and later discovered that his biological father was in fact a Jewish architect, Otto Kohn, a survivor of the Holocaust. He has a brother, Pavel Forman, 12 years older, a Czech painter who, after the 1968 invasion, emigrated to Australia. In his youth, he wanted to become a theatrical producer, bypassing theater. Through his biological father, he is a half-brother of mathematician Joseph J. Kohn.
After the war, Forman attended the elite King George boarding school in the spa town Poděbrady, where his fellow students included Václav Havel, the Mašín brothers and future film-makers Ivan Passer and Jerzy Skolimowski. He later studied screenwriting at the Academy of Performing Arts in Prague. He was assistant of Alfred Radok, creator of Laterna Magika. During the Warsaw Pact invasion of Czechoslovakia in summer 1968, he left Europe for the United States.
Forman's first wife was Czech movie star Jana Brejchová. They met during the making of the movie "Štěňata" (1957). They divorced in 1962. Forman has twin sons with his second wife, Czech actress Věra Křesadlová-Formanová. They separated in 1969. Both sons, Petr Forman and Matěj Forman, born 1964, are involved in the theatre. Then Forman married Martina Zbořilová on November 28, 1999. They also have twin sons, Jim and Andy (born 1999, named for comics Jim Carrey and Andy Kaufman), and reside in Connecticut, USA.
In 2006, he received the Hanno R. Ellenbogen Citizenship Award presented by the Prague Society for International Cooperation.
He is a professor emeritus at Columbia University.
The asteroid 11333 Forman was named after Forman.
In 2009 a documentary film about Forman directed by Miloš Šmídmajer was produced – "Miloš Forman: Co te nezabije...".
Forman has written poems and published an autobiography called "Turnaround".
Career.
Along with future favorite cinematographer Miroslav Ondříček and longtime schoolfriend Ivan Passer, Forman filmed the silent documentary "Semafor" about Semafor theater. Forman's first important production was the documentary "Audition" whose subject was competing singers. He directed several Czech comedies in Czechoslovakia. However, during the Prague Spring and the ensuing 1968 invasion, he was in Paris negotiating the production of his first American film. His employer, a Czech studio, fired him, claiming that he had been out of the country illegally. He moved to New York, where he later became a professor of film at Columbia University and co-chair (with his former teacher František Daniel) of Columbia's film department. One of his protégés was future director James Mangold, whom Forman had advised about scriptwriting.
In 1977, he became a naturalized citizen of the United States.
In 1985 he headed the Cannes film festival and in 2000 did the same for the Venice festival. He presided over a ceremony of Caesar in 1988.
In 1997, he received the Crystal Globe award for outstanding artistic contribution to world cinema at the Karlovy Vary International Film Festival. Forman performed alongside actor Edward Norton in Norton's directorial debut, "Keeping the Faith" (2000), as the wise friend to Norton's conflicted priest.
In April 2007 the jazz opera "Dobře placená procházka" premiered at the Prague National Theatre, directed by Forman's son, Petr Forman.
Forman received an honorary degree in 2009 from Emerson College in Boston, USA.
He regularly collaborated with cinematographer Miroslav Ondříček.
Films.
"Loves of a Blonde".
"Loves of a Blonde" is one of the best–known movies of Czechoslovak New Wave and has won awards at the Venice and Locarno film festivals. It was also nominated for the Academy Award for Best Foreign Language Film in 1967.
"The Fireman's Ball".
A 1967 Czechoslovak-Italian co-production, this was Forman's first color film. It is one of the best–known movies of Czechoslovak New Wave. On the face of it a naturalistic representation of an ill-fated social event in a provincial town, the film has been seen by both movie scholars and the then-authorities in Czechoslovakia as a biting satire on East European Communism, which resulted in it being banned for many years in Forman's home country.
It was nominated for the Academy Award for Best Foreign Film.
"Taking Off".
The first movie Forman made in the United States, "Taking Off" won the Grand Prix at the 1971 Cannes Film Festival. The film starred Lynn Carlin and Buck Henry, and also featured Linnea Heacock as Jeannie.
"One Flew Over the Cuckoo's Nest".
In spite of initial difficulties, he started directing in the United States, and achieved success in 1975 with the adaptation of Ken Kesey's novel "One Flew Over the Cuckoo's Nest" starring Jack Nicholson and Louise Fletcher. The film won Oscars in the five most important categories: Best Director, Best Actor, Best Actress, Best Picture and Best Adapted Screenplay, one of only three films in history to do so, along with "It Happened One Night" and "The Silence of the Lambs", and firmly established Forman's reputation.
"Hair".
The success of "One Flew Over the Cuckoo's Nest" allowed Forman to direct the long-planned film "Hair" (a rock musical) in 1979, based on the Broadway musical by James Rado, Gerome Ragni, and Galt MacDermot. The film starred Treat Williams, John Savage and Beverly D'Angelo.
"Amadeus".
Forman's next important achievement was the adaption of Peter Shaffer's "Amadeus" in 1984—retelling the story of Wolfgang Amadeus Mozart and Antonio Salieri. The internationally acclaimed film starred Tom Hulce, Elizabeth Berridge and F. Murray Abraham. The movie won eight Oscars, including Best Picture, Best Director and Best Actor (Abraham).
"Valmont".
His adaptation of Pierre Choderlos de Laclos's novel "Les Liaisons dangereuses", it had its premiere on November 17, 1989. Another film adaptation by Stephen Frears had been released the previous year and received much acclaim. The film starred Colin Firth, Meg Tilly and Annette Bening.
"The People vs. Larry Flynt".
The 1996 biographical film of pornographic publisher Larry Flynt brought Forman another Oscar nomination. The film starred Woody Harrelson, Courtney Love and Edward Norton.
"Man on the Moon".
The biography of famous actor and avant-garde comic Andy Kaufman (Jim Carrey, who won a Golden Globe for his performance) premiered on December 22, 1999. The film also starred Danny DeVito, Courtney Love and Paul Giamatti.
"Goya's Ghosts".
This free biography of Spanish painter Francisco Goya (American-Spanish co-production) premiered on November 8, 2006. The film starred Natalie Portman, Javier Bardem, Stellan Skarsgård and Randy Quaid.
Influence on the Czech language.
Forman's early films are popular among Czechs. Many situations and phrases from his movies have passed into common use. For example, the Czech term "zhasnout" ("to switch lights off") from "The Fireman's Ball", associated with petty theft in the film, has been used to describe the large-scale asset stripping that occurred in the country during the 1990s.
Awards, nominations and honours.
Academy Awards
Golden Globe
Cannes
Berlinale
BAFTA
César Award
David di Donatello
European Film Academy
The state prize of Klement Gottwald
Czech Lion
List of Greatest Czechs
Doctor of Humane Letters 

</doc>
<doc id="51334" url="https://en.wikipedia.org/wiki?curid=51334" title="Antigua (disambiguation)">
Antigua (disambiguation)

Antigua can refer to:

</doc>
<doc id="51335" url="https://en.wikipedia.org/wiki?curid=51335" title="Amartya Sen">
Amartya Sen

Amartya Kumar Sen (pronounced ; born 3 November 1933) is an Indian economist and philosopher of Bengali ethnicity, who since 1972 has taught and worked in the United Kingdom and the United States. Sen has made contributions to welfare economics, social choice theory, economic and social justice, economic theories of famines, and indexes of the measure of well-being of citizens of developing countries. He was awarded the Nobel Memorial Prize in Economic Sciences in 1998 and Bharat Ratna in 1999 for his work in welfare economics. He was also awarded the inaugural Charleston-EFG John Maynard Keynes Prize in recognition of his work on welfare economics in February 2015 during a reception at the Royal Academy in the UK.
He is currently the Thomas W. Lamont University Professor and Professor of Economics and Philosophy at Harvard University. He served as the chancellor of Nalanda University. He is also a senior fellow at the Harvard Society of Fellows, a distinguished fellow of All Souls College, Oxford, an honorary fellow of Darwin College, Cambridge and a Fellow of Trinity College, Cambridge, where he served as Master from 1998 to 2004.
Early life and education.
Sen was born in a Bengali family in Manikganj (in British India, now Bangladesh), to Ashutosh Sen and Amita Sen. Rabindranath Tagore gave Amartya Sen his name (Bengali অমর্ত্য "ômorto", lit. "immortal"). Sen's family was from Wari and Manikganj, Dhaka, both in present-day Bangladesh. His father Ashutosh Sen was a professor of chemistry at Dhaka University who moved with his family to West Bengal in 1945 and worked at various government institutions, including the West Bengal Public Service Commission (of which he was the chairman), and the Union Public Service Commission. Sen's mother Amita Sen was the daughter of Kshiti Mohan Sen, a well-known scholar of ancient and medieval India and close associate of Rabindranath Tagore. He served as the Vice Chancellor of Visva-Bharati University for some years.
Sen began his high-school education at St Gregory's School in Dhaka in 1940. From fall 1941, Sen studied at Patha Bhavana, Santiniketan. The school had many progressive features: at the school, any focus on examinations or competitive testing was deeply frowned upon. In addition, the school stressed cultural diversity, and embraced influences from the rest of the world. In 1951, he went to Presidency College, Kolkata, where he earned a B.A. in Economics with First Class, with a minor in Mathematics, as a graduating student of the University of Calcutta. While at Presidency, Sen was diagnosed with oral cancer, and given a 15% chance of living five years. With radiation treatment, he survived, and in 1953 he moved to Trinity College, Cambridge, where he earned a second B.A. in Pure Economics in 1955 with a First Class, topping the list as well. He was elected President of the Cambridge Majlis. While Sen was officially a Ph.D student at Cambridge (though he had finished his research in 1955-6), he was offered the position of Professor and Head of the Economics Department of the newly created Jadavpur University in Calcutta, and he became the youngest chairman to head the Department of Economics. He served in that position, starting the new Economics Department, during 1956 to 1958.
Meanwhile, Sen was elected to a Prize Fellowship at Trinity College, which gave him four years of freedom to do anything he liked; he made the radical decision to study philosophy. Sen explained: "The broadening of my studies into philosophy was important for me not just because some of my main areas of interest in economics relate quite closely to philosophical disciplines (for example, social choice theory makes intense use of mathematical logic and also draws on moral philosophy, and so does the study of inequality and deprivation), but also because I found philosophical studies very rewarding on their own". His interest in philosophy, however, dates back to his college days at Presidency, where he read books on philosophy and debated philosophical themes.
In Cambridge, there were major debates between supporters of Keynesian economics on the one hand, and the "neo-classical" economists skeptical of Keynes, on the other. However, because of a lack of enthusiasm for social choice theory in both Trinity and Cambridge, Sen had to choose a different subject for his Ph.D. thesis, which was on "The Choice of Techniques" in 1959, though the work had been completed much earlier (except for some valuable advice from his adjunct supervisor in India, Professor A.K. Dasgupta, given to Sen while teaching and revising his work at Jadavpur) under the supervision of the "brilliant but vigorously intolerant" post-Keynesian, Joan Robinson. Quentin Skinner notes that Sen was a member of the secret society Cambridge Apostles during his time at Cambridge.
Research.
Sen's work on 'Choice of Technique' complemented that of Maurice Dobb. In a Developing country, the Dobb-Sen strategy relied on maximising investible surpluses, maintaining constant real wages and using the entire increase in labour productivity, due to technological change, to raise the rate of accumulation. In other words, workers were expected to demand no improvement in their standard of living despite having become more productive. 
Sen's papers in the late 1960s and early 1970s helped develop the theory of social choice, which first came to prominence in the work by the American economist Kenneth Arrow, who, while working at the RAND Corporation, had most famously shown that all voting rules, be they majority rule or two thirds-majority or status quo, must inevitably conflict with what might arguably appear to be some basic democratic norm. Sen's contribution to the literature was to show under what conditions Arrow's impossibility theorem would indeed come to pass as well as to extend and enrich the theory of social choice, informed by his interests in history of economic thought and philosophy.
In 1981, Sen published "Poverty and Famines: An Essay on Entitlement and Deprivation" (1981), a book in which he argued that famine occurs not only from a lack of food, but from inequalities built into mechanisms for distributing food. Sen also argued that the Bengal famine was caused by an urban economic boom that raised food prices, thereby causing millions of rural workers to starve to death when their wages did not keep up.
Sen's interest in famine stemmed from personal experience. As a nine-year-old boy, he witnessed the Bengal famine of 1943, in which three million people perished. This staggering loss of life was unnecessary, Sen later concluded. He presents data that there was an adequate food supply in Bengal at the time, but particular groups of people including rural landless labourers and urban service providers like haircutters did not have the means to buy food as its price rose rapidly due to factors that include British military acquisition, panic buying, hoarding, and price gouging, all connected to the war in the region. In "Poverty and Famines", Sen revealed that in many cases of famine, food supplies were not significantly reduced. In Bengal, for example, food production, while down on the previous year, was higher than in previous non-famine years. Thus, Sen points to a number of social and economic factors, such as declining wages, unemployment, rising food prices, and poor food-distribution, which led to starvation. His capabilities approach focuses on positive freedom, a person's actual ability to be or do something, rather than on negative freedom approaches, which are common in economics and simply focuses on non-interference. In the Bengal famine, rural laborers' negative freedom to buy food was not affected. However, they still starved because they were not positively free to do anything, they did not have the functioning of nourishment, nor the capability to escape morbidity.
In addition to his important work on the causes of famines, Sen's work in the field of development economics has had considerable influence in the formulation of the "Human Development Report", published by the United Nations Development Programme. This annual publication that ranks countries on a variety of economic and social indicators owes much to the contributions by Sen among other social choice theorists in the area of economic measurement of poverty and inequality.
Sen's revolutionary contribution to development economics and social indicators is the concept of "capability" developed in his article "Equality of What". He argues that governments should be measured against the concrete capabilities of their citizens. This is because top-down development will always trump human rights as long as the definition of terms remains in doubt (is a "right" something that must be provided or something that simply cannot be taken away?). For instance, in the United States citizens have a hypothetical "right" to vote. To Sen, this concept is fairly empty. In order for citizens to have a capacity to vote, they first must have "functionings". These "functionings" can range from the very broad, such as the availability of education, to the very specific, such as transportation to the polls. Only when such barriers are removed can the citizen truly be said to act out of personal choice. It is up to the individual society to make the list of minimum capabilities guaranteed by that society. For an example of the "capabilities approach" in practice, see Martha Nussbaum's "Women and Human Development".
He wrote a controversial article in "The New York Review of Books" entitled "More Than 100 Million Women Are Missing" (see Missing women of Asia), analyzing the mortality impact of unequal rights between the genders in the developing world, particularly Asia. Other studies, including one by Emily Oster, had argued that this is an overestimation, though Oster has since then recanted her conclusions.
In 1998, Sen further advanced and redefined the capability approach in his book "Development as Freedom". Sen argues that development should be viewed as an effort to advance the real freedoms that individuals enjoy, rather than simply focusing on metrics such as GDP or income-per-capita. Sen was inspired by violent acts he had witnessed as a child leading up to the Partition of India in 1947. On one morning, a Muslim laborer named Kader Mia stumbled through the rear gate of Sen's family home, bleeding from a knife wound in his back. Because of his extreme poverty, he had come to Sen's primarily Hindu neighborhood searching for work; his choices were the starvation of his family or the risk of death in coming to the neighborhood. The price of Kader Mia's economic unfreedom was his death. This experience led Sen to begin thinking about economic unfreedom from a young age. 
In "Development as Freedom", Sen outlines five specific types of freedoms: political freedoms, economic facilities, social opportunities, transparency guarantees, and protective security. Political freedoms, the first of these, refers to the ability of the people to have a voice in government and to be able to scrutinize the authorities. Economic facilities concern both the resources within the market and the market mechanism itself. Any focus on income and wealth in the country would serve to increase the economic facilities for the people. Social opportunities deal with the establishments that provide benefits like healthcare or education for the populace, allowing individuals to live better lives. Transparency guarantees allow individuals to interact with some degree of trust and knowledge of the interaction. Protective security is the system of social safety nets that prevent a group affected by poverty being subjected to terrible misery. Before Sen's work, these had been viewed as only the ends of development; luxuries afforded to countries that focus on increasing income. However, Sen argues that the increase in real freedoms should be both the ends and the means of development.
Welfare economics seeks to evaluate economic policies in terms of their effects on the well-being of the community. Sen, who devoted his career to such issues, was called the "conscience of his profession". His influential monograph "Collective Choice and Social Welfare" (1970), which addressed problems related to individual rights (including formulation of the liberal paradox), justice and equity, majority rule, and the availability of information about individual conditions, inspired researchers to turn their attention to issues of basic welfare. Sen devised methods of measuring poverty that yielded useful information for improving economic conditions for the poor. For instance, his theoretical work on inequality provided an explanation for why there are fewer women than men in India and China despite the fact that in the West and in poor but medically unbiased countries, women have lower mortality rates at all ages, live longer, and make a slight majority of the population. Sen claimed that this skewed ratio results from the better health treatment and childhood opportunities afforded boys in those countries, as well as sex-selective abortions.
Governments and international organizations handling food crises were influenced by Sen's work. His views encouraged policy makers to pay attention not only to alleviating immediate suffering but also to finding ways to replace the lost income of the poor—for example through public works—and to maintain stable prices for food. A vigorous defender of political freedom, Sen believed that famines do not occur in functioning democracies because their leaders must be more responsive to the demands of the citizens. In order for economic growth to be achieved, he argued, social reforms—such as improvements in education and public health—must precede economic reform.
In 2009, Sen published a book called "The Idea of Justice". Based on his previous work in welfare economics and social choice theory, but also on his philosophical thoughts, he presented his own theory of justice that he meant to be an alternative to the influential modern theories of justice of John Rawls or John Harsanyi. In opposition to Rawls but also earlier justice theoreticians Immanuel Kant, Jean-Jacques Rousseau or David Hume, and inspired by the philosophical works of Adam Smith and Mary Wollstonecraft, Sen developed a theory that is both comparative and realizations-oriented (instead of being transcendental and institutional). However, he still regards institutions and processes as being important. As an alternative to Rawls's veil of ignorance, Sen chose the thought experiment of an impartial spectator as the basis of his theory of justice. He also stressed the importance of public discussion (understanding democracy in the sense of John Stuart Mill) and a focus on people's capabilities (an approach that he had co-developed), including the notion of universal human rights, in evaluating various states with regard to justice.
Professional career.
Sen began his career both as a teacher and a research scholar in the Department of Economics, Jadavpur University. Between 1960 and 1961, Sen was a visiting Professor at Massachusetts Institute of Technology in the United States, where he got to know Paul Samuelson, Robert Solow, Franco Modigliani, and Norbert Wiener. He was also a visiting Professor at UC-Berkeley and Cornell. He taught as Professor of Economics between 1963 and 1971 at the Delhi School of Economics (where he completed his "magnum opus" Collective Choice and Social Welfare in 1969). During this time he was also a frequent visitor to various other premiere Indian economic schools and centres of excellence like Jawaharlal Nehru University, Indian Statistical Institute, Centre for Development Studies, Gokhale Institute of Politics and Economics and Centre for Studies in Social Sciences. Sen was a companion of distinguished economists like Manmohan Singh (Ex-Prime Minister of India and a veteran economist responsible for liberalizing the Indian economy), K. N. Raj (Advisor to various Prime Ministers and a veteran economist who was the founder of Centre for Development Studies, Trivandrum, which is one of India's premier think tanks and schools) and Jagdish Bhagwati (who is known to be one of the greatest Indian economists in the field of International Trade and currently teaches at Columbia University). This is a period considered to be a Golden Period in the history of DSE. In 1972, he joined the London School of Economics as a Professor of Economics where he taught until 1977. From 1977 to 1986 he taught at the University of Oxford, where he was first a Professor of Economics and Fellow of Nuffield College, and then the Drummond Professor of Political Economy and a Fellow of All Souls College from 1980. In 1987, he joined Harvard as the Thomas W. Lamont University Professor of Economics. In 1998 he was appointed as Master of Trinity College, Cambridge, becoming the first Asian head of an Oxbridge college. In January 2004, Sen returned to Harvard. He also established the Eva Colorni Trust at the former London Guildhall University in the name of his deceased wife.
Nalanda Project.
In May 2007, he was appointed as chairman of Nalanda Mentor Group to examine the framework of international cooperation, and proposed structure of partnership, which would govern the establishment of Nalanda International University Project as an international centre of education seeking to revive the ancient center of higher learning which was present in India from the 5th century to 1197.
On 19 July 2012, Sen was named the first chancellor of the proposed Nalanda University (NU). Teaching began in August 2014. On 20 February 2015, Amartya Sen withdrew his candidature for a second term.
Membership and associations.
He has served as president of the Econometric Society (1984), the International Economic Association (1986–1989), the Indian Economic Association (1989) and the American Economic Association (1994). He has also served as President of the Development Studies Association and the Human Development and Capabilities Association. He serves as the Chair of the International Advisory Board of the Center for Human and Economic Development Studies at Peking University in China.
Sen has been called "the Conscience of the profession" and "the Mother Teresa of Economics" for his work on famine, human development theory, welfare economics, the underlying mechanisms of poverty, gender inequality, and political liberalism. However, he denies the comparison to Mother Teresa, saying that he has never tried to follow a lifestyle of dedicated self-sacrifice. Amartya Sen also added his voice to the campaign against the anti-gay Section 377 of the Indian Penal Code.
Media and culture.
A 57-minute documentary named "Amartya Sen: A Life Re-examined" directed by Suman Ghosh details his life and work.
A 2001 portrait of Sen by Annabel Cullen is in Trinity College's collection. A 2003 portrait of Sen hangs in the National Portrait Gallery in London.
In 2011, he was present at the "Rabindra Utsab" ceremony at Bangabandhu International Conference Centre (BICC), Bangladesh. He unveiled the cover of Sruti Gitobitan, a Rabindrasangeet album comprising all the 2222 Tagore songs, brought out by Rezwana Chowdhury Bannya, principal of Shurer Dhara School of Music.
Controversies.
Amartya Sen was critical of Indian Prime Minister Narendra Modi when he was announced as the prime ministerial candidate by the BJP. In April 2014, he said that Modi would not make a good Prime Minister. But later in December 2014, he changed his views and said that Narendra Modi did give people a sense of faith that things can happen. In February 2015, Sen opted out of seeking a second term for the chancellor post of Nalanda university stating that the Government of India was not keen on him continuing in the post. Sen is very critical of the present BJP government.
Personal life and beliefs.
Sen has been married three times. His first wife was Nabaneeta Dev Sen, an Indian writer and scholar, by whom he had two daughters: Antara, a journalist and publisher, and Nandana, a Bollywood actress. Their marriage broke up shortly after they moved to London in 1971. In 1978 Sen married Eva Colorni, an Italian economist, and the couple had two children, a daughter Indrani, who is a journalist in New York, and a son Kabir, a hip hop artist, MC, and music teacher at Shady Hill School. Eva died of cancer in 1985. In 1991, Sen married Emma Georgina Rothschild, who serves as the Jeremy and Jane Knowles Professor of History at Harvard University.
The Sens have a house in Cambridge, Massachusetts, which is the base from which they teach during the academic year. They also have a home in Cambridge, England, where Sen is a Fellow of Trinity College, Cambridge, and Rothschild is a Fellow of Magdalene College. He usually spends his winter holidays at his home in Santiniketan in West Bengal, India, where he used to go on long bike rides until recently. Asked how he relaxes, he replies: "I read a lot and like arguing with people."
Sen is an atheist and holds that this can be associated with one of the atheist schools in Hinduism, the Lokayata. In an interview for the magazine "California", which is published by the University of California, Berkeley, he noted:
Awards and honours.
Sen has received over 90 honorary degrees from universities around the world.
Bibliography.
Books.
1960–1979
1980–1989
1990–1999
2000–2009
2010 onwards

</doc>
<doc id="51336" url="https://en.wikipedia.org/wiki?curid=51336" title="Barbuda">
Barbuda

Barbuda is an island in the Eastern Caribbean, and forms part of the state of Antigua and Barbuda. It has a population of about 1,638 (at the 2011 Census), most of whom live in the town of Codrington.
Location.
Barbuda is located north of Antigua, in the middle of the Leeward Islands. To the south are the islands of Montserrat and Guadeloupe, and to the west and north west are Nevis, St. Kitts, St. Barts, and St. Martin.
History.
The Ciboney were the first to inhabit the island of Barbuda in 2400 BC, but Arawak and Carib Indians populated the island when Christopher Columbus landed on his second voyage in 1493. Early settlements by the Spanish and French were succeeded by the English, who formed a colony in 1666.
In 1685 Barbuda was leased to brothers Christopher and John Codrington, who had founded the town of Codrington. The Codrington family produced food on their land in Barbuda, and also transported slaves as labour for their sugarcane plantations on Antigua. There was more than one slave rebellion at Codrington during the 1740s, during which slaves rose against managers. All the slaves were freed in 1834.
Barbuda was for a time used by the Codringtons as a "nursery" for slaves.
In 1719, Codrington and the island of Barbuda had its first census (of both people and livestock), conducted by Sir William Codrington (1715–1790).
The first map of Barbuda was made in the second half of the 18th century. At that time there were substantial buildings in the Highland area, a castle in Codrington, a fort at River, now known as the Martello Tower, and houses at Palmetto Point, Coco Point, and Castle Hill. The map shows eight catching pens for holding captured runaway slaves, indicating that this was a serious problem. There were several defensive cannon gun battery units around the island perimeter. There was a large plantation in the Meadow and Guava area and another large plantation in the Highlands area.
On November 1, 1981, the island gained its independence as an integral part of Antigua and Barbuda, a member of the Commonwealth of Nations. In a 1989 election the Barbuda Independence Movement received too few votes to qualify for a seat in the national parliament.
Points of interest.
Barbuda is home to the Frigate Bird Sanctuary, which is located in the Codrington Lagoon. Other points of interest include Highland House (the 18th-century home of the Codrington family) and the Indian Cave, which contains ancient Amerindian petroglyphs.
Tourism.
Barbuda's climate and geography is conducive to tourism. Many tourists are attracted by the island's beaches. Activities include a bird sanctuary, swimming, snorkeling, fishing, and caving. Only two operating resorts are located on the island, the rest are abandoned after poor management, difficult infrastructure and hurricane damage.
Geography.
The total land area is 160.56 km2. The capital and largest city is Codrington, with an estimated population of 1,000. The island is mostly coral limestone with little topographical variation. The "highlands" area on the eastern side of the island has hills rising to , but the majority of the island is very flat, with many lagoons in the northwest corner.
Climate.
The climate is classified as tropical marine which means that there is little seasonal temperature variation. In January and February, the coolest months, the average daily high temperature is . While in July and August, the warmest months, the average daily high is .

</doc>
<doc id="51341" url="https://en.wikipedia.org/wiki?curid=51341" title="560s BC">
560s BC


</doc>
<doc id="51342" url="https://en.wikipedia.org/wiki?curid=51342" title="570s BC">
570s BC


</doc>
<doc id="51343" url="https://en.wikipedia.org/wiki?curid=51343" title="590s BC">
590s BC


</doc>
<doc id="51344" url="https://en.wikipedia.org/wiki?curid=51344" title="600s BC (decade)">
600s BC (decade)


</doc>
<doc id="51345" url="https://en.wikipedia.org/wiki?curid=51345" title="610s BC">
610s BC


</doc>
