<doc id="49180" url="https://en.wikipedia.org/wiki?curid=49180" title="Fuzzy logic">
Fuzzy logic

Fuzzy logic is a form of many-valued logic in which the truth values of variables may be any real number between 0 and 1, considered to be "fuzzy". By contrast, in Boolean logic, the truth values of variables may only be 0 or 1, often called "crisp" values. Fuzzy logic has been employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. Furthermore, when linguistic variables are used, these degrees may be managed by specific (membership) functions.
The term "fuzzy logic" was introduced with the 1965 proposal of fuzzy set theory by Lotfi Zadeh. Fuzzy logic had however been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.
Fuzzy logic has been applied to many fields, from control theory to artificial intelligence.
Overview.
Classical logic only permits conclusions which are either true or false. For example, the notion that 1+1=2 is a fundamental mathematical truth. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a colour. In such instances, the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum.
Humans and animals often operate using fuzzy evaluations in many everyday situations. In the case where someone is tossing an object into a container from a distance, the person does not compute exact values for the object weight, density, distance, direction, container height and width, and air resistance to determine the force and angle to toss the object. Instead the person instinctively applies quick “fuzzy” estimates, based upon previous experience, to determine what output values of force, direction and vertical angle to use to make the toss.
Both degrees of truth and probabilities range between 0 and 1 and hence may seem similar at first. For example, let a 100 ml glass contain 30 ml of water. Then we may consider two concepts: empty and full. The meaning of each of them can be represented by a certain fuzzy set. Then one might define the glass as being 0.7 empty and 0.3 full. Note that the concept of emptiness would be subjective and thus would depend on the observer or designer. Another designer might, equally well, design a set membership function where the glass would be considered full for all values down to 50 ml. It is essential to realize that fuzzy logic uses degrees of truth as a mathematical model of vagueness, while probability is a mathematical model of ignorance.
Applying truth values.
A basic application might characterize various sub-ranges of a continuous variable. For instance, a temperature measurement for anti-lock brakes might have several separate membership functions defining particular temperature ranges needed to control the brakes properly. Each function maps the same temperature value to a truth value in the 0 to 1 range. These truth values can then be used to determine how the brakes should be controlled.
In this image, the meanings of the expressions "cold", "warm", and "hot" are represented by functions mapping a temperature scale. A point on that scale has three "truth values" — one for each of the three functions. The vertical line in the image represents a particular temperature that the three arrows (truth values) gauge. Since the red arrow points to zero, this temperature may be interpreted as "not hot". The orange arrow (pointing at 0.2) may describe it as "slightly warm" and the blue arrow (pointing at 0.8) "fairly cold".
Linguistic variables.
While variables in mathematics usually take numerical values, in fuzzy logic applications, the non-numeric are often used to facilitate the expression of rules and facts.
A linguistic variable is "young" or its antonym "old". However, the value of linguistic variables is that they can be modified via linguistic hedges applied to primary terms. These linguistic hedges can be associated with certain functions.
Fuzzification operations can map mathematical input values into fuzzy membership functions. And the opposite de-fuzzifying operations can be used to map a fuzzy output membership functions into a “crisp” output value that can be then used for decision or control purposes.
Forming a consensus of Inputs and Fuzzy Rules.
Since the fuzzy system output is a consensus of all of the inputs and all of the rules, Fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values. These rule weightings can be based upon the priority, reliability or consistency of each rule. These rule weightings may be static or can be changed dynamically, even based upon the output from other rules.
Early applications.
The Japanese were the first to utilize fuzzy logic for practical applications. The first notable application was on the high-speed train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride. It has also been used in recognition of hand written symbols in Sony pocket computers, flight aid for helicopters, controlling of subway systems in order to improve driving comfort, precision of halting, and power economy, improved fuel consumption for automobiles, single-button control for washing machines, automatic motor control for vacuum cleaners with recognition of surface condition and degree of soiling, and prediction systems for early recognition of earthquakes through the Institute of Seismology Bureau of Meteorology, Japan.
Example.
Hard science with IF-THEN rules.
Fuzzy set theory defines fuzzy operators on fuzzy sets. The problem in applying this is that the appropriate fuzzy operator may not be known.
For example, the logic for a simple temperature regulator that uses a fan might look like this:
<syntaxhighlight lang="text">
IF temperature IS very cold THEN stop fan
IF temperature IS cold THEN fan speed is zero
IF temperature IS warm THEN fan speed is moderate
IF temperature IS hot THEN fan speed is high
</syntaxhighlight>
Using this rulebase and the previous image, we would expect the output fan speed to be a combination of zero and moderate, which would evaluated as some degree of slow when the input value is a combination of cold and warm and not hot. The fan speed will continue to get slower as the input temperature gets colder until the input temperature is 100% cold, 0% warm and 0% hot, at which point the output fan speed will be zero. As the temperature input gets warmer and hotter, the output fan speed will continue to get faster until the input temperature is 0% cold, 0%warm and 100% hot, at which point the fan speed output will be high.
If the fuzzy membership functions cover 100% of the input variable domain, then it can be proven that the behavior of the fuzzy system is fully deterministic over the entire input domain and nowhere ambiguous. This determinism is very important for use in control and decision systems.
There is no "ELSE" – all of the rules are evaluated, because the temperature might be "cold" and "normal" at the same time to different degrees.
The AND, OR, and NOT operators of Boolean logic exist in fuzzy logic, usually defined as the minimum, maximum, and complement; when they are defined this way, they are called the "Zadeh operators". So for the fuzzy variables x and y:
<syntaxhighlight lang="text">
NOT x = (1 - truth(x))
x AND y = minimum(truth(x), truth(y))
x OR y = maximum(truth(x), truth(y))
</syntaxhighlight>
There are also other operators, more linguistic in nature, called "hedges" that can be applied. These are generally adverbs such as "very", or "somewhat", which modify the meaning of a set using a mathematical formula.
Define with multiply.
<syntaxhighlight lang="text">
x AND y = x*y
x OR y = 1-(1-x)*(1-y)
</syntaxhighlight>
1-(1-x)*(1-y) comes from this:
<syntaxhighlight lang="text">
x OR y = NOT( AND( NOT(x), NOT(y) ) )
x OR y = NOT( AND(1-x, 1-y) )
x OR y = NOT( (1-x)*(1-y) )
x OR y = 1-(1-x)*(1-y)
</syntaxhighlight>
Define with sigmoid.
<syntaxhighlight lang="text">
sigmoid(x)=1/(1+e^-x)
sigmoid(x)+sigmoid(-x) = 1
(sigmoid(x)+sigmoid(-x))*(sigmoid(y)+sigmoid(-y))*(sigmoid(z)+sigmoid(-z)) = 1
</syntaxhighlight>
Logical analysis.
In mathematical logic, there are several formal systems of "fuzzy logic"; most of them belong among so-called t-norm fuzzy logic.
Propositional fuzzy logics.
The most important propositional fuzzy logics are:-
Predicate fuzzy logics.
These extend the above-mentioned fuzzy logics by adding universal and existential quantifiers in a manner similar to the way that predicate logic is created from propositional logic. The semantics of the universal (resp. existential) quantifier in t-norm fuzzy logics is the infimum (resp. supremum) of the truth degrees of the instances of the quantified subformula.
Decidability issues for fuzzy logic.
The notions of a "decidable subset" and "recursively enumerable subset" are basic ones for classical mathematics and classical logic. Thus the question of a suitable extension of these concepts to fuzzy set theory arises. A first proposal in such a direction was made by E.S. Santos by the notions of "fuzzy Turing machine", "Markov normal fuzzy algorithm" and "fuzzy program" (see Santos 1970). Successively, L. Biacino and G. Gerla argued that the proposed definitions are rather questionable and therefore they proposed the following ones. Denote by "Ü" the set of rational numbers in Then a fuzzy subset "s" : "S" formula_1[0,1 of a set "S" is recursively enumerable if a recursive map "h" : "S"×"N" formula_1"Ü" exists such that, for every "x" in "S", the function "h"("x","n") is increasing with respect to "n" and "s"("x") = lim "h"("x","n").
We say that "s" is "decidable" if both "s" and its complement –"s" are recursively enumerable. An extension of such a theory to the general case of the L-subsets is possible (see Gerla 2006).
The proposed definitions are well related with fuzzy logic. Indeed, the following theorem holds true (provided that the deduction apparatus of the considered fuzzy logic satisfies some obvious effectiveness property).
Theorem. Any axiomatizable fuzzy theory is recursively enumerable. In particular, the fuzzy set of logically true formulas is recursively enumerable in spite of the fact that the crisp set of valid formulas is not recursively enumerable, in general. Moreover, any axiomatizable and complete theory is decidable.
It is an open question to give supports for a "Church thesis" for fuzzy mathematics, the proposed notion of recursive enumerability for fuzzy subsets is the adequate one. To this aim, an extension of the notions of fuzzy grammar and fuzzy Turing machine should be necessary. Another open question is to start from this notion to find an extension of Gödel's theorems to fuzzy logic.
Fuzzy databases.
Once fuzzy relations are defined, it is possible to develop fuzzy relational databases. The first fuzzy relational database, FRDB, appeared in Maria Zemankova's dissertation. Later, some other models arose like the Buckles-Petry model, the Prade-Testemale Model, the Umano-Fukami model or the GEFRED model by J.M. Medina, M.A. Vila et al. In the context of fuzzy databases, some fuzzy querying languages have been defined, highlighting the SQLf by P. Bosc et al. and the FSQL by J. Galindo et al. These languages define some structures in order to include fuzzy aspects in the SQL statements, like fuzzy conditions, fuzzy comparators, fuzzy constants, fuzzy constraints, fuzzy thresholds, linguistic labels and so on.
Comparison to probability.
Fuzzy logic and probability address different forms of uncertainty. While both fuzzy logic and probability theory can represent degrees of certain kinds of subjective belief, fuzzy set theory uses the concept of fuzzy set membership, i.e., "how much" a variable is in a set (there is not necessarily any uncertainty about this degree), and probability theory uses the concept of subjective probability, i.e., "how probable" is it that a variable is in a set (it either entirely is or entirely is not in the set in reality, but there is uncertainty around whether it is or is not). The technical consequence of this distinction is that fuzzy set theory relaxes the axioms of classical probability, which are themselves derived from adding uncertainty, but not degree, to the crisp true/false distinctions of classical Aristotelian logic.
Bruno de Finetti argues that only one kind of mathematical uncertainty, probability, is needed, and thus fuzzy logic is unnecessary. However, Bart Kosko shows in Fuzziness vs. Probability that probability theory is a subtheory of fuzzy logic, as questions of degrees of belief in mutually-exclusive set membership in probability theory can be represented as certain cases of non-mutually-exclusive graded membership in fuzzy theory. In that context, he also derives Bayes' theorem from the concept of fuzzy subsethood. Lotfi A. Zadeh argues that fuzzy logic is different in character from probability, and is not a replacement for it. He fuzzified probability to fuzzy probability and also generalized it to possibility theory. (cf.)
More generally, fuzzy logic is one of many different extensions to classical logic intended to deal with issues of uncertainty outside of the scope of classical logic, the inapplicability of probability theory in many domains, and the paradoxes of Dempster-Shafer theory.
Relation to ecorithms.
Leslie Valiant, a winner of the Turing Award, uses the term "ecorithms" to describe how many less exact systems and techniques like fuzzy logic (and "less robust" logic) can be applied to learning algorithms. Valiant essentially redefines machine learning as evolutionary. Ecorithms and fuzzy logic also have the common property of dealing with possibilities more than probabilities, although feedback and feed forward, basically stochastic "weights," are a feature of both when dealing with, for example, dynamical systems.
In general use, ecorithms are algorithms that learn from their more complex environments (Hence Eco) to generalize, approximate and simplify solution logic. Like fuzzy logic, they are methods used to overcome continuous variables or systems too complex to completely enumerate or understand discretely or exactly. 
Compensatory fuzzy logic.
Compensatory fuzzy logic (CFL) is a branch of fuzzy logic with modified rules for conjunction and disjunction. When the truth value of one component of a conjunction or disjunction is increased or decreased, the other component is decreased or increased to compensate. This increase or decrease in truth value may be offset by the increase or decrease in another component. An offset may be blocked when certain thresholds are met. Proponents claim that CFL allows for better computational semantic behaviors.http://web.mit.edu/6.863/www/fall2012/projects/writeups/semantic-similarity-betweenverbs.pdf
Compensatory Fuzzy Logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.

</doc>
<doc id="49181" url="https://en.wikipedia.org/wiki?curid=49181" title="588 Achilles">
588 Achilles

588 Achilles, provisional designation 1906 TG, is a large and dark asteroid, classified as Jupiter trojan, the first and 6th-largest of its kind ever discovered by astronomers. It was discovered on 22 February 1906, by the German astronomer Max Wolf at Heidelberg Observatory in southern Germany. It measures about 135 kilometers in diameter and was named after Achilles from Greek mythology.
The D-type asteroid, classified as a DU-subtype in the Tholen taxonomic scheme, orbits the Sun at a distance of 4.4–6.0 AU in the Lagrangian point of the Sun–Jupiter System once every 11 years and 10 months (4,337 days). Its orbit shows an eccentricity of 0.15 and an inclination of 10 degrees from the plane of the ecliptic. The asteroid is the first known example of the stable solution of the three-body problem worked out by French mathematician Joseph Lagrange in 1772, after whom the minor planet 1006 Lagrangea is named. After the discovery of other asteroids with similar orbital characteristics, which were also named after heroes from the Trojan War "(see below)", the term "Trojan asteroids" or "Jupiter trojans" became commonly used. In addition, a rule was established that the point was the "Greek camp", whereas the point was the "Trojan camp", though not before each camp had acquired a "spy" (624 Hektor in the Greek camp and 617 Patroclus in the Trojan camp).
Photometric observations of this asteroid during 1994 were used to build a light-curve showing a rotation period of hours with a brightness variation of magnitude. This result is in good agreement with prior studies. According to the surveys carried out by the Infrared Astronomical Satellite, IRAS, the Japanese Akari satellite, and the NEOWISE mission of NASA's Wide-field Infrared Survey Explorer, the body's surface has a very low albedo in the range between 0.033 and 0.043, and a corresponding diameter of 130.1 to 135.4 kilometers.
The minor planet's name was suggested by Austrian astronomer Johann Palisa. It was named after Achilles, the legendary hero from Greek mythology and central figure in Homer's "Iliad" which tells the accounts of the Trojan War "(also see 5700 Homerus and 6604 Ilias)". As an infant, Archilles was plunged in the River Styx by his mother Thetis "(also see 17 Thetis)", thus rendering his body invulnerable excepting the heel by which he was held. He slew Hector ("see also 624 Hektor"), the greatest Trojan warrior. He was eventually killed by an arrow in the heel by Paris ("see 3317 Paris").

</doc>
<doc id="49184" url="https://en.wikipedia.org/wiki?curid=49184" title="How the Self Controls Its Brain">
How the Self Controls Its Brain

How the Self Controls Its Brain is a book by Sir John Eccles, proposing a theory of philosophical dualism, and offering a justification of how there can be mind-brain action without violating the principle of the conservation of energy. The model was developed jointly with the nuclear physicist Friedrich Beck in the period 1991-1992.
Eccles called the fundamental neural units of the cerebral cortex ""dendrons"", which are cylindrical bundles of neurons arranged vertically in the six outer layers or laminae of the cortex, each cylinder being about 60 micrometres in diameter. Eccles proposed that each of the 40 million dendrons is linked with a mental unit, or ""psychon"", representing a unitary conscious experience. In willed actions and thought, psychons act on dendrons and, for a moment, increase the probability of the firing of selected neurons through quantum tunneling effect in synaptic exocytosis, while in perception the reverse process takes place.

</doc>
<doc id="49185" url="https://en.wikipedia.org/wiki?curid=49185" title="Trofim Lysenko">
Trofim Lysenko

Trofim Denisovich Lysenko (, ; 20 November 1976) the son of Denis and Oksana Lysenko, was born to a peasant family in Karlivka, Poltava Governorate (in present-day Poltava Oblast, Ukraine)on September 29th, 1898. He later attended the Kiev Agricultural Institute where he found himself interested in agriculture (now the National University of Life and Environmental Sciences of Ukraine). Here he worked on a few different projects, one being the effects of temperature variation on the life cycle of plants. This later led him to consider how he may use this work to convert winter wheat into spring wheat. This process came to be known as "vernalization". Lysenko was an early proponent of soft inheritance and rejected Mendelian genetics in favor of pseudoscientific ideas termed Lysenkoism.
His experimental research in improved crop yields earned the support of Soviet leader Joseph Stalin, especially following the famine and loss of productivity resulting from resistance to forced collectivization in several regions of the Soviet Union in the early 1930s. In 1940, he became director of the Institute of Genetics within the USSR's Academy of Sciences, and Lysenko's anti-Mendelian doctrines were further secured in Soviet science and education by the exercise of political influence and power. Scientific dissent from Lysenko's theories of environmentally acquired inheritance was formally outlawed in 1948.
Though Lysenko remained at his post in the Institute of Genetics until 1965, his influence on Soviet agricultural practice had declined by the 1950s.
Early rise.
In the beginning when working at the university, he worked with converting winter wheat into spring wheat. The conversion of winter wheat into spring wheat by Lysenko was not a new discovery. However, this work has been initially supported by Nikolai Vavilov, Vavilov was the one who initially supported Lysenko and encouraged him on his work. At this current time, Lysenko had a difficult time trying to grow various crops (such as peas and wheat), through the harsh winters. However, when he did have successes,Lysenko was praised in the Soviet newspaper "Pravda" for his claims to have discovered a method to fertilize fields without using fertilizers or minerals, and to have shown that a winter crop of peas could be grown in Azerbaijan, "turning the barren fields of the Transcaucasus green in winter, so that cattle will not perish from poor feeding, and the peasant Turk will live through the winter without trembling for tomorrow." At the time it could be safely said that Lysenko and Vavilov were colleagues, but with the rise of Joseph Stalin in the Soviet, the paths for these two scientists would change: Lysenko would prevail and Vavilov would be left in the dust. 
Lysenko argued that there is not only competition, but also mutual assistance among individuals within a species, and that mutual assistance also exists between different species.
According to Lysenko,
Work in Agriculture.
As we have previously seen mentioned, Lysenko worked with different wheat crops in trying to convert them to grow in different seasons. Other investigations Lysenko found himself curious with, was the effect of heat on plant growth. He believed that every plant needed a determinate amount of heat throughout its lifetime. He attempted to correlate the time and the amount of heat needed by a particular plant requires to go through various phases of development. To get his data he looked at the amount of growth, how many days went by, and the temperature on those days. In trying to determine the effects he had a made a small statistical reasoning error. This is a general trend that can be seen throughout majority of his works and his major "findings". He was confronted by Maksimov who was an expert on thermal plant development. Lysenko did not take well to this or any criticism for that matter. After this encounter, Lysenko boldly claimed that mathematics has no place in biology. 
In 1927, at 29 years of age, working at an agricultural experiment station in Azerbaijan, he embarked on the research that would lead to his 1928 paper on vernalization, which drew wide attention because of its potential practical implications for Soviet agriculture. Severe cold and lack of winter snow had destroyed many early winter-wheat seedlings. By treating wheat seeds with moisture as well as cold, Lysenko induced them to bear a crop when planted in spring. Lysenko coined the term "Jarovization" to describe this chilling process, which he used to make the seeds of winter cereals behave like spring cereals ("Jarovoe"). However, this method had already been known by farmers since the 1800s, and had recently been discussed in detail by Gustav Gassner as "vernalization" (from the Latin "vernus", of the Spring). Lysenko's claims for increased yields were based on plantings over a few hectares, and he believed that the vernalized transformation could be inherited, that the offspring of a vernalized plant would themselves possess the capabilities as the generation that preceded it. That it too would be able to withstand harsh winters or imperfect weather conditions.
Lysenko's genetic theory.
Lysenko rejected Mendelian genetic inheritance theory in favor of his own logic. He believed Mendel's theory to be too reactionary or idealist. Lysenko's ideas were for the most part his own and not directly derived from any already established ideas such as Mendelian genetics theory, Lamarckism, or ideas from Darwin. Lysenko shaped his genetic concepts in series to support a simple practical purpose in breeding and improving wheat. His ideas were also shaped with the caution to disprove other claims made by his fellow geneticists at the time. His ideas and genetic claims later began to be coined "Lysenkoism". Lysenko claimed that his ideas were not associated to Lamarckism and that they were all their own, but this is not entirely true. It is possible to see that there are similar ideas between the two group's, such as acquired characteristics. Some of ideas can seem to be vitalistic, for example, he describes that plants are self- sacrificing. They do not die to a lack of sunlight or moisture, but instead die so that healthy ones may live, and that when they die they will deposit themselves over the other growing roots to help them grow. 
Lysenko believed that in one generation of hybridized crop, the desired individual could be selected and mated again and continue to produce the same desired product, and not worrying about separation/segregation in future breeds. For this to work, he must assume that after a lifetime of developing (acquiring) the best set of traits to survive that those must be passed down to the next generation. This assumption disregards the potential for variation or mutation. Trofim did not believe that genes or DNA existed, and only spoke about them to say that they did not exist. He instead believed that any body, once alive, obtained heredity. This meant that the entirety of the body was able to pass on the information of that organism, and was not dependent on a special element such as DNA or genes. This threw biologists at this time for a turn, because it went against all the knowledge that they were gaining during this time. It also went against all Mendelian principles that most biologists/geneticists had been using to build their ideas on. His ideas also were not sufficient enough for most other scientists, it did not truly explain the trend of inheritance. Some scientists during this time and history of science writers believed that his methods were pseudoscientific and not truly genetics.
Another theory of Lysenko's was that to obtain more milk from a cow, it did not depend on the genetics, but instead on how they are treated. The better they are taken care of and handled, the more produce that would be obtained. With this being said, Lysenko and his followers were well known for taking very good care of their livestock. Lysenkoites also believed that fertilization was not random, but instead there is specific selection for the best mate. For reasons like these, many people coin Lysenkoism as being pseudoscientific.
After World War II had ended, Lysenko took an interest in the works of Olga Lepeshiskaia, an elder physician who claimed to be able to create cells from egg yolk and non-cellular matter. She recognized common ground between her ideas and Lysenko's. By combining both of their ideas it was possible to proclaim that cells could grow from non-cellular material, and that the predicted ratios of Mendelian genetics and meiosis were incorrect, thus undermining the basis for modern cytology as well as genetics. 
Politics.
In the Soviet Union during the early and mid twentieth century, the country was under a time of war and then reformation. Political oppression was present causing tension among the state but also promoting the flourishing of science. This was possible due to the flow of resources and demand for results. Working with various plants such as wheat and peas, Lysenko aimed to manipulate them to increase the production, quality, and quantity. However, Lysenko in particular more so impressed political officials with his success in motivating peasants to return to farming. The Soviet's Collectivist reforms forced the confiscation of agricultural landholdings from peasant farmers and heavily damaged the country's overall food production, and the dispossessed peasant farmers posed new problems for the regime. Many had abandoned the farms altogether; many more waged resistance to collectivization by poor work quality and pilfering. The dislocated and disenchanted peasant farmers were a major political concern to the Soviet leadership. Lysenko emerged during this period by advocating radical but unproven agricultural methods, and also promising that the new methods provided wider opportunities for year-round work in agriculture. Lysenko proved himself very useful to the Soviet leadership by reengaging peasants to return to work, helping to secure from them a personal stake in the overall success of the Soviet revolutionary experiment.
Due to close partnership this brought between Stalin and Lysenko, Lysenko bore quite a bit of influence over genetics in the Soviet Union during the early and mid twentieth century. Lysenko eventually became the director of Genetics for the Academy of Sciences, which gave him even more control over genetics. He remained in the position for several years until some time after the fall of Stalin and later Khruschchev, when he was relieved of his duties.
Lysenkoism also played well into the Soviet theme occurring at that time, which was to "create the new Soviet man". The logic was that if people are able to inherit the acquired characteristics, it could be possible to create a better society. This lent hope to the revolutionary leaders of the Soviet union, in that peasants could be turned into exceptional citizens. This, however, was never one of Lysenko's intentions. He had never wanted to apply his techniques and principles onto humans, and was in fact strongly against it. He disagreed with the Eugenics movement entirely. 
When the political support finally ceased, Lysenkoism collapsed and became no more. Some of those who had supported him remained faithful and hid in silence, others dropped their previous notions in order to obtain jobs elsewhere and to potentially hold some influence in the science community. This meant that there were still Lysenkoites amongst and influencing the scientific community. They had not been dealt with as those who had stood against them had. 
After Stalin.
Following Stalin's death in 1953, Lysenko retained his position, with the support of the new leader Nikita Khrushchev. However, mainstream scientists re-emerged, and found new willingness within Soviet government leadership to tolerate criticism of Lysenko, the first opportunity since the late 1920s. In 1962 three of the most prominent Soviet physicists, Yakov Borisovich Zel'dovich, Vitaly Ginzburg, and Pyotr Kapitsa, presented a case against Lysenko, proclaiming his work as false science. They also denounced Lysenko's application of political power to silence opposition and eliminate his opponents within the scientific community. These denunciations occurred during a period of structural upheaval in Soviet government, during which the major institutions were purged of the strictly ideological and political machinations which had controlled the work of the Soviet Union's scientific community for several decades under Stalin.
In 1964, physicist Andrei Sakharov spoke out against Lysenko in the General Assembly of the Academy of Sciences:
The Soviet press was soon filled with anti-Lysenkoite articles and appeals for the restoration of scientific methods to all fields of biology and agricultural science. In 1965 Lysenko was removed from his post as director of the Institute of Genetics at the Academy of Sciences and restricted to an experimental farm in Moscow's Lenin Hills (the Institute itself was soon dissolved). After Khrushchev's dismissal in 1964, the president of the Academy of Sciences declared that Lysenko's immunity to criticism had officially ended. An expert commission was sent to investigate records kept at Lysenko's experimental farm. His secretive methods and ideas were revealed. A few months later, a devastating critique of Lysenko was made public. As a result, Lysenko was immediately disgraced in the Soviet Union.
After Lysenko's monopoly on biology and agronomy had ended, it took many years for these sciences to recover in Russia. 
Lysenko died in Moscow in 1976, and was interred in the Kuntsevo Cemetery.
Works.
"Heredity and Its Variability" (1945)
"The Science of Biology Today" (1948)
Further reading.
Graham, Loren, "Lysenko's Ghost: Epigenetics and Russia," (Cambridge: Harvard University Press), 2016

</doc>
<doc id="49186" url="https://en.wikipedia.org/wiki?curid=49186" title="Pope Anacletus">
Pope Anacletus

Pope Anacletus (died c. 92), also known as Cletus, was the third Bishop of Rome, following Saint Peter and Pope Linus. Anacletus served as pope between c. 79 and his death, c. 92.
Name and etymology.
The name "Cletus" in Ancient Greek means "one who has been called," and "Anacletus" means "one who has been called back." Also "Anencletus" (Greek: Ανέγκλητος) means "unimpeachable."
The Roman Martyrology mentions the Pope in question only under the name of "Cletus." The "Annuario Pontificio" gives both forms as alternatives. Eusebius, Saint Irenaeus, Saint Augustine and Optatus all suggest that both names refer to the same individual.
Papacy.
St. Cletus/Anacletus was traditionally understood to have been a Roman who served as pope for twelve years. The "Annuario Pontificio" states, "For the first two centuries, the dates of the start and the end of the pontificate are uncertain." It gives the years 80 to 92 as the reign of Pope Cletus/Anacletus. Other sources give the years 77 to 88.
According to tradition, Pope Anacletus divided Rome into twenty-five parishes. One of the few surviving records concerning his papacy mentions him as having ordained an uncertain number of priests.
Burial.
He died and was buried next to his predecessor, Saint Linus, in St. Peter's Basilica, in what is now Vatican City. His name (as Cletus) is included in the Roman Canon of the Mass.
Veneration.
The Tridentine Calendar reserved 26 April as the feast day of Saint Cletus, who the church honoured jointly with Saint Marcellinus, and 13 July for solely Saint Anacletus. In 1960, Pope John XXIII, while keeping the 26 April feast, which mentions the saint under the name given to him in the Canon of the Mass, removed 13 July as a feast day for Saint Anacletus. The 14 February 1961 Instruction of the Congregation for Rites on the application to local calendars of Pope John XXIII's motu proprio "Rubricarum instructum" of 25 July 1960, decreed that "the feast of 'Saint Anacletus,' on whatever ground and in whatever grade it is celebrated, is transferred to 26 April, under its right name, 'Saint Cletus.'" Use of this calendar, which is included in the 1962 edition of the Roman Missal, continues to be authorized under the conditions indicated in the motu proprio "Summorum Pontificum"; but the feast has been removed from the General Roman Calendar since 1969. Although the day of his death is unknown, Saint Cletus continues to be listed in the Roman Martyrology among the saints of 26 April.

</doc>
<doc id="49189" url="https://en.wikipedia.org/wiki?curid=49189" title="Frauenburg">
Frauenburg

Frauenburg may refer to the following places:

</doc>
<doc id="49195" url="https://en.wikipedia.org/wiki?curid=49195" title="George Westinghouse">
George Westinghouse

George Westinghouse, Jr. (October 6, 1846 – March 12, 1914) was an American entrepreneur and engineer who invented the railway air brake and was a pioneer of the electrical industry, gaining his first patent at the age of 22. Based in Pittsburgh, Pennsylvania for much of his career, Westinghouse was one of Thomas Edison's main rivals in the early implementation of the American electricity system. Westinghouse's electricity distribution system, based on alternating current, ultimately prevailed over Edison's insistence on direct current. In 1911 Westinghouse received the AIEE's Edison Medal "For meritorious achievement in connection with the development of the alternating current system."
Early years.
George Westinghouse was born in 1846 in Central Bridge, New York, the son of Emeline (Vedder) and George Westinghouse, Sr., a machine shop owner. From his youth, he was talented at machinery and business. At the age of fifteen, as the Civil War broke out, Westinghouse enlisted in the New York National Guard and served until his parents urged him to return home. In April 1863 he persuaded his parents to allow him to re-enlist, whereupon he joined Company M of the 16th New York Cavalry and earned promotion to the rank of corporal. In December 1864 he resigned from the Army to join the Navy, serving as Acting Third Assistant Engineer on the gunboat "USS Muscoota" through the end of the war. After his military discharge in August 1865, he returned to his family in Schenectady and enrolled at Union College. However, he lost interest in the curriculum and dropped out in his first term there.
Westinghouse was 19 years old when he created his first invention, the rotary steam engine. He also devised the Westinghouse Farm Engine. At age 21 he invented a "car replacer", a device to guide derailed railroad cars back onto the tracks, and a reversible frog, a device used with a railroad switch to guide trains onto one of two tracks.
In 1867, Westinghouse met and soon married Marguerite Erskine Walker. They were married for 47 years, and had one son, George Westinghouse III, who had six children. The couple made their first home in Pittsburgh, Pennsylvania. They later acquired houses in Lenox, Massachusetts, where they summered, and in Washington, District of Columbia.
Air brakes.
At about this time, he witnessed a train wreck where two engineers saw one another, but were unable to stop their trains in time using the existing brakes. Brakemen had to run from car to car, on catwalks atop the cars, applying the brakes manually on each car.
In 1869, at age 22, Westinghouse invented a railroad braking system using compressed air. The Westinghouse system used a compressor on the locomotive, a reservoir and a special valve on each car, and a single pipe running the length of the train (with flexible connections) which both refilled the reservoirs and controlled the brakes, allowing the engineer to apply and release the brakes simultaneously on all cars. It is a failsafe system, in that any rupture or disconnection in the train pipe will apply the brakes throughout the train. It was patented by Westinghouse on October 28, 1873. The Westinghouse Air Brake Company (WABCO) was subsequently organized to manufacture and sell Westinghouse's invention. It was in time nearly universally adopted by railways. Modern trains use brakes in various forms based on this design. The same conceptual design of fail-safe air brake is also found on heavy trucks.
Westinghouse pursued many improvements in railway signals (which then used oil lamps). In 1881 he founded the Union Switch and Signal Company to manufacture his signaling and switching inventions.
Electric power distribution.
In 1879 Thomas Edison invented an improved incandescent light bulb, and realized the need for an electrical distribution system to provide power for lighting. On September 4, 1882, Edison switched on the world's first electric power distribution system, providing 110 volts direct current (DC) to 59 customers in lower Manhattan, around his Pearl Street Station. The main drawback with Edison's low-voltage DC power network was its short transmission range, with centralized plants only able to supply customers within a mile of each plant.
Westinghouse's interests in gas distribution and telephone switching led him to become interested in electrical power distribution. In 1884 he started developing his own DC lighting system and hired physicist William Stanley to work on it. Westinghouse became aware of the new European AC systems in 1885 when he read about them in the UK technical journal "Engineering". An AC power system allowed voltages to be "stepped up" by a transformer for distribution long distances without the severe power losses suffered by DC systems, and then "stepped down" by a transformer for consumer use. With AC's potential to achieve greater economies of scale with large centralized power plants, and its ability to supply electricity long distance in cities with more disperse populations, Westinghouse saw a way to build a truly competitive system instead of simply building another barely competitive DC lighting system using patents just different enough to get around the Edison patents. The Edison DC system of centralized DC plants with short transmission range also meant that there was a patchwork of un-supplied customers between Edison's plants that Westinghouse could easily supply with AC power. 
In 1885 Westinghouse imported a number of Gaulard-Gibbs transformers (developed by Lucien Gaulard of France and John D. Gibbs of England, and demonstrated in London in 1881) and a Siemens AC generator, to begin experimenting with AC networks in Pittsburgh. AC transformers were not new, but the Gaulard-Gibbs design was one of the first that could handle high power and be readily manufactured.
Stanley, assisted by engineers Albert Schmid and Oliver B. Shallenberger developed the Gaulard-Gibbs transformer design into the first practical transformer used in an AC system. In 1886, Westinghouse and Stanley installed the first multiple-voltage AC power system in Great Barrington, Massachusetts. The network was driven by a hydroelectric generator that produced 500 volts AC. The voltage was stepped up to 3,000 volts for transmission, and then stepped back down to 100 volts to drive electric lights. That same year, Westinghouse formed the "Westinghouse Electric & Manufacturing Company"; in 1889 he renamed it as "Westinghouse Electric Corporation".
The Westinghouse company installed 30 more AC-lighting systems within a year and by the end of 1887 it had 68 alternating current power stations to Edison's 121 DC based stations. The expansion of Westinghouse's AC power distribution system led him into a bitter confrontation with Edison and his DC power system in a feud that became known as the "War of Currents". Edison mentioned to colleagues at the end of 1886 that Westinghouse would ""kill a customer within six months"" with his new AC system and first spoke out (privately) in a late 1887 letter to a New York State committee trying to determine a new, more humane system of execution to replace hanging, say the best method would be to wire the prisoner to a Westinghouse AC generator. In February 1888 Edison began a public media campaign claiming that high voltage AC systems were inherently dangerous. Westinghouse responded that the risks could be managed and were outweighed by the benefits. Edison tried to have legislation enacted in several states to limit power transmission voltages to 800 volts, but failed.
Westinghouse also had to deal with an AC rival, the Thomson-Houston Electric Company. They had built 22 power stations by the end of 1887 and by 1889 had bought out a third AC competitor, the Brush Electric Company. Thomson-Houston was expanding their business while trying to avoid patent conflicts with Westinghouse, arranging deals such as coming to agreements over lighting company territory, paying a royalty to use the Stanley transformer patent, and allowing Westinghouse to use their Sawyer-Man incandescent bulb patent.
After a young New York City boy was killed from accidentally touching a fallen telegraph wire that had been energized with alternating current, an electrical consultant named Harold P. Brown sent a June 5, 1888 letter to the editor of the "New York Evening Post". The letter claimed that alternating current was inherently dangerous and asked why the ""public must submit to constant danger from sudden death"" just so utilities could use a cheaper AC system. Within a few days, Brown was lobbying in the newspapers for server regulations on AC power. Within two months he would publicly electrocute a dog with AC at Columbia College to prove how dangerous it was. Westinghouse pointed out, in letters to various newspapers, the number of fires caused by DC equipment and that Brown was obviously in the employ of Edison, something Brown denied.
Brown, after becoming a consultant to the New York board in charge of finding a new method of executing condemned prisoners, went on to be instrumental in making sure that AC would be used in the newly developed electric chair. In August 1889, the "New York Sun" published letters stolen from Brown's office that seemed to show that Brown was receiving directions from, and being paid by, the Edison company and the Thomson-Houston company. Thomson-Houston also worked with Brown to secretly acquire 3 secondhand Westinghouse AC generators for prisons to use in their executions. In August 1890, a convict named William Kemmler became the first man to be executed by electrocution. Westinghouse hired the best lawyer of the day to defend Kemmler, and the lawyer condemned electrocution as a form of "cruel and unusual punishment", not allowable under the US Constitution.
The War of Currents would end with financiers, such as J. P. Morgan, pushing Edison Electric towards AC and pushing out Thomas Edison. The Edison Machine Works started pursuing AC development in 1890. By 1892, Thomas Edison was no longer in control of his own company. It was merged with the Thomson-Houston Electric Company into General Electric, a conglomerate now armed with all of Thomson-Houston's AC patents.
During this period Westinghouse continued to pour money and engineering resources into the goal of building a completely integrated AC system. To gain control of the Sawyer-Man lamp patents he bought Consolidated Electric Light in 1888. In April 1888 Westinghouse engineer Oliver B. Shallenberger developed an induction meter that used a rotating magnetic field for measuring alternating current. The same basic meter technology remains in use today in the early 21st century. That same year, inventor Nikola Tesla demonstrated a polyphase brushless AC induction motor, also based on a rotating magnetic field. An AC electric motor was one of the final elements an AC system needed to compete with DC systems. In July 1888, George Westinghouse licensed Nikola Tesla's American patents for the induction motor and transformer designs. Westinghouse also purchased an American patent option on a possible prior design for an induction motor from the Italian physicist and electrical engineer Galileo Ferraris. He wanted to avoid the rather substantial amount of money being asked to secure the Tesla license, but Westinghouse concluded that it was too risky not to obtain what may have been an earlier patent by Tesla. The acquisition of a feasible AC motor gave Westinghouse a key patent in building a completely integrated AC system, but the financial strain of buying up patents and hiring the engineers needed to build it meant development of Tesla's motor had to be put on hold for a while.
In 1891 Westinghouse built a hydroelectric AC power plant, the Ames Hydroelectric Generating Plant. The plant supplied power to the Gold King Mine 3.5 miles away. This was the first successful demonstration of long-distance transmission of industrial-grade alternating current power and used two 100-hp Westinghouse alternators, one working as a generator producing 3000 volt, 133 Hertz, single-phase AC, and the other used as an AC motor. At the beginning of 1893 Westinghouse engineer Benjamin Lamme had made great progress developing an efficient version of Tesla's induction motor and Westinghouse Electric started branding their complete polyphase phase AC system as the "Tesla Polyphase System", announcing Tesla's patents gave them patent priority over other AC systems and their intentions to sue patent infringes.
In 1893, George Westinghouse won the bid to light the 1893 World's Columbian Exposition in Chicago with alternating current, beating a General Electric bid by one million dollars. This World's Fair devoted a building to electrical exhibits. It was a key event in the history of AC power, as Westinghouse demonstrated the safety, reliability, and efficiency of a fully integrated alternating current system to the American public.
Westinghouses demonstration that they could build a complete AC system at the Colombian Exposition was instrumental in them getting the contract for building a two phase AC generating system, the Adams Power Plant, at Niagara Falls in 1895. In order to keep the other big electric company in play for future projects, the contract to build the three-phase AC distribution system was awarded to General Electric. The early to mid 1890s saw General Electric, backed by financier J. P. Morgan, involved in costly take over attempts and patent battles with Westinghouse Electric. A patent sharing agreement was finally signed between the two companies in 1896.
Other projects.
In 1889, Westinghouse purchased several mining claims in the Patagonia Mountains of southeastern Arizona and formed the Duquesne Mining & Reduction Company. A year later he founded what is now the ghost town of Duquesne to use as his company headquarters. He lived in a large Victorian frame house, which still stands, but in disrepair. Duquesne grew to over a 1,000 residents and the mine reached its peak production in the mid-1910s.
With AC networks expanding, Westinghouse turned his attention to electrical power production. At the outset, the available generating sources were hydroturbines where falling water was available, and reciprocating steam engines where it was not. Westinghouse felt that reciprocating steam engines were clumsy and inefficient, and wanted to develop some class of "rotating" engine that would be more elegant and efficient.
One of his first inventions had been a rotary steam engine, but it had proven impractical. The British engineer Charles Algernon Parsons began experimenting with steam turbines in 1884, beginning with a 10-horsepower (7.5 kW) . Westinghouse bought rights to the Parsons turbine in 1885, and improved the Parsons technology and increased its scale.
In 1898 Westinghouse demonstrated a 300 kilowatt unit, replacing reciprocating engines in his air-brake factory. The next year he installed a 1.5 megawatt, 1,200 rpm unit for the Hartford Electric Light Company.
Westinghouse then developed steam turbines for maritime propulsion. Large turbines were most efficient at about 3,000 rpm, while an efficient propeller operated at about 100 rpm. That required reduction gearing, but building reduction gearing that could operate at high rpm and at high power was difficult, since a slight misalignment would shake the power train to pieces. Westinghouse and his engineers devised an automatic alignment system that made turbine power practical for large vessels.
Westinghouse remained productive and inventive almost all his life. Like Edison, he had a practical and experimental streak. At one time, Westinghouse began to work on heat pumps that could provide heating and cooling, and believed that he might be able to extract enough power in the process for the system to run itself.
Any modern engineer would clearly see that Westinghouse was after a perpetual motion machine, and the British physicist Lord Kelvin, one of Westinghouse's correspondents, told him that he would be violating the laws of thermodynamics. Westinghouse replied that might be the case, but it made no difference. If he couldn't build a perpetual-motion machine, he would still have a heat pump system that he could patent and sell.
With the introduction of the automobile after the turn of the century, Westinghouse went back to earlier inventions and devised a compressed air shock absorber for automobile suspensions.
Westinghouse remained a captain of American industry until 1907, when a financial panic led to his resignation from control of the Westinghouse company. By 1911, he was no longer active in business, and his health was in decline.
George Westinghouse died on March 12, 1914, in New York City, at age 67. He was initially interred in Woodlawn Cemetery, Bronx, NY then removed on December 14, 1915. As a Civil War veteran, he was buried in Arlington National Cemetery, along with his wife Marguerite, who survived him by three months. She had also initially interred in Woodlawn and removed and reinterred at the same time as George. Although a shrewd and determined businessman, Westinghouse was a conscientious employer and wanted to make fair deals with his business associates.
Labor relations.
A six-day workweek was the rule when George Westinghouse inaugurated the first Saturday half holiday in his Pittsburgh factory in 1881.
Honors and awards.
In 1918 his former home, Solitude, was razed and the land given to the City of Pittsburgh to establish Westinghouse Park. In 1930, the Westinghouse Memorial, funded by his employees, was placed in Schenley Park in Pittsburgh. Also named in his honor, George Westinghouse Bridge is near the site of his Turtle Creek plant. Its plaque reads:
The George Westinghouse, Jr., Birthplace and Boyhood Home in Central Bridge, New York was listed on the National Register of Historic Places in 1986.

</doc>
<doc id="49197" url="https://en.wikipedia.org/wiki?curid=49197" title="Antiviral drug">
Antiviral drug

Antiviral drugs are a class of medication used specifically for treating viral infections. Like antibiotics and broad-spectrum antibiotics for bacteria, most antivirals are used for specific viral infections, while a broad-spectrum antiviral is effective against a wide range of viruses. Unlike most antibiotics, antiviral drugs do not destroy their target pathogen; instead they inhibit their development.
Antiviral drugs are one class of antimicrobials, a larger group which also includes antibiotic (also termed antibacterial), antifungal and antiparasitic drugs, or antiviral drugs based on monoclonal antibodies. Most antivirals are considered relatively harmless to the host, and therefore can be used to treat infections. They should be distinguished from viricides, which are not medication but deactivate or destroy virus particles, either inside or outside the body. Natural antivirals are produced by some plants such as eucalyptus.
Medical uses.
Most of the antiviral drugs now available are designed to help deal with HIV, herpes viruses, the hepatitis B and C viruses, and influenza A and B viruses. Researchers are working to extend the range of antivirals to other families of pathogens.
Designing safe and effective antiviral drugs is difficult, because viruses use the host's cells to replicate. This makes it difficult to find targets for the drug that would interfere with the virus without also harming the host organism's cells. Moreover, the major difficulty in developing vaccines and anti-viral drugs is due to viral variation.
The emergence of antivirals is the product of a greatly expanded knowledge of the genetic and molecular function of organisms, allowing biomedical researchers to understand the structure and function of viruses, major advances in the techniques for finding new drugs, and the intense pressure placed on the medical profession to deal with the human immunodeficiency virus (HIV), the cause of the deadly acquired immunodeficiency syndrome (AIDS) pandemic.
The first experimental antivirals were developed in the 1960s, mostly to deal with herpes viruses, and were found using traditional trial-and-error drug discovery methods. Researchers grew cultures of cells and infected them with the target virus. They then introduced into the cultures chemicals which they thought might inhibit viral activity, and observed whether the level of virus in the cultures rose or fell. Chemicals that seemed to have an effect were selected for closer study.
This was a very time-consuming, hit-or-miss procedure, and in the absence of a good knowledge of how the target virus worked, it was not efficient in discovering effective antivirals which had few side effects. Only in the 1980s, when the full genetic sequences of viruses began to be unraveled, did researchers begin to learn how viruses worked in detail, and exactly what chemicals were needed to thwart their reproductive cycle.
Virus life cycle.
Viruses consist of a genome and sometimes a few enzymes stored in a capsule made of protein (called a capsid), and sometimes covered with a lipid layer (sometimes called an 'envelope'). Viruses cannot reproduce on their own, and instead propagate by subjugating a host cell to produce copies of themselves, thus producing the next generation.
Researchers working on such "rational drug design" strategies for developing antivirals have tried to attack viruses at every stage of their life cycles. Some species of mushrooms have been found to contain multiple antiviral chemicals with similar synergistic effects.
Viral life cycles vary in their precise details depending on the species of virus, but they all share a general pattern:
Limitations of vaccines.
Vaccines bolster the body's immune system to better attack viruses in the "complete particle" stage, outside of the organism's cells. They traditionally consist of an attenuated (a live weakened) or inactivated (killed) version of the virus. These vaccines can, in very rare cases, harm the host by inadvertently infecting the host with a full-blown viral occupancy. Recently "subunit" vaccines have been devised that consist strictly of protein targets from the pathogen. They stimulate the immune system without doing serious harm to the host. In either case, when the real pathogen attacks the subject, the immune system responds to it quickly and blocks it.
Vaccines are very effective on stable viruses, but are of limited use in treating a patient who has already been infected. They are also difficult to successfully deploy against rapidly mutating viruses, such as influenza (the vaccine for which is updated every year) and HIV. Antiviral drugs are particularly useful in these cases.
Anti-viral targeting.
The general idea behind modern antiviral drug design is to identify viral proteins, or parts of proteins, that can be disabled. These "targets" should generally be as unlike any proteins or parts of proteins in humans as possible, to reduce the likelihood of side effects. The targets should also be common across many strains of a virus, or even among different species of virus in the same family, so a single drug will have broad effectiveness. For example, a researcher might target a critical enzyme synthesized by the virus, but not the patient, that is common across strains, and see what can be done to interfere with its operation.
Once targets are identified, candidate drugs can be selected, either from drugs already known to have appropriate effects, or by actually designing the candidate at the molecular level with a computer-aided design program.
The target proteins can be manufactured in the lab for testing with candidate treatments by inserting the gene that synthesizes the target protein into bacteria or other kinds of cells. The cells are then cultured for mass production of the protein, which can then be exposed to various treatment candidates and evaluated with "rapid screening" technologies.
Approaches by life cycle stage.
Before cell entry.
One anti-viral strategy is to interfere with the ability of a virus to infiltrate a target cell. The virus must go through a sequence of steps to do this, beginning with binding to a specific "receptor" molecule on the surface of the host cell and ending with the virus "uncoating" inside the cell and releasing its contents. Viruses that have a lipid envelope must also fuse their envelope with the target cell, or with a vesicle that transports them into the cell, before they can uncoat.
This stage of viral replication can be inhibited in two ways:
This strategy of designing drugs can be very expensive, and since the process of generating anti-idiotypic antibodies is partly trial and error, it can be a relatively slow process until an adequate molecule is produced.
Entry inhibitor.
A very early stage of viral infection is viral entry, when the virus attaches to and enters the host cell. A number of "entry-inhibiting" or "entry-blocking" drugs are being developed to fight HIV. HIV most heavily targets the immune system's white blood cells known as "helper T cells", and identifies these target cells through T-cell surface receptors designated "CD4" and "CCR5". Attempts to interfere with the binding of HIV with the CD4 receptor have failed to stop HIV from infecting helper T cells, but research continues on trying to interfere with the binding of HIV to the CCR5 receptor in hopes that it will be more effective.
HIV infects a cell through fusion with the cell membrane, which requires two different cellular molecular participants, CD4 and a chemokine receptor (differing depending on the cell type). Approaches to blocking this virus/cell fusion have shown some promise in preventing entry of the virus into a cell. At least one of theses entry inhibitors—a biomimetic peptide marketed under the brand name Fuzeon—has received FDA approval and has been in use for some time. Potentially, one of the benefits from the use of an effective entry-blocking or entry-inhibiting agent is that it potentially may not only prevent the spread of the virus within an infected individual but also the spread from an infected to an uninfected individual.
One possible advantage of the therapeutic approach of blocking viral entry (as opposed to the currently dominant approach of viral enzyme inhibition) is that it may prove more difficult for the virus to develop resistance to this therapy than for the virus to mutate or evolve its enzymatic protocols.
Uncoating inhibitor.
Inhibitors of uncoating have also been investigated.
Amantadine and rimantadine have been introduced to combat influenza. These agents act on penetration and uncoating.
Pleconaril works against rhinoviruses, which cause the common cold, by blocking a pocket on the surface of the virus that controls the uncoating process. This pocket is similar in most strains of rhinoviruses and enteroviruses, which can cause diarrhea, meningitis, conjunctivitis, and encephalitis.
During viral synthesis.
A second approach is to target the processes that synthesize virus components after a virus invades a cell.
Reverse transcription.
One way of doing this is to develop nucleotide or nucleoside analogues that look like the building blocks of RNA or DNA, but deactivate the enzymes that synthesize the RNA or DNA once the analogue is incorporated. This approach is more commonly associated with the inhibition of reverse transcriptase (RNA to DNA) than with "normal" transcriptase (DNA to RNA).
The first successful antiviral, acyclovir, is a nucleoside analogue, and is effective against herpesvirus infections. The first antiviral drug to be approved for treating HIV, zidovudine (AZT), is also a nucleoside analogue.
An improved knowledge of the action of reverse transcriptase has led to better nucleoside analogues to treat HIV infections. One of these drugs, lamivudine, has been approved to treat hepatitis B, which uses reverse transcriptase as part of its replication process. Researchers have gone further and developed inhibitors that do not look like nucleosides, but can still block reverse transcriptase.
Another target being considered for HIV antivirals include RNase H – which is a component of reverse transcriptase that splits the synthesized DNA from the original viral RNA.
On 10 August 2011 researchers at MIT announced the publication of a new method of inhibiting RNA, the process selectively affected infected cells. The team named the process "Double-stranded RNA Activated Caspase Oligomerizer" (DRACO). According to the lead researcher "In theory, should work against all viruses."
Integrase.
Another target is integrase, which splices the synthesized DNA into the host cell genome.
Transcription.
Once a virus genome becomes operational in a host cell, it then generates messenger RNA (mRNA) molecules that direct the synthesis of viral proteins. Production of mRNA is initiated by proteins known as transcription factors. Several antivirals are now being designed to block attachment of transcription factors to viral DNA.
Translation/antisense.
Genomics has not only helped find targets for many antivirals, it has provided the basis for an entirely new type of drug, based on "antisense" molecules. These are segments of DNA or RNA that are designed as complementary molecule to critical sections of viral genomes, and the binding of these antisense segments to these target sections blocks the operation of those genomes. A phosphorothioate antisense drug named fomivirsen has been introduced, used to treat opportunistic eye infections in AIDS patients caused by cytomegalovirus, and other antisense antivirals are in development. An antisense structural type that has proven especially valuable in research is morpholino antisense.
Morpholino oligos have been used to experimentally suppress many viral types:
Translation/ribozymes.
Yet another antiviral technique inspired by genomics is a set of drugs based on ribozymes, which are enzymes that will cut apart viral RNA or DNA at selected sites. In their natural course, ribozymes are used as part of the viral manufacturing sequence, but these synthetic ribozymes are designed to cut RNA and DNA at sites that will disable them.
A ribozyme antiviral to deal with hepatitis C has been suggested, and ribozyme antivirals are being developed to deal with HIV. An interesting variation of this idea is the use of genetically modified cells that can produce custom-tailored ribozymes. This is part of a broader effort to create genetically modified cells that can be injected into a host to attack pathogens by generating specialized proteins that block viral replication at various phases of the viral life cycle.
Protein processing and targeting.
Interference with post translational modifications or with targeting of viral proteins in the cell is also possible.
Protease inhibitors.
Some viruses include an enzyme known as a protease that cuts viral protein chains apart so they can be assembled into their final configuration. HIV includes a protease, and so considerable research has been performed to find "protease inhibitors" to attack HIV at that phase of its life cycle. Protease inhibitors became available in the 1990s and have proven effective, though they can have unusual side effects, for example causing fat to build up in unusual places. Improved protease inhibitors are now in development.
Protease inhibitors have also been seen in nature. A protease inhibitor was isolated from the Shiitake mushroom ("Lentinus edodes"). The presence of this may explain the Shiitake mushrooms noted antiviral activity "in vitro".
Assembly.
Rifampicin acts at the assembly phase.
Release phase.
The final stage in the life cycle of a virus is the release of completed viruses from the host cell, and this step has also been targeted by antiviral drug developers. Two drugs named zanamivir (Relenza) and oseltamivir (Tamiflu) that have been recently introduced to treat influenza prevent the release of viral particles by blocking a molecule named neuraminidase that is found on the surface of flu viruses, and also seems to be constant across a wide range of flu strains.
Immune system stimulation.
A second category of tactics for fighting viruses involves encouraging the body's immune system to attack them, rather than attacking them directly. Some antivirals of this sort do not focus on a specific pathogen, instead stimulating the immune system to attack a range of pathogens.
One of the best-known of this class of drugs are interferons, which inhibit viral synthesis in infected cells. One form of human interferon named "interferon alpha" is well-established as part of the standard treatment for hepatitis B and C, and other interferons are also being investigated as treatments for various diseases.
A more specific approach is to synthesize antibodies, protein molecules that can bind to a pathogen and mark it for attack by other elements of the immune system. Once researchers identify a particular target on the pathogen, they can synthesize quantities of identical "monoclonal" antibodies to link up that target. A monoclonal drug is now being sold to help fight respiratory syncytial virus in babies, and antibodies purified from infected individuals are also used as a treatment for hepatitis B.
Acquired resistance.
Almost all anti-microbials, including anti-virals, are subject to drug resistance as the pathogens mutate over time, becoming less susceptible to the treatment. For instance, a recent study published in Nature Biotechnology emphasized the urgent need for augmentation of oseltamivir (Tamiflu) stockpiles with additional antiviral drugs including zanamivir (Relenza) based on an evaluation of the performance of these drugs in the scenario that the 2009 H1N1 'Swine Flu' neuraminidase (NA) were to acquire the tamiflu-resistance (His274Tyr) mutation which is currently widespread in seasonal H1N1 strains.

</doc>
<doc id="49198" url="https://en.wikipedia.org/wiki?curid=49198" title="Reductionism">
Reductionism

Reductionism refers to several related but different philosophical positions regarding the connections between phenomena, or theories, "reducing" one to another, usually considered "simpler" or more "basic". "The Oxford Companion to Philosophy" suggests that it is "one of the most used and abused terms in the philosophical lexicon" and suggests a three part division:
Reductionism can be applied to objects, phenomena, explanations, theories, and meanings.
In the sciences, application of methodological reductionism attempts explanation of entire systems in terms of their individual, constituent parts and their interactions. Thomas Nagel speaks of "psychophysical reductionism" (the attempted reduction of psychological phenomena to physics and chemistry), as do others and "physico-chemical reductionism" (the attempted reduction of biology to physics and chemistry), again as do others. In a very simplified and sometimes contested form, such reductionism is said to imply that a system is "nothing but" the sum of its parts. However, a more nuanced view is that a system is composed entirely of its parts, but the system will have features that none of the parts have. "The point of mechanistic explanations is usually showing "how" the higher level features arise from the parts."
Other definitions are used by other authors. For example, what Polkinghorne calls "conceptual" or "epistemological" reductionism is the definition provided by Blackburn and by Kim: that form of reductionism concerning a program of replacing the facts or entities entering statements claimed to be true in one area of discourse with other facts or entities from another area, thereby providing a relationship between them. Such a connection is provided where the same idea can be expressed by "levels" of explanation, with higher levels reducible if need be to lower levels. This use of levels of understanding in part expresses our human limitations in grasping a lot of detail. However, "most philosophers would insist that our role in conceptualizing reality need for an hierarchy of "levels" of understanding does not change the fact that different levels of organization in reality do have different "properties"."
As this introduction suggests, there are a variety of forms of reductionism, discussed in more detail in subsections below.
Reductionism strongly reflects a certain perspective on causality. In a reductionist framework, the phenomena that can be explained completely in terms of relations between other more fundamental phenomena, are called epiphenomena. Often there is an implication that the epiphenomenon exerts no causal agency on the fundamental phenomena that explain it. The epiphenomena are sometimes said to be "nothing but" the outcome of the workings of the fundamental phenomena, although the epiphenomena might be more clearly and efficiently described in very different terms. There is a tendency to avoid taking an epiphenomenon as being important in its own right. This attitude may extend to cases where the fundamentals are not clearly able to explain the epiphenomena, but are expected to by the speaker. In this way, for example, morality can be deemed to be "nothing but" evolutionary adaptation, and consciousness can be considered "nothing but" the outcome of neurobiological processes.
Reductionism does not preclude the existence of what might be called emergent phenomena, but it does imply the ability to understand those phenomena completely in terms of the processes from which they are composed. This reductionist understanding is very different from emergentism, which intends that what emerges in "emergence" is more than the sum of the processes from which it emerges.
Types.
Most philosophers delineate three types of reductionism and antireductionism.
Ontological reductionism.
Ontological reductionism is the belief that reality is composed of a minimum number of kinds of entities or substances. This claim is usually metaphysical, and is most commonly a form of monism, in effect claiming that all objects, properties and events are reducible to a single substance. (A dualist who is an ontological reductionist would believe that everything is reducible to two substances — as one possible example, a dualist might claim that reality is composed of "matter" and "spirit".)
Nancey Murphy has claimed that there are two species of ontological reductionism: one that denies that wholes are anything more than their parts; and the stronger thesis of atomist reductionism that wholes are not "really real". She admits that the phrase "really real" is apparently senseless but nonetheless has tried to explicate the supposed difference between the two.
Ontological reductionism denies the idea of ontological emergence, and claims that emergence is an epistemological phenomenon that only exists through analysis or description of a system, and does not exist on a fundamental level.
Ontological reductionism takes two different forms: "token ontological reductionism" and "type ontological reductionism".
Token ontological reductionism is the idea that every item that exists is a sum item. For perceivable items, it says that every perceivable item is a sum of items at a smaller level of complexity. Token ontological reduction of biological things to chemical things is generally accepted.
Type ontological reductionism is the idea that every type of item is a sum type of item, and that every perceivable type of item is a sum of types of items at a lower level of complexity. Type ontological reduction of biological things to chemical things is often rejected.
Michael Ruse has criticized ontological reductionism as an improper argument against vitalism.
Methodological reductionism.
Methodological reductionism is the position that the best scientific strategy is to attempt to reduce explanations to the smallest possible entities. Methodological reductionism would thus hold that the atomic explanation of a substance's boiling point is preferable to the chemical explanation, and that an explanation based on even smaller particles (quarks and leptons, perhaps) would be even better.
Methodological reductionism, therefore, is the position that all scientific theories either can or should be reduced to a single super~theory through the process of theoretical reduction.
Theory reductionism.
Theory reduction is the process by which one theory absorbs another. For example, both Kepler's laws of the motion of the planets and Galileo's theories of motion worked out for terrestrial objects are reducible to Newtonian theories of mechanics, because all the explanatory power of the former are contained within the latter. Furthermore, the reduction is considered to be beneficial because Newtonian mechanics is a more general theory—that is, it explains more events than Galileo's or Kepler's. Theoretical reduction, therefore, is the reduction of one explanation or theory to another—that is, it is the absorption of one of our ideas about a particular thing into another idea.
In science.
Reductionist thinking and methods form the basis for many of the well-developed areas of modern science, including much of physics, chemistry and cell biology. Classical mechanics in particular is seen as a reductionist framework, and statistical mechanics can be viewed as a reconciliation of macroscopic thermodynamic laws with the reductionist approach of explaining macroscopic properties in terms of microscopic components.
In science, reductionism implies that certain fields of study are based on areas that study smaller spatial scales or organizational units. While it is commonly accepted that the foundations of chemistry are based in physics, and molecular biology is rooted in chemistry, similar statements become controversial when one considers less rigorously defined intellectual pursuits. For example, claims that sociology is based on psychology, or that economics is based on sociology and psychology would be met with reservations. These claims are difficult to substantiate even though there are clear connections between these fields (for instance, most would agree that psychology can affect and inform economics). The limit of reductionism's usefulness stems from emergent properties of complex systems, which are more common at certain levels of organization. For example, certain aspects of evolutionary psychology and sociobiology are rejected by some who claim that complex systems are inherently irreducible and that a holistic approach is needed to understand them.
Some strong reductionists believe that the behavioral sciences should become "genuine" scientific disciplines based on genetic biology, and on the systematic study of culture (see Richard Dawkins's concept of memes). In his book "The Blind Watchmaker", Dawkins introduced the term "hierarchical reductionism" to describe the view that complex systems can be described with a hierarchy of organizations, each of which is only described in terms of objects one level down in the hierarchy. He provides the example of a computer, which under hierarchical reductionism is explained in terms of the operation of hard drives, processors, and memory, but not on the level of AND OR gates, or on the even lower level of electrons in a semiconductor medium.
Others argue that inappropriate use of reductionism limits our understanding of complex systems. In particular, ecologist Robert Ulanowicz says that science must develop techniques to study ways in which larger scales of organization influence smaller ones, and also ways in which feedback loops create structure at a given level, independently of details at a lower level of organization. He advocates (and uses) information theory as a framework to study propensities in natural systems. Ulanowicz attributes these criticisms of reductionism to the philosopher Karl Popper and biologist Robert Rosen.
The idea that phenomena such as emergence and work within the field of complex systems theory pose limits to reductionism has been advocated by Stuart Kauffman. Emergence is strongly related to nonlinearity. The limits of the application of reductionism are claimed to be especially evident at levels of organization with higher amounts of complexity, including living cells, neural networks, ecosystems, society, and other systems formed from assemblies of large numbers of diverse components linked by multiple feedback loops.
Nobel laureate P.W. Anderson used the idea that symmetry breaking is an example of an emergent phenomenon in his 1972 "Science" paper "More is different" to make an argument about the limitations of reductionism. One observation he made was that the sciences can be arranged roughly in a linear hierarchy — particle physics, many body physics, chemistry, molecular biology, cellular biology, physiology, psychology, social sciences — in that the elementary entities of one science obeys the laws of the science that precedes it in the hierarchy; yet this does not imply that one science is just an applied version of the science that precedes it. He writes that "At each stage, entirely new laws, concepts and generalizations are necessary, requiring inspiration and creativity to just as great a degree as in the previous one. Psychology is not applied biology nor is biology applied chemistry."
Disciplines such as cybernetics and systems theory embrace a non-reductionist view of science, sometimes going as far as explaining phenomena at a given level of hierarchy in terms of phenomena at a higher level, in a sense, the opposite of a reductionist approach.
In mathematics.
In mathematics, reductionism can be interpreted as the philosophy that all mathematics can (or ought to) be built on a common foundation, which is usually axiomatic set theory. Ernst Zermelo was one of the major advocates of such a view; he also developed much of axiomatic set theory. It has been argued that the generally accepted method of justifying mathematical axioms by their usefulness in common practice can potentially undermine Zermelo's reductionist program.
As an alternative to set theory, Jouko Väänänen has argued for second-order logic as a foundation for mathematics instead of set theory, whereas others have argued for category theory as a foundation for certain aspects of mathematics.
The incompleteness theorems of Kurt Gödel, published in 1931, raised doubts about the attainability of an axiomatic foundation for all of mathematics. Any such foundation would have to include axioms powerful enough to describe the arithmetic of the natural numbers (a subset of all mathematics). Yet Gödel proved that for any self-consistent recursive axiomatic system powerful enough to describe the arithmetic of the natural numbers, there are propositions about the natural numbers that cannot be proved from the axioms, but which we can prove in the natural language with which we described the axioms. (Such propositions are known as formally undecidable propositions.)
In religion.
Religious reductionism generally attempts to explain religion by boiling it down to certain nonreligious causes. A few examples of reductionistic explanations for the presence of religion are: that religion can be reduced to humanity's conceptions of right and wrong, that religion is fundamentally a primitive attempt at controlling our environments, that religion is a way to explain the existence of a physical world, and that religion confers an enhanced survivability for members of a group and so is reinforced by natural selection. Anthropologists Edward Burnett Tylor and James George Frazer employed some religious reductionist arguments. Sigmund Freud held that religion is nothing more than an illusion, or even a mental illness, and Marx claimed that religion is "the sigh of the oppressed," and the opium of the people providing only "the illusory happiness of the people," thus providing two influential examples of reductionistic views against the idea of religion.
In linguistics.
Linguistic reductionism is the idea that everything can be described or explained in a language with a limited number of core concepts, and combinations of those concepts.
In philosophy.
The concept of downward causation poses an alternative to reductionism within philosophy. This view is developed and explored by Peter Bøgh Andersen, Claus Emmeche, Niels Ole Finnemann, and Peder Voetmann Christiansen, among others. These philosophers explore ways in which one can talk about phenomena at a larger-scale level of organization exerting causal influence on a smaller-scale level, and find that some, but not all proposed types of downward causation are compatible with science. In particular, they find that constraint is one way in which downward causation can operate. The notion of causality as constraint has also been explored as a way to shed light on scientific concepts such as self-organization, natural selection, adaptation, and control.
Free will.
Philosophers of the Enlightenment worked to insulate human free will from reductionism. Descartes separated the material world of mechanical necessity from the world of mental free will. German philosophers introduced the concept of the "noumenal" realm that is not governed by the deterministic laws of "phenomenal" nature, where every event is completely determined by chains of causality. The most influential formulation was by Immanuel Kant, who distinguished between the causal deterministic framework the mind imposes on the world—the phenomenal realm—and the world as it exists for itself, the noumenal realm, which included free will. To insulate theology from reductionism, 19th century post-Enlightenment German theologians moved in a new direction, led by Friedrich Schleiermacher and Albrecht Ritschl. They took the Romantic approach of rooting religion in the inner world of the human spirit, so that it is a person's feeling or sensibility about spiritual matters that comprises religion.
Antireductionism.
The antireductionist takes this position as a minimum requirement upon the reductionist: "What is unclear is how the pre-theoretical intuitions example, of free will are to be accommodated theoretically within favored analyses... At the very least the anti-reductionist is owed an account of why the intuitions arise if they are not accurate."
A contrast to the reductionist approach is holism or emergentism. Holism is the idea that things can have properties, (emergent properties), as a whole that are not explainable from the sum of their parts. The principle of holism was concisely summarized by Aristotle in the Metaphysics: "The whole is more than the sum of its parts".
The term greedy reductionism, coined by Daniel Dennett, is used to criticize inappropriate use of reductionism.
Alternatives.
The development of systems thinking has provided methods for tackling issues in a holistic rather than a reductionist way, and many scientists approach their work in a holistic paradigm. When the terms are used in a scientific context, holism and reductionism refer primarily to what sorts of models or theories offer valid explanations of the natural world; the scientific method of falsifying hypotheses, checking empirical data against theory, is largely unchanged, but the approach guides which theories are considered. The conflict between reductionism and holism in science is not universal—it usually centers on whether or not a holistic or reductionist approach is appropriate in the context of studying a specific system or phenomenon.
In many cases (such as the kinetic theory of gases), given a good understanding of the components of the system, one can predict all the important properties of the system as a whole. In other systems, emergent properties of the system are said to be almost impossible to predict from knowledge of the parts of the system. Complexity theory studies systems and properties of the latter type.
Alfred North Whitehead set his metaphysical thinking in opposition to reductionism. He refers to this as the "fallacy of the misplaced concreteness". His scheme set out to frame a rational, general understanding of things, that was derived from our reality.
Sven Erik Jorgensen, an ecologist, lays out both theoretical and practical arguments for a holistic approach in certain areas of science, especially ecology. He argues that many systems are so complex that it will not ever be possible to describe all their details. Drawing an analogy to the Heisenberg uncertainty principle in physics, he argues that many interesting and relevant ecological phenomena cannot be replicated in laboratory conditions, and thus cannot be measured or observed without influencing and changing the system in some way. He also points to the importance of interconnectedness in biological systems. His viewpoint is that science can only progress by outlining what questions are unanswerable and by using models that do not attempt to explain everything in terms of smaller hierarchical levels of organization, but instead model them on the scale of the system itself, taking into account some (but not all) factors from levels both higher and lower in the hierarchy.
Criticism.
"Fragmentalism" is an alternative term for ontological reductionism, although "fragmentalism" is frequently used in a pejorative sense. Anti-realists use the term fragmentalism in arguments that the world does not exist of separable entities, instead consisting of wholes. For example, advocates of this position hold that: The linear deterministic approach to nature and technology promoted a fragmented perception of reality, and a loss of the ability to foresee, to adequately evaluate, in all their complexity, global crises in ecology, civilization and education.
An alternative usage of this term is in cognitive psychology. Here, George Kelly developed "constructive alternativism" as a form of personal construct psychology, this provided an alternative to what he saw as "accumulative fragmentalism". In this theory, knowledge is seen as the construction of successful mental models of the exterior world, rather than the accumulation of independent "nuggets of truth".

</doc>
<doc id="49200" url="https://en.wikipedia.org/wiki?curid=49200" title="Queen Elizabeth 2">
Queen Elizabeth 2

Queen Elizabeth 2, often referred to simply as QE2, is an ocean liner built for the Cunard Line which was operated by Cunard as both a transatlantic liner and a cruise ship from 1969 to 2008. She was designed for the transatlantic service from her home port of Southampton, UK, to New York, and was named after the earlier Cunard liner . She served as the flagship of the line from 1969 until succeeded by in 2004. Designed in Cunard's then headquarters and regional offices in Liverpool and Southampton respectively, and built in Clydebank, Scotland, she was considered the last of the great transatlantic ocean liners until the construction of the Queen Mary 2 was announced.
Before she was refitted with a diesel power plant in 1986/87, "QE2" was also the last oil-fired passenger steamship to cross the Atlantic in scheduled liner service. During almost forty years of service, "Queen Elizabeth 2" undertook regular world cruises and latterly operated predominantly as a cruise ship, sailing out of Southampton, England. "QE2" had no running mate and never ran a year-round weekly transatlantic express service to New York. "QE2" did, however, continue the Cunard tradition of regular scheduled transatlantic crossings every year of her service life. "QE2" was never designated RMS, or Royal Mail Ship, instead carrying the SS and later MV or MS prefixes in official documents.
"QE2" retired from active Cunard service on 27 November 2008. She was acquired by Istithmar, the private equity arm of Dubai World, which planned to begin conversion of the vessel to a 500-room floating hotel moored at the Palm Jumeirah, Dubai. The 2008 financial crisis however intervened and the ship remained idle. Subsequent conversion plans were announced by Istithmar in 2012 and by the "Oceanic Group" in 2013 but these both stalled. As of January 2016 the ship remains laid up in Dubai while the port operator claimed that there were future plans for the ship and no intent to scrap her.
Characteristics.
The ship has a gross tonnage of 70,327 and is long. She had a top speed of with her original steam turbines; this was increased to when the vessel was re-engined with a diesel-electric powerplant.
History.
Concept and construction.
By the mid 1960s transatlantic travel was dominated by air travel due to its speed and low cost relative to the sea route, and expansion of air travel showed no signs of slowing down. Conversely, and "Queen Elizabeth" were becoming increasingly expensive to operate, and both internally and externally were relics of the pre-war years. Cunard did not want to give up the business of passenger service, and so gambled $80 million on a new ocean liner to replace the original ageing "Queens".
Realising the decline of transatlantic trade, and the rising costs of fuel and labour, Cunard decided their new ship had to be smaller and cheaper to operate than her predecessors. The new ship was designed to run at the same service speed of as the previous "Queens", using half the fuel. Staff was also reduced from the levels on the older vessels. "QE2" would also be able to transit the Panama Canal and her draught was seven feet less than her predecessors, allowing her to enter ports that the old "Queens" could not, and compete with the new generation of cruise ships.
The interior and superstructure for the QE2 was designed by James Gardner. His design for the ocean liner was described by The Council of Industrial Design as that of a "very big yacht" and with a "look was sleek and purposeful".
Originally designated "Q4" (a previous ship design "Q3" had been abandoned due to falling passenger revenues on the North Atlantic), she was to be a three class liner. However, looking to "France", designs were changed to make "Q4" a two-class liner that could be modified into a single class cruise ship; transatlantic line voyages in the summer would be two-class, while warmer water cruises in the winter would be single-class.
"Queen Elizabeth 2" was built by the John Brown Shipyard in Clydebank, Scotland. The keel was laid down on 5 July 1965, as hull number 736 on the same plot where iconic liners such as , , "Queen Mary", and "Queen Elizabeth" had been constructed. She was launched and named on 20 September 1967 by Queen Elizabeth II, using the same pair of gold scissors her mother and grandmother used to launch "Queen Elizabeth" and "Queen Mary", respectively. On 19 November 1968 she left John Brown's fitting out berth, and travelled down the River Clyde to the Firth of Clyde Dry Dock at Inchgreen, Greenock, for final trials and commissioning. After sea trials in the Irish Sea a "shakedown cruise" to Las Palmas set out on 22 April 1969.
Service history.
Early career.
"Queen Elizabeth 2"s maiden voyage, from Southampton to New York, commenced on 2 May 1969, taking 4 days, 16 hours, and 35 minutes. However, Prince Charles was the first "civilian" passenger to board the ship, on her voyage from the shipyard in Clydebank to drydock in Greenock. On board for the short journey was her Master Designate and first captain, William (Bil) Warwick. In 1971, she participated in the rescue of some 500 passengers from the burning French Line ship .
On 17 May 1972, while travelling from New York to Southampton, she was the subject of a bomb threat. She was searched by her crew, and a combined Special Air Service and Special Boat Service team which parachuted into the sea to conduct a search of the ship. No bomb was found, but the hoaxer was arrested by the FBI.
The following year "QE2" undertook two chartered cruises through the Mediterranean to Israel in commemoration of the 25th anniversary of the state's founding. As it was then known, The Coronia Restaurant was koshered for Passover, and Jewish passengers were able to celebrate Passover on the ship.
Falklands War.
In May 1982 the ship took part in the Falklands War, carrying 3,000 troops and 650 volunteer crew to the south Atlantic. She was refitted in Southampton in preparation for war service, including the installation of two helicopter pads, the transformation of public lounges into dormitories, the installation of fuel pipes that ran through the ship down to the engine room to allow for refuelling at sea, and the covering of carpets with 2,000 sheets of hardboard. A quarter of the ship’s length was reinforced with steel plating, an anti-magnetic coil was fitted to combat naval mines. Over 650 Cunard crew members volunteered for the voyage to look after the 3,000 members of the Fifth Infantry Brigade, which the ship transported to South Georgia. During the voyage the ship was blacked out and the radar switched off to avoid detection, steaming on without modern aids.
The "QE2" returned to the UK in June 1982, where she was greeted in Southampton Water by The Queen Mother on board the Royal Yacht "Britannia". Peter Jackson, the captain of the QE2 responded to the Queen Mother's welcome: "Please convey to Her Majesty Queen Elizabeth our thanks for her kind message. Cunard's Queen Elizabeth 2 is proud to have been of service to Her Majesty's Forces." The ship underwent conversion back to passenger service, with her funnel being painted in the traditional Cunard orange with black stripes which are known as "Hands", at the same time the hull's exterior was repainted an unconventional light pebble grey. This colour proved unpopular with passengers, as well as difficult to maintain and so the hull reverted to traditional colours in 1983. Later that year, QE2 was fitted with a magrodome over her Quarter Deck pool.
Diesel era and Project Lifestyle.
"QE2" once again experienced mechanical problems following her annual overhaul in November 1983. Boiler problems caused Cunard to cancel a cruise, and, in October 1984, an electrical fire caused a complete loss of power. The ship was delayed for several days before power could be restored. Instead of replacing the QE2 with a newer vessel, Cunard decided that it was more prudent to simply make improvements to her. Therefore, in 1986/87, "QE2" underwent one of her most significant refurbishments when she was converted from steam power to diesel. Nine MAN B&W diesel electric engines, new propellers and a heat recovery system ( to utilise heat expelled by the engines) were fitted. With this new propulsion system, "QE2" was expected to serve another 20 years with Cunard. The passenger accommodation was also modernised.
On 7 August 1992, the hull was extensively damaged when she ran aground south of Cuttyhunk Island near Martha's Vineyard, while returning from a five-day cruise to Halifax, Nova Scotia along the east coast of the United States and Canada. A combination of her speed, an uncharted shoal and underestimating the increase in the ship's draft due to the effect of squat led to the ship's hull scraping rocks on the ocean floor. The accident resulted in the passengers disembarking earlier than scheduled at nearby Newport, Rhode Island and the ship being taken out of service while temporary repairs were made in drydock at Boston. Several days later, divers found red paint on previously uncharted rocks in the vicinity of where the ship was said to have hit bottom.
By the mid 1990s it was decided that "QE2" was due for a new look and in 1994 the ship was given a multimillion-pound refurbishment in Hamburg code named Project Lifestyle.
On 11 September 1995, "QE2" encountered a rogue wave, estimated at , caused by Hurricane Luis in the North Atlantic Ocean about 200 miles south of eastern Newfoundland. One year later, during her twentieth world cruise, she completed her four millionth mile. The ship had sailed the equivalent of 185 times around the planet.
"QE2" celebrated the 30th anniversary of her maiden voyage in Southampton in 1999. In three decades she had 1,159 voyages, sailed and carried over two million passengers.
Later years.
Following the 1998 acquisition of the Cunard Line by Carnival Corporation, in 1999 "QE2" was given a US$30 million refurbishment which included refreshing various public rooms, and a new colour palette in the passenger cabins. The Royal Promenade, which formerly housed upscale shops such as Burberry, H. Stern and Aquascutum, were replaced by boutiques typical of cruise ships, selling perfumes, watches and logo items. During this refit the hull was stripped to bare metal, and the ship repainted in the traditional Cunard colours of matte black (Federal Grey) with a white superstructure.
In 2004 the vessel stopped plying the traditional "transatlantic" route and began full-time cruising, the transatlantic route having been assigned to Cunard's new flagship, the . However, the "QE2" still undertook an annual world cruise and regular trips around the Mediterranean. By this time, she lacked the amenities to rival newer, larger cruise ships, but she still had unique features such as her ballrooms, hospital, and 6000 book library. "QE2" retained her title of one of the fastest cruise ships afloat (28.5 knots), with fuel economy at this speed at 49.5 ft (15m) to the gallon. While cruising at slower speeds efficiency was improved to 125 ft per gallon.
At the end of her 2005 world cruise, some pieces of her artwork were damaged when some crew members who had become inebriated at an on-board crew party, went on a vandalism rampage through the public areas of the ship. A unique tapestry of the "QE2", commissioned for the launch of the ship, was thrown overboard by a drunken crewman. An oil painting of Queen Elizabeth II and two other tapestries were damaged, along with a part of the entertainment area and a lifeboat. The crew members involved were dismissed from service, with charges pending.
On 5 November 2004 the "QE2" became Cunard's longest serving express liner, surpassing the RMS "Aquitania"s 35 years, while on 4 September 2005, during a call to the port of Sydney, Nova Scotia, "QE2" became the longest serving Cunarder ever, surpassing the s record.
On 20 February 2007 the "QE2", while on her annual world cruise, met her running mate and successor flagship "QM2" (herself on her maiden world cruise) in Sydney Harbour, Australia. This was the first time two Cunard "Queens" had been together in Sydney since the original "Queen Mary" and "Queen Elizabeth" served as troop ships in 1941.
Retirement and Final Cunard voyage.
On 18 June 2007 it was announced by Cunard that "QE2" had been purchased by the Dubai investment company Istithmar for $100 million. Her retirement in part was forced by the oncoming June 2010 implementation of the International Convention for the Safety of Life at Sea (SOLAS) regulations, which would have forced large and expensive structural changes to have been implemented to the ship.
In a ceremonial display before her retirement, the "QE2" met the and the "Queen Mary 2" near the Statue of Liberty in New York City harbour on 13 January 2008, with a celebratory fireworks display; the "QE2" and "QV" had made a tandem crossing of the Atlantic for the meet. This marked the first time three "Cunard Queens" had been present in the same location (Cunard stated this would be the last time these three particular ships would meet, due to the impending retirement of the "QE2". However, due to a change in the "QE2"s schedule, the three ships met again in Southampton on 22 April 2008).
"QE2" shared the harbour at Zeebrugge with "Queen Victoria" on 19 July 2008, where the two Cunarders exchanged whistle blasts.
On 3 October 2008, "QE2" set off from Cork for Douglas Bay on her farewell tour of the British Isles, before heading for Liverpool. She left Liverpool and arrived in Belfast on 4 October 2008, before moving to Greenock the next day (the ship's height with funnel makes it impossible to pass under the Erskine Bridge so Clydebank is not reachable). There she was escorted by Royal Navy destroyer and visited by . The farewell was viewed by large crowds and concluded with a firework display. "QE2" then sailed around Scotland to the Firth of Forth on 7 October 2008, where she anchored in the shadow of the Forth Bridge. The next day, following an RAF flypast, she left amidst a flotilla of small craft to head to Newcastle upon Tyne, before returning to Southampton. QE2-South Queensferry.jpg
"QE2" completed her final Atlantic crossing from New York to Southampton in tandem with her successor, "QM2". The two liners departed New York on 16 October and arrived in Southampton on 22 October. This marked the end of "QE2"s transatlantic voyages.
On her final arrival into Southampton, "QE2" (on 11 November 2008, with 1,700 passengers and 1,000 crew on board) ran aground in the Solent at the Southampton Water entrance at 5.26 am. BBC reported "Cunard has confirmed it touched the bottom at the Brambles Turn sandbank (sandback) near Calshot, Southampton Water, with three tugs attached to her stern (0530 GMT). A fourth tug secured a line to the ship's bow." Solent Coastguard stated: "Five tugs were sent out to assist her getting off the sandbank, and she was pulled off just before 6.10 am. She had been refloated and was under way under her own power and heading back to her berth in Southampton. She had only partially gone aground, and the tugs pulled her off."
Once safely back at her berth, preparations continued for her farewell celebrations. These were led by Prince Philip, Duke of Edinburgh who toured the ship at great length. He visited areas of interest including the Engine Control Room. He also met with current and former crew members. During this time, divers were sent down to inspect the hull for any possible damage caused by the vessel's earlier mishap – none were found.
The "QE2" left Southampton Docks for the final time at 1915 GMT on 11 November 2008, to begin her farewell voyage by the name of ""QE2"s Final Voyage". Her ownership passed to Nakheel Properties, a company of Dubai World, on 26 November. The decommissioning of the ship was particularly poignant for the "QE2"s only permanent resident, Beatrice Muller, aged 89, who lived on board in retirement for fourteen years, at a cost of some £3,500 (~€4,300, ~$5,400) per month.
At the time of her retirement "QE2" had sailed nearly six million miles, carried 2.5 million passengers and completed 806 transatlantic crossings.
Istithmar, Nakheel, "QE2" in Dubai and Cape Town hotel proposal.
Her final voyage from Southampton to Dubai began on 11 November 2008, arriving on 26 November in a flotilla of 120 smaller vessels, led by MY "Dubai", the personal yacht of Sheikh Mohammed, ruler of Dubai, in time for her official handover the following day.
She was greeted with a fly-past from an Emirates Airbus A380 jet and a huge fireworks display, while thousands of people gathered at the Mina Rashid, waving the flags of Great Britain and the United Arab Emirates. Since her arrival in Dubai "QE2" has remained moored at Port Rashid. Shortly after her final passengers were disembarked, she was moved forward to the cargo area of the port, to free up the passenger terminal for other cruise vessels.
She was expected to be refurbished and berthed permanently at Nakheel's Palm Jumeirah as "a luxury floating hotel, retail, museum and entertainment destination." The refurbishment planned to see the "QE2" transformed into a tourist destination in Dubai, however due to the Global Economic Crisis "QE2" has remained moored at Port Rashid awaiting a decision on her future.
"QE2" remains an oceangoing vessel, and as such, Ronald Warwick (former Captain of "QE2", "QM2" and a retired Commodore of the Cunard Line) was initially employed by V-Ships (who have managed "QE2" since Cunard handed her over) as the vessel's legal master, but has subsequently been replaced by other V-Ships captains. Since 2009, she has been captained by William Cooper.
It was anticipated that the "QE2" would be moved to the Dubai Drydocks sometime in 2009 to begin a series of far-reaching refurbishments which would result in her being converted into a floating hotel however, no confirmed destination for the QE2's retirement and reopening has been announced.
Due to the 2008 global recession, fears have been sparked that "QE2"s refurbishment and hotel conversion will not take place, and that the ship may be resold. These rumours have since resulted in owners, Istithmar, issuing a series of press releases stating that plans for QE2's conversion are ongoing, with no intention to sell. However, since arriving in Dubai the only visible exterior change to "QE2" is the painting out of the Cunard titles from the ship's superstructure.
"QE2" was joined in Mina Rashid by "QM2" on Saturday, 21 March 2009 while "QM2" visited Dubai as part of her 2009 World Cruise. She was joined once again by the "QV" on Sunday, 29 March 2009 as a part of her 2009 World Cruise. "QM2" and "QV" again visited "QE2" in 2010 and on 31 March 2011 the new Queen Elizabeth "(QE)" called at Dubai during her maiden world cruise – photos were arranged by Cunard to capture the occasion. QM2 called in Dubai 2 days after QE left.
In April 2009, an alleged concept model of the post refurbished "Hotel QE2" was shown for sale on an online auction website. The model depicts a much altered "QE2".
In June 2009, the "Southampton Daily Echo" reported that "QE2" would return to the UK as an operating Cruise Ship. However, on 20 July 2009 the current owners Nakheel confirmed rumours that "QE2" will reposition to Cape Town for use as a floating Hotel.
On 24 June 2009, "QE2" made her first journey after nearly eight months of inactivity since the liner arrived in Dubai. She manoeuvered under her own power into the Dubai Drydocks for inspection and hull repainting before her (then planned) voyage to Cape Town's V&A Waterfront to serve there as a floating hotel for the FIFA World Cup 2010 and beyond.
On 10 July 2009, it was revealed that "QE2" might sail to Cape Town, South Africa, to become a floating hotel (for use primarily during the 2010 FIFA World Cup), in a Dubai World sponsored venture at the V&A Waterfront. This was confirmed by Nakheel on 20 July 2009.
In preparation for this expected voyage the ship was placed into the Dubai Dry-dock and underwent an extensive exterior refurbishment. During this refit, the ship's underwater hull was repainted and inspected.
Shortly after her refit, "QE2" was registered under the flag of Vanuatu, and Port Vila (her new home port) was painted on her stern, replacing Southampton.
"QE2" returned to Port Rashid where it was anticipated she would soon sail for Cape Town. The arrival of "QE2" in Cape Town was expected to create many local jobs including Hotel staff, restaurant staff, chefs, cleaners and shop attendants, all being sourced from the local workforce.
But, in January 2010, it was confirmed she would not be brought to Cape Town.
At present the vessel remains moored in Dubai amid a cloud of uncertainty regarding her future.
2010 sale and relocation speculation.
In early 2010, due to the continued poor financial performance of Dubai World, there was much media speculation that "QE2", along with other assets owned by Istithmar, Dubai World's private-equity arm, would be sold to raise capital. Despite this sale speculation, a number of alternative locations for QE2 have been cited including London, Singapore, Clydebank, Japan and Fremantle, the latter showing interest in using QE2 as a hotel for the ISAF Sailing World Championships to be held in December 2011. However, as at June 2010 Nakheel's official statement regarding QE2 is that "a number of options being considered for QE2".
2011 drifting.
On 28 January 2011 during a heavy dust storm, "QE2" broke loose from her moorings and drifted out into the channel at Port Rashid. She was attended by pilots and tugs and safely returned to berth at Port Rashid. Images of "QE2"’s unexpected movements appeared on-line after being taken by an observer in the ship in front of QE2.
Warm layup.
Throughout 2011 and 2012, "QE2" remained berthed at Port Mina Rashid in Dubai . She was maintained in a seaworthy condition and generated her own power. Each of her nine diesel generators were turned over and used to power the ship. A live-in crew of approximately 50 people maintained "QE2" to a high standard. Activities include painting, maintenance, cabin checks, and overhauls of machinery. Istithmar were considering plans for "QE2" which may involve the ship sailing to an alternative location under her own power.
On 21 March 2011 "QM2" called in Dubai and docked close to "QE2". During the departure, the two ships sounded their horns.
2011 return to Liverpool plan, Port Rashid and "QE2" development plans.
On 28 September 2011 news broke that a plan was being formulated to return QE2 to the United Kingdom by berthing her in Liverpool. Liverpool has an historic connection with Cunard Line being the first British home for the line as well as housing the iconic Cunard Building.
It was revealed that Liverpool Vision, the economic development company responsible for Liverpool's regeneration, has been involved in confidential discussions with Out of Time Concepts, a company headed by a former Chief Engineer on the ship, who recently advised its current owners on plans to turn it into a luxury hotel in Dubai.
In a letter from Out of Time Concepts to Liverpool Vision it is explained that "The free global media attention derived from bringing home the "QE2" will without question promote Liverpool's new waterfront developments, its amazing architecture, its maritime and world heritage sites, its museums, its culture and its history".
On the same week that the Liverpool Vision plans were revealed, Nakheel has said that plans for "QE2" to be berthed at The Palm have been dropped because they now plan to build 102 houses on the site which was once intended to be named the QE2 Precinct.
Nakheel suggested that "QE2", under the ownership of Istithmar, will remain at Port Rashid to become an integral part of the growing cruise terminal. "The "QE2" will be placed in a much better location", Ali Rashid Lootah, the chairman of Nakheel, told Dubai's The National newspaper "The Government of Dubai is developing an up-to-date modern cruise terminal which will mean a better environment", confirming the ship would remain in Dubai for the foreseeable future.
2011/2012 New Year's Party aboard "QE2".
On 31 December 2011 the "QE2" was the location of a lavish New Year's Eve party in Dubai. The black tie event was run by Global Event Management and included over 1,000 guests. Global Event Management were offering events aboard QE2 in Dubai for 2012 and 2013.
July 2012: hotel announcement.
On 2 July 2012 in a coordinated press release, the ship's owner, operator and Port Rashid operator, DP Ports, jointly announced "QE2" would re-open as a 300 bed hotel after an 18-month refit. The release claims the ship was to be refitted to restore original features, including her 1994–2008 'Heritage Trail' of classic Cunard artefacts. The ship was to be berthed alongside a redeveloped Port Rashid cruise terminal which would double as a maritime museum.
Scrapping in China, "QE2" London and "QE2" Asia.
On 23 December 2012, it was reported that "QE2" had been sold for scrapping in China for £20 million, after a bid to return her to the UK was rejected. With monthly berthing and maintenance charges of £650,000, it was reported that a Chinese salvage crew arrived at the vessel on 21 December, to replace a crew of 40 which has been maintaining the vessel since it arrived at Port Rashid. However, Cunard dismissed the reports as "pure speculation". When the ship was sold in 2007, a clause in the contract which started from her retirement in 2009 stipulated a 10-year "no onward sale" clause, without payment of a full purchase price default penalty.
The ""QE2" London" Plan had included a £20 million bid for "QE2" and a further £40 million refurbishment that was supposed to create more than 2,000 jobs in London, with the "QE2" docked near the O2 Arena. It had reportedly obtained the support of London Mayor Boris Johnson.
On 17 January 2013, the Dubai Drydocks World announced that the "QE2" would be sent to an unknown location in Asia to serve as a floating luxury hotel, shopping mall, and museum. Despite this move, the QE2 London team stated on the same day that "We believe our investors can show Dubai that "QE2" London is still the best proposal".
"Bring "QE2" Home" proposals.
Cunard's 175 anniversary celebrations on 25 May 2015 led to renewed interest in the "QE2". John Chillingworth secured the backing of London mayor Boris Johnson for a plan to anchor the ship opposite The O2 Arena at Greenwich. A move to London however would require the ship to pass through the Thames Barrier. In late 2015 there was disagreement between ship preservation advocates and harbour authorities on whether a dead ship of her size could safely manoeuvre through the barrier. John Houston suggested returning the ship to Greenock as a maritime attraction, hotel and events space. Inverclyde Council leader Stephen McCabe has called on the UK and Scottish governments to campaign to buy the ship, saying that “Bringing the "QE2" home is a Herculean task, one that requires national support in Scotland and perhaps across the UK, if it has any chance of happening.” In January 2016 Aubrey Fawcett, the chair of the working group to regenerate the Clyde, admitted defeat in this effort as "QE2's" owners refused to respond to any requests regarding her condition or sale. ""Consequently, we must conclude that it is highly unlikely that Scotland features in the future plans for the vessel.""
QE2's Movements in 2015.
On 12 August 2015, the QE2 was observed to have been moved from her berth within Dubai Dry Docks, where she had been since January 2013, to a more open location within Port Rashid.
On 17 November 2015, QE2 was again moved within Port Rashid, to the former cruise terminal.
It is not known whether these recent moves are connected with any of the publicly known plans regarding the ships fate.
Design.
Exterior.
Like both and , "QE2" has a flared stem and clean forecastle. What was controversial at the time was that Cunard decided not to paint the funnel with the line's distinctive colour and pattern, something that had been done on all merchant vessels since the first Cunard ship, the , sailed in 1840. Instead the funnel was painted white and black, with the Cunard orange-red appearing only on the inside of the wind scoop. This practice ended in 1983 when "QE2" returned from service in the Falklands War, and the funnel has been painted in Cunard traditional colours (orange and black), with black horizontal bands (known as "hands") ever since. The original pencil-like funnel was replaced in 1986 with a more robust one, when the ship was converted from steam to diesel power.
Large quantities of aluminium were used in the framing and cladding of "QE2"s superstructure. This decision was designed to save weight, reducing the draft of the ship and lowering the fuel consumption, but it also posed the possibility of corrosion problems that can occur with joining the dissimilar metals together, so a jointing compound was coated between the steel and aluminium surfaces to prevent this happening. The low melting point of aluminium caused concern when "QE2" was serving as a troop ship during the Falklands War: some feared that if the ship were struck by a missile, as was , her upper decks would collapse quickly due to fire, thereby causing greater casualties.
In 1972, the first penthouse suites were added in an aluminium structure on Signal Deck and Sports Deck (now "Sun Deck"), behind the ship's bridge, and in 1977 this structure was expanded to include more suites with balconies, making "QE2" one of the first ships to offer private terraces to passengers since "Normandie" in the 1930s.
"QE2"s balcony accommodation was expanded for the final time during "QE2"s 1986/87 refurbishment in Bremerhaven. During this refit the ship was given a new wider funnel built using panels from the original. It retained the traditional Cunard colours.
"QE2"s final structural changes included the reworking of the aft decks during the 1994 refit (following the removal of the Magrodome and the addition of an undercover area on Sun Deck during her 2005 refit creating a space known as Funnel Bar.
Interiors.
"Queen Elizabeth 2"s interior configuration was laid out in a horizontal fashion, similar to "France", where the spaces dedicated to the two classes were spread horizontally on specific decks, in contrast to the vertical class divisions of older liners. Where "QE2" differed from "France" was that the first class deck (Quarter Deck) was below the deck dedicated to tourist class (Upper Deck). Originally there were to be main lounges serving three classes, layered one atop the other, but when Cunard decided to make the ship a two class vessel, only two main lounges were needed. Instead of completely reconfiguring the Boat Deck, the ship's architects simply opened a well in the deck between what were to have been the second and third class lounges, creating a double height space known as the Double Room (now the Grand Lounge). This too was unconventional in that it designated a grander two-storey space for tourist class passengers, while first class passengers gathered in the standard height Queen's Room. However, the configuration for segregated Atlantic crossings gave first class passengers the theatre balcony on Boat Deck, while tourist class used the orchestra level on Upper Deck.
Over the span of her thirty-nine-year seagoing career, "QE2" has had a number of interior refits and alterations.
The year she came into service, 1969, was also the year of the Apollo 11 mission, when the Concorde's prototype was unveiled, and the previous year Stanley Kubrick's film "" premiered. In keeping with those times, originally Cunard broke from the traditional interiors of their previous liners for "QE2", especially the Art Deco style of the previous "Queens". Instead modern materials like plastic laminates, aluminium and Perspex were used. Furniture was modular and abstract art was used throughout public rooms and cabins.
The Midships Lobby on Two Deck, where first class passengers boarded for transatlantic journeys and all passengers boarded for cruises, was a circular room with a sunken seating area in the centre with green leather clad banquettes, and surrounded by a chrome railing. As a kingpin to this was a flared, white, trumpet shaped, up lit column.
Another room where "QE2"s advanced interior design was demonstrated was the first class lounge, the Queen's Room on Quarter Deck. This space, in colours of white and tan, featured a recessed, slotted ceiling, and indirect lighting. As well, the columns were flared in the same fashion as the one in the Midships Lobby, with recessed up lighting, and also reflecting the shape of the bases of the tables and leather shell chairs. The Theatre Bar on Upper Deck featured red chairs, red drapes, a red egg crate fibreglass screen, and even a red baby grand piano. Some more traditional materials like wood veneer were used as highlights throughout the ship, especially in passenger corridors and staterooms.
There was also an Observation Bar on Quarter Deck, a successor to its namesake, located in a similar location, on both previous "Queens", which offered views through large windows over the ship's bow. This room was lost in "QE2"s 1972 refit, becoming galley space with the forward-facing windows plated over.
In the 1994 refit almost all of the remaining original decor was replaced, with Cunard opting to reverse the original design direction of "QE2"s designers and use the line's traditional ocean liners as inspiration. The green velvet and leather Midships Bar became the Art Deco inspired Chart Room, receiving an original, custom designed piano from "Queen Mary". The (by now) blue dominated Theatre Bar was transformed into the Golden Lion Pub, which mimics a traditional Edwardian pub. Some original elements were retained including the flared columns in the Queens Room and Mid-Ships Lobby which were incorporated into the reworked designs.
By the time of her retirement, the Synagogue was the only room that had remained unaltered since 1969. However it was reported that during "QE2"s 22 October five night voyage, the Synagogue was carefully dismantled before being removed from the ship prior to her final sailing to Dubai.
Artwork and artefacts.
The "Queen Elizabeth 2" holds pieces of artwork, as well as maritime artefacts drawn from Cunard's long history of operating merchant vessels.
In the Mauretania Restaurant sits Althea Wynne's sculpture of the "White Horses" of the Atlantic Ocean. There are bronze busts of both Sir Samuel Cunard (outside the Yacht Club) and Queen Elizabeth II (in the Queen's Room). The Princess Grill holds four life-size statues of human forms representing the four elements, done by sculptor Janine Janet in marine materials like shell and coral. The Chart Room's frieze was designed by Brody Nevenshwander, and depicts the words of T. S. Eliot, Sir Francis Drake, and John Masefield. The Midships Lobby holds a solid silver model of the "Queen Elizabeth 2" made by Asprey of Bond Street in 1975, which was lost until a photograph was found in 1997 that led to the discovery of the model itself, and its placement on the "QE2" in 1999.
In "E" stairway hangs three custom designed tapestries, commissioned from Helena Hernmarck for the ship's launch, that depict the Queen as well as the launch of the ship. These tapestries, which were originally hung in "D" Stairway, Quarter Deck, outside the Columbia Restaurant, were damaged in 2005, as mentioned in the Service history (above). They were originally made with golden threads; however much of this was lost when they were cleaned incorrectly as part of the 1987 refit.
There are numerous photographs, oils and pastels of members of the Royal Family throughout the vessel, and silver plaques commemorating the visits of every member of the Royal Family, as well as other dignitaries like South African president Nelson Mandela.
Amongst the artefacts on board is a set of antique Japanese armour presented to the "QE2" by the Governor of Kagoshima, Japan, during her 1979 world cruise, and a Wedgwood vase presented to the ship by Lord Wedgwood.
Items from previous Cunard ships include a brass relief plaque with a fish motif from the first , and an Art Deco bas-relief titled "Winged Horse and Clouds", by Norman Foster from RMS Queen Elizabeth. There is also a vast array of Cunard postcards, porcelain, flatware, boxes, linen, and Lines Bros Tri-ang Minic model ships. One of her key pieces is a replica of the figurehead from Cunard's first ship, RMS Britannia, carved from Quebec yellow pine by Cornish sculptor Charles Moore, and presented to the ship by Lloyd's of London. On the Upper Deck sits the silver Boston Commemorative Cup, presented to "Britannia" by the City of Boston in 1840. This cup was lost for decades until being found in a pawn shop in Halifax, Nova Scotia. On "2" Deck is a bronze entitled "Spirit of the Atlantic" which was designed by Barney Seale for the second . A large wooden plaque was presented to the "QE2" by First Sea Lord Sir John Fieldhouse to commemorate the ship's service as a Hired Military Transport (HMT) in the Falklands War.
There is also an extensive collection of large-scale models of Cunard ships throughout the "QE2".
Most of these items were sold by Cunard to Istithmar when they purchased "QE2".
Crew accommodation.
The majority of crew were accommodated in two- or four-berth cabins, with showers and toilets at the end of each alleyway. These were located forward and aft on decks three through six. At the time she entered service, the crew areas were a significant improvement over those aboard and ; however the ship's age and the lack of renovation of the crew area during her 40 years of service, in contrast to passenger areas, which were updated periodically, meant that this accommodation was considered basic by the end of her career. Officers were accommodated in single cabins with private en-suite bathrooms located on Sun Deck.
There were three crew bars, one named "The Pig & Whistle". ("The Pig" for short and a tradition aboard Cunard ships), "Castaways" and the "Fo'c's'le Club". A fourth bar, dedicated for the officers is located at the forward end of Boat Deck. Named "The Officers Wardroom" this area enjoyed forward facing views and was often opened to passengers for cocktail parties hosted by the senior officers. The crew mess was situated at the forward end of One Deck, adjacent to the crew services office.
Technical.
After the ship was launched, the "QE2" was fitted out with a steam turbine propulsion system utilising three Foster Wheeler E.S.D II boilers, which provided steam for the two Brown-Pametrada turbines. The turbines were rated with a maximum power output figure of 110,000 shaft horsepower (normally operating at 94,000 hp) and were coupled to two six-bladed fixed-pitch propellers.
The steam turbines were plagued with problems from the time the ship first entered service and, despite being technically advanced and fuel-efficient in 1968, her consumption of 600 tons of fuel oil every twenty four hours was more than expected for such a ship by the 1980s. After seventeen years of service the availability of spare parts was becoming difficult due to the outdated design of the boilers and turbines, and Cunard decided that the options were to either do nothing for the remainder of the ship's life, re-configure the existing engines, or re-engine the vessel with a more efficient diesel-electric powerplant. Ultimately it was decided to replace the engines, as it was calculated that the savings in fuel costs and maintenance would pay for themselves over four years, and give the vessel a minimum of another twenty years of service, whereas the other options would only provide short-term relief. Her steam turbines had taken her to a record breaking total of 2,622,858 miles in 18 years.
During the ship's 1986 to 1987 refit, the steam turbines were removed and scrapped. The engine rooms were then fitted with nine German MAN L58/64 nine-cylinder, medium-speed diesel engines, each weighing approximately 120 tons. Using a diesel-electric configuration, each engine drives a generator, each developing 10.5 MW of electrical power at 10,000 volts. This electrical plant, in addition to powering the ship's auxiliary and hotel services through transformers, drives the two main propulsion motors, one on each propeller shaft. These motors produce 44 MW each and are of synchronised salient-pole construction, nine metres in diameter and weighing more than 400 tons each. The ship's service speed of can be maintained using only seven of the diesel-electric sets. Her maximum power output with the new engine configuration running was now 130,000 hp, which is greater than the previous system's 110,000 hp. Using the same IBF-380 (Bunker C) fuel, the new configuration yielded a 35% fuel saving over the previous system. During the re-engining process, her funnel was replaced by a wider one to accommodate the exhaust pipes for the nine B&W diesel engines.
Also during refit, the fixed-pitch propellers were replaced with variable-pitch propellers. The old steam engines required astern turbines to move the ship backwards or stop her moving forward. The pitch of the new variable pitch blades, however, could simply be reversed, causing a reversal of propeller thrust while maintaining the same direction of propeller rotation, allowing the ship shorter stopping times and improved handling characteristics. The new propellers were originally fitted with "Grim Wheels", named after their inventor, Dr.-Ing. Otto Grim. These were free-spinning propeller blades fitted behind the main propellers, with long vanes protruding from the centre hub. These were designed to recover lost propeller thrust and reduce fuel consumption by 2.5 to 3%. However, after the trial of these wheels, when the ship was drydocked, the majority of the vanes on each wheel were discovered to have broken off, and so the wheels were removed and the project abandoned.
Other machinery includes nine heat recovery boilers, coupled with two oil-fired boilers to produce steam for heating fuel, domestic water, swimming pools, laundry equipment, and galleys. Four flash evaporators and a reverse-osmosis unit desalinate sea water to produce 1000 tons of fresh water daily. There is also a sanitation system and sewage disposal plant, air conditioning plant, and an electro-hydraulic steering system.
Name.
Form of name.
The name of the liner as it appears on the bow and stern is "Queen Elizabeth 2", with upper and lower case lettering and an Arabic numeral 2 as opposed to the Roman numeral II. As such, it is commonly pronounced in speech as "Queen Elizabeth Two". Soon after launching, the name was shortened in common use as "QE2".
Background.
In 1934 "Queen Mary" was named by and after Mary of Teck and in 1938 "Queen Elizabeth" was named by and after Elizabeth Bowes-Lyon, who were both at the time of the naming married to the reigning monarch. These two previous Cunarders both had capitalised bow names, as "QUEEN ELIZABETH" and "QUEEN MARY".
Cunard practice at the time of naming "QE2" was to re use the existing name of its former ships, for example, launching the in 1938 after the previous was scrapped in 1935.
The original "Queen Elizabeth" was still in service with Cunard when "QE2" was launched in 1967, although she was retired and sold before "QE2" entered revenue service with Cunard in 1969.
The addition of a 2 in this manner was unknown at the time, but it was not unknown for Roman numerals to denote ships in service with the same name. Two non Cunard ships were named "Queen Mary II", a Clyde steamer, and "Mauretania II", a Southampton steamer of Red Funnel, since the Cunard ships already had the names without Roman numerals.
Launch.
As was Cunard practice at the time, the name of the liner was not to be publicly revealed until the launch. Dignitaries were invited to the "Launch of Cunard Liner No. 736", as no name had yet been painted on the bow.
The Queen launched the ship with the words "I name this ship "Queen Elizabeth the Second"," the normal short form of address of the monarch, Elizabeth II herself.
The following day, the "New York Times" and "British Times" printed the name as "Queen Elizabeth II", the short form of written style of the monarch. However, when the liner left the shipyard in 1968 she bore the name "Queen Elizabeth 2" on her bow, and has continued to do so ever since.
1969 authorised history.
In an authorised history of the "QE2" published in 1969, various explanations of events occur.
These state that, as at the launch ceremony, an envelope and card were also held in New York in case of transmission failure, and when opened the card was found to read the name Queen Elizabeth, and that the decision to add "The Second" to the name was an alteration by the Queen. The book quotes the Cunard chairman Sir Basil Smallpeice as saying "The "Queen Mary" after her Grandmother, the "Queen Elizabeth" after her mother, and now this magnificent ship after herself."
Following the unexpected addition of "the Second" by the Queen, the book attributes the use of upper and lower case lettering and a numeric "2" – rather than a Roman "II" – to the decision by Cunard to use a more modern typeface to suit the style of the 1960s. The book also surmises that the naming of the liner after the reigning monarch, in the form Queen Elizabeth II, was potentially offensive to some Scots, as the title of Queen Elizabeth II (of the United Kingdom) relates to the lineage of the throne of England (the Tudor monarch Elizabeth I having reigned only in England).
Ron Warwick, former Captain.
A later account by Ronald Warwick (son of William "Bil" Warwick, first master of "QE2"), who was also Master of both "QE2" and later the first captain of "QM2", supports the account that the Queen initiated the surprise move of naming the liner after herself rather than simply Queen Elizabeth as had originally been planned (the name having been made vacant by the retirement of the current liner before the new one was commissioned). The name had been given to the Queen in a sealed envelope which she didn't open. The book, referencing his autobiography, states that the Cunard chairman Sir Basil was delighted with this development, it being in keeping with the previous Queen liners, and the 2 was added by Cunard for differentiation of the ship while still denoting it was named after the Queen.
Cunard website.
From at least 2002 the official Cunard website stated that "The new ship is not named after the Queen but is simply the second ship to bear the name – hence the use of the Arabic 2 in her name, rather than the Roman II used by the Queen", however, in a change in 2007 this information had been removed.
Other accounts.
Other later accounts repeat the position that Cunard originally intended to name the ship "Queen Elizabeth" and the addition of a 2 by the Queen was a surprise to Cunard, in 1990 and 2008, although two books by William H. Miller state that Queen Elizabeth 2 was the name agreed on before the launch between Cunard officials and the Queen.
Accounts that repeat the position that "QE2" was not named after the reigning monarch have been published in 1991, 1999, 2004, 2005, and 2008. In 2008, "The Telegraph" goes further to state the ship is named not only as the second ship named "Queen Elizabeth", but is specifically named after the wife of King George VI. In contradiction however, some modern accounts continue to publish that the "QE2" was named after the reigning monarch, in 2001 and 2008.
Post "QE2" Cunard naming practice.
Cunard continued the 2 suffix naming practice introduced with "QE2" with the launch of the new ocean liner in 2003, named after the previous "Queen Mary", but her second "consort" ship was named , not "QE3", the first being named .

</doc>
<doc id="49201" url="https://en.wikipedia.org/wiki?curid=49201" title="Biplane">
Biplane

A biplane is a fixed-wing aircraft with two main wings stacked one above the other. The first aircraft to fly, the Wright Flyer, used a biplane wing arrangement, as did most aircraft in the early years of aviation. While a biplane wing structure has a structural advantage over a monoplane, it produces more drag than a similar unbraced or cantilever monoplane wing. Improved structural techniques, materials and the quest for greater speed made the biplane configuration obsolete for most purposes by the late 1930s.
Biplanes offer several advantages over conventional cantilever monoplane designs: they permit lighter wing structures, low wing loading and smaller span for a given wing area. However, interference between the airflow over each wing increases drag substantially, and biplanes generally need extensive bracing, which causes additional drag.
Biplanes are distinguished from tandem wing arrangements, where the wings are placed forward and aft, instead of above and below.
The term is also occasionally used in biology, to describe the wings of some flying animals.
Aviation.
Overview.
In a biplane aircraft, two wings are placed one above the other. Each provides part of the lift, although they are not able to produce twice as much lift as a single wing of similar size and shape because the upper and the lower are working on nearly the same portion of the atmosphere and thus interfere with each other's behaviour. For example, in a wing of aspect ratio 6, and a wing separation distance of one chord length, the biplane configuration will only produce about 20 percent more lift than a single wing of the same planform.
In the biplane configuration, the lower wing is usually attached to the fuselage, while the upper wing is raised above the fuselage with an arrangement of cabane struts, although other arrangements have been used. Either or both of the main wings can support ailerons, while flaps are more usually positioned on the lower wing. Bracing is nearly always added between the upper and lower wings, in the form of wires (tension members) and/or slender interplane struts positioned symmetrically on either side of the fuselage.
Advantages and disadvantages.
The primary advantage of the biplane over the more traditional single plane or monoplane is to combine great stiffness with light weight. Stiffness requires structural depth and, where early monoplanes had to have this added with complicated extra bracing, the box kite or biplane naturally has a deep structure and is therefore easier to make both light and strong. A braced monoplane wing must support itself fully, while the two wings of a biplane help to stiffen each other. The biplane is therefore inherently stiffer than the monoplane. Also, the structural forces in the spars of a biplane wing tend to be lower, so the wing can use less material to obtain the same overall strength and is therefore much lighter. A disadvantage of the biplane was the need for extra struts to space the wings apart, although the bracing required by early monoplanes reduced this disadvantage.
The low power supplied by the engines available in the first years of aviation meant that aeroplanes could only fly slowly. This required an even lower stalling speed, which in turn required a low wing loading, combining both large wing area with light weight. A biplane wing of a given span and chord has twice the area of a monoplane the same size and so can fly more slowly, or for a given flight speed can lift more weight. Alternatively, a biplane wing of the same area as a monoplane has lower span and chord, reducing the structural forces and allowing it to be lighter.
Biplanes suffer aerodynamic interference between the two planes. This means that a biplane does not in practice obtain twice the lift of the similarly-sized monoplane. The farther apart the wings are spaced the less the interference, but the longer the spacing struts must be.
Given the slow speed and low power of early aircraft, the drag penalty of the wires and struts and the mutual interference of airflows were relatively minor and acceptable factors. As engine powers rose after World War One, the thick-winged cantilever monoplane became practicable and, with its inherently lower drag and higher speed, from around 1918 it began to replace the biplane in most fields of aviation.
The smaller biplane wing also allows greater maneuverability. During World War One, this further enhanced the dominance of the biplane and, despite the need for speed, military aircraft were among the last to abandon the biplane form. Specialist sports Aerobatic biplanes are still occasionally made.
Stagger.
Biplanes were originally designed with the wings positioned directly one above the other. Moving the upper wing forward relative to the lower one is called positive stagger or, more often, simply stagger. It can help increase lift and reduce drag by reducing the aerodynamic interference effects between the two wings. Many biplanes have such staggered wings. A common example from the 1930s is the layout found for the Waco Standard Cabin series.
It is also possible to place the lower wing's leading edge ahead of the upper wing, giving negative stagger. This is usually done in a given design for practical engineering reasons. Examples of negative stagger include the Sopwith Dolphin and Beechcraft Staggerwing. However, forward stagger is more common because it improves both downward visibility and ease of cockpit access for open cockpit biplanes.
Bays.
The space enclosed by a set of interplane struts is called a "bay", hence a biplane or triplane with one set of such struts connecting the wings on each side of the aircraft is a "single-bay biplane." 
This provided sufficient strength for smaller aircraft such as the First World War-era Fokker D.VII fighter and the Second World War de Havilland Tiger Moth basic trainer. The larger two-seat Curtiss JN-4 Jenny is a "two bay biplane", the extra bay being necessary as overlong bays are prone to flexing and can fail.
The SPAD S.XIII fighter, while appearing to be a two bay biplane, has only one bay, but has the midpoints of the rigging braced with additional struts, however these are not structurally contiguous from top to bottom wing. The Sopwith 1½ Strutter has a W shape cabane, however as it doesn't connect the wings to each other, it doesn't add to the number of bays.
Large transport and bombing biplanes often needed still more bays to provide sufficient strength. These are sometimes referred to as "multi-bay biplanes". 
A small number of biplanes, such as the Zeppelin-Lindau D.I have no interplane struts and are referred to as being "strutless".
Sesquiplane.
The sesquiplane is a common variation on the biplane where one wing (usually the lower) is significantly smaller than the other either in span, chord or both. The name means "one-and-a-half wings." The arrangement may reduce interference drag between the wings whilst retaining the biplane's structural advantage. The 1920s Pander E is an example of an aircraft with a lower wing of exactly half the span and nearly one quarter (23%) of the area of the upper one. Some designs keep the upper and lower spans nearly equal, whilst reducing the lower chord of which the best known examples are probably the Nieuport military aircraft — from the Nieuport 10 through to the Nieuport 27, all designed by Gustave Delage during the Great War. The later Waco Custom Cabin series proved to be a popular example in general aviation. Another notable sesquiplane design was the German WWI Junkers J.I, the first all-metal plane to be mass-produced.
Ultralight aircraft.
Although most ultralights are monoplanes, the low speeds and simple construction involved have inspired a small number of biplane ultralights, such as Larry Mauro's "Easy Riser" (1975- ). Mauro also made a version powered with solar cells driving an electric motor - called the Solar Riser. Mauro's "Easy Riser" was used by the man who became known as "Father Goose", Bill Lishman.
Other biplane ultralights include the Belgian-designed Aviasud Mistral, the German FK12 Comet (1997- ), and the Lite Flyer Biplane.
Another biplane microlight is the Murphy Renegade.
History.
The biplane configuration was developed from the box kite, invented by the Australian Lawrence Hargrave. By 1896 Octave Chanute was flying biplane hang gliders and concluded that the externally braced biplane offered better prospects for powered flight than the monoplane. The "Wright Flyer" biplane of 1903 became the first successful powered aeroplane.
Throughout the pioneer years, both biplanes and monoplanes were common, but by the outbreak of the First World War biplanes had gained favour due to a number of monoplane structural failures that resulted in the RFC's "Monoplane Ban" in the when all monoplanes in military service were withdrawn, while the French also withdrew most monoplanes from combat roles and relegated them to training.
During the period from 1914 to 1925 most new aircraft were biplanes although by 1918, the Germans were experimenting with a new generation of monoplanes such as the Junkers D.I and Fokker D.VIII that might have ended the biplane's advantages earlier had the war not ended when it had, and the French already had the Morane-Saulnier AI strut braced parasol monoplane in service. Sesquiplane types, which were biplanes with abbreviated lower wings included the French Nieuport 17 and German Albatros D.III. The sesquiplane format offered slightly lower drag than a conventional biplane while being stronger than a monoplane.
As the available engine-power and speed increased, the drag penalty of external bracing limited aircraft performance. In order to fly faster, it would be necessary to do away with the external bracing to create an aerodynamically clean wing. Early cantilever designs were too weak or too heavy. The 1917 Fokker V.4 prototype (identified by some as the V.3) was a cantilever triplane but suffered excessive flexing of the wings, while the Junkers J 1 of 1915 was overweight and had a poor rate of climb. 
By the 1930s biplanes had reached their performance limits, and monoplanes were predominant, particularly in continental Europe where monoplanes had been common from the end of World War I. At the start of World War II, several air forces still had biplane combat aircraft in front line use but they were clearly noncompetitive, and most were used in specialist roles, such as training or shipboard operation, until shortly after the end of the war. The British Gloster Gladiator and the Italian Fiat CR.42 biplane fighters were both still operational after 1939. The German Heinkel He 50 and the Soviet Polikarpov Po-2 were both used in the night ground attack role until late in the war; the latter was even used by the Korean People's Air Force in the Korean War for night bombing. The British Fleet Air Arm flew Fairey Swordfish torpedo bombers from its aircraft carriers in the anti-submarine warfare role until the end of the war because they could operate from the decks of small escort carriers. The Swordfish was especially well suited to its role, outliving its intended replacement.
Later biplane trainers included the de Havilland Tiger Moth in the RAF and RCAF, Stampe SV.4 in the French and Belgian Air Forces, the Boeing Stearman in the USAAF and the Naval Aircraft Factory N3N with the US Navy. In later civilian use, the Stearman became particularly associated with stunt flying such as wing-walking.
Modern biplane designs still exist in specialist niche roles such as aerobatics and agricultural aircraft with the competition aerobatics role and format for such a biplane well-defined in the mid-1930s by the Udet U 12 Flamingo. The Pitts Special dominated aerobatics for many years after World War II and is still in production, while the WACO Classic YMF is a reproduction of the original Waco design.
The vast majority of biplane designs have been fitted with reciprocating engines of comparatively low power; exceptions include the Antonov An-3 and WSK-Mielec M-15 Belphegor, fitted with turboprop and turbofan engines respectively. Some older biplane designs, such as the Grumman Ag Cat and the aforementioned An-2 (in the form of the An-3) are available in upgraded versions with turboprop engines.
The two most produced biplane designs are the 1913 British Avro 504 (8,970 built by November 1918) and the 1928 Soviet Polikarpov Po-2 (over 20,000 built), and coincidentally with the Po-2 being the direct replacement for the Soviet copies of the Avro 504, named "Avrushka" .
In avian evolution.
It has been suggested the feathered dinosaur "Microraptor" glided, and perhaps even flew, on four wings, which were held in a biplane-like arrangement. This was made possible by the presence of flight feathers on both the forelimbs and hindlimbs of "Microraptor", and it has been suggested the earliest flying ancestors of birds may have possessed this morphology, with the monoplane arrangement of modern birds evolving later.

</doc>
<doc id="49202" url="https://en.wikipedia.org/wiki?curid=49202" title="Ignacy Krasicki">
Ignacy Krasicki

Ignacy Krasicki (3 February 173514 March 1801), from 1766 Prince-Bishop of Warmia (in German, "Ermland") and from 1795 Archbishop of Gniezno (thus, Primate of Poland), was Poland's leading Enlightenment poet ("the Prince of Poets"), a critic of the clergy, Poland's La Fontaine, author of the first Polish novel, playwright, journalist, encyclopedist, and translator from French and Greek.
His most notable literary works were his "Fables and Parables" (1779), "Satires" (1779), and poetic letters and religious lyrics, in which the artistry of his poetic language reached its summit.
Life.
Krasicki was born in Dubiecko, on southern Poland's San River, into a family bearing the title of count of the Holy Roman Empire. He was related to the most illustrious families in the Polish-Lithuanian Commonwealth and spent his childhood surrounded with the love and solicitude of his own family.
He attended a Jesuit school in Lwów, then studied at a Warsaw Catholic seminary (1751–54). In 1759 he took holy orders and continued his education in Rome (1759–61). Two of his brothers also entered the priesthood.
Returning to Poland, Krasicki became secretary to the Primate of Poland and developed a friendship with future King Stanisław August Poniatowski. When Poniatowski was elected king (1764), Krasicki became his chaplain. He participated in the King's famous "Thursday dinners" and co-founded the "Monitor", the preeminent Polish Enlightenment periodical, sponsored by the King.
In 1766 Krasicki, after having served that year as coadjutor to Prince-Bishop of Warmia Adam Stanisław Grabowski, was himself elevated to Prince-Bishop of Warmia and "ex officio" membership in the Senate of the Commonwealth. This office gave him a high standing in the social hierarchy and a sense of independence. It did not, however, prove a quiet haven. The Warmia cathedral chapter welcomed its superior coolly, fearing changes. At the same time, there were growing provocations and pressures from Prussia, preparatory to seizure of Warmia in the First Partition of the Polish-Lithuanian Commonwealth. Krasicki protested publicly against external intervention.
In 1772, as a result of the First Partition, instigated by Prussia's King Frederick II ("the Great"), Krasicki became a Prussian subject. He did not, however, pay homage to Warmia's new master.
He now made frequent visits to Berlin, Potsdam and Sanssouci at the bidding of Frederick, with whom he cultivated an acquaintance. This created a difficult situation for the poet-bishop who, while a friend of the Polish king, maintained close relations with the Prussian king. These realities could not but influence the nature and direction of Krasicki's subsequent literary productions, perhaps nowhere more so than in the "Fables and Parables" (1779).
Soon after the First Partition, Krasicki officiated at the 1773 opening of Berlin's St. Hedwig's Cathedral, which Frederick had built for Catholic immigrants to Brandenburg and Berlin. In 1786 Krasicki was called to the Prussian Academy of Sciences. His residences in the castle of the bishops of Warmia at Lidzbark Warmiński (in German, "Heilsberg") and in the summer palace of the bishops of Warmia at Smolajny became centers of artistic patronage for all sectors of partitioned Poland.
After Frederick the Great's death, Krasicki continued relations with Frederick's successor.
In 1795, six years before his death, Krasicki was elevated to Archbishop of Gniezno (thus, to Primate of Poland).
Krasicki was honored by Poland's King Stanisław August Poniatowski with the Order of the White Eagle and the Order of Saint Stanisław, as well as with a special 1780 medal featuring the Latin device, ""Dignum laude virum Musa vetat mori"" ("The Muse will not let perish a man deserving of glory"); and by Prussia's King Frederick the Great, with the Order of the Red Eagle.
Upon his death in Berlin in 1801, Krasicki was laid to rest at St. Hedwig's Cathedral, which he had consecrated. In 1829 his remains were transferred to Poland's Gniezno Cathedral.
Czesław Miłosz describes Krasicki:
Works.
Ignacy Krasicki was the leading literary representative of the Polish Enlightenment—a prose writer and poet highly esteemed by his contemporaries, who admired his works for their wit, imagination and fluid style.
Krasicki's literary writings lent splendor to the reign of Poland's King Stanisław August Poniatowski, while not directly advocating the King's political program.
Krasicki, the leading representative of Polish classicism, debuted as a poet with the strophe-hymn, ""Święta miłości kochanej ojczyzny"" ("O Sacred Love of the Beloved Country"). He was then nearing forty. It was thus a late debut that brought the extraordinary success of this strophe, which Krasicki would incorporate as part of song IX in his mock-heroic poem, "Myszeida" (Mouseiad, 1775). In "O Sacred Love," Krasicki formulated a universal idea of patriotism, expressed in high style and elevated tone. The strophe would later, for many years, serve as a national anthem and see many translations, including three into French.
The Prince Bishop of Warmia gave excellent Polish form to all the genres of European classicism. He also blazed paths for new genres. Prominent among these was the first modern Polish novel, "Mikołaja Doświadczyńskiego przypadki" (The Adventures of Nicholas Experience, 1776), a synthesis of all the varieties of the Enlightenment novel: the social-satirical, the adventure ("à la" "Robinson Crusoe"), the Utopian and the didactic.
Tradition has it that Krasicki's mock-heroic poem, "Monachomachia" (War of the Monks, 1778), was inspired by a conversation with Frederick II at the palace of Sanssouci, where Krasicki was staying in an apartment that had once been used by Voltaire. At the time, the poem's publication caused a public scandal.
The most enduring literary monument of the Polish Enlightenment is Krasicki's fables: "Bajki i Przypowieści" (Fables and Parables, 1779) and "Bajki nowe" (New Fables, published posthumously in 1802). The poet also set down his trenchant observations of the world and human nature in "Satyry" (Satires, 1779).
Other works by Krasicki include the novels, "Pan Podstoli" (Lord High Steward, published in three parts, 1778, 1784 and posthumously 1803), which would help inspire works by Mickiewicz, and "Historia" (History, 1779); the epic, "Wojna chocimska" (The Chocim War, 1780, about the Khotyn War); and numerous others, in homiletics, theology and heraldry.
Krasicki also published, in 1781, a two-volume encyclopedia, "Zbiór potrzebniejszych wiadomości" (A Collection of Needful Knowledge), the second Polish general encyclopedia after Benedykt Chmielowski's "Nowe Ateny" (The New Athens, 1745–46).
Krasicki wrote "Listy o ogrodach" (Letters about Gardens) and articles in the "Monitor", which he had co-founded, and in his own newspaper, "Co Tydzień" (Each Week).
Krasicki translated, into Polish, Plutarch, "Ossian", fragments of Dante's "Divine Comedy", and works by Anacreon, Boileau, Hesiod and Theocritus. He wrote a 1772 essay "On the Translation of Books" (""O przekładaniu ksiąg"") and another, published posthumously in 1803, "On Translating Books" (""O tłumaczeniu ksiąg"").
Fame.
Krasicki's major works won European fame and were translated into Latin, French, German, Italian, Russian, Czech, Croatian, Slovene, and Hungarian. The broad reception of his works was sustained throughout the 19th century.
Krasicki has been the subject of works by poets of the Polish EnlightenmentStanisław Trembecki, Franciszek Zabłocki, Wojciech Mierand in the 20th century, by Konstanty Ildefons Gałczyński. He has been the hero of prose works by Wincenty Pol, Adolf Nowaczyński and Henryk Sienkiewicz.
Literary reflection.
Scholars have viewed Krasicki's "Fables" and "Satires" as adaptive to the culture for which they were written, and as politically charged. The characterizations were not based on reconstructions of individuals from direct observation, but were fictional constructs that reflected society's actual values. Krasicki held that Poles, and humanity generally, were governed by greed, folly, and vice.
Target audience.
Evidence for this is found in the preface, "To the Children,", targeted not to children but to villagers, congregations, and the commonalty. The fables were meant to bring attention to major questions of the day, and to advocate for social reforms. Although the "New Fables", the sequel to the "Fables and Parables", were published posthumously in 1803, the better known "Fables and Parables" found their audience between 1735 and Krasicki's death in 1801, most of them being published after the Frst Partition of Poland, of 1772. The fables usually find their meaning in the final line, through the symbology of the tale rather than through a complex presentation of ideology, thereby readily conveying even to the illiterate the moral and the Enlightenment ideal behind it.
Enlightenment contributions.
Katraynza Zechenter argues in "The Polish Review" that Western historians have generally overlooked Krasicki's works, and that the publisher of "Polish Fables" overlooked the importance of the "political and social context contributing to fable's origin." However, it is easy to see Krasicki's influence on his contemporaries and on the early 19th century, as in the case of Gabriela Puzynina, a Polish princess, poet, and diarist. In 1846 she started a newspaper for the intelligentsia of Vilnius and Warsaw, and furthered the establishment of Krasicki's "Fables" in Poland's suppressed political life. In her "Diary of the Years 1815-1843", Puzynina focuses on the fable of "Birds in a Cage" as a commentary on the Partitions of Poland.
See also.
Cath. Encycl.,
Univ. Gda, Prof.dr.hab Ir. Kadulska

</doc>
<doc id="49207" url="https://en.wikipedia.org/wiki?curid=49207" title="Laura Gemser">
Laura Gemser

Laurette Marcia "Laura" Gemser (born October 5, 1950, Surabaya, Indonesia) is a Dutch actress of Indo descent, now Italian citizen. She is known for her work with director Joe D'Amato and Bruno Mattei, in particular, for doing a set of exploitation-style and "Black Emanuelle" films.
Gemser has also been credited as Moira Chen, most notably in "Love Is Forever" (1983).
Life and career.
Gemser left Indonesia in 1955, at the age of four, and moved with her parents to the Netherlands. She grew up in the Dutch city of Utrecht, where she attended the MULO Regentesseschool high school. After that, she attended the Artibus Art School in Utrecht, where she specialized in fashion design. After modelling in various magazines in the Netherlands and Belgium, in 1974 she moved to Italy to star in the erotic film "Amore libero - Free Love". The film was a box office success and launched the career of Gemser, who later became internationally recognised after starring in a number of "Black Emanuelle" films in the 1970s.
Her most mainstream and well-received role was as Laotian refugee Keo Sirisomphone in Michael Landon's 1983 American television movie, "Love is Forever", in which she was credited as Moira Chen. Gemser continued to do films, at times working with her actor husband, Gabriele Tinti. In the 1990s, she left the movies to do costume designing for film. In addition, she lost her husband, who died of cancer in 1991. Today she lives in retirement and low profile in Rome.

</doc>
<doc id="49209" url="https://en.wikipedia.org/wiki?curid=49209" title="Carburetor">
Carburetor

A carburetor (American and Canadian spelling), carburator, carburettor, or carburetter (Commonwealth spelling) is a device that blends air and fuel for an internal combustion engine. It is sometimes colloquially shortened to "carb" in North America or "carby" in Australia. To carburate or carburet (and thus carburation or carburetion, respectively) is to blend the air and fuel or to equip (an engine) with a carburetor for that purpose.
Carburetors have largely been supplanted in the automotive industry by fuel injection. They are still common on small engines for lawn mowers, rototillers, and other equipment.
Etymology.
The word "carburetor" comes from the French "carbure" meaning "carbide". "Carburer" means to combine with carbon (compare also carburizing). In fuel chemistry, the term has the more specific meaning of increasing the carbon (and therefore energy) content of a fluid by mixing it with a volatile hydrocarbon.
History and development.
The first carburetor was invented by Samuel Morey in 1826.
A carburetor was invented by an Italian, Luigi De Cristoforis, in 1876. Another carburetor was developed by Enrico Bernardi at the University of Padua in 1882, for his Motrice Pia, the first petrol combustion engine (one cylinder, 121.6 cc) prototyped on 5 August 1882.
A carburetor was among the early patents by Karl Benz as he developed internal combustion engines and their components.
Early carburetors were the surface carburetor type, in which air is charged with fuel by being passed over the surface of gasoline.
In 1885, Wilhelm Maybach and Gottlieb Daimler developed a float carburetor for their engine based on the atomizer nozzle. The Daimler-Maybach carburetor was copied extensively, leading to patent lawsuits, but British courts rejected the Daimler company's claim of priority in favor of Edward Butler's 1884 spray carburetor used on his Petrol Cycle.
Hungarian engineers János Csonka and Donát Bánki patented a carburetor for a stationary engine in 1893.
Frederick William Lanchester of Birmingham, England, experimented with the wick carburetor in cars. In 1896, Frederick and his brother built the first gasoline-driven car in England: a single cylinder internal combustion engine with chain drive. Unhappy with the performance and power, they re-built the engine the next year into a two-cylinder horizontally opposed version using his new wick carburetor design.
Carburetors were the usual method of fuel delivery for most US-made gasoline-fueled engines up until the late 1980s, when fuel injection became the preferred method. This change was dictated more by the requirements of catalytic converters than by any inherent inefficiency of carburation; a catalytic converter requires much more precise control over the fuel / air mixture, to closely control the amount of oxygen in the exhaust gases. In the U.S. market, the last carbureted cars were:
In Australia, some cars continued to use carburetors well into the 1990s; these included the Honda Civic (1993), the Ford Laser (1994), the Mazda 323 and Mitsubishi Magna sedans (1996), the Daihatsu Charade (1997), and the Suzuki Swift (1999). Low-cost commercial vans and 4WDs in Australia continued with carburetors even into the 2000s, the last being the Mitsubishi Express van in 2003. Elsewhere, certain Lada cars used carburetors until 2006. Many motorcycles still use carburetors for simplicity's sake, since a carburetor does not require an electrical system to function. Carburetors are also still found in small engines and in older or specialized automobiles, such as those designed for stock car racing, though NASCAR's 2011 Sprint Cup season was the last one with carbureted engines; electronic fuel injection was used beginning with the 2012 race season in Cup.
In Europe, carburetor-engined cars were being gradually phased out by the end of the 1980s in favor of fuel injection, which was already the established type of engine on more expensive vehicles including luxury and sports models. EEC legislation required all vehicles sold and produced in member countries to have a catalytic converter after December 1992; among the last carburetor-engined models produced in these countries were most of the Ford Fiesta MK2 range (1989) as well as cheaper versions of the Nissan Primera (1990) and Peugeot's 106 and 405 range - the French built 106 went into production just over a year before carburetor engines were outlawed in the EEC.
Principles.
The carburetor works on Bernoulli's principle: the faster air moves, the lower its static pressure, and the higher its dynamic pressure. The throttle (accelerator) linkage does not directly control the flow of liquid fuel. Instead, it actuates carburetor mechanisms which meter the flow of air being pulled into the engine. The speed of this flow, and therefore its pressure, determines the amount of fuel drawn into the airstream.
When carburetors are used in aircraft with piston engines, special designs and features are needed to prevent fuel starvation during inverted flight. Later engines used an early form of fuel injection known as a pressure carburetor.
Most production carbureted engines, as opposed to fuel-injected, have a single carburetor and a matching intake manifold that divides and transports the air fuel mixture to the intake valves, though some engines (like motorcycle engines) use multiple carburetors on split heads. Multiple carburetor engines were also common enhancements for modifying engines in the USA from the 1950s to mid-1960s, as well as during the following decade of high-performance muscle cars, fueling different chambers of the engine's intake manifold.
Older engines used updraft carburetors, where the air enters from below the carburetor and exits through the top. This had the advantage of never flooding the engine, as any liquid fuel droplets would fall out of the carburetor instead of into the intake manifold; it also lent itself to use of an oil bath air cleaner, where a pool of oil below a mesh element below the carburetor is sucked up into the mesh and the air is drawn through the oil-covered mesh; this was an effective system in a time when paper air filters did not exist.
Beginning in the late 1930s, downdraft carburetors were the most popular type for automotive use in the United States. In Europe, the sidedraft carburetors replaced downdraft as free space in the engine bay decreased and the use of the SU-type carburetor (and similar units from other manufacturers) increased. Some small propeller-driven aircraft engines still use the updraft carburetor design.
Outboard motor carburetors are typically sidedraft, because they must be stacked one on top of the other in order to feed the cylinders in a vertically oriented cylinder block.
The main disadvantage of basing a carburetor's operation on Bernoulli's Principle is that, being a fluid dynamic device, the pressure reduction in a Venturi tends to be proportional to the square of the intake air speed. The fuel jets are much smaller and limited mainly by viscosity, so that the fuel flow tends to be proportional to the pressure difference. So jets sized for full power tend to starve the engine at lower speed and part throttle. Most commonly this has been corrected by using multiple jets. In SU and other movable jet carburetors, it was corrected by varying the jet size. For cold starting, a different principle was used in multi-jet carburetors. A flow resisting valve called a choke, similar to the throttle valve, was placed upstream of the main jet to reduce the intake pressure and suck additional fuel out of the jets.
Operation.
Under all engine operating conditions, the carburetor must:
This job would be simple if air and gasoline (petrol) were ideal fluids; in practice, however, their deviations from ideal behavior due to viscosity, fluid drag, inertia, etc. require a great deal of complexity to compensate for exceptionally high or low engine speeds. A carburetor must provide the proper fuel/air mixture across a wide range of ambient temperatures, atmospheric pressures, engine speeds and loads, and centrifugal forces:
In addition, modern carburetors are required to do this while maintaining low rates of exhaust emissions.
To function correctly under all these conditions, most carburetors contain a complex set of mechanisms to support several different operating modes, called "circuits".
Basics.
A carburetor basically consists of an open pipe through which the air passes into the inlet manifold of the engine. The pipe is in the form of a Venturi: it narrows in section and then widens again, causing the airflow to increase in speed in the narrowest part. Below the Venturi is a butterfly valve called the throttle valve — a rotating disc that can be turned end-on to the airflow, so as to hardly restrict the flow at all, or can be rotated so that it (almost) completely blocks the flow of air. This valve controls the flow of air through the carburetor throat and thus the quantity of air/fuel mixture the system will deliver, thereby regulating engine power and speed. The throttle is connected, usually through a cable or a mechanical linkage of rods and joints or rarely by pneumatic link, to the accelerator pedal on a car or the equivalent control on other vehicles or equipment.
Fuel is introduced into the air stream through small holes at the narrowest part of the Venturi and at other places where pressure will be lowered when not running on full throttle. Fuel flow is adjusted by means of precisely calibrated orifices, referred to as "jets", in the fuel path.
Off-idle circuit.
As the throttle is opened up slightly from the fully closed position, the throttle plate uncovers additional fuel delivery holes behind the throttle plate where there is a low pressure area created by the throttle plate/Valve blocking air flow; these allow more fuel to flow as well as compensating for the reduced vacuum that occurs when the throttle is opened, thus smoothing the transition to metering fuel flow through the regular open throttle circuit.
Main open-throttle circuit.
As the throttle is progressively opened, the manifold vacuum is lessened since there is less restriction on the airflow, reducing the flow through the idle and off-idle circuits. This is where the Venturi shape of the carburetor throat comes into play, due to Bernoulli's principle (i.e., as the velocity increases, pressure falls). The Venturi raises the air velocity, and this high speed and thus low pressure sucks fuel into the airstream through a nozzle or nozzles located in the center of the Venturi. Sometimes one or more additional "booster Venturis" are placed coaxially within the primary Venturi to increase the effect.
As the throttle is closed, the airflow through the Venturi drops until the lowered pressure is insufficient to maintain this fuel flow, and the idle circuit takes over again, as described above.
Bernoulli's principle, which is a function of the velocity of the fluid, is a dominant effect for large openings and large flow rates, but since fluid flow at small scales and low speeds (low Reynolds number) is dominated by viscosity, Bernoulli's principle is ineffective at idle or slow running and in the very small carburetors of the smallest model engines. Small model engines have flow restrictions ahead of the jets to reduce the pressure enough to suck the fuel into the air flow. Similarly the idle and slow running jets of large carburetors are placed after the throttle valve where the pressure is reduced partly by viscous drag, rather than by Bernoulli's principle. The most common rich mixture device for starting cold engines was the choke, which works on the same principle.
Power valve.
For open throttle operation a richer mixture will produce more power, prevent pre-ignition detonation, and keep the engine cooler. This is usually addressed with a spring-loaded "power valve", which is held shut by engine vacuum. As the throttle opens up, the vacuum decreases and the spring opens the valve to let more fuel into the main circuit. On two-stroke engines, the operation of the power valve is the reverse of normal — it is normally "on" and at a set rpm it is turned "off". It is activated at high rpm to extend the engine's rev range, capitalizing on a two-stroke's tendency to rev higher momentarily when the mixture is lean.
Alternative to employing a power valve, the carburetor may utilize a "metering rod" or "step-up rod" system to enrich the fuel mixture under high-demand conditions. Such systems were originated by Carter Carburetor in the 1950s for the primary two Venturis of their four barrel carburetors, and step-up rods were widely used on most 1-, 2-, and 4-barrel Carter carburetors through the end of production in the 1980s. The step-up rods are tapered at the bottom end, which extends into the main metering jets. The tops of the rods are connected to a vacuum piston and/or a mechanical linkage which lifts the rods out of the main jets when the throttle is opened (mechanical linkage) and/or when manifold vacuum drops (vacuum piston). When the step-up rod is lowered into the main jet, it restricts the fuel flow. When the step-up rod is raised out of the jet, more fuel can flow through it. In this manner, the amount of fuel delivered is tailored to the transient demands of the engine. Some 4-barrel carburetors use metering rods only on the primary two Venturis, but some use them on both primary and secondary circuits, as in the Rochester Quadrajet.
Accelerator pump.
Liquid gasoline, being denser than air, is slower than air to react to a force applied to it. When the throttle is rapidly opened, airflow through the carburetor increases immediately, faster than the fuel flow rate can increase. This transient oversupply of air causes a lean mixture, which makes the engine misfire (or "stumble")—an effect opposite to that which was demanded by opening the throttle. This is remedied by the use of a small piston or diaphragm pump which, when actuated by the throttle linkage, forces a small amount of gasoline through a jet into the carburetor throat. This extra shot of fuel counteracts the transient lean condition on throttle tip-in. Most accelerator pumps are adjustable for volume and/or duration by some means. Eventually the seals around the moving parts of the pump wear such that pump output is reduced; this reduction of the accelerator pump shot causes stumbling under acceleration until the seals on the pump are renewed.
The accelerator pump is also used to "prime" the engine with fuel prior to a cold start. Excessive priming, like an improperly adjusted choke, can cause "flooding". This is when too much fuel and not enough air are present to support combustion. For this reason, most carburetors are equipped with an "unloader" mechanism: The accelerator is held at wide open throttle while the engine is cranked, the unloader holds the choke open and admits extra air, and eventually the excess fuel is cleared out and the engine starts.
Choke.
When the engine is cold, fuel vaporizes less readily and tends to condense on the walls of the intake manifold, starving the cylinders of fuel and making the engine difficult to start; thus, a "richer mixture" (more fuel to air) is required to start and run the engine until it warms up. A richer mixture is also easier to ignite.
To provide the extra fuel, a "choke" is typically used; this is a device that restricts the flow of air at the entrance to the carburetor, before the Venturi. With this restriction in place, extra vacuum is developed in the carburetor barrel, which pulls extra fuel through the main metering system to supplement the fuel being pulled from the idle and off-idle circuits. This provides the rich mixture required to sustain operation at low engine temperatures.
In addition, the choke can be connected to a cam (the "fast idle cam") or other such device which prevents the throttle plate from closing fully while the choke is in operation. This causes the engine to idle at a higher speed. Fast idle serves as a way to help the engine warm up quickly, and give a more stable idle while cold by increasing airflow throughout the intake system which helps to better atomize the cold fuel.
In many carbureted cars, the choke is controlled by a cable connected to a pull-knob on the dashboard operated by the driver. In some carbureted cars it is automatically controlled by a thermostat employing a bimetallic spring, which is exposed to engine heat, or to an electric heating element. This heat may be transferred to the choke thermostat via simple convection, via engine coolant, or via air heated by the exhaust. More recent designs use the engine heat only indirectly: A sensor detects engine heat and varies electric current to a small heating element, which acts upon the bimetallic spring to control its tension, thereby controlling the choke. A "choke unloader" is a linkage arrangement that forces the choke open against its spring when the vehicle's accelerator is moved to the end of its travel. This provision allows a "flooded" engine to be cleared out so that it will start.
Some carburetors do not have a choke but instead use a mixture enrichment circuit, or "enrichment". Typically used on small engines, notably motorcycles, enrichments work by opening a secondary fuel circuit below the throttle valves. This circuit works exactly like the idle circuit, and when engaged it simply supplies extra fuel when the throttle is closed.
Classic British motorcycles, with side-draft slide throttle carburetors, used another type of "cold start device", called a "tickler". This is simply a spring-loaded rod that, when depressed, manually pushes the float down and allows excess fuel to fill the float bowl and flood the intake tract. If the "tickler" is held down too long it also floods the outside of the carburetor and the crankcase below, and is therefore a fire hazard.
Other elements.
The interactions between each circuit may also be affected by various mechanical or air pressure connections and also by temperature sensitive and electrical components. These are introduced for reasons such as response, fuel efficiency or automobile emissions control. Various air bleeds (often chosen from a precisely calibrated range, similarly to the jets) allow air into various portions of the fuel passages to enhance fuel delivery and vaporization. Extra refinements may be included in the carburetor/manifold combination, such as some form of heating to aid fuel vaporization such as an early fuel evaporator.
Fuel supply.
Float chamber.
To ensure a ready mixture, the carburetor has a "float chamber" (or "bowl") that contains a quantity of fuel at near-atmospheric pressure, ready for use. This reservoir is constantly replenished with fuel supplied by a fuel pump. The correct fuel level in the bowl is maintained by means of a float controlling an inlet valve, in a manner very similar to that employed in a cistern (e.g. a toilet tank). As fuel is used up, the float drops, opening the inlet valve and admitting fuel. As the fuel level rises, the float rises and closes the inlet valve. The level of fuel maintained in the float bowl can usually be adjusted, whether by a setscrew or by something crude such as bending the arm to which the float is connected. This is usually a critical adjustment, and the proper adjustment is indicated by lines inscribed into a window on the float bowl, or a measurement of how far the float hangs below the top of the carburetor when disassembled, or similar. Floats can be made of different materials, such as sheet brass soldered into a hollow shape, or of plastic; hollow floats can spring small leaks and plastic floats can eventually become porous and lose their flotation; in either case the float will fail to float, fuel level will be too high, and the engine will not run unless the float is replaced. The valve itself becomes worn on its sides by its motion in its "seat" and will eventually try to close at an angle, and thus fails to shut off the fuel completely; again, this will cause excessive fuel flow and poor engine operation. Conversely, as the fuel evaporates from the float bowl, it leaves sediment, residue, and varnishes behind, which clog the passages and can interfere with the float operation. This is particularly a problem in automobiles operated for only part of the year and left to stand with full float chambers for months at a time; commercial fuel stabilizer additives are available that reduce this problem.
The fuel stored in the chamber (bowl) can be a problem in hot climates. If the engine is shut off while hot, the temperature of the fuel will increase, sometimes boiling ("percolation"). This can result in flooding and difficult or impossible restarts while the engine is still warm, a phenomenon known as "heat soak". Heat deflectors and insulating gaskets attempt to minimize this effect. The Carter Thermo-Quad carburetor has float chambers manufactured of insulating plastic (phenolic), said to keep the fuel 20 degrees Fahrenheit (11 degrees Celsius) cooler.
Usually, special vent tubes allow atmospheric pressure to be maintained in the float chamber as the fuel level changes; these tubes usually extend into the carburetor throat. Placement of these vent tubes is critical to prevent fuel from sloshing out of them into the carburetor, and sometimes they are modified with longer tubing. Note that this leaves the fuel at atmospheric pressure, and therefore it cannot travel into a throat which has been pressurized by a supercharger mounted upstream; in such cases, the entire carburetor must be contained in an airtight pressurized box to operate. This is not necessary in installations where the carburetor is mounted upstream of the supercharger, which is for this reason the more frequent system. However, this results in the supercharger being filled with compressed fuel/air mixture, with a strong tendency to explode should the engine backfire; this type of explosion is frequently seen in drag races, which for safety reasons now incorporate pressure releasing blow-off plates on the intake manifold, breakaway bolts holding the supercharger to the manifold, and shrapnel-catching ballistic nylon blankets surrounding the superchargers.
Diaphragm chamber.
If the engine must be operated in any orientation (for example a chain saw or a model airplane), a float chamber is not suitable. Instead, a diaphragm chamber is used. A flexible diaphragm forms one side of the fuel chamber and is arranged so that as fuel is drawn out into the engine, the diaphragm is forced inward by ambient air pressure. The diaphragm is connected to the needle valve and as it moves inward it opens the needle valve to admit more fuel, thus replenishing the fuel as it is consumed. As fuel is replenished the diaphragm moves out due to fuel pressure and a small spring, closing the needle valve. A balanced state is reached which creates a steady fuel reservoir level, which remains constant in any orientation.
Multiple carburetor barrels.
While basic carburetors have only one Venturi, many carburetors have more than one Venturi, or "barrel". Two barrel and four barrel configurations are commonly used to accommodate the higher air flow rate with large engine displacement. Multi-barrel carburetors can have non-identical primary and secondary barrel(s) of different sizes and calibrated to deliver different air/fuel mixtures; they can be actuated by the linkage or by engine vacuum in "progressive" fashion, so that the secondary barrels do not begin to open until the primaries are almost completely open. This is a desirable characteristic which maximizes airflow through the primary barrel(s) at most engine speeds, thereby maximizing the pressure "signal" from the Venturis, but reduces the restriction in airflow at high speeds by adding cross-sectional area for greater airflow. These advantages may not be important in high-performance applications where part throttle operation is irrelevant, and the primaries and secondaries may all open at once, for simplicity and reliability; also, V-configuration engines, with two cylinder banks fed by a single carburetor, may be configured with two identical barrels, each supplying one cylinder bank. In the widely seen V8 and 4-barrel carburetor combination, there are often two primary and two secondary barrels.
The first four-barrel carburetors, with two primary bores and two secondary bores, were the Carter WCFB and identical Rochester 4GC simultaneously introduced on the 1952 Cadillacs, Oldsmobiles and Buick Roadmaster. Oldsmobile referred the new carburetor as the “Quadri-Jet” (original spelling) while Buick called it the “Airpower”.
The "spread-bore" four-barrel carburetor, first released by Rochester in the 1965 model year as the "Quadrajet" has a much greater "spread" between the sizes of the primary and secondary throttle bores. The primaries in such a carburetor are quite small relative to conventional four-barrel practice, while the secondaries are quite large. The small primaries aid low-speed fuel economy and driveability, while the large secondaries permit maximum performance when it is called for. To tailor airflow through the secondary Venturis, each of the secondary throats has an air valve at the top. This is configured much like a choke plate, and is lightly spring-loaded into the closed position. The air valve opens progressively in response to engine speed and throttle opening, gradually allowing more air to flow through the secondary side of the carburetor. Typically, the air valve is linked to metering rods which are raised as the air valve opens, thereby adjusting secondary fuel flow.
Multiple carburetors can be mounted on a single engine, often with progressive linkages; two four-barrel carburetors (often referred to as "dual-quads") were frequently seen on high performance American V8s, and multiple two barrel carburetors are often now seen on very high performance engines. Large numbers of small carburetors have also been used (see photo), though this configuration can limit the maximum air flow through the engine due to the lack of a common plenum; with individual intake tracts, not all cylinders are drawing air at once as the engine's crankshaft rotates.
Carburetor adjustment.
The fuel and air mixture is too "rich" when it has an excess of fuel, and too "lean" when there is not enough. The mixture is adjusted by one or more needle valves on an automotive carburetor, or a pilot-operated lever on piston-engined aircraft (since the mixture changes with air density and therefore altitude). Independent of air density the (stoichiometric) air to gasoline ratio is 14.7:1, meaning that for each mass unit of gasoline, 14.7 mass units of air are required. There are different stoichiometric ratios for other types of fuel.
Ways to check carburetor mixture adjustment include: measuring the carbon monoxide, hydrocarbon, and oxygen content of the exhaust using a gas analyzer, or directly viewing the color of the flame in the combustion chamber through a special glass-bodied spark plug sold under the name "Colortune"; the flame color of stoichiometric burning is described as a "Bunsen blue", turning to yellow if the mixture is rich and whitish-blue if too lean. Another method, widely used in aviation, is to measure the exhaust gas temperature, which is close to maximum for an optimally adjusted mixture and drops off steeply when the mixture is either too rich or too lean.
The mixture can also be judged by removing and scrutinizing the spark plugs. Black, dry, sooty plugs indicate a mixture too rich; white or light gray plugs indicate a lean mixture. A proper mixture is indicated by brownish-gray plugs.
On high-performance two-stroke engines, the fuel mixture can also be judged by observing piston wash. Piston wash is the color and amount of carbon buildup on the top (dome) of the piston. Lean engines will have a piston dome covered in black carbon, and rich engines will have a clean piston dome that appears new and free of carbon buildup. This is often the opposite of intuition. Commonly, an ideal mixture will be somewhere in-between the two, with clean dome areas near the transfer ports but some carbon in the center of the dome.
When tuning two-strokes It is important to operate the engine at the rpm and throttle input that it will most often be operated at. This will typically be wide-open or close to wide-open throttle. Lower RPM and idle can operate rich/lean and sway readings, due to the design of carburetors to operate well at high air-speed through the Venturi and sacrifice low air-speed performance.
Where multiple carburetors are used the mechanical linkage of their throttles must be properly synchronized for smooth engine running and consistent fuel/air mixtures to each cylinder.
Feedback carburetors.
In the 1980s, many American-market vehicles used "feedback" carburetors that dynamically adjusted the fuel/air mixture in response to signals from an exhaust gas oxygen sensor to provide a stoichiometric ratio to enable the optimal function of the catalytic converter. Feedback carburetors were mainly used because they were less expensive than fuel injection systems; they worked well enough to meet 1980s emissions requirements and were based on existing carburetor designs. Frequently, feedback carburetors were used in lower trim versions of a car (whereas higher trim versions were equipped with fuel injection). However, their complexity compared to both non-feedback carburetors and to fuel injection made them problematic and difficult to service. Eventually falling hardware prices and tighter emissions standards caused fuel injection to supplant carburetors in new-vehicle production.
Catalytic carburetors.
A catalytic carburetor mixes fuel vapor with water and air in the presence of heated catalysts such as nickel or platinum. This is generally reported as a 1940s-era product that would allow kerosene to power a gasoline engine (requiring lighter hydrocarbons). However reports are inconsistent; commonly they are included in descriptions of "200 MPG carburetors" intended for gasoline use. There seems to be some confusion with some older types of fuel vapor carburetors (see vaporizors below). There is also very rarely any useful reference to real-world devices. Poorly referenced material on the topic should be viewed with suspicion.
Vaporizers.
Internal combustion engines can be configured to run on many kinds of fuel, including gasoline, kerosene, tractor vaporizing oil (TVO), vegetable oil, diesel fuel, biodiesel, ethanol fuel (alcohol), and others. Multifuel engines, such as petrol-paraffin engines, can benefit from an initial vaporization of the fuel when they are running less volatile fuels. For this purpose, a vaporizer (or vaporiser) is placed in the intake system. The vaporizer uses heat from the exhaust manifold to vaporize the fuel. For example, the original Fordson tractor and various subsequent Fordson models had vaporizers. When Henry Ford & Son Inc designed the original Fordson (1916), the vaporizer was used to provide for kerosene operation. When TVO became common in various countries (including the United Kingdom and Australia) in the 1940s and 1950s, the standard vaporizers on Fordson models were equally useful for TVO. Widespread adoption of diesel engines in tractors made the use of tractor vaporizing oil obsolete.

</doc>
<doc id="49214" url="https://en.wikipedia.org/wiki?curid=49214" title="Clef">
Clef

A clef (from French: "clef" "key") is a musical symbol used to indicate the pitch of written notes. Placed on one of the lines at the beginning of the stave, it indicates the name and pitch of the notes on that line. This line serves as a reference point by which the names of the notes on any other line or space of the stave may be determined. Only one clef that references a note in a space rather than on a line has ever been used.
There are three types of clef used in modern music notation: "F", "C", and "G". Each type of clef assigns a different reference note to the line (and in rare cases, the space) on which it is placed. (G and F clefs are placed as treble and bass clefs, respectively, in the vast majority of modern music.)
Once one of these clefs has been placed on one of the lines of the stave, the other lines and spaces can be read in relation to it.
The use of three different clefs makes it possible to write music for all instruments and voices, even though they may have very different tessituras (that is, even though some sound much higher or lower than others). This would be difficult to do with only one clef, since the modern stave has only five lines, and the number of pitches that can be represented on the stave, even with ledger lines, is not nearly equal to the number of notes the orchestra can produce. The use of different clefs for various instruments and voices allows each part to be written comfortably on the stave with a minimum of ledger lines. To this end, the G-clef is used for high parts, the C-clef for middle parts, and the F-clef for low parts—with the notable exception of transposing parts, which are written at a pitch different from their sound, often even in a different octave.
Placement on the stave.
To facilitate writing for different tessituras, any of the clefs may theoretically be placed on any of the lines of the stave. The further down on the stave a clef is placed, the higher the tessitura it is for; conversely, the higher up the clef, the lower the tessitura.
Since there are five lines on the stave, and three clefs, it might seem that there would be fifteen possible clefs. Six of these, however, are redundant clefs (for example, a G-clef on the third line would be the same as a C-clef on the first line). That leaves nine possible "distinct" clefs, all of which have been used historically: the G-clef on the two bottom lines, the F-clef on the three top lines, and the C-clef on any line of the stave except the topmost, earning the name of "movable C-clef". (The C-clef on the topmost line is redundant because it is equivalent to the F-clef on the third line; both options have been used.)
Each of these clefs has a different name based on the tessitura for which it is best suited.
In modern music, only four clefs are used regularly: the treble clef, the bass clef, the alto clef, and the tenor clef. Of these, the treble and bass clefs are by far the most common.
Individual clefs.
Here follows a complete list of the clefs, along with a list of instruments and voice parts notated with them. Each clef is shown in its proper position on the stave, followed by its reference note.
An obelisk (†) after the name of a clef indicates that that clef is no longer in common use.
G-clefs.
Treble clef.
Where the G-clef is placed on the second line of the stave, it is called the treble clef. This is the most common clef used today, and the only G-clef still in use. For this reason, the terms G-clef and treble clef are often seen as synonymous. The treble clef was historically used to mark a treble, or pre-pubescent, voice part.
Among the instruments that use treble clef are the violin, flute, oboe, bagpipe, cor anglais, all clarinets, all saxophones, horn, trumpet, cornet, vibraphone, xylophone, mandolin, recorder; it is also used for euphonium, baritone horn, and guitar (which sound an octave lower). Treble clef is the upper stave of the grand stave used for harp and keyboard instruments. It is also sometimes used, along with tenor clef, for the highest notes played by bass-clef instruments such as the cello, double bass (which sounds an octave lower), bassoon, and trombone. The viola also sometimes uses treble clef for very high notes. Treble clef is used for the soprano, mezzo-soprano, alto, contralto and tenor voices. The tenor voice sounds an octave lower, and is often written using an octave clef (see below) or double-treble clef.
French violin clef†.
Where the G-clef is placed on the first line of the stave, it is called the French clef or French violin clef. It is identical to the bass clef transposed up two octaves.
F-clefs.
Bass clef.
Where the F-clef is placed on the fourth line, it is called the bass clef. This is the only F-clef used today so that the terms "F-clef" and "bass clef" are often regarded as synonymous.
This clef is used for the cello, euphonium, double bass, bass guitar, bassoon, contrabassoon, trombone, baritone horn, tuba, and timpani. It is also used for the lowest notes of the horn, and for the baritone and bass voices. Tenor voice is notated in bass clef where the tenor and bass are written on the same stave. Bass clef is the bottom clef in the grand stave for harp and keyboard instruments. The contrabassoon, double bass, and electric bass sound an octave lower than the written pitch; no notation is usually made of this fact, but some composers/publishers will place an "8" beneath the clef for these instruments on the conductor's full score to differentiate from instruments that naturally sound within the clef (see "Octave clefs" below).
Baritone clef†.
Where the F-clef is placed on the third line, it is called the baritone clef.
This clef was used for the left hand of keyboard music (particularly in France; see Bauyn manuscript) as well as the baritone part in vocal music.
The baritone clef has the less common variant as a C clef placed on the 5th line which is exactly equivalent (see below).
Sub-bass clef†.
Where the F-clef is placed on the fifth line, it is called the sub-bass clef. It is identical to the treble clef transposed down 2 octaves.
This clef was used by Ockeghem and Heinrich Schütz to write low bass parts, making a late appearance in Bach's Musical Offering.
C-clefs.
Alto clef.
Where the C-clef is placed on the third line of the stave, it is called the alto clef.
This clef (sometimes called the viola clef) is currently used for the viola, the viola da gamba, the alto trombone, and the mandola. It is also associated with the countertenor voice and therefore called the counter-tenor (or countertenor) clef, A vestige of this survives in Sergei Prokofiev's use of the clef for the cor anglais, as in his symphonies. It occasionally turns up in keyboard music to the present day (Brahms's Organ chorales, John Cage's "Dream" for piano)."
Tenor clef.
Where the C-clef is placed on the fourth line of the stave, it is called the tenor clef.
This clef is used for the upper ranges of the bassoon, cello, euphonium, double bass, and trombone. These instruments use bass clef for their low- to mid-ranges; treble clef is also used for their upper extremes. Where used for the double bass, the sound is an octave lower than the written pitch. The tenor violin parts were also drafted in this clef (see e.g. Giovanni Battista Vitali's Op. 11). Formerly, it was used by the tenor part in vocal music but its use has been largely supplanted either with an octave version of the treble clef where written alone or the bass clef where combined on one stave with the bass part.
Baritone clef†.
Where the C-clef is placed on the 5th line of the stave, it is called the baritone clef. It is precisely the equivalent to the other more common form of the baritone clef, an F clef placed on the 3rd line (see above).
Mezzo-soprano clef†.
Where the C-clef is placed on the second line of the stave, it is called the mezzo-soprano clef.
Soprano clef†.
Where the C-clef is placed on the first line of the stave, it is called the soprano clef.
This clef was used for the right hand of keyboard music (particularly in France; see Bauyn manuscript) as well as in vocal music for sopranos.
Other clefs.
Octave clefs.
Starting in the 18th-century treble clef has been used for transposing instruments that sound an octave lower, such as the guitar; it has also been used for the tenor voice. To avoid ambiguity, modified clefs are sometimes used, especially in the context of choral writing; of those shown, the C clef on the third space, easily confused with the tenor clef, is the rarest.
This is most often found in tenor parts in SATB settings, in which a treble clef is written with the numeral eight below it, indicating that the pitches sound an octave below the written value. As the true tenor clef has fallen into disuse in vocal writings, this "octave-dropped" treble clef is often called the tenor clef. The same clef is sometimes used for the octave mandolin. In some scores, the same concept is construed by using a double clef—two G-clefs overlapping one another.
Tenor banjo is commonly notated in treble clef. However, notation varies between the written pitch sounding an octave lower (as in guitar music and called octave pitch in most tenor banjo methods) and music sounding at the written pitch (called actual pitch). An attempt has been made to use a treble clef with a diagonal line through the upper half of the clef to indicate octave pitch, but this is not always used.
At the other end of the spectrum, treble clefs with an "8" positioned above the clef may be used in piccolo, penny whistle, soprano recorder, and other high woodwind parts.
The F clef can also be notated with an octave marker. The F clef notated to sound an octave lower is used for contrabass instruments such as the double bass and contrabassoon. The F clef notated to sound an octave higher is used for the bass recorder. However, both of these are extremely rare (and, in fact, the countertenor clef is largely intended to be humorous as with the works of P.D.Q. Bach). In Italian scores up to Gioachino Rossini's Overture to "William Tell", the cor anglais was written in bass clef an octave lower than sounding. The unmodified bass clef is so common that performers of instruments and voice parts whose ranges lie below the stave simply learn the number of ledger lines for each note through common use, and if a line's true notes lie significantly above the bass clef the composer or publisher will often simply write the part in either the true treble clef or notated an octave down.
Use of octave-marked clefs appears to have increased as computers have become more important in musical transcription. Performers will normally know the right octave to use with or without the octave marking. However, the appropriate use of octave marking ensures that music files (such as MIDI files) generate tones in their proper octaves.
Neutral clef.
The "neutral" or "percussion" clef is not a clef in the same sense that the F, C, and G clefs are. It is simply a convention that indicates that the lines and spaces of the stave are each assigned to a percussion instrument with no precise pitch. With the exception of some common drum-kit and marching percussion layouts, the keying of lines and spaces to instruments is not standardised, so a legend or indications above the stave are necessary to indicate what is to be played. Percussion instruments with identifiable pitches do not use the neutral clef, and timpani (notated in bass clef) and mallet percussion (noted in treble clef or on a grand stave) are usually notated on different staves than unpitched percussion.
Staves with a neutral clef do not always have five lines. Commonly, percussion staves only have one line, although other configurations can be used.
The neutral clef is sometimes used where non-percussion instruments play non-pitched extended techniques, such as hitting the body of a violin, violoncello or acoustic guitar, or where a vocal choir is instructed to clap, stomp, or snap, but more often the rhythms are written with X marks in the instrument's normal stave with a comment placed above as to the appropriate rhythmic action.
Tablature.
For guitars and other fretted instruments, it is possible to notate tablature in place of ordinary notes. In this case, a TAB-sign is often written instead of a clef. The number of lines of the stave is not necessarily five: one line is used for each string of the instrument (so, for standard six-stringed guitars, six lines would be used, four lines for the traditional bass guitar). Numbers on the lines show on which fret the string should be played. This Tab-sign, like the Percussion clef, is not a clef in the true sense, but rather a symbol employed instead of a clef.
History.
Originally, instead of a special clef symbol, the reference line of the stave was simply labeled with the name of the note it was intended to bear: F and C and, more rarely, "G". These were the most often-used 'clefs', or "litteræ-clavis" (key-letters), in Gregorian chant notation. Over time the shapes of these letters became stylised, leading to their current versions.
Many other clefs were used, particularly in the early period of chant notation, including most of the notes from the low Γ ("gamma", the note written today on the bottom line of the bass clef) up to the G above middle C, written with a small letter "g", and including two forms of lowercase "b" (for the note just below middle C): round for B, and square for B. In order of frequency of use, these clefs were: "F", "c", "f", "C", "D", "a", "g", "e", Γ, "B", and the round/square "b".
In the polyphonic period up to 1600, unusual clefs were occasionally used for parts with extremely high or low written tessituras. For very low bass parts, the Γ clef is found on the middle, fourth, or fifth lines of the stave (e.g., in Pierre de La Rue’s Requiem and in a mid-16th-century dance book published by the Hessen brothers); for very high parts, the high-D clef ("d"), and the even higher "ff" clef (e.g., in the "Mulliner Book") were used to represent the notes written on the fourth and top lines of the treble clef, respectively.
Varying shapes of different clefs persisted until very recent times. The F-clef was, until as late as the 1980s in some cases (such as hymnals), written like this:
In printed music from the 16th and 17th centuries, the C clef often assumed a ladder-like form, in which the two horizontal rungs surround the stave line indicated as C:
The C-clef was formerly written in a more angular way, sometimes still used, or an, even more, simplified "K"-shape, when writing the clef by hand. 
In modern Gregorian chant notation, the C clef is written (on a four-line stave) in the form and the F clef as .
The flourish at the top of the G-clef probably derives from a cursive "S" for "sol", the name for "G" in solfege.
C clefs (along with G, F, Gamma, D, and A clefs) were formerly used to notate vocal music. Nominally, the soprano voice parts were written in first- or second-line C clef ("soprano clef" or "mezzo-soprano clef") or second-line G clef ("treble clef"), the alto or tenor voices in third-line C clef ("alto clef"), the tenor voice in fourth-line C clef ("tenor clef") and the bass voice in third- fourth- or fifth-line F clef ("baritone", "bass", or "sub-bass clef"). However, in practice transposition was applied to fit the range of the music to the available voices, so that almost any clef might be used by all voice types.
In more modern publications, four-part harmony on parallel staves is usually written more simply as:
This may be reduced to two staves, the soprano/alto stave with a treble clef, and tenor/bass stave marked with the bass clef.
Further uses.
Clef combinations played a role in the modal system toward the end of the 16th century, and it has been suggested certain clef combinations in the polyphonic music of 16th-century vocal polyphony are reserved for authentic (odd-numbered) modes, and others for plagal (even-numbered) modes, but the precise implications have been the subject of much scholarly debate.
Music can be transposed at sight if a different clef is mentally substituted for the written one. For example, to play an A-clarinet part, a B-clarinet player may mentally substitute tenor clef for the written treble clef. Concert-pitch music in bass clef can be read on an E instrument as if it were in treble clef. (Notes will not always sound in the correct octave.) The written key signature must always be adjusted to the proper key for the instrument being played.

</doc>
<doc id="49220" url="https://en.wikipedia.org/wiki?curid=49220" title="C (musical note)">
C (musical note)

In terms of musical pitch, C or Do is the first note of the fixed-Do solfège scale. Its enharmonic is B (B-sharp, His in some European countries), which is by definition a diatonic semitone below C.
Middle C.
When the A440 pitch standard is used to tune a musical instrument, Middle C has a frequency around 261.6 Hz. Middle C is designated C4 in scientific pitch notation because of the note's position as the fourth C key on a standard 88-key piano keyboard. 
Another system known as scientific pitch assigns a frequency of 256 Hz but, while numerically convenient, this is not used by orchestras. Other note-octave systems, including those used by some makers of digital music keyboards, may refer to Middle C differently. In MIDI, Middle C is note number 60.
The C4 designation is the most commonly recognized in auditory science, and in musical studies it is often used in place of the Helmholtz designation c'. 
While the expression "Middle C" is generally clear across instruments and clefs, some musicians naturally use the term to refer to the C note in the middle of their specific instrument's range. C4 may be called "Low C" by someone playing a Western concert flute, which has a higher and narrower playing range than the piano, while C5 (523.251 Hz) would be Middle C. This technically inaccurate practice has led some pedagogues to encourage standardizing on C4 as the definitive Middle C in instructional materials across all instruments.
In vocal music, the term Soprano C, sometimes called "High C" or "Top C," is the C two octaves above Middle C. It is so named because it is considered the defining note of the soprano voice type. It is C6 in scientific pitch notation (1046.502 Hz) and c''' in Helmholtz notation. The term Tenor C is sometimes used in vocal music to refer to C5, as it is the highest required note in the standard tenor repertoire. The term Low C is sometimes used in vocal music to refer to C2 because this is considered the divide between true basses and bass-baritones: a "basso" can sing this note easily while other male voices, including bass-baritones, cannot.
In organ music, the term Tenor C can refer to an organ builder's term for small C or C3 (130.813 Hz), the note one octave below Middle C. In stoplists it usually means that a rank is not full compass, omitting the bottom octave. 
For the frequency of each note on a standard piano, see piano key frequencies.
B sharp.
Twelve just perfect fifths (B) and seven octaves do not align as in equal temperament.
This difference, 23.46 cents (531441/524288), is known as the Pythagorean comma.

</doc>
<doc id="49229" url="https://en.wikipedia.org/wiki?curid=49229" title="Staff (music)">
Staff (music)

In Western musical notation, the staff, or stave, is a set of five horizontal lines and four spaces that each represent a different musical pitch—or, in the case of a percussion staff, different percussion instruments. Appropriate music symbols, depending on the intended effect, are placed on the staff according to their corresponding pitch or function. Musical notes are placed by pitch, percussion notes are placed by instrument, and rests and other symbols are placed by convention.
The absolute pitch of each line of a non-percussive staff is indicated by the placement of a clef symbol at the appropriate vertical position on the left-hand side of the staff (possibly modified by conventions for specific instruments). For example, the treble clef, also known as the G clef, is placed on the second line (counting upwards), fixing that line as the pitch first G above 'middle C'.
The lines and spaces are numbered from bottom to top; the bottom line is the "first line" and the top line is the "fifth line".
The musical staff is analogous to a mathematical graph of pitch with respect to time. Pitches of notes are given by their vertical position on the staff and notes are played from left to right. Unlike a graph, however, the number of semitones represented by a vertical step from a line to an adjacent space depends on the key, and the exact timing of the beginning of each note is not directly proportional to its horizontal position; rather, exact timing is encoded by the musical symbol chosen for each note in addition to the tempo.
A time signature to the right of the clef indicates the relationship between timing counts and note symbols, while bar lines group notes on the staff into measures.
Staff positions.
The vertical position of the notehead on the staff indicates which note to play: higher-pitched notes are marked higher on the staff. The notehead can be placed with the center of its notehead intersecting a line ("on a line"), or in between the lines touching the lines above and below ("in a space"). Notes outside the range of the staff are placed on or between ledger lines—lines the width of the note they need to hold—added above or below the staff.
Exactly which staff positions represent which notes is determined by a clef placed at the beginning of the staff. The clef identifies a particular line as a specific note, and all other notes are determined relative to that line. For example, the treble clef puts the G above middle C on the second line. The interval between adjacent staff positions is one step in the diatonic scale. Once fixed by a clef, the notes represented by the positions on the staff can be modified by the key signature, or by accidentals on individual notes. A clefless staff may be used to represent a set of percussion sounds; each line typically represents a different instrument.
Ensemble staves.
A single vertical line drawn to the left of multiple staves creates a system, indicating that the music on all the staves is to be played simultaneously. A bracket is an additional vertical line joining staves, to show groupings of instruments that function as a unit, such as the string section of an orchestra. A brace is used to join multiple staves that represent a single instrument, such as a piano, organ, harp, or marimba. Sometimes, a second bracket is used to show instruments grouped in pairs, such as the first and second oboes, or the first and second violins in an orchestra. In some cases, a brace is used for this purpose instead of a bracket.
When more than one system appears on a single page, often two parallel diagonal strokes are placed on the left side of the score to separate them.
Four-part SATB vocal settings, especially in hymnals, use a "divisi" notation on a two-staff system with soprano and alto voices sharing the upper staff, and tenor and bass voices on the lower staff.
Confusingly, the German "System" (often in the combined forms "Liniensystem" or "Notensystem") and Italian "sistema" refer to a single staff; "Akkolade" (German) and "accollatura" (Italian) designate systems in the English sense.
Grand staff.
When music on two staves is joined by a brace, or is intended to be played at once by a single performer (usually a keyboard instrument or the harp), a great stave (British English) or grand staff (American English) is created.
Typically, the upper staff uses a treble clef and the lower staff has a bass clef. In this instance, middle C is centered between the two staves, and it can be written on the first ledger line below the upper staff or the first ledger line above the lower staff. Very rarely, a centered line with a small alto clef is written, and usually used to indicate that B, C, or D notes on the line can be played with either hand (ledger lines are not used from a center alto as this creates confusion).
When playing the piano or harp, the upper staff is normally played with the right hand and the lower staff with the left hand. In music intended for the organ, a grand staff comprises three staves, one for each hand on the manuals and one for the feet on the pedalboard.
History.
Early Western medieval notation was written with neumes, which did not specify exact pitches but only the shape of the melodies, i.e. indicating when the musical line went up or down; presumably these were intended as mnemonics for melodies which had been taught by rote.
During the 9th through 11th centuries a number of systems were developed to specify pitch more precisely, including diastematic neumes whose height on the page corresponded with their absolute pitch level (Longobardian and Beneventan manuscripts from Italy show this technique around AD 1000). Digraphic notation, using letter names similar to modern note names in conjunction with the neumes, made a brief appearance in a few manuscripts, but a number of manuscripts used one or more horizontal lines to indicate particular pitches.
The treatise Musica enchiriadis (AD 900) uses Daseian notation for indicating specific pitches, but the modern use of staff lines is attributed to Guido d'Arezzo (AD 990-1050), whose four-line staff is still used (though without the red and yellow coloring he recommended) in Gregorian chant publications today. Five-line staves appeared in Italy in the 13th century, and staves with four, five, and six lines were used as late as 1600.

</doc>
<doc id="49233" url="https://en.wikipedia.org/wiki?curid=49233" title="Robert Goddard (disambiguation)">
Robert Goddard (disambiguation)

Robert H. Goddard (1882–1945) was an American scientist and pioneer of modern rocketry.
Robert Goddard may also refer to:

</doc>
<doc id="49234" url="https://en.wikipedia.org/wiki?curid=49234" title="Chromatic scale">
Chromatic scale

The chromatic scale is a musical scale with twelve pitches, each a semitone above or below another. On a modern piano or other equal-tempered instrument, all the semitones have the same size (100 cents). In other words, the notes of an equal-tempered chromatic scale are equally spaced. An equal-tempered chromatic scale is a nondiatonic scale having no tonic because of the symmetry of its equally spaced notes.
The most common conception of the chromatic scale before the 13th century was the Pythagorean chromatic scale. Due to a different tuning technique, the twelve semitones in this scale have two slightly different sizes. Thus, the scale is not perfectly symmetric. Many other tuning systems, developed in the ensuing centuries, share a similar asymmetry. Equally spaced pitches are provided only by equal temperament tuning systems, which are widely used in contemporary music.
The term "chromatic" derives from the Greek word "chroma", meaning "color". Chromatic notes are traditionally understood as harmonically inessential embellishments, shadings, or inflections of "diatonic" notes.
Notation.
The chromatic scale may be notated in a variety of ways.
Ascending and descending:
The chromatic scale has no set spelling agreed upon by all. Its spelling is, however, often dependent upon major or minor key signatures and whether the scale is ascending or descending. The images above, therefore, are only examples of chromatic scale notations. As an abstract theoretical entity (that is, outside a particular musical context), the chromatic scale is usually notated such that no scale degree is used more than twice in succession (for instance G flat - G natural - G sharp).
Total chromatic.
The "total chromatic" (or "aggregate") is the set of all twelve pitch classes. An "array" is a succession of aggregates. See also: Tone row.
Tuning.
In 5-limit just intonation the chromatic scale is tuned as follows, with flats higher than their enharmonic sharps:
In Pythagorean tuning (3-limit just intonation) the chromatic scale is tuned as follows, with sharps higher than their enharmonic flats:

</doc>
<doc id="49239" url="https://en.wikipedia.org/wiki?curid=49239" title="St. Hedwig's Cathedral">
St. Hedwig's Cathedral

St. Hedwig's Cathedral () is a Roman Catholic cathedral on the Bebelplatz in Berlin, Germany. It is the seat of the archbishop of Berlin.
History and architecture.
It was built in the 18th century as the first Catholic church in Prussia after the Protestant Reformation by permission of King Frederick II. The intention of Frederick was to offer the numerous Catholic immigrants who had arrived in Berlin, especially those from Upper Silesia, a place of worship. The church was therefore dedicated to the patron of Silesia and Brandenburg, Saint Hedwig of Andechs. The building was designed by Georg Wenzeslaus von Knobelsdorff modeled after the Pantheon in Rome and construction started in 1747, interrupted and delayed several times by economic problems. It was not opened until November 1, 1773, when the king's friend, Ignacy Krasicki, the Bishop of Warmia (later Archbishop of Gniezno), officiated at the cathedral's consecration.
After the Kristallnacht pogroms that took place over the night of 9–10 November 1938, Bernhard Lichtenberg, a canon of the cathedral chapter of St Hedwig since 1931, prayed publicly for Jews in the evening prayer following. Lichtenberg was later jailed by the Nazis and died on the way to the concentration camp at Dachau. In 1965 Lichtenberg's remains were transferred to the crypt at St. Hedwig's.
The cathedral burned down completely in 1943 during air raids on Berlin but was reconstructed from 1952 to 1963.

</doc>
<doc id="49240" url="https://en.wikipedia.org/wiki?curid=49240" title="Patrice Lumumba">
Patrice Lumumba

Patrice Émery Lumumba (2 July 1925 – 17 January 1961) was a Congolese independence leader and the first democratically elected leader of the Congo as prime minister. As founder and leader of the mainstream "Mouvement National Congolais" (MNC) party, Lumumba played an important role in campaigning for independence from Belgium.
Shortly after Congolese independence in 1960, a mutiny broke out in the army, marking the begin of the Congo Crisis. Lumumba attempted to solicit support from the Soviet Union against Katangan secessionists. This led to growing differences with President Joseph Kasa-Vubu and chief-of-staff Joseph-Désiré Mobutu as well as to foreign opposition from the United States and Belgium. Lumumba was subsequently imprisoned by state authorities under Mobutu and executed by firing squad under the command of Katangan authorities. The United Nations, which he had asked to come to the Congo, did not intervene to save him. Belgium, the United Kingdom, and the United States have all been accused of involvement in Lumumba's death, the latter as part of Cold War rivalry with the Soviet Union, a country the Americans were determined should not gain access to Congo's uranium riches used to make nuclear bombs.
Early life and career.
Lumumba was born to a farmer, François Tolenga Otetshima, and his wife, Julienne Wamato Lomendja, in Onalua in the Katakokombe region of the Kasai province of the Belgian Congo. He was a member of the Tetela ethnic group and was born with the name Élias Okit'Asombo. His original surname means "heir of the cursed" and is derived from the Tetela words "okitá/okitɔ́" ('heir, successor') and "asombó" ('cursed or bewitched people who will die quickly'). He had three brothers (Ian Clark, Émile Kalema, and Louis Onema Pene Lumumba) and one half-brother (Tolenga Jean). Raised in a Catholic family, he was educated at a Protestant primary school, a Catholic missionary school, and finally the government post office training school, passing the one-year course with distinction. Lumumba spoke Tetela, French, Lingala, Swahili, and Tshiluba.
He worked in Léopoldville (now Kinshasa) and Stanleyville (now Kisangani) as a postal clerk and as a travelling beer salesman. In 1951, he married Pauline Opangu. In 1955, Lumumba became regional head of the "Cercles" of Stanleyville and joined the Liberal Party of Belgium, where he worked on editing and distributing party literature. After traveling on a three-week study tour in Belgium, he was arrested in 1955 on charges of embezzlement. His two-year sentence was commuted to twelve months after it was confirmed by Belgian lawyer Jules Chrome that Lumumba had returned the funds, and he was released in July 1956.
Leader of MNC.
After his release, he helped found the broad-based MNC in 1958, later becoming the organization's president. Lumumba and his team represented the MNC at the All-African Peoples' Conference in Accra, Ghana, in December 1958. At this international conference, hosted by Pan-African President Kwame Nkrumah of Ghana, Lumumba further solidified his Pan-Africanist beliefs.
In late October 1959, Lumumba, as leader of the organization, was arrested for inciting an anti-colonial riot in Stanleyville where thirty people were killed; he was sentenced to 69 months in prison. The trial's start date of 18 January 1960 was also the first day of the Congolese Round Table Conference in Brussels to finalize the future of the Congo.
Despite Lumumba's imprisonment at the time, the MNC won a convincing majority in the December local elections in the Congo. As a result of strong pressure from delegates upset with Lumumba's trial, he was released and allowed to attend the Brussels conference.
Independence, and election as Prime Minister.
The conference culminated on 27 January with a declaration of Congolese independence, setting 30 June 1960, as the independence date with national elections from 11–25 May 1960. Lumumba and the MNC won this election and the right to form a government, with the announcement on 23 June 1960 of 34-year-old Lumumba as Congo's first prime minister and Joseph Kasa-Vubu as its president. In accordance with the constitution, on 24 June the new government passed a vote of confidence and was ratified by the Congolese Chamber and Senate.
Independence Day was celebrated on 30 June in a ceremony attended by many dignitaries, including the king of Belgium, Baudouin and the foreign press. Baudouin's speech praised developments under colonialism, his reference to the "genius" of his great-granduncle Leopold II of Belgium, glossing over atrocities with a death toll estimated at 10 million committed during the Congo Free State. The King continued, "Don't compromise the future with hasty reforms, and don't replace the structures that Belgium hands over to you until you are sure you can do better... Don't be afraid to come to us. We will remain by your side, give you advice." While President Kasa-Vubu thanked the King, Lumumba, who was not scheduled to speak, delivered an impromptu speech which reminded the audience that the independence of the Congo was not granted magnanimously by Belgium:
The King spoke of his father's great work in the country and asked its new leaders to measure up to their example. The speech of President Kasa-Vubu assured the King that they would try hard. Lumumba spoke of the suffering of the Congolese under Belgian colonialism, of "injustice, oppression and exploitation". Neither the audience nor the King and his entourage were accustomed to hearing of the negatives that lay behind the pageantry and paternalism; it stirred the crowd while simultaneously humiliating and alienating the King. Lumumba was later harshly criticised for what many in the Western world—but virtually none in Africa—described as the inappropriate nature of his speech. Some media claimed at the time that he ended his speech by ad-libbing, "Nous ne sommes plus vos macaques!" (We are no longer your monkeys!)—referring to a common slur used against Africans by Belgians, however, these words are neither in his written text nor in radio tapes of his speech.
Actions as Prime Minister.
A few days after Congo gained its independence, Lumumba made the fateful decision to raise the pay of all government employees except for the army. Many units of the army also had strong objections toward the uniformly Belgian officers; General Émile Janssens, the army head, told them their lot would not change after independence, and they rebelled in protest. The rebellions quickly spread throughout the country, leading to a general breakdown in law and order. Although the trouble was highly localized, the country seemed to be overrun by gangs of soldiers and looters, causing a media sensation, particularly over Europeans fleeing the country.
The province of Katanga declared independence under regional premier Moïse Tshombe on 11 July 1960 with support from the Belgian government and mining companies such as Union Minière. Despite the arrival of UN troops, unrest continued. Since the United Nations refused to help suppress the rebellion in Katanga, Lumumba sought Soviet aid in the form of arms, food, medical supplies, trucks, and planes to help move troops to Katanga. Lumumba's decisive actions to turn to the Soviet Union alarmed western countries such as the USA, who were still in the midst of the Cold War. The USA saw Patrice Lumumba as a communist, and there is speculation that the CIA assassinated him as a result.
Deposition.
In September, the President dismissed Lumumba from government. Lumumba immediately protested the legality of the President's actions. In retaliation, Lumumba declared Kasa-Vubu deposed and won a vote of confidence in the Senate, while the newly appointed prime minister failed to gain parliament's confidence. The country was torn by two political groups claiming legal power over the country.
On 14 September, a coup d'état organised by Colonel Joseph Mobutu incapacitated both Lumumba and Kasa-Vubu. Lumumba was placed under house arrest at the Prime Minister's residence, with UN troops positioned around the house. Nevertheless, Lumumba decided to rouse his supporters in Haut-Congo. Smuggled out of his residence at night, he escaped to Stanleyville, where his intention apparently was to set up his own government and army.
Pursued by troops loyal to Mobutu he was finally captured in Port Francqui on 1 December 1960 and flown to Léopoldville (now Kinshasa) in ropes, not handcuffs. Mobutu claimed Lumumba would be tried for inciting the army to rebellion and other crimes.
UN response.
Secretary-General of the United Nations Dag Hammarskjöld made an appeal to Kasa-Vubu asking that Lumumba be treated according to due process. The Soviet Union denounced Hammarskjöld and the First World as responsible for Lumumba's arrest and demanded his release.
The United Nations Security Council was called into session on 7 December 1960 to consider Soviet demands that the UN seek Lumumba's immediate release, the immediate restoration of Lumumba as head of the Congo government, the disarming of the forces of Mobutu, and the immediate evacuation of Belgians from the Congo. Hammarskjöld, answering Soviet criticism of his Congo operations, said that if the UN forces were withdrawn from the Congo "I fear everything will crumble."
The threat to the UN cause was intensified by the announcement of the withdrawal of their contingents by Yugoslavia, the United Arab Republic, Ceylon, Indonesia, Morocco, and Guinea. The pro-Lumumba resolution was defeated on 14 December 1960 by a vote of 8–2. On the same day, a Western resolution that would have given Hammarskjöld increased powers to deal with the Congo situation was vetoed by the Soviet Union.
Final days and execution.
Lumumba was sent first on 3 December, to Thysville military barracks Camp Hardy, 150 km (about 100 miles) from Léopoldville. However, when security and disciplinary breaches threatened his safety, it was decided that he should be transferred to the State of Katanga, which had recently declared independence from Congo.
Lumumba was forcibly restrained on the flight to Elizabethville (now Lubumbashi) on 17 January 1961. On arrival, he was conducted under arrest to Brouwez House where he was brutally beaten and tortured by Katangan and Belgian officers, while President Tshombe and his cabinet decided what to do with him.
Later that night, Lumumba was driven to an isolated spot where three firing squads had been assembled. The Belgian Commission (see below) has found that the execution was carried out by Katanga's authorities. However, declassified documents revealed that the CIA had plotted to assassinate Lumumba, and may have carried out those actions with the help of the Katanga authorities.
It reported that President Tshombe and two other ministers were present with four Belgian officers under the command of Katangan authorities. Lumumba and two ministers from his newly formed independent government (who had also been tortured), Maurice Mpolo and Joseph Okito, were lined up against a tree and shot one at a time. The execution is thought to have taken place on 17 January 1961, between 21:40 and 21:43 (according to the Belgian report.) The Belgians and their counterparts wished to get rid of the bodies, and did so by digging up and dismembering the bodies, then having them dissolved in sulphuric acid while the bones were ground and scattered.
Announcement of death.
No statement was released until three weeks later despite rumours that Lumumba was dead. His death was formally announced on Katangan radio, when it was alleged that he escaped and was killed by enraged villagers.
After the announcement of Lumumba's death, street protests were organized in several European countries; in Belgrade, capital of Yugoslavia, protesters sacked the Belgian embassy and confronted the police, and in London a crowd marched from Trafalgar Square to the Belgian embassy, where a letter of protest was delivered and where protesters clashed with police. A demonstration at the United Nations Security Council turned violent and spilled over into the streets of New York City.
Foreign involvement in his death.
According to "Democracy Now!", "Lumumba's pan-Africanism and his vision of a united Congo gained him many enemies. Both Belgium and the United States actively sought to have him killed. The CIA ordered his assassination but could not complete the job. Instead, the United States and Belgium covertly funneled cash and aid to rival politicians who seized power and arrested Lumumba."
Both Belgium and the US were clearly influenced in their unfavourable stance towards Lumumba by the Cold War. He seemed to gravitate towards the Soviet Union, although according to Sean Kelly, who covered the events as a correspondent for the Voice of America, this was not because he was a communist, but the USSR was the only place he could find support for his country's effort to rid itself of colonial rule, although this opinion is strongly denied by US government officials. The US was the first country from which Lumumba requested help. Lumumba, for his part, not only denied being a communist, but said he found colonialism and communism to be equally deplorable, and professed his personal preference for neutrality between the East and West.
Belgian involvement.
According to David Akerman, Ludo De Witte and Kris Hollington, the firing squads were commanded by a Belgian, Captain Julien Gat; another Belgian, Police Commissioner Verscheure, had overall command of the execution site. De Witte found written orders from the Belgian government requesting Lumumba's execution and documents on various arrangements, such as death squads.
On 18 January, panicked by reports that the burial of the three bodies had been observed, members of the execution team went to dig up the bodies and move them to a place near the border with Northern Rhodesia for reburial. Belgian Police Commissioner Gerard Soete later admitted in several accounts that he and his brother led the exhumation (and also a second exhumation, below.) Police Commissioner Frans Verscheure also took part. According to Adam Hochschild, author of a book on the Congo rubber terror, Lumumba's body was later cut up and dissolved in acid by two Belgian agents. On the afternoon and evening of 21 January, Commissioner Soete and his brother dug up Lumumba's corpse for the second time, cut it up with a hacksaw, and dissolved it in concentrated sulfuric acid. In an interview on Belgian television in a program on the assassination of Lumumba in 1999, Soete displayed a bullet and two teeth that he boasted he had saved from Lumumba's body.
The Belgian Commission investigating Lumumba's assassination concluded that: (1) Belgium wanted Lumumba arrested, (2) Belgium was not particularly concerned with Lumumba's physical well being, and (3) although informed of the danger to Lumumba's life, Belgium did not take any action to avert his death, but the report also specifically denied that Belgium ordered Lumumba's assassination.
It has been argued that Belgium was legally culpable for failing to prevent the assassination from taking place due to its own 'Good Samaritan' laws. Belgium has also been accused of breaching its obligation (under U.N. Resolution 290 of 1949) to refrain from acts or threats "aimed at impairing the freedom, independence or integrity of another state." In February 2002, the Belgian government apologized to the Congolese people, and admitted to a "moral responsibility" and "an irrefutable portion of responsibility in the events that led to the death of Lumumba".
United States involvement.
The report of 2001 by the Belgian Commission mentions that there had been previous U.S. and Belgian plots to kill Lumumba. Among them was a Central Intelligence Agency-sponsored attempt to poison him, which may have come on orders from U.S. President Dwight D. Eisenhower. CIA chemist Sidney Gottlieb was a key person in this by devising a poison resembling toothpaste. In September 1960, Gottlieb brought a vial of the poison to the Congo with plans to place it on Lumumba's toothbrush. However, the plot was later abandoned; the plan is said to have been scrapped because the local CIA Station Chief, Larry Devlin, refused permission.
However, as Kalb points out in her book, "Congo Cables", the record shows that many communications by Devlin at the time urged elimination of Lumumba. Also, the CIA station chief helped to direct the search to capture Lumumba for his transfer to his enemies in Katanga; was involved in arranging his transfer to Katanga; and the CIA base chief in Elizabethville was in direct touch with the killers the night Lumumba was killed. Furthermore, John Stockwell indicates that a CIA agent had the body in the trunk of his car in order to try to get rid of it. Stockwell, who knew Devlin well, felt Devlin knew more than anyone else about the murder.
The inauguration of U.S. President John F. Kennedy in January 1961 caused fear among Mobutu's faction and within the CIA that the incoming administration would shift its favor to the imprisoned Lumumba. Lumumba was killed three days before Kennedy's inauguration on 20 January, though Kennedy would not learn of the killing until 13 February.
Church Committee.
In 1975, the Church Committee went on record with the finding that CIA chief Allen Dulles had ordered Lumumba's assassination as "an urgent and prime objective". Furthermore, declassified CIA cables quoted or mentioned in the Church report and in Kalb (1972) mention two specific CIA plots to murder Lumumba: the poison plot and a shooting plot. Although some sources claim that CIA plots ended when Lumumba was captured, that is not stated or shown in the CIA records.
Rather, those records show two still-partly-censored CIA cables from Elizabethville on days significant in the murder: 17 January, the day Lumumba died, and 18 January, the day of the first exhumation. The former, after a long censored section, talks about where they need to go from there. The latter expresses thanks for Lumumba being sent to them and then says that, had Elizabethville base known he was coming, they would have "baked a snake". This cable goes on to state that the writer's sources (not yet declassified) said that after being taken from the airport Lumumba was imprisoned by "all white guards".
The Committee later found that while the CIA had conspired to kill Lumumba, it was not directly involved in the actual murder.
U.S. government documents.
Declassified documents revealed that the CIA had plotted to assassinate Lumumba. These documents indicate that the Congolese leaders who killed Lumumba, including Mobutu Sese Seko and Joseph Kasa-Vubu, received money and weapons directly from the CIA. This same disclosure showed that at that time the U.S. government believed that Lumumba was a communist.
A recently declassified interview with then-US National Security Council minutekeeper Robert Johnson revealed that U.S. President Dwight D. Eisenhower had said "something CIA chief Allen Dulles to the effect that Lumumba should be eliminated". The interview from the Senate Intelligence Committee's inquiry on covert action was released in August 2000.
In December 2013, the U.S. State Department admitted that President Eisenhower authorized the murder of Lumumba. The CIA Chief, Allan Dulles, allocated $100,000 to accomplish the act, but this plan was not carried out.
British involvement.
In April 2013, in a letter to the "London Review of Books", a British parliamentarian, Lord Lea of Crondall reported having discussed Lumumba's death with Baroness Park of Monmouth shortly before she died in March 2010. Park had been an MI6 officer posted to Leopoldville at the time of Lumumba's death, and was later a semi-official spokesperson for MI6 in the House of Lords.
According to Lea, when he mentioned "the uproar" surrounding Lumumba's abduction and murder, and recalled the theory that MI6 might have had "something to do with it", she replied, "We did. I organised it." BBC reported that, subsequently, "Whitehall sources" described the claims of MI6 involvement as "speculative".
Legacy.
Political.
Patrice Lumumba was Prime Minister of The Congo for 81 days, from 23 June to 14 September 1960. To his supporters, Lumumba was an altruistic man of strong character. He favoured a unitary Congo and opposed division of the country along ethnic or regional lines. Like many other African leaders, he supported pan-Africanism and liberation for colonial territories. He proclaimed his regime one of "positive neutralism," defined as a return to African values and rejection of any imported ideology, including that of the Soviet Union: "We are not Communists or Catholics. We are African nationalists."
2006 Congolese elections.
The image of Patrice Lumumba continues to serve as an inspiration in contemporary Congolese politics. In the 2006 elections, several parties claimed to be motivated by his ideas, including the People's Party for Reconstruction and Democracy (PPRD), the political party initiated by the incumbent President Joseph Kabila. Antoine Gizenga, who served as Lumumba's Deputy Prime Minister in the post-independence period, was a 2006 Presidential candidate under the Unified Lumumbist Party (Parti Lumumbiste Unifié (PALU)) and was named prime minister at the end of the year. Other political parties that directly utilise his name include the Mouvement National Congolais-Lumumba (MNC-L) and the Mouvement Lumumbiste (MLP).
Family and politics.
Patrice Lumumba's family is actively involved in contemporary Congolese politics. Patrice Lumumba was married to Pauline Lumumba and had five children; François was the eldest followed by Patrice Junior, Julienne, Roland and Guy-Patrice Lumumba. François was 10 years old when Patrice died. Before his imprisonment, Patrice arranged for his wife and children to move into exile in Egypt, where François spent his childhood, then went to Hungary for education (he holds a doctorate in political economics).
Lumumba's youngest son, Guy-Patrice, born six months after his father's death, was an independent presidential candidate in the 2006 elections, but received less than 10% of the vote.

</doc>
<doc id="49241" url="https://en.wikipedia.org/wiki?curid=49241" title="Gustav Holst">
Gustav Holst

Gustav Theodore Holst (born Gustavus Theodore von Holst; 21 September 1874 – 25 May 1934) was an English composer, arranger and teacher. Best known for his orchestral suite "The Planets", he composed a large number of other works across a range of genres, although none achieved comparable success. His distinctive compositional style was the product of many influences, Richard Wagner and Richard Strauss being most crucial early in his development. The subsequent inspiration of the English folksong revival of the early 20th century, and the example of such rising modern composers as Maurice Ravel, led Holst to develop and refine an individual style.
There were professional musicians in the previous three generations of Holst's family, and it was clear from his early years that he would follow the same calling. He hoped to become a pianist, but was prevented by neuritis in his right arm. Despite his father's reservations, he pursued a career as a composer, studying at the Royal College of Music under Charles Villiers Stanford. Unable to support himself by his compositions, he played the trombone professionally and later became a teacher—a great one, according to his colleague Ralph Vaughan Williams. Among other teaching activities he built up a strong tradition of performance at Morley College, where he served as musical director from 1907 until 1924, and pioneered music education for women at St Paul's Girls' School, where he taught from 1905 until his death in 1934, raising standards and so laying the foundation for several professional musicians. He was the founder of a series of Whitsun music festivals, which ran from 1916 for the remainder of his life. Holst's works were played frequently in the early years of the 20th century, but it was not until the international success of "The Planets" in the years immediately after the First World War that he became a well-known figure. A shy man, he did not welcome this fame, and preferred to be left in peace to compose and teach.
In his later years his uncompromising, personal style of composition struck many music lovers as too austere, and his brief popularity declined. Nevertheless, he was a significant influence on a number of younger English composers, including Edmund Rubbra, Michael Tippett and Benjamin Britten. Apart from "The Planets" and a handful of other works, his music was generally neglected until the 1980s, since when recordings of much of his output have been available.
Life and career.
Early years.
Family background.
Holst was born in Cheltenham, Gloucestershire, the elder of the two children of Adolph von Holst, a professional musician, and his wife, Clara Cox, "née" Lediard. She was of mostly British descent, daughter of a respected Cirencester solicitor; the Holst side of the family was of mixed Swedish, Latvian and German ancestry, with at least one professional musician in each of the previous three generations.
Holst's great-grandfather, Matthias Holst, born in Riga, Latvia, was of German origin; he served as composer and harp-teacher to the Imperial Russian Court in St Petersburg. Matthias's son Gustavus, who moved to England with his parents as a child in 1802, was a composer of salon-style music and a well-known harp teacher. He appropriated the aristocratic prefix "von" and added it to the family name in the hope of gaining enhanced prestige and attracting pupils.
Holst's father, Adolph von Holst, became organist and choirmaster at All Saints' Church, Cheltenham; he also taught, and gave piano recitals. His wife, Clara, a former pupil, was a talented singer and pianist. They had two sons; Gustav's younger brother, Emil Gottfried, became known as Ernest Cossart, a successful actor in the West End, New York and Hollywood. Clara died in February 1882, and the family moved to another house in Cheltenham, where Adolph recruited his sister Nina to help raise the boys. Gustav recognised her devotion to the family and dedicated several of his early compositions to her. In 1885 Adolph married Mary Thorley Stone, another of his pupils. They had two sons, Matthias (known as "Max") and Evelyn ("Thorley"). Mary von Holst was absorbed in theosophy and not greatly interested in domestic matters. All four of Adolph's sons were subject to what one biographer calls "benign neglect", and Gustav in particular was "not overburdened with attention or understanding, with a weak sight and a weak chest, both neglected—he was 'miserable and scared'."
Childhood and youth.
Holst was taught to play the piano and the violin; he enjoyed the former very much more than the latter. At the age of twelve he took up the trombone at Adolph's suggestion, thinking that playing a brass instrument might improve his asthma. Holst was educated at Cheltenham Grammar School between 1886 and 1891. He started composing in or about 1886; inspired by Macaulay's poem "" he began, but soon abandoned, an ambitious setting of the work for chorus and orchestra. His early compositions included piano pieces, organ voluntaries, songs, anthems and a symphony (from 1892). His main influences at this stage were Mendelssohn, Chopin, Grieg and above all Sullivan. Adolph tried to steer his son away from composition, hoping that he would have a career as a pianist. Holst's health played a decisive part in his musical future; he had never been strong, and in addition to his asthma and poor eyesight he suffered from neuritis, which made playing the piano difficult. He said that the affected arm was "like a jelly overcharged with electricity".
After Holst left school in 1891, Adolph paid for him to spend four months in Oxford studying counterpoint with George Frederick Sims, organist of Merton College. On his return Holst obtained his first professional appointment, aged seventeen, as organist and choirmaster at Wyck Rissington, Gloucestershire. The post brought with it the conductorship of the Bourton-on-the-Water Choral Society, which offered no extra remuneration but provided valuable experience that enabled him to hone his conducting skills. In November 1891 Holst gave what was perhaps his first public performance as a pianist; he and his father played the Brahms "Hungarian Dances" at a concert in Cheltenham. The programme for the event gives his name as "Gustav" rather than "Gustavus"; he was called by the shorter version from his early years.
Royal College of Music.
In 1892 Holst wrote the music for an operetta in the style of Gilbert and Sullivan, "Lansdown Castle, or The Sorcerer of Tewkesbury". The piece was performed at Cheltenham Corn Exchange in February 1893; it was well received and its success encouraged him to persevere with composing. He applied for a scholarship at the Royal College of Music (RCM) in London, but the composition scholarship for that year was won by Samuel Coleridge-Taylor. Holst was accepted as a non-scholarship student, and Adolph borrowed £100 to cover the first year's expenses. Holst left Cheltenham for London in May 1893. Money was tight, and partly from frugality and partly from his own inclination he became a vegetarian and a teetotaller. Two years later he was finally granted a scholarship, which slightly eased his financial difficulties, but he retained his austere personal regime.
Holst's professors at the RCM were Frederick Sharpe (piano), William Stephenson Hoyte (organ), George Case (trombone), George Jacobi (instrumentation) and the director of the college, Hubert Parry (history). After preliminary lessons with W. S. Rockstro and Frederick Bridge, Holst was granted his wish to study composition with Charles Villiers Stanford. To support himself during his studies Holst played the trombone professionally, at seaside resorts in the summer and in London theatres in the winter. His daughter and biographer, Imogen Holst, records that from his fees as a player "he was able to afford the necessities of life: board and lodging, manuscript paper, and tickets for standing room in the gallery at Covent Garden Opera House on Wagner evenings". He secured an occasional engagement in symphony concerts, playing in 1897 under the baton of Richard Strauss at the Queen's Hall.
Like many musicians of his generation, Holst came under Wagner's spell. He had recoiled from the music of "Götterdämmerung" when he heard it at Covent Garden in 1892, but encouraged by his friend and fellow-student Fritz Hart he persevered and quickly became an ardent Wagnerite. Wagner supplanted Sullivan as the main influence on his music, and for some time, as Imogen put it, "ill-assimilated wisps of "Tristan" inserted themselves on nearly every page of his own songs and overtures." Stanford admired some of Wagner's works, and had in his earlier years been influenced by him, but Holst's sub-Wagnerian compositions met with his disapprobation: "It won't do, me boy; it won't do". Holst respected Stanford, describing him to a fellow-pupil, Herbert Howells, as "the one man who could get any one of us out of a technical mess", but he found that his fellow students, rather than the faculty members, had the greater influence on his development.
In 1895, shortly after celebrating his twenty-first birthday, Holst met Ralph Vaughan Williams, who became a lifelong friend and had more influence on Holst's music than anybody else. Stanford emphasised the need for his students to be self-critical, but Holst and Vaughan Williams became one another's chief critics; each would play his latest composition to the other while still working on it. Vaughan Williams later observed, "What one really learns from an Academy or College is not so much from one's official teachers as from one's fellow-students ... discussed every subject under the sun from the lowest note of the double bassoon to the philosophy of "Jude the Obscure". In 1949 he wrote of their relationship, "Holst declared that his music was influenced by that of his friend: the converse is certainly true." 1895 was also the bicentenary of Henry Purcell, which was marked by various performances including Stanford conducting "Dido and Aeneas" at the Lyceum Theatre; the work profoundly impressed Holst, who over twenty years later confessed to a friend that his search for "the (or a) musical idiom of the English language" had been inspired "unconsciously" by "hearing the recits in Purcell's Dido".
Another influence was William Morris. In Vaughan Williams's words, "It was now that Holst discovered the feeling of unity with his fellow men which made him afterwards a great teacher. A sense of comradeship rather than political conviction led him, while still a student, to join the Kelmscott House Socialist Club in Hammersmith." At Kelmscott House, Morris's home, Holst attended lectures by his host and Bernard Shaw. His own socialism was moderate in character, but he enjoyed the club for its good company and his admiration of Morris as a man. His ideals were influenced by Morris's but had a different emphasis. Morris had written, "I do not want art for a few any more than education for a few, or freedom for a few. I want all persons to be educated according to their capacity, not according to the amount of money which their parents happen to have". Holst said, "'Aristocracy in art'—art is not for all but only for the chosen few—but the only way to find those few is to bring art to everyone—then the artists have a sort of masonic signal by which they recognise each other in the crowd." He was invited to conduct the Hammersmith Socialist Choir, teaching them madrigals by Thomas Morley, choruses by Purcell, and works by Mozart, Wagner and himself. One of his choristers was (Emily) Isobel Harrison (1876–1969), a beautiful soprano two years his junior. He fell in love with her; she was at first unimpressed by him, but she came round and they were engaged, though with no immediate prospect of marriage given Holst's tiny income.
Professional musician.
In 1898 the RCM offered Holst a further year's scholarship, but he felt that he had learned as much as he could there and that it was time, as he put it, to "learn by doing". Some of his compositions were published and performed; the previous year "The Times" had praised his song "Light Leaves Whisper", "a moderately elaborate composition in six parts, treated with a good deal of expression and poetic feeling". Occasional successes notwithstanding, Holst found that "man cannot live by composition alone"; he took posts as organist at various London churches, and continued playing the trombone in theatre orchestras. In 1898 he was appointed first trombonist and "répétiteur" with the Carl Rosa Opera Company and toured with the Scottish Orchestra. Though a capable rather than a virtuoso player he won the praise of the leading conductor Hans Richter, for whom he played at Covent Garden. His salary was only just enough to live on, and he supplemented it by playing in a popular orchestra called the "White Viennese Band", conducted by Stanislas Wurm. Holst enjoyed playing for Wurm, and learned much from him about drawing rubato from players. Nevertheless, longing to devote his time to composing, Holst found the necessity of playing for "the Worm" or any other light orchestra "a wicked and loathsome waste of time". Vaughan Williams did not altogether agree with his friend about this; he admitted that some of the music was "trashy" but thought it had been useful to Holst nonetheless: "To start with, the very worst a trombonist has to put up with is as nothing compared to what a church organist has to endure; and secondly, Holst is above all an orchestral composer, and that sure touch which distinguishes his orchestral writing is due largely to the fact that he has been an orchestral player; he has learnt his art, both technically and in substance, not at second hand from text books and models, but from actual live experience."
With a modest income secured, Holst was able to marry Isobel; the ceremony was at Fulham Register Office on 22 June 1901. Their marriage lasted until his death; there was one child, Imogen, born in 1907. In 1902 Dan Godfrey and the Bournemouth Municipal Orchestra premiered Holst's "Cotswold Symphony", the slow movement of which is a lament for Morris, who had died in October 1896, three years before Holst began work on the piece. In 1903 Adolph von Holst died, leaving a small legacy. Holst and his wife decided, as Imogen later put it, that "as they were always hard up the only thing to do was to spend it all at once on a holiday in Germany".
Composer and teacher.
While in Germany, Holst reappraised his professional life, and in 1903 he decided to abandon orchestral playing to concentrate on composition. His earnings as a composer were too little to live on, and two years later he accepted the offer of a teaching post at James Allen's Girls' School, Dulwich, which he held until 1921. He also taught at the Passmore Edwards Settlement, where among other innovations he gave the British premieres of two Bach cantatas. The two teaching posts for which he is probably best known were director of music at St Paul's Girls' School, Hammersmith, from 1905 until his death, and director of music at Morley College from 1907 to 1924. Vaughan Williams wrote of the former establishment: "Here he did away with the childish sentimentality which schoolgirls were supposed to appreciate and substituted Bach and Vittoria; a splendid background for immature minds." Several of Holst's pupils at St Paul's went on to distinguished careers, notably the soprano Joan Cross, and the oboist and cor anglais player Helen Gaskell, who made history by becoming the first woman to join the woodwind section of the New Queen's Hall Orchestra, subsequently joining the BBC Symphony Orchestra on its foundation. Of Holst's impact on Morley College, Vaughan Williams wrote: " bad tradition had to be broken down. The results were at first discouraging, but soon a new spirit appeared and the music of Morley College, together with its offshoot the 'Whitsuntide festival' ... became a force to be reckoned with". Before Holst's appointment, Morley College had not treated music very seriously—the bad tradition to which Vaughan Williams referred; at first Holst's exacting demands drove many students away. He persevered, and gradually built up a class of dedicated music-lovers.
According to the composer Edmund Rubbra, who studied under him in the early 1920s, Holst was "a teacher who often came to lessons weighted, not with the learning of Prout and Stainer, but with a miniature score of "Petrushka" or the then recently published Mass in G minor of Vaughan Williams". He never sought to impose his own ideas on his composition pupils. Rubbra recalled that he would divine a student's difficulties and gently guide him to finding the solution for himself. "I do not recall that Holst added one single note of his own to anything I wrote, but he would suggest—if I agreed!—that, given such and such a phrase, the following one would be better if it took such and such a course; if I did not see this, the point would not be insisted upon ... He frequently took away of his abhorrence of unessentials."
As a composer Holst was frequently inspired by literature. He set poetry by Thomas Hardy and Robert Bridges and, a particular influence, Walt Whitman, whose words he set in "Dirge for Two Veterans" and "The Mystic Trumpeter" (1904). He wrote an orchestral "Walt Whitman Overture" in 1899. While on tour with the Carl Rosa company Holst had read some of Max Müller's books, which inspired in him a keen interest in Sanskrit texts, particularly the Rig Veda hymns. He found the existing English versions of the texts unconvincing, and decided to make his own translations, despite his lack of skills as a linguist. He enrolled in 1909 at University College, London to study the language. Imogen commented on his translations: "He was not a poet, and there are occasions when his verses seem naïve. But they never sound vague or slovenly, for he had set himself the task of finding words that would be 'clear and dignified' and that would 'lead the listener into another world'." His settings of translations of Sanskrit texts included "Sita" (1899–1906), a three-act opera based on an episode in the "Ramayana" (which he eventually entered for a competition for English opera set by the Milan music publisher Tito Ricordi); "Savitri" (1908), a chamber opera based on a tale from the "Mahabharata"; four groups of "Hymns from the Rig Veda" (1908–14); and two texts originally by Kālidāsa: "Two Eastern Pictures" (1909–10) and "The Cloud Messenger" (1913).
Towards the end of the nineteenth century, British musical circles had experienced a new interest in national folk music. Some composers, such as Sullivan and Elgar, remained indifferent, but Parry, Stanford, John Stainer and Alexander Mackenzie were founding members of the Folk-Song Society. Parry considered that by recovering English folk song, English composers would find an authentic national voice; he commented, "in true folk-songs there is no sham, no got-up glitter, and no vulgarity". Vaughan Williams was an early and enthusiastic convert to this cause, going round the English countryside collecting and noting down folk songs. These had an influence on Holst. Though not as passionate on the subject as his friend, he incorporated a number of folk melodies in his own compositions and made several arrangements of folk songs collected by others. The "Somerset Rhapsody" (1906–07), was written at the suggestion of the folk-song collector Cecil Sharp and made use of tunes that Sharp had noted down. Holst described its performance at the Queen's Hall in 1910 as "my first real success". A few years later Holst became excited by another musical renaissance—the rediscovery of English madrigal composers. Weelkes was his favourite of all the Tudor composers, but Byrd also meant much to him.
Holst was a keen rambler. He walked extensively in England, Italy, France and Algeria. In 1908 he travelled to Algeria on medical advice as a treatment for asthma and the depression that he suffered after his opera "Sita" failed to win the Ricordi prize. This trip inspired the suite "Beni Mora", which incorporated music he heard in the Algerian streets. Vaughan Williams wrote of this exotic work, "if it had been played in Paris rather than London it would have given its composer a European reputation, and played in Italy would probably have caused a riot."
1910s.
In June 1911 Holst and his Morley College students gave the first performance since the seventeenth century of Purcell's "The Fairy-Queen". The full score had been lost soon after Purcell's death in 1695, and had only recently been found. Twenty-eight Morley students copied out the complete vocal and orchestral parts. There were 1,500 pages of music and it took the students almost eighteen months to copy them out in their spare time. A concert performance of the work was given at The Old Vic, preceded by an introductory talk by Vaughan Williams. "The Times" praised Holst and his forces for "a most interesting and artistic performance of this very important work".
After this success, Holst was disappointed the following year by the lukewarm reception of his choral work "The Cloud Messenger". He again went travelling, accepting an invitation from H. Balfour Gardiner to join him and the brothers Clifford and Arnold Bax in Spain. During this holiday Clifford Bax introduced Holst to astrology, an interest that later inspired his suite "The Planets". Holst cast his friends' horoscopes for the rest of his life and referred to astrology as his "pet vice".
In 1913, St Paul's Girls' School opened a new music wing, and Holst composed "St Paul's Suite" for the occasion. The new building contained a sound-proof room, handsomely equipped, where he could work undisturbed. Holst and his family moved to a house in Brook Green, very close to the school. For the previous six years they had lived in a pretty house overlooking the Thames at Barnes, but the river air, frequently foggy, affected his breathing. For use at weekends and during school holidays, Holst and his wife bought a cottage in Thaxted, Essex, surrounded by mediaeval buildings and ample rambling opportunities. In 1917 they moved to a house in the centre of the town, where they stayed until 1925.
At Thaxted, Holst became friendly with the Rev Conrad Noel, known as the "Red Vicar", who supported the Independent Labour Party and espoused many causes unpopular with conservative opinion. Noel also encouraged the revival of folk-dancing and processionals as part of church ceremonies, innovations which caused controversy among traditionally-minded churchgoers. Holst became an occasional organist and choirmaster at Thaxted Parish Church; he also developed an interest in bell-ringing. He started an annual music festival at Whitsuntide in 1916; students from Morley College and St Paul's Girls' School performed together with local participants. Holst's "a cappella" carol, "This Have I Done For My True Love", was dedicated to Noel in recognition of his interest in the ancient origins of religion (the composer always referred to the work as "The Dancing Day"). It received its first performance during the Third Whitsun Festival at Thaxted in May 1918. During that festival, Noel, a staunch supporter of Russia's October Revolution, demanded in a Saturday message during the service that there should be a greater political commitment from those who participated in the church activities; his claim that several of Holst's pupils (implicitly those from St Paul's Girls' School) were merely "camp followers" caused offence. Holst, anxious to protect his students from being embroiled in ecclesiastical conflict, moved the Whitsun Festival to Dulwich, though he himself continued to help with the Thaxted choir and to play the church organ on occasion.
First World War.
At the outbreak of the First World War, Holst tried to enlist but was rejected as unfit for military service. He felt frustrated that he could not contribute to the war effort. His wife became a volunteer ambulance driver; Vaughan Williams went on active service to France as did Holst's brother Emil; Holst's friends the composers George Butterworth and Cecil Coles were killed in battle. He continued to teach and compose; he worked on "The Planets" and prepared his chamber opera "Savitri" for performance. It was first given in December 1916 by students of the London School of Opera at the Wellington Hall in St John's Wood. It attracted no attention at the time from the main newspapers, though when professionally staged five years later it was greeted as "a perfect little masterpiece." In 1917 he wrote "The Hymn of Jesus" for chorus and orchestra, a work which remained unperformed until after the war.
In 1918, as the war neared its end, Holst finally had the prospect of a job that offered him the chance to serve. The music section of the YMCA's education department needed volunteers to work with British troops stationed in Europe awaiting demobilisation. Morley College and St Paul's Girls' School offered him a year's leave of absence, but there remained one obstacle: the YMCA felt that his surname looked too German to be acceptable in such a role. He formally changed "von Holst" to "Holst" by deed poll in September 1918. He was appointed as the YMCA's musical organiser for the Near East, based in Salonica.
Holst was given a spectacular send-off. The conductor Adrian Boult recalled, "Just before the Armistice, Gustav Holst burst into my office: 'Adrian, the YMCA are sending me to Salonica quite soon and Balfour Gardiner, bless his heart, has given me a parting present consisting of the Queen's Hall, full of the Queen's Hall Orchestra for the whole of a Sunday morning. So we're going to do "The Planets", and you've got to conduct'." There was a burst of activity to get things ready in time. The girls at St Paul's helped to copy out the orchestral parts, and the women of Morley and the St Paul's girls learned the choral part in the last movement. The performance was given on 29 September to an invited audience including Sir Henry Wood and most of the professional musicians in London. Five months later, when Holst was in Greece, Boult introduced "The Planets" to the general public, at a concert in February 1919; Holst sent him a long letter full of suggestions, but failed to convince him that the suite should be played in full. The conductor believed that about half an hour of such radically new music was all the public could absorb at first hearing, and he gave only five of the seven movements on that occasion.
Holst enjoyed his time in Salonica, from where he was able to visit Athens, which greatly impressed him. His musical duties were wide-ranging, and even obliged him on occasion to play the violin in the local orchestra: "it was great fun, but I fear I was not of much use". He returned to England in June 1919.
Post-war.
On his return from Greece, Holst resumed his teaching and composing. In addition to his existing work he accepted a lectureship in composition at the University of Reading and joined Vaughan Williams in teaching composition at their "alma mater" the RCM. Inspired by Adrian Boult's conducting classes at the RCM, Holst tried to further pioneer music education for women by proposing to the High Mistress of St Paul's Girls' School that he should invite Boult to give classes at the school: "It would be glorious if the SPGS turned out the only women conductors in the world!" In his soundproof room at SPGS he composed the "Ode to Death", a setting of a poem by Whitman, which according to Vaughan Williams is considered by many to be Holst's most beautiful choral work.
Holst, in his forties, suddenly found himself in demand. The New York Philharmonic and Chicago Symphony Orchestra vied to be the first to play "The Planets" in the US. The success of that work was followed in 1920 by an enthusiastic reception for "The Hymn of Jesus", described in "The Observer" as "one of the most brilliant and one of the most sincere pieces of choral and orchestral expression heard for some years." "The Times" called it "undoubtedly the most strikingly original choral work which has been produced in this country for many years." To his surprise and dismay Holst was becoming famous. Celebrity was something wholly foreign to his nature. As the music scholar Byron Adams puts it, "he struggled for the rest of his life to extricate himself from the web of garish publicity, public incomprehension and professional envy woven about him by this unsought-for success." He turned down honours and awards offered to him, and refused to give interviews or autographs.
Holst's comic opera "The Perfect Fool" (1923) was widely seen as a satire of "Parsifal", though Holst firmly denied it. The piece, with Maggie Teyte in the leading soprano role and Eugene Goossens conducting, was enthusiastically received at its premiere in the Royal Opera House. At a concert in Reading in 1923, Holst slipped and fell, suffering concussion. He seemed to make a good recovery, and he felt up to accepting an invitation to the US, lecturing and conducting at the University of Michigan. After he returned he found himself more and more in demand, to conduct, prepare his earlier works for publication, and, as before, to teach. The strain caused by these demands on him was too great; on doctor's orders he cancelled all professional engagements during 1924, and retreated to Thaxted. In 1925 he resumed his work at St Paul's Girls' School, but did not return to any of his other posts.
Later years.
Holst's productivity as a composer benefited almost at once from his release from other work. His works from this period include the "First Choral Symphony" to words by Keats (a "Second Choral Symphony" to words by George Meredith exists only in fragments). A short Shakespearian opera, "At the Boar's Head", followed; neither had the immediate popular appeal of "A Moorside Suite" for brass band of 1928.
In 1927 Holst was commissioned by the New York Symphony Orchestra to write a symphony. Instead, he wrote an orchestral piece "Egdon Heath", inspired by Thomas Hardy's Wessex. It was first performed in February 1928, a month after Hardy's death, at a memorial concert. By this time the public's brief enthusiasm for everything Holstian was waning, and the piece was not well received in New York. Olin Downes in "The New York Times" opined that "the new score seemed long and undistinguished". The day after the American performance, Holst conducted the City of Birmingham Orchestra in the British premiere. "The Times" acknowledged the bleakness of the work but allowed that it matched Hardy's grim view of the world: ""Egdon Heath" is not likely to be popular, but it says what the composer wants to say, whether we like it or not, and truth is one aspect of duty." Holst had been distressed by hostile reviews of some of his earlier works, but he was indifferent to critical opinion of "Egdon Heath", which he regarded as, in Adams's phrase, his "most perfectly realized composition".
Towards the end of his life Holst wrote the "Choral Fantasia" (1930) and he was commissioned by the BBC to write a piece for military band; the resulting prelude and scherzo "Hammersmith" was a tribute to the place where he had spent most of his life. The composer and critic Colin Matthews considers the work "as uncompromising in its way as "Egdon Heath", discovering, in the words of Imogen Holst, 'in the middle of an over-crowded London ... the same tranquillity that he had found in the solitude of Egdon Heath'". The work was unlucky in being premiered at a concert that also featured the London premiere of Walton's "Belshazzar's Feast", by which it was somewhat overshadowed.
Holst wrote a score for a British film, "The Bells" (1931), and was amused to be recruited as an extra in a crowd scene. Both film and score are now lost. He wrote a "jazz band piece" that Imogen later arranged for orchestra as "Capriccio". Having composed operas throughout his life with varying success, Holst found for his last opera, "The Wandering Scholar", what Matthews calls "the right medium for his oblique sense of humour, writing with economy and directness".
Harvard University offered Holst a lectureship for the first six months of 1932. Arriving via New York he was pleased to be reunited with his brother, Emil, whose acting career under the name of Ernest Cossart had taken him to Broadway; but Holst was dismayed by the continual attentions of press interviewers and photographers. He enjoyed his time at Harvard, but was taken ill while there: a duodenal ulcer prostrated him for some weeks. He returned to England, joined briefly by his brother for a holiday together in the Cotswolds. His health declined, and he withdrew further from musical activities. One of his last efforts was to guide the young players of the St Paul's Girls' School orchestra through one of his final compositions, the "Brook Green Suite", in March 1934.
Holst died in London on 25 May 1934, at the age of 59, of heart failure following an operation on his ulcer. His ashes were interred at Chichester Cathedral in Sussex, close to the memorial to Thomas Weelkes, his favourite Tudor composer. Bishop George Bell gave the memorial oration at the funeral, and Vaughan Williams conducted music by Holst and himself.
Music.
Style.
Holst's absorption of folksong, not only in the melodic sense but in terms of its simplicity and economy of expression, helped to develop a style that many of his contemporaries, even admirers, found austere and cerebral. This is contrary to the popular identification of Holst with "The Planets", which Matthews believes has masked his status as a composer of genuine originality. Against charges of coldness in the music, Imogen cites Holst's characteristic "sweeping modal tunes mov reassuringly above the steps of a descending bass", while Michael Kennedy points to the 12 Humbert Wolfe settings of 1929, and the 12 Welsh folksong settings for unaccompanied chorus of 1930–31, as works of true warmth.
Many of the characteristics that Holst employed—unconventional time signatures, rising and falling scales, ostinato, bitonality and occasional polytonality—set him apart from other English composers. Vaughan Williams remarked that Holst always said in his music what he wished to say, directly and concisely; "He was not afraid of being obvious when the occasion demanded, nor did he hesitate to be remote when remoteness expressed his purpose". Kennedy has surmised that Holst's economy of style was in part a product of the composer's poor health: "the effort of writing it down compelled an artistic economy which some felt was carried too far". However, as an experienced instrumentalist and orchestra member, Holst understood music from the standpoint of his players and made sure that, however challenging, their parts were always practicable. According to his pupil Jane Joseph, Holst fostered in performance "a spirit of practical comradeship ... none could know better than he the boredom possible to a professional player, and the music that rendered boredom impossible".
Early works.
Although Holst wrote a large number of works—particularly songs—during his student days and early adulthood, almost everything he wrote before 1904 he later classified as derivative "early horrors". Nevertheless, the composer and critic Colin Matthews recognises even in these apprentice works an "instinctive orchestral flair". Of the few pieces from this period which demonstrate some originality, Matthews pinpoints the G minor String Trio of 1894 (unperformed until 1974) as the first underivative work produced by Holst. Matthews and Imogen Holst each highlight the "Elegy" movement in "The Cotswold Symphony" (1899–1900) as among the more accomplished of the apprentice works, and Imogen discerns glimpses of her father's real self in the 1899 "Suite de ballet" and the "Ave Maria" of 1900. She and Matthews have asserted that Holst found his genuine voice in his setting of Whitman's verses, "The Mystic Trumpeter" (1904), in which the trumpet calls that characterise Mars in "The Planets" are briefly anticipated. In this work, Holst first employs the technique of bitonality—the use of two keys simultaneously.
Experimental years.
At the beginning of the 20th century, according to Matthews, it appeared that Holst might follow Schoenberg into late Romanticism. Instead, as Holst recognised afterwards, his encounter with Purcell's "Dido and Aeneas" prompted his searching for a "musical idiom of the English language"; the folksong revival became a further catalyst for Holst to seek inspiration from other sources during the first decade or so of the new century.
Indian period.
Holst's interest in Indian mythology, shared by many of his contemporaries, first became musically evident in the opera "Sita" (1901–06). During the opera's long gestation, Holst worked on other Indian-themed pieces. These included "Maya" (1901) for violin and piano, regarded by the composer and writer Raymond Head as "an insipid salon-piece whose musical language is dangerously close to Stephen Adams". Then, through Vaughan Williams, Holst discovered and became an admirer of the music of Ravel, whom he considered a "model of purity" on the level with Haydn, another composer he greatly admired. The combined influence of Ravel, Hindu spiritualism and English folk tunes enabled Holst to get beyond the once all-consuming influences of Wagner and Richard Strauss and to forge his own style. Imogen Holst has acknowledged Holst's own suggestion (written to Vaughan Williams): "ne ought to follow Wagner until he leads you to fresh things". She notes that although much of his grand opera, "Sita", is "'good old Wagnerian bawling' ... towards the end a change comes over the music, and the beautifully calm phrases of the hidden chorus representing the Voice of the Earth are in Holst's own language."
According to Rubbra, the publication in 1911 of Holst's Rig Veda Hymns was a landmark event in the composer's development: "Before this, Holst's music had, indeed, shown the clarity of utterance which has always been his characteristic, but harmonically there was little to single him out as an important figure in modern music." Dickinson describes these vedic settings as pictorial rather than religious; although the quality is variable the sacred texts clearly "touched vital springs in the composer's imagination". While the music of Holst's Indian verse settings remained generally western in character, in some of the vedic settings he experimented with Indian "raga" (scales).
The chamber opera "Savitri" (1908) is written for three solo voices, a small hidden female chorus, and an instrumental combination of two flutes, a cor anglais and a double string quartet. The music critic John Warrack comments on the "extraordinary expressive subtlety" with which Holst deploys the sparse forces: "... two unaccompanied vocal lines opening the work skilfully convey the relationship between Death, steadily advancing through the forest, and Savitri, her frightened answers fluttering round him, unable to escape his harmonic pull". Head describes the work as unique in its time for its compact intimacy, and considers it Holst's most successful attempt to end the domination of Wagnerian chromaticism in his music. Dickinson considers it a significant step, "not towards opera, but towards an idiomatic pursuit of [Holst's vision". Of the Kālidāsa texts, Dickinson dismisses "The Cloud Messenger" (1910–12) as an "accumulation of desultory incidents, opportunistic dramatic episodes and ecstatic outpourings" which illustrate the composer's creative confusion during that period; the "Two Eastern Pictures" (1911), in Dickinson's view, provide "a more memorable final impression of Kālidāsa".
Folksong and other influences.
Holst's settings of Indian texts formed only a part of his compositional output in the period 1900 to 1914. A highly significant factor in his musical development was the English folksong revival, evident in the orchestral suite "A Somerset Rhapsody" (1906–07), a work that was originally to be based around eleven folksong themes; this was later reduced to four. Observing the work's kinship with Vaughan Williams's "Norfolk Rhapsody", Dickinson remarks that, with its firm overall structure, Holst's composition "rises beyond the level of ... a song-selection". Imogen acknowledges that Holst's discovery of English folksongs "transformed his orchestral writing", and that the composition of "A Somerset Rhapsody" did much to banish the chromaticisms that had dominated his early compositions. In the "Two Songs without Words" of 1906, Holst showed that he could create his own original music using the folk idiom. An orchestral folksong fantasy "Songs of the West", also written in 1906, was withdrawn by the composer and never published, although it emerged in the 1980s in the form of an arrangement for wind band by James Curnow.
In the years before the First World War, Holst composed in a variety of genres. Matthews considers the evocation of a North African town in the "Beni Mora" suite of 1908 the composer's most individual work to that date; the third movement gives a preview of minimalism in its constant repetition of a four-bar theme. Holst wrote two suites for military band, in E flat (1909) and F major (1911) respectively, the first of which became and remains a brass band staple. This piece, a highly original and substantial musical work, was a signal departure from what Short describes as "the usual transcriptions and operatic selections which pervaded the band repertoire". Also in 1911 he wrote "Hecuba's Lament", a setting of Gilbert Murray's translation from Euripides built on a seven-beat refrain designed, says Dickinson, to represent Hecuba's defiance of divine wrath. In 1912 Holst composed two psalm settings, in which he experimented with plainsong; the same year saw the enduringly popular "St Paul's Suite" (a "gay but retrogressive" piece according to Dickinson), and the failure of his large scale orchestral work "Phantastes".
Full flowering.
"The Planets".
Holst conceived the idea of "The Planets" in 1913, partly as a result of his interest in astrology, and also from his determination, despite the failure of "Phantastes", to produce a large-scale orchestral work. The chosen format may have been influenced by Schoenberg's "Fünf Orchesterstücke", and shares something of the aesthetic, Matthews suggests, of Debussy's "Nocturnes" or "La mer". Holst began composing "The Planets" in 1914; the movements appeared not quite in their final sequence; "Mars" was the first to be written, followed by "Venus" and "Jupiter". "Saturn", "Uranus" and "Neptune" were all composed during 1915, and "Mercury" was completed in 1916.
Each planet is represented with a distinct character; Dickinson observes that "no planet borrows colour from another". In "Mars", a persistent, uneven rhythmic cell consisting of five beats, combined with trumpet calls and harmonic dissonance provides battle music which Short asserts is unique in its expression of violence and sheer terror, "... Holst's intention being to portray the reality of warfare rather than to glorify deeds of heroism". In "Venus", Holst incorporated music from an abandoned vocal work, "A Vigil of Pentecost", to provide the opening; the prevalent mood within the movement is of peaceful resignation and nostalgia. "Mercury" is dominated by uneven metres and rapid changes of theme, to represent the speedy flight of the winged messenger. "Jupiter" is renowned for its central melody, in Dickinson's view "a fantastic relaxation in which many retain a far from sneaking delight". Dickinson and other critics have decried the later use of the tune in the patriotic hymn "I Vow to Thee, My Country"—despite Holst's full complicity.
For "Saturn", Holst again used a previously-composed vocal piece, "Dirge and Hymeneal", as the basis for the movement, where repeated chords represent the relentless approach of old age. "Uranus", which follows, has elements of Berlioz's "Symphonie fantastique" and Dukas's "The Sorcerer's Apprentice", in its depiction of the magician who "disappears in a whiff of smoke as the sonic impetus of the movement diminishes from fff to ppp in the space of a few bars". "Neptune", the final movement, concludes with a wordless female chorus gradually receding, an effect which Warrack likens to "unresolved timelessness ... never ending, since space does not end, but drifting away into eternal silence". Apart from his concession with "I Vow to Thee", Holst insisted on the unity of the whole work, and opposed the performance of individual movements. Nevertheless, Imogen writes that the piece has "suffered from being quoted in snippets as background music".
Maturity.
During and after the composition of "The Planets", Holst wrote or arranged numerous vocal and choral works, many of them for the wartime Thaxted Whitsun Festivals, 1916–18. They include the "Six Choral Folksongs" of 1916, based on West Country tunes, of which "Swansea Town", with its "sophisticated tone", is deemed by Dickinson to be the most memorable. Holst downplayed such music as "a limited form of art" in which "mannerisms are almost inevitable"; the composer Alan Gibbs, however, believes Holst's set at least equal to Vaughan Williams's "Five English Folk Songs" of 1913. Holst's first major work after "The Planets" was the "Hymn of Jesus", completed in 1917. The words are from a Gnostic text, the apocryphal Acts of St John, using a translation from the Greek which Holst prepared with assistance from Clifford Bax and Jane Joseph. Head comments on the innovative character of the "Hymn": "At a stroke Holst had cast aside the Victorian and Edwardian sentimental oratorio, and created the precursor of the kind of works that John Tavener, for example, was to write in the 1970s". Matthews has written that the "Hymn"s "ecstatic" quality is matched in English music "perhaps only by Tippett's "The Vision of Saint Augustine""; the musical elements include plainsong, two choirs distanced from each other to emphasise dialogue, dance episodes and "explosive chordal dislocations".
In the "Ode to Death" (1918–19), the quiet, resigned mood is seen by Matthews as an "abrupt volte-face" after the life-enhancing spirituality of the "Hymn". Warrack refers to its aloof tranquillity; Imogen Holst believed the "Ode" expressed Holst's private attitude to death. The piece has rarely been performed since its premiere in 1922, although the composer Ernest Walker thought it was Holst's finest work to that date. The influential critic Ernest Newman considered "The Perfect Fool" "the best of modern British operas", but its unusually short length (about an hour) and parodic, whimsical nature—described by "The Times" as "a brilliant puzzle"—put it outside the operatic mainstream. Only the ballet music from the opera, which "The Times" called "the most brilliant thing in a work glittering with brilliant moments", has been regularly performed since 1923. Holst's libretto attracted much criticism, although Edwin Evans remarked on the rare treat in opera of being able to hear the words being sung.
Later works.
Before his enforced rest in 1924, Holst demonstrated a new interest in counterpoint, in his "Fugal Overture" of 1922 for full orchestra and the neo-classical" Fugal Concerto" of 1923, for flute, oboe and strings. In his final decade he mixed song settings and minor pieces with major works and occasional new departures; the 1925 "Terzetto" for flute, violin and oboe, each instrument playing in a different key, is cited by Imogen as Holst's only "successful" chamber work. Of the "Choral Symphony" completed in 1924, Matthews writes that, after several movements of real quality, the finale is a rambling anticlimax. Holst's penultimate opera, "At the Boar's Head" (1924), is based on tavern scenes from Shakespeare's "Henry IV, Parts 1" and "2". The music, which is largely derived from old English melodies gleaned from Cecil Sharp and other collections, has pace and verve; the contemporary critic Harvey Grace discounted the lack of originality, a facet which he said "can be shown no less convincingly by a composer's handling of material than by its invention".
"Egdon Heath" (1927) was Holst's first major orchestral work after "The Planets". Matthews summarises the music as "elusive and unpredictable; three main elements: a pulseless wandering melody strings, a sad brass processional, and restless music for strings and oboe." The mysterious dance towards the end is, says Matthews, "the strangest moment in a strange work". Richard Greene in "Music & Letters" describes the piece as "a larghetto dance in a siciliano rhythm with a simple, stepwise, rocking melody", but lacking the power of "The Planets" and, at times, monotonous to the listener. A more popular success was the "Moorside Suite" for brass band, written as a test piece for the National Brass Band Festival championships of 1928. While written within the traditions of north-country brass band music, the suite, Short says, bears Holst's unmistakable imprint, "from the skipping 6/8 of the opening Scherzo, to the vigorous melodic fourths of the concluding March, the intervening Nocturne bearing a family resemblance to the slow-moving procession of "Saturn"".
After this, Holst tackled his final attempt at opera in a cheerful vein, with "The Wandering Scholar" (1929–30), to a text by Clifford Bax. Imogen refers to the music as "Holst at his best in a scherzando (playful) frame of mind"; Vaughan Williams commented on the lively, folksy rhythms: "Do you think there's a "little" bit too much 6/8 in the opera?" Short observes that the opening motif makes several reappearances without being identified with a particular character, but imposes musical unity on the work.
Holst composed few large-scale works in his final years. "A Choral Fantasia" of 1930 was written for the Three Choirs Festival at Gloucester; beginning and ending with a soprano soloist, the work, also involving chorus, strings, brass and percussion, includes a substantial organ solo which, says Imogen Holst, "knows something of the 'colossal and mysterious' loneliness of "Egdon Heath"". Apart from his final uncompleted symphony, Holst's remaining works were for small forces; the eight "Canons" of 1932 were dedicated to his pupils, though in Imogen's view that they present a formidable challenge to the most professional of singers. The "Brook Green Suite" (1932), written for the orchestra of St Paul's School, was a late companion piece to the "St Paul's Suite". The "Lyric Movement" for viola and small orchestra (1933) was written for Lionel Tertis. Quiet and contemplative, and requiring little virtuosity from the soloist, the piece was slow to gain popularity among violists. Robin Hull, in "Penguin Music Magazine", praised the work's "clear beauty—impossible to mistake for the art of any other composer"; in Dickinson's view, however, it remains "a frail creation". Holst's final composition, the orchestral scherzo movement of a projected symphony, contains features characteristic of much of Holst's earlier music—"a summing up of Holst's orchestral art", according to Short. Dickinson suggests that the somewhat casual collection of material in the work gives little indication of the symphony that might have been written.
Recordings.
Holst made some recordings, conducting his own music. For the Columbia company he recorded "Beni Mora", the "Marching Song" and the complete "Planets" with the London Symphony Orchestra (LSO) in 1922, using the acoustic process. The limitations of early recording prevented the gradual fade-out of women's voices at the end of "Neptune", and the lower strings had to be replaced by a tuba to obtain an effective bass sound. With an anonymous string orchestra Holst recorded the "St Paul's Suite" and "Country Song" in 1925. Columbia's main rival, HMV, issued recordings of some of the same repertoire, with an unnamed orchestra conducted by Albert Coates. When electrical recording came in, with dramatically improved recording quality, Holst and the LSO re-recorded "The Planets" for Columbia in 1926.
In the early LP era little of Holst's music was available on disc. Only six of his works are listed in the 1955 issue of "The Record Guide": "The Planets" (recordings under Boult on HMV and Nixa, and another under Sir Malcolm Sargent on Decca); the "Perfect Fool" ballet music; the "St Paul's Suite"; and three short choral pieces. In the stereo LP and CD eras numerous recordings of "The Planets" were issued, performed by orchestras and conductors from round the world. By the early years of the 21st century most of the major and many of the minor orchestral and choral works had been issued on disc. The 2008 issue of "The Penguin Guide to Recorded Classical Music" contained seven pages of listings of Holst's works on CD. Of the operas, "Savitri", "The Wandering Scholar", and "At the Boar's Head" have been recorded.
Legacy.
Warrack emphasises that Holst acquired an instinctive understanding—perhaps more so than any English composer—of the importance of folksong. In it he found "a new concept not only of how melody might be organized, but of what the implications were for the development of a mature artistic language". Holst did not found or lead a school of composition; nevertheless, he exercised influences over both contemporaries and successors. According to Short, Vaughan Williams described Holst as "the greatest influence on my music", although Matthews asserts that each influenced the other equally. Among later composers, Michael Tippett is acknowledged by Short as Holst's "most significant artistic successor", both in terms of compositional style and because Tippett, who succeeded Holst as director of music at Morley College, maintained the spirit of Holst's music there. Of an early encounter with Holst, Tippett later wrote: "Holst seemed to look right inside me, with an acute spiritual vision". Kennedy observes that "a new generation of listeners ... recognized in Holst the fount of much that they admired in the music of Britten and Tippett". Holst's pupil Edmund Rubbra acknowledged how he and other younger English composers had adopted Holst's economy of style: "With what enthusiasm did we pare down our music to the very bone".
Short cites other English composers who are in debt to Holst, in particular William Walton and Benjamin Britten, and suggests that Holst's influence may have been felt further afield. Above all, Short recognises Holst as a composer for the people, who believed it was a composer's duty to provide music for practical purposes—festivals, celebrations, ceremonies, Christmas carols or simple hymn tunes. Thus, says Short, "many people who may never have heard any of [Holst's] major works ... have nevertheless derived great pleasure from hearing or singing such small masterpieces as the carol 'In the Bleak Midwinter'".
On 27 September 2009, after a weekend of concerts at Chichester Cathedral in memory of Holst, a new memorial was unveiled to mark the 75th anniversary of the composer's death. It is inscribed with words from the text of "The Hymn of Jesus": "The heavenly spheres make music for us". In April 2011 a BBC television documentary, "Holst: In the Bleak Midwinter", charted Holst's life with particular reference to his support for socialism and the cause of working people.
Notes and references.
Notes
References

</doc>
<doc id="49242" url="https://en.wikipedia.org/wiki?curid=49242" title="Irina Privalova">
Irina Privalova

Irina Anatoljewna Privalova (; Sergeyeva on 22 November 1968) is a Russian athlete who has won a gold medal at the Olympics.
She first competed in the sprint events, winning two Olympic medals in the 100 m and 200 m in 1992 whilst representing the Unified Team. Irina Privalova had been a formidable competitor during most of the 1990s but had not yet won an outdoor world championship gold medal. In 2000, she gambled successfully and switched to the 400 m hurdles discipline winning the Olympic title in Sydney 2000 in 53.02 s and a bronze in the 4 x 400 m relay team for Russia. It has been suggested that this change at the late age of 31 was because her chances of defeating Marion Jones (the overwhelming favourite for the 100 m/200 m double) were slim and the 400 m was also a repeat showdown between Marie-José Pérec and Cathy Freeman from the 1996 Atlanta Games. (That repeat showdown would not happen after Pérec left the Sydney games.)
Irina Privalova is currently the world indoor record holder in the 50 m (5.96 s), 60 m (6.92 s) sprints. She has also been the world indoor champion at the 60 m (7.02 s in 1991), 200 m (22.15 s in 1993), and 400 m (50.23 s in 1995) events.
Privalova achieved her best time (10.77 s) in the 100 m for nine years in 2008.
References.
<br>
<br>

</doc>
<doc id="49243" url="https://en.wikipedia.org/wiki?curid=49243" title="The Planets">
The Planets

The Planets, Op. 32, is a seven-movement orchestral suite by the English composer Gustav Holst, written between 1914 and 1916. Each movement of the suite is named after a planet of the Solar System and its corresponding astrological character as defined by Holst.
From its premiere to the present day, the suite has been enduringly popular, influential, widely performed and frequently recorded. The work was not heard in a complete public performance, however, until some years after it was completed. Although there were four performances between September 1918 and October 1920, they were all either private (the first performance, in London) or incomplete (two others in London and one in Birmingham). The premiere was at the Queen's Hall on 29 September 1918, conducted by Holst's friend Adrian Boult before an invited audience of about 250 people. The first complete public performance was finally given in London by Albert Coates conducting the London Symphony Orchestra on 15 November 1920.
Background.
The concept of the work is astrological rather than astronomical (which is why Earth is not included): each movement is intended to convey ideas and emotions associated with the influence of the planets on the psyche, not the Roman deities. The idea of the work was suggested to Holst by Clifford Bax, who introduced him to astrology when the two were part of a small group of English artists holidaying in Majorca in the spring of 1913; Holst became quite a devotee of the subject, and would cast his friends' horoscopes for fun. Holst also used Alan Leo's book "What is a Horoscope?" as a springboard for his own ideas, as well as for the subtitles (i.e., "The Bringer of...") for the movements.
When composing "The Planets" Holst initially scored the work for piano duet, except for "Neptune", which was scored for a single organ, as Holst believed that the sound of the piano was too percussive for a world as mysterious and distant as Neptune. Holst then scored the suite for a large orchestra, in which form it became enormously popular. Holst's use of orchestration was very imaginative and colourful, showing the influence of such contemporary composers as Igor Stravinsky and Arnold Schoenberg, as well as such late Russian romantics as Nikolai Rimsky-Korsakov and Alexander Glazunov. Its novel sonorities helped make the work an immediate success with audiences at home and abroad. Although "The Planets" remains Holst's most popular work, the composer himself did not count it among his best creations and later in life complained that its popularity had completely surpassed his other works. He was, however, partial to his own favourite movement, "Saturn".
Premieres.
The orchestral premiere of "The Planets" suite, conducted at Holst's request by Adrian Boult, was held at short notice on 29 September 1918, during the last weeks of World War I, in the Queen's Hall with the financial support of Holst's friend and fellow composer H. Balfour Gardiner. It was hastily rehearsed; the musicians of the Queen's Hall Orchestra first saw the complicated music only two hours before the performance, and the choir for "Neptune" was recruited from pupils from St Paul's Girls' School (where Holst taught). It was a comparatively intimate affair, attended by around 250 invited associates, but Holst regarded it as the public premiere, inscribing Boult's copy of the score, "This copy is the property of Adrian Boult who first caused the Planets to shine in public and thereby earned the gratitude of Gustav Holst."
A public concert was given in London under the auspices of the Royal Philharmonic Society on 27 February 1919, conducted by Boult. Five of the seven movements were played in the order Mars, Mercury, Saturn, Uranus, and Jupiter. It was Boult's decision not to play all seven movements at this concert. He felt that when the public were being given a totally new language like that, "half an hour of it was as much as they could take in". The anonymous critic in Hazell's Annual called it "an extraordinarily complex and clever suite". At a Queen's Hall symphony concert on 22 November of that year, Holst conducted Venus, Mercury and Jupiter (this was the first public performance of Venus). There was another incomplete public performance, in Birmingham, on 10 October 1920, with five movements (Mars, Venus, Mercury, Saturn and Jupiter). It is not clear whether this performance was conducted by Appleby Matthews or the composer.
His daughter Imogen recalled, "He hated incomplete performances of "The Planets", though on several occasions he had to agree to conduct three or four movements at Queen's Hall concerts. He particularly disliked having to finish with Jupiter, to make a 'happy ending', for, as he himself said, 'in the real world the end is not happy at all'".
The first complete performance of the suite at a public concert did not occur until 15 November 1920; the London Symphony Orchestra (LSO) was conducted by Albert Coates. This was the first time the movement "Neptune" had been heard in a public performance, all the other movements having been given earlier public airings.
The composer conducted a complete performance for the first time on 13 October 1923, with the Queen's Hall Orchestra at a Promenade Concert. Holst conducted the LSO in two recorded performances of "The Planets": the first was an acoustic recording made in sessions between 1922 and 1924 (now available on Pavilion Records' Pearl label); the second was made in 1926, and utilised the then-new electrical recording process (in 2003, this was released on compact disc by IMP and later on Naxos outside the United States). Because of the time constraints of the 78rpm format, the tempi are often much faster than is usually the case today.
Instrumentation.
The work is scored for a large orchestra consisting of four flutes (third doubling first piccolo and fourth doubling second piccolo and "bass flute in G", actually an alto flute), three oboes (third doubling bass oboe), one English horn, three clarinets in B-flat and A, one bass clarinet in B-flat, three bassoons, one contrabassoon; six horns in F, four trumpets in C, two trombones, one bass trombone, one tenor tuba in B-flat (actually a euphonium scored for treble clef), one bass tuba; a percussion section with six timpani (requiring two players), bass drum, snare drum, cymbals, triangle, tam-tam, tambourine, glockenspiel, xylophone, tubular bells; celesta, pipe organ; 2 harps and strings. In "Neptune", two three-part women's choruses (S S A) located in an adjoining room which is to be screened from the audience are added.
Structure.
The suite has seven movements, each named after a planet and its corresponding astrological character (see Planets in astrology):
Holst's original title, as seen on the handwritten full score, was "Seven Pieces for Large Orchestra". Holst almost certainly attended an early performance of Schoenberg's "Five Pieces for Orchestra" in 1914 (the year he wrote "Mars", "Venus" and "Jupiter"), and owned a score of it, the only Schoenberg score he ever owned. Each movement of Holst's work was originally called only by the second part of each title (I "The Bringer of War", II "The Bringer of Peace" and so on); the present titles were added in time for the first (incomplete) public performance in September 1919, though they were never added to the original score.
A typical performance of all seven movements is about fifty minutes long, though Holst's own electric recording from 1926 is just over forty-two and a half minutes.
One explanation for the suite's structure, presented by Holst scholar Raymond Head, is the ruling of astrological signs of the zodiac by the planets: if the signs are listed along with their ruling planets in the traditional order starting with Aries, ignoring duplication and the luminaries (the Sun and Moon), the order of the movements corresponds. Critic David Hurwitz offers an alternative explanation for the piece's structure: that "Jupiter" is the centrepoint of the suite and that the movements on either side are in mirror images. Thus "Mars" involves motion and "Neptune" is static; "Venus" is sublime while "Uranus" is vulgar, and "Mercury" is light and "scherzando" while "Saturn" is heavy and plodding. This hypothesis is lent credence by the fact that the two outer movements, "Mars" and "Neptune", are both written in rather unusual quintuple meter.
Holst suffered neuritis in his right arm, which caused him to seek help from several amanuenses in scoring "The Planets". This is clear from the number of different hands apparent in the full score.
"Neptune" was one of the first pieces of orchestral music to have a fade-out ending, although several composers (including Joseph Haydn in the finale of his Farewell Symphony) had achieved a similar effect by different means. Holst stipulates that the women's choruses are "to be placed in an adjoining room, the door of which is to be left open until the last bar of the piece, when it is to be slowly and silently closed", and that the final bar (scored for choruses alone) is "to be repeated until the sound is lost in the distance". Although commonplace today, the effect bewitched audiences in the era before widespread recorded sound—after the initial 1918 run-through, Holst's daughter Imogen (in addition to watching the charwomen dancing in the aisles during "Jupiter") remarked that the ending was "unforgettable, with its hidden chorus of women's voices growing fainter and fainter... until the imagination knew no difference between sound and silence".
Additional Astronomy Themed Compositions by Holst.
Betelgeuse.
Betelgeuse was the subject of Holst's setting to music of the 1927 poem Betelgeuse by Humbert Wolfe in his 12 Humbert Wolfe Settings, Op. 48 (1929).
Additions by other composers.
Several attempts have been made, for a variety of reasons, to append further music to Holst's suite, though by far the most common presentation of the music in the concert hall and on record remains Holst's original seven-movement version.
Pluto.
Pluto was discovered in 1930, four years before Holst's death, and was hailed by astronomers as the ninth planet. Holst, however, expressed no interest in writing a movement for the new planet. He had become disillusioned by the popularity of the suite, believing that it took too much attention away from his other works.
In the March 1972 final broadcast of his Young People's Concerts series, conductor Leonard Bernstein led the New York Philharmonic through a fairly straight interpretation of the suite, though Bernstein discarded the Saturn movement because he thought the theme of old age was irrelevant to a concert for children. The broadcast concluded with an improvised performance he called "Pluto, the Unpredictable." The 26 March 1972 performance may be viewed on the Kultur DVD set.
In 2000, the Hallé Orchestra commissioned the English composer Colin Matthews, an authority on Holst, to write a new eighth movement, which he called "Pluto, the Renewer". Dedicated to the late Imogen Holst, Gustav Holst's daughter, it was first performed in Manchester on 11 May 2000, with Kent Nagano conducting the Hallé Orchestra. Matthews also changed the ending of "Neptune" slightly so that movement would lead directly into "Pluto".
On 24 August 2006, the International Astronomical Union (IAU) defined what it means to be a "planet" within the Solar System. This definition excluded Pluto as a planet and added it as a member of the new category "dwarf planet", along with Eris and Ceres.
Following the IAU decision, Kenyon D. Wilson composed a trombone quintet piece entitled "Songs of Distant Earth." The title comes from Arthur C. Clarke's novel of the same name. The composition contains five movements, one centered on each of the five known dwarf planets: Eris, Pluto, Haumea, Makemake, and Ceres.
"Asteroids".
In 2006, the Berlin Philharmonic, with Sir Simon Rattle and EMI Classics, commissioned four composers (Kaija Saariaho, Matthias Pintscher, Mark-Anthony Turnage, and Brett Dean) and recorded an additional, four-movement suite based on asteroids in the Solar System. The four movements were:
Adaptations of "The Planets".
Hymns.
Holst adapted the melody of the central section of "Jupiter" in 1921 to fit the metre of a poem beginning "I Vow to Thee, My Country". As a hymn tune it has the title "Thaxted", after the town in Essex where Holst lived for many years, and it has also been used for other hymns, such as "O God beyond all praising". It is by far the best-known melody of the suite.
"I Vow to Thee, My Country" was written between 1908 and 1918 by Sir Cecil Spring Rice and became known as a response to the human cost of World War I. The hymn was first performed in 1925 and quickly became a patriotic anthem. Although Holst had no such patriotic intentions when he originally composed the music, these adaptations have encouraged others to draw upon the score in similar ways throughout the 20th Century.
The melody was also adapted and set to lyrics by Charlie Skarbek and titled 'World In Union'. The song is used as the theme song for the Rugby World Cup and appears in most television coverage and before matches.
Notes and references.
Notes
References

</doc>
<doc id="49244" url="https://en.wikipedia.org/wiki?curid=49244" title="NaN">
NaN

In computing, NaN, standing for not a number, is a numeric data type value representing an undefined or unrepresentable value, especially in floating-point calculations. Systematic use of NaNs was introduced by the IEEE 754 floating-point standard in 1985, along with the representation of other non-finite quantities like infinities.
Two separate kinds of NaNs are provided, termed quiet NaNs and signaling NaNs. Quiet NaNs are used to propagate errors resulting from invalid operations or values, whereas signaling NaNs can support advanced features such as mixing numerical and symbolic computation or other extensions to basic floating-point arithmetic. For example, 0/0 is undefined as a real number, and so represented by NaN; the square root of a negative number is imaginary, and thus not representable as a real floating-point number, and so is represented by NaN; and NaNs may be used to represent missing values in computations.
Floating point.
In floating-point calculations, NaN is not the same as infinity, although both are typically handled as special cases in floating-point representations of real numbers as well as in floating-point operations. An invalid operation is also not the same as an arithmetic overflow (which might return an infinity) or an arithmetic underflow (which would return the smallest normal number, a denormal number, or zero).
IEEE 754 NaNs are represented with the exponent field filled with ones (like infinity values), and some non-zero number in the significand (to make them distinct from infinity values); this representation allows the definition of multiple distinct NaN values, depending on which bits are set in the significand, but also on the value of the leading sign bit (not all applications are required to provide distinct semantics for those distinct NaN values).
For example, a bit-wise IEEE floating-point standard single precision (32-bit) NaN would be: s111 1111 1xxx xxxx xxxx xxxx xxxx xxxx where "s" is the sign (most often ignored in applications) and "x" is non-zero (the value zero encodes infinities). Some bits from "x" (usually and preferably the first one) are used to determine the type of NaN: quiet NaN or signaling NaN. The remaining bits encode a payload (most often ignored in applications).
Floating-point operations other than ordered comparisons normally propagate a quiet NaN ("qNaN"). Floating-point operations on a signaling NaN ("sNaN") signal the invalid operation exception, the default exception action is then the same as for qNaN operands and they produce a qNaN if producing a floating-point result.
A comparison with a NaN always returns an "unordered result" even when comparing with itself. The comparison predicates are either signaling or non-signaling; the signaling versions signal the invalid operation exception for such comparisons. The equality and inequality predicates are non-signaling so "x" = "x" returning false can be used to test if "x" is a quiet NaN. The other standard comparison predicates are all signaling if they receive a NaN operand, the standard also provides non-signaling versions of these other predicates. The predicate "isNaN(x)" determines if a value is a NaN and never signals an exception, even if "x" is a signaling NaN.
The propagation of quiet NaNs through arithmetic operations allows errors to be detected at the end of a sequence of operations without extensive testing during intermediate stages. However, note that depending on the language and the function, NaNs can silently be removed in expressions that would give a constant result for all other floating-point values e.g. NaN^0, which may be defined as 1, so in general a later test for a set INVALID flag is needed to detect all cases where NaNs are introduced (see Function definition below for further details).
In section 6.2 of the revised IEEE 754-2008 standard there are two anomalous functions (the maxnum and minnum functions that return the maximum of two operands that are expected to be numbers) that favor numbers — if just one of the operands is a NaN then the value of the other operand is returned.
Operations generating NaN.
There are three kinds of operations that can return NaN:
NaNs may also be explicitly assigned to variables, typically as a representation for missing values. Prior to the IEEE standard, programmers often used a special value (such as −99999999) to represent undefined or missing values, but there was no guarantee that they would be handled consistently or correctly.
NaNs are not necessarily generated in all the above cases. If an operation can produce an exception condition and traps are not masked then the operation will cause a trap instead. If an operand is a quiet NaN, and there isn't also a signaling NaN operand, then there is no exception condition and the result is a quiet NaN. Explicit assignments will not cause an exception even for signaling NaNs.
Quiet NaN.
Quiet NaNs, or qNaNs, do not raise any additional exceptions as they propagate through most operations. The exceptions are where the NaN cannot simply be passed through unchanged to the output, such as in format conversions or certain comparison operations (which do not "expect" a NaN input).
Signaling NaN.
Signaling NaNs, or sNaNs, are special forms of a NaN that when consumed by most operations should raise the invalid operation exception and then, if appropriate, be "quieted" into a qNaN that may then propagate. They were introduced in IEEE 754. There have been several ideas for how these might be used:
When encountered a trap handler could decode the sNaN and return an index to the computed result. In practice this approach is faced with many complications. The treatment of the sign bit of NaNs for some simple operations (such as absolute value) is different from that for arithmetic operations. Traps are not required by the standard. There are other approaches to this sort of problem that would be more portable.
Function definition.
There are differences of opinion about the proper definition for the result of a numeric function that receives a quiet NaN as input. One view is that the NaN should propagate to the output of the function in all cases to propagate the indication of an error. Another view, and the one taken by the ISO C99 and IEEE 754-2008 standards in general, is that if the function has multiple arguments and the output is uniquely determined by all the non-NaN inputs (including infinity), then that value should be the result. Thus for example the value returned by hypot(±∞,qNaN) and hypot(qNaN,±∞) is +∞.
The problem is particularly acute for the exponentiation function pow(x,y) = xy. The expressions 00, ∞0 and 1∞ are considered indeterminate forms when they occur as limits (just like ∞ × 0), and the question of whether zero to the zero power should be defined as 1 has divided opinion.
If the output is considered as undefined when a parameter is undefined, then pow(1,qNaN) should produce a qNaN. However, math libraries have typically returned 1 for pow(1,y) for any real number y, and even when y is an infinity. Similarly, they produce 1 for pow(x,0) even when x is 0 or an infinity. The rationale for returning the value 1 for the indeterminate forms was that the value of functions at singular points can be taken as a particular value if that value is in the limit the value for all but a vanishingly small part of a ball around the limit value of the parameters. The 2008 version of the IEEE 754 standard says that pow(1,qNaN) and pow(qNaN,0) should both return 1 since they return 1 whatever else is used instead of quiet NaN. Moreover, ISO C99, and later IEEE 754-2008, chose to specify pow(−1,±∞) = 1 instead of qNaN; the reason of this choice is given in the C rationale: "Generally, C99 eschews a NaN result where a numerical value is useful. [...] The result of pow(−2,∞) is +∞, because all large positive floating-point values are even integers."
To satisfy those wishing a more strict interpretation of how the power function should act, the 2008 standard defines two additional power functions: pown(x,n), where the exponent must be an integer, and powr(x,y), which returns a NaN whenever a parameter is a NaN or the exponentiation would give an indeterminate form.
Integer NaN.
Most fixed-size integer formats do not have any way of explicitly indicating invalid data. Converting NaN to an integer type, or performing an integer operation whose floating-point equivalent would produce NaN, usually throws an exception. In Java, such operations throw instances of codice_1. In C, they lead to undefined behavior.
Perl's package uses "NaN" for the result of strings that don't represent valid integers.
Display.
Different operating systems and programming languages may have different string representations of NaN.
Since, in practice, encoded NaNs have both a sign and optional 'diagnostic information' (sometimes called a "payload"), these will often be found in string representations of NaNs, too, for example:
Encoding.
In IEEE 754 standard-conforming floating-point storage formats, NaNs are identified by specific, pre-defined bit patterns unique to NaNs. The sign bit does not matter. Binary format NaNs are represented with the exponential field filled with ones (like infinity values), and some non-zero number in the significand field (to make them distinct from infinity values). The original IEEE 754 standard from 1985 (IEEE 754-1985) only described binary floating-point formats, and did not specify how the signaling/quiet state was to be tagged. In practice, the most significant bit of the significand field determined whether a NaN is signaling or quiet. Two different implementations, with reversed meanings, resulted:
The former choice has been preferred as it allows the implementation to quiet a signaling NaN by just setting the signaling/quiet bit to 1. The reverse is not possible with the latter choice because setting the signaling/quiet bit to 0 could yield an infinity.
The 2008 revision of the IEEE 754 standard (IEEE 754-2008) makes formal recommendations for the encoding of the signaling/quiet state.
The state/value of the remaining bits (i.e. other than the ones used to identify a NaN as NaN, including the quiet/signaling bits) are not defined by the standard. This value is called the 'payload' of the NaN. If an operation has a single NaN input and propagates it to the output, the result NaN's payload should be that of the input NaN (this is not always possible for binary formats when the signaling/quiet state is encoded by an 'is_signaling' flag, as explained above). If there are multiple NaN inputs, the result NaN's payload should be from one of the input NaNs; the standard does not specify which.

</doc>
<doc id="49246" url="https://en.wikipedia.org/wiki?curid=49246" title="Biological determinism">
Biological determinism

Biological determinism is the belief that human behavior is controlled solely by an individual's genes or some component of physiology.
In context.
Gender assignment.
Lynda Birke's "In Pursuit of Difference" argues that the discipline of human biology often presents "clear-cut differences" between sexes with regards to chromosomes, genetics, and inheritance. However, while obvious physical differences between males and females exist and develop during puberty, hormonal differences are "not absolute". There is a broad range of reproductive anatomy that doesn't necessarily fit the "gender definition" of male or female. According to the Intersex Society of North America, "a person may be born with mosaic genetics", differing in their chromosomal configuration.
Homosexuality.
Though scientists are unsure as to whether homosexuality can be attributed to biological or social factors, LGBT rights activists have used the theories of biological determinism to support their cause. This has become a frequent point of dissension between pro-gay individuals and anti-gay individuals. Because a single cause has not been determined as the cause of homosexuality, many scholars theorize that a combination of biological and social causes determine one's sexual orientation. Gay rights advocates believe that proving that homosexuality has a definite biological basis will prove it to be an unchangeable characteristic, thus allowing homosexuals to be protected under the Fourteenth Amendment in the United States. One area of research that has been a valuable tool for gay rights activists has been Dean Hamer's work studying the "gay gene". Another researcher who worked with Hamer in finding evidence for biological influence in male homosexuality was Simon LeVay, a neuroscientist. In 1991, LeVay published an article in "Science" journal that detailed the difference in hypothalamic structures between homosexual and heterosexual men. His findings in studying the INAH-3 implied that "sexual orientation has a biological substrate". Though his research showed that there was a biological basis in sexual orientation, LeVay cautioned against people interpreting his article to say that he found that homosexuality is genetic, emphasizing that he had not "locate a gay center in the brain-- INAH3 is less likely to be the sole gay nucleus of the brain than part of a chain of nuclei engaged in men and women's sexual behavior." He merely hoped that his work would serve as a catalyst in working towards finding more evidence that homosexuality is at least partly genetic.
Sociobiology.
Sociobiology emerged in the 1970s with E. O. Wilson's book "". The existence of a putative altruism gene is debated. Kaplan and Rogers claim "Most sociobiologists agree that no such gene could exist for so long in a population as it would soon be lost because it would not compete successfully against the 'selfish' genes", and argue that "Genes and environment are not discrete opposites; they are both entirely integrated aspects of the developmental process." Consequently, genes can’t be "selfish", as "genes are expressed as biochemical processes; behavior is expressed by the whole organism."
Social construction.
Richard Lewontin, Steven Rose, and Leon Kamin were interested in the way that biological determinism was present in science. They wanted to figure out how much of it was true, and how much of it was socially constructed according to certain beliefs and societal norms and determined gender roles within society. In their book "Not in Our Genes" they explore the possibilities of biological determinism. In their studies, they found some very interesting evidence that points to the fact that biological determinism in science is actually greatly affected by certain norms and tendencies within society. According to them, biological determinism is more constructed by society than by anything else. In a study that was performed on girls who were relatively "masculinized", biological determinists John Money and Anke Ehrhardt looked for ways to describe femininity that fit into the common definition of it, such as clothing preference or using makeup. Although these scientists believed that they were providing evidence to support their definitions of femininity within nature, they fell into the trap of labeling these girls according to Western social standards. As Lewontin points out, this experiment not only embraces the stereotypes that already existed, but it also “ignores the existence of societies in which women wear pants, or in which men wear skirts, or in which men enjoy and appropriate jewelry to themselves.” Lewontin, Rose, and Kamin realize that biological determinism is clouded and, can in fact, be shaped according to the standards and norms of the society one lives in. Therefore, they choose to take a different approach. They decide to look at numbers and statistics instead of simple social experiments which can be easily misinterpreted. When they look at the numbers and statistics of men and women over the years, they discover that the differences between men and women are no longer as pronounced as they had been in the past. All of a sudden, there are more women in the work place holding higher ranking jobs. More women are excelling in areas that used to be male dominant, such as sports. And, even biologically, women are beginning to catch up to men in height while men are beginning to catch up in life expectancy. However, these changes are mostly visible in numbers and statistics, and social differences between men and women are still easily observed. Lewontin, Rose, and Kamin argue, however, that these differences are imposed by society itself.
The standard model for the difference between sex/gender states that there is a clear-cut dichotomy between males & females, with no overlap. Because of this norm, we have a historically constructed viewpoint of “average”, meaning that society holds the idea that one must be either male or female, feminine or masculine. Anne Fausto-Sterling's article “Of Gender and Genitals” discusses how this standard model shapes doctors’ ideas about gender and what is socially acceptable. She claims that (according to the standard model) “Bodies in the "normal" range are culturally intelligible as males or females, but the rules for living as male or female are strict”, meaning that we are culturally “trained” in believing that there is a sexual binary and anything outside of those confines is rejected.

</doc>
<doc id="49248" url="https://en.wikipedia.org/wiki?curid=49248" title="Byron White">
Byron White

Byron Raymond "Whizzer" White (June 8, 1917 – April 15, 2002) won fame both as a football halfback and as an associate justice of the Supreme Court of the United States. Born and raised in Colorado, White played in the National Football League for three seasons and practiced law for 15 years before his Supreme Court appointment. White was the Colorado state chair of John F. Kennedy's 1960 presidential campaign.
White was appointed to the Supreme Court by Kennedy in 1962. He viewed his own court decisions as based on the facts of each case rather than as representative of a specific legal philosophy. He retired in 1993 and is the twelfth longest-serving justice in Supreme Court history. He died in Denver at the age of 84. He was the first Supreme Court Justice from the state of Colorado.
Education.
White was born in Fort Collins, Colorado, the son of Maude Elizabeth (Burger) and Alpha Albert White. He was raised in the nearby town of Wellington, Colorado, where he obtained his high school diploma in 1930. After graduating at the top of his high school class, White attended the University of Colorado at Boulder on a scholarship. He joined the Phi Gamma Delta fraternity and served as student body president his senior year. Graduating in 1938, he won a Rhodes Scholarship to the University of Oxford and, after having deferred it for a year to play football, he went on to attend Hertford College, Oxford.
Football.
White was an All-American football halfback for the Colorado Buffaloes of the University of Colorado at Boulder, where he acquired the nickname "Whizzer" from a newspaper columnist. The nickname would follow him throughout his later legal and Supreme Court career, to White's chagrin. He also played basketball and baseball. After graduation he signed with the NFL's Pittsburgh Pirates (now Steelers), after they selected him in Round 1 of the 1938 NFL Draft, and he played there during the 1938 season. He led the league in rushing in his rookie season and became the game's highest-paid player.
After Oxford, White played for the Detroit Lions from 1940 to 1941. In three NFL seasons, he played in 33 games. He led the league in rushing yards in 1938 and 1940, and he was one of the first "big money" NFL players, making $15,000 a year.
His career was cut short when he entered the United States Navy during World War II; after the war, he elected to attend law school rather than return to football. He was elected to the College Football Hall of Fame in 1954.
Military service.
During World War II, White served as an intelligence officer in the United States Navy stationed in the Pacific Theatre. He had originally wanted to join the Marines but was kept out due to being colorblind. He wrote the intelligence report on the sinking of future President John F. Kennedy's "PT-109". White was awarded two Bronze Star medals.
Personal life.
White first met his wife Marion (died January 2009), the daughter of the president of the University of Colorado, when she was in high school and he was a college football star. During World War II, Marion served in the WAVES while her future husband was a Navy intelligence officer. They married in 1946 and had two children: a son named Charles and a daughter named Nancy. At the time of his death, White and his wife had moved back to Colorado and were living in Denver.
Legal career.
After World War II, he attended Yale Law School, graduating "magna cum laude" in 1946.
After serving as a law clerk to Chief Justice Fred Vinson, White returned to Denver.
White practiced in Denver for roughly fifteen years with the law firm now known as Davis Graham & Stubbs. This was a time in which the Denver business community flourished, and White rendered legal service to that flourishing community. White was for the most part a transactional attorney. He drafted contracts and advised insolvent companies, and he argued the occasional case in court.
During the 1960 presidential election, White put his football celebrity to use as chair of John F. Kennedy's campaign in Colorado. White had first met the candidate when White was a Rhodes scholar and Kennedy's father, Joseph Kennedy, was Ambassador to the Court of St. James. During the Kennedy administration, White served as United States Deputy Attorney General, the number two man in the Justice Department, under Robert F. Kennedy. He took the lead in protecting the Freedom Riders in 1961, negotiating with Alabama Governor John Malcolm Patterson.
Supreme Court.
Acquiring renown within the Kennedy Administration for his humble manner and sharp mind, he was appointed by Kennedy in 1962 to succeed Justice Charles Evans Whittaker, who retired for disability. Kennedy said at the time: "He has excelled at everything. And I know that he will excel on the highest court in the land." The 44-year-old White was approved by a voice vote. He would serve until his retirement in 1993. His Supreme Court tenure was the fourth-longest of the 20th century.
Upon the request of Vice President-Elect Al Gore, Justice White administered the oath of office on January 20, 1993 to the 45th U.S. Vice President. It was the only time White administered an oath of office to a Vice President.
During his service on the high court, White wrote 994 opinions. He was fierce in questioning attorneys in court, and his votes and opinions on the bench reflect an ideology that has been notoriously difficult for popular journalists and legal scholars alike to pin down. He was seen as a disappointment by some Kennedy supporters who wished he would have joined the more liberal wing of the court in its opinions on "Miranda v. Arizona" and "Roe v. Wade".
White often took a narrow, fact-specific view of cases before the Court and generally refused to make broad pronouncements on constitutional doctrine or adhere to a specific judicial philosophy. He preferred to take what he viewed as a practical approach to the law to one based in any legal philosophy. In the tradition of the New Deal, White frequently supported a broad view and expansion of governmental powers. He consistently voted against creating constitutional restrictions on the police, dissenting in the landmark 1966 case of "Miranda v. Arizona". In his dissent in that case he noted that aggressive police practices enhance the individual rights of law-abiding citizens. His jurisprudence has sometimes been praised for adhering to the doctrine of judicial restraint.
Substantive due process doctrine.
Frequently a critic of the doctrine of "substantive due process", which involves the judiciary reading substantive content into the term "liberty" in the Due Process Clause of the Fifth Amendment and Fourteenth Amendment, White's first published opinion as a Supreme Court Justice, a sole dissent in "Robinson v. California" (1962), foreshadowed his career-long distaste for the doctrine. In "Robinson", he criticized the remainder of the Court's unprecedented expansion of the Eighth Amendment's prohibition of "cruel and unusual punishment" to strike down a California law providing for civil commitment of drug addicts. He argued that the Court was "imposing its own philosophical predilections" on the state in this exercise of judicial power, although its historic "allergy to substantive due process" would never permit it to strike down a state's economic regulatory law in such a manner.
In the same vein, he dissented in the controversial 1973 case of "Roe v. Wade". But White voted to strike down a state ban on contraceptives in the 1965 case of "Griswold v. Connecticut", although he did not join the majority opinion, which famously asserted a "right of privacy" on the basis of the "penumbras" of the Bill of Rights. White and Justice William Rehnquist were the only dissenters from the Court's decision in "Roe", though White's dissent used stronger language, suggesting that "Roe" was "an exercise in raw judicial power" and criticizing the decision for "interposing a constitutional barrier to state efforts to protect human life." White, who usually adhered firmly to the doctrine of "stare decisis", remained a critic of "Roe" throughout his term on the bench.
White explained his general views on the validity of substantive due process at length in his dissent in "Moore v. City of East Cleveland":
The Judiciary, including this Court, is the most vulnerable and comes nearest to illegitimacy when it deals with judge-made constitutional law having little or no cognizable roots in the language or even the design of the Constitution. Realizing that the present construction of the Due Process Clause represents a major judicial gloss on its terms, as well as on the anticipation of the Framers, and that much of the underpinning for the broad, substantive application of the Clause disappeared in the conflict between the Executive and the Judiciary in 1930s and 1940s, the Court should be extremely reluctant to breathe still further substantive content into the Due Process clause so as to strike down legislation adopted by a State or city to promote its welfare. Whenever the Judiciary does so, it unavoidably pre-empts for itself another part of the governance of the country without express constitutional authority.
White parted company with Rehnquist in strongly supporting the Supreme Court decisions striking down laws that discriminated on the basis of sex, agreeing with Justice William J. Brennan in 1973's "Frontiero v. Richardson" that laws discriminating on the basis of sex should be subject to strict scrutiny. However, only three justices joined Brennan's plurality opinion in "Frontiero"; in later cases gender discrimination cases would be subjected to intermediate scrutiny (see "Craig v. Boren").
White wrote the majority opinion in "Bowers v. Hardwick" (1986), which upheld Georgia's anti-sodomy law against a substantive due process attack.
The Court is most vulnerable and comes nearest to illegitimacy when it deals with judge-made constitutional law having little or no cognizable roots in the language or design of the Constitution... There should be, therefore, great resistance to ... redefining the category of rights deemed to be fundamental. Otherwise, the Judiciary necessarily takes to itself further authority to govern the country without express constitutional authority.
White's opinion in "Bowers" typified White's fact-specific, deferential style of deciding cases: White's opinion treated the issue in that case as presenting only the question of whether homosexuals had a fundamental right to engage in sexual activity, even though the statute in "Bowers" potentially applied to heterosexual sodomy (see "Bowers", 478 U.S. 186, 188, n. 1. Georgia, however, conceded during oral argument that the law would be inapplicable to married couples under the precedent set forth in "Griswold v. Connecticut".). A year after White's death, "Bowers" was overruled in "Lawrence v. Texas" (2003).
Death penalty.
White took a middle course on the issue of the death penalty: he was one of five justices who voted in "Furman v. Georgia" (1972) to strike down several state capital punishment statutes, voicing concern over the arbitrary nature in which the death penalty was administered. The Furman decision ended capital punishment in the U.S. until 1977, when Gary Gilmore, who decided not to appeal his death sentence, was executed by firing squad. White, however, was not against the death penalty in all forms: he voted to uphold the death penalty statutes at issue in "Gregg v. Georgia" (1976), even the mandatory death penalty schemes struck down by the Court.
White accepted the position that the Eighth Amendment to the United States Constitution required that all punishments be "proportional" to the crime; thus, he wrote the opinion in "Coker v. Georgia" (1977), which invalidated the death penalty for rape of a 16-year-old married girl. However, his first reported Supreme Court decision was a dissent in "Robinson v. California" (1962), in which he criticized the Court for extending the reach of the Eighth Amendment. In "Robinson" the Court for the first time expanded the constitutional prohibition of “cruel and unusual punishments” from examining the nature of the punishment imposed and whether it was an uncommon punishment − as, for example, in the cases of flogging, branding, banishment, or electrocution − to deciding whether any punishment at all was appropriate for the defendant’s conduct. White said: “If this case involved economic regulation, the present Court's allergy to substantive due process would surely save the statute and prevent the Court from imposing its own philosophical predilections upon state legislatures or Congress.” Consistent with his view in "Robinson", White thought that imposing the death penalty on minors was constitutional, and he was one of the three dissenters in "Thompson v. Oklahoma" (1988), a decision that declared that the death penalty as applied to offenders below 16 years of age was unconstitutional as a cruel and unusual punishment.
Abortion.
Along with Justice William Rehnquist, White dissented in "Roe v. Wade" (the dissenting decision was in the companion case, "Doe v. Bolton"), castigating the majority for holding that the U.S. Constitution "values the convenience, whim or caprice of the putative mother more than the life or potential life of the fetus."
Civil rights.
White consistently supported the Court's post-"Brown v. Board of Education" attempts to fully desegregate public schools, even through the controversial line of forced busing cases. He voted to uphold affirmative action remedies to racial inequality in an education setting in the famous "Regents of the University of California v. Bakke" case of 1978. Though White voted to uphold federal affirmative action programs in cases such as "Metro Broadcasting, Inc. v. FCC", 497 U.S. 547 (1990) (later overruled by "Adarand Constructors v. Peña", 515 U.S. 200 (1995)), White voted to strike down an affirmative action plan regarding state contracts in "Richmond v. J.A. Croson Co." (1989).
White dissented in "Runyon v. McCrary" (1976), which held that federal law prohibited private schools from discriminating on the basis of race. White argued that the legislative history of Title 42 U.S.C. § 1981 (popularly known as the "Ku Klux Klan Act") indicated that the Act was not designed to prohibit private racial discrimination, but only state-sponsored racial discrimination (as had been held in the "Civil Rights Cases" of 1883). White was concerned about the potential far-reaching impact of holding private racial discrimination illegal, which if taken to its logical conclusion might ban many varied forms of voluntary self-segregation, including social and advocacy groups that limited their membership to blacks: "Whether such conduct should be condoned or not, whites and blacks will undoubtedly choose to form a variety of associational relationships pursuant to contracts which exclude members of the other race. Social clubs, black and white, and associations designed to further the interests of blacks or whites are but two examples". "Runyon" was essentially overruled by 1989's "Patterson v. McLean Credit Union", which itself was superseded by the Civil Rights Act of 1991.
Relationships with other justices.
White said he was most comfortable on Rehnquist's court. He once said of Earl Warren, "I wasn't exactly in his circle." On the Burger Court, the Chief Justice was fond of assigning important criminal procedure and individual rights opinions to White, because of his frequently conservative views on these questions.
Court operations and retirement.
White frequently urged the Supreme Court to consider cases when federal appeals courts were in conflict on issues of federal law, believing that a primary role of the Supreme Court was to resolve such conflicts. Thus, White voted to grant certiorari more often than many of his colleagues, and he wrote numerous opinions dissenting from denials of certiorari. After White (along with fellow Justice Harry Blackmun, who also took a liberal line in voting to grant certiorari) retired, the number of cases heard each session of the Court declined steeply.
White disliked the politics of Supreme Court appointments. He retired in 1993, during Bill Clinton's presidency, saying that "someone else should be permitted to have a like experience." Clinton appointed Justice Ruth Bader Ginsburg, a judge from the Court of Appeals for the D.C. Circuit and a former Columbia University law professor, to succeed him.
Later years and death.
After retiring from the Supreme Court, White occasionally sat with lower federal courts. He maintained chambers in the federal courthouse in Denver until shortly before his death. He also served for the Commission on Structural Alternatives for the Federal Courts of Appeals.
White died of pneumonia on April 15, 2002 at the age of 84. He was the last living Warren Court Justice, and died the day before the fortieth anniversary of his swearing in as a Justice. From his death until the retirement of Sandra Day O'Connor, there were no living former Justices.
His remains are interred at All Souls Walk at the St. John's Cathedral in Denver.
Then-Chief Justice Rehnquist said White "came as close as anyone I have known to meriting Matthew Arnold's description of Sophocles: 'He saw life steadily and he saw it whole.' All of us who served with him will miss him."
Awards and honors.
The NFL Players Association gives the Byron "Whizzer" White NFL Man of the Year Award to one player each year for his charity work. Michael McCrary, who was involved in "Runyon v. McCrary", grew up to be a professional football player and won the award in 2000.
The federal courthouse in Denver that houses the Tenth Circuit is named after White.
White was posthumously awarded the Presidential Medal of Freedom in 2003 by President George W. Bush.
White was inducted into the Rocky Mountain Athletic Conference Hall of Fame on July 14, 2007, in addition to being a member of the College Football Hall of Fame and the University of Colorado's Athletic Hall of Fame, where he is enshrined as "The Greatest Buff Ever".
One of White's former law clerks, Dennis J. Hutchinson, wrote an unofficial biography of him called "The Man Who Once was Whizzer White".

</doc>
<doc id="49251" url="https://en.wikipedia.org/wiki?curid=49251" title="Dili">
Dili

Dili () is the capital, largest city, chief port and commercial centre of East Timor.
Geography and administration.
Dili lies on the northern coast of Timor island, the easternmost of the Lesser Sunda Islands. It is the seat of the administration of the district of Dili, which is the administrative entity of the area and includes the island of Atauro and some cities close to Dili city. The city is divided into the subdistricts of Nain Feto, Vera Cruz, Dom Aleixo and Cristo Rei and is divided into several sucos, which are headed by an elected "chefe de suco". 18 of the 26 sucos of the four subdistricts are categorised as urban.
There is no city administration beside the district administrator, who was appointed by state government. The East Timorese government started to plan in 2009 to change the status of districts into municipalities. These will have an elected mayor and council.
Demography.
The 2010 census recorded a population of 193,563 in the areas of Dili district classified as urban, with a population of 234,331 in the whole district including rural areas such as Atauro and Metinaro.
Dili is a melting pot of the different ethnic groups of East Timor, due partly to the internal migration of young men from around the country in search of work. This has led to a gender imbalance, with the male population significantly larger than the female. Between 2001 and 2004, the population of Dili district grew by 12.58%, with only 54% of the district's inhabitants born in the city. 7% were born in Bacau, 5% each in Viqueque and Bobonaro 4% in Ermera, and the remainder in other districts or overseas.
Climate.
Dili has a Tropical wet and dry climate under the Köppen climate classification.
History.
Dili was settled about 1520 by the Portuguese, who made it the capital of Portuguese Timor in 1769. It was proclaimed a city in January 1864. During World War II, Portugal and its colonies remained neutral, but the Allies saw East Timor as a potential target for Japanese invasion, and Australian and Dutch forces briefly occupied the island in 1941. In the night of 19 February 1942, the Japanese attacked with a force of around 20,000 men, and occupied Dili before spreading out across the rest of the colony. On 26 September 1945, control of the island was officially returned to Portugal by the Japanese.
East Timor unilaterally declared independence from Portugal on 28 November 1975. However, nine days later, on 7 December, Indonesian forces invaded Dili. On 17 July 1976, Indonesia annexed East Timor, which it designated the 27th province of Indonesia, "Timor Timur" (Indonesian for "East Timor"), with Dili as its capital. A guerrilla war ensued from 1975 to 1999 between Indonesian and pro-independence forces, during which tens of thousands of East Timorese and some foreign civilians were killed. Media coverage of the 1991 Dili Massacre helped revitalise international support for the East Timorese independence movement.
In 1999, East Timor was placed under UN supervision and on 20 May 2002, Dili became the capital of the newly independent Democratic Republic of Timor-Leste. In May 2006, fighting and rioting sparked by conflict between elements of the military caused significant damage to the city and led to foreign military intervention to restore order.
Buildings and monuments.
Most buildings were damaged or destroyed in the violence of 1999, orchestrated by the Indonesian military and local pro-Indonesia militias (see Operation Scorched Earth). However, the city still has many buildings from the Portuguese era. The former Portuguese Governor's office is now the office of the Prime Minister. It was previously also used by the Indonesian-appointed Governor, and by the United Nations Transitional Administration in East Timor (UNTAET).
Even under Indonesian rule, during which the Portuguese language was banned, Portuguese street names like "Avenida Marechal Carmona" remained unchanged, although they were prefixed with the Indonesian word "Jalan" or 'road'. The Roman Catholic Church at Motael became a focus for resistance to Indonesian occupation. Legacies of Jakarta's occupation are the Church of the Immaculate Conception, seat of the Roman Catholic Diocese of Díli, purportedly the largest cathedral in Southeast Asia, and the 'Integration Monument', commemorating the Indonesian annexation of the territory in 1976. Featuring a statue of an East Timorese in traditional dress, breaking the chains round his wrists, the monument has not been demolished.
The Cristo Rei of Dili is a 27-metre (88.6 ft) tall statue of Jesus situated on top of a globe at the end of a peninsula in Dili. It is one of the town's landmarks. It was a present from the Indonesian Government during occupation for the 20th anniversary of East Timor's integration into Indonesia.
Education.
Schools in Dili include St. Joseph’s High School (Colégio de São José).
There are four International schools in Dili, a Portuguese school by the name of Escola Portuguesa Ruy Cinatti, an Australian managed school by the name of Dili International School, an American government sponsored school called QSI International School of Dili and the Maharlika International School (Formerly Dili Education & Development Center), a Philippine International School. East Timor's major higher education institution, the Universidade Nacional de Timor-Leste, is based in Dili.
Transportation.
Dili is served by Presidente Nicolau Lobato International Airport, named after independence leader Nicolau Lobato. This is the only functioning international airport in East Timor, though there are airstrips in Baucau, Suai and Oecusse used for domestic flights. Until recently, Dili's airport runway has been unable to accommodate aircraft larger than the Boeing 737 or C-130 Hercules, but in January 2008, the Portuguese charter airline EuroAtlantic Airways operated a direct flight from Lisbon using a Boeing 757, carrying 140 members of the Guarda Nacional Republicana.
Under Portuguese rule, Baucau Airport, which has a much longer runway, was used for international flights, but following the Indonesian invasion this was taken over by the Indonesian military and closed to civilian traffic.
Twin towns – Sister cities.
Dili is twinned with the following places:

</doc>
<doc id="49253" url="https://en.wikipedia.org/wiki?curid=49253" title="Urysohn's lemma">
Urysohn's lemma

In topology, Urysohn's lemma is a lemma that states that a topological space is normal if and only if any two disjoint closed subsets can be separated by a function.
Urysohn's lemma is commonly used to construct continuous functions with various properties on normal spaces. It is widely applicable since all metric spaces and all compact Hausdorff spaces are normal. The lemma is generalized by (and usually used in the proof of) the Tietze extension theorem.
The lemma is named after the mathematician Pavel Samuilovich Urysohn.
Formal statement.
Two disjoint closed subsets "A" and "B" of a topological space "X" are said to be separated by neighbourhoods if there are neighbourhoods "U" of "A" and "V" of "B" that are also disjoint. "A" and "B" are said to be separated by a function if there exists a continuous function "f" from "X" into the unit interval [0,1] such that "f"("a") = 0 for all "a" in "A" and "f"("b") = 1 for all "b" in "B". Any such function is called a Urysohn function for "A" and "B".
A normal space is a topological space in which any two disjoint closed sets can be separated by neighbourhoods. Urysohn's lemma states that a topological space is normal if and only if any two disjoint closed sets can be separated by a continuous function.
The sets "A" and "B" need not be precisely separated by "f", i.e., we do not, and in general cannot, require that "f"("x") ≠ 0 and ≠ 1 for "x" outside of "A" and "B". This is possible only in perfectly normal spaces.
Urysohn's lemma has led to the formulation of other topological properties such as the 'Tychonoff property' and 'completely Hausdorff spaces'. For example, a corollary of the lemma is that normal "T"1 spaces are Tychonoff.
Sketch of proof.
For every dyadic fraction "r" ∈ (0,1), we are going to construct an open subset "U"("r") of "X" such that:
Once we have these sets, we define "f"("x") = 1 if "x" ∉ "U"("r") for any "r"; otherwise "f"("x") = inf { "r" : "x" ∈ "U"("r") } for every "x" ∈ "X". Using the fact that the dyadic rationals are dense, it is then not too hard to show that "f" is continuous and has the property "f"("A") ⊆ {0} and "f"("B") ⊆ {1}.
In order to construct the sets "U"("r"), we actually do a little bit more: we construct sets "U"("r") and "V"("r") such that
Since the complement of "V"("r") is closed and contains "U"("r"), the latter condition then implies condition (2) from above.
This construction proceeds by mathematical induction. First define "U"(1) = "X" \ "B" and "V"(0) = "X" \ "A". Since "X" is normal, we can find two disjoint open sets "U"(1/2) and "V"(1/2) which contain "A" and "B", respectively. Now assume that "n"≥1 and the sets "U"("k"/2"n") and "V"("k"/2"n") have already been constructed for "k" = 1...,2"n"-1. Since "X" is normal, for any "a" ∈ { 0,1...,2"n"-1 }, we can find two disjoint open sets which contain "X" \ "V"("a"/2"n") and "X" \ "U"(("a"+1)/2"n"), respectively. Call these two open sets "U"((2"a"+1)/2"n"+1) and "V"((2"a"+1)/2"n"+1), and verify the above three conditions.
The Mizar project has completely formalized and automatically checked a proof of Urysohn's lemma in the URYSOHN3 file.

</doc>
<doc id="49256" url="https://en.wikipedia.org/wiki?curid=49256" title="Age of the Earth">
Age of the Earth

The age of the Earth is 4.54 ± 0.05 billion years This dating is based on evidence from radiometric age dating of meteorite material and is consistent with the radiometric ages of the oldest-known terrestrial and lunar samples.
Following the development of radiometric age dating in the early 20th century, measurements of lead in uranium-rich minerals showed that some were in excess of a billion years old.For the abstract, see: </ref>
The oldest such minerals analyzed to date—small crystals of zircon from the Jack Hills of Western Australia—are at least 4.404 billion years old. Comparing the mass and luminosity of the Sun to those of other stars, it appears that the Solar System cannot be much older than those rocks. Calcium-aluminium-rich inclusions  – the oldest known solid constituents within meteorites that are formed within the Solar System – are 4.567 billion years old, giving an age for the solar system and an upper limit for the age of Earth.
It is hypothesised that the accretion of Earth began soon after the formation of the calcium-aluminium-rich inclusions and the meteorites. Because the exact amount of time this accretion process took is not yet known, and the predictions from different accretion models range from a few million up to about 100 million years, the exact age of Earth is difficult to determine. It is also difficult to determine the exact age of the oldest rocks on Earth, exposed at the surface, as they are aggregates of minerals of possibly different ages.
Development of modern geologic concepts.
Studies of strata, the layering of rocks and earth, gave naturalists an appreciation that Earth may have been through many changes during its existence. These layers often contained fossilized remains of unknown creatures, leading some to interpret a progression of organisms from layer to layer.
Nicolas Steno in the 17th century was one of the first naturalists to appreciate the connection between fossil remains and strata. His observations led him to formulate important stratigraphic concepts (i.e., the "law of superposition" and the "principle of original horizontality"). In the 1790s, William Smith hypothesized that if two layers of rock at widely differing locations contained similar fossils, then it was very plausible that the layers were the same age. William Smith's nephew and student, John Phillips, later calculated by such means that Earth was about 96 million years old.
The naturalist Mikhail Lomonosov suggested in the mid-18th century that Earth had been created separately from the rest of the universe, several hundred thousand years before. Lomonosov's ideas were mostly speculative. In 1779 the Comte du Buffon tried to obtain a value for the age of Earth using an experiment: He created a small globe that resembled Earth in composition and then measured its rate of cooling. This led him to estimate that Earth was about 75,000 years old.
Other naturalists used these hypotheses to construct a history of Earth, though their timelines were inexact as they did not know how long it took to lay down stratigraphic layers. In 1830, geologist Charles Lyell, developing ideas found in James Hutton's works, popularized the concept that the features of Earth were in perpetual change, eroding and reforming continuously, and the rate of this change was roughly constant. This was a challenge to the traditional view, which saw the history of Earth as static, with changes brought about by intermittent catastrophes. Many naturalists were influenced by Lyell to become "uniformitarians" who believed that changes were constant and uniform.
Early calculations.
In 1862, the physicist Lord Kelvin published calculations that fixed the age of Earth at between 20 million and 400 million years.
He assumed that Earth had formed as a completely molten object, and determined the amount of time it would take for the near-surface to cool to its present temperature. His calculations did not account for heat produced via radioactive decay (a process then unknown to science) or, more significantly, convection inside the Earth, which allows more heat to escape from the interior to warm rocks near the surface.
Geologists such as Charles Lyell had trouble accepting such a short age for Earth. For biologists, even 100 million years seemed much too short to be plausible. In Darwin's theory of evolution, the process of random heritable variation with cumulative selection requires great durations of time. (According to modern biology, the total evolutionary history from the beginning of life to today has taken since 3.5 to 3.8 billion years ago, the amount of time which passed since the last universal ancestor of all living organisms as shown by geological dating.)
In a lecture in 1869, Darwin's great advocate, Thomas H. Huxley, attacked Thomson's calculations, suggesting they appeared precise in themselves but were based on faulty assumptions. The physicist Hermann von Helmholtz (in 1856) and astronomer Simon Newcomb (in 1892) contributed their own calculations of 22 and 18 million years respectively to the debate: they independently calculated the amount of time it would take for the Sun to condense down to its current diameter and brightness from the nebula of gas and dust from which it was born. Their values were consistent with Thomson's calculations. However, they assumed that the Sun was only glowing from the heat of its gravitational contraction. The process of solar nuclear fusion was not yet known to science.
In 1895 John Perry challenged Kelvin's figure on the basis of his assumptions on conductivity, and Oliver Heaviside entered the dialogue, considering it "a vehicle to display the ability of his operator method to solve problems of astonishing complexity."
Other scientists backed up Thomson's figures. Charles Darwin's son, the astronomer George H. Darwin, proposed that Earth and Moon had broken apart in their early days when they were both molten. He calculated the amount of time it would have taken for tidal friction to give Earth its current 24-hour day. His value of 56 million years added additional evidence that Thomson was on the right track.
The last estimate Thomson gave, in 1897, was: "that it was more than 20 and less than 40 million year old, and probably much nearer 20 than 40". In 1899 and 1900, John Joly calculated the rate at which the oceans should have accumulated salt from erosion processes, and determined that the oceans were about 80 to 100 million years old.
Radiometric dating.
Overview.
By their chemical nature, rock minerals contain certain elements and not others; but in rocks containing radioactive isotopes, the process of radioactive decay generates exotic elements over time. By measuring the concentration of the stable end product of the decay, coupled with knowledge of the half life and initial concentration of the decaying element, the age of the rock can be calculated. Typical radioactive end products are argon from decay of potassium-40, and lead from decay of uranium and thorium. If the rock becomes molten, as happens in Earth's mantle, such nonradioactive end products typically escape or are redistributed. Thus the age of the oldest terrestrial rock gives a minimum for the age of Earth, assuming that no rock has been intact for longer than the Earth itself.
Convective mantle and radioactivity.
In 1892, Thomson had been made Lord Kelvin in appreciation of his many scientific accomplishments. Kelvin calculated the age of Earth by using thermal gradients, and arrived at an estimate of 100 million years old. He did not realize that Earth has a highly viscous fluid mantle, and this invalidated his estimate. In 1895, John Perry produced an age-of-Earth estimate of 2 to 3 billion years using a model of a convective mantle and thin crust. Kelvin stuck by his estimate of 100 million years, and later reduced it to about 20 million years.
The discovery of radioactivity introduced another factor in the calculation. After Henri Becquerel's initial discovery in 1896, Marie and Pierre Curie discovered the radioactive elements polonium and radium in 1898; and in 1903, Pierre Curie and Albert Laborde announced that radium produces enough heat to melt its own weight in ice in less than an hour. Geologists quickly realized that this upset the assumptions underlying most calculations of the age of Earth.
These had assumed that the original heat of the Earth and Sun had dissipated steadily into space, but radioactive decay meant that this heat had been continually replenished. George Darwin and John Joly were the first to point this out, in 1903.
Invention of radiometric dating.
Radioactivity, which had overthrown the old calculations, yielded a bonus by providing a basis for new calculations, in the form of radiometric dating.
Ernest Rutherford and Frederick Soddy jointly had continued their work on radioactive materials and concluded that radioactivity was due to a spontaneous transmutation of atomic elements. In radioactive decay, an element breaks down into another, lighter element, releasing alpha, beta, or gamma radiation in the process. They also determined that a particular isotope of a radioactive element decays into another element at a distinctive rate. This rate is given in terms of a "half-life", or the amount of time it takes half of a mass of that radioactive material to break down into its "decay product".
Some radioactive materials have short half-lives; some have long half-lives. Uranium and thorium have long half-lives, and so persist in Earth's crust, but radioactive elements with short half-lives have generally disappeared. This suggested that it might be possible to measure the age of Earth by determining the relative proportions of radioactive materials in geological samples. In reality, radioactive elements do not always decay into nonradioactive ("stable") elements directly, instead, decaying into other radioactive elements that have their own half-lives and so on, until they reach a stable element. Such "decay series", such as the uranium-radium and thorium series, were known within a few years of the discovery of radioactivity, and provided a basis for constructing techniques of radiometric dating.
The pioneers of radioactivity were chemist Bertram B. Boltwood and the energetic Rutherford. Boltwood had conducted studies of radioactive materials as a consultant, and when Rutherford lectured at Yale in 1904, Boltwood was inspired to describe the relationships between elements in various decay series. Late in 1904, Rutherford took the first step toward radiometric dating by suggesting that the alpha particles released by radioactive decay could be trapped in a rocky material as helium atoms. At the time, Rutherford was only guessing at the relationship between alpha particles and helium atoms, but he would prove the connection four years later.
Soddy and Sir William Ramsay had just determined the rate at which radium produces alpha particles, and Rutherford proposed that he could determine the age of a rock sample by measuring its concentration of helium. He dated a rock in his possession to an age of 40 million years by this technique. Rutherford wrote,
Rutherford assumed that the rate of decay of radium as determined by Ramsay and Soddy was accurate, and that helium did not escape from the sample over time. Rutherford's scheme was inaccurate, but it was a useful first step.
Boltwood focused on the end products of decay series. In 1905, he suggested that lead was the final stable product of the decay of radium. It was already known that radium was an intermediate product of the decay of uranium. Rutherford joined in, outlining a decay process in which radium emitted five alpha particles through various intermediate products to end up with lead, and speculated that the radium-lead decay chain could be used to date rock samples. Boltwood did the legwork, and by the end of 1905 had provided dates for 26 separate rock samples, ranging from 92 to 570 million years. He did not publish these results, which was fortunate because they were flawed by measurement errors and poor estimates of the half-life of radium. Boltwood refined his work and finally published the results in 1907.
Boltwood's paper pointed out that samples taken from comparable layers of strata had similar lead-to-uranium ratios, and that samples from older layers had a higher proportion of lead, except where there was evidence that lead had leached out of the sample. His studies were flawed by the fact that the decay series of thorium was not understood, which led to incorrect results for samples that contained both uranium and thorium. However, his calculations were far more accurate than any that had been performed to that time. Refinements in the technique would later give ages for Boltwood's 26 samples of 410 million to 2.2 billion years.
Arthur Holmes establishes radiometric dating.
Although Boltwood published his paper in a prominent geological journal, the geological community had little interest in radioactivity. Boltwood gave up work on radiometric dating and went on to investigate other decay series. Rutherford remained mildly curious about the issue of the age of Earth but did little work on it.
Robert Strutt tinkered with Rutherford's helium method until 1910 and then ceased. However, Strutt's student Arthur Holmes became interested in radiometric dating and continued to work on it after everyone else had given up. Holmes focused on lead dating, because he regarded the helium method as unpromising. He performed measurements on rock samples and concluded in 1911 that the oldest (a sample from Ceylon) was about 1.6 billion years old. These calculations were not particularly trustworthy. For example, he assumed that the samples had contained only uranium and no lead when they were formed.
More important research was published in 1913. It showed that elements generally exist in multiple variants with different masses, or "isotopes". In the 1930s, isotopes would be shown to have nuclei with differing numbers of the neutral particles known as "neutrons". In that same year, other research was published establishing the rules for radioactive decay, allowing more precise identification of decay series.
Many geologists felt these new discoveries made radiometric dating so complicated as to be worthless. Holmes felt that they gave him tools to improve his techniques, and he plodded ahead with his research, publishing before and after the First World War. His work was generally ignored until the 1920s, though in 1917 Joseph Barrell, a professor of geology at Yale, redrew geological history as it was understood at the time to conform to Holmes's findings in radiometric dating. Barrell's research determined that the layers of strata had not all been laid down at the same rate, and so current rates of geological change could not be used to provide accurate timelines of the history of Earth.
Holmes's persistence finally began to pay off in 1921, when the speakers at the yearly meeting of the British Association for the Advancement of Science came to a rough consensus that Earth was a few billion years old, and that radiometric dating was credible. Holmes published "The Age of the Earth, an Introduction to Geological Ideas" in 1927 in which he presented a range of 1.6 to 3.0 billion years. No great push to embrace radiometric dating followed, however, and the die-hards in the geological community stubbornly resisted. They had never cared for attempts by physicists to intrude in their domain, and had successfully ignored them so far The growing weight of evidence finally tilted the balance in 1931, when the National Research Council of the US National Academy of Sciences decided to resolve the question of the age of Earth by appointing a committee to investigate. Holmes, being one of the few people on Earth who was trained in radiometric dating techniques, was a committee member, and in fact wrote most of the final report.
Thus, Arthur Holmes' report concluded that radioactive dating was the only reliable means of pinning down geological time scales. Questions of bias were deflected by the great and exacting detail of the report. It described the methods used, the care with which measurements were made, and their error bars and limitations.
Modern radiometric dating.
Radiometric dating continues to be the predominant way scientists date geologic timescales. Techniques for radioactive dating have been tested and fine-tuned on an ongoing basis since the 1960s. Forty or so different dating techniques have been utilized to date, working on a wide variety of materials. Dates for the same sample using these different techniques are in very close agreement on the age of the material.
Possible contamination problems do exist, but they have been studied and dealt with by careful investigation, leading to sample preparation procedures being minimized to limit the chance of contamination.
Why meteorites were used.
An age of 4.55 ± 0.07 billion years, very close to today's accepted age, was determined by Clair Cameron Patterson using uranium-lead isotope dating (specifically lead-lead dating) on several meteorites including the Canyon Diablo meteorite and published in 1956.
The quoted age of Earth is derived, in part, from the Canyon Diablo meteorite for several important reasons and is built upon a modern understanding of cosmochemistry built up over decades of research.
Most geological samples from Earth are unable to give a direct date of the formation of Earth from the solar nebula because Earth has undergone differentiation into the core, mantle, and crust, and this has then undergone a long history of mixing and unmixing of these sample reservoirs by plate tectonics, weathering and hydrothermal circulation.
All of these processes may adversely affect isotopic dating mechanisms because the sample cannot always be assumed to have remained as a closed system, by which it is meant that either the parent or daughter nuclide (a species of atom characterised by the number of neutrons and protons an atom contains) or an intermediate daughter nuclide may have been partially removed from the sample, which will skew the resulting isotopic date. To mitigate this effect it is usual to date several minerals in the same sample, to provide an isochron. Alternatively, more than one dating system may be used on a sample to check the date.
Some meteorites are furthermore considered to represent the primitive material from which the accreting solar disk was formed. Some have behaved as closed systems (for some isotopic systems) soon after the solar disk and the planets formed. To date, these assumptions are supported by much scientific observation and repeated isotopic dates, and it is certainly a more robust hypothesis than that which assumes a terrestrial rock has retained its original composition.
Nevertheless, ancient Archaean lead ores of galena have been used to date the formation of Earth as these represent the earliest formed lead-only minerals on the planet and record the earliest homogeneous lead-lead isotope systems on the planet. These have returned age dates of 4.54 billion years with a precision of as little as 1% margin for error.
Statistics for several meteorites that have undergone isochron dating are as follows:
Canyon Diablo meteorite.
The Canyon Diablo meteorite was used because it is a very large representative of a particularly rare type of meteorite that contains sulfide minerals (particularly troilite, FeS), metallic nickel-iron alloys, plus silicate minerals.
This is important because the presence of the three mineral phases allows investigation of isotopic dates using samples that provide a great separation in concentrations between parent and daughter nuclides. This is particularly true of uranium and lead. Lead is strongly chalcophilic and is found in the sulfide at a much greater concentration than in the silicate, versus uranium. Because of this segregation in the parent and daughter nuclides during the formation of the meteorite, this allowed a much more precise date of the formation of the solar disk and hence the planets than ever before.
The age determined from the Canyon Diablo meteorite has been confirmed by hundreds of other age determinations, from both terrestrial samples and other meteorites. The meteorite samples, however, show a spread from 4.53 to 4.58 billion years ago. This is interpreted as the duration of formation of the solar nebula and its collapse into the solar disk to form the Sun and the planets. This 50 million year time span allows for accretion of the planets from the original solar dust and meteorites.
The moon, as another extraterrestrial body that has not undergone plate tectonics and that has no atmosphere, provides quite precise age dates from the samples returned from the Apollo missions. Rocks returned from the Moon have been dated at a maximum of around 4.4 and 4.5 billion years old. Martian meteorites that have landed upon Earth have also been dated to around 4.5 billion years old by lead-lead dating. Lunar samples, since they have not been disturbed by weathering, plate tectonics or material moved by organisms, can also provide dating by direct electron microscope examination of cosmic ray tracks. The accumulation of dislocations generated by high energy cosmic ray particle impacts provides another confirmation of the isotopic dates. Cosmic ray dating is only useful on material that has not been melted, since melting erases the crystalline structure of the material, and wipes away the tracks left by the particles.
Altogether, the concordance of age dates of both the earliest terrestrial lead reservoirs and all other reservoirs within the Solar System found to date are used to support the fact that Earth and the rest of the Solar System formed at around 4.53 to 4.58 billion years ago.
Helioseismic verification.
The radiometric date of meteorites can be verified with studies of the Sun. The Sun can be dated using helioseismic methods that strongly agree with the radiometric dates found for the oldest meteorites.

</doc>
<doc id="49257" url="https://en.wikipedia.org/wiki?curid=49257" title="Digital audio broadcasting">
Digital audio broadcasting

Digital audio broadcasting (DAB) is a digital radio technology for broadcasting radio stations, used in several countries across Europe and Asia Pacific.
The DAB standard was initiated as a European research project in the 1980s. The Norwegian Broadcasting Corporation (NRK) launched the very first DAB channel in the world on 1 June 1995 (NRK Klassisk), and the BBC and SR launched their first DAB digital radio broadcasts in September 1995. DAB receivers have been available in many countries since the end of the 1990s.
DAB may offer more radio programmes over a specific spectrum than analogue FM radio. DAB is more robust with regard to noise and multipath fading for mobile listening, since DAB reception quality first degrades rapidly when the signal strength falls below a critical threshold, whereas FM reception quality degrades slowly with the decreasing signal.
Audio quality varies depending on the bitrate used and audio material. Most stations use a bit rate of 128 kbit/s or less with the MP2 audio codec, which requires 160 kbit/s to achieve perceived FM quality. 128 kbit/s gives better dynamic range or signal-to-noise ratio than FM radio, but a more smeared stereo image, and an upper cut-off frequency of 14 kHz, corresponding to 15 kHz of FM radio. However, "CD sound quality" with MP2 is possible "with 256…192 kbps".
An upgraded version of the system was released in February 2007, which is called DAB+. DAB is not forward compatible with DAB+, which means that DAB-only receivers are not able to receive DAB+ broadcasts. However, broadcasters can mix DAB and DAB+ programs inside the same transmission and so make a progressive transition to DAB+. DAB+ is approximately twice as efficient as DAB due to the adoption of the AAC+ audio codec, and DAB+ can provide high quality audio with bit rates as low as 64 kbit/s. Reception quality is also more robust on DAB+ than on DAB due to the addition of Reed-Solomon error correction coding.
In spectrum management, the bands that are allocated for public DAB services, are abbreviated with T-DAB, where the "T" stands for terrestrial.
More than 20 countries provide DAB transmissions, and several countries, such as Norway, Australia, Italy, Malta, Switzerland, The Netherlands and Germany, are transmitting DAB+ stations. See Countries using DAB/DMB.
History.
DAB has been under development since 1981 at the "Institut für Rundfunktechnik" (IRT). In 1985 the first DAB demonstrations were held at the WARC-ORB in Geneva and in 1988 the first DAB transmissions were made in Germany. Later DAB was developed as a research project for the European Union (EUREKA), which started in 1987 on initiative by a consortium formed in 1986. The MPEG-1 Audio Layer II ("MP2") codec was created as part of the EU147 project. DAB was the first standard based on orthogonal frequency division multiplexing (OFDM) modulation technique, which since then has become one of the most popular transmission schemes for modern wideband digital communication systems.
A choice of audio codec, modulation and error-correction coding schemes and first trial broadcasts were made in 1990. Public demonstrations were made in 1993 in the United Kingdom. The protocol specification was finalized in 1993 and adopted by the ITU-R standardization body in 1994, the European community in 1995 and by ETSI in 1997. Pilot broadcasts were launched in several countries in 1995.
The UK was the first country to receive a wide range of radio stations via DAB. Commercial DAB receivers began to be sold in 1999 and over 50 commercial and BBC services were available in London by 2001.
By 2006, 500 million people worldwide were in the coverage area of DAB broadcasts, although by this time sales had only taken off in the United Kingdom and Denmark. In 2006 there were approximately 1,000 DAB stations in operation world wide.
The standard was coordinated by the European DAB forum, formed in 1995 and reconstituted to the World DAB Forum in 1997, which represents more than 30 countries. In 2006 the World DAB Forum became the World DMB Forum which now presides over both the DAB and DMB standard.
In October 2005, the World DMB Forum instructed its Technical Committee to carry out the work needed to adopt the AAC+ audio codec and stronger error correction coding. This work led to the launch of the new DAB+ system.
Technology.
Bands and modes.
DAB uses a wide-bandwidth broadcast technology and typically spectra have been allocated for it in Band III (174–240 MHz) and L band (1,452–1,492 MHz), although the scheme allows for operation almost anywhere above 30 MHz. The US military has reserved L-Band in the USA only, blocking its use for other purposes in America, and the United States has reached an agreement with Canada to restrict L-Band DAB to terrestrial broadcast to avoid interference.
DAB has a number of country specific transmission modes (I, II, III and IV). For worldwide operation a receiver must support all 4 modes:
Protocol stack.
From an OSI model protocol stack viewpoint, the technologies used on DAB inhabit the following layers: the audio codec inhabits the presentation layer. Below that is the data link layer, in charge of statistical time division multiplexing and frame synchronization. Finally, the physical layer contains the error-correction coding, OFDM modulation, and dealing with the over-the-air transmission and reception of data. Some aspects of these are described below.
Audio codec.
The older version of DAB that is being used in Denmark*, Ireland*, Norway*, Switzerland* and the UK, uses the MPEG-1 Audio Layer 2 audio codec, which is also known as "MP2" due to computer files using those characters for their file extension. Denmark, Ireland, Norway, and Switzerland also use DAB+.
The new DAB+ standard has adopted the HE-AAC version 2 audio codec, commonly known as 'AAC+' or 'aacPlus'. AAC+ is approximately three-times more efficient than MP2, which means that broadcasters using DAB+ will be able to provide far higher audio quality or far more stations than they can on DAB, or, as is most likely, a combination of both higher audio quality and more stations will be provided.
One of the most important decisions regarding the design of a digital radio system is the choice of which audio codec to use, because the efficiency of the audio codec determines how many radio stations can be carried on a multiplex at a given level of audio quality. The capacity of a DAB multiplex is fixed, so the more efficient the audio codec is, the more stations can be carried, and vice versa. Similarly, for a fixed bit-rate level, the more efficient the audio codec is the higher the audio quality will be.
Error-correction coding.
Error-correction coding (ECC) is an important technology for a digital communication system because it determines how robust the reception will be for a given signal strength – stronger ECC will provide more robust reception than a weaker form.
The old version of DAB uses punctured convolutional coding for its ECC. The coding scheme uses unequal error protection (UEP), which means that parts of the audio bit-stream that are more susceptible to errors causing audible disturbances are provided with more protection (i.e. a lower code rate) and vice versa. However, the UEP scheme used on DAB results in there being a grey area in between the user experiencing good reception quality and no reception at all, as opposed to the situation with most other wireless digital communication systems that have a sharp "digital cliff", where the signal rapidly becomes unusable if the signal strength drops below a certain threshold. When DAB listeners receive a signal in this intermediate strength area they experience a "burbling" sound which interrupts the playback of the audio.
The new DAB+ standard has incorporated Reed-Solomon ECC as an "inner layer" of coding that is placed around the byte interleaved audio frame but inside the "outer layer" of convolutional coding used by the older DAB system, although on DAB+ the convolutional coding uses equal error protection (EEP) rather than UEP since each bit is equally important in DAB+. This combination of Reed-Solomon coding as the inner layer of coding, followed by an outer layer of convolutional coding – so-called "concatenated coding" – became a popular ECC scheme in the 1990s, and NASA adopted it for its deep-space missions. One slight difference between the concatenated coding used by the DAB+ system and that used on most other systems is that it uses a rectangular byte interleaver rather than Forney interleaving in order to provide a greater interleaver depth, which increases the distance over which error bursts will be spread out in the bit-stream, which in turn will allow the Reed-Solomon error decoder to correct a higher proportion of errors.
The ECC used on DAB+ is far stronger than is used on DAB, which, with all else being equal (i.e. if the transmission powers remained the same), would translate into people who currently experience reception difficulties on DAB receiving a much more robust signal with DAB+ transmissions. It also has a far steeper "digital cliff", and listening tests have shown that people prefer this when the signal strength is low compared to the shallower digital cliff on DAB.
Modulation.
Immunity to fading and inter-symbol interference (caused by multipath propagation) is achieved without equalization by means of the OFDM and DQPSK modulation techniques. For details, see the OFDM system comparison table.
Using values for the most commonly used transmission mode on DAB, Transmission Mode I (TM I), the OFDM modulation consists of 1,536 subcarriers that are transmitted in parallel. The useful part of the OFDM symbol period is 1 millisecond, which results in the OFDM subcarriers each having a bandwidth of 1 kHz due to the inverse relationship between these two parameters, and the overall OFDM channel bandwidth is 1,537 kHz. The OFDM guard interval for TM I is 246 microseconds, which means that the overall OFDM symbol duration is 1.246 milliseconds. The guard interval duration also determines the maximum separation between transmitters that are part of the same single-frequency network (SFN), which is approximately 74 km for TM I.
Single-frequency networks.
OFDM allows the use of single-frequency networks (SFN), which means that a network of transmitters can provide coverage to a large area – up to the size of a country – where all transmitters use the same transmission frequency. Transmitters that are part of an SFN need to be very accurately synchronised with other transmitters in the network, which requires the transmitters to use very accurate clocks.
When a receiver receives a signal that has been transmitted from the different transmitters that are part of an SFN, the signals from the different transmitters will typically have different delays, but to OFDM they will appear to simply be different multipaths of the same signal. Reception difficulties can arise, however, when the relative delay of multipaths exceeds the OFDM guard interval duration, and there are frequent reports of reception difficulties due to this issue when there is a "lift", such as when there's high pressure, due to signals travelling farther than usual, and thus the signals are likely to arrive with a relative delay that is greater than the OFDM guard interval.
Low power "gap-filler" transmitters can be added to an SFN as and when desired in order to improve reception quality, although the way SFNs have been implemented in the UK up to now they have tended to consist of higher power transmitters being installed at main transmitter sites in order to keep costs down.
Bit rates.
An ensemble has a maximum bit rate that can be carried, but this depends on which error protection level is used. However, all DAB multiplexes can carry a total of 864 "capacity units". The number of capacity units, or CU, that a certain bit-rate level requires depends on the amount of error correction added to the transmission, as described above. In the UK, most services transmit using 'protection level three', which provides an average ECC code rate of approximately ½, equating to a maximum bit rate per multiplex of 1,184 kbit/s.
Services and ensembles.
Various different services are embedded into one ensemble (which is also typically called a multiplex). These services can include:
DAB+.
The term DAB most commonly refers both to a specific DAB standard using the MP2 audio codec, but can sometimes refer to a whole family of DAB related standards, such as DAB+, DMB and DAB-IP.
DAB+.
WorldDAB, the organisation in charge of the DAB standards, announced DAB+, a major upgrade to the DAB standard in 2006, when the HE-AAC v2 audio codec (also known as eAAC+) was adopted. The new standard, which is called DAB+, has also adopted the MPEG Surround audio format and stronger error correction coding in the form of Reed-Solomon coding. DAB+ has been standardised as European Telecommunications Standards Institute (ETSI) TS 102 563.
As DAB is not forward compatible with DAB+, older DAB receivers can not receive DAB+ broadcasts. However, DAB receivers that will be able to receive the new DAB+ standard via a firmware upgrade went on sale in July 2007. If a receiver is DAB+ compatible, there will be a sign on the product packaging.
DAB+ broadcasts have launched in several countries like Australia, Czech Republic, Denmark, Germany, Hong Kong, Italy, Malta, Norway, Poland, Switzerland, and The Netherlands. Malta was the first country to launch DAB+ in Europe. Several other countries are also expected to launch DAB+ broadcasts over the next few years, such as Austria, Hungary and Asian countries, such as Thailand, Vietnam and Indonesia. South Africa began a DAB+ technical pilot in November 2014 on channel 13F in Band 3. If DAB+ stations launch in established DAB countries, they can transmit alongside existing DAB stations that use the older MPEG-1 Audio Layer II audio format, and most existing DAB stations are expected to continue broadcasting until the vast majority of receivers support DAB+.
Ofcom in the UK has published a consultation with the intention to set up a new multiplex containing a mix of DAB and DAB+ services, with the intention of moving services to this format in the long term. In February 2016 3 DAB+ stations launched in the UK as part of a new national network.
DMB.
Digital multimedia broadcasting (DMB) and DAB-IP are suitable for mobile radio and TV both because they support MPEG 4 AVC and WMV9 respectively as video codecs. However, a DMB video subchannel can easily be added to any DAB transmission, as it was designed to be carried on a DAB subchannel. DMB broadcasts in Korea carry conventional MPEG 1 Layer II DAB audio services alongside their DMB video services.
Norway, South Korea and France are countries currently broadcasting DMB.
Countries using DAB.
More than 30 countries provide DAB, DAB+ and/or DMB broadcasts, either as a permanent technology or as test transmissions.
DAB and AM/FM compared.
Traditionally radio programmes were broadcast on different frequencies via AM and FM, and the radio had to be tuned into each frequency, as needed. This used up a comparatively large amount of spectrum for a relatively small number of stations, limiting listening choice. DAB is a digital radio broadcasting system that through the application of multiplexing and compression combines multiple audio streams onto a relatively narrow band centred on a single broadcast frequency called a DAB ensemble.
Within an overall target bit rate for the DAB ensemble, individual stations can be allocated different bit rates. The number of channels within a DAB ensemble can be increased by lowering average bit rates, but at the expense of the quality of streams. Error correction under the DAB standard makes the signal more robust but reduces the total bit rate available for streams.
FM HD Radio versus DAB.
Some countries have implemented Eureka-147 digital audio broadcasting (DAB). DAB broadcasts a single station that is approximately 1,500 kilohertz wide (~1,000 kilobits per second). That station is then subdivided into multiple digital streams of between 9 and 12 programs. In contrast FM HD radio shares its digital broadcast with the traditional 200 kilohertz-wide channels, with capability of 300 kbit/s per station (pure digital mode).
The first generation DAB uses the MPEG-1 Audio Layer II (MP2) audio codec which has less efficient compression than newer codecs. The typical bitrate for DAB programs is only 128 kbit/s and as a result most radio stations on DAB have a lower sound quality than FM, prompting a number of complaints among the audiophile community. As with DAB+ or T-DMB in Europe, FM HD Radio uses a codec based upon the MPEG-4 HE-AAC standard.
HD Radio is a proprietary system from the company Ibiquity. DAB is an open standard deposited at ETSI.
Use of frequency spectrum and transmitter sites.
DAB gives substantially higher spectral efficiency, measured in programmes per MHz and per transmitter site, than analogue communication. This has led to an increase in the number of stations available to listeners, especially outside of the major urban areas.
Numerical example: Analog FM requires 0.2 MHz per programme. The frequency reuse factor in most countries is approximately 15, meaning that only one out of 15 transmitter sites can use the same channel frequency without problems with co-channel interference, i.e. cross-talk. Assuming a total availability of 102 FM channels at a bandwidth of 0.2MHz over the Band II spectrum of 87.5 to 108.0 MHz, an average of 102/15 = 6.8 radio channels are possible on each transmitter site (plus lower-power local transmitters causing less interference). This results in a system spectral efficiency of 1 / 15 / (0.2 MHz) = 0.30 programmes/transmitter/MHz. DAB with 192 kbit/s codec requires 1.536 MHz * 192 kbit/s / 1,136 kbit/s = 0.26 MHz per audio programme. The frequency reuse factor for local programmes and multi-frequency broadcasting networks (MFN) is typically 4 or 5, resulting in 1 / 4 / (0.26 MHz) = 0.96 programmes/transmitter/MHz. This is 3.2 times as efficient as analog FM for local stations. For single frequency network (SFN) transmission, for example of national programmes, the channel re-use factor is 1, resulting in 1/1/0.25 MHz = 3.85 programmes/transmitter/MHz, which is 12.7 times as efficient as FM for national and regional networks.
Note the above capacity improvement may not always be achieved at the L-band frequencies, since these are more sensitive to obstacles than the FM band frequencies, and may cause shadow fading for hilly terrain and for indoor communication. The number of transmitter sites or the transmission power required for full coverage of a country may be rather high at these frequencies, to avoid the system becoming noise limited rather than limited by co-channel interference.
Sound quality.
The original objectives of converting to digital transmission were to enable higher fidelity, more stations and more resistance to noise, co-channel interference and multipath than in analogue FM radio. However, the leading countries in implementing DAB on stereo radio stations use compression to such a degree that it produces lower sound quality than that received from non-mobile FM broadcasts. This is because of the bit rate levels being too low for the MPEG Layer 2 audio codec to provide high fidelity audio quality.
The BBC Research & Development department states that at least 192 kbit/s is necessary for a high fidelity stereo broadcast :
When BBC in July 2006 reduced the bit-rate of transmission of Radio 3 from 192 kbit/s to 160 kbit/s, the resulting degradation of audio quality prompted a number of complaints to the Corporation. BBC later announced that following this testing of new equipment, it would resume the previous practice of transmitting Radio 3 at 192 kbit/s whenever there were no other demands on bandwidth.
Despite the above a survey of DAB listeners (including mobile) has shown most find DAB to have equal or better sound quality than FM.
Notwithstanding the above, BBC Radio 4 has extended the periods it broadcasts programmes with a lower bit rate (80 kbit/s) and in mono in 2012, such as the "Today" programme, rather than 128 kbit/s and in stereo. Programmes which had traditionally been broadcast on BBC Radio 4 DAB in stereo (from 1999 to 2011), can now only be heard in the evenings in mono, even though the same programmes still go out in stereo on Radio 4 FM, Digital TV and On-Line. The BBC have issued a statement stating that stereo is still their default for BBC Radio 4 DAB, however after the Olympics, this does not appear to be the case in the evenings, making FM broadcasts (in good reception areas) superior. As very few car radios are currently fitted with DAB if the BBC switch FM off as indicated later in the decade, some listeners may be forced to receive mono broadcasts in the future, a somewhat backward step.
An Audio Quality comparison of PCM, DAB, DAB+, FM and AM is available here
Benefits of DAB.
Current AM and FM terrestrial broadcast technology is well established, compatible, and cheap to manufacture. Benefits of DAB over analogue systems are explained below.
Improved features for users.
DAB radios automatically tune to all the available stations, offering a list for the user to select from.
DAB can carry "radiotext" (in DAB terminology, "Dynamic Label Segment", or DLS) from the station giving real-time information such as song titles, music type and news or traffic updates. Advance programme guides can also be transmitted. A similar feature also exists on FM in the form of the RDS. (However, not all FM receivers allow radio stations to be stored by name.)
DAB receivers can display time of day as encoded into transmissions, so is automatically corrected when travelling between time zones and when changing to or from Daylight Saving. This is not implemented on all receivers, and some display time only when in "Standby" mode. (Similar Features on RDS: 4A Groups)
Some radios offer a pause facility on live broadcasts, caching the broadcast stream on local flash memory, although this function is limited.
More stations.
DAB is not more bandwidth efficient than analogue measured in programmes per MHz of a specific transmitter (the so-called link spectral efficiency). It is less susceptible to co-channel interference (cross talk), which makes it possible to reduce the reuse distance, i.e. use the same radio frequency channel more densely. The system spectral efficiency (the average number of radio programmes per MHz and transmitter) is a factor three more efficient than analogue FM for local radio stations, as can be seen in the above numerical example. For national and regional radio networks, the efficiency is improved by more than an order of magnitude due to the use of SFNs. In that case, adjacent transmitters use the same frequency.
In certain areas – particularly rural areas – the introduction of DAB gives radio listeners a greater choice of radio stations. For instance, in South Norway, radio listeners experienced an increase in available stations from 6 to 21 when DAB was introduced in November 2006.
Reception quality.
The DAB standard integrates features to reduce the negative consequences of multipath fading and signal noise, which afflict existing analogue systems.
Also, as DAB transmits digital audio, there is no hiss with a weak signal, which can happen on FM. However, radios in the fringe of a DAB signal, can experience a "bubbling mud" sound interrupting the audio and/or the audio cutting out altogether.
Due to sensitivity to doppler shift in combination with multipath propagation, DAB reception range (but not audio quality) is reduced when travelling speeds of more than 120 to 200 km/h, depending on carrier frequency.
Less unlicensed ("pirate") station interference.
The specialised nature and cost of DAB broadcasting equipment provide barriers to unlicensed ("pirate") stations broadcasting on DAB. In cities such as London with large numbers of undocumented radio stations broadcasting on FM, this means that some stations can be reliably received via DAB in areas where they are regularly difficult or impossible to receive on FM due to undocumented radio interference.
Variable bandwidth.
Mono talk radio, news and weather channels and other non-music programs need significantly less bandwidth than a typical music radio station, which allows DAB to carry these programmes at lower bit rates, leaving more bandwidth to be used for other programs.
However, this had led to the situation where some stations are being broadcast in mono, see "music radio stations broadcasting in mono" for more details.
Transmission costs.
It is common belief that DAB is more expensive to transmit than FM. It is true that DAB uses higher frequencies than FM and therefore there is a need to compensate with more transmitters, higher radiated powers, or a combination, to achieve the same coverage. However, the last couple of years has seen significant improvement in power efficiency for DAB-transmitters.
This efficiency originates from the ability a DAB network has in broadcasting more channels per network. One network can broadcast 6–10 channels (with MPEG audio codec) or 10–16 channels (with HE AAC codec). Hence, it is thought that the replacement of FM-radios and FM-transmitters with new DAB-radios and DAB-transmitters will not cost any more as opposed to newer FM facilities.
Lower transmission costs are supported by independent network studies from Teracom (Sweden) and SSR/SRG (Switzerland). Among other things they show that DAB is as low as one-sixth of the cost of FM transmission.
Disadvantages of DAB.
Reception quality.
The reception quality on DAB can be poor even for people who live well within the coverage area. The reason for this is that the old version of DAB uses weak error correction coding, so that when there are a lot of errors with the received data not enough of the errors can be corrected and a "bubbling mud" sound occurs. In some cases a complete loss of signal can happen. This situation will be improved upon in the new DAB standard (DAB+, discussed below) that uses stronger error correction coding and as additional transmitters are built.
Audio Quality.
Broadcasters have been criticized for ‘squeezing in’ more stations per ensemble than recommended, by:
Signal delay.
The nature of a Single-frequency network (SFN) is such that the transmitters in a network must broadcast the same signal at the same time. To achieve synchronization, the broadcaster must counter any differences in propagation time incurred by the different methods and distances involved in carrying the signal from the multiplexer to the different transmitters. This is done by applying a delay to the incoming signal at the transmitter based on a timestamp generated at the multiplexer, created taking into account the maximum likely propagation time, with a generous added margin for safety. Delays in the receiver due to digital processing (e.g. deinterleaving) add to the overall delay perceived by the listener. The signal is delayed by 2–4 seconds depending on the decoding circuitry used. This has disadvantages:
Time signals, on the contrary, are not a problem in a well-defined network with a fixed delay. The DAB multiplexer adds the proper offset to the distributed time information. The time information is also independent from the (possibly varying) audio decoding delay in receivers since the time is not embedded inside the audio frames. This means that built in clocks in receivers will be spot on.
Coverage.
As DAB is at a relatively early stage of deployment, DAB coverage is poor in nearly all countries in comparison to the high population coverage provided by FM.
Exceptions include Norway, which will have 99.5% coverage by the end of 2014, and the United Kingdom, where 95% population coverage has been achieved for certain stations.
Compatibility.
In 2006 tests began using the much improved HE-AAC codec for DAB+. Virtually none of the receivers made before 2008 support the new codec, however, thus making them partially obsolete once DAB+ broadcasts begin and completely obsolete once the old MPEG-1 Layer 2 stations are switched off. New receivers are both DAB and DAB+ compatible; however, the issue is exacerbated by some manufacturers disabling the DAB+ features on otherwise compatible radios to save on licensing fees when sold in countries without current DAB+ broadcasts.
Power requirements.
As DAB requires digital signal processing techniques to convert from the received digitally encoded signal to the analogue audio content, the complexity of the electronic circuitry required to do this is higher. This translates into needing more power to effect this conversion than compared to an analogue FM to audio conversion, meaning that portable receiving equipment will tend to have a shorter battery life, or require higher power (and hence more bulk). This means that they use more energy than analogue Band II VHF receivers. However, thanks to increased integration (radio-on-chip), DAB receiver is getting closer to FM one. For example, NXP semiconductor is producing both FM only and FM+DAB radio-on-chip, with similar power consumption.
As an indicator of this increased power consumption in the early days of DAB, some radio manufacturers quoted the length of time their receivers can play on a single charge. For a commonly used FM/DAB-receiver from manufacturer PURE, this is stated as: DAB 10 hours, FM 22 hours.. Currently, PURE manufacturer doesn't indicate any more power consumption difference between FM and DAB modes.
FM radio switch-off.
Norway is the only country that has announced a complete switch-off of national FM radio stations. Switch off will start on 11 January 2017. The switch-off will not affect local and in some way regional radio stations.
At the "WorldDMB seminar" held in Riva del Garda, Italy, on 14 April 2013, it was announced that in Norway there will be 99.5% DAB coverage by 2014, and that the country is planning to switch-off its national and regional FM radio services in 2017. No subsequent date has been announced for such a move.
Other Nordic countries like Denmark and Sweden are evaluating a switch-off by 2022.
UK is considering a progressive switch-off in the period 2017–2022.

</doc>
<doc id="49260" url="https://en.wikipedia.org/wiki?curid=49260" title="Willy Brandt">
Willy Brandt

Willy Brandt (; born Herbert Ernst Karl Frahm; 18 December 1913 – 8 October 1992) was a German statesman and politician, who was leader of the Social Democratic Party of Germany (SPD) from 1964 to 1987 and served as Chancellor of the Federal Republic of Germany from 1969 to 1974. He was awarded the Nobel Peace Prize in 1971 for his efforts to strengthen cooperation in western Europe through the EEC and to achieve reconciliation between West Germany and the countries of Eastern Europe. He was the first Social Democrat chancellor since 1930.
Fleeing to Norway and then Sweden during the Nazi regime and working as a leftist journalist, he took the name Willy Brandt as a pseudonym to avoid detection by Nazi agents, and then formally adopted the name in 1948. Brandt was originally considered one of the leaders of the right wing of the SPD, and earned initial fame as Governing Mayor of West Berlin. He served as Foreign Minister and as the 5th Vice Chancellor in Kurt Georg Kiesinger's cabinet, and became chancellor in 1969. As chancellor, he maintained West Germany's close alignment with the United States and focused on strengthening European integration in western Europe, while launching the new policy of "Ostpolitik" aimed at improving relations with Eastern Europe. Brandt was controversial on both the right wing, for his "Ostpolitik", and on the left wing, for his support of American policies, including the Vietnam War, and right-wing authoritarian regimes. The Brandt Report became a recognised measure for describing the general North-South divide in world economics and politics between an affluent North and a poor South. Brandt was also known for his fierce anti-communist policies at the domestic level, culminating in the Radikalenerlass (Anti-Radical Decree) in 1972.
Brandt resigned as chancellor in 1974, after Günter Guillaume, one of his closest aides, was exposed as an agent of the Stasi, the East German secret service.
Early life and the war.
Willy Brandt was born Herbert Ernst Carl Frahm in the Free City of Lübeck (German Empire) on 18 December 1913. His mother was Martha Frahm, a single parent, who worked as a cashier for a department store. His father was an accountant from Hamburg named John Möller, whom Brandt never met. As his mother worked six days a week, he was mainly brought up by his mother's stepfather, Ludwig Frahm, and his second wife, Dora.
After passing his Abitur in 1932 at "Johanneum zu Lübeck", he became an apprentice at the shipbroker and ship's agent F. H. Bertling. He joined the "Socialist Youth" in 1929 and the Social Democratic Party (SPD) in 1930. He left the SPD to join the more left wing Socialist Workers Party (SAP), which was allied to the POUM in Spain and the Independent Labour Party in Britain. In 1933, using his connections with the port and its ships, he left Germany for Norway to escape Nazi persecution. It was at this time that he adopted the pseudonym Willy Brandt to avoid detection by Nazi agents. In 1934, he took part in the founding of the International Bureau of Revolutionary Youth Organizations, and was elected to its Secretariat.
Brandt was in Germany from September to December 1936, disguised as a Norwegian student named Gunnar Gaasland. He was married to Gertrud Meyer from Lübeck in a marriage of convenience to protect her from deportation. Meyer had joined Brandt in Norway in July 1933. In 1937, during the Civil War, Brandt worked in Spain as a journalist. In 1938, the German government revoked his citizenship, so he applied for Norwegian citizenship. In 1940, he was arrested in Norway by occupying German forces, but was not identified as he wore a Norwegian uniform. On his release, he escaped to neutral Sweden. In August 1940, he became a Norwegian citizen, receiving his passport from the Norwegian embassy in Stockholm, where he lived until the end of the war. Willy Brandt lectured in Sweden on 1 December 1940 at Bommersvik College about problems experienced by the social democrats in Nazi Germany and the occupied countries at the start of the Second World War. In exile in Norway and Sweden Brandt learned Norwegian and Swedish. Brandt spoke Norwegian fluently, and retained a close relationship with Norway.
In late 1946, Brandt returned to Berlin, working for the Norwegian government. In 1948, he joined the Social Democratic Party of Germany (SPD) and became a German citizen again, formally adopting the pseudonym Willy Brandt as his legal name.
Politician.
From 3 October 1957 to 1966, Willy Brandt was Governing Mayor of Berlin, during a period of increasing tension in East-West relations that led to the construction of the Berlin Wall. In Brandt's first year as mayor, he also served as the president of the Bundesrat in Bonn. Brandt was outspoken against the Soviet repression of the Hungarian Uprising in 1956 and against Nikita Khrushchev's 1958 proposal that Berlin receive the status of a "free city". He was supported by the influential publisher Axel Springer. As mayor of West Berlin, Brandt accomplished much in the way of urban development. New hotels, office-blocks and flats were constructed, while both Schloss Charlottenburg and the Reichstag building were restored. Sections of the "Stadtring" Bundesautobahn 100 inner city motorway were opened, while a major housing programme was carried out, with roughly 20,000 new dwellings built each year during his time in office.
At the start of 1961, U.S. President John F. Kennedy saw Brandt as the wave of the future in West Germany and was hoping he would replace Konrad Adenauer as chancellor following elections later in the year. Kennedy made this preference clear by inviting Brandt, the West German opposition leader, to an official meeting at the White House a month before meeting with Adenauer, the country's leader. The diplomatic snub strained relations between Kennedy and Adenauer further during an especially tense time for Berlin. However, following the building of the Berlin Wall in August 1961, Brandt was disappointed and angry with Kennedy. Speaking in Berlin three days later, Brandt criticized Kennedy, asserting "Berlin expects more than words. It expects political action." He also wrote Kennedy a highly critical public letter in which he warned that the development was liable "to arouse doubts about the ability of the three Powers to react and their determination" and he called the situation "a state of accomplished extortion".
Brandt became the Chairman of the SPD in 1964, a post that he retained until 1987, longer than any other party Chairman since its foundation by August Bebel. Brandt was the SPD candidate for the chancellorship in 1961, but he lost to Konrad Adenauer's conservative Christian Democratic Union of Germany (CDU). In 1965, Brandt ran again, but lost to the popular Ludwig Erhard. Erhard's government was short-lived, however, and in 1966 a grand coalition between the SPD and CDU was formed, with Brandt as Foreign Minister and as the 5th Vice-Chancellor of Germany.
Chancellor.
At the 1969 elections, again with Brandt as the leading candidate, the SPD became stronger, and after three weeks of negotiations, the SPD formed a coalition government with the smaller Free Democratic Party of Germany (FDP). Brandt was elected Chancellor of the Federal Republic of Germany.
Foreign policy.
As chancellor, Brandt developed his "Neue Ostpolitik" ("New Eastern Policy"). Brandt was active in creating a degree of rapprochement with East Germany, and also in improving relations with the Soviet Union, Poland, Czechoslovakia, and other Eastern Bloc (communist) countries. A seminal moment came in December 1970 with the famous "Warschauer Kniefall" in which Brandt, apparently spontaneously, knelt down at the monument to victims of the Warsaw Ghetto Uprising. The uprising occurred during the Nazi German military occupation of Poland, and the monument is to those killed by the German troops who suppressed the uprising and deported remaining ghetto residents to the concentration camps for extermination.
"Time" magazine in the U.S. named Brandt as its Man of the Year for 1970, stating, "Willy Brandt is in effect seeking to end World War II by bringing about a fresh relationship between East and West. He is trying to accept the real situation in Europe, which has lasted for 25 years, but he is also trying to bring about a new reality in his bold approach to the Soviet Union and the East Bloc." President Richard Nixon also was pushing détente on behalf of the United States. The policies of Nixon and Henry Kissinger, after some initial suspicion, amounted to co-opting Brandt's Ostpolitik.
In 1971, Brandt received the Nobel Peace Prize for his work in improving relations with East Germany, Poland, and the Soviet Union. Brandt negotiated a peace treaty with Poland, and agreements on the boundaries between the two countries, signifying the official and long-delayed end of World War II. Brandt negotiated parallel treaties and agreements with Czechoslovakia.
In West Germany, Brandt's "Neue Ostpolitik" was extremely controversial, dividing the populace into two camps. One camp embraced all of the conservative parties, and most notably those West German residents and their families who had been driven west ("die Heimatvertriebenen") by Stalinist ethnic cleansing from Historical Eastern Germany, especially the part that was given to Poland as a consequence of the end of the war; western Czechoslovakia (the Sudetenland); and the rest of Eastern Europe, such as in Romania. These groups of displaced Germans and their descendants loudly voiced their opposition to Brandt's policy, calling it "illegal" and "high treason".
A different camp supported and encouraged Brandt's "Neue Ostpolitik" as aiming at "Wandel durch Annäherung" ("change through rapprochement"), encouraging change through a policy of engagement with the (communist) Eastern Bloc, rather than trying to isolate those countries diplomatically and commercially. Brandt's supporters claim that the policy did help to break down the Eastern Bloc's "siege mentality", and also helped to increase its awareness of the contradictions in its brand of Socialism/Communism, which – together with other events – eventually led to the downfall of Eastern European Communism.
Domestic policies.
Political and social changes.
West Germany in the late 1960s was shaken by student disturbances and a general "change of the times" that not all Germans were willing to accept or approve. What had seemed a stable, peaceful nation, happy with its outcome of the "Wirtschaftswunder" ("economic miracle") faced economic turbulence. The German baby-boom generation wanted to come to terms with the deeply conservative, bourgeois, and demanding parent generation. The baby-boomer students were the most outspoken, and they accused their "parental generation" of being outdated and old-fashioned and even of having a Nazi past. Compared to their forebears, the "skeptical generation" was much more capricious, willing to embrace more extreme socialist ideology (such as Maoism), and public heroes (such as Ho Chi Minh, Fidel Castro, and Che Guevara), while living a looser and more promiscuous lifestyle. Students and young apprentices could afford to move out of their parents' homes, and left-wing politics was considered to be "chic", as well as taking part in American-style political demonstrations against having American military forces in South Vietnam.
Brandt's popularity.
Brandt's predecessor as chancellor, Kurt Georg Kiesinger, had been a member of the Nazi party, and was a more old-fashioned conservative-liberal intellectual. Brandt, having fought the Nazis and having faced down communist Eastern Germany during several crises while he was the mayor of Berlin, became a controversial, but credible, figure in several different factions. As the Minister of Foreign Affairs in Kiesinger's grand coalition cabinet, Brandt helped to gain further international approval for Western Germany, and he laid the foundation stones for his future "Neue Ostpolitik". There was a wide public-opinion gap between Kiesinger and Brandt in the West German polls.
Both men had come to their own terms with the new baby boomer lifestyles. Kiesinger considered them to be "a shameful crowd of long-haired drop-outs who needed a bath and someone to discipline them". On the other hand, Brandt needed a while to get into contact with, and to earn credibility among, the "Ausserparlamentarische Opposition" (APO) ("the extra-parliamentary opposition"). The students questioned West German society in general, seeking social, legal, and political reforms. Also, the unrest led to a renaissance of right-wing parties in some of the Bundeslands' (German states under the Bundesrepublik) Parliaments.
Brandt, however, represented a figure of change, and he followed a course of social, legal, and political reforms. In 1969, Brandt gained a small majority by forming a coalition with the FDP. In his first speech before the Bundestag as the chancellor, Brandt set forth his political course of reforms ending the speech with his famous words, "Wir wollen mehr Demokratie wagen" (literally: "Let's dare more democracy", or more figuratively, "We want to take a chance on more Democracy"). This speech made Brandt, as well as the Social Democratic Party, popular among most of the students and other young West German baby-boomers who dreamed of a country that would be more open and more colorful than the frugal and still somewhat-authoritarian Bundesrepublik that had been built after World War II. However, Brandt's "Neue Ostpolitik" lost him a large part of the German refugee voters from East Germany, who had been significantly pro-SPD in the postwar years.
Chancellor of domestic reform.
Although Brandt is perhaps best known for his achievements in foreign policy, his government oversaw the implementation of a broad range of social reforms, and was known as a "Kanzler der inneren Reformen" ('Chancellor of domestic reform'). According to the historian David Childs, "Brandt was anxious that his government should be a reforming administration and a number of reforms were embarked upon". Within a few years, the education budget rose from 16 billion to 50 billion DM, while one out of every three DM spent by the new government was devoted to welfare purposes. As noted by the journalist and historian Marion Dönhoff,
"People were seized by a completely new feeling about life. A mania for large scale reforms spread like wildfire, affecting schools, universities, the administration, family legislation. In the autumn of 1970 Jürgen Wischnewski of the SPD declared, 'Every week more than three plans for reform come up for decision in cabinet and in the Assembly.'"
According to Helmut Schmidt, Willy Brandt's domestic reform programme had accomplished more than any previous programme for a comparable period. More funds were allocated towards housing, transportation, schools, and communication, while substantial federal benefits were provided for farmers. Various measures were introduced to extend health care coverage, while federal aid to sports organisations was increased. A number of liberal social reforms were instituted whilst the welfare state was significantly expanded (with total public spending on social programs nearly doubling between 1969 and 1975), with health, housing, and social welfare legislation bringing about welcome improvements, and by the end of the Brandt Chancellorship West Germany had one of the most advanced systems of welfare in the world.
Substantial increases were made in social security benefits such as injury and sickness benefits, pensions, unemployment benefits, housing allowances, basic subsistence aid allowances, and family allowances and living allowances. In the government's first budget, sickness benefits were increased by 9.3%, pensions for war widows by 25%, pensions for the war wounded by 16%, and retirement pensions by 5%. Numerically, pensions went up by 6.4% (1970), 5.5% (1971), 9.5% (1972), 11.4% (1973), and 11.2% (1974). Adjusted for changes in the annual price index, pensions went up in real terms by 3.1% (1970), 0.3% (1971), 3.9% (1972), 4.4% (1973), and 4.2% (1974). Between 1972 and 1974, the purchasing power of pensioners went up by 19%. In 1970, war pensions were increased by 16%. War victim’s pensions went up by 5.5% in January 1971, and by 6.3% in January 1972. By 1972, war pensions for orphans and parents had gone up by around 40%, and for widows by around 50%. Between 1970 and 1972, the “Landabgaberente” (land transfer pension) went up by 55%.
In 1970, seagoing pilots became retrospectively insurable, and gained full social security as members of the Non-Manual Workers Insurance Institute. That same year, a special regulation came into force for District Master Chimney Sweeps, making them fully insurable under the Craftsman's Insurance Scheme. An increase was made in tax-free allowances for children, which enabled 1,000,000 families to claim an allowance for the second child, compared to 300,000 families previously. The Second Modification and Supplementation Law (1970) increased the allowance for the third child from DM 50 to DM 60, raised the income-limit for the second child allowance from DM 7,800 to DM 13,200; subsequently increased to DM 15,000 by the third modification law (December 1971), DM 16,800 by the fourth modification law (November 1973), and to DM 18,360 by the fifth modification law (December 1973). A flexible retirement age after 62 years was introduced (1972) for invalids and handicapped persons, and social assistance was extended to those who previously had to be helped by their relatives. From 1971, special subventions were provided to enable young farmers to quit farming “and facilitate their entry into the non-agricultural pension system by means of back payments.”
The Third Modification Law (1974) extended individual entitlements to social assistance by means of higher-income limits compatible with receipt of benefits and lowered age limits for certain special benefits. Rehabilitation measures were also extended, child supplements were expressed as percentages of standard amounts and were thus indexed to their changes, and grandparents of recipients were exempted from potential liability to reimburse expenditure of social assistance carrier. The Third Social Welfare Amendment Act (1974) brought considerable improvements for the handicapped, those in need of care, and older persons, and a new fund of 100 million marks for disabled children was established. Allowances for retraining and advanced training and for refugees from East Germany were also increased, together with federal grants for sport. In addition, increases were made in the pensions of 2.5 million war victims. Following a sudden increase in the price of oil, a law was passed in December 1973 granting recipients of social assistance and housing allowances a single heating-oil allowance (a procedure repeated in the winter of 1979 during the Schmidt Administration). Improvements and automatic adjustments of maintenance allowances for participants in vocational training measures were also carried out.
There was determined, by statutory regulation issued in February 1970, the category of persons most seriously disabled “to whom, with regard to maintenance aid, an increased demand (50% of the appropriate rate) is being conceded, and, within the scope of relief in special living conditions: a higher rate of nursing aid.” In 1971, the retirement age for miners was lowered to 50. An April 1972 law providing for "promotion of social aid services" aimed to remedy, through various beneficial measures (particularly in the field of national insurance and working conditions), the staff-shortage suffered by social establishments in their medico-social, educational and other work. A bill to harmonize re-education benefit and another bill relating to severely handicapped persons became law in May and September 1972 respectively. For those in the armed forces, the Federal Cost of Moving Act increased the relocation allowance (with effect from the 1st of November 1973), with the basic allowances raised by DM 50 and DM 100 respectively, while extra allowances for families were raised to a uniform amount of 125 DM.
To assist family planning and marriage and family guidance, the government allocated DM 2 232 000 in 1973 for the payment and for the basic and further training of staff. A special effort was also made in 1973 to organize the recreation of handicapped persons, with a holiday guide for the handicapped issued with the aid of the Federal Ministry of Family and Youth Affairs and Health in order to help them find suitable holiday accommodation for themselves and their families. From 1972 to 1973, the total amount of individual aids granted by Guarantee Fund for the integration of young immigrants increased from 17 million DM to 26 million DM. Under a law passed in April 1974, the protection hitherto granted to the victims of war or industrial accidents for the purpose of their occupational and social reintegration was extended to all handicapped persons, whatever the cause of their handicap, provided that their capacity to work had been reduced by at least 50%.
A law on explosives (Sprengstoffgesetz) was the subject of two application ordinances (on the 17th of November 1970 and the 24th of August 1971) and a general regulatory provision (the 19th of May 1971), which covered respectively the application of the law to nationals of EC Member States, the duty of employers to notify in time the inspection authorities of detonation plans, the interpretation of the purpose and field of application of the law, authorizations for transport of explosives, and control and recognition of training courses on work with explosives. Taking into account the enormous high peaks of air traffic noise and its concentration at a limited number of airports, the Law for Protection against Aircraft Noise of 1971 sought to balance two conflicting demands, the first being the legitimate demand by industry, business and the public for an efficient air-traffic-system, and secondly, the understandable and by no means less legitimate claims of the affected people for protection and compensation. The legislation regulated the establishment of so-called "Lärmschutzzonen" (protection areas against aircraft noise) for all 11 international airports and for those 34 military airports used for jet air craft, and the law also authorised the Federal Department of the Interior to decree protection areas for each of those mentioned airports with approval by the "Bundesrat," the representation of the German Federal States.
In the field of health care, various measures were introduced to improve the quality and availability of health care provision. Free hospital care was introduced for 9 million recipients of social relief, while a contributory medical service for 23 million panel patients was introduced. Pensioners were exempted from paying a 2% health insurance contribution, while improvements in health insurance provision were carried out, as characterised by an expanded sickness insurance scheme, with the inclusion of preventative treatment. The income limit for compulsory sickness insurance was indexed to changes in the wage level (1970) and the right to medical cancer screening for 23.5 million people was introduced. In January 1971, the reduction of sickness allowance in case of hospitalisation was discontinued. That same year, compulsory health insurance was extended to the self-employed. In 1970, the government included nonmedical psychotherapists and psychoanalysts in the national health insurance program.
Pupils, students and children in kindergartens were incorporated into the accident insurance scheme, which benefited 11 million children. Free medical checkups were introduced that same year, while the Farmers' Sickness Insurance Law (1972) introduced compulsory sickness insurance for independent farmers, family workers in agriculture, and pensioners under the farmers' pension scheme, medical benefits for all covered groups, and cash benefits for family workers under compulsory coverage for pension insurance. Participation in employer's health insurance was extended to four million employees. A Development Law of December 1970 made it possible for all employees voluntarily to become members of the statutory sickness insurance. The level of income for compulsory sickness insurance was indexed to 75% of the respective assessment level for pension insurance, while voluntarily insured employees were granted a claim to an allowance towards their sickness insurance from their employer. This law also introduced a new type of sickness insurance benefit, namely facilities for the early diagnosis of disease. Apart from the discretionary service of disease prevention which had existed since 1923, insured persons now had a right in certain circumstances to medical examinations aimed at the early diagnosis of disease. According to one study, this marked a change in the concept of sickness insurance: it now aimed at securing good health.
The Hospital Financing Law (1972) secured the supply of hospitals and reduced the cost of hospital care, "defined the financing of hospital investment as a public responsibility, single states to issue plans for hospital development, and the federal government to bear the cost of hospital investment covered in the plans, rates for hospital care thus based on running costs alone, hospitals to ensure that public subsidies together with insurance fund payments for patients cover total costs". The Benefit Improvement Law (1973) made entitlement to hospital care legally binding (entitlements already enjoyed in practice), abolished time limits for hospital care, introduced entitlement to household assistance under specific conditions, and also introduced entitlement to leave of absence from work and cash benefits in the event of a child's illness. In 1971, to encourage the growth of registered family holiday centres, the Federal Government granted subsidies for the building and appointing of 28 of these centres at a total cost of 8 million DM. Free preliminary investigations were introduced for 2.5 million children up until the age of 4 for the early detection and correction of developmental disorders, and health research was expanded. Federal grants were increased, especially for the Cancer Research Centre in Heidelberg, while a Federal Institute for Sport Science was set up, together with the Institute for Social Medicine and Epidemiology in Berlin. In addition, funding for new rehabilitation facilities was increased.
The Pension Reform Law (1972) guaranteed all retirees a minimum pension regardless of their contributions and institutionalized the norm that the standard pension (of average earners with forty years of contributions) should not fall below 50% of current gross earnings. The 1972 pension reforms improved eligibility conditions and benefits for nearly every subgroup of the West German population. The income replacement rate for employees who made full contributions was raised to 70% of average earnings. The reform also replaced 65 as the mandatory retirement age with a "retirement window" ranging between 63 and 65 for employees who had worked for at least thirty-five years. Employees who qualified as disabled and had worked for at least thirty-five years were extended a more generous retirement window, which ranged between the ages of 60 and 62. Women who had worked for at least fifteen years (ten of which had to be after the age of age 40) and the long-term unemployed were also granted the same retirement window as the disabled. In addition, there were no benefit reductions for employees who had decided to retire earlier than the age of 65. The legislation also changed the way in which pensions were calculated for low-income earners who had been covered for twenty-five or more years. If the pension benefit fell below a specified level, then such workers were allowed to substitute a wage figure of 75% of the average wage during this period, thus creating something like a minimum wage benefit.
Voluntary retirement at 63 with no deductions in the level of benefits was introduced, together with the index-linking of war victim's pensions to wage increases. Guaranteed minimum pension benefits for all West Germans were introduced, along with automatic pension increases for war widows (1970). Fixed minimum rates for women in receipt of very low pensions were also introduced, together with equal treatment for war widows. Improvements in pension provision were made for women and the self-employed, a new minimum pension for workers with at least twenty-five years' insurance was introduced, faster pension indexation was implemented, with the annual adjustment of pensions brought forward by six months, and the Seventh Modification Law (1973) linked the indexation of farmers' pensions to the indexation of the general pension insurance scheme.
In education, the Brandt Administration sought to widen educational opportunities for all West Germans. An addition was made to the Basic Law which gave the Federal Government some responsibility for educational planning. A big increase in spending on education was carried out, with educational expenses per head of the population multiplied by five, while the government presided over an increase in the number of teachers. Generous public stipends were introduced for students to cover their living costs, while West German universities were converted from elite schools into mass institutions. The school leaving age was raised to 16, and spending on research and education was increased by nearly 300% between 1970 and 1974. Working through a planning committee set up for the “joint task” of university development, the Federal Government started to make investment costs in 1971. Fees for higher or further education were abolished, while a considerable increase in the number of higher education institutions took place. A much needed school and college construction program was carried out, together with the introduction of postgraduate support for highly qualified graduates, providing them with the opportunity to earn their doctorates or undertake research studies.
Grants were introduced for pupils from lower income groups to stay on at school, together with grants for those going into any kind of higher or further education. Increases were also made in educational allowances, as well as spending on science. In 1972, the government allocated 2.1 million DM in grants to promote marriage and family education. Under the Approbationsordnung (medical education profession act) of 1970, the subject of psychosomatic medicine and psychotherapy at German universities became a compulsory subject for medical students, and that same year education of clinical and biomedical engineers was introduced. The Brandt Administration also introduced enabling legislation for the introduction of comprehensives, but left it to the Lander "to introduce them at their discretion." While the more left-wing Lander "rapidly began to do so," other Lander found "all sorts of pretexts for delaying the scheme." By the mid-Eighties, Berlin had 25 comprehensives while Bavaria only had 1, and in most Lander comprehensives were still viewed as "merely experimental."
In the field of housing, various measures were carried out to benefit householders, such as in improving the rights of tenants and increasing rental subsidies. The determination of the income of families taken into consideration for housing allowances was simplified, and increased levels of protection and support for low-income tenants and householders were introduced which led to a drop in the number of eviction notices. By 1974, three times as much was paid out in rent subsidies as in 1969, and nearly one and a half million households received rental assistance. Increases were made in public housing subsidies, as characterised by a 36% increase in the social housing budget in 1970 and by the introduction of a programme for the construction of 200,000 public housing units (1971). From 1970 to 1971, an 18.1% increase in building permits for social housing units was made. Other reforms aimed at improving tenants' rights included protection against conversion of rental housing into condominiums, the prohibition of the misappropriation of living space, new regulation of the apartment broker system, and a fee scale for engineers and architects. In addition, the income limits for eligibility for social housing were raised and adapted in order of general income trends. 
A loose form of rent regulation was introduced under the name of "Vergleichmieten" ('comparable rents'), together with the provision of "for family-friendly housing" freight or rent subsidies to owners of apartments or houses whose ceiling had been adapted to increased expenses or incomes (1970). In addition, a law for the creation of property for workers was passed, under which a married worker would normally keep up to 95% of his pay, and graded tax remission for married wage-earners applied up to a wage of 48,000 marks, which indicated the economic prosperity of West Germany at that time. The Town Planning Act (1971) encouraged the preservation of historical heritage and helped open up the way to the future of many German cities, while the Urban Renewal Act (1971) helped the states to restore their inner cities and to develop new neighbourhoods.
The Second Housing Allowance Law of December 1970 simplified the administration of housing allowances and extended entitlements, increased the income limit to 9,600 DM per year plus 2,400 DM for each family member, raised the general deduction on income to determine reckonable income from 15% to 20%, allowance rates listed in tables replacing complicated calculation procedure based on "bearable rent burdens." The Housing Construction Modification Law (1971) increased the income-limit for access to low rent apartments under the social housing programme from 9,000 DM to 12,000 DM per annum plus 3,000 DM (instead of 2,400) for each family member. The law also introduced special subsidies to reduce the debt burden for builders not surpassing the regular income-limit by more than 40%. Under a 1973 law, the limits were increased to 1,000 DM plus 9,000 DM and 4,200 DM for additional family members. The Rent Improvement Law (1971) strengthened the position of tenants. Under this legislation, notice was to be ruled illegal "where appropriate substitute accommodation not available; landlords obliged to specify reasons for notice," whilst the Eviction Protection Law (1971) established tenant protection against rent rises and notice. The notice was only lawful if in the "justified interest of the landlord." Under this law, higher rents were not recognised as "justified interest." The Second Eviction Protection Law (1972) made the tenant protection introduced under the Eviction Protection Law of 1971 permanent. Under this new law, the notice was only lawful where the landlord proved justified personal interest in the apartment. In addition, rent increases were only lawful if not above normal comparable rents in the same area.
Directives on the housing of foreign workers came into force in April 1971. These directives imposed certain requirements for space, hygiene, safety, and amenities in the accommodation offered by employers. That same year, the Federal Government granted a sum of 17 million DM to the Länder for the improvement and modernization of housing built before 21 June 1948. In addition, according to a 1971 regulation of the Board of the Federal Labour Office, “construction of workers’ hostels qualified for government financial support under certain conditions.” The "German Council for town development", which was set up by virtue of Article 89 of a law to foster urban building, was partly aimed at planning a favourable environment for families (such as the provision of playgrounds). In 1971, the Federal Labour Office made available DM 425 million in the form of loans to provide 157 293 beds in 2 494 hostels. A year later, the Federal Government (Bund), the Lander and the Federal Labour Office promoted the construction of dwellings for migrant workers. They set aside 10 million DM for this purpose, which allowed the financing of 1650 family dwellings that year.
Development measures were begun in 1972 with federal financial aid granted to the Lander for improvement measures relating to towns and villages, and in the 1972 budget, DM 50 million was earmarked, i.e. a third of the total cost of some 300 schemes. A council for urban development was formed in May 1972 with the purpose of promoting future work and measures in the field of urban renovation. In 1973, the government provided assistance of DM 28 million for the modernisation of old dwellings. New rules were introduced regarding improvements in the law relating to rented property, and control of the rise in rents and protection against cancellation of leases also safeguarded the rights of migrant workers in the sphere of housing. A law of July 1973 fixed the fundamental and minimum requirements regarding workers' dwellings, mainly concerning space, ventilation and lighting, protection against damp, heat and noise, power and heating facilities and sanitary installations.
In regards to civil rights, the Brandt Administration introduced a broad range of socially liberal reforms aimed at making West Germany a more open society. Greater legal rights for women were introduced, as exemplified by the standardisation of pensions, divorce laws, regulations governing use of surnames, and the introduction of measures to bring more women into politics. The voting age was lowered from 21 to 18, the age of eligibility for political office was lowered to 21, and the age of majority was lowered to 18 in March 1974. The Third Law for the Liberalization of the Penal Code (1970) liberalised "the right to political demonstration", while equal rights were granted to illegitimate children that same year. A 1971 amendment to a federal civil service reform bill enabled fathers to apply for part-time civil service work. In 1971, corporal punishment was banned in schools, and that same year a new Highway Code was introduced. In 1973, a measure was introduced that facilitated the adoption of young children by reducing the minimum age for adoptive parents from 35 to 25.
A number of reforms were also carried out to the armed forces, as characterised by a reduction in basic military training from 18 to 15 months, a reorganisation of education and training, and personnel and procurement procedures. Education for the troops was improved, a personnel reshuffle of top management in the Bundeswehr was carried out, academic education was mandated for officers beyond their basic military training, and a new recruiting policy for Bundeswehr personnel was introduced with the intention of building an army that reflected West Germany’s pluralistic society. Defense Minister Helmut Schmidt led the development of the first Joint Service Regulation ZDv 10/1 (Assistance for Innere Fuehrung, classified: restricted), which revitalized the concept of Innere Fuehrung while also affirming the value of the “citizen in uniform.” According to one study, as a result of this reform, “a strong civil mindset displaced the formerly dominant military mindset,” and forced the Bundeswehr’s elder generation to accept a new type of soldier envisioned by Schmidt. 
In 1970, the Armed Forces Vocational Schools and the Vocational Advancement Organization extended their services for the first time to conscripts, “so far as military duty permitted.” New enlistment bonuses were authorized and previous bonus schemes were improved, and new pay regulations were introduced that improved the financial situation of military personnel and civil servants. In July 1973, the 3rd Amendment to the Civilian Service Act came into force; “a prerequisite for the creation of additional civilian service places for recognized conscientious objectors.” The amendment provided that men recognized as conscientious objectors while performing military service should immediately be transferred to a civilian service assignment. The maximum amount for servicemen enlisting for at least 12 years was increased from DM 6,000 to DM 9,000, and from October 1971 onwards, long-term personnel were paid grants towards the cost ‘of attending educational institutes of the “second educational route" or participating in state-recognized general education courses provided by private correspondence schools and the “television college."' In 1972, two Bundeswehr universities were established; a reform which, according to one historian, “fought against the closed nature of the military and guaranteed that officers would be better able to successfully interact with the civilian world.” In addition, the position of non-commissioned officers was improved.
A women's policy machinery at the national level was established in 1972 while amnesty was guaranteed in minor offences connected with demonstrations. From 1970 onwards, parents as well as landlords were no longer legally prohibited “to give or rent rooms or flats to unmarried couples or to allow them to stay overnight.” In October 1972, the legal aid system was improved with the compensation paid to private lawyers for legal services to the poor increased. The Bausparkassen Act of 1972 placed all Bausparkassen (from January 1974 onwards) under the supervision of the Federal Banking Supervisory Office, and confined Bausparkassen “to the contract saving business and related activities.” The Animal Protection Act, passed in 1972, introduced various safeguards for animals such as not permitting the causing of pain, injury, or suffering to an animal without justification, and limiting experiments to the minimum number of animals necessary. In 1971, rules were introduced making it possible for former guestworkers “to receive an unlimited residence permit after a five-year stay.”
Legislation aimed at safeguarding consumers was also implemented under the Brandt Administration. The consumer's right of withdrawal in case of hire purchase was strengthened in March 1974, and fixed prices for branded products were abolished by law in January that same year, which meant that manufacturers' recommended prices were not binding for retailers. In addition, a progressive anticartel law was passed. A 1969 law on explosive materials was supplemented by two orders; the first (made in November 1969) establishing a committee of experts for explosive materials, while the second order (made the following month) included details for the implementation of the law on explosive materials. An Act of December 1959 on the peaceful use of nuclear energy and protection against its dangers was amended by an Act of June 1970 that established a tax levied for the costs for permissions and surveillance measures. The Law on Compensation for Measures of Criminal Prosecution and Penalties, passed in March 1971, provided for standardized compensation in certain situations.
In terms of working conditions, a number of reforms were introduced aimed at strengthening the rights of workers both at home and in the workplace. The Sickness Act of 1970 provided equal treatment of workers and employees in the event of incapacity for work, while maternity leave was increased. Legislation was introduced in 1970 which ensured continued payment of wages for workers disabled by illness. In 1970 all employees unit for work (with the exception of women in receipt of maternity benefits and temporarily and inconsiderably employed persons) were provided with an unconditional legal claim against their employer to continued payment of their gross wage for a period of 6 weeks, as also in the case of spa treatment approved by an Insurance Fund, the Fund bearing the full cost thereof. Previously, payment of employer's supplement and sick pay were only made from the day on which the doctor certified unfitness for work. In 1972, an Act on Agency Work was passed which sought to prevent works agencies from providing job placement services and aimed to provide job minimum protection for employees in agency work. A law on the hiring out of manpower, passed in October 1972, contained provisions to stipulate prior authorization for the hiring out of manpower, to draw a distinction between the system governing workers hired out and the placing of workers, to regulate and improve the rights of hired out workers pertaining to working conditions and social insurance, and provide for more severe penalties and fines to be imposed on offenders.
Improvements were also made in income and work conditions for home workers, accident insurance was extended to non-working adults, and the Border Zone Assistance Act (1971) increased levels of assistance to the declining zonal peripheral area. The Occupational Safety Act (1973) required employers to provide company doctors and safety experts. A directive on protection against noise at the place of work was adopted in November 1970. If measurements showed or there was reason to assume that a noise level guide value of 90 dB( A) may be exceeded at the place of work, then the authority had to instruct the employer to arrange check-ups of the employees concerned and these employees had to use personal noise protection devices. A matching fund program for 15 million employees was also introduced, which stimulated them to accumulate capital.
A ministerial order of January 1970 extended protection in cases of partial unemployment to home workers, while an ordinance of August 1970 fixed the conditions of health necessary for service in the merchant navy. A general provision of October 1970 determined in detail the circumstances in which the competent authority must take action on the basis of the act on the technical means of work. The requirement also stipulated the extent to which the technical standards established by national and international organisations can be regarded as “rules of the art.” In a directive of 10 November 1970, the Minister of Labour and Social Affairs recommended to the higher authorities for work protection of the "Lander" to bring in the directive published, in agreement with the Ministry of Labour, by the German Engineers' Association on the evaluation of work station noise in relation to loss of hearing, in order to improve safeguards for workers against the noises in question. In September 1971, an ordinance was published concerning dangerous working materials; safeguarding persons using these materials against the dangers involved. In August 1971, a law came into force directed at reducing atmospheric pollution from lead compounds in four-stroke engine fuels. As a safeguard against radiation, a decree on the system of authorisations for medicaments treated with ionizing radiation or containing radioactive substances, in its version of 8 August 1967, was remodelled by a new Decree of 10 May 1971 which added some radionuclides to the list of medicaments which doctors in private practice were authorized to use. A law on individual promotion of vocational training came into force in October 1971, which provided for financial grants for attendance at further general or technical teaching establishments from the second year of studies at higher technical schools, academies and higher education establishments, training centres of second degree, or certain courses of television teaching. Grants were also made in certain cases for attendance at training centres located outside the Federal Republic.
By a decree of the Federal Minister for Labour and Social Order, the Federal Institute for Industrial Protection became the Federal Agency for Industrial Protection and Accident Research. Amongst its designated tasks included the promotion of industrial protection, accident prevention on the journey to and from work and accident prevention in the home and leisure activities, the encouragement of training and advanced training in the area of industrial protection, and to promote and coordinate accident research. A regulation was issued in 1972 which permitted for the first time the employment of women as drivers of trams, omnibuses and lorries, while further regulations laid down new provisions for lifts and work with compressed air. The Factory Constitution Law (1971) strengthened the rights of individual employees "to be informed and to be heard on matters concerning their place of work." The Works Council was provided with greater authority while trade unions were given the right of entry into the factory "provided they informed the employer of their intention to do so," while a law was passed to encourage wider share ownership by workers and other rank-and-file employees. The Industrial Relations Law (1972) and the Personnel Representation Act (1974) broadened the rights of employees in matters which immediately affected their places of work, while also improving the possibilities for codetermination on operations committees, together with access of trade unions to companies.
The Works Constitution Act of 1972 required in cases of collective dismissal at an establishment normally employing more than twenty employees that management and the works council must negotiate a social plan that stipulates compensation for workers who lose their jobs. In cases where the two parties could not agree on a social plan, the law provided for binding arbitration. In 1972, the rights of works councils to information from management were not only strengthened, but works councils were also provided with full codetermination rights on issues such as working time arrangements in the plant, the setting of piece rates, plant wage systems, the establishment of vacation times, work breaks, overtime, and short-time work. Legislation was passed which acknowledged for the first time the presence of trade unions in the workplace, expanded the means of action of the works councils, and improved their work basics as well as those of the youth councils.
A law of January 1972 on the organization of labour in enterprises significantly extended the works council's right of cooperation and co-management in the matter of vocational training. That same year, the Safety Institute of the Federal Republic of Germany was transformed into a public Federal Agency (Bundesanstalt) with significantly enlarged powers, in the context of which special emphasis would be placed on its new task of promoting and coordinating research in the area of accident prevention. New provisions were introduced for the rehabilitation of severely disabled people ("Schwerbehinderte") and accident victims. The Severely Disabled Persons Act of April 1974 obliged all employers with more than fifteen employees to ensure that 6% of their workforce consisted of people officially recognised as being severely handicapped. Employers who failed to do so were assessed 100 DM per month for every job falling before the required quota. These compensatory payments were used to "subsidise the adaptation of workplaces to the requirements of those who were severely handicapped."
A law passed in January 1974, designed to protect members of the supervisory boards of companies who are undergoing training, was aimed at ensuring that the representatives of young workers and youthful members of works councils still undergoing training could perform their duties with greater independence and without fear of disadvantageous consequences for their future careers. On request, workers' representatives on completion of their training courses had to have an employment relationship of unlimited duration. In the field of transport, the Municipal Transportation Finance Law of 1971 established federal guidelines for subsidies to municipal governments, while the Federal Transport Plan of 1973 provided a framework for all transport, including public transport.
A federal environmental programme was established in 1971, and in 1972 laws were passed to regulate garbage elimination and air pollution via emission. Matching grants covering 90% of infrastructure development were allocated to local communities, which led to a dramatic increase in the number of public swimming pools and other facilities of consumptive infrastructure throughout West Germany. The federal crime-fighting apparatus was also modernised, while a Foreign Tax Act was passed which limited the possibility of tax evasion. In addition, efforts were made to improve the railways and motorways. In 1971, a law was passed setting the maximum lead content at 0.4 grams per liter of gasoline, and in 1972 DDT was banned. The Federal Immissions Control Law, passed in March 1974, provided protection from noxious gases, noise, and air-borne particulate matter.
Under the Brandt Administration, West Germany attained a lower rate of inflation than in other industrialised countries at that time, while a rise in the standard of living took place, helped by the floating and revaluation of the mark. This was characterised by the real incomes of employees increasing more sharply than incomes from entrepreneurial work, with the proportion of employees' incomes in the overall national income rising from 65% to 70% between 1969 and 1973, while the proportion of income from entrepreneurial work and property fell over that same period from just under 35% to 30%.
1972 crisis.
Brandt's "Ostpolitik" led to a meltdown of the narrow majority Brandt's coalition enjoyed in the "Bundestag". In October 1970, FDP deputies Erich Mende, Heinz Starke, and Siegfried Zoglmann crossed the floor to join the CDU. On 23 February 1972, SPD deputy Herbert Hupka, who was also leader of the "Bund der Vertriebenen", joined the CDU in disagreement with Brandt's reconciliatory efforts towards the east. On 23 April 1972, Wilhelm Helms (FDP) left the coalition ; the FDP politicians Knud von Kühlmann-Stumm and Gerhard Kienbaum also declared that they would vote against Brandt; thus, Brandt had lost his majority. On 24 April 1972 a vote of no confidence was proposed and it was voted on three days later. Had this motion passed, Rainer Barzel would have replaced Brandt as chancellor. To everyone's surprise, the motion failed: Barzel got only 247 votes out of 260 ballots; for an absolute majority, 249 votes would have been necessary. There were also 10 votes against the motion and three invalid ballots. Most deputies of SPD and FDP did not take part in the voting, as not voting had the same effect as voting for Brandt. 
New elections.
Though Brandt remained chancellor, he had lost his majority. Subsequent initiatives in parliament, most notably on the budget, failed. Because of this stalemate, the Bundestag was dissolved and new elections were called. During the 1972 campaign, many popular West German artists, intellectuals, writers, actors and professors supported Brandt and the SPD. Among them were Günter Grass, Walter Jens, and even the soccer player Paul Breitner. Brandt's ' as well as his reformist domestic policies were popular with parts of the young generation and he led the SPD to its best-ever federal election result in late 1972. The ', Brandt's landslide win was the beginning of the end; and Brandt's role in government started to decline.
Many of Brandt's reforms met with resistance from state governments (dominated by CDU/CSU). The spirit of reformist optimism was cut short by the 1973 oil crisis and the major public services strike 1974, which gave Germany's trade unions, led by Heinz Kluncker, a big wage increase but reduced Brandt's financial leeway for further reforms. Brandt was said to be more a dreamer than a manager and was personally haunted by depression. To counter any notions about being sympathetic to Communism or soft on left-wing extremists, Brandt implemented tough legislation that barred "radicals" from public service ("").
Guillaume affair.
Around 1973, West German security organizations received information that one of Brandt's personal assistants, Günter Guillaume, was a spy for the East German intelligence services. Brandt was asked to continue working as usual, and he agreed to do so, even taking a private vacation with Guillaume. Guillaume was arrested on 24 April 1974, and many blamed Brandt for having a communist spy in his inner circle.
Brandt resigned from his position as chancellor on 6 May 1974, but he remained member of the Bundestag and chairman of the Social Democrats through 1987.
This espionage affair is widely considered to have been just the trigger for Brandt's resignation, not the fundamental cause. As Brandt himself later said, "I was exhausted, for reasons which had nothing to do with the affair Guillaume espionage scandal going on at the time."
Brandt was dogged by scandals about serial adultery, and reportedly also struggled with alcohol and depression. There was also the economic fallout on West Germany of the 1973 oil crisis, which may seem to have given enough stress to finish off Brandt as the Chancellor.
Guillaume had been an espionage agent for East Germany, who was supervised by Markus Wolf, the head of the 
Main Directorate for Reconnaissance ("Hauptverwaltung Aufklärung" or HVA—the foreign intelligence service) of the East German Ministry for State Security. Wolf stated after the reunification that the resignation of Brandt had never been intended, and that the planting and handling of Guillaume had been one of the largest mistakes of the East German secret services.
Brandt was succeeded as the Chancellor of the Bundesrepublik by his fellow Social Democrat, Helmut Schmidt. For the rest of his life, Brandt remained suspicious that his fellow Social Democrat (and longtime rival) Herbert Wehner had been scheming for Brandt's downfall. However, there is scant evidence to corroborate this suspicion.
Ex-Chancellor.
After his term as the Chancellor, Brandt retained his seat in the Bundestag, and he remained the Chairman of the Social Democratic Party through 1987. Beginning in 1987, Brandt stepped down to become the Honorary Chairman of the party. Brandt was also a member of the European Parliament from 1979 to 1983.
Socialist International.
For sixteen years, Brandt was the president of the Socialist International (1976–92), during which period the number of Socialist International's mainly European member parties grew until there were more than a hundred socialist, social democratic, and labour political parties around the world. For the first seven years, this growth in SI membership had been prompted by the efforts of the Socialist International's Secretary-General, the Swede Bernt Carlsson. However, in early 1983, a dispute arose about what Carlsson perceived as the SI president's authoritarian approach. Carlsson then rebuked Brandt saying: "this is a Socialist International — not a German International".
Next, against some vocal opposition, Brandt decided to move the next Socialist International Congress from Sydney, Australia to Portugal. Following this SI Congress in April 1983, Brandt retaliated against Carlsson by forcing him to step down from his position. However, the Austrian Prime Minister, Bruno Kreisky, argued on behalf of Brandt: "It is a question of whether it is better to be pure or to have greater numbers".
Carlsson was succeeded by the Finn, Pentti Väänänen as Secretary General of the Socialist International 
During Willy Brandt's presidency the SI developed activities and dialogue on a number of International issues. This concerned the East-West conflict and arms race where the SI held high level consultations with the leaderships of the United States and the Soviet Union. The SI met with such leaders as President Jimmy Carter and Vice-Presidents Walter Mondale and George Bush. They also met with the Secretaries General Leonid Brezhnev and Michail Gorbachev and with the Soviet President Andrei Gromyko. The SI also developed active contacts to promote dialogue concerning regional conflicts. Those included the Middle East, where they helped to build contacts between Israel and the PLO, and also in Southern Africa and Central America.
Brandt Report.
In 1977, Brandt was appointed as the chairman of the Independent Commission for International Developmental Issues. This produced a report in 1980, which called for drastic changes in the global attitude towards development in the Third World. This became known as the Brandt Report.
Reunification.
In October 1979, Brandt met with the East German dissident, Rudolf Bahro, who had written "The Alternative". Bahro and his supporters were attacked by the East German state security organization Stasi, headed by Erich Mielke, for his writings, which had laid the theoretical foundation of a leftist opposition to the ruling SED party and its dependent allies, and which promoted new and changed parties. All of this is now described as "change from within". Brandt had asked for Bahro's release, and Brandt welcomed Bahro's theories, which advanced the debate within his own Social Democratic Party. In late 1989, Brandt became one of the first leftwing leaders in West Germany to publicly favor a quick reunification of Germany, instead of some sort of two-state federation or other kind of interim arrangement. Brandt's public statement "Now grows together what belongs together," was widely quoted in those days.
Hostages in Iraq.
One of Brandt's last public appearances was in flying to Baghdad, Iraq, to free Western hostages held by Saddam Hussein, following the Iraqi invasion of Kuwait in 1990. Brandt secured the release of a large number of them, and on 9 November 1990, his airplane landed with 174 freed hostages on board at the Frankfurt Airport.
Death and memorials.
Willy Brandt died of colon cancer at his home in Unkel, a town on the Rhine River, on 8 October 1992, and was given a state funeral. He was buried at the cemetery at Zehlendorf in Berlin.
The Federal Chancellor Willy Brandt Foundation was erected in 1994. It serves to honor the memory of Brandt's political accomplishents and his commitment to peace, freedom and democracy. The foundation runs two permanent exhibitions – one in Berlin, and the other in Lübeck, where Brandt was born. Other works of the foundation include the edition of Brandt's papers, speeches and letters (the Berlin Edition), historical research as well as organizing lectures and international conferences.
When the SPD moved its headquarters from Bonn back to Berlin in the mid-1990s, the new headquarters was named the "Willy Brandt Haus". One of the buildings of the European Parliament in Brussels was named after him in 2008.
On 6 December 2000, a memorial to Willy Brandt and "Warschauer Kniefall" was unveiled in Warsaw, Poland.
German artist Johannes Heisig painted several portraits of Brandt of which one was unveiled as part of an honoring event at German Historical Institute Washington, DC on 18 March 2003. Spokesmen amongst others were former German Federal Minister Egon Bahr and former U.S. Secretary of state Henry Kissinger.
In 2009, the Willy-Brandt-Memorial was opened up in Nuremberg at the Willy-Brandt Square. It was created by the artist .
In 2009, the University of Erfurt renamed its graduate school of public administration as the Willy Brandt School of Public Policy. A private German-language secondary school in Warsaw, Poland, is also named after Brandt.
The main boulevard on the north entrance to Montenegrin capital Podgorica was named Willy Brandt Boulevard in 2011.
Willy Brandt also has an unusual memorial in Hammersmith in London, United Kingdom. In 1963, when he was Mayor of West Berlin, Brandt travelled to Hammersmith with a street lamp from West Berlin, and presented it to the Mayor of Hammersmith to mark its twinning with Neukölln. The lamp now stands on the wall of Westcott Lodge, facing Furnival Gardens, with a commemorative plaque below it.
Brandt's family.
From 1941 until 1948 Brandt was married to Anna Carlotta Thorkildsen (the daughter of a Norwegian father and a German-American mother). They had a daughter, Ninja Brandt (born in 1940). After Brandt and Thorkildsen were divorced in 1948, Brandt married the Norwegian-born German writer Rut Hansen in the same year. Hansen and Brandt had three sons: (born in 1948), (born in 1951) and (born in 1961). After 32 years of marriage, Willy Brandt and Rut Hansen Brand divorced in 1980, and from the day that they were divorced they never saw each other again. On 9 December 1983, Brandt married (born in 1946).

</doc>
<doc id="49261" url="https://en.wikipedia.org/wiki?curid=49261" title="Corporate personhood">
Corporate personhood

Corporate personhood is the legal notion that a corporation, separately from its associated human beings (like owners, managers, or employees), has some, but not all, of the legal rights and responsibilities enjoyed by natural persons (physical humans). For example, corporations have the right to enter into contracts with other parties and to sue or be sued in court in the same way as natural persons or unincorporated associations of persons.
Corporate personhood in the United States.
As a matter of interpretation of the word "person" in the Fourteenth Amendment, U.S. courts have extended certain constitutional protections to corporations. Some opponents of corporate personhood seek to amend the U.S. Constitution to limit these rights to those provided by state law and state constitutions.
The basis for allowing corporations to assert protection under the U.S. Constitution is that they are organizations of people, and the people should not be deprived of their constitutional rights when they act collectively. In this view, treating corporations as "persons" is a convenient legal fiction which allows corporations to sue and to be sued, provides a single entity for easier taxation and regulation, simplifies complex transactions that would otherwise involve, in the case of large corporations, thousands of people, and protects the individual rights of the shareholders as well as the right of association.
Generally, corporations are not able to claim constitutional protections that would not otherwise be available to persons acting as a group. For example, the Supreme Court has not recognized a Fifth Amendment right against self-incrimination for a corporation, since the right can be exercised only on an individual basis. In "United States v. Sourapas and Crest Beverage Company", "[suggested the use of the word 'taxpayer' several times in the regulations requires the fifth-amendment self-incrimination warning be given to a corporation." The Court did not agree.
Since the Supreme Court's ruling in "Citizens United v. Federal Election Commission" in 2010, upholding the rights of corporations to make political expenditures under the First Amendment, there have been several calls for a U.S. Constitutional amendment to abolish Corporate Personhood. While the Citizens United majority opinion makes no reference to corporate personhood or the Fourteenth Amendment, Justice Stevens' dissent claims that the majority opinion relies on an incorrect treatment of corporations' First Amendment rights as identical to those of individuals.
Historical background in the United States.
During the colonial era, British corporations were chartered by the crown to do business in North America. This practice continued in the early United States. They were often granted monopolies as part of the chartering process. For example, the controversial Bank Bill of 1791 chartered a 20-year corporate monopoly for the First Bank of the United States. Although the Federal government has from time to time chartered corporations, the general chartering of corporations has been left to the states. In the late 18th and early 19th centuries, corporations began to be chartered in greater numbers by the states, under general laws allowing for incorporation at the initiative of citizens, rather than through specific acts of the legislature.
The degree of permissible government interference in corporate affairs was controversial from the earliest days of the nation. In 1790, John Marshall, a private attorney and a veteran of the Continental Army, represented the board of the College of William and Mary, in litigation that required him to defend the corporation's right to reorganize itself and in the process remove professors, "The Rev John Bracken v. The Visitors of Wm & Mary College" (7 Va. 573; 1790 Supreme Court of Virginia). The Supreme Court of Virginia ruled that the original crown charter provided the authority for the corporation's Board of Visitors to make changes including the reorganization.
As the 19th century matured, manufacturing in the U.S. became more complex as the Industrial Revolution generated new inventions and business processes. The favored form for large businesses became the corporation because the corporation provided a mechanism to raise the large amounts of investment capital large business required, especially for capital intensive yet risky projects such as railroads.
The Civil War accelerated the growth of manufacturing and the power of the men who owned the large corporations. Businessmen such as Mark Hanna, sugar trust magnate Henry O. Havemeyer, banker J. P. Morgan, steel makers Charles M. Schwab and Andrew Carnegie, and railroad owners Cornelius Vanderbilt and Jay Gould created corporations which influenced legislation at the local, state, and federal levels as they built businesses that spanned multiple states and communities. After the adoption of the 14th Amendment in 1868, there was some question as to whether the Amendment applied to other than freed slaves, and whether its protections could be invoked by corporations and other organizations of persons.
The primary purpose of the 14th Amendment was undoubtedly to protect freed slaves. However, the Amendment applies to all Americans, not only freed slaves and their descendants.
Following the reasoning of the Dartmouth College case and other precedents (see below, Case law in the United States), corporations could exercise the rights of their shareholders and these shareholders were entitled to some of the legal protections against arbitrary state action. Their cause was strengthened by the adoption of general incorporation statutes in the states in the late 19th century, most notably in New Jersey and Delaware, which allowed anyone to form corporations without any particular government grant or authorization, and thus without the government-granted monopolies that had been common in charters granted by the Crown or by acts of the legislature. See Delaware General Corporation Law. In "Santa Clara County v. Southern Pacific Railroad" (1886), the Supreme Court held, "ipse dixit", that the Fourteenth Amendment applied to corporations. Since then the Court has repeatedly reaffirmed this protection.
Case law in the United States.
In 1818, the United States Supreme Court decided "Trustees of Dartmouth College v. Woodward" – 17 U.S. 518 (1819), writing: "The opinion of the Court, after mature deliberation, is that this corporate charter is a contract, the obligation of which cannot be impaired without violating the Constitution of the United States. This opinion appears to us to be equally supported by reason, and by the former decisions of this Court." Beginning with this opinion, the U.S. Supreme Court has continuously recognized corporations as having the same rights as natural persons to contract and to enforce contracts.
Seven years after the Dartmouth College opinion, the Supreme Court decided "Society for the Propagation of the Gospel in Foreign Parts v. Town of Pawlet" (1823), in which an English corporation dedicated to missionary work, with land in the U.S., sought to protect its rights to the land under colonial-era grants against an effort by the state of Vermont to revoke the grants. Justice Joseph Story, writing for the court, explicitly extended the same protections to corporate-owned property as it would have to property owned by natural persons. Seven years later, Chief Justice Marshall stated: "The great object of an incorporation is to bestow the character and properties of individuality on a collective and changing body of men."
In the 1886 case "Santa Clara v. Southern Pacific" – 118 U.S. 394 (1886), the Chief Justice Waite of the Supreme Court orally directed the lawyers that the Fourteenth Amendment equal protection clause guarantees constitutional protections to corporations in addition to natural persons, and the oral argument should focus on other issues in the case. In the Santa Clara case the court reporter, Bancroft Davis, noted in the headnote to the opinion that the Chief Justice Morrison Waite began oral argument by stating, "The court does not wish to hear argument on the question whether the provision in the Fourteenth Amendment to the Constitution, which forbids a State to deny to any person within its jurisdiction the equal protection of the laws, applies to these corporations. We are all of the opinion that it does." While the headnote is not part of the Court's opinion and thus not precedent, two years later, in "Pembina Consolidated Silver Mining Co. v. Pennsylvania" – 125 U.S. 181 (1888), the Court clearly affirmed the doctrine, holding, "Under the designation of 'person' there is no doubt that a private corporation is included the Fourteenth Amendment. Such corporations are merely associations of individuals united for a special purpose and permitted to do business under a particular name and have a succession of members without dissolution." This doctrine has been reaffirmed by the Court many times since.
The 14th Amendment does not insulate corporations from all government regulation, any more than it relieves individuals from all regulatory obligations. Thus, for example, in "Northwestern Nat Life Ins. Co. v. Riggs" (203 U.S. 243 (1906)), the Court accepted that corporations are for legal purposes "persons," but still ruled that the Fourteenth Amendment was not a bar to many state laws which effectively limited a corporation's right to contract business as it pleased. However, this was not because corporations were not protected under the Fourteenth Amendment - rather, the Court's ruling was that the Fourteenth Amendment did not prohibit the type of regulation at issue, whether of a corporation or of sole proprietorship or partnership.
Opinions by two long serving Supreme Court judges, Hugo Black and William O. Douglas, indicate the extent to which corporate personhood is not an all-or-nothing doctrine, but rather relates to the purpose of government regulation and the underlying rights of the individuals making up the corporation. In a case challenging corporate tax rates, Justice Black wrote:
If the people of this nation wish to deprive the states of their sovereign rights to determine what is a fair and just tax upon corporations doing a purely local business within their own state boundaries, there is a way provided by the Constitution to accomplish this purpose. That way does not lie along the course of judicial amendment to that fundamental charter. An amendment having that purpose could be submitted by Congress as provided by the Constitution. I do not believe that the Fourteenth Amendment had that purpose, nor that the people believed it had that purpose, nor that it should be construed as having that purpose.
Justice Douglas, dissenting in "Wheeling Steel Corp. v. Glander" (337 U.S. 562, 1949), gave an opinion similar to, but shorter than, the one quoted above, to which Justice Black concurred.
By the time of those opinions, political contributions to candidates in federal races by corporations had been prohibited since the Tillman Act of 1907, even though individual contributions remained unlimited. Yet both Justice Black and Justice Douglas dissented from the Supreme Court's 1957 decision in "United States v. United Auto Workers", 352 U.S. 567 (1957), in which the Court, on procedural grounds, overruled a lower court decision striking down the prohibition on corporate and union political expenditures:
We deal here with a problem that is fundamental to the electoral process and to the operation of our democratic society. It is whether a union can express its views on the issues of an election and on the merits of the candidates, unrestrained and unfettered by the Congress. The principle at stake is not peculiar to unions. It is applicable as well to associations of manufacturers, retail and wholesale trade groups, consumers' leagues, farmers' unions, religious groups, and every other association representing a segment of American life and taking an active part in our political campaigns and discussions. It is as important an issue as has come before the Court, for it reaches the very vitals of our system of government.
Under our Constitution, it is We The People who are sovereign. The people have the final say. The legislators are their spokesmen. The people determine through their votes the destiny of the nation. It is therefore important -- vitally important -- that all channels of communication be open to them during every election, that no point of view be restrained or barred, and that the people have access to the views of every group in the community. Thus the two justices would have adjudicated the case and upheld the lower court opinion striking down the ban on corporate and union spending.
Although it is now well settled law that the 14th Amendment extends to corporations, the extent to which it should attach to corporations has continued to draw criticism from liberal legal theorists.
Legislation in the United States.
The laws of the United States hold that a legal entity (like a corporation or non-profit organization) shall be treated under the law as a person except when otherwise noted. This rule of construction is specified in 1 U.S.C. §1 (United States Code), which states:
In determining the meaning of any Act of Congress, unless the context indicates otherwise--
the words "person" and "whoever" include corporations, companies, associations, firms, partnerships, societies, and joint stock companies, as well as individuals;
This federal statute has many consequences. For example, a corporation is allowed to own property and enter contracts. It can also sue and be sued and held liable under both civil and criminal law. As well, because the corporation is legally considered the "person," individual shareholders are not legally responsible for the corporation's debts and damages beyond their investment in the corporation. Similarly, individual employees, managers, and directors are liable for their own malfeasance or lawbreaking while acting on behalf of the corporation, but are not generally liable for the corporation's actions. Among the most frequently discussed and controversial consequences of corporate personhood in the United States is the extension of a limited subset of the same constitutional rights.
Corporations as legal entities have always been able to perform commercial activities, similar to a person acting as a sole proprietor, such as entering into a contract or owning property. Therefore, corporations have always had a 'legal personality' for the purposes of conducting business while shielding individual shareholders from personal liability (i.e., protecting personal assets which were not invested in the corporation).
Broadcaster Thom Hartmann has argued that the Santa Clara County case was not intended to extend equal protection to corporations. Chief Justice Waite wrote in private correspondence; "we avoided meeting the question." Hartmann writes that correspondence between Waite and Bancroft Davis (available in the Library of Congress) demonstrates Waite did not intend to create a legal precedent.
Ralph Nader, Phil Radford and others have argued that a strict originalist philosophy should reject the doctrine of corporate personhood under the Fourteenth Amendment. Indeed, Chief Justice William Rehnquist repeatedly criticized the Court's invention of corporate constitutional "rights," most famously in his dissenting opinion in the 1978 case "First National Bank of Boston v. Bellotti"; though, in "Bellotti", Justice Rehnquist's objections are based on his "views of the limited application of the First Amendment to the States" and not on whether corporations qualify as "persons" under the Fourteenth Amendment. Nonetheless, these justices' rulings have continued to affirm the assumption of corporate personhood, as the Waite court did, and Justice Rehnquist himself eventually endorsed the right of corporations to spend in elections (the majority view in Bellotti) in his dissenting opinion in "McConnell v. FEC".
Corporate political spending.
A central point of debate in recent years has been what role corporate money plays and should play in democratic politics. This is part of the larger debate on campaign finance reform and the role which money may play in politics.
In the United States, legal milestones in this debate include:
The corporate personhood aspect of the campaign finance debate turns on "Buckley v. Valeo" (1976) and "Citizens United v. Federal Election Commission" (2010): "Buckley" ruled that political spending is protected by the First Amendment right to free speech, while "Citizens United" ruled that corporate political spending is protected, holding that corporations have a First Amendment right to free speech. Opponents of these decisions have argued that if all corporate rights under the Constitution were abolished, it would clear the way for greater regulation of campaign spending and contributions. It should be noted, however, that neither decision relied on the concept of corporate personhood, and the Buckley decision in particular deals with the rights of individuals and political committees, not corporations.
Misconceptions.
In debates on this topic it is sometimes asserted that the notion of corporate personhood implies that corporations are entitled to all of the rights and privileges that apply to natural persons (i.e., human beings). However, the definition of corporate personhood includes some, but not all of said rights and privileges. Whether a corporation is a "person" possessing any one of those rights or privileges is properly decided by applying basic logic, common sense, and relevant and valid law to an examination of generally accepted reasons why the state grants existence to the legal fiction of the corporate form, so that on the one hand courts may hold that corporations must have the right to own property or enter into contracts, or to be subject to municipal zoning laws that apply to "persons" without necessarily having the same speech rights enjoyed by natural persons and without having the right to vote and without counting as a second "person" for the purpose of driving in a carpool lane.

</doc>
<doc id="49263" url="https://en.wikipedia.org/wiki?curid=49263" title="Lake Baringo">
Lake Baringo

Lake Baringo is, after Lake Turkana, the most northern of the Kenyan Rift Valley lakes, with a surface area of about and an elevation of about . The lake is fed by several rivers, Molo, Perkerra and Ol Arabel, and has no obvious outlet; the waters are assumed to seep through lake sediments into the faulted volcanic bedrock. It is one of the two freshwater lakes in the Rift Valley in Kenya, the other being Lake Naivasha.
It lies off the beaten track in a hot and dusty setting and over 470 species of birds have been recorded there, occasionally including migrating flamingos. A Goliath heronry is located on a rocky islet in the lake known as Gibraltar.
Description.
The lake is part of the East African Rift system. The Tugen Hills, an uplifted fault block of volcanic and metamorphic rocks, lies west of the lake. The Laikipia Escarpment lies to the east.
Water flows into the lake from the Mau Hills and Tugen Hills. It is a critical habitat and refuge for more than 500 species of birds and fauna, some of the migratory waterbird species being significant regionally and globally. The lake also provides an invaluable habitat for seven fresh water fish species. One, "Oreochromis niloticus baringoensis" (a Nile tilapia subspecies), is endemic to the lake. Lake fishing is important to local social and economic development. Additionally the area is a habitat for many species of animals including the hippopotamus ("Hippopotamus amphibius"), Nile crocodile ("Crocodylus niloticus") and many other mammals, amphibians, reptiles and the invertebrate communities.
While stocks of Nile tilapia in the lake are now low, the decline of this species has been mirrored by the success of another, the marbled lungfish ("Protopterus aethiopicus") which was introduced to the lake in 1974 and which now provides the majority of fish output from the lake. Water levels have been reduced by droughts and over-irrigation. The lake is commonly turbid with sediment, partly due to intense soil erosion in the catchment, especially on the Loboi Plain south of the lake.
The lake has several small islands, the largest being Ol Kokwe Island. Ol Kokwe, an extinct volcanic centre related to Korosi volcano north of the lake, has several hot springs and fumaroles, some of which have precipitated sulfur deposits. A group of hot springs discharge along the shoreline at Soro near the northeastern corner of the island.
Several important archaeological and palaeontological sites, some of which have yielded fossil hominoids and hominins, are present in the Miocene to Pleistocene sedimentary sequences of the Tugen Hills.
The main town near the lake is Marigat, while smaller settlements include Kampi ya Samaki and Loruk. The area is increasingly visited by tourists and is situated at the southern end of a region of Kenya inhabited largely by pastoralist ethnic groups including Il Chamus, Rendille, Turkana and Kalenjin. Accommodation (hotels, self-catering cottages and camping sites) as well as boating services are available at and near Kampi-Ya-Samaki on the western shore, as well as on several of the islands in the lake.

</doc>
