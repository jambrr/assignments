<doc id="50562" url="https://en.wikipedia.org/wiki?curid=50562" title="Smyrna">
Smyrna

Smyrna () was an Ancient Greek city located at a central and strategic point on the Aegean coast of Anatolia. This place is known today as İzmir, Turkey. Due to its advantageous port conditions, its ease of defence and its good inland connections, Smyrna rose to prominence. Two sites of the ancient city are today within the boundaries of İzmir. The first site, probably founded by indigenous peoples, rose to prominence during the Archaic Period as one of the principal ancient Greek settlements in western Anatolia. The second, whose foundation is associated with Alexander the Great, reached metropolitan proportions during the period of the Roman Empire. Most of the present-day remains of the ancient city date from the Roman era, the majority from after a 2nd-century AD earthquake.
In practical terms, a distinction is often made between these. Old Smyrna was the initial settlement founded around the 11th century BC, first as an Aeolian settlement, and later taken over and developed during the Archaic Period by the Ionians. Smyrna proper was the new city which residents moved to as of the 4th century BC and whose foundation was inspired by Alexander the Great.
Old Smyrna was located on a small peninsula connected to the mainland by a narrow isthmus at the northeastern corner of the inner Gulf of İzmir, at the edge of a fertile plain and at the foot of Mount Yamanlar. This Anatolian settlement commanded the gulf. Today, the archeological site, named "Bayraklı Höyüğü," is approximately inland, in the Tepekule neighbourhood of Bayraklı at .
"New" Smyrna developed simultaneously on the slopes of the Mount Pagos (Kadifekale today) and alongside the coastal strait, immediately below where a small bay existed until the 18th century.
The core of the late Hellenistic and early Roman Smyrna is preserved in the large area of İzmir Agora Open Air Museum at this site. Research is being pursued at the sites of both the old and the new cities. This has been conducted since 1997 for Old Smyrna and since 2002 for the Classical Period city, in collaboration between the İzmir Archaeology Museum and the Metropolitan Municipality of İzmir.
History.
Etymology.
Several explanations have been offered for its name. A Greek myth derived the name from an eponymous Amazon named "Σμύρνα" (Smyrna), which was also the name of a quarter of Ephesus. This is the basis of Myrina, a city of Aeolis.
In inscriptions and coins, the name often was written as "Ζμύρνα" ("Zmyrna"), "Ζμυρναῖος" ("Zmyrneos"), "of Smyrna".
The name Smyrna may also have been taken from the ancient Greek word for myrrh, "smyrna", which was the chief export of the city in ancient times.
Third millennium to 687 BC.
The region was settled at least as of the beginning of the third millennium BC, or perhaps earlier, as the recent finds in Yeşilova Höyük suggests. It could have been a city of the autochthonous Leleges before the Greek colonists started to settle along the coast of Asia Minor as of the beginning of the first millennium BC. Throughout antiquity Smyrna was a leading city-state of Ionia, with influence over the Aegean shores and islands. Smyrna was also among the cities that claimed Homer as a resident.
The early Aeolian Greek settlers of Lesbos and Cyme, expanding eastwards, occupied the valley of Smyrna. It was one of the confederacy of Aeolian city-states, marking the Aeolian frontier with the Ionian colonies.
Strangers or refugees from the Ionian city of Colophon settled in the city. During an uprising in 688 BC, they took control of the city, making it the thirteenth of the Ionian city-states. Revised mythologies said it was a colony of Ephesus. In 688 BC, the Ionian boxer Onomastus of Smyrna won the prize at Olympia, but the "coup" was probably then a recent event. The Colophonian conquest is mentioned by Mimnermus (before 600 BC), who counts himself equally of Colophon and of Smyrna. The Aeolic form of the name was retained even in the Attic dialect, and the epithet "Aeolian Smyrna" remained current long after the conquest.
Smyrna was located at the mouth of the small river Hermus and at the head of a deep arm of the sea ("Smyrnaeus Sinus") that reached far inland. This enabled Greek trading ships to sail into the heart of Lydia, making the city part of an essential trade route between Anatolia and the Aegean. During the 7th century BC, Smyrna rose to power and splendor. One of the great trade routes which cross Anatolia descends the Hermus valley past Sardis, and then, diverging from the valley, passes south of Mount Sipylus and crosses a low pass into the little valley where Smyrna lies between the mountains and the sea. Miletus and later Ephesus were situated at the sea end of the other great trade route across Anatolia; they competed for a time successfully with Smyrna; but after both cities' harbors silted up, Smyrna was without a rival.
The Meles River, which flowed by Smyrna, is famous in literature and was worshiped in the valley. A common and consistent tradition connects Homer with the valley of Smyrna and the banks of the Meles; his figure was one of the stock types on coins of Smyrna, one class of which numismatists call "Homerian." The epithet "Melesigenes" was applied to him; the cave where he was wont to compose his poems was shown near the source of the river; his temple, the "Homereum", stood on its banks. The steady equable flow of the Meles, alike in summer and winter, and its short course, beginning and ending near the city, are celebrated by Aristides and Himerius. The stream rises from abundant springs east of the city and flows into the southeast extremity of the gulf.
The archaic city ("Old Smyrna") contained a temple of Athena from the 7th century BC.
Lydian period.
When the Mermnad kings raised the Lydian power and aggressiveness, Smyrna was one of the first points of attack. Gyges (ca. 687—652 BC) was, however, defeated on the banks of the Hermus, the situation of the battlefield showing that the power of Smyrna extended far to the east. A strong fortress was built probably by the Smyrnaean Ionians to command the valley of Nymphi, the ruins of which are still imposing, on a hill in the pass between Smyrna and Nymphi.
According to Theognis (c. 500 BC), it was pride that destroyed Smyrna. Mimnermus laments the degeneracy of the citizens of his day, who could no longer stem the Lydian advance. Finally, Alyattes II (609—560 BC) conquered the city and sacked it, and though Smyrna did not cease to exist, the Greek life and political unity were destroyed, and the "polis" was reorganized on the village system. Smyrna is mentioned in a fragment of Pindar and in an inscription of 388 BC, but its greatness was past.
Hellenistic period.
Alexander the Great conceived the idea of restoring the Greek city in a scheme that was, according to Strabo, actually carried out under Antigonus (316—301 BC) and Lysimachus (301 BC—281 BC), who enlarged and fortified the city. The ruined acropolis of the ancient city, the "crown of Smyrna", had been on a steep peak about high, which overhangs the northeast extremity of the gulf. Modern İzmir was constructed atop the later Hellenistic city, partly on the slopes of a rounded hill the Greeks called "Pagos" near the southeast end of the gulf, and partly on the low ground between the hill and the sea. The beauty of the Hellenistic city, clustering on the low ground and rising tier over tier on the hillside, was frequently praised by the ancients and is celebrated on its coins.
Smyrna is shut in on the west by a hill now called Deirmen Tepe, with the ruins of a temple on the summit. The walls of Lysimachus crossed the summit of this hill, and the acropolis occupied the top of Pagus. Between the two the road from Ephesus entered the city by the Ephesian gate, near which was a gymnasium. Closer to the acropolis the outline of the stadium is still visible, and the theatre was situated on the north slopes of Pagus. Smyrna possessed two harbours. The outer harbour was simply the open roadstead of the gulf, and the inner was a small basin with a narrow entrance partially filled up by Tamerlane in 1402 AD.
The streets were broad, well paved and laid out at right angles; many were named after temples: the main street, called the Golden, ran across the city from west to east, beginning probably from the temple of Zeus Akraios on the west slope of Pagus, and running round the lower slopes of Pagus (like a necklace on the statue, to use the favorite terms of Aristides the orator) towards Tepecik outside the city on the east, where probably stood the temple of Cybele, worshipped under the name of Meter Sipylene, the patroness of the city. The name is from the nearby Mount Sipylus, which bounds the valley of the city's backlands. The plain towards the sea was too low to be properly drained, and in rainy weather, the streets of the lower town were deep with mud and water.
At the end of the Hellenistic period, in 197 BC, the city suddenly cut its ties with King Eumenes of Pergamum and instead appealed to Rome for help. Because Rome and Smyrna had no ties until then, Smyrna created a cult of Rome to establish a bond, and the cult eventually became widespread through the whole Roman Empire. As of 195 BC, the city of Rome started to be deified, in the cult to the goddess Roma. In this sense, the Smyrneans can be considered as the creators of the goddess Roma.
In 133 BC, when the last Attalid king Attalus III died without an heir, his will conferred his entire kingdom, including Smyrna, to the Romans. They organized it into the Roman province of Asia, making Pergamum the capital. Smyrna, however, as a major seaport, became a leading city in the newly constituted province.
Roman and Byzantine period.
As one of the principal cities of Roman Asia, Smyrna vied with Ephesus and Pergamum for the title "First City of Asia."
A Christian church and a bishopric existed here from a very early time, probably originating in the considerable Jewish colony. It was one of the seven churches addressed in the Book of Revelation. Saint Ignatius of Antioch visited Smyrna and later wrote letters to its bishop, Polycarp. A mob of Jews and pagans abetted the martyrdom of Polycarp in AD 153. Saint Irenaeus, who heard Polycarp as a boy, was probably a native of Smyrna. Another famous resident of the same period was Aelius Aristides.
Polycrates reports a succession of bishops including Polycarp of Smryna, as well as others in nearby cities such as Melito of Sardis. Related to that time the German historian W. Bauer wrote:
Asian Jewish Christianity received in turn the knowledge that henceforth the "church" would be open without hesitation to the Jewish influence mediated by Christians, coming not only from the apocalyptic traditions, but also from the synagogue with its practices concerning worship, which led to the appropriation of the Jewish passover observance. Even the observance of the sabbath by Christians appears to have found some favor in Asia...we find that in post-apostolic times, in the period of the formation of ecclesiastical structure, the Jewish Christians in these regions come into prominence.
In the late 2nd century, Irenaeus also noted:
Polycarp also was not only instructed by apostles, and conversed with many who had seen Christ, but was also, by apostles in Asia, appointed bishop of the Church in Smyrna…always taught the things which he had learned from the apostles, and which the Church has handed down, and which alone are true. To these things all the Asiatic Churches testify, as do also those men who have succeeded Polycarp. 
Tertullian wrote c. 208 AD.
Anyhow the heresies are at best novelties, and have no continuity with the teaching of Christ. Perhaps some heretics may claim Apostolic antiquity: we reply: Let them publish the origins of their churches and unroll the catalogue of their bishops till now from the Apostles or from some bishop appointed by the Apostles, as the Smyrnaeans count from Polycarp and John, and the Romans from Clement and Peter; let heretics invent something to match this.
Hence, apparently the church in Smyrna was one of only two that Tertullian felt could have had some type of apostolic succession. During the mid-3rd century, however, changes occurred in Asia Minor, and most there became affiliated with the Greco-Roman churches.
When Constantinople became the seat of government, the trade between Anatolia and the West diminished in importance, and Smyrna declined. The Seljuk commander Tzachas seized Smyrna in 1084 and used it as a base for naval raids, but the city was recovered by the general John Doukas. The city was several times ravaged by the Turks, and had become quite ruinous when the Nicaean emperor John III Doukas Vatatzes rebuilt it about 1222.
Ottoman period.
Ibn Batuta found it still in great part a ruin when the homonymous chieftain of the Beylik of Aydın had conquered it about 1330 and made his son Umur governor. It became the port of the emirate. Soon afterwards the Knights of Saint John established themselves in the town but failed to conquer the citadel. In 1402, Tamerlane stormed the town and massacred almost all the inhabitants. The Mongol conquest was only temporary, but Smyrna was recovered by the Turks under the Aydın dynasty after which it became Ottoman, when the Ottomans took over the lands of Aydın.
Greek influence was so strong in the area that the Turks called it "Smyrna of the infidels" (Gavur İzmir). While Turkish sources track the emergence of the term to the 14th century when two separate parts of the city were controlled by two different powers, the upper İzmir being Muslim and the lower part of the city Christian.
During the late 19th and early 20th century, the city was an important financial and cultural center of the Greek world. Out of the 391 factories 322 belonged to local Greeks, while 3 out of the 9 banks were backed by Greek capital. Education was also dominated by the local Greek communities with 67 male and 4 female schools in total. The Ottomans continued to control the area, with the exception of the 1919–1922 period, when the city was assigned to Greece by the Treaty of Sèvres.
The most important Greek educational institution of the region was the Evangelical School that operated from 1733 to 1922.
Post World War I.
After the end of the First World War Greece occupied Smyrna from 15 May 1919 and put in place a military administration. The Greek premier Venizelos had plans to annex Smyrna and he seemed to be realizing his objective in the Treaty of Sèvres, signed 10 August 1920. (However, this treaty was not ratified by the parties; the Treaty of Peace of Lausanne replaced it.)
The occupation of Smyrna came to an end when the Turkish army of Kemal Atatürk entered the city on September 9, 1922, at the end of the Greco-Turkish War (1919–1922). Four days after the Turks regained control of the city, the Great Fire of Smyrna resulted in the massacre of the Greek and Armenian populations. The death toll is estimated to range from 10,000 to 100,000.
Agora.
The remains of the ancient agora of Smyrna constitute today the space of "İzmir Agora Museum" in İzmir's Namazgah quarter, although its area is commonly referred to as "Agora" by the city's inhabitants.
Situated on the northern slopes of the Pagos hills, it was the commercial, judicial and political nucleus of the ancient city, its center for artistic activities and for teaching.
"İzmir Agora Open Air Museum" consists of five parts, including the agora area, the base of the northern basilica gate, the stoa and the ancient shopping centre.
The agora of Smyrna was built during the Hellenistic era. After a destructive earthquake in 178 AD, Smyrna was rebuilt in the Roman period (2nd century AD) under the emperor Marcus Aurelius, according to an urban plan drawn by Hippodamus of Miletus. The bust of the emperor's wife Faustina on the second arch of the western stoa confirms this fact.
It was constructed on a sloping terrain in three floors, close to the city center. The terrain is 165 m wide and 200 m long. It is bordered on all sides by porticos. Because a Byzantine and later an Ottoman cemetery were located over the ruins of the agora, it was preserved from modern constructions. This agora is now the largest and the best preserved among Ionian agoras. The agora is now surrounded by modern buildings that still cover its eastern and southern parts.
The agora was used until the Byzantine period.
On entering the courtyard, to the left is the western stoa, in the back the basilica and on the right side the Ottoman cemetery. The courtyard was surrounded by porticoes on three sides. The basilica and the western portico were built over an infrastructure of basements with round arches to protect them against future earthquakes. The eastern end and the southern porticoes consisted of a two-floor compounded structure. Beneath the basilica was a covered market place. The design of the basement has a strong resemblance with the crypto-porticus constructions of the western provinces.
The monumental entrance at the eastern side was one of the most magnificent and arched structures of the Hellenistic era.
A two-storied stoa, 17.5 m wide, was constructed at the eastern and western side of the agora. Each stoa was divided in three galleries by two rows of columns. Each stoa had an upper story. The stoas were protected from sun and rain by a roof. These impressive structures measured 75 m by 18 m. The southern part of the western stoa has many water channels and large water reservoirs, pointing to the presence of water in the agora.
Excavations.
Although Smyrna was explored by Charles Texier in the 19th century and the German consul in İzmir had purchased the land around the ancient theater in 1917 to start excavations, the first scientific digs can be said to have started in 1927. Most of the discoveries were made by archaeological exploration carried as an extension during the period between 1931 and 1942 by the German archaeologist Rudolf Naumann and Selâhattin Kantar, the director of İzmir and Ephesus museums. They uncovered a three-floor, rectangular compound with stairs in the front, built on columns and arches around a large courtyard in the middle of the building.
New excavations in the agora began in 1996. They have continued since 2002 under the sponsorship of the Metropolitan Municipality of İzmir. A primary school adjacent to the agora that had burned in 1980 was not reconstructed. Instead, its space was incorporated into the historical site. The area of the agora was increased to . This permitted the evacuation of a previously unexplored zone. The archaeologists and the local authorities, means permitting, are also keenly eyeing a neighbouring multi-storey car park, which is known to cover an important part of the ancient settlement. During the present renovations the old restorations in concrete are gradually being replaced by marble.
The new excavation has uncovered the agora's northern gate. It has been concluded that embossed figures of the goddess Hestia found in these digs were a continuation of the Zeus altar uncovered during the first digs. Statues of the gods Hermes, Dionysos, Eros and Heracles have also been found, as well as many statues, heads, embossments, figurines and monuments of people and animals, made of marble, stone, bone, glass, metal and terracotta. Inscriptions found here list the people who provided aid to Smyrna after the earthquake of 178 AD.
Economy.
In the early 20th-century, there were mills spinning thread. As of 1920, there were two factories in Smyrna dyeing yarn, which were owned by British companies. These companies employed over 60,000 people. During this time, there was also a French owned cotton spinning mill. The city also produced soap made of refuse olive oil. An ironworks, also owned by the British, produced tools and equipment. Those tools were used to extract tannin from valonia oak. As of 1920, the ironwork was exporting 5,000 tons of product a year. The city also produced wooden boxes, which were used for fig and raisin storage. The wood for the boxes was imported from Austria and Romania.
Toponyms.
Several American cities have been named after Smyrna, including Smyrna, Georgia; Smyrna, Tennessee; Smyrna, Delaware; Smyrna, Michigan; and New Smyrna Beach, Florida.

</doc>
<doc id="50563" url="https://en.wikipedia.org/wiki?curid=50563" title="Sucrose">
Sucrose

Sucrose is a common, naturally occurring carbohydrate found in many plants and plant parts. Saccharose is an obsolete name for sugars in general, especially sucrose. The molecule is a disaccharide combination of the monosaccharides glucose and fructose with the formula C12H22O11.
Sucrose is often extracted and refined from either cane or beet sugar for human consumption. Modern industrial sugar refinement processes often involves bleaching and crystallization also, producing a white, odorless, crystalline powder with a sweet taste of pure sucrose, devoid of vitamins and minerals. This refined form of sucrose is commonly referred to as table sugar or just sugar. It plays a central role as an additive in food production and food consumption all over the world. About 175 million metric tons of sucrose sugar were produced worldwide in 2013.
The word "sucrose" was coined in 1857 by the English chemist William Miller from the French "sucre" ("sugar") and the generic chemical suffix for sugars "-ose". The abbreviated term "Suc" is often used for "sucrose" in scientific literature.
Physical and chemical properties.
Structural O-α-D-glucopyranosyl-(1→2)-β-D-fructofuranoside.
In sucrose, the components glucose and fructose are linked via an ether bond between C1 on the glucosyl subunit and C2 on the fructosyl unit. The bond is called a glycosidic linkage. Glucose exists predominantly as two isomeric "pyranoses" (α and β), but only one of these forms links to the fructose. Fructose itself exists as a mixture of "furanoses", each of which having α and β isomers, but only one particular isomer links to the glucosyl unit. What is notable about sucrose is that, unlike most disaccharides, the glycosidic bond is formed between the reducing ends of both glucose and fructose, and not between the reducing end of one and the nonreducing end of the other. This linkage inhibits further bonding to other saccharide units. Since it contains no anomeric hydroxyl groups, it is classified as a non-reducing sugar.
Sucrose crystallizes in the monoclinic space group P21 with room-temperature lattice parameters "a" = 1.08631 nm, "b" = 0.87044 nm, "c" = 0.77624 nm, β = 102.938°.
The purity of sucrose is measured by polarimetry, through the rotation of plane-polarized light by a solution of sugar. The specific rotation at 20 °C using yellow "sodium-D" light (589 nm) is +66.47°. Commercial samples of sugar are assayed using this parameter. Sucrose does not deteriorate at ambient conditions.
Thermal and oxidative degradation.
The formula for sucrose decomposition can be represented as a 2 step reaction: the first is dehydration to pure carbon and water, then carbon oxidizes to CO2 with O2 from air.
Sucrose does not melt at high temperatures. Instead, it decomposes—at —to form caramel. Like other carbohydrates, it combusts to carbon dioxide and water. Mixing sucrose with the oxidizer potassium nitrate produces the fuel known as rocket candy that is used to propel amateur rocket motors.
This reaction is somewhat simplified though. Some of the carbon does get fully oxidized to carbon dioxide, and other reactions, such as the water-gas shift reaction also take place. A more accurate theoretical equation is:
Sucrose burns with chloric acid, formed by the reaction of hydrochloric acid and potassium chlorate:
Sucrose can be dehydrated with sulfuric acid to form a black, carbon-rich solid, as indicated in the following idealized equation:
Hydrolysis.
Hydrolysis breaks the glycosidic bond converting sucrose into glucose and fructose. Hydrolysis is, however, so slow that solutions of sucrose can sit for years with negligible change. If the enzyme sucrase is added, however, the reaction will proceed rapidly. Hydrolysis can also be accelerated with acids, such as cream of tartar or lemon juice, both weak acids. Likewise, gastric acidity converts sucrose to glucose and fructose during digestion the bond between them being an acetal bond which can be broken by an acid.
Synthesis and biosynthesis of sucrose.
The biosynthesis of sucrose proceeds via the precursors UDP-glucose and fructose 6-phosphate, catalyzed by the enzyme sucrose-6-phosphate synthase. The energy for the reaction is gained by the cleavage of Uridine diphosphate (UDP).
Sucrose is formed by plants and cyanobacteria but not by other organisms. Sucrose is found naturally in many food plants along with the monosaccharide fructose. In many fruits, such as pineapple and apricot, sucrose is the main sugar. In others, such as grapes and pears, fructose is the main sugar.
Chemical synthesis.
Although sucrose is almost invariably isolated from natural sources, its chemical synthesis was first achieved in 1953 by Raymond Lemieux.
Sources.
In nature, sucrose is present in many plants, and in particular their roots, fruits and nectars, because it serves as a way to store energy, primarily from photosynthesis. Many mammals, birds, insects and bacteria accumulate and feed on the sucrose in plants and for some it is their main food source. Seen from a human consumption perspective, honeybees are especially important because they accumulate sucrose and produce honey, an important foodstuff all over the world. The carbohydrates in honey itself primarily consists of fructose and glucose with trace amounts of sucrose only.
As fruits ripen, their sucrose content usually rise sharply, but some fruits contain almost no sucrose at all. This includes grapes, cherries, blueberries, blackberries, figs, pomegranates, tomatoes, avocados, lemons and limes.
Sucrose is a naturally occurring sugar, but with the advent of industrialization, it has been increasingly refined and consumed in all kinds of processed foods.
Production.
History of sucrose refinement.
The production of table sugar has a long history. Some scholars claim Indians discovered how to crystallize sugar during the Gupta dynasty, around AD 350.
Other scholars point to the ancient manuscripts of China, dated to the 8th century BC, where one of the earliest historical mentions of sugar cane is included along with the fact that their knowledge of sugar cane was derived from India. Further, it appears that by about 500 BC, residents of present-day India began making sugar syrup and cooling it in large flat bowls to make raw table sugar crystals that were easier to store and transport. In the local Indian language, these crystals were called "khanda" (खण्ड), which is the source of the word "candy".
The army of Alexander the Great was halted on the banks of river Indus by the refusal of his troops to go further east. They saw people in the Indian subcontinent growing sugarcane and making "granulated, salt-like sweet powder", locally called "sākhar" (साखर), pronounced as "sakcharon" (ζακχαρον) in Greek (Modern Greek, "zachari" ζάχαρη). On their return journey, the Greek soldiers carried back some of the "honey-bearing reeds". Sugarcane remained a limited crop for over a millennium. Sugar was a rare commodity and traders of sugar became wealthy. Venice, at the height of its financial power, was the chief sugar-distributing center of Europe. Arabs started producing it in Sicily and Spain. Only after the Crusades did it begin to rival honey as a sweetener in Europe. The Spanish began cultivating sugarcane in the West Indies in 1506 (Cuba in 1523). The Portuguese first cultivated sugarcane in Brazil in 1532.
Sugar remained a luxury in much of the world until the 18th century. Only the wealthy could afford it. In the 18th century, the demand for table sugar boomed in Europe and by the 19th century it had become regarded as a human necessity. The use of sugar grew from use in tea, to cakes, confectionery and chocolates. Suppliers marketed sugar in novel forms, such as solid cones, which required consumers to use a sugar nip, a pliers-like tool, in order to break off pieces.
The demand for cheaper table sugar drove, in part, colonization of tropical islands and nations where labor-intensive sugarcane plantations and table sugar manufacturing could thrive. Growing sugar cane crop in hot humid climates, and producing table sugar in high temperature sugar mills was harsh, inhumane work. The demand for cheap and docile labor for this work, in part, first drove slave trade from Africa (in particular West Africa), followed by indentured labor trade from South Asia (in particular India). Millions of slaves, followed by millions of indentured laborers were brought into the Caribbean, Indian Ocean, Pacific Islands, East Africa, Natal, north and eastern parts of South America, and southeast Asia. The modern ethnic mix of many nations, settled in the last two centuries, has been influenced by table sugar.
Beginning in the late 18th century, the production of sugar became increasingly mechanized. The steam engine first powered a sugar mill in Jamaica in 1768, and, soon after, steam replaced direct firing as the source of process heat. During the same century, Europeans began experimenting with sugar production from other crops. Andreas Marggraf identified sucrose in beet root and his student Franz Achard built a sugar beet processing factory in Silesia (Prussia). However, the beet-sugar industry really took off during the Napoleonic Wars, when France and the continent were cut off from Caribbean sugar. In 2010, about 20 percent of the world's sugar was produced from beets.
Today, a large beet refinery producing around 1,500 tonnes of sugar a day needs a permanent workforce of about 150 for 24-hour production.
Current trends.
Table sugar (sucrose) comes from plant sources. Two important sugar crops predominate: sugarcane ("Saccharum spp.") and sugar beets ("Beta vulgaris"), in which sugar can account for 12% to 20% of the plant's dry weight. Minor commercial sugar crops include the date palm ("Phoenix dactylifera"), sorghum ("Sorghum vulgare"), and the sugar maple ("Acer saccharum"). Sucrose is obtained by extraction of these crops with hot water, concentration of the extract gives syrups, from which solid sucrose can be crystallized. In 2013, worldwide production of table sugar amounted to 175 million tonnes.
Most cane sugar comes from countries with warm climates, because sugarcane does not tolerate frost. Sugar beets, on the other hand, grow only in cooler temperate regions and do not tolerate extreme heat. About 80 percent of sucrose is derived from sugarcane, the rest almost all from sugar beets.
In 2010, Brazil, India, European Union, China, Thailand, and United States were the major sugar-producing countries in the world. Brazil produced about 40 million tonnes of table sugar in 2013, while India produced 25 million, EU-27 countries 16 million, China 14 million, Thailand about 10 million, and United States over 7 million. The country rankings for table sugar production change with each year's sugarcane crop harvest and as new sugar production plants are commissioned worldwide.
Viewed by region, Asia predominates in cane sugar production, with large contributions from India, China, Thailand, and other countries combining to account for 40% of global production in 2006. South America comes in second place with 32% of global production; Africa and Central America each produce 8% and Australia 5%. The United States, the Caribbean, and Europe make up the remainder, with roughly 3% each.
Beet sugar comes from regions with cooler climates: northwest and eastern Europe, northern Japan, plus some areas in the United States (including California). In the northern hemisphere, the beet-growing season ends with the start of harvesting around September. Harvesting and processing continues until March in some cases. The availability of processing plant capacity and the weather both influence the duration of harvesting and processing – the industry can store harvested beets until processed, but a frost-damaged beet becomes effectively unprocessable.
Brazil is the world's largest sugar exporter at 29 million tonnes in the year 2013. The European Union (EU) has become the world's second-largest sugar exporter. The Common Agricultural Policy of the EU sets maximum quotas for members' production to match supply and demand, and a price. Europe exports excess production quota (approximately 5 million tonnes in 2003). Part of this, "quota" sugar, gets subsidised from industry levies, the remainder (approximately half) sells as "C quota" sugar at market prices without subsidy. These subsidies and a high import tariff make it difficult for other countries to export to the EU states, or to compete with the Europeans on world markets.
The United States sets high sugar prices to support its producers, with the effect that many former purchasers of sugar have switched to corn syrup (beverage manufacturers) or moved out of the country (candy manufacturers).
India consumes the most sugar at 26 million tonnes of table sugar in 2013. EU-27 is in second place at 18 million and China is third at above 16 million.
Low prices of sugar are expected to stimulate global consumption and trade, with exports forecast 4 percent higher at 59 million tons.
The low prices of glucose syrups produced from wheat and corn (maize) threaten the traditional sugar market. Used in combination with artificial sweeteners, they can allow drink manufacturers to produce very low-cost goods.
High-fructose corn syrup.
In the USA there are tariffs on the importation of sugar, and subsidies for the production of maize (corn). High-fructose corn syrup (HFCS) is derived from corn, and significantly cheaper there than sucrose as a sweetener. This has led to sucrose being partially displaced in U.S. industrial food production by HFCS and other non-sucrose natural sweeteners.
Types.
Cane.
Since the 6th century BC, cane sugar producers have crushed the harvested vegetable material from sugarcane in order to collect and filter the juice. They then treat the liquid (often with lime (calcium oxide)) to remove impurities and then neutralize it. Boiling the juice then allows the sediment to settle to the bottom for dredging out, while the scum rises to the surface for skimming off. In cooling, the liquid crystallizes, usually in the process of stirring, to produce sugar crystals. Centrifuges usually remove the uncrystallized syrup. The producers can then either sell the sugar product for use as is, or process it further to produce lighter grades. The later processing may take place in another factory in another country.
Sugarcane is a major component of Brazilian agriculture; the country is a top producer of sugarcane products, such as crystallized sugar and ethanol (ethanol fuel). The sucrose found in sugarcane produces ethanol when fermented and distilled. Brazil has implemented ethanol as an alternative fuel on a national scale.
Beet.
Beet sugar producers slice the washed beets, then extract the sugar with hot water in a "diffuser". An alkaline solution ("milk of lime" and carbon dioxide from the lime kiln) then serves to precipitate impurities (see carbonatation). After filtration, evaporation concentrates the juice to a content of about 70% solids, and controlled crystallisation extracts the sugar. A centrifuge removes the sugar crystals from the liquid, which gets recycled in the crystalliser stages. When economic constraints prevent the removal of more sugar, the manufacturer discards the remaining liquid, now known as molasses, or sells it on to producers of animal feed.
Sieving the resultant white sugar produces different grades for selling.
Cane versus beet.
It is difficult to distinguish between fully refined sugar produced from beet and cane. One way is by isotope analysis of carbon. Cane uses C4 carbon fixation, and beet uses C3 carbon fixation, resulting in a different ratio of 13C and 12C isotopes in the sucrose. Tests are used to detect fraudulent abuse of European Union subsidies or to aid in the detection of adulterated fruit juice.
Sugar cane tolerates hot climates better, but the production of sugar cane needs approximately four times as much water as the production of sugar beet, therefore some countries that traditionally produced cane sugar (such as Egypt) have built new beet sugar factories since about 2008. Some sugar factories process both sugar cane and sugar beets and extend their processing period in that way.
The production of sugar leaves residues that differ substantially depending on the raw materials used and on the place of production. While cane molasses is often used in food preparation, humans find molasses from sugar beets unpalatable, and it consequently ends up mostly as industrial fermentation feedstock (for example in alcohol distilleries), or as animal feed. Once dried, either type of molasses can serve as fuel for burning.
Pure beet sugar is difficult to find, so labelled, in the marketplace. Although some brands label their product clearly as "pure cane sugar", beet sugar is almost always labeled simply as sugar or pure sugar. Interviews with the 5 major beet sugar-producing companies revealed that many store brands or "private label" sugar products are pure beet sugar. The lot code can be used to identify the company and the plant from which the sugar came, enabling beet sugar to be identified if the codes are known.
Culinary sugars.
Mill white.
Mill white, also called plantation white, crystal sugar or superior sugar is produced from raw sugar. It is exposed to sulfur dioxide during the production to reduce the concentration of color compounds and helps prevent further color development during the crystallization process. Although common to sugarcane-growing areas, this product does not store or ship well. After a few weeks, its impurities tend to promote discoloration and clumping; therefore this type of sugar is generally limited to local consumption.
Blanco directo.
Blanco directo, a white sugar common in India and other south Asian countries, is produced by precipitating many impurities out of cane juice using phosphoric acid and calcium hydroxide, similar to the carbonatation technique used in beet sugar refining. Blanco directo is more pure than mill white sugar, but less pure than white refined.
White refined.
White refined is the most common form of sugar in North America and Europe. Refined sugar is made by dissolving and purifying raw sugar using phosphoric acid similar to the method used for blanco directo, a carbonatation process involving calcium hydroxide and carbon dioxide, or by various filtration strategies. It is then further purified by filtration through a bed of activated carbon or bone char. Beet sugar refineries produce refined white sugar directly without an intermediate raw stage.
White refined sugar is typically sold as granulated sugar, which has been dried to prevent clumping and comes in various crystal sizes for home and industrial use:
Brown sugar comes either from the late stages of cane sugar refining, when sugar forms fine crystals with significant molasses content, or from coating white refined sugar with a cane molasses syrup (blackstrap molasses). Brown sugar's color and taste becomes stronger with increasing molasses content, as do its moisture-retaining properties. Brown sugars also tend to harden if exposed to the atmosphere, although proper handling can reverse this.
Measurement.
Dissolved sugar content.
Scientists and the sugar industry use degrees Brix (symbol °Bx), introduced by Adolf Brix, as units of measurement of the mass ratio of dissolved substance to water in a liquid. A 25 °Bx sucrose solution has 25 grams of sucrose per 100 grams of liquid; or, to put it another way, 25 grams of sucrose sugar and 75 grams of water exist in the 100 grams of solution.
The Brix degrees are measured using an infrared sensor. This measurement does not equate to Brix degrees from a density or refractive index measurement, because it will specifically measure dissolved sugar concentration instead of all dissolved solids. When using a refractometer, one should report the result as "refractometric dried substance" (RDS). One might speak of a liquid as having 20 °Bx RDS. This refers to a measure of percent by weight of "total" dried solids and, although not technically the same as Brix degrees determined through an infrared method, renders an accurate measurement of sucrose content, since sucrose in fact forms the majority of dried solids. The advent of in-line infrared Brix measurement sensors has made measuring the amount of dissolved sugar in products economical using a direct measurement.
Consumption.
Refined sugar was a luxury before the 18th century. It became widely popular in the 18th century, then graduated to becoming a necessary food in the 19th century. This evolution of taste and demand for sugar as an essential food ingredient unleashed major economic and social changes. Eventually, table sugar became sufficiently cheap and common enough to influence standard cuisine and flavored drinks.
Sucrose forms a major element in confectionery and desserts. Cooks use it for sweetening — its fructose component, which has almost double the sweetness of glucose, makes sucrose distinctively sweet in comparison to other carbohydrates. It can also act as a food preservative when used in sufficient concentrations. Sucrose is important to the structure of many foods, including biscuits and cookies, cakes and pies, candy, and ice cream and sorbets. It is a common ingredient in many processed and so-called "junk foods."
Nutritional information.
Refined sugar is 99.9% sucrose, therefore providing only carbohydrate value as a major dietary nutrient and 390 kilocalories per 100 g serving (USDA data, right table). There are no micronutrients having significant content (right table).
Metabolism of sucrose.
In humans and other mammals, sucrose is broken down into its constituent monosaccharides, glucose and fructose, by sucrase or isomaltase glycoside hydrolases, which are located in the membrane of the microvilli lining the duodenum. The resulting glucose and fructose molecules are then rapidly absorbed into the bloodstream. In bacteria and some animals, sucrose is digested by the enzyme invertase.
Sucrose is an easily assimilated macronutrient that provides a quick source of energy, provoking a rapid rise in blood glucose upon ingestion. Sucrose, as a pure carbohydrate, has an energy content of 3.94 kilocalories per gram (or 17 kilojoules per gram).
Overconsumption of sucrose has been linked with adverse health effects.
Dental caries or tooth decay may be caused by oral bacteria converting sugars, including sucrose, from food into acids that corrodes tooth enamel.
When large amounts of refined food that contain high percentages of sucrose are consumed, beneficial nutrients can be displaced from the diet, which can contribute to an increased risk for chronic disease. The rapidity with which sucrose raises blood glucose can cause problems for people suffering from defective glucose metabolism, such as persons with hypoglycemia or diabetes mellitus.
Sucrose can contribute to the development of metabolic syndrome. In an experiment with rats that were fed a diet one-third of which was sucrose, the sucrose first elevated blood levels of triglycerides, which induced visceral fat and ultimately resulted in insulin resistance. Another study found that rats fed sucrose-rich diets developed high triglycerides, hyperglycemia, and insulin resistance. A 2004 study recommended that the consumption of sucrose-containing drinks should be limited due to the growing number of people with obesity and insulin resistance.
Human health.
Human beings have long sought sugars, but aside from wild honey and fruits, have not had access to the large quantities that characterize the modern diet. Studies have indicated potential links between consumption of free sugars including sucrose (particularly prevalent in processed foods) and health hazards, including obesity and tooth decay. It is also considered as a source of endogenous glycation processes since it metabolises into glucose and fructose in the body.
Tooth decay.
Tooth decay (dental caries) has become a prominent health hazard associated with the consumption of sugars, especially sucrose. Oral bacteria such as "Streptococcus mutans" live in dental plaque and metabolize "any" sugars (not just sucrose, but also glucose, lactose, fructose, and cooked starches) into lactic acid. The resultant lactic acid lowers the pH of the tooth's surface, stripping it of minerals in the process known as tooth decay.
All 6-carbon sugars and disaccharides based on 6-carbon sugars can be converted by dental plaque bacteria into acid that demineralizes teeth, but sucrose may be uniquely useful to "Streptococcus sanguinis" (formerly "Streptococcus sanguis") and "Streptococcus mutans". Sucrose is the only dietary sugar that can be converted to sticky glucans (dextran-like polysaccharides) by extracellular enzymes. These glucans allow the bacteria to adhere to the tooth surface and to build up thick layers of plaque. The anaerobic conditions deep in the plaque encourage the formation of acids, which leads to carious lesions. Thus, sucrose could enable "S. mutans", "S. sanguinis" and many other species of bacteria to adhere strongly and resist removal, e.g. by flow of saliva (although they are easily removed by brushing). The glucans and levans (fructose polysaccharides) produced by the plaque bacteria also act as a reserve food supply for the bacteria.
Such a special role of sucrose in the formation of tooth decay is much more significant in light of the almost universal use of sucrose as the most desirable sweetening agent. Widespread replacement of sucrose by high-fructose corn syrup (HFCS) has not diminished the danger from sucrose. If smaller amounts of sucrose are present in the diet, they will still be sufficient for the development of thick, anaerobic plaque and plaque bacteria will metabolise other sugars in the diet, such as the glucose and fructose in HFCS.
Glycemic index.
Sucrose is a disaccharide made up of 50% glucose and 50% fructose and has a glycemic index of 65. Sucrose is digested rapidly, but has a relatively low glycemic index due to its content of fructose, which has a minimal effect on blood glucose.
As with other sugars, sucrose is digested into its components via the enzyme sucrase to glucose (blood sugar) and fructose. The glucose component is transported into the blood (90%) and excess glucose is converted to temporary storage in the liver – named glycogen. The fructose is either bonded to cellulose and transported out the GI tract or processed by the liver into citrates, aldehydes, and, for the most part, lipid droplets (fat).
As the glycemic index measures the speed at which glucose is released into the bloodstream a refined sugar containing glucose is considered high-glycemic. As with other sugars, over-consumption may cause an increase in blood sugar levels from a normal 90 mg/dL to up over 150 mg/dL. (5 mmol/l to over 8.3 mmol/l).
Diabetes mellitus.
Diabetes mellitus, a disease that causes the body to metabolize sugar poorly, occurs when either:
When glucose builds up in the bloodstream, it can cause two problems:
Authorities advise diabetics to avoid sugar-rich foods to prevent adverse reactions.
Obesity.
The National Health and Nutrition Examination Survey I and their follow-on studies as part of a series indicate that the population in the United States has increased its proportion of energy consumption from carbohydrates and decreased its proportion from total fat while obesity has increased. This implies, along with the United Nations report cited below, that obesity may correlate better with sugar consumption than with fat consumption, and that reducing fat consumption while increasing sugar consumption may increase the level of obesity. The following table summarizes this study (based on the proportion of energy intake from different food sources for US Adults 20–74 years old, as carried out by the U.S. Department of Health and Human Services, Centers for Disease Control and Prevention, National Center for Health Statistics, Hyattsville, MD):
Added sugar is not always evident in food products. While expected in desserts, candies, and soft drinks, it is also added to a wide range of non-sweet items such as bread, crackers, potato chips, peanut butter, soup, salad dressing, ketchup, mayonnaise, and many other common sauces. Forms of added sugar include technically accurate, but misleading, terms such as cane juice, evaporated cane juice, corn syrup and corn syrup solids, malt syrup, rice syrup, dextrose, maltose, maltodextrin, molasses, treacle, and xylose.
A 2002 study conducted by the U.S. National Academy of Sciences concluded that, due to discrepancies in data from different studies, it could not set a tolerable upper intake level, since "there is no clear and consistent association between increased intakes of added sugars and BMI." However, it explains that this may be due to the underreporting of the consumption of added sugars. (BMI, or "body mass index," is a measure of weight and height used to estimate body fat.)
Gout.
The occurrence of gout is connected with an excess production of uric acid. A diet rich in sucrose may lead to gout as it raises the level of insulin, which prevents excretion of uric acid from the body. As the concentration of uric acid in the body increases, so does the concentration of uric acid in the joint liquid and beyond a critical concentration, the uric acid begins to precipitate into crystals. Researchers have implicated sugary drinks high in fructose in a surge in cases of gout.
UN dietary recommendation.
In 2015, the World Health Organization (WHO) published a new guideline on sugars intake for adults and children, as a result of an extensive review of the available scientific evidence by a multidisciplinary group of experts. The guideline recommends that both adults and children reduce the intake of free sugars (monosaccharides and disaccharides added to foods and beverages by the manufacturer, cook or consumer, and sugars naturally present in honey, syrups, fruit juices and fruit juice concentrates) to less than 10% of total energy intake. A reduction to below 5% of total energy intake brings additional health benefits, especially in what regards dental caries. 
Religious concerns.
The sugar refining industry often uses bone char (calcinated animal bones) for decolorizing. About 25% of sugar produced in the U.S. is processed using bone char as a filter, the remainder being processed with activated carbon. As bone char does not seem to remain in finished sugar, Jewish religious leaders consider sugar filtered through it to be pareve and therefore kosher.
Trade and economics.
One of the most widely traded commodities in the world throughout history, sugar accounts for around 2% of the global dry cargo market. International sugar prices show great volatility, ranging from around 3 to over 60 cents per pound in the 50 years. About 100 of the world's 180 countries produce sugar from beet or cane, a few more refine raw sugar to produce white sugar, and all countries consume sugar. Consumption of sugar ranges from around 3 kilograms per person per annum in Ethiopia to around 40 kg/person/yr in Belgium. Consumption per capita rises with income per capita until it reaches a plateau of around 35 kg per person per year in middle income countries.
Many countries subsidize sugar production heavily. The European Union, the United States, Japan, and many developing countries subsidize domestic production and maintain high tariffs on imports. Sugar prices in these countries have often exceeded prices on the international market by up to three times; , with world market sugar futures prices strong, such prices typically exceed world prices by two times.
Within international trade bodies, especially in the World Trade Organization, the "G20" countries led by Brazil have long argued that, because these sugar markets in essence exclude cane sugar imports, the G20 sugar producers receive lower prices than they would under free trade. While both the European Union and United States maintain trade agreements whereby certain developing and less developed countries (LDCs) can sell certain quantities of sugar into their markets, free of the usual import tariffs, countries outside these preferred trade régimes have complained that these arrangements violate the "most favoured nation" principle of international trade. This has led to numerous tariffs and levies in the past.
In 2004, the WTO sided with a group of cane sugar exporting nations (led by Brazil and Australia) and ruled the EU sugar-régime and the accompanying ACP-EU Sugar Protocol (whereby a group of African, Caribbean, and Pacific countries receive preferential access to the European sugar market) illegal. In response to this and to other rulings of the WTO, and owing to internal pressures on the EU sugar-régime, the European Commission proposed on 22 June 2005 a radical reform of the EU sugar-régime, cutting prices by 39% and eliminating all EU sugar exports.
The African, Caribbean, Pacific and least developed country sugar exporters reacted with dismay to the EU sugar proposals. On 25 November 2005, the Council of the EU agreed to cut EU sugar prices by 36% as from 2009. In 2007, it seemed
that the U.S. Sugar Program could become the next target for reform. However, some commentators expected heavy lobbying from the U.S. sugar industry, which donated $2.7 million to US House and US Senate incumbents in the 2006 US election, more than any other group of US food-growers.
Especially prominent lobbyists include The Fanjul Brothers, so-called "sugar barons" who made the single individual contributions of soft money to both the Democratic and Republican parties in the political system of the United States of America.
Small quantities of sugar, especially specialty grades of sugar, reach the market as 'fair trade' commodities; the fair trade system produces and sells these products with the understanding that a larger-than-usual fraction of the revenue will support small farmers in the developing world. However, whilst the Fairtrade Foundation offers a premium of $60.00 per tonne to small farmers for sugar branded as "Fairtrade",
government schemes such the U.S. Sugar Program and the ACP Sugar Protocol offer premiums of around $400.00 per tonne above world market prices. However, the EU announced on 14 September 2007 that it had offered "to eliminate all duties and quotas on the import of sugar into the EU".
The US Sugar Association has launched a campaign to promote sugar over artificial substitutes. The Association aggressively challenges many common beliefs regarding negative side-effects of sugar consumption. The campaign aired a high-profile television commercial during the 2007 Primetime Emmy Awards on FOX Television. The Sugar Association uses the trademark tagline "Sugar: sweet by nature."

</doc>
<doc id="50564" url="https://en.wikipedia.org/wiki?curid=50564" title="Gray code">
Gray code

The reflected binary code (RBC), also known as Gray code after Frank Gray, is a binary numeral system where two successive values differ in only one bit (binary digit).
The reflected binary code was originally designed to prevent spurious output from electromechanical switches. Today, Gray codes are widely used to facilitate error correction in digital communications such as digital terrestrial television and some cable TV systems.
Name.
The code was later named after Gray by others who used it. Two different 1953 patent applications use "Gray code" as an alternative name for the "reflected binary code"; one of those also lists "minimum error code" and "cyclic permutation code" among the names. A 1954 patent application refers to "the Bell Telephone Gray code".
Motivation.
Many devices indicate position by closing and opening switches. If that device uses natural binary codes, positions 3 and 4 are next to each other but all three bits of the binary representation differ:
The problem with natural binary codes is that, with physical, mechanical switches, it is very unlikely that switches will change states exactly in synchrony. In the transition between the two states shown above, all three switches change state. In the brief period while all are changing, the switches will read some spurious position. Even without keybounce, the transition might look like 011 — 001 — 101 — 100. When the switches appear to be in position 001, the observer cannot tell if that is the "real" position 001, or a transitional state between two other positions. If the output feeds into a sequential system, possibly via combinational logic, then the sequential system may store a false value.
The reflected binary code solves this problem by changing only one switch at a time, so there is never any ambiguity of position,
Notice that the Gray code for decimal 7 rolls over to decimal 0 with only one switch change. This is called the "cyclic" property of a Gray code. In the standard Gray coding the least significant bit follows a repetitive pattern of 2 on, 2 off the next digit a pattern of 4 on, 4 off; and so forth.
More formally, a Gray code is a code assigning to each of a contiguous set of integers, or to each member of a circular list, a word of symbols such that each two adjacent code words differ by one symbol. These codes are also known as "single-distance codes", reflecting the Hamming distance of 1 between adjacent codes. There can be more than one Gray code for a given word length, but the term was first applied to a particular binary code for the non-negative integers, the "binary-reflected Gray code", or BRGC, the three-bit version of which is shown above.
History and practical application.
Reflected binary codes were applied to mathematical puzzles before they became known to engineers. The French engineer Émile Baudot used Gray codes in telegraphy in 1878. He received the French Legion of Honor medal for his work. The Gray code is sometimes attributed, incorrectly, to Elisha Gray (in "Principles of Pulse Code Modulation", K. W. Cattermole, for example).
Frank Gray, who became famous for inventing the signaling method that came to be used for compatible color television, invented a method to convert analog signals to reflected binary code groups using vacuum tube-based apparatus. The method and apparatus were patented in 1953 and the name of Gray stuck to the codes. The "PCM tube" apparatus that Gray patented was made by Raymond W. Sears of Bell Labs, working with Gray and William M. Goodall, who credited Gray for the idea of the reflected binary code.
The use of his eponymous codes that Gray was most interested in was to minimize the effect of error in the conversion of analog signals to digital; his codes are still used today for this purpose, and others.
Position encoders.
Gray codes are used in position encoders (linear encoders and rotary encoders), in preference to straightforward binary encoding. This avoids the possibility that, when several bits change in the binary representation of an angle, a misread will result from some of the bits changing before others. Originally, the code pattern was electrically conductive, supported (in a rotary encoder) by an insulating disk. Each track had its own stationary metal spring contact; one more contact made the connection to the pattern. That common contact was connected by the pattern to whichever of the track contacts were resting on the conductive pattern. However, sliding contacts wear out and need maintenance, which favors optical encoders.
Regardless of the care in aligning the contacts, and accuracy of the pattern, a natural-binary code would have errors at specific disk positions, because it is impossible to make all bits change at exactly the same time as the disk rotates. The same is true of an optical encoder; transitions between opaque and transparent cannot be made to happen simultaneously for certain exact positions. Rotary encoders benefit from the cyclic nature of Gray codes, because consecutive positions of the sequence differ by only one bit. This means that, for a transition from state A to state B, timing mismatches can only affect when the A → B transition occurs, rather than inserting one or more (up to "N" − 1 for an "N"-bit codeword) false intermediate states, as would occur if a standard binary code were used.
Towers of Hanoi.
The binary-reflected Gray code can also be used to serve as a solution guide for the Towers of Hanoi problem, as well as the classical Chinese rings puzzle, a sequential mechanical puzzle mechanism. It also forms a Hamiltonian cycle on a hypercube, where each bit is seen as one dimension.
Genetic algorithms.
Due to the Hamming distance properties of Gray codes, they are sometimes used in genetic algorithms. They are very useful in this field, since mutations in the code allow for mostly incremental changes, but occasionally a single bit-change can cause a big leap and lead to new properties.
Karnaugh maps.
Gray codes are also used in labelling the axes of Karnaugh maps.
Error correction.
In modern digital communications, Gray codes play an important role in error correction. For example, in a digital modulation scheme such as QAM where data is typically transmitted in symbols of 4 bits or more, the signal's constellation diagram is arranged so that the bit patterns conveyed by adjacent constellation points differ by only one bit. By combining this with forward error correction capable of correcting single-bit errors, it is possible for a receiver to correct any transmission errors that cause a constellation point to deviate into the area of an adjacent point. This makes the transmission system less susceptible to noise.
Communication between clock domains.
Digital logic designers use Gray codes extensively for passing multi-bit count information between synchronous logic that operates at different clock frequencies. The logic is considered operating in different "clock domains". It is fundamental to the design of large chips that operate with many different clocking frequencies.
Cycling through states with minimal effort.
If a system has to cycle through all possible combinations of on-off states of some set of controls, and the changes of the controls require non-trivial expense (e.g. time, wear, human work), a Gray code minimizes the number of setting changes to just one change for each combination of states. An example would be testing a piping system for all combinations of settings of its manually operated valves.
Gray code counters and arithmetic.
A typical use of Gray code counters is building a FIFO (first-in, first-out) data buffer that has read and write ports that exist in different clock domains. The input and output counters inside such a dual-port FIFO are often stored using Gray code to prevent invalid transient states from being captured when the count crosses clock domains.
The updated read and write pointers need to be passed between clock domains when they change, to be able to track FIFO empty and full status in each domain. Each bit of the pointers is sampled non-deterministically for this clock domain transfer. So for each bit, either the old value or the new value is propagated. Therefore, if more than one bit in the multi-bit pointer is changing at the sampling point, a "wrong" binary value (neither new nor old) can be propagated. By guaranteeing only one bit can be changing, Gray codes guarantee that the only possible sampled values are the new or old multi-bit value. Typically Gray codes of power-of-two length are used.
Sometimes digital buses in electronic systems are used to convey quantities that can only increase or decrease by one at a time, for example the output of an event counter which is being passed between clock domains or to a digital-to-analog converter. The advantage of Gray codes in these applications is that differences in the propagation delays of the many wires that represent the bits of the code cannot cause the received value to go through states that are out of the Gray code sequence. This is similar to the advantage of Gray codes in the construction of mechanical encoders, however the source of the Gray code is an electronic counter in this case. The counter itself must count in Gray code, or if the counter runs in binary then the output value from the counter must be reclocked after it has been converted to Gray code, because when a value is converted from binary to Gray code, it is possible that differences in the arrival times of the binary data bits into the binary-to-Gray conversion circuit will mean that the code could go briefly through states that are wildly out of sequence. Adding a clocked register after the circuit that converts the count value to Gray code may introduce a clock cycle of latency, so counting directly in Gray code may be advantageous. A Gray code counter was patented in 1962 , and there have been many others since. In recent times a Gray code counter can be implemented as a state machine in Verilog. In order to produce the next count value, it is necessary to have some combinational logic that will increment the current count value that is stored in Gray code. Probably the most obvious way to increment a Gray code number is to convert it into ordinary binary code, add one to it with a standard binary adder, and then convert the result back to Gray code. This approach was discussed in a paper in 1996 and then subsequently patented by someone else in 1998 . Other methods of counting in Gray code are discussed in a report by R. W. Doran, including taking the output from the first latches of the master-slave flip flops in a binary ripple counter.
Perhaps the most common electronic counter with the "only one bit changes at a time" property is the Johnson counter.
Constructing an "n"-bit Gray code.
The binary-reflected Gray code list for "n" bits can be generated recursively from the list for "n" − 1 bits by reflecting the list (i.e. listing the entries in reverse order), concatenating the original list with the reversed list, prefixing the entries in the original list with a binary 0, and then prefixing the entries in the reflected list with a binary 1. For example, generating the "n" = 3 list from the "n" = 2 list:
The one-bit Gray code is "G"1 = (0, 1). This can be thought of as built recursively as above from a zero-bit Gray code "G"0 = ( Λ ) consisting of a single entry of zero length. This iterative process of generating "G""n"+1 from "G""n" makes the following properties of the standard reflecting code clear:
These characteristics suggest a simple and fast method of translating a binary value into the corresponding Gray code. Each bit is inverted if the next higher bit of the input value is set to one. This can be performed in parallel by a bit-shift and exclusive-or operation if they are available: the "n"th Gray code is obtained by computing formula_1
A similar method can be used to perform the reverse translation, but the computation of each bit depends on the computed value of the next higher bit so it cannot be performed in parallel. Assuming formula_2 is the formula_3th gray-coded bit (formula_4 being the most significant bit), and formula_5 is the formula_3th binary-coded bit (formula_7 being the most-significant bit), the reverse translation can be given recursively: formula_8, and formula_9. Alternatively, decoding a Gray code into a binary number can be described as a prefix sum of the bits in the Gray code, where each individual summation operation in the prefix sum is performed modulo two.
To construct the binary-reflected Gray code iteratively, at step 0 start with the formula_10, and at step formula_11 find the bit position of the least significant 1 in the binary representation of formula_3 and flip the bit at that position in the previous code formula_13 to get the next code formula_14. The bit positions start 0, 1, 0, 2, 0, 1, 0, 3, ... . See find first set for efficient algorithms to compute these values.
Converting to and from Gray code.
The following functions in C convert between binary numbers and their associated Gray codes. While it may seem that gray-to-binary conversion requires each bit to be handled one at a time, faster algorithms exist.
Special types of Gray codes.
In practice, a "Gray code" almost always refers to a binary-reflected Gray code (BRGC).
However, mathematicians have discovered other kinds of Gray codes.
Like BRGCs, each consists of a lists of words, where each word differs from the next in only one digit (each word has a Hamming distance of 1 from the next word).
"n"-ary Gray code.
There are many specialized types of Gray codes other than the binary-reflected Gray code. One such type of Gray code is the "n"-ary Gray code, also known as a non-Boolean Gray code. As the name implies, this type of Gray code uses non-Boolean values in its encodings.
For example, a 3-ary (ternary) Gray code would use the values {0, 1, 2}. The ("n", "k")-"Gray code" is the "n"-ary Gray code with "k" digits.
The sequence of elements in the (3, 2)-Gray code is: {00, 01, 02, 12, 10, 11, 21, 22, 20}. The ("n", "k")-Gray code may be constructed recursively, as the BRGC, or may be constructed iteratively. An algorithm to iteratively generate the ("N", "k")-Gray code is presented (in C):
<syntaxhighlight lang="C" enclose="div">
// inputs: base, digits, value
// output: gray
// Convert a value to a graycode with the given base and digits.
// Iterating through a sequence of values would result in a sequence
// of Gray codes in which only one digit changes at a time.
void to_gray(unsigned base, unsigned digits, unsigned value, unsigned gray)
 unsigned baseN; // Stores the ordinary base-N number, one digit per entry
 unsigned i; // The loop variable
 // Put the normal baseN number into the baseN array. For base 10, 109 
 // would be stored as [9,0,1]
 for (i = 0; i < digits; i++) {
 baseN = value % base;
 value = value / base;
 // Convert the normal baseN number into the graycode equivalent. Note that
 // the loop starts at the most significant digit and goes down.
 unsigned shift = 0;
 while (i--) {
 // The gray digit gets shifted down by the sum of the higher
 // digits.
 gray= (baseN[i + shift) % base;
 shift = shift + base - gray; // Subtract from base so shift is positive
// EXAMPLES
// input: value = 1899, base = 10, digits = 4
// output: baseN[] = gray[ = [0,1,7,1]
// input: value = 1900, base = 10, digits = 4
// output: baseN[] = gray[ = [0,1,8,1]
</syntaxhighlight>
There are other graycode algorithms for ("n","k")-Gray codes. The ("n","k")-Gray code produced by the above algorithm is always cyclical; some algorithms, such as that by Guan, lack this property when k is odd. On the other hand, while only one digit at a time changes with this method, it can change by wrapping (looping from "n" − 1 to 0). In Guan's algorithm, the count alternately rises and falls, so that the numeric difference between two graycode digits is always one.
Gray codes are not uniquely defined, because a permutation of the columns of such a code is a Gray code too. The above procedure produces a code in which the lower the significance of a digit, the more often it changes, making it similar to normal counting methods.
See also Skew binary number system, a variant ternary number system where at most 2 digits change on each increment, as each increment can be done with at most one digit carry operation.
Balanced Gray code.
Although the binary reflected Gray code is useful in many scenarios, it is not optimal in certain cases because of a lack of "uniformity". In balanced Gray codes, the number of changes in different coordinate positions are as close as possible. To make this more precise, let "G" be an "R"-ary complete Gray cycle having transition sequence formula_15; the "transition counts (spectrum)" of "G" are the collection of integers defined by
A Gray code is "uniform" or "uniformly balanced" if its transition counts are all equal, in which case we have formula_17
for all "k". Clearly, when formula_18, such codes exist only if "n" is a power of 2. Otherwise, if "n" does not divide formula_19 evenly, it is possible to construct "well-balanced" codes where every transition count is either formula_20 or formula_21. Gray codes can also be "exponentially balanced" if all of their transition counts are adjacent powers of two, and such codes exist for every power of two.
For example, a balanced 4-bit Gray code has 16 transitions, which can be evenly distributed among all four positions (four transitions per position), making it uniformly balanced:
whereas a balanced 5-bit Gray code has a total of 32 transitions, which cannot be evenly distributed among the positions. In this example, four positions have six transitions each, and one has eight:
We will now show a construction for well-balanced binary Gray codes which allows us to generate an "n"-digit balanced Gray code for every "n". The main principle is to inductively construct an ("n" + 2)-digit Gray code formula_22 given an "n"-digit Gray code "G" in such a way that the balanced property is preserved. To do this, we consider partitions of formula_23 into an even number "L" of non-empty blocks of the form
where formula_25, and formula_26 (mod formula_27). This partition induces an formula_28-digit Gray code given by
If we define the "transition multiplicities" formula_33 to be the number of times the digit in position "i" changes between consecutive blocks in a partition, then for the ("n" + 2)-digit Gray code induced by this partition the transition spectrum formula_34 is
The delicate part of this construction is to find an adequate partitioning of a balanced "n"-digit Gray code such that the code induced by it remains balanced. Uniform codes can be found when formula_36 and formula_37, and this construction can be extended to the "R"-ary case as well.
Monotonic Gray codes.
Monotonic codes are useful in the theory of interconnection networks, especially for
minimizing dilation for linear arrays of processors.
If we define the "weight" of a binary string to be the number of 1s in
the string, then although we clearly cannot have a Gray code with strictly
increasing weight, we may want to approximate this by having the code run
through two adjacent weights before reaching the next one.
We can formalize the concept of monotone Gray codes as follows: consider the
partition of the hypercube formula_38 into "levels" of vertices
that have equal weight, i.e.
for formula_40. These levels satisfy formula_41. Let formula_42 be the subgraph of formula_43 induced by formula_44, and let formula_45 be the edges in formula_42. A monotonic Gray code is then a Hamiltonian path in formula_43 such that whenever formula_48 comes before formula_49 in the path, then formula_50.
An elegant construction of monotonic "n"-digit Gray codes for any "n" is based on the idea of recursively building subpaths formula_51 of length formula_52 having edges in formula_53. We define formula_54, formula_55 whenever formula_56 or formula_57, and
otherwise. Here, formula_59 is a suitably defined permutation and formula_60 refers
to the path "P" with its coordinates permuted by formula_61. These paths give rise to
two monotonic "n"-digit Gray codes formula_62 and formula_63 given by
The choice of formula_59 which ensures that these codes are indeed Gray codes turns out to be formula_66. The first few values of formula_51 are shown in the table below.
These monotonic Gray codes can be efficiently implemented in such a way that each subsequent element can be generated in "O"("n") time. The algorithm is most easily described using coroutines.
Monotonic codes have an interesting connection to the Lovász conjecture,
which states that every connected vertex-transitive graph contains a Hamiltonian
path. The "middle-level" subgraph formula_68 is vertex-transitive (that is, its automorphism group is transitive, so that each vertex has the same "local environment"" and cannot be differentiated from the others, since we can relabel the coordinates as well as the binary digits to obtain an automorphism) and the problem of finding a Hamiltonian path in this subgraph is called the "middle-levels problem", which can provide insights into the more general
conjecture. The question has been answered affirmatively for formula_69, and the preceding construction for monotonic codes ensures a Hamiltonian path of length at least 0.839"N" where "N" is the number of vertices in the middle-level
subgraph.
Beckett–Gray code.
Another type of Gray code, the Beckett–Gray code, is named for Irish playwright Samuel Beckett, who was interested in symmetry. His play "Quad" features four actors and is divided into sixteen time periods. Each period ends with one of the four actors entering or leaving the stage. The play begins with an empty stage, and Beckett wanted each subset of actors to appear on stage exactly once. Clearly the set of actors currently on stage can be represented by a 4-bit binary Gray code. Beckett, however, placed an additional restriction on the script: he wished the actors to enter and exit so that the actor who had been on stage the longest would always be the one to exit. The actors could then be represented by a first in, first out queue, so that (of the actors onstage) the actor being dequeued is always the one who was enqueued first. Beckett was unable to find a Beckett–Gray code for his play, and indeed, an exhaustive listing of all possible sequences reveals that no such code exists for "n" = 4. It is known today that such codes do exist for "n" = 2, 5, 6, 7, and 8, and do not exist for "n" = 3 or 4. An example of an 8-bit Beckett–Gray code can be found in Donald Knuth's "Art of Computer Programming". According to Sawada and Wong, the search space for "n" = 6 can be explored in 15 hours, and more than 9,500 solutions for the case "n" = 7 have been found.
Snake-in-the-box codes.
Snake-in-the-box codes, or "snakes", are the sequences of nodes of induced paths in an "n"-dimensional hypercube graph, and coil-in-the-box codes, or "coils", are the sequences of nodes of induced cycles in a hypercube. Viewed as Gray codes, these sequences have the property of being able to detect any single-bit coding error. Codes of this type were first described by W. H. Kautz in the late 1950s; since then, there has been much research on finding the code with the largest possible number of codewords for a given hypercube dimension.
Single-track Gray code.
Yet another kind of Gray code is the single-track Gray code (STGC) developed by N. B. Spedding and refined by Hiltgen, Paterson and Brandestini in "Single-track Gray codes" (1996). The STGC is a cyclical list of "P" unique binary encodings of length n such that two consecutive words differ in exactly one position, and when the list is examined as a "P" × "n" matrix, each column is a cyclic shift of the first column.
The name comes from their use with rotary encoders, where a number of tracks are being sensed by contacts, resulting for each in an output of 0 or 1. To reduce noise due to different contacts not switching at exactly the same moment in time, one preferably sets up the tracks so that the data output by the contacts are in Gray code. To get high angular accuracy, one needs lots of contacts; in order to achieve at least 1 degree accuracy, one needs at least 360 distinct positions per revolution, which requires a minimum of 9 bits of data, and thus the same number of contacts.
If all contacts are placed at the same angular position, then 9 tracks are needed to get a standard BRGC with at least 1 degree accuracy. However, if the manufacturer moves a contact to a different angular position (but at the same distance from the center shaft), then the corresponding "ring pattern" needs to be rotated the same angle to give the same output. If the most significant bit (the inner ring in Figure 1) is rotated enough, it exactly matches the next ring out. Since both rings are then identical, the inner ring can be cut out, and the sensor for that ring moved to the remaining, identical ring (but offset at that angle from the other sensor on that ring). Those two sensors on a single ring make a quadrature encoder. That reduces the number of tracks for a "1 degree resolution" angular encoder to 8 tracks. Reducing the number of tracks still further can't be done with BRGC.
For many years, Torsten Sillke and other mathematicians believed that it was impossible to encode position on a single track such that consecutive positions differed at only a single sensor, except for the 2-sensor, 1-track quadrature encoder. So for applications where 8 tracks were too bulky, people used single-track incremental encoders (quadrature encoders) or 2-track "quadrature encoder + reference notch" encoders.
N. B. Spedding, however, registered a patent in 1994 with several examples showing that it was possible. Although it is not possible to distinguish 2"n" positions with "n" sensors on a single track, it "is" possible to distinguish close to that many. For example, when "n" is itself a power of 2, "n" sensors can distinguish 2"n" − 2"n" positions. Hiltgen and Paterson published a paper in 2001 exhibiting a single-track gray code with exactly 360 angular positions, constructed using 9 sensors. Since this number is larger than 28 = 256, more than 8 sensors are required by any code, although a BRGC could distinguish 512 positions with 9 sensors.
An STGC for "P" = 30 and "n" = 5 is reproduced here:
Each column is a cyclic shift of the first column, and from any row to the next row only one bit changes.
The single-track nature (like a code chain) is useful in the fabrication of these wheels (compared to BRGC), as only one track is needed, thus reducing their cost and size.
The Gray code nature is useful (compared to chain codes, also called De Bruijn sequences), as only one sensor will change at any one time, so the uncertainty during a transition between two discrete states will only be plus or minus one unit of angular measurement the device is capable of resolving.
2-Dimensional Gray code.
Two-dimensional Gray codes are used in communication to minimize the number of bit errors in Quadrature amplitude modulation adjacent points in the constellation. In a typical encoding the horizontal and vertical adjacent constellation points differ by a single bit, and diagonal adjacent points differ by 2 bits.
Gray isometry.
The bijective mapping { 0 ↔ 00, 1 ↔ 01, 2 ↔ 11, 3 ↔ 10 } establishes an isometry between the metric space over the finite field formula_70 with the metric given by the Hamming distance and the metric space over the finite ring formula_71 (the usual modulo arithmetic) with the metric given by the Lee distance. The mapping is suitably extended to an isometry of the Hamming spaces formula_72 and formula_73. Its importance lies in establishing a correspondence between various "good" but not necessarily linear codes as Gray-map images in formula_70 of ring-linear codes from formula_71.

</doc>
<doc id="50565" url="https://en.wikipedia.org/wiki?curid=50565" title="Executive">
Executive

Executive may refer to:

</doc>
<doc id="50567" url="https://en.wikipedia.org/wiki?curid=50567" title="Gold code">
Gold code

A Gold code, also known as Gold sequence, is a type of binary sequence, used in telecommunication (CDMA) and satellite navigation (GPS). Gold codes are named after Robert Gold. Gold codes have bounded small cross-correlations within a set, which is useful when multiple devices are broadcasting in the same frequency range. A set of Gold code sequences consists of 2"n" − 1 sequences each one with a period of 2"n" − 1.
A set of Gold codes can be generated with the following steps. Pick two maximum length sequences of the same length 2"n" − 1 such that their absolute cross-correlation is less than or equal to 2("n"+2)/2, where "n" is the size of the LFSR used to generate the maximum length sequence (Gold '67). The set of the 2"n" − 1 exclusive-ors of the two sequences in their various phases (i.e. translated into all relative positions) is a set of Gold codes. The highest absolute cross-correlation in this set of codes is 2("n"+2)/2 + 1 for even "n" and 2("n"+1)/2 + 1 for odd "n".
The exclusive or of two different Gold codes from the same set is another Gold code in some phase.
Within a set of Gold codes about half of the codes are balancedthe number of ones and zeros differs by only one.
Gold codes are used in GPS. The GPS C/A ranging codes are Gold code of period 1,023.

</doc>
<doc id="50568" url="https://en.wikipedia.org/wiki?curid=50568" title="Personal rapid transit">
Personal rapid transit

Personal rapid transit (PRT), also referred to as podcars, is a public transport mode featuring small automated vehicles operating on a network of specially built guideways. PRT is a type of automated guideway transit (AGT), a class of system which also includes larger vehicles all the way to small subway systems.
PRT vehicles are sized for individual or small group travel, typically carrying no more than 3 to 6 passengers per vehicle. Guideways are arranged in a network topology, with all stations located on sidings, and with frequent merge/diverge points. This allows for nonstop, point-to-point travel, bypassing all intermediate stations. The point-to-point service has been compared to a taxi or a horizontal lift (elevator).
As of July 2013, four PRT systems are operational: The world's oldest and most extensive PRT system is in Morgantown, West Virginia. It has been in continuous operation since 1975. Colloquially known merely as 'the PRT,' West Virginia University's system moves student and visitors alike to a number of popular destinations throughout the city. Since 2010 a 10-vehicle 2getthere system has operated at Masdar City, UAE, and since 2011 a 21-vehicle Ultra PRT system has run at London Heathrow Airport. A 40-vehicle Vectus system with in-line stations officially opened in Suncheon, South Korea, in April 2014 after a year of testing. Expansion of the Masdar system was cancelled just after the pilot scheme opened. Numerous other PRT systems have been proposed but not implemented, including many substantially larger than those now operating.
Overview.
Most mass transit systems move people in groups over scheduled routes. This has inherent inefficiencies. For passengers, time is wasted by waiting for the next arrival, indirect routes to their destination, stopping for passengers with other destinations, and often confusing or inconsistent schedules. Slowing and accelerating large weights can undermine public transport's benefit to the environment while slowing other traffic. Personal rapid transit systems attempt to eliminate these wastes by moving small groups nonstop in automated vehicles on fixed tracks. Passengers can ideally board a pod immediately upon arriving at a station, and can — with a sufficiently extensive network of tracks — take relatively direct routes to their destination without stops.
Perhaps most importantly, PRT systems offer many traits similar to automobiles. For example, they offer privacy and the ability to choose one's own schedule. PRT may in fact allow for quicker transportation than cars during rush hour, since automated vehicles avoid unnecessary slowing. A PRT system can also transport freight.
The low weight of PRT's small vehicles allows smaller guideways and support structures than mass transit systems like light rail. The smaller structures translate into lower construction costs, smaller easements, and less visually obtrusive infrastructure.
As it stands, a city-wide deployment with many lines and closely spaced stations, as envisioned by proponents, has yet to be constructed. Past projects have failed because of financing, cost overruns, regulatory conflicts, political issues, misapplied technology, and flaws in design, engineering or review.
However, the theory remains active. For example, from 2002–2005, the EDICT project, sponsored by the European Union, conducted a study on the feasibility of PRT in four European cities. The study involved 12 research organizations, and concluded that PRT:
The report also concluded that, despite these advantages, public authorities will not commit to building PRT because of the risks associated with being the first public implementation.
The PRT acronym was introduced formally in 1978 by J. Edward Anderson. The Advanced Transit Association (ATRA), a group which advocates the use of technological solutions to transit problems, compiled a definition in 1988 that can be seen here.
List of operational automated transit networks (ATN) systems.
Currently, five advanced transit networks (ATN) systems are operational, and several more are in the planning stage.
List of ATN suppliers.
The following list summarizes several well-known automated transit networks (ATN) suppliers as of 2014.
History.
Origins.
Modern PRT concepts began around 1953 when Donn Fichter, a city transportation planner, began research on PRT and alternative transportation methods. In 1964, Fichter published a book which proposed an automated public transit system for areas of medium to low population density. One of the key points made in the book was Fichter's belief that people would not leave their cars in favor of public transit unless the system offered flexibility and end-to-end transit times that were much better than existing systems – flexibility and performance he felt only a PRT system could provide. Several other urban and transit planners also wrote on the topic and some early experimentation followed, but PRT remained relatively unknown.
Around the same time, Edward Haltom was studying monorail systems. Haltom noticed that the time to start and stop a conventional large monorail train, like those of the Wuppertal Schwebebahn, meant that a single line could only support between 20 and 40 vehicles an hour. In order to get reasonable passenger movements on such a system, the trains had to be large enough to carry hundreds of passengers (see headway for a general discussion). This, in turn, demanded large guideways that could support the weight of these large vehicles, driving up capital costs to the point where he considered them unattractive.
Haltom turned his attention to developing a system that could operate with shorter timings, thereby allowing the individual cars to be smaller while preserving the same overall route capacity. Smaller cars would mean less weight at any given point, which meant smaller and less expensive guideways. To eliminate the backup at stations, the system used "offline" stations that allowed the mainline traffic to bypass the stopped vehicles. He designed the Monocab system using six-passenger cars suspended on wheels from an overhead guideway. Like most suspended systems, it suffered from the problem of difficult switching arrangements. Since the car rode on a rail, switching from one path to another required the rail to be moved, a slow process that limited the possible headways.
UMTA is formed.
By the late 1950s the problems with urban sprawl were becoming evident in the United States. When cities improved roads and the transit times were lowered, suburbs developed at ever increasing distances from the city cores, and people moved out of the downtown areas. Lacking pollution control systems, the rapid rise in car ownership and the longer trips to and from work were causing significant air quality problems. Additionally, movement to the suburbs led to a flight of capital from the downtown areas, one cause of the rapid urban decay seen in the US.
Mass transit systems were one way to combat these problems. Yet during this period, the federal government was feeding the problems by funding the development of the Interstate Highway System, while at the same time funding for mass transit was being rapidly scaled back. Public transit ridership in most cities plummeted.
In 1962, President John F. Kennedy charged Congress with the task of addressing these problems. These plans came to fruition in 1964, when President Lyndon B. Johnson signed the Urban Mass Transportation Act of 1964 into law, thereby forming the Urban Mass Transportation Administration. The UMTA was set up to fund mass transit developments in the same fashion that the earlier Federal Aid Highway Act of 1956 had helped create the Interstate Highways. That is, the UMTA would help cover the capital costs of building out new infrastructure.
PRT research starts.
However, planners who were aware of the PRT concept were worried that building more systems based on existing technologies would not help the problem, as Fitcher had earlier noted. Proponents suggested that systems would have to offer the flexibility of a car:
The reason for the sad state of public transit is a very basic one - the transit systems just do not offer a service which will attract people away from their automobiles. Consequently, their patronage comes very largely from those who cannot drive, either because they are too young, too old, or because they are too poor to own and operate an automobile. Look at it from the standpoint of a commuter who lives in a suburb and is trying to get to work in the central business district (CBD). If he is going to go by transit, a typical scenario might be the following: he must first walk to the closest bus stop, let us say a five or ten minute walk, and then he may have to wait up to another ten minutes, possibly in inclement weather, for the bus to arrive. When it arrives, he may have to stand unless he is lucky enough to find a seat. The bus will be caught up in street congestion and move slowly, and it will make many stops completely unrelated to his trip objective. The bus may then let him off at a terminal to a suburban train. Again he must wait, and, after boarding the train, again experience a number of stops on the way to the CBD, and possibly again he may have to stand in the aisle. He will get off at the station most convenient to his destination and possibly have to transfer again onto a distribution system. It is no wonder that in those cities where ample inexpensive parking is available, most of those who can drive do drive.
In 1966, the United States Department of Housing and Urban Development was asked to "undertake a project to study … new systems of urban transportation that will carry people and goods … speedily, safely, without polluting the air, and in a manner that will contribute to sound city planning." The resulting report was published in 1968 and proposed the development of PRT, as well as other systems such as dial-a-bus and high-speed interurban links.
In the late 1960s, the Aerospace Corporation, an independent non-profit corporation set up by the US Congress, spent substantial time and money on PRT, and performed much of the early theoretical and systems analysis. However, this corporation is not allowed to sell to non-federal government customers. In 1969, members of the study team published the first widely publicized description of PRT in "Scientific American".
In 1978 the team also published a book. These publications sparked off a sort of "transit race" in the same sort of fashion as the space race, with countries around the world rushing to join what appeared to be a future market of immense size.
The oil crisis of 1973 made vehicle fuels more expensive, which naturally interested people in alternative transportation.
System developments.
In 1967, aerospace giant Matra started the Aramis project in Paris. After spending about 500 million francs, the project was canceled when it failed its qualification trials in November 1987. The designers tried to make Aramis work like a "virtual train", but control software issues caused cars to bump unacceptably. The project ultimately failed.
Between 1970 and 1978, Japan operated a project called "Computer-controlled Vehicle System" (CVS). In a full-scale test facility, 84 vehicles operated at speeds up to on a guideway; one-second headways were achieved during tests. Another version of CVS was in public operation for six months from 1975–1976. This system had 12 single-mode vehicles and four dual-mode vehicles on a track with five stations. This version carried over 800,000 passengers. CVS was cancelled when Japan's Ministry of Land, Infrastructure and Transport declared it unsafe under existing rail safety regulations, specifically in respect of braking and headway distances.
On March 23, 1973, U.S. Urban Mass Transportation Administration (UMTA) administrator Frank Herringer testified before Congress: "A DOT program leading to the development of a short, one-half to one-second headway, high-capacity PRT (HCPRT) system will be initiated in fiscal year 1974." However, this HCPRT program was diverted into a modest technology program. According to PRT supporter J. Edward Anderson, this was "because of heavy lobbying from interests fearful of becoming irrelevant if a genuine PRT program became visible." From that time forward people interested in HCPRT were unable to obtain UMTA research funding.
In 1975, the Morgantown Personal Rapid Transit project was completed. It has five off-line stations that enable non-stop, individually programmed trips along an track serviced by a fleet of 71 cars. This is a crucial characteristic of PRT. However, it is not considered a PRT system because its vehicles are too heavy and carry too many people. When it carries many people, it operates in a point-to-point fashion, instead of running like an automated people mover from one end of the line to the other. During periods of low usage all cars make a full circuit stopping at every station in both directions. Morgantown PRT is still in continuous operation at West Virginia University in Morgantown, West Virginia, with about 15,000 riders per day (). It successfully demonstrates automated control, but was not sold to other sites because the steam-heated track has proven too expensive for a system that requires an operation and maintenance budget of $5 million annually.
From 1969 to 1980, Mannesmann Demag and MBB cooperated to build the "Cabinentaxi" urban transportation system in Germany. Together the firms formed the Cabintaxi Joint Venture. They created an extensive PRT technology that was considered fully developed by the German government and its safety authorities. The system was to have been installed in Hamburg, but budget cuts stopped the proposed project before the start of construction. With no other potential projects on the horizon, the joint venture disbanded, and the fully developed PRT technology was never installed. Cabintaxi Corporation, a US-based company, obtained the technology in 1985, and remains active in the private-sector market for transportation systems.
Later developments.
In the 1990s, Raytheon invested heavily in a system called PRT 2000, based on technology developed by J. Edward Anderson at the University of Minnesota. Raytheon failed to install a contracted system in Rosemont, Illinois, near Chicago, when estimated costs escalated to US$50 million per mile, allegedly due to design changes that increased the weight and cost of the system relative to Anderson's original design. In 2000, rights to the technology reverted to the University of Minnesota, and were subsequently purchased by Taxi2000.
In 2002, 2getthere operated 25 4-passenger "CyberCabs" at Holland's 2002 Floriade horticultural exhibition. These transported passengers along a track spiraling up to the summit of Big Spotters Hill. The track was approximately long (one-way) and featured only two stations. The six-month operations were intended to research the public acceptance of PRT-like systems. The CyberCab as designed for the exhibition was very open. It was comparable to a Neighborhood Electric Vehicle, except it steered itself using magnetic guidance points embedded in the pavement.
Ford Research proposed a dual-mode system called PRISM. It would use public guideways with privately purchased but certified dual-mode vehicles. The vehicles would weigh less than . Most energy use occurs on highways, so small, elevated guideways would inductively power highway use and recharge batteries for off-guideway use. Central computers could do routing.
In January 2003, the prototype ULTra ("Urban Light Transport") system in Cardiff, Wales, was certified to carry passengers by the UK Railway Inspectorate on a test track. ULTra was selected in October 2005 by BAA plc for London's Heathrow Airport. Since May 2011 a three-station system has been open to the public, transporting passengers from a remote parking lot to terminal 5. In May 2013 Heathrow Airport Limited included in its draft five-year (2014-2019) master plan a scheme to use the PRT system to connect terminal 2 and terminal 3 to their respective business car parks. The proposal was not included in the final plan due to spending priority given to other capital projects and has been deferred.
In June 2006, a Korean/Swedish consortium, Vectus Ltd, started constructing a test track in Uppsala, Sweden. This test system was presented at the 2007 PodCar City conference in Uppsala. A 40-vehicle, 2-station, system called "SkyCube" was opened in Suncheon, South Korea, in April 2014.
The Vectus project was based on The Fornebu/Oslo PRT Project. At the time, the urban development area around Telenor's new headquarter (at the Fornebu area near Oslo) was subject to intense debates as to various more or less innovative public transport systems. The idea of a PRT came up as a possible local solution as well as a business opportunity. In 2000, the Fornebu/Oslo PRT Project started as a part of an internal educational exercise within ICT strategy innovation within Telenor ASA, a major ICT corporation. As the poster shows, the student project was later transformed into a fast working concept, technology and business development project with various industry partners and a project group of around 10. The Korean steel company POSCO joined in, and developed the project further in Uppsala, Sweden, in part through new partners, but with all essential elements from the Fornebu/Oslo PRT Project, as further industrial or governmental support found in the Oslo area vanished. The poster describes the consortium and main results from the Oslo PRT project period. Key persons in this concept development phase were - as to technology and operational features development - Ingmar Andreasson, Göteborg, Sweden; Jan Orsten, independent traffic planner, Oslo; Alan Forster, Force Ltd, Great Britain; and Andrew Howard, HWG Ltd, Great Britain. Beyond the general conceptual description, the ICT systems were developed by Noventus AB and others at later stages.
In 2007, the Polish PRT system MISTER was prototyped, and permission was given to install it in two Polish cities. MISTER is a typical overhead PRT system engineered for economical aerial reuse of streets' rights of way, that still gives ground-level access to wheelchairs and freight.
System design.
Among the handful of prototype systems (and the larger number that exist on paper) there is a substantial diversity of design approaches, some of which are controversial.
Vehicle design.
Vehicle weight influences the size and cost of a system's guideways, which are in turn a major part of the capital cost of the system. Larger vehicles are more expensive to produce, require larger and more expensive guideways, and use more energy to start and stop. If vehicles are too large, point-to-point routing also becomes more expensive. Against this, smaller vehicles have more surface area per passenger (thus have higher total air resistance which dominates the energy cost of keeping vehicles moving at speed), and larger motors are generally more efficient than smaller ones.
The number of riders who will share a vehicle is a key unknown. In the U.S., the average car carries 1.16 persons, and most industrialized countries commonly average below two people; not having to share a vehicle with strangers is a key advantage of private transport. Based on these figures, some have suggested that two passengers per vehicle (such as with UniModal), or even a single passenger per vehicle is optimum. Other designs use a car for a model, and choose larger vehicles, making it possible to accommodate families with small children, riders with bicycles, disabled passengers with wheelchairs, or a pallet or two of freight.
Propulsion.
All current designs (except for the human-powered Shweeb) are powered by electricity. In order to reduce vehicle weight, power is generally transmitted via lineside conductors rather than using on-board batteries. According to the designer of Skyweb/Taxi2000, J. Edward Anderson, the lightest system is a linear induction motor (LIM) on the car, with a stationary conductive rail for both propulsion and braking. LIMs are used in a small number of rapid transit applications, but most designs use rotary motors. Most such systems retain a small on-board battery to reach the next stop after a power failure.
ULTra uses on-board batteries, recharged at stops. This increases the safety, and reduces the complexity, cost and maintenance of the guideway. As a result, a street-level ULTRa guideway resembles a sidewalk with curbs and is very inexpensive to construct. ULTRa resembles a small automated electric car, and uses similar components.
Switching.
Most designers avoid track switching, instead advocating vehicle-mounted switches or conventional steering. Those designers say that vehicle-switching permits faster switching, so vehicles can be closer together. It also simplifies the guideway, makes junctions less visually obtrusive and reduces the impact of malfunctions, because a failed switch on one vehicle is less likely to affect other vehicles. Other designers point out that track-switching simplifies the vehicles, reducing the number of small moving parts in each car. Track-switching replaces in-vehicle mechanisms with larger track-moving components, that can be designed for durability with little regard for weight or size.
Track switching greatly increases headway distance. A vehicle must wait for the previous vehicle to clear the track, for the track to switch and for the switch to be verified. If the track switching is faulty, vehicles must be able to stop before reaching the switch, and all vehicles approaching the failed junction would be affected.
Mechanical vehicle switching minimizes inter-vehicle spacing or headway distance, but it also increases the minimum distances between consecutive junctions. A mechanically switching vehicle, maneuvering between two adjacent junctions with different switch settings, cannot proceed from one junction to the next. The vehicle must adopt a new switch position, and then wait for the in-vehicle switch's locking mechanism to be verified. If the vehicle switching is faulty, that vehicle must be able to stop before reaching the next switch, and all vehicles approaching the failed vehicle would be affected.
Conventional steering allows a simpler 'track' consisting only of a road surface with some form of reference for the vehicle's steering sensors. Switching would be accomplished by the vehicle following the appropriate reference line- maintaining a set distance from the left roadway edge would cause the vehicle to diverge left at a junction, for example.
Infrastructure design.
Guideways.
There is some debate over the best type of guideway. Proposals include beams similar to monorails, bridge-like trusses supporting internal tracks, and cables embedded in a roadway. Most designs put the vehicle on top of the track, which reduces visual intrusion and cost as well as easing ground-level installation. An overhead track is necessarily higher, but may also be narrower. Most designs use the guideway to distribute power and data communications, including to the vehicles. The Morgantown PRT failed its cost targets because of its steam-heated track, so most proposals plan to resist snow and ice in ways that should be less expensive. Masdar's system has been limited because it attempted to dedicate ground-level to PRT guideways. This led to unrealistically expensive buildings and roads.
Stations.
Proposals usually have stations close together, and located on side tracks so that through traffic can bypass vehicles picking up or dropping off passengers. Each station might have multiple berths, with perhaps one-third of the vehicles in a system being stored at stations waiting for passengers. Stations are envisioned to be minimalistic, without facilities such as rest rooms. For elevated stations, an elevator may be required for accessibility.
At least one system, MISTER, provides wheelchair and freight access by using a cogway in the track, so that the vehicle itself can go from a street-level stop to an overhead track.
Some designs have included substantial extra expense for the track needed to decelerate to and accelerate from stations. In at least one system, Aramis, this nearly doubled the width and cost of the required right-of-way and caused the nonstop passenger delivery concept to be abandoned. Other designs have schemes to reduce this cost, for example merging vertically to reduce the footprint.
When user demand is low, surplus vehicles could be configured to stop at empty stations at strategically placed points around the network. This enables an empty vehicle to quickly be despatched to wherever it is required, with minimal waiting time for the passenger.
Operational characteristics.
Headway distance.
Spacing of vehicles on the guideway influences the maximum passenger capacity of a track, so designers prefer smaller headway distances. Computerized control theoretically permits closer spacing than the two-second headways recommended for cars at speed, since multiple vehicles can be braked simultaneously. There are also prototypes for automatic guidance of private cars based on similar principles.
Very short headways are controversial. The UK Railway Inspectorate has evaluated the ULTra design and is willing to accept one-second headways, pending successful completion of initial operational tests at more than 2 seconds. In other jurisdictions, existing rail regulations apply to PRT systems (see CVS, above); these typically calculate headways in terms of absolute stopping distances, which would restrict capacity and make PRT systems unfeasible. No regulatory agency has yet endorsed headways below one second, although proponents believe that regulators may be willing to reduce headways as operational experience increases.
Capacity.
PRT is usually proposed as an alternative to rail systems, so comparisons tend to be with rail. PRT vehicles seat fewer passengers than trains and buses, and must offset this by combining higher average speeds, diverse routes, and shorter headways. Proponents assert that equivalent or higher overall capacity can be achieved by these means.
Single line capacity.
With two-second headways and four-person vehicles, a single PRT line can achieve theoretical maximum capacity of 7,200 passengers per hour. However, most estimates assume that vehicles will not generally be filled to capacity, due to the point-to-point nature of PRT. At a more typical average vehicle occupancy of 1.5 persons per vehicle, the maximum capacity is 2,700 passengers per hour. Some researchers have suggested that rush hour capacity can be improved if operating policies support ridesharing.
Capacity is inversely proportional to headway. Therefore, moving from two-second headways to one-second headways would double PRT capacity. Half-second headways would quadruple capacity. Theoretical minimum PRT headways would be based on the mechanical time to engage brakes, and these are much less than a half second. Although no regulatory agency has as yet (June 2006) approved headways shorter than two seconds, researchers suggest that high capacity PRT (HCPRT) designs could operate safely at half-second headways. Using the above figures, capacities above 10,000 passengers per hour seem in reach.
In simulations of rush hour or high-traffic events, about one-third of vehicles on the guideway need to travel empty to resupply stations with vehicles in order to minimize response time. This is analogous to trains and buses travelling nearly empty on the return trip to pick up more rush hour passengers.
Grade separated light rail systems can move 15,000 passengers per hour on a fixed route, but these are usually fully grade separated systems. Street level systems typically move up to 7,500 passengers per hour. Heavy rail subways can move 50,000 passengers per hour. As with PRT, these estimates depend on having enough trains. Neither light nor heavy rail scales well for off-peak operation.
Networked PRT capacity.
The above discussion compares line or corridor capacity and may therefore not be relevant for a networked PRT system, where several parallel lines (or parallel components of a grid) carry traffic. In addition, Muller estimated that while PRT may need more than one guideway to match the capacity of a conventional system, the capital cost of the multiple guideways may still be less than that of the single guideway conventional system. Thus comparisons of line capacity should also consider the cost per line.
PRT systems should require much less horizontal space than existing metro systems, with individual cars being typically around 50% as wide for side-by-side seating configurations, and less than 33% as wide for single-file configurations. This is an important factor in densely populated, high-traffic areas.
Travel speed.
For a given peak speed, nonstop journeys are about three times as fast as those with intermediate stops. This is not just because of the time for starting and stopping. Scheduled vehicles are also slowed by boardings and exits for multiple destinations.
Therefore, a given PRT seat transports about three times as many passenger miles per day as a seat performing scheduled stops. So PRT should also reduce the number of needed seats threefold for a given number of passenger miles.
While a few PRT designs have operating speeds of 100 km/h (60 mph), and one as high as 241 km/h (150 mph), most are in the region of 40–70 km/h (25–45 mph). Rail systems generally have higher maximum speeds, typically 90–130 km/h (55–80 mph) and sometimes well in excess of 160 km/h (100 mph), but average travel speed is reduced about threefold by scheduled stops and passenger transfers.
Ridership attraction.
If PRT designs deliver the claimed benefit of being substantially faster than cars in areas with heavy traffic, simulations suggest that PRT could attract many more car drivers than other public transit systems. Standard mass transit simulations accurately predict that 2% of trips (including cars) will switch to trains. Similar methods predict that 11% to 57% of trips would switch to PRT, depending on its costs and delays.
Control algorithms.
The typical control algorithm places vehicles in imaginary moving "slots" that go around the loops of track. Real vehicles are allocated a slot by track-side controllers. Traffic jams are prevented by placing north/south vehicles in even slots, and east/west vehicles in odd slots. At intersections, the traffic in these systems can interpenetrate without slowing.
On-board computers maintain their position by using a negative feedback loop to stay near the center of the commanded slot. Early PRT vehicles measured their position by adding up the distance using odometers, with periodic check points to compensate for cumulative errors. Next-generation GPS and radio location could measure positions as well.
Another system, "pointer-following control", assigns a path and speed to a vehicle, after verifying that the path does not violate the safety margins of other vehicles. This permits system speeds and safety margins to be adjusted to design or operating conditions, and may use slightly less energy. The maker of the ULTra PRT system reports that testing of its control system shows lateral (side-to-side) accuracy of 1 cm, and docking accuracy better than 2 cm.
Safety.
Computer control eliminates errors from human drivers, so PRT designs in a controlled environment should be much safer than private motoring on roads. Most designs enclose the running gear in the guideway to prevent derailments. Grade-separated guideways would prevent conflict with pedestrians or manually controlled vehicles. Other public transit safety engineering approaches, such as redundancy and self-diagnosis of critical systems, are also included in designs.
The Morgantown system, more correctly described as a Group Rapid Transit (GRT) type of Automated Guideway Transit system (AGT), has completed 110 million passenger-miles without serious injury. According to the U.S. Department of Transportation, AGT systems as a group have higher injury rates than any other form of rail-based transit (subway, metro, light rail, or commuter rail) though still much better than ordinary buses or cars. More recent research by the British company ULTra PRT reported that AGT systems have a better safety than more conventional, non-automated modes.
As with many current transit systems, personal passenger safety concerns are likely to be addressed through CCTV monitoring, and communication with a central command center from which engineering or other assistance may be dispatched.
Energy efficiency.
The energy efficiency advantages claimed by PRT proponents include two basic operational characteristics of PRT: an increased average load factor; and the elimination of intermediate starting and stopping.
Average load factor, in transit systems, is the ratio of the total number of riders to the total theoretical capacity. A transit vehicle running at full capacity has a 100% load factor, while an empty vehicle has 0% load factor. If a transit vehicle spends half the time running at 100% and half the time running at 0%, the "average" load factor is 50%. Higher average load factor corresponds to lower energy consumption per passenger, so designers attempt to maximize this metric.
Scheduled mass transit (i.e. buses or trains) trades off service frequency and load factor. Buses and trains must run on a predefined schedule, even during off-peak times when demand is low and vehicles are nearly empty. So to increase load factor, transportation planners try to predict times of low demand, and run reduced schedules or smaller vehicles at these times. This increases passengers' wait times. In many cities, trains and buses do not run at all at night or on weekends.
PRT vehicles, in contrast, would only move in response to demand, which places a theoretical lower bound on their average load factor. This allows 24-hour service without many of the costs of scheduled mass transit. 
ULTra PRT estimates its system will consume 839 BTU per passenger mile (0.55 MJ per passenger km). By comparison, cars consume 3,496 BTU, and personal trucks consume 4,329 BTU per passenger mile.
Due to PRT's efficiency, some proponents say solar becomes a viable power source. PRT elevated structures provide a ready platform for solar collectors, therefore some proposed designs include solar power as a characteristic of their networks.
For bus and rail transit, the energy per passenger-mile depends on the ridership and the frequency of service. Therefore, the energy per passenger-mile can vary significantly from peak to non-peak times. In the US, buses consume an average of 4,318 BTU/passenger-mile, transit rail 2,750 BTU/passenger-mile, and commuter rail 2,569 BTU/passenger-mile.
Opposition and controversy.
Opponents to PRT schemes have expressed a number of concerns:
Technical feasibility debate.
The Ohio, Kentucky, Indiana (OKI) Central Loop Report compared the Taxi 2000 PRT concept proposed by the Skyloop Committee to other transportation modes (bus, light rail and vintage trolley). In the Taxi 2000 PRT system, the Loop Study Advisory Committee identified "significant environmental, technical and potential fire and life safety concerns…" and the PRT system was "…still an unproven technology with significant questions about cost and feasibility of implementation." Skyloop contested this conclusion, arguing that Parsons Brinckerhoff changed several aspects of the system design without consulting with Taxi 2000, then rejected this modified design. Despite the report's concerns regarding the implementation obstacles of PRT, the report did conclude that compared to the other alternatives, PRT offered the most acceptable point-to-point travel times, the most reliable service levels, the highest level of frequency of service and geography coverage, and was most able to maintain schedule. The report further concluded that, compared to the other alternatives, PRT would have over three times the ridership of the next closest alternative, including new transit riders over nine times higher than the next closest alternative.
Vukan R. Vuchic, professor of Transportation Engineering at the University of Pennsylvania and a proponent of traditional forms of transit, has stated his belief that the combination of small vehicles and expensive guideway makes it highly impractical in both cities (not enough capacity) and suburbs (guideway too expensive). According to Vuchic: "...the PRT concept combines two mutually incompatible elements of these two systems: very small vehicles with complicated guideways and stations. Thus, in central cities, where heavy travel volumes could justify investment in guideways, vehicles would be far too small to meet the demand. In suburbs, where small vehicles would be ideal, the extensive infrastructure would be economically unfeasible and environmentally unacceptable."
PRT supporters claim that Vuchic's conclusions are based on flawed assumptions. PRT proponent J.E. Anderson wrote, in a rebuttal to Vuchic: "I have studied and debated with colleagues and antagonists every objection to PRT, including those presented in papers by Professor Vuchic, and find none of substance. Among those willing to be briefed in detail and to have all of their questions and concerns answered, I find great enthusiasm to see the system built."
The manufacturers of ULTra acknowledge that current forms of their system would provide insufficient capacity in high-density areas such as central London, and that the investment costs for the tracks and stations are comparable to building new roads, making the current version of ULTra more suitable for suburbs and other moderate capacity applications, or as a supplementary system in larger cities.
Regulatory concerns.
Possible regulatory concerns include emergency safety, headways, and accessibility for the disabled. Many jurisdictions regulate PRT systems as if they were trains. At least one successful prototype, CVS, failed deployment because it could not obtain permits from regulators.
Several PRT systems have been proposed for California, but the California Public Utilities Commission (CPUC) states that its rail regulations apply to PRT, and these require railway-sized headways. The degree to which CPUC would hold PRT to "light rail" and "rail fixed guideway" safety standards is not clear because it can grant particular exemptions and revise regulations.
Other forms of automated transit have been approved for use in California, notably the Airtrain system at SFO. CPUC decided not to require compliance with General Order 143-B (for light rail) since Airtrain has no on-board operators. They did require compliance with General Order 164-D which mandates a safety and security plan, as well as periodic on-site visits by an oversight committee.
If safety or access considerations require the addition of walkways, ladders, platforms or other emergency/disabled access to or egress from PRT guideways, the size of the guideway may be increased. This may impact the feasibility of a PRT system, though the degree of impact would depend on both the PRT design and the municipality.
Concerns about PRT research.
Wayne D. Cottrell of the University of Utah conducted a critical review of PRT academic literature since the 1960s. He concluded that there are several issues that would benefit from more research, including urban integration, risks of PRT investment, bad publicity, technical problems, and competing interests from other transport modes. He suggests that these issues, "while not unsolvable, are formidable," and that the literature might be improved by better introspection and criticism of PRT. He also suggests that more government funding is essential for such research to proceed, especially in the United States.
New urbanist opinion.
Several proponents of new urbanism, an urban design movement that advocates for walkable cities, have expressed opinions on PRT.
Peter Calthorpe and Sir Peter Hall have supported the concept, but James Howard Kunstler disagrees: "If we're going to replace the car why do it with something that's not only like the car, but not really as good as the car? It just seems crazy." He also referred to PRT proponents as "a particular kind of crank".
Group rapid transit.
Group rapid transit (GRT) is similar to personal rapid transit but with higher-occupancy vehicles and grouping of passengers with potentially different origin-destination pairs. In this respect GRT can be seen as a sort of horizontal elevator. Such systems may have fewer direct-to-destination trips than single-destination PRT but still have fewer average stops than conventional transit, acting more as an automated share taxi system than a private cab system. Such a system may have advantages over low-capacity PRT in some applications, such as where higher passenger density is required or advantageous. It is also conceivable for a GRT system to have a range of vehicle sizes to accommodate different passenger load requirements, for example at different times of day or on routes with less or more average traffic. Such a system may constitute an "optimal" surface transportation routing solution in terms of balancing trip time and convenience with resource efficiency.
GRT has principally been proposed as a corridor service, where it can potentially provide a travel time improvement over conventional rail or bus and can also interface with PRT systems. However, GRT's necessary grouping of passengers makes it much less attractive in applications with lower passenger density or where few origin-destination pairs are shared among passengers.
Automated transit networks (ATN) is an umbrella term for GRT and PRT. While they have long been considered separate systems, Vectus is developing GRT vehicles formed by combining multiple PRT vehicles. The larger vehicles are designed to accommodate standees and operate on the same guideway as the PRT vehicles. The door spacing of the larger vehicles matches the door spacing of PRT vehicles stopped in a station, allowing the GRT vehicles to share the same station infrastructure too. The concept is intended to allow GRT to serve high-demand station pairs during peak periods, while PRT serves all stations at all times in a network which includes the high-demand station pairs as well as other stations.
The same passenger grouping and destination scheduling approach is used in some advanced elevators, in the form of a destination control system.

</doc>
<doc id="50571" url="https://en.wikipedia.org/wiki?curid=50571" title="Transportation engineering">
Transportation engineering

Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods (transport) . It is a sub-discipline of civil engineering and of industrial engineering. Transportation engineering is a major component of the civil engineering and mechanical engineering disciplines, according to specialisation of academic courses and main competences of the involved territory. The importance of transportation engineering within the civil and industrial engineering profession can be judged by the number of divisions in ASCE (American Society of Civil Engineers) that are directly related to transportation. There are six such divisions (Aerospace; Air Transportation; Highway; Pipeline; Waterway, Port, Coastal and Ocean; and Urban Transportation) representing one-third of the total 18 technical divisions within the ASCE (1987).
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors. Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (how many trips for what purpose), trip distribution (destination choice, where is the traveler going), mode choice (what mode is being taken), and route assignment (which streets or routes are being used). More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important to civil engineers, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, as practiced by civil engineers, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track). 
Before any planning occurs the Engineer must take what is known as an inventory of the area or if it is appropriate, the previous system in place. This inventory or database must include information on (1)population, (2)land use, (3)economic activity, (4)transportation facilities and services, (5)travel patterns and volumes, (6)laws and ordinances, (7)regional financial resources, (8)community values and expectations. These inventories help the engineer create business models to complete accurate forecasts of the future conditions of the systemReview.
Operations and management involve traffic engineering, so that vehicles move smoothly on the road or track. Older techniques include signs, signals, markings, and tolling. Newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. Human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.
Highway engineering.
Engineers in this specialization:
Railroad engineering.
Railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or even monorails). Typical tasks would include determining horizontal and vertical alignment design, station location and design, and construction cost estimating. Railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control.
Railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands. In the United States, railway engineers work with elected officials in Washington, D.C. on rail transportation issues to make sure that the rail system meets the country's transportation needs.
Port and harbor engineering.
Port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities. This is not to be confused with marine engineering.
Airport engineering.
Airport engineers design and construct airports. Airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. These engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.

</doc>
<doc id="50578" url="https://en.wikipedia.org/wiki?curid=50578" title="Queueing theory">
Queueing theory

Queueing theory is the mathematical study of waiting lines, or queues. In queueing theory a model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.
Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing
and the design of factories, shops, offices and hospitals.
Single queueing nodes.
Single queueing nodes are usually described using Kendall's notation in the form "A"/"S"/"C" where "A" describes the time between arrivals to the queue, "S" the size of jobs and "C" the number of servers at the node. Many theorems in queueing theory can be proved by reducing queues to mathematical systems known as Markov chains, first described by Andrey Markov in his 1906 paper.
Agner Krarup Erlang, a Danish engineer who worked for the Copenhagen Telephone Exchange, published the first paper on what would now be called queueing theory in 1909. He modeled the number of telephone calls arriving at an exchange by a Poisson process and solved the M/D/1 queue in 1917 and M/D/k queueing model in 1920. In Kendall's notation:
The M/M/1 queue is a simple model where a single server serves jobs that arrive according to a Poisson process and have exponentially distributed service requirements. In an M/G/1 queue the G stands for general and indicates an arbitrary probability distribution. The M/G/1 model was solved by Felix Pollaczek in 1930, a solution later recast in probabilistic terms by Aleksandr Khinchin and now known as the Pollaczek–Khinchine formula.
After the 1940s queueing theory became an area of research interest to mathematicians. In 1953 David George Kendall solved the GI/M/k queue and introduced the modern notation for queues, now known as Kendall's notation. In 1957 Pollaczek studied the GI/G/1 using an integral equation. John Kingman gave a formula for the mean waiting time in a G/G/1 queue: Kingman's formula.
The matrix geometric method and matrix analytic methods have allowed queues with phase-type distributed inter-arrival and service time distributions to be considered.
Problems such as performance metrics for the M/G/k queue remain an open problem.
Service disciplines.
Various scheduling policies can be used at queuing nodes:
Queueing networks.
Networks of queues are systems in which a number of queues are connected by customer routing. When a customer is serviced at one node it can join another node and queue for service, or leave the network. For a network of "m" the state of the system can be described by an "m"–dimensional vector ("x"1,"x"2...,"x""m") where "x""i" represents the number of customers at each node.
The first significant results in this area were Jackson networks, for which an efficient product-form stationary distribution exists and the mean value analysis which allows average metrics such as throughput and sojourn times to be computed. If the total number of customers in the network remains constant the network is called a closed network and has also been shown to have a product–form stationary distribution in the Gordon–Newell theorem. This result was extended to the BCMP network where a network with very general service time, regimes and customer routing is shown to also exhibit a product-form stationary distribution. The normalizing constant can be calculated with the Buzen's algorithm, proposed in 1973.
Networks of customers have also been investigated, Kelly networks where customers of different classes experience different priority levels at different service nodes. Another type of network are G-networks first proposed by Erol Gelenbe in 1993: these networks do not assume exponential time distributions like the classic Jackson Network.
Routing algorithms.
In discrete time networks where there is a constraint on which service nodes can be active at any time, the max-weight scheduling algorithm chooses a service policy to give optimal throughput in the case that each job visits only a single service node. In the more general case where jobs can visit more than one node, backpressure routing gives optimal throughput.
A network scheduler must choose a queuing algorithm, which affects the characteristics of the larger network.
Mean field limits.
Mean field models consider the limiting behaviour of the empirical measure (proportion of queues in different states) as the number of queues ("m" above) goes to infinity. The impact of other queues on any given queue in the network is approximated by a differential equation. The deterministic model converges to the same stationary distribution as the original model.
Fluid limits.
Fluid models are continuous deterministic analogs of queueing networks obtained by taking the limit when the process is scaled in time and space, allowing heterogeneous objects. This scaled trajectory converges to a deterministic equation which allows the stability of the system to be proven. It is known that a queueing network can be stable, but have an unstable fluid limit.
Heavy traffic/diffusion approximations.
In a system with high occupancy rates (utilisation near 1) a heavy traffic approximation can be used to approximate the queueing length process by a reflected Brownian motion, Ornstein–Uhlenbeck process or more general diffusion process. The number of dimensions of the RBM is equal to the number of queueing nodes and the diffusion is restricted to the non-negative orthant.

</doc>
<doc id="50579" url="https://en.wikipedia.org/wiki?curid=50579" title="Gaussian quadrature">
Gaussian quadrature

[[File:Comparison Gaussquad trapezoidal.svg|thumb|upright=2|alt=Comparison between 2-point Gaussian and trapezoidal quadrature.|Comparison between 2-point Gaussian and trapezoidal quadrature. 
The blue line is the polynomial formula_1, whose integral in [-1, 1] is 2/3. The trapezoidal rule returns the integral of the orange dashed line, equal to formula_2. The 2-point Gaussian quadrature rule returns the integral of the black dashed curve, equal to formula_3. Such a result is exact since the green region has the same area as the red regions.]]
In numerical analysis, a quadrature rule is an approximation of the definite integral of a function, usually stated as a weighted sum of function values at specified points within the domain of integration.
(See numerical integration for more on quadrature rules.) An "n"-point Gaussian quadrature rule, named after Carl Friedrich Gauss, is a quadrature rule constructed to yield an exact result for polynomials of degree or less by a suitable choice of the points and weights for . The domain of integration for such a rule is conventionally taken as [−1, 1], so the rule is stated as
Gaussian quadrature as above will only produce good results if the function "f"("x") is well approximated by a polynomial function within the range . The method is not, for example, suitable for functions with singularities. However, if the integrated function can be written as formula_5, where is approximately polynomial and is known, then alternative weights formula_6 and points formula_7 that depend on the weighting function may give better results, where
Common weighting functions include formula_9 (Chebyshev–Gauss) and formula_10 (Gauss–Hermite).
It can be shown (see Press, et al., or Stoer and Bulirsch) that the evaluation points are just the roots of a polynomial belonging to a class of orthogonal polynomials.
Gauss–Legendre quadrature.
For the simplest integration problem stated above, i.e. with formula_11, the associated polynomials are Legendre polynomials, "P""n"("x"), and the method is usually known as Gauss–Legendre quadrature. With the -th polynomial normalized to give "P""n"(1) = 1, the -th Gauss node, , is the -th root of ; its weight is given by 
Some low-order rules for solving the integration problem are listed below.
Change of interval.
An integral over must be changed into an integral over before applying the Gaussian quadrature rule. This change of interval can be done in the following way:
Applying the Gaussian quadrature rule then results in the following approximation:
Other forms.
The integration problem can be expressed in a slightly more general way by introducing a positive weight function ω into the integrand, and allowing an interval other than . That is, the problem is to calculate
for some choices of "a", "b", and ω. For "a" = −1, "b" = 1, and ω("x") = 1, the problem is the same as that considered above.
Other choices lead to other integration rules. Some of these are tabulated below. Equation numbers are given for Abramowitz and Stegun (A & S).
Fundamental theorem.
Let be a nontrivial polynomial of degree "n" such that
If we pick the "n" nodes to be the zeros of , then there exist "n" weights which make the Gauss-quadrature computed integral exact for all polynomials of degree 2"n" − 1 or less. Furthermore, all these nodes will lie in the open interval ("a", "b") .
The polynomial is said to be an orthogonal polynomial of degree "n" associated to the weight function . It is unique up to a constant normalization factor. The idea underlying the proof is that, because of its sufficiently low degree, can be divided by formula_17 to produce a quotient of degree strictly lower than "n", and a remainder of still lower degree, so that both will be orthogonal to formula_17, by the defining property of formula_17. Thus
Because of the choice of nodes "x""i", the corresponding relation
holds also. The exactness of the computed integral for formula_22 then follows from corresponding exactness for polynomials of degree only "n" or less (as is formula_23).
General formula for the weights.
The weights can be expressed as
where formula_25 is the coefficient of formula_26 in formula_27. To prove this, note that using Lagrange interpolation one can express in terms of formula_28 as
because has degree less than and is thus fixed by the values it attains at different points. Multiplying both sides by and integrating from to yields
The weights are thus given by 
This integral expression for formula_32 can be expressed in terms of the orthogonal polynomials formula_33 and formula_34 as follows.
We can write
where formula_36 is the coefficient of formula_37 in formula_33. Taking the limit of x to formula_39 yields using L'Hôpital's rule
We can thus write the integral expression for the weights as
In the integrand, writing
yields
provided formula_44, because 
is a polynomial of degree k-1 which is then orthogonal to formula_33. So, if is a polynomial of at most nth degree we have
We can evaluate the integral on the right hand side for formula_48 as follows. Because formula_49 is a polynomial of degree n-1, we have
where is a polynomial of degree formula_51. Since is orthogonal to formula_34 we have
We can then write
The term in the brackets is a polynomial of degree formula_51, which is therefore orthogonal to formula_34. The integral can thus be written as
According to Eq. (2), the weights are obtained by dividing this by formula_58 and that yields the expression in Eq. (1).
formula_32 can also be expressed in terms of the orthogonal polynomials formula_33 and now formula_61. In the 3-term recurrence relation formula_62 the term with formula_63 vanishes, so formula_64 in Eq. (1) can be replaced by formula_65.
Proof that the weights are positive.
Consider the following polynomial of degree 2n-2
where as above the are the roots of the polynomial formula_33. Since the degree of f(x) is less than 2n-1, the Gaussian quadrature formula involving the weights and nodes obtained from formula_33 applies. Since formula_69 for j not equal to i, we have
Since both formula_71 and f(x) are non-negative functions, it follows that formula_72.
Computation of Gaussian quadrature rules.
For computing the nodes and weights of Gaussian quadrature rules, the fundamental tool is the three-term recurrence relation satisfied by the set of orthogonal polynomials associated to the corresponding weight function. For points, these nodes and weights can be computed in "O"("n"2) operations by an algorithm derived by Gautschi (1968).
Gautschi's theorem.
Gautschi's theorem (Gautschi, 1968) states that orthogonal polynomials formula_73 with formula_74 for formula_75 for a scalar product formula_76 to be specified later, degreeformula_77 and leading coefficient one (i.e. monic orthogonal polynomials) satisfy the recurrence relation
for formula_79 where is the maximal degree which can be taken to be infinity, and where formula_80. First of all, it is obvious that the polynomials defined by the recurrence relation starting with formula_81 have leading coefficient one and correct degree. Given the starting point by formula_82, the orthogonality of formula_73 can be shown by induction. For formula_84 one has
Now if formula_86 are orthogonal, then also formula_87, because in
all scalar products vanish except for the first one and the one where formula_89 meets the same orthogonal polynomial. Therefore,
However, if the scalar product satisfies formula_91 (which is the case for Gaussian quadrature), the recurrence relation reduces to a three-term recurrence relation: For formula_92 is a polynomial of degree less or equal to . On the other hand, formula_73 is orthogonal to every polynomial of degree less or equal to . Therefore, one has formula_94 and formula_95 for . The recurrence relation then simplifies to 
or
(with the convention formula_98) where
(the last because of formula_100, since formula_101 differs from formula_73 by a degree less than ).
The Golub-Welsch algorithm.
The three-term recurrence relation can be written in the matrix form formula_103 where formula_104 is the formula_105th standard basis vector, i.e. formula_106, and is the so-called Jacobi matrix:
The zeros formula_108 of the polynomials up to degree which are used as nodes for the Gaussian quadrature can be found by computing the eigenvalues of this tridiagonal matrix. This procedure is known as "Golub–Welsch algorithm".
For computing the weights and nodes, it is preferable to consider the symmetric tridiagonal matrix formula_109 with elements 
where formula_114 is the integral of the weight function
See, for instance, for further details.
Error estimates.
The error of a Gaussian quadrature rule can be stated as follows . For an integrand which has continuous derivatives,
for some in , where is the monic (i.e. the leading coefficient is 1) orthogonal polynomial of degree and where
In the important special case of , we have the error estimate 
Abscissas: is the formula_119st zero of formula_120.
Weights:
Remainder:
Some of the weights are:

</doc>
<doc id="50580" url="https://en.wikipedia.org/wiki?curid=50580" title="Orange County, Texas">
Orange County, Texas

Orange County is a county in the U.S. state of Texas. As of the 2010 census, its population was 81,837. The county seat is Orange.
Orange County is included in the Beaumont-Port Arthur, TX Metropolitan Statistical Area. It is located in the very southeastern corner of Texas, with a boundary with Louisiana, within the Golden Triangle of Texas.
History.
Orange County was formed in 1852 from portions of Jefferson County. It was named after the orange fruit, the common citrus fruit grown by the early settlers of this County near the mouth of the Sabine River.
Due to periodic spells of quite cold winter weather (frosts) in Orange County, it is no longer the home of orange trees and citrus orchards. The production of those fruits in Texas long ago was moved a long way southwest into the Rio Grande Valley, where the weather is almost always warm all winter long. Citrus trees produce their fruit in the wintertime, which makes them especially vulnerable to frost and icy weather.
A similar thing has happened in Florida, where orchards of citrus trees no longer exist in either Citrus County or Orange County because of bad winter freezes in some years. In both Florida and Texas, the citrus agriculture has been moved farther south in search of milder winters, and away from the periodic frosts.
During World War II, Orange County was the home of a large amount of shipbuilding for the navies the United States and allied countries. The major shipbuilder, the Consolidated Steel Corporation was located in the town of Orange, and among the warships that it built were the (1942), the first warship built there, the (1943), and the (1945–46), the last warship built there. During the war, the Consolidate Steel Corporation employed as many as 20,000 people at its shipyard in Orange, Texas.
Geography.
According to the U.S. Census Bureau, the county has a total area of , of which is land and (12%) is water.
Orange County is bordered on its east by the Sabine River, on its southeast by Sabine Lake, and on the northwest by the Neches River.
The geography of Orange County varies relatively little, with an elevation that reaches 33 feet (10 meters) above sea level at very few points within the county. Orange County is very flat, and its soil is quite sandy, as could be expected in a county along the Gulf of Mexico. (Sandy soil is also common in southern Louisiana, Mississippi, and Alabama, and in western and southern Florida.) There are saltwater marshes in much of the southeastern part of Orange County that borders the Sabine River. There are piney woods (sometimes capitalized) in the northern part of the county.
Demographics.
As of the census of 2000, there were 84,966 people, 31,642 households, and 23,794 families residing in the county. The population density was 238 people per square mile (92/km²). There were 34,781 housing units at an average density of 98 per square mile (38/km²). The racial makeup of the county was 87.98% White, 8.38% Black or African American, 0.56% Native American, 0.78% Asian, 0.03% Pacific Islander, 1.12% from other races, and 1.15% from two or more races. 3.62% of the population were Hispanic or Latino of any race.
There were 31,642 households out of which 35.30% had children under the age of 18 living with them, 58.80% were married couples living together, 12.10% had a female householder with no husband present, and 24.80% were non-families. 21.70% of all households were made up of individuals and 9.30% had someone living alone who was 65 years of age or older. The average household size was 2.65 and the average family size was 3.08.
In the county, the population was spread out with 27.30% under the age of 18, 8.70% from 18 to 24, 28.10% from 25 to 44, 23.20% from 45 to 64, and 12.70% who were 65 years of age or older. The median age was 36 years. For every 100 females there were 96.40 males. For every 100 females age 18 and over, there were 92.60 males.
The median income for a household in the county was $37,586, and the median income for a family was $44,152. Males had a median income of $40,185 versus $21,859 for females. The per capita income for the county was $17,554. About 11.40% of families and 13.80% of the population were below the poverty line, including 18.50% of those under age 18 and 12.40% of those age 65 or over.
Government.
The Orange County Courthouse serves as the court for the region.
Orange County lies in Texas House District 21, represented beginning in 2015 by the Republican Dade Phelan of Beaumont.
Economy.
Primary economic activities in Orange County are the petroleum refining industry, paper milling, rice farming, and shrimping.
Orange County was formerly a center for the building of warships, and there is still a large U.S. Navy ghost fleet (reserve fleet) in Jefferson County - from which currently, many old warships are being cleaned of water pollution sources and then scrapped for their metals. Thus, there is still employment for residents of Orange County in shipbreaking.
Transportation.
Airports.
Orange County Airport operates general aviation flights.
Nearby Southeast Texas Regional Airport (Port Arthur) operates commercial flights.
Education.
The county is served by 5 school districts:
West Orange-Cove Consolidated Independent School District.
The district's Superintendent of Schools is James Colbert Jr.
Bridge City ISD.
The Superintendent is Jamey Harrison. It includes:
Little Cypress-Mauriceville Consolidated ISD.
The Superintendent is Dr. Pauline Hargrove. It includes:

</doc>
<doc id="50582" url="https://en.wikipedia.org/wiki?curid=50582" title="Zircon">
Zircon

Zircon ( or ; including hyacinth or yellow zircon) is a mineral belonging to the group of nesosilicates. Its chemical name is zirconium silicate and its corresponding chemical formula is ZrSiO4. A common empirical formula showing some of the range of substitution in zircon is (Zr1–y, REEy)(SiO4)1–x(OH)4x–y. Zircon forms in silicate melts with large proportions of high field strength incompatible elements. For example, hafnium is almost always present in quantities ranging from 1 to 4%. The crystal structure of zircon is tetragonal crystal system. The natural color of zircon varies between colorless, yellow-golden, red, brown, blue, and green. Colorless specimens that show gem quality are a popular substitute for diamond and are also known as "Matura diamond".
The name derives from the Persian "zargun" meaning golden-colored. This word is corrupted into "jargoon", a term applied to light-colored zircons. The English word "zircon" is derived from "Zirkon," which is the German adaptation of this word. Red zircon is called ""hyacinth"", from the flower "hyacinthus", whose name is of Ancient Greek origin.
Properties.
Zircon is ubiquitous in the crust of Earth. It occurs as a common accessory mineral in igneous rocks (as primary crystallization products), in metamorphic rocks and as detrital grains in sedimentary rocks. Large zircon crystals are rare. Their average size in granite rocks is about 0.1–0.3 mm, but they can also grow to sizes of several centimeters, especially in mafic pegmatites and carbonatites. Zircon is also very resistant to heat and corrosion.
Because of their uranium and thorium content, some zircons undergo metamictization. Connected to internal radiation damage, these processes partially disrupt the crystal structure and partly explain the highly variable properties of zircon. As zircon becomes more and more modified by internal radiation damage, the density decreases, the crystal structure is compromised, and the color changes.
Zircon occurs in many colors, including reddish brown, yellow, green, blue, gray and colorless. The color of zircons can sometimes be changed by heat treatment. Common brown zircons can be transformed into colorless and blue zircons by heating to 800 to 1000 °C. In geological settings, the development of pink, red, and purple zircon occurs after hundreds of millions of years, if the crystal has sufficient trace elements to produce color centers. Color in this red or pink series is annealed in geological conditions above the temperature about 350 °C.
Applications.
Zircon is mainly consumed as an opacifier, and has been known to be used in the decorative ceramics industry. It is also the principal precursor not only to metallic zirconium, although this application is small, but also to all compounds of zirconium including zirconium dioxide (ZrO2), one of the most refractory materials known.
Occurrence.
Zircon is a common accessory to trace mineral constituent of most granite and felsic igneous rocks. Due to its hardness, durability and chemical inertness, zircon persists in sedimentary deposits and is a common constituent of most sands. Zircon is rare within mafic rocks and very rare within ultramafic rocks aside from a group of ultrapotassic intrusive rocks such as kimberlites, carbonatites, and lamprophyre, where zircon can occasionally be found as a trace mineral owing to the unusual magma genesis of these rocks.
Zircon forms economic concentrations within heavy mineral sands ore deposits, within certain pegmatites, and within some rare alkaline volcanic rocks, for example the Toongi Trachyte, Dubbo, New South Wales Australia in association with the zirconium-hafnium minerals eudialyte and armstrongite.
Australia leads the world in zircon mining, producing 37% of the world total and accounting for 40% of world EDR (economic demonstrated resources) for the mineral.
Radiometric dating.
Zircon has played an important role during the evolution of radiometric dating. Zircons contain trace amounts of uranium and thorium (from 10 ppm up to 1 wt%) and can be dated using several modern analytical techniques. Because zircons can survive geologic processes like erosion, transport, even high-grade metamorphism, they contain a rich and varied record of geological processes. Currently, zircons are typically dated by uranium-lead (U-Pb), fission-track, and U+Th/He techniques.
Zircons from Jack Hills in the Narryer Gneiss Terrane, Yilgarn Craton, Western Australia, have yielded U-Pb ages up to 4.404 billion years, interpreted to be the age of crystallization, making them the oldest minerals so far dated on Earth. In addition, the oxygen isotopic compositions of some of these zircons have been interpreted to indicate that more than 4.4 billion years ago there was already water on the surface of the Earth. This interpretation is supported by additional trace element data, but is also the subject of debate. In 2015, "remains of biotic life" were found in 4.1 billion-year-old rocks in the Jack Hills of Western Australia. According to one of the researchers, "If life arose relatively quickly on Earth ... then it could be common in the universe."
Similar minerals.
Hafnon (HfSiO4), xenotime (YPO4), béhierite, schiavinatoite ((Ta,Nb)BO4), thorite (ThSiO4), and coffinite (USiO4) all share the same crystal structure (VIIIX IVY O4) as zircon.

</doc>
<doc id="50585" url="https://en.wikipedia.org/wiki?curid=50585" title="Philadelphia">
Philadelphia

Philadelphia () is the largest city in the Commonwealth of Pennsylvania and the fifth-most populous in the United States, with an estimated population in 2014 of 1,560,297. In the Northeastern United States, at the confluence of the Delaware and Schuylkill rivers, Philadelphia is the economic and cultural anchor of the Delaware Valley, a metropolitan area home to 7.2 million people and the eighth-largest combined statistical area in the United States.
In 1682, William Penn founded the city to serve as capital of the Pennsylvania Colony. Philadelphia played an instrumental role in the American Revolution as a meeting place for the Founding Fathers of the United States, who signed the Declaration of Independence in 1776 and the Constitution in 1787. Philadelphia was one of the nation's capitals in the Revolutionary War, and served as temporary U.S. capital while Washington, D.C., was under construction. In the 19th century, Philadelphia became a major industrial center and railroad hub that grew from an influx of European immigrants. It became a prime destination for African-Americans in the Great Migration and surpassed two million occupants by 1950.
Based on the similar shifts underway the nation's economy after 1960, Philadelphia experienced a loss of manufacturing companies and jobs to lower taxed regions of the USA and often overseas. As a result, the economic base of Philadelphia, which had historically been manufacturing, declined significantly. In addition, consolidation in several American industries (retailing, financial services and health care in particular) reduced the number of companies headquartered in Philadelphia. The economic impact of these changes would reduce Philadelphia's tax base and the resources of local government. Philadelphia struggled through a long period of adjustment to these economic changes, coupled with significant demographic change as wealthier residents moved into the nearby suburbs and more immigrants moved into the city. The city in fact approached bankruptcy in the late 1980s. Revitalization began in the 1990s, with gentrification turning around many neighborhoods and reversing its decades-long trend of population loss.
The area's many universities and colleges make Philadelphia a top international study destination, as the city has evolved into an educational and economic hub. With a gross domestic product of $388 billion, Philadelphia ranks ninth among world cities and fourth in the nation. Philadelphia is the center of economic activity in Pennsylvania and is home to seven Fortune 1000 companies. The Philadelphia skyline is growing, with several nationally prominent skyscrapers. The city is known for its arts, culture, and history, attracting over 39 million domestic tourists in 2013. Philadelphia has more outdoor sculptures and murals than any other American city, and Fairmount Park is the largest landscaped urban park in the world. The 67 National Historic Landmarks in the city helped account for the $10 billion generated by tourism. Philadelphia is the birthplace of the United States Marine Corps, and is also the home of many U.S. firsts, including the first library (1731), first hospital (1751) and medical school (1765), first Capitol (1777), first stock exchange (1790), first zoo (1874), and first business school (1881). Philadelphia is the only World Heritage City in the United States.
History.
Before Europeans arrived, the Philadelphia area was home to the Lenape (Delaware) Indians in the village of Shackamaxon. The Lenape are a Native American tribe and First Nations band government. They are also called Delaware Indians and their historical territory was along the Delaware River watershed, western Long Island and the Lower Hudson Valley. Most Lenape were pushed out of their Delaware homeland during the 18th century by expanding European colonies, exacerbated by losses from intertribal conflicts. Lenape communities were weakened by newly introduced diseases, mainly smallpox, and violent conflict with Europeans. Iroquois people occasionally fought the Lenape. Surviving Lenape moved west into the upper Ohio River basin. The American Revolutionary War and United States' independence pushed them further west. In the 1860s, the United States government sent most Lenape remaining in the eastern United States to the Indian Territory (present-day Oklahoma and surrounding territory) under the Indian removal policy. In the 21st century, most Lenape now reside in the US state of Oklahoma, with some communities living also in Wisconsin, Ontario (Canada) and in their traditional homelands.
Europeans came to the Delaware Valley in the early 17th century, with the first settlements founded by the Dutch, who in 1623 built Fort Nassau on the Delaware River opposite the Schuylkill River in what is now Brooklawn, New Jersey. The Dutch considered the entire Delaware River valley to be part of their New Netherland colony. In 1638, Swedish settlers led by renegade Dutch established the colony of New Sweden at Fort Christina (present day Wilmington, Delaware) and quickly spread out in the valley. In 1644, New Sweden supported the Susquehannocks in their military defeat of the English colony of Maryland. In 1648, the Dutch built Fort Beversreede on the west bank of the Delaware, south of the Schuylkill near the present-day Eastwick section of Philadelphia, to reassert their dominion over the area. The Swedes responded by building Fort Nya Korsholm, named New Korsholm after a town that is now in Finland. In 1655, a Dutch military campaign led by New Netherland Director-General Peter Stuyvesant took control of the Swedish colony, ending its claim to independence, although the Swedish and Finnish settlers continued to have their own militia, religion, and court, and to enjoy substantial autonomy under the Dutch. The English conquered the New Netherland colony in 1664, but the situation did not really change until 1682, when the area was included in William Penn's charter for Pennsylvania.
In 1681, in partial repayment of a debt, Charles II of England granted William Penn a charter for what would become the Pennsylvania colony. Despite the royal charter, Penn bought the land from the local Lenape to be on good terms with the Native Americans and ensure peace for his colony. Penn made a treaty of friendship with Lenape chief Tammany under an elm tree at Shackamaxon, in what is now the city's Fishtown section. Penn named the city Philadelphia, which is Greek for brotherly love (from "philos", "love" or "friendship", and "adelphos", "brother"). As a Quaker, Penn had experienced religious persecution and wanted his colony to be a place where anyone could worship freely. This tolerance, far more than afforded by most other colonies, led to better relations with the local Native tribes and fostered Philadelphia's rapid growth into America's most important city.
Penn planned a city on the Delaware River to serve as a port and place for government. Hoping that Philadelphia would become more like an English rural town instead of a city, Penn laid out roads on a grid plan to keep houses and businesses spread far apart, with areas for gardens and orchards. The city's inhabitants did not follow Penn's plans, as they crowded by the Delaware River, the port, and subdivided and resold their lots. Before Penn left Philadelphia for the last time, he issued the Charter of 1701 establishing it as a city. It became an important trading center, poor at first, but with tolerable living conditions by the 1750s. Benjamin Franklin, a leading citizen, helped improve city services and founded new ones, such as fire protection, a library, and one of the American colonies' first hospitals.
A number of important philosophical societies were formed, which were centers of the city's intellectual life: the Philadelphia Society for Promoting Agriculture (1785), the Pennsylvania Society for the Encouragement of Manufactures and the Useful Arts (1787), the Academy of Natural Sciences (1812), and the Franklin Institute (1824). These worked to develop and finance new industries and attract skilled and knowledgeable immigrants from Europe.
Philadelphia's importance and central location in the colonies made it a natural center for America's revolutionaries. By the 1750s, Philadelphia had surpassed Boston to become the largest city and busiest port in British America, and second in the British Empire, behind London. The city hosted the First Continental Congress before the American Revolutionary War; the Second Continental Congress, which signed the United States Declaration of Independence, during the war; and the Constitutional Convention (1787) after the war. Several battles were fought in and near Philadelphia as well.
Philadelphia served as the temporary capital of the United States, 1790–1800, while the Federal City was under construction in the District of Columbia. In 1793, the largest yellow fever epidemics in U.S. history killed at least 4,000 and up to 5,000 people in Philadelphia, roughly 10% of the city's population.
The state government left Philadelphia in 1799, and the federal government was moved to Washington, DC in 1800 with completion of the White House and Capitol. The city remained the young nation's largest with a population of nearly 50,000 at the turn of the 19th century; it was a financial and cultural center. Before 1800, its free black community founded the African Methodist Episcopal Church (AME), the first independent black denomination in the country, and the first black Episcopal Church. The free black community also established many schools for its children, with the help of Quakers. New York City soon surpassed Philadelphia in population, but with the construction of roads, canals, and railroads, Philadelphia became the first major industrial city in the United States.
Throughout the 19th century, Philadelphia had a variety of industries and businesses, the largest being textiles. Major corporations in the 19th and early 20th centuries included the Baldwin Locomotive Works, William Cramp and Sons Ship and Engine Building Company, and the Pennsylvania Railroad. Industry, along with the U.S. Centennial, was celebrated in 1876 with the Centennial Exposition, the first official World's Fair in the United States. Immigrants, mostly Irish and German, settled in Philadelphia and the surrounding districts. The rise in population of the surrounding districts helped lead to the Act of Consolidation of 1854, which extended the city limits of Philadelphia from the 2 square miles of present-day Center City to the roughly 130 square miles of Philadelphia County. 
These immigrants were largely responsible for the first general strike in North America in 1835, in which workers in the city won the ten-hour workday. The city was a destination for thousands of Irish immigrants fleeing the Great Famine in the 1840s; housing for them was developed south of South Street, and was later occupied by succeeding immigrants. They established a network of Catholic churches and schools, and dominated the Catholic clergy for decades. Anti-Irish, anti-Catholic Nativist riots had erupted in Philadelphia in 1844. In the latter half of the century, immigrants from Russia, Eastern Europe and Italy; and African Americans from the southern U.S. settled in the city. Between 1880 and 1930, the African-American population of Philadelphia increased from 31,699 to 219,559. Twentieth-century black newcomers were part of the Great Migration out of the rural South to northern and midwestern industrial cities.
In the American Civil War, Philadelphia was represented by the Washington Grays (Philadelphia).
By the 20th century, Philadelphia had become known as "corrupt and contented", with a complacent population and an entrenched Republican political machine. The first major reform came in 1917 when outrage over the election-year murder of a police officer led to the shrinking of the Philadelphia City Council from two houses to just one. In July 1919, Philadelphia was one of more than 36 industrial cities nationally to suffer a race riot of ethnic whites against blacks during Red Summer, in post-World War I unrest, as recent immigrants competed with blacks for jobs. In the 1920s, the public flouting of Prohibition laws, mob violence, and police involvement in illegal activities led to the appointment of Brigadier General Smedley Butler of the U.S. Marine Corps as director of public safety, but political pressure prevented any long-term success in fighting crime and corruption.
In 1940, non-Hispanic whites constituted 86.8% of the city's population. The population peaked at more than two million residents in 1950, then began to decline with the restructuring of industry, which led to the loss of many middle-class union jobs. In addition, suburbanization had been drawing off many of the wealthier residents to outlying railroad commuting towns and newer housing. Revitalization and gentrification of neighborhoods began in the late 1970s and continues into the 21st century, with much of the development in the Center City and University City areas of the city. After many of the old manufacturers and businesses left Philadelphia or shut down, the city started attracting service businesses and began to more aggressively market itself as a tourist destination. Glass-and-granite skyscrapers were built in Center City. Historic areas such as Independence National Historical Park located in Old City and Society Hill were renovated during the reformist mayoral era of the 1950s through the 1980s. They are now among the most desirable living areas of Center City. This has slowed the city's 40-year population decline after it lost nearly one-quarter of its population.
Geography.
Topography.
Philadelphia is at 39° 57′ north latitude and 75° 10′ west longitude, and the 40th parallel north passes through the northern parts of the city. The city encompasses , of which is land and , or 5.29%, is water. Bodies of water include the Delaware and Schuylkill rivers, and Cobbs, Wissahickon, and Pennypack creeks.
The lowest point is above sea level, while the highest point is in Chestnut Hill, about above sea level (near the intersection of Germantown Avenue and Bethlehem Pike).
Philadelphia sits on the Fall Line that separates the Atlantic Coastal Plain from the Piedmont. The rapids on the Schuylkill River at East Falls were inundated by the completion of the Fairmount Dam.
The city is the seat of its own county. The adjacent counties are Montgomery to the north; Bucks to the northeast; Burlington County, New Jersey, to the east; Camden County, New Jersey, to the southeast; Gloucester County, New Jersey, to the south; and Delaware County to the west.
Cityscape.
City planning.
Philadelphia's central city was created in the 17th century following the plan by William Penn's surveyor Thomas Holme. Center City is structured with long straight streets running east-west and north-south forming a grid pattern. The original city plan was designed to allow for easy travel and to keep residences separated by open space that would help prevent the spread of fire. The Delaware River and Schuylkill Rivers served as early boundaries between which the city's early street plan was kept within. In addition, Penn planned the creation of five public parks in the city which were renamed in 1824 (in parenthesis): Centre Square, North East Publick Square (Franklin Square), Northwest Square (Logan Square), Southwest Square (Rittenhouse Square), and Southeast Square (Washington Square). Center City has grown into the second-most populated downtown area in the United States, after Midtown Manhattan in New York City, with an estimated 183,240 residents in 2015.
Philadelphia's neighborhoods are divided into large sections—North, Northeast, Northwest, West, South and Southwest Philadelphia—all of which surround Center City, which corresponds closely with the city's limits before consolidation in 1854. Each of these large areas contains numerous neighborhoods, some of whose boundaries derive from the boroughs, townships, and other communities that made up Philadelphia County before their absorption into the city.
The City Planning Commission, tasked with guiding growth and development of the city, has divided the city into 18 planning districts as part of the "Philadelphia2035" physical development plan. Much of the city's 1980 zoning code was overhauled from 2007–2012 as part of a joint effort between former mayors John F. Street and Michael Nutter. The zoning changes were intended to rectify incorrect zoning mapping that would streamline future community preferences and development, which the city forecasts an additional 100,000 residents and 40,000 jobs to be added to Philadelphia in 2035.
The Philadelphia Housing Authority is the largest landlord in Pennsylvania. Established in 1937, it is the nation's fourth-largest housing authority, housing about 84,000 people and employing 1,250. In 2013, its budget was $371 million. The Philadelphia Parking Authority works to ensure adequate parking for city residents, businesses and visitors.
Architecture.
Philadelphia's architectural history dates back to Colonial times and includes a wide range of styles. The earliest structures were of logs construction, but brick structures were common by 1700. During the 18th century, the cityscape was dominated by Georgian architecture, including Independence Hall and Christ Church.
In the first decades of the 19th century, Federal architecture and Greek Revival architecture were dominated by Philadelphia architects such as Benjamin Latrobe, William Strickland, John Haviland, John Notman, Thomas U. Walter, and Samuel Sloan. Frank Furness is considered Philadelphia's greatest architect of the second half of the 19th century, but his contemporaries included John McArthur, Jr., Addison Hutton, Wilson Eyre, the Wilson Brothers, and Horace Trumbauer. In 1871, construction began on the Second Empire-style Philadelphia City Hall. The Philadelphia Historical Commission was created in 1955 to preserve the cultural and architectural history of the city. The commission maintains the Philadelphia Register of Historic Places, adding historic buildings, structures, sites, objects and districts as it sees fit.
In 1932, Philadelphia became home to the first International Style skyscraper in the United States, The PSFS Building, designed by George Howe and William Lescaze. It is the United States' first modern skyscraper and considered the most important one built in the first part of the 20th century.
The City Hall remained the tallest building in the city until 1987 when One Liberty Place was constructed. Numerous glass and granite skyscrapers were built in Philadelphia's Center City from the late 1980s onwards. In 2007, the Comcast Center surpassed One Liberty Place to become the city's tallest building. The Comcast Innovation and Technology Center is under construction in Center City and is planned to reach a height of 1,121 feet (342 meters); upon completion, the tower is expected to be the tallest skyscraper in the United States outside of New York City and Chicago.
For much of Philadelphia's history, the typical home has been the row house. The row house was introduced to the United States via Philadelphia in the early 19th century and, for a time, row houses built elsewhere in the United States were known as "Philadelphia rows". A variety of row houses are found throughout the city, from Victorian-style homes in North Philadelphia to twin row houses in West Philadelphia. While newer homes are scattered throughout the city, much of the housing is from the early 20th century or older. The great age of the homes has created numerous problems, including blight and vacant lots in many parts of the city, while other neighborhoods such as Society Hill, which has the largest concentration of 18th-century architecture in the United States, have been rehabilitated and gentrified.
Climate.
Under the Köppen climate classification, Philadelphia falls in the northern periphery of the humid subtropical climate zone (Köppen "Cfa"). Summers are typically hot and muggy, fall and spring are generally mild, and winter is cold. Snowfall is highly variable, with some winters bringing only light snow and others bringing several major snowstorms, with the normal seasonal snowfall standing at ; snow in November or April is rare, and a sustained snow cover is rare. Precipitation is generally spread throughout the year, with eight to twelve wet days per month, at an average annual rate of , but historically ranging from in 1922 to in 2011. The most rain recorded in one day occurred on July 28, 2013, when fell at Philadelphia International Airport.
The January daily average is , though, in a normal winter, the temperature frequently rises to during thaws and dips to for 2 or 3 nights. July averages , although heat waves accompanied by high humidity and heat indices are frequent; highs reach or exceed on 27 days of the year. The average window for freezing temperatures is November 6 thru April 2, allowing a growing season of 217 days. Early fall and late winter are generally dry; February's average of makes it the area's driest month. The dewpoint in the summer averages between to .
Seasonal snowfall accumulation has ranged from trace amounts in 1972–73 to in the winter of 2009–10. The city's heaviest single-storm snowfall, at , occurred in January 1996.
The highest recorded temperature was on August 7, 1918, but + temperatures are uncommon. The lowest officially recorded temperature was on February 9, 1934, but with the last such occurrence being January 19, 1994, temperatures at or below the mark are rare. The record low maximum is on February 10, 1899 and December 30, 1880, while the record high minimum is on July 23, 2011 and July 24, 2010.
In the American Lung Association 2015 State of the Air report, Philadelphia County received an ozone grade of F and a 24-hour particle pollution rating of C. The county passed the annual particle pollution rating.
Demographics.
According to the 2014 United States Census estimates, there were 1,560,297 people residing in the City of Philadelphia, representing a 2.2% increase since 2010. From the 1960s up until 2006, the city's population declined year after year. It eventually reached a low of 1,488,710 residents in 2006 before beginning to rise again. Since 2006, Philadelphia added 71,587 residents in eight years. A study done by the city projected that the population would increase to about 1,630,000 residents by 2035, an increase of about 100,000 from 2010.
The racial makeup of the city in 2014 was 45.3% White (35.8% Non-Hispanic), 44.1% Black or African American, 0.8% Native American and Alaska Native, 7.2% Asian, 0.1% Native Hawaiian and Other Pacific Islander, 2.5% Two or More Races, and 13.6% were Hispanic or Latino.
In comparison, the 2010 Census Redistricting Data indicated that the racial makeup of the city was 661,839 (43.4%) African American, 626,221 (41.0%) White, 6,996 (0.5%) Native American, 96,405 (6.3%) Asian (2.0% Chinese, 1.2% Indian, 0.9% Vietnamese, 0.6% Cambodian, 0.4% Korean, 0.3% Filipino, 0.2% Pakistani, 0.1% Indonesian), 744 (0.0%) Pacific Islander, 90,731 (5.9%) from other races, and 43,070 (2.8%) from two or more races. Hispanic or Latino of any race were 187,611 persons (12.3%); 8.0% of Philadelphia is Puerto Rican, 1.0% Dominican, 1.0% Mexican, 0.3% Cuban, and 0.3% Colombian. The racial breakdown of Philadelphia's Hispanic/Latino population was 63,636 (33.9%) White, 17,552 (9.4%) African American, 3,498 (1.9%) Native American, 884 (0.47%) Asian, 287 (0.15%) Pacific Islander, 86,626 (46.2%) from other races, and 15,128 (8.1%) from two or more races.
According to a 2014 study by the Pew Research Center, 68% of the population of the city identified themselves as Christians, with 41% professing attendance at a variety of churches that could be considered Protestant, and 26% professing Roman Catholic beliefs. while 24% claim no religious affiliation. The same study says that other religions (including Judaism, Buddhism, Islam, and Hinduism) collectively make up about 8% of the population
The average population density was 11,457 people per square mile (4,405.4/km²). The Census reported that 1,468,623 people (96.2% of the population) lived in households, 38,007 (2.5%) lived in non-institutionalized group quarters, and 19,376 (1.3%) were institutionalized.
The Black American population in Philadelphia is the third-largest in the country, after New York City and Chicago. Historically, West Philadelphia and North Philadelphia were largely black neighborhoods, but many are leaving these areas in favor of the Northeast and Southwest sections of Philadelphia. There is a higher proportion of Muslims in the Black American population than most cities in America. West Philadelphia also has significant Caribbean and African immigrant populations.
The Puerto Rican population in Philadelphia is the second-largest after New York City, and the second fastest-growing after Orlando. There are large Puerto Rican and Dominican populations in North Philadelphia and the Northeast, as well as a significant Mexican population in South Philadelphia.
Philadelphia has significant Asian populations mainly hailing from countries like India, China, Vietnam, and South Korea. Chinatown and the Northeast have the largest Asian presences, with a large Korean community in Olney, Philadelphia. South Philadelphia is also home to large Cambodian, Vietnamese, and Chinese communities. It has the fifth largest Muslim population among American cities.
Languages.
, 79.12% (1,112,441) of Philadelphia residents age 5 and older spoke English at home as a primary language, while 9.72% (136,688) spoke Spanish, 1.64% (23,075) Chinese, 0.89% (12,499) Vietnamese, 0.77% (10,885) Russian, 0.66% (9,240) French, 0.61% (8,639) other Asian languages, 0.58% (8,217) African languages, 0.56% (7,933) Cambodian (Mon-Khmer), and Italian was spoken as a main language by 0.55% (7,773) of the population over the age of five. In total, 20.88% (293,544) of Philadelphia's population age 5 and older spoke a mother language other than English.
Economy.
Philadelphia is the center of economic activity in Pennsylvania with the headquarters of seven Fortune 1000 companies located within city limits. According to the Bureau of Economic Analysis, the Philadelphia area had a total gross metropolitan product of $347 billion in 2010, the seventh-largest metropolitan economy in the United States. Philadelphia was rated by the GaWC5 as an 'Alpha- City' in its categorization of world cities.
Philadelphia's economic sectors include information technology, manufacturing, oil refining, food processing, health care, biotechnology, tourism, and financial services. Financial activities account for the largest sector of the metropolitan area's economy, and it is one of the largest health education and research centers in the United States.
The city is home to the Philadelphia Stock Exchange and some of the area's largest companies including cable television and internet provider Comcast, insurance companies Colonial Penn, CIGNA, Independence Blue Cross, energy company Sunoco, food services company Aramark and Crown, chemical makers Rohm and Haas and FMC, pharmaceutical company GlaxoSmithKline, Boeing Rotorcraft Systems, and automotive parts retailer Pep Boys.
Philadelphia's an annualized unemployment rate was 7.8% in 2014, down from 10.0%the previous year. This is higher than the national average of 6.2%. Similarly, the rate of new jobs added to the city's economy lagged behind the national job growth. In 2014, about 8,800 jobs were added to the city's economy. Sectors with the largest number of jobs added were in education and health services, leisure and hospitality, and professional and business services. Declines were seen in the city's manufacturing and government sectors.
While about 31.9% of the city's population is not in the labor force, the city's largest employers are the federal and city governments, respectively. Philadelphia's largest private employer is the University of Pennsylvania followed by the Children's Hospital of Philadelphia. A study commissioned by the city's government projected 40,000 jobs to be added to the city by 2035, raising the city's 2010 number of jobs from 675,000 total to an estimated 715,000 jobs.
Philadelphia's history attracts many tourists, with the Independence National Historical Park (which includes the Liberty Bell, Independence Hall, and other historical sites) receiving over 3.6 million visitors in 2014. The Greater Philadelphia region was visited by 39 million people in 2013 generating $10 billion in economic impact.
Culture.
Philadelphia is home to many national historical sites that relate to the founding of the United States. Independence National Historical Park is the center of these historical landmarks being one of the country's 22 UNESCO World Heritage Sites. Independence Hall, where the Declaration of Independence was signed, and the Liberty Bell are the city's most famous attractions. Other historic sites include homes for Edgar Allan Poe, Betsy Ross, and Thaddeus Kosciuszko, early government buildings like the First and Second Banks of the United States, Fort Mifflin, and the Gloria Dei (Old Swedes') Church. Philadelphia alone has 67 National Historic Landmarks, the third most of any city in the country. 
Philadelphia's major science museums include the Franklin Institute, which contains the Benjamin Franklin National Memorial; the Academy of Natural Sciences; the Mütter Museum; and the University of Pennsylvania Museum of Archaeology and Anthropology. History museums include the National Constitution Center, the Atwater Kent Museum of Philadelphia History, the National Museum of American Jewish History, the African American Museum in Philadelphia, the Historical Society of Pennsylvania, the Grand Lodge of Free and Accepted Masons in the state of Pennsylvania and The Masonic Library and Museum of Pennsylvania and Eastern State Penitentiary. Philadelphia is home to the United States' first zoo and hospital, as well as Fairmount Park, one of America's oldest and largest urban parks.
The city is home to important archival repositories, including the Library Company of Philadelphia, established in 1731, and the Athenaeum of Philadelphia, founded in 1814. The Presbyterian Historical Society, the country's oldest continuous denominational historical society, is also located there.
Accent.
The Philadelphia dialect, which is spread throughout the Delaware Valley and South Jersey, is part of Mid-Atlantic American English, and as such it is identical in many ways to the Baltimore dialect. Unlike the Baltimore dialect, however, the Philadelphia accent also shares many similarities with the New York accent. Thanks to over a century of linguistics data collected by researchers at the University of Pennsylvania, the Philadelphia dialect under sociolinguist William Labov has been one of the best-studied forms of American English.
Arts.
The city contains many art museums, such as the Pennsylvania Academy of the Fine Arts and the Rodin Museum, which holds the largest collection of work by Auguste Rodin outside France. The city's major art museum, the Philadelphia Museum of Art, is one of the largest art museums in the United States. Its long flight of steps to the main entrance became famous after the film "Rocky" (1976).
The city is home to the Philadelphia Sketch Club, one of the country's oldest artists' clubs, and The Plastic Club, started by women excluded from the Sketch Club. It has a profusion of art galleries, many of which participate in the First Friday event. The first Friday of every month, galleries in Old City are open late. Annual events include film festivals and parades, the most famous being the New Year's Day Mummers Parade.
Areas such as South Street and Old City have a vibrant night life. The Avenue of the Arts in Center City contains many restaurants and theaters, such as the Kimmel Center for the Performing Arts, which is home to the Philadelphia Orchestra, generally considered one of the top five orchestras in the United States, and the Academy of Music, the nation's oldest continually operating opera house, home to the Opera Company of Philadelphia and the Pennsylvania Ballet. The Wilma Theatre and Philadelphia Theatre Company have new buildings constructed in the last decade on the avenue. They produce a variety of new works. Several blocks to the east are the Walnut Street Theatre, America's oldest theatre and the largest subscription theater in the world; as well as the Lantern Theatre at St. Stephens Church, one of a number of smaller venues.
Philadelphia has more public art than any other American city. In 1872, the Association for Public Art (formerly the Fairmount Park Art Association) was created, the first private association in the United States dedicated to integrating public art and urban planning. In 1959, lobbying by the Artists Equity Association helped create the Percent for Art ordinance, the first for a U.S. city. The program, which has funded more than 200 pieces of public art, is administered by the Philadelphia Office of Arts and Culture, the city's art agency.
Philadelphia has more murals than any other U.S. city, thanks in part to the 1984 creation of the Department of Recreation's Mural Arts Program, which seeks to beautify neighborhoods and provide an outlet for graffiti artists. The program has funded more than 2,800 murals by professional, staff and volunteer artists and educated more than 20,000 youth in underserved neighborhoods throughout Philadelphia.
Philadelphia artists have had a prominent national role in popular music. In the 1970s, Philadelphia soul influenced the music of that and later eras. On July 13, 1985, Philadelphia hosted the American end of the Live Aid concert at John F. Kennedy Stadium. The city reprised this role for the Live 8 concert, bringing some 700,000 people to the Ben Franklin Parkway on July 2, 2005. Philadelphia is home to the world-renowned Philadelphia Boys Choir & Chorale, which has performed its music all over the world. Dr. Robert G. Hamilton, founder of the choir, is a notable native Philadelphian. The Philly Pops is another famous Philadelphia music group. The city has played a major role in the development and support of American rock music and rap music. Hip-hop/Rap artists such as The Roots, DJ Jazzy Jeff & The Fresh Prince, The Goats, Freeway, Schoolly D, Eve, and Lisa "Left Eye" Lopes hail from the city.
Cuisine.
The city is known for its hoagies, scrapple, soft pretzels, water ice, Irish potato candy, Tastykake, and is home to the cheesesteak, developed by German and Italian immigrants. Philadelphia boasts a number of cheesesteak establishments, however two locations in South Philadelphia are perhaps the most famous among tourists: Pat's King of Steaks and its across the street rival Geno's Steaks.
Its high-end restaurants include Morimoto, "Iron Chef" Masaharu Morimoto's first restaurant, Vetri, famous on the East Coast for its take on Northern Italian cuisine, and Lacroix, a staple restaurant situated in Rittenhouse Square. Italian specialties have been supplemented by many new Vietnamese and other Asian restaurants, both budget and high-end.
McGillin's Olde Ale House, located on Drury Street in Center City, is the oldest continuously operated tavern in the city.
Philadelphia is also home to a landmark eatery founded in 1892, the Reading Terminal Market. The enclosed public market hosts over a hundred merchants offering Pennsylvania Dutch specialties, artisan cheese and meat, locally grown groceries, and specialty and ethnic foods.
Marijuana.
Philadelphia has decriminalized small amounts of marijuana in the city, reducing penalties for possession and public use to minor fines and community service. The move makes Philadelphia the largest city in the United States to decriminalize pot.
Sports.
Philadelphia's professional sports teams date at least to the 1860 founding of baseball's Athletics. The city is one of 12 U.S. cities to have all four major sports: the Philadelphia Phillies in the National League of Major League Baseball, the Philadelphia Eagles of the National Football League, the Philadelphia Flyers of the National Hockey League, and the Philadelphia 76ers of the National Basketball Association.
The Philadelphia metro area is also home of the Philadelphia Union of Major League Soccer. The Union play their home games at Talen Energy Stadium, a soccer-specific stadium in Chester, Pennsylvania. Philadelphia began play in MLS in 2010, after beating several other cities in competition for the rights to an MLS expansion franchise.
The city's professional teams went without a championship from 1983, when the 76ers won the NBA Championship, until 2008, when the Phillies won the World Series. In 2004, ESPN ranked Philadelphia second on its list of The Fifteen Most Tortured Sports Cities. The failure was sometimes attributed in jest to the "Curse of Billy Penn." The sports fans of Philadelphia are known for being referred to as the "Meanest Fans in America".
Major-sport professional sports teams that originated in Philadelphia but ultimately moved to other cities include the Golden State Warriors basketball team and the Oakland Athletics baseball team.
Philadelphia is also the home city of the Philadelphia Spinners, a professional ultimate team that is part of the Major League Ultimate. They are one of the original eight teams of the American Ultimate Disc League that began in April 2012. They played at Franklin Field and won the inaugural AUDL championship. , the Spinners play in the newer MLU at various stadiums through the city and surrounding southern suburbs.
Rowing has been popular in Philadelphia since the 18th century. Boathouse Row is a symbol of Philadelphia's rich rowing history, and each Big Five member has its own boathouse. Philadelphia hosts numerous local and collegiate rowing clubs and competitions, including the annual Dad Vail Regatta, the largest intercollegiate rowing event in the U.S, the Stotesbury Cup Regatta, and the Head of the Schuylkill Regatta, all of which are held on the Schuylkill River. The regattas are hosted and organized by the Schuylkill Navy, an association of area rowing clubs that has produced numerous Olympic rowers.
Philadelphia is home to professional, semi-professional and elite amateur teams in cricket, rugby league (Philadelphia Fight), rugby union and other sports. Major sporting events in the city include the Penn Relays, Philadelphia Marathon, Broad Street Run, and the Philadelphia International Championship bicycle race. The Collegiate Rugby Championship is played every June at Talen Energy Stadium; the CRC is broadcast live on NBC and regularly draws attendances of 18,000.
Philadelphia is home to the Philadelphia Big 5, a group of five Division I college basketball programs. The Big 5 are Saint Joseph's University, University of Pennsylvania, La Salle University, Temple University, and Villanova University. The sixth NCAA Division I school in Philadelphia is Drexel University. At least one of the teams is competitive nearly every year and at least one team has made the NCAA tournament for the past four decades.
Parks.
The total parkland amounts to about . Philadelphia's largest park, Fairmount Park, encompasses of this parkland and includes 63 neighborhood and regional parks. The largest tract of Fairmount Park is on the west side of the city along the Schuylkill River and Wissahickon Creek and includes the Philadelphia Zoo.
The total expenditures of the park in 2005 were $164 million. Fairmount Park is the world's largest landscaped urban park.
Law and government.
From a governmental perspective, Philadelphia County is a legal nullity, as all county functions were assumed by the city in 1952, which has been coterminous with the county since 1854.
Philadelphia's 1952 Home Rule Charter was written by the City Charter Commission, which was created by the Pennsylvania General Assembly in an Act of April 21, 1949, and a city ordinance of June 15, 1949. The existing City Council received a proposed draft on February 14, 1951, and the electors approved it in an election held April 17, 1951. The first elections under the new Home Rule Charter were held in November 1951, and the newly elected officials took office in January 1952.
The city uses the strong-mayor version of the mayor-council form of government, which is headed by one mayor, in whom executive authority is vested. Elected at-large, the mayor is limited to two consecutive four-year terms under the city's home rule charter, but can run for the position again after an intervening term. The Mayor is Jim Kenney, who replaced Michael Nutter, who served two terms from 2009 to January 2016. Kenney, as all Philadelphia mayors have been since 1952, is a member of the Democratic Party, which tends to dominate local politics so thoroughly that the Democratic Mayoral primary is often more widely covered than the general election. The legislative branch, the Philadelphia City Council, consists of ten council members representing individual districts and seven members elected at large. Democrats currently hold 14 seats, with Republicans representing two allotted at-large seats for the minority party, as well as the Northeast-based Tenth District. The current council president is Darrell Clarke.
Courts.
The Philadelphia County Court of Common Pleas (First Judicial District) is the trial court of general jurisdiction for Philadelphia, hearing felony-level criminal cases and civil suits above the minimum jurisdictional limit of $7000 (excepting small claims cases valued between $7000 and $12000 and landlord-tenant issues heard in the Municipal Court) under its original jurisdiction; it also has appellate jurisdiction over rulings from the Municipal and Traffic Courts and over decisions of certain Pennsylvania state agencies (e.g. the Pennsylvania Liquor Control Board). It has 90 legally trained judges elected by the voters. It is funded and operated largely by city resources and employees. The current District Attorney is Seth Williams, a Democrat. The last Republican to hold the office is Ron Castille, who left in 1991 and is currently the Chief Justice of the Pennsylvania Supreme Court.
The Philadelphia Municipal Court handles matters of limited jurisdiction as well as landlord-tenant disputes, appeals from traffic court, preliminary hearings for felony-level offenses, and misdemeanor criminal trials. It has 25 legally trained judges elected by the voters.
Philadelphia Traffic Court is a court of special jurisdiction that hears violations of traffic laws. It has seven judges elected by the voters. As with magisterial district judges, the judges need not be lawyers, but must complete the certifying course and pass the qualifying examination administered by the Minor Judiciary Education Board.
Pennsylvania's three appellate courts also have sittings in Philadelphia. The Supreme Court of Pennsylvania, the court of last resort in the state, regularly hears arguments in Philadelphia City Hall. Also, the Superior Court of Pennsylvania and the Commonwealth Court of Pennsylvania sit in Philadelphia several times a year. Judges for these courts are elected at large. Each court has a prothonotary's office in Philadelphia as well.
Additionally, Philadelphia is home to the federal United States District Court for the Eastern District of Pennsylvania and the Court of Appeals for the Third Circuit, both of which are housed in the James A. Byrne United States Courthouse.
Politics.
As of December 31, 2009, there were 1,057,038 registered voters in Philadelphia. Registered voters constitute 68.3% of the total population.
From the American Civil War until the mid-20th century, Philadelphia was a bastion of the Republican Party, which arose from the staunch pro-Northern views of Philadelphia residents during and after the war (Philadelphia was chosen as the host city for the first Republican National Convention in 1856). After the Great Depression, Democratic registrations increased, but the city was not carried by Democrat Franklin D. Roosevelt in his landslide victory of 1932 (in which Pennsylvania was one of the few states won by Republican Herbert Hoover). Four years later, however, voter turnout surged and the city finally flipped to the Democrats. Roosevelt carried Philadelphia with over 60% of the vote in 1936. The city has remained loyally Democratic in every presidential election since. It is now one of the most Democratic in the country; in 2008, Democrat Barack Obama drew 83% of the city's vote. Obama's win was even greater in 2012, capturing 85% of the vote.
Philadelphia once comprised six congressional districts. However, as a result of the city's declining population, it now has only four: the 1st district, represented by Bob Brady; the 2nd, represented by Chaka Fattah; the 8th, represented by Mike Fitzpatrick; and the 13th, represented by Brendan Boyle. All but Fitzpatrick are Democrats. Although they are usually swamped by Democrats in city, state and national elections, Republicans still have some support in the area, primarily in the northeast. A Republican represented a significant portion of Philadelphia in the House as late as 1983, and Sam Katz ran competitive mayoral races as the Republican nominee in both 1999 and 2003.
Pennsylvania's longest-serving Senator, Arlen Specter, was from Philadelphia; he served as a Republican from 1981 and as a Democrat from 2009, losing that party's primary in 2010 and leaving office in January 2011. He was also the city's District Attorney from 1966 to 1974.
Philadelphia has hosted various national conventions, including in 1848 (Whig), 1856 (Republican), 1872 (Republican), 1900 (Republican), 1936 (Democratic), 1940 (Republican), 1948 (Republican), 1948 (Progressive), and 2000 (Republican). Philadelphia will host the 2016 Democratic National Convention. Philadelphia has been home to one Vice President, George M. Dallas, and one Civil War general who won his party's nomination for president but lost in the general election: George B. McClellan.
Crime.
Like many American cities, Philadelphia saw a gradual yet pronounced rise in crime in the years following World War II. There were 525 murders in 1990, a rate of 31.5 per 100,000. There were an average of about 600 murders a year for most of the 1990s. The murder count dropped in 2002 to 288, then rose four years later to 406 in 2006 and 392 in 2007. A few years later, Philadelphia began to see a rapid drop in homicides and violent crime. In 2013, there were 246 murders, which is a decrease of over 25% from the previous year, and a decrease of over 44% since 2007. And in 2014, there were 248 homicides, up by one since 2013.
In 2006, Philadelphia's homicide rate of 27.7 per 100,000 people was the highest of the country's 10 most populous cities. In 2012, Philadelphia had the fourth-highest homicide rate among the country's most populous cities. And in 2014, the rate dropped to 16.0 homicides per 100,000 residents placing Philadelphia as the sixth-highest city in the country.
In 2004, there were 7,513.5 crimes per 200,000 people in Philadelphia. Among its neighboring Mid-Atlantic cities in the same population group, Baltimore and Washington, D.C. were ranked second- and third-most dangerous cities in the United States, respectively. Camden, New Jersey, a city across the Delaware River from Philadelphia, was ranked as the most dangerous city in the United States.
The number of shootings in the city has declined significantly in the last 10 years. Shooting incidents peaked in 2006 when 1,857 shootings were recorded. That number has dropped 44 percent to 1,047 shootings in 2014. Similarly, major crimes in the city has decreased gradually in the last ten years since its peak in 2006 when 85,498 major crimes were reported. In the past three years, the number of reported major crimes fell 11 percent to a total of 68,815. Violent crimes, which include homicide, rape, aggravated assault, and robbery, decreased 14 percent in the past three years with a reported 15,771 occurrences in 2014. Based on the rate of violent crimes per 1,000 residents in American cities with 25,000 people or more, Philadelphia was ranked as the 54th most dangerous city in 2015.
Education.
Primary and secondary education.
Education in Philadelphia is provided by many private and public institutions. The School District of Philadelphia runs the city's public schools. The Philadelphia School District is the eighth largest school district in the United States with 142,266 students in 218 public schools and 86 charter schools .
The city's K-12 enrollment in district run schools has dropped in the last five years from 156,211 students in 2010 to 130,104 students in 2015. During the same time period, the enrollment in charter schools has increased from 33,995 students in 2010 to 62,358 students in 2015. This consistent drop in enrollment has led the city to close 24 of its public schools in 2013. During the 2014 school year, the city spent an average of $12,570 per pupil, below the average among comparable urban school districts.
Graduation rates among district-run schools, meanwhile, have steadily increased in the last ten years. In 2005, Philadelphia had a district graduation rate of 52%. This number has increased to 65% in 2014, still below the national and state averages. Scores on the state's standardized test, the Pennsylvania System of School Assessment (PSSA) have trended upward from 2005 to 2011 but have decreased since. In 2005, the district-run schools scored an average of 37.4% on math and 35.5% on reading. The city's schools reached its peak scores in 2011 with 59.0% on math and 52.3% on reading. In 2014, the scores dropped significantly to 45.2% on math and 42.0% on reading.
Of the city's public high schools, including charter schools, only four performed above the national average on the SAT (1497) in 2014: Masterman, Central, Girard, and MaST Community Charter School. All other district-run schools were below average.
Higher education.
Philadelphia has the third-largest student concentration on the East Coast, with over 120,000 college and university students enrolled within the city and nearly 300,000 in the metropolitan area. There are over 80 colleges, universities, trade, and specialty schools in the Philadelphia region. One of the founding members of the Association of American Universities is in city, the University of Pennsylvania, an Ivy League institution with claims to being the oldest university in the country.
The city's largest private school by number of students is Temple University, followed by Drexel University. Along with the University of Pennsylvania, Temple University and Drexel University make up the city's major research universities. The city is also home to five schools of medicine: Drexel University College of Medicine, Perelman School of Medicine at the University of Pennsylvania, Philadelphia College of Osteopathic Medicine, Temple University School of Medicine, and the Thomas Jefferson University. Hospitals, universities, and higher education research institutions in Philadelphia's four congressional districts received more than $252 million in National Institutes of Health grants in 2015.
Other institutions of higher learning within the city's borders include:
Media.
Newspapers.
Philadelphia's two major daily newspapers are "The Philadelphia Inquirer", which is the eighteenth largest newspaper and third-oldest surviving daily newspaper in the country, and the "Philadelphia Daily News". Both newspapers were purchased from The McClatchy Company (after buying out Knight Ridder) in 2006 by Philadelphia Media Holdings and operated by the group until the organization declared bankruptcy in 2010. After two years of financial struggle, the two newspapers were sold to Interstate General Media in 2012. The two newspapers have a combined circulation of about 500,000 readers.
The city also has a number of other, smaller newspapers and magazine in circulation such as the "Philadelphia Tribune", which serves the African-American community, the "Philadelphia", a monthly regional magazine; "Philadelphia Weekly", an weekly-printed alternative newspaper; "Philadelphia City Paper" another weekly-printed newspaper; "Philadelphia Gay News", which services the LGBT community; "The Jewish Exponent" a weekly-printed newspaper servicing the Jewish community; "Philadelphia Metro", free daily newspaper; and "Al Día", a weekly newspaper servicing the Latino community.
In addition, there are several student-run newspapers including "The Daily Pennsylvanian", "The Temple News", and "The Triangle".
Radio and television.
The first experimental radio license was issued in Philadelphia in August 1912 to St. Joseph's College. The first commercial broadcasting radio stations appeared in 1922: first WIP, then owned by Gimbel's department store, on March 17, followed the same year by WFIL, WOO, WCAU and WDAS. The highest-rated stations in Philadelphia include soft rock WBEB, KYW Newsradio, and urban adult contemporary WDAS-FM. Philadelphia is served by three major non-commercial public radio stations, WHYY-FM (NPR), WRTI (jazz, classical), and WXPN-FM (adult alternative music), as well as several smaller stations.
Rock stations WMMR and WYSP had historically been intense rivals. However, in 2011, WYSP switched to sports talk as WIP-FM, which broadcasts all Philadelphia Eagles games. WMMR's "The Preston and Steve Show" has been the area's top-rated morning show since Howard Stern left broadcast radio for satellite-based Sirius Radio.
Four urban stations (WUSL ("Power 99"), WPHI ("Hot 107.9"), WDAS and WRNB ("Old School 100.3")) are popular choices on the FM dial. WBEB is the city's Adult Contemporary station, while WZMP ("Wired 96.5") is the major Rhythmic Top 40 station.
In the 1930s, the experimental station W3XE, owned by Philco, became the first television station in Philadelphia; it became NBC's first affiliate in 1939, and later became KYW-TV (CBS). WCAU-TV, WPVI-TV, WHYY-TV, WPHL-TV, and WTXF-TV had all been founded by the 1970s. In 1952, WFIL (now WPVI) premiered the television show "Bandstand", which later became the nationally broadcast "American Bandstand" hosted by Dick Clark. Today, as in many large metropolitan areas, each of the commercial networks has an affiliate, and call letters have been replaced by corporate IDs: CBS3, 6ABC, NBC10, Fox29, Telefutura28, Telemundo62, Univision65, plus My PHL 17 and CW Philly 57. The region is served also by public broadcasting stations WYBE-TV (Philadelphia), WHYY-TV (Wilmington, Delaware and Philadelphia), WLVT-TV (Lehigh Valley), and NJTV (New Jersey). In September 2007, Philadelphia approved a Public-access television cable TV channel.
Until September 2014, Philadelphia was the only media market in the United States with owned-and-operated stations of all five English-language major broadcast networks (NBC – WCAU, CBS – KYW-TV, ABC – WPVI-TV, Fox – WTXF-TV and The CW – WPSG); three of the major Spanish-language networks (Univision, UniMas and Telemundo) also have O&Os serving the market (respectively, WUVP-DT, WFPA-CD and WWSI).
The city is also the nation's fourth-largest consumer in media market, as ranked by the Nielsen Media Research, with over 2.9 million TV homes.
Infrastructure.
Transportation.
Philadelphia is served by the Southeastern Pennsylvania Transportation Authority (SEPTA), which operates buses, trains, rapid transit, trolleys, and trackless trolleys throughout Philadelphia, the four Pennsylvania suburban counties of Bucks, Chester, Delaware, and Montgomery, in addition to service to Mercer County, New Jersey and New Castle County, Delaware. The city's subway, opened in 1907, is the third-oldest in America.
In 1981, large sections of the SEPTA Regional Rail service to the far suburbs of Philadelphia were discontinued due to lack of funding. Several projects have been proposed to extend rail service back to these areas, but lack of funding has again been the chief obstacle to implementation. These projects include the proposed Schuylkill Valley Metro to Wyomissing, PA, and extension of the Media/Elwyn line back to Wawa, PA. SEPTA's Airport Regional Rail Line Regional Rail offers direct service to the Philadelphia International Airport.
Philadelphia's 30th Street Station is a major railroad station on Amtrak's Northeast Corridor, which offers access to Amtrak, SEPTA, and NJ Transit lines.
The PATCO Speedline provides rapid transit service to Camden, Collingswood, Westmont, Haddonfield, Woodcrest (Cherry Hill), Ashland (Voorhees), and Lindenwold, New Jersey, from stations on Locust Street between 16th and 15th, 13th and 12th, and 10th and 9th Streets, and on Market Street at 8th Street.
Airports.
Two airports serve Philadelphia: the Philadelphia International Airport (PHL), straddling the southern boundary of the city, and the Northeast Philadelphia Airport (PNE), a general aviation reliever airport in Northeast Philadelphia. Philadelphia International Airport provides scheduled domestic and international air service, while Northeast Philadelphia Airport serves general and corporate aviation. In 2013, Philadelphia International Airport was the 15th busiest airport in the world measured by traffic movements (i.e. takeoffs and landings). It is also the second largest hub and primary international hub for American Airlines.
Roads.
William Penn initially planned a Philadelphia that had numbered streets traversing north and south and "tree" named streets traversing east and west, with the two main streets Broad Street and High Street converging at Centre Square. The plans have since expanded to include major highways that span other major sections of Philadelphia.
Interstate 95 runs through the city along the Delaware River as a main north-south artery known as the Delaware Expressway. The city is also served by the Schuylkill Expressway, a portion of Interstate 76 that runs along the Schuylkill River. It meets the Pennsylvania Turnpike at King of Prussia, Pennsylvania, providing access to Harrisburg, Pennsylvania and points west. Interstate 676, the Vine Street Expressway, was completed in 1991 after years of planning. A link between I-95 and I-76, it runs below street level through Center City, connecting to the Ben Franklin Bridge at its eastern end.
Roosevelt Boulevard and the Roosevelt Expressway (U.S. 1) connect Northeast Philadelphia with Center City. Woodhaven Road (Route 63), built in 1966, and Cottman Avenue (Route 73) serve the neighborhoods of Northeast Philadelphia, running between Interstate 95 and the Roosevelt Boulevard (U.S. 1). The Fort Washington Expressway (Route 309) extends north from the city's northern border, serving Montgomery County and Bucks County. U.S. 30, extending east-west from West Philadelphia to Lancaster, is known as Lancaster Avenue throughout most of the city and through the adjacent Main Line suburbs.
Interstate 476, commonly nicknamed the "Blue Route" through Delaware County, bypasses the city to the west, serving the city's western suburbs, as well as providing a link to Allentown and points north. Similarly, Interstate 276, the Pennsylvania Turnpike's Delaware River Extension, acts as a bypass and commuter route to the north of the city as well as a link to the New Jersey Turnpike to New York.
However, other planned freeways have been canceled, such as an Interstate 695 running southwest from downtown; two freeways connecting Interstate 95 to Interstate 76 that would have replaced Girard Avenue and South Street; and a freeway upgrade of Roosevelt Boulevard.
The Delaware River Port Authority operates four bridges in the Philadelphia area across the Delaware River to New Jersey: the Walt Whitman Bridge (I-76), the Benjamin Franklin Bridge (I-676 and US 30), the Betsy Ross Bridge (Route 90), and the Commodore Barry Bridge (US 322). The Tacony-Palmyra Bridge connects PA Route 73 in the Tacony section of Northeast Philadelphia with New Jersey's Route 73 in Palmyra, Camden County, and is maintained by the Burlington County Bridge Commission.
Bus service.
Philadelphia is also a major hub for Greyhound Lines, which operates 24-hour service to points east of the Mississippi River. Most of Greyhound's services in Philadelphia operate to/from the Philadelphia Greyhound Terminal, located at 1001 Filbert Street in Center City Philadelphia. In 2006, the Philadelphia Greyhound Terminal was the second busiest Greyhound terminal in the United States, after the Port Authority Bus Terminal in New York. Besides Greyhound, six other bus operators provide service to the Center City Greyhound terminal: Bieber Tourways, Capitol Trailways, Martz Trailways, Peter Pan Bus Lines, Susquehanna Trailways, and the bus division for New Jersey Transit. Other services include Megabus and Bolt Bus.
Rail.
Since the early days of rail transport in the United States, Philadelphia has served as hub for several major rail companies, particularly the Pennsylvania Railroad and the Reading Railroad. The Pennsylvania Railroad first operated Broad Street Station, then 30th Street Station and Suburban Station, and the Reading Railroad operated out of Reading Terminal, now part of the Pennsylvania Convention Center. The two companies also operated competing commuter rail systems in the area, known collectively as the Regional Rail system. The two systems today, for the most part still intact but now connected, operate as a single system under the control of the SEPTA, the regional transit authority. Additionally, the PATCO Speedline subway system and NJ Transit's Atlantic City Line operate successor services to southern New Jersey.
Philadelphia, once home to more than 4,000 trolleys on 65 lines, is one of the few North American cities to maintain streetcar lines. Today, SEPTA operates five "subway-surface" trolleys that run on street-level tracks in West Philadelphia and subway tunnels in Center City. SEPTA also recently reintroduced trolley service to the Girard Avenue Line, Route 15.
Today, Philadelphia is a regional hub of the federally owned Amtrak system, with 30th Street Station being a primary stop on the Washington-Boston Northeast Corridor and the Keystone Corridor to Harrisburg and Pittsburgh. 30th Street also serves as a major station for services via the Pennsylvania Railroad's former Pennsylvania Main Line to Chicago. 30th Street is Amtrak's third-busiest station in numbers of passengers as of fiscal year 2013.
Walkability.
A 2015 study by Walk Score ranked Philadelphia the fourth most walkable major city in the United States.
Utilities.
Historically, Philadelphia sourced its water by the Fairmount Water Works, the nation's first major urban water supply system. In 1909, Water Works was decommissioned as the city transitioned to modern sand filtration methods. Today, the Philadelphia Water Department (PWD) provides drinking water, wastewater collection, and stormwater services for Philadelphia, as well as surrounding counties. PWD draws about 57 percent of its drinking water from the Delaware River and the balance from the Schuylkill River. The public wastewater system consists of three water pollution control plants, 21 pumping stations, and about 3,657 miles of sewers. A 2007 investigation by the Environmental Protection Agency found elevated levels of Iodine-131 in the city's potable water. In 2012, the EPA's readings discovered that the city had the highest readings of I-131 in the nation. The city campaigned against an Associated Press report that the high levels of I-131 were the results of local gas drilling in the Upper Delaware River.
PECO Energy Company, founded as the Philadelphia Electric Company in 1881, provides electricity to over 1.6 million customers in the southeastern Pennsylvania. The company has over 500 power substations and 29,000 miles of distribution of transmission lines in its service making it the largest combination utility in the state.
Philadelphia Gas Works (PGW), overseen by the Pennsylvania Public Utility Commission, is the nation's largest municipally owned natural gas utility. It serves over 500,000 homes and businesses in the Philadelphia area. Founded in 1836, the company came under city ownership in 1987 and has been providing the majority of gas distributed within city limits. In 2014, the Philadelphia City Council refused to conduct hearings on a $1.86 billion sale of PGW, part of a two-year effort that was proposed by the mayor. The refusal led to the prospective buyer terminating its offer.
Southeastern Pennsylvania was assigned the 215 area code in 1947 when the North American Numbering Plan of the "Bell System" went into effect. The geographic area covered by the code was split nearly in half in 1994 when area code 610 was created, with the city and its northern suburbs retaining 215. Overlay area code 267 was added to the 215 service area in 1997, and 484 was added to the 610 area in 1999. A plan in 2001 to introduce a third overlay code to both service areas (area code 445 to 215, area code 835 to 610) was delayed and later rescinded.
An effort was approved on 2005 to provide low-cost, citywide Wi-Fi service to the city. Wireless Philadelphia would have been the first municipal internet utility offering in a large US city, but the plan was abandoned in 2008 as EarthLink pushed back the completion date several times. Mayor Nutter's administration closed the project in 2009 after an attempt to revitalize it failed.
Twin towns – Sister cities.
Philadelphia has eight official sister cities, as designated by the Citizen Diplomacy International – Philadelphia:
Philadelphia also has three partnership cities or regions:
Philadelphia has dedicated landmarks to its sister cities. Dedicated in June 1976, the Sister Cities Plaza, a site of located at 18th and Benjamin Franklin Parkway, honors Philadelphia's relationships with Tel Aviv and Florence which were its first sister cities. Another landmark, the Toruń Triangle, honoring the sister city relationship with Toruń, Poland, was constructed in 1976, west of the United Way building at 18th Street and the Benjamin Franklin Parkway. In addition, the Triangle contains the Copernicus monument. Renovations were made to Sister Cities Park in mid-2011 and on May 10, 2012, SCP was reopened and currently features an interactive fountain honoring Philadelphia's ten sister and friendship cities, a café and visitor's center, children's play area, outdoor garden, and boat pond, as well as pavilion built to environmentally friendly standards.
The Chinatown Gate, erected in 1984 and crafted by artisans of Tianjin, stands astride the intersection of 10th and Arch Streets as an elaborate and colorful symbol of the sister city relationship. The CDI of Philadelphia has participated in the U.S. Department of State's "Partners for Peace" project with Mosul, Iraq, as well as accepting visiting delegations from dozens of other countries.

</doc>
<doc id="50588" url="https://en.wikipedia.org/wiki?curid=50588" title="Expo 67">
Expo 67

The 1967 International and Universal Exposition or Expo 67, as it was commonly known, was a general exhibition, Category One World's Fair held in Montreal, Quebec, Canada, from April 27 to October 29, 1967. It is considered to be the most successful World's Fair of the 20th century with the most attendees to that date and 62 nations participating. It also set the single-day attendance record for a world's fair, with 569,500 visitors on its third day.
Expo 67 was Canada's main celebration during its centennial year. The fair had been intended to be held in Moscow, to help the Soviet Union celebrate the Russian Revolution's 50th anniversary; however, for various reasons, the Soviets decided to cancel, and Canada was awarded it in late 1962.
The project was not well supported in Canada at first. It took the determination of Montreal's mayor, Jean Drapeau, and a new team of managers to guide it past political, physical and temporal hurdles. Defying a computer analysis that said it could not be done, the fair opened on time.
After Expo 67 ended in October 1967, the site and most of the pavilions continued on as an exhibition called Man and His World, open during the summer months from 1968 until 1984. By that time, most of the buildings — which had not been designed to last beyond the original exhibition — had deteriorated and were dismantled. Today, the islands that hosted the world exhibition are mainly used as parkland and for recreational use, with only a few remaining structures from Expo 67 to show that the event was held there.
History.
Background.
The idea of hosting the 1967 World Exhibition dates back to 1956, but it was in 1958 that Conservative Senator Mark Drouin suggested during his speech at the Brussels Exhibition that Canada should host a World Exhibition to celebrate its centennial. The exposition was offered first to Toronto but politicians there rejected the idea. However, Montreal's mayor, Sarto Fournier, backed the proposal, allowing Canada to make a bid to the Bureau International des Expositions (BIE). At the BIE's May 5, 1960 meeting in Paris, Moscow was awarded the fair after five rounds of voting that eliminated Austria's and then Canada's bids. In April 1962, the Soviets scrapped plans to host the fair because of financial constraints and security concerns. Montreal's new mayor, Jean Drapeau, lobbied the Canadian government to try again for the fair, which they did. On November 13, 1962, the BIE changed the location of the World Exhibition to Canada, and Expo 67 went on to become the fourth-best attended BIE-sanctioned world expositions, after Shanghai, Osaka, and Paris.
Several sites were proposed as the main Expo grounds. One location that was considered was Mount Royal Park, to the north of the downtown core. But it was Drapeau's idea to create new islands in the St. Lawrence river, and to enlarge the existing Saint Helen's Island. The choice overcame opposition from Montreal's surrounding municipalities, and also prevented land speculation.
Key people.
Expo did not get off to a smooth start; in 1963, many top organizing committee officials resigned. The main reason for the resignations was Mayor Drapeau's choice of the site on new islands to be created around the existing St. Helen's Island and also that a computer program predicted that the event could not possibly be constructed in time. Another more likely reason for the mass resignations was that on April 22, 1963, the federal Liberal government of Prime Minister Lester Pearson took power. This meant that former Prime Minister John Diefenbaker's Progressive Conservative government appointees to the board of directors of the Canadian Corporation for the 1967 World Exhibition were likely forced to resign.
Canadian diplomat Pierre Dupuy was named Commissioner General, after Diefenbaker appointee Paul Bienvenu resigned from the post in 1963. One of the main responsibilities of the Commissioner General was to attract other nations to build pavilions at Expo. Dupuy would spend most of 1964 and 1965 soliciting 125 countries, spending more time abroad than in Canada. Dupuy's 'right-hand' man was Robert Fletcher Shaw, the deputy commissioner general and vice-president of the corporation. He also replaced a Diefenbaker appointee, C.F. Carsley, Deputy Commissioner General. Shaw was a professional engineer and builder, and is widely credited for the total building of the Exhibition. Dupuy hired Andrew Kniewasser as the general manager. The management group became known as "Les Durs" - the tough guys - and they were in charge of creating, building and managing Expo. "Les Durs" consisted of: Jean-Claude Delorme, Legal Counsel and Secretary of the Corporation; Dale Rediker, Director of Finances; Colonel Edward Churchill, Director of Installations; Philippe de Gaspé Beaubien, Director of Operations, dubbed "The Mayor of Expo"; Pierre de Bellefeuille, Director of Exhibitors; and Yves Jasmin, Director of Information, Advertising and Public Relations. To this group the chief architect Édouard Fiset was added. All ten were honoured by the Canadian government as recipients of the Order of Canada, Companions for Dupuy and Shaw, Officers for the others.
Jasmin wrote a book, in French, "La petite histoire d'Expo 67", about his 45-month experience at Expo and created the Expo 67 Foundation (available on the web site under that name) to commemorate the event for future generations.
As historian Pierre Berton put it, the cooperation between Canada's French- and English-speaking communities "was the secret of Expo's success — 'the Québécois flair, the English-Canadian pragmatism.'" However, Berton also points out that this is an over-simplification of national stereotypes. Arguably Expo did, for a short period anyway, bridge the 'Two Solitudes.'
Montebello conference produces theme.
In May 1963, a group of prominent Canadian thinkers — including Alan Jarvis, director of the National Gallery of Canada; novelists Hugh MacLennan and Gabrielle Roy; J. Tuzo Wilson, geophysicist; and Claude Robillard, town planner — met for three days at the Seigneury Club in Montebello, Quebec. The theme, "Man and His World", was based on the 1939 book entitled "Terre des Hommes" (translated as "Wind, Sand and Stars") by Antoine de Saint-Exupéry. In Roy's introduction to the Expo 67 corporation's book, entitled "Terre des Hommes/Man and His World", she elucidates the theme:
The organizers also created seventeen theme elements for Man and his World:
Construction begins.
Construction started on August 13, 1963, with an elaborate ceremony hosted by Mayor Drapeau on barges anchored in the St. Lawrence River. Ceremonially, construction began when Prime Minister Lester B. Pearson pulled a lever that signalled a front-end loader to dump the first batch of fill to enlarge "Saint Helen's Island", and Quebec premier Jean Lesage spread the fill with a bulldozer. The 25 million tons of fill needed to construct the islands was coming from the Montreal Metro's excavations, a public works project that was already under construction before Expo was awarded to Montreal. Expo's initial construction period mainly centred on enlarging Saint Helen's Island, creating the artificial island of Notre Dame Island and lengthening and enlarging the Mackay Pier which became the Cité du Havre. While construction continued, the land rising out of Montreal harbour was not owned by the Expo Corporation yet. After the final mounds of earth completed the islands, the grounds that would hold the fair were officially transferred from the City of Montreal to the corporation on June 20, 1964. This gave Colonel Churchill only 1042 days to have everything built and functioning for opening day. To get Expo built in time, Churchill used the then new project management tool known as the critical path method (CPM). On April 28, 1967, opening day, everything was ready, with one exception: Habitat 67, which was then displayed as a work in progress.
Building and enlarging the islands, along with the new Concorde Bridge built to connect them with the site-specific mass transit system known as the Montreal Expo Express, plus a boat pier, cost more than the Saint Lawrence Seaway project did only five years earlier: this was even before any buildings or infrastructure were constructed. With the initial phase of construction completed, it is easy to see why the budget for the exhibition was going to be larger than anyone expected. In the fall of 1963, Expo's general manager, Andrew Kniewasser, presented the master plan and the preliminary budget of $167 million for construction: it would balloon to over $439 million by 1967. The plan and budget narrowly passed a vote in Pearson's federal cabinet, passing by one vote, and then it was officially submitted on December 23, 1963.
Logo.
The logo was designed by Montreal artist Julien Hébert. The basic unit of the logo is an ancient symbol of man. Two of the symbols (pictograms of "man") are linked as to represent friendship. The icon was repeated in a circular arrangement to represent "friendship around the world". The logotype is lower-case Optima font. It did not enjoy unanimous support from federal politicians, as some of them tried to kill it with a motion in the Canadian House of Commons.
Theme songs.
The official Expo 67 theme song was composed by Stéphane Venne and was titled: "Hey Friend, Say Friend/Un Jour, Un Jour". Complaints were made about the suitability of the song, as its lyrics mentioned neither Montreal nor Expo 67.
However, the song that most Canadians associate with Expo was written by Bobby Gimby, a veteran commercial jingle writer who composed the popular Centennial tune "Ca-na-da". Gimby earned the name the "Pied Piper of Canada".
The theme song "Something to Sing About", used for the Canadian pavilion, had been written for a 1963 television special. The Ontario pavilion also had its own theme song: "A Place to Stand, A Place to Grow", which has evolved to become an unofficial theme song for the province.
Expo opens.
Official opening ceremonies were held on Thursday afternoon, April 27, 1967. The ceremonies were an invitation-only event, held at Place des Nations. Canada's Governor General, Roland Michener, proclaimed the exhibition open after the Expo flame was ignited by Prime Minister Pearson. On hand were over 7,000 media and invited guests including 53 heads of state. Over 1,000 reporters covered the event, broadcast in NTSC Colour, live via satellite, to a worldwide audience of over 700 million viewers and listeners.
Expo 67 opened to the public on the morning of Friday, April 28, 1967, with a space age-style countdown. A capacity crowd at Place d'Accueil participated in the atomic clock-controlled countdown that ended when the exhibition opened precisely at 9:30 a.m. EST. An estimated crowd of between 310,000 and 335,000 visitors showed up for opening day, as opposed to the expected crowd of 200,000. The first person through the Expo gates at "Place d'Accueil" was Al Carter, a 41-year-old jazz drummer from Chicago, who was recognized for his accomplishment by Expo 67's director of operations Philippe de Gaspé Beaubien. Beaubien presented Carter with a gold watch for his feat.
On opening day, there was considerable comment on the uniform of the hostesses from the UK Pavilion. The dresses had been designed to the then-new miniskirt style, popularized a year earlier by Mary Quant.
In conjunction with the opening of Expo 67, the Canadian Post Office Department issued a 5¢ stamp commemorating the fair, designed by Harvey Thomas Prosser.
Entertainment, Ed Sullivan Show, and VIPs.
The World Festival of Art and Entertainment at Expo 67 featured art galleries, opera, ballet and theatre companies, orchestras, jazz groups, famous Canadian pop musicians and other cultural attractions. Many pavilions had music and performance stages, where visitors could find free concerts and shows. Most of the featured entertainment took place in the following venues: Place des Arts, Expo Theatre, Place des Nations, La Ronde, and Automotive Stadium.
The La Ronde amusement park was always intended to be a lasting legacy of the fair. Most of its rides and booths were permanent. When the Expo fairgrounds closed nightly, at around 10:00 p.m., visitors could still visit La Ronde, which closed at 2:30 a.m.
In addition, "The Ed Sullivan Show" was broadcast live on May 7 and May 21 from Expo 67. Stars on the shows included America's The Supremes, Britain's Petula Clark and Australia's The Seekers.
Another attraction was the Canadian Armed Forces Tattoo 1967 at the Autostad in Montreal.
The fair was visited by many of the most notable people of the day, including Canada's monarch, Queen Elizabeth II, Lyndon Johnson, Princess Grace, Jacqueline Kennedy, Robert F. Kennedy, Ethiopia's emperor Haile Selassie, Charles de Gaulle, Bing Crosby, Harry Belafonte, Maurice Chevalier, Maharishi Mahesh Yogi and Marlene Dietrich. Musicians like Thelonious Monk, Grateful Dead, Tiny Tim, The Tokens and Jefferson Airplane entertained the crowds.
Problems.
Despite its successes, there were problems: Front de libération du Québec militants had initially threatened to disrupt the exhibition, but were inactive during this period. Vietnam war protesters picketed during the opening day, April 28. American President Lyndon B. Johnson's visit became a focus of war protesters. The Cuba pavilion attracted threats that it would be destroyed by anti-Castro forces that never materialized. In June, the Arab-Israeli conflict in the Middle East flared up again in the Six Day War, which resulted in Kuwait pulling out of the fair in protest to the way Western nations dealt with the war. The president of France, Charles De Gaulle, caused an international incident on July 24 when he addressed thousands at Montreal City Hall by yelling out the now famous words ""Vive Montréal... Vive le Québec... Vive le Québec Libre!"" 
In September, the most serious problem turned out to be a 30-day transit strike. By the end of July, estimates predicted that Expo would exceed 60 million visitors, but the strike cut deeply into attendance and revenue figures, just as the fair was cruising to its conclusion. Another major problem, beyond the control of Expo's management, was guest accommodation and lodging. Logexpo was created to direct visitors to accommodations in the Montreal area, which usually meant that visitors would stay at the homes of people they were unfamiliar with, rather than traditional hotels or motels. The Montreal populace opened their homes to thousands of guests. Unfortunately for some visitors, they were sometimes sent to less than respectable establishments where operators took full advantage of the tourist trade. Management of Logexpo was refused to Expo and was managed by a Quebec provincial authority. Still, Expo would get most of the blame for directing visitors to these establishments. But overall, a visit to Expo from outside Montreal was still seen as a bargain.
Expo ends.
Expo 67 closed on Sunday afternoon, October 29, 1967. The fair had been scheduled to close two days earlier, however a two-day extension granted by the International Exhibitions Bureau allowed it to continue over the weekend. On the final day 221,554 visitors added to the more than 50 million (50,306,648) that attended Expo 67 at a time when Canada's population was only 20 million, setting a per-capita record for World Exhibition attendance that still stands.
Starting at 2:00 p.m., Expo Commissioner General Pierre Dupuy officiated over the medal ceremony, in which participating nations and organizations received gold and silver medallions, and over the ceremony in which national flags were lowered in the reverse order to which they had been raised, with Canada's flag lowered first and Nigeria's lowered last. After Prime Minister Pearson doused the Expo flame, Governor General Michener closed Expo at Place des Nations with the mournful spontaneous farewell: "It is with great regret that I declare that the Universal and International Exhibition of 1967 has come to an official end." All rides and the minirail were shut down by 3:50 p.m., and the Expo grounds closed at 4:00 p.m., with the last Expo Express train leaving for "Place d'Accueil" at that time. A fireworks display, that went on for an hour, was Expo's concluding event.
Expo performed better financially than expected. Expo was intended to have a deficit, shared between the federal, provincial and municipal levels of government. Significantly better-than-expected attendance revenue reduced the debt to well below the original estimates. The final financial statistics, in 1967 Canadian dollars, were: revenues of $221,239,872, costs of $431,904,683, and a deficit of $210,664,811.
Pavilions.
Expo 67 featured 90 pavilions representing Man and His World themes, nations, corporations, and industries including the U.S. pavilion, a geodesic dome designed by Buckminster Fuller. Expo 67 also featured the Habitat 67 modular housing complex designed by architect Moshe Safdie, which was later purchased by private individuals and is still occupied.
The most popular pavilion was the Soviet Union's exhibit. It attracted about 13 million visitors. Rounding out the top five pavilions, in terms of attendance were: the Canadian Pavilion (11 million visitors), the United States (9 million), France (8.5 million), and Czechoslovakia (8 million).
The participating countries were
Countries conspicuously absent were Spain, South Africa, the People's Republic of China, and many South American countries.
Legacy.
Man and His World (1968-1984).
After 1967, the exposition struggled for several summer seasons as a standing collection of international pavilions known as "Man and His World". However, as attendance declined, the physical condition of the site deteriorated, and less and less of it was open to the public. After the 1971 season, the entire Notre Dame Island site closed and three years later completely rebuilt around the new rowing and canoe sprint (then flatwater canoeing) basin for Montreal's 1976 Summer Olympics. Space for the basin, the boathouses, the changing rooms and other buildings was obtained by demolishing many of the former pavilions and cutting in half the area taken by the artificial lake and the canals.
In 1976, a fire destroyed the acrylic outer skin of Buckminster Fuller's dome, and the previous year the Ontario pavilion was gutted by a major fire. With the site falling into disrepair, and several pavilions left abandoned and vandalized, it began to resemble ruins of a futuristic city.
In 1980 the Notre Dame Island site was reopened (primarily for the Floralies) making both islands simultaneously accessible again, albeit only for a brief time. Minor thematic exhibitions were held at the Atlantic pavilion and Quebec pavilion (and continued even several years beyond). After the 1981 season, the Saint Helen's Island site permanently closed, shutting out the majority of attractions. Man and His World was able to continue in a limited fashion with the small number of pavilions left standing on Notre Dame Island. However, the few remaining original exhibits closed permanently in 1984.
Park and surviving relics.
After the Man and His World summer exhibitions were discontinued, with most pavilions and remnants demolished between 1985 and 1986, the former site for Expo 67 on Saint Helen's Island and Notre Dame Island was incorporated into a municipal park run by the city of Montreal. In 2000, the park was renamed from Parc des Îles to Parc Jean-Drapeau, after Mayor Jean Drapeau, who had brought the exhibition to Montreal. In 2006, the corporation that runs the park also changed its name from the "Société du parc des Îles" to the "Société du parc Jean-Drapeau". Two prominent buildings remain in use on the former Expo grounds: the American pavilion's metal-lattice skeleton from its Buckminster Fuller dome, now enclosing an environmental sciences museum called the Montreal Biosphère; and Habitat 67, now a condominium residence. Also, the French and Quebec pavilions now form the Montreal Casino. La Toundra Hall is part of the surviving structural remains of the Canadian pavilion. It is now a special events hall with dining facilities available. Another part of the pavilion serves as the administration building of Parc Jean-Drapeau. Katimavik's distinctive inverted pyramid and much of the rest of the Canadian pavilion were dismantled during the 1970s.
Place des Nations, where the opening and closing ceremonies were held, still remains however in an abandoned and deteriorating state. The Jamaican, Tunisian and Korean pavilion (roof only) plus the CIBC banking center also survive. In Cite du Havre the Expo Theater, Administration and Fine Arts buildings still remain. Other remaining structures include sculptures, lampposts and landscaping. The Montreal Metro subway station Berri Uqam still has a "Man and His World" logo and welcome sign above the pedestrian tunnel entrance to the yellow line. La Ronde survives, and since 2001 it was leased to the New York amusement park company Six Flags.
The Alcan Aquarium built for the Expo remained in operation for a number of decades until its closure in 1991. The Expo 67 parking lot had been converted into Victoria STOLport, an experimental short-take off airport for a brief time in the 1970s.
Another attraction on today's Notre Dame Island site is the Circuit Gilles Villeneuve race track that is used for the Canadian Grand Prix. The Olympic basin is used today by many local rowing clubs. A beach was built on the shores of the remaining artificial lake. There are many acres of parkland and cycle paths on both Saint Helen's Island and the western tip of Notre Dame Island. The site has been used for a number of events such as a BIE sponsored international botanical festival, "Les floralies". The young trees and shrubs planted for Expo 67 are now mature. The plants introduced during the botanical events have flourished also.
Expo's lasting effects.
In a political and cultural context, Expo 67 was seen as a landmark moment in Canadian history. In 1968, as a salute to the cultural impact the exhibition had on the city, Montreal's Major League baseball team, the Expos (now the Washington Nationals), was named after the event. 1967 was also the year that invited Expo guest Charles De Gaulle, on July 24, addressed thousands at Montreal City Hall by yelling out the now famous words: "Vive Montréal... Vive le Québec... Vive le Québec Libre!" (See Vive le Québec libre speech). De Gaulle was rebutted in Ottawa by Prime Minister Lester B. Pearson: "Canadians do not need to be liberated, Canada will remain united and will reject any effort to destroy her unity." In the years that followed, the tensions between the English and French communities would continue. As an early 21st-century homage to the fair, satirists Bowser and Blue wrote a full-length musical set at Expo 67 called "The Paris of America", which ran for six sold-out weeks at Centaur Theatre in Montreal in April and May 2003.
Expo 67 was one of the most successful World Exhibitions, and is still regarded fondly by Canadians. In Montreal, 1967 is often referred to as "the last good year" before economic decline, Quebec Sovereignism (seen as negative from a federalist viewpoint), deteriorating infrastructure and political apathy became common. In this way, it has much in common with the 1964-65 New York World's Fair. In 2007, a new group, Expo 17, was looking to bring a smaller-scale — BIE sanctioned — exposition to Montreal for Expo 67's 50th anniversary and Canada's sesquicentennial in 2017. Expo 17 hoped a new world's fair would regenerate the spirit of Canada's landmark centennial project.
In popular culture.
• An episode of the 1970s television series "Battlestar Galactica", "Greetings from Earth Part 2", was filmed at the Expo site in 1979. The Expo structures were used to represent a city on an alien world where the people had all been killed by a long-ago war.
• The 1979 film Quintet was shot entirely on the site of Expo during winter months, utilizing abandoned pavilions and other ruins to portray a post-apocalyptic landscape.

</doc>
<doc id="50589" url="https://en.wikipedia.org/wiki?curid=50589" title="Spanking">
Spanking

Spanking is a type of corporal punishment involving the act of striking the buttocks of another person to cause physical pain, generally with an open hand (more commonly referred to in some countries as "slapping" or "smacking"). More severe forms of spanking, such as switching, paddling, belting, caning, whipping, and birching, involve the use of an implement instead of a hand. Parents commonly spank children or adolescents in response to undesired behavior. Boys are more frequently spanked than girls, both at home and in school. Some countries have outlawed the spanking of children in every setting, including homes, schools, and penal institutions, but most allow it when done by a parent or guardian. Adults may spank other adults as well, often in an erotic context.
Terminology.
In North America, the word "spanking" has often been used as a synonym for an official paddling in school, and sometimes even as a euphemism for the formal corporal punishment of adults in an institution.
In British English, most dictionaries define "spanking" as being given only with the open hand.
In American English, dictionaries define spanking as being administered with either the open hand or an implement such as a paddle. Thus, the standard form of corporal punishment in US schools (use of a paddle) is often referred to as a spanking, whereas its pre-1997 English equivalent (strokes of the cane) would never have been so described.
The word "licks" is also a common term in West Indian countries, especially Trinidad & Tobago.
In Britain, Ireland, Australia and New Zealand, the word "smacking" is generally used in preference to "spanking" when describing striking with an open hand, rather than with an implement. Whereas a spanking is invariably administered to the bottom, "smacking" is less specific and may refer to slapping the child's hands, arms or legs as well as its bottom.
In the home.
Spanking of children by their parents is a common form of corporal punishment used in families in the Western world. It is normally done with one or more slaps on the child's buttocks with a bare hand, although, not uncommonly, various objects are used to spank children. Historically, boys have been spanked more than girls. In the United States, the spanking of infants is common, with toddler-age children being spanked the most. The main reasons parents give for spanking children are to make children more compliant, and to promote better behavior, especially to put a stop to children's aggressive behaviors. Research shows, however, that spanking, or indeed any form of corporal punishment, tends to have the opposite effect. Children who are physically punished more often tend to obey parents less with time, and to develop more aggressive behaviors, including toward other children. There are also a number of documented adverse physical, mental, and emotional effects of spanking and other forms of corporal punishment, including various physical injuries, increased anxiety, depression, and antisocial behavior.
Although parents and other advocates of spanking often say that it is necessary to promote child discipline, studies have shown that parents tend to apply physical punishment inconsistently, spanking more often when angry or under stress. The use of corporal punishment by parents increases the likelihood that children will suffer physical abuse, and most documented cases of physical abuse in Canada and the United States begin as disciplinary spankings.
In schools.
Corporal punishment, usually delivered with an implement (such as a paddle or cane) rather than with the open hand, used to be a common form of school discipline in many countries, but it is now banned in most of the western world. These bans have been controversial, and in many cultures opinion remains sharply divided as to the efficacy or suitability of spanking as a punishment for misbehaviour by school students.
Formal caning, notably for teenage boys, remains a common form of discipline in schools in several Asian and African countries, especially those with a British heritage; in these cultures it is referred to as "caning" and not "spanking".
The Supreme Court of the United States in 1977 held that the paddling of school students was not "per se" unlawful. However, 31 states have now banned paddling in public schools. It is still common in some schools in the South.
Adult spanking.
Erotic spanking can be a part of intimate activities such as sexual intercourse, foreplay or sexual roleplay. Spanking of a wife by her husband as a punishment also does occur in some instances, often based on a literalist interpretation of the Bible.
Ritual spanking traditions.
There are some rituals or traditions which involve spanking. For example, on the first day of the lunar Chinese new year holidays, a week-long 'Spring Festival', the most important festival for Chinese people all over the world, thousands of Chinese visit the Taoist "Dong Lung Gong" temple in Tungkang to go through the century-old ritual to get rid of bad luck, men by receiving spankings and women by being whipped, with the number of strokes to be administered (always lightly) by the temple staff being decided in either case by the god Wang Ye and by burning incense and tossing two pieces of wood, after which all go home happily, believing their luck will improve.
On Easter Monday, there is a Slavic tradition of hitting girls and young ladies with woven willow switches (Czech: "pomlázka"; Slovak: "korbáč") and dousing them with water.
In Slovenia, there is a jocular tradition that anyone who succeeds in climbing to the top of Mount Triglav receives a spanking or birching.
According to Ovid's Fasti (ii.305), during the ancient Roman festival of the Lupercalia naked men ran through the streets of the city, carrying straps with which they swatted the outstretched palms of the hands of women lining the racecourse who wished to become pregnant.
In North America, there is a tradition of "birthday spankings" where the birthday girl or boy receives the same number of hits as her/his age (plus "one to grow on") during the birthday party. Birthday spankings are administered over the clothes and usually by close friends or family members, and are generally playful swats not meant to cause real pain.
References.
Notes

</doc>
<doc id="50591" url="https://en.wikipedia.org/wiki?curid=50591" title="United States Postal Service">
United States Postal Service

The United States Postal Service, also known as the Post Office, U.S. Mail, or Postal Service, often abbreviated as USPS, is an independent agency of the United States federal government responsible for providing postal service in the United States. It is one of the few government agencies explicitly authorized by the United States Constitution.
The U.S. Mail traces its roots to 1775 during the Second Continental Congress, where Benjamin Franklin was appointed the first postmaster general. The Post Office Department was created in 1792 from Franklin's operation, elevated to a cabinet-level department in 1872, and transformed in 1971 into the U.S. Postal Service as an independent agency under the Postal Reorganization Act.
The USPS as of February 2015 has 617,254 active employees and operated 211,264 vehicles in 2014. The USPS is the operator of the largest civilian vehicle fleet in the world. The USPS is legally obligated to serve all Americans, regardless of geography, at uniform price and quality. The USPS has exclusive access to letter boxes marked "U.S. Mail" and personal letterboxes in the United States, but still competes against private package delivery services, such as UPS and has part use with FedEx Express.
The USPS has not directly received taxpayer-dollars since the early 1980s with the exception of subsidies for costs associated with the disabled and overseas voters. Since the 2006 all-time peak mail volume, after which Congress passed the Postal Accountability and Enhancement Act, (which mandated $5.5 billion per year to be paid into an account to fully prefund employee retirement health benefits, a requirement exceeding that of other government and private organizations ), revenue dropped sharply due to recession-influenced declining mail volume, prompting the postal service to look to other sources of revenue while cutting costs to reduce its budget deficit. The USPS lost US$5.5 billion in fiscal 2014, and its revenue was US$67.8 billion.
History.
Foundations.
In the early years of the North American colonies, many attempts were made to initiate a postal service. These early attempts were of small scale and usually involved a colony, Massachusetts Bay Colony for example, setting up a location in Boston where one could post a letter back home to England. Other attempts focused on a dedicated postal service between two of the larger colonies, such as Massachusetts and Virginia, but the available services remained limited in scope and disjointed for many years. For example, informal independently-run postal routes operated in Boston as early as 1639, with a Boston to New York City service starting in 1672.
A central postal organization came to the colonies in 1691, when Thomas Neale received a 21-year grant from the British Crown for a North American Postal Service. On February 17, 1691, a grant of "letters patent" from the joint sovereigns, William and Mary, empowered him:
"to erect, settle, and establish within the chief parts of their majesties' colonies and plantations in America, an office or offices for receiving and dispatching letters and pacquets, and to receive, send, and deliver the same under such rates and sums of money as the planters shall agree to give, and to hold and enjoy the same for the term of twenty-one years."
The patent included the exclusive right to establish and collect a formal postal tax on official documents of all kinds. The tax was repealed a year later. Neale appointed Andrew Hamilton, Governor of New Jersey, as his deputy postmaster. The first postal service in America commenced in February 1692. Rates of postage were fixed and authorized, and measures were taken to establish a post office in each town in Virginia. Massachusetts and the other colonies soon passed postal laws, and a very imperfect post office system was established. Neale's patent expired in 1710, when Parliament extended the English postal system to the colonies. The chief office was established in New York City, where letters were conveyed by regular packets across the Atlantic.
Before the Revolution, there was only a trickle of business or governmental correspondence between the colonies. Most of the mail went back and forth to counting houses and government offices in London. The Revolution made Philadelphia, the seat of the Continental Congress, the information hub of the new nation. News, new laws, political intelligence, and military orders circulated with a new urgency, and a postal system was necessary. Journalists took the lead, securing post office legislation that allowed them to reach their subscribers at very low cost, and to exchange news from newspapers between the thirteen states. Overthrowing the London-oriented imperial postal service in 1774-1775, printers enlisted merchants and the new political leadership, and created new postal system. The "United States Post Office" (USPO) was created on July 26, 1775, by decree of the Second Continental Congress. Benjamin Franklin headed it briefly.
Before the Revolution, individuals like Benjamin Franklin and William Goddard were the colonial postmasters who managed the mails then and were the general architects of a postal system that started out as an alternative to the Crown Post.
The official post office was created in 1792 as the Post Office Department (USPOD). It was based on the Constitutional authority empowering Congress "To establish post offices and post roads". The 1792 law provided for a greatly expanded postal network, and served editors by charging newspapers an extremely low rate. The law guaranteed the sanctity of personal correspondence, and provided the entire country with low-cost access to information on public affairs, while establishing a right to personal privacy.
Rufus Easton was appointed by Thomas Jefferson first postmaster of St. Louis under the recommendation of Postmaster General Gideon Granger. Rufus Easton was the first postmaster and built the first post office west of the Mississippi. At the same time Easton was appointed by Thomas Jefferson, judge of Louisiana Territory, the largest territory in North America. Bruce Adamson wrote that: "Next to Benjamin Franklin, Rufus Easton was one of the most colorful people in United States Postal History." It was Easton who educated Abraham Lincoln's Attorney General, Edward Bates. In 1815 Edward Bates moved into the Easton home and lived there for years at Third and Elm. Today this is the site of the Jefferson Memorial Park. In 1806 Postmaster General Gideon Granger wrote a three-page letter to Easton, begging him not to partake in a duel with vice-president Aaron Burr. Two years earlier it was Burr who had shot and killed Alexander Hamilton. Many years later in 1852, Easton's son, Major-General Langdon Cheves Easton, was commissioned by William T. Sherman, at Fort Union to delivery a letter to Independence, Missouri. Sherman wrote: “In the Spring of 1852, General Sherman mentioned that the quartermaster, Major L.C. Easton, at Fort Union, New Mexico, had occasion to send some message east by a certain date, and contracted with Aubrey to carry it to the nearest post office (then Independence, Missouri), making his compensation conditional on the time consumed. He was supplied with a good horse, and an order on the outgoing trains for exchange. Though the whole route was infested with hostile Indians, and not a house on it, Aubrey started alone with his rifle. He was fortunate in meeting several outward-bound trains, and thereby made frequent changes of horses, some four or five, and reached Independence in six days, having hardly rested or slept the whole way." 
To cover long distances, the Post Office used a hub-and-spoke system, with Washington as the hub and chief sorting center. By 1869, with 27,000 local post offices to deal with, it had changed to sorting mail en route in specialized railroad mail cars, called Railway Post Offices, or RPOs. The system of postal money orders began in 1864. Free mail delivery began in the larger cities in 1863.
19th century.
The postal system played a crucial role in national expansion. It facilitated expansion into the West by creating an inexpensive, fast, convenient communication system. Letters from early settlers provided information and boosterism to encourage increased migration to the West, helped scattered families stay in touch and provide assistance, assisted entrepreneurs in finding business opportunities, and made possible regular commercial relationships between merchants in the west and wholesalers and factories back east. The postal service likewise assisted the Army in expanding control over the vast western territories. The widespread circulation of important newspapers by mail, such as the "New York Weekly Tribune," facilitated coordination among politicians in different states. The postal service helped integrate established areas with the frontier, creating a spirit of nationalism and providing a necessary infrastructure.
The Post Office in the 19th century was a major source of federal patronage. Local postmasterships were rewards for local politicians—often the editors of party newspapers. About 3/4 of all federal civilian employees worked for the Post Office. In 1816 it employed 3341 men, and in 1841, 14,290. The volume of mail expanded much faster than the population, as it carried annually 100 letters and 200 newspapers per 1000 white population in 1790, and 2900 letters and 2700 newspapers per thousand in 1840.
The Post Office Department was enlarged during the tenure of President Andrew Jackson. As the Post Office expanded, difficulties were experienced due to a lack of employees and transportation. The Post Office's employees at that time were still subject to the so-called "spoils" system, where faithful political supporters of the executive branch were appointed to positions in the post office and other government corporations as a reward for their patronage. These appointees rarely had prior experience in postal service and mail delivery. This system of political patronage was replaced in 1883, after passage of the Pendleton Civil Service Reform Act.
Ten years before waterways were declared post roads in 1823, the Post Office used steamboats to carry mail between post towns where no roads existed. Once it became clear that the postal system in the United States needed to expand across the entire country, the use of the railroad to transport the mail was instituted in 1832, on one line in Pennsylvania. All railroads in the United States were designated as post routes, after passage of the Act of July 7, 1838. Mail service by railroad increased rapidly thereafter.
An Act of Congress provided for the issuance of stamps on March 3, 1847, and the Postmaster General immediately let a contract to the New York City engraving firm of Rawdon, Wright, Hatch, and Edson. The first stamp issue of the U.S. was offered for sale on July 1, 1847, in New York City, with Boston receiving stamps the following day and other cities thereafter. The 5-cent stamp paid for a letter weighing less than and traveling less than 300 miles, the 10-cent stamp for deliveries to locations greater than 300 miles, or twice the weight deliverable for the 5-cent stamp.
In 1847, the U.S. Mail Steamship Company acquired the contract which allowed it to carry the U.S. mails from New York, with stops in New Orleans and Havana, to the Isthmus of Panama for delivery in California. The same year, the Pacific Mail Steamship Company had acquired the right to transport mail under contract from the United States Government from the Isthmus of Panama to California. In 1855, William Henry Aspinwall completed the Panama Railway, providing rail service across the Isthmus and cutting to three weeks the transport time for the mails, passengers and goods to California. This remained an important route until the completion of the transcontinental railroad in 1869. Railroad companies greatly expanded mail transport service after 1862, and the Railway Mail Service was inaugurated in 1869.
Rail cars designed to sort and distribute mail while rolling were soon introduced. RMS employees sorted mail "on-the-fly" during the journey, and became some of the most skilled workers in the postal service. An RMS sorter had to be able to separate the mail quickly into compartments based on its final destination, before the first destination arrived, and work at the rate of 600 pieces of mail an hour. They were tested regularly for speed and accuracy.
Parcel Post service began with the introduction of International Parcel Post between the USA and foreign countries in 1887. That same year, the U.S. Post Office (predecessor of the USPS) and the Postmaster General of Canada established parcel-post service between the two nations. A bilateral parcel-post treaty between the independent (at the time) Kingdom of Hawaii and the USA was signed on 19 December 1888 and put into effect early in 1889. Parcel-post service between the USA and other countries grew with the signing of successive postal conventions and treaties. While the Post Office agreed to deliver parcels sent into the country under the UPU treaty, it did not institute a domestic parcel-post service for another twenty-five years.
20th century.
The advent of Rural Free Delivery (RFD) in the U.S. in 1896, and the inauguration of a domestic parcel post service by Postmaster General Frank H. Hitchcock in 1913, greatly increased the volume of mail shipped nationwide, and motivated the development of more efficient postal transportation systems. Many rural customers took advantage of inexpensive Parcel Post rates to order goods and products from businesses located hundreds of miles away in distant cities for delivery by mail. From the 1910s to the 1960s, many college students and others used parcel post to mail home dirty laundry, as doing so was less expensive than washing the clothes themselves.
After four-year-old Charlotte May Pierstorff was mailed from her parents to her grandparents in Idaho in 1914, mailing of people was prohibited. In 1917, the Post Office imposed a maximum daily mailable limit of two hundred pounds per customer per day after a business entrepreneur, W.H. Coltharp, used inexpensive parcel-post rates to ship more than eighty thousand masonry bricks some four hundred seven miles via horse-drawn wagon and train for the construction of a bank building in Vernal, Utah.
The advent of parcel post also led to the growth of Mail order businesses that substantially increased rural access to modern goods over what was typically stocked in local general stores.
In 1912, carrier service was announced for establishment in towns of second and third class with $100,000 appropriated by Congress. From January 1, 1911, until July 1, 1967, the United States Post Office Department operated the United States Postal Savings System. An Act of Congress of June 25, 1910, established the Postal Savings System in designated Post Offices, effective January 1, 1911. The legislation aimed to get money out of hiding, attract the savings of immigrants accustomed to the postal savings system in their native countries, provide safe depositories for people who had lost confidence in banks, and furnish more convenient depositories for working people. The law establishing the system directed the Post Office Department to redeposit most of the money in the system in local banks, where it earned 2.5 percent interest.
The system paid 2-percent interest per year on deposits. The half percent difference in interest was intended to pay for the operation of the system. Certificates were issued to depositors as proof of their deposit. Depositors in the system were initially limited to hold a balance of $500, but this was raised to $1,000 in 1916 and to $2,500 in 1918. The initial minimum deposit was $1. In order to save smaller amounts for deposit, customers could purchase a 10-cent postal savings card and 10-cent postal savings stamps to fill it. The card could be used to open or add to an account when its value, together with any attached stamps, amounted to one or more dollars, or it could be redeemed for cash. At its peak in 1947, the system held almost $3.4 billion in deposits, with more than four million depositors using 8,141 postal units.
The United States Postal Service played a role during World War I, enacting the Espionage and Trading with the Enemy Acts. Also monitoring foreign mail and acting as counter-espionage to help secure allied victory.
On August 12, 1918, the Post Office Department took over airmail service from the United States Army Air Service (USAAS). Assistant Postmaster General, Otto Praeger, appointed Benjamin B. Lipsner to head the civilian-operated Air Mail Service. One of Lipsner's first acts was to hire four pilots, each with at least 1,000 hours flying experience, paying them an average of $4,000 per year ($ today). The Post Office Department used new Standard JR-1B biplanes specially modified to carry the mail while the war was still in progress, but following the war operated mostly World War I surplus military de Havilland DH-4 aircraft.
During 1918, the Post Office hired an additional 36 pilots. In its first year of operation, the Post Office completed 1,208 airmail flights with 90 forced landings. Of those, 53 were due to weather and 37 to engine failure. By 1920, the Air Mail service had delivered 49 million letters. Domestic air mail became obsolete in 1975, and international air mail in 1995, when the USPS began transporting First-Class mail by air on a routine basis.
The Post Office was one of the first government departments to regulate obscene materials on a national basis. When the U.S. Congress passed the Comstock laws of 1873, it became illegal to send through the U.S. mail any material considered obscene or indecent, or which promoted abortion issues, birth control, or alcohol consumption.
On March 18, 1970, postal workers in New York City — upset over low wages and poor working conditions, and emboldened by the Civil Rights movement — organized a strike against the United States government. The strike initially involved postal workers in only New York City, but it eventually gained support of over 210,000 United States Post Office Department workers across the nation. While the strike ended without any concessions from the Federal government, it did ultimately allow for postal worker unions and the government to negotiate a contract which gave the unions most of what they wanted, as well as the signing of the Postal Reorganization Act by President Richard Nixon on August 12, 1970. The Act replaced the cabinet-level Post Office Department with the independent United States Postal Service, and took effect on July 1, 1971.
Current operations.
The United States Postal Service employs some 617,000 workers, making it the third-largest civilian employer in the United States behind the federal government and Wal-Mart. In a 2006 U.S. Supreme Court decision, the Court noted: "Each day, according to the Government's submissions here, the United States Postal Service delivers some 660 million pieces of mail to as many as 142 million delivery points." As of 2014, the USPS operates 31,000 post offices and locations in the U.S., and delivers 155 billion pieces of mail annually.
The USPS operates one of the largest civilian vehicle fleets in the world, with an estimated 211,264 vehicles, the majority of which are the easily identified Chevrolet/Grumman LLV (Long-Life Vehicle), and the newer Ford/Utilimaster FFV (Flex-Fuel Vehicle), originally also referred to as the "CRV" (Carrier Route Vehicle). It is by geography and volume the globe's largest postal system, delivering 40% of the world's mail. For every penny increase in the national average price of gasoline, the USPS spends an extra $8 million per year to fuel its fleet.
The number of gallons of fuel used in 2009 was 444 million, at a cost of billion. The fleet is notable in that many of its vehicles are right-hand drive, an arrangement intended to give drivers the easiest access to roadside mailboxes. Some Rural Letter Carriers use personal vehicles. Standard postal-owned vehicles do not have license plates. These vehicles are identified by a seven digit number displayed on the front and rear.
The Department of Defense and the USPS jointly operate a postal system to deliver mail for the military; this is known as the Army Post Office (for Army and Air Force postal facilities) and the Fleet Post Office (for Navy, Marine Corps and Coast Guard postal facilities).
In February 2013, the Postal Service announced that on Saturdays it would only deliver packages, mail-order medicines, Priority Mail, and Express Mail, effective August 10, 2013. However, this change was reversed by federal law in the Consolidated and Further Continuing Appropriations Act, 2013. They now deliver packages on Sunday for Amazon.com only. During the four weeks preceding Christmas in 2014 and 2015 packages from all mail classes and senders were delivered on Sunday in some areas.
Five-year plans.
In October 2008, the Postal Service released "Vision 2013", a five-year plan required by law starting in 1993. One planned improvement is the introduction of the Intelligent Mail Barcode, which will allow pieces of mail to be tracked through the delivery system, as competitors like UPS and FedEx currently do.
Initiatives.
In 2011, numerous media outlets reported that the USPS was going out of business. The USPS's strategy came under fire as new technologies emerged and the USPS was not finding ways to generate new sources of revenue.
Budget.
In 2014, the Postal Service collected $67.8 billion in revenue.
Revenue decline and planned cuts.
In 2012, the USPS had its fifth straight annual operating loss, in the amount of $15.7 billion, of which $11.1 billion was the accrual of unpaid mandatory retiree health payments.
Declining mail volume.
First Class mail volume peaked in 2001 and has declined 29% from 1998 to 2008, due to the increasing use of email and the World Wide Web for correspondence and business transactions.
FedEx and United Parcel Service (UPS) directly compete with USPS Express Mail and package delivery services, making nationwide deliveries of urgent letters and packages.
Lower volume means lower revenues to support the fixed commitment to deliver to every address once a day, six days a week. According to an official report on November 15, 2012, the U.S. Postal Service lost $15.9 billion its 2012 fiscal year.
Internal streamlining and delivery slowdown.
In response, the USPS has increased productivity each year from 2000 to 2007, through increased automation, route re-optimization, and facility consolidation. Despite these efforts, the organization saw an $8.5 billion budget shortfall in 2010, and was losing money at a rate of about $3 billion per quarter in 2011.
On December 5, 2011 the USPS announced it would close more than half of its mail processing centers, eliminate 28,000 jobs and reduce overnight delivery of First-Class Mail. This will close down 252 of its 461 processing centers. (At peak mail volume in 2006, the USPS operated 673 facilities.) As of May 2012, the plan was to start the first round of consolidation in summer 2012, pause from September to December, and begin a second round in February 2014; 80% of first class mail would still be delivered overnight through the end of 2013. New delivery standards were issued in January 2015, and the majority of single-piece (not presorted) first-class mail is now being delivered in two days instead of one. Large commercial mailers can still have first-class mail delivered overnight if delivered directly to a processing center in the early morning, though as of 2014 this represented only 11% of first-class mail. Unsorted first-class mail will continued to be delivered anywhere in the contiguous United States within three days.
Post office closures.
In July 2011, the USPS announced a plan to close about 3,700 small post offices. Various representatives in Congress protested, and the Senate passed a bill that would have kept open all post offices further than 10 miles from the next office. In May 2012, the service announced it had modified its plan. Instead, rural post offices would remain open with reduced retail hours (some as little as two hours per day) unless there was a community preference for a different option. In a survey of rural customers, 20% preferred the "Village Post Office" replacement (where a nearby private retail store would provide basic mail services with expanded hours), 15% preferred merger with another Post Office, and 11% preferred expanded rural delivery services. Approximately 40% of postal revenue already comes from online purchases or private retail partners including Walmart, Staples, Office Depot, Walgreens, Sam's Club, Costco, and grocery stores. The National Labor Relations Board agreed to hear the American Postal Workers Union's arguments that these counters should be manned by postal employees who earn far more and have "a generous package of health and retirement benefits".
Elimination of Saturday delivery averted.
On January 28, 2009, Postmaster General John E. Potter testified before the Senate that, if the Postal Service could not readjust its payment toward the contractually funding earned employee retiree health benefits, as mandated by the Postal Accountability & Enhancement Act of 2006, the USPS would be forced to consider cutting delivery to five days per week during June, July, and August.
H.R. 22, addressing this issue, passed the House of Representatives and Senate and was signed into law on September 30, 2009. However, Postmaster General Potter continued to advance plans to eliminate Saturday mail delivery.
On June 10, 2009, the National Rural Letter Carriers' Association (NRLCA) was contacted for its input on the USPS's current study of the impact of five-day delivery along with developing an implementation plan for a five-day service plan. A team of Postal Service headquarters executives and staff has been given a time frame of sixty days to complete the study. The current concept examines the impact of five-day delivery with no business or collections on Saturday, with Post Offices with current Saturday hours remaining open.
On Thursday, April 15, 2010, the House Committee on Oversight and Government Reform held a hearing to examine the status of the Postal Service and recent reports on short and long term strategies for the financial viability and stability of the USPS entitled "Continuing to Deliver: An Examination of the Postal Service's Current Financial Crisis and its Future Viability." At which, PMG Potter testified that by the year 2020, the USPS cumulative losses could exceed $238 billion, and that mail volume could drop 15 percent from 2009.
In February 2013, the USPS announced that in order to save about $2 billion per year, Saturday delivery service would be discontinued except for packages, mail-order medicines, Priority Mail, Express Mail, and mail delivered to Post Office boxes, beginning August 10, 2013. However the Consolidated and Further Continuing Appropriations Act, 2013, passed in March, reversed the cuts to Saturday delivery.
Retirement funding and payment defaults.
The Postal Accountability and Enhancement Act of 2006 (PAEA) obligates the USPS to fund the present value of earned retirement obligations (essentially past promises which have not yet come due) within a ten-year time span. In contrast, private businesses in the United States have no legal obligation to pay for retirement costs at promise-time rather than retirement-time, but about one quarter do.
The Office of Personnel Management (OPM) is the main bureaucratic organization responsible for the human resources aspect of many federal agencies and their employees. The PAEA created the Postal Service Retiree Health Benefit Fund (PSRHB) after Congress removed the Postal Service contribution to the Civil Service Retirement System (CSRS). Most other employees that contribute to the CSRS have 7% deducted from their wages.
On September 30, 2014, the USPS failed to make $5.7 billion payment on this debt, the fourth such defaulted payment.
Rate increases.
Congress has limited rate increases for First-Class Mail to the cost of inflation, unless approved by the Postal Regulatory Commission. A 3¢ surcharge above inflation increased the rate to 49¢ in January, 2014, but this was approved by the Commission for two years only.
Reform packages, delivery changes, and alcohol delivery.
Comprehensive reform packages considered in the 113th Congress include S.1486 and H.R.2748. These include the efficiency measure, supported by Postmaster General Patrick Donahoe of ending door-to-door delivery of mail for some or most of the 35 million addresses that currently receive it, replacing that with either curbside boxes or nearby "cluster boxes". This would save $4.5 billion per year out of the $30 billion delivery budget; door-to-door city delivery costs annually on average $353 per stop, curbside $224, and cluster box $160 (and for rural delivery, $278, $176, and $126, respectively).
S.1486, also with the support of Postmaster Donahoe, would also allow the USPS to ship alcohol in compliance with state law, from manufacturers to recipients with ID to show they are over 21. This is projected to raise approximately $50 million per year. (Shipping alcoholic beverages is currently illegal under (f).)
In 2014, the Postal Service was requesting reforms to worker's compensation, moving from a pension to defined contribution retirement savings plan, and paying senior retiree health care costs out of Medicare funds, as is done for private-sector workers.
Governance and organization.
The Board of Governors of the United States Postal Service sets policy, procedure, and postal rates for services rendered, and has a similar role to a corporate board of directors. Of the eleven members of the Board, nine are appointed by the President and confirmed by the United States Senate (see ). The nine appointed members then select the United States Postmaster General, who serves as the board's tenth member, and who oversees the day-to-day activities of the service as Chief Executive Officer (see ). The ten-member board then nominates a Deputy Postmaster General, who acts as Chief Operating Officer, to the eleventh and last remaining open seat.
The independent Postal Regulatory Commission (formerly the Postal Rate Commission) is also controlled by appointees of the President confirmed by the Senate. It oversees postal rates and related concerns, having the authority to approve or reject USPS proposals.
The USPS is often mistaken for a government-owned corporation (e.g., Amtrak) because it operates much like a business, but as noted above, it is legally defined as an "independent establishment of the executive branch of the Government of the United States", () as it is controlled by Presidential appointees and the Postmaster General. As a quasi-governmental agency, it has many special privileges, including sovereign immunity, eminent domain powers, powers to negotiate postal treaties with foreign nations, and an exclusive legal right to deliver first-class and third-class mail. Indeed, in 2004, the U.S. Supreme Court ruled in a unanimous decision that the USPS was not a government-owned corporation, and therefore could not be sued under the Sherman Antitrust Act.
The U.S. Supreme Court has also upheld the USPS's statutory monopoly on access to letter boxes against a First Amendment freedom of speech challenge; it thus remains illegal in the U.S. for "anyone", other than the employees and agents of the USPS, to deliver mailpieces to letter boxes marked "U.S. Mail."
The Postal Service also has a Mailers' Technical Advisory Committee and local Postal Customer Councils, which are advisory and primarily involve business customers.
Universal service obligation and monopoly status.
Article I, section 8, Clause 7 of the United States Constitution grants Congress the power to establish post offices and post roads, which has been interpreted as a de facto Congressional monopoly over the delivery of first class residential mail - which has been defined as non-urgent residential letters (not packages). Accordingly, no other system for delivering first class residential mail – public or private – has been tolerated, absent Congress's consent.
The mission of the Postal Service is to provide the American public with trusted universal postal service. While not explicitly defined, the Postal Service's universal service obligation (USO) is broadly outlined in statute and includes multiple dimensions: geographic scope, range of products, access to services and facilities, delivery frequency, affordable and uniform pricing, service quality, and security of the mail. While other carriers may claim to voluntarily provide delivery on a broad basis, the Postal Service is the only carrier with a "legal obligation" to provide all the various aspects of universal service.
Proponents of universal service principles claim that since any obligation must be matched by the financial capability to meet that obligation, the postal monopoly was put in place as a funding mechanism for the USO, and it has been in place for over a hundred years. It consists of two parts: the Private Express Statutes (PES) and the mailbox access rule. The PES refers to the Postal Service's monopoly on the delivery of letters, and the mailbox rule refers to the Postal Service's exclusive access to customer mailboxes.
Proponents of universal service principles further claim that eliminating or reducing the PES or mailbox rule would have an impact on the ability of the Postal Service to provide affordable universal service. If, for example, the PES and the mailbox rule were to be eliminated, and the USO maintained, then either billions of dollars in tax revenues or some other source of funding would have to be found.
Some proponents of universal service principles suggest that private communications that are protected by the veil of government promote the exchange of free ideas and communications. This separates private communications from the ability of a private for-profit or non-profit organization to corrupt. Security for the individual is in this way protected by the United States Post Office, maintaining confidentiality and anonymity, as well as government employees being much less likely to be instructed by superiors to engage in nefarious spying. It is seen by some as a dangerous step to extract the universal service principle from the post office, as the untainted nature of private communications is preserved as assurance of the protection of individual freedom of privacy.
However, as the recent notice of a termination of mail service to residents of the Frank Church—River of No Return Wilderness indicates, mail service has been contracted to private firms such as Arnold Aviation for many decades. KTVB-TV reported:
The Postal Regulatory Commission's 2008 report on universal postal service and the postal monopoly.
The Postal Act of 2006 required the Postal Regulatory Commission (PRC) to submit a report to the President and Congress on universal postal service and the postal monopoly in December 2008. The report must include any recommended changes. The Postal Service report supports the requirement that the PRC is to consult with and solicit written comments from the Postal Service. In addition, the Government Accountability Office is required to evaluate broader business model issues by 2011.
On October 15, 2008, the Postal Service submitted a report to the PRC on its position related to the Universal Service Obligation (USO). It said no changes to the USO and restriction on mailbox access were necessary at this time, but increased regulatory flexibility was required to ensure affordable universal service in the future. In 2013, the Postal Service announced that starting August 2013, Saturday delivery would be discontinued.
Obligations of the USO include uniform prices, quality of service, access to services, and six-day delivery to every part of the country. To assure financial support for these obligations, the postal monopoly provides the Postal Service the exclusive right to deliver letters and restricts mailbox access solely for mail. The report argued that eliminating or reducing either aspect of the monopoly "would have a devastating impact on the ability...to provide the affordable universal service that the country values so highly." Relaxing access to the mailbox would also pose security concerns, increase delivery costs, and hurt customer service, according to the Post Office. The report notes:
Most of these alternatives are not actually free in some communities. For example, in the Chicago metropolitan area and many other major metros one must get a background check from police and pay a daily fee for the right to solicit or post commercial messages on private property.
Regarding the monopoly on delivery of letters, the report notes that the monopoly is not complete, as there is an exception for letters where either "the amount paid for private carriage of the letter equals at least six times the current rate for the first ounce of a single-piece First-Class Mail letter (also known as the “base rate” or “base tariff”)" or "the letter weighs at least 12.5 ounces."
The Postal Service said that the USO should continue to be broadly defined and there should be no changes to the postal monopoly. Any changes would have far-reaching effects on customers and the trillion dollar mailing industry. "A more rigidly defined USO would ... ultimately harm the American public and businesses," according to the report, which cautions that any potential change must be studied carefully and the effects fully understood.
Competitors.
FedEx and United Parcel Service (UPS) directly compete with USPS Express Mail and package delivery services, making nationwide deliveries of urgent letters and packages. Due to the postal monopoly, they are not allowed to deliver non-urgent letters and may not directly ship to U.S. Mail boxes at residential and commercial destinations. However both companies have transit agreements with the USPS in which an item can be dropped off with either FedEx or UPS who will then provide shipment up to the destination post office serving the intended recipient where it will be transferred for delivery to the U.S. Mail destination, including Post Office Box destinations. These services also deliver packages which are larger and heavier than USPS will accept. DHL Express was the third major competitor until February 2009, when it ceased domestic delivery operations in the United States.
A variety of other transportation companies in the United States move cargo around the country, but either have limited geographic scope for delivery points, or specialize in items too large to be mailed. Many of the thousands of courier companies focus on same-day delivery, for example, by bicycle messenger.
Although USPS and FedEx are direct competitors, USPS contracts with FedEx for air transport of Priority and Express Priority Mail.
Alternative transmission methods.
The Post Office Department owned and operated the first public telegraph lines in the United States, starting in 1844 from Washington to Baltimore, and eventually extending to New York, Boston, Buffalo, and Philadelphia. In 1847 the telegraph system was privatized, except for a period during World War I, when it was used to accelerate the delivery of letters arriving at night.
Between 1942 and 1945 "V-Mail" (for "Victory Mail") service was available for military mail. Letters were converted into microfilm and reprinted near the destination, to save room on transport vehicles for military cargo.
From 1982 to 1985 Electronic Computer Originated Mail, known as E-COM was accepted for bulk mailings. Text was transmitted electronically to one of 25 post offices nationwide. The Postal Service would print the mail and put it in special envelopes bearing a blue E-COM logo. Delivery was assured within 2 days.
To improve accuracy and efficiency, the Postal Service introduced the Intelligent Mail program to complement the zip™ code system. This system, which was intended to replace the depreciated POSTNET system, allows bulk mailers to use pre-printed bar codes to assist in mail delivery and sorting. Additional features, called Enhanced, or Full-Service, Intelligent Mail Barcodes allow for mail tracking of bulk mail through the postal system up to the final delivery Post Office.
Criticism of the universal service requirement and the postal monopoly.
Critics of the universal service requirement and the statutory postal monopoly include several professional economists advocating for the privatization of the mail delivery system, or at least a relaxation of the universal service model that currently exists. Rick Geddes argued in 2000:
Furthermore, some economists have argued that because public enterprises may pursue objectives different than profit maximization, they might have more of an incentive than profit-maximizing firms to behave anticompetitively through policies such as predatory pricing, misstating costs, and creating barriers to entry. To resolve those issues, one economist proposes a cost-allocation model that would determine the optimal allocation of USPS's common costs by finding the share of costs that would maximize USPS profits from its competitive products. Postal regulators could use such a cost model to ensure that the Postal Service is not abusing its statutory monopoly by subsidizing price cuts in competitive product markets with revenue obtained from the monopolized market.
Law enforcement agencies.
Postal Inspection Service.
The United States Postal Inspection Service (USPIS) is one of the oldest law enforcement agencies in the U.S. Founded by Benjamin Franklin, its mission is to protect the Postal Service, its employees, and its customers from crime and protect the nation's mail system from criminal misuse.
Postal Inspectors enforce over 200 federal laws providing for the protection of mail in investigations of crimes that may adversely affect or fraudulently use the U.S. Mail, the postal system or postal employees.
The USPIS has the power to enforce the USPS monopoly by conducting search and seizure raids on entities they suspect of sending non-urgent mail through overnight delivery competitors. According to the American Enterprise Institute, a private conservative think tank, the USPIS raided Equifax offices in 1993 to ascertain if the mail they were sending through Federal Express was truly "extremely urgent." It was found that the mail was not, and Equifax was fined $30,000.
Lastly, the PIS oversees the activities of the Postal Police Force who patrol in and around selected high-risk postal facilities in major metropolitan areas in the United States and its territories.
Office of Inspector General.
The United States Postal Service Office of Inspector General (OIG) was authorized by law in 1996. Prior to the 1996 legislation, the Postal Inspection Service performed the duties of the OIG. The Inspector General, who is independent of postal management, is appointed by and reports directly to the nine presidentially appointed, Senate–confirmed members of the Board of Governors of the United States Postal Service.
The primary purpose of the OIG is to prevent, detect and report fraud, waste and program abuse, and promote efficiency in the operations of the Postal Service. The OIG has "oversight" responsibility for all activities of the Postal Inspection Service.
How delivery services work.
Elements of addressing and preparing domestic mail.
All mailable articles (e.g., letters, flats, machinable parcels, irregular parcels, etc.) shipped within the United States must comply with an array of standards published in the USPS Domestic Mail Manual (DMM). Before addressing the mailpiece, one must first comply with the various mailability standards relating to attributes of the actual mailpiece such as: minimum/maximum dimensions and weight, acceptable mailing containers, proper mailpiece sealing/closure, utilization of various markings, and restrictions relating to various hazardous (e.g., explosives, flammables, etc.) and restricted (e.g., cigarettes, smokeless tobacco, etc.) materials, as well as others articulated in § 601 of the DMM.
The USPS specifies the following key elements when preparing the face of a mailpiece:
Domestic First-Class Mail costs 47¢ for envelopes (35¢ for post cards) and upwards, depending on the weight and dimensions of the letter and the class.
Mail going to naval vessels is known as the Fleet Post Office (FPO) and to Army or Air Force installations use the city abbreviation APO (Army Post Office or Air Force Post Office).
Undeliverable mail that cannot be readily returned, including mail without return addresses, is treated as dead mail at a Mail Recovery Center in Atlanta, Georgia or Saint Paul, Minnesota.
The USPS maintains a list of proper abbreviations.
The format of a return address is similar. Though some style manuals recommend using a comma between the city and state name when typesetting addresses in other contexts, for optimal automatic character recognition, the Post Office does not recommend this when addressing mail. The official recommendation is to use all upper case block letters with proper formats and abbreviations, and leave out all punctuation except for the hyphen in the ZIP+4 code. If the address is unusually formatted or illegible enough, it will require hand-processing, delaying that particular item. The USPS publishes the entirety of their postal addressing standards.
Customers can look up ZIP codes and verify addresses using USPS Web Tools available on the official USPS website and Facebook page, as well as on third-party sites.
Paying postage.
The actual postage can be paid via:
All unused U.S. postage stamps issued since 1861 are still valid as postage at their indicated value. Stamps with no value shown or denominated by a letter are also still valid, although the value depends upon the particular stamp. For some stamps issued without a printed value, the current value is the original value. But some stamps beginning in 1988 or earlier, including "Forever Stamps" that were issued beginning in April 2007, and all 1st class mail 1st ounce stamps beginning 2011-01-21, the value is the current value of a 1st class mail 1st ounce stamps. (The USPS calls these "Forever Stamps". The generic name is non-denominated postage.)
Forever stamps are sold at the First-Class Mail postage rate at the time of purchase, but will always be valid for First-Class Mail (1 oz and under), no matter how rates rise in the future. Britain has had a similar stamp since 1989. The cost of mailing a First-Class letter increased to 49 cents on January 26, 2014.
Postage meters.
A postage meter is a mechanical device used to create and apply physical evidence of postage (or franking) to mailed matter. Postage meters are regulated by a country's postal authority; for example, in the United States, the United States Postal Service specifies the rules for the creation, support, and use of postage meters. A postage meter imprints an amount of postage, functioning as a postage stamp, a cancellation and a dated postmark all in one. The meter stamp serves as proof of payment and eliminates the need for adhesive stamps.
PC postage.
In addition to using standard stamps, postage can now be printed in the form of an electronic stamp, or e-stamp, from a personal computer using a system called Information Based Indicia. This online PC Postage method relies upon application software on the customer's computer contacting a postal security device at the office of the postal service.
Other electronic postage payment methods.
Electronic Verification System (eVS) is the Postal Service's integrated mail management technology that centralizes payment processing and electronic postage reports. Part of an evolving suite of USPS electronic payment services called PostalOne!, eVS allows mailers shipping large volumes of parcels through the Postal Service a way to circumvent use of hard-copy manifests, postage statements and drop-shipment verification forms. Instead, mailers can pay postage automatically through a centralized account and track payments online.
Beginning in August 2007, the Postal Service began requiring mailers shipping Parcel Select packages using a permit imprint to use eVS for manifesting their packages.
Stamp copyright and reproduction.
All U.S. postage stamps issued under the former United States Post Office Department and other postage items that were released before 1978 are not subject to copyright, but stamp designs since 1978 are copyrighted. The United States Copyright Office in section 313.6(C)(1) of the Third Edition of the Compendium of U.S. Copyright Office Practices holds that "Works prepared by officers or employees of the U.S. Postal Service... are not considered works of the U.S. Government" and are therefore eligible for registration. Thus, the USPS holds copyright to such materials released since 1978 under Title 17 of the United States Code. Written permission is required for use of copyrighted postage stamp images, although under USPS rules, permission is "generally" not required for "educational use", "news reporting" or "philatelic advertising use," but users must cite USPS as the source of the image and include language such as "© United States Postal Service. All rights reserved."
Service level choices.
General domestic services.
As of April 2011, domestic postage levels for low-volume mailers include:
The Post Office will not deliver packages heavier than or if the length (the package's longest dimension) plus the girth (the measurement around the package at its largest point in the two shorter dimensions) is greater than combined ( for Parcel Post).
Bulk mail.
Discounts are available for large volumes of mail. Depending on the postage level, certain conditions might be required or optional for an additional discount:
In addition to bulk discounts on Express, Priority, and First-Class Mail, the following postage levels are available for bulk mailers:
Extra services.
Depending on the type of mail, additional services are available for an additional fee:
International services.
In May 2007, the USPS restructured international service names to correspond with domestic shipping options. Formerly, USPS International services were categorized as Airmail (Letter Post), Economy (Surface) Parcel Post, Airmail Parcel Post, Global Priority, Global Express, and Global Express Guaranteed Mail. The former Airmail (Letter Post) is now First-Class Mail International, and includes small packages weighing up to four pounds (1.8 kg). Economy Parcel Post was discontinued for international service, while Airmail Parcel Post was replaced by Priority Mail International. Priority Mail International Flat-Rate packaging in various sizes was introduced, with the same conditions of service previously used for Global Priority. Global Express is now Express Mail International, while Global Express Guaranteed is unchanged. The international mailing classes with a tracking ability are Express, Express Guaranteed, and Priority (except that tracking is not available for Priority Mail International Flat Rate Envelopes or Priority Mail International Small Flat Rate Boxes).
One of the major changes in the new naming and services definitions is that USPS-supplied mailing boxes for Priority and Express mail are now allowed for international use. These services are offered to ship letters and packages to almost every country and territory on the globe. The USPS provides much of this service by contracting with a private parcel service, FedEx.
On May 14, 2007, the USPS canceled all outgoing international surface mail (sometimes known as "sea mail") from the United States, citing increased costs and reduced demand due to competition from airmail services such as FedEx and UPS. The decision has been criticized by the Peace Corps and military personnel overseas, as well as independent booksellers and other small businesses who rely on international deliveries.
The USPS provides an service for international shipment of printed matter; previously surface M-bags existed, but with the 2007 elimination of surface mail, only airmail M-bags remain. The term "M-bag" is not expanded in USPS publications; M-bags are simply defined as "direct sacks of printed matter ... sent to a single foreign addressee at a single address"; however, the term is sometimes referred to informally as "media bag", as the bag can also contain "discs, tapes, and cassettes", in addition to books, for which the usual umbrella term is "media"; some also refer to them as "mail bags".
Military mail is billed at domestic rates when being sent from the United States to a military outpost, and is free when sent by deployed military personnel. The overseas logistics are handled by the Military Postal Service Agency in the Department of Defense. Outside of forward areas and active operations, military mail First-Class takes 7–10 days, Priority 10–15 days, and Parcel Post about 24 days.
Three independent countries with a Compact of Free Association with the U.S. (Palau, the Marshall Islands, and the Federated States of Micronesia) have a special relationship with the United States Postal Service:
Sorting and delivery process.
Processing of standard sized envelopes and cards is highly automated, including reading of handwritten addresses. Mail from individual customers and public postboxes is collected by mail carriers into plastic tubs, which are taken to one of approximately 251 Processing and Distribution Centers (P&DC) across the United States. Each P&DC sort mails for a given region (typically with a radius of around ) and connects with the national network for interregional mail.
At the P&DC, mail is emptied into hampers which are then automatically dumped into a Dual Pass Rough Cull System (DPRCS). As mail travels through the DPRCS, large items, such as packages and mail bundles, are removed from the stream. As the remaining mail enters the first machine for processing standard mail, the Advanced Facer-Canceler System (AFCS), pieces that passed through the DPRCS but do not conform to physical dimensions for processing in the AFCS (e.g., large envelopes or overstuffed standard envelopes) are automatically diverted from the stream. Mail removed from the DPRCS and AFCS is manually processed or sent to parcel sorting machines.
In contrast to the previous system, which merely canceled and postmarked the upper right corner of the envelope, thereby missing any stamps which were inappropriately placed, the Advanced Facer-Canceler System locates indicia (stamp or metered postage mark), regardless of the orientation of the mail as it enters the machine, and cancels it by applying a postmark. Detection of indicia enables the AFCS to determine the orientation of each mailpiece and sort it accordingly, rotating pieces as necessary so all mail is sorted right-side up and faced in the same direction in each output bin.
Mail is output by the machine into three categories: mail already affixed with a bar code and addressed (such as business reply envelopes and cards); mail with machine printed (typed) addresses; and mail with handwritten addresses. Additionally, machines with a recent Optical Character Recognition (OCR) upgrade have the capability to read the address information, including handwritten, and sort the mail based on local or outgoing ZIP codes.
Mail with typed addresses goes to a Multiline Optical Character Reader (MLOCR) which reads the ZIP Code and address information and prints the appropriate bar code onto the envelope. Mail (actually the scanned image of the mail) with handwritten addresses (and machine-printed ones that are not easily recognized) goes to the Remote Bar Coding System. It also corrects spelling errors and, where there is an error, omission, or conflict in the written address, identifies the most likely correct address.
When it has decided on a correct address, it prints the appropriate bar code onto the envelopes, similarly to the MLOCR system. RBCS also has facilities in place, called Remote Encoding Centers, that have humans look at images of mail pieces and enter the address data. The address data is associated with the image via an ID Tag, a fluorescent barcode printed by mail processing equipment on the back of mail pieces.
Processed mail is imaged by the Mail Isolation Control and Tracking (MICT) system to allow easier tracking of hazardous substances. Images are taken at more than 200 mail processing centers, and are destroyed after being retained for 30 days.
If a customer has filed a change of address card and his or her mail is detected in the mailstream with the old address, the mailpiece is sent to a machine that automatically connects to a Computerized Forwarding System database to determine the new address. If this address is found, the machine will paste a label over the former address with the current address. The mail is returned to the mailstream to forward to the new location.
Mail with addresses that cannot be resolved by the automated system are separated for human intervention. If a local postal worker can read the address, he or she manually sorts it out according to the ZIP code on the article. If the address cannot be read, mail is either returned to the sender (First-Class Mail with a valid return address) or is sent to the Mail Recovery Center in Atlanta, Georgia (formerly known as Dead Letter Offices, originated by Benjamin Franklin in the 1770s) where it receives more intense scrutiny, including being opened to determine if any of the contents are a clue. If no valid address can be determined, the items are held for 90 days in case of inquiry by the customer; and if they are not claimed then they are either destroyed or auctioned off at the monthly Postal Service Unclaimed Parcel auction to raise money for the service.
Once the mail is bar coded, it is automatically sorted by a Delivery Bar Code System that reads the bar code and determines the destination of the mailpiece to postal stations.
Regional mail is trucked to the appropriate local post office or kept in the building for carrier routes served directly from the P&DC. Out-of-region mail is trucked to the airport and then flown, usually as baggage on commercial airlines, to the airport nearest the destination station. At the destination P&DC, mail is once again read by a DBCS which sorts the items into their local destinations, including grouping them by individual mail carrier.
At the carrier route level, 95% of letters arrive pre-sorted; the remaining mail must be sorted by hand. The Post Office is working to increase the percentage of automatically sorted mail, including a pilot program to sort "flats".
FedEx provides the air transport service for Priority and Express Mail. Priority Mail and Express Mail are transported from Priority Mail processing centers to the airport where they are handed off to FedEx. FedEx then flies them to the destination airport where they are handed off back to the postal service for final transport to the local post office and delivery.
Types of postal facilities.
Although its customer service centers are called post offices in regular speech, the USPS recognizes several types of postal facilities, including the following:
While common usage refers to all types of postal facilities as "substations", the USPS Glossary of Postal Terms does not define or even list that word. Post Offices often share facilities with other governmental organizations located within a city's central business district. In those locations, often Courthouses and Federal Buildings, the building is owned by the General Services Administration while the U.S. Postal Services operates as a tenant. The USPS retail system has approximately 36,000 post offices, stations, and branches. Temporary stations are also set up for applying pictorial cancellations.
Automated Postal Centers.
In 2004 the USPS began deploying Automated Postal Centers (APC). APCs are unattended kiosks that are capable of weighing, franking, and storing packages for later pickup as well as selling domestic and international postage stamps. Since its introduction, APCs do not take cash payments - they only accept credit or debit cards. Similarly, traditional vending machines are available at many post offices to purchase stamps, though these are being phased out in many areas. Due to increasing use of Internet services, as of June 2009, no retail post office windows are open 24 hours; overnight services are limited to those provided by an Automated Postal Center.
Evolutionary Network Development (END) program.
In February 2006, the USPS announced that they plan to replace the nine existing facility-types with five processing facility-types:
Over a period of years, these facilities are expected to replace Processing & Distribution Centers, Customer Service Facilities, Bulk Mail Centers, Logistic and Distribution Centers, annexes, the Hub and Spoke Program, Air Mail Centers, and International Service Centers.
The changes are a result of the declining volumes of single-piece First-Class Mail, population shifts, the increase in drop shipments by advertising mailers at destinating postal facilities, advancements in equipment and technology, redundancies in the existing network, and the need for operational flexibility.
Airline and rail division.
The United States Postal Service does not directly own or operate any aircraft or trains, although both were formerly operated. The mail and packages are flown on airlines with which the Postal Service has a contractual agreement. The contracts change periodically. Depending on the contract, aircraft may be painted with the USPS paint scheme. Contract airlines have included: UPS, Emery Worldwide, Ryan International Airlines, FedEx Express, American Airlines, United Airlines, and Express One International. The Postal Service also contracts with Amtrak to carry some mail between certain cities such as Chicago and Minneapolis – Saint Paul.
The last air delivery route in the continental U.S., to residents in the Frank Church—River of No Return Wilderness, was scheduled to be ended in June 2009. The weekly bush plane route, contracted out to an air taxi company, had in its final year an annual cost of $46,000, or $2400/year per residence, over ten times the average cost of delivering mail to a residence in the United States. This decision has been reversed by the U.S. Postmaster General.
Parcel forwarding and private interchange.
Private US parcel forwarding or US mail forwarding companies focusing on personal shopper, relocation, Ex-pat and mail box services often interface with the United States Postal Service for transporting of mail and packages for their customers.
Delivery timing.
Delivery days.
From 1810, mail was delivered seven days a week. In 1828, local religious leaders noticed a decline in Sunday-morning church attendance because of local post offices' doubling as gathering places. These leaders appealed to the government to intervene and close post offices on Sundays. The government, however, declined, and mail was delivered 7 days a week until 1912.
Today, U.S. Mail (with the exception of Express Mail) is not delivered on Sunday, except in a few towns in which the local religion has had an effect on the policy, such as Loma Linda, California, which has a significant Seventh-day Adventist population and where U.S. Mail is delivered Sunday through Friday, with the exception of observed federal holidays.
Saturday delivery was temporarily suspended in April 1957, because of lack of funds, but quickly restored.
Budget problems prompted consideration of dropping Saturday delivery starting around 2009. This culminated in a 2013 announcement that regular mail services would be cut to five days a week, which was reversed by Congress before it could take effect. (See the section Revenue decline and planned cuts.)
Direct delivery vs. customer pickup.
Originally, mail was not delivered to homes and businesses, but to post offices. In 1863, "city delivery" began in urban areas with enough customers to make this economical. This required streets to be named, houses to be numbered, with sidewalks and lighting provided, and these street addresses to be added to envelopes. The number of routes served expanded over time. In 1891, the first experiments with Rural Free Delivery began in less densely populated areas. There is currently an effort to reduce direct delivery in favor of mailbox clusters.
To compensate for high mail volume and slow long-distance transportation which saw mail arrive at post offices throughout the day, deliveries were made multiple times a day. This ranged from twice for residential areas to up to seven times for the central business district of Brooklyn, New York. In the late 19th century, mail boxes were encouraged, saving carriers the time it took to deliver directly to the addressee in person; in the 1910s and 1920s, they were phased in as a requirement for service. In the 1940s, multiple daily deliveries began to be reduced, especially on Saturdays. By 1990, the last twice-daily deliveries in New York City were eliminated.
Today, mail is delivered once a day on-site to most private homes and businesses. The USPS still distinguishes between city delivery (where carriers generally walk and deliver to mailboxes hung on exterior walls or porches, or to commercial reception areas) and rural delivery (where carriers generally drive). With "curbside delivery", mailboxes are at the ends of driveways, on the nearest convenient road. "Central point delivery" is used in some locations, where several nearby residences share a "cluster" of individual mailboxes in a single housing.
Some customers choose to use post office boxes for an additional fee, for privacy or convenience. This provides a locked box at the post office to which mail is addressed and delivered (usually earlier in the day than home delivery). Customers in less densely populated areas where there is no city delivery and who do not qualify for rural delivery may receive mail only through post office boxes. High-volume business customers can also arrange for special pick-up.
Another option is the old-style general delivery, for people who have neither post office boxes nor street addresses. Mail is held at the post office until they present identification and pick it up.
Some customers receive free post office boxes if the USPS declines to provide door-to-door delivery to their location or a nearby box. People with medical problems can request door-to-door delivery. Homeless people are also eligible for post office boxes at the discretion of the local postmaster, or can use general delivery.
Special delivery.
From 1885 to 1997, a service called special delivery was available, which caused a separate delivery to the final location earlier in the day than the usual daily rounds.
Same-day trials.
In December 2012, the USPS began a limited one-year trial of same-day deliveries directly from retailers or distribution hubs to residential addresses in the same local area, a service it dubbed "Metro Post". The trial was initially limited to San Francisco and the only retailer to participate in the first few weeks was 1-800-FLOWERS.
In March 2013, the USPS faced new same-day competition for e-commerce deliveries from Google Shopping Express.
In November 2013, the Postal Service began regular package delivery on Sundays for Amazon customers in New York and Los Angeles, which it expanded to 15 cities in May 2014. Amazon Sunday delivery has now been expanded to most major markets as of September, 2015.
Other competition in this area includes online grocers such as AmazonFresh, Webvan, and delivery services operated by grocery stores like Peapod and Safeway.
Forwarding and holds.
Residential customers can fill out a form to forward mail to a new address, and can also send pre-printed forms to any of their frequent correspondents. They can also put their mail on "hold", for example, while on vacation. The Post Office will store mail during the hold, instead of letting it overflow in the mailbox. These services are not available to large buildings and customers of a commercial mail receiving agency, where mail is subsorted by non-Post Office employees into individual mailboxes.
Financial services.
Postal money orders provide a safe alternative to sending cash through the mail, and are available in any amount up to $1000. Like a bank cheque, money orders are cashable only by the recipient. Unlike a personal bank check, they are prepaid and therefore cannot be returned because of insufficient funds. Money orders are a declining business for the USPS, as companies like PayPal, PaidByCash and others are offering electronic replacements.
From 1911 to 1967, the Postal Service also operated the United States Postal Savings System, not unlike a savings and loan association with the amount of the deposit limited.
A January 2014 report by the Inspector General of the USPS suggested that the agency could earn $8.9 billion per year in revenue by providing financial services, especially in areas where there are no local banks but there is a local post office, and to customers who currently do not have bank accounts.
Employment in the USPS.
The Postal Service is the nation's second-largest civilian employer. , it employed 574,000 personnel, divided into offices, processing centers, and actual post offices. The United States Postal Service would rank 29th on the 2010 Fortune 500 list, if considered a private company.
Labor unions representing USPS employees include: The American Postal Workers Union (APWU), which represents postal clerks and maintenance, motor vehicle, mail equipment shops, material distribution centers, and operating services and facilities services employees, postal nurses, and IT and accounting; the National Association of Letter Carriers (NALC), which represents city letter carriers; the National Rural Letter Carriers' Association (NRLCA), which represents rural letter carriers; and the National Postal Mail Handlers Union (NPMHU).
USPS employees are divided into three major crafts according to the work they engage in:
Other non-managerial positions in the USPS include:
Though the USPS employs many individuals, as more Americans send information via email, fewer postal workers are needed to work dwindling amounts of mail. Post offices and mail facilities are constantly downsizing, replacing craft positions with new machines and consolidating mail routes through the MIARAP (Modified Interim Alternate Route Adjustment Process) agreement. A major round of job cuts, early retirements, and a construction freeze were announced on March 20, 2009.
Workplace violence.
In the early 1990s, widely publicized workplace shootings by disgruntled employees at USPS facilities led to a Human Resource effort to provide care for stressed workers and resources for coworker conflicts. Due to media coverage, postal employees gained a reputation among the general public as more likely to be mentally ill. The USPS Commission on a Safe and Secure Workplace found that "Postal workers are only a third as likely as those in the national workforce to be victims of homicide at work." In the documentary Murder by Proxy: How America Went Postal, it was argued that this number failed to factor out workers killed by external subjects rather than by fellow employees.
These series of events in turn has influenced American culture, as seen in the slang term "going postal" (see Patrick Sherrill for information on his August 20, 1986, rampage) and the computer game "Postal". Also, in the opening sequence of "", a yell of "Disgruntled postal workers" is heard, followed by the arrival of postal workers with machine guns. In an episode of "Seinfeld", the mailman character Newman, explained in a dramatic monologue that postal workers "go crazy and kill everyone" because the mail never stops. In "The Simpsons" episode "Sunday, Cruddy Sunday," Nelson Muntz asks Postmaster Bill if he has "ever gone on a killing spree"; Bill replies, "The day of the gun-toting, disgruntled postman shooting up the place went out with the Macarena".
The series of massacres led the US Postal Service to issue a rule prohibiting the possession of any type of firearms (except for those issued to Postal Inspectors) in all designated USPS facilities.
In 2016, a video footage was released showing a group of police officers from the New York City Police Department (NYPD) arresting a US Postal Service officer while he was in the middle of his deliveries. The footage showed that the officers were dressed in civilian clothing. The NYPD is reportedly investigating alleged disorderly conduct.
See also.
Unions of the U.S. Postal Service:
History:
International associations:
Workplace violence

</doc>
<doc id="50592" url="https://en.wikipedia.org/wiki?curid=50592" title="Hyperlexia">
Hyperlexia

Hyperlexia is a syndrome characterized by a child's precocious ability to read, combined with difficulty in understanding and using verbal language, and problems with social interactions. It was initially identified by Norman E. Silberberg and Margaret C. Silberberg (1967), who defined it as the precocious ability to read words without prior training in learning to read typically before the age of 5. They indicated that children with hyperlexia have a significantly higher word-decoding ability than their reading comprehension levels.
Hyperlexic children are characterized by having average or above-average IQs, and word-reading ability well above what would be expected given their age. First named and scientifically described in 1967 (Silverberg and Silverberg), it can be viewed as a superability in which word recognition ability goes far above expected levels of skill. Some hyperlexics, however, have trouble understanding speech. Some experts believe that most or perhaps all children with hyperlexia lie on the autism spectrum. However, one expert, Darold Treffert, proposes that hyperlexia has subtypes, only some of which overlap with autism. Between 5 and 10 percent of children with autism have been estimated to be hyperlexic.
Hyperlexic children are often fascinated by letters or numbers. They are extremely good at decoding language and thus often become very early readers. Some hyperlexic children learn to spell long words (such as "elephant") before they are two years old and learn to read whole sentences before they turn three.
An fMRI study of a single child showed that hyperlexia may be the neurological opposite of dyslexia.
Etymology.
The word hyperlexia is derived from the Greek terms "hyper" ("over") and "léxis" ("diction", "word").
Development.
Despite hyperlexic children's precocious reading ability, they may struggle to communicate. Often, hyperlexic children will have a precocious ability to read but will learn to speak only by rote and heavy repetition, and may also have difficulty learning the rules of language from examples or from trial and error, which may result in social problems. Their language may develop using echolalia, often repeating words and sentences. Often, the child has a large vocabulary and can identify many objects and pictures, but cannot put their language skills to good use. Spontaneous language is lacking and their pragmatic speech is delayed. Hyperlexic children often struggle with Who? What? Where? Why? and How? questions. Between the ages of 4 and 5 years old, many children make great strides in communicating.
The social skills of a child with hyperlexia often lag tremendously. Hyperlexic children often have far less interest in playing with other children than do their peers.
Types of hyperlexia.
In one paper, Darold Treffert proposes three types of hyperlexia. Specifically:
A different paper by Rebecca Williamson Brown, OD proposes only two types of hyperlexia. These are:
Non-English Studies.
In studies in Cantonese and Korean, subjects were able to read non-words in their native orthography without a delay relative to the speed with which they read real words in their native orthography. There is a delay noted with exception words in English, including the examples: chaos, unique, and enough. These studies also illustrate difficulties in understanding what it is that they are reading. The findings suggest that non-hyperlexic readers rely more heavily on word semantics in order to make inferences about word meaning. 
In the Cantonese study, distinguish homographs and determine the readings for rarely used characters. In this study, the subject also made errors of phonetic analogy and regularization of sound. The authors of the study, Weekes, Wong, Iao, To, and Su, suggest that the two routes model for reading Chinese characters may be in effect for hyperlexics. The two routes model describes understanding of Chinese characters in a purely phonetic sense and the understanding of Chinese characters in a semantic sense.
The semantics deficit is also illustrated in the study of Korean hyperlexics through a priming experiment. Non-hyperlexic children read words primed with a related image faster than non-primed words while hyperlexics read them at the same pace. Lee Sunghee and Hwang Mina, the authors of the Korean study, also found that hyperlexics have fewer errors in non-word reading than non-hyperlexics. They suggest that this may be because of an imbalance in the phonological, orthographical, and semantic understandings of the subjects’ native language and writing system, in this case, Hangul. This combination of the parts of linguistics is known as connectionist theory, in which non-words are distinguished from words by differences in interaction between phonology, orthography, and semantics.
In the Lee and Hwang study, the subjects scored lower on general language test and vocabulary tests than the average for their age groups. Literacy education in South Korea involves teaching students entire words, rather than starting with the relationship between phonemes and letters in Hangul, despite evidence that letter name knowledge is useful for learning to read words that have not been taught. The results suggest that hyperlexics are able to obtain the relations between letters (or the smallest unit of the writing system) and their phonemes without knowing the names.
Comprehension difficulties can also be a result of hyperlexia. Semantics and comprehension both have ties to meaning. Semantics relates to the meaning of a certain word while comprehension is the understanding of a longer text. In both studies, interpretation and meaning based tests proved difficult for the hyperlexic subjects. In the Weeks study, the subject was unable to identify characters based on the logographic aspect of the writing system, and in the Lee and Hwang study, priming was ineffective in decreasing reading times for hyperlexics. 
Acquisition.
Although it is generally associated with autism, a 69-year-old woman appears to have been made hyperlexic because of a "cerebral infarction in the left anterior cingulate cortex and corpus callosum".

</doc>
<doc id="50593" url="https://en.wikipedia.org/wiki?curid=50593" title="BNF">
BNF

BNF may stand for:
In computer science:
In science:
In politics:
Other uses:

</doc>
<doc id="50595" url="https://en.wikipedia.org/wiki?curid=50595" title="Flash memory">
Flash memory

Flash memory is an electronic (solid-state) non-volatile computer storage medium that can be electrically erased and reprogrammed.
Toshiba developed flash memory from EEPROM (electrically erasable programmable read-only memory) in the early 1980s and introduced it to the market in 1984. The two main types of flash memory are named after the NAND and NOR logic gates. The individual flash memory cells exhibit internal characteristics similar to those of the corresponding gates.
Whereas EPROMs had to be completely erased before being rewritten, NAND-type flash memory may be written and read in blocks (or pages) which are generally much smaller than the entire device. NOR-type flash allows a single machine word (byte) to be writtento an erased locationor read independently.
The NAND type operates primarily in memory cards, USB flash drives, solid-state drives (those produced in 2009 or later), and similar products, for general storage and transfer of data. NAND or NOR flash memory is also often used to store configuration data in numerous digital products, a task previously made possible by EEPROM or battery-powered static RAM. One key disadvantage of flash memory is that it can endure relatively small number of write cycles in a specific block.
Example applications of both types of flash memory include personal computers, PDAs, digital audio players, digital cameras, mobile phones, synthesizers, video games, scientific instrumentation, industrial robotics, and medical electronics. In addition to being non-volatile, flash memory offers fast read access times, although not as fast as static RAM or ROM. Its mechanical shock resistance helps explain its popularity over hard disks in portable devices, as does its high durability, ability to withstand high pressure, temperature and immersion in water, etc.
Although flash memory is technically a type of EEPROM, the term "EEPROM" is generally used to refer specifically to non-flash EEPROM which is erasable in small blocks, typically bytes. Because erase cycles are slow, the large block sizes used in flash memory erasing give it a significant speed advantage over non-flash EEPROM when writing large amounts of data. , flash memory cost much less than byte-programmable EEPROM and had become the dominant memory type wherever a system required a significant amount of non-volatile solid-state storage.
History.
Flash memory (both NOR and NAND types) was invented by Fujio Masuoka while working for Toshiba circa 1980. According to Toshiba, the name "flash" was suggested by Masuoka's colleague, Shōji Ariizumi, because the erasure process of the memory contents reminded him of the flash of a camera. Masuoka and colleagues presented the invention at the "IEEE 1984 International Electron Devices Meeting" (IEDM) held in San Francisco.
Intel Corporation saw the massive potential of the invention and introduced the first commercial NOR type flash chip in 1988. NOR-based flash has long erase and write times, but provides full address and data buses, allowing random access to any memory location. This makes it a suitable replacement for older read-only memory (ROM) chips, which are used to store program code that rarely needs to be updated, such as a computer's BIOS or the firmware of set-top boxes. Its endurance may be from as little as 100 erase cycles for an on-chip flash memory, to a more typical 10,000 or 100,000 erase cycles, up to 1,000,000 erase cycles. NOR-based flash was the basis of early flash-based removable media; CompactFlash was originally based on it, though later cards moved to less expensive NAND flash.
NAND flash has reduced erase and write times, and requires less chip area per cell, thus allowing greater storage density and lower cost per bit than NOR flash; it also has up to 10 times the endurance of NOR flash. However, the I/O interface of NAND flash does not provide a random-access external address bus. Rather, data must be read on a block-wise basis, with typical block sizes of hundreds to thousands of bits. This makes NAND flash unsuitable as a drop-in replacement for program ROM, since most microprocessors and microcontrollers require byte-level random access. In this regard, NAND flash is similar to other secondary data storage devices, such as hard disks and optical media, and is thus, highly suitable for use in mass-storage devices, such as memory cards. The first NAND-based removable media format was SmartMedia in 1995, and many others have followed, including:
A new generation of memory card formats, including RS-MMC, miniSD and microSD, feature extremely small form factors. For example, the microSD card has an area of just over 1.5 cm2, with a thickness of less than 1 mm. microSD capacities range from 64 MB to 200 GB, as of March 2015.
Principles of operation.
Flash memory stores information in an array of memory cells made from floating-gate transistors. In single-level cell (SLC) devices, each cell stores only one bit of information. In multi-level cell (MLC) devices, including triple-level cell (TLC) devices, can store more than one bit per cell.
The floating gate may be conductive (typically polysilicon in most kinds of flash memory) or non-conductive (as in SONOS flash memory).
Floating-gate transistor.
In flash memory, each memory cell resembles a standard MOSFET, except that the transistor has two gates instead of one. On top is the control gate (CG), as in other MOS transistors, but below this there is a floating gate (FG) insulated all around by an oxide layer. The FG is interposed between the CG and the MOSFET channel. Because the FG is electrically isolated by its insulating layer, electrons placed on it are trapped until they are removed by another application of electric field (e.g. Applied voltage or UV as in EPROM). Counter-intuitively, placing electrons on the FG sets the transistor to the logical "0" state. Once the FG is charged, the electrons in it screen (partially cancel) the electric field from the CG, thus, increasing the threshold voltage (VT1) of the cell. This means that now a higher voltage(VT2) must be applied to the CG to make the channel conductive. In order to read a value from the transistor, an intermediate voltage between the threshold voltages (VT1 & VT2) is applied to the CG. If the channel conducts at this intermediate voltage, the FG must be uncharged (if it were charged, we would not get conduction because the intermediate voltage is less than VT2), and hence, a logical "1" is stored in the gate. If the channel does not conduct at the intermediate voltage, it indicates that the FG is charged, and hence, a logical "0" is stored in the gate. The presence of a logical "0" or "1" is sensed by determining whether there is current flowing through the transistor when the intermediate voltage is asserted on the CG. In a multi-level cell device, which stores more than one bit per cell, the amount of current flow is sensed (rather than simply its presence or absence), in order to determine more precisely the level of charge on the FG.
Internal charge pumps.
Despite the need for high programming and erasing voltages, virtually all flash chips today require only a single supply voltage, and produce the high voltages using on-chip charge pumps.
Over half the energy used by a 1.8 V NAND flash chip is lost in the charge pump itself. Since boost converters are inherently more efficient than charge pumps, researchers developing low-power SSDs have proposed returning to the dual Vcc/Vpp supply voltages used on all the early flash chips, driving the high Vpp voltage for all flash chips in a SSD with a single shared external boost converter.
In spacecraft and other high-radiation environments, the on-chip charge pump is the first part of the flash chip to fail, although flash memories will continue to work—in read-only mode—at much higher radiation levels.
NOR flash.
In NOR flash, each cell has one end connected directly to ground, and the other end connected directly to a bit line.
This arrangement is called "NOR flash" because it acts like a NOR gate: when one of the word lines (connected to the cell's CG) is brought high, the corresponding storage transistor acts to pull the output bit line low. NOR flash continues to be the technology of choice for embedded applications requiring a discrete non-volatile memory device. The low read latencies characteristic of NOR devices allow for both direct code execution and data storage in a single memory product.
Programming.
A single-level NOR flash cell in its default state is logically equivalent to a binary "1" value, because current will flow through the channel under application of an appropriate voltage to the control gate, so that the bitline voltage is pulled down. A NOR flash cell can be programmed, or set to a binary "0" value, by the following procedure:
Erasing.
To erase a NOR flash cell (resetting it to the "1" state), a large voltage "of the opposite polarity" is applied between the CG and source terminal, pulling the electrons off the FG through quantum tunneling. Modern NOR flash memory chips are divided into erase segments (often called blocks or sectors). The erase operation can be performed only on a block-wise basis; all the cells in an erase segment must be erased together. Programming of NOR cells, however, generally can be performed one byte or word at a time.
NAND flash.
NAND flash also uses floating-gate transistors, but they are connected in a way that resembles a NAND gate: several transistors are connected in series, and the bit line is pulled low only if all the word lines are pulled high (above the transistors' VT). These groups are then connected via some additional transistors to a NOR-style bit line array in the same way that single transistors are linked in NOR flash.
Compared to NOR flash, replacing single transistors with serial-linked groups adds an extra level of addressing. Whereas NOR flash might address memory by page then word, NAND flash might address it by page, word and bit. Bit-level addressing suits bit-serial applications (such as hard disk emulation), which access only one bit at a time. Execute-in-place applications, on the other hand, require every bit in a word to be accessed simultaneously. This requires word-level addressing. In any case, both bit and word addressing modes are possible with either NOR or NAND flash.
To read data, first the desired group is selected (in the same way that a single transistor is selected from a NOR array). Next, most of the word lines are pulled up above the VT of a programmed bit, while one of them is pulled up to just over the VT of an erased bit. The series group will conduct (and pull the bit line low) if the selected bit has not been programmed.
Despite the additional transistors, the reduction in ground wires and bit lines allows a denser layout and greater storage capacity per chip. (The ground wires and bit lines are actually much wider than the lines in the diagrams.) In addition, NAND flash is typically permitted to contain a certain number of faults (NOR flash, as is used for a BIOS ROM, is expected to be fault-free). Manufacturers try to maximize the amount of usable storage by shrinking the size of the transistors.
Writing and erasing.
NAND flash uses tunnel injection for writing and tunnel release for erasing. NAND flash memory forms the core of the removable USB storage devices known as USB flash drives, as well as most memory card formats and solid-state drives available today.
Vertical NAND.
Vertical NAND (V-NAND) memory stacks memory cells vertically and uses a charge trap flash architecture. The vertical layers allow larger areal bit densities without requiring smaller individual cells.
Structure.
V-NAND uses a charge trap flash geometry (pioneered in 2002 by AMD) that stores charge on an embedded silicon nitride film. Such a film is more robust against point defects and can be made thicker to hold larger numbers of electrons. V-NAND wraps a planar charge trap cell into a cylindrical form.
An individual memory cell is made up of one planar polysilicon layer containing a hole filled by multiple concentric vertical cylinders. The hole's polysilicon surface acts as the gate electrode. The outermost silicon dioxide cylinder acts as the gate dielectric, enclosing a silicon nitride cylinder that stores charge, in turn enclosing a silicon dioxide cylinder as the tunnel dielectric that surrounds a central rod of conducting polysilicon which acts as the conducting channel.
Memory cells in different vertical layers do not interfere with each other, as the charges cannot move vertically through the silicon nitride storage medium, and the electric fields associated with the gates are closely confined within each layer. The vertical collection is electrically identical to the serial-linked groups in which conventional NAND flash memory is configured.
Construction.
Growth of a group of V-NAND cells begins with an alternating stack of conducting (doped) polysilicon layers and insulating silicon dioxide layers.
The next step is to form a cylindrical hole through these layers. In practice, a 128 Gibit V-NAND chip with 24 layers of memory cells requires about 2.9 billion such holes. Next the hole's inner surface receives multiple coatings, first silicon dioxide, then silicon nitride, then a second layer of silicon dioxide. Finally, the hole is filled with conducting (doped) polysilicon.
Performance.
As of 2013, V-NAND flash architecture allows read and write operations twice as fast as conventional NAND and can last up to 10 times as long, while consuming 50 percent less power. They offer comparable physical bit density using 10-nm lithography, but may be able to increase bit density by up to two orders of magnitude.
Limitations.
Block erasure.
One limitation of flash memory is that, although it can be read or programmed a byte or a word at a time in a random access fashion, it can be erased only a block at a time. This generally sets all bits in the block to 1. Starting with a freshly erased block, any location within that block can be programmed. However, once a bit has been set to 0, only by erasing the entire block can it be changed back to 1. In other words, flash memory (specifically NOR flash) offers random-access read and programming operations, but does not offer arbitrary random-access rewrite or erase operations. A location can, however, be rewritten as long as the new value's 0 bits are a superset of the over-written values. For example, a nibble value may be erased to 1111, then written as 1110. Successive writes to that nibble can change it to 1010, then 0010, and finally 0000. Essentially, erasure sets all bits to 1, and programming can only clear bits to 0. File systems designed for flash devices can make use of this capability, for example, to represent sector metadata.
Although data structures in flash memory cannot be updated in completely general ways, this allows members to be "removed" by marking them as invalid. This technique may need to be modified for multi-level cell devices, where one memory cell holds more than one bit.
Common flash devices such as USB flash drives and memory cards provide only a block-level interface, or flash translation layer (FTL), which writes to a different cell each time to wear-level the device. This prevents incremental writing within a block; however, it does not help the device from being prematurely worn out by intensive write patterns.
Memory wear.
Another limitation is that flash memory has a finite number of program–erase cycles (typically written as P/E cycles). Most commercially available flash products are guaranteed to withstand around 100,000 P/E cycles before the wear begins to deteriorate the integrity of the storage. Micron Technology and Sun Microsystems announced an SLC NAND flash memory chip rated for 1,000,000 P/E cycles on 17 December 2008.
The guaranteed cycle count may apply only to block zero (as is the case with TSOP NAND devices), or to all blocks (as in NOR). This effect is mitigated in some chip firmware or file system drivers by counting the writes and dynamically remapping blocks in order to spread write operations between sectors; this technique is called wear leveling. Another approach is to perform write verification and remapping to spare sectors in case of write failure, a technique called bad block management (BBM). For portable consumer devices, these wearout management techniques typically extend the life of the flash memory beyond the life of the device itself, and some data loss may be acceptable in these applications. For high reliability data storage, however, it is not advisable to use flash memory that would have to go through a large number of programming cycles. This limitation is meaningless for 'read-only' applications such as thin clients and routers, which are programmed only once or at most a few times during their lifetimes.
In December 2012, Taiwanese engineers from Macronix revealed their intention to announce at the 2012 IEEE International Electron Devices Meeting that it has figured out how to improve NAND flash storage read/write cycles from 10,000 to 100 million cycles using a “self-healing” process that uses a flash chip with “onboard heaters that could anneal small groups of memory cells.” The built-in thermal annealing replaces the usual erase cycle with a local high temperature process that not only erases the stored charge, but also repairs the electron-induced stress in the chip, giving write cycles of at least 100 million. The result is a chip that can be erased and rewritten over and over, even when it should theoretically break down. As promising as Macronix’s breakthrough could be for the mobile industry, however, there are no plans for a commercial product to be released any time in the near future.
Read disturb.
The method used to read NAND flash memory can cause nearby cells in the same memory block to change over time (become programmed). This is known as read disturb. The threshold number of reads is generally in the hundreds of thousands of reads between intervening erase operations. If reading continually from one cell, that cell will not fail but rather one of the surrounding cells on a subsequent read. To avoid the read disturb problem the flash controller will typically count the total number of reads to a block since the last erase. When the count exceeds a target limit, the affected block is copied over to a new block, erased, then released to the block pool. The original block is as good as new after the erase. If the flash controller does not intervene in time, however, a read disturb error will occur with possible data loss if the errors are too numerous to correct with an error-correcting code.
X-ray effects.
Most flash ICs come in ball grid array (BGA) packages, and even the ones that do not are often mounted on a PCB next to other BGA packages.
After PCB Assembly, boards with BGA packages are often X-rayed to see if the balls are making proper connections to the proper pad, or if the BGA needs rework.
These X-rays can erase programmed bits in a flash chip (convert programmed "0" bits into erased "1" bits).
Erased bits ("1" bits) are not affected by X-rays.
Some manufacturers are now making X-ray proof SD and USB memory devices.
Low-level access.
The low-level interface to flash memory chips differs from those of other memory types such as DRAM, ROM, and EEPROM, which support bit-alterability (both zero to one and one to zero) and random access via externally accessible address buses.
NOR memory has an external address bus for reading and programming. For NOR memory, reading and programming are random-access, and unlocking and erasing are block-wise. For NAND memory, reading and programming are page-wise, and unlocking and erasing are block-wise.
NOR memories.
Reading from NOR flash is similar to reading from random-access memory, provided the address and data bus are mapped correctly. Because of this, most microprocessors can use NOR flash memory as execute in place (XIP) memory, meaning that programs stored in NOR flash can be executed directly from the NOR flash without needing to be copied into RAM first. NOR flash may be programmed in a random-access manner similar to reading. Programming changes bits from a logical one to a zero. Bits that are already zero are left unchanged. Erasure must happen a block at a time, and resets all the bits in the erased block back to one. Typical block sizes are 64, 128, or 256 KiB.
Bad block management is a relatively new feature in NOR chips. In older NOR devices not supporting bad block management, the software or device driver controlling the memory chip must correct for blocks that wear out, or the device will cease to work reliably.
The specific commands used to lock, unlock, program, or erase NOR memories differ for each manufacturer. To avoid needing unique driver software for every device made, special Common Flash Memory Interface (CFI) commands allow the device to identify itself and its critical operating parameters.
Besides its use as random-access ROM, NOR flash can also be used as a storage device, by taking advantage of random-access programming. Some devices offer read-while-write functionality so that code continues to execute even while a program or erase operation is occurring in the background. For sequential data writes, NOR flash chips typically have slow write speeds, compared with NAND flash.
Typical NOR flash does not need an error correcting code.
NAND memories.
NAND flash architecture was introduced by Toshiba in 1989. These memories are accessed much like block devices, such as hard disks. Each block consists of a number of pages. The pages are typically 512 or 2,048 or 4,096 bytes in size. Associated with each page are a few bytes (typically 1/32 of the data size) that can be used for storage of an error correcting code (ECC) checksum.
Typical block sizes include:
While reading and programming is performed on a page basis, erasure can only be performed on a block basis.
NAND devices also require bad block management by the device driver software, or by a separate controller chip. SD cards, for example, include controller circuitry to perform bad block management and wear leveling. When a logical block is accessed by high-level software, it is mapped to a physical block by the device driver or controller. A number of blocks on the flash chip may be set aside for storing mapping tables to deal with bad blocks, or the system may simply check each block at power-up to create a bad block map in RAM. The overall memory capacity gradually shrinks as more blocks are marked as bad.
NAND relies on ECC to compensate for bits that may spontaneously fail during normal device operation. A typical ECC will correct a one-bit error in each 2048 bits (256 bytes) using 22 bits of ECC, or a one-bit error in each 4096 bits (512 bytes) using 24 bits of ECC. If the ECC cannot correct the error during read, it may still detect the error. When doing erase or program operations, the device can detect blocks that fail to program or erase and mark them bad. The data is then written to a different, good block, and the bad block map is updated.
Hamming codes are the most commonly used ECC for SLC NAND flash.
Reed-Solomon codes and Bose-Chaudhuri-Hocquenghem codes are commonly used ECC for MLC NAND flash.
Some MLC NAND flash chips internally generate the appropriate BCH error correction codes.
Most NAND devices are shipped from the factory with some bad blocks. These are typically marked according to a specified bad block marking strategy. By allowing some bad blocks, the manufacturers achieve far higher yields than would be possible if all blocks had to be verified good. This significantly reduces NAND flash costs and only slightly decreases the storage capacity of the parts.
When executing software from NAND memories, virtual memory strategies are often used: memory contents must first be paged or copied into memory-mapped RAM and executed there (leading to the common combination of NAND + RAM). A memory management unit (MMU) in the system is helpful, but this can also be accomplished with overlays. For this reason, some systems will use a combination of NOR and NAND memories, where a smaller NOR memory is used as software ROM and a larger NAND memory is partitioned with a file system for use as a non-volatile data storage area.
NAND sacrifices the random-access and execute-in-place advantages of NOR. NAND is best suited to systems requiring high capacity data storage. It offers higher densities, larger capacities, and lower cost. It has faster erases, sequential writes, and sequential reads.
Standardization.
A group called the Open NAND Flash Interface Working Group (ONFI) has developed a standardized low-level interface for NAND flash chips. This allows interoperability between conforming NAND devices from different vendors. The ONFI specification version 1.0 was released on 28 December 2006. It specifies:
The ONFI group is supported by major NAND flash manufacturers, including Hynix, Intel, Micron Technology, and Numonyx, as well as by major manufacturers of devices incorporating NAND flash chips.
One major flash device manufacturer, Toshiba, has chosen to use an interface of their own design known as Toggle Mode (and now Toggle V2.0). This interface isn't pin-to-pin compatible with the ONFI specification. The result is a product designed for one vendor's devices may not be able to use another vendor's devices.
A group of vendors, including Intel, Dell, and Microsoft, formed a Non-Volatile Memory Host Controller Interface (NVMHCI) Working Group. The goal of the group is to provide standard software and hardware programming interfaces for nonvolatile memory subsystems, including the "flash cache" device connected to the PCI Express bus.
Distinction between NOR and NAND flash.
NOR and NAND flash differ in two important ways:
These two are linked by the design choices made in the development of NAND flash. A goal of NAND flash development was to reduce the chip area required to implement a given capacity of flash memory, and thereby to reduce cost per bit and increase maximum chip capacity so that flash memory could compete with magnetic storage devices like hard disks.
NOR and NAND flash get their names from the structure of the interconnections between memory cells. In NOR flash, cells are connected in parallel to the bit lines, allowing cells to be read and programmed individually. The parallel connection of cells resembles the parallel connection of transistors in a CMOS NOR gate. In NAND flash, cells are connected in series, resembling a NAND gate. The series connections consume less space than parallel ones, reducing the cost of NAND flash. It does not, by itself, prevent NAND cells from being read and programmed individually.
Each NOR flash cell is larger than a NAND flash cell — 10 F2 vs 4 F2 — even when using exactly the same semiconductor device fabrication and so each transistor, contact, etc. is exactly the same size—because NOR flash cells require a separate metal contact for each cell.
When NOR flash was developed, it was envisioned as a more economical and conveniently rewritable ROM than contemporary EPROM and EEPROM memories. Thus random-access reading circuitry was necessary. However, it was expected that NOR flash ROM would be read much more often than written, so the write circuitry included was fairly slow and could erase only in a block-wise fashion. On the other hand, applications that use flash as a replacement for disk drives do not require word-level write address, which would only add to the complexity and cost unnecessarily.
Because of the series connection and removal of wordline contacts, a large grid of NAND flash memory cells will occupy perhaps only 60% of the area of equivalent NOR cells (assuming the same CMOS process resolution, for example, 130 nm, 90 nm, or 65 nm). NAND flash's designers realized that the area of a NAND chip, and thus the cost, could be further reduced by removing the external address and data bus circuitry. Instead, external devices could communicate with NAND flash via sequential-accessed command and data registers, which would internally retrieve and output the necessary data. This design choice made random-access of NAND flash memory impossible, but the goal of NAND flash was to replace mechanical hard disks, not to replace ROMs.
Write endurance.
The write endurance of SLC floating-gate NOR flash is typically equal to or greater than that of NAND flash, while MLC NOR and NAND flash have similar endurance capabilities. Examples of endurance cycle ratings listed in datasheets for NAND and NOR flash are provided. 
However, by applying certain algorithms and design paradigms such as wear leveling and memory over-provisioning, the endurance of a storage system can be tuned to serve specific requirements.
Computation of NAND flash memory endurance is a challenging subject that depends on SLC/MLC/TLC memory type as well as the use pattern. In order to compute the longevity of the NAND flash, one must account for the size of the memory chip, the type of memory (e.g. SLC/MLC/TLC), and use pattern.
Flash file systems.
Because of the particular characteristics of flash memory, it is best used with either a controller to perform wear leveling and error correction or specifically designed flash file systems, which spread writes over the media and deal with the long erase times of NOR flash blocks. The basic concept behind flash file systems is the following: when the flash store is to be updated, the file system will write a new copy of the changed data to a fresh block, remap the file pointers, then erase the old block later when it has time.
In practice, flash file systems are used only for memory technology devices (MTDs), which are embedded flash memories that do not have a controller. Removable flash memory cards and USB flash drives have built-in controllers to perform wear leveling and error correction so use of a specific flash file system does not add any benefit.
Capacity.
Multiple chips are often arrayed to achieve higher capacities for use in consumer electronic devices such as multimedia players or GPSs. The capacity of flash chips generally follows Moore's Law because they are manufactured with many of the same integrated circuits techniques and equipment.
Consumer flash storage devices typically are advertised with usable sizes expressed as a small integer power of two (2, 4, 8, etc.) and a designation of megabytes (MB) or gigabytes (GB); e.g., 512 MB, 8 GB. This includes SSDs marketed as hard drive replacements, in accordance with traditional hard drives, which use decimal prefixes. Thus, an SSD marked as "64 GB" is at least 64 × 1,0003 bytes (64 GB). Most users will have slightly less capacity than this available for their files, due to the space taken by file system metadata.
The flash memory chips inside them are sized in strict binary multiples, but the actual total capacity of the chips is not usable at the drive interface.
It is considerably larger than the advertised capacity in order to allow for distribution of writes (wear leveling), for sparing, for error correction codes, and for other metadata needed by the device's internal firmware.
In 2005, Toshiba and SanDisk developed a NAND flash chip capable of storing 1 GB of data using multi-level cell (MLC) technology, capable of storing two bits of data per cell. In September 2005, Samsung Electronics announced that it had developed the world’s first 2 GB chip.
In March 2006, Samsung announced flash hard drives with a capacity of 4 GB, essentially the same order of magnitude as smaller laptop hard drives, and in September 2006, Samsung announced an 8 GB chip produced using a 40 nm manufacturing process.
In January 2008, SanDisk announced availability of their 16 GB MicroSDHC and 32 GB SDHC Plus cards.
More recent flash drives (as of 2012) have much greater capacities, holding 64, 128, and 256 GB.
A joint development at Intel and Micron will allow the production of 32 layer 3.5 terabyte (TB) NAND flash sticks and 10 TB standard-sized SSDs. The device includes 5 packages of 16 x 48 GB TLC dies, using a floating gate cell design.
Flash chips continue to be manufactured with capacities under or around 1 MB, e.g., for BIOS-ROMs and embedded applications.
Transfer rates.
Flash memory devices are typically much faster at reading than writing. Performance also depends on the quality of storage controllers which become more critical when devices are partially full. Even when the only change to manufacturing is die-shrink, the absence of an appropriate controller can result in degraded speeds.
Applications.
Serial flash.
Serial flash is a small, low-power flash memory that uses a serial interface, typically Serial Peripheral Interface Bus (SPI), for sequential data access. When incorporated into an embedded system, serial flash requires fewer wires on the PCB than parallel flash memories, since it transmits and receives data one bit at a time. This may permit a reduction in board space, power consumption, and total system cost.
There are several reasons why a serial device, with fewer external pins than a parallel device, can significantly reduce overall cost:
There are two major SPI flash types. The first type is characterized by small pages and one or more internal SRAM page buffers allowing a complete page to be read to the buffer, partially modified, and then written back (for example, the Atmel AT45 "DataFlash" or the Micron Technology Page Erase NOR Flash). The second type has larger sectors. The smallest sectors typically found in an SPI flash are 4 kB, but they can be as large as 64 kB. Since the SPI flash lacks an internal SRAM buffer, the complete page must be read out and modified before being written back, making it slow to manage. "SPI flash" is cheaper than "DataFlash" and is therefore a good choice when the application is code shadowing.
The two types are not easily exchangeable, since they do not have the same pinout, and the command sets are incompatible.
Firmware storage.
With the increasing speed of modern CPUs, parallel flash devices are often much slower than the memory bus of the computer they are connected to. Conversely, modern SRAM offers access times below 10 ns, while DDR2 SDRAM offers access times below 20 ns. Because of this, it is often desirable to shadow code stored in flash into RAM; that is, the code is copied from flash into RAM before execution, so that the CPU may access it at full speed. Device firmware may be stored in a serial flash device, and then copied into SDRAM or SRAM when the device is powered-up. Using an external serial flash device rather than on-chip flash removes the need for significant process compromise (a process that is good for high-speed logic is generally not good for flash and vice versa). Once it is decided to read the firmware in as one big block it is common to add compression to allow a smaller flash chip to be used. Typical applications for serial flash include storing firmware for hard drives, Ethernet controllers, DSL modems, wireless network devices, etc.
Flash memory as a replacement for hard drives.
One more recent application for flash memory is as a replacement for hard disks. Flash memory does not have the mechanical limitations and latencies of hard drives, so a solid-state drive (SSD) is attractive when considering speed, noise, power consumption, and reliability. Flash drives are gaining traction as mobile device secondary storage devices; they are also used as substitutes for hard drives in high-performance desktop computers and some servers with RAID and SAN architectures.
There remain some aspects of flash-based SSDs that make them unattractive. The cost per gigabyte of flash memory remains significantly higher than that of hard disks. Also flash memory has a finite number of P/E cycles, but this seems to be currently under control since warranties on flash-based SSDs are approaching those of current hard drives. In addition, deleted files on SSDs can remain for an indefinite period of time before being overwritten by fresh data; erasure or shred techniques or software that work well on magnetic hard disk drives have no effect on SSDs, compromising security and forensic examination.
For relational databases or other systems that require ACID transactions, even a modest amount of flash storage can offer vast speedups over arrays of disk drives.
In June 2006, Samsung Electronics released the first flash-memory based PCs, the Q1-SSD and Q30-SSD, both of which used 32 GB SSDs, and were at least initially available only in South Korea.
A solid-state drive was offered as an option with the first Macbook Air introduced in 2008, and from 2010 onwards, all Macbook Air laptops shipped with an SSD. Starting in late 2011, as part of Intel's Ultrabook initiative, an increasing number of ultra thin laptops are being shipped with SSDs standard.
There are also hybrid techniques such as hybrid drive and ReadyBoost that attempt to combine the advantages of both technologies, using flash as a high-speed non-volatile cache for files on the disk that are often referenced, but rarely modified, such as application and operating system executable files.
Flash memory as RAM.
As of 2012, there are attempts to use flash memory as the main computer memory, DRAM.
Archival or long-term storage.
It is unclear how long flash memory will persist under archival conditions—i.e., benign temperature and humidity with infrequent access with or without prophylactic rewrite. Anecdotal evidence suggests that the technology is reasonably robust on the scale of years.
An article from CMU in 2015 writes that "Today's flash devices, which do not require flash refresh, have a typical retention age of 1 year at room temperature." And that temperature can lower the retention time exponentially. The phenomenon can be modeled by Arrhenius law.
Industry.
One source states that, in 2008, the flash memory industry includes about US$9.1 billion in production and sales. Other sources put the flash memory market at a size of more than US$20 billion in 2006, accounting for more than eight percent of the overall semiconductor market and more than 34 percent of the total semiconductor memory market.
In 2012, the market was estimated at $26.8 billion.
Flash scalability.
Due to its relatively simple structure and high demand for higher capacity, NAND flash memory is the most aggressively scaled technology among electronic devices. The heavy competition among the top few manufacturers only adds to the aggressiveness in shrinking the design rule or process technology node. While the expected shrink timeline is a factor of two every three years per original version of Moore's law, this has recently been accelerated in the case of NAND flash to a factor of two every two years. 
As the feature size of flash memory cells reaches the minimum limit, further flash density increases will be driven by greater levels of MLC, possibly 3-D stacking of transistors, and improvements to the manufacturing process. The decrease in endurance and increase in uncorrectable bit error rates that accompany feature size shrinking can be compensated by improved error correction mechanisms. Even with these advances, it may be impossible to economically scale flash to smaller and smaller dimensions as the number of electron holding capacity reduces. Many promising new technologies (such as FeRAM, MRAM, PMC, PCM, ReRAM, and others) are under investigation and development as possible more scalable replacements for flash.

</doc>
<doc id="50597" url="https://en.wikipedia.org/wiki?curid=50597" title="EEPROM">
EEPROM

EEPROM (also written E2PROM and pronounced "e-e-prom", "double-e-prom" or "e-squared-prom") stands for Electrically Erasable Programmable Read-Only Memory and is a type of non-volatile memory used in computers and other electronic devices to store relatively small amounts of data but allowing individual bytes to be erased and reprogrammed.
EEPROMs are organized as arrays of floating-gate transistors. EEPROMs can be programmed and erased in-circuit, by applying special programming signals. Originally, EEPROMs were limited to single byte operations which made them slower, but modern EEPROMs allow multi-byte page operations. It also has a limited life for erasing and reprogramming, now reaching a million operations in modern EEPROMs. In an EEPROM that is frequently reprogrammed while the computer is in use, the life of the EEPROM is an important design consideration.
Unlike most other kinds of non-volatile memory, an EEPROM typically allows bytes to be read, erased, and re-written individually. EPROMs are erased by exposing a chip to ultra-violet light to erase its entire contents. Flash EPROMs are electrically erased and programmed but only as groups of bytes, ranging from tens to tens of thousands of bytes for different devices.
History.
Eli Harari at Hughes Aircraft invented the EEPROM in 1977 utilising Fowler-Nordheim tunneling through a thin floating gate. Hughes went on to produce the first EEPROM devices.
In 1978, George Perlegos at Intel developed the Intel 2816, which was built on earlier EPROM technology, but used a thin gate oxide layer enabling the chip to erase its own bits without a UV source. Perlegos and others later left Intel to form Seeq Technology, which used on-device charge pumps to supply the high voltages necessary for programming EEPROMs.
Electrical Interface.
EEPROM devices use a serial or parallel interface for data input/output.
Serial bus devices.
The common serial interfaces are SPI, I²C, Microwire, UNI/O, and 1-Wire. These use from 1 to 4 device pins and allow devices to use packages with 8-pins or less.
A typical EEPROM serial protocol consists of three phases: OP-Code Phase, Address Phase and Data Phase. The OP-Code is usually the first 8-bits input to the serial input pin of the EEPROM device (or with most I²C devices, is implicit); followed by 8 to 24 bits of addressing depending on the depth of the device, then the read or write data.
Each EEPROM device typically has its own set of OP-Code instructions mapped to different functions. Common operations on SPI EEPROM devices are:
Other operations supported by some EEPROM devices are:
Parallel bus devices.
Parallel EEPROM devices typically have an 8-bit data bus and an address bus wide enough to cover the complete memory. Most devices have chip select and write protect pins. Some microcontrollers also have integrated parallel EEPROM.
Operation of a parallel EEPROM is simple and fast when compared to serial EEPROM, but these devices are larger due to the higher pin count (28 pins or more) and have been decreasing in popularity in favor of serial EEPROM or Flash.
Other devices.
EEPROM memory is used to enable features in other types of products that are not strictly memory products. Products such as real-time clocks, digital potentiometers, digital temperature sensors, among others, may have small amounts of EEPROM to store calibration information or other data that needs to be available in the event of power loss.
It was also used on video game cartridges to save game progress and configurations, before the usage of external and internal flash memories.
Failure modes.
There are two limitations of stored information; endurance, and data retention.
During rewrites, the gate oxide in the floating-gate transistors gradually accumulates trapped electrons. The electric field of the trapped electrons adds to the electrons in the floating gate, lowering the window between threshold voltages for zeros vs ones. After sufficient number of rewrite cycles, the difference becomes too small to be recognizable, the cell is stuck in programmed state, and endurance failure occurs. The manufacturers usually specify the maximum number of rewrites being 1 million or more.
During storage, the electrons injected into the floating gate may drift through the insulator, especially at increased temperature, and cause charge loss, reverting the cell into erased state. The manufacturers usually guarantee data retention of 10 years or more.
Related types.
Flash memory is a later form of EEPROM. In the industry, there is a convention to reserve the term EEPROM to byte-wise erasable memories compared to block-wise erasable flash memories. EEPROM occupies more die area than flash memory for the same capacity, because each cell usually needs a read, a write, and an erase transistor, while flash memory erase circuits are shared by large blocks of cells (often 512×8).
Newer non-volatile memory technologies such as FeRAM and MRAM are slowly replacing EEPROMs in some applications, but are expected to remain a small fraction of the EEPROM market for the foreseeable future.
Comparison with EPROM and EEPROM/Flash.
The difference between EPROM and EEPROM lies in the way that the memory programs and erases. EEPROM can be programmed and erased electrically using field electron emission (more commonly known in the industry as "Fowler–Nordheim tunneling").
EPROMs can't be erased electrically, and are programmed via hot carrier injection onto the floating gate. Erase is via an ultraviolet light source, although in practice many EPROMs are encapsulated in plastic that is opaque to UV light, making them "one-time programmable".
Most NOR Flash memory is a hybrid style—programming is through hot carrier injection and erase is through Fowler–Nordheim tunneling.
In Popular Culture.
The Stanford Graduate Students in Electrical Engineering (GSEE) has annually hosted a dance (i.e. prom) called EEPROM since 2012. 

</doc>
<doc id="50601" url="https://en.wikipedia.org/wiki?curid=50601" title="Cystic fibrosis">
Cystic fibrosis

Cystic fibrosis (CF) is a genetic disorder that affects mostly the lungs but also the pancreas, liver, kidneys, and intestine. Long-term issues include difficulty breathing and coughing up mucus as a result of frequent lung infections. --> Other signs and symptoms include sinus infections, poor growth, fatty stool, clubbing of the fingers and toes, and infertility in males, among others. --> Different people may have different degrees of symptoms.
CF is inherited in an autosomal recessive manner. --> It is caused by the presence of mutations in both copies of the gene for the cystic fibrosis transmembrane conductance regulator (CFTR) protein. Those with a single working copy are carriers and otherwise mostly normal. CFTR is involved in production of sweat, digestive fluids, and mucus. When CFTR is not functional, secretions which are usually thin instead become thick. The condition is diagnosed by a sweat test and genetic testing. Screening of infants at birth takes place in some areas of the world.
There is no cure for cystic fibrosis. Lung infections are treated with antibiotics which may be given intravenously, inhaled, or by mouth. --> Sometimes the antibiotic azithromycin is used long term. --> Inhaled hypertonic saline and salbutamol may also be useful. --> Lung transplantation may be an option if lung function continues to worsen. --> Pancreatic enzyme replacement and fat-soluble vitamin supplementation are important, especially in the young. --> While not well supported by evidence, many people use airway clearance techniques such as chest physiotherapy. The average life expectancy is between 42 and 50 years in the developed world. Lung problems are responsible for death in 80% of people with cystic fibrosis.
CF is most common among people of Northern European ancestry and affects about one out of every 3,000 newborns. About one in 25 people are carriers. It is least common in Africans and Asians. It was first recognized as a specific disease by Dorothy Andersen in 1938, with descriptions that fit the condition occurring at least as far back as 1595. The name "cystic fibrosis" refers to the characteristic fibrosis and cysts that form within the pancreas.
Signs and symptoms.
The main signs and symptoms of cystic fibrosis are salty-tasting skin, poor growth, and poor weight gain despite normal food intake, accumulation of thick, sticky mucus, frequent chest infections, and coughing or shortness of breath. Males can be infertile due to congenital absence of the vas deferens. Symptoms often appear in infancy and childhood, such as bowel obstruction due to meconium ileus in newborn babies. As the children grow, they exercise to release mucus in the alveoli. Ciliated epithelial cells in the person have a mutated protein that leads to abnormally viscous mucus production. The poor growth in children typically presents as an inability to gain weight or height at the same rate as their peers and is occasionally not diagnosed until investigation is initiated for poor growth. The causes of growth failure are multifactorial and include chronic lung infection, poor absorption of nutrients through the gastrointestinal tract, and increased metabolic demand due to chronic illness.
In rare cases, cystic fibrosis can manifest itself as a coagulation disorder. Vitamin K is normally absorbed from breast milk, formula, and later, solid foods. This absorption is impaired in some cystic fibrosis patients. Young children are especially sensitive to vitamin K malabsorptive disorders because only a very small amount of vitamin K crosses the placenta, leaving the child with very low reserves and limited ability to absorb vitamin K from dietary sources after birth. Because factors II, VII, IX, and X (clotting factors) are vitamin K–dependent, low levels of vitamin K can result in coagulation problems. Consequently, when a child presents with unexplained bruising, a coagulation evaluation may be warranted to determine whether there is an underlying disease.
Lungs and sinuses.
Lung disease results from clogging of the airways due to mucus build-up, decreased mucociliary clearance, and resulting inflammation. Inflammation and infection cause injury and structural changes to the lungs, leading to a variety of symptoms. In the early stages, incessant coughing, copious phlegm production, and decreased ability to exercise are common. Many of these symptoms occur when bacteria that normally inhabit the thick mucus grow out of control and cause pneumonia.
In later stages, changes in the architecture of the lung, such as pathology in the major airways (bronchiectasis), further exacerbate difficulties in breathing. Other signs include coughing up blood (hemoptysis), high blood pressure in the lung (pulmonary hypertension), heart failure, difficulties getting enough oxygen to the body (hypoxia), and respiratory failure requiring support with breathing masks, such as bilevel positive airway pressure machines or ventilators. "Staphylococcus aureus", "Haemophilus influenzae", and "Pseudomonas aeruginosa" are the three most common organisms causing lung infections in CF patients. In addition to typical bacterial infections, people with CF more commonly develop other types of lung disease. Among these is allergic bronchopulmonary aspergillosis, in which the body's response to the common fungus "Aspergillus fumigatus" causes worsening of breathing problems. Another is infection with "Mycobacterium avium" complex (MAC), a group of bacteria related to tuberculosis, which can cause a lot of lung damage and does not respond to common antibiotics.
Mucus in the paranasal sinuses is equally thick and may also cause blockage of the sinus passages, leading to infection. This may cause facial pain, fever, nasal drainage, and headaches. Individuals with CF may develop overgrowth of the nasal tissue (nasal polyps) due to inflammation from chronic sinus infections. Recurrent sinonasal polyps can occur in as many as 10% to 25% of CF patients. These polyps can block the nasal passages and increase breathing difficulties.
Cardiorespiratory complications are the most common cause of death (~80%) in patients at most CF centers in the United States.
Gastrointestinal.
Prior to prenatal and newborn screening, cystic fibrosis was often diagnosed when a newborn infant failed to pass feces (meconium). Meconium may completely block the intestines and cause serious illness. This condition, called meconium ileus, occurs in 5–10% of newborns with CF. In addition, protrusion of internal rectal membranes (rectal prolapse) is more common, occurring in as many as 10% of children with CF, and it is caused by increased fecal volume, malnutrition, and increased intra–abdominal pressure due to coughing.
The thick mucus seen in the lungs has a counterpart in thickened secretions from the pancreas, an organ responsible for providing digestive juices that help break down food. These secretions block the exocrine movement of the digestive enzymes into the duodenum and result in irreversible damage to the pancreas, often with painful inflammation (pancreatitis). The pancreatic ducts are totally plugged in more advanced cases, usually seen in older children or adolescents. This causes atrophy of the exocrine glands and progressive fibrosis.
The lack of digestive enzymes leads to difficulty absorbing nutrients with their subsequent excretion in the feces, a disorder known as malabsorption. Malabsorption leads to malnutrition and poor growth and development because of calorie loss. Resultant hypoproteinemia may be severe enough to cause generalized edema. Individuals with CF also have difficulties absorbing the fat-soluble vitamins A, D, E, and K.
In addition to the pancreas problems, people with cystic fibrosis experience more heartburn, intestinal blockage by intussusception, and constipation. Older individuals with CF may develop distal intestinal obstruction syndrome when thickened feces cause intestinal blockage.
Exocrine pancreatic insufficiency occurs in the majority (85% to 90%) of patients with CF. It is mainly associated with "severe" CFTR mutations, where both alleles are completely nonfunctional (e.g. ΔF508/ΔF508). It occurs in 10% to 15% of patients with one "severe" and one "mild" CFTR mutation where there still is a little CFTR activity, or where there are two "mild" CFTR mutations. In these milder cases, there is still sufficient pancreatic exocrine function so that enzyme supplementation is not required. There are usually no other GI complications in pancreas-sufficient phenotypes, and in general, such individuals usually have excellent growth and development. Despite this, idiopathic chronic pancreatitis can occur in a subset of pancreas-sufficient individuals with CF, and is associated with recurrent abdominal pain and life-threatening complications.
Thickened secretions also may cause liver problems in patients with CF. Bile secreted by the liver to aid in digestion may block the bile ducts, leading to liver damage. Over time, this can lead to scarring and nodularity (cirrhosis). The liver fails to rid the blood of toxins and does not make important proteins, such as those responsible for blood clotting. Liver disease is the third most common cause of death associated with CF.
Endocrine.
The pancreas contains the islets of Langerhans, which are responsible for making insulin, a hormone that helps regulate blood glucose. Damage of the pancreas can lead to loss of the islet cells, leading to a type of diabetes that is unique to those with the disease. This cystic fibrosis-related diabetes (CFRD) shares characteristics that can be found in type 1 and type 2 diabetics, and is one of the principal nonpulmonary complications of CF. Vitamin D is involved in calcium and phosphate regulation. Poor uptake of vitamin D from the diet because of malabsorption can lead to the bone disease osteoporosis in which weakened bones are more susceptible to fractures. In addition, people with CF often develop clubbing of their fingers and toes due to the effects of chronic illness and low oxygen in their tissues.
Infertility.
Infertility affects both men and women. At least 97% of men with cystic fibrosis are infertile, but not sterile and can have children with assisted reproductive techniques. The main cause of infertility in men with cystic fibrosis is congenital absence of the vas deferens (which normally connects the testes to the ejaculatory ducts of the penis), but potentially also by other mechanisms such as causing no sperm, teratospermia, and few sperm with poor motility. Many men found to have congenital absence of the vas deferens during evaluation for infertility have a mild, previously undiagnosed form of CF. Approximately 20% of women with CF have fertility difficulties due to thickened cervical mucus or malnutrition. In severe cases, malnutrition disrupts ovulation and causes amenorrhea.
Cause.
CF is caused by a mutation in the gene cystic fibrosis transmembrane conductance regulator (CFTR). The most common mutation, ΔF508, is a deletion (Δ signifying deletion) of three nucleotides that results in a loss of the amino acid phenylalanine (F) at the 508th position on the protein. This mutation accounts for two-thirds (66–70%) of CF cases worldwide and 90% of cases in the United States; however, there are over 1500 other mutations that can produce CF. Although most people have two working copies (alleles) of the CFTR gene, only one is needed to prevent cystic fibrosis. CF develops when neither allele can produce a functional CFTR protein. Thus, CF is considered an autosomal recessive disease.
The CFTR gene, found at the q31.2 locus of chromosome 7, is 230,000 base pairs long, and creates a protein that is 1,480 amino acids long. More specifically the location is between base pair 117,120,016 to 117,308,718 on the long arm of chromosome 7, region 3, band 1, sub-band 2, represented as 7q31.2. Structurally, CFTR is a type of gene known as an ABC gene. The product of this gene (the CFTR) is a chloride ion channel important in creating sweat, digestive juices and mucus. This protein possesses two ATP-hydrolyzing domains, which allows the protein to use energy in the form of ATP. It also contains two domains comprising 6 alpha helices apiece, which allow the protein to cross the cell membrane. A regulatory binding site on the protein allows activation by phosphorylation, mainly by cAMP-dependent protein kinase. The carboxyl terminal of the protein is anchored to the cytoskeleton by a PDZ domain interaction.
In addition, there is increasing evidence that genetic modifiers besides CFTR modulate the frequency and severity of the disease. One example is mannan-binding lectin, which is involved in innate immunity by facilitating phagocytosis of microorganisms. Polymorphisms in one or both mannan-binding lectin alleles that result in lower circulating levels of the protein are associated with a threefold higher risk of end-stage lung disease, as well as an increased burden of chronic bacterial infections.
Pathophysiology.
There are several mutations in the "CFTR" gene, and different mutations cause different defects in the CFTR protein, sometimes causing a milder or more severe disease. These protein defects are also targets for drugs which can sometimes restore their function. ΔF508-CFTR, which occurs in >90% of patients in the U.S., creates a protein that does not fold normally and is not appropriately transported to the cell membrane, resulting in its degradation. Other mutations result in proteins that are too short (truncated) because production is ended prematurely. Other mutations produce proteins that: do not use energy normally, do not allow chloride, iodide, and thiocyanate to cross the membrane appropriately, degrade at a faster rate than normal. Mutations may also lead to fewer copies of the CFTR protein being produced.
The protein created by this gene is anchored to the outer membrane of cells in the sweat glands, lungs, pancreas, and all other remaining exocrine glands in the body. 
The protein spans this membrane and acts as a channel connecting the inner part of the cell (cytoplasm) to the surrounding fluid. This channel is primarily responsible for controlling the movement of halogens from inside to outside of the cell; however, in the sweat ducts it facilitates the movement of chloride from the sweat duct into the cytoplasm. When the CFTR protein does not resorb ions in sweat ducts, chloride and thiocyanate released from sweat glands are trapped inside the ducts and pumped to the skin. Additionally hypothiocyanite, OSCN, cannot be produced by the immune defense system. Because chloride is negatively charged, this modifies the electrical potential inside and outside the cell that normally causes cations to cross into the cell. Sodium is the most common cation in the extracellular space. The excess chloride within sweat ducts prevents sodium resorption by epithelial sodium channels and the combination of sodium and chloride creates the salt, which is lost in high amounts in the sweat of individuals with CF. This lost salt forms the basis for the sweat test.
Most of the damage in CF is due to blockage of the narrow passages of affected organs with thickened secretions. These blockages lead to remodeling and infection in the lung, damage by accumulated digestive enzymes in the pancreas, blockage of the intestines by thick faeces, etc. There are several theories on how the defects in the protein and cellular function cause the clinical effects. The most current theory suggests that defective ion transport leads to dehydration in the airway epithelia, thickening mucus. In airway epithelial cells, the cilia exist in between the cell's apical surface and mucus in a layer known as Airway Surface Liquid (ASL). The flow of ions from the cell and into this layer is determined by ion channels like CFTR. CFTR not only allows Chloride ions to be drawn from the cell and into the ASL, but it also regulates another channel called ENac. ENac allows sodium ions to leave the ASL and enter the respiratory epithelium. CFTR normally inhibits this channel, but if the CFTR is defective, then sodium will flow freely from the ASL and into the cell. As water follows sodium, the depth of ASL will be depleted and the cilia will be left in the mucous layer. As cilia cannot effectively move in a thick viscous environment, there is deficient mucociliary clearance and a buildup of mucous, clogging small airways. The accumulation of more viscous, nutrient-rich mucus in the lungs allows bacteria to hide from the body's immune system, causing repeated respiratory infections. The presence of the same CFTR proteins in pancreatic duct and skin cells are what cause symptoms in these systems.
Chronic infections.
The lungs of individuals with cystic fibrosis are colonized and infected by bacteria from an early age. These bacteria, which often spread among individuals with CF, thrive in the altered mucus, which collects in the small airways of the lungs. This mucus leads to the formation of bacterial microenvironments known as biofilms that are difficult for immune cells and antibiotics to penetrate. Viscous secretions and persistent respiratory infections repeatedly damage the lung by gradually remodeling the airways, which makes infection even more difficult to eradicate.
Over time, both the types of bacteria and their individual characteristics change in individuals with CF. In the initial stage, common bacteria such as "Staphylococcus aureus" and "Haemophilus influenzae" colonize and infect the lungs. Eventually, "Pseudomonas aeruginosa" (and sometimes "Burkholderia cepacia") dominates. By 18 years of age, 80% of patients with classic CF harbor "P. aeruginosa", and 3.5% harbor "B. cepacia". Once within the lungs, these bacteria adapt to the environment and develop resistance to commonly used antibiotics. "Pseudomonas" can develop special characteristics that allow the formation of large colonies, known as "mucoid" "Pseudomonas", which are rarely seen in people that do not have CF.
One way infection spreads is by passing between different individuals with CF. In the past, people with CF often participated in summer "CF Camps" and other recreational gatherings. Hospitals grouped patients with CF into common areas and routine equipment (such as nebulizers) was not sterilized between individual patients. This led to transmission of more dangerous strains of bacteria among groups of patients. As a result, individuals with CF are now routinely isolated from one another in the healthcare setting, and healthcare providers are encouraged to wear gowns and gloves when examining patients with CF to limit the spread of virulent bacterial strains.
CF patients may also have their airways chronically colonized by filamentous fungi (such as "Aspergillus fumigatus", "Scedosporium apiospermum", "Aspergillus terreus") and/or yeasts (such as "Candida albicans"); other filamentous fungi less commonly isolated include "Aspergillus flavus" and "Aspergillus nidulans" (occur transiently in CF respiratory secretions) and "Exophiala dermatitidis" and "Scedosporium prolificans" (chronic airway-colonizers); some filamentous fungi like "Penicillium emersonii" and "Acrophialophora fusispora" are encountered in patients almost exclusively in the context of CF. Defective mucociliary clearance characterizing CF is associated with local immunological disorders. In addition, the prolonged therapy with antibiotics and the use of corticosteroid treatments may also facilitate fungal growth. Although the clinical relevance of the fungal airway colonization is still a matter of debate, filamentous fungi may contribute to the local inflammatory response and therefore to the progressive deterioration of the lung function, as often happens with allergic broncho-pulmonary aspergillosis (ABPA) – the most common fungal disease in the context of CF, involving a Th2-driven immune response to Aspergillus.
Diagnosis and monitoring.
Cystic fibrosis may be diagnosed by many different methods including newborn screening, sweat testing, and genetic testing. As of 2006 in the United States, 10 percent of cases are diagnosed shortly after birth as part of newborn screening programs. The newborn screen initially measures for raised blood concentration of immunoreactive trypsinogen. Infants with an abnormal newborn screen need a sweat test to confirm the CF diagnosis. In many cases, a parent makes the diagnosis because the infant tastes salty. Trypsinogen levels can be increased in individuals who have a single mutated copy of the "CFTR" gene (carriers) or, in rare instances, in individuals with two normal copies of the "CFTR" gene. Due to these false positives, CF screening in newborns can be controversial. Most states and countries do not screen for CF routinely at birth. Therefore, most individuals are diagnosed after symptoms (e.g. sinopulmonary disease and GI manifestations) prompt an evaluation for cystic fibrosis. The most commonly used form of testing is the sweat test. Sweat-testing involves application of a medication that stimulates sweating (pilocarpine). To deliver the medication through the skin, iontophoresis is used to, whereby one electrode is placed onto the applied medication and an electric current is passed to a separate electrode on the skin. The resultant sweat is then collected on filter paper or in a capillary tube and analyzed for abnormal amounts of sodium and chloride. People with CF have increased amounts of sodium and chloride in their sweat. In contrast, people with CF have less thiocyanate and hypothiocyanite in their saliva and mucus (Banfi et al.). CF can also be diagnosed by identification of mutations in the CFTR gene.
People with CF may be listed in a disease registry that allows researchers and doctors to track health results and identify candidates for clinical trials.
Prenatal.
Couples who are pregnant or planning a pregnancy can have themselves tested for the CFTR gene mutations to determine the risk that their child will be born with cystic fibrosis. Testing is typically performed first on one or both parents and, if the risk of CF is high, testing on the fetus is performed. The American College of Obstetricians and Gynecologists (ACOG) recommends testing for couples who have a personal or close family history of CF, and they recommend that carrier testing be offered to all Caucasian couples and be made available to couples of other ethnic backgrounds.
Because development of CF in the fetus requires each parent to pass on a mutated copy of the CFTR gene and because CF testing is expensive, testing is often performed initially on one parent. If testing shows that parent is a CFTR gene mutation carrier, the other parent is tested to calculate the risk that their children will have CF. CF can result from more than a thousand different mutations, and as of 2006 it is not possible to test for each one. Testing analyzes the blood for the most common mutations such as ΔF508—most commercially available tests look for 32 or fewer different mutations. If a family has a known uncommon mutation, specific screening for that mutation can be performed. Because not all known mutations are found on current tests, a negative screen does not guarantee that a child will not have CF.
During pregnancy, testing can be performed on the placenta (chorionic villus sampling) or the fluid around the fetus (amniocentesis). However, chorionic villus sampling has a risk of fetal death of 1 in 100 and amniocentesis of 1 in 200; a recent study has indicated this may be much lower, approximately 1 in 1,600.
Economically, for carrier couples of cystic fibrosis, when comparing preimplantation genetic diagnosis (PGD) with natural conception (NC) followed by prenatal testing and abortion of affected pregnancies, PGD provides net economic benefits up to a maternal age of approximately 40 years, after which NC, prenatal testing and abortion has higher economic benefit.
Management.
While there are no cures for cystic fibrosis, there are several treatment methods. The management of cystic fibrosis has improved significantly over the past 70 years. While infants born with cystic fibrosis 70 years ago would have been unlikely to live beyond their first year, infants today are likely to live well into adulthood. Recent advances in the treatment of cystic fibrosis have meant that an individual with cystic fibrosis can live a fuller life less encumbered by their condition. The cornerstones of management are proactive treatment of airway infection, and encouragement of good nutrition and an active lifestyle. Pulmonary rehabilitation as a management of cystic fibrosis continues throughout a person's life, and is aimed at maximizing organ function, and therefore quality of life. At best, current treatments delay the decline in organ function. Because of the wide variation in disease symptoms, treatment typically occurs at specialist multidisciplinary centers, and is tailored to the individual. Targets for therapy are the lungs, gastrointestinal tract (including pancreatic enzyme supplements), the reproductive organs (including assisted reproductive technology (ART)) and psychological support.
The most consistent aspect of therapy in cystic fibrosis is limiting and treating the lung damage caused by thick mucus and infection, with the goal of maintaining quality of life. Intravenous, inhaled, and oral antibiotics are used to treat chronic and acute infections. Mechanical devices and inhalation medications are used to alter and clear the thickened mucus. These therapies, while effective, can be extremely time-consuming.
Antibiotics.
Many people with CF are on one or more antibiotics at all times, even when healthy, to prophylactically suppress infection. Antibiotics are absolutely necessary whenever pneumonia is suspected or there has been a noticeable decline in lung function, and are usually chosen based on the results of a sputum analysis and the person's past response. This prolonged therapy often necessitates hospitalization and insertion of a more permanent IV such as a peripherally inserted central catheter (PICC line) or Port-a-Cath. Inhaled therapy with antibiotics such as tobramycin, colistin, and aztreonam is often given for months at a time to improve lung function by impeding the growth of colonized bacteria. Inhaled antibiotic therapy helps lung function by fighting infection, but also has significant drawbacks like development of antibiotic resistance, tinnitus and changes in the voice. Oral antibiotics such as ciprofloxacin or azithromycin are given to help prevent infection or to control ongoing infection. The aminoglycoside antibiotics (e.g. tobramycin) used can cause hearing loss, damage to the balance system in the inner ear or kidney problems with long-term use. To prevent these side-effects, the amount of antibiotics in the blood is routinely measured and adjusted accordingly.
Other treatments for lung disease.
Several mechanical techniques are used to dislodge sputum and encourage its expectoration. In the hospital setting, chest physiotherapy (CPT) is utilized; a respiratory therapist percusses an individual's chest with his or her hands several times a day, to loosen up secretions. Devices that recreate this percussive therapy include the ThAIRapy Vest and the intrapulmonary percussive ventilator (IPV). Newer methods such as Biphasic Cuirass Ventilation, and associated clearance mode available in such devices, integrate a cough assistance phase, as well as a vibration phase for dislodging secretions. These are portable and adapted for home use.
Ivacaftor is an oral medication for the treatment of cystic fibrosis due to a number of specific mutations. It improves lung function by about 10%; however, as of 2014 is expensive.
Aerosolized medications that help loosen secretions include dornase alfa and hypertonic saline. Dornase is a recombinant human deoxyribonuclease, which breaks down DNA in the sputum, thus decreasing its viscosity. Denufosol is an investigational drug that opens an alternative chloride channel, helping to liquefy mucus. It is unclear if inhaled corticosteroids are useful.
As lung disease worsens, mechanical breathing support may become necessary. Individuals with CF may need to wear special masks at night that help push air into their lungs. These machines, known as bilevel positive airway pressure (BiPAP) ventilators, help prevent low blood oxygen levels during sleep. BiPAP may also be used during physical therapy to improve sputum clearance. During severe illness, a tube may be placed in the throat (a procedure known as a tracheostomy) to enable breathing supported by a ventilator.
For children, preliminary studies show massage therapy may help people and their families quality of life. It is unclear what effect pneumococcal vaccination has as it has not been studied as of 2014.
Transplantation.
Lung transplantation often becomes necessary for individuals with cystic fibrosis as lung function and exercise tolerance decline. Although single lung transplantation is possible in other diseases, individuals with CF must have both lungs replaced because the remaining lung might contain bacteria that could infect the transplanted lung. A pancreatic or liver transplant may be performed at the same time in order to alleviate liver disease and/or diabetes. Lung transplantation is considered when lung function declines to the point where assistance from mechanical devices is required or someone's survival is threatened.
Other aspects.
Newborns with intestinal obstruction typically require surgery, whereas adults with distal intestinal obstruction syndrome typically do not. Treatment of pancreatic insufficiency by replacement of missing digestive enzymes allows the duodenum to properly absorb nutrients and vitamins that would otherwise be lost in the feces. However, the best dosage and form of pancreatic enzyme replacement is unclear, as are the risks and long-term effectiveness of this treatment.
So far, no large-scale research involving the incidence of atherosclerosis and coronary heart disease in adults with cystic fibrosis has been conducted. This is likely due to the fact that the vast majority of people with cystic fibrosis do not live long enough to develop clinically significant atherosclerosis or coronary heart disease.
Diabetes is the most common non-pulmonary complication of CF. It mixes features of type 1 and type 2 diabetes, and is recognized as a distinct entity, cystic fibrosis-related diabetes (CFRD). While oral anti-diabetic drugs are sometimes used, the only recommended treatment is the use of insulin injections or an insulin pump, and, unlike in type 1 and 2 diabetes, dietary restrictions are not recommended.
Development of osteoporosis can be prevented by increased intake of vitamin D and calcium, and can be treated by bisphosphonates, although adverse effects can be an issue. Poor growth may be avoided by insertion of a feeding tube for increasing calories through supplemental feeds or by administration of injected growth hormone.
Sinus infections are treated by prolonged courses of antibiotics. The development of nasal polyps or other chronic changes within the nasal passages may severely limit airflow through the nose, and over time reduce the person's sense of smell. Sinus surgery is often used to alleviate nasal obstruction and to limit further infections. Nasal steroids such as fluticasone are used to decrease nasal inflammation.
Female infertility may be overcome by assisted reproduction technology, particularly embryo transfer techniques. Male infertility caused by absence of the vas deferens may be overcome with testicular sperm extraction (TESE), collecting sperm cells directly from the testicles. If the collected sample contains too few sperm cells to likely have a spontaneous fertilization, intracytoplasmic sperm injection can be performed. Third party reproduction is also a possibility for women with CF. It is unclear if taking antioxidants affects outcomes.
Prognosis.
The prognosis for cystic fibrosis has improved due to earlier diagnosis through screening, better treatment and access to health care. In 1959, the median age of survival of children with cystic fibrosis in the United States was six months. In 2010, survival is estimated to be 37 years for women and 40 for men. In Canada, median survival increased from 24 years in 1982 to 47.7 in 2007.
Of those with cystic fibrosis who are more than 18 years old as of 2009, 92% had graduated from high school, 67% had at least some college education, 15% were disabled and 9% were unemployed, 56% were single and 39% were married or living with a partner.
Quality of life.
Chronic illnesses can be very difficult to manage. Cystic fibrosis (CF) is a chronic illness that affects the "digestive and respiratory tracts resulting in generalized malnutrition and chronic respiratory infections". The thick secretions clog the airways in the lungs, which often cause inflammation and severe lung infections. If it is compromised, it affects the quality of life of someone with CF and their ability to complete such tasks as everyday chores. 
It is important for CF patients to understand the detrimental relationship that chronic illnesses place on the quality of life. According to Schmitz and Goldbeck (2006), the fact that cystic fibrosis significantly increases emotional stress on both the individual and the family, "and the necessary time-consuming daily treatment routine may have further negative effects on quality of life (QOL)". However, Havermans and colleagues (2006) have shown that young outpatients with CF who have participated in the CFQ-R (Cystic Fibrosis Questionnaire-Revised) "rated some QOL domains higher than did their parents".</ref> Consequently, outpatients with CF have a more positive outlook for themselves. 
Furthermore, there are many ways to improve the QOL in CF patients. Exercise is promoted to increase lung function. Integrating an exercise regimen into the CF patient’s daily routine can significantly improve the quality of life. There is no definitive cure for cystic fibrosis. However, there are diverse medications used, such as mucolytics, bronchodilators, steroids, and antibiotics, that have the purpose of loosening mucus, expanding airways, decreasing inflammation, and fighting lung infections.
Epidemiology.
Cystic fibrosis is the most common life-limiting autosomal recessive disease among people of European heritage. In the United States, approximately 30,000 individuals have CF; most are diagnosed by six months of age. In Canada, there are approximately 4,000 people with CF. Approximately 1 in 25 people of European descent, and one in 30 of Caucasian Americans, is a carrier of a cystic fibrosis mutation. Although CF is less common in these groups, approximately 1 in 46 Hispanics, 1 in 65 Africans and 1 in 90 Asians carry at least one abnormal CFTR gene. Ireland has the world's highest prevalence of cystic fibrosis, at 1:1353.
Although technically a rare disease, cystic fibrosis is ranked as one of the most widespread life-shortening genetic diseases. It is most common among nations in the Western world. An exception is Finland, where only one in 80 people carry a CF mutation. The World Health Organization states that "In the European Union, 1 in 2000–3000 newborns is found to be affected by CF". In the United States, 1 in 3,500 children are born with CF. In 1997, about 1 in 3,300 caucasian children in the United States was born with cystic fibrosis. In contrast, only 1 in 15,000 African American children suffered from cystic fibrosis, and in Asian Americans the rate was even lower at 1 in 32,000.
Cystic fibrosis is diagnosed in males and females equally. For reasons that remain unclear, data has shown that males tend to have a longer life expectancy than females, however recent studies suggest this gender gap may no longer exist perhaps due to improvements in health care facilities, while a recent study from Ireland identified a link between the female hormone estrogen and worse outcomes in CF.
The distribution of CF alleles varies among populations. The frequency of ΔF508 carriers has been estimated at 1:200 in northern Sweden, 1:143 in Lithuanians, and 1:38 in Denmark. No ΔF508 carriers were found among 171 Finns and 151 Saami people. ΔF508 does occur in Finland, but it is a minority allele there. Cystic fibrosis is known to occur in only 20 families (pedigrees) in Finland.
Evolution.
The ΔF508 mutation is estimated to be up to 52,000 years old. Numerous hypotheses have been advanced as to why such a lethal mutation has persisted and spread in the human population. Other common autosomal recessive diseases such as sickle-cell anemia have been found to protect carriers from other diseases, a concept known as heterozygote advantage. Resistance to the following have all been proposed as possible sources of heterozygote advantage:
History.
It is supposed that CF appeared about 3,000 BC because of migration of peoples, gene mutations, and new conditions in nourishment. Although the entire clinical spectrum of CF was not recognized until the 1930s, certain aspects of CF were identified much earlier. Indeed, literature from Germany and Switzerland in the 18th century warned "Wehe dem Kind, das beim Kuß auf die Stirn salzig schmekt, er ist verhext und muss bald sterbe" or "Woe to the child who tastes salty from a kiss on the brow, for he is cursed and soon must die," recognizing the association between the salt loss in CF and illness.
In the 19th century, Carl von Rokitansky described a case of fetal death with meconium peritonitis, a complication of meconium ileus associated with cystic fibrosis. Meconium ileus was first described in 1905 by Karl Landsteiner. In 1936, Guido Fanconi published a paper describing a connection between celiac disease, cystic fibrosis of the pancreas, and bronchiectasis.
In 1938 Dorothy Hansine Andersen published an article, "Cystic Fibrosis of the Pancreas and Its Relation to Celiac Disease: a Clinical and Pathological Study," in the "American Journal of Diseases of Children". She was the first to describe the characteristic cystic fibrosis of the pancreas and to correlate it with the lung and intestinal disease prominent in CF. She also first hypothesized that CF was a recessive disease and first used pancreatic enzyme replacement to treat affected children. In 1952 Paul di Sant’Agnese discovered abnormalities in sweat electrolytes; a sweat test was developed and improved over the next decade.
The first linkage between CF and another marker (Paroxonase) was found in 1985 by Hans Eiberg, indicating that only one locus exists for CF. In 1988 the first mutation for CF, ΔF508 was discovered by Francis Collins, Lap-Chee Tsui and John R. Riordan on the seventh chromosome. Subsequent research has found over 1,000 different mutations that cause CF.
Because mutations in the CFTR gene are typically small, classical genetics techniques had been unable to accurately pinpoint the mutated gene. Using protein markers, gene-linkage studies were able to map the mutation to chromosome 7. Chromosome-walking and -jumping techniques were then used to identify and sequence the gene. In 1989 Lap-Chee Tsui led a team of researchers at the Hospital for Sick Children in Toronto that discovered the gene responsible for CF. Cystic fibrosis represents a classic example of how a human genetic disorder was elucidated strictly by the process of forward genetics.
Research.
Gene therapy.
Gene therapy has been explored as a potential cure for cystic fibrosis. Results from trials have shown limited success as of 2013. A small study published in 2015 found a small benefit.
The focus of much cystic fibrosis gene therapy research is aimed at trying to place a normal copy of the CFTR gene into affected cells. Transferring the normal CFTR gene into the affected epithelium cells would result in the production of functional CFTR in all target cells, without adverse reactions or an inflammation response. Studies have shown that to prevent the lung manifestations of cystic fibrosis, only 5–10% the normal amount of CFTR gene expression is needed. Multiple approaches have been tested for gene transfer, such as liposomes and viral vectors in animal models and clinical trials. However, both methods were found to be relatively inefficient treatment options. The main reason is that very few cells take up the vector and express the gene, so the treatment has little effect. Additionally, problems have been noted in cDNA recombination, such that the gene introduced by the treatment is rendered unusable. There has been a functional repair in culture of CFTR by CRISPR/Cas9 in intestinal stem cell organoids of cystic fibrosis patients.
Small molecules.
A number of small molecules that aim at compensating various mutations of the CFTR gene are under development. One approach is to develop drugs that get the ribosome to overcome the stop codon and synthesize a full-length CFTR protein. About 10% of CF result from a premature stop codon in the DNA, leading to early termination of protein synthesis and truncated proteins. These drugs target nonsense mutations such as G542X, which consists of the amino acid glycine in position 542 being replaced by a stop codon. Aminoglycoside antibiotics interfere with protein synthesis and error-correction. In some cases, they can cause the cell to overcome a premature stop codon by inserting a random amino acid, thereby allowing expression of a full-length protein.
The aminoglycoside gentamicin has been used to treat lung cells from CF patients in the laboratory to induce the cells to grow full-length proteins. Another drug targeting nonsense mutations is ataluren, which is undergoing Phase III clinical trials .
Other.
It is unclear as of 2014 if ursodeoxycholic acid is useful for those with cystic fibrosis-related liver disease.

</doc>
<doc id="50603" url="https://en.wikipedia.org/wiki?curid=50603" title="Multiple sclerosis">
Multiple sclerosis

Multiple sclerosis (MS) is a demyelinating disease in which the insulating covers of nerve cells in the brain and spinal cord are damaged. This damage disrupts the ability of parts of the nervous system to communicate, resulting in a range of signs and symptoms, including physical, mental, and sometimes psychiatric problems. Specific symptoms can include double vision, blindness in one eye, muscle weakness, trouble with sensation, or trouble with coordination. MS takes several forms, with new symptoms either occurring in isolated attacks (relapsing forms) or building up over time (progressive forms). Between attacks, symptoms may disappear completely; however, permanent neurological problems often remain, especially as the disease advances. 
While the cause is not clear, the underlying mechanism is thought to be either destruction by the immune system or failure of the myelin-producing cells. Proposed causes for this include genetics and environmental factors such as being triggered by a viral infection. MS is usually diagnosed based on the presenting signs and symptoms and the results of supporting medical tests.
There is no known cure for multiple sclerosis. Treatments attempt to improve function after an attack and prevent new attacks. Medications used to treat MS, while modestly effective, can have side effects and be poorly tolerated. --> Physical therapy can help with people's ability to function. Many people pursue alternative treatments, despite a lack of evidence. The long-term outcome is difficult to predict, with good outcomes more often seen in women, those who develop the disease early in life, those with a relapsing course, and those who initially experienced few attacks. Life expectancy is on average 5 to 10 years lower than that of an unaffected population.
Multiple sclerosis is the most common autoimmune disorder affecting the central nervous system. In 2013, about 2.3 million people were affected globally with rates varying widely in different regions and among different populations. That year about 20,000 people died from MS, up from 12,000 in 1990. The disease usually begins between the ages of 20 and 50 and is twice as common in women as in men. MS was first described in 1868 by Jean-Martin Charcot. The name "multiple sclerosis" refers to the numerous scars (sclerae—better known as plaques or lesions) that develop on the white matter of the brain and spinal cord.* </ref> A number of new treatments and diagnostic methods are under development.
Signs and symptoms.
A person with MS can have almost any neurological symptom or sign, with autonomic, visual, motor, and sensory problems being the most common. The specific symptoms are determined by the locations of the lesions within the nervous system, and may include loss of sensitivity or changes in sensation such as tingling, pins and needles or numbness, muscle weakness, very pronounced reflexes, muscle spasms, or difficulty in moving; difficulties with coordination and balance (ataxia); problems with speech or swallowing, visual problems (nystagmus, optic neuritis or double vision), feeling tired, acute or chronic pain, and bladder and bowel difficulties, among others. Difficulties thinking and emotional problems such as depression or unstable mood are also common. Uhthoff's phenomenon, a worsening of symptoms due to exposure to higher than usual temperatures, and Lhermitte's sign, an electrical sensation that runs down the back when bending the neck, are particularly characteristic of MS. The main measure of disability and severity is the expanded disability status scale (EDSS), with other measures such as the multiple sclerosis functional composite being increasingly used in research.
The condition begins in 85% of cases as a clinically isolated syndrome (CIS) over a number of days with 45% having motor or sensory problems, 20% having optic neuritis, and 10% having symptoms related to brainstem dysfunction, while the remaining 25% have more than one of the previous difficulties. The course of symptoms occurs in two main patterns initially: either as episodes of sudden worsening that last a few days to months (called relapses, exacerbations, bouts, attacks, or flare-ups) followed by improvement (85% of cases) or as a gradual worsening over time without periods of recovery (10-15% of cases). A combination of these two patterns may also occur or people may start in a relapsing and remitting course that then becomes progressive later on. Relapses are usually not predictable, occurring without warning. Exacerbations rarely occur more frequently than twice per year. Some relapses, however, are preceded by common triggers and they occur more frequently during spring and summer. Similarly, viral infections such as the common cold, influenza, or gastroenteritis increase their risk. Stress may also trigger an attack. Women with MS who become pregnant experience fewer relapses; however, during the first months after delivery the risk increases. Overall, pregnancy does not seem to influence long-term disability. Many events have not been found to affect relapse rates including vaccination, breast feeding, physical trauma, and Uhthoff's phenomenon.
Causes.
The cause of MS is unknown; however, it is believed to occur as a result of some combination of genetic and environmental factors such as infectious agents. Theories try to combine the data into likely explanations, but none has proved definitive. While there are a number of environmental risk factors and although some are partly modifiable, further research is needed to determine whether their elimination can prevent MS.
Geography.
MS is more common in people who live farther from the equator, although exceptions exist. These exceptions include ethnic groups that are at low risk far from the equator such as the Samis, Amerindians, Canadian Hutterites, New Zealand Māori, and Canada's Inuit, as well as groups that have a relatively high risk close to the equator such as Sardinians, inland Sicilians, Palestinians and Parsis. The cause of this geographical pattern is not clear. While the north-south gradient of incidence is decreasing, as of 2010 it is still present.
MS is more common in regions with northern European populations and the geographic variation may simply reflect the global distribution of these high-risk populations. Decreased sunlight exposure resulting in decreased vitamin D production has also been put forward as an explanation. A relationship between season of birth and MS lends support to this idea, with fewer people born in the northern hemisphere in November as compared to May being affected later in life. Environmental factors may play a role during childhood, with several studies finding that people who move to a different region of the world before the age of 15 acquire the new region's risk to MS. If migration takes place after age 15, however, the person retains the risk of his home country. There is some evidence that the effect of moving may still apply to people older than 15.
Genetics.
MS is not considered a hereditary disease; however, a number of genetic variations have been shown to increase the risk. Some of these genes appear to have higher levels of expression in microglial cells than expected by chance. The probability of developing the disease is higher in relatives of an affected person, with a greater risk among those more closely related. In identical twins both are affected about 30% of the time, while around 5% for non-identical twins and 2.5% of siblings are affected with a lower percentage of half-siblings. If both parents are affected the risk in their children is 10 times that of the general population. MS is also more common in some ethnic groups than others.
Specific genes that have been linked with MS include differences in the human leukocyte antigen (HLA) system—a group of genes on chromosome 6 that serves as the major histocompatibility complex (MHC). That changes in the HLA region are related to susceptibility has been known since the 1980s, and additionally this same region has been implicated in the development of other autoimmune diseases such as diabetes type I and systemic lupus erythematosus. The most consistent finding is the association between multiple sclerosis and alleles of the MHC defined as DR15 and DQ6. Other loci have shown a protective effect, such as HLA-C554 and HLA-DRB1*11. Overall, it has been estimated that HLA changes account for between 20 and 60% of the genetic predisposition. Modern genetic methods (genome-wide association studies) have discovered at least twelve other genes outside the HLA locus that modestly increase the probability of MS.
Infectious agents.
Many microbes have been proposed as triggers of MS, but none have been confirmed. Moving at an early age from one location in the world to another alters a person's subsequent risk of MS. An explanation for this could be that some kind of infection, produced by a widespread microbe rather than a rare one, is related to the disease. Proposed mechanisms include the hygiene hypothesis and the prevalence hypothesis. The hygiene hypothesis proposes that exposure to certain infectious agents early in life is protective, the disease being a response to a late encounter with such agents. The prevalence hypothesis proposes that the disease is due to an infectious agent more common in regions where MS is common and where in most individuals it causes an ongoing infection without symptoms. Only in a few cases and after many years does it cause demyelination. The hygiene hypothesis has received more support than the prevalence hypothesis.
Evidence for a virus as a cause include: the presence of oligoclonal bands in the brain and cerebrospinal fluid of most people with MS, the association of several viruses with human demyelination encephalomyelitis, and the occurrence of demyelination in animals caused by some viral infection. Human herpes viruses are a candidate group of viruses. Individuals having never been infected by the Epstein–Barr virus are at a reduced risk of getting MS, whereas those infected as young adults are at a greater risk than those having had it at a younger age. Although some consider that this goes against the hygiene hypothesis, since the non-infected have probably experienced a more hygienic upbringing, others believe that there is no contradiction, since it is a first encounter with the causative virus relatively late in life that is the trigger for the disease. Other diseases that may be related include measles, mumps and rubella.
Other.
Smoking has been shown to be an independent risk factor for MS. Stress may be a risk factor although the evidence to support this is weak. Association with occupational exposures and toxins—mainly solvents—has been evaluated, but no clear conclusions have been reached. Vaccinations were studied as causal factors; however, most studies show no association. Several other possible risk factors, such as diet and hormone intake, have been looked at; however, evidence on their relation with the disease is "sparse and unpersuasive". Gout occurs less than would be expected and lower levels of uric acid have been found in people with MS. This has led to the theory that uric acid is protective, although its exact importance remains unknown.
Pathophysiology.
The three main characteristics of MS are the formation of lesions in the central nervous system (also called plaques), inflammation, and the destruction of myelin sheaths of neurons. These features interact in a complex and not yet fully understood manner to produce the breakdown of nerve tissue and in turn the signs and symptoms of the disease. Additionally, MS is believed to be an immune-mediated disorder that develops from an interaction of the individual's genetics and as yet unidentified environmental causes. Damage is believed to be caused, at least in part, by attack on the nervous system by a person's own immune system.
Lesions.
The name "multiple sclerosis" refers to the scars (sclerae – better known as plaques or lesions) that form in the nervous system. These lesions most commonly affect the white matter in the optic nerve, brain stem, basal ganglia, and spinal cord, or white matter tracts close to the lateral ventricles. The function of white matter cells is to carry signals between grey matter areas, where the processing is done, and the rest of the body. The peripheral nervous system is rarely involved.
To be specific, MS involves the loss of oligodendrocytes, the cells responsible for creating and maintaining a fatty layer—known as the myelin sheath—which helps the neurons carry electrical signals (action potentials). This results in a thinning or complete loss of myelin and, as the disease advances, the breakdown of the axons of neurons. When the myelin is lost, a neuron can no longer effectively conduct electrical signals. A repair process, called remyelination, takes place in early phases of the disease, but the oligodendrocytes are unable to completely rebuild the cell's myelin sheath. Repeated attacks lead to successively less effective remyelinations, until a scar-like plaque is built up around the damaged axons. These scars are the origin of the symptoms and during an attack magnetic resonance imaging (MRI) often shows more than ten new plaques. This could indicate that there are a number of lesions below which the brain is capable of repairing itself without producing noticeable consequences. Another process involved in the creation of lesions is an abnormal increase in the number of astrocytes due to the destruction of nearby neurons. A number of lesion patterns have been described.
Inflammation.
Apart from demyelination, the other sign of the disease is inflammation. Fitting with an immunological explanation, the inflammatory process is caused by T cells, a kind of lymphocyte that plays an important role in the body's defenses. T cells gain entry into the brain via disruptions in the blood–brain barrier. The T cells recognize myelin as foreign and attack it, explaining why these cells are also called "autoreactive lymphocytes".
The attack of myelin starts inflammatory processes, which triggers other immune cells and the release of soluble factors like cytokines and antibodies. Further breakdown of the blood–brain barrier in turn causes a number of other damaging effects such as swelling, activation of macrophages, and more activation of cytokines and other destructive proteins. Inflammation can potentially reduce transmission of information between neurons in at least three ways. The soluble factors released might stop neurotransmission by intact neurons. These factors could lead to or enhance the loss of myelin, or they may cause the axon to break down completely.
Blood–brain barrier.
The blood–brain barrier is a part of the capillary system that prevents the entry of T cells into the central nervous system. --> It may become permeable to these types of cells secondary to an infection by a virus or bacteria. --> After it repairs itself, typically once the infection has cleared, T cells may remain trapped inside the brain. Gadolinium cannot cross a normal BBB and, therefore, Gadolinium-enhanced MRI is used to show BBB breakdowns.
Diagnosis.
Multiple sclerosis is typically diagnosed based on the presenting signs and symptoms, in combination with supporting medical imaging and laboratory testing. It can be difficult to confirm, especially early on, since the signs and symptoms may be similar to those of other medical problems. The McDonald criteria, which focus on clinical, laboratory, and radiologic evidence of lesions at different times and in different areas, is the most commonly used method of diagnosis with the Schumacher and Poser criteria being of mostly historical significance. While the above criteria allow for a non-invasive diagnosis, some state that the only definitive proof is an autopsy or biopsy where lesions typical of MS are detected.
Clinical data alone may be sufficient for a diagnosis of MS if an individual has had separate episodes of neurological symptoms characteristic of the disease. In those who seek medical attention after only one attack, other testing is needed for the diagnosis. The most commonly used diagnostic tools are neuroimaging, analysis of cerebrospinal fluid and evoked potentials. Magnetic resonance imaging of the brain and spine may show areas of demyelination (lesions or plaques). Gadolinium can be administered intravenously as a contrast agent to highlight active plaques and, by elimination, demonstrate the existence of historical lesions not associated with symptoms at the moment of the evaluation. Testing of cerebrospinal fluid obtained from a lumbar puncture can provide evidence of chronic inflammation in the central nervous system. The cerebrospinal fluid is tested for oligoclonal bands of IgG on electrophoresis, which are inflammation markers found in 75–85% of people with MS. The nervous system in MS may respond less actively to stimulation of the optic nerve and sensory nerves due to demyelination of such pathways. These brain responses can be examined using visual- and sensory-evoked potentials.
Clinical course.
Several phenotypes (commonly named types), or patterns of progression, have been described. Phenotypes use the past course of the disease in an attempt to predict the future course. They are important not only for prognosis but also for treatment decisions. In 1996, the United States National Multiple Sclerosis Society described four clinical courses. This set of courses was later reviewed by an international panel in 2013, adding clinically isolated syndrome (CIS) and radiologically isolated syndrome (RIS) as phenotypes, but leaving the main structure untouched.
The relapsing-remitting subtype is characterized by unpredictable relapses followed by periods of months to years of relative quiet (remission) with no new signs of disease activity. Deficits that occur during attacks may either resolve or leave problems, the latter in about 40% of attacks and being more common the longer a person has had the disease. This describes the initial course of 80% of individuals with MS. When deficits always resolve between attacks, this is sometimes referred to as "benign MS", although people will still build up some degree of disability in the long term. On the other hand, the term "malignant multiple sclerosis" is used to describe people with MS having reached significant level of disability in a short period. The relapsing-remitting subtype usually begins with a clinically isolated syndrome (CIS). In CIS, a person has an attack suggestive of demyelination, but does not fulfill the criteria for multiple sclerosis. 30 to 70% of persons experiencing CIS later develop MS.
Secondary progressive MS occurs in around 65% of those with initial relapsing-remitting MS, who eventually have progressive neurologic decline between acute attacks without any definite periods of remission. Occasional relapses and minor remissions may appear. The most common length of time between disease onset and conversion from relapsing-remitting to secondary progressive MS is 19 years.
The primary progressive subtype occurs in approximately 10–20% of individuals, with no remission after the initial symptoms. It is characterized by progression of disability from onset, with no, or only occasional and minor, remissions and improvements. The usual age of onset for the primary progressive subtype is later than of the relapsing-remitting subtype. It is similar to the age that secondary progressive usually begins in relapsing-remitting MS, around 40 years of age.
Progressive relapsing MS describes those individuals who, from onset, have a steady neurologic decline but also have clear superimposed attacks. This is the least common of all subtypes.
Unusual types of MS have been described; these include Devic's disease, Balo concentric sclerosis, Schilder's diffuse sclerosis, and Marburg multiple sclerosis. There is debate on whether they are MS variants or different diseases. Multiple sclerosis behaves differently in children, taking more time to reach the progressive stage. Nevertheless, they still reach it at a lower average age than adults usually do.
Management.
Although there is no known cure for multiple sclerosis, several therapies have proven helpful. The primary aims of therapy are returning function after an attack, preventing new attacks, and preventing disability. As with any medical treatment, medications used in the management of MS have several adverse effects. Alternative treatments are pursued by some people, despite the shortage of supporting evidence.
Acute attacks.
During symptomatic attacks, administration of high doses of intravenous corticosteroids, such as methylprednisolone, is the usual therapy, with oral corticosteroids seeming to have a similar efficacy and safety profile. Although, in general, effective in the short term for relieving symptoms, corticosteroid treatments do not appear to have a significant impact on long-term recovery. The consequences of severe attacks that do not respond to corticosteroids might be treatable by plasmapheresis.
Disease-modifying treatments.
Relapsing remitting multiple sclerosis.
As of 2014, nine disease-modifying treatments have been approved by regulatory agencies for relapsing-remitting multiple sclerosis (RRMS) including: interferon beta-1a, interferon beta-1b, glatiramer acetate, mitoxantrone, natalizumab, fingolimod, teriflunomide, dimethyl fumarate and alemtuzumab. Their cost effectiveness as of 2012 is unclear.
In RRMS they are modestly effective at decreasing the number of attacks. The interferons and glatiramer acetate are first-line treatments and are roughly equivalent, reducing relapses by approximately 30%. Early-initiated long-term therapy is safe and improves outcomes. Natalizumab reduces the relapse rate more than first-line agents; however, due to issues of adverse effects is a second-line agent reserved for those who do not respond to other treatments or with severe disease. Mitoxantrone, whose use is limited by severe adverse effects, is a third-line option for those who do not respond to other medications. Treatment of clinically isolated syndrome (CIS) with interferons decreases the chance of progressing to clinical MS. Efficacy of interferons and glatiramer acetate in children has been estimated to be roughly equivalent to that of adults. The role of some newer agents such as fingolimod, teriflunomide, and dimethyl fumarate, as of 2011, is not yet entirely clear.
Progressive multiple sclerosis.
No treatment has been shown to change the course of primary progressive MS and as of 2011 only one medication, mitoxantrone, has been approved for secondary progressive MS. In this population tentative evidence supports mitoxantrone moderately slowing the progression of the disease and decreasing rates of relapses over two years.
Adverse effects.
The disease-modifying treatments have several adverse effects. One of the most common is irritation at the injection site for glatiramer acetate and the interferons (up to 90% with subcutaneous injections and 33% with intramuscular injections). Over time, a visible dent at the injection site, due to the local destruction of fat tissue, known as lipoatrophy, may develop. Interferons may produce flu-like symptoms; some people taking glatiramer experience a post-injection reaction with flushing, chest tightness, heart palpitations, and anxiety, which usually lasts less than thirty minutes. More dangerous but much less common are liver damage from interferons, systolic dysfunction (12%), infertility, and acute myeloid leukemia (0.8%) from mitoxantrone, and progressive multifocal leukoencephalopathy occurring with natalizumab (occurring in 1 in 600 people treated).
Fingolimod may give rise to hypertension and slowed heart rate, macular edema, elevated liver enzymes or a reduction in lymphocyte levels. Tentative evidence supports the short-term safety of teriflunomide, with common side effects including: headaches, fatigue, nausea, hair loss, and limb pain. There have also been reports of liver failure and PML with its use and it is dangerous for fetal development. Most common side effects of dimethyl fumarate are flushing and gastrointestinal problems. While dimethyl fumarate may lead to a reduction in the white blood cell count there were no reported cases of opportunistic infections during trials.
Associated symptoms.
Both medications and neurorehabilitation have been shown to improve some symptoms, though neither changes the course of the disease. Some symptoms have a good response to medication, such as an unstable bladder and spasticity, while others are little changed. For neurologic problems, a multidisciplinary approach is important for improving quality of life; however, it is difficult to specify a 'core team' as many health services may be needed at different points in time. Multidisciplinary rehabilitation programs increase activity and participation of people with MS but do not influence impairment level. There is limited evidence for the overall efficacy of individual therapeutic disciplines, though there is good evidence that specific approaches, such as exercise, and psychology therapies, in particular cognitive behavioral approaches are effective.
Alternative treatments.
Over 50% of people with MS may use complementary and alternative medicine, although percentages vary depending on how alternative medicine is defined. The evidence for the effectiveness for such treatments in most cases is weak or absent. Treatments of unproven benefit used by people with MS include dietary supplementation and regimens, vitamin D, relaxation techniques such as yoga, herbal medicine (including medical cannabis), hyperbaric oxygen therapy, self-infection with hookworms, reflexology, and acupuncture. Regarding the characteristics of users, they are more frequently women, have had MS for a longer time, tend to be more disabled and have lower levels of satisfaction with conventional healthcare.
Prognosis.
[[File:Multiple sclerosis world map - DALY - WHO2004.svg|thumb|Disability-adjusted life year for multiple sclerosis per 100,000 inhabitants in 2004
The expected future course of the disease depends on the subtype of the disease; the individual's sex, age, and initial symptoms; and the degree of disability the person has. Female sex, relapsing-remitting subtype, optic neuritis or sensory symptoms at onset, few attacks in the initial years and especially early age at onset, are associated with a better course.
The average life expectancy is 30 years from the start of the disease, which is 5 to 10 years less than that of unaffected people. Almost 40% of people with MS reach the seventh decade of life. Nevertheless, two-thirds of the deaths are directly related to the consequences of the disease. Suicide is more common, while infections and other complications are especially dangerous for the more disabled. Although most people lose the ability to walk before death, 90% are capable of independent walking at 10 years from onset, and 75% at 15 years. 
Epidemiology.
MS is the most common autoimmune disorder of the central nervous system. As of 2010, the number of people with MS was 2–2.5 million (approximately 30 per 100,000) globally, with rates varying widely in different regions. It is estimated to have resulted in 18,000 deaths that year. In Africa rates are less than 0.5 per 100,000, while they are 2.8 per 100,000 in South East Asia, 8.3 per 100,000 in the Americas, and 80 per 100,000 in Europe. Rates surpass 200 per 100,000 in certain populations of Northern European descent. The number of new cases that develop per year is about 2.5 per 100,000.
Rates of MS appear to be increasing; this, however, may be explained simply by better diagnosis. Studies on populational and geographical patterns have been common and have led to a number of theories about the cause.
MS usually appears in adults in their late twenties or early thirties but it can rarely start in childhood and after 50 years of age. The primary progressive subtype is more common in people in their fifties. Similar to many autoimmune disorders, the disease is more common in women, and the trend may be increasing. As of 2008, globally it is about two times more common in women than in men. In children, it is even more common in females than males, while in people over fifty, it affects males and females almost equally.
History.
Medical discovery.
Robert Carswell (1793–1857), a British professor of pathology, and Jean Cruveilhier (1791–1873), a French professor of pathologic anatomy, described and illustrated many of the disease's clinical details, but did not identify it as a separate disease. Specifically, Carswell described the injuries he found as "a remarkable lesion of the spinal cord accompanied with atrophy". Under the microscope, Swiss pathologist Georg Eduard Rindfleisch (1836–1908) noted in 1863 that the inflammation-associated lesions were distributed around blood vessels. 
The French neurologist Jean-Martin Charcot (1825–1893) was the first person to recognize multiple sclerosis as a distinct disease in 1868. Summarizing previous reports and adding his own clinical and pathological observations, Charcot called the disease "sclerose en plaques".
Diagnosis.
The first attempt to establish a set of diagnostic criteria was also due to Charcot in 1868. He published what now is known as the "Charcot Triad", consisting in nystagmus, intention tremor, and telegraphic speech (scanning speech) Charcot also observed cognition changes, describing his patients as having a "marked enfeeblement of the memory" and "conceptions that formed slowly".
Diagnosis was based in Charcot triad and clinical observation until Schumacher made the first attempt to standardize criteria in 1965 by introducing some fundamental requirements: Dissemination of the lesions in time (DIT) and space (DIS), and that "signs and symptoms cannot be explained better by another disease process". Both requirements were later inherited by Poser criteria and McDonald criteria, whose 2010 version is currently in use.
During the 20th century theories about the cause and pathogenesis were developed and effective treatments began to appear in the 1990s.
Historical cases.
There are several historical accounts of people who probably had MS and lived before or shortly after the disease was described by Charcot.
A young woman called Halldora who lived in Iceland around 1200 suddenly lost her vision and mobility but, after praying to the saints, recovered them seven days after. Saint Lidwina of Schiedam (1380–1433), a Dutch nun, may be one of the first clearly identifiable people with MS. From the age of 16 until her death at 53, she had intermittent pain, weakness of the legs, and vision loss—symptoms typical of MS. Both cases have led to the proposal of a "Viking gene" hypothesis for the dissemination of the disease.
Augustus Frederick d'Este (1794–1848), son of Prince Augustus Frederick, Duke of Sussex and Lady Augusta Murray and the grandson of George III of the United Kingdom, almost certainly had MS. D'Este left a detailed diary describing his 22 years living with the disease. His diary began in 1822 and ended in 1846, although it remained unknown until 1948. His symptoms began at age 28 with a sudden transient visual loss (amaurosis fugax) after the funeral of a friend. During his disease, he developed weakness of the legs, clumsiness of the hands, numbness, dizziness, bladder disturbances, and erectile dysfunction. In 1844, he began to use a wheelchair. Despite his illness, he kept an optimistic view of life. Another early account of MS was kept by the British diarist W. N. P. Barbellion, nom-de-plume of Bruce Frederick Cummings (1889–1919), who maintained a detailed log of his diagnosis and struggle. His diary was published in 1919 as "The Journal of a Disappointed Man".
Research.
Medications.
There is ongoing research looking for more effective, convenient, and tolerable treatments for relapsing-remitting MS; creation of therapies for the progressive subtypes; neuroprotection strategies; and effective symptomatic treatments.
During the 2000s and 2010s, there has been approval of several oral drugs that are expected to gain in popularity and frequency of use. Several more oral drugs are under investigation, including ozanimod and laquinimod. Laquinimod was announced in August 2012 and is in a third phase III trial after mixed results in the previous ones. Similarly, studies aimed to improve the efficacy and ease of use of already existing therapies are occurring. This includes the use of new preparations such as the PEGylated version of interferon-β-1a, which it is hoped may be given at less frequent doses with similar effects. Request for approval of "peginterferon beta-1a" is expected during 2013.
Monoclonal antibodies have also raised high levels of interest. Alemtuzumab, daclizumab, and CD20 monoclonal antibodies such as rituximab, ocrelizumab and ofatumumab have all shown some benefit and are under study as potential treatments. Their use has also been accompanied by the appearance of potentially dangerous adverse effects, the most important of which being opportunistic infections. Related to these investigations is the development of a test for JC virus antibodies, which might help to determine who is at greater risk of developing progressive multifocal leukoencephalopathy when taking natalizumab. While monoclonal antibodies will probably have some role in the treatment of the disease in the future, it is believed that it will be small due to the risks associated with them.
Another research strategy is to evaluate the combined effectiveness of two or more drugs. The main rationale for using a number of medications in MS is that the involved treatments target different mechanisms and, therefore, their use is not necessarily exclusive. Synergies, in which one drug improves the effect of another are also possible, but there can also be drawbacks such as the blocking of the action of the other or worsened side-effects. There have been several trials of combined therapy, yet none have shown positive enough results to be considered as a useful treatment for MS.
Research on neuroprotection and regenerative treatments, such as stem cell therapy, while of high importance, are in the early stages. Likewise, there are not any effective treatments for the progressive variants of the disease. Many of the newest drugs as well as those under development are probably going to be evaluated as therapies for PPMS or SPMS.
Disease biomarkers.
While diagnostic criteria are not expected to change in the near future, work to develop biomarkers that help with diagnosis and prediction of disease progression is ongoing. New diagnostic methods that are being investigated include work with anti-myelin antibodies, and studies with serum and cerebrospinal fluid, but none of them has yielded reliably positive results.
At the current time, there are no laboratory investigations that can predict prognosis. Several promising approaches have been proposed including: interleukin-6, nitric oxide and nitric oxide synthase, osteopontin, and fetuin-A. Since disease progression is the result of degeneration of neurons, the roles of proteins showing loss of nerve tissue such as neurofilaments, tau, and N-acetylaspartate are under investigation. Other effects include looking for biomarkers that distinguish between those who will and will not respond to medications.
Improvement in neuroimaging techniques such as positron emission tomography (PET) or magnetic resonance imaging (MRI) carry a promise for better diagnosis and prognosis predictions, although the effect of such improvements in daily medical practice may take several decades. Regarding MRI, there are several techniques that have already shown some usefulness in research settings and could be introduced into clinical practice, such as double-inversion recovery sequences, magnetization transfer, diffusion tensor, and functional magnetic resonance imaging. These techniques are more specific for the disease than existing ones, but still lack some standardization of acquisition protocols and the creation of normative values. There are other techniques under development that include contrast agents capable of measuring levels of peripheral macrophages, inflammation, or neuronal dysfunction, and techniques that measure iron deposition that could serve to determine the role of this feature in MS, or that of cerebral perfusion. Similarly, new PET radiotracers might serve as markers of altered processes such as brain inflammation, cortical pathology, apoptosis, or remylienation. Antibiodies against the Kir4.1 potassium channel may be related to MS.
Chronic cerebrospinal venous insufficiency.
In 2008, vascular surgeon Paolo Zamboni suggested that MS involves narrowing of the veins draining the brain, which he referred to as chronic cerebrospinal venous insufficiency (CCSVI). He found CCSVI in all patients with MS in his study, performed a surgical procedure, later called in the media the "liberation procedure" to correct it, and claimed that 73% of participants improved. This theory received significant attention in the media and among those with MS, especially in Canada. Concerns have been raised with Zamboni's research as it was neither blinded nor controlled, and its assumptions about the underlying cause of the disease is not backed by known data. Also, further studies have either not found a similar relationship or found one that is much less strong, raising serious objections to the hypothesis. The "liberation procedure" has been criticized for resulting in serious complications and deaths with unproven benefits. It is, thus, as of 2013 not recommended for the treatment of MS. Additional research investigating the CCSVI hypothesis are under way.

</doc>
<doc id="50604" url="https://en.wikipedia.org/wiki?curid=50604" title="Interacting boson model">
Interacting boson model

The interacting boson model (IBM) is a model in nuclear physics in which
nucleons (protons or neutrons) pair up, essentially
acting as a single particle with boson properties, with
integral spin of 0, 2 or 4.
The IBM-I treats both types of nucleons the same and considers only pairs of nucleons coupled to
total angular momentum 0 and 2, called respectively, s and d bosons. 
The IBM-II treats protons and neutrons separately.
History.
This model was invented by Akito Arima and Francesco Iachello.

</doc>
