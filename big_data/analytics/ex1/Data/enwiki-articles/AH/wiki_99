<doc id="56101" url="https://en.wikipedia.org/wiki?curid=56101" title="Ethnic conflict">
Ethnic conflict

An ethnic conflict is a conflict between ethnic groups.
It contrasts with civil war, in which a single nation or ethnic group fights among itself, and conventional warfare, in which two or more sovereign states (which may or may not be nation states) are in conflict for reasons other than ethnicity.
The Yugoslav Wars, the First Chechen War, the Nagorno-Karabakh War, the Rwandan Civil War, the War in Darfur, and the Internal conflict in Myanmar are examples of ethnic wars triggered by secessionist movements in the 1990s. These wars led to the breakup of multi-ethnic states along ethnic lines.
Academic explanations of ethnic conflict generally fall into one of three schools of thought: primordialist, instrumentalist or constructivist. Recently, several political scientists have argued for either top-down or bottom-up explanations for ethnic conflict. Intellectual debate has also focused on whether ethnic conflict has become more prevalent since the end of the Cold War, and on devising ways of managing conflicts, through instruments such as consociationalism and federalisation.
Definition.
Ethnic groups share a "myth of common descent". Ethnic conflicts involve two contending groups. While the source of the conflict may be political, social, or economic, the combatants must expressly fight for an ethnic issue or for their ethnic group's position within society. This final criterion differentiates ethnic conflict from other forms of armed struggle.
Ethnic conflict does not necessarily have to be violent. In a multiethnic society where freedom of speech is protected, ethnic conflict can be an everyday feature of plural democracies. For example, ethnic conflict might be a non-violent struggle for resources divided among ethnic groups. However, the subject of the confrontation must be either directly or symbolically linked with an ethnic group. In healthy multiethnic democracies, these conflicts are usually institutionalized and "channeled through parliaments, assemblies and bureaucracies or through non-violent demonstrations and strikes." While democracy cannot always prevent ethnic conflict flaring up into violence, institutionalized ethnic conflict does ensure that ethnic groups can articulate their demands in a peaceful manner, which reduces the likelihood of violence. On the other hand, in authoritarian systems, ethnic minorities are often unable to express their grievances. Grievances are instead allowed to fester which might lead to long phases of ethnic silence followed by a violent outburst.
Theories of ethnic conflict.
The causes of ethnic conflict are debated by political scientists and sociologists. Explanations generally fall into one of three schools of thought: primordialist, instrumentalist, and constructivist. More recent scholarship draws on all three schools.
Primordialist accounts.
Proponents of primordialist accounts argues that “thnic groups and nationalities exist because there are traditions of belief and action towards primordial objects such as biological features and especially territorial location”. Primordialist accounts rely on strong ties of kinship among members of ethnic groups. Donald L. Horowitz argues that this kinship “makes it possible for ethnic groups to think in terms of family resemblances”.
Clifford Geertz, a founding scholar of primordialism, asserts that each person has a natural connection to perceived kinsmen. In time and through repeated conflict, essential ties to one's ethnie will coalesce and will interfere with ties to civil society. Ethnic groups will consequently always threaten the survival of civil governments but not the existence of nations formed by one ethnic group. Thus, ethnic conflict in multi-ethnic society is inevitable through a primordial lens.
There are a number of political scientists who argue that the root causes of ethnic conflict do not involve ethnicity "per se" but rather institutional, political, and economic factors. These scholars argue that the concept of ethnic war is misleading because it leads to an essentialist conclusion that certain groups are doomed to fight each other when in fact the wars between them occur are often the result of political decisions.
Moreover, primordial accounts do not account for the spatial and temporal variations in ethnic violence. If these "ancient hatreds" are always simmering under the surface and are at the forefront of people's consciousness, then we should see ethnic groups constantly ensnared in violence. Howeber, ethnic violence occurs in sporadic outbursts. For example, Varshney points out that although Yugoslaiva broke up due to ethnic violence in the 1990s, it did experience a long peace before the USSR collapsed. Therefore, it is unlikely that primordial ethnic differences alone caused the outbreak of violence in the 1990s.
However, primordialists have reformulated the "ancient hatreds" hypothesis and have focussed more on the role of human nature. Peterson argues that the existence of hatred and animosity does not have to be rooted in history for it to play a role in shaping human behavior and action: "If "ancient hatred" means a hatred consuming the daily thoughts of great masses of people, then the "ancient hatreds" argument deserves to be readily dismissed. However, if hatred is conceived as a historically formed "schema" that guides action in some situations, then the conception should be taken more seriously."
However, it is difficult to measure the importance of emotions in leading to outbreaks of ethnic violence and identify the factors that influence the intensity of hatred that ethnic groups harbor towards each other over time.
Instrumentalist accounts.
Anthony Smith notes that the instrumentalist account “came to prominence in the 1960s and 1970s in the United States, in the debate about (white) ethnic persistence in what was supposed to have been an effective melting pot”. This new theory sought explained persistence as the result of the actions of community leaders, “who used their cultural groups as sites of mass mobilization and as constituencies in their competition for power and resources, because they found them more effective than social classes”. In this account of ethnic identification, ethnicity and race are viewed as instrumental means to achieve particular ends.
Whether ethnicity is a fixed perception or not is not crucial in the instrumentalist accounts. Moreover, the scholars of this school generally do not oppose the view that ethnic difference plays a part in many conflicts. They simply claim that ethnic difference is not sufficient to explain conflicts.
Mass mobilization of ethnic groups can only be successful if there are latent ethnic differences to be exploited, otherwise politicians would not even attempt to make political appeals based on ethnicity and would focus instead on economic or ideological appeals. Hence, it is difficult to completely discount the role of inherent ethnic differences.
Furthermore, ethnic mass mobilization is likely to be plagued by collective action problems, especially if ethnic protests are likely to lead to violence. Instrumentalist scholars have tried to respond to these shortcomings. For example, Hardin argues that ethnic mobilization faces problems of coordination and not collective action. He points out that a charismatic leader acts as a focal point around which members of an ethnic group coalesce. The existence of such an actor helps to clarify beliefs about the behavior of others within an ethnic group.
Constructivist accounts.
A third, constructivist, set of accounts stress the importance of the socially constructed nature of ethnic groups, drawing on Benedict Anderson's concept of the imagined community. Proponents of this account point to Rwanda as an example because the Tutsi/Hutu distinction was codified by the Belgian colonial power in the 1930s on the basis of cattle ownership, physical measurements and church records. Identity cards were issued on this basis, and these documents played a key role in the genocide of 1994.
Constructivist narratives of historical master cleavages are unable to account for local and regional variations in ethnic violence. For example, Varshney highlights that in the 1960s "racial violence in the USA was heavily concentrated in northern cities; southern cities though intensely politically engaged, did not have riots". A constructivist master narrative is often a country level variable whereas we often have to study incidences of ethnic violence at the regional and local level.
Scholars of ethnic conflict and civil wars have introduced theories that draw insights from all three traditional schools of thought. In "The Geography of Ethnic Violence", for example, Monica Duffy Toft shows how ethnic group settlement patterns, socially constructed identities, charismatic leaders, issue indivisibility, and state concern with precedent setting can lead rational actors to escalate a dispute to violence, even when doing so is likely to leave contending groups much worse off. Such research addresses empirical puzzles that are difficult to explain using primordialist, instrumentalist, or constructivist approaches alone. As Varshney notes, "pure essentialists and pure instrumentalists do not exist anymore".
Ethnic conflict in the post–Cold War world.
The term "ethnicity" as used today arose in the mid-20th century, replacing the terminology of "races" or "nations" used in the 19th century. Regular warfare was formerly conceived as conflicts between nations, and only with the rise of multi-ethnic societies and the shift to asymmetric warfare did the concept of "ethnic conflict" arise as separate from generic "war".
This has been the case since the collapse of the multi-ethnic Soviet Union and of the relatively homogeneous Yugoslavia in the 1990s, both of which were followed by ethnic conflicts that escalated to violence and civil war.
The end of the Cold War thus sparked interest in two important questions about ethnic conflict: whether ethnic conflict was on the rise and whether given that some ethnic conflicts had escalated into serious violence, what, if anything, could scholars of large-scale violence (security studies, strategic studies, interstate politics) offer by way of explanation.
One of the most debated issues relating to ethnic conflict is whether it has become more or less prevalent in the post–Cold War period. At the end of the Cold War, academics including Samuel P. Huntington and Robert D. Kaplan predicted a proliferation of conflicts fuelled by civilisational clashes, tribalism, resource scarcity and overpopulation.
The post–Cold War period has witnessed a number of ethnically-informed secessionist movements, predominantly within the former communist states. Conflicts have involved secessionist movements in the former Yugoslavia, Transnistria in Moldova, Armenians in Azerbaijan, Abkhaz and Ossetians in Georgia. Outside the former communist bloc, ethno-separatist strife in the same period has occurred in areas such as Sri Lanka, West Papua, Chiapas, East Timor, the Basque Country Southern Sudan and Hazaras in Afghanistan under the Taliban.
However, some theorists contend that this does not represent a rise in the incidence of ethnic conflict, because many of the proxy wars fought during the Cold War as ethnic conflicts were actually hot spots of the Cold War. Research shows that the fall of Communism and the increase in the number of capitalist states were accompanied by a decline in total warfare, interstate wars, ethnic wars, revolutionary wars, and the number of refugees and displaced persons. Indeed, some scholars have questioned whether the concept of ethnic conflict is useful at all. Others have attempted to test the "clash of civilisations" thesis, finding it to be difficult to operationalise and that civilisational conflicts have not risen in intensity in relation to other ethnic conflicts since the end of the Cold War.
A key question facing scholars who attempt to adapt their theories of interstate violence to explain or predict large-scale ethnic violence is whether ethnic groups could be considered "rational" actors.
Prior to the end of the Cold War, the consensus among students of large-scale violence was that ethnic groups should be considered irrational actors, or semi-rational at best. If true, general explanations of ethnic violence would be impossible. In the years since, however, scholarly consensus has shifted to consider that ethnic groups may in fact be counted as rational actors, and the puzzle of their apparently irrational actions (for example, fighting over territory of little or no intrinsic worth) must therefore be explained in some other way. As a result, the possibility of a general explanation of ethnic violence has grown, and collaboration between comparativist and international-relations subfields has resulted in increasingly useful theories of ethnic conflict.
The book "" by Amy Chua argues that democratization may give political power to an ethnic majority that is poor compared to an ethnic minority that has become more economically successful. This may cause conflict, persecution, and even genocide of the minority.
Ethnic conflict mitigation.
A number of scholars have attempted to synthesize the methods available for the resolution, management or transformation of ethnic conflict. John Coakley, for example, has developed a typology of the methods of conflict resolution that have been employed by states, which he lists as: indigenization, accommodation, assimilation, acculturation, population transfer, boundary alteration, genocide and ethnic suicide. Greg Meyjes suggests that the degree to which ethnic tensions stem from inter-group disparity, dominance, discrimination, and repression has been critically unaccounted for—and proposes a cultural rights approach to understanding and managing ethnic conflicts.
John McGarry and Brendan O'Leary have developed a taxonomy of eight macro-political ethnic conflict regulation methods, which they note are often employed by states in combination with each other. They include a number of methods that they note are clearly morally unacceptable.
Institutional ethnic conflict resolution.
With increasing interest in the field of ethnic conflict, many policy analysts and political scientists theorized potential resolutions and tracked the results of institutional policy implementation. As such, many theories concerning which institutions are most appropriate to address ethnic conflict have been developed.
Consociationalism.
Consociationalism is a power sharing agreement which coopts the leaders of ethnic groups into the central state's government. Each nation or ethnic group is represented in the government through a supposed spokesman for the group. In the power sharing agreement, each group has veto powers to varying degrees, dependent on the particular state. Moreover, the norm of proportional representation is dominant: each group is represented in the government in a percentage that reflects the ethnicity's demographic presence in the state. Another requirement for Arend Lijphart is that the government must be composed of a "grand coalition" of the ethnic group leaders which supposes a top-down approach to conflict resolution.
In theory, this leads to self governance and protection for the ethnic group. Many scholars maintain that since ethnic tension erupts into ethnic violence when the ethnic group is threatened by a state, then veto powers should allow the ethnic group to avoid legislative threats. Switzerland is often characterized as a successful consociationalist state.
A recent example of a consociational government is the post-conflict Bosnian government that was agreed upon in the Dayton Accords in 1995. A tripartite presidency was chosen and must have a Croat, a Serb, and a Bosniak. The presidents take turns acting as the forefront executive in terms of 8 months for 4 years. Many have credited this compromise of a consociational government in Bosnia for the end of the violence and the following long-lasting peace.
In contrast to Lijphart, several political scientists and policy analysts have condemned consociationalism. One of the many critiques is that consociationalism locks in ethnic tensions and identities. This assumes a primordial stance that ethnic identities are permanent and not subject to change. Furthermore, this does not allow for any "others" that might want to partake in the political process. As of 2012 a Jewish Bosnian is suing the Bosnian government from precluding him from running for presidential office since only a Croat, Serb, or Bosniak can run under the consociational government.
Another critique points to the privileging of ethnic identity over personal political choice. Howard has deemed consociationalism as a form of ethnocracy and not a path to true pluralistic democracy. Consociationalism assumes that a politician will best represent the will of his co-ethnics above other political parties. This might lead to the polarization of ethnicity and the loss of non-ethnic ideological parties.
Federalism.
The theory of implementing federalism in order to curtail ethnic conflict assumes that self-governance reduces "demands for sovereignty". Hechter argues that some goods such as language of education and bureaucracy must be provided as local goods, instead of statewide, in order to satisfy more people and ethnic groups. Some political scientists such as Stroschein contend that ethno-federalism, or federalism determined along ethnic lines, is "asymmetric" as opposed to the equal devolution of power found in non-ethnic federal states, such as the United States. In this sense, special privileges are granted to specific minority groups as concessions and incentives to end violence or mute conflict.
The Soviet Union divided its structure into ethno-federal sub-states termed Union Republics. The sub-state was named after a titular minority who inhabited the area as a way to Sovietize nationalist sentiments during the 1920s. Brubaker asserts that these titular republics were formed in order to absorb any potential elite led nationalist movements against the Soviet center by incentivizing elite loyalty through advancement in the Soviet political structure.
Thus, federalism provides some self-governance for local matters in order to satisfy some of the grievances which might cause ethnic conflict among the masses. Moreover, federalism brings in the elites and ethnic entrepreneurs into the central power structure; this prevents a resurgence of top-down ethnic conflict.
Nevertheless, after the fall of the USSR many critiques of federalism as an institution to resolve ethnic conflict emerged. The devolution of power away from the central state can weaken ties to the central state. Moreover, the parallel institutions created to serve a particular nation or ethnic group might provide significant resources for secession from the central state. As most states are unwilling to give up an integral portion of their territory, secessionist movements may trigger violence.
Furthermore, some competing elite political players may not be in power; they would remain unincorporated into the central system. These competing elites can gain access through federal structures and their resources to solidify their political power in the structure. According to V.P. Gagnon this was the case in the former Yugoslavia and its violent disintegration into its ethno-federal sub-states. Ethnic entrepreneurs were able to take control of the institutionally allocated resources to wage war on other ethnic groups.
Non-territorial autonomy.
A recent theory of ethnic tension resolution is non-territorial autonomy or NTA.
Secession and independence.
In some cases, international actors will advocate for secession and independence as a method for solving ethnic conflict.

</doc>
<doc id="56102" url="https://en.wikipedia.org/wiki?curid=56102" title="Manneken Pis">
Manneken Pis

Manneken Pis (, meaning "Little man Pee" in Dutch) is a landmark small bronze sculpture (61 cm) in Brussels, depicting a naked little boy urinating into a fountain's basin. It was designed by Hiëronymus Duquesnoy the Elder and put in place in 1618 or 1619.
Location.
The famous statue is located at the junction of Rue de l'Étuve/Stoofstraat and Rue du Chêne/Eikstraat. To find it, one takes the left lane next to the Brussels Town Hall from the famous Grand Place and walks a few hundred metres southwest via Rue Charles Buls/Karel Bulsstraat.
History and legends.
The 61 cm tall bronze statue on the corner of Rue de l'Etuve and Rue des Grands Carmes was made in 1619 by Brussels sculptor Hieronimus Duquesnoy the Elder, father of the more famous . The figure has been repeatedly stolen: the current statue dates from 1965. The original restored version is kept at the Maison du Roi/Broodhuis on the Grand Place.
There are several legends behind this statue, but the most famous is the one about Duke Godfrey III of Leuven. In 1142, the troops of this two-year-old lord were battling against the troops of the Berthouts, the lords of Grimbergen, in Ransbeke (now Neder-Over-Heembeek). The troops put the infant lord in a basket and hung the basket in a tree to encourage them. From there, the boy urinated on the troops of the Berthouts, who eventually lost the battle.
Another legend states that in the 14th century, Brussels was under siege by a foreign power. The city had held its ground for some time, so the attackers conceived of a plan to place explosive charges at the city walls. A little boy named happened to be spying on them as they were preparing. He urinated on the burning fuse and thus saved the city. There was at the time (middle of the 15th century, perhaps as early as 1388) a similar statue made of stone. The statue was stolen several times.
Another story (told often to tourists) tells of a wealthy merchant who, during a visit to the city with his family, had his beloved young son go missing. The merchant hastily formed a search party that scoured all corners of the city until the boy was found happily urinating in a small garden. The merchant, as a gift of gratitude to the locals who helped out during the search, had the fountain built.
Another legend was that a small boy went missing from his mother when shopping in the centre of the city. The woman, panic-stricken by the loss of her child, called upon everyone she came across, including the mayor of the city. A city-wide search began and when at last the child was found, he was urinating on the corner of a small street. The story was passed down over time and the statue erected as a tribute to the well-known legend.
Another legend tells of the young boy who was awoken by a fire and was able to put out the fire with his urine, in the end this helped stop the king's castle from burning down.
Traditions.
The statue is dressed in costume several times each week, according to a published schedule which is posted on the railings around the fountain. His wardrobe consists of several hundred different costumes, many of which may be viewed in a permanent exhibition inside the City Museum, located in the Grand Place, immediately opposite the Town Hall. The costumes are managed by the non-profit association "The Friends of Manneken-Pis", who review hundreds of designs submitted each year, and select a small number to be produced and used.
Although the proliferation of costumes is of twentieth-century origin, the occasional use of costumes dates back almost to the date of casting, the oldest costume on display in the City Museum being of seventeenth-century origin. The changing of the costume on the figure is a colourful ceremony, often accompanied by brass band music. Many costumes represent the national dress of nations whose citizens come to Brussels as tourists; others are the uniforms of assorted trades, professions, associations, and branches of the civil and military services.
On occasion, the statue is hooked up to a keg of beer. Cups will be filled up with the beer flowing from the statue and given out to people passing by.
The statue has been stolen seven times, the last time in January 1963, by students of the Antwerp student association "De Wikings" of the Sint-Ignatius Handelshogeschool (Higher Business Education), now part of the Antwerp University, who "hijacked" Manneken Pis for five days before handing it over to the Antwerp authorities. The local and international press covered the story, contributing to the students' collection of funds donated to two orphanages.
There is also a statue of Manneken Pis in Tokushima, Japan, which was a present from the Belgian embassy (Tokushima being twinned with Brussels).
Since 1987, the Manneken has had a female equivalent, Jeanneke Pis, located on the east side of the "Impasse de la Fidélité/Getrouwheidsgang".
Replicas.
Although the "Manneken Pis" in Brussels is the best-known, others exist. There is an ongoing dispute over which Manneken Pis is the oldest - the one in Brussels or the one in Geraardsbergen. Similar statues can also be found in the Belgian cities of Koksijde, Hasselt, Ghent, Bruges, in the town of Braine-l'Alleud (where it is called "Il Gamin Quipiche"), and in the French Flemish village of Broxeele, a town with the same etymology as "Brussels".
In Bali, Indonesia, there is a Belgian restaurant called Mannekepis. It even has the exact replica of the statue standing in front of the restaurant, urinating.
In many countries, replicas in brass or fiberglass are commonplace swimming or garden-pool decorations. Many copies exist worldwide as garden ornaments. Manneken Pis has also been adapted into such risqué souvenir items as ashtrays and corkscrews.
In September 2002, a Belgian-born waffle-maker in Florida, named Assayag, set up a replica in front of his waffle stand in the Orlando Fashion Square mall in Orlando, Florida. He recalled the legend as 'the boy who saved Brussels from fire by extinguishing it with his urine' (confusing the legend with an incident in "Gulliver's Travels" perhaps). Some shocked shoppers made a formal complaint. Mall officials said that the waffle-shop owner did not follow procedures when he put up the statue and was therefore in violation of his lease.
In contrast, there is a similar statue in Rio de Janeiro in front of the quarters of Botafogo de Futebol e Regatas, a famous football club from Brazil. There, the presence of the statue is taken lightly, and it has even been adopted as a mascot by the club. Fans usually dress it with the club's jersey after important wins.
A working replica of Manneken Pis stands on the platform of Hamamatsuchō Station in Tokyo, Japan. The statue is a great source of pride for station workers who dress it in various costumes—traditional and otherwise—at different times of year.
In popular culture.
A promotional expansion for the board game "7 Wonders" allows a player to build an eighth wonder of the world: Manneken-Pis.
"Manneken Pis" is also the name of a book by Vladimir Radunsky.
The film "The Party", starring Peter Sellers, includes a reproduction of the statue in the house's extended water feature. The statue's peeing can be changed at an extended intercom panel, and Sellers, as Hrundi V. Bakshi, soaks a guest when he hits the wrong button.
In the 1986 film "The Money Pit", the lead character, Walter Fielding, played by Tom Hanks, accidentally falls in a construction area where workers are renovating his home. In a sequence reminiscent of a Rube Goldberg machine, he stumbles through a window, across the roof, down a scaffold, finally into a wheeled bin in which he rolls down a hill and is dumped into a fountain resting directly under a replica of the Manneken Pis.
The eponymous colony ship in Macross 7 has a park with what appears to be a parody of the original Manneken Pis statue.

</doc>
<doc id="56103" url="https://en.wikipedia.org/wiki?curid=56103" title="Stevie Smith">
Stevie Smith

Florence Margaret Smith, known as Stevie Smith (20 September 1902 – 7 March 1971) was an English poet and novelist.
Life.
Stevie Smith, born Florence Margaret Smith in Kingston upon Hull, was the second daughter of Ethel and Charles Smith. She was called "Peggy" within her family, but acquired the name "Stevie" as a young woman when she was riding in the park with a friend who said that she reminded him of the jockey Steve Donoghue.
Her father was a shipping agent, a business that he had inherited from his father. As the company and his marriage began to fall apart, he ran away to sea and Smith saw very little of her father after that. He appeared occasionally on 24-hour shore leave and sent very brief postcards ("Off to Valparaiso, Love Daddy").
When she was three years old she moved with her mother and sister to Palmers Green in North London where Smith would live until her death in 1971. She resented the fact that her father had abandoned his family. Later, when her mother became ill, her aunt Madge Spear (whom Smith called "The Lion Aunt") came to live with them, raised Smith and her elder sister Molly and became the most important person in Smith's life. Spear was a feminist who claimed to have "no patience" with men and, as Smith wrote, "she also had 'no patience' with Hitler". Smith and Molly were raised without men and thus became attached to their own independence, in contrast to what Smith described as the typical Victorian family atmosphere of "father knows best".
When Smith was five she developed tubercular peritonitis and was sent to a sanatorium near Broadstairs, Kent, where she remained for three years. She related that her preoccupation with death began when she was seven, at a time when she was very distressed at being sent away from her mother. Death and fear fascinated her and provide the subjects of many of her poems. Her mother died when Smith was 16.
When suffering from the depression to which she was subject all her life she was so consoled by the thought of death as a release that, as she put it, she did not have to commit suicide. She wrote in several poems that death was "the only god who must come when he is called". Smith suffered throughout her life from an acute nervousness, described as a mix of shyness and intense sensitivity.
In the Poem "A House of Mercy", she wrote of her childhood house in North London:
<poem>
It was a house of female habitation,
Two ladies fair inhabited the house,
And they were brave. For although Fear knocked loud
Upon the door, and said he must come in,
They did not let him in.
</poem>
Smith was educated at Palmers Green High School and North London Collegiate School for Girls. She spent the remainder of her life with her aunt, and worked as private secretary to Sir Neville Pearson with Sir George Newnes at Newnes Publishing Company in London from 1923 to 1953. Despite her secluded life, she corresponded and socialised widely with other writers and creative artists, including Elisabeth Lutyens, Sally Chilver, Inez Holden, Naomi Mitchison, Isobel English and Anna Kallin. 
After she retired from Sir Neville Pearson's service following a nervous breakdown she gave poetry readings and broadcasts on the BBC that gained her new friends and readers among a younger generation. Sylvia Plath became a fan of her poetry and sent Smith a letter in 1962, describing herself as "a desperate Smith-addict." Plath expressed interest in meeting in person but committed suicide soon after sending the letter.
Smith was described by her friends as being naive and selfish in some ways and formidably intelligent in others, having been raised by her aunt as both a spoiled child and a resolutely autonomous woman. Likewise, her political views vacillated between her aunt's Toryism and her friends' left-wing tendencies. Smith was celibate for most of her life, although she rejected the idea that she was lonely as a result, alleging that she had a number of intimate relationships with friends and family that kept her fulfilled. She never entirely abandoned or accepted the Anglican faith of her childhood, describing herself as a "lapsed atheist", and wrote sensitively about theological puzzles;"There is a God in whom I do not believe/Yet to this God my love stretches." Her 14-page essay of 1958, "The Necessity of Not Believing", concludes: "There is no reason to be sad, as some people are sad when they feel religion slipping off from them. There is no reason to be sad, it is a good thing."
Smith died of a brain tumour on 7 March 1971. Her last collection, "Scorpion and other Poems" was published posthumously in 1972, and the "Collected Poems" followed in 1975. Three novels were republished and there was a successful play based on her life, "Stevie", written by Hugh Whitemore. It was filmed in 1978 by Robert Enders and starred Glenda Jackson and Mona Washbourne.
Fiction.
Smith wrote three novels, the first of which, "Novel on Yellow Paper", was published in 1936. Apart from death, common subjects in her writing include loneliness; myth and legend; absurd vignettes, usually drawn from middle-class British life, war, human cruelty and religion. All her novels are lightly fictionalised accounts of her own life, which got her into trouble at times as people recognised themselves. Smith said that two of the male characters in her last book are different aspects of George Orwell, who was close to Smith. There were rumours that they were lovers; he was married to his first wife at the time.
"Novel on Yellow Paper" (Cape, 1936).
Smith's first novel is structured as the random typings of a bored secretary, Pompey. She plays word games, retells stories from classical and popular culture, remembers events from her childhood, gossips about her friends and describes her family, particularly her beloved Aunt. As with all Smith's novels, there is an early scene where the heroine expresses feelings and beliefs which she will later feel significant, although ambiguous, regret for. In "Novel on Yellow Paper" that belief is anti-Semitism, where she feels elation at being the "only Goy" at a Jewish party. This apparently throwaway scene acts as a timebomb, which detonates at the centre of the novel when Pompey visits Germany as the Nazis are gaining power. With horror, she acknowledges the continuity between her feeling "Hurray for being a Goy" at the party and the madness that is overtaking Germany. The German scenes stand out in the novel, but perhaps equally powerful is her dissection of failed love. She describes two unsuccessful relationships, first with the German Karl and then with the suburban Freddy. The final section of the novel describes with unusual clarity the intense pain of her break-up with Freddy.
"Over the Frontier" (Cape, 1938).
Smith herself dismissed her second novel as a failed experiment, but its attempt to parody popular genre fiction to explore profound political issues now seems to anticipate post-modern fiction. If anti-Semitism was one of the key themes of "Novel on Yellow Paper", "Over the Frontier" is concerned with militarism. In particular, she asks how the necessity of fighting Fascism can be achieved without descending into the nationalism and dehumanisation that fascism represents. After a failed romance the heroine, Pompey, suffers a breakdown and is sent to Germany to recuperate. At this point the novel changes style radically, as Pompey becomes part of an adventure/spy yarn in the style of John Buchan or Dornford Yates. As the novel becomes increasingly dreamlike, Pompey crosses over the frontier to become a spy and soldier. If her initial motives are idealistic, she becomes seduced by the intrigue and, ultimately, violence. The vision Smith offers is a bleak one: "Power and cruelty are the strengths of our lives, and only in their weakness is there love."
"The Holiday" (Chapman and Hall, 1949).
Smith's final novel is her own favourite, and most fully realised. It is concerned with personal and political malaise in the immediate post-war period. Most of the characters are either employed in the army or civil service in post-war reconstruction, and its heroine, Celia, works for the Ministry as a cryptographer and propagandist. "The Holiday" describes a series of hopeless relationships. Celia and her cousin Caz are in love, but cannot pursue their affair since it is believed that, because of their parents' adultery, they are half-brother and sister. Celia's other cousin Tom is in love with her, Basil is love with Tom, Tom is estranged from his father, Celia's beloved Uncle Heber, who pines for a reconciliation; and Celia's best friend Tiny longs for the married Vera. These unhappy, futureless but intractable relationships are mirrored by the novel's political concerns. The unsustainability of the British Empire and the uncertainty over Britain's post-war role are constant themes, and many of the characters discuss their personal and political concerns as if they were seamlessly linked. Caz is on leave from Palestine and is deeply disillusioned, Tom goes mad during the war, and it is telling that the family scandal that blights Celia and Caz's lives took place in India. Just as Pompey's anti-semitism was tested in "Novel on Yellow Paper", so Celia's traditional nationalism and sentimental support for colonialism is challenged throughout "The Holiday".
Poetry.
Smith's first volume of poetry, the self-illustrated "A Good Time Was Had By All", was published in 1937 and established her as a poet. Soon her poems were found in periodicals. Her style was often very dark; her characters were perpetually saying "goodbye" to their friends or welcoming death. At the same time her work has an eerie levity and can be very funny though it is neither light nor whimsical. "Stevie Smith often uses the word 'peculiar' and it is the best word to describe her effects" (Hermione Lee). She was never sentimental, undercutting any pathetic effects with the ruthless honesty of her humour.
"A good time was had by all" itself became a catch phrase, still occasionally used to this day. Smith said she got the phrase from parish magazines, where descriptions of church picnics often included this phrase. This saying has become so familiar that it is recognised even by those who are unaware of its origin. Variations appear in pop culture, including "Being for the Benefit of Mr. Kite!" by the Beatles.
Though her poems were remarkably consistent in tone and quality throughout her life, their subject matter changed over time, with less of the outrageous wit of her youth and more reflection on suffering, faith and the end of life. Her best-known poem is "Not Waving but Drowning". She was awarded the Cholmondeley Award for Poets in 1966 and won the Queen's Gold Medal for poetry in 1969. She published nine volumes of poems in her lifetime (three more were released posthumously).

</doc>
<doc id="56106" url="https://en.wikipedia.org/wiki?curid=56106" title="Wildfire">
Wildfire

A wildfire or wildland fire is an uncontrolled fire in an area of combustible vegetation that occurs in the countryside area. Depending on the type of vegetation that is burned, a wildfire can also be classified as a brush fire, bush fire, forest fire, desert fire, grass fire, hill fire, peat fire, vegetation fire, or veld fire. A wildfire differs from other fires by its extensive size, the speed at which it can spread out from its original source, its potential to change direction unexpectedly, and its ability to jump gaps such as roads, rivers and fire breaks. Wildfires are characterized in terms of the cause of ignition, their physical properties such as speed of propagation, the combustible material present, and the effect of weather on the fire.
Bushfires in Australia are a common occurrence; because of the generally hot and dry climate, they pose a great risk to life and infrastructure during all times of the year, though mostly throughout the hotter months of summer and spring. In the United States, there are typically between 60,000 and 80,000 wildfires that occur each year, burning 3 million to 10 million acres (12,000 to 40,000 square kilometres) of land depending on the year. Fossil records and human history contain accounts of wildfires, as wildfires can occur in periodic intervals. Wildfires can cause extensive damage, both to property and human life, but they also have various beneficial effects on wilderness areas. Some plant species depend on the effects of fire for growth and reproduction, although large wildfires may also have negative ecological effects.
Strategies of wildfire prevention, detection, and suppression have varied over the years, and international wildfire management experts encourage further development of technology and research. One of the more controversial techniques is controlled burning: permitting or even igniting smaller fires to minimize the amount of flammable material available for a potential wildfire. While some wildfires burn in remote forested regions, they can cause extensive destruction of homes and other property located in the wildland-urban interface: a zone of transition between developed areas and undeveloped wilderness.
The name "wildfire" was once a synonym for Greek fire but now refers to any large or destructive conflagration. Wildfires differ from other fires in that they take place outdoors in areas of grassland, woodlands, bushland, scrubland, peatland, and other wooded areas that act as a source of fuel, or combustible material. All wildfires can be characterized by their physical properties, fuel type, or weather's effect on the fire, regardless of the fire's cause or outcome.
Wildfire behaviour and severity result from the combination of factors such as available fuels, physical setting, and weather. While wildfires can be large, uncontrolled disasters that burn through or more, they can also be as small as or less. Although smaller events may be included in wildfire modeling, most do not earn press attention. This can be problematic because public fire policies, which relate to fires of all sizes, are influenced more by the way the media portrays catastrophic wildfires than by small fires.
Causes.
Wildfires are 'quasi-natural' hazards, meaning that they are not entirely natural features (like volcanoes, earthquakes and tropical storms). This is because they are caused by human activity as well. The four major natural causes of wildfire ignitions are lightning, volcanic eruption, sparks from rockfalls, and spontaneous combustion. The thousands of coal seam fires that are burning around the world, such as those in Centralia, Burning Mountain, and several coal-sustained fires in China, can also flare up and ignite nearby flammable material. The most common human sources of wildfires are arson, discarded cigarettes, sparks from equipment, and power line arcs (as detected by arc mapping). Ignition of wildland fires via contact with hot rifle bullet fragments is possible under the right conditions. In societies experiencing shifting cultivation where land is cleared quickly and farmed until the soil loses fertility, slash and burn clearing is often considered the least expensive way to prepare land for future use. Forested areas cleared by logging encourage the dominance of flammable grasses, and abandoned logging roads overgrown by vegetation may act as fire corridors. Annual grassland fires in southern Vietnam can be attributed in part to the destruction of forested areas by US military herbicides, explosives, and mechanical land clearing and burning operations during the Vietnam War.
The most common cause of wildfires varies throughout the world. In Canada and northwest China, for example, lightning is the major source of ignition. In other parts of the world, human involvement is a major contributor. In Mexico, Central America, South America, Africa, Southeast Asia, Fiji, and New Zealand, wildfires can be attributed to human activities such as animal husbandry, agriculture, and land-conversion burning. Human carelessness is a major cause of wildfires in China and in the Mediterranean Basin. In the United States and Australia, the source of wildfires can be traced to both lightning strikes and human activities such as machinery sparks and cast-away cigarette butts."
On a yearly basis in the United States, typically more than six times the number of wildfires are caused by human means such as campfires and controlled agricultural burns than by natural means. However, in any given year there could be far more acres burned by wildfires that are started by natural means than by human means as well as vice versa. For example, in 2010, almost 1.4 million acres were burned by human-caused wildfires, and over 2 million acres were burned by naturally-caused wildfires. However, far more acres were burned by human-caused fires in 2011, when almost 5.4 million acres were burned by human-caused wildfires, and only about 3.4 million acres were caused by naturally-derived wildfires.
Fuel type.
The spread of wildfires varies based on the flammable material present and its vertical arrangement. For example, fuels uphill from a fire are more readily dried and warmed by the fire than those downhill, yet burning logs can roll downhill from the fire to ignite other fuels. Fuel arrangement and density is governed in part by topography, as land shape determines factors such as available sunlight and water for plant growth. Overall, fire types can be generally characterized by their fuels as follows:
Physical properties.
Wildfires occur when all of the necessary elements of a fire triangle come together in a susceptible area: an ignition source is brought into contact with a combustible material such as vegetation, that is subjected to sufficient heat and has an adequate supply of oxygen from the ambient air. A high moisture content usually prevents ignition and slows propagation, because higher temperatures are required to evaporate any water within the material and heat the material to its fire point. Dense forests usually provide more shade, resulting in lower ambient temperatures and greater humidity, and are therefore less susceptible to wildfires. Less dense material such as grasses and leaves are easier to ignite because they contain less water than denser material such as branches and trunks. Plants continuously lose water by evapotranspiration, but water loss is usually balanced by water absorbed from the soil, humidity, or rain. When this balance is not maintained, plants dry out and are therefore more flammable, often a consequence of droughts.
A wildfire "front" is the portion sustaining continuous flaming combustion, where unburned material meets active flames, or the smoldering transition between unburned and burned material. As the front approaches, the fire heats both the surrounding air and woody material through convection and thermal radiation. First, wood is dried as water is vaporized at a temperature of . Next, the pyrolysis of wood at releases flammable gases. Finally, wood can smoulder at or, when heated sufficiently, ignite at . Even before the flames of a wildfire arrive at a particular location, heat transfer from the wildfire front warms the air to , which pre-heats and dries flammable materials, causing materials to ignite faster and allowing the fire to spread faster. High-temperature and long-duration surface wildfires may encourage flashover or "torching": the drying of tree canopies and their subsequent ignition from below. 
Wildfires have a rapid "forward rate of spread" (FROS) when burning through dense, uninterrupted fuels. They can move as fast as in forests and in grasslands. Wildfires can advance tangential to the main front to form a "flanking" front, or burn in the opposite direction of the main front by "backing". They may also spread by "jumping" or "spotting" as winds and vertical convection columns carry "firebrands" (hot wood embers) and other burning materials through the air over roads, rivers, and other barriers that may otherwise act as firebreaks. Torching and fires in tree canopies encourage spotting, and dry ground fuels that surround a wildfire are especially vulnerable to ignition from firebrands. Spotting can create "spot fires" as hot embers and firebrands ignite fuels downwind from the fire. In Australian bushfires, spot fires are known to occur as far as from the fire front.
Especially large wildfires may affect air currents in their immediate vicinities by the stack effect: air rises as it is heated, and large wildfires create powerful updrafts that will draw in new, cooler air from surrounding areas in thermal columns. Great vertical differences in temperature and humidity encourage pyrocumulus clouds, strong winds, and fire whirls with the force of tornadoes at speeds of more than . Rapid rates of spread, prolific crowning or spotting, the presence of fire whirls, and strong convection columns signify extreme conditions.
The thermal heat from wildfire can cause significant weathering of rocks and boulders, heat can rapidly expand a boulder and thermal shock can occur, which may result in an object's structure to fail.
Effect of weather.
Heat waves, droughts, cyclical climate changes such as El Niño, and regional weather patterns such as high-pressure ridges can increase the risk and alter the behavior of wildfires dramatically. Years of precipitation followed by warm periods can encourage more widespread fires and longer fire seasons. Since the mid-1980s, earlier snowmelt and associated warming has also been associated with an increase in length and severity of the wildfire season in the Western United States. However, one individual element does not always cause an increase in wildfire activity. For example, wildfires will not occur during a drought unless accompanied by other factors, such as lightning (ignition source) and strong winds (mechanism for rapid spread). A 2015 study indicates that the increase in fire risk in California is attributable to human-induced climate change.
Intensity also increases during daytime hours. Burn rates of smoldering logs are up to five times greater during the day due to lower humidity, increased temperatures, and increased wind speeds. Sunlight warms the ground during the day which creates air currents that travel uphill. At night the land cools, creating air currents that travel downhill. Wildfires are fanned by these winds and often follow the air currents over hills and through valleys. Fires in Europe occur frequently during the hours of 12:00 p.m. and 2:00 p.m. Wildfire suppression operations in the United States revolve around a 24-hour "fire day" that begins at 10:00 a.m. due to the predictable increase in intensity resulting from the daytime warmth.
Ecology.
Wildfires are common in climates that are sufficiently moist to allow the growth of vegetation but feature extended dry, hot periods. Such places include the vegetated areas of Australia and Southeast Asia, the veld in southern Africa, the fynbos in the Western Cape of South Africa, the forested areas of the United States and Canada, and the Mediterranean Basin. Fires can be particularly intense during days of strong winds, periods of drought, and during warm summer months. Global warming may increase the intensity and frequency of droughts in many areas, creating more intense and frequent wildfires.
Although some ecosystems rely on naturally occurring fires to regulate growth, many ecosystems suffer from too much fire, such as the chaparral in southern California and lower elevation deserts in the American Southwest. The increased fire frequency in these ordinarily fire-dependent areas has upset natural cycles, destroyed native plant communities, and encouraged the growth of fire-intolerant vegetation and non-native weeds. Invasive species, such as "Lygodium microphyllum" and "Bromus tectorum", can grow rapidly in areas that were damaged by fires. Because they are highly flammable, they can increase the future risk of fire, creating a positive feedback loop that increases fire frequency and further destroys native growth.
In the Amazon Rainforest, drought, logging, cattle ranching practices, and slash-and-burn agriculture damage fire-resistant forests and promote the growth of flammable brush, creating a cycle that encourages more burning. Fires in the rainforest threaten its collection of diverse species and produce large amounts of CO2. Also, fires in the rainforest, along with drought and human involvement, could damage or destroy more than half of the Amazon rainforest by the year 2030. Wildfires generate ash, destroy available organic nutrients, and cause an increase in water runoff, eroding away other nutrients and creating flash flood conditions. A 2003 wildfire in the North Yorkshire Moors destroyed of heather and the underlying peat layers. Afterwards, wind erosion stripped the ash and the exposed soil, revealing archaeological remains dating back to 10,000 BC. Wildfires can also have an effect on climate change, increasing the amount of carbon released into the atmosphere and inhibiting vegetation growth, which affects overall carbon uptake by plants.
In tundra there is a natural pattern of accumulation of fuel and wildfire which varies depending on the nature of vegetation and terrain. Research in Alaska has shown fire-event return intervals, (FRIs) that typically vary from 150 to 200 years with dryer lowland areas burning more frequently than wetter upland areas.
Plant adaptation.
Plants in wildfire-prone ecosystems often survive through adaptations to their local fire regime. Such adaptations include physical protection against heat, increased growth after a fire event, and flammable materials that encourage fire and may eliminate competition. For example, plants of the genus "Eucalyptus" contain flammable oils that encourage fire and hard sclerophyll leaves to resist heat and drought, ensuring their dominance over less fire-tolerant species. Dense bark, shedding lower branches, and high water content in external structures may also protect trees from rising temperatures. Fire-resistant seeds and reserve shoots that sprout after a fire encourage species preservation, as embodied by pioneer species. Smoke, charred wood, and heat can stimulate the germination of seeds in a process called "serotiny". Exposure to smoke from burning plants promotes germination in other types of plants by inducing the production of the orange butenolide.
Grasslands in Western Sabah, Malaysian pine forests, and Indonesian "Casuarina" forests are believed to have resulted from previous periods of fire. Chamise deadwood litter is low in water content and flammable, and the shrub quickly sprouts after a fire. Cape lilies lie dormant until flames brush away the covering, then blossom almost overnight. Sequoia rely on periodic fires to reduce competition, release seeds from their cones, and clear the soil and canopy for new growth. Caribbean Pine in Bahamian pineyards have adapted to and rely on low-intensity, surface fires for survival and growth. An optimum fire frequency for growth is every 3 to 10 years. Too frequent fires favor herbaceous plants, and infrequent fires favor species typical of Bahamian dry forests.
Atmospheric effects.
Most of the Earth's weather and air pollution resides in the troposphere, the part of the atmosphere that extends from the surface of the planet to a height of about . The vertical lift of a severe thunderstorm or pyrocumulonimbus can be enhanced in the area of a large wildfire, which can propel smoke, soot, and other particulate matter as high as the lower stratosphere. Previously, prevailing scientific theory held that most particles in the stratosphere came from volcanoes, but smoke and other wildfire emissions have been detected from the lower stratosphere. Pyrocumulus clouds can reach over wildfires. Satellite observation of smoke plumes from wildfires revealed that the plumes could be traced intact for distances exceeding . Computer-aided models such as CALPUFF may help predict the size and direction of wildfire-generated smoke plumes by using atmospheric dispersion modeling.
Wildfires can affect climate and weather and have major impacts on atmospheric pollution. To determine climate variability and its impact, tools such as remote sensing, fire danger rating systems, and fire behavior models have been used.[http://apps.webofknowledge.com.libezp.lib.lsu.edu/full_record.do?product=UA&search_mode=GeneralSearch&qid=1&SID=3A7lyhAIveCBgjBAcZa&page=1&doc=8] Wildfire emissions contain fine particulate matter which can cause cardiovascular and respiratory problems. Increased fire byproducts in the troposphere can increase ozone concentration beyond safe levels. Forest fires in Indonesia in 1997 were estimated to have released between 0.81 and 2.57 gigatonnes (0.89 and 2.83 billion short tons) of CO2 into the atmosphere, which is between 13%–40% of the annual global carbon dioxide emissions from burning fossil fuels. Atmospheric models suggest that these concentrations of sooty particles could increase absorption of incoming solar radiation during winter months by as much as 15%.
History.
In the Welsh Borders, the first evidence of wildfire is rhyniophytoid plant fossils preserved as charcoal, dating to the Silurian period (about ). Smoldering surface fires started to occur sometime before the Early Devonian period . Low atmospheric oxygen during the Middle and Late Devonian was accompanied by a decrease in charcoal abundance. Additional charcoal evidence suggests that fires continued through the Carboniferous period. Later, the overall increase of atmospheric oxygen from 13% in the Late Devonian to 30-31% by the Late Permian was accompanied by a more widespread distribution of wildfires. Later, a decrease in wildfire-related charcoal deposits from the late Permian to the Triassic periods is explained by a decrease in oxygen levels.
Wildfires during the Paleozoic and Mesozoic periods followed patterns similar to fires that occur in modern times. Surface fires driven by dry seasons are evident in Devonian and Carboniferous progymnosperm forests. Lepidodendron forests dating to the Carboniferous period have charred peaks, evidence of crown fires. In Jurassic gymnosperm forests, there is evidence of high frequency, light surface fires. The increase of fire activity in the late Tertiary is possibly due to the increase of C4-type grasses. As these grasses shifted to more mesic habitats, their high flammability increased fire frequency, promoting grasslands over woodlands. However, fire-prone habitats may have contributed to the prominence of trees such as those of the genera "Eucalyptus", "Pinus" and "Sequoia", which have thick bark to withstand fires and employ serotiny.
Human involvement.
The human use of fire for agricultural and hunting purposes during the Paleolithic and Mesolithic ages altered the preexisting landscapes and fire regimes. Woodlands were gradually replaced by smaller vegetation that facilitated travel, hunting, seed-gathering and planting. In recorded human history, minor allusions to wildfires were mentioned in the Bible and by classical writers such as Homer. However, while ancient Hebrew, Greek, and Roman writers were aware of fires, they were not very interested in the uncultivated lands where wildfires occurred. Wildfires were used in battles throughout human history as early thermal weapons. From the Middle ages, accounts were written of occupational burning as well as customs and laws that governed the use of fire. In Germany, regular burning was documented in 1290 in the Odenwald and in 1344 in the Black Forest. In the 14th century Sardinia, firebreaks were used for wildfire protection. In Spain during the 1550s, sheep husbandry was discouraged in certain provinces by Philip II due to the harmful effects of fires used in transhumance. As early as the 17th century, Native Americans were observed using fire for many purposes including cultivation, signaling, and warfare. Scottish botanist David Douglas noted the native use of fire for tobacco cultivation, to encourage deer into smaller areas for hunting purposes, and to improve foraging for honey and grasshoppers. Charcoal found in sedimentary deposits off the Pacific coast of Central America suggests that more burning occurred in the 50 years before the Spanish colonization of the Americas than after the colonization. In the post-World War II Baltic region, socio-economic changes led more stringent air quality standards and bans on fires that eliminated traditional burning practices.
Wildfires typically occurred during periods of increased temperature and drought. An increase in fire-related debris flow in alluvial fans of northeastern Yellowstone National Park was linked to the period between AD 1050 and 1200, coinciding with the Medieval Warm Period. However, human influence caused an increase in fire frequency. Dendrochronological fire scar data and charcoal layer data in Finland suggests that, while many fires occurred during severe drought conditions, an increase in the number of fires during 850 BC and 1660 AD can be attributed to human influence. Charcoal evidence from the Americas suggested a general decrease in wildfires between 1 AD and 1750 compared to previous years. However, a period of increased fire frequency between 1750 and 1870 was suggested by charcoal data from North America and Asia, attributed to human population growth and influences such as land clearing practices. This period was followed by an overall decrease in burning in the 20th century, linked to the expansion of agriculture, increased livestock grazing, and fire prevention efforts. A meta-analysis found that 17 times as much land burned annually in California before 1800 compared to today (1,800,000 hectares/year compared to 102,000 hectares/year).
Prevention.
Wildfire prevention refers to the preemptive methods of reducing the risk of fires as well as lessening its severity and spread. Effective prevention techniques allow supervising agencies to manage air quality, maintain ecological balances, protect resources, and to limit the effects of future uncontrolled fires. North American firefighting policies may permit naturally caused fires to burn to maintain their ecological role, so long as the risks of escape into high-value areas are mitigated. However, prevention policies must consider the role that humans play in wildfires, since, for example, 95% of forest fires in Europe are related to human involvement. Sources of human-caused fire may include arson, accidental ignition, or the uncontrolled use of fire in land-clearing and agriculture such as the slash-and-burn farming in Southeast Asia.
In the mid-19th century, explorers from the HMS "Beagle" observed Australian Aborigines using fire for ground clearing, hunting, and regeneration of plant food in a method later named fire-stick farming. Such careful use of fire has been employed for centuries in the lands protected by Kakadu National Park to encourage biodiversity. In 1937, U.S. President Franklin D. Roosevelt initiated a nationwide fire prevention campaign, highlighting the role of human carelessness in forest fires. Later posters of the program featured Uncle Sam, leaders of the Axis powers of World War II, characters from the Disney movie "Bambi", and the official mascot of the U.S. Forest Service, Smokey Bear.
Wildfires are caused by a combination of natural factors such as topography, fuels and weather. Other than reducing human infractions, only fuels may be altered to affect future fire risk and behavior. Wildfire prevention programs around the world may employ techniques such as "wildland fire use" and "prescribed or controlled burns". "Wildland fire use" refers to any fire of natural causes that is monitored but allowed to burn. "Controlled burns" are fires ignited by government agencies under less dangerous weather conditions.
Vegetation may be burned periodically to maintain high species diversity and frequent burning of surface fuels limits fuel accumulation, thereby reducing the risk of crown fires. Using strategic cuts of trees, fuels may also be removed by handcrews in order to clean and clear the forest, prevent fuel build-up, and create access into forested areas. Chain saws and large equipment can be used to thin out ladder fuels and shred trees and vegetation to a mulch. Multiple fuel treatments are often needed to influence future fire risks, and wildfire models may be used to predict and compare the benefits of different fuel treatments on future wildfire spread.
However, controlled burns are reportedly "the most effective treatment for reducing a fire’s rate of spread, fireline intensity, flame length, and heat per unit of area" according to Jan Van Wagtendonk, a biologist at the Yellowstone Field Station. Additionally, while fuel treatments are typically limited to smaller areas, effective fire management requires the administration of fuels across large landscapes in order to reduce future fire size and severity.
Building codes in fire-prone areas typically require that structures be built of flame-resistant materials and a defensible space be maintained by clearing flammable materials within a prescribed distance from the structure. Communities in the Philippines also maintain fire lines wide between the forest and their village, and patrol these lines during summer months or seasons of dry weather. Fuel buildup can result in costly, devastating fires as new homes, ranches, and other development are built adjacent to wilderness areas. Continued growth in fire-prone areas and rebuilding structures destroyed by fires has been met with criticism.
However, the population growth along the wildland-urban interface discourages the use of current fuel management techniques. Smoke is an irritant and attempts to thin out the fuel load is met with opposition due to desirability of forested areas, in addition to other wilderness goals such as endangered species protection and habitat preservation. The ecological benefits of fire are often overridden by the economic and safety benefits of protecting structures and human life. For example, while fuel treatments decrease the risk of crown fires, these techniques destroy the habitats of various plant and animal species. Additionally, government policies that cover the wilderness usually differ from local and state policies that govern urban lands.
Policy.
History of wildfire policy in the U.S..
Since the turn of the 20th century, various federal and state agencies have been involved in wildland fire management in one form or another. In the early 20th century, for example, the federal government, through the U.S. Army and the U.S. Forest Service, solicited fire suppression as a primary goal of managing the nation’s forests. At this time in history fire was viewed as a threat to timber, an economically important natural resource. As such, rational decisions were made to devote public funds to fire suppression and fire prevention efforts. For example, the Forest Fire Emergency Fund Act of 1908 permitted deficit spending in the case of emergency fire situations. As a result, the U.S. Forest Service was able to acquire a deficit of over $1 million in 1910 due to emergency fire suppression efforts. Following the same tone of timber resource protection, the U.S. Forest Service adopted the “10 AM Policy” in 1935. Through this policy the agency advocated the control of all fires by 10 o’clock of the morning following the discovery of a wildfire. Fire prevention was also heavily advocated through public education campaigns such as Smokey Bear. Through these and similar public education campaigns the general public was, in a sense, trained to perceive all wildfire as a threat to civilized society and natural resources. The negative sentiment towards wildland fire prevailed and helped to shape wildland fire management objectives throughout most of the 20th century.
Beginning in the 1970s public perception of wildland fire management began to shift. Despite strong funding for fire suppression in the first half of the 20th century, massive wildfires continued to be prevalent across the landscape of North America. Natural resource professionals and ordinary citizens alike became curious about the ecological effects of wildfire. Ecologists were beginning to recognize the presence and ecological importance of natural lightning-ignited wildfires across the United States. Along with this new discovery of fire knowledge and the emergence of fire ecology as a science came an effort to apply fire to land in a controlled manner. It was learned that suppression of fire in certain ecosystems actually increases the likelihood that a wildfire will occur and increases the intensity of those wildfires. This was in fact happening across the United States. However, suppression is still the main tactic when a fire is set by a human or if it threatens life or property.
By the 1980s funding efforts began to support prescribed burning. In light of emerging information about wildland fire, rational thought justified funding prescribed burning in order to prevent catastrophic wildfire events. In 2001, the United States Government implemented a National Fire Plan and the budget increased from $108 million in 2000 to $401 million for the reduction of hazardous fuels. In this way, the costs of implementing prescribed burns were thought to be less than the costs imposed on society by catastrophic wildfires. In addition to using prescribed fire to reduce the chance of catastrophic wildfires, mechanical methods have recently been adopted as well. Mechanical methods include the use of chippers and other machinery to remove hazardous fuels and thereby reduce the risk of wildfire events. Today the United States philosophy remains that, “fire, as a critical natural process, will be integrated into land and resource management plans and activities on a landscape scale, and across agency boundaries. Response to wildfire is based on ecological, social and legal consequences of fire. The circumstance under which a fire occurs, and the likely consequences and public safety and welfare, natural and cultural resources, and values to be protected dictate the appropriate management response to fire” (United States Department of Agriculture Guidance for Implementation of Federal Wildland Fire Management Policy, 13 February 2009). The five federal regulatory agencies managing forest fire response and planning for 676 million acres in the United States are the Department of the Interior, the Bureau of Land Management, the Bureau of Indian Affairs, the National Park Service, the United States Department of Agriculture-Forest Service and the United States Fish and Wildlife Services. Several hundred million U.S. acres of wildfire management are also conducted by state, county, and local fire management organizations. In 2014, legislators proposed The Wildfire Disaster Funding Act to provide $2.7 billion fund appropriated by congress for the USDA and Department of Interior to use in fire suppression. The bill is a reaction to United States Forest Service and Department of Interior costs of Western Wildfire suppression appending that amounted to $3.5 billion in 2013.
The Condition Class System.
The Condition Class System is used in the United States to provide “national-level data on the current condition of fuel and vegetation.” The USDA Forest Service developed this for the purpose of allocating fire funding and resources, prioritizing fuel usage and restoration activities, and evaluating wildfire management progress. There are primary and secondary determinants used to rank forest systems into condition class and fire regimes. Condition Class “indicates the departure from normal fire return intervals” and is categorized as low, medium, or high. The more a fire departs from normal pattern, the higher is its condition class. A fire regime is the “historical pattern of fire in forests” and the Roman numerals I, II, III, IV and V are used for the classification. Primary determinants are the structure of the forest, the amount of trees, tree density and the characteristics of the combustible fuel. The United States Department of Agriculture and the United States Department of Interior use the Condition Class System in the LANDFIRE project to make assessments of federal land. However, the LANDFIRE project revealed in 2003 that this type of analysis is not detailed enough to use at a local level. Federal agencies are required to take record and report "acres treated", using different prevention tactics, under the National Fire Plan Operations Reporting System (NFPORS).
Wildland-urban interface policy.
An aspect of wildfire policy that is gaining attention is the wildland-urban interface (WUI). More and more people are living in “red zones,” or areas that are at high risk of wildfires. FEMA and the NFPA develop specific policies to guide homeowners and builders in how to build and maintain structures at the WUI and how protect against catastrophic losses. For example, NFPA-1141 is a standard for fire protection infrastructure for land development in wildland, rural and suburban areas and NFPA-1144 is a standard for reducing structure ignition hazards from wildland fire. For a full list of these policies and guidelines, see http://www.nfpa.org/categoryList.asp?categoryID=124&URL=Codes%20&%20Standards. Compensation for losses in the WUI are typically negotiated on an incident-by-incident basis. This is generating discussion about the burden of responsibility for funding and fighting a fire in the WUI, in that, if a resident chooses to live in a known red zone, should he or she retain a higher level of responsibility for funding home protection against wildfires.
One initiative aimed at helping U.S. WUI communities live more safely with fire is called fire-adapted communities.
Economics of fire management policy.
Similar to that of military operations, fire management is often very expensive in the U.S. and the rest of the world. Today, it is not uncommon for suppression operations for a single wildfire to exceed costs of $1 million in just a few days. The United States Department of Agriculture allotted $2.2 billion for wildfire management in 2012. Although fire suppression offers many benefits to society, other options for fire management exist. While these options cannot completely replace fire suppression as a fire management tool, other options can play an important role in overall fire management and can therefore affect the costs of fire suppression.
The application of fire management tools requires making certain tradeoffs. Below is a sample of some costs and benefits associated with the tools currently used in fire management. Current approaches to fire management are an almost complete turnaround compared to historic approaches. In fact, it is commonly accepted that past fire suppression, along with other factors, has resulted in larger, more intense wildfire events which are seen today. In economic terms, expenditures used for wildfire suppression in the early 20th century have contributed to increased suppression costs which are being realized today. As is the case with many public policy issues, costs and benefits associated with particular fire management tools are difficult to accurately quantify. Ultimately, costs and benefits should be weighed against one another on a case-by-case basis in planning wildland fire management operations.
Depending on the tradeoffs that a land manager is willing to make, a combination of the following fire management tools could be used. For instance, prescribed fire and/or mechanical fuels reduction could be used to help prevent or lessen the intensity of a wildfire thereby reducing or eliminating suppression costs. In addition, prescribed fire and/or mechanical fuels reduction could be used to improve soil conditions in fields or in forests to the benefit of wildlife or natural resources. On the other hand, the use of prescribed fire requires much advanced planning and can have negative impacts on human health in nearby communities.
Costs and Benefits of Wildland Fire Management Tools
Detection.
Fast and effective detection is a key factor in wildfire fighting. Early detection efforts were focused on early response, accurate results in both daytime and nighttime, and the ability to prioritize fire danger. Fire lookout towers were used in the United States in the early 20th century and fires were reported using telephones, carrier pigeons, and heliographs. Aerial and land photography using instant cameras were used in the 1950s until infrared scanning was developed for fire detection in the 1960s. However, information analysis and delivery was often delayed by limitations in communication technology. Early satellite-derived fire analyses were hand-drawn on maps at a remote site and sent via overnight mail to the fire manager. During the Yellowstone fires of 1988, a data station was established in West Yellowstone, permitting the delivery of satellite-based fire information in approximately four hours.
Currently, public hotlines, fire lookouts in towers, and ground and aerial patrols can be used as a means of early detection of forest fires. However, accurate human observation may be limited by operator fatigue, time of day, time of year, and geographic location. Electronic systems have gained popularity in recent years as a possible resolution to human operator error. A government report on a recent trial of three automated camera fire detection systems in Australia did, however, conclude "...detection by the camera systems was slower and less reliable than by a trained human observer". These systems may be semi- or fully automated and employ systems based on the risk area and degree of human presence, as suggested by GIS data analyses. An integrated approach of multiple systems can be used to merge satellite data, aerial imagery, and personnel position via Global Positioning System (GPS) into a collective whole for near-realtime use by wireless Incident Command Centers.
A small, high risk area that features thick vegetation, a strong human presence, or is close to a critical urban area can be monitored using a local sensor network. Detection systems may include wireless sensor networks that act as automated weather systems: detecting temperature, humidity, and smoke. These may be battery-powered, solar-powered, or "tree-rechargeable": able to recharge their battery systems using the small electrical currents in plant material. Larger, medium-risk areas can be monitored by scanning towers that incorporate fixed cameras and sensors to detect smoke or additional factors such as the infrared signature of carbon dioxide produced by fires. Additional capabilities such as night vision, brightness detection, and color change detection may also be incorporated into sensor arrays.
Satellite and aerial monitoring through the use of planes, helicopter, or UAVs can provide a wider view and may be sufficient to monitor very large, low risk areas. These more sophisticated systems employ GPS and aircraft-mounted infrared or high-resolution visible cameras to identify and target wildfires. Satellite-mounted sensors such as Envisat's Advanced Along Track Scanning Radiometer and European Remote-Sensing Satellite's Along-Track Scanning Radiometer can measure infrared radiation emitted by fires, identifying hot spots greater than . The National Oceanic and Atmospheric Administration's Hazard Mapping System combines remote-sensing data from satellite sources such as Geostationary Operational Environmental Satellite (GOES), Moderate-Resolution Imaging Spectroradiometer (MODIS), and Advanced Very High Resolution Radiometer (AVHRR) for detection of fire and smoke plume locations. However, satellite detection is prone to offset errors, anywhere from for MODIS and AVHRR data and up to for GOES data. Satellites in geostationary orbits may become disabled, and satellites in polar orbits are often limited by their short window of observation time. Cloud cover and image resolution and may also limit the effectiveness of satellite imagery.
in 2015 a new fire detection tool is in operation at the U.S. Department of Agriculture (USDA) Forest Service (USFS) which uses data from the Suomi National Polar-orbiting Partnership (NPP) satellite to detect smaller fires in more detail than previous space-based products. The high-resolution data is used with a computer model to predict how a fire will change direction based on weather and land conditions. The active fire detection product using data from Suomi NPP’s Visible Infrared Imaging Radiometer Suite (VIIRS) increases the resolution of fire observations to 1,230 feet (375 meters). Previous NASA satellite data products available since the early 2000s observed fires at 3,280 foot (1 kilometer) resolution. The data is one of the intelligence tools used by the USFS and Department of Interior agencies across the United States to guide resource allocation and strategic fire management decisions. The enhanced VIIRS fire product enables detection every 12 hours or less of much smaller fires and provides more detail and consistent tracking of fire lines during long duration wildfires – capabilities critical for early warning systems and support of routine mapping of fire progression. Active fire locations are available to users within minutes from the satellite overpass through data processing facilities at the USFS Remote Sensing Applications Center, which uses technologies developed by the NASA Goddard Space Flight Center Direct Readout Laboratory in Greenbelt, Maryland. The model uses data on weather conditions and the land surrounding an active fire to predict 12–18 hours in advance whether a blaze will shift direction. The VIIRS fire detection product has been applied to these models, successfully verifying the wildfire simulations. The state of Colorado decided to incorporate the weather-fire model in its firefighting efforts beginning with the 2016 fire season.
In 2014, an international campaign was organized in South Africa’s Kruger National Park to validate fire detection products including the new VIIRS active fire data. In advance of that campaign, the Meraka Institute of the Council for Scientific and Industrial Research in Pretoria, South Africa, an early adopter of the VIIRS 375m fire product, put it to use during several large wildfires in Kruger.
The demand for timely, high-quality fire information has increased in recent years. Wildfires in the United States burn an average of 7 million acres of land each year. For the last 10 years, the USFS and Department of Interior have spent a combined average of about $1.5 billion annually on wildfire suppression. Large catastrophic wildfires have become common, especially in association with extended drought and extreme weather.
Suppression.
Wildfire suppression depends on the technologies available in the area in which the wildfire occurs. In less developed nations the techniques used can be as simple as throwing sand or beating the fire with sticks or palm fronds. In more advanced nations, the suppression methods vary due to increased technological capacity. Silver iodide can be used to encourage snow fall, while fire retardants and water can be dropped onto fires by unmanned aerial vehicles, planes, and helicopters. Complete fire suppression is no longer an expectation, but the majority of wildfires are often extinguished before they grow out of control. While more than 99% of the 10,000 new wildfires each year are contained, escaped wildfires can cause extensive damage. Worldwide damage from wildfires is in the billions of euros annually. Wildfires in Canada and the US burn an average of per year.
Above all, fighting wildfires can become deadly. A wildfire's burning front may also change direction unexpectedly and jump across fire breaks. Intense heat and smoke can lead to disorientation and loss of appreciation of the direction of the fire, which can make fires particularly dangerous. For example, during the 1949 Mann Gulch fire in Montana, USA, thirteen smokejumpers died when they lost their communication links, became disoriented, and were overtaken by the fire. In the Australian February 2009 Victorian bushfires, at least 173 people died and over 2,029 homes and 3,500 structures were lost when they became engulfed by wildfire.
Wildland firefighting safety.
Wildland fire fighters face several life-threatening hazards including heat stress, fatigue, smoke and dust, as well as the risk of other injuries such as burns, cuts and scrapes, animal bites, and even rhabdomyolysis.
Especially in hot weather condition, fires present the risk of heat stress, which can entail feeling heat, fatigue, weakness, vertigo, headache, or nausea. Heat stress can progress into heat strain, which entails physiological changes such as increased heart rate and core body temperature. This can lead to heat-related illnesses, such as heat rash, cramps, exhaustion or heat stroke. Various factors can contribute to the risks posed by heat stress, including strenuous work, personal risk factors such as age and fitness, dehydration, sleep deprivation, and burdensome personal protective equipment. Rest, cool water, and occasional breaks are crucial to mitigating the effects of heat stress.
Smoke, ash, and debris can also pose serious respiratory hazards to wildland fire fighters. The smoke and dust from wildfires can contain gases such as carbon monoxide, sulfur dioxide and formaldehyde, as well as particulates such as ash and silica. To reduce smoke exposure, wildfire fighting crews should, whenever possible, rotate firefighters through areas of heavy smoke, avoid downwind firefighting, use equipment rather than people in holding areas, and minimize mop-up. Camps and command posts should also be located upwind of wildfires. Protective clothing and equipment can also help minimize exposure to smoke and ash.
Firefighters are also at risk of cardiac events including strokes and heart attacks. Fire fighters should maintain good physical fitness. Fitness programs, medical screening and examination programs which include stress tests can minimize the risks of firefighting cardiac problems. Other injury hazards wildland fire fighters face include slips, trips and falls, burns, scrapes and cuts from tools and equipment, being struck by trees, vehicles, or other objects, plant hazards such as thorns and poison ivy, snake and animal bites, vehicle crashes, electrocution from power lines or lightning storms, and unstable building structures.
Fire retardant.
Fire retardants are used to help slow wildfires, coat fuels, and lessen oxygen availability as required by various firefighting situations. They are composed of nitrates, ammonia, phosphates and sulfates, as well as other chemicals and thickening agents. The choice of whether to apply retardant depends on the magnitude, location and intensity of the wildfire. Fire retardants are used to reach inaccessible geographical regions where ground firefighting crews are unable to reach a wildfire or in any occasion where human safety and structures are endangered. In certain instances, fire retardant may also be applied ahead of wildfires for protection of structures and vegetation as a precautionary fire defense measure.
The application of aerial fire retardants creates an atypical appearance on land and water surfaces and has the potential to change soil chemistry. Fire retardant can decrease the availability of plant nutrients in the soil by increasing the acidity of the soil. Fire retardant may also affect water quality through leaching, eutrophication, or misapplication. Fire retardant’s effects on drinking water remain inconclusive. Dilution factors, including water body size, rainfall, and water flow rates lessen the concentration and potency of fire retardant. Wildfire debris (ash and sediment) clog rivers and reservoirs increasing the risk for floods and erosion that ultimately slow and/or damage water treatment systems. There is continued concern of fire retardant effects on land, water, wildlife habitats, and watershed quality, additional research is needed. However, on the positive side, fire retardant (specifically its nitrogen and phosphorus components) has been shown to have a fertilizing effect on nutrient-deprived soils and thus creates a temporary increase in vegetation.
Current USDA procedure maintains that the aerial application of fire retardant in the United States must clear waterways by a minimum of 300 feet in order to safeguard effects of retardant runoff. Aerial uses of fire retardant are required to avoid application near waterways and endangered species (plant and animal habitats). After any incident of fire retardant misapplication, the U.S. Forest Service requires reporting and assessment impacts be made in order to determine mitigation, remediation, and/or restrictions on future retardant uses in that area.
Modeling.
Wildfire modeling is concerned with numerical simulation of wildfires in order to comprehend and predict fire behavior. Wildfire modeling can ultimately aid wildfire suppression, increase the safety of firefighters and the public, and minimize damage. Using computational science, wildfire modeling involves the statistical analysis of past fire events to predict spotting risks and front behavior. Various wildfire propagation models have been proposed in the past, including simple ellipses and egg- and fan-shaped models. Early attempts to determine wildfire behavior assumed terrain and vegetation uniformity. However, the exact behavior of a wildfire's front is dependent on a variety of factors, including windspeed and slope steepness. Modern growth models utilize a combination of past ellipsoidal descriptions and Huygens' Principle to simulate fire growth as a continuously expanding polygon. Extreme value theory may also be used to predict the size of large wildfires. However, large fires that exceed suppression capabilities are often regarded as statistical outliers in standard analyses, even though fire policies are more influenced by catastrophic wildfires than by small fires.
Cellular automata models have increasingly been used to model forest fire events. The initial model proposed by Drossel-Schwarbl (1992) identified forest fires as self-organised critical systems because the frequency-size distribution adheres to a power law relationship. More highly parameterised models have since supported this initial power law claim within forest fire models and real data obtained from forest fires in Australia and the US have shown the power law relationship to hold over a certain range. This has implications for predicting the size of forest fires based on observed frequency in areas where environmental conditions make the area susceptible to wildfire. Cellular automata models have also been used to model the response of forest fire events to changes in type, amount and distribution of combustible material, as well as weather conditions.
Human risk and exposure.
Wildfire risk is the chance that a wildfire will start in or reach a particular area and the potential loss of human values if it does. Risk is dependent on variable factors such as human activities, weather patterns, availability of wildfire fuels, and the availability or lack of resources to suppress a fire. Wildfires have continually been a threat to human populations. However, human induced geographical and climatic changes are exposing populations more frequently to wildfires and increasing wildfire risk. It is speculated that the increase in wildfires arises from a century of wildfire suppression coupled with the rapid expansion of human developments into fire-prone wildlands. Wildfires are naturally occurring events that aid in promoting forest health. The consequence of suppressing wildfires has led to an overgrowth in forest vegetation, which provides excess fuel that increases the severity, range, and duration of a wildfire. Global warming and climate changes are causing an increase in temperatures and more droughts nationwide which also contributes to an increase in wildfire risk.
Regional burden of wildfires in the United States.
Nationally, the burden of wildfires is disproportionally heavily distributed in the southern and western regions. The Geographic Area Coordinating Group (GACG) divides the United States and Alaska into 11 geographic areas for the purpose of emergency incident management. One particular area of focus is wildland fires. A national assessment of wildfire risk in the United States based on GACG identified regions (with the slight modification of combining Southern and Northern California, and the West and East Basin); indicate that California (50.22% risk) and the Southern Area (15.53% risk) are the geographic areas with the highest wildfire risk. The western areas of the nation are experiencing an expansion of human development into and beyond what is called the wildland-urban interface (WUI). When wildfires inevitably occur in these fire-prone areas, often communities are threatened due to their proximity to fire-prone forest. The south is one of the fastest growing regions with 88 million acres classified as WUI. The south consistently has the highest number of wildfires per year. More than 50, 000 communities are estimated to be at high to very high risk of wildfire damage. These statistics are greatly attributable to the South’s year-round fire season.
Wildfires risk to human health.
The most noticeable adverse effect of wildfires is the destruction of property and biomass. However, the release of hazardous chemicals from the burning of wildland fuels significantly impacts health in humans. Wildfire smoke is composed primarily of carbon dioxide and water vapor. Other common smoke components present in lower concentrations are carbon monoxide, formaldehyde, acrolein, polyaromatic hydrocarbons, and benzene. Small particulates suspended in air which come in solid form or in liquid droplets are also present in smoke. 80 -90% of wildfire smoke, by mass, is within the fine particle size class of 2.5 micrometers in diameter or smaller. Despite carbon dioxides high concentration in smoke, it poses low health risk due to its low toxicity. Carbon monoxide and fine particulate matter, particularly 2.5 µm in diameter and smaller, have been identified as the major health threats. Other chemicals are considered to be significant hazards but are found in concentrations that are too low to cause detectable health effects.
The degree of wildfire smoke exposure to an individual is dependent on the length, severity, duration, and proximity of the fire. People are exposed directly to smoke via the respiratory tract though inhalation of air pollutants. Indirectly, communities are exposed to wildfire debris that can contaminate soil and water supplies. Firefighters are at the greatest risk for acute and chronic health effects resulting from wildfire smoke exposure. Due to firefighter’s occupational duties, they are frequently exposed to hazardous chemicals at a close proximity for longer periods of time. A case study on the exposure of wildfire smoke among wildland firefighters, show that firefighters are exposed to significant levels of carbon monoxide and respiratory irritants above OSHA permissible exposure limits (PEL) and ACGIH threshold limit values (TLV). 5-10% are overexposed. The study obtained exposure concentrations for one wildland firefighter over a 10-hour shift spent holding down a fireline. The firefighter was exposed to a wide range of carbon monoxide and respiratory irritant (combination of particulate matter 3.5 µm and smaller, acrolein, and formaldehype) levels. Carbon monoxide levels reached up to 160ppm and the TLV irritant index value reached a high of 10. In contrast, the OSHA PEL for carbon monoxide is 30ppm and for the TLV respiratory irritant index, the calculated threshold limit value is 1; any value above 1 exceeds exposure limits.
Residents in communities surrounding wildfires are exposed to lower concentrations of chemicals, but they are at a greater risk for indirect exposure through water or soil contamination. Exposure to residents is greatly dependent on individual susceptibility. Vulnerable persons such as children (ages 0–4), the elderly (ages 65 and older), smokers, and pregnant women are at an increased risk due to already compromised body systems, even when the exposures are present at low chemical concentrations and for relatively short exposure periods. The U.S. Environmental Protection Agency (EPA) developed the Air Quality Index (AQI), a public resource that provides national air quality standard concentrations for common air pollutants. The public can use this index as a tool to determine their exposure to hazardous air pollutants based on visibility range. Additionally, there is evidence of an increase in material stress, as documented by researchers M.H. O'Donnell and A.M. Behie, thus affecting birth outcomes. In Australia, studies show that the male infants born with drastically higher average birth weights were born in mostly severely fire-affected areas. This is attributed to the fact that maternal signals directly affect fetal growth patterns.[http://apps.webofknowledge.com.libezp.lib.lsu.edu/full_record.do?product=UA&search_mode=GeneralSearch&qid=1&SID=3A7lyhAIveCBgjBAcZa&page=2&doc=16]
Health effects.
Inhalation of smoke from a wildfire can be a health hazard. Wildfire smoke is primarily composed of carbon dioxide, water vapor, particulate matter, organic chemicals, nitrogen oxides and other compounds. The principle health concern is the inhalation of particulate matter and carbon monoxide.
Particulate matter (PM) is a type of air pollution made up of particles of dust and liquid droplets. They are characterized into two categories based on the diameter of the particle. Coarse particles are between 2.5 micrometers and 10 micrometers and fine particles measure 2.5 micrometers and less. Both sizes can be inhaled. Coarse particles are filtered by the upper airways and can cause eye and sinus irritation as well as sore throat and coughing. The fine particles are more problematic because, when inhaled, they can be deposited deep into the lungs, where they are absorbed into the bloodstream. This is particularly hazardous to the very young, elderly and those with chronic conditions such as asthma, chronic obstructive pulmonary disease (COPD), cystic fibrosis and cardiovascular conditions. The illnesses most commonly with exposure to fine particle from wildfire smoke are bronchitis, exacerbation of asthma or COPD, and pneumonia. Symptoms of these complications include wheezing and shortness of breath and cardiovascular symptoms include chest pain, rapid heart rate and fatigue.
Carbon monoxide (CO) is a colorless, odorless gas that can be found at the highest concentration at close proximity to a smoldering fire. For this reason, carbon monoxide inhalation is a serious threat to the health of wildfire firefighters. CO in smoke can be inhaled into the lungs where it is absorbed into the bloodstream and reduces oxygen delivery to the body's vital organs. At high concentrations, it can cause headache, weakness, dizziness, confusion, nausea, disorientation, visual impairment, coma and even death. However, even at lower concentrations, such as those found at wildfires, individuals with cardiovascular disease may experience chest pain and cardiac arrhythmia. A recent study tracking the number and cause of wildfire firefighter deaths from 1990-2006 found that 21.9% of the deaths occurred from heart attacks.
Another important and somewhat less obvious health effect of wildfires is psychiatric diseases and disorders. Both adults and children from countries ranging from the United States and Canada to Greece and Australia who were directly and indirectly affected by wildfires were found by researchers to demonstrate several different mental conditions linked to their experience with the wildfires. These include post-traumatic stress disorder (PTSD), depression, anxiety, and phobias.
In a new twist to wildfire health effects, former uranium mining sites were burned over in the summer of 2012 near North Fork, Idaho. This prompted concern from area residents and Idaho State Department of Environmental Quality officials over the potential spread of radiation in the resultant smoke, since those sites had never been completely cleaned up from radioactive remains.
Epidemiology.
The EPA has defined acceptable concentrations of particulate matter in the air, through the National Ambient Air Quality Standards and monitoring of ambient air quality has been mandated. Due to these monitoring programs and the incidence of several large wildfires near populated areas, epidemiological studies have been conducted and demonstrate an association between human health effects and an increase in fine particulate matter due to wildfire smoke.
An increase in PM emitted from the Hayman fire in Colorado in June 2002, was associated with an increase in respiratory symptoms in patients with COPD.
Looking at the wildfires in Southern California in October 2003 in a similar manner, investigators have shown an increase in hospital admissions due to asthma during peak concentrations of PM. Children participating in the Children’s Health Study were also found to have an increase in eye and respiratory symptoms, medication use and physician visits. Recently, it was demonstrated that mothers who were pregnant during the fires gave birth to babies with a slightly reduced average birth weight compared to those who were not exposed to wildfire during birth. Suggesting that pregnant women may also be at greater risk to adverse effects from wildfire. Worldwide it is estimated that 339,000 people die due to the effects of wildfire smoke each year.
Types of wildfires.
Please note that there are many types of wildfires. I will be classifying them as ground, surface, or crown wildfires. Ground fires burn in the humus, and usually burn out by themselves. These burn the humus, roots of plants, mycelium, and other matter in the humus. Only some manage to become surface fires, or crown fires. surface fires burn above the humus but do not burn tall trees. Surface fires start as ground fires and may turn into crown fires. These burn small trees, bushes, shrubs, grasses, etc. If they manage to spread into the tallest of trees they are called crown wildfires. These burn even the tallest of trees. Firestorms are another type of wildfire, but they are not one of the base types of wildfires. They have strong gusts of wind created by the way of heat transfer called convection.

</doc>
<doc id="56107" url="https://en.wikipedia.org/wiki?curid=56107" title="Metropolis–Hastings algorithm">
Metropolis–Hastings algorithm

In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.
History.
The algorithm was named after Nicholas Metropolis, who was an author along with Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller of the 1953 paper "Equation of State Calculations by Fast Computing Machines" which first proposed the algorithm for the case of symmetrical proposal distributions, and W. K. Hastings who extended it to the more general case in 1970.
There is controversy over the credit for discovery of the algorithm.
Edward Teller states in his memoirs that the five authors of the 1953 paper worked
together for "days (and nights)".
M. Rosenbluth, in an oral history recorded shortly before his death credits E. Teller with posing the
original problem, himself with solving it, and A.W. Rosenbluth (his wife) with programming the computer.
According to M. Rosenbluth, neither Metropolis nor A.H. Teller participated in any way.
Rosenbluth's account of events is supported by other contemporary recollections.
According to Roy Glauber and Emilio Segrè, the original algorithm was invented by Enrico Fermi
and reinvented by Stan Ulam.
Intuition.
The Metropolis–Hastings algorithm can draw samples from any probability distribution "P"("x"), provided you can compute the value of a function "f"("x") that is "proportional" to the density of "P". The lax requirement that "f"("x") should be merely proportional to the density, rather than exactly equal to it, makes the Metropolis–Hastings algorithm particularly useful, because calculating the necessary normalization factor is often extremely difficult in practice.
The Metropolis–Hastings algorithm works by generating a sequence of sample values in such a way that, as more and more sample values are produced, the distribution of values more closely approximates the desired distribution, "P"("x"). These sample values are produced iteratively, with the distribution of the next sample being dependent only on the current sample value (thus making the sequence of samples into a Markov chain). Specifically, at each iteration, the algorithm picks a candidate for the next sample value based on the current sample value. Then, with some probability, the candidate is either accepted (in which case the candidate value is used in the next iteration) or rejected (in which case the candidate value is discarded, and current value is reused in the next iteration)−the probability of acceptance is determined by comparing the values of the function "f"("x") of the current and candidate sample values with respect to the desired distribution "P"("x").
For the purpose of illustration, the Metropolis algorithm, a special case of the Metropolis–Hastings algorithm where the proposal function is symmetric, is described below.
Metropolis algorithm (symmetric proposal distribution)
Let "f"("x") be a function that is proportional to the desired probability distribution "P"("x") (a.k.a. a target distribution).
This algorithm proceeds by randomly attempting to move about the sample space, sometimes accepting the moves and sometimes remaining in place. Note that the acceptance ratio formula_5 indicates how probable the new proposed sample is with respect to the current sample, according to the distribution formula_6. If we attempt to move to a point that is more probable than the existing point (i.e. a point in a higher-density region of formula_6), we will always accept the move. However, if we attempt to move to a less probable point, we will sometimes reject the move, and the more the relative drop in probability, the more likely we are to reject the new point. Thus, we will tend to stay in (and return large numbers of samples from) high-density regions of formula_6, while only occasionally visiting low-density regions. Intuitively, this is why this algorithm works, and returns samples that follow the desired distribution formula_6.
Compared with an algorithm like adaptive rejection sampling that directly generates independent samples from a distribution, Metropolis–Hastings and other MCMC algorithms have a number of disadvantages:
On the other hand, most simple rejection sampling methods suffer from the "curse of dimensionality", where the probability of rejection increases exponentially as a function of the number of dimensions. Metropolis–Hastings, along with other MCMC methods, do not have this problem to such a degree, and thus are often the only solutions available when the number of dimensions of the distribution to be sampled is high. As a result, MCMC methods are often the methods of choice for producing samples from hierarchical Bayesian models and other high-dimensional statistical models used nowadays in many disciplines.
In multivariate distributions, the classic Metropolis–Hastings algorithm as described above involves choosing a new multi-dimensional sample point. When the number of dimensions is high, finding the right jumping distribution to use can be difficult, as the different individual dimensions behave in very different ways, and the jumping width (see above) must be "just right" for all dimensions at once to avoid excessively slow mixing. An alternative approach that often works better in such situations, known as Gibbs sampling, involves choosing a new sample for each dimension separately from the others, rather than choosing a sample for all dimensions at once. This is especially applicable when the multivariate distribution is composed out of a set of individual random variables in which each variable is conditioned on only a small number of other variables, as is the case in most typical hierarchical models. The individual variables are then sampled one at a time, with each variable conditioned on the most recent values of all the others. Various algorithms can be used to choose these individual samples, depending on the exact form of the multivariate distribution: some possibilities are the adaptive rejection sampling methods, the adaptive rejection Metropolis sampling algorithm or its improvements (see matlab code), a simple one-dimensional Metropolis–Hastings step, or slice sampling.
Formal derivation of the Metropolis-Hastings algorithm.
The purpose of the Metropolis–Hastings algorithm is to generate a collection of states according to a desired distribution P(x). To accomplish this, the algorithm uses a Markov process which asymptotically reaches a unique stationary distribution π(x) such that π(x)=P(x) .
A Markov process is uniquely defined by its transition probabilities, formula_11, the probability of transitioning from any given state, x, to any other given state, x'. It has a unique stationary distribution π(x) when the following two conditions are met:
The Metropolis–Hastings algorithm involves designing a Markov process (by constructing transition probabilities) which fulfils the two above conditions, such that its stationary distribution π(x) is chosen to be "P(x)". The derivation of the algorithm starts with the condition of detailed balance:
formula_13
which is re-written as
formula_14.
The approach is to separate the transition in two sub-steps; the proposal and the acceptance-rejection. The proposal distribution formula_15 is the conditional probability of proposing a state x' given x, and the acceptance distribution formula_16 the conditional probability to accept the proposed state x'. The transition probability can be written as the product of them:
formula_17 .
Inserting this relation the previous equation, we have
formula_18 .
The next step in the derivation is to choose an acceptance that fulfils the condition above. One common choice is the Metropolis choice:
formula_19
i.e., we always accept when the acceptance is bigger than 1, and we reject accordingly when the acceptance is smaller than 1.
This is the required quantity for the algorithm.
The Metropolis–Hastings algorithm thus consists in the following:
The saved states are in principle drawn from the distribution formula_22, as step 4 ensures they are de-correlated.
The value of T must be chosen according to different factors such as the proposal distribution and, formally, it has to be of the order of the autocorrelation time of the Markov process.
It is important to notice that it is not clear, in a general problem, which distribution formula_15 one should use; it is a free parameter of the method which has to be adjusted to the particular problem in hand.
Step-by-step instructions.
Suppose the most recent value sampled is formula_24. To follow the Metropolis–Hastings algorithm, we next draw a new proposal state formula_25 with probability density formula_26, and calculate a value
where
is the probability (e.g., Bayesian posterior) ratio between the proposed sample formula_25 and the previous sample formula_24, and
is the ratio of the proposal density in two directions (from formula_24 to formula_25 and "vice versa").
This is equal to 1 if the proposal density is symmetric.
Then the new state formula_34 is chosen according to the following rules.
The Markov chain is started from an arbitrary initial value formula_37 and the algorithm is run for many iterations until this initial state is "forgotten". 
These samples, which are discarded, are known as "burn-in". The remaining set of accepted values of formula_38 represent a sample from the distribution formula_22.
The algorithm works best if the proposal density matches the shape of the target distribution formula_6 from which direct sampling is difficult, that is formula_41.
If a Gaussian proposal density formula_42 is used the variance parameter formula_43 has to be tuned during the burn-in period.
This is usually done by calculating the "acceptance rate", which is the fraction of proposed samples that is accepted in a window of the last formula_44 samples.
The desired acceptance rate depends on the target distribution, however it has been shown theoretically that the ideal acceptance rate for a one-dimensional Gaussian distribution is approx 50%, decreasing to approx 23% for an formula_44-dimensional Gaussian target distribution.
If formula_43 is too small the chain will "mix slowly" (i.e., the acceptance rate will be high but successive samples will move around the space slowly and the chain will converge only slowly to formula_6). On the other hand,
if formula_43 is too large the acceptance rate will be very low because the proposals are likely to land in regions of much lower probability density, so formula_49 will be very small and again the chain will converge very slowly.

</doc>
<doc id="56108" url="https://en.wikipedia.org/wiki?curid=56108" title="Penrose triangle">
Penrose triangle

The Penrose triangle, also known as the Penrose tribar, or the impossible tribar, is an impossible object. It was first created by the Swedish artist Oscar Reutersvärd in 1934. The psychologist Lionel Penrose and his mathematician son Roger Penrose independently devised and popularised it in the 1950s, describing it as "impossibility in its purest form". It is featured prominently in the works of artist M. C. Escher, whose earlier depictions of impossible objects partly inspired it.
Impossible object.
The tribar appears to be a solid object, made of three straight beams of square cross-section which meet pairwise at right angles at the vertices of the triangle they form. The beams may be broken, forming cubes or cuboids.
This combination of properties cannot be realized by any 3-dimensional object in ordinary Euclidean space. Such an object can exist in certain Euclidean 3-manifolds. There also exist 3-dimensional solid shapes each of which, when viewed from a certain angle, appears the same as the 2-dimensional depiction of the Penrose triangle on this page (such as - for example - the image to the left depicting a sculpture in Perth, Australia). The term "Penrose triangle" can refer to the 2-dimensional depiction or the impossible object itself.
M.C. Escher's lithograph "Waterfall" (1961) depicts a watercourse that flows in a zigzag along the long sides of two elongated Penrose triangles, so that it ends up two stories higher than it began. The resulting waterfall, forming the short sides of both triangles, drives a water wheel. Escher helpfully points out that in order to keep the wheel turning some water must occasionally be added to compensate for evaporation.
If a line is traced around the Penrose triangle, a 3-loop Möbius strip is formed.
Although the tribar is named one of the impossible objects, there exist many more that fit into the same category. Other impossible objects include the devil's fork, the dancing elephant, and impossible arch. 
Other Penrose polygons.
While it is possible to construct analogies to the Penrose triangle with other regular polygons to create a Penrose polygon, the visual effect is not as striking, and as the sides increase, the object seems merely to be warped or twisted.

</doc>
<doc id="56109" url="https://en.wikipedia.org/wiki?curid=56109" title="Brown rat">
Brown rat

The brown rat, also referred to as common rat, street rat, sewer rat, Hanover rat, Norway rat, brown Norway rat, Norwegian rat, or wharf rat ("Rattus norvegicus") is one of the best known and most common rats.
One of the largest muroids, it is a brown or grey rodent with a body up to long, and a similar tail length; the male weighs on average and the female . Thought to have originated in northern China, this rodent has now spread to all continents except Antarctica, and is the dominant rat in Europe and much of North America—making it by at least this particular definition the most successful mammal on the planet after humans. With rare exceptions, the brown rat lives wherever humans live, particularly in urban areas.
Selective breeding of "Rattus norvegicus" has produced the laboratory rat, a model organism in biological research, as well as pet rats.
Naming and etymology.
Originally called the "Hanover rat" by people wishing to link problems in 18th century England with the House of Hanover, it is not known for certain why the brown rat is named "Rattus norvegicus" (Norwegian rat), as it did not originate from Norway. However, the English naturalist John Berkenhout, author of the 1769 book "Outlines of the Natural History of Great Britain", is most likely responsible for popularizing the misnomer. Berkenhout gave the brown rat the binomial name "Rattus norvegicus", believing it had migrated to England from Norwegian ships in 1728, although no brown rat had entered Norway at that time.
By the early to middle part of the 19th century, British academics were aware that the brown rat was not native to Norway, hypothesizing (incorrectly) that it may have come from Ireland, Gibraltar or across the English Channel with William the Conqueror. As early as 1850, however, a more correct understanding of the rat's origins was beginning to develop. The British novelist Charles Dickens acknowledged the misnomer in his weekly journal, "All the Year Round," writing:
"Now there is a mystery about the native country of the best known species of rat, the common brown rat. It is frequently called, in books and otherwise, the 'Norway rat', and it is said to have been imported into this country in a ship-load of timber from Norway. Against this hypothesis stands the fact that when the brown rat had become common in this country, it was unknown in Norway, although there was a small animal like a rat, but really a lemming, which made its home there."
Academics began to understand the origins and corrected etymology of the brown rat towards the end of the 19th century, as seen in the 1895 text "Natural History" by American scholar Alfred Henry Miles:
"The brown rat is the species common in England, and best known throughout the world. It is said to have travelled from Persia to England less than two hundred years ago and to have spread from thence to other countries visited by English ships."
Though the assumptions surrounding this species' origins were not yet entirely accurate, by the 20th century, it was established among naturalists that the brown rat did not originate in Norway, rather the species came from central Asia and (likely) China. Despite this, this species' common name of "Norway rat" is still in use today.
Description.
The fur is coarse and usually brown or dark grey, while the underparts are lighter grey or brown. The brown rat is a rather large true murid and can weigh twice as much as a black rat and many times more than a house mouse. The length is commonly in the range of , with the tail a further , thus being roughly the same length as the body. Adult body weight averages in males and about in females. Exceptionally large individuals can reportedly reach but are not expected outside of domestic specimens. Stories of rats attaining sizes as big as cats are exaggerations, or misidentifications of other rodents, such as the coypu and muskrat. In fact it is common for breeding wild brown rats to weigh (sometimes considerably) less than .
Brown rats have acute hearing, are sensitive to ultrasound, and possess a very highly developed olfactory sense. Their average heart rate is 300 to 400 beats per minute, with a respiratory rate of around 100 per minute. The vision of a pigmented rat is poor, around 20/600, while a non-pigmented (albino) with no melanin in its eyes has both around 20/1200 vision and a terrible scattering of light within its vision. Brown rats are dichromates which perceive colors rather like a human with red-green colorblindness, and their colour saturation may be quite faint. Their blue perception, however, also has UV receptors, allowing them to see ultraviolet lights that some species cannot.
Biology and behavior.
The brown rat is nocturnal and is a good swimmer, both on the surface and underwater, and has been observed climbing slim round metal poles several feet in order to reach garden bird feeders. Brown rats dig well, and often excavate extensive burrow systems. A 2007 study found brown rats to possess metacognition, a mental ability previously only found in humans and some primates, but further analysis suggested they may have been following simple operant conditioning principles.
Communication.
Brown rats are capable of producing ultrasonic vocalizations. As pups, young rats use different types of ultrasonic cries to elicit and direct maternal search behavior, as well as to regulate their mother's movements in the nest. Although pups will produce ultrasounds around any other rats at 7 days old, by 14 days old they significantly reduce ultrasound production around male rats as a defensive response. Adult rats will emit ultrasonic vocalizations in response to predators or perceived danger; the frequency and duration of such cries depends on the sex and reproductive status of the rat. The female rat will also emit ultrasonic vocalizations during mating.
Chirping.
Rats may also emit short, high frequency, ultrasonic, socially induced vocalization during rough and tumble play, before receiving morphine, or mating, and when tickled. The vocalization, described as a distinct "chirping", has been likened to laughter, and is interpreted as an expectation of something rewarding. Like most rat vocalizations, the chirping is too high in pitch for humans to hear without special equipment. Bat detectors are often used by pet owners for this purpose.
In clinical studies, the chirping is associated with positive emotional feelings, and social bonding occurs with the tickler, resulting in the rats becoming conditioned to seek the tickling. However, as the rats age, the tendency to chirp appears to decline.
Rat chirp also can be used for mosquito control.
Other ultrasonic vocalizations, including a lower-frequency 'boom' or 'whoom' noise can be produced by bucks in a calm state, when grooming or settling down to sleep.
Audible communication.
Brown rats also produce communicative noises capable of being heard by humans. The most commonly heard in domestic rats is bruxing, or teeth-grinding, which is most usually triggered by happiness, but can also be 'self-comforting' in stressful situations, such as a visit to the vet. The noise is best described as either a quick clicking or 'burring' sound, varying from animal to animal.
In addition, they commonly squeak along a range of tones from high, abrupt pain squeaks to soft, persistent 'singing' sounds during confrontations.
Diet.
The brown rat is a true omnivore and will consume almost anything, but cereals form a substantial part of its diet. Martin Schein, founder of the Animal Behavior Society in 1964, studied the diet of brown rats and came to the conclusion that the most-liked foods of brown rats include scrambled eggs, macaroni and cheese, and cooked corn kernels. According to Schein, the least-liked foods were raw beets, peaches, and raw celery.
Foraging behavior is often population-specific, and varies by environment and food source. Brown rats living near a hatchery in West Virginia catch fingerling fish.
Some colonies along the banks of the Po River in Italy will dive for mollusks, a practice demonstrating social learning among members of this species. Rats on the island of Norderoog in the North Sea stalk and kill sparrows and ducks.
Reproduction and life cycle.
The brown rat can breed throughout the year if conditions are suitable, with a female producing up to five litters a year. The gestation period is only 21 days, and litters can number up to 14, although seven is common. They reach sexual maturity in about five weeks. Under ideal conditions (for the rat), this means that the population of females could increase by a factor of three and a half (half a litter of 7) in 8 weeks (5 weeks for sexual maturity and 3 weeks of gestation), corresponding to a population growing by a factor of 10 in just 15 weeks. The maximum life span is up to three years, although most barely manage one. A yearly mortality rate of 95% is estimated, with predators and interspecies conflict as major causes.
When lactating, female rats display a 24-hour rhythm of maternal behavior, and will usually spend more time attending to smaller litters than large ones.
Brown rats live in large, hierarchical groups, either in burrows or subsurface places, such as sewers and cellars. When food is in short supply, the rats lower in social order are the first to die. If a large fraction of a rat population is exterminated, the remaining rats will increase their reproductive rate, and quickly restore the old population level.
Social behavior.
Rats commonly groom each other and sleep together. Rats are said to establish an order of hierarchy, so one rat will be dominant over another one. Groups of rats tend to "play fight", which can involve any combination of jumping, chasing, tumbling, and "boxing". Play fighting involves rats going for each other's necks, while serious fighting involves strikes at the others' back ends. If living space becomes limited, rats may turn to aggressive behavior, which may result in the death of some animals, reducing the burden over the living space.
Rats, like most mammals, also form family groups of a mother and her young. This applies to both groups of males and females. However, rats are territorial animals, meaning that they usually act aggressively or scared of strange rats. Rats will fluff up their hair, hiss, squeal, and move their tails around when defending their territory. Rats will chase each other, groom each other, sleep in group nests, wrestle with each other, have dominance squabbles, communicate, and play in various other ways with each other. Huddling is an additional important part of rat socialization. Huddling is often supposed to have a heat-conserving function. Nestling rats especially depend on heat from their mother, since they cannot regulate their own temperature. Huddling is an extreme form of herding. Other forms of interaction include, crawling under, which is literally the act of crawling underneath one another,walking over, also explained in the name, then there is allo-grooming, so-called to distinguish it from self-grooming. And lastly there is another type of contact called nosing, where a rat gently pushes with its nose at another rat near the neck.
Burrowing.
Rats are known to burrow extensively, both in the wild and in captivity, if given access to a suitable substrate. Rats generally begin a new burrow adjacent to an object or structure, as this provides a sturdy "roof" for the section of the burrow nearest to the ground's surface. Burrows usually develop to eventually include multiple levels of tunnels, as well as a secondary entrance. Older male rats will generally not burrow, while young males and females will burrow vigorously.
Burrows provide rats with shelter and food storage, as well as safe, thermo-regulated nest sites. Rats use their burrows to escape from perceived threats in the surrounding environment; for example, rats will retreat to their burrows following a sudden, loud noise or while fleeing an intruder. Burrowing can therefore be described as a "pre-encounter defensive behavior", as opposed to a "post-encounter defensive behavior", such as flight, freezing, or avoidance of a threatening stimulus.
Distribution and habitat.
Likely originating from the plains of Asia, northern China and Mongolia, the brown rat spread to other parts of the world sometime in the Middle Ages. The question of when brown rats became commensal with humans remains unsettled, but as a species, they have spread and established themselves along routes of human migration and now live almost everywhere humans are.
The brown rat may have been present in Europe as early as 1553, a conclusion drawn from an illustration and description by Swiss naturalist Conrad Gesner in his book "Historiae animalium", published 1551–1558. Though Gesner's description could apply to the black rat, his mention of a large percentage of albino specimens—not uncommon among wild populations of brown rats—adds credibility to this conclusion. Reliable reports dating to the 18th century document the presence of the brown rat in Ireland in 1722, England in 1730, France in 1735, Germany in 1750, and Spain in 1800, becoming widespread during the Industrial Revolution. It did not reach North America until around 1750–1755.
As it spread from Asia, the brown rat generally displaced the black rat in areas where humans lived. In addition to being larger and more aggressive, the change from wooden structures and thatched roofs to bricked and tiled buildings favored the burrowing brown rats over the arboreal black rats. In addition, brown rats eat a wider variety of foods, and are more resistant to weather extremes.
In the absence of humans, brown rats prefer damp environments, such as river banks. However, the great majority are now linked to man-made environments, such as sewage systems. 
It is often said that there are as many rats in cities as people, but this varies from area to area depending on climate, living conditions, etc. Brown rats in cities tend not to wander extensively, often staying within of their nest if a suitable concentrated food supply is available, but they will range more widely where food availability is lower. There is great debate over the size of the population of rats in New York City, with estimates from almost 100 million rats to as few as 250,000. Experts suggest that New York is a particularly attractive place for rats because of its aging infrastructure, high moisture, and high poverty rates. In addition to sewers, rats are very comfortable living in alleyways and residential buildings, as there is usually a large and continuous food source in those areas.
In the United Kingdom, some figures show that the rat population has been rising, with estimations that 81 million rats reside in the UK. Those figures would mean that there are 1.3 rats per person in the country. High rat populations in the UK are often attributed to the mild climate, which allow them higher survival rates during the winter months.
The only brown rat-free zones in the world are the continent of Antarctica, some (although not all) parts of the Arctic, some especially isolated islands, the province of Alberta in Canada, and certain conservation areas in New Zealand.
Antarctica is almost completely covered by ice and has no permanent human inhabitants, making it uninhabitable by rats. The Arctic has extremely cold winters that rats cannot survive outdoors, and the human population density is extremely low, making it difficult for rats to travel from one habitation to another, although they have arrived in many coastal areas by ship. When the occasional rat infestation is found and eliminated, the rats are unable to reinfest it from an adjacent one. Isolated islands are also able to eliminate rat populations because of low human population density and the geographic distance from other rat populations.
Alaska.
Rat Island in Alaska was infested with brown rats after a Japanese shipwreck in 1780. They had a devastating effect on the native bird life. An eradication program was started in 2007 and the island was declared rat free in June 2009.
Alberta, Canada.
Alberta, Canada, is the largest rat-free populated area in the world. Rat invasions of Alberta were stopped and rats were eliminated by very aggressive government rat control measures, starting during the 1950s.
The only species of "Rattus" that is capable of surviving the climate of Alberta is the brown rat, which can only survive in the prairie region of the province, and even then must overwinter in buildings. Although it is a major agricultural area, Alberta is far from any seaport and only a portion of its eastern boundary with Saskatchewan provides a favorable entry route for rats. Brown rats cannot survive in the wild boreal forest to the north, the Rocky Mountains to the west, nor can they safely cross the semiarid High Plains of Montana to the south. The first brown rat did not reach Alberta until 1950, and in 1951, the province launched a rat-control program that included shooting, poisoning, and gassing rats, and bulldozing, burning down, and blowing up rat-infested buildings. The effort was backed by legislation that required every person and every municipality to destroy and prevent the establishment of designated pests. If they failed, the provincial government could carry out the necessary measures and charge the costs to the landowner or municipality.
In the first year of the rat control program, of arsenic trioxide were spread throughout 8,000 buildings on farms along the Saskatchewan border. However, in 1953 the much safer and more effective rodenticide, warfarin was introduced to replace arsenic. Warfarin is an anticoagulant that was approved as a drug for human use in 1954 and is much safer to use near humans and other large animals than arsenic. By 1960, the number of rat infestations in Alberta had dropped to below 200 per year. In 2002, the province finally recorded its first year with zero rat infestations, and from 2002 to 2007 there were only two infestations found. After an infestation of rats in the Medicine Hat landfill was found in 2012, the province's rat-free status was questioned, but provincial government rat control specialists brought in excavating machinery, dug out, shot, and poisoned 147 rats in the landfill, and no live rats were found thereafter. In 2013, the number of rat infestations in Alberta dropped to zero again. Alberta defines an infestation as two or more rats found at the same location, since a single rat cannot reproduce. About a dozen single rats enter Alberta in an average year and are killed by provincial rat control specialists before they can reproduce.
Only zoos, universities, and research institutes are allowed to keep caged rats in Alberta, and possession of unlicensed rats (including pet rats) by anyone else is punishable by a penalty of up to $5,000 or up to 60 days in jail.
The adjacent and similarly landlocked province of Saskatchewan initiated a rat control program in 1972, and has managed to reduce the number of rats in the province substantially, although they have not been eliminated. The Saskatchewan rat control program has considerably reduced the number of rats trying to enter Alberta.
New Zealand.
First arriving before 1800 (perhaps on James Cook's vessels), brown rats have posed a serious threat to many of New Zealand's native animals. Rat eradication programmes within New Zealand have led to rat-free zones on offshore islands and even on fenced "ecological islands" on the mainland. Before an eradication effort was launched in 2001, the sub-Antarctic Campbell Island had the highest population density of brown rats in the world.
Diseases.
Similar to other rodents, brown rats may carry a number of pathogens, which can result in disease, including Weil's disease, rat bite fever, cryptosporidiosis, viral hemorrhagic fever, Q fever and hantavirus pulmonary syndrome. In the United Kingdom, brown rats are an important reservoir for "Coxiella burnetii," the bacterium that causes Q fever, with seroprevalence for the bacteria found to be as high as 53% in some wild populations.
This species can also serve as a reservoir for "Toxoplasma gondii", the parasite that causes toxoplasmosis, though the disease usually spreads from rats to humans when domestic cats feed on infected brown rats. The parasite has a long history with the brown rat, and there are indications that the parasite has evolved to alter an infected rat's perception to cat predation, making it more susceptible to predation and increasing the likelihood of transmission.
Surveys and specimens of brown rat populations throughout the world have shown this species is often associated with outbreaks of trichinosis, but the extent to which the brown rat is responsible in transmitting "Trichinella" larvae to humans and other synanthropic animals is at least somewhat debatable. "Trichinella pseudospiralis", a parasite previously not considered to be a potential pathogen in humans or domestic animals, has been found to be pathogenic in humans and carried by brown rats.
Brown rats are sometimes mistakenly thought to be a major reservoir of bubonic plague, a possible cause of the Black Death. However, the bacterium responsible, "Yersinia pestis", is commonly endemic in only a few rodent species and is usually transmitted zoonotically by rat fleas—common carrier rodents today include ground squirrels and wood rats. However, brown rats may suffer from plague, as can many nonrodent species, including dogs, cats, and humans. The original carrier for the plague-infected fleas thought to be responsible for the Black Death was the black rat, and it has been hypothesized that the displacement of black rats by brown rats led to the decline of bubonic plague. This theory has, however, been deprecated, as the dates of these displacements do not match the increases and decreases in plague outbreaks.
In captivity.
Uses in science.
Selective breeding of albino brown rats rescued from being killed in a now-outlawed sport called rat baiting has produced the albino laboratory rat. Like mice, these rats are frequently subjects of medical, psychological and other biological experiments, and constitute an important model organism. This is because they grow quickly to sexual maturity and are easy to keep and to breed in captivity. When modern biologists refer to "rats", they almost always mean "Rattus norvegicus".
As pets.
The brown rat is kept as a pet in many parts of the world. Australia, the United Kingdom, and the United States are just a few of the countries that have formed fancy rat associations similar in nature to the American Kennel Club, establishing standards, orchestrating events, and promoting responsible pet ownership.
The many different types of domesticated brown rats include variations in coat patterns, as well as the style of the coat, such as Hairless or Rex, and more recently developed variations in body size and structure, including dwarf and tailless fancy rats.
External links.
Overviews
"Rattus norvegicus" genome and use as model animal

</doc>
<doc id="56110" url="https://en.wikipedia.org/wiki?curid=56110" title="Impossible object">
Impossible object

An impossible object (also known as an impossible figure or an undecidable figure) is a type of optical illusion. It consists of a two-dimensional figure which is instantly and subconsciously interpreted by the visual system as representing a projection of a three-dimensional object.
In most cases the impossibility becomes apparent after viewing the figure for a few seconds. However, the initial impression of a 3D object remains even after it has been contradicted. There are also more subtle examples of impossible objects where the impossibility does not become apparent spontaneously and it is necessary to consciously examine the geometry of the implied object to determine that it is impossible.
The unsettling nature of impossible objects occurs because of our natural desire to interpret 2D drawings as three-dimensional objects. This is why a drawing of a Necker cube would be most likely seen as a cube, rather than "two squares connected with diagonal lines, a square surrounded by irregular planar figures, or any other planar figure." With an impossible object, looking at different parts of the object makes one reassess the 3D nature of the object, which confuses the mind.
Impossible objects are of interest to psychologists, mathematicians and artists without falling entirely into any one discipline.
Notable examples.
Notable impossible objects include:
History.
An early example of an impossible object comes from "Apolinère Enameled", a 1916 advertisement painted by Marcel Duchamp. It depicts a girl painting a bed-frame with white enamelled paint, and deliberately includes conflicting perspective lines, to produce an impossible object. To emphasise the deliberate impossibility of the shape, a piece of the frame is missing.
Swedish artist Oscar Reutersvärd was one of the first to deliberately design many impossible objects. He has been called "the father of impossible figures". In 1934 he drew the Penrose triangle, some years before the Penroses. In Reutersvärd's version the sides of the triangle are broken up into cubes.
In 1956, British psychiatrist Lionel Penrose and his son, mathematician Roger Penrose, submitted a short article to the "British Journal of Psychology" titled "Impossible Objects: A Special Type of Visual Illusion". This was illustrated with the Penrose triangle and Penrose stairs. The article referred to Escher, whose work had sparked their interest in the subject, but not Reutersvärd, of whom they were unaware. The article was published in 1958.
From the 1930s onwards, Dutch artist M.C. Escher produced many drawings featuring paradoxes of perspective gradually working towards impossible objects. In 1957, he produced his first drawing containing a true impossible object: "Cube with Magic Ribbons". He produced many further drawings featuring impossible objects, sometimes with the entire drawing being an impossible object. Waterfall and Belvedere are good examples of impossible constructions. His work did much to draw the attention of the public to impossible objects.
Some contemporary artists are also experimenting with impossible figures, for example, Jos de Mey, Shigeo Fukuda, Sandro del Prete, István Orosz (Utisz), Guido Moretti, Tamás F. Farkas and Mathieu Hamaekers.
Constructed impossible objects.
Although possible to represent in two dimensions, it is not geometrically possible for such an object to exist in the physical world. However some models of impossible objects have been constructed, such that when they are viewed from a very specific point, the illusion is maintained. Rotating the object or changing the viewpoint breaks the illusion, and therefore many of these models rely on forced perspective or having parts of the model appearing to be further or closer than they actually are.
The notion of an "interactive impossible object" is an impossible object that can be viewed from any angle without breaking the illusion.

</doc>
<doc id="56111" url="https://en.wikipedia.org/wiki?curid=56111" title="David Stirling">
David Stirling

Colonel Sir Archibald David Stirling, (15 November 1915 – 4 November 1990) was a British mountaineer, World War II British Army officer, and the founder of the Special Air Service.
Life before the war.
Stirling was born at his family's ancestral home, Keir House in the parish of Lecropt, Perthshire. He was the son of Brigadier General Archibald Stirling, of Keir, and Margaret Fraser, daughter of Simon Fraser, the Lord Lovat, (a descendant of Charles II, King of Scots). His cousin was Simon Fraser, 15th Lord Lovat, and his paternal grandparents were Sir William Stirling-Maxwell, 9th Baronet and Lady Anna Maria Leslie-Melville. Raised in the Roman Catholic faith of his mother, he was educated at the Benedictine Ampleforth College and Trinity College, Cambridge. A tall and athletic figure (he was tall). He was training to climb Mount Everest when World War II broke out.
World War II and the founding of the SAS.
Stirling was commissioned into the Scots Guards from Ampleforth College Contingent Officer Training Corps on 24 July 1937. In June 1940 he volunteered for the new No. 8 Commando under Lieutenant-Colonel Robert Laycock which became part of Force Z (later named "Layforce"). After Layforce (and No.8 Commando) were disbanded on 1 August 1941, Stirling remained convinced that due to the mechanised nature of war a small team of highly trained soldiers with the advantage of surprise could exact greater damage to the enemy's ability to fight than an entire platoon. 
Believing that taking his idea up through the chain of command was unlikely to work, Stirling decided to go straight to the top. On crutches following a parachuting accident, he stealthily entered Middle East headquarters in Cairo (under, through or over a fence) in an effort to see Commander-in-Chief General Claude Auchinleck. Spotted by guards he ran into one office, only to come face-to-face with an officer he had previously fallen out with. Retreating rapidly to shouts of "Guards, Guards", he dodged into another office and came face to face with Deputy Commander Middle East General Ritchie. Stirling explained his plan to Ritchie, the latter immediately convincing Auchinleck (in the office next door) to allow Stirling to form a new Special Forces unit. The unit was given the deliberately misleading name "L Detachment, Special Air Service Brigade" to reinforce an existing deception of a parachute brigade existing in North Africa.
Short of equipment, particularly tents and related gear, at the outset when they set up base at Kibrit Air Base, the first operation of the new SAS was to relieve a well-equipped New Zealand unit of small tents, a large tent and contents including a bar and a piano. A truck and a series of bluffs managed to convince curious onlookers and the New Zealand unit that all was well. 
After a brief period of training, an initial attempt at attacking a German airfield by parachute landing on 16 November 1941 in support of Operation Crusader was disastrous. 42 of his 61 officers and men were killed, wounded or captured far from the target after being blown off course or landing in the wrong area, during one of the biggest storms for thirty years. Escaping only with the help of the Long Range Desert Group (LRDG) who were designated to pick up the unit after the attack, Stirling agreed that approaching by land under the cover of night would be safer and more effective than parachuting. As quickly as possible he organised raids on ports using this simple method, often bluffing through checkpoints at night using the language skills of some of his soldiers. 
Under his leadership, the Lewes bomb, the first hand-held dual explosive and incendiary device, was invented by Jock Lewes. American jeeps, which were able to deal with the harsh desert terrain better than other transport, were cut down, adapted and fitted with obsolete RAF machine guns. He also pioneered the use of small groups to escape detection. Stirling often led from the front, his SAS units driving through enemy airfields to shoot up aircraft and crew, replacing the early operational strategy of attaching bombs to enemy aircraft on foot. The first jeep-borne airfield raid occurred on the night of 7–8 July 1942 when Stirling's SAS group attacked Bagush airfield along with five other Axis airfields all in the same night. After returning to Cairo on 16 July, Stirling collected a consignment of more Willys Bantam Jeeps for further airfield raids. His biggest success was on the night of 26–27 July 1942 when his SAS squadron with 18 jeeps raided the Sidi Haneish landing strip and destroyed over 20 German aircraft for the loss of one man killed. After a drive through the desert and evading enemy patrols and aircraft, Stirling and his men reached the safety of friendly lines on 29 July.
These hit-and-run operations eventually proved Stirling's undoing; he was captured by the Germans in January 1943. Although he escaped, he was subsequently re-captured by the Italians, who took great delight in the embarrassment this caused to their German allies. A further four escape attempts were made, before Stirling was finally sent to Colditz Castle, where he remained for the rest of the war. After his capture, his brother Bill Stirling, along with Paddy Mayne, took command of the SAS. 
In North Africa, in the fifteen months before Stirling's capture, the SAS had destroyed over 250 aircraft on the ground, dozens of supply dumps, wrecked railways and telecommunications, and had put hundreds of enemy vehicles out of action. Field Marshal Montgomery described Stirling as "mad, quite mad" but believed that men like Stirling were needed in time of war. According to John Aspinall, Stirling reputedly personally strangled 41 men.
Private military company.
Worried that Britain was losing its power after the war, Stirling organised deals to provide British weapons and military personnel to other countries, like Saudi Arabia, for various privatised foreign policy operations. Along with several associates, Stirling formed Watchguard International Ltd, formerly with offices in Sloane Street (where the Chelsea Hotel later opened) before moving to South Audley Street in Mayfair.
Business was chiefly with the Gulf States. He was linked, along with Denys Rowley, to a failed attempt to the overthrow Libyan ruler Muammar Gaddafi in 1970 or 1971. Stirling was the founder of private military company KAS International, also known as KAS Enterprises.
Watchguard International Ltd was a private military company, registered in Jersey in 1965 by Stirling and John Woodhouse. Woodhouse's first assignment was to go to Yemen to report on the state of the royalist forces when a cease-fire was declared. At the same time Stirling was cultivating his contacts in the Iranian government and exploring the chances of obtaining work in Africa. The company operated in Zambia and in Sierra Leone, providing training teams and advising on security matters, but its founders' maverick ways of doing business caused its eventual downfall. Woodhouse resigned as Director of Operations after a series of disagreements and Stirling ceased to take an active part in 1972.
Great Britain 75.
In mid-1970s Great Britain, Stirling became increasingly worried that an "undemocratic event" would occur and decided to take action. He created an organisation called Great Britain 75 and recruited members from the aristocratic clubs in Mayfair; mainly ex-military men (often former SAS members). The plan was simple. Should civil unrest result in the breakdown of normal Government operations, they would take over its running. He describes this in detail in an interview from 1974, part of which is featured in Adam Curtis's documentary "The Mayfair Set", episode 1: "Who Pays Wins".
In August 1974, before Stirling was ready to go public with GB75, the pacifist magazine "Peace News" obtained and published his plans, and eventually Stirling – dismayed by the right-wing character of many of those seeking to join GB75 – abandoned the scheme.
Undermining trade unionism.
During the mid to late 1970s, Stirling created a secret organisation designed to undermine trade unionism from within. He recruited like minded individuals from within the trade union movement, with the express intention that they should cause as much trouble during conferences as permissible. One such member was Kate Losinska, who was Head of the Civil and Public Services Association. Funding for this "operation" came primarily from his friend Sir James Goldsmith.
Later life.
Stirling was the founder of the Capricorn Africa Society – a society for promoting an Africa free from racial discrimination. Founded in 1949, while Africa was still under colonial rule, it had its high point at the 1956 Salima Conference. However, because of his emphasis on a qualified and highly elitist voting franchise, similar to Disraeli's "fancy franchises", educated Africans were divided on it. Conversely, many white settlers believed it to be too liberal. Consequently, the society's attempt to deal with the problem of different levels of social development in a non-racial way was ineffective, although it received a surprising validation when the South African Communist Party used Stirling's multi-racial elitist model for its 1955 "Congress Alliance" when taking over the African National Congress of South Africa. Stirling resigned as Chairman of the Society in 1959. That year, following gambling losses he was obliged to note "John Aspinall - I owe you £173,500" in the accountant's ledger. One night in 1967 he lost a further £150,000. In 1968 he won substantial damages in libel against Len Deighton, among others.
Honours.
Stirling was knighted in 1990, and died later that year, 11 days before his 75th birthday. In 2002 the SAS memorial, a statue of Stirling standing on a rock, was opened on the Hill of Row near his family's estate at Park of Keir. Two bronze plaques were stolen from the statue sometime around the end of May 2014. The current Laird of the Keir estate is his nephew Archie Stirling, a millionaire businessman and former Scots Guards officer.

</doc>
<doc id="56112" url="https://en.wikipedia.org/wiki?curid=56112" title="Necker cube">
Necker cube

The Necker cube is an optical illusion first published as a rhomboid in 1832 by Swiss crystallographer Louis Albert Necker.
Ambiguity.
The Necker cube is an ambiguous line drawing.
The effect is interesting because each part of the picture is ambiguous by itself, yet the human visual system picks an interpretation of each part that makes the whole consistent. The Necker cube is sometimes used to test computer models of the human visual system to see whether they can arrive at consistent interpretations of the image the same way humans do.
Humans do not usually see an inconsistent interpretation of the cube. A cube whose edges cross in an inconsistent way is an example of an impossible object, specifically an impossible cube (compare Penrose triangle).
With the cube on the left, most people see the lower-left face as being in front most of the time. This is possibly because people view objects from above, with the top side visible, far more often than from below, with the bottom visible, so the brain "prefers" the interpretation that the cube is viewed from above . Another reason behind this may be due to the brain's natural preference of viewing things from left to right, therefore seeing the leftmost square as being in front.
There is evidence that by focusing on different parts of the figure one can force a more stable perception of the cube. The intersection of the two faces that are parallel to the observer forms a rectangle, and the lines that converge on the square form a "y-junction" at the two diagonally opposite sides. If an observer focuses on the upper "y-junction" the lower left face will appear to be in front. The upper right face will appear to be in front if the eyes focus on the lower junction. Blinking while being on the second perception will probably cause you to switch to the first one.
The Necker cube has shed light on the human visual system. The phenomenon has served as evidence of the human brain being a neural network with two distinct equally possible interchangeable stable states. Sidney Bradford, blind from the age of ten months but regaining his sight following an operation at age 52, did not perceive the ambiguity that normal-sighted observers do, but rather perceived only a flat image.
Apparent viewpoint.
The orientation of the Necker cube can also be altered by shifting the observer's point of view. When seen from apparent above, one face tends to be seen closer; and in contrast, when seen from a subjective viewpoint that is below, a different face comes to the fore.
References in popular culture.
The Necker cube is discussed to such extent in Robert J. Sawyer's 1998 science fiction novel "Factoring Humanity" that "Necker" becomes a verb, meaning to impel one's brain to switch from one perspective or perception to another.

</doc>
<doc id="56114" url="https://en.wikipedia.org/wiki?curid=56114" title="Urbanization">
Urbanization

Urbanization is a population shift from rural to urban areas, "the gradual increase in the proportion of people living in urban areas", and the ways in which each society adapts to the change. It is predominantly the process by which towns and cities are formed and become larger as more people begin living and working in central areas. The United Nations projected that half of the world's population would live in urban areas at the end of 2008. It is predicted that by 2050 about 64% of the developing world and 86% of the developed world will be urbanized. That is equivalent to approximately 3 billion urbanites by 2050, much of which will occur in Africa and Asia. Notably, the United Nations has also recently projected that nearly all global population growth from 2016 to 2030 will be absorbed by cities, about 1.1 billion new urbanites over the next 14 years.
Urbanization is relevant to a range of disciplines, including geography, sociology, economics, urban planning, and public health. The phenomenon has been closely linked to modernization, industrialization, and the sociological process of rationalization. Urbanization can be seen as a specific condition at a set time (e.g. the proportion of total population or area in cities or towns) or as an increase in that condition over time. So urbanization can be quantified either in terms of, say, the level of urban development relative to the overall population, or as the rate at which the urban proportion of the population is increasing. Urbanization creates enormous social, economic and environmental changes, which provide an opportunity for sustainability with the “potential to use resources more efficiently, to create more sustainable land use and to protect the biodiversity of natural ecosystems.” 
Urbanization is not merely a modern phenomenon, but a rapid and historic transformation of human social roots on a global scale, whereby predominantly rural culture is being rapidly replaced by predominantly urban culture. The first major change in settlement patterns was the accumulation of hunter-gatherers into villages many thousand years ago. Village culture is characterized by common bloodlines, intimate relationships, and communal behavior whereas urban culture is characterized by distant bloodlines, unfamiliar relations, and competitive behavior. This unprecedented movement of people is forecast to continue and intensify during the next few decades, mushrooming cities to sizes unthinkable only a century ago.
Today, in Asia the urban agglomerations of Osaka, Karachi, Jakarta, Mumbai, Shanghai, Manila, Seoul and Beijing are each already home to over 20 million people, while Delhi and Tokyo are forecast to approach or exceed 40 million people each within the coming decade. Outside Asia, Mexico City, São Paulo, New York, Lagos, Los Angeles, and Cairo are, or soon will be, home to over 20 million people.
History.
From the development of the earliest cities in Mesopotamia and Egypt until the 18th century, an equilibrium existed between the vast majority of the population who engaged in subsistence agriculture in a rural context, and small centres of populations in the towns where economic activity consisted primarily of trade at markets and manufactures on a small scale. Due to the primitive and relatively stagnant state of agriculture throughout this period the ratio of rural to urban population remained at a fixed equilibrium.
With the onset of the agricultural and industrial revolution in the late 18th century this relationship was finally broken and an unprecedented growth in urban population took place over the course of the 19th century, both through continued migration from the countryside and due to the tremendous demographic expansion that occurred at that time. In England the proportion of the population living in cities jumped from 17% in 1801 to 72% in 1891 (for other countries the figure was: 37% in France, 41% in Prussia and 28% in the United States).
As labourers were freed up from working the land due to higher agricultural productivity they converged on the new industrial cities like Manchester and Birmingham which were experiencing a boom in commerce, trade and industry. Growing trade around the world also allowed cereals to be imported from North America and refrigerated meat from Australasia and South America. Spatially, cities also expanded due to the development of public transport systems, which facilitated commutes of longer distances to the city centre for the working class.
Urbanization rapidly spread across the Western world and, since the 1950s, it has begun to take hold in the developing world as well. At the turn of the 20th century, just 15% of the world population lived in cities. According to the UN the year 2007 witnessed the turning point when more than 50% of the world population were living in cities, for the first time in human history.
Movement.
As more and more people leave villages and farms to live in cities, urban growth results. The rapid growth of cities like Chicago in the late 19th century, Tokyo in the mid 20th, and Delhi in the 21st century can be attributed largely to rural-urban migration. This kind of growth is especially commonplace in developing countries. This phenomenal growth can be attributed not just to the lure of economic opportunities, but also to loss or degradation of farmland and pastureland due to development, pollution, land grabs, or conflict; the attraction and anonymity of hedonistic pleasures of urban areas; proximity and ease of mass transport; and the opportunity to assert individualism.
Urban centres are seen by many as an opportunity to "escape traditional patriarchy and experience new freedoms": this includes greater access to education, health, and employment. However, for many who seek these opportunities the opposite occurs, resulting in extreme poverty, exclusion, vulnerability and marginalization due to urban sprawl where "urban land is expanding much faster than the urban population". This results in a strain on the urban area: the urban poor are forced to create slums, and then ultimately face unhealthy living conditions without access to the very opportunities they sought in the first place. Urbanization isn't just developing cities street but it's also redeveloping a people and a culture by displacement and economic hardship.The United Nations Population Fund (UNFPA) estimated that residents in slums had risen to about 863 million in 2012 from over 650 million in 1990.
The rapid urbanization of the world's population over the 20th century is described in the 2005 Revision of the UN World Urbanization Prospects report. The global proportion of urban population rose dramatically from 13% (220 million) in 1900, to 29% (732 million) in 1950, and 49% (3.2 billion) in 2005. The same report projected that the figure is likely to rise to 60% (4.9 billion) by 2030. It is expected that from 2007 to 2050, the global urban population will nearly double (from 3.3 billion to 6.4 billion), absorbing all population growth and as well as inflows from rural areas.
According to the UNFPA State of the World Population 2007 report, sometime in the middle of 2007, the majority of people worldwide lived in towns or cities, for the first time in history; this is referred to as the arrival of the "Urban Millennium" or the "tipping point". In future it is estimated 93% of urban growth will occur in developing nations, with 80% of urban growth occurring in Asia and Africa.
Urbanization rates vary between countries. The United States and United Kingdom have a far higher urbanization level than India, Swaziland and Niger, but a far slower annual urbanization rate, since much less of the population is living in a rural area. Some nations make a distinction between suburban and urban areas, while others do not; indeed, human conditions within such areas differ greatly.
Causes.
Urbanization occurs as individual, commercial flight, social and government action reduce the time and expense of commuting and transportation and improve opportunities for jobs, education, housing, and transportation. Living in a city can provide opportunities of proximity, diversity, and marketplace competition. As against this, there may be alienation issues, stress, increased cost of living, and negative social aspects that result from mass marginalization. Suburbanization, which is happening in the cities of the largest developing countries, may be regarded as an attempt to balance these negative aspects of urban life while still allowing access to the large extent of shared resources.
In cities, money, services, wealth and opportunities are centralized. Many rural inhabitants come to the city to seek their fortune and alter their social position. Businesses, which provide jobs and exchange capital, are more concentrated in urban areas. Whether the source is trade or tourism, it is also through the ports or banking systems, commonly located in cities, that foreign money flows into a country.
Many people move into cities for the economic opportunities, but this does not fully explain the very high recent urbanization rates in places like China and India. Rural flight is a contributing factor to urbanization. In rural areas, often on small family farms or collective farms in villages, it has historically been difficult to access manufactured goods, though the relative overall quality of life is very subjective, and may certainly surpass that of the city. Farm living has always been susceptible to unpredictable environmental conditions, and in times of drought, flood or pestilence, survival may become extremely problematic.
In a New York Times article concerning the acute migration away from farming in Thailand, life as a farmer was described as "hot and exhausting". "Everyone says the farmer works the hardest but gets the least amount of money". In an effort to counter this impression, the Agriculture Department of Thailand is seeking to promote the impression that farming is "honorable and secure".
However, in Thailand, urbanization has also resulted in massive increases in problems such as obesity. City life, especially in modern urban slums of the developing world, is certainly hardly immune to pestilence or climatic disturbances such as floods, yet continues to strongly attract migrants. Examples of this were the 2011 Thailand floods and 2007 Jakarta flood. Urban areas are also far more prone to violence, drugs, and other urban social problems. In the United States, industrialization of agriculture has negatively affected the economy of small and middle-sized farms and strongly reduced the size of the rural labour market.
Particularly in the developing world, conflict over land rights due to the effects of globalization has led to less politically powerful groups, such as farmers, losing or forfeiting their land, resulting in obligatory migration into cities. In China, where land acquisition measures are forceful, there has been far more extensive and rapid urbanization (54%) than in India (36%), where peasants form militant groups (e.g. Naxalites) to oppose such efforts. Obligatory and unplanned migration often results in rapid growth of slums. This is also similar to areas of violent conflict, where people are driven off their land due to violence. Bogota, Colombia is one example of this.
Cities offer a larger variety of services, including specialist services not found in rural areas. These services requires workers, resulting in more numerous and varied job opportunities. Elderly people may be forced to move to cities where there are doctors and hospitals that can cater for their health needs. Varied and high quality educational opportunities are another factor in urban migration, as well as the opportunity to join, develop, and seek out social communities.
Urbanization also creates opportunities for women that are not available in rural areas. This creates a gender-related transformation where women are engaged in paid employment and have access to education. This may cause fertility to decline. However, women are sometimes still at a disadvantage due to their unequal position in the labour market, their inability to secure assets independently from male relatives and exposure to violence.
People in cities are more productive than in rural areas. An important question is whether this is due to agglomeration effects or whether cities simply attract those who are more productive. Economists have recently shown that there exists a large productivity gain due to locating in dense agglomerations. It is thus possible that agents locate in cities in order to benefit from these agglomeration effects.
Dominant conurbation.
The dominant conurbation(s) of a country can benefit to a greater extent from the same things cities offer, making them magnets for not just the non-urban population, but also urban and suburban population from other cities. Dominant conurbations are quite often primate cities, but do not have to be. For instance Greater Manila is rather a conurbation than a city: its 20 million overall population (over 20% national population) make it very much a primate city, but Quezon City (2.7 million), the largest municipality in Greater Manila, and Manila (1.6 million), the capital, are not. A conurbation's dominance can be measured by output, wealth, and especially population, each expressed as a percentage of an entire country. Greater Seoul is one conurbation with massive dominance over South Korea, it is home to 50% of the entire national population.
Though Greater Busan-Ulsan (15%, 8 million) and Greater Osaka (14%, 18 million) exhibit strong dominance in their respective countries, yet they are losing population to their even more dominant rivals, Seoul and Tokyo respectively.
Economic effects.
As cities develop, effects can include a dramatic increase and change in costs, often pricing the local working class out of the market, including such functionaries as employees of the local municipalities. For example, Eric Hobsbawm's book "The age of revolution: 1789–1848" (published 1962 and 2005) chapter 11, stated "Urban development in our period was a gigantic process of class segregation, which pushed the new labouring poor into great morasses of misery outside the centres of government and business and the newly specialized residential areas of the bourgeoisie. The almost universal European division into a 'good' west end and a 'poor' east end of large cities developed in this period." This is likely due the prevailing south-west wind which carries coal smoke and other airborne pollutants downwind, making the western edges of towns preferable to the eastern ones. Similar problems now affect the developing world, rising inequality resulting from rapid urbanization trends. The drive for rapid urban growth and often efficiency can lead to less equitable urban development. Think tanks such as the Overseas Development Institute have proposed policies that encourage labor-intensive growth as a means of absorbing the influx of low-skilled and unskilled labor. One problem these migrant workers are involved with is the growth of slums. In many cases, the rural-urban low skilled or unskilled migrant workers, attracted by economic opportunities in urban areas, cannot find a job and afford housing in cities and have to dwell in slums. Urban problems, along with infrastructure developments, are also fueling suburbanization trends in developing nations, though the trend for core cities in said nations tends to continue to become ever denser. Urbanization is often viewed as a negative trend, but there are positives in the reduction of expenses in commuting and transportation while improving opportunities for jobs, education, housing, and transportation. Living in cities permits individuals and families to take advantage of the opportunities of proximity and diversity. While cities have a greater variety of markets and goods than rural areas, infrastructure congestion, monopolization, high overhead costs, and the inconvenience of cross-town trips frequently combine to make marketplace competition harsher in cities than in rural areas.
In many developing countries where economies are growing, the growth is often erratic and based on a small number of industries. For young people in these countries barriers exist such as, lack of access to financial services and business advisory services, difficulty in obtaining credit to start a business, and lack of entrepreneurial skills, in order for them to access opportunities in these industries. Investment in human capital so that young people have access to quality education and infrastructure to enable access to educational facilities is imperative to overcoming economic barriers.
Environmental effects.
The existence of Urban heat islands has become a growing concern over the years. An urban heat island is formed when industrial and urban areas produce and retain heat. Much of the solar energy that reaches rural areas is consumed by evaporation of water from vegetation and soil. In cities, where there is less vegetation and exposed soil, most of the sun's energy is instead absorbed by buildings and asphalt; leading to higher surface temperatures. Vehicles, factories and industrial and domestic heating and cooling units release even more heat. As a result, cities are often 1 to 3 °C (1.8 to 5.4 °F) warmer than surrounding landscapes. Impacts also include reducing soil moisture and a reduction in reabsorption of carbon dioxide emissions.
The occurrence of eutrophication in bodies of water is another effect large urban populations have on the environment. When rain occurs in these large cities, the rain filters down the pollutants such as CO2 and other green house gases in the air onto the ground below. Then, those chemicals are washed directly into rivers, streams and oceans, causing a decline in water quality and damaging marine ecosystems.
In his book "Whole Earth Discipline", Stewart Brand argues that the effects of urbanization are primarily positive for the environment. First, the birth rate of new urban dwellers falls immediately to replacement rate, and keeps falling, reducing environmental stresses caused by population growth. Secondly, emigration from rural areas reduces destructive subsistence farming techniques, such as improperly implemented slash and burn agriculture.
In July 2013 a report issued by the United Nations Department of Economic and Social Affairs warned that with 2.4 billion more people by 2050, the amount of food produced will have to increase by 70%, straining food resources, especially in countries already facing food insecurity due to changing environmental conditions. The mix of changing environmental conditions and the growing population of urban regions, according to UN experts, will strain basic sanitation systems and health care, and potentially cause a humanitarian and environmental disaster.
Health effects.
In the developing world, urbanization does not seem to translate into a significant increase in life expectancy. Rapid urbanization has brought increased mortality from non-communicable diseases associated with lifestyle, including cancer and heart disease. Differences in mortality from contagious diseases vary depending on the particular disease.
Urban health levels are better in comparison those in rural areas on average. However, residents in poor areas such as slums and informal settlements suffer "disproportionately from disease, injury, premature death, and the combination of ill-health and poverty entrenches disadvantage over time." Many urban poor have difficulty accessing health services due to an increasing requirement to pay for them; so people resort to less qualified and unregulated providers.
While urbanization is associated with improvements in public hygiene, sanitation and access to health care, it also entails changes in occupational, dietary and exercise patterns. It can have mixed effects on health patterns, alleviating some problems and accentuating others. For instance, in children urbanization is associated with a lower risk of under-nutrition but a higher risk of overweight. Overall, body mass index and cholesterol levels increase sharply with national income and the degree of urbanization. Agriculturist has new studied on urbanization and globalization. Fast food is the food of chose which is causing health decline. Easier access to non-traditional foods may lead to less healthy dietary patterns. In India prevalence of diabetes in urban areas appears to be more than twice as high as in rural areas. In general, major risk factors for chronic diseases are more prevalent in urban environments.
Changing forms.
Different forms of urbanization can be classified depending on the style of architecture and planning methods as well as historic growth of areas.
In cities of the developed world urbanization traditionally exhibited a concentration of human activities and settlements around the downtown area, the so-called "in-migration". In-migration refers to migration from former colonies and similar places. The fact that many immigrants settle in impoverished city centres led to the notion of the "peripheralization of the core", which simply describes that people who used to be at the periphery of the former empires now live right in the centre.
Recent developments, such as inner-city redevelopment schemes, mean that new arrivals in cities no longer necessarily settle in the centre. In some developed regions, the reverse effect, originally called counter urbanization has occurred, with cities losing population to rural areas, and is particularly common for richer families. This has been possible because of improved communications, and has been caused by factors such as the fear of crime and poor urban environments. It has contributed to the phenomenon of shrinking cities experienced by some parts of the industrialized world.
When the residential area shifts outward, this is called suburbanization. A number of researchers and writers suggest that suburbanization has gone so far to form new points of concentration outside the downtown both in developed and developing countries such as India. This networked, poly-centric form of concentration is considered by some emerging pattern of urbanization. It is called variously exurbia, edge city (Garreau, 1991), network city (Batten, 1995), or postmodern city (Dear, 2000). Los Angeles is the best-known example of this type of urbanization. Interestingly, in the United States, this process has reversed as of 2011, with "re-urbanization" occurring as "suburban flight" due to chronically high transport costs.
Rural migrants are attracted by the possibilities that cities can offer, but often settle in shanty towns and experience extreme poverty. The inability of countries to provide adequate housing for these rural migrants is related to overurbanization, a phenomenon in which the rate of urbanization grows more rapidly that the rate of economic development, leading to high unemployment and high demand for resources. In the 1980s, this was attempted to be tackled with the urban bias theory which was promoted by Michael Lipton.
Most of the urban poor in developing countries able to find work can spend their lives in insecure, poorly paid jobs. According to research by the Overseas Development Institute pro-poor urbanization will require labour-intensive growth, supported by labour protection, flexible land use regulation and investments in basic services.'
Urbanization can be planned urbanization or organic. Planned urbanization, i.e.: planned community or the garden city movement, is based on an advance plan, which can be prepared for military, aesthetic, economic or urban design reasons. Examples can be seen in many ancient cities; although with exploration came the collision of nations, which meant that many invaded cities took on the desired planned characteristics of their occupiers. Many ancient organic cities experienced redevelopment for military and economic purposes, new roads carved through the cities, and new parcels of land were cordoned off serving various planned purposes giving cities distinctive geometric designs. UN agencies prefer to see urban infrastructure installed before urbanization occurs. Landscape planners are responsible for landscape infrastructure (public parks, sustainable urban drainage systems, greenways etc.) which can be planned before urbanization takes place, or afterward to revitalize an area and create greater livability within a region. Concepts of control of the urban expansion are considered in the American Institute of Planners.
As the population continues to grow and urbanize at unprecedented rates, new urbanism and smart growth techniques will create a successful transition into developing environmentally, economically, and socially sustainable cities. Smart Growth and New Urbanism’s principles include walkability, mixed-use development, comfortable high-density design, land conservation, social equity, and economic diversity. Mixed-use communities work to fight gentrification with affordable housing to promote social equity, decrease automobile dependency to lower use of fossil fuels, and promote a localized economy. Walkable communities have a 38% higher average GDP per capita than less walkable urban metros (Leinberger, Lynch). By combining economic, environmental, and social sustainability, cities will become equitable, resilient, and more appealing than urban sprawl that overuses land, promotes automobile use, and segregates the population economically.
See also.
Contributors to urbanization:
Historical:
Regional:

</doc>
<doc id="56116" url="https://en.wikipedia.org/wiki?curid=56116" title="Salting the earth">
Salting the earth

Salting the earth, or sowing with salt, is the ritual of spreading salt on conquered cities to symbolize a curse on their re-inhabitation. It originated as a symbolic practice in the ancient Near East and became a well-established folkloric motif in the Middle Ages. There is no evidence that sufficient amounts of salt were used to render large tracts of land unusable.
Destroying cities.
The custom of purifying or consecrating a destroyed city with salt and cursing anyone who dared to rebuild it was widespread in the ancient Near East, but historical accounts are unclear as to what the sowing of salt meant in that process.
Various Hittite and Assyrian texts speak of ceremonially strewing salt, minerals, or plants (weeds, "cress", or "kudimmu", which are associated with salt and desolation) over destroyed cities, including Hattusa, Taidu, Arinna, Hunusa, Irridu, and Susa. The "Book of Judges" (9:45) says that Abimelech, the judge of the Israelites, sowed his own capital, Shechem, with salt, c. 1050 BC, after quelling a revolt against him. This may have been part of a ḥērem ritual (see Salt in the Bible).
Starting in the 19th century, various texts claim that the Roman general Scipio Aemilianus Africanus plowed over and sowed the city of Carthage with salt after defeating it in the Third Punic War (146 BC), sacking it, and forcing the survivors into slavery. However, no ancient sources exist documenting the salting itself. The Carthage story is a later invention, probably modeled on the story of Shechem. The ritual of symbolically drawing a plow over the site of a city is, however, mentioned in ancient sources, though not in reference to Carthage specifically.
When Pope Boniface VIII destroyed Palestrina in 1299, he ordered that it be plowed "following the old example of Carthage in Africa", and also salted. "I have run the plough over it, like the ancient Carthage of Africa, and I have had salt sown upon it..." The text is not clear as to whether he thought Carthage was salted. Later accounts of other saltings in the destructions of medieval Italian cities are now rejected as unhistorical: Padua by Attila (452)--perhaps in a parallel between Attila and the ancient Assyrians; Milan by Frederick Barbarossa (1162); and Semifonte by the Florentines (1202).
The English epic poem "Siege of Jerusalem" (c. 1370) recounts that Titus commanded the sowing of salt on the Temple, but this episode is not found in Josephus.
Punishing traitors.
In Spain and the Spanish Empire, salt was poured onto the land owned by a convicted traitor (often one who was executed and his head placed on a "picota", or pike, afterwards) after his house was demolished.
Likewise, in Portugal, salt was poured onto the land owned by a convicted traitor. The last known event of this sort was the destruction of the Duke of Aveiro's palace in Lisbon in 1759, due to his participation in the Távora affair (a conspiracy against King Joseph I of Portugal). His palace was demolished and his land was salted. A stone memorial now perpetuates the memory of the shame of the Duke, where it is written:
In this place were put to the ground and salted the houses of José Mascarenhas, stripped of the honours of Duque de Aveiro and others... Put to Justice as one of the leaders of the most barbarous and execrable upheaval that... was committed against the most royal and sacred person of the Lord Joseph I. In this infamous land nothing may be built for all time.
In the Portuguese colony of Brazil, the leader of the Inconfidência Mineira, Tiradentes, was sentenced to death and his house was "razed and salted, so that never again be built up on the floor, ... and even the floor will rise up a standard by which the memory is preserved (preserving) the infamy of this heinous offender..." He suffered further indignities, being hanged and quartered, his body parts carried to various parts of the country where his fellow revolutionaries had met, and his children deprived of their property and honor.
Legends.
An ancient legend says that Odysseus feigned madness by yoking a horse and an ox to his plow and sowing salt.

</doc>
<doc id="56117" url="https://en.wikipedia.org/wiki?curid=56117" title="Shareholder rights plan">
Shareholder rights plan

A shareholder rights plan, colloquially known as a "poison pill", is a type of defensive tactic used by a corporation's board of directors against a takeover. Typically, such a plan gives shareholders the right to buy more shares at a discount if one shareholder buys a certain percentage or more of the company's shares. The plan could be triggered, for instance, if any one shareholder buys 20% of the company's shares, at which point every shareholder (except the one who possesses 20%) will have the right to buy a new issue of shares at a discount. If every other shareholder is able to buy more shares at a discount, such purchases would dilute the bidder's interest, and the cost of the bid would rise substantially. Knowing that such a plan could be activated, the bidder could be disinclined to take over the corporation without the board's approval, and would first negotiate with the board in order to revoke the plan.
The plan can be issued by the board of directors as an "option" or a "warrant" attached to existing shares, and only be revoked at the discretion of the board.
In the field of mergers and acquisitions, shareholder rights plans were devised in the early 1980s as a way to prevent takeover bidders from negotiating a price for sale of shares directly with shareholders, and instead forcing the bidder to negotiate with the board.
Shareholder rights plans, or poison pills, are controversial because they hinder an active market for corporate control. Further, giving directors the power to deter takeovers puts directors in a position to enrich themselves, as they may effectively ask to be compensated for the price of consenting to a takeover. Shareholder rights plans are unlawful without shareholder approval in many jurisdictions such as the United Kingdom, frowned upon in others such as throughout the European Union, and lawful only if used "proportionately" in others, including Delaware in the United States.
History.
The poison pill was invented by mergers and acquisitions lawyer Martin Lipton of Wachtell, Lipton, Rosen & Katz in 1982, as a response to tender-based hostile takeovers. Poison pills became popular during the early 1980s in response to the wave of takeovers by corporate raiders such as Carl Icahn. The term "poison pill" derives its original meaning from a poison pill physically carried by various spies throughout history, a pill which was taken by the spies when they were discovered to eliminate the possibility of being interrogated by an enemy.
It was reported in 2001 that since 1997, for every company with a poison pill which successfully resisted a hostile takeover, there were 20 companies with poison pills that accepted takeover offers. The trend since the early 2000s has been for shareholders to vote against poison pill authorization, since poison pills are designed to resist takeovers, whereas from the point of view of a shareholder, takeovers can be financially rewarding.
Some have argued that poison pills are detrimental to shareholder interests because they perpetuate existing management. For instance, Microsoft originally made an unsolicited bid for Yahoo!, but subsequently dropped the bid after Yahoo! CEO Jerry Yang threatened to make the takeover as difficult as possible unless Microsoft raised the price to US$37 per share. One Microsoft executive commented, "They are going to burn the furniture if we go hostile. They are going to destroy the place." Yahoo has had a shareholders rights plan in place since 2001. Analysts suggested that Microsoft's raised offer of $33 per share was already too expensive, and that Yang was not bargaining in good faith, which later led to several shareholder lawsuits and an aborted proxy fight from Carl Icahn. Yahoo's stock price plunged after Microsoft withdrew the bid, and Jerry Yang faced a backlash from stockholders that eventually led to his resignation.
Overview.
In publicly held companies, "poison pills" refer to various methods to deter takeover bids. Takeover bids are attempts by a bidder to obtain control of a target company, either by soliciting proxies to get elected to the board or by acquiring a controlling block of shares and using the associated votes to get elected to the board. Once in control of the board, the bidder can manage the target. As discussed below, targets have various takeover defenses available, and several types of defense have been called "poison pills" because they harm not only the bidder, but the target (or its shareholders) as well. Currently, the most common type of takeover defense is a shareholder rights plan.
Because the board of directors of the company can redeem or otherwise eliminate a standard poison pill, it does not typically preclude a proxy fight or other takeover attempts not accompanied by an acquisition of a significant block of the company's stock. It can, however, prevent shareholders from entering into certain agreements that can assist in a proxy fight, such as an agreement to pay another shareholder's expenses. In combination with a staggered board of directors, however, a shareholder rights plan can be a defense.
The goal of a shareholder rights plan is to force a bidder to negotiate with the target's board and not directly with the shareholders. The effects are twofold:
Common types of poison pills.
The target issues a large number of new shares, often preferred shares, to existing shareholders. These new shares usually have severe redemption provisions, such as allowing them to be converted into a large number of common shares if a takeover occurs. This immediately dilutes the percentage of the target owned by the acquirer, and makes it more expensive to acquire 50% of the target's stock.
The target takes on large debts in an effort to make the debt load too high to be attractive—the acquirer would eventually have to pay the debts.
The company buys a number of smaller companies using a stock swap, diluting the value of the target's stock.
Under this scenario, the target company re-phases all its employees' stock-option grants to ensure they immediately become vested if the company is taken over. Many employees can then exercise their options and then dump the stocks. With the release of the "golden handcuffs", many discontented employees may quit immediately after having cashed in their stock options. This poison pill is designed to create an exodus of talented employees, reducing a corporate value as a target. In many high-tech businesses, attrition of talented human resources may result in a diluted or empty shell being left behind for the new owner.
For instance, PeopleSoft guaranteed its customers in June 2003 that if it were acquired within two years, presumably by its rival Oracle, and product support were reduced within four years, its customers would receive a refund of between two and five times the fees they had paid for their Peoplesoft software licenses. While the acquisition ultimately prevailed, the hypothetical cost to Oracle was valued at as much as US$1.5 billion.
In a voting plan, a company will charter preferred stock with superior voting rights over that of common shareholders. If an unfriendly bidder acquired a substantial quantity of the target firm's voting common stock, it then still would not be able to exercise control over its purchase. For example, ASARCO established a voting plan in which 99% of the company's common stock would only harness 16.5% of the total voting power.
In addition to these pills, a "dead-hand" provision allows only the directors who introduce the poison pill to remove it (for a set period after they have been replaced), thus potentially delaying a new board’s decision to sell a company.
Constraints and legal status.
The legality of poison pills had been unclear when they were first put to use in the early 1980s. However, the Delaware Supreme Court upheld poison pills as a valid instrument of takeover defense in its 1985 decision in Moran v. Household International, Inc. However, many jurisdictions other than the U.S. have held the poison pill strategy as illegal, or place restraints on their use.
In Canada, almost all shareholders rights plans are "chewable", meaning they contain a permitted bid concept such that a bidder who is willing to conform to the requirements of a permitted bid can acquire the company by take-over bid without triggering a flip-in event. Shareholder rights plans in Canada are also weakened by the ability of a hostile acquirer to petition the provincial securities regulators to have the company's pill overturned. Generally, the courts will overturn the pill to allow shareholders to decide whether they want to tender to a bid for the company. However, the company may be allowed to maintain it for long enough to run an auction to see if a white knight can be found. A notable Canadian case before the securities regulators in 2006 involved the poison pill of Falconbridge Ltd. which at the time was the subject of a friendly bid from Inco and a hostile bid from Xstrata plc, which was a 20% shareholder of Falconbridge. Xstrata applied to have Falconbridge's pill invalidated, citing among other things that the Falconbridge had had its pill in place without shareholder approval for more than nine months and that the pill stood in the way of Falconbridge shareholders accepting Xstrata's all-cash offer for Falconbridge shares. Despite similar facts with previous cases in which securities regulators had promptly taken down pills, the Ontario Securities Commission ruled that Falconbridge's pill could remain in place for a further limited period as it had the effect of sustaining the auction for Falconbridge by preventing Xstrata increasing its ownership and potentially obtaining a blocking position that would prevent other bidders from obtaining 100% of the shares.
In the United Kingdom, poison pills are not allowed under the Takeover Panel rules. The rights of public shareholders are protected by the Panel on a case-by-case, principles-based regulatory regime. One disadvantage of the Panel's prohibition of poison pills is that it allows bidding wars to be won by hostile bidders who buy shares of their target in the marketplace during "raids". Raids have helped bidders win targets such as BAA plc and AWG plc when other bidders were considering emerging at higher prices. If these companies had poison pills, they could have prevented the raids by threatening to dilute the positions of their hostile suitors if they exceeded the statutory levels (often 10% of the outstanding shares) in the rights plan. The London Stock Exchange itself is another example of a company that has seen significant stakebuilding by a hostile suitor, in this case the NASDAQ. The LSE's ultimate fate is currently up in the air, but NASDAQ's stake is sufficiently large that it is essentially impossible for a third party bidder to make a successful offer to acquire the LSE.
Takeover law is still evolving in continental Europe, as individual countries slowly fall in line with requirements mandated by the European Commission. Stakebuilding is commonplace in many continental takeover battles such as Scania AB. Formal poison pills are quite rare in continental Europe, but national governments hold golden shares in many "strategic" companies such as telecom monopolies and energy companies. Governments have also served as "poison pills" by threatening potential suitors with negative regulatory developments if they pursue the takeover. Examples of this include Spain's adoption of new rules for the ownership of energy companies after E.ON of Germany made a hostile bid for Endesa and France's threats to punish any potential acquiror of Groupe Danone.
Other takeover defenses.
Poison pill is sometimes used more broadly to describe other types of takeover defenses that involve the target taking some action. Although the broad category of takeover defenses (more commonly known as "shark repellents") includes the traditional shareholder rights plan poison pill. Other anti-takeover protections include:
Shareholder input.
More companies are giving shareholders a say on poison pills. As of June 15, 2009, 21 companies that had adopted or extended a poison pill had publicly disclosed they plan to put the poison pill to a shareholder vote within a year. That was up from 2008's full year total of 18, and was the largest number ever reported since the early 1980s, when the pill was invented.

</doc>
<doc id="56118" url="https://en.wikipedia.org/wiki?curid=56118" title="Tierra Amarilla, New Mexico">
Tierra Amarilla, New Mexico

Tierra Amarilla is a small unincorporated community near the Carson National Forest in the northern part of the U.S. state of New Mexico. It is the county seat of Rio Arriba County.
"Tierra Amarilla" is Spanish for "Yellow Earth". The name refers to clay deposits found in the Chama River Valley and used by Native American peoples. Tewa and Navajo toponyms for the area also refer to the yellow clay.
History.
There is evidence of 5000 years of habitation in the Chama River Valley including pueblo sites south of Abiquiu. The area served as a trade route for peoples in the present-day Four Corners region and the Rio Grande Valley. Navajos later used the valley as a staging area for raids on Spanish settlements along the Rio Grande. Written accounts of the Tierra Amarilla locality by pathfinding Spanish friars in 1776 described it as suitable for pastoral and agricultural use. The route taken by the friars from Santa Fe to California became the Spanish Trail. During the Californian Gold Rush the area became a staging point for westward fortune seekers.
Tierra Amarilla Grant.
The "Tierra Amarilla Grant" was created in 1832 by the Mexican government for Manuel Martinez and settlers from Abiquiu. The land grant encompassed a more general area than the contemporary community known as "Tierra Amarilla". The grant holders were unable to maintain a permanent settlement due to "raids by Utes, Navajos and Jicarilla Apaches" until early in the 1860s. In 1860 the United States Congress confirmed the land grant as a private grant, rather than a community grant, due to mistranslated and concealed documents. Although a land patent for the grant required the completion of a geographical survey before issuance, some of Manuel Martinez' heirs began to sell the land to Anglo speculators. In 1880 Thomas Catron sold some of the grant to the Denver and Rio Grande Railway for the construction of their San Juan line and a service center at Chama. By 1883 Catron had consolidated the deeds he held for the whole of the grant sans the original villages and their associated fields. In 1950, the descendants of the original grant holder's court petitions to reclaim communal land were rebuked.
Rio Arriba's county seat.
In 1866 the United States Army established Camp Plummer just south of Los Ojos (established in 1860) to rein in already decreased Native American activity on the grant. The military encampment was deserted in 1869. Las Nutrias, the site of the contemporary community, was founded nearby c.1862. The first post office in Las Nutrias was established in 1866 and bore the name "Tierra Amarilla", as did the present one which was established in 1870 after an approximately two-year absence. In 1877 a U.S. Army lieutenant described the village as "the center of the Mexican population of northwestern New Mexico". The territorial legislature located Rio Arriba's county seat in Las Nutrias and renamed the village in 1880. The Denver and Rio Grande Railway's 1881 arrival at Chama, about ten miles to the north, had profound effects on the development of the region by bringing the area out of economic and cultural isolation.
When Tierra Amarilla was designated as the county seat the villagers set about building a courthouse. This structure was demolished to make way for the present one, which was built in 1917 and gained notoriety fifty years later when it was the location of a gunfight between land rights activists and authorities. The neoclassical design by Isaac Rapp is now on the National Register of Historic Places.
Courthouse raid.
The Alianza Federal de Mercedes, led by Reies Tijerina, raided the Rio Arriba County Courthouse in 1967. Attempting to make a citizen's arrest of the district attorney "to bring attention to the unscrupulous means by which government and Anglo settlers had usurped Hispanic land grant properties", an armed struggle in the courthouse ensued resulting in Tijerina and his group fleeing to the south with two prisoners as hostages. Eulogio Salazar, a prison guard, was shot and Daniel Rivera, a sheriff's deputy, was badly injured. The National Guard, FBI and New Mexico State Police successfully pursued Tijerina, who was sentenced to less than three years.
Geography.
The Brazos Cliffs are a prominent nearby landmark and attraction, and a popular destination for rock climbers. Also nearby are the artificial Heron Lake and El Vado Lake.
Tierra Amarillas' Elevation is 7,524 feet above sea level.
Layout.
The settlement is situated in a cluster of villages along United States Route 84 and the Chama River. The layout of the villages, including the one that became Tierra Amarilla, do not follow the urban planning principles of the Laws of the Indies.
Demographics.
Tierra Amarilla has the ZIP code of 87575. The ZIP Code Tabulation Area for ZIP Code 87575 had a population of 750 at the 2000 census.

</doc>
<doc id="56119" url="https://en.wikipedia.org/wiki?curid=56119" title="Takeover">
Takeover

In business, a takeover is the purchase of one company (the "target") by another (the "acquirer", or "bidder"). In the UK, the term refers to the acquisition of a public company whose shares are listed on a stock exchange, in contrast to the acquisition of a private company.
Types of takeover.
Friendly takeovers.
A "friendly takeover" is an acquisition which is approved by the management. Before a bidder makes an offer for another company, it usually first informs the company's board of directors. In an ideal world, if the board feels that accepting the offer serves the shareholders better than rejecting it, it recommends the offer be accepted by the shareholders.
In a private company, because the shareholders and the board are usually the same people or closely connected with one another, private acquisitions are usually friendly. If the shareholders agree to sell the company, then the board is usually of the same mind or sufficiently under the orders of the equity shareholders to cooperate with the bidder. This point is not relevant to the UK concept of takeovers, which always involve the acquisition of a public company.
Hostile takeovers.
A "hostile takeover" allows a bidder to take over a target company whose management is unwilling to agree to a merger or takeover. A takeover is considered "hostile" if the target company's board rejects the offer, and if the bidder continues to pursue it, or the bidder makes the offer directly after having announced its firm intention to make an offer. Development of the hostile tender is attributed to Louis Wolfson.
A hostile takeover can be conducted in several ways. A tender offer can be made where the acquiring company makes a public offer at a fixed price above the current market price. Tender offers in the United States are regulated by the Williams Act. An acquiring company can also engage in a proxy fight, whereby it tries to persuade enough shareholders, usually a simple majority, to replace the management with a new one which will approve the takeover. Another method involves quietly purchasing enough stock on the open market, known as a "creeping tender offer", to effect a change in management. In all of these ways, management resists the acquisition, but it is carried out anyway.
In the United States, a common defense tactic against hostile takeovers is to use section 16 of the Clayton Act to seek an injunction, arguing that section 7 of the act would be violated if the offeror acquired the target's stock.
The main consequence of a bid being considered hostile is practical rather than legal. If the board of the target cooperates, the bidder can conduct extensive due diligence into the affairs of the target company, providing the bidder with a comprehensive analysis of the target company's finances. In contrast, a hostile bidder will only have more limited, publicly available information about the target company available, rendering the bidder vulnerable to hidden risks regarding the target company's finances. An additional problem is that takeovers often require loans provided by banks in order to service the offer, but banks are often less willing to back a hostile bidder because of the relative lack of target information which is available to them.
A well known example of an extremely hostile takeover was Oracle's hostile bid to acquire PeopleSoft.
Reverse takeovers.
A "reverse takeover" is a type of takeover where a private company acquires a public company. This is usually done at the instigation of the larger, private company, the purpose being for the private company to effectively float itself while avoiding some of the expense and time involved in a conventional IPO. However, in the UK under AIM rules, a reverse take-over is an acquisition or acquisitions in a twelve-month period which for an AIM company would:
An individual or organization, sometimes known as a corporate raider, can purchase a large fraction of the company's stock and, in doing so, get enough votes to replace the board of directors and the CEO. With a new agreeable management team, the stock is a much more attractive investment[why?], which would likely result in a price rise and a profit for the corporate raider and the other shareholders.
Backflip takeovers.
A "backflip takeover" is any sort of takeover in which the acquiring company turns itself into a subsidiary of the purchased company. This type of takeover can occur when a larger but less well-known company purchases a struggling company with a very well-known brand. Examples include:
Financing a takeover.
Funding.
Often a company acquiring another pays a specified amount for it. This money can be raised in a number of ways. Although the company may have sufficient funds available in its account, remitting payment entirely from the acquiring company's cash on hand is unusual. More often, it will be borrowed from a bank, or raised by an issue of bonds. Acquisitions financed through debt are known as leveraged buyouts, and the debt will often be moved down onto the balance sheet of the acquired company. The acquired company then has to pay back the debt. This is a technique often used by private equity companies. The debt ratio of financing can go as high as 80% in some cases. In such a case, the acquiring company would only need to raise 20% of the purchase price.
Loan note alternatives.
Cash offers for public companies often include a "loan note alternative" that allows shareholders to take a part or all of their consideration in loan notes rather than cash. This is done primarily to make the offer more attractive in terms of taxation. A conversion of shares into cash is counted as a disposal that triggers a payment of capital gains tax, whereas if the shares are converted into other securities, such as loan notes, the tax is rolled over.
All share deals.
A takeover, particularly a reverse takeover, may be financed by an all share deal. The bidder does not pay money, but instead issues new shares in itself to the shareholders of the company being acquired. In a reverse takeover the shareholders of the company being acquired end up with a majority of the shares in, and so control of, the company making the bid. The company has managerial rights.
All-cash deals.
If a takeover of a company consists of simply an offer of an amount of money per share, (as opposed to all or part of the payment being in shares or loan notes) then this is an all-cash deal. This does not define how the purchasing company sources the cash- that can be from existing cash resources; loans; or a separate issue of shares.
Mechanics.
In the United Kingdom.
Takeovers in the UK (meaning acquisitions of public companies only) are governed by the City Code on Takeovers and Mergers, also known as the 'City Code' or 'Takeover Code'. The rules for a takeover can be found in what is primarily known as 'The Blue Book'. The Code used to be a non-statutory set of rules that was controlled by city institutions on a theoretically voluntary basis. However, as a breach of the Code brought such reputational damage and the possibility of exclusion from city services run by those institutions, it was regarded as binding. In 2006, the Code was put onto a statutory footing as part of the UK's compliance with the European Takeover Directive (2004/25/EC).
The Code requires that all shareholders in a company should be treated equally. It regulates when and what information companies must and cannot release publicly in relation to the bid, sets timetables for certain aspects of the bid, and sets minimum bid levels following a previous purchase of shares.
In particular:
The Rules Governing the Substantial Acquisition of Shares, which used to accompany the Code and which regulated the announcement of certain levels of shareholdings, have now been abolished, though similar provisions still exist in the Companies Act 1985.
Strategies.
There are a variety of reasons why an acquiring company may wish to purchase another company. Some takeovers are "opportunistic" - the target company may simply be very reasonably priced for one reason or another and the acquiring company may decide that in the long run, it will end up making money by purchasing the target company. The large holding company Berkshire Hathaway has profited well over time by purchasing many companies opportunistically in this manner.
Other takeovers are "strategic" in that they are thought to have secondary effects beyond the simple effect of the profitability of the target company being added to the acquiring company's profitability. For example, an acquiring company may decide to purchase a company that is profitable and has good distribution capabilities in new areas which the acquiring company can use for its own products as well. A target company might be attractive because it allows the acquiring company to enter a new market without having to take on the risk, time and expense of starting a new division. An acquiring company could decide to take over a competitor not only because the competitor is profitable, but in order to eliminate competition in its field and make it easier, in the long term, to raise prices. Also a takeover could fulfill the belief that the combined company can be more profitable than the two companies would be separately due to a reduction of redundant functions.
Agency problems.
Takeovers may also benefit from principal–agent problems associated with top executive compensation. For example, it is fairly easy for a top executive to reduce the price of his/her company's stock – due to information asymmetry. The executive can accelerate accounting of expected expenses, delay accounting of expected revenue, engage in off-balance-sheet transactions to make the company's profitability appear temporarily poorer, or simply promote and report severely conservative (i.e. pessimistic) estimates of future earnings. Such seemingly adverse earnings news will be likely to (at least temporarily) reduce share price. (This is again due to information asymmetries since it is more common for top executives to do everything they can to window dress their company's earnings forecasts). There are typically very few legal risks to being 'too conservative' in one's accounting and earnings estimates.
A reduced share price makes a company an easier takeover target. When the company gets bought out (or taken private) – at a dramatically lower price – the takeover artist gains a windfall from the former top executive's actions to surreptitiously reduce share price. This can represent tens of billions of dollars (questionably) transferred from previous shareholders to the takeover artist. The former top executive is then rewarded with a golden handshake for presiding over the fire sale that can sometimes be in the hundreds of millions of dollars for one or two years of work. (This is nevertheless an excellent bargain for the takeover artist, who will tend to benefit from developing a reputation of being very generous to parting top executives). This is just one example of some of the principal–agent / perverse incentive issues involved with takeovers.
Similar issues occur when a publicly held asset or non-profit organization undergoes privatization. Top executives often reap tremendous monetary benefits when a government owned or non-profit entity is sold to private hands. Just as in the example above, they can facilitate this process by making the entity appear to be in financial crisis. This perception can reduce the sale price (to the profit of the purchaser) and make non-profits and governments more likely to sell. It can also contribute to a public perception that private entities are more efficiently run, reinforcing the political will to sell off public assets.
Pros and cons of takeover.
While pros and cons of a takeover differ from case to case, there are a few reoccurring ones worth mentioning.
Pros:
Cons:
Takeovers also tend to substitute debt for equity. In a sense, any government tax policy of allowing for deduction
of interest expenses but not of dividends, has essentially provided a substantial subsidy to takeovers.
It can punish more-conservative or prudent management that do not allow their companies to leverage themselves
into a high-risk position. High leverage will lead to high profits if circumstances go well, but can lead
to catastrophic failure if circumstances do not go favorably. This can create substantial negative externalities
for governments, employees, suppliers and other stakeholders.
Occurrence.
Corporate takeovers occur frequently in the United States, Canada, United Kingdom, France and Spain. They happen only occasionally in Italy because larger shareholders (typically controlling families) often have special board voting privileges designed to keep them in control. They do not happen often in Germany because of the dual board structure, nor in Japan because companies have interlocking sets of ownerships known as keiretsu, nor in the People's Republic of China because the state owned majority owns most publicly listed companies.
Tactics against hostile takeover.
There are quite a few tactics or techniques which can be used to deter a hostile takeover.

</doc>
<doc id="56120" url="https://en.wikipedia.org/wiki?curid=56120" title="Hispanic">
Hispanic

The term Hispanic (, , , , "hispàno") broadly refers to the people, nations, and cultures that have a historical link to Spain. It commonly applies to countries once colonized by Spain, particularly the countries of Latin America, and the Philippines. It could be argued that the term should apply to all Spanish-speaking cultures or countries, as the historical roots of the word specifically pertain to the Iberian region. It is difficult to label a nation or culture with one term, such as "Hispanic," as the ethnicities, customs, traditions, and art forms (music, literature, dress, architecture, cuisine, and others) vary greatly by country and region. The Spanish language and Spanish culture are the main traditions.
"Hispanic" originally referred to the people of ancient Roman Hispania, which roughly comprised the Iberian Peninsula including the contemporary states of Spain, Portugal, Andorra, and the British Overseas Territory of Gibraltar.
Terminology.
The term "Hispanic" derives from "Hispanicus" (which derived from "Hispania"), "Hispania" may in turn derive from Latin "Hispanicus", or from Greek Ισπανία "Hispania" and Ισπανός "Hispanos", probably from Celtiberian or from Basque "Ezpanna". In English the word is attested from the 16th century (and in late 19th century in American English).
The words "Spain", "Spanish", and "Spaniard" are of the same etymology as "Hispanus", ultimately.
"Hispanus" was the Latin name given to a person from Hispania during Roman rule. In English, the term "Hispano-Roman" is sometimes used. The Hispano-Romans were composed of people from many different tribes. Some famous "Hispani" (plural of "Hispanus") were Marcus Annaeus Lucanus, Martial, Prudentius, Theodosius I, and Magnus Maximus and Maximus of Hispania.
Here follows a comparison of several terms related to "Hispanic":
"Hispania" was the Roman name for the whole territory of the Iberian Peninsula. Initially, this territory was divided into two provinces: Hispania Citerior and Hispania Ulterior. In 27 B.C, Hispania Ulterior was divided into two new provinces, Hispania Baetica and Hispania Lusitania, while Hispania Citerior was renamed Hispania Tarraconensis. This division of Hispania explains the usage of the singular and plural forms (Spain, and The Spains) used to refer to the peninsula and its kingdoms in the Middle Ages.
Before the marriage of Queen Isabella I of Castile and King Ferdinand II of Aragon in 1469, the four Christian kingdoms of the Iberian Peninsula—the Kingdom of Portugal, the Crown of Aragon, the Crown of Castile, and the Kingdom of Navarre—were collectively called The Spains. This revival of the old Roman concept in the Middle Ages appears to have originated in Provençal, and was first documented at the end of the 11th century. In the Council of Constance, the four kingdoms shared one vote.
The word "Lusitanian", relates to Lusitania or Portugal, also in reference to the Lusitanians, possibly one of the first Indo-European tribes to settle in Europe. From this tribe's name had derived the name of the Roman province of Lusitania, and "Lusitania" remains the name of Portugal in Latin.
The terms "Spain" and "the Spains" were not interchangeable. Spain was a geographic territory, home to several kingdoms (Christian and Muslim), with separate governments, laws, languages, religions, and customs, and was the historical remnant of the Hispano-Gothic unity. Spain was not a political entity until much later, and when referring to the Middle Ages, one should not be confounded with the nation-state of today. The term "The Spains" referred specifically to a collective of juridico-political units, first the Christian kingdoms, and then the different kingdoms ruled by the same king.
With the "Decretos de Nueva Planta", Philip V started to organize the fusion of his kingdoms that until then were ruled as distinct and independent, but this unification process lacked a formal and juridic proclamation.
Although colloquially and literally the expression "King of Spain" or "King of the Spains" was already widespread, it did not refer to a unified nation-state. It was only in the constitution of 1812 that was adopted the name "Españas" (Spains) for the Spanish nation and the use of the title of "king of the Spains". The constitution of 1876 adopts for the first time the name "Spain" for the Spanish nation and from then on the kings would use the title of "king of Spain".
The expansion of the Spanish Empire between 1492 and 1898 brought thousands of Spanish migrants to the conquered lands, who established settlements, mainly in the Americas, but also in other distant parts of the world (as in the Philippines, the lone Spanish territory in Asia), producing a number of multiracial populations. Today, the term "Hispanic" is typically applied to the varied populations of these places, including those with little or no Spanish ancestry.
Definitions in ancient Rome.
The Latin gentile adjectives that belong to Hispania are "Hispanus, Hispanicus," and "Hispanienses." A Hispanus is someone who is a native of Hispania with no foreign parents, while children born in Hispania of (Latin) Roman parents were Hispaniensis. "Hispaniensis" means 'connected in some way to Hispania', as in "Exercitus Hispaniensis" ('the Spanish army') or "mercatores Hispanienses" ('Spanish merchants'). "Hispanicus" implies 'of' or 'belonging to' Hispania or the Hispanus or of their fashion as in "glaudius Hispanicus". The gentile adjectives were not ethnolinguistic but derived primarily on a geographic basis, from the toponym Hispania as the people of Hispania spoke different languages, although Livy said they could all understand each other, not making clear if they spoke dialects of the same language or were polyglots.
The first recorded use of an anthroponym derived from the toponym Hispania is attested in one of the five fragments, of Ennius in 236 B.C. who wrote "Hispane, non Romane memoretis loqui me" ("Remember that I speak like a Spaniard not a Roman") as having been said by a native of Hispania.
Definitions in Portugal and Spain.
The term Hispanic signifies the cultural resonance, among other elements and characteristics, of the descendants of the people who inhabited ancient Hispania (Iberian Peninsula). It has been used throughout history for many purposes, including drawing a contrast to the Moors and differentiating explorers and settlers.
Technically speaking, persons from Portugal or of Portuguese extraction are referred to as Lusitanians or "Lusófonos" (Lusophone). In Portugal, Hispanic refers to something related to ancient Hispania, Spain or the Spanish language and culture, not Portugal. Portugal and Spain do not have exactly the same definition for the term Hispanic, but they do share the etymology for the word (pt: "hispânico", es: "hispánico").
The Royal Spanish Academy (Spanish: Real Academia Española, RAE), the official royal institution responsible for regulating the Spanish language defines the term "Hispano" (which means "Hispanic" in Spanish) as:
The correct modern term to identify Portuguese and Spanish cultures under a single nomenclature is "Iberian", and the one to refer to cultures derived from both countries in the Americas is "Iberian-American". These designations can be mutually recognized by people in Portugal and Brazil, unlike "Hispanic", which is totally void of any self-identification in those countries, and quite on the opposite, serves the purpose of marking a clear distinction in relation to neighboring countries´ culture.
In Spanish, the term "hispano" as in "hispanoamericano", refers to the people of Spanish origin who live in the Americas; it also refers to a relationship to Hispania or to the Spanish language. There are people in Hispanic America that are not of Spanish origin, as the original people of these areas are Amerindians.
Definitions in the United States.
The term "Hispanic" was first used in the United States to define the growing number of the American population with Spanish ancestry. The term does not include immigrants from Spanish speaking countries however.
Today, organizations in the United States use the term as a broad catchall to refer to persons with a historical and cultural relationship with Spain, regardless of race and ethnicity. The U.S. Census Bureau defines the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race" and states that Hispanics or Latinos can be of any race, any ancestry, any ethnicity. Generically, this limits the definition of Hispanic or Latino to people from the Caribbean, Central and South America, or other Hispanic (Spanish or Portuguese) culture or origin, regardless of race. Latino can refer to males or females, while Latina refers to only females.
Because of the technical distinctions involved in defining "race" vs. "ethnicity," there is confusion among the general population about the designation of Hispanic identity. Currently, the United States Census Bureau defines six race categories:
According to census reports, of the above races the largest number of Hispanic or Latinos are of the White race, the second largest number come from the Native American/American Indian race who are the indigenous people of the Americas. The inhabitants of Easter Island are Pacific Islanders and since the island belongs to Chile they are theoretically Hispanic or Latinos.
Because Hispanic roots are considered aligned with a European ancestry (Spain/Portugal), Hispanic/Latino ancestry is defined solely as an "ethnic "designation (similar to being Norse or Germanic). Therefore, a person of Hispanic descent is typically defined using both race and ethnicity as an identifier—i.e., Black-Hispanic, White-Hispanic, Asian-Hispanic, Amerindian-Hispanic or "other race" Hispanic.
Officially, however, the U.S. Government has defined "Hispanic or Latino" persons as being "persons who trace their origin or descent to Mexico, Puerto Rico, Cuba, Central and South America, and other Spanish cultures". This includes Spain and Portugal which is the origin of Hispanic/Iberian culture. The United States Census uses the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Hispanic culture or origin regardless of race."
The U.S. Office of Management and Budget currently defines "Hispanic or Latino" as "a person of Mexican, Puerto Rican, Cuban, South or Central American, or other Spanish culture or origin, regardless of race". The 2010 Census asked if the person was "Spanish/Hispanic/Latino". The United States Census uses the ethnonym "Hispanic or Latino" to refer to "a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture or origin regardless of race." The Census Bureau also explains that "rigin can be viewed as the heritage, nationality group, lineage, or country of birth of the person or the person's ancestors before their arrival in the United States. People who identify their origin as Hispanic, Latino, or Spanish may be of any race."
The U.S. Department of Transportation defines "Hispanic" as, "persons of Mexican, Puerto Rican, Cuban, Dominican, Central or South American, or other Spanish or Portuguese culture or origin, regardless of race." This definition has been adopted by the Small Business Administration as well as by many federal, state, and municipal agencies for the purposes of awarding government contracts to minority owned businesses. 
The Congressional Hispanic Caucus and the Congressional Hispanic Conference include representatives of Spanish and Portuguese, Puerto Rican and Mexican descent. The Hispanic Society of America is dedicated to the study of the arts and cultures of Spain, Portugal, and Latin America. The Hispanic Association of Colleges and Universities, proclaimed champions of Hispanic success in higher education, is committed to Hispanic educational success in the U.S., Puerto Rico, Ibero-America, Spain and Portugal.
The U.S. Equal Employment Opportunity Commission encourages any individual who believes that he or she is Hispanic to self-identify as Hispanic. The United States Department of Labor - Office of Federal Contract Compliance Programs encourages the same self-identification. As a result, any individual who traces his or her origins to part of the Spanish Empire or Portuguese Empire may self-identify as Hispanic, because an employer may not override an individual's self-identification.
The 1970 Census was the first time that a "Hispanic" identifier was used and data collected with the question. The definition of "Hispanic" has been modified in each successive census.
In a recent study, most Spanish-speakers of Spanish or Hispanic American descent do not prefer the term "Hispanic" or "Latino" when it comes to describing their identity. Instead, they prefer to be identified by their country of origin. When asked if they have a preference for either being identified as "Hispanic" or "Latino," the Pew study finds that "half (51%) say they have no preference for either term." A majority (51%) say they most often identify themselves by their family’s country of origin, while 24% say they prefer a pan-ethnic label such as Hispanic or Latino. Among those 24% who have a preference for a pan-ethnic label, "'Hispanic' is preferred over 'Latino' by more than a two-to-one margin—33% versus 14%." Twenty-one percent prefer to be referred to simply as "Americans."
Hispanicization.
Hispanicization is the process by which a place or a person absorbs characteristics of Hispanic society and culture. Modern hispanization of a place, namely in the United States, might be illustrated by Spanish-language media and businesses. Hispanization of a person might be illustrated by speaking Spanish, making and eating Hispanic American food, listening to Spanish language music or participating in Hispanic festivals and holidays - Hispanization of those outside the Hispanic community as opposed to assimilation of Hispanics into theirs.
One reason that some people believe the assimilation of Hispanics in the U.S. is not comparable to that of other cultural groups is that Hispanic and Latino Americans have been living in parts of North America for centuries, in many cases well before the English-speaking culture became dominant. For example, California, Texas, Colorado, New Mexico (1598), Arizona, Nevada, Florida and Puerto Rico have been home to Spanish-speaking peoples since the 16th century, long before the U.S. existed. (But it should be noted that the language of the Native Americans existed before this, until the invasion and forced assimilation by the Spanish.) These and other Spanish-speaking territories were part of the Viceroyalty of New Spain, and later Mexico (with the exception of Florida and Puerto Rico), before these regions joined or were taken over by the United States in 1848. Some cities in the U.S. were founded by Spanish settlers as early as the 16th century, prior to the creation of the Thirteen Colonies. For example, San Miguel de Gualdape, Pensacola and St. Augustine, Florida were founded in 1526, 1559 and 1565 respectively. Santa Fe, New Mexico was founded in 1604, and Albuquerque was established in 1660. El Paso was founded in 1659, San Antonio in 1691, Laredo, Texas in 1755, San Diego in 1769, San Francisco in 1776, San Jose, California in 1777, New Iberia, Louisiana in 1779, and Los Angeles in 1781. Therefore, in many parts of the U.S., the Hispanic cultural legacy predates English/British influence. For this reason, many generations have largely maintained their cultural traditions and Spanish language well before the United States was created. However, Spanish-speaking persons in many Hispanic areas in the U.S. amounted to only a few thousand people when they became part of the United States; a large majority of current Hispanic residents are descended from Hispanics who entered the United States in the mid-to-late 20th and early 21st centuries.
Language retention is a common index to assimilation; according to the 2000 census, about 75 percent of all Hispanics spoke Spanish in the home. Spanish language retention rates vary geographically; parts of Texas and New Mexico have language retention rates over 90 percent, whereas in parts of Colorado and California, retention rates are lower than 30 percent. The degree of retention of Spanish as the native language is based on recent arrival from countries where Spanish is spoken. As is true of other immigrants, those who were born in other countries still speak their native language. Later generations are increasingly less likely to speak the language spoken in the country of their ancestors, as is true of other immigrant groups.
Spanish-speaking countries and regions.
Today, Spanish is among the most commonly spoken first languages of the world. During the period of the Spanish Empire from 1492 and 1898, many people migrated from Spain to the conquered lands. The Spaniards brought with them the Castilian language and culture, and in this process that lasted several centuries, created a global empire with a diverse population.
Miscegenation between peoples in the colonies led to the creation of the new mixed peoples, chiefly half-caste and mulattos, in many countries.
Culturally, Spaniards are typically European, but they also have small traces of many peoples from the rest of Europe, the Near East and the Mediterranean areas of northern Africa.
The Hispanic countries, including Spain, are also inhabited by peoples of non-Spanish ancestry, to widely varying extents.
Culture.
The Miguel de Cervantes Prize is awarded to Hispanic writers, whereas the Latin Grammy Award recognizes Hispanic and Portuguese musicians, and the Platino Awards as given to outstanding Hispanic films.
Music.
Folk and popular dance and music also varies greatly among Hispanics. For instance, the music from Spain is a lot different from the Hispanic American, although there is a high grade of exchange between both continents. In addition, due to the high national development of the diverse nationalities and regions of Spain, there is a lot of music in the different languages of the Peninsula (Catalan, Galician and Basque, mainly). See, for instance, Music of Catalonia or Rock català, Music of Galicia, Cantabria and Asturias, and Basque music. Flamenco is also a very popular music style in Spain, especially in Andalusia. Spanish ballads "romances" can be traced in Mexico as "corridos" or in Argentina as "milongas", same structure but different scenarios.
On the other side of the ocean, Hispanic America is also home to a wide variety of music, even though "Latin" music is often erroneously thought of, as a single genre. Hispanic Caribbean music tends to favor complex polyrhythms of African origin. Mexican music shows combined influences of mostly Spanish and Native American origin, while traditional Northern Mexican music — norteño and banda — is more influenced by country-and-western music and the polka, brought by Central European settlers to Mexico. The music of Hispanic Americans — such as tejano music — has influences in rock, jazz, R&B, pop, and country music as well as traditional Mexican music such as Mariachi. Meanwhile, native Andean sounds and melodies are the backbone of Peruvian and Bolivian music, but also play a significant role in the popular music of most South American countries and are heavily incorporated into the folk music of Ecuador and Chile and the tunes of Colombia, and again in Chile where they play a fundamental role in the form of the greatly followed nueva canción. In U.S. communities of immigrants from these countries it is common to hear these styles. Latin pop, Rock en Español, Latin hip-hop, Salsa, Merengue, colombian cumbia and Reggaeton styles tend to appeal to the broader Hispanic population, and varieties of Cuban music are popular with many Hispanics of all backgrounds.
Literature.
Spanish-language literature and folklore is very rich and is influenced by a variety of countries. There are thousands of writers from many places, and dating from the Middle Ages to the present. Some of the most recognized writers are Miguel de Cervantes Saavedra (Spain), Lope de Vega (Spain), Calderón de la Barca (Spain), Jose Rizal (Philippines), Carlos Fuentes (Mexico), Octavio Paz (Mexico), Miguel Ángel Asturias (Guatemala), George Santayana (US), José Martí (Cuba), Sabine Ulibarri (US), Federico García Lorca (Spain), Miguel de Unamuno (Spain), Gabriel García Márquez (Colombia), Rafael Pombo (Colombia), Horacio Quiroga (Uruguay), Rómulo Gallegos (Venezuela), Luis Rodriguez Varela (Philippines), Rubén Darío (Nicaragua), Mario Vargas Llosa (Peru), Giannina Braschi (Puerto Rico), Cristina Peri Rossi (Uruguay), Luisa Valenzuela (Argentina), Roberto Quesada (Honduras), Julio Cortázar (Argentina), Pablo Neruda (Chile), Gabriela Mistral (Chile), Jorge Luis Borges (Argentina), Pedro Henríquez Ureña (Dominican Republic), Ernesto Sabato (Argentina), Juan Tomás Ávila Laurel (Equatorial Guinea), Ciro Alegría (Peru), Joaquin Garcia Monge (Costa Rica), and Jesus Balmori (Philippines).
Sports.
In the majority of the Hispanic countries, association football is the most popular sport. The men's national teams of Argentine, Uruguay and Spain have won the FIFA World Cup a total five times. The Spanish La Liga is one of the most popular in the world, known for FC Barcelona and Real Madrid. Meanwhile, the Argentine Primera División and Mexican Primera División are two of the strongest leagues in the Americas.
However, baseball is the most popular sport in some Central American and Caribbean countries (especially Cuba, Dominican Republic, Puerto Rico and Venezuela), as well as in the diaspora in the United States. Notable Hispanic teams in early baseball are the All Cubans, Cuban Stars and New York Cubans. The Hispanic Heritage Baseball Museum recognizes Hispanic baseball personalities. Nearly 30 percent (22 percent foreign-born Latinos) of MLB players today have Hispanic heritage.
Several Hispanic sportspeople have been successful worldwide, such as Diego Maradona, Alfredo di Stefano, Lionel Messi, Diego Forlán (association football), Juan Manuel Fangio, Juan Pablo Montoya, Eliseo Salazar, Fernando Alonso, Marc Gené, Carlos Sainz (auto racing), Ángel Nieto, Dani Pedrosa, Jorge Lorenzo, Marc Márquez, Marc Coma, Nani Roma (motorcycle racing), Emanuel Ginóbili, Pau Gasol, Marc Gasol (basketball), Julio César Chávez, Saúl Álvarez, Carlos Monzón (boxing), Miguel Indurain, Alberto Contador, Santiago Botero, Rigoberto Urán, Nairo Quintana (cycling), Roberto de Vicenzo, Ángel Cabrera, Sergio García, Severiano Ballesteros, José María Olazábal (golf), Luciana Aymar (field hockey), Rafael Nadal, Marcelo Ríos, Guillermo Vilas, Gabriela Sabatini, Juan Martín del Potro (tennis).
Notable Hispanic sports television networks are ESPN Latin America, Fox Sports Latin America and TyC Sports.
Religion.
With regard to religious affiliation among Spanish-speakers, Christianity — specifically Roman Catholicism — is usually the first religious tradition that comes to mind . The Spaniards and the Portuguese took the Roman Catholic faith to Ibero-America and the Philippines, and Roman Catholicism remains the predominant religion amongst most Hispanics. A small but growing number of Hispanics belong to a Protestant denomination.
There are also Spanish-speaking Jews, most of whom are the descendants of Ashkenazi Jews who migrated from Europe (German Jews, Russian Jews, Polish Jews, etc.) to Hispanic America, particularly Argentina, Uruguay, Peru and Cuba (Argentina is host to the third largest Jewish population in the Western Hemisphere, after the United States and Canada) in the 19th century and following World War II. Many Spanish-speaking Jews also originate from the small communities of reconverted descendants of anusim — those whose Spanish Sephardi Jewish ancestors long ago hid their Jewish ancestry and beliefs in fear of persecution by the Spanish Inquisition in the Iberian Peninsula and Ibero-America. The Spanish Inquisition led to a large number of forced conversions of Spanish Jews.
Genetic studies on the (male) Y-chromosome conducted by the University of Leeds in 2008 appear to support the idea that the number of forced conversions have been previously underestimated significantly. They found that twenty percent of Spanish males have Y-chromosomes associated with Sephardic Jewish ancestry. This may imply that there were more forced conversions than was previously thought.
There are also thought to be many Catholic-professing descendants of marranos and Spanish-speaking crypto-Jews in the Southwestern United States and scattered through Hispanic America. Additionally, there are Sephardic Jews who are descendants of those Jews who fled Spain to Turkey, Syria, and North Africa, some of whom have now migrated to Hispanic America, holding on to some Spanish/Sephardic customs, such as the Ladino language, which mixes Spanish, Hebrew, Arabic and others, though written with Hebrew and Latin characters. Though, it should be noted, that Ladinos were also African slaves captive in Spain held prior to the colonial period in the Americas. (See also History of the Jews in Hispanic America and List of Hispanic American Jews.)
Among the Spanish-speaking Catholics, most communities celebrate their homeland's patron saint, dedicating a day for this purpose with festivals and religious services. Some Spanish-speakers syncretize Roman Catholicism and African or Native American rituals and beliefs. Such is the case of Santería, popular with Afro-Cubans, which combines old African beliefs in the form of Roman Catholic saints and rituals. Other syncretistic beliefs include Spiritism and Curanderismo.
While a tiny minority, there are some Muslims in Latin America, in the US, and in the Philippines. Those in the Philippines live predominantly in the Autonomous Region in Muslim Mindanao province.
In the United States, some 65% of Hispanics and Latinos report themselves Catholic and 21% Protestant, with 13% having no affiliation. A minority among the Roman Catholics, about one in five, are charismatics. Among the Protestant, 85% are "Born-again Christians" and belong to Evangelical or Pentecostal churches. Among the smallest groups, less than 4%, are Jewish.
Cultural heritage according to UNESCO.
The Hispanic world, according to the United Nations World Heritage Committee, has contributed substantially more than any other ethnicity to the cultural heritage of the world. A World Heritage Cultural Site is a place such as a building, city, complex, or monument that is listed by the United Nations Educational, Scientific and Cultural Organization (UNESCO) as being of special cultural significance. Of a total of 802 Cultural World Heritage Sites recognized by the United Nations as of July 2015, 114 are located in Hispanic countries. Spain alone has 39 cultural sites, only second in the world to Italy.

</doc>
<doc id="56121" url="https://en.wikipedia.org/wiki?curid=56121" title="Market liquidity">
Market liquidity

In business, economics or investment, market liquidity is a market's ability to purchase or sell an asset without causing drastic change in the asset's price. Equivalently, an asset's market liquidity (or simply "an asset's liquidity") describes the asset's ability to sell quickly without having to reduce its price to a significant degree. Liquidity is about how big the trade-off is between the speed of the sale and the price it can be sold for. In a liquid market, the trade-off is mild: selling quickly will not reduce the price much. In a relatively illiquid market, selling it quickly will require cutting its price by some amount.
Money, or cash, is the most liquid asset, because it can be "sold" for goods and services instantly with no loss of value. There is no wait for a suitable buyer of the cash. There is no trade-off between speed and value. It can be used immediately to perform economic actions like buying, selling, or paying debt, meeting immediate wants and needs.
If an asset is moderately (or very) liquid, it has moderate (or high) liquidity. In an alternative definition, liquidity can mean the amount of highly liquid assets. If a business has moderate liquidity, it has a moderate amount of very liquid assets. If a business has sufficient liquidity, it has a sufficient amount of very liquid assets and the ability to meet its payment obligations.
An act of exchanging a less liquid asset for a more liquid asset is called liquidation. Often liquidation is trading the less liquid asset for cash, also known as selling it. An asset's liquidity can change. For the same asset, its liquidity can change through time or between different markets, such as in different countries. The change in the asset's liquidity is just based on the market liquidity for the asset at the particular time or in the particular country, etc. The liquidity of a product can be measured as how often it is bought and sold.
Liquidity is defined formally in many accounting regimes and has in recent years been more strictly defined. For instance, the US Federal Reserve intends to apply quantitative liquidity requirements based on Basel III liquidity rules as of fiscal 2012. Bank directors will also be required to know of, and approve, major liquidity risks personally. Other rules require diversifying counterparty risk and portfolio stress testing against extreme scenarios, which tend to identify unusual market liquidity conditions and avoid investments that are particularly vulnerable to sudden liquidity shifts.
Overview.
A liquid asset has some or all of the following features: It can be sold rapidly, with minimal loss of value, any time within market hours. The essential characteristic of a liquid market is that there are always ready and willing buyers and sellers. It is similar to, but distinct from, market depth, which relates to the trade-off between quantity being sold and the price it can be sold for, rather than the liquidity trade-off between speed of sale and the price it can be sold for. A market may be considered both deep and liquid if there are ready and willing buyers and sellers in large quantities.
An illiquid asset is an asset which is not readily salable (without a drastic price reduction, and sometimes not at any price) due to uncertainty about its value or the lack of a market in which it is regularly traded. The mortgage-related assets which resulted in the subprime mortgage crisis are examples of illiquid assets, as their value was not readily determinable despite being secured by real property. Before the crisis, they had moderate liquidity because it was believed that their value was generally known.
Speculators and market makers are key contributors to the liquidity of a market, or asset. Speculators are individuals or institutions that seek to profit from anticipated increases or decreases in a particular market price. Market makers seek to profit by charging for immediacy of execution: either implicitly by earning a bid/ask spread or explicitly by charging execution commissions. By doing this, they provide the capital needed to facilitate the liquidity. The risk of illiquidity need not apply only to individual investments: whole portfolios are subject to market risk. Financial institutions and asset managers that oversee portfolios are subject to what is called "structural" and "contingent" liquidity risk. Structural liquidity risk, sometimes called funding liquidity risk, is the risk associated with funding asset portfolios in the normal course of business. Contingent liquidity risk is the risk associated with finding additional funds or replacing maturing liabilities under potential, future stressed market conditions. When a central bank tries to influence the liquidity (supply) of money, this process is known as open market operations.
Effect on asset values.
The market liquidity of assets affects their prices and expected returns. Theory and empirical evidence suggests that investors require higher return on assets with lower market liquidity to compensate them for the higher cost of trading these assets. That is, for an asset with given cash flow, the higher its market liquidity, the higher its price and the lower is its expected return. In addition, risk-averse investors require higher expected return if the asset’s market-liquidity risk is greater. This risk involves the exposure of the asset return to shocks in overall market liquidity, the exposure of the asset own liquidity to shocks in market liquidity and the effect of market return on the asset’s own liquidity. Here too, the higher the liquidity risk, the higher the expected return on the asset or the lower is its price.
One example of this is comparison of assets with and without a liquid secondary market. The liquidity discount is the reduced promised yield or expected return for such assets, like the difference between newly issued U.S. Treasury bonds compared to off the run treasuries with the same term to maturity. Initial buyers know that other investors are less willing to buy off-the-run treasuries, so the newly issued bonds have a higher price (and hence lower yield).
Futures.
In the futures markets, there is no assurance that a liquid market may exist for offsetting a commodity contract at all times. Some future contracts and specific delivery months tend to have increasingly more trading activity and have higher liquidity than others. The most useful indicators of liquidity for these contracts are the trading volume and open interest.
There is also dark liquidity, referring to transactions that occur off-exchange and are therefore not visible to investors until after the transaction is complete. It does not contribute to public price discovery.
Banking.
In banking, liquidity is the ability to meet obligations when they come due without incurring unacceptable losses. Managing liquidity is a daily process requiring bankers to monitor and project cash flows to ensure adequate liquidity is maintained. Maintaining a balance between short-term assets and short-term liabilities is critical. For an individual bank, clients' deposits are its primary liabilities (in the sense that the bank is meant to give back all client deposits on demand), whereas reserves and loans are its primary assets (in the sense that these loans are owed to the bank, not by the bank). The investment portfolio represents a smaller portion of assets, and serves as the primary source of liquidity. Investment securities can be liquidated to satisfy deposit withdrawals and increased loan demand. Banks have several additional options for generating liquidity, such as selling loans, borrowing from other banks, borrowing from a central bank, such as the US Federal Reserve bank, and raising additional capital. In a worst-case scenario, depositors may demand their funds when the bank is unable to generate adequate cash without incurring substantial financial losses. In severe cases, this may result in a bank run. Most banks are subject to legally mandated requirements intended to help avoid a liquidity crisis.
Banks can generally maintain as much liquidity as desired because bank deposits are insured by governments in most developed countries. A lack of liquidity can be remedied by raising deposit rates and effectively marketing deposit products. However, an important measure of a bank's value and success is the cost of liquidity. A bank can attract significant liquid funds. Lower costs generate stronger profits, more stability, and more confidence among depositors, investors, and regulators.
Stock market.
In the market, liquidity has a slightly different meaning, although still tied to how easily assets, in this case shares of stock, can be converted to cash. The market for a stock is said to be liquid if the shares can be rapidly sold and the act of selling has little impact on the stock's price. Generally, this translates to where the shares are traded and the level of interest that investors have in the company. Another way to judge liquidity in a company's stock is to look at the bid/ask spread. For liquid stocks, such as Microsoft or General Electric, the spread is often just a few pennies - much less than 1% of the price. For illiquid stocks, the spread can be much larger, amounting to a few percent of the trading price. In today's stock market, high-frequency trading firms purport of themselves, to contribute to nearly 50% of all liquidity.
Liquidity positively impacts the stock market. When stock prices rise, it is said to be due to a confluence of extraordinarily high levels of liquidity on household and business balance sheets, combined with a simultaneous normalization of liquidity preferences. On the margin, this drives a demand for equity investments.
Proxies.
One way to calculate the liquidity of the banking system of a country is to divide liquid assets to short term liabilities.

</doc>
<doc id="56122" url="https://en.wikipedia.org/wiki?curid=56122" title="Sorghum">
Sorghum

Sorghum is a genus of plants in the grass family. Seventeen of the twenty-five species are native to Australia, with some extending to Africa, Asia, Mesoamerica, and certain islands in the Indian and Pacific Oceans.
One species is grown for grain, while many others are used as fodder plants, either intentionally cultivated or allowed to grow naturally, in pasture lands. The plants are cultivated in warm climates worldwide and naturalized in many places. "Sorghum" is in the subfamily Panicoideae and the tribe Andropogoneae (the tribe of big bluestem and sugarcane).
Cultivation and uses.
One species, "Sorghum bicolor", native to Africa with many cultivated forms now, is an important crop worldwide, used for food (as grain and in sorghum syrup or "sorghum molasses"), animal fodder, the production of alcoholic beverages, and biofuels. Most varieties are drought- and heat-tolerant, and are especially important in arid regions, where the grain is one of the staples for poor and rural people. These varieties form important components of pastures in many tropical regions. "S. bicolor" is an important food crop in Africa, Central America, and South Asia, and is the "fifth-most important cereal crop grown in the world" according to the U.S. Grains Council.
Some species of sorghum can contain levels of hydrogen cyanide, hordenine, and nitrates lethal to grazing animals in the early stages of the plants' growth. When stressed by drought or heat, plants can also contain toxic levels of cyanide and/or nitrates at later stages in growth.
Another "Sorghum" species, Johnson grass ("S. halapense"), is classified as an invasive species in the US by the Department of Agriculture.
Nutrition.
In a 100 gram amount, raw sorghum provides 329 calories, 72% carbohydrates, 4% fat and 11% protein (table). Sorghum supplies numerous essential nutrients in rich content (20% or more of the Daily Value, DV), including protein, the B vitamins, niacin, thiamin and vitamin B6, and several dietary minerals, including iron (26% DV) and manganese (76% DV) (table). Sorghum nutrient contents generally are similar to those of raw oats (see nutrition table).
Diversity.
Many species once considered part of "Sorghum", but now considered better suited to other genera include: "Andropogon, Arthraxon, Bothriochloa, Chrysopogon, Cymbopogon, Danthoniopsis, Dichanthium, Diectomis, Diheteropogon, Exotheca, Hyparrhenia, Hyperthelia, Monocymbium, Parahyparrhenia, Pentameris, Pseudosorghum, Schizachyrium, "and" Sorghastrum".

</doc>
<doc id="56124" url="https://en.wikipedia.org/wiki?curid=56124" title="Lakota people">
Lakota people

The Lakȟóta people (pronounced ; also known as Teton, Thítȟuŋwaŋ ("prairie dwellers"), and Teton Sioux (from Nadouessioux - 'snake' or 'enemy') are an indigenous people of the Great Plains of North America. They are part of a confederation of seven related Sioux tribes, the Očhéthi Šakówiŋ or seven council fires, and speak Lakota, one of the three major dialects of the Sioux language.
The Lakota are the westernmost of the three Siouan language groups, occupying lands in both North and South Dakota. The seven bands or "sub-tribes" of the Lakota are:
Notable Lakota persons include Tȟatȟáŋka Íyotake (Sitting Bull) from the Húnkpapȟa band; Touch the Clouds from the Miniconjou band; and, Tȟašúŋke Witkó (Crazy Horse), Maȟpíya Lúta (Red Cloud), Heȟáka Sápa (Black Elk), Siŋté Glešká (Spotted Tail), and Billy Mills from the Oglala band.
History.
Siouan language speakers may have originated in the lower Mississippi River region and then migrated to or originated in the Ohio Valley. They were agriculturalists and may have been part of the Mound Builder civilization during the 9th–12th centuries CE. In the late 16th and early 17th centuries, Dakota-Lakota-Nakota speakers lived in the upper Mississippi Region in present-day Minnesota, Wisconsin, Iowa, and the Dakotas. Conflicts with Anishnaabe and Cree peoples pushed the Lakota west onto the Great Plains in the mid- to late-17th century.
Early Lakota history is recorded in their Winter counts (Lakota: "waníyetu wówapi"), pictorial calendars painted on hides or later recorded on paper. The Battiste Good winter count records Lakota history back to 900 CE, when White Buffalo Calf Woman gave the Lakota people the White Buffalo Calf Pipe.
Around 1730, Cheyenne people introduced the Lakota to horses, called "šuŋkawakaŋ" ("dog power/mystery/wonder"). After their adoption of horse culture, Lakota society centered on the buffalo hunt on horseback. The total population of the Sioux (Lakota, Santee, Yankton, and Yanktonai) was estimated at 28,000 by French explorers in 1660. The Lakota population was first estimated at 8,500 in 1805, growing steadily and reaching 16,110 in 1881. The Lakota were, thus, one of the few Native American tribes to increase in population in the 19th century. The number of Lakota has now increased to about 70,000, of whom about 20,500 still speak the Lakota language.
After 1720, the Lakota branch of the Seven Council Fires split into two major sects, the Saône who moved to the Lake Traverse area on the South Dakota–North Dakota–Minnesota border, and the Oglála-Sičháŋǧu who occupied the James River valley. However, by about 1750 the Saône had moved to the east bank of the Missouri River, followed 10 years later by the Oglála and Brulé (Sičháŋǧu).
The large and powerful Arikara, Mandan, and Hidatsa villages had long prevented the Lakota from crossing the Missouri. However, the great smallpox epidemic of 1772–1780 destroyed three-quarters of these tribes. The Lakota crossed the river into the drier, short-grass prairies of the High Plains. These newcomers were the Saône, well-mounted and increasingly confident, who spread out quickly. In 1765, a Saône exploring and raiding party led by Chief Standing Bear discovered the Black Hills (the "Paha Sapa"), then the territory of the Cheyenne. Ten years later, the Oglála and Brulé also crossed the river. In 1776, the Lakota defeated the Cheyenne, who had earlier taken the region from the Kiowa. The Cheyenne then moved west to the Powder River country, and the Lakota made the Black Hills their home.
The United States government did not enforce the treaty restriction against unauthorized settlement. Lakota and other bands attacked settlers and even emigrant trains, causing public pressure on the U.S. Army to punish the hostiles. On September 3, 1855, 700 soldiers under American General William S. Harney avenged the Grattan Massacre by attacking a Lakota village in Nebraska, killing about 100 men, women, and children. A series of short "wars" followed, and in 1862–1864, as refugees from the "Dakota War of 1862" in Minnesota fled west to their allies in Montana and Dakota Territory. Increasing illegal settlement after the American Civil War caused war once again.
The Black Hills were considered sacred by the Lakota, and they objected to mining. Between 1866 and 1868 the U.S. Army fought the Lakota and their allies along the Bozeman Trail over U.S. Forts built to protect miners traveling along the trail. Oglala Chief Red Cloud led his people to victory in Red Cloud's War. In 1868, the United States signed the Fort Laramie Treaty of 1868, exempting the Black Hills from all white settlement forever. Four years later gold was discovered there, and prospectors descended on the area.
The attacks on settlers and miners were met by military force conducted by army commanders such as Lieutenant Colonel George Armstrong Custer. General Philip Sheridan encouraged his troops to hunt and kill the buffalo as a means of "destroying the Indians' commissary."
The allied Lakota and Arapaho bands and the unified Northern Cheyenne were involved in much of the warfare after 1860. They fought a successful delaying action against General George Crook's army at the Battle of the Rosebud, preventing Crook from locating and attacking their camp, and a week later defeated the U.S. 7th Cavalry in 1876 at the Battle of the Greasy Grass. Custer attacked a camp of several tribes, much larger than he realized. Their combined forces, led by Chief Crazy Horse killed 258 soldiers, wiping out the entire Custer battalion in the Battle of the Little Bighorn, and inflicting more than 50% casualties on the regiment.
Their victory over the U.S. Army would not last, however. The U.S. Congress authorized funds to expand the army by 2,500 men. The reinforced US Army defeated the Lakota bands in a series of battles, finally ending the Great Sioux War in 1877. The Lakota were eventually confined onto reservations, prevented from hunting buffalo and forced to accept government food distribution.
In 1877, some of the Lakota bands signed a treaty that ceded the Black Hills to the United States; however, the nature of this treaty and its passage were controversial. The number of Lakota leaders that actually backed the treaty is highly disputed. Low-intensity conflicts continued in the Black Hills. Fourteen years later, Sitting Bull was killed at Standing Rock reservation on December 15, 1890. The U.S. Army attacked Spotted Elk (aka Bigfoot), Mnicoujou band of Lakota at the Wounded Knee Massacre on December 29, 1890, at Pine Ridge.
Today, the Lakota are found mostly in the five reservations of western South Dakota: Rosebud Indian Reservation (home of the Upper Sičhánǧu or Brulé), Pine Ridge Indian Reservation (home of the Oglála), Lower Brule Indian Reservation (home of the Lower Sičhaŋǧu), Cheyenne River Indian Reservation (home of several other of the seven Lakota bands, including the Mnikȟówožu, Itázipčho, Sihásapa and Oóhenumpa), and Standing Rock Indian Reservation (home of the Húŋkpapȟa), also home to people from many bands. Lakota also live on the Fort Peck Indian Reservation in northeastern Montana, the Fort Berthold Indian Reservation of northwestern North Dakota, and several small reserves in Saskatchewan and Manitoba. Their ancestors fled to "Grandmother's Queen Victoria's Land" (Canada) during the Minnesota or Black Hills War.
Large numbers of Lakota live in Rapid City and other towns in the Black Hills, and in metro Denver. Lakota elders joined the Unrepresented Nations and Peoples Organization (UNPO) to seek protection and recognition for their cultural and land rights.
Government.
United States.
Legally and by treaty a semi-autonomous "nation" within the United States, the Lakota Sioux are represented locally by officials elected to councils for the several reservations and communities in the Dakotas, Minnesota, Nebraska. They are represented on the state and national level by the elected officials from the political districts of their respective states and Congressional Districts. Band or reservation members living both on and off the individual reservations are eligible to vote in periodic elections for that reservation. Each reservation has a unique local government style and election cycle based on its own constitution or articles of incorporation. Most follow a multi-member tribal council model with a chairman or president elected directly by the voters.
Tribal governments have significant leeway, as semi-autonomous political entities, in deviating from state law (e.g. Indian gaming.) They are ultimately subject to supervisory oversight by the United States Congress and executive regulation through the Bureau of Indian Affairs. The nature and legitimacy of those relationships continue to be a matter of dispute.
Canada.
There are nine bands of Dakota and Lakota in Manitoba and southern Saskatchewan, with a total of 6,000 registered members. They are recognized as First Nations but are not considered "treaty Indians". As First Nations they receive rights and entitlements through the Indian and Northern Affairs Canada department. However, as they are not recognized as treaty Indians, they did not participate in the land settlement and natural resource revenues. The Dakota rejected a $60 million land rights settlement in 2008.
Independence movement.
Beginning in 1974, some Lakota activists have taken steps to become independent from the United States, in an attempt to form their own fully independent nation. These steps have included drafting their own "declaration of continuing independence" and using Constitutional and International Law to solidify their legal standing.
A 1980 U.S. Supreme Court decision awarded $122 million to eight bands of Sioux Indians as compensation for land claims, but the court did not award land. The Lakota have refused the settlement.
In September 2007, the United Nations passed a non-binding Resolution on the Rights of Indigenous Peoples. Canada, the United States, Australia and New Zealand refused to sign.
On December 20, 2007, a group of Lakota under the name Lakota Freedom Delegation traveled to Washington D.C. to announce a withdrawal of the Lakota Sioux from all treaties with the United States government. These activists had no standing under any elected BIA tribal government. The group claimed official standing under the traditional Lakota Treaty Councils, representing the traditional "Tiyóšpaye" (matriarchal family units). These have been the traditional form of Lakota governance.
Longtime political activist Russell Means said, "We have 33 treaties with the United States that they have not lived by." He was part of the delegation's declaring the Lakota a sovereign nation with property rights over thousands of square miles in South Dakota, North Dakota, Nebraska, Wyoming and Montana. The group stated that they do not act for or represent the tribal governments set up by the BIA or those Lakota who support the BIA system of government.
The Lakota Freedom Delegation did not include any elected leaders from any of the tribes. Russell Means had previously run for president of the Oglala Sioux tribe and twice been defeated. Several elected BIA tribal governments issued statements distancing themselves from the independence declaration, with some saying they were watching the independent movement closely. Although some Indigenous nations and groups around the world made statements in support, no elected Lakota tribal governments endorsed the declaration.
In January 2008, the Lakota Freedom Delegation split into two groups. One group was led by "Canupa Gluha Mani" (Duane Martin Sr.). He is a leader of "Cante Tenza", the traditional Strongheart Warrior Society, that has included leaders such as Sitting Bull and Crazy Horse. This group is called "Lakota Oyate". The other group is called the "Republic of Lakotah" and is led by Russell Means. In December 2008, Lakota Oyate received the support and standing of the traditional treaty council of the Oglala Tiospayes.
Current activism.
The Lakota People made national news when NPR's "Lost Children, Shattered Families investigative story aired. It exposed what many critics consider to be the "kidnapping" of Lakota children from their homes by the state of South Dakota's Department of Social Services (D.S.S.). Lakota activists such as Madonna Thunder Hawk and Chase Iron Eyes, along with the Lakota People’s Law Project, have alleged that Lakota grandmothers are illegally denied the right to foster their own grandchildren. They are currently working to redirect federal funding away from the state of South Dakota's D.S.S. to new tribal foster care programs. This would be an historic shift away from the state's traditional control over Lakota foster children.
Ethnonyms.
The name "Lakota" comes from the Lakota autonym, "Lakota" "feeling affection, friendly, united, allied". The early French historic documents did not distinguish a separate Teton division, instead grouping them with other "Sioux of the West," Santee and Yankton bands.
The names "Teton" and "Tetuwan" come from the Lakota name "thítȟuŋwaŋ", the meaning of which is obscure. This term was used to refer to the Lakota by non-Lakota Sioux groups. Other derivations include: ti tanka, Tintonyanyan, Titon, Tintonha, Thintohas, Tinthenha, Tinton, Thuntotas, Tintones, Tintoner, Tintinhos, Ten-ton-ha, Thinthonha, Tinthonha, Tentouha, Tintonwans, Tindaw, Tinthow, Atintons, Anthontans, Atentons, Atintans, Atrutons, Titoba, Tetongues, Teton Sioux, Teeton, Ti toan, Teetwawn, Teetwans, Ti-t’-wawn, Ti-twans, Tit’wan, Tetans, Tieton, and Teetonwan.
Early French sources call the Lakota "Sioux" with an additional modifier, such as Sioux of the West, West Schious, Sioux des prairies, Sioux occidentaux, Sioux of the Meadows, Nadooessis of the Plains, Prairie Indians, Sioux of the Plain, Maskoutens-Nadouessians, Mascouteins Nadouessi, and Sioux nomades.
Today many of the tribes continue to officially call themselves "Sioux". In the 19th and 20th centuries, this was the name which the US government applied to all Dakota/Lakota people. However, some tribes have formally or informally adopted traditional names: the Rosebud Sioux Tribe is also known as the Sičháŋǧu Oyáte (Brulé Nation), and the Oglala often use the name Oglála Lakȟóta Oyáte, rather than the English "Oglala Sioux Tribe" or OST. (The alternate English spelling of Ogallala is deprecated, even though it is closer to the correct pronunciation.) The Lakota have names for their own subdivisions. The Lakota also are Western of the three Sioux groups, occupying lands in both North and South Dakota.
Reservations.
Today, one half of all enrolled Sioux live off the Reservation.
Lakota reservations recognized by the U.S. government include:
Some Lakota also live on other Sioux reservations in eastern South Dakota, Minnesota, and Nebraska:
In addition several Lakota live on Wood Mountain Indian Reserve often Wood Mountain First Nation northwest of Wood Mountain Post now a Saskatchewan historic site.

</doc>
<doc id="56125" url="https://en.wikipedia.org/wiki?curid=56125" title="Sioux">
Sioux

The Sioux are a Native American tribe and First Nations band government in North America. The term can refer to any ethnic group within the Great Sioux Nation or any of the nation's many language dialects. The Sioux comprise three major divisions based on Siouan dialect and subculture: the Santee, the Yankton-Yanktonai, and the Lakota.
The Santee ("Isáŋyathi"; "Knife"), also called Eastern Dakota, reside in the extreme east of the Dakotas, Minnesota and northern Iowa. The Yankton and Yanktonai ("Iháŋktȟuŋwaŋ" and "Iháŋktȟuŋwaŋna"; "Village-at-the-end" and "Little village-at-the-end"), collectively also referred to as the Western Dakota or by the endonym "Wičhíyena", reside in the Minnesota River area. They are considered to be the middle Sioux, and have in the past been erroneously classified as Nakota. The Lakota, also called Teton ("Thítȟuŋwaŋ"; possibly "Dwellers on the prairie"), are the westernmost Sioux, known for their hunting and warrior culture.
Today, the Sioux maintain many separate tribal governments scattered across several reservations, communities, and reserves in North Dakota, South Dakota, Nebraska, Minnesota, and Montana in the United States; and Manitoba and southern Saskatchewan in Canada.
Etymology.
The name "Sioux" is an abbreviated form of "Nadouessioux" borrowed into Canadian French from "Nadoüessioüak" from the early Odawa exonym: "naadowesiwag" "Sioux". Jean Nicolet recorded the use in 1640. The Proto-Algonquian form "*na·towe·wa", meaning "Northern Iroquoian", has reflexes in several daughter languages that refer to a small rattlesnake (massasauga, "Sistrurus"). This information was interpreted by some that the Odawa borrowing was an insult. However, this Proto-Algonquian term most likely was ultimately derived from a form "*-a·towe·", meaning simply "to speak a foreign language", which would make it similar to the etymology of the Greek "Barbarian". Later this was extended in meaning in some Algonquian languages to refer to the massasauga. Thus, contrary to many accounts, the old Odawa word "naadowesiwag" did not equate the Sioux with snakes. This is not confirmed though, since usage over the previous decades has led to this term having negative connotations to those tribes to which it refers. This would explain why many tribes have rejected this term as an exonym. One source states that the name "Sioux" derives from a Chippewa word meaning "little snake"; Ojibwe, or Chippewa, is a dialectic variant of Odawa. The current Ojibwe term for the The Sioux and related groups is "Bwaan", meaning "roaster" (Bwaan "NA", Bwaanag "NA' pl."). Presumably, this refers to the style of cooking The Sioux used in the past.
Some of the tribes have formally or informally adopted traditional names: the Rosebud Sioux Tribe is also known as the "Sičháŋǧu Oyáte", and the Oglala often use the name "Oglála Lakȟóta Oyáte", rather than the English "Oglala Sioux Tribe" or OST. The alternative English spelling of Ogallala is considered improper.
Očhéthi Šakówiŋ.
The historical Sioux referred to the Great Sioux Nation as the Očhéthi Šakówiŋ (pronounced ), meaning "Seven Council Fires". Each fire was a symbol of an oyate (people or nation). The seven nations that comprise the Sioux are: Bdewákaŋthuŋwaŋ (Mdewakanton), Waȟpéthuŋwaŋ (Wahpeton), Waȟpékhute (Wahpekute), Sisíthuŋwaŋ (Sisseton), the Iháŋkthuŋwaŋ (Yankton), Iháŋkthuŋwaŋna (Yanktonai), and the Thítȟuŋwaŋ (Teton or Lakota). The Seven Council Fires would assemble each summer to hold council, renew kinships, decide tribal matters, and participate in the Sun Dance. The seven divisions would select four leaders known as Wičháša Yatápika from among the leaders of each division. Being one of the four leaders was considered the highest honor for a leader; however, the annual gathering meant the majority of tribal administration was cared for by the usual leaders of each division. The last meeting of the Seven Council Fires was in 1850.
Today the Teton, Santee (mixture of the four Dakota tribes) and the Minnesota Dakota, and Yankton/Yanktonai are usually known, respectively, as the Lakota, Eastern Dakota, or Western Dakota. In any of the three main dialects, "Lakota" or "Dakota" translate to mean "friend," or more properly, "ally." Usage of Lakota or Dakota may then refer to the alliance that once bound the Great Sioux Nation.
History.
First contacts with Europeans.
The Dakota are first recorded to have resided at the source of the Mississippi River during the seventeenth century. By 1700 some had migrated to present-day South Dakota. Late in the 17th century, the Dakota entered into an alliance with French merchants. The French were trying to gain advantage in the struggle for the North American fur trade against the English, who had recently established the Hudson's Bay Company.
Relationship with French traders.
The first recorded encounter between the Sioux and the French occurred when Radisson and Groseilliers reached what is now Wisconsin during the winter of 1659-60. Later visiting French traders and missionaries included Claude-Jean Allouez, Daniel Greysolon Duluth, and Pierre-Charles Le Sueur who wintered with Dakota bands in early 1700. In 1736 a group of Sioux killed Jean Baptiste de La Vérendrye and twenty other men on an island in Lake of the Woods. However, trade with the French continued until after the French gave up North America in 1763.
Relationship with Pawnees.
Author and historian Mark van de Logt wrote: "Although military historians tend to reserve the concept of "total war" for conflicts between modern industrial nations, the term nevertheless most closely approaches the state of affairs between the Pawnees and the Sioux and Cheyennes. Both sides directed their actions not solely against warrior-combatants but against the people as a whole. Noncombatants were legitimate targets. ... It is within this context that the military service of the Pawnee Scouts must be viewed."
The battle of Massacre Canyon on August 5, 1873, was the last major battle between the Pawnee and the Sioux.
Dakota War of 1862.
By 1862, shortly after a failed crop the year before and a winter starvation, the federal payment was late. The local traders would not issue any more credit to the Santee and one trader, Andrew Myrick, went so far as to say, "If they're hungry, let them eat grass." On August 17, 1862 the Dakota War began when a few Santee men murdered a white farmer and most of his family. They inspired further attacks on white settlements along the Minnesota River. The Santee attacked the trading post. Later settlers found Myrick among the dead with his mouth stuffed full of grass.
On November 5, 1862 in Minnesota, in courts-martial, 303 Santee Sioux were found guilty of rape and murder of hundreds of American settlers. They were sentenced to be hanged. No attorneys or witnesses were allowed as a defense for the accused, and many were convicted in less than five minutes of court time with the judge. President Abraham Lincoln commuted the death sentences of 284 of the warriors, while signing off on the hanging of 38 Santee men on December 26, 1862 in Mankato, Minnesota. It was the largest mass-execution in U.S. history.
Afterwards, the US suspended treaty annuities to the Dakota for four years and awarded the money to the white victims and their families. The men remanded by order of President Lincoln were sent to a prison in Iowa, where more than half died.
During and after the revolt, many Santee and their kin fled Minnesota and Eastern Dakota to Canada, or settled in the James River Valley in a short-lived reservation before being forced to move to Crow Creek Reservation on the east bank of the Missouri. A few joined the Yanktonai and moved further west to join with the Lakota bands to continue their struggle against the United States military.
Others were able to remain in Minnesota and the east, in small reservations existing into the 21st century, including Sisseton-Wahpeton, Flandreau, and Devils Lake (Spirit Lake or Fort Totten) Reservations in the Dakotas. Some ended up in Nebraska, where the Santee Sioux Tribe today has a reservation on the south bank of the Missouri.
Those who fled to Canada now have descendants residing on nine small Dakota Reserves, five of which are located in Manitoba (Sioux Valley, Long Plain, Dakota Tipi, Birdtail Creek, and Oak Lake and the remaining four (Standing Buffalo, Moose Woods [White Cap, Round Plain , and Wood Mountain) in Saskatchewan.
Red Cloud's War.
Red Cloud's War (also referred to as the Bozeman War) was an armed conflict between the Lakota and the United States Army in the Wyoming Territory and the Montana Territory from 1866 to 1868. The war was fought over control of the Powder River Country in north central Wyoming.
The war is named after Red Cloud, a prominent Sioux chief who led the war against the United States following encroachment into the area by the U.S. military. The war ended with the Treaty of Fort Laramie. The Sioux victory in the war led to their temporarily preserving their control of the Powder River country.
Great Sioux War of 1876–77.
The Great Sioux War comprised a series of battles between the Lakota and allied tribes such as the Cheyenne against the United States military. The earliest engagement was the Battle of Powder River, and the final battle was the Wolf Mountain. Included are the Battle of the Rosebud, Battle of the Little Bighorn, Battle of Warbonnet Creek, Battle of Slim Buttes, Battle of Cedar Creek, and the Dull Knife Fight. The Great Sioux War of 1876–77 was also known as the Black Hills War, and was centered on the Lakota tribes of the Sioux, although several natives believe that the primary target of the United States military was the Northern Cheyenne tribe. The series of battles occurred in Montana territory, Dakota territory, and Wyoming territory, and resulted in a victory for the United States military.
Wounded Knee Massacre.
The massacre at Wounded Knee Creek was the last major armed conflict between the Lakota and the United States. It was described as a "" by General Nelson A. Miles in a letter to the Commissioner of Indian Affairs.
On December 29, 1890, five hundred troops of the U.S. 7th Cavalry, supported by four Hotchkiss guns (a lightweight artillery piece capable of rapid fire), surrounded an encampment of the Lakota bands of the Miniconjou and Hunkpapa with orders to escort them to the railroad for transport to Omaha, Nebraska.
By the time it was over, 25 troopers and more than 150 Lakota Sioux lay dead, including men, women, and children. It remains unknown which side was responsible for the first shot; some of the soldiers are believed to have been the victims of "friendly fire" because the shooting took place at point-blank range in chaotic conditions. Around 150 Lakota are believed to have fled the chaos, many of whom may have died from hypothermia.
Reserves and First Nations.
Later in the 19th century, the railroads hired hunters to exterminate the buffalo herds, the Indians' primary food supply. The Santee and Lakota were forced to accept white-defined reservations in exchange for the rest of their lands, and domestic cattle and corn in exchange for buffalo. They became dependent upon annual federal payments guaranteed by treaty.
In Minnesota, the treaties of Traverse des Sioux and Mendota in 1851 left the Sioux with a reservation twenty miles (32 km) wide on each side of the Minnesota River.
Today, one half of all enrolled Sioux in the United States live off the reservation. Enrolled members in any of the Sioux tribes in the United States are required to have ancestry that is at least 1/4 degree Sioux (the equivalent to one grandparent).
In Canada, the Canadian government recognizes the tribal community as First Nations. The land holdings of the these First Nations are called Indian Reserves.
20th century activism.
Wounded Knee incident.
Beginning in the late 1960s, young Native Americans began to agitate for improved conditions, respect for their civil rights, and better programs in education and economic development. Dramatic protests were conceived and carried out, such as the occupation of Alcatraz Island in California.
The Wounded Knee incident began February 27, 1973 when the town of Wounded Knee, South Dakota was seized by followers of the American Indian Movement. The occupiers controlled the town for 71 days while various state and federal law enforcement agencies such as the Federal Bureau of Investigation and the United States Marshals Service laid siege. Two members of A.I.M. were killed by gunfire during the incident.
Republic of Lakota.
The "Lakota Freedom Delegation", a group of controversial Native American activists, declared on December 19, 2007 the Lakota were withdrawing from all treaties signed with the United States to regain sovereignty over their nation. One of the activists, Russell Means, claimed that the action is legal and cites Natural, International and U.S. law. The group considers Lakota to be a sovereign nation, although as yet the state is generally unrecognized. The proposed borders reclaim thousands of square kilometres of North and South Dakota, Wyoming, Nebraska and Montana.
Current activism.
The Lakota Sioux made national news when NPR's "Lost Children, Shattered Families  investigative story aired. It exposed what many critics consider to be the "kidnapping" of Lakota children from their homes by the state of South Dakota's Department of Social Services (D.S.S.). Lakota activists such as Madonna Thunder Hawk and Chase Iron Eyes, along with the People's Law Project, have alleged that Lakota grandmothers are illegally denied the right to foster their own grandchildren. They are currently working to redirect federal funding away from the state of South Dakota's D.S.S. to new tribal foster care programs. This would be a historic shift away from the state's traditional control over Lakota foster children.
In early 2014 a Lakota group launched Mazacoin, a digital currency that is claimed to be the "national currency of the traditional Lakota Nation".
Political organization.
The historical political organization was based on individual participation and the cooperation of many to sustain the tribe's way of life. Leaders were chosen based upon noble birth and demonstrations of chiefly virtues, such as bravery, fortitude, generosity, and wisdom.
Linguistics.
The Sioux comprise three closely related language groups:
The earlier linguistic three-way division of the Sioux language identified "Lakota", "Dakota", and "Nakota" as dialects of a single language, where Lakota = Teton, Dakota = Santee-Sisseton and Nakota = Yankton-Yanktonai. However, the latest studies show that Yankton-Yanktonai never used the autonym "Nakhóta", but pronounced their name roughly the same as the Santee (i.e. "Dakȟóta").
These later studies identify Assiniboine and Stoney as two separate languages, with Sioux being the third language. Sioux has three similar dialects: Lakota, Western Dakota (Yankton-Yanktonai) and Eastern Dakota (Santee-Sisseton). Assiniboine and Stoney speakers refer to themselves as "Nakhóta" or "Nakhóda" (cf. Nakota).
The term "Dakota" has also been applied by anthropologists and governmental departments to refer to all Sioux groups, resulting in names such as "Teton Dakota", "Santee Dakota", etc. This was mainly because of the misrepresented translation of the Ottawa word from which "Sioux" is derived.
Modern geographic divisions.
The Sioux maintain many separate tribal governments scattered across several reservations and communities in North America: in the Dakotas, Minnesota, Nebraska, and Montana in the United States; and in Manitoba, southern Saskatchewan and Alberta in Canada.
The earliest known European record of the Sioux identified them in Minnesota, Iowa, and Wisconsin. After the introduction of the horse in the early 18th century, the Sioux dominated larger areas of land—from present day Central Canada to the Platte River, from Minnesota to the Yellowstone River, including the Powder River country.
Santee (Isáŋyathi or Eastern Dakota).
The Santee migrated north and westward from the Southeast United States, first into Ohio, then to Minnesota. Some came up from the Santee River and Lake Marion, area of South Carolina. The Santee River was named after them, and some of their ancestors' ancient earthwork mounds have survived along the portion of the dammed-up river that forms Lake Marion. In the past, they were a Woodland people who thrived on hunting, fishing and farming.
Migrations of Anishinaabe/Chippewa (Ojibwe) people from the east in the 17th and 18th centuries, with muskets supplied by the French and British, pushed the Dakota further into Minnesota and west and southward. The US gave the name "Dakota Territory" to the northern expanse west of the Mississippi River and up to its headwaters.
Iháŋkthuŋwaŋ-Iháŋkthuŋwaŋna (Yankton-Yanktonai or Western Dakota).
The Iháŋkthuŋwaŋ-Iháŋkthuŋwaŋna, also known by the anglicized spelling Yankton (Iháŋkthuŋwaŋ: "End village") and Yanktonai (Iháŋkthuŋwaŋna: "Little end village") divisions consist of two bands or two of the seven council fires. According to "Nasunatanka" and "Matononpa" in 1880, the Yanktonai are divided into two sub-groups known as the Upper Yanktonai and the Lower Yanktonai (Hunkpatina).
They were involved in quarrying pipestone. The Yankton-Yanktonai moved into northern Minnesota. In the 18th century, they were recorded as living in the Mankato region of Minnesota.
Lakota (Teton or Thítȟuŋwaŋ).
The Sioux likely obtained horses sometime during the seventeenth century (although some historians date the arrival of horses in South Dakota to 1720, and credit the Cheyenne with introducing horse culture to the Lakota). The Teton (Lakota) division of the Sioux emerged as a result of this introduction. Dominating the northern Great Plains with their light cavalry, the western Sioux quickly expanded their territory further to the Rocky Mountains (which they call "Heska", "white mountains"). The Lakota once subsisted on the buffalo hunt, and on corn. They acquired corn mostly through trade with the eastern Sioux and their linguistic cousins, the Mandan and Hidatsa along the Missouri. The name Teton or Thítȟuŋwaŋ is archaic among the people, who prefer to call themselves "Lakȟóta".
Ethnic divisions.
The Sioux are divided into three ethnic groups, the larger of which are divided into sub-groups, and further branched into bands.
Today, many Sioux also live outside their reservations.
Notable Sioux.
Contemporary.
Contemporary Sioux people are listed under the tribes to which they belong.
By individual tribe.
Crow Creek Sioux Tribe of the Crow Creek Reservation
Legacy.
A Manitoba Historical Plaque was erected at the Spruce Woods Provincial Park by the province to commemorate Assiniboin (Nakota) First Nation's role in Manitoba's heritage.

</doc>
<doc id="56126" url="https://en.wikipedia.org/wiki?curid=56126" title="Battle of the Little Bighorn">
Battle of the Little Bighorn

The Battle of the Little Bighorn, known to Lakota as the Battle of the Greasy Grass, and commonly referred to as Custer's Last Stand, was an armed engagement between combined forces of the Lakota, Northern Cheyenne, and Arapaho tribes, against the 7th Cavalry Regiment of the United States Army. The battle, which occurred June 25–26, 1876, near the Little Bighorn River in eastern Montana Territory, was the most prominent action of the Great Sioux War of 1876.
The fight was an overwhelming victory for the Lakota, Northern Cheyenne, and Arapaho, led by several major war leaders, including Crazy Horse and Chief Gall, inspired by the visions of Sitting Bull (Tȟatȟáŋka Íyotake). The U.S. 7th Cavalry, including the Custer Battalion, a force of 700 men led by George Armstrong Custer, suffered a severe defeat. Five of the 7th Cavalry's twelve companies were annihilated; Custer was killed, as were two of his brothers, a nephew, and a brother-in-law. The total U.S. casualty count included 268 dead and 55 severely wounded (6 died from their injuries later), including 4 Crow Indian scouts and 2 Pawnee Indian scouts.
Public response to the Great Sioux War varied at the time. The battle, and Custer's actions in particular, have been studied extensively by historians.
Background.
Tension between the native inhabitants of the Great Plains of the United States and the encroaching white European settlers in the latter half of the 19th Century resulted in a series of conflicts known as the Sioux Wars, which took place between 1854 and 1890. Even though many of the native peoples eventually agreed to relocate to ever-shrinking reservations, a number of them resisted, at times fiercely.
The 1876 Sun Dance Gathering.
Among the Plains Tribes, the long-standing tradition known as the Sun Dance was the most important religious event of the year. It was a time for the annual renewal of life's necessities, for making personal vows, and for seeking visions. It was a holy time, full of continuous prayer. Towards the end of spring in 1876, the Lakota and the Cheyenne held a Sun Dance attended as well by a number of "Agency Indians" who had slipped away from their reservations. During a Sun Dance around June 5, 1876, on Rosebud Creek in Montana, Sitting Bull reportedly had a vision of "soldiers falling into his camp like grasshoppers from the sky." At the same time, U.S. military officials were conducting a summer campaign to force the Lakota and the Cheyenne back to their reservations, using infantry and cavalry in a so-called "three-pronged approach".
The 1876 U.S. Military Campaign.
Col. John Gibbon's column of six companies (A, B, E, H, I, and K) of the 7th Infantry and four companies (F, G, H, and L) of the 2nd Cavalry marched east from Fort Ellis in western Montana on March 30, to patrol the Yellowstone River. Brig. Gen. George Crook's column of ten companies (A, B, C, D, E, F, G, I, L, and M) of the 3rd Cavalry, five (A, B, D, E, and I) of the 2nd Cavalry, two companies (D and F) of the 4th Infantry, and three companies (C, G, and H) of the 9th Infantry, moved north from Fort Fetterman in the Wyoming Territory on May 29, marching toward the Powder River area. Brig. Gen. Alfred Terry's column, including twelve companies (A, B, C, D, E, F, G, H, I, K, L, and M) of the 7th Cavalry under Lieutenant Colonel George Armstrong Custer's immediate command, Companies C and G of the 17th U.S. Infantry, and the Gatling gun detachment of the 20th Infantry departed westward from Fort Abraham Lincoln in the Dakota Territory on May 17. They were accompanied by teamsters and packers with 150 wagons and a large contingent of pack mules that reinforced Custer. Companies C, D, and I of the 6th U.S. Infantry, moved along the Yellowstone River from Fort Buford on the Missouri River to set up a supply depot and joined Terry on May 29 at the mouth of the Powder River. They were later joined there by the steamboat "Far West", which was loaded with 200 tons of supplies from Fort Lincoln.
The Battle of the Rosebud.
The coordination and planning began to go awry on June 17, 1876 when Crook's column retreated after the Battle of the Rosebud. Surprised and according to some accounts astonished by the unusually large numbers of Native Americans, Crook held the field at the end of the battle but felt compelled by his losses to pull back, regroup, and wait for reinforcements. Unaware of Crook's battle, Gibbon and Terry proceeded, joining forces in early June near the mouth of the Rosebud Creek. They reviewed Terry's plan calling for Custer's regiment to proceed south along the Rosebud while Terry and Gibbon's united forces would move in a westerly direction toward the Bighorn and Little Bighorn rivers. As this was the likely location of Native encampments, all army elements were to converge around June 26 or 27, attempting to engulf the Native Americans. On June 22, Terry ordered the 7th Cavalry, composed of 31 officers and 566 enlisted men under Custer, to begin a reconnaissance in force and pursuit along the Rosebud, with the prerogative to "depart" from orders if Custer saw "sufficient reason." Custer had been offered the use of Gatling guns but declined, believing they would slow his command.
Little Bighorn.
While the Terry/Gibbon column was marching toward the mouth of the Little Bighorn, on the evening of June 24, Custer's scouts arrived at an overlook known as the Crow's Nest, east of the Little Bighorn River. At sunrise on June 25, Custer's scouts reported they could see a massive pony herd and signs of the Native American village roughly in the distance. After a night's march, the tired officer who was sent with the scouts could see neither, and when Custer joined them, he was also unable to make the sighting. Custer's scouts also spotted the regimental cooking fires that could be seen from away, disclosing the regiment's position.
Custer contemplated a surprise attack against the encampment the following morning of June 26, but he then received a report informing him several hostiles had discovered the trail left by his troops. Assuming his presence had been exposed, Custer decided to attack the village without further delay. On the morning of June 25, Custer divided his 12 companies into three battalions in anticipation of the forthcoming engagement. Three companies were placed under the command of Major Marcus Reno (A, G, and M), and three were placed under the command of Capt. Frederick Benteen (H, D, and K). Five companies (C, E, F, I, and L) remained under Custer's immediate command. The 12th, Company B under Capt. Thomas McDougall, had been assigned to escort the slower pack train carrying provisions and additional ammunition.
Unknown to Custer, the group of Native Americans seen on his trail were actually leaving the encampment on the Big Horn and did not alert the village. Custer's scouts warned him about the size of the village, with Mitch Bouyer reportedly saying, "General, I have been with these Indians for 30 years, and this is the largest village I have ever heard of." Custer's overriding concern was that the Native American group would break up and scatter. The command began its approach to the village at noon and prepared to attack in full daylight.
Prelude.
7th Cavalry organization.
The 7th Cavalry was created just after the American Civil War. Many men were veterans of the war, including most of the leading officers. A significant portion of the regiment had previously served four-and-a-half years at Ft. Riley, Kansas, during which time it fought one major engagement and numerous skirmishes, experiencing casualties of 36 killed and 27 wounded. Six other troopers had died of drowning and 51 from cholera epidemics. While stationed in Kansas, the 7th Cavalry had attacked Black Kettle's Southern Cheyenne camp on the Washita River in the Battle of Washita River, an attack which was at the time labeled a "massacre of innocent Indians" by the Indian Bureau.
Half of the 7th Cavalry's companies had just returned from 18 months of constabulary duty in the Deep South, having been recalled to Fort Abraham Lincoln to reassemble the regiment for the campaign. About 20 percent of the troopers had been enlisted in the prior seven months (139 of an enlisted roll of 718), were only marginally trained, and had no combat or frontier experience. A sizable number of these recruits were immigrants from Ireland, England and Germany, just as many of the veteran troopers had been before their enlistments. Archaeological evidence suggests that many of these troopers were malnourished and in poor physical condition, despite being the best-equipped and supplied regiment in the army.
Of the 45 officers and 718 troopers then assigned to the 7th Cavalry (including a second lieutenant detached from the 20th Infantry and serving in Company L), 14 officers (including the regimental commander, Col. Samuel D. Sturgis) and 152 troopers did not accompany the 7th during the campaign. The ratio of troops detached for other duty (approximately 22%) was not unusual for an expedition of this size, and part of the officer shortage was chronic, due to the Army's rigid seniority system: three of the regiment's 12 captains were permanently detached, and two had never served a day with the 7th since their appointment in July 1866. Three second lieutenant vacancies (in E, H, and L Companies) were also unfilled.
Military assumptions prior to the battle.
Number of Indian warriors.
As the Army moved into the field on its expedition, it was operating with incorrect assumptions as to the number of Indians it would encounter. The Army's assumptions were based on inaccurate information provided by the Indian Agents that no more than 800 hostiles were in the area. The Indian Agents based the 800 number on the number of Lakota led by Sitting Bull and other leaders off the reservation in protest of US Government policies. This was a correct estimate until several weeks before the battle, when the "reservation Indians" joined Sitting Bull's ranks for the summer buffalo hunt. However, the agents did not take into account the many thousands of "reservation Indians" who had "unofficially" left the reservation to join their "uncooperative non-reservation cousins led by Sitting Bull". The latter were those groups who had indicated that they were not going to cooperate with the US Government and live on reservation lands. Thus, Custer unknowingly faced thousands of Indians, in addition to the 800 non-reservation "hostiles". All Army plans were based on the incorrect numbers. While after the battle, Custer was severely criticized for not having accepted reinforcements and for dividing his forces, it must be understood that he had accepted the same official Government estimates of hostiles in the area which Terry and Gibbon also accepted. Historian James Donovan states that when Custer asked interpreter Fred Gerard for his opinion on the size of the opposition, he estimated the force at between 1,500 to 2,500 warriors.
Additionally, Custer was more concerned with preventing the escape of the Lakota and Cheyenne than with fighting them. From his own observation, as reported by his bugler John Martin (Martini), Custer assumed the warriors had been sleeping in on the morning of the battle, to which virtually every native account attested later, giving Custer a false estimate of what he was up against. When he and his scouts first looked down on the village from Crow's Nest across the Little Bighorn River, they could only see the herd of ponies. Looking from a hill away after parting with Reno's command, Custer could observe only women preparing for the day, and young boys taking thousands of horses out to graze south of the village. Custer's Crow scouts told him it was the largest native village they had ever seen. When the scouts began changing back into their native dress right before the battle, Custer released them from his command. While the village was enormous in size, Custer thought there were far fewer warriors to defend the village. He assumed most of the warriors were still asleep in their tipis.
Finally, Custer may have assumed that in the event of his encountering Native Americans, his subordinate Benteen with the pack train would quickly come to his aid. Rifle volleys were a standard way of telling supporting units to come to another unit's aid. In a subsequent official 1879 Army investigation requested by Major Reno, the Reno Board of Inquiry (RCOI), Benteen and Reno's men testified that they heard distinct rifle volleys as late as 4:30 pm during the battle.
Custer had wanted to take a day and scout the village before attacking; however, when men went back after supplies dropped by the pack train, they discovered they were being back-trailed by Indians. Reports from his scouts also revealed fresh pony tracks from ridges overlooking his formation. It became apparent that the warriors in the village were either aware of or would soon be aware of his approach. Fearing that the village would break up into small bands that he would have to chase, Custer began to prepare for an immediate attack.
The role of Indian noncombatants in Custer's strategy.
Lt. Colonel George A. Custer's field strategy was designed to engage noncombatants at the encampments at the Battle of the Little Bighorn, so as to capture women, children, the elderly or disabled to serve as hostages and human shields. Custer's battalions were poised to "ride into the camp and secure noncombatant hostages" and "forc the warriors to surrender". Author Evan S. Connell observed that if Custer could occupy the village before widespread resistance developed, the Sioux and Cheyenne warriors "would be obliged to surrender, because if they started to fight, they would be shooting their own families."
Custer asserted in his book "My Life on the Plains", published just two years before the Battle of the Little Bighorn, that:
On Custer's decision to advance up the bluffs and descend on the village from the east, Lt. Edward Godfrey of Company K surmised:
The Sioux and Cheyenne fighters were acutely aware of the danger posed by the military engagement of noncombatants and that "even a semblance of an attack on the women and children" would draw the warriors back to the village, according to historian John S. Gray. Such was their concern that a "feint" by Capt. Yates' E and F Companies at the mouth of Medicine Tail Coulee (Minneconjou Ford) caused hundreds of warriors to disengage from the Reno valley fight and return to deal with the threat to the village.
Some authors and historians, based on archeological evidence and reviews of native testimony, speculate that Custer attempted to cross the river at a point they refer to as Ford D. According to Richard A.Fox, James Donovan, and others, Custer proceeded with a wing of his battalion (Yates' Troops E and F) north and opposite the Cheyenne circle at that crossing, which provided "access to the and children fugitives." Yates's force "posed an immediate threat to fugitive Indian families…" gathering at the north end of the huge encampment.then persisted in his efforts to "seize women and children" even as hundreds of warriors were massing around Keogh's wing on the bluffs. Yates' wing, descending to the Little Bighorn River at Ford D, encountered "light resistance", undetected by the Indian forces ascending the bluffs east of the village. Custer was almost within "striking distance of the refugees" before being repulsed by Indian defenders and forced back to Custer Ridge. That hypothesis, by Fox's own admission, is not universally accepted, and as a result of conflicting physical evidence and variations in differing Lakota accounts, Custer's precise movements remain impossible to ascertain.
Battle engagements.
Reno's attack.
The first group to attack was Major Reno's second detachment (Companies A, G and M), conducted after receiving orders from Custer written out by Lt. William W. Cooke, as Custer's Crow scouts reported Sioux tribe members were alerting the village. Ordered to charge, Reno began that phase of the battle. The orders, made without accurate knowledge of the village's size, location, or the warriors' propensity to stand and fight, had been to pursue the Native Americans and "bring them to battle." Reno's force crossed the Little Bighorn at the mouth of what is today Reno Creek around 3:00 pm. They immediately realized that the Lakota and Northern Cheyenne were present "in force and not running away."
Reno advanced rapidly across the open field towards the northwest, his movements masked by the thick bramble of trees that ran along the southern banks of the Little Bighorn river. The same trees on his front right shielded his movements across the wide field over which his men rapidly rode, first with two approximately forty-man companies abreast and eventually with all three charging abreast. The trees also obscured Reno's view of the Native American village until his force had passed that bend on his right front and was suddenly within arrow shot of the village. The tepees in that area were occupied by the Hunkpapa Sioux. Neither Custer nor Reno had much idea of the length, depth and size of the encampment they were attacking, as the village was hidden by the trees. When Reno came into the open in front of the south end of the village, he sent his Arikara/Ree and Crow Indian scouts forward on his exposed left flank. Realizing the full extent of the village's width, Reno quickly suspected what he would later call "a trap" and stopped a few hundred yards short of the encampment.
He ordered his troopers to dismount and deploy in a skirmish line, according to standard army doctrine. In this formation, every fourth trooper held the horses for the troopers in firing position, with five to ten yards separating each trooper, officers to their rear and troopers with horses behind the officers. This formation reduced Reno's firepower by 25 percent. As Reno's men fired into the village and killed, by some accounts, several wives and children of the Sioux leader, Chief Gall (in Lakota, "Phizí"), mounted warriors began streaming out to meet the attack. With Reno's men anchored on their right by the impassable tree line and bend in the river, the Indians rode hard against the exposed left end of Reno's line. After about 20 minutes of long-distance firing, Reno had taken only one casualty, but the odds against him had risen (Reno estimated five to one) and Custer had not reinforced him. Trooper Billy Jackson reported that by then, the Indians had begun massing in the open area shielded by a small hill to the left of the Reno's line and to the right of the Indian village. From this position the Indians mounted an attack of more than 500 warriors against the left and rear of Reno's line, turning Reno's exposed left flank. They forced a hasty withdrawal into the timber along the bend in the river. Here the Indians pinned Reno and his men down and set fire to the brush to try to drive the soldiers out of their position.
After giving orders to mount, dismount and mount again, Reno told his men, "All those who wish to make their escape follow me," and led a disorderly rout across the river toward the bluffs on the other side. The retreat was immediately disrupted by Cheyenne attacks at close quarters. Later Reno reported that three officers and 29 troopers had been killed during the retreat and subsequent fording of the river. Another officer and 13–18 men were missing. Most of these missing men were left behind in the timber, although many eventually rejoined the detachment. Reno's hasty retreat may have been precipitated by the death of Reno's Arikara Scout Bloody Knife, who had been shot in the head as he sat on his horse next to Reno, his blood and brains splattering the side of Reno's face.
Reno and Benteen on Reno Hill.
Atop the bluffs, known today as Reno Hill, Reno's shaken troops were joined by Captain Benteen's column (Companies D, H and K), arriving from the south. This force had been on a lateral scouting mission when it had been summoned by Custer's messenger, Italian bugler John Martin (Giovanni Martini) with the hand-written message "Benteen. Come on, Big Village, Be quick, Bring packs. P.S. Bring Packs.". Benteen's coincidental arrival on the bluffs was just in time to save Reno's men from possible annihilation. Their detachments were reinforced by McDougall's Company B and the pack train. The 14 officers and 340 troopers on the bluffs organized an all-around defense and dug rifle pits using whatever implements they had among them, including knives. This practice had become standard during the last year of the American Civil War, with both Union and Confederate troops utilizing knives, eating utensils, mess plates and pans, to dig effective battlefield fortifications.
Despite hearing heavy gunfire from the north, including distinct volleys at 4:20 pm, Benteen concentrated on reinforcing Reno's badly wounded and hard-pressed detachment, rather than continuing on toward Custer. Benteen's apparent reluctance to reach Custer prompted later criticism that he had failed to follow orders. Around 5:00 pm, Capt. Thomas Weir and Company D moved out to make contact with Custer. They advanced a mile, to what is today Weir Ridge or Weir Point, and could see in the distance Native warriors on horseback shooting at objects on the ground. By this time, roughly 5:25 pm, Custer's battle may have concluded. The conventional historical understanding is that what Weir witnessed was most likely warriors killing the wounded soldiers and shooting at dead bodies on the "Last Stand Hill" at the northern end of the Custer battlefield. Some contemporary historians have suggested that what Weir witnessed was a fight on what is now called Calhoun Hill. The destruction of Keogh's battalion may have begun with the collapse of L, I and C Company (half of it) following the combined assaults led by Crazy Horse, White Bull, Hump, Chief Gall and others. Other Native accounts contradict this understanding, however, and the time element remains a subject of debate. The other entrenched companies eventually followed Weir by assigned battalions, first Benteen, then Reno, and finally the pack train. Growing Native attacks around Weir Ridge forced all seven companies to return to the bluff before the pack train, with the ammunition, had moved even a quarter mile. There, they remained pinned down for another day, but the Natives were unable to breach this tightly held position.
Benteen displayed calmness and courage by exposing himself to Native fire and was hit in the heel of his boot by a Native bullet. At one point, he personally led a counterattack to push back Natives who had continued to crawl through the grass closer to the soldier's positions.
Custer's fight.
The precise details of Custer's fight are largely conjectural since none of his men (the five companies under his immediate command) survived the battle. The accounts of surviving Indians are conflicting and unclear.
While the gunfire heard on the bluffs by Reno and Benteen's men was probably from Custer's fight, the soldiers on Reno Hill were unaware of what had happened to Custer until General Terry's arrival on June 27. They were reportedly stunned by the news. When the army examined the Custer battle site, soldiers could not determine fully what had transpired. Custer's force of roughly 210 men had been engaged by the Lakota and Northern Cheyenne about 3.5 miles (6 km) to the north. Evidence of organized resistance included apparent breastworks made of dead horses on Custer Hill. By this time, the Lakota and Cheyenne had already removed most of their dead from the field. The soldiers identified the 7th Cavalry's dead as best as possible and hastily buried them where they fell. By the time troops came to recover the bodies, they found most of the dead stripped of their clothing, ritually mutilated, and in an advanced state of decomposition, making identification of many impossible.
Custer was found with shots to the left chest and left temple. Either wound would have been fatal, though he appeared to have bled from only the chest wound, meaning his head wound may have been delivered post-mortem. He also suffered a wound to the arm. Some Lakota oral histories assert that Custer committed suicide to avoid capture and subsequent torture, though this is usually discounted since the wounds were inconsistent with his known right-handedness. (Other Native accounts note several soldiers committing suicide near the end of the battle.) His body was found near the top of Custer Hill, which also came to be known as "Last Stand Hill." There the United States erected a tall memorial obelisk inscribed with the names of the 7th Cavalry's casualties.
Several days after the battle, Curley, Custer's Crow scout who had left Custer near Medicine Tail Coulee, recounted the battle, reporting that Custer had attacked the village after attempting to cross the river. He was driven back, retreating toward the hill where his body was found. As the scenario seemed compatible with Custer's aggressive style of warfare and with evidence found on the ground, it was the basis of many popular accounts of the battle.
According to Pretty Shield, the wife of Goes-Ahead (another Crow scout for the 7th Cavalry), Custer was killed while crossing the river: "...and he died there, died in the water of the Little Bighorn, with Two-bodies, and the blue soldier carrying his flag". In this account, Custer was allegedly killed by a Lakota called Big-nose. However, in Chief Gall's version of events, as recounted to Lt. Edward Settle Godfrey, Custer did not attempt to ford the river and the nearest that he came to the river or village was his final position on the ridge. Chief Gall's statements were corroborated by other Indians, notably the wife of Spotted Horn Bull. Given that no bodies of men or horses were found anywhere near the ford, Godfrey himself concluded "that Custer did not go to the ford with any body of men".
Cheyenne oral tradition credits Buffalo Calf Road Woman with striking the blow that knocked Custer off his horse before he died.
Custer at Minneconjou Ford.
Having isolated Reno's force and driven them away from the encampment, the bulk of the native warriors were free to pursue Custer. The route taken by Custer to his "Last Stand" remains a subject of debate. One possibility is that after ordering Reno to charge, Custer continued down Reno Creek to within about a half mile (800 m) of the Little Bighorn, but then turned north, and climbed up the bluffs, reaching the same spot to which Reno would soon retreat. From this point on the other side of the river, he could see Reno charging the village. Riding north along the bluffs, Custer could have descended into a drainage called Medicine Tail Coulee, which led to the river. Some historians believe that part of Custer's force descended the coulee, going west to the river and attempting unsuccessfully to cross into the village. According to some accounts, a small contingent of Indian sharpshooters opposed this crossing.
White Cow Bull claimed to have shot a leader wearing a buckskin jacket off his horse in the river. While no other Indian account supports this claim, if White Bull did shoot a buckskin-clad leader off his horse, some historians have argued that Custer may have been seriously wounded by him. Some Indian accounts claim that besides wounding one of the leaders of this advance, a soldier carrying a company guidon was also hit. Troopers had to dismount to help the wounded men back onto their horses. The fact that each of the non-mutilation wounds to Custer's body (a bullet wound below the heart and a shot to the left temple) would have been instantly fatal casts doubt on his being wounded and remounted.
Reports of an attempted fording of the river at Medicine Tail Coulee might explain Custer's purpose for Reno's attack, that is, a coordinated "hammer-and-anvil" maneuver, with Reno's holding the Indians at bay at the southern end of the camp, while Custer drove them against Reno's line from the north. Other historians have noted that if Custer did attempt to cross the river near Medicine Tail Coulee, he may have believed it was the north end of the Indian camp, although it was only the middle. Some Indian accounts, however, place the Northern Cheyenne encampment and the north end of the overall village to the left (and south) of the opposite side of the crossing. The location of the north end of the village remains in dispute, however.
Edward Curtis, the famed ethnologist and photographer of the Native American Indians, made a detailed personal study of the Battle, interviewing many of those who had fought or taken part in it. First he went over the ground covered by the troops with the three Crow scouts White Man Runs Him, Goes Ahead, and Hairy Moccasin, and then again with Two Moons and a party of Cheyenne warriors. He also visited the Lakota country and interviewed Red Hawk "whose recollection of the fight seemed to be particularly clear". Finally, he went over the battlefield once more with the three Crow scouts, but also accompanied by General Charles Woodruff "as I particularly desired that the testimony of these men might be considered by an experienced army officer". Finally, Curtis visited the country of the Arikara and interviewed the scouts of that tribe who had been with Custer's command. Based on all the information he gathered, Curtis concluded that Custer had indeed ridden down the Medicine Tail Coulee and then towards the river where he probably planned to ford it. However, "the Indians had now discovered him and were gathered closely on the opposite side". They were soon joined by a large force of Sioux who (no longer engaging Reno) rushed down the valley. This was the beginning of their attack on Custer who was forced to turn and head for the hill where he would make his famous 'last stand'. Thus, wrote Curtis, "Custer made no attack, the whole movement being a retreat".
Other views of Custer's actions at Minneconjou Ford.
Other historians claim that Custer never approached the river, but rather continued north across the coulee and up the other side, where he gradually came under attack. According to this theory, by the time Custer realized he was badly outnumbered, it was too late to break back to the south where Reno and Benteen could have provided assistance. Two men from the 7th Cavalry, the young Crow scout "Ashishishe" (known in English as Curley) and the trooper Peter Thompson, claimed to have seen Custer engage the Indians. The accuracy of their recollections remains controversial, as accounts by battle participants and assessments by historians almost universally discredit Thompson's claim.
Archaeological evidence and reassessment of Indian testimony has led to a new interpretation of the battle. In the 1920s, battlefield investigators discovered hundreds of .45–55 shell cases along the ridge line, known today as Nye-Cartwright Ridge, between South Medicine Tail Coulee and the next drainage at North Medicine Tail (also known as Deep Coulee). Some historians believe Custer divided his detachment into two (and possibly three) battalions, retaining personal command of one while presumably delegating Captain George W. Yates to command the second.
Evidence from the 1920s supports the theory that at least one of the companies made a feint attack southeast from Nye-Cartwright Ridge straight down the center of the "V" formed by the intersection at the crossing of Medicine Tail Coulee on the right and Calhoun Coulee on the left. The intent may have been to relieve pressure on Reno's detachment (according to the Crow scout Curley, possibly viewed by both Mitch Bouyer and Custer) by withdrawing the skirmish line into the timber on the edge of the Little Bighorn River. Had the US troops come straight down Medicine Tail Coulee, their approach to the Minneconjou Crossing and the northern area of the village would have been masked by the high ridges running on the northwest side of the Little Bighorn River.
That they might have come southeast, from the center of Nye-Cartwright Ridge, seems to be supported by Northern Cheyenne accounts of seeing the approach of the distinctly white-colored horses of Company E, known as the Grey Horse Company. Its approach was seen by Indians at that end of the village. Behind them, a second company, further up on the heights, would have provided long-range cover fire. Warriors could have been drawn to the feint attack, forcing the battalion back towards the heights, up the north fork drainage, away from the troops' providing cover fire above. The covering company would have moved towards a reunion, delivering heavy volley fire and leaving the trail of expended cartridges discovered 50 years later.
The "Last Stand".
In the end, the hilltop was probably too small to accommodate the survivors and wounded. Fire from the southeast made it impossible for Custer's men to secure a defensive position all around Last Stand Hill where the soldiers put up their most dogged defense. According to Lakota accounts, far more of their casualties occurred in the attack on Last Stand Hill than anywhere else. The extent of the soldiers' resistance indicated they had few doubts about their prospects for survival. According to Cheyenne and Sioux testimony, the command structure rapidly broke down, although smaller "last stands" were apparently made by several groups. Custer's remaining companies (E, F, and half of C,) were soon killed.
By almost all accounts, the Lakota annihilated Custer's force within an hour of engagement. David Humphreys Miller, who between 1935 and 1955 interviewed the last Lakota survivors of the battle, wrote that the Custer fight lasted less than one-half hour. Other Native accounts said the fighting lasted only "as long as it takes a hungry man to eat a meal." The Lakota asserted that Crazy Horse personally led one of the large groups of warriors who overwhelmed the cavalrymen in a surprise charge from the northeast, causing a breakdown in the command structure and panic among the troops. Many of these men threw down their weapons while Cheyenne and Sioux warriors rode them down, "counting coup" with lances, coup sticks, and quirts. Some Native accounts recalled this segment of the fight as a "buffalo run."
Custer's final resistance.
Recent archaeological work at the battlefield indicates that officers on Custer Hill restored some tactical control. E Company rushed off Custer Hill toward Little Big Horn River but failed and resulted in total destruction, leaving behind some 50 to 60 men. The remainder of the battle took on the nature of a running fight. Modern archaeology and historical Indian accounts indicate that Custer's force may have been divided into three groups, with the Indians' attempting to prevent them from effectively reuniting. Indian accounts describe warriors (including women) running up from the village to wave blankets in order to scare off the soldiers' horses. One 7th cavalry trooper claimed finding a number of stone "mallets" consisting of a round cobble weighing 8-10 pounds (about 4 kg) with a rawhide handle, which he believed had been used by the Indian women to finish off the wounded. Fighting dismounted, the soldiers' skirmish lines were overwhelmed. Army doctrine would have called for one man in four to be a horseholder behind the skirmish lines and, in extreme cases, one man in eight. Later, the troops would have bunched together in defensive positions and are alleged to have shot their remaining horses as cover. As individual troopers were wounded or killed, initial defensive positions would have been abandoned as untenable.
Under threat of attack, the first US soldiers on the battlefield three days later hurriedly buried the troopers in shallow graves, more or less where they had fallen. A couple of years after the battle, markers were placed where men were believed to have fallen, so the placement of troops has been roughly construed. The troops evidently died in several groups, including on Custer Hill, around Captain Myles Keogh, and strung out towards the Little Big Horn River.
Last break-out attempt by 28 troopers.
Modern documentaries suggest that there may not have been a "Last Stand", as traditionally portrayed in popular culture. Instead, archaeologists suggest that, in the end, Custer's troops were not surrounded but rather overwhelmed by a single charge. This scenario corresponds to several Indian accounts stating Crazy Horse's charge swarmed the resistance, with the surviving soldiers fleeing in panic. Many of these troopers may have ended up in a deep ravine 300–400 yards away from what is known today as Custer Hill. At least 28 bodies (the most common number associated with burial witness testimony), including that of scout Mitch Bouyer, were discovered in or near that gulch, their deaths possibly the battle's final actions. Although the marker for Mitch Bouyer has been accounted for as being accurate through archaeological and forensic testing, it is some 65 yards away from Deep Ravine. Other archaeological explorations done in Deep Ravine have found no human remains associated with the battle. According to Indian accounts, about 40 men made a desperate stand around Custer on Custer Hill, delivering volley fire. The great majority of the Indian casualties were probably suffered during this closing segment of the battle, as the soldiers and Indians on Calhoun Ridge were more widely separated and traded fire at greater distances for most of their portion of the Battle than did the soldiers and Indians on Custer Hill.
Aftermath.
After the Custer force was annihilated, the Lakota and Northern Cheyenne regrouped to attack Reno and Benteen. The fight continued until dark (approximately 9:00 pm) and for much of the next day, with the outcome in doubt. Reno credited Benteen's leadership with repulsing a severe attack on the portion of the perimeter held by Companies H and M. On June 27, the column under General Terry approached from the north, and the Indians drew off in the opposite direction. The Crow scout White Man Runs Him was the first to tell General Terry's officers that Custer's force had "been wiped out." Reno and Benteen's wounded troops were given what treatment was available at that time; five later died of their wounds. One of the regiment's three surgeons had been with Custer's column, while another, Dr. DeWolf, had been killed during Reno's retreat. The only remaining doctor was Assistant Surgeon Henry R. Porter.
News of the defeat arrived in the East as the U.S. was observing its centennial. The Army began to investigate, although its effectiveness was hampered by a concern for survivors, and the reputation of the officers.
From the Indian perspective, the aftermath of the Battle of the Little Bighorn had far-reaching consequences. It was the beginning of the end of the Indian Wars, and has even been referred to as "the Indians' last stand" in the area. Within 48 hours after the battle, the large encampment on the Little Bighorn broke up into smaller groups as the resources of grass for the horses and game could not sustain a large congregation of people.
Oglala Sioux Black Elk recounted the exodus this way: "We fled all night, following the Greasy Grass. My two younger brothers and I rode in a pony-drag, and my mother put some young pups in with us. They were always trying to crawl out and I was always putting them back in, so I didn't sleep much."
The scattered Sioux and Cheyenne feasted and celebrated during July with no threat from soldiers. After their celebrations, many of the Indians slipped back to the reservation. Soon, the number of warriors who still remained at large and hostile amounted to only about 600. Both Crook and Terry remained immobile for seven weeks after the Bighorn battle, awaiting reinforcements and unwilling to venture out against the Indians until they had at least 2,000 men. Crook and Terry finally took the field against the Indians in August. General Nelson A. Miles took command of the effort in October 1876. In May 1877, Sitting Bull escaped to Canada. Within days, Crazy Horse surrendered at Fort Robinson. The Great Sioux War ended on May 7 with Miles' defeat of a remaining band of Miniconjou Sioux.
As for the Black Hills, the Manypenny Commission structured an arrangement in which the Sioux would cede the land to United States or the government would cease to supply rations to the reservations. Threatened with starvation, the Indians ceded "Paha Sapa" to the United States, but the Sioux never accepted the legitimacy of the transaction. After lobbying Congress to create a forum to decide their claim, and subsequent litigation spanning 40 years, the United States Supreme Court in the 1980 decision United States v. Sioux Nation of Indians acknowledged the United States had taken the Black Hills without just compensation. The Sioux refused the money offered, and continue to insist on their right to occupy the land.
Battle participants.
Notable scouts/interpreters in the battle.
The 7th Cavalry was accompanied by a number of scouts and interpreters:
Arapaho participation.
Modern-day accounts include Arapaho warriors in this fight, but the five Arapaho men were at the encampments only by accident. While on a hunting trip they came close to the village by the river and were captured and almost killed by the Lakota who believed the hunters were scouts for the US Army. Two Moon, a Northern Cheyenne leader, interceded to save their lives.
Order of battle.
Native Americans
United States Army, Lieutenant Colonel George A. Custer, 7th United States Cavalry Regiment, Commanding.
Casualties.
Native American casualties.
Native American casualties estimates have differed widely, from as few as 36 dead (from Native American listings of the dead by name) to as many as 300. The Sioux chief Red Horse told Col. W. H. Wood in 1877 that the Native American suffered 136 dead and 160 wounded during the battle. In 1881, Red Horse told Dr. C. E. McChesney the same numbers but in a series of drawings done by Red Horse to illustrate the battle, Red Horse drew only sixty figures representing Lakota and Cheyenne casualties. Of those sixty figures only thirty some are portrayed with a conventional Plains Indian method of indicating death. In the last 140 years historians were able to identify multiple Indians names pertaining to the same individual which has greatly reduced previously inflated numbers. Today a list of positively known casualties exists that lists 99 names, attributed and consolidated to 31 identified warriors plus six unnamed women and four unnamed children. 
7th Cavalry casualties.
The 7th Cavalry suffered 52 percent casualties: 16 officers and 242 troopers killed or died of wounds, 1 officer and 51 troopers wounded. Every soldier in the five companies with Custer was killed (3 Indian scouts and several troopers had left that column before the battle; an Indian scout, Curley, was the only survivor to leave after the battle had begun), although for years rumors persisted of survivors. Among the dead were Custer's brothers Boston and Thomas, his brother-in-law James Calhoun, and his nephew Henry Reed. The sole surviving animal reportedly discovered on the battlefield by General Terry's troops was Captain Keogh's horse, Comanche, although other horses were believed to have been taken by the Indians.
In 1878, the army awarded 24 Medals of Honor to participants in the fight on the bluffs for bravery, most for risking their lives to carry water from the river up the hill to the wounded. Few on the non-Indian side questioned the conduct of the enlisted men, but many questioned the tactics, strategy and conduct of the officers. Indian accounts spoke of soldiers' panic-driven flight and suicide by those unwilling to fall captive to the Indians. While such stories were gathered by Thomas Bailey Marquis in a book in the 1930s, it was not published until 1976 because of the unpopularity of such assertions. Although soldiers may have believed captives would be tortured, Indians usually killed men outright and took as captive for adoption only young women and children. Indian accounts also noted the bravery of soldiers who fought to the death.
Legacy.
Reconstitution of the 7th Cavalry – July 1876.
Beginning in July, the 7th Cavalry was assigned new officers and recruiting efforts begun to fill the depleted ranks. The regiment, reorganized into eight companies, remained in the field as part of the Terry Expedition, now based on the Yellowstone River at the mouth of the Big Horn and reinforced by Gibbon's column. On August 8, 1876, after Terry was further reinforced with the 5th Infantry, the expedition moved up Rosebud Creek in pursuit of the Lakota. It met with Crook's command, similarly reinforced, and the combined force, almost 4,000 strong, followed the Lakota trail northeast toward the Little Missouri River. Persistent rain and lack of supplies forced the column to dissolve and return to its varying starting points. The 7th Cavalry returned to Fort Abraham Lincoln to reconstitute.
The expansion of the US Army.
The US Congress authorized appropriations to expand the Army by 2,500 men to meet the emergency after the defeat of the 7th Cavalry. For a session, the Democratic Party-controlled House of Representatives abandoned its campaign to reduce the size of the Army. Word of Custer's fate reached the 44th United States Congress as a conference committee was attempting to reconcile opposing appropriations bills approved by the House and the Republican Senate. They approved a measure to increase the size of cavalry companies to 100 enlisted men on July 24. The committee temporarily lifted the ceiling on the size of the Army by 2,500 on August 15.
"Sell or Starve".
As a result of the defeat in June 1876, Congress responded by attaching what the Sioux call the "sell or starve" rider () to the Indian Appropriations Act of 1876 (enacted August 15, 1876) which cut off all rations for the Sioux until they terminated hostilities and ceded the Black Hills to the United States. The Agreement of 1877 (, enacted February 28, 1877) officially took away Sioux land and permanently established Indian reservations.
Battle controversies.
Reno's conduct.
The Battle Of The Little Bighorn was the subject of an 1879 U.S. Army Court of Inquiry in Chicago, held at Reno's request, during which his conduct was scrutinized. Some testimony by non-Army officers suggested that he was drunk and a coward. The court found Reno's conduct to be without fault. Since the battle, Thomas Rosser, James O'Kelly, and others continued to question the conduct of Reno due to his hastily ordered retreat. Defenders of Reno at the trial noted that, while the retreat was disorganized, Reno did not withdraw from his position until it became apparent that he was outnumbered and outflanked by the Indians. Contemporary accounts also point to the fact that Reno's scout, Bloody Knife, was shot in the head, spraying him with blood, possibly increasing his own panic and distress.
Custer's errors.
General Terry and others claimed that Custer made strategic errors from the start of the campaign. For instance, he refused to use a battery of Gatling guns, and turned down General Terry's offer of an additional battalion of the 2nd Cavalry. Custer believed that the Gatling guns would impede his march up the Rosebud and hamper his mobility. His rapid march en route to the Little Big Horn averaged nearly a day, so his assessment appears to have been accurate. Custer planned "to live and travel like Indians; in this manner the command will be able to go wherever the Indians can," he wrote in his "Herald" dispatch.
By contrast, each Gatling gun had to be hauled by four horses, and soldiers often had to drag the heavy guns by hand over obstacles. Each of the heavy, hand-cranked weapons could fire up to 350 rounds a minute, an impressive rate, but they were known to jam frequently. During the Black Hills Expedition two years earlier, a Gatling gun had turned over, rolled down a mountain, and shattered to pieces. Lieutenant William Low, commander of the artillery detachment, was said to have almost wept when he learned he had been excluded from the strike force.
Custer believed that the 7th Cavalry could handle any Indian force and that the addition of the four companies of the 2nd would not alter the outcome. When offered the 2nd Cavalry, he reportedly replied that the 7th "could handle anything." There is evidence that Custer suspected that he would be outnumbered by the Indians, although he did not know by how many. By dividing his forces, Custer could have caused the defeat of the entire column, had it not been for Benteen's and Reno's linking up to make a desperate yet successful stand on the bluff above the southern end of the camp.
The historian James Donovan believed that Custer's dividing his force into four smaller detachments (including the pack train) can be attributed to his inadequate reconnaissance; he also ignored the warnings of his Crow scouts and Charley Reynolds. By the time the battle began, Custer had already divided his forces into three battalions of differing sizes, of which he kept the largest. His men were widely scattered and unable to support each other. Wanting to prevent any escape by the combined tribes to the south, where they could disperse into different groups, Custer believed that an immediate attack on the south end of the camp was the best course of action.
Admiration for Custer.
Criticism of Custer was not universal. While investigating the battlefield, Lieutenant General Nelson A. Miles wrote in 1877, "The more I study the moves here the Little Big Horn, the more I have admiration for Custer." Facing major budget cutbacks, the U.S. Army wanted to avoid bad press and found ways to exculpate Custer. They blamed the defeat on the Indians' alleged possession of numerous repeating rifles and the overwhelming numerical superiority of the warriors.
The widowed Elizabeth Bacon Custer, who never remarried, wrote three popular books in which she fiercely protected her husband's reputation. She lived until 1933, thus preventing much serious research until most of the evidence was long gone. In addition, Captain Frederick Whittaker's 1876 book idealizing Custer was hugely successful. Custer as a heroic officer fighting valiantly against savage forces was an image popularized in "Wild West" extravaganzas hosted by showman "Buffalo Bill" Cody, Pawnee Bill, and others. It wasn't until over half a century later that historians took another look at the battle and Custers decisions that led to his death and loss of half his command and found much to criticize.
Weapons used at the Battle of the Little Bighorn.
Lakota and Cheyenne.
The Lakota and Cheyenne warriors that opposed Custer's forces possessed a wide array of weaponry, from Stone Age war clubs and lances to the most advanced firearms of the day. The typical firearms carried by the Lakota and Cheyenne combatants were muzzleloaders, more often a cap-lock smoothbore, the so-called Indian trade musket or Leman guns distributed to Indians by the US government at treaty conventions. Less common were surplus .58 caliber rifled muskets of American Civil War vintage such as the Enfield and Springfield. Metal cartridge weapons were prized by native combatants, such as the Henry and the Spencer lever-action rifles, as well as Sharps breechloaders. Bows and arrows were utilized by younger braves in lieu of the more potent firearms; effective up to 30 yards (27 meters), the arrows could readily maim or disable an opponent.
Sitting Bull's forces had no assured means to supply themselves with firearms and ammunition. Nonetheless, they could usually procure these through post-traders, licensed or unlicensed, and from gunrunners who operated in the Dakota Territory: "…a horse or a mule for a repeater…buffalo hides for ammunition." Custer's highly regarded guide, "Lonesome" Charley Reynolds, informed his superior in early 1876 that Sitting Bull's forces were amassing weapons, including numerous Winchester repeating rifles and abundant ammunition.
Of the guns owned by Lakota and Cheyenne fighters at the Little Bighorn, approximately 200 were repeating rifles corresponding to about 1 of 10 of the encampment's two thousand able-bodied fighters who participated in the battle
7th Cavalry.
The troops under Custer's command carried two regulation firearms authorized and issued by the U.S. Army in early 1876: the breech-loading, single-shot Springfield Model 1873 carbine, and the 1873 Colt single-action revolver. The regulation M1860 saber or "Long Knives" were not carried by troopers upon Custer's order.
With the exception of a number of officers and scouts who opted for personally owned and more expensive rifles and handguns, the 7th Cavalry was uniformly armed.
Ammunition allotments provided 100 carbine rounds per trooper, carried on an cartridge belt and in saddlebags on their mounts. An additional 50 carbine rounds per man were reserved on the pack train that accompanied the regiment to the battlefield. Each trooper had 24 rounds for his Colt handgun.
The opposing forces, though not equally matched in the number and type of arms, were comparably outfitted, and neither side held a overwhelming advantage in weaponry.
Lever-action Repeaters vs. Single-shot Breechloaders.
Two hundred or more Lakota and Cheyenne combatants are known to have been armed with Henry, Winchester, or similar lever-action repeating rifles at the battle. Virtually every trooper in the 7th Cavalry fought with the single-shot, breech-loading Springfield carbine and the Colt revolver.
Historians have asked whether the repeating rifles conferred a distinct advantage on Sitting Bull's villagers that contributed to their victory over Custer's carbine-armed soldiers.
Historian Michael L. Lawson offers a scenario based on archaeological collections at the "Henryville" site, which yielded plentiful Henry rifle cartridge casings from approximately 20 individual guns. Lawson speculates that, though less powerful than the Springfield carbines, the Henry repeaters provided a barrage of fire at a critical point, driving Lieutenant James Calhoun's L Company from Calhoun Hill and Finley Ridge, forcing them to flee in disarray back to Captain Myles Keogh's I Company, and leading to the disintegration of that wing of Custer's Battalion.
Model 1873 Springfield carbine and the US Army.
After exhaustive testing – including comparisons to domestic and foreign single-shot and repeating rifles – the Army Ordnance Board (whose members included officers Marcus Reno and Alfred Terry) authorized the Springfield as the official firearm for the United State Army.
The Springfield, manufactured in a .45-70 long rifle version for the infantry and a .45-55 light carbine version for the cavalry, was judged a solid firearm that met the long-term and geostrategic requirements of the United States fighting forces.
British historian Mark Gallear maintains that US government experts rejected the lever-action repeater designs, deeming them ineffective in the event of a clash with fully equipped European armies, or in case of an outbreak of another American civil conflict. Gallear's analysis minimizes the allegation that rapid depletion of ammunition in lever-action models influenced the decision in favor of the single-shot Springfield. The Indian War, in this context, appears as a minor theatre of conflict, whose contingencies were unlikely to govern the selection of standard weaponry for an emerging industrialized nation.
The Springfield carbine is praised for its "superior range and stopping power" by historian James Donovan, and author Charles M. Robinson reports that the rifle could be "loaded and fired much more rapidly than its muzzle loading predecessors, and had twice the range of repeating rifles such as the Winchester, Henry and Spencer."
Gallear points out that lever-action rifles, after a burst of rapid discharge, still required a reloading interlude that lowered their overall rate of fire; Springfield breechloaders "in the long run, had a higher rate of fire, which was sustainable throughout a battle."
The breechloader design patent for the Springfield's Erskine S. Allin "trapdoor" system was owned by the US government and the firearm could be easily adapted for production with existing machinery at the Springfield Armory in Massachusetts.
Malfunction of the Springfield Carbine extractor mechanism.
The question as to whether the reported malfunction of the Model 1873 Springfield carbine issued to the 7th Cavalry contributed to their defeat has been debated for years.
That the weapon experienced jamming of the extractor is not contested, but its contribution to Custer's defeat is considered negligible. This conclusion is supported by evidence from archaeological studies performed at the battlefield, where the recovery of Springfield cartridge casing, bearing tell-tale scratch marks indicating manual extraction, were rare.
The flaw in the ejector mechanism was known to the Army Ordnance Board at the time of the selection of the Model 1873 rifle and carbine, and was not considered a significant shortcoming in the overall worthiness of the shoulder arm. With the ejector failure in US Army tests as low as 1:300, the Springfield carbine was vastly more reliable than the muzzle-loading Springfields used in the Civil War.
Gallear addresses the post-battle testimony concerning the copper .45-55 cartridges supplied to the troops in which an officer is said to have cleared the chambers of spent cartridges for a number of Springfield carbines. This testimony of widespread fusing of the casings offered to the Chief of Ordnance at the Reno Court of Inquiry in 1879 conflicts with the archaeological evidence collected at the battlefield. Field data showed that possible extractor failures occurred at a rate of approximately 1:30 firings at the Custer Battlefield and at a rate of 1:37 at the Reno-Benteen Battlefield.
Historian Thom Hatch observes that the Model 1873 Springfield, despite the known ejector flaw, remained the standard issue shoulder arm for US troops until the early 1890s. when the copper-cased, inside-primed cartridges were replaced with brass.
The Gatling gun controversy.
General Alfred Terry's Dakota column included a single battery of artillery, comprising two Rodman guns (3-inch Ordnance rifle) and two Gatling guns. (According to historian Evan S. Connell, the precise number of Gatlings has not been established, ranging from two to three).
Custer's decision to reject Terry's offer of the rapid-fire Gatlings has raised questions among historians as to why he refused them and what advantage their availability might have conferred on his forces at the Battle of the Little Bighorn.
One factor concerned Major Marcus Reno's recent 8-day reconnaissance-in-force of the Powder-Tongue-Rosebud Rivers, June 10 to 18. This deployment had demonstrated that artillery pieces mounted on gun carriages and hauled by horses no longer fit for cavalry mounts (so-called condemned horses) were cumbersome over mixed terrain and vulnerable to breakdowns. Custer, valuing the mobility of the 7th Cavalry and recognizing Terry's acknowledgement of the regiment as "the primary strike force" preferred to remain unencumbered by the Gatling guns. Custer insisted that the artillery was superfluous to his success, in that the 7th Cavalry alone was sufficient to cope with any force they should encounter, informing Terry: "The 7th can handle anything it meets". In addition to these practical concerns, a strained relationship with Major James Brisbin induced Custer's polite refusal to integrate Brisbin's Second Cavalry unit – and the Gatling guns – into his strike force, as it would disrupt any hierarchical arrangements that Custer presided over.
Historians have acknowledged the fire power inherent in the Gatling gun: they were capable of firing 350 .45-70 caliber rounds per minute. Jamming caused by black powder residue could lower that rate, raising questions as to their reliability under combat conditions. Researchers have further questioned the effectiveness of the guns under the tactics that Custer was likely to face with the Lakota and Cheyenne warriors. The Gatlings, mounted high on carriages, required the battery crew to stand upright during its operation, making them easy targets for Lakota and Cheyenne sharpshooters.
Historian Robert M. Utley, in a section entitled "Would Gatling Guns had Saved Custer?" presents two judgments from Custer's contemporaries: General Henry J. Hunt, expert in the tactical use of artillery in Civil War, stated that Gatlings "would probably have saved the command", whereas General Nelson A. Miles, participant in the Great Sioux War declared " were useless for Indian fighting." 
Battle survivor claims.
Soldiers under Custer's direct command were annihilated on the first day of battle. However, over 120 men and women would come forward over the course of the next 70 years claiming they were "the lone survivor" of Custer's Last Stand. The phenomenon became so widespread that one historian remarked, "Had Custer had all of those who claimed to be "the lone survivor" of his two battalions he would have had at least a brigade behind him when he crossed the Wolf Mountains and rode to the attack."
The historian Earl Alonzo Brininstool suggested he had collected at least 70 "lone survivor" stories. Michael Nunnally, an amateur Custer historian, wrote a booklet describing 30 such accounts. W. A. Graham claimed that even Mrs. Libby Custer received dozens of letters from men, in shocking detail, about their sole survivor experience. At least 125 alleged "single survivor" tales have been confirmed in the historical record as of July 2012.
Frank Finkel, from Dayton, Washington, had such a convincing story that historian Charles Kuhlman believed the alleged survivor, going so far as to write a lengthy defense of Finkel's participation in the battle. Douglas Ellison—mayor of Medora, North Dakota, and an amateur historian—also wrote a book in support of the veracity of Finkel's claim, but most scholars reject it. 
Some of these survivors held a form of celebrity status in the United States, among them Raymond Hatfield "Arizona Bill" Gardner and Frank Tarbeaux. A few even published their own autobiographies including their deeds at the Little Bighorn.
Almost as soon as men came forward implying or directly pronouncing their unique role in the battle, there were others who were equally opposed to any such claims. Theodore Goldin, a battle participant who later became a controversial historian on the event, wrote that:
I'm sorely afraid, Tony, that we will have to class Hayward's story, like that of so many others, as pure, unadulterated B. S.
As a clerk at headquarters I had occasion to look over the morning reports of at least the six troops at Lincoln almost daily, and never saw his name there, or among the list of scouts employed from time to time...I am hoping that some day all of these damned fakirs will die and it will be safe for actual participants in the battle to admit and insist that they were there, without being branded and looked upon as a lot of damned liars. Actually, there have been times when I have been tempted to deny that I ever heard of the 7th Cavalry, much less participated with it in that engagement...My Medal of Honor and its inscription have served me as proof positive that I was at least in the vicinity at the time in question, otherwise I should be tempted to deny all knowledge of the event.
Battlefield preservation.
The site was first preserved as a United States national cemetery in 1879, to protect the graves of the 7th Cavalry troopers. In 1946 it was redesignated as the "Custer Battlefield National Monument", reflecting its association with the general. In 1967, Major Marcus Reno was reinterred in the cemetery with honors, including an eleven-gun salute. Beginning in the early 1970s there was concern within the National Park Service over the name Custer Battlefield National Monument, recognizing the larger history of the battle between two cultures, hearings on the name change were held in Billings on June 10 and during the following months in 1991 Congress renamed the site the "Little Bighorn Battlefield National Monument".
United States memorialization on the battlefield began in 1879 with a temporary monument to U.S. dead. In 1881 the current marble obelisk was erected in their honor. In 1890 marble blocks were added to mark the places where the U.S. cavalry soldiers fell.
Nearly 100 years later, ideas about the meaning of the battle have become more inclusive. The United States government acknowledged that Native American sacrifices also deserved recognition at the site. The 1991 bill changing the name of the national monument also authorized an Indian Memorial to be built near Last Stand Hill in honor of Lakota and Cheyenne warriors. The commissioned work by Native artist Colleen Cutschall is shown in the photograph at right. On Memorial Day 1999, in consultation with tribal representatives, the US added two red granite markers to the battlefield to note where Native American warriors fell. As of December 2006, a total of ten warrior markers have been added (three at the Reno-Benteen Defense Site, seven on the Little Bighorn Battlefield).
The Indian Memorial, themed "Peace Through Unity" l is an open circular structure that stands from the 7th Cavalry obelisk. Its walls have some of the names of Indians who died at the site, as well as Native accounts of the battle. The open circle of the structure is symbolic, as for many tribes, the circle is sacred. The "spirit gate" window facing the Cavalry monument is symbolic as well, welcoming the dead cavalrymen into the memorial.

</doc>
<doc id="56127" url="https://en.wikipedia.org/wiki?curid=56127" title="Cleopatra Selene">
Cleopatra Selene

Cleopatra Selene may refer to:

</doc>
<doc id="56128" url="https://en.wikipedia.org/wiki?curid=56128" title="Cheyenne">
Cheyenne

The Cheyenne ( ) are one of the groups of indigenous people of the Great Plains and their language is of the Algonquian language family. The Cheyenne comprise two Native American groups, the Só'taeo'o or Só'taétaneo'o (more commonly spelled as Suhtai or Sutaio) and the Tsétsêhéstâhese (also spelled Tsitsistas). These tribes merged in the early 19th century. Today, the Cheyenne people are split into two federally recognized groups: Southern Cheyenne, who are enrolled in the Cheyenne and Arapaho Tribes in Oklahoma, and the Northern Cheyenne, who are enrolled in the Northern Cheyenne Tribe of the Northern Cheyenne Indian Reservation in Montana.
The Cheyenne lived in the area of what is now Minnesota at the time of their first contact with the Europeans. They were at times allied with the Lakota and Arapaho. They migrated west across the Mississippi River and into North and South Dakota in the early 18th century. They adopted the horse culture and through ritual ceremonies and structure, developed a more centralized authority than other Plains Indians of the 19th century. Having settled the Black Hills of South Dakota and the Powder River Country of present-day Montana, they introduced the horse culture to Lakota bands about 1730. Allied with the Arapaho, the Cheyenne pushed the Kiowa to the Southern Plains. In turn, they were pushed west by the more numerous Lakota.
The Cheyenne Nation or Tsêhéstáno was at one time composed of ten bands that spread across the Great Plains from southern Colorado to the Black Hills in South Dakota. When they gathered, the bands' leaders would meet in formal council. They performed an annual Arrow Renewal ceremony and Sun Dance. They fought their traditional enemies, the Crow and later (1856–79) the United States Army forces. In the mid-19th century, the bands began to split, with some bands choosing to remain near the Black Hills, while others chose to remain near the Platte Rivers of central Colorado.
The Northern Cheyenne, known in Cheyenne either as Notameohmésêhese, meaning "Northern Eaters" or simply as Ohmésêhese meaning "Eaters", live in southeastern Montana on the Northern Cheyenne Indian Reservation. Tribal enrollment figures, as of late 2014, indicate that there are approximately 10,840 members, of which about 4,939 reside on the reservation. Approximately 91% of the population are Native Americans (full or part race), with 72.8% identifying themselves as Cheyenne. Slightly more than one quarter of the population five years or older spoke a language other than English.
The Southern Cheyenne, known in Cheyenne as Heévâhetaneo'o meaning "Roped People", together with the Southern Arapaho, form the Cheyenne and Arapaho Tribes, in western Oklahoma. Their combined population is 12,130, . In 2003, approximately 8,000 of these identified themselves as Cheyenne, although with continuing intermarriage it has become increasingly difficult to separate the tribes.
Name.
The Tsétsêhéstâhese (more commonly as the Tsitsistas; singular: Tsétsêhéstaestse), which translates to "those who are like this". These two tribes had companions.
Though the identity of the "Šahíya" is not known, many Great Plains tribes assume it means Cree or some other people who spoke an Algonquian language related to Cree and Cheyenne. The Cheyenne word for Ojibwe" is "Sáhea'eo'o," a word that sounds similar to the "Dakota" word "Šahíya"."
Another of the common etymologies for "Cheyenne" is "a bit like the of an alien speech" (literally, "red-talker"). According to George Bird Grinnell, the Dakota had referred to themselves and fellow Siouan-language bands as "white talkers", and those of other language families, such as the Algonquian Cheyenne, as "red talkers" ("Šahíyena").
The etymology of the name Tsitsistas (technically Tsétsėhéstȧhese), which the Cheyennes call themselves, is uncertain. According to the Cheyenne dictionary, offered online by Chief Dull Knife College, there is no definitive consensus and various studies of the origins and the translation of the word has been suggested. Grinnell's record is typical; he states "They call themselves Tsistsistas Tsitsistas is the correct pronunciation, which the books commonly give as meaning "people". It most likely means related to one another, similarly bred, like us, our people, or us. The term for the Cheyenne homeland is "Tsiihistano".
Language.
The Cheyenne of Montana and Oklahoma speak the Cheyenne language, known as "Tsêhésenêstsestôtse" (common spelling: Tsisinstsistots). Approximately 800 people speak Cheyenne in Oklahoma. There are only a handful of vocabulary differences between the two locations. The Cheyenne alphabet contains 14 letters. The Cheyenne language is one of the larger Algonquian-language group. Formerly, the Só'taeo'o (Só'taétaneo'o) or Suhtai (Sutaio) bands of Southern and Northern Cheyennes spoke "Só'taéka'ękóne" or "Só'taenęstsestôtse", a language so close to "Tsêhésenêstsestôtse" (Cheyenne language), that it is sometimes termed a Cheyenne dialect.
History.
Early history.
The earliest known written historical record of the Cheyenne comes from the mid-17th century, when a group of Cheyenne visited the French Fort Crevecoeur, near present-day Peoria, Illinois. The Cheyenne at this time lived between the Mississippi River and Mille Lacs Lake in present-day Minnesota. The Cheyenne economy was based on the collection of wild rice and hunting, especially of bison, which lived in the prairies 70–80 miles west of the Cheyenne villages.
According to tribal history, during the 17th century, the Cheyenne had been driven by the Assiniboine (Hóheeheo'o - ″wrapped ones or swaddled″, adaptive from the Lakota/Dakota word "Hóhe", meaning “rebels”) from the Great Lakes region to present-day Minnesota and North Dakota, where they established villages. The most prominent of the ancient Cheyenne villages is Biesterfeldt Village, in eastern North Dakota along the Sheyenne River. The tribal history also relates that they first reached the Missouri River in 1676. A more recent analysis of early records posits that at least some of the Cheyenne remained in the Mille Lac region of Minnesota until about 1765, when the Ojibwe defeated the Dakota with firearms — pushing the Cheyenne, in turn, to the Minnesota River, where they were reported in 1766.
On the Missouri River, the Cheyenne came into contact with the neighboring Mandan, Hidatsa (Tsé-heše'émâheónese - „people, who have soil houses (that is, earth lodges“) and Arikara people (Ónoneo'o), and they adopted many of their cultural characteristics. They were first of the later Plains tribes into the Black Hills and Powder River Country. About 1730, they introduced the horse to Lakota bands (Ho'óhomo'eo'o - “the invited ones (to Cheyenne lands i.e. the Black Hills)”). Conflict with migrating Lakota and Ojibwe people forced the Cheyenne further west, and they, in turn, pushed the Kiowa to the south.
By 1776, the Lakota had overwhelmed the Cheyenne and taken over much of their territory near the Black Hills. In 1804, Lewis and Clark visited a surviving Cheyenne village in North Dakota. Such European American explorers learned many different names for the Cheyenne, and did not realize how the different sections were forming a unified tribe.
The Cheyenne Nation is descended from two related tribes, the Tsétsêhéstâhese / Tsitsistas (Cheyenne proper) and Só'taeo'o / Só'taétaneo'o (better known as Suhtai or Sutaio), the latter may have joined the Tsétsêhéstâhese in the early 18th century. Their oral history relays that both tribal peoples are characterized, and represented by two cultural heroes or prophets who received divine articles from their god Ma'heo'o (″Sacred Being, God″, commonly in English Maheo, Mahiu, this is a post-missionary term, formerly the plural Ma'heono was used), which the Só'taeo'o called He'emo (″Goddess, Female Sacred Being, God″, equivalent to "Ma'heo'o" in the Tsétsêhéstâhese dialect).
The Tsétsêhéstâhese / Tsitsistas prophet Motsé'eóeve (Sweet Medicine Standing, Sweet Root Standing, commonly called Sweet Medicine) had received the "Maahótse" (in English known as "Mahuts", a bundle of (Sacred) Arrows or the (Sacred) Arrows Bundle) at "Nóávóse" (″medicine(sacred)-hill″, name for Bear Butte, northwest of Rapid City, South Dakota), which they carried when they waged tribal-level war and were kept in the "maahéome" (Arrow Lodge or Arrow Tepee). He organized the structure of Cheyenne society, their military or war societies led by prominent warriors, their system of legal justice, and the Council of Forty-four peace chiefs, the latter was formed from four "véhoo'o" (chiefs or leaders) of the ten principal "manaho" (bands) and an additional four ″Old Man″ meeting to deliberate at regular tribal gatherings, centered around the Sun Dance. Sweet Medicine is the Cheyenne prophet who predicted the coming of the horse, cow, whiteman, etc. to the Cheyennes (1987:6-16; 1987:99); he was named for "motsé'eonȯtse" (sweetgrass), which they used as an Incense and for purification, as oblations to ancestors, for protection of spirits, and keeping out of evil and harm, as well to paint pipes in the Sun Dance and the Sacred Arrow ceremonies. The "Maahótse" (Sacred Arrows) are cared until today by the Southern Cheyenne and Southern Só'taeo'o. The "Keeper of the Sacred Arrows" (called by the whites also: "Keeper of the Medicine Arrows") must be a Southern Cheyenne and cannot be of the Só'taeo'o (Northern or Southern alike).
The Só'taeo'o / Só'taétaneo'o prophet Tomȯsévėséhe ("Tomosevsehe", former English-spelling "Tomsivsi", commonly called Erect Horns) had received the "Ésevone (old term, also meaning ″buffalo herd, female buffalo″, therefore the Só'taeo'o were also known as "Buffalo People")" or "Hóhkėha'e (new term)" (former English-spelling "Is'siwun" - Sacred (Buffalo) Hat, also known as Buffalo Hat, Sacred Hat or Sacred (Buffalo) Hat Bundle) at "Toh'nihvoos" (″Stone Hammer Mountain″) near the Great Lakes in the present state of Minnesota, the "Ésevone / Hóhkėha'e (Sacred Buffalo Hat)" is kept in the "vonȧhéome (old term)" or "hóhkėha'éome (new term)" (Sacred Hat Lodge, Sacred Hat Tepee), Erect Horns gave them the accompanying ceremonies and the Sun Dance (Hestȯsenestȯtse or Hoxéhevėhomó'hestȯtse), his vision convinced the tribe to abandon their earlier sedentary agricultural traditions to adopt nomadic Plains horse culture (therefore the Só'taeo'o were earlier on the plains in the west than the Tsétsêhéstâhese). They replaced their earth lodges with portable tipis and switched their diet from fish and agricultural produce, to mainly bison and wild fruits and vegetables. Their lands ranged from the upper Missouri River into what is now Wyoming, Montana, Colorado, and South Dakota. The "Ésevone / Hóhkėha'e (Sacred Buffalo Hat)" is kept among the Northern Cheyenne and Northern Só'taeo'o. The "Tséá'enōvȧhtse" (″Sacred (Buffalo) Hat Keeper″ or ″Keeper of the Sacred (Buffalo) Hat″) must belong to the Só'taeo'o (Northern or Southern alike). In the 1870's tribal leaders became disenchanted with the keeper of the bundle demanded the keeper Broken Dish give up the bundle; he agreed but his wife did not and desecrated the Sacred Hat and its contents; a ceremonial pipe and a buffalo horn were lost. In 1908 a Cheyanne named Three Fingers gave the horn back to the Hat; 
the pipe came into possession of a Cheyanne named Burnt All Over who gave it to Hattie Goit of Poteau, Oklahoma who in 1911 gave the pipe to the Oklahoma Historical Society. In 1997 the Oklahoma Historal Society negoiated with the Northern Cheyanne to return the pipe to the tribal keeper of the Sacred Medicine Hat Bundle James Black Wolf.
The "Maahótse" (Sacred Arrows) are symbols of male power and the power of the "Ésevone / Hóhkėha'e" (Sacred Buffalo Hat) is female. The Sacred Buffalo Hat and the Sacred Arrows together form the two great covenants of the Cheyenne Nation. Through these two bundles, Ma'heo'o assures continual life and blessings for the people.
Historical Cheyenne bands.
Northern Cheyenne (known in Cheyenne either as Notameohmésêhese or Notameohmésėhétaneo'o meaning "Northern Eaters" or simply as Ohmésêhese / Ôhmésêheseo'o meaning "Eaters")
Lesser northern bands (not represented in the Council of Forty-Four):
Southern Cheyenne (known in Cheyenne as Heévâhetaneo'o meaning "Roped People" - after the most populous band, also commonly known as Sówoniá - "the Southern People")
lesser southern bands (not represented in the Council of Forty-Four):
The Heviksnipahis (Iviststsinihpah, also known as the Tsétsêhéstâhese / Tsitsistas proper), Heévâhetaneo'o (Hevhaitaneo), Masikota (in Lakotiyapi: Sheo), Omísis (Ôhmésêheseo'o, the Notameohmésêhese proper), Só'taeo'o / Só'taétaneo'o (Suhtai or Sutaio, Northern and Southern), Wotápio (Wutapai), Oévemanaho (Oivimána or Oévemana, Northern and Southern), Hesé'omeétaneo'o (Hisiometaneo or Issiometaniu), Oo'kóhta'oná (Ohktounna or Oqtóguna) and the Hónowa (Háovȯhnóvȧhese or Nėstamenóoheo'o) were the ten principal bands that had the right to send four chief delegates representing them in the Council of Forty-Four.
After the "Masikota" and "Oo'kóhta'oná" bands had been almost wiped out through a cholera epidemic in 1849, the remaining Masikota joined the Dog Soldiers warrior society ("Hotamétaneo'o"). They effectively became a separate band and in 1850 took over the position in the camp circle formerly occupied by the Masikota. The members often opposed policies of peace chiefs such as Black Kettle. Over time, the Dog Soldiers took a prominent leadership role in the wars against the whites. In 1867, most of the band were killed by United States Army forces in the Battle of Summit Springs.
Due to an increasing division between the Dog Soldiers and the council chiefs with respect to policy towards the whites, the Dog Soldiers became separated from the other Cheyenne bands. They effectively became a "third division" of the Cheyenne people, between the Northern Cheyenne, who ranged north of the Platte River, and the Southern Cheyenne, who occupied the area north of the Arkansas River.
Expansion on the Plains.
After being pushed south and westward by the Lakota, the unified Cheyenne people began to create and expand a new territory of their own. Sometime around 1811 the Cheyenne made a formal alliance with the Arapaho people (Hetanevo'eo'o - „People of the Sky“, „Cloud People“, because of their close interaction also known as Héstanėheo'o - “people, mankind, tribe of people”), which would remain strong throughout their history and into modern times. The alliance helped the Cheyenne expand their territory which stretched from southern Montana, through most of Wyoming, the eastern half of Colorado, far western Nebraska, and far western Kansas. As early as 1820, traders and explorers reported contact with Cheyenne at present-day Denver, Colorado and on the Arkansas River. They were probably hunting and trading in that area earlier. They may have migrated to the south for winter. The Hairy Rope band is reputed to have been the first band to move south, capturing wild horses as far south as the Cimarron River Valley. In response to the construction of Bent’s Fort by Charles Bent, a friend of the Cheyenne who established a popular trading area for the Cheyenne, a large portion of the tribe moved further south and stayed around the area. The other part of the tribe continued to live along the headwaters of the North Platte and Yellowstone rivers. The groups became the Southern Cheyenne, known as Sówoníă (Southerners) and the Northern Cheyenne, known as O'mǐ'sǐs (Eaters). The separation of the tribe was only a geographic one and the two divisions had regular and close contact.
In the southern portion of their territory the Cheyenne and Arapaho warred with the allied Comanche, Kiowa, and Plains Apache. Numerous battles were fought including a notable fight along the Washita River in 1836 with the Kiowa which resulted in the death of 48 Cheyenne warriors of the Bowstring society. In summer 1838, many Cheyenne and Arapaho attacked a camp of Kiowa and Comanche along Wolf Creek in Oklahoma resulting in heavy losses from both sides. Conflict with the Comanche, Kiowa, and Plains Apache ended in 1840 when the tribes made an alliance with each other. The new alliance allowed the Cheyenne to enter the Llano Estacado in the Texas and Oklahoma panhandles and northeastern New Mexico to hunt bison and trade. Their expansion in the south and alliance with the Kiowa led to their first raid into Mexico in 1853. The raid ended in disaster with heavy resistance from Mexican lancers, resulting in all but three of the war party being killed. To the north the Cheyenne made a strong alliance with the Lakota Sioux, which allowed them to expand their territory into part of their former lands around the Black Hills. They managed to escape the smallpox epidemics, which swept across the plains from white settlements in 1837-39, by heading into the Rocky Mountains, but were greatly affected by the Cholera epidemic in 1849. Contact with Euro-Americans was mostly light, with most contact involving mountain men, traders, explorers, treaty makers, and painters.
Enemies and warrior culture.
Like many other plains Indian nations, the Cheyenne were a horse and warrior people who developed as skilled and powerful mounted warriors. A warrior was viewed by the people not as a maker of war but as a protector, provider, and leader. Warriors gained rank in Cheyenne society by performing and accumulating various acts of bravery in battle known as coups. The title of war chief could be earned by any warrior who performs enough of the specific coups required to become a war chief. Specific warrior societies developed among the Cheyenne as with other plains nations. Each society had selected leaders who would invite those that they saw worthy enough to their society lodge for initiation into the society. Often, societies would have minor rivalries; however, they might work together as a unit when warring with an enemy. Military societies played an important role in Cheyenne government. Society leaders were often in charge of organizing hunts and raids as well as ensuring proper discipline and the enforcement of laws within the nation. Each of the six distinct warrior societies of the Cheyenne would take turns assuming the leadership role within the nation. The four original military societies of the Cheyenne were the Swift Fox Society, Elk Horn Scrapper or Crooked Lance Society, Shield Society, and the Bowstring Men Society. The fifth society is split between the Crazy Dog Society and the famous Dog Soldiers. The sixth society is the Contrary Warrior Society, most notable for riding backwards into battle as a sign of bravery. All six societies and their various branches exist among the Southern and Northern Cheyenne Nations in present times. Warriors used a combination of traditional weapons such as various types of war clubs, tomahawks, bows and arrows, and lances as well as non-traditional weapons such as revolvers, rifles, and shotguns acquired through raid and trade.
The enemies of the Cheyenne included the Crow (Óoetaneo'o - “crow (bird) people”), Shoshone (Sósone'eo'o), Blackfeet (Mo'ôhtávêhahtátaneo'o, same literally meaning), Flathead (Kȧhkoestséataneo'o - “flat-headed-people”), Nez Perce (Otaesétaneo'o - “pierced nose people”), Arikara, Gros Ventre (Hestóetaneo'o - “beggars for meat”, “spongers” or Môhónooneo'o - lit. “scouting all over ones”), Assiniboine, and Plains Cree (Vóhkoohétaneo'o - “rabbit people”) to the north and west of Cheyenne territory. To the east of Cheyenne Territory they fought with the Sioux, Pawnee (Ho'néhetaneo'o - “wolf people”, possibly an adaptive from the Skiri/Skidi Pawnee or Wolf Pawnee), Ponca (Onéhao'o), Kaw (Oo'kóhtâxétaneo'o - “cut hair people”), Iowa, Ho-Chunk and Omaha (Onéhao'o). South of Cheyenne territory they fought with the Kiowa (Vétapâhaetó'eo'o - “greasy wood ones”), Comanche (Šé'šenovotsétaneo'o - “snake people”), Ute (Mo'ȯhtávėhetaneo'o - “black (skinned) people”), Plains Apache (Mȯhtséheonetaneo'o - “occupied.comp-people”), Osage (Oo'kóhtâxétaneo'o - “cut hair people”), Wichita people, various Apache tribes and Navajo (Hotamó'keeho - “Indians from out west”; collective name for tribes of the Southswest and Great Basin). Many of the enemies the Cheyenne fought were only encountered occasionally, such as on a long distance raid or hunt. Some of their enemies, particularly the Indian peoples of the eastern great plains such as the Pawnee and Osage would act as Indian Scouts for the US Army, providing valuable tracking skills and information regarding Cheyenne habits and fighting strategies to US soldiers. Some of their enemies such as the Lakota would later in their history become their strong allies, helping the Cheyenne fight against the United States Army during Red Cloud's War and the Great Sioux War of 1876. The Comanche, Kiowa and Plains Apache became allies of the Cheyenne towards the end of the Indian wars on the southern plains, fighting together during conflicts such as the Red River War.
Relationship with the Arapaho.
The Cheyenne and Arapaho people formed an alliance around 1811 that helped them expand their territories and strengthen their presence on the plains. Like the Cheyenne, the Arapaho language is part of the Algonquian group, although the two languages are not mutually intelligible. The Arapaho remained strong allies with the Cheyenne and helped them fight alongside the Sioux during Red Cloud's War and the Great Sioux War of 1876, also known commonly as the Black Hills War. On the southern plains, the Arapaho and Cheyenne allied with the Comanche, Kiowa, and Plains Apache to fight invading settlers and US soldiers. The Arapaho were present with the Cheyenne at the Sand Creek Massacre when a peaceful encampment of mostly women, children, and the elderly were attacked and massacred by US soldiers. Both major divisions of the Cheyenne, the Northern Cheyenne and Southern Cheyenne were allies to the Arapaho who like the Cheyenne are split into northern and southern divisions. The Southern Cheyenne and Southern Arapaho were assigned to the same reservation in Oklahoma Indian Territory and remained together as the federally recognized Cheyenne and Arapaho Tribes after the reservation was opened to American settlement and into modern times. The Northern Arapaho were to be assigned a reservation of their own or share one with the Cheyenne however the government failed to provide them with either and placed them on the already established Wind River Indian Reservation in Wyoming with their former enemies the Shoshone.
Treaty of 1825.
In the summer of 1825, the tribe was visited on the upper Missouri by a US treaty commission consisting of General Henry Atkinson and Indian agent Benjamin O'Fallon, accompanied by a military escort of 476 men. General Atkinson and his fellow commissioner left Fort Atkinson on May 16, 1825. Ascending the Missouri, they negotiated treaties of friendship and trade with tribes of the upper Missouri, including the Arikara, the Cheyenne, the Crow, the Mandan, the Ponca, and several bands of the Sioux. At that time, the US had competition on the upper Missouri from British traders, who came down from Canada.
The treaties acknowledged that the tribes lived within the United States, vowed perpetual friendship between the US and the tribes, and, recognizing the right of the United States to regulate trade, the tribes promised to deal only with licensed traders. The tribes agreed to forswear private retaliation for injuries, and to return or indemnify the owner of stolen horses or other goods. The commission's efforts to contact the Blackfoot and the Assiniboine were unsuccessful. During their return to Fort Atkinson at the Council Bluff in Nebraska, the commission had successful negotiations with the Ota, the Pawnee and the Omaha.
Effects of the Emigrant Trail.
Increased traffic of emigrants along the related Oregon, Mormon and California trails, beginning in the early 1840s, heightened competition with Native Americans for scarce resources of water and game in arid areas. With resource depletion along the trails, the Cheyenne became increasingly divided into the Northern Cheyenne and Southern Cheyenne, where they could have adequate territory for sustenance.
During the California Gold Rush, emigrants brought in cholera. It spread in mining camps and waterways due to poor sanitation. The disease was generally a major cause of death for emigrants, about one-tenth of whom died during their journeys.
Perhaps from traders, the cholera epidemic reached the Plains Indians in 1849, resulting in severe loss of life during the summer of that year. Historians estimate about 2,000 Cheyenne died, one-half to two-thirds of their population. There were significant losses among other tribes as well, which weakened their social structures. Perhaps because of severe loss of trade during the 1849 season, Bent's Fort was abandoned and burned.
Fort Laramie Treaty of 1851.
In 1846, Thomas Fitzpatrick was appointed US Indian agent for the upper Arkansas and Platte River. His efforts to negotiate with the Northern Cheyenne, the Arapaho and other tribes led to a great council at Fort Laramie in 1851. Treaties were negotiated by a commission consisting of Fitzpatrick and David Dawson Mitchell, US Superintendent of Indian Affairs, with the Indians of the northern plains.
To reduce intertribal warfare on the Plains, the government officials "assigned" territories to each tribe and had them pledge mutual peace. In addition, the government secured permission to build and maintain roads for European-American travelers and traders through Indian country on the Plains, such as the Emigrant Trail and the Santa Fe Trail, and to maintain forts to guard them. The tribes were compensated with annuities of cash and supplies for such encroachment on their territories. The Fort Laramie Treaty of 1851 affirmed the Cheyenne and Arapaho territory on the Great Plains between the North Platte River and the Arkansas. This territory included what is now Colorado, east of the Front Range of the Rockies and north of the Arkansas River; Wyoming and Nebraska, south of the North Platte River; and extreme western Kansas.
Punitive US expedition of 1857.
In April 1856, an incident at the Platte River Bridge (near present-day Casper, Wyoming), resulted in the wounding of a Cheyenne warrior. He returned to the Cheyenne on the plains. During the summer of 1856, Indians attacked travelers along the Emigrant Trail near Fort Kearny. In retaliation, the US Cavalry attacked a Cheyenne camp on Grand Island in Nebraska. They killed ten Cheyenne warriors and wounded eight or more.
Cheyenne parties attacked at least three emigrant settler parties before returning to the Republican River. The Indian agent at Fort Laramie negotiated with the Cheyenne to reduce hostilities, but the Secretary of War ordered the 1st Cavalry Regiment (1855) to carry out a punitive expedition under the command of Colonel Edwin V. Sumner. He went against the Cheyenne in the spring of 1857. Major John Sedgwick led part of the expedition up the Arkansas River, and via Fountain Creek to the South Platte River. Sumner's command went west along the North Platte to Fort Laramie, then down along the Front Range to the South Platte. The combined force of 400 troops went east through the plains searching for Cheyenne.
Under the influence of the medicine man White Bull (also called Ice) and Grey Beard (also called Dark), the Cheyenne went into battle believing that strong spiritual medicine would prevent the soldiers' guns from firing. They were told that if they dipped their hands in a nearby spring, they had only to raise their hands to repel army bullets. Hands raised, the Cheyenne surrounded the advancing troops as they advanced near the Solomon River. Sumner ordered a cavalry charge and the troops charged with drawn sabers; the Cheyenne fled. With tired horses after long marches, the cavalry could not engage more than a few Cheyenne, as their horses were fresh.
This was the first battle which the Cheyenne fought against the US Army. Casualties were few on each side; J.E.B. Stuart, then a young lieutenant, was shot in the breast while attacking a Cheyenne warrior with a sabre. The troops continued on and two days later burned a hastily abandoned Cheyenne camp; they destroyed lodges and the winter supply of buffalo meat.
Sumner continued to Bent's Fort. To punish the Cheyenne, he distributed their annuities to the Arapaho. He intended further punitive actions, but the Army ordered him to Utah because of an outbreak of trouble with the Mormons (this would be known as the Utah War). The Cheyenne moved below the Arkansas into Kiowa and Comanche country. In the fall, the Northern Cheyenne returned to their country north of the Platte.
Pike's Peak Gold Rush.
Starting in 1859 with the Colorado Gold Rush, European-American settlers moved into lands reserved for the Cheyenne and other Plains Indians. Travel greatly increased along the Emigrant Trail along the South Platte River and some emigrants stopped before going on to California. For several years there was peace between settlers and Indians. The only conflicts were related to the endemic warfare between the Cheyenne and Arapaho of the plains and the Utes of the mountains.
US negotiations with Black Kettle and other Cheyenne favoring peace resulted in the Treaty of Fort Wise: it established a small reservation for the Cheyenne in southeastern Colorado in exchange for the territory agreed to in the Fort Laramie Treaty of 1851. Many Cheyenne did not sign the treaty, and they continued to live and hunt on their traditional grounds in the Smokey Hill and Republican basins, between the Arkansas and the South Platte, where there were plentiful buffalo.
Efforts to make a wider peace continued, but in the spring of 1864, John Evans, governor of Colorado Territory, and John Chivington, commander of the Colorado Volunteers, a citizens militia, began a series of attacks on Indians camping or hunting on the plains. They killed any Indian on sight and initiated the Colorado War. General warfare broke out and Indians made many raids on the trail along the South Platte, which Denver depended on for supplies. The Army closed the road from August 15 until September 24, 1864.
On November 29, 1864, the Colorado Militia attacked a Cheyenne and Arapaho encampment under Chief Black Kettle, although it flew a flag of truce and indicated its allegiance to the US government. The Sand Creek massacre, as it came to be known, resulted in the death of between 150 and 200 Cheyenne, mostly unarmed women and children. The survivors fled northeast and joined the camps of the Cheyenne on the Smokey Hill and Republican rivers. There warriors smoked the war pipe, passing it from camp to camp among the Sioux, Cheyenne and Arapaho.
In January 1865, they planned and carried out an attack with about 1000 warriors on Camp Rankin, a stage station and fort at Julesburg. The Indians made numerous raids along the South Platte, both east and west of Julesburg, and raided the fort again in early February. They captured much loot and killed many European Americans. Most of the Indians moved north into Nebraska on their way to the Black Hills and the Powder River. (See Battle of Julesburg, Battle of Mud Springs, Battle of Rush Creek, Powder River Expedition, Battle of Platte Bridge)
Black Kettle continued to desire peace and did not join in the second raid or in the plan to go north to the Powder River country. He left the large camp and returned with 80 lodges of his tribesmen to the Arkansas River, where he intended to seek peace with the US.
Battle of Washita River.
Four years later, on November 27, 1868, George Armstrong Custer and his troops attacked Black Kettle's band at the Battle of Washita River. Although his band was camped on a defined reservation, complying with the government's orders, some of its members had been linked to raiding into Kansas by bands operating out of the Indian Territory. Custer claimed 103 Cheyenne "warriors" and an unspecified number of women and children killed whereas different Cheyenne informants named between 11 and 18 men (mostly 10 Cheyenne, 2 Arapaho, 1 Mexican trader) and between 17 and 25 women and children killed in the village.
There are conflicting claims as to whether the band was hostile or friendly. Historians believe that Chief Black Kettle, head of the band, was not part of the war party but the peace party within the Cheyenne nation. But, he did not command absolute authority over members of his band and the European Americans did not understand this. When younger members of the band took part in raiding parties, European Americans blamed the entire band for the incidents and casualties..
Battle of the Little Bighorn.
The Northern Cheyenne fought in the Battle of the Little Bighorn, which took place on June 25, 1876. The Cheyenne, together with the Lakota, other Sioux warriors and a small band of Arapaho, killed General George Armstrong Custer and much of his 7th Cavalry contingent of soldiers. Historians have estimated that the population of the Cheyenne, Lakota and Arapaho encampment along the Little Bighorn River was approximately 10,000, making it one of the largest gatherings of Native Americans in North America in pre-reservation times. News of the event traveled across the United States and reached Washington, D.C., just as the nation was celebrating its Centennial. Public reaction arose in outrage against the Cheyenne.
Northern Cheyenne Exodus.
Following the Battle of the Little Bighorn, the US Army increased attempts to capture the Cheyenne. In 1879, after the Dull Knife Fight, when Crazy Horse surrendered at Fort Robinson, a few Cheyenne chiefs and their people surrendered as well. They were Dull Knife, Standing Elk and Wild Hog with around 130 Cheyenne. Later that year Two Moons surrendered at Fort Keogh, with 300 Cheyenne. The Cheyenne wanted and expected to live on the reservation with the Sioux in accordance to an April 29, 1868 treaty of Fort Laramie, which both Dull Knife and Little Wolf had signed.
As part of a US increase in troops following the Battle of the Little Bighorn, the Army reassigned Colonel Ranald S. Mackenzie and his Fourth Cavalry to the Department of the Platte. Stationed initially at Camp Robinson, they formed the core of the Powder River Expedition. It departed in October 1876 to locate the northern Cheyenne villages. On November 25, 1876, his column discovered and defeated a village of Northern Cheyenne in the Dull Knife Fight in Wyoming Territory. After the soldiers destroyed the lodges and supplies, and confiscated the horses, the Northern Cheyenne soon surrendered. They hoped to remain with the Sioux in the north but the US pressured them to locate with the Southern Cheyenne on their reservation in Indian Territory. After a difficult council, the Northern Cheyenne eventually agreed to go South.
When the Northern Cheyenne arrived at Indian Territory, conditions were very difficult: rations were inadequate, there were no buffalo near the reservation and, according to several sources, there was malaria among the people. On 9 September 1878, a portion of the Northern Cheyenne, led by Little Wolf and Dull Knife started their trek back to the north. On reaching the northern area, they split into two bands. That led by Dull Knife (mostly women, children and elders) surrendered and were taken to Fort Robinson, where subsequent events became known as the Fort Robinson tragedy. Dull Knife's group was first offered food and firewood and then, after a week and a half, they were told to go back to Indian territory. When they said no, they were then locked in the wooden barracks with no food, water or firewood for heat for four days. Most escaped in an estimated forty degrees below zero on January 9, 1879, but all were recaptured or killed.
Eventually the US forced the Northern Cheyenne onto a reservation, in southern Montana.
Northern Cheyenne Indian Reservation.
The Cheyenne who traveled to Fort Keogh (present day Miles City, Montana), including Little Wolf, settled near the fort. Many of the Cheyenne worked with the army as scouts. The Cheyenne scouts were pivotal in helping the Army find Chief Joseph and his band of Nez Percé in northern Montana. Fort Keogh became a staging and gathering point for the Northern Cheyenne. Many families began to migrate south to the Tongue River watershed area, where they established homesteads.
The US established the Tongue River Indian Reservation, now named the Northern Cheyenne Indian Reservation, of by the executive order of President Chester A. Arthur November 16, 1884. It excluded Cheyenne who had homesteaded further east near the Tongue River. The western boundary is the Crow Indian Reservation. On March 19, 1900, President William McKinley extended the reservation to the west bank of the Tongue River, making a total of . Those who had homesteaded east of the Tongue River were relocated to the west of the river.
The Northern Cheyenne, who were sharing the Lakota land at Pine Ridge Indian Reservation were finally allowed to return to the Tongue River on their own reservation. Along with the Lakota and Apache, the Cheyenne were the last nations to be subdued and placed on reservations. (The Seminole tribe of Florida never made a treaty with the US government.)
The Northern Cheyenne were given the right to remain in the north, near the Black Hills, land which they consider sacred. The Cheyenne also managed to retain their culture, religion and language. Today, the Northern Cheyenne Nation is one of the few American Indian nations to have control over the majority of its land base, currently 98%.
Culture.
Over the past 400 years, the Cheyenne have changed their lifestyles. In the 16th century, they lived in the regions near the Great Lakes. They farmed corn, squash, and beans, and harvested wild rice like other indigenous peoples of the Northeastern Woodlands. They migrated west in the 18th century and hunted bison on the Great Plains. By the mid-19th century, the US forced them onto reservations.
The traditional Cheyenne government system is a politically unified system. The central traditional government system of the Cheyenne is the Arrow Keeper, followed by the Council of Forty-Four. Early in Cheyenne history, three related tribes, known as the "Heviqsnipahis", the "Só'taeo'o" and the "Masikota", unified themselves to form the "Tsé-tsêhéstâhese" or the "Like Hearted People" who are known today as the "Cheyenne". The unified tribe then divided themselves into ten principal bands:
Each of the ten bands had four seated chief delegates; the remaining four chiefs were the principal advisers of the other delegates. Smaller bands or sub-bands had no right to send delegates to the council. This system also regulated the Cheyenne military societies that developed for planning warfare, enforcing rules, and conducting ceremonies.
Anthropologists debate about Cheyenne society organization. On the plains, it appears that they had a bilateral band kinship system. However, some anthropologists reported that the Cheyenne had a matrilineal band system. Studies into whether, and if so, how much the Cheyenne developed a matrilineal clan system are continuing.
Traditional Cheyenne plains culture.
While they participated in nomadic Plains horse culture, men hunted and occasionally fought with and raided other tribes. The women tanned and dressed hides for clothing, shelter, and other uses. They also gathered roots, berries, and other useful plants. From the products of hunting and gathering, the women also made lodges, clothing, and other equipment. Their lives were active and physically demanding. The range of the Cheyenne was first the area in and near the Black Hills, but later all the Great Plains from Dakota to the Arkansas River.
Role models.
A Cheyenne woman has a higher status if she is part of an extended family with distinguished ancestors. Also, if she is friendly and compatible with her female relatives and does not have members in her extended family who are alcoholics or otherwise in disrepute. It is expected of all Cheyenne women to be hardworking, chaste, modest, skilled in traditional crafts, knowledgeable about Cheyenne culture and history and speak Cheyenne fluently. Tribal powwow princesses are expected to have these characteristics.
Ethnobotany.
An infusion of the pulverized leaves and blossoms of tansy is used for dizziness and weakness.

</doc>
<doc id="56129" url="https://en.wikipedia.org/wiki?curid=56129" title="Noetherian ring">
Noetherian ring

In mathematics, more specifically in the area of abstract algebra known as ring theory, a Noetherian ring is a ring that satisfies the ascending chain condition on ideals; that is, given any chain of ideals:
there exists an "n" such that:
There are other equivalent formulations of the definition of a Noetherian ring and these are outlined later in the article. 
Noetherian rings are named after Emmy Noether. 
The notion of a Noetherian ring is of fundamental importance in both commutative and noncommutative ring theory, due to the role it plays in simplifying the ideal structure of a ring. For instance, the ring of integers and the polynomial ring over a field are both Noetherian rings, and consequently, such theorems as the Lasker–Noether theorem, the Krull intersection theorem, and the Hilbert's basis theorem hold for them. Furthermore, if a ring is Noetherian, then it satisfies the descending chain condition on "prime ideals". This property suggests a deep theory of dimension for Noetherian rings beginning with the notion of the Krull dimension.
Characterizations.
For noncommutative rings, it is necessary to distinguish between three very similar concepts:
For commutative rings, all three concepts coincide, but in general they are different. There are rings that are left-Noetherian and not right-Noetherian, and vice versa.
There are other, equivalent, definitions for a ring "R" to be left-Noetherian:
Similar results hold for right-Noetherian rings.
For a commutative ring to be Noetherian it suffices that every prime ideal of the ring is finitely generated. (The result is due to I. S. Cohen.)
Examples.
Rings that are not Noetherian tend to be (in some sense) very large. Here are some examples of non-Noetherian rings:
However, a non-Noetherian ring can be a subring of a Noetherian ring. Since any integral domain is a subring of a field, any integral domain that is not Noetherian provides an example. To give a less trivial example, 
Indeed, there are rings that are right Noetherian, but not left Noetherian, so that one must be careful in measuring the "size" of a ring this way. For example, if "L" is a subgroup of Q2 isomorphic to Z, let "R" be the ring of homomorphisms "f" from Q2 to itself satisfying "f"("L") ⊂ "L". Choosing a basis, we can describe the same ring "R" as
This ring is right Noetherian, but not left Noetherian; the subset "I"⊂"R" consisting of elements with "a"=0 and "γ"=0 is a left ideal that is not finitely generated as a left "R"-module.
If "R" is a commutative subring of a left Noetherian ring "S", and "S" is finitely generated as a left "R"-module, then "R" is Noetherian. (In the special case when "S" is commutative, this is known as Eakin's theorem.) However this is not true if "R" is not commutative: the ring "R" of the previous paragraph is a subring of the left Noetherian ring "S" = Hom(Q2,Q2), and "S" is finitely generated as a left "R"-module, but "R" is not left Noetherian.
A unique factorization domain is not necessarily a noetherian ring. It does satisfy a weaker condition: the ascending chain condition on principal ideals.
A valuation ring is not Noetherian unless it is a principal ideal domain. It gives an example of a ring that arises naturally in algebraic geometry but is not Noetherian.
Primary decomposition.
In the ring Z of integers, an arbitrary ideal is of the form ("n") for some integer "n" (where ("n") denotes the set of all integer multiples of "n"). If "n" is non-zero, and is neither 1 nor −1, by the fundamental theorem of arithmetic, there exist primes "pi", and positive integers "ei", with formula_6. In this case, the ideal ("n") may be written as the intersection of the ideals ("piei"); that is, formula_7. This is referred to as a "primary decomposition" of the ideal ("n"). 
In general, an ideal "Q" of a ring is said to be "primary" if "Q" is proper and whenever "xy" ∈ "Q", either "x" ∈ "Q" or "yn" ∈ "Q" for some positive integer "n". In Z, the primary ideals are precisely the ideals of the form ("pe") where "p" is prime and "e" is a positive integer. Thus, a primary decomposition of ("n") corresponds to representing ("n") as the intersection of finitely many primary ideals. 
Since the fundamental theorem of arithmetic applied to a non-zero integer "n" that is neither 1 nor −1 also asserts uniqueness of the representation formula_6 for "pi" prime and "ei" positive, a primary decomposition of ("n") is essentially "unique". 
For all of the above reasons, the following theorem, referred to as the "Lasker–Noether theorem", may be seen as a certain generalization of the fundamental theorem of arithmetic:
Lasker-Noether Theorem. Let "R" be a commutative Noetherian ring and let "I" be an ideal of "R". Then "I" may be written as the intersection of finitely many primary ideals with distinct radicals; that is:
with "Qi" primary for all "i" and Rad("Qi") ≠ Rad("Qj") for "i" ≠ "j". Furthermore, if:
is decomposition of "I" with Rad("Pi") ≠ Rad("Pj") for "i" ≠ "j", and both decompositions of "I" are "irredundant" (meaning that no proper subset of either {"Q"1, ..., "Qt"} or {"P"1, ..., "Pk"} yields an intersection equal to "I"), "t" = "k" and (after possibly renumbering the "Qi") Rad("Qi") = Rad("Pi") for all "i".
For any primary decomposition of "I", the set of all radicals, that is, the set {Rad("Q"1), ..., Rad("Qt")} remains the same by the Lasker–Noether theorem. In fact, it turns out that (for a Noetherian ring) the set is precisely the assassinator of the module "R"/"I"; that is, the set of all annihilators of "R"/"I" (viewed as a module over "R") that are prime.

</doc>
<doc id="56130" url="https://en.wikipedia.org/wiki?curid=56130" title="Artinian">
Artinian

Artinian may refer to:

</doc>
<doc id="56133" url="https://en.wikipedia.org/wiki?curid=56133" title="Battle Angel Alita">
Battle Angel Alita

Battle Angel Alita, known in Japan as , is a manga series created by Yukito Kishiro in 1990 and originally published in Shueisha's "Business Jump" magazine. Two of the nine-volume comics were adapted into two anime original video animation episodes titled "Battle Angel" for North American release by ADV Films and the UK and Australian release by Manga Entertainment. Manga Entertainment also dubbed "Battle Angel Alita" into English.
The series is set in the post-apocalyptic future and focuses on Alita, a cyborg who has lost all memories and is found in a garbage heap by a cybernetics doctor who rebuilds and takes care of her. She discovers that there is one thing she remembers, the legendary cyborg martial art Panzer Kunst, which leads to her becoming a Hunter Warrior or bounty hunter. The story traces Alita's attempts to rediscover her past and the characters whose lives she impacts on her journey. The manga series is continued in ' and '.
Plot.
"Battle Angel Alita" tells the story of Alita ("Gally" in the original Japanese version), an amnesiac female cyborg. Her intact head and chest, in suspended animation, are found by cybermedic expert Daisuke Ido in the local dump. Ido manages to revive her, and finding she has lost her memory, names her Alita after his deceased cat. The rebuilt Alita soon discovers that she remembers the legendary martial art Panzer Kunst, although she does not recall anything else. Alita uses her Panzer Kunst to first become a mercenary Hunter-Warrior, killing cyborg criminals in the Scrapyard, and then as a player in the brutal sport of Motorball. While in combat, Alita awakens memories of her earlier life on Mars. She becomes involved with the floating city of Tiphares as one of their agents, and is sent to hunt criminals down. Foremost is the mad genius Desty Nova, who clashes with Alita before becoming her ally.
The futuristic dystopian world of "Battle Angel Alita" revolves around the city of Scrapyard, grown up around a massive scrap heap that rains down from Tiphares (Salem in the anime). Ground dwellers have no access to Tiphares and are forced to make a living in the sprawl below. Many are heavily modified by cybernetics to better cope with their hard life.
Tiphares exploits the Scrapyard and surrounding farms, paying mercenaries (called Hunter-Warriors) to hunt criminals and arranging violent sports to keep the population entertained. Massive tubes connect the Scrapyard to Tiphares, and the city uses robots for carrying out errands and providing security on the ground. Occasionally, Tiphareans (such as Ido Daisuke and Desty Nova) are exiled and sent to the ground. Aside from the robots and exiles, there is little contact between the two cities.
The story takes place in the former United States. According to a map, printed in the eighth volume, Scrapyard/Tiphares is near Kansas City, Missouri, and the Necropolis is Colorado Springs, Colorado. Radio KAOS is at Dallas. Figure's coastal hometown is Alhambra. Desty Nova's Granite Inn is built out of a military base – NORAD at Cheyenne Mountain Complex, Colorado.
"Battle Angel Alita" is eventually revealed to take place in the 26th century. The sequel "Battle Angel Alita: Last Order" introduces a calendar era called "Era Sputnik" which has en epoch of AD 1957. The original "Battle Angel Alita" series begins in es. 577 (AD 2533) and ends in es. 590 (AD 2546), "Battle Angel Alita: Last Order" is mostly set in es. 591 (AD 2547), and "Gunnm Mars Chronicle" currently alternates between es. 370 (AD 2326) and es. 594 (AD 2550).
Characters.
"Battle Angel Alita" features a diverse cast of characters, many of whom shift in and out of focus as the story progresses. Some are never to be seen again following the conclusion of a story arc, while others make recurring appearances. The one character who remains a constant throughout is Alita, the protagonist and title character, a young cyborg with amnesia struggling to uncover her forgotten past through the only thing she remembers from it: by fighting. Early on in the story, Daisuke Ido, a bounty-hunting cybernetic doctor who finds and revives Alita plays a major role as well, but midway through the manga he becomes marginalized as focus begins to increasingly shift to Desty Nova, an eccentric nanotechnology scientist who has fled from Tiphares. Nova is the mastermind behind many of the enemies and trials that Alita faces, but does not make an actual appearance until more than two years into the story, although he is alluded to early on. Finally, Kaos, Desty Nova's son, a frail and troubled radio DJ with psychometric powers, also begins to play a crucial role after he comes in contact with Alita. He broadcasts his popular radio show from the wastelands outside the Scrapyard, staying away from the increasing conflict between Tiphares and the rebel army Barjack.
Production.
Besides renaming "Gally" to "Alita", the North American version of the manga also changed the city of "Salem" to "Tiphares", after Tiferet. Since Kishiro also used the name "Jeru" for the facility atop "Salem", "Jeru" was renamed "Ketheres" in the translation, after Keter. To further develop the Biblical theme in the original series, "Salem"'s main computer was named "Melchizedek", "the king of Salem" and "priest to the Most High God".
Media.
Manga.
The manga was first published in Shueisha's "Business Jump" magazine. It was then serialized from 1990 to 1995 in nine "tankōbon". In the U.S., Viz originally released the story in a 25-page comic book, it then followed the same volume format as its Japanese counterpart. "Battle Angel Alita" was licensed for international release in a number of languages and regions. It was published in Spain by Planeta DeAgostini, in Brazil by Editora JBC, in France and Netherlands by Glenat, in Poland by JPF, in Germany by Carlsen and in Taiwan by Tong Li Publishing.
Another series titled was published in "Ultra Jump" from January 24, 1997 to December 19, 2006. It was released in a single volume on December 19, 2007. It is composed of four short side stories: "Home", "Christmas Eve Serenade", "Sonic Finger", and "Barjack Rhapsody".
A 6-volume special edition titled "Gunnm: Complete Edition" was released in Japan on December 23, 1998. The series was released in B5 format and contains the original story, but with a different ending accommodating for the continuation of the story in "". Also Included are rough sketches, a timeline and the first three "Gunnm Another Stories" short stories.
A novel was released on April 4, 1997, by JUMP j-BOOKS, as part of the Japanese publisher Shueisha.
OVA.
A two episode OVA was released in 1993, incorporating elements from the first two volumes of the manga with changes to the characters and storyline. According to Kishiro, only two episodes were originally planned. At the time, he was too busy with the manga "to review the plan coolly", nor was he serious about an anime adaptation. It remains the only anime adaptation of "Battle Angel Alita" to date and there are no plans to revive it.
A 3-minute 3D-CGI rendered movie clip is included in volume 6 of the Japanese "Gunnm: Complete Edition". It showcases Alita in a Third League Motorball race with players from two of her races such as "Armor" Togo, Degchalev, and Valdicci, and depicts events from both of those races.
Film.
Director James Cameron has rights to the film adaptation of "Battle Angel". Cameron is said to be a big fan of the manga, and he was waiting until CGI technology was sufficiently advanced to make a live-action 3D film with effects comparable to "Avatar". However, he stated he would work on "Avatar" sequels before starting Alita.
Cameron's producer Jon Landau says, "I am sure you will get to see "Battle Angel". It is one of my favourite stories, a great story about a young woman's journey to self-discovery. It is a film that begs the question: What does it mean to be human? Are you human if you have a heart, a brain or a soul? I look forward to giving the audience the film." It will likely not hit screens before 2017. Landau half-jokingly stated that the project may be titled "Alita: The Battle Angel", because of Cameron's tradition in naming his films with either an "A" or a "T".
Cameron's film would be a live-action adaptation of the first four volumes of the manga series; "What I’m going to do is take the spine story and use elements from the first four books. So, the Motorball from books three and four, and parts of the story of one and two will all be in the movie". He has also stated that he has no one in mind for casting yet. 
In October 2015, it was reported that Robert Rodriguez will direct the film with Cameron and Landau producing. On April 26, 2016, both The Hollywood Reporter and Variety reports that Maika Monroe, Rosa Salazar, Zendaya Coleman and Bella Thorne are in the running for the lead role.
Video game.
' is an action RPG video game for the PlayStation by Banpresto. It is an adaptation of the manga, following Alita (Gally) from her discovery in the Tiphares dump heap by Daisuke Ido up through and beyond her career as a TUNED agent. The story includes additional elements that Kishiro had conceived when he ended the original manga in 1995, but was unable to implement at the time, which involved Alita going into outer space. He then expanded the story, which formed the basis for the manga '.

</doc>
<doc id="56134" url="https://en.wikipedia.org/wiki?curid=56134" title="List of U.S. state songs">
List of U.S. state songs

Forty-nine of the fifty U.S. states that make up the United States of America have one or more state songs, which are selected by each state legislature, and/or state governor, as a symbol (or emblem) of that particular U.S. state. New Jersey does not have an official state song, while Virginia's previous state song, "Carry Me Back to Old Virginny", adopted in 1940, was rescinded due to its racist language by the Virginia General Assembly. In 2015, "Our Great Virginia" was made the new state song of Virginia.
Some U.S. states have more than one official state song, and may refer to some of their official songs by other names; for example, Arkansas officially has two state songs, plus a state anthem, and a state historical song. Arizona has a song that was written specifically as a state anthem in 1915, as well as the 1981 country hit "Arizona", which it adopted as the alternate state anthem in 1982.
Two individuals, Stephen Foster, and John Denver, have written or co-written two state songs. Foster's two state songs, "Old Folks at Home" (better known as "Swanee Ribber" or "Suwannee River") (for adopted by Florida), and "My Old Kentucky Home" are among the best-known songs in the U.S. On March 12, 2007, the Colorado Senate passed a resolution to make Denver's trademark 1972 hit "Rocky Mountain High" one of the state's two official state songs, sharing duties with its predecessor, "Where the Columbines Grow". On March 7, 2014, the West Virginia Legislature approved a resolution to make Denver's "Take Me Home, Country Roads" the official state song of West Virginia. Governor Earl Ray Tomblin signed the resolution into law on March 8, 2014.
Other well-known state songs include "Yankee Doodle", "You Are My Sunshine", "Rocky Top", and "Home on the Range"; a number of others are popular standards, including "Oklahoma!" (from the Rodgers and Hammerstein musical), Hoagy Carmichael's "Georgia on My Mind", "Tennessee Waltz", "Missouri Waltz", and "On the Banks of the Wabash, Far Away". Many of the others are much less well-known, especially outside the state.
Territories.
Some American overseas territories, although not U.S. states, have songs and marches of their own.

</doc>
<doc id="56135" url="https://en.wikipedia.org/wiki?curid=56135" title="Touchstone (assaying tool)">
Touchstone (assaying tool)

A touchstone is a small tablet of dark stone such as fieldstone, slate, or lydite, used for assaying precious metal alloys. It has a finely grained surface on which soft metals leave a visible trace.
History.
The touchstone was used in ancient Greece. Its role in the introduction of monetary economy was explored by science historian James Burke in the second episode of his 1978 BBC television series "Connections". It was also used by the Indus Valley Civilization about 3500 BCE for testing the purity of soft metals.
Usage.
Drawing a line with gold on a touchstone will leave a visible trace. Because different alloys of gold have different colours (see gold) the unknown sample can be compared to samples of known purity. This method has been used since ancient times. In modern times, additional tests can be done. The trace will react in different ways to specific concentrations of nitric acid or aqua regia, thereby identifying the quality of the gold. Thus, 24 carat gold is not affected but 14 carat gold will show chemical activity.

</doc>
<doc id="56136" url="https://en.wikipedia.org/wiki?curid=56136" title="Yukito Kishiro">
Yukito Kishiro


</doc>
<doc id="56139" url="https://en.wikipedia.org/wiki?curid=56139" title="Butanol">
Butanol

Butanol (also called butyl alcohol) is a four-carbon alcohol with a formula of C4H9OH, which occurs in four isomeric structures, from a straight-chain primary alcohol to a branched-chain tertiary alcohol; all are a butyl or isobutyl group linked to a hydroxyl group (sometimes represented as BuOH, "n"-BuOH, and "i"-BuOH). These are "n"-butanol, 2-butanol, "tert"-butanol, and isobutanol. Butanol is primarily used as a solvent, as an intermediate in chemical synthesis, and as a fuel. It is sometimes also called biobutanol when produced biologically.
Isomers.
The unmodified term "butanol" usually refers to the straight chain isomer with the alcohol functional group at the terminal carbon, which is also known as "n"-butanol or 1-butanol. The straight chain isomer with the alcohol at an internal carbon is "sec"-butanol or 2-butanol. The branched isomer with the alcohol at a terminal carbon is isobutanol or 2-methyl-1-propanol, and the branched isomer with the alcohol at the internal carbon is "tert"-butanol or 2-methyl-2-propanol.
The butanol isomers have different melting and boiling points. "n"-Butanol and isobutanol have limited solubility, "sec"-butanol has substantially greater solubility, while "tert"-butanol is fully miscible with water. The hydroxyl group makes the molecule polar, promoting solubility in water, while the longer hydrocarbon chain mitigates the polarity and reduces solubility. The shorter chain molecules of methanol, ethanol, propanol, and tert-butanol are fully miscible with water, while n-butanol is only moderately soluble because of the diminishing polarity in the longer hydrocarbon group.
Toxicity.
Like many alcohols, butanol is considered toxic. It has shown low order of toxicity in single dose experiments to laboratory animals. and is considered safe enough for use in cosmetics. Brief, repeated overexposure with the skin can result in depression of the central nervous system, as with other short-chain alcohols. Exposure may also cause severe eye irritation and moderate skin irritation. The main dangers are from prolonged exposure to fumes. In extreme cases this includes suppression of the central nervous system and even death. Under most circumstances, butanol is quickly metabolized to carbon dioxide. It has not been shown to damage DNA or cause cancer.
Uses.
Biobutanol.
Butanol is considered as a potential biofuel (butanol fuel). Butanol at 85 percent strength can be used in cars designed for gasoline (petrol) without any change to the engine (unlike 85% ethanol), and it contains more energy for a given volume than ethanol and almost as much as gasoline, and a vehicle using butanol would return fuel consumption more comparable to gasoline than ethanol. Butanol can also be added to diesel fuel to reduce soot emissions.
Other uses.
Butanol is used as a solvent for a wide variety of chemical and textile processes, in organic synthesis, and as a chemical intermediate. It is also used as a paint thinner and a solvent in other coating applications where a relatively slow evaporating latent solvent is preferable, as with lacquers and ambient-cured enamels. It is also used as a component of hydraulic and brake fluids.
Butanol is used in the synthesis of 2-butoxyethanol. A major application for Butanol is as a reactant with Acrylic Acid to produce butyl acrylate, a primary ingredient of water based acrylic paint.
It is also used as a base for perfumes, but on its own has a highly alcoholic aroma.
Salts of butanol are chemical intermediates; for example, alkali metal salts of "tert"-butanol are "tert"-butoxides.
Production.
Since the 1950s, most butanol in the United States is produced commercially from fossil fuels. The most common process starts with propene (propylene), which is put through a hydroformylationreaction to form butyraldehyde, which is then reduced with hydrogen to 1-butanol and/or 2-butanol. Tert-butanol is derived from isobutane as a co-product of propylene oxide production. Butanol can also be produced by fermentation of biomass by bacteria. Prior to the 1950s, "Clostridium acetobutylicum" was used in industrial fermentation to produce butanol. Research in the past few decades showed results of other microorganisms that can produce butanol through fermentation.

</doc>
<doc id="56140" url="https://en.wikipedia.org/wiki?curid=56140" title="History of Norway">
History of Norway

The history of Norway has been influenced to an extraordinary degree by the terrain and the climate of the region. About 10,000 BC, following the retreat of the great inland ice sheets, the earliest inhabitants migrated north into the territory which is now Norway. They traveled steadily northwards along the coastal areas, warmed by the Gulf Stream, where life was more bearable. In order to survive they fished and hunted reindeer (and other prey). Between 5,000 BC and 4,000 BC the earliest agricultural settlements appeared around the Oslofjord. Gradually, between 1500 BC and 500 BC, these agricultural settlements spread into the southern areas of Norway - whilst the inhabitants of the northern regions continued to hunt and fish.
The Neolithic period started 4000 BC. The Migration Period caused the first chieftains to take control and the first defenses to be made. From the last decades of the 8th century Norwegians started expanding across the seas to the British Isles and later Iceland and Greenland. The Viking Age also saw the unification of the country. Christianization took place during the 11th century and Nidaros became an archdiocese. The population expanded quickly until 1349 (Oslo: 3,000; Bergen: 7,000; Trondheim: 4,000) when it was halved by the Black Death and successive plagues. Bergen became the main trading port, controlled by the Hanseatic League. Norway entered the Kalmar Union with Denmark and Sweden in 1397.
After Sweden left the union in 1523, Norway became the junior partner in Denmark–Norway. The Reformation was introduced in 1537 and absolute monarchy imposed in 1661. In 1814 Norway was ceded from Denmark to Sweden and a constitution was passed. Norway declared its independence but was then occupied by Sweden, although the Parliament was allowed to continue to exist. Industrialization started in the 1840s and from the 1860s large-scale emigration to North America took place. In 1884 the king appointed Johan Sverdrup as prime minister, thus establishing parliamentarism. The union with Sweden was dissolved in 1905. From the 1880s to the 1920s, Norwegians such as Roald Amundsen carried out a series of important polar expeditions.
Shipping and hydroelectricity were important sources of income for the country. The following decades saw a fluctuating economy and the rise of the labor movement. Germany occupied Norway between 1940 and 1945 during the Second World War, after which Norway joined NATO and underwent a period of reconstruction under public planning. Oil was discovered in 1969 and by 1995 Norway was the world's second-largest exporter. This resulted in a large increase of wealth. From the 1980s Norway started deregulation in many sectors and experienced a banking crisis.
Prehistory.
Norway's coastline rose from glaciation with the end of the last glacial period about 12,000 BC. The first immigration took place during this period as the Norwegian coast offered good conditions for sealing, fishing and hunting. They were nomadic and by 9300 BC they were at Magerøya. Increased ice receding from 8000 BC caused settlement along the entire coastline. The Stone Age consisted of the Komsa culture in Troms and Finnmark and the Fosna culture further south. The Nøstvet culture took over from the Fosna culture ca. 7000 BC, which adapted to a warmer climate which gave increased forestation and new mammals for hunting. The oldest human skeleton ever discovered in Norway was found in shallow water off Sogne in 1994 and has been carbon dated to 6,600 BC. Ca. 4000 BC people in the north started using slate tools, earthenware, skis, sleds and large skin boats.
The first farming and thus the start of the Neolithic period, began ca. 4000 BC around the Oslofjord, with the technology coming from southern Scandinavia. The break-through occurred between 2900 and 2500 BC, when oats, barley, pigs, cattle, sheep and goats became common and spread as far north as Alta. This period also saw the arrival of the Corded Ware culture, who brought new weapons, tools and an Indo-European dialect, from which the Norwegian language developed.
Nordic Bronze Age (1800–500 BC).
The Bronze Age started in 1800 BC and involved innovations such as plowing fields with ards, permanents farms with houses and yards, especially in the fertile areas around the Oslofjord, Trondheimsfjord, Mjøsa and Jæren. Some yields were so high that it allowed farmers to trade furs and skins for luxury items, especially with Jutland. About 1000 BC, speakers of Uralic languages arrived in the north and assimilated with the indigenous population, becoming the Sami people. However, according to Ante Aikio the immigration that brought the Sami languages to Sweden and Norway happened no earlier than about 500 AD.
A climate shift with colder weather started about 500 BC. The forests, which had previously consisted of elm, lime, ash and oak, were replaced with birch, pine and spruce. The climate changes also meant that farmers started building more structures for shelter. Knowledge of ironworking was introduced from the Celts, resulting in better weapons and tools.
Nordic Iron Age (500 BC–800 AD).
The Iron Age allowed for easier cultivation and thus new areas were cleared as the population grew with the increased harvests. A new social structure evolved: when sons married, they would remain in the same house; such an extended family was a clan. They would offer protection from other clans; if conflicts arose, the issue would be decided at a "thing", a sacred place where all freemen from the surrounding area would assemble and could determine punishments for crimes, such as paying fines in food.
From the first century AD a cultural influence from the Roman Empire took place. Norwegians adapted letters and created their own alphabet, runes. Trading with Romans also took place, largely furs and skins in exchange for luxury goods. Some Scandinavians also served as Roman mercenaries. Some of the most powerful farmers became chieftains. They functioned as priests and accepted sacrifices from farmers which were again used to pay soldiers, creating a hird. Thus they were able to rule an area of several settlements and tribes.
The chieftains' power increased during the Migration Period between 400 to 550 as other Germanic tribes migrated northwards and local farmers wanted protection. This also resulted in the construction of simple fortifications. A plague hit southern Norway in the 6th century, with hundreds of farms being depopulated. Most were repopulated in the 7th century, which also saw the construction of several fishing hamlets and a boom in trade of iron and soapstone across the North Sea. Some chieftains were able to control most of the trade and grew in power throughout the 8th century.
Viking Age.
The Viking Age was a period of Scandinavian expansion through trade, colonization and raids. The first raid was against Lindisfarne in 793 and is considered the beginning of the Viking Age. This was possible because of the development of the longship, suitable for travel across the sea, and advanced navigation techniques.
Vikings were well equipped, had chain mail armor, were well trained and had a psychological advantage over Christian counterparts since they believed that being killed in combat would result in them going to Valhalla. In addition to gold and silver, an important outcome from the raids were thralls, which were brought to the Norwegian farms as a slave workforce. While the men were out at sea, the management of the farm was under the control of the women.
The lack of suitable farming land in Western Norway caused Norwegians to travel to the sparsely populated areas such as Shetland, Orkney, the Faroe Islands and the Hebrides to colonize—the latter which became the Kingdom of the Isles. Norwegian Vikings settled on the west coast of Ireland ca. 800 and founded the island's first cities, including Dublin. Their arrival caused the petty Celtic kings to ally, and by 900 they had driven out the Norwegians.
Norwegians discovered Iceland in ca. 870 and within sixty years the island had been divided among four hundred chieftains. Led by Erik the Red, a group of Norwegians settled on Greenland in the 980s. His son, Leif Ericson, discovered Newfoundland in ca. 1000, naming it Vinland. Unlike Greenland, no permanent settlement was established there.
In the mid-9th century the largest chieftains of the petty kingdoms started a major power struggle. Harald Fairhair started the process of unifying Norway when he entered an alliance with the Earls of Lade and was able to unify the country after the decisive Battle of Hafrsfjord. He set up the very basics of a state administration with stewards in the most important former chieftain estates. His son Håkon the Good, who assumed the crown in 930, established two large things, Gulating for Western Norway and Frostating for Trøndelag, in which the king met with the freemen to make decisions. He also established the ledang, a conscription-based military. After his death in 960, war broke out between the Fairhair dynasty and the Earls of Lade in alliance with Danish kings.
Middle Ages.
Christianization and abolishing the rites in Norse mythology was first attempted by Olav Tryggvason, but he was killed in the Battle of Svolder in 1000. Olav Haraldsson, starting in 1015, made the "things" pass church laws, destroyed heathen hofs, built churches and created an institution of priests. Many chieftains feared that the Christianization would rob them of power in lieu of their roles as "Goðar" in traditional Norse Paganism. The two sides met in the Battle of Stiklestad, where Haraldsson was killed. The church elevated Haraldsson to sainthood, allowing Nidaros (today Trondheim) to become the Christian center of Norway. Within a few years the Danish rule had become sufficiently unpopular that Norway again became united.
From the 1040s to 1130 the country was at peace. In 1130 the civil war era broke out on the basis of unclear succession laws, which allowed all the king's sons to rule jointly. For periods there could be peace, before a lesser son allied himself with a chieftain and started a new conflict. The Archdiocese of Nidaros was created in 1152 and attempted to control the appointment of kings. The church inevitably had to take sides in the conflicts, with the civil wars also becoming an issue regarding the church's influence of the king. The wars ended in 1217 with the appointment of Håkon Håkonsson, who introduced clear law of succession.
From 1000 to 1300 the population increased from 150,000 to 400,000, resulting both in more land being cleared and the subdivision of farms. While in the Viking Age all farmers owned their own land, by 1300 seventy percent of the land was owned by the king, the church, or the aristocracy. This was a gradual process which took place because of farmers borrowing money in poor times and not being able to repay. However, tenants would always remain free men and the large distances and often scattered ownership meant that they enjoyed much more freedom than their continental peers. In the 13th century about twenty percent of a farmer's yield went to the king, church and landowners.
14th century.
The 14th century is described as Norway's Golden Age, with peace and increase in trade, especially with the British Islands, although Germany became increasingly important towards the end of the century. Throughout the High Middle Ages the king established Norway as a state with a central administration with local representatives.
In 1349 the Black Death spread to Norway and had within a year killed a third of the population. Later plagues reduced the population to half the starting point by 1400. Many communities were entirely wiped out, resulting in an abundance of land, allowing farmers to switch to more animal husbandry. The reduction in taxes weakened the king's position, and many aristocrats lost the basis for their surplus, reducing some to mere farmers. High tithes to church made it increasingly powerful and the archbishop became a member of the Council of State.
The Hanseatic League took control over Norwegian trade during the 14th century and established a trading center in Bergen. In 1380 Olaf Haakonsson inherited both the Norwegian and Danish thrones, creating a union between the two countries. In 1397, under Margaret I, the Kalmar Union was created between the three Scandinavian countries. She waged war against the Germans, resulting in a trade blockade and higher taxation on Norwegians, which resulted in a rebellion. However, the Norwegian Council of State was too weak to pull out of the union.
Margaret pursued a centralising policy which inevitably favoured Denmark, because it had a greater population than Norway and Sweden combined. Margaret also granted trade privileges to the Hanseatic merchants of Lübeck in Bergen in return for recognition of her right to rule, and these hurt the Norwegian economy. The Hanseatic merchants formed a state within a state in Bergen for generations. Even worse were the pirates, the "Victual Brothers", who launched three devastating raids on the port (the last in 1427).
Norway slipped ever more to the background under the Oldenburg dynasty (established 1448). There was one revolt under Knut Alvsson in 1502. Norwegians had some affection for King Christian II, who resided in the country for several years. Norway took no part in the events which led to Swedish independence from Denmark in the 1520s.
Denmark–Norway.
Sweden was able to pull out of the Kalmar Union in 1523, thus creating Denmark–Norway under the rule of a king in Copenhagen. Frederick I of Denmark favoured Martin Luther's Reformation, but it was not popular in Norway, where the Church was the one national institution and the country was too poor for the clergy to be very corrupt. Initially, Frederick agreed not to try to introduce Protestantism to Norway but in 1529 he changed his mind. Norwegian resistance was led by Olav Engelbrektsson, Archbishop of Trondheim, who invited the old king Christian II back from his exile in Holland. Christian returned but was ambushed and spent the rest of his life in prison. Then Frederick died and a three-way war of succession broke out between the supporters of his eldest son Christian (III), his younger Catholic brother Hans and the followers of Christian II. Olaf Engelbrektsson again tried to lead a Catholic Norwegian resistance movement but he found little support. Christian III triumphed and sent him into exile and in 1536 Christian demoted Norway from a kingdom to a mere Danish province. The Reformation was imposed in 1537, strengthening the king's power. All church valuables were sent to Copenhagen and the forty percent of the land, which was owned by the church, came under the control of the king. Danish was introduced as a written language, although Norwegian remained distinct dialects. Professional administration was now needed and power shifted from the provincial nobility to the royal administration: district stipendiary magistrates were appointed as judges and the sheriffs became employees of the crown rather than of the local nobility. In 1572 a governor-general was appointed for Norway with a seat at Akershus Fortress in Oslo. From the 1620s professional military officers were employed.
The 17th century saw a series of wars between Denmark–Norway and Sweden. The Kalmar War between 1611–13 saw 8,000 Norwegian peasants conscripted. Despite lack of training, Denmark–Norway won and Sweden abandoned its claims to the area between Tysfjord and Varangerfjord. With the Danish participation in the Thirty Years' War in 1618–48, a new conscription system was created in which the country was subdivided into 6,000 "ledg", each required to support one soldier. Denmark–Norway lost the war and was forced to cede Jämtland and Härjedalen to Sweden. The Second Northern War in 1657 to 1660 resulted in Bohuslän being ceded to Sweden. The Danish monarchy became an absolutist and hereditary one in Norway in 1661. A new administrative system was introduced. Departments organized by portfolio were established in Copenhagen, while Norway was divided into counties, each led by a district governor, and further subdivided into bailiwicks. About 1,600 government officials were appointed throughout the country. Ulrik Fredrik Gyldenløve was the most famous viceroy of Norway (1664-1699).
The population of Norway increased from 150,000 in 1500 to 900,000 in 1800. By 1500 most deserted farms were repossessed. The period under absolutism increased the ratio of self-owning farmers from twenty to fifty percent, largely through sales of crown land to fiance the lost wars. Crofts became common in the absolutism period, especially in Eastern Norway and Trøndelag, with the smallholder living at the mercy of the farmer. There were 48,000 smallholders in 1800. Compared to Denmark, taxes were very low in Norway, typically at four to ten percent of the harvest, although the number of farms per "legd" decreased from four to two in the 1670s. Confirmation was introduced in 1736; as it required people to read, elementary education was introduced. The Norwegian economy improved with the introduction of the water-driven saw in the early 16th century. Norway had huge resources of timber but did not have the means to exploit much of it in the Middle Ages as only hand-tools were available. The new saw mills which sprang up in the fjords changed this. In 1544 a deal was struck with the Netherlands (then part of the Holy Roman Empire) and the Dutch controlled the export of Norwegian timber for the next 150 years. Amsterdam was built on piles from Norway. Tree-felling was done in the winter when farm-work was impossible and it was easy to get the felled trees across the snow to the rivers. In the spring, the logs floated down the rivers to the saw mills by the sea. By the mid-16th century the power of the Hanseatic League in Bergen was broken; though German craftsmen remained, they had to accept Danish rule. Many Norwegians earned a living as sailors in foreign ships, especially Dutch ones. The crews in both sides of the Anglo-Dutch Wars contained Norwegians. Norway benefitted from the many European wars of the 18th century. As a neutral power it was able to expand its share of the shipping market. It also supplied timber to foreign navies.
The entire period saw mercantilism as the basis for commerce, which involved import regulations and tariffs, monopolies and privileges throughout the county granted to burghers. The lumber industry became important in the 17th century through exports especially to England. To avoid deforestation, a royal decree closed a large number of sawmills in 1688; because this mostly affected farmers with small mills, by the mid 18th century only a handful of merchants controlled the entire lumber industry. Mining increased in the 17th century, the largest being the silver mines in Kongsberg and the copper mines in Røros. Fishing continued to be an important income for farmers along the coast, but from the 18th century dried cod started being salted, which required fishermen to buy salt from merchants. The first important period of Norwegian shipping was between 1690 and 1710, but the advantage was lost with Denmark–Norway entering the Great Northern War in 1709. However, Norwegian shipping regained its strength towards the end of the century.
Throughout the period, Bergen was the largest town in the country; its population of 14,000 in the mid 18th century was twice the size of Christiania (later Oslo) and Trondheim combined. Eight townships with privileges existed in 1660—by 1800 this had increased to twenty-three. During this period up to two-thirds of the country's audited national income was transferred to Copenhagen. In the last decades of the century, Hans Nielsen Hauge started the Haugean movement, which demanded the right to preach the word of God freely. The University of Oslo was established in 1811.
Union with Sweden.
Denmark–Norway entered the Napoleonic Wars on France's side in 1807. This had a devastating effect on the Norwegian economy as the Royal Navy hindered export by ship and import of food. Sweden invaded Norway the following year, but after several Norwegian victories a cease-fire was signed in 1809. After pressure from Norwegian merchants license trade was permitted with corn from Denmark to Eastern Norway in exchange for Norwegian timber export to the United Kingdom. Following the Battle of Leipzig in 1813, the Treaty of Kiel signed on 14 January 1814 ceded Norway to Sweden.
Christian Frederik, heir to the Danish crown, had since 1813 been governor-general of Norway. He traveled to Trondheim to gain support for his person and assembled twenty-one prominent citizens at Eidsvoll on 16 February 1814 where he laid claim to the throne. They rejected a new absolute monarch and instead wanted a liberal constitution; therefore, representatives from the entire country would be elected to create a constitution. The 112 members of the Constituent Assembly gathered and, after six weeks of discussion, concluded the work on the Constitution of Norway on 17 May 1814. Power would be split between the king—a position to which Christian Frederik was appointed—and the Parliament of Norway. King Carl Johan of Sweden invaded Norway in late July; at the Convention of Moss on 14 August Norway surrendered while Sweden accepted the constitution. The union between Sweden and Norway under Carl Johan was approved by Parliament on 4 November.
The Napoleonic Wars sent Norway into an economic crisis, as nearly all the merchants had gone bankrupt during the blockade. Recovery was difficult because of export tariffs and the country underwent strong inflation. The Norwegian speciedaler was established as a currency by the Bank of Norway when it was established in 1816, financed through a silver tax which lasted until 1842. Under threat of a coup d'état by Carl Johan, Norway reluctantly paid the debt stated in the Treaty of Kiel, despite never having ratified it. Constitution Day on 17 May became an important political rally every year; in 1829 the Swedish governor-general Baltzar von Platen resigned after he used force against demonstrators in the Battle of the Square. The first half of the century was dominated by the ca. 2,000 officials, as there were few bourgeois and no aristocracy following an 1821 decision to abolish nobility. From the 1832 election, farmers became more conscious of electing themselves, resulting in a majority of farmers in Parliament. This resulted in rural tax cuts and higher import tariffs, shifting the tax burden to the cities. They also passed the Local Committees Act, which established elected municipal councils from 1838. Cultural expression from the 1840s to the 1870s was dominated by the romantic nationalism, which emphasized the uniqueness of Norway.
The textile industry started in the 1840s, which was followed up with mechanical workshops to build new machinery as the British embargo hindered import of textile machinery. An economic crisis hit the country from 1848, resulting in Marcus Thrane establishing the first trade unions and demanding that quality for the law independent of social class. Parliament passed a series of laws abandoning economic privileges and easing domestic trade during the 1840s and 1850s. Population increase forced the clearing of new land, although some of the growth came in the cities. The population of Christiania reached 40,000 in 1855. By 1865 the population reached 1.7 million; the large increase was largely caused by better nutrition from herring and potatoes, a sharp decrease of infant mortality and increased hygiene. Emigration to North America started in 1825, with the first mass emigration commencing in the 1860s. By 1930, 800,000 people had emigrated, the majority settling in the Midwestern United States.
The population decrease resulted in a labor shortage in the agriculture, which again resulted in increased use of machinery and thus capital. The government stimulated the process through the creation of the Mortgage Bank in 1851 and the State Agricultural College eight years later. The 19th century saw a large increase of road construction and steamship services commenced along the coast. The first railway, the Trunk Line between Christiania and Eidsvoll opened in 1854, followed a year later by the first telegraph line. Export industry commenced with steam-powered sawmills in the 1860s, followed by canned herring, wood pulp and cellulose. From 1850 to 1880 the Norwegian shipping industry enjoyed a large boom, stimulated by the abolishing of the British Navigation Acts. By 1880 there were 60,000 Norwegian seamen and the country had the world's third-largest merchant marine. As the first coast-to-coast railway, the Røros Line connected the capital to Trondheim in 1877. Norway joined the Scandinavian Monetary Union in 1875 and introduced the Norwegian krone with a gold standard, along with the metric system being introduced.
Annual parliamentary sessions were introduced from 1869 and in 1872 ministers were, though a constitutional amendment, required to meet in Parliament to defend their policies. The king, despite having no constitutional right to do so, vetoed the amendment in three successive parliaments. The 1882 election saw the first two parties, the Liberals and Conservatives, run for election, and subsequently the majority succeeded at impeaching the cabinet. In 1884 the king appointed majority leader Johan Sverdrup as prime minister, thus establishing parliamentarism as the first European country. The Liberal Party introduced a series of legal reforms, such as increasing the voting rights to about half of all men, settling the language conflict by establishing two official written standards, Riksmål and Landsmål, introduced juries, seven years of compulsory education and, as the first European country, universal suffrage for men in 1889.
The 1880s and 1890s saw the rise of the labor movement and trade unions became common; the Norwegian Confederation of Trade Unions was established in 1899 and the Norwegian Employers' Confederation the following year. The Labor Party had its first parliamentary members elected in 1903. The women's issue became increasingly dominant through the 1880s and they were gradually permitted to take secondary and tertiary education. Norwegian support of the union decreased towards the end of the 1890s, especially following the 1897 Swedish abolition of the free trade agreement and the lack of a Norwegian foreign minister. Negotiations of independence commenced, but were not effective because of shifting governments and the Swedish threat of war.
Independence.
With the four-party Michelsen's Cabinet appointed in 1905, Parliament voted to establish a Norwegian consular service. This was rejected by the king and on 7 June Parliament unanimously approved the dissolution of the union. In the following dissolution referendum, only 184 people voted in favor of a union. The government offered the Norwegian crown to Denmark's Prince Carl, who after a plebiscite became Haakon VII. The following ten years, Parliament passed a series of social reforms, such as sick pay, factory inspection, a ten-hour working day and worker protection laws. Waterfalls for hydroelectricity became an important resource in this period and the government secured laws to hinder foreigners from controlling waterfalls, mines and forests. Large industrial companies established in these years were Elkem, Norsk Hydro and Sydvaranger. The Bergen Line was completed in 1909, the Norwegian Institute of Technology was established the following year and women's suffrage was introduced in 1913—as the second country in the world. From the 1880s to the 1920s, Norwegians carried out a series of polar expeditions. The most important explorers were Fridtjof Nansen, Roald Amundsen and Otto Sverdrup. Amundsen's expedition in 1911 became the first to reach the South Pole.
Norway adopted a policy of neutrality from 1905; during World War I the Norwegian merchant marine was largely used in support of the British, resulting in Norway being classified as The Neutral Ally. Half the Norwegian fleet was sunk and 2,000 seamen were killed by the German Atlantic U-boat Campaign. Some merchants made huge profits from trade and shipping during the war, resulting in an increased division between the classes. The interwar period was dominated by economic instability caused among other by strikes, lock-outs and the monetary policy causing deflation to compensate for too much money having been issued during the war and thus hindering investments. Especially fishermen were hit hard in the period, while farmers retained market prices through organizing regulations. Unemployment peaked at ten percent between 1931 and 1933. Although industrial production increased by eighty percent from 1915 to 1939, the number of jobs remained stable. The Norwegian School of Economics was established in 1936.
Norway had nine governments between 1918 and 1935, nearly all minority and lasting an average eighteen months. The Agrarian Party was established in 1920, although this period saw a rise of support for the Conservatives. The Labor Party split in 1921, with the left wing establishing the Communist Party. Although strong during the 1920s, they were marginalized through the 1930s. A short-lived Labor Government reigned in 1928, but did not establish a sound parliamentary support until the 1935 Nygaardsvold's Cabinet, based on an alliance with the Agrarian Party. During the 1920s and 1930s, Norway established three dependencies, Bouvetøya, Peter I Island and Queen Maud Land, annexed Jan Mayen and secured sovereignty of Svalbard through the Svalbard Treaty. Norway's first civil airport, Stavanger, opened in 1937.
World War II.
From the start of World War II in 1939, Norway retained a strict neutrality. Both Britain and Germany realized the strategic location; both made plans to invade Norway, regardless of Norwegian opposition. The Germans struck first and attacked Norway on April 9, 1940. After furious battles with the Norwegians and British forces, Germany prevailed and controlled Norway until the end of the war. The German goal was to use Norway to control access to the North Sea and the Atlantic, and to station air and naval forces to stop convoys from Britain to the USSR.
Government in exile.
The government in exile, including the royal family, escaped to London. Politics were suspended and the government coordinated action with the Allies, retained control of a world-wide diplomatic and consular service, and operated the huge Norwegian merchant marine. It organized and supervised the resistance within Norway. One long-term impact was the abandonment of a traditional Scandinavian policy of neutrality; Norway became a founding member of NATO in 1949. Norway at the start of the war had the world's fourth largest merchant fleet, at 4.8 million tons, including a fifth of the world's oil tankers. The Germans captured about 20% of the fleet but the remainder, about 1000 ships, were taken over by the government. Although half the ships were sunk, the earnings paid the expenses of the government.
Quisling regime.
Vidkun Quisling proclaimed himself prime minister and appointed a government with members from the National Unity Party. He was quickly set aside and replaced by Josef Terboven, but reinstated in 1942. The Norwegian Campaign continued in Northern Norway and the government fled to London on 7 June. The German occupation resulted in a brutalization of society and 30,000 people were imprisoned. 55,000 people joined the National Unity Party, which became the only legal party. But the nazification process failed after the Supreme Court resigned and both organized sports and bishops boycotted the new regime. A resistance movement was established and became coordinate from London from 1943. Stokker reports that hostile humour against the Germans helped maintain morale and build a wall against collaboration. Jokes made the rounds dripping with contempt for the oppressors, ridicule of Nazi ideology, stressing the cruelty of the Nazis and mocking their inflated self-image. People on the street asked, "Do you know the difference between the Nazis and a bucket of manure? The bucket." In Post Office lines they explained, "It's rumored that we're getting new stamps bearing Quisling's likeness, but distribution has been delayed because no one knows which side to spit on." The jokes worked to educate Norwegians about the occupation, and encourage a sense of solidarity. At the time of German surrender on 8 May 1945, there were 360,000 German soldiers in the country.
Postwar.
In the following legal purge, 53,000 people were sentenced for treason and 25 were executed. The post-war years saw an increased interest in Scandinavism, resulting in Scandinavian Airlines System in 1946, the Nordic Council in 1952 and the Nordic Passport Union along with the metric system being introduced. Reconstruction after the war gave Norway the highest economic growth in Europe until 1950, partially created through rationing private consumption allowing for higher industrial investments. The Labor Party retained power throughout the period and enforced a policy of public planning. The University of Bergen was created in 1946. The 1950s saw a boom in construction of hydroelectricity and the state built the steel mill Norsk Jernverk and two aluminum works. State banks such as the State Housing Bank, the State Educational Loan Fund and Postbanken allowed for governmental control over private debt. Oslo hosted the 1952 Winter Olympics.
Norway retained its neutrality policy until 1947, focusing on its membership in the United Nations, where Trygve Lie had become the first secretary-general. Norway joined the Marshall Plan in 1947, receiving US$400 million in American support. Anti-communism grew with a Soviet proposal for joint control over Svalbard and especially after the 1948 Czechoslovak coup d'état, after which the Communist Party lost all influence. Norway started negotiations for the creation of a Scandinavian defense union, but instead opted to become a founding member of the North Atlantic Treaty Organization (NATO). However, Norway never allowed permanently stationed foreign troops or nuclear weapons on Norwegian soil to avoid agitating the Soviet Union, with which Norway from 1944 shared a land border with. NATO financed large parts of the Norwegian military investments, which ultimately resulted in a numerous airports being built throughout the 1950s and 1960s.
Sales of cars were deregulated in October 1960, the same year as the Norwegian Broadcasting Corporation introduced Norway's first television broadcasts. Norway feared competition from Swedish industry and Danish agriculture and chose to not join any free trade organizations until 1960, when it joined the European Free Trade Association. Throughout the post-war period both fishing and agriculture became more mechanized, the agricultural subsidies rose to the third-highest in the world and the number of small-scale farms and fishermen fell dramatically. The Socialist People's Party was created in 1961 by former Labor politicians who disagreed with the Labor Party's NATO, nuclear and European policies. Following the Kings Bay Affair the right-winged Lyng's Cabinet ruled for a month. The right-wing coalition Borten's Cabinet won the 1965 election, sat for six years and started a trend of shifting Labor and right-winged governments. Norwegianization of Samis halted after the war and Sami rights became an increasing issue, with a council being established in 1964.
The completion of the Nordland Line to Bodø in 1962 concluded the construction of new railway routes, while the first part of the Oslo Metro opened in 1966. A social security net was gradually introduced after the war: child allowance was introduced in 1946 and the Social Care Act was introduced in 1964. The 1960s saw good times for the heavy industry and Norway became Europe's largest exporter of aluminum and the world's largest exporter of ferroalloys. The University of Trondheim and the University of Tromsø both opened in 1968, one year before a network of regional colleges started being opened. Influenced by American culture and similar actions abroad, youth and students started an uproar against cultural norms. The 1960s saw an increased focus on environmentalism, especially through activism, based on ever-more conversion of waterfalls to hydro stations, pollution and the dilapidation of herring stocks. Rondane National Park was created as the country's first in 1962 and the Ministry of the Environment was the first in the world when it was established in 1972. A network of regional airports were built in Western and Northern Norway in the late 1960s and early 1970s. Membership in the European Economic Community was rejected in a 1972 referendum.
Oil Age.
Prospecting in the North Sea started in 1966 and in 1969 Phillips Petroleum found oil in the Ekofisk field—which proved to be among the ten largest fields in the world. Operations of the fields was split between foreign operators, the state-owned Statoil, the partially state-owned Norsk Hydro and Saga Petroleum. Ekofisk experienced a major blowout in 1977 and 123 people were killed when the Alexander Kielland accommodation rig capsized in 1980; these incidents led to a strengthening of petroleum safety regulations. The oil industry not only created jobs in production, but a large number of supply and technology companies were established. Stavanger became the center of this industry. High petroleum taxes and dividends from Statoil gave high income from the oil industry to the government.
Norway established its exclusive economic zone in the 1970s, receiving an area of . A series of border disputes followed; agreements were reached with Denmark and Iceland in the 1990s, but the border in the Barents Sea was not agreed upon until 2010. Between 1973 and 1981 the country was ruled by the Labor Party, who carried out a series of reforms such as new school system. Farmers received increased subsidies and from 1974 women were permitted to inherit farms. Abortion on demand was legalized in 1978. Loans guaranteed in future oil income allowed Norway to avoid a recession during the mid-1970s. But by 1977 high wages had made Norwegian industry uncompetitive and a soaring forced cut-backs in public and private spending. Fish farming became a new, profitable industry along the coast.
An immigration surplus was established in the late 1960s, largely from Western Europe and the United States—from the 1970s increasingly expertise in oil. The period also saw an increased immigration of unskilled labor from developing countries, especially Pakistan, although regulations from 1975 slowed this significantly. Oslo became the center-point of immigration. The Alta controversy started in the 1970s when Statkraft planned to dam the Alta River. The case united the environmental and Sami interest groups; although Alta Power Station was built, the issue shifted the political climate and made large-scale hydroelectricity project difficult to built. The Sami Parliament was established in 1989.
The Conservative Party won the 1981 elections and carried out a large deregulation reform: taxes were cut, local private radio stations were permitted, cable television was established by private companies, regulations on borrowing money were removed and foreigners were permitted to buy securities. An economic crisis hit in 1986 when foreigners started selling Norwegian krone, which ultimately forced an increase in taxes and Prime Minister Kåre Willoch was forced to resign. The Progress Party, located to the left of the Conservatives, had its break-through in the late 1980s. The high wages in the oil industry made low-skill manufacturing industries uncompetitive and the Labor Party closed a number of public industrial companies which were receiving large subsidies. The 1980s saw a trebling of people on disability, largely amongst the oldest in the workforce. Crime rates rose.
The subsea Vardø Tunnel opened in 1982 and since the country has built subsea tunnels to connect island communities to the mainland. From the 1980s, the largest cities introduced toll rings to finance new road projects. A banking crisis hit Norway in the late 1980s, causing the largest banks, such as Den norske Bank, Christiania Bank and Fokus Bank, to be nationalized. Norsk Data, a manufacturer of minicomputers, became Norway's second largest company by 1985, just to go bankrupt by 1993. Unemployment reached record-high levels in the early 1990s.
By 1990 Norway was Europe's largest oil producer and by 1995 it was the world's second-largest oil exporter. Membership in the European Union was rejected in a 1994 referendum, with Norway instead joining the European Economic Area and later also the Schengen Area. Large public investments in the 1990s were a new National Hospital and Oslo Airport, Gardermoen—connected to the capital with Norway's first high-speed railway, the Gardermoen Line. A number of large government companies, such as Statoil, Telenor and Kongsberg were privatized. Lillehammer hosted the 1994 Winter Olympics. The end of the Cold War resulted in cooperation with Russia and reduced military activity. The Norwegian Armed Forces shifted their focus from defending an invasion to being mobile for use in NATO operations abroad and participated in the NATO bombing of Yugoslavia, the War in Afghanistan and the Libyan Civil War. The 2011 Norway attacks saw a Norwegian far-right terrorist bomb the Government Headquarters and Workers' Youth League camp at Utøya, killing 77 people.
In national elections in September 2013, voters ended eight years of Labor rule. A coalition of the Conservative Party and the populist anti-immigration Progress Party, was elected on promises of tax cuts, more spending on infrastructure and education, better services and stricter rules on immigration. The transition comes as Norway's economy is in good condition with low unemployment. Center-right leader Erna Solberg will form a new government after Labor Prime Minister Jens Stoltenberg admitted defeat. Solberg said her win was "a historic election victory for the right-wing parties".

</doc>
<doc id="56142" url="https://en.wikipedia.org/wiki?curid=56142" title="Thiruvananthapuram">
Thiruvananthapuram

Thiruvananthapuram (Malayalam: തിരുവനന്തപുരം "Tiruvaṉantapuram", ), formerly known as Trivandrum, is the capital and largest city of the Indian state of Kerala. It is on the west coast of India near the extreme south of the mainland. Referred to by Mahatma Gandhi as the "evergreen city of India", it is characterised by its undulating terrain of low coastal hills and busy commercial alleys. With a population of 957,730 inhabitants Thiruvananthapuram contributes nearly 80% of the state's software exports and is a major IT hub.
The city is home to central and state government offices and organisations. Apart from being the political nerve centre of Kerala, it is an academic hub and is home to several educational institutions including the University of Kerala and the Trivandrum medical college and to many science and technology institutions, the most prominent being the Indian Space Research Organisation (ISRO), Vikram Sarabhai Space Centre (VSSC), Centre for Development of Advanced Computing (C-DAC), College of Engineering Thiruvananthapuram (CET), College of Architecture (C.A.T), Government Engineering College, Trivandrum (GECBH), Sree Chitra Thirunal College of Engineering (SCTCE),the Jawaharlal Nehru Tropical Botanic Garden and Research Institute (JNTBGRI), Central Tuber Crops Research Institute, Technopark, the Indian Institute of Space Science and Technology (IIST), the Indian Institute of Information Technology and Management, Kerala, Indian Institute of Science, Education and Research (IISER), the Centre for Development Studies, the Centre for Development of Imaging Technology (C-DIT), the National Institute for Interdisciplinary Science and Technology, the International Centre for Free and Open Source Software (ICFOSS), the Centre for Earth Science Studies, Rajiv Gandhi Centre for Biotechnology and the Sree Chitira Thirunal Institute for Medical Science and Technology.
Considered one of the 10 greenest cities in India, Thiruvananthapuram is classified as a tier-II Indian city along with Kochi and was ranked as the best city in Kerala to live in a 2012 "Times of India" survey. The city was also ranked as the best city in India for Housing and Transport by a survey conducted by India Today.
Toponomy.
The city gets its name from the Malayalam word "thiru-anantha-puram" , meaning the "City of Lord Ananta." The name derives from the deity of the Sri Padmanabhaswamy temple at the centre of the city. Anantha is the serpent Shesha on whom Padmanabha or Vishnu reclines. This temple of Vishnu reclining on Anantha remains the city's iconic landmark. 
Thiruvananthapuram is also known in literature and popular reference as "Ananthapuri", the Sanskrit version of Thiruvananthapuram, and as "Syanandurapuri" (meaning, the city of bliss), in Carnatic kirtanas composed by Swathi Thirunal, a Maharaja of Travancore.
The city was officially referred to as "Trivandrum" until 1991, when the government decided to reinstate the city's original name Thiruvananthapuram.
History.
Thiruvananthapuram is an ancient region with trading traditions dating back to 1000 BCE. The city was the trading post of spices, sandalwood and ivory. However, the ancient political and cultural history was almost entirely independent from that of the rest of Kerala. The early rulers of the city were the Ays. With their fall in the 10th century, the city was taken over by the rulers of Venad.
The rise of modern Thiruvananthapuram began with accession of Marthanda Varma in 1729 as the founding ruler of the princely state of Travancore ("Thiruvithamkoor" in the local vernacular). Thiruvananthapuram was made the capital of Travancore in 1745 after shifting the capital from Padmanabhapuram in Kanyakumari district. The city developed into a major intellectual and artistic centre during this period. The city's golden age in history was during the mid-19th century under the reign of Maharaja Swathi Thirunal and Maharaja Ayilyam Thirunal. This era saw the establishment of the first English school (1834), the Observatory (1837), the General Hospital (1839), the Oriental Research Institute & Manuscripts Library and the University College (1873). The first mental hospital in the state was started during the same period. Sanskrit College, Ayurveda College, Law College and a second-grade college for women were started by Moolam Thirunal (1885–1924).
The early 20th century was an age of tremendous political and social changes in the city. The Sree Moolam Assembly, established in 1904, was the first democratically elected legislative council in any Indian state. Despite not being under direct control of the British Empire at any time, the city featured prominently in India's freedom struggle. The Indian National Congress had a very active presence in Thiruvananthapuram. A meeting of the Indian National Congress presided by Dr. Pattabhi Sitaramaiah was held here in 1938.
The Thiruvananthapuram Municipality started in 1920. The municipality was converted into a Corporation on 30 October 1940, during the period of Chitra Thirunal Bala Rama Varma, who took over in 1931. The city witnessed many-sided progress during his period. The promulgation of "Temple Entry Proclamation" (1936) was an act that underlined social emancipation. This era also saw the establishment of the University of Travancore in 1937, which later became Kerala University.
With the end of the British rule in 1947, Travancore chose to join the Indian union, after toying with the idea of independence till as late as 1949. It had declared itself to be independent on 18 June 1947. An assassination attempt on the Dewan, Sir C P Ramaswamy Iyer and his exit turned the tables on the votaries of an "American Model" Travancore. The first popular ministry headed by Pattom Thanu Pillai was installed in office on 24 March 1948. In 1949, Thiruvananthapuram became the capital of Thiru-Kochi, the state formed by the integration of Travancore with its northern neighbour Kochi, which was the first princely state to accede to the Indian Union. The king of Travancore, Chitra Thirunal Bala Rama Varma, became the Rajpramukh of the Travancore-Cochin Union from 1 July 1949 until 31 October 1956. When the state of Kerala was formed on 1 November 1956, Thiruvananthapuram became its capital.
With the establishment of Thumba Equatorial Rocket Launching Station (TERLS) in 1962, Thiruvananthapuram became the cradle of India's ambitious space programme. The first Indian space rocket was developed and launched from the Vikram Sarabhai Space Centre (VSSC) in the outskirts of the city in 1963. Several establishments of the Indian Space Research Organisation (ISRO) were later established in Thiruvananthapuram.
A major milestone in the city's recent history was the establishment of Technopark—India's first IT park—in 1995. Technopark has developed into the largest IT park in India in geographical area, employing around 48,000 people in 300 companies. This placed Thiruvananthapuram on the IT map of India.
Geography and climate.
Thiruvananthapuram is built on seven hills by the sea shore and is at on the west coast, near the southern tip of mainland India. The city is on the west coast of India and is bounded by Laccadive Sea to its west and the Western Ghats to its east. The city spans an area of and the greater metropolitan area spans an area of . The average elevation is above sea level.
The Geological Survey of India has identified Thiruvananthapuram as a moderately earthquake-prone urban centre and categorised it in the Seismic III Zone.
Thiruvananthapuram lies on the shores of Karamana and Killi rivers. Vellayani, Thiruvallam and Akkulam backwaters lies in the city.
Climate.
The city has a climate that borders between a tropical savanna climate and a tropical monsoon climate. As a result, it does not experience distinct seasons. The mean maximum temperature 34 °C and the mean minimum temperature is 21 °C. The humidity is high and rises to about 90% during the monsoon season.
Thiruvananthapuram is the first city along the path of the south-west monsoons and gets its first showers in early June. The city gets heavy rainfall of around 1700 mm per year. It also gets rain from the receding north-east monsoons which hit the city by October. The dry season sets in by December. December, January and February are the coldest months while March, April and May are the hottest. The lowest temperature in the city core recorded during winter was 16.4 °C on, and the highest temperature recorded in summer is 38.0 °C.
Economy.
The economy of Thiruvananthapuram city was earlier based on the tertiary sector with about 60% of the workforce being employed as government servants. Large-scale industrial establishments are low compared to other south Indian state capitals like Bangalore, Hyderabad and Chennai.
Thiruvananthapuram was listed as one of the top 10 cites in India on Vibrancy Index and Consumption Index by a study conducted by global financial services firm Morgan Stanley. The opening of many private television channels in the state made Thiruvananthapuram the home of studios and related industries. India's first animation park, Kinfra Film and Video Park, is here.
Technopark is home to several companies including Oracle Corporation, Infosys, TCS, Visual Graphics Computing Services, Ernst & Young Global Shared Services Center, Allianz Cornhill, RR Donnelley, UST Global, Tata Elxsi, IBS Software Services, NeST Software, SunTec Business Solutions etc. The park has around 285 companies employing over 40,000 professionals. This is the first CMMI Level 4 assessed Technology Park which spreads over 330 acres with about of built-up space As Phase IV expansion, Technopark is developing 450 acres of land in Pallippuram, 5 km north from the main campus as Technocity.
Tourism has contributed heavily to the economy of Thiruvananthapuram. A large number of foreign tourists visit every year.
There are around 20 government owned and 60 privately owned medium and large-scale industrial units in Thiruvananthapuram. The major employers are the KSIDC, Milma, Keltron, VSSC, ISRO LPSC, Travancore Titanium and Hindustan Latex, all government owned. There are also about 30,000 small scale industrial units employing around 115,000 people. Traditional industries include handloom and coir.
Commercial activity is low mainly due to the underdevelopment of ports. However, this is expected to change with the construction of the Deep Water Container Transshipment Port at Vizhinjam. Situated close to the city, Vizhinjam is very close to international shipping routes and the east-west shipping axis and hardly require maintenance dredging. Other major organisations of economic interest are the BrahMos Aerospace, Chithranjali Film Complex, Kinfra Apparel Park, Kinfra Film and Video Park, Kerala Hitech Industries (KELTECH), Kerala Automobiles Limited and the English Indian Clays Ltd.
Administration and law.
The state legislative assembly and Secretariat are here as Thiruvananthapuram is the capital of Kerala. The city is the headquarters of the Thiruvananthapuram district. The foreign missions in the city are the Consulate of Maldives and Honorary Consulate of Russia.
There is a 2008 plea to reinstate a bench of the Kerala High Court in the city which was earlier cancelled in 1957 due to setting up of the High Court of Kerala at Ernakulam.
The city is administered by the Thiruvananthapuram Corporation which headed by the mayor and is responsible for the overall supervision and control of the administrative functions of the Municipal Corporation. The city council is democratically elected and comprises 100 members representing the wards. Several agencies work under or in partnership with the Corporation including the Thiruvananthapuram Development Authority (TRIDA) and Thiruvananthapuram Road Development Company Limited (TRDCL).
The Corporation of Trivandrum ranked 2nd out of 21 cities for best governance and administrative practices in India in 2014. It scored 3.9 on 10 compared to the national average of 3.3. It is the only city in India with a local ombudsman.
The city comes under the Thiruvananthapuram Lok Sabha constituency. The city corporation area contributes to four legislative assembly seats: Kazhakuttam, Vattiyoorkavu, Thiruvananthapuram, and Nemom.
The police is headed by a Police Commissioner, an officer of Deputy Inspector General rank in the Indian Police Service.
The city is divided into three police sub-divisions headed by Assistant Commissioners. There are two traffic sub-divisions. A women's cell and a narcotics control cell operate in the city. The other units of Thiruvananthapuram City Police include Crime Detachment, City Special Branch, Dog Squad, Mounted Police, District Crime Records Bureau, Foreigners Registration Office (FRO), Tourist Police and District Armed Reserve. There are two state Armed Police Battalions and a unit of the Central Reserve Police Force (CRPF) based in Thiruvananthapuram. The CRPF has a Group Headquarters at Pallipuram. There is a large army cantonment in Pangode where some regiments of the Indian Army are based.
Infrastructure.
The city is fully electrified by Kerala State Electricity Board (KSEB). The district is divided into three circles: Transmission circle, Thiruvananthapuram city and Kattakkada. Domestic consumers account for 43% of the total power consumption, or 90 million units per month. Thiruvananthapuram district has one 220 kV, nine 110 kV and six 66 kV electrical substations. A 400 kV substation has just been commissioned by the Power Grid Corporation and will ensure high-quality power supply to the city.
The water supply schemes cover 100% in the city limits. It is 84% of the urban and 69% of the rural population, when the district is considered. Peppara and Aruvikkara dams are the main sources of water for distribution. The new project plan for improving the water supply with Japanese aid covers Thiruvananthapuram city and six suburban panchayats having urban characteristics.
Thiruvananthapuram is the only city in the state to have a scientific sewage treatment facility. The entire sewage is disposed off at the Muttathara Sewage Treatment Plant, which can handle 107 million litres a day (mld). However, only 32 mld of sewage is currently disposed of at the plant. This sewage plant is India’s largest and Kerala’s first modern sewage treatment plant.
The sewerage system in the city was implemented at the time of the Travancore Kingdom and modernised in 1938. This scheme for the disposal of sullage and sewage is an underground system. The whole system is controlled by Kerala Water Authority now. The city area is divided into seven blocks for the execution of the sewerage system, two commissioned in the 1990s and two after 2000. The sewerage was pumped to a stilling chamber at the Sewerage Treatment Plant (STP) at Valiyathura and was disposed through sewage farming. The Dairy Development Department maintains this sewage farm, and fodder cultivation is done here. There is no revenue generation from this scheme, and the sewerage system in the city is a service provided to the residents. Now the sewage is treated at the Muttathara STP.
Tourism.
Thiruvananthapuram is a destination for domestic and international tourists. There are many tourist destinations in or near the city including Kovalam beach, Sanghumukham Beach, Napier museum and Zoo (Yann Martel wrote his book "Life of Pi" after studying a disabled lion, Simba, for months here), Agasthyarkoodam peak, Neyyar Wildlife Sanctuary and Neyyar Dam, Kuthira Malika palace, Sree Padmanabha Swamy temple, Ponmudi, Poovar, Varkala Cliffs and beaches and many others.
Kanyakumari, Thiruvattar, Padmanabhapuram Palace and Tirpparappu waterfalls, are near the city, in the adjoining Kanyakumari District (Nagercoil), in the state of Tamil Nadu.
The eponymous Sree Padmanabhaswamy Temple circled by the East Fort is at the center of a busy shopping hub. The temple attracts millions of visitors every year. Visitors are required to adhere to special dress code before entering. Recent court battle challenges the custodianship of the Royal family over the temple. The controversy centres on the estimated properties of over $20 billion housed in the vaults of the temple.
Transport.
Road.
The NH-47, which runs from Salem to Kanyakumari, connects the city to Kollam, Kochi, Thrissur and Palakkad. The Main Central Road (MC Road) which is an arterial State Highway in Kerala and designated as SH 1 starts from Kesavadasapuram in the city.
The Thiruvananthpuram Road Development Company Limited is an SPV to develop the road network in Thiruvananthapuram city. It is the first intra-city project in the country.
The intra-city public transport is dominated by the state-owned KSRTC, though there are significant numbers of private buses plying within the city limits.
Within the city, city buses, taxis and autorickshaws provide transportation. Scooters, motorcycles and cars are the favoured means of personal transportation.
There are bus services operated by private operators that provide access within city limits and beyond. The city services of KSRTC operate from seven depots: City depot, Vikas Bhavan, Peroorkada, Pappanamcode, Pothencode, Kaniyapuram and Vellanad. These services were revamped in 2005 with the introduction of modern buses and electronic ticketing mechanisms. The central bus station is in Thampanoor, opposite Thiruvananthapuram Central Station. It connects Thiruvananthapuram with other parts of Kerala and other states. The central bus terminal is 1 km away at East Fort (Kizhakke kotta), near the Padmanabha Swamy temple.
Interstate buses: Tamil Nadu State Transport Corporation of Tirunelveli Division ply buses between Nagercoil and Thiruvananthpuram and many other parts of Kanyakumari district. It has a depot of SETC which operates long distance services towards Chennai and Bangalore via Nagercoil, Madurai. Also, KSRTC, private bus operators and the Karnataka SRTC ply services to destinations in Tamil Nadu, Karnataka and Hyderabad.
Rail.
Thiruvananthapuram comes under the Southern Railway zone of the Indian Railways. Kollam-Thiruvananthapuram trunk line connects Trivandrum with other major cities in India. There are five railway stations in the city limits including the Thiruvananthapuram central station. Thiruvananthapuram Pettah, Kochuveli and Veli stations are towards north direction and Thiruvananthapuram Nemom is in south direction from the central station. The Central railway station is at Thampanoor in the heart of the city and is about 5 km from the new international air terminal and nearly 8 km from the domestic air terminal. Kochuveli railway station is developed to ease congestion on central station and it act as satellite station to Thiruvananthapuram Central. Some of the long distance trains operate from this station. The Thiruvananthapuram Rajdhani Express connects the city to New Delhi, the capital of India.
The city is well connected by rail to almost all major cities in India such as New Delhi, Mumbai, Vellore, Chennai, Coimbatore, Salem, Kolkata, Bangalore, Tirunelveli and Hyderabad. Thiruvananthapuram is the first major South Indian city on the longest train route in India, Kanyakumari to Dibrugarh.
Trivandrum Light Metro Rail.
The Government of Kerala is considering a proposal to construct two light metro systems in the city of Thiruvananthapuram and Kozhikode. Earlier, the proposal was to construct monorail systems. The monorail proposed was to start from Pallippuram and terminate at Neyyattinkara covering a distance of . Thirty-five stops were proposed with multi-storeyed parking lots at the stations. However, the monorail projects have been dropped and the government is considering the possibility of light metros in these corporation areas. Construction of Trivandrum Light Metro Rail is still not yet started because of the project is awaiting an approval from The ministry of urban development.
Suburban rail.
A new suburban corridor by Railways in Thiruvananthapuram – Kollam – Haripad/Chengannur routes for which MRVC is tasked to conduct study and submit report. Ten trains, each with 7 bogies will transport passengers between Thiruvananthapuram-Kollam-Chengannur-Harippad section. Suburban Corridor is modelled on the lines of the Mumbai Suburban Rail where around 3,000 suburban trains ply every day. Funds have also been allocated for the project in the 2016 Rail budget.
Air.
Thiruvananthapuram is served by the Thiruvananthapuram International Airport , the first international airport in India outside the four metropolitan cities then and third largest airport of kerala state. It has direct connectivity to the Middle East, Singapore, Maldives and Sri Lanka and is one of the gateways to the tourism-rich state of Kerala. The airport is qualified for all-weather and night operations. One of its major advantage is the prevailing weather that does not go to extremes, allowing flight operations without disruption year round. The international terminal is approximately due west and the domestic terminal is approximately from the central business district.
The importance of the airport is also because it is the southernmost airport in India and the closest option for neighbouring countries like Maldives and Sri Lanka; it is the only option to Maldives from India. Apart from the regular scheduled flights, charter flights, primarily carrying tourists, serve the airport.
Sea.
The work on infrastructure development for the Deep Water Container Trans-shipment Port at Vizhinjam has began in 2015. It is being built by Adani Ports. It will be built in three phases and expected to be a key competitor in the ports business (especially for container transshipment), with the international shipping lanes between Europe and East Asia lying very close and with major ports like Colombo, Kochi and Tuticorin in close proximity.
The exponential growth of the services and IT-based sectors coupled with its prominence as the state capital and tourist center has caused considerable strain on the transport infrastructure of the city. To ease the strain, projects such as TCRIP are underway (first phase is completed) including the construction of flyovers and under passes. In the first phase, 42 km of six-lane and four-lane dual carriage ways are being built.
Demographics.
The city has a population of 752,490 according to the 2011 census, and 1,687,406 in the Urban Agglomeration. In the city, the density of population is about 5,284 people per square kilometre. There are more women in Thiruvananthapuram than men; the sex ratio is 2,064 females to every 2,000 males.
In October 2010, the area of the city was increased from 86 wards to 100 wards by adding Sreekaryam, Vattiyoorkavu, Kudappanakunnu, Vizhinjam and Kazhakuttam panchayats into the corporation.
The city has now an area of 214.86 km² and a population of 2,957,730 inhabitants with 867,739 males and 889,991 females.
Hindus comprise 65% of the population, Christians are about 18% of the population, and Muslims are about 12% , No Religion The remains 3%, others remaining 2% practise other religions.
The major language spoken is English, Malayalam & Tamil. Hindi is widely understood. As a result of the migration of people for government jobs, the population of the city is a heady mix of people from all parts of Kerala. Thiruvananthapuram is home to a prominent minority of Tamil speakers, owing to their migration from the Southern Travancore region and adjoining districts of Tirunelveli and Rameshwaram in the past. The city also has a few Tulu, Konkani, Dhivehi, Hindi, Telugu, and Urdu speakers.There is also a minority of Maldevians in the city.
Unemployment is a serious issue in Thiruvananthapuram. The increase in the unemployment rate was from 8.8% in 1998 to 34.3% in 2003, thus registering a 25.5% absolute and a 289.7% relative increase in five years,but currently unemployment is decreased in at a high rate of 2.3% because of increased amount of IT companies in city. Thiruvananthapuram taluk ranks third in Kerala with 36.3% of its population unemployed. The in-migration of the unemployed from other districts boosts this high unemployment rate. Even with its unemployment rate, the city has its fair share of migrant labourers from other states. Thiruvananthapuram has a high suicide rate, which went up from 17.2 per lakh in 1995 to 38.5 per lakh in 2002. In 2004, the rate came down slightly to 36.6 per lakh.
As per 2001 census, the populace below the poverty line in the city was 11,667. A BPL survey indicated the urban poor population as 120,367. Majority of these populace lives in slums and coastal fishing areas.
Culture and Food.
The cultural background of Thiruvananthapuram originates from the efforts of the rulers of erstwhile Travancore, who took an active interest in the development of arts and culture. Thiruvananthapuram has produced several great artists, the most famous ones being Maharaja Swathi Thirunal, Irayimman Thampi and Raja Ravi Varma.
Maharaja Swathi Thirunal was a great composer and played a vital role in the development of Carnatic music. There is a music college in his name in the city – Swathi Thirunal College of Music.
Raja Ravi Varma was a famous painter of international renown. His contributions to Indian art are substantial. Most of his famous paintings are preserved at the Sree Chithra Art Gallery in the city. The Padmanabha Swamy Temple and the fort surrounding it, the Napier Museum and Zoo, the VJT hall are among the prominent heritage buildings in the city.
The Veli lake and Shankumugham beach are home to sculptures of the noted sculptor Kanayi Kunhiraman. Many people, including Mahatma Gandhi have admired the city's greenery.
Thiruvananthapuram appears as a laid back and quiet city to a casual observer. However, there are considerable cultural activities. They are more active during the festival season of Onam in August/September and during the tourist season later in the year. The state government organises the tourism week celebrations every year during the Onam with cultural events conducted at centres in the city. The other major events include the annual flower show, the Attukal "Pongala", the"Aaraat" of Padmanabha Swamy Temple, Urs at Beemapally,etc.
The CVN Kalari at East Fort is a well-known centre for training in Kerala's indigenous martial art—the Kalaripayattu. The Margi centre offers training in many of Kerala's traditional arts including Kathakali.
The general cuisine is Keralite cuisine, which is characterised by an abundance of coconut and spices. This includes predominantly vegetarian Naadan (country) and non vegetarian Malabar and Kuttanad recipes. Other South Indian cuisines, as well as Chinese and North Indian cuisines are popular.Arabian, Thai cuisine and Branded fast food Domino's pizza, Pizza hut, KFC, SUBWAY joints are patronised. One of the popular eateries for vegetarian South Indian fare is Arya Nivas at Thampanoor.
Thiruvananthapuram has libraries, the prominent ones being the State Central Library (Thiruvananthapuram Public library, Est. 1829), the University Library, Thiruvananthapuram Children's Library, Manuscripts Library and the Centre for Development Studies Library. The British Library (est. 1964) was very near to the Government Secretariat adjacent to the YMCA Hostel.The British Council closed it down, citing financial constraints.
A shopping mall, Mall of Travancore (MOT), with an area of 600,000 plus sq. ft. is under construction on the Chaakka Bypass. Developed by the Malabar Group, it will be the second largest mall in Kerala on completion.Trivandrum is expected to be the growing megacity in India and world .
Education.
Thiruvananthapuram is an academic hub. The University of Kerala campus is here. The regional headquarters of Indira Gandhi National Open University (IGNOU) is in Thiruvananthapuram. There are many professional education colleges including 15 engineering colleges, three medical colleges, three Ayurveda colleges, two homeopathy colleges, six other medical related colleges, and two law colleges in the city and its suburbs. The College of Engineering, Thiruvananthapuram, Government Engineering College, Barton Hill, and Sree Chitra Thirunal College of Engineering are the main engineering colleges in the city. The Trivandrum medical college is the first and most prestigious medical college in the state of Kerala. The Asian School of Business and IIITM-K are two of the management study institutions in the city, both situated inside Technopark. The College of Architecture, Trivandrum, V.C.A.T. provides architecture education in Trivandrum. The Indian Institute of Space Science and Technology is in the city. Centre for Development Studies and Centre for Development of Imaging Technology (C-DIT) are in city limits.
The schools in the city are classified as Aided, Unaided and Government schools. The government schools are run directly by the state government and follow the syllabus prescribed by the state government. The aided schools also follow the state syllabus. In addition to this, there are five Kendriya Vidyalayas run directly by the Central government, which follow the CBSE syllabus, and private schools run by educational trusts or boards which follow CBSE and/or ICSE syllabus and/or NIOS syllabus and/or state syllabus. In 1961, the first ISC school, Loyola School, was started in the city. The school is in Sreekariyam and is affiliated to the CISCE, CBSE and SCERT and was the first in the city to introduce the ISC course with its Board in Delhi and affiliation to Cambridge University. The first international school in Kerala, the Trivandrum International School, was started in the outskirts of the city in August 2003.
The literacy rate in Thiruvananthapuram, according to the 2001 census, is 89.36 percent; 92.68 percent among males and 86.26 percent among females.
Science and technology.
Thiruvananthapuram is a research and development hub in space science, information technology, bio-technology, and medicine. It is home to the Indian Institute of Science Education and Research, Vikram Sarabhai Space Centre (VSSC), Liquid Propulsion Systems Centre (LPSC), Thumba Equatorial Rocket Launching Station (TERLS), Indian Institute of Space Science and Technology (IIST), Rajiv Gandhi Centre for Biotechnology (RGCB), Jawaharlal Nehru Tropical Botanical Garden and Research Institute (JNTBGRI), ER&DC – CDAC, CSIR – National Institute of Interdisciplinary Science and Technology, Free Software Foundation of India (FSFI), Regional Cancer Centre (RCC), Sree Chitra Thirunal Institute of Medical Sciences and Technology (SCTIMST), Centre for Earth Science Studies (CESS), Central Tuber Crops Research Institute (CTCRI), Kerala Science and Technology Museum, Priyadarsini Planetarium, The Oriental Research Institute & Manuscripts Library, Kerala Highway Research Institute and Kerala Fisheries Research Institute. A scientific institution named National centre for molecular materials, for the research and development of biomedical devices and space electronics is to be established in Thiruvananthapuram. College of Architecture Thiruvananthapuram(CAT), which specialises only on the architecture course, is another institution proposed to set up in the suburbs of the city.
Media.
Daily newspapers are available in English, Malayalam and Tamil. The English newspapers with editions from Thiruvananthapuram are "The New Indian Express", "The Hindu", "The Deccan Chronicle" and "The Times of India". The major Malayalam newspapers are "Mathrubhumi", "Malayala Manorama", "Kerala Kaumudi", "Deshabhimani", Madhyamam, "Janmabhumi", " Chandrika", "Thejas", "Siraj", Kerala Kaumudi Flash, Deepika and Rashtra Deepika.
Most of the Malayalam TV channels are based in Thiruvananthapuram. The government-owned Doordarshan began broadcasting from here in 1981. Asianet, the first private Malayalam channel, began its telecasts in 1991. The other channels now based in Thiruvananthapuram are Amrita TV, Kairali TV, Kairali We (Youth channel of Kairali), Mathrubhumi News, Kaumudy TV, JaiHind TV, Asianet News, Asianet Movies, Asianet Plus (Youth channel of Asianet) and People (News and current affairs channel of Kairali TV). The local cable services are provided by Asianet Satellite Communications Limited, Connecttel Communications Pvt Ltd, Trivandrum Cable Network Pvt Ltd and Siti Cable and they provide a bouquet of local channels in addition to all the Indian channels. DTH services are available through Doordarshan Direct Plus, Tata Sky, Sun Direct, Big TV, Airtel digital TV, Videocon d2h and Dish TV.
All India Radio has an AM (1161 MHz) and an FM (Ananthapuri FM; 101.9 MHz) station for the city. It has a short wave transmitter relaying the AM programming over frequencies, intended for listeners in far-flung areas of Kerala and beyond. FM radio channels broadcast from Thiruvananthapuram are Ananthapuri FM (AIR) 101.9 MHz, Gyanvani from IGNOU 105.6 MHz, Big FM 92.7 MHz, Club FM 94.3 MHz, Radio Mirchi 98.3 MHz, Red FM 93.5 MHz and Radio DC(Low power CRS) 90.4 MHz.
Thiruvananthapuram city contains the largest number of theatres in Kerala. There are over 18 cinema halls which screen films in Malayalam, Tamil, English and Hindi. There are two film studios in the city— Chithranjali and Merryland. The Kinfra Film and Video Park, near the Technopark, is one of the most advanced film and animation production centres in India. Leading firms like Prasad Labs have set up their facilities here. The International Film Festival of Kerala (IFFK) is held in November/December and is acknowledged as one of the leading events of its kind in India.
The wireline telephone services are provided by BSNL, Reliance, AirTel and Tata Indicom. The main GSM networks operating in the city are BSNL CellOne, Airtel, Tata Docomo, Idea Cellular, Vodafone, Reliance and Virgin Mobile. The main CDMA providers are Reliance, MTS and Tata Indicom. The number of mobile phone connections has increased exponentially since the late 1990s.
Major broadband internet services are provided by BSNL Broadband, Asianet Dataline and Siti Cable. Private providers like Reliance, Tata Communications (VSNL), Airtel and Satyam also have their presence in the city. The major dial-up internet providers are BSNL NetOne, Kerala Online and KelNet among others. Thiruvananthapuram also holds the distinction of having been the first 100% Digital SSA (Secondary Switching Area) in India.
Sports.
The most popular games are Football and Cricket. Basketball, Badminton and Volleyball are popular, mostly in schools. The Kerala Cricket Association (KCA) is headquartered in Thiruvananthapuram. The HQ complex of KCA, has advance facilities including two practice turfs with nets, bowling machines, gymnasium with multi-gym and equipment for aerobic training, lecture hall and library, an astro-turf indoor coaching facility, fully furnished accommodation for coaches and players, a physiotherapy clinic, functional office facilities and guest rooms.
Trivandrum has a multi-functional cricket cum football stadium at Kazhakkoottam and a shooting range at Vattiyoorkkavu.
The Trivandrum International Stadium is a multi-purpose (cricket/football) stadium in Thiruvananthapuram. It is the first stadium in the country coming up on DBOT (Design-Build-Operate and Transfer) basis. It is also the first stadium in the country to be developed on annuity mode. It is the proposed venue for the opening/closing ceremonies of the 35th National Games to be held in Kerala. The playing arena in the stadium is constructed in line with FIFA regulations and ICC norms. It will have facilities for indoor sports like table tennis, basketball, badminton etc., a gymnasium and spa, a club house with five-star facilities, tennis court, Olympic size swimming pool, open convention cum trade cum exhibition centre, retail outlets, food courts, club facilities, car parking, etc.
The Chandrasekharan Nair Stadium, in the heart of the city, is a prominent football stadium and has hosted both national and international level matches. The University Stadium has hosted two international cricket matches. This stadium is under the University of Kerala and is equipped with synthetic tracks for athletics games. The Central Stadium, which has facilities for athletics, football, basketball, volleyball and cricket practice nets, is situated on the eastern side of the Government Secretariat. The Jimmy George Indoor Stadium, the GV Raja Sports School and Lakshmi Bhai National College for Physical Education (LNCPE) are the other major sports establishments in the city.
The city has a golf course known as Thiruvananthapuram Golf Club. It is one of the oldest golf courses in India, more than 150 years old.
The city has the Thiruvananthapuram Tennis Club (TTC) at Kowdiar. The city fields two football clubs—SBT-Thiruvananthapuram and Titanium—in the second division of the National Football League.
The city has a fully equipped modern swimming pool near the Jimmy George Sports Complex at Vellayambalam. Many state level and national level swimming competitions are held in this complex. It also holds coaching camps for those who are interested in learning swimming.
Strategic importance.
Thiruvananthapuram is a strategically important city in Southern India. Being the largest city in India's deep south, it is important for military logistics and civil aviation in the southern part of the country. It is the headquarters of the Southern Air Command (SAC) of the Indian Air Force.
Due to the strategic importance of the city, the Indian Air Force authorities have planned to establish an aerospace command in SAC.
The plan for setting up a new "Tri-Service Command", which will integrate all the three forces under a single command, is also in the pipeline.
Being the Indian city with the closest air link to the small island nation of Maldives and Sri Lanka, the city's medical and health infrastructure caters to the needs of the patients from both countries, especially Maldives. Thiruvananthapuram provides a key link in the movement of goods and passengers to and from southern parts of Tamil Nadu into Kerala, the state border being just 30 km from the city centre.
Notable people.
See List of people from Thiruvananthapuram

</doc>
<doc id="56143" url="https://en.wikipedia.org/wiki?curid=56143" title="Kochi (disambiguation)">
Kochi (disambiguation)

Kochi or Kōchi may refer to:

</doc>
