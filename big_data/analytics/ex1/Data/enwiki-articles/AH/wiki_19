<doc id="49266" url="https://en.wikipedia.org/wiki?curid=49266" title="Buchenwald concentration camp">
Buchenwald concentration camp

Buchenwald concentration camp (German: "Konzentrationslager (KZ) Buchenwald", ; literally, in English: "beech forest") was a German Nazi concentration camp established on the "Ettersberg" (Etter Mountain) near Weimar, Germany, in July 1937, one of the first and the largest of the concentration camps on German soil, following Dachau's opening just over four years earlier.
Prisoners from all over Europe and the Soviet Union—Jews, Poles and other Slavs, the mentally ill and physically-disabled from birth defects, religious and political prisoners, Roma and Sinti, Freemasons, Jehovah's Witnesses (then called Bible Students), criminals, homosexuals, and prisoners of war—worked primarily as forced labor in local armaments factories. From 1945 to 1950, the camp was used by the Soviet occupation authorities as an internment camp, known as NKVD special camp number 2.
Today the remains of Buchenwald serve as a memorial and permanent exhibition and museum.
History.
In 1934, the Nazis constructed Buchenwald concentration camp, near Weimar, Germany. Embedded in the camp's main entrance gate is the slogan Jedem das Seine (literally "to each his own", but figuratively "everyone gets what he deserves”). The camp was operational until its liberation in 1945. Between 1945 and 1950, it was used by the Soviet Union as an NKVD special camp for Germans. On January 6, 1950, the Soviets handed over Buchenwald to the East German Ministry of Internal Affairs.
The camp was to be named "K. L. Ettersberg", but this was changed to "Buchenwald" ("beech forest"), since "Ettersberg" carried too many associations with Goethe, who strolled through the woods (his lover Charlotte von Stein lived there) and supposedly wrote his "Wanderer's Nightsong", or, alternatively, the Walpurgisnacht passages of his "" under the oak tree which remained in the center of the camp after the forest was cleared for its construction: this tree is the famous Goethe Oak. Quickly the fate of the oak became associated with the fate of Germany: if the one was to fall, so was the other.
Between April 1938 and April 1945, some 238,380 people of various nationalities including 350 Western Allied prisoners of war (POW)s were incarcerated in Buchenwald. One estimate places the number of deaths at 56,000.
During an American bombing raid on August 24, 1944 that was directed at a nearby armaments factory, several bombs, including incendiaries, also fell on the camp, resulting in heavy casualties amongst the prisoners (2,000 prisoners wounded & 388 killed by the raid).
Today the remains of the camp serve as a memorial and permanent exhibition and museum administered by the Buchenwald and Mittelbau-Dora Memorials Foundation, which also oversees the camp's memorial at Mittelbau-Dora.
People.
Camp commandants.
Buchenwald’s first commandant was Karl-Otto Koch, who ran the camp from 1937 to July 1941. His second wife, Ilse Koch, became notorious as "Die Hexe von Buchenwald" ("the witch of Buchenwald") for her cruelty and brutality. Koch had a zoo built by the prisoners in the camp, with a bear pit ("Bärenzwinger") facing the "Appellplatz", the assembly square where prisoner "roll-calls" were conducted.
Koch himself was eventually imprisoned at Buchenwald by the Nazi authorities for incitement to murder. The charges were lodged by Prince Waldeck and Dr. Morgen, to which were later added charges of corruption, embezzlement, black market dealings, and exploitation of the camp workers for personal gain. Other camp officials were charged, including Ilse Koch. The trial resulted in Karl Koch being sentenced to death for disgracing both himself and the SS; he was executed by firing squad on April 5, 1945, one week before American troops arrived. Ilse Koch was sentenced to a term of four years' imprisonment after the war. Her sentence was reduced to two years and she was set free. She was subsequently arrested again and sentenced to life imprisonment by the post-war German authorities; she committed suicide in a Bavarian prison cell in September 1967.
The second commandant of the camp was Hermann Pister (1942–1945). He was tried in 1947 (Dachau Trials) and sentenced to death, but died in September 1948 of a heart condition before the sentence could be carried out.
Female prisoners and overseers.
The number of women held in Buchenwald was somewhere between 500 and 1,000. The first female inmates were twenty political prisoners who were accompanied by a female SS guard ("Aufseherin"); these women were brought to Buchenwald from Ravensbrück in 1941 and forced into sexual slavery at the camp's brothel. The SS later fired the SS woman on duty in the brothel for corruption, her position was taken over by “brothel mothers” as ordered by SS chief Heinrich Himmler.
The majority of women prisoners, however, arrived in 1944 and 1945 from other camps, mainly Auschwitz, Ravensbrück, and Bergen Belsen. Only one barrack was set aside for them; this was overseen by the female block leader ("Blockführerin") Franziska Hoengesberg, who came from Essen when it was evacuated. All the women prisoners were later shipped out to one of Buchenwald's many female satellite camps in Sömmerda, Buttelstedt, Mühlhausen, Gotha, Gelsenkirchen, Essen, Lippstadt, Weimar, Magdeburg, and Penig, to name a few. No female guards were permanently stationed at Buchenwald.
When the Buchenwald camp was evacuated, the SS sent the male prisoners to other camps, and the five-hundred remaining women (including one of the secret annexe members who lived with Anne Frank, "Mrs. van Daan", real name Auguste van Pels), were taken by train and on foot to the Theresienstadt concentration camp and ghetto in the protectorate of Bohemia and Moravia. Many, including van Pels, died sometime between April and May 1945. Because the female prisoner population at Buchenwald was comparatively small, the SS only trained female overseers at the camp and "assigned" them to one of the female subcamps. Twenty-two known female guards had personnel files at the camp, but it is unlikely that any of them stayed at Buchenwald for longer than a few days.
Ilse Koch served as head supervisor ("Oberaufseherin") of 22 other female guards and hundreds of women prisoners in the main camp. More than 530 women served as guards in the vast Buchenwald system of subcamps and external commands across Germany. Only 22 women served/trained in Buchenwald, compared to over 15,500 men.
Anna Fest was a guard at Ravensbrück, who was later tried and acquitted.
Ulla Erna Frieda Jürß was a guard at Ravensbrück, who was convicted of her crimes.
Allied airmen.
Although it was highly unusual for German authorities to send Western Allied POWs to concentration camps, Buchenwald held a group of 168 aviators for two months. These men were from the United States, United Kingdom, Canada, Australia, New Zealand and Jamaica. They all arrived at Buchenwald on August 20, 1944.
All these airmen were in aircraft that had crashed in occupied France. Two explanations are given for them being sent to a concentration camp: first, that they had managed to make contact with the French Resistance, some were disguised as civilians, and they were carrying false papers when caught; they were therefore categorized by the Germans as spies, which meant their rights under the Geneva Convention were not respected. The second explanation is that they had been categorised as "Terrorflieger" ("terror aviators"). The aviators were initially held in Gestapo prisons and headquarters in France. In April or August 1944, they and other Gestapo prisoners were packed into covered goods wagons (US: boxcars) and sent to Buchenwald. The journey took five days, during which they received very little food or water. One aviator recalled their arrival at Buchenwald:
They were subjected to the same treatment and abuse as other Buchenwald prisoners until October 1944, when a change in policy saw the aviators dispatched to Stalag Luft III, a regular POW camp; nevertheless, two airmen died at Buchenwald. Those classed as "Terrorflieger" had been scheduled for execution after October 24; their rescue was effected by Luftwaffe officers who visited Buchenwald and, on their return to Berlin, demanded the airmen's release.
Buchenwald was also the main imprisonment for a number of Norwegian university students from 1943 until the end of the war. The students, being Norwegian, got better treatment than most, but had to resist Nazi schooling for months. They became remembered for resisting forced labor in a minefield, as the Nazis wished to use them as cannon fodder. An incident connected to this is remembered as the 'Strike at Burkheim'. The Norwegian students in Buchenwald lived in a warmer, stone-construction house and had their own clothes.
Death toll at Buchenwald.
Causes of death.
A primary cause of death was illness due to harsh camp conditions, with starvation—and its consequent illnesses—prevalent. Malnourished and suffering from disease, many were literally "worked to death" under the "Vernichtung durch Arbeit" policy (extermination through labor), as inmates only had the choice between slave labor or inevitable execution. Many inmates died as a result of human experimentation or fell victim to arbitrary acts perpetrated by the SS guards. Other prisoners were simply murdered, primarily by shooting and hanging.
Walter Gerhard Martin Sommer was an SS "Hauptscharführer" who served as a guard at the concentration camps of Dachau and Buchenwald. Known as the "Hangman of Buchenwald", he was considered a depraved sadist who reportedly ordered Otto Neururer and Mathias Spannlang, two Austrian priests, to be crucified upside-down. Sommer was especially infamous for hanging prisoners from trees with their wrists behind their backs in the "singing forest", so named because of the screams which emanated from this wooded area.
Summary executions of Soviet POWs were also carried out at Buchenwald. At least 1,000 men were selected in 1941–2 by a task force of three Dresden Gestapo officers and sent to the camp for immediate liquidation by a gunshot to the back of the neck, the infamous "Genickschuss".
The camp was also a site of large-scale trials for vaccines against epidemic typhus in 1942 and 1943. In all 729 inmates were used as test subjects, of whom 154 died. Other "experimentation" occurred at Buchenwald on a smaller scale. One such experiment aimed at determining the precise fatal dose of a poison of the alkaloid group; according to the testimony of one doctor, four Russian POWs were administered the poison, and when it proved not to be fatal they were "strangled in the crematorium" and subsequently "dissected". Among various other experiments was one which, in order to test the effectiveness of a balm for wounds from incendiary bombs, involved inflicting "very severe" white phosphorus burns on inmates. When challenged at trial over the nature of this testing, and particularly over the fact that the testing was designed in some cases to cause death and only to measure the time which elapsed until death was caused, one Nazi doctor's defence was that, although a doctor, he was a "legally appointed executioner".
Number of deaths.
The SS left behind accounts of the number of prisoners and people coming to and leaving the camp, categorizing those leaving them by release, transfer, or death. These accounts are one of the sources of estimates for the number of deaths in Buchenwald. According to SS documents, 33,462 died. These documents were not, however, necessarily accurate: Among those executed before 1944, many were listed as "transferred to the Gestapo". Furthermore, from 1941, Soviet POWs were executed in mass killings. Arriving prisoners selected for execution were not entered into the camp register and therefore were not among the 33,462 dead listed.
One former Buchenwald prisoner, Armin Walter, calculated the number of executions by the number of shootings in the back of the head. His job at Buchenwald was to set up and care for a radio installation at the facility where people were executed; he counted the numbers, which arrived by telex, and hid the information. He says that 8,483 Soviet prisoners of war were shot in this manner.
According to the same source, the total number of deaths at Buchenwald is estimated at 56,545. This number is the sum of:
This total (56,545) corresponds to a death rate of 24 percent, assuming that the number of persons passing through the camp according to documents left by the SS, 240,000 prisoners, is accurate.
Liberation from Nazi Germany.
On April 4, 1945, the US 89th Infantry Division overran Ohrdruf, a subcamp of Buchenwald. It was the first Nazi camp liberated by US troops.
Buchenwald was partially evacuated by the Germans from April 6, 1945. In the days before the arrival of the American army, thousands of the prisoners were forced to join the evacuation marches.
Thanks in large part to the efforts of Polish engineer (and short-wave radio-amateur, his pre-war callsign was: SP2BD) Gwidon Damazyn, an inmate since March 1941, a secret short-wave transmitter and small generator were built and hidden in the prisoners' movie room. On April 8 at noon, Damazyn and Russian prisoner Konstantin Ivanovich Leonov sent the Morse code message prepared by leaders of the prisoners' underground resistance (supposedly Walter Bartel and Harry Kuhn):
The text was repeated several times in English, German, and Russian. Damazyn sent the English and German transmissions, while Leonov sent the Russian version. Three minutes after the last transmission sent by Damazyn, the headquarters of the US Third Army responded:
According to Teofil Witek, a fellow Polish prisoner who witnessed the transmissions, Damazyn fainted after receiving the message.
After this news had been received, Communist inmates stormed the watchtowers and killed the remaining guards, using arms they had been collecting since 1942 (one machine gun and 91 rifles). (See Buchenwald Resistance)
A detachment of troops of the US 9th Armored Infantry Battalion, from the 6th Armored Division, part of the US Third Army, and under the command of Captain Frederic Keffer, arrived at Buchenwald on April 11, 1945 at 3:15 P.M., (now the permanent time of the clock at the entrance gate). The soldiers were given a hero's welcome, with the emaciated survivors finding the strength to toss some liberators into the air in celebration.
Later in the day, elements of the US 83rd Infantry Division overran Langenstein, one of a number of smaller camps comprising the Buchenwald complex. There, the division liberated over 21,000 prisoners, ordered the mayor of Langenstein to send food and water to the camp, and hurried medical supplies forward from the 20th Field Hospital.
Third Army Headquarters sent elements of the 80th Infantry Division to take control of the camp on the morning of Thursday, April 12, 1945. Several journalists arrived on the same day, perhaps with the 80th, including Edward R. Murrow, whose radio report of his arrival and reception was broadcast on CBS and became one of his most famous:
Soviet Special Camp 2.
After liberation, between 1945 and February 10, 1950, the camp was administered by the Soviet Union and served as Special Camp No. 2 of the NKVD. It was part of a "special camps" network operating since 1945, formally integrated into the Gulag in 1948. Another infamous "special camp" in Soviet occupied Germany was the former Nazi concentration camp Sachsenhausen (special camp No. 7).
Between August 1945 and the dissolution on March 1, 1950, 28,455 prisoners, including 1,000 women, were held by the Soviet Union at Buchenwald. A total of 7,113 people died in Special Camp Number 2, according to the Soviet records. They were buried in mass graves in the woods surrounding the camp. Their relatives did not receive any notification of their deaths. Prisoners comprised alleged opponents of Stalinism, and alleged members of the Nazi party or Nazi organization, others were imprisoned due to identity confusion and arbitrary arrests. The NKVD would not allow any contact of prisoners with the outside world and did not attempt to determine the guilt of any individual prisoner.
On January 6, 1950, Soviet Minister of Internal Affairs Kruglov ordered all special camps, including Buchenwald, to be handed over to the East German Ministry of Internal Affairs.
Demolition of the camp.
In October 1950, it was decreed that the camp would be demolished. The main gate, the crematorium, the hospital block, and two guard towers were spared. All prisoner barracks and other buildings were razed. Foundations of some still exist and many others have been rebuilt. According to the Buchenwald Memorial website, "the combination of obliteration and preservation was dictated by a specific concept for interpreting the history of Buchenwald Concentration Camp."
The first monument to victims was erected days after the initial liberation. Intended to be completely temporary, it was built by prisoners and made of wood. A second monument to commemorate the dead was erected in 1958 by the GDR near the mass graves. Inside the camp, there is a stainless steel monument in the place of the first monument, the surface of which is maintained at , the temperature of human skin, all year round.
Notable inmates.
Camp literature.
Survivors who have written about their camp experiences include Jorge Semprún, who in "Quel beau dimanche!" describes conversations involving Goethe and Léon Blum, and Ernst Wiechert, whose "Der Totenwald" was written in 1939 but not published until 1945, and which likewise involved Goethe. Scholars have investigated how camp inmates used art to help deal with their circumstances, and according to Theodor Ziolkowski writers often did so by turning to Goethe. Artist Léon Delarbre sketched, besides other scenes of camp life, the Goethe Oak, under which he used to sit and write. One of the few prisoners who escaped from the camp, the Belgian Edmond Vandievoet, recounted his experiences in a book whose English title is "I escaped from a Nazi Death Camp" Jourdan, 2015. In his work "Night", Elie Wiesel talks about his stay in Buchenwald, including his father's death.
There is an account of the Soviet NKVD camp, by former inmate Maria Linke. Born in tsarist-era Russia, daughter of a German foundry manager, she was taken into custody due to her fluent Russian.
Modern times.
Today the remains of Buchenwald serves as a memorial and permanent exhibition and museum administrated by Buchenwald and Mittelbau-Dora Memorials Foundation, which also administrates the camp memorial at Mittelbau-Dora.
Visit from President Obama and Chancellor Merkel.
On June 5, 2009, U.S. President Barack Obama and German Chancellor Angela Merkel visited Buchenwald after a tour of Dresden Castle and Church of Our Lady. During the visit they were accompanied by Elie Wiesel and Bertrand Herz, both survivors of the camp. Dr. Volkhard Knigge, the director of the Buchenwald and Mittelbau-Dora Memorials Foundation and honorary professor of University of Jena, guided the four guests through the remainder of the site of the camp. During the visit Elie Wiesel, who together with Bertrand Herz were sent to the Little camp as 16-year-old boys, said, "if these trees could talk." His statement marked the irony about the beauty of the landscape and the horrors that took place within the camp. President Obama mentioned during his visit that he had heard stories as a child from his great uncle, who was part of the 89th Infantry Division, the first Americans to reach the camp at Ohrdruf, one of Buchenwald's satellites.

</doc>
<doc id="49274" url="https://en.wikipedia.org/wiki?curid=49274" title="Tove Jansson">
Tove Jansson

Tove Marika Jansson (Finland ; 9 August 1914 – 27 June 2001) was a Swedish-speaking Finnish novelist, painter, illustrator and comic strip author. For her contribution as a children's writer she received the Hans Christian Andersen Medal in 1966.
Brought up by artistic parents, Jansson studied art from 1930 to 1938 in Stockholm, Helsinki and then Paris. Her first solo art exhibition was in 1943. At the same time, she was writing short stories and articles for publication, as well as creating the graphics for book covers and other purposes. She continued to work as an artist for the rest of her life, alongside her writing.
Jansson is best known as the author of the "Moomin" books for children. The first such book, "The Moomins and the Great Flood", appeared in 1945, though it was the next two books, "Comet in Moominland" and "Finn Family Moomintroll", published in 1946 and 1948 respectively, that brought her fame.
Starting with the semi-autobiographical "Bildhuggarens dotter" ("Sculptor's Daughter") in 1968, she wrote six novels and five books of short stories for adults.
Biography.
Tove Jansson was born in Helsinki, Grand Duchy of Finland, Russian Empire. Her family, part of the Swedish-speaking (Swedish: "finlandssvensk") minority of Finland, was an artistic one: her father Viktor Jansson was a sculptor and her mother Signe Hammarsten-Jansson was a graphic designer and illustrator. Tove's siblings also became artists: Per Olov Jansson became a photographer and Lars Jansson an author and cartoonist. Whilst their home was in Helsinki, the family spent many of their summers in a rented cottage on an island near Porvoo, 50 km east of Helsinki.
Jansson studied at University College of Arts, Crafts and Design in Stockholm in 1930–33, the Graphic School of the Finnish Academy of Fine Arts in 1933–1937 and finally at L'École d'Adrien Holy and L'École des Beaux-Arts in Paris in 1938. She displayed a number of artworks in exhibitions during the 30s and early 40s, and her first solo exhibition was held in 1943.
Aged 14, she wrote and illustrated her first picture book "Sara och Pelle och näckens bläckfiskar" ("Sara and Pelle and the Water Sprite's Octopuses") although it was not published until 1933, and had drawings published in magazines in the 1920s. During the 1930s she made several trips to other European countries, and wrote and illustrated short stories and articles which were also published in magazines, periodicals and daily papers.
During this period, Jansson designed many book covers, adverts and postcards, and, following her mother, she drew illustrations for "Garm", an anti-fascist Finnish-Swedish satirical magazine.
Briefly engaged in the 1940s to Atos Wirtanen, she later during her studies met her future partner Tuulikki Pietilä. The two women collaborated on many works and projects, including a model of the Moominhouse, in collaboration with Pentti Eistola. This is now exhibited at the Moomin museum in Tampere.
Jansson wrote and illustrated her first Moomin book, "The Moomins and the Great Flood", in 1945, during World War II. She said later that the war had depressed her and she had wanted to write something naïve and innocent. This first book was hardly noticed, but the next Moomin books, "Comet in Moominland" (1946) and "Finn Family Moomintroll" (1948), made her famous. She went on to write six more Moomin books, a number of picture books and comic strips. Her fame spread quickly and she became Finland's most widely read author abroad. For her "lasting contribution to children's literature" she received the biennial, international Hans Christian Andersen Award for Writing in 1966.
Jansson continued painting and writing for the rest of her life, although her contributions to the Moomin series became rare after 1970. Her first foray outside children's literature was "Bildhuggarens dotter" ("Sculptor's Daughter"), a semi-autobiographical book written in 1968. After that, she authored five more novels, including "Sommarboken" ("The Summer Book") and five collections of short stories. Although she had a studio in Helsinki, she lived many summers on a small island called Klovharu, one of the Pellinki Islands near the town of Porvoo. Jansson's and Pietilä's travels and summers spent together on the Klovharu island in Pellinki have been captured on several hours of film, shot by Pietilä. Several documentaries have been made of this footage, the latest being "Haru, yksinäinen saari" ("Haru, the lonely island") (1998) and "Tove ja Tooti Euroopassa" ("Tove and Tooti in Europe") (2004).
Jansson died on 27 June 2001. She was 86 years old. 
Career.
Comic strip artist.
Tove Jansson worked as illustrator and cartoonist for the Swedish-language satirical magazine "Garm" from the 1930s to 1953. One of her political cartoons achieved a brief international fame: she drew Adolf Hitler as a crying baby in diapers, surrounded by Neville Chamberlain and other great European leaders, who tried to calm the baby down by giving it slices of cake – Austria, Poland, Czechoslovakia, etc. Jansson also produced illustrations during this period for the Christmas magazines "Julen" and "Lucifer" (just as her mother had earlier) as well as several smaller productions. Her earliest comic strips were produced for productions including "Lunkentus" ("Prickinas och Fabians äventyr", 1929), "Vårbrodd" ("Fotbollen som Flög till Himlen", 1930), and "Allas Krönika" ("Palle och Göran gå till sjöss", 1933).
The figure of the Moomintroll appeared first in Jansson's political cartoons, where it was used as a signature character near the artist's name. This "Proto-Moomin," then called Snork or Niisku, was thin and ugly, with a long, narrow nose and devilish tail. Jansson said that she had designed the Moomins in her youth: after she lost a philosophical quarrel about Immanuel Kant with one of her brothers, she drew "the ugliest creature imaginable" on the wall of their outhouse and wrote under it "Kant". This Moomin later gained weight and a more pleasant appearance, but in the first Moomin book "The Moomins and the Great Flood" (originally "Småtrollen och den stora översvämningen"), the Immanuel-Kant-Moomin is still perceptible. The name "Moomin" comes from Tove Jansson's uncle, Einar Hammarsten: when she was studying in Stockholm and living with her Swedish relatives, her uncle tried to stop her pilfering food by telling her that a "Moomintroll" lived in the kitchen closet and breathed cold air down people's necks.
In 1952, after "Comet in Moominland" and "Finn Family Moomintroll" had been translated into English, a British publisher asked if Tove Jansson would be interested in drawing comic strips about the Moomins. Jansson had already drawn a long Moomin comic adventure, "Mumintrollet och jordens undergång" ("Moomintrolls and the End of the World"), based loosely on "Comet in Moominland", for the Swedish-language newspaper "Ny Tid", and she accepted the offer. The comic strip "Moomintroll", started in 1954 in the "Evening News", a newspaper for the London area and London commuters (no longer in business). Tove Jansson drew 21 long Moomin stories from 1954 to 1959, writing them at first by herself and then with her brother Lars Jansson. She eventually gave the strip up because the daily work of a comic artist did not leave her time to write books and paint, but Lars took over the strip and continued it until 1975.
The series was published in book form in Swedish, and books 1 to 6 have been published in English, "Moomin: The Complete Tove Jansson Comic Strip".
Inspiration for Moomins.
Critics have interpreted various Moomin characters as being inspired by real people, especially members of the author's family, and Jansson spoke in interviews about the backgrounds of, and possible models for, her characters.
Pietilä's personality inspired the character Too-Ticky in "Moominland Midwinter". and Moomintroll and Little My have been seen as psychological self-portraits of the artist.
The Moomins, generally speaking, relate strongly to Jansson's own family – they were bohemian, lived close to nature and were very tolerant towards diversity. Moominpappa and Moominmamma are often seen as portraits of Jansson's parents. Jansson remained close to her mother until her mother's death in 1970; even after Tove had become an adult, the two often travelled together, and during her final years Signe also lived with Tove part-time.
Author.
Jansson is principally known as the author of the Moomin books. Jansson created the Moomins, a family of trolls who are white, round and smooth in appearance, with large snouts that make them vaguely resemble hippopotamuses.
The first Moomin book, "The Moomins and the Great Flood", was written in 1945. Although the primary characters are Moominmamma and Moomintroll, most of the principal characters of later stories were only introduced in the next book, so "The Moomins and the Great Flood" is frequently considered a forerunner to the main series. The book was not a success (and was the last Moomin book to be translated into English), but the next two installments in the Moomin series, "Comet in Moominland" (1946) and "Finn Family Moomintroll" (1948), brought Jansson some fame. The original title of "Finn Family Moomintroll", "Trollkarlens Hatt", translates as "The Magician's Hat".
The style of the Moomin books changed as time went by. The first books, up to "Moominland Midwinter" (1957), are adventure stories that include floods, comets and supernatural events. "The Moomins and the Great Flood" deals with Moominmamma and Moomintroll's flight through a dark and scary forest, where they encounter various dangers. In "Comet in Moominland", a comet nearly destroys the Moominvalley (some critics have considered this an allegory of nuclear weapons). "Finn Family Moomintroll" deals with adventures brought on by the discovery of a magician's hat. "The Exploits of Moominpappa" (1950) tells the story of Moominpappa's adventurous youth and cheerfully parodies the genre of memoirs. Finally, "Moominsummer Madness" (1955) pokes fun at the world of the theatre: the Moomins explore an empty theatre and perform Moominpappa's pompous hexametric melodrama.
"Moominland Midwinter" marks a turning point in the series. The books take on more realistic settings ("realistic" in the context of the Moomin universe) and the characters start to acquire some psychological depth. "Moominland Midwinter" focuses on Moomintroll, who wakes up in the middle of the winter (Moomins hibernate from November to April, as mentioned on the back of the book), and has to cope with the strange and unfriendly world he finds. The short story collection "Tales from Moominvalley" (1962) and the novels "Moominpappa at Sea" (1965) and "Moominvalley in November" (1970) are serious and psychologically searching books, far removed from the light-heartedness and cheerful humor of "Finn Family Moomintroll".
After "Moominvalley in November" Tove Jansson stopped writing about Moomins and started writing for adults. The Summer Book is the best known of her adult fiction translated into English. It is a work of charm, subtlety and simplicity, describing the summer stay on an island of a young girl and her grandmother.
In addition to the Moomin novels and short stories, Tove Jansson also wrote and illustrated four original and highly popular picture books: "The Book about Moomin, Mymble and Little My" (1952), "Who will Comfort Toffle?" (1960), "The Dangerous Journey" (1977) and "An Unwanted Guest" (1980). As the Moomins' fame grew, two of the original novels, "Comet in Moominland" and "The Exploits of Moominpappa", were revised by Jansson and republished. The revised versions were, however, never translated into English.
Painter and illustrator.
Although she became known first and foremost as an author, Tove Jansson considered her careers as author and painter to be of equal importance. She painted her whole life, changing style from the classical impressionism of her youth to the highly abstract modernist style of her later years. Jansson displayed a number of artworks in exhibitions during the 1930s and early 1940s, and her first solo exhibition was held in 1943. Despite generally positive reviews, criticism induced Jansson to refine her style such that in her 1955 solo exhibition her style had become less overloaded in terms of detail and content. Between 1960 and 1970 Jansson held five more solo exhibitions.
Jansson also created a series of commissioned murals and public works throughout her career, which may still be viewed in their original locations. These works of Jansson's included:
In addition to providing the illustrations for her own Moomin books, Jansson also illustrated Swedish translations of classics such as J. R. R. Tolkien's "The Hobbit" and Lewis Carroll's "The Hunting of the Snark" and "Alice's Adventures in Wonderland" (some used later in Finnish translations as well). She also illustrated her late work, "The Summer Book" (1972).
Theatre.
Several stage productions have been made from Jansson's Moomin series, including a number that Jansson herself was involved in.
The earliest production was a 1949 theatrical version of "Comet in Moominland" performed at Åbo Svenska Teater.
In the early 1950s, Jansson collaborated on Moomin-themed children's plays with Vivica Bandler. In 1952, Jansson designed stage settings and dresses for "Pessi and Illusia", a ballet by Ahti Sonninen ("Radio tekee murron") which was performed at the Finnish National Opera. By 1958, Jansson began to become directly involved in theater as Lilla Teater produced "Troll i kulisserna" ("Troll in the wings"), a play with lyrics by Jansson and music composed by Erna Tauro. The production was a success, and later performances were held in Sweden and Norway.
In 1974 the first Moomin opera was produced, with music composed by Ilkka Kuusisto.
Jansson's cultural legacy.
The biennial Hans Christian Andersen Award conferred by the International Board on Books for Young People is the highest recognition available to a writer or illustrator of children's books. Jansson received the writing award in 1966.
Jansson's books, originally written in Swedish, have been translated into 45 languages. After the "Kalevala" and books by Mika Waltari, they are the most widely translated works of Finnish literature.
The Moomin Museum in Tampere displays much of Jansson's work on the Moomins. There is also a Moomin theme park named Moomin World in Naantali.
Tove Jansson was selected as the main motif in the 2004 minting of a Finnish commemorative coin, the €10 Tove Jansson and Finnish Children's Culture commemorative coin. The obverse depicts a combination of Tove Jansson portrait with several objects: the skyline, an artist's palette, a crescent and a sailing boat. The reverse design features three Moomin characters. In 2014 she was again featured on a commemorative coin, minted at €10 and €20 values, being the only person other than the former Finnish president Urho Kekkonen to be granted two such coins. She was also featured on a €2 commemorative coin that entered general circulation in June 2014.
Since 1988, Finland's Post has released several postage stamp sets and one postal card with Moomin motifs. In 2014, Jansson herself was featured on a Finnish stamp set.
In 2014 the City of Helsinki honored Jansson by renaming a park in Katajanokka as Tove Jansson's Park (, ). The park is located near Jansson's childhood home.
In March 2014 the Ateneum Art Museum opened a major centenary exhibition showcasing Jansson's works as an artist, an illustrator, a political caricaturist and the creator of the Moomins. The exhibition drew nearly 300,000 visitors in six months. After Helsinki the exhibition embarked on a tour in Japan to visit five Japanese museums.

</doc>
<doc id="49281" url="https://en.wikipedia.org/wiki?curid=49281" title="Hydronium">
Hydronium

In chemistry, hydronium is the common name for the aqueous cation , the type of oxonium ion produced by protonation of water. It is the positive ion present when an Arrhenius acid is dissolved in water, as Arrhenius acid molecules in solution give up a proton (a positive hydrogen ion, H+) to the surrounding water molecules (H2O).
Determination of pH.
It is the presence of hydronium ion relative to hydroxide that determines a solution's pH. The molecules in pure water auto-dissociate into hydronium and hydroxide ions in the following equilibrium:
In pure water, there is an equal number of hydroxide and hydronium ions, so it has a neutral pH of 7 (at 25 °C). A pH value less than 7 indicates an acidic solution, and a pH value more than 7 indicates a basic solution.
Nomenclature.
According to IUPAC nomenclature of organic chemistry, the hydronium ion should be referred to as "oxonium". "Hydroxonium" may also be used unambiguously to identify it. A draft IUPAC proposal also recommends the use of oxonium and "oxidanium" in organic and inorganic chemistry contexts, respectively.
An oxonium ion is any ion with a trivalent oxygen cation. For example, a protonated hydroxyl group is an oxonium ion, but not a hydronium.
Structure.
Since and N have the same number of electrons, is isoelectronic with ammonia. As shown in the images above, has a trigonal pyramid geometry with the oxygen atom at its apex. The H–O–H bond angle is approximately 113°, and the center of mass is very close to the oxygen atom. Because the base of the pyramid is made up of three identical hydrogen atoms, the molecule's symmetric top configuration is such that it belongs to the C3v point group. Because of this symmetry and the fact that it has a dipole moment, the rotational selection rules are Δ"J" = ±1 and Δ"K" = 0. The transition dipole lies along the "c"-axis and, because the negative charge is localized near the oxygen atom, the dipole moment points to the apex, perpendicular to the base plane.
Acids and acidity.
Hydronium is the cation that forms from water in the presence of hydrogen ions. These hydrons do not exist in a free state: they are extremely reactive and are solvated by water. An acidic solute is generally the source of these hydrons; however, hydroniums exist even in pure water. This special case of water reacting with water to produce hydronium (and hydroxide) ions is commonly known as the self-ionization of water. The resulting hydronium ions are few and short-lived. pH is a measure of the relative activity of hydronium and hydroxide ions in aqueous solutions. In acidic solutions, hydronium is the more active, its excess proton being readily available for reaction with basic species.
Hydronium is very acidic: at 25 °C, its p"K"a is −1.7. It is the most acidic species that can exist in water (assuming sufficient water for dissolution): any stronger acid will ionize and protonate a water molecule to form hydronium. The acidity of hydronium is the implicit standard used to judge the strength of an acid in water: strong acids must be better proton donors than hydronium, otherwise a significant portion of acid will exist in a non-ionized state. Unlike hydronium in neutral solutions that result from water's autodissociation, hydronium ions in acidic solutions are long-lasting and concentrated, in proportion to the strength of the dissolved acid.
pH was originally conceived to be a measure of the hydrogen ion concentration of aqueous solution. We now know that virtually all such free protons quickly react with water to form hydronium; acidity of an aqueous solution is therefore more accurately characterized by its hydronium concentration. In organic syntheses, such as acid catalyzed reactions, the hydronium ion () can be used interchangeably with the H+ ion; choosing one over the other has no significant effect on the mechanism of reaction.
Solvation.
Researchers have yet to fully characterize the solvation of hydronium ion in water, in part because many different meanings of solvation exist. A freezing-point depression study determined that the mean hydration ion in cold water is approximately : on average, each hydronium ion is solvated by 6 water molecules which are unable to solvate other solute molecules.
Some hydration structures are quite large: the magic ion number structure (called "magic" because of its increased stability with respect to hydration structures involving a comparable number of water molecules – this is a similar usage of the word "magic" as in nuclear physics) might place the hydronium inside a dodecahedral cage. However, more recent ab initio method molecular dynamics simulations have shown that, on average, the hydrated proton resides on the surface of the cluster. Further, several disparate features of these simulations agree with their experimental counterparts suggesting an alternative interpretation of the experimental results.
Isolation of the hydronium ion monomer in liquid phase was achieved in a nonaqueous, low nucleophilicity superacid solution (). The ion was characterized by high resolution nuclear magnetic resonance.
A 2007 calculation of the enthalpies and free energies of the various hydrogen bonds around the hydronium cation in liquid protonated water at room temperature and a study of the proton hopping mechanism using molecular dynamics showed that the hydrogen-bonds around the hydronium ion (formed with the three water ligands in the first solvation shell of the hydronium) are quite strong compared to those of bulk water.
A new model was proposed by Stoyanov based on infrared spectroscopy in which the proton exists as an ion. The positive charge is thus delocalized over 6 water molecules.
Solid hydronium salts.
For many strong acids, it is possible to form crystals of their hydronium salt that are relatively stable. These salts are sometimes called "acid monohydrates". As a rule, any acid with an ionization constant of or higher may do this. Acids whose ionization constant is below generally cannot form stable salts. For example, hydrochloric acid has an ionization constant of , and mixtures with water at all proportions are liquid at room temperature. However, perchloric acid has an ionization constant of , and if liquid anhydrous perchloric acid and water are combined in a 1:1 molar ratio, solid hydronium perchlorate forms.
The hydronium ion also forms stable compounds with the carborane superacid . X-ray crystallography shows a C3v symmetry for the hydronium ion with each proton interacting with a bromine atom each from three carborane anions 320 pm apart on average. The salt is also soluble in benzene. In crystals grown from a benzene solution the solvent co-crystallizes and a · (benzene)3 cation is completely separated from the anion. In the cation three benzene molecules surround hydronium forming pi-cation interactions with the hydrogen atoms. The closest (non-bonding) approach of the anion at chlorine to the cation at oxygen is 348 pm.
There are also many examples of hydrated hydronium ions known, such as the ion in , the and ions both found in .
Interstellar H3O+.
Motivation for study.
Hydronium is an abundant molecular ion in the interstellar medium and is found in diffuse and dense molecular clouds as well as the plasma tails of comets. Interstellar sources of hydronium observations include the regions of Sagittarius B2, Orion OMC-1, Orion BN–IRc2, Orion KL, and the comet Hale–Bopp.
Interstellar hydronium is formed by a chain of reactions started by the ionization of into by cosmic radiation. can produce either or through dissociative recombination reactions, which occur very quickly even at the low (≥10 K) temperatures of dense clouds. This leads to hydronium playing a very important role in interstellar ion-neutral chemistry.
Astronomers are especially interested in determining the abundance of water in various interstellar climates due to its key role in the cooling of dense molecular gases through radiative processes. However, H2O does not have many favorable transitions for ground based observations. Although observations of HDO (the deuterated version of water) could potentially be used for estimating H2O abundances, the ratio of HDO to is not known very accurately.
Hydronium, on the other hand, has several transitions that make it a superior candidate for detection and identification in a variety of situations. This information has been used in conjunction with laboratory measurements of the branching ratios of the various dissociative recombination reactions to provide what are believed to be relatively accurate and H2O abundances without requiring direct observation of these species.
Interstellar chemistry.
As mentioned previously, is found in both diffuse and dense molecular clouds. By applying the reaction rate constants ("α", "β", and "γ") corresponding to all of the currently available characterized reactions involving , it is possible to calculate "k"("T") for each of these reactions. By multiplying these "k"("T") by the relative abundances of the products, the relative rates (in cm3/s) for each reaction at a given temperature can be determined. These relative rates can be made in absolute rates by multiplying them by the []2. By assuming "T" = 10 K for a dense cloud and "T" = 50 K for a diffuse cloud, the results indicate that most dominant formation and destruction mechanisms were the same for both cases. It should be mentioned that the relative abundances used in these calculations correspond to TMC-1, a dense molecular cloud, and that the calculated relative rates are therefore expected to be more accurate at "T" = 10 K. The three fastest formation and destruction mechanisms are listed in the table below, along with their relative rates. Note that the rates of these six reactions are such that they make up approximately 99% of hydronium ion's chemical interactions under these conditions. Finally, it should also be noted that all three destruction mechanisms in the table below are classified as dissociative recombination reactions.
It is also worth noting that the relative rates for the formation reactions in the table above are the same for a given reaction at both temperatures. This is due to the reaction rate constants for these reactions having "β" and "γ" constants of 0, resulting in "k" = "α" which is independent of temperature.
Since all three of these reactions produce either or OH, these results reinforce the strong connection between their relative abundances and that of . The rates of these six reactions are such that they make up approximately 99% of hydronium ion's chemical interactions under these conditions.
Astronomical detections.
As early as 1973 and before the first interstellar detection, chemical models of the interstellar medium (the first corresponding to a dense cloud) predicted that hydronium was an abundant molecular ion and that it played an important role in ion-neutral chemistry. However, before an astronomical search could be underway there was still the matter of determining hydronium's spectroscopic features in the gas phase, which at this point were unknown. The first studies of these characteristics came in 1977, which was followed by other, higher resolution spectroscopy experiments. Once several lines had been identified in the laboratory, the first interstellar detection of H3O+ was made by two groups almost simultaneously in 1986. The first, published in June 1986, reported observation of the "J" = 1 − 2 transition at in OMC-1 and Sgr B2. The second, published in August, reported observation of the same transition toward the Orion-KL nebula.
These first detections have been followed by observations of a number of additional H3O+ transitions. The first observations of each subsequent transition detection are given below in chronological order:
In 1991, the 3 − 2 transition at was observed in OMC-1 and Sgr B2. One year later, the 3 − 2 transition at was observed in several regions, the clearest of which was the W3 IRS 5 cloud.
The first far-IR 4 − 3 transition at 69.524 µm (4.3121 THz) was made in 1996 near Orion BN-IRc2. In 2001, three additional transitions of H3O+ in were observed in the far infrared in Sgr B2; 2 − 1 transition at 100.577 µm (2.98073 THz), 1 − 1 at 181.054 µm (1.65582 THz) and 2 − 1 at 100.869 µm (2.9721 THz).

</doc>
<doc id="49284" url="https://en.wikipedia.org/wiki?curid=49284" title="Methylchloroisothiazolinone">
Methylchloroisothiazolinone

Methylchloroisothiazolinone (5-chloro-2-methyl-4-isothiazolin-3-one), also referred to as MCI, is a preservative with antibacterial and antifungal effects within the group of isothiazolinones. It is effective against gram-positive and gram-negative bacteria, yeast, and fungi.
Methylchloroisothiazolinone is found in many water-based personal care products and cosmetics. Methylchloroisothiazolinone was first used in cosmetics in the 1970s. It is also used in glue production, detergents, paints, fuels, and other industrial processes. Methylchloroisothiazolinone is known by the registered tradename Kathon CG when used in combination with methylisothiazolinone.
Methylchloroisothiazolinone may be used in combination with other preservatives including ethylparaben, benzalkonium chloride, or 2-bromo-2-nitropropane-1,3-diol.
Safety.
In pure form or in high concentrations, methylchloroisothiazolinone is a skin and membrane irritant and causes chemical burns. In the United States, accepted concentrations are 15 ppm in rinse-offs and 8 ppm in other cosmetics. In Canada, the accepted concentrations are 15 ppm and 7.5 ppm respectively. The Canadian levels are also reflected in a paper published in the International Journal of Toxicology.
The International Agency for Research on Cancer (IARC), does not currently list methylchloroisothiazolinone as a known, probable, or possible human carcinogen, nor have "in vivo" tests found evidence of carcinogenic activity.
Allergic contact dermatitis.
Methylchloroisothiazolinone is an allergen and can cause severe skin reactions in some people. Since 2013 the use of the product particularly in cosmetics has received increased media coverage in the UK. In 2013 GP doctors asked cosmetics companies to remove it from products. Some cosmetics with the MI in have been investigated by the BBC Watchdog programme. The first publication of the preservative as a contact allergen was in 1988. A common indication of an allergic reaction is eczema-like symptoms including redness and itching, and upon longer exposure also burning sensations and blisters on the part of the skin that is exposed to the allergen. Cases of photoaggravated allergic contact dermatitis, i.e. worsening of skin lesions after sun exposure, have also been reported. Continued exposure can lead to high sensitization which will be triggered each time the individual comes in contact with the allergen due to the memory T-cells that will remain in the local skin area.

</doc>
<doc id="49287" url="https://en.wikipedia.org/wiki?curid=49287" title="USS Winston S. Churchill">
USS Winston S. Churchill

USS "Winston S. Churchill" (DDG-81) is an guided missile destroyer of the United States Navy. She is the 31st destroyer of an originally planned 62-ship class. "Churchill" is named after British Prime Minister Sir Winston Churchill. Her home port is in Naval Station Norfolk, Virginia. She is a component of Carrier Strike Group Twelve.
Design.
The ship is the first of the Flight IIA variants fitted with the 62-caliber Mark 45 Mod 4 naval gun system. The guns' longer barrels allows more complete combustion of the propellant, reducing barrel flare and improving projectile velocity and firepower against ship and shore targets; additionally, the Mk 45 mod 4 uses a modified gun-house, designed to reduce its radar signature. "Churchill" is armed with Tomahawk, Standard and ASROC (VLA) missiles.
The vessel additionally contains two hangars, not present in earlier destroyers; these can house Light Airborne Multi-Purpose System (LAMPS) Sikorsky SH-60B or MH-60R Seahawk helicopters. These LAMPS can be fitted with air-to-surface missiles for surface ship attacks, and torpedoes for submarine attacks.
The ship is also fitted with the AN/SPY-1D phased array radar - this represents a significant advancement in the detection capabilities of the Aegis weapon system and provides enhanced resistance to electronic countermeasures. The radar can guide more than one hundred missiles at once to targets as far as .
Naming.
On 29 November 1995, on a visit to the United Kingdom, President Bill Clinton announced to both Houses of Parliament that the new ship would be named after former British Prime Minister and Honorary Citizen of the United States, Sir Winston Churchill. It would make it the first warship of the United States Navy to be named after a non-American citizen since 1975, and the first destroyer and only the fourth American warship named after a British citizen.
Other American warships named after Britons were , an armed merchantman named after King Alfred the Great; , a continental frigate, named after Sir Walter Raleigh (though three later USS "Raleigh"s—and two Confederate warships—would be named after the North Carolina city, which did not exist at the time) and , named after The 3rd Earl of Effingham who resigned his commission rather than fight the Americans during the American Revolutionary War. The former frigate was also named after a person from a country in the Commonwealth of Nations, the ill-fated Australian Prime Minister Harold Holt, however, this is the first ship to be named after a modern British hero, and British Prime Minister.
Service history.
The contract to build "Churchill" was awarded to the Bath Iron Works Corporation on 6 January 1995, and the keel was laid down on 7 May 1998. "Churchill" was launched on 17 April 1999, delivered 13 October 2000, and commissioned 10 March 2001. The launch and christening of the ship was co-sponsored by Lady Soames, the daughter of Winston Churchill, and Mrs. Janet Cohen, wife of the Secretary of Defense. Her first commanding officer was Commander (now Rear Admiral) Michael T. Franken.
"Churchill" is the only U.S. Navy vessel to have a Royal Navy Officer permanently assigned to the ship's company (usually a Navigation Officer). The U.S. Navy had a permanent U.S. Navy Officer on the Royal Navy ship, , until its decommission on 8 July 2005. "Churchill" is also the only U.S. Naval vessel to fly a foreign ensign. Being named after a Briton, the Royal Navy's White Ensign is honorarily flown on special occasions from the ship's mast, on the port side, whereas the American flag is flown from the starboard side. However, during normal operations, only the US flag is flown on the center of the main mast.
On 14 May 2001, "Churchill" underwent shock trials off the coast of Florida. These trials subjected the ship to several close-range underwater detonations, each consisting of 7 tons of high explosives, and were performed to collect data concerning ship survivability and damage resistance in a modern threat environment. "Churchill" sustained minor damage during these three tests. On 14 September 2001, (three days after the 11 September 2001 attacks), the German Navy destroyer passed close abeam "Churchill" and rendered honors by manning the rails, flying the Stars and Stripes at half-mast, and the display of a banner reading "We Stand By You." An e-mail sent by an ensign on board "Churchill" described the occasion.
In January 2003, "Churchill" deployed with the battle group in support of the Iraq War's Operation Iraqi Freedom, firing several Tomahawk missiles. "Churchill" returned to Norfolk at the end of May 2003.
On 22 August 2005, "Churchill" was involved in a minor collision with the destroyer off the coast of Jacksonville, Florida. Both ships suffered minor damage, and no injuries were reported. Both ships returned to their homeport at Naval Station Norfolk under their own power.
On 22 January 2006 "Churchill" captured a suspected pirate vessel in the Indian Ocean as part of an ongoing effort to help maintain law and order in the region.
On 26 September 2010, "Churchill" came across a disabled skiff in the Gulf of Aden. After attempts to repair the skiff's engines failed "Churchill" took the vessel under tow towards Somalia. On 27 September the skiff sank when the 85 passengers rushed to one side of the skiff during a food delivery causing the vessel to capsize. "Churchill" was able to rescue 61 of the passengers and continued towards Somalia on 28 September.

</doc>
<doc id="49295" url="https://en.wikipedia.org/wiki?curid=49295" title="Fine-structure constant">
Fine-structure constant

In physics, the fine-structure constant, also known as Sommerfeld's constant, commonly denoted "α" (the Greek letter "alpha"), is a fundamental physical constant characterizing the strength of the electromagnetic interaction between elementary charged particles. It is related to the elementary charge (the electromagnetic coupling constant) "e", which characterizes the strength of the coupling of an elementary charged particle with the electromagnetic field, by the formula . Being a dimensionless quantity, it has the same numerical value in all systems of units. Arnold Sommerfeld introduced the fine-structure constant in 1916.
Definition.
Some equivalent definitions of "α" in terms of other fundamental physical constants are:
where:
The definition reflects the relationship between "α" and the electromagnetic coupling constant "e", which equals .
In non-SI units.
In electrostatic cgs units, the unit of electric charge, the statcoulomb, is defined so that the Coulomb constant, "k"e, or the permittivity factor, 4"πε"0, is 1 and dimensionless. Then the expression of the fine-structure constant, as commonly found in older physics literature, becomes
In natural units, commonly used in high energy physics, where , the value of the fine-structure constant is
As such, the fine-structure constant is just another, albeit dimensionless, quantity determining (or determined by) the elementary charge: in terms of such a natural unit of charge.
Measurement.
The 2014 CODATA recommended value of "α" is
This has a relative standard uncertainty of 0.32 parts per billion.
For reasons of convenience, historically the value of the reciprocal of the fine-structure constant is often specified. The 2014 CODATA recommended value is given by
While the value of "α" can be "estimated" from the values of the constants appearing in any of its definitions, the theory of quantum electrodynamics (QED) provides a way to measure "α" directly using the quantum Hall effect or the anomalous magnetic moment of the electron. The theory of QED predicts a relationship between the dimensionless magnetic moment of the electron and the fine-structure constant "α" (the magnetic moment of the electron is also referred to as "Landé "g"-factor" and symbolized as "g"). The most precise value of "α" obtained experimentally (as of 2012) is based on a measurement of "g" using a one-electron so-called "quantum cyclotron" apparatus, together with a calculation via the theory of QED that involved 12,672 tenth-order Feynman diagrams:
This measurement of "α" has a precision of 0.25 parts per billion. This value and uncertainty are about the same as the latest experimental results.
Physical interpretations.
The fine-structure constant, "α", has several physical interpretations. "α" is:
When perturbation theory is applied to quantum electrodynamics, the resulting perturbative expansions for physical results are expressed as sets of power series in "α". Because "α" is much less than one, higher powers of "α" are soon unimportant, making the perturbation theory practical in this case. On the other hand, the large value of the corresponding factors in quantum chromodynamics makes calculations involving the strong nuclear force extremely difficult.
Variation with energy scale.
According to the theory of the renormalization group, the value of the fine-structure constant (the strength of the electromagnetic interaction) grows logarithmically as the energy scale is increased. The observed value of "α" is associated with the energy scale of the electron mass; the electron is a lower bound for this energy scale because it (and the positron) is the lightest charged object whose quantum loops can contribute to the running. Therefore, 1/137.036 is the value of the fine-structure constant at zero energy. Moreover, as the energy scale increases, the strength of the electromagnetic interaction approaches that of the other two fundamental interactions, a fact important for grand unification theories. If quantum electrodynamics were an exact theory, the fine-structure constant would actually diverge at an energy known as the Landau pole. This fact makes quantum electrodynamics inconsistent beyond the perturbative expansions.
History.
Arnold Sommerfeld introduced the fine-structure constant in 1916, as part of his theory of the relativistic deviations of atomic spectral lines from the predictions of the Bohr model. The first physical interpretation of the fine-structure constant "α" was as the ratio of the velocity of the electron in the first circular orbit of the relativistic Bohr atom to the speed of light in the vacuum. Equivalently, it was the quotient between the minimum angular momentum allowed by relativity for a closed orbit, and the minimum angular momentum allowed for it by quantum mechanics. It appears naturally in Sommerfeld's analysis, and determines the size of the splitting or fine-structure of the hydrogenic spectral lines.
Is the fine-structure constant actually constant?
Physicists have pondered whether the fine-structure constant is in fact constant, or whether its value differs by location and over time. A varying "α" has been proposed as a way of solving problems in cosmology and astrophysics. String theory and other proposals for going beyond the Standard Model of particle physics have led to theoretical interest in whether the accepted physical constants (not just "α") actually vary.
Past rate of change.
The first experimenters to test whether the fine-structure constant might actually vary examined the spectral lines of distant astronomical objects and the products of radioactive decay in the Oklo natural nuclear fission reactor. Their findings were consistent with no variation in the fine-structure constant between these two vastly separated locations and times.
More recently, improved technology has made it possible to probe the value of "α" at much larger distances and to a much greater accuracy. In 1999, a team led by John K. Webb of the University of New South Wales claimed the first detection of a variation in "α". Using the Keck telescopes and a data set of 128 quasars at redshifts 0.5 < "z" < 3, Webb "et al." found that their spectra were consistent with a slight increase in "α" over the last 10–12 billion years. Specifically, they found that
In 2004, a smaller study of 23 absorption systems by Chand "et al.", using the Very Large Telescope, found no measureable variation:
However, in 2007 simple flaws were identified in the analysis method of Chand "et al.", discrediting those results.
King "et al." have used Markov Chain Monte Carlo methods to investigate the algorithm used by the UNSW group to determine formula_17 from the quasar spectra, and have found that the algorithm appears to produce correct uncertainties and maximum likelihood estimates for formula_17 for particular models. This suggests that the statistical uncertainties and best estimate for formula_17 stated by Webb "et al." and Murphy "et al." are robust.
Lamoreaux and Torgerson analyzed data from the Oklo natural nuclear fission reactor in 2004, and concluded that "α" has changed in the past 2 billion years by 45 parts per billion. They claimed that this finding was "probably accurate to within 20%." Accuracy is dependent on estimates of impurities and temperature in the natural reactor. These conclusions have to be verified.
In 2007, Khatri and Wandelt of the University of Illinois at Urbana-Champaign realized that the 21 cm hyperfine transition in neutral hydrogen of the early Universe leaves a unique absorption line imprint in the cosmic microwave background radiation. They proposed using this effect to measure the value of "α" during the epoch before the formation of the first stars. In principle, this technique provides enough information to measure a variation of 1 part in (4 orders of magnitude better than the current quasar constraints). However, the constraint which can be placed on "α" is strongly dependent upon effective integration time, going as "t"−1/2. The European LOFAR radio telescope would only be able to constrain Δ"α"/"α" to about 0.3%. The collecting area required to constrain Δ"α"/"α" to the current level of quasar constraints is on the order of 100 square kilometers, which is economically impracticable at the present time.
Present rate of change.
In 2008, Rosenband "et al." used the frequency ratio of and in single-ion optical atomic clocks to place a very stringent constraint on the present time variation of "α", namely Δ"α̇"/"α" = per year. Note that any present day null constraint on the time variation of alpha does not necessarily rule out time variation in the past. Indeed, some theories that predict a variable fine-structure constant also predict that the value of the fine-structure constant should become practically fixed in its value once the universe enters its current dark energy-dominated epoch.
Spatial variation - Australian dipole.
In September 2010 researchers from Australia said they had identified a dipole-like structure in the variation of the fine-structure constant across the observable universe. They used data on quasars obtained by the Very Large Telescope, combined with the previous data obtained by Webb at the Keck telescopes. The fine-structure constant appears to have been larger by one part in 100,000 in the direction of the southern hemisphere constellation Ara, 10 billion years ago. Similarly, the constant appeared to have been smaller by a similar fraction in the northern direction, 10 billion years ago.
In September and October 2010, after Webb's released research, physicists Chad Orzel and Sean M. Carroll suggested various approaches of how Webb's observations may be wrong. Orzel argues that the study may contain wrong data due to subtle differences in the two telescopes, in which one of the telescopes the data set was slightly high and on the other slightly low, so that they cancel each other out when they overlapped. He finds it suspicious that the triangles in the plotted graph of the quasars are so well-aligned (triangles representing sources examined with both telescopes). Carroll suggested a totally different approach; he looks at the fine-structure constant as a scalar field and claims that if the telescopes are correct and the fine-structure constant varies smoothly over the universe, then the scalar field must have a very small mass. However, previous research has shown that the mass is not likely to be extremely small. Both of these scientists' early criticisms point to the fact that different techniques are needed to confirm or contradict the results, as Webb, et al., also concluded in their study.
In October 2011, Webb "et al." reported
a variation in α dependent on both redshift and spatial direction. They report "the combined data set fits a spatial dipole" with an increase in α with redshift in one direction and a decrease in the other. "ndependent VLT and Keck samples give consistent dipole directions and amplitudes..."
Anthropic explanation.
The anthropic principle is a controversial argument of why the fine-structure constant has the value it does: stable matter, and therefore life and intelligent beings, could not exist if its value were much different. For instance, were "α" to change by 4%, stellar fusion would not produce carbon, so that carbon-based life would be impossible. If "α" were > 0.1, stellar fusion would be impossible and no place in the universe would be warm enough for life as we know it.
Numerological explanations.
As a dimensionless constant which does not seem to be directly related to any mathematical constant, the fine-structure constant has long fascinated physicists.
Arthur Eddington argued that the value could be "obtained by pure deduction" and he related it to the Eddington number, his estimate of the number of protons in the Universe. This led him in 1929 to conjecture that its reciprocal was precisely the integer 137. Other physicists neither adopted this conjecture nor accepted his arguments but by the 1940s experimental values for 1/α deviated sufficiently from 137 to refute Eddington's argument.
The fine-structure constant so intrigued physicist Wolfgang Pauli that he collaborated with psychiatrist Carl Jung in a quest to understand its significance. Similarly, Max Born believed if the value of alpha were any different, the universe would be degenerate, and thus that 1/137 was a law of nature.
Richard Feynman, one of the originators and early developers of the theory of quantum electrodynamics (QED), referred to the fine-structure constant in these terms:
Conversely, statistician I. J. Good argued that a numerological explanation would only be acceptable if it came from a more fundamental theory that also provided a Platonic explanation of the value.
Attempts to find a mathematical basis for this dimensionless constant have continued up to the present time. However, no numerological explanation has ever been accepted by the community.

</doc>
<doc id="49298" url="https://en.wikipedia.org/wiki?curid=49298" title="Haidinger's brush">
Haidinger's brush

Haidinger's brush is an entoptic phenomenon first described by Austrian
physicist Wilhelm Karl von Haidinger in 1844.
Many people are able to perceive polarization of light.
It may be seen as a yellowish horizontal bar or bow-tie shape (with "fuzzy" ends, hence the name "brush") visible in the center of the visual field against the blue sky viewed while facing away from the sun, or on any bright background. It typically occupies roughly 3–5 degrees of vision, about twice or three times the width of one's thumb held at arm's length. The direction of light polarization is perpendicular to the yellow bar (i.e., vertical if the bar is horizontal). Fainter bluish or purplish areas may be visible between the yellow brushes (see illustration). Haidinger's brush may also be seen by looking at a white area on many LCD flat panel computer screens (due to the polarization effect of the display), in which case it is often diagonal.
Physiological causes of Haidinger's brush.
Haidinger's brush is usually attributed to the dichroism of the xanthophyll pigment of the macula. In this effect due to the Fresnel laws, the unguided oblique rays in the cylindrical geometry of the foveal blue cones, along with their distribution, produce an extrinsic dichroism. The brush's size is consistent with the size of the macula. The macula's dichroism is thought to arise from some of its pigment molecules being arranged circularly. The small proportion of circularly arranged molecules accounts for the faintness of the phenomenon. Xanthophyll pigments tend to be parallel to visive nerves that, because the fovea is not flat, are almost orthogonal to the fovea in its central part while being nearly parallel in its outer region. As a result, two different areas of the fovea can be sensitive to two different degrees of polarization.
Seeing Haidinger's brush.
Many people find it difficult to see Haidinger's brush initially. It is very faint, much more so than generally indicated in illustrations, and, like other stabilized images, tends to appear and disappear. 
It is most easily seen when it can be made to move. Since it is always positioned on the macula, there is no way to make it move laterally, but it can be made to rotate, by viewing a white surface through a rotating polarizer, or by slowly tilting one's head to one side.
To see Haidinger's brush, start by using a polarizer, such as a lens from a pair of polarizing sunglasses. Gaze at an evenly lit, textureless surface through the lens and rotate the polarizer.
An option is to use the polarizer built into a computer's LCD screen. Look at a white area on the screen, and slowly tilt the head (a CRT monitor has no polarizer, and will not work for this purpose unless a separate polarizer is used).
It appears with more distinctness against a blue background. With practice, it is possible to see it in the naturally polarized light of a blue sky. Minnaert recommends practicing first with a polarizer, then trying it without. The areas of the sky with the strongest polarization are those 90 degrees away from the sun. Minnaert says that after a minute of gazing at the sky, "a kind of marble effect will appear. This is followed shortly by Haidinger's brush." He comments that not all observers see it in the same way. Some see the yellow pattern as solid and the blue pattern as interrupted, as in the illustrations on this page. Some see the blue as solid and the yellow as interrupted, and some see it alternating between the two states.
Use of Haidinger's brush.
The fact that the sensation of Haidinger's brush corresponds with the visual field correlate of the macula means that it can be utilised in training people to look at objects with their macula. People with certain types of strabismus may undergo an adaptation whereupon they look at the object of attention not with their fovea (at the centre of the macula) but with an eccentric region of the retina. This adaptation is known as eccentric fixation. To aid in training a person to look at an object with their fovea rather than their eccentric retinal zone, a training device can be used. One such apparatus utilises a rotating polarised plate backlit with a bright white light. Wearing blue spectacles (to enhance the Haidinger's brush image) and an occluder over the other eye, the user will hopefully notice the Haidinger's brush where their macula correlates with their visual field. The goal of the training is for the user to learn to look at the test object in such a way that the Haidinger's brush overlaps the test object (and the viewer is thus now looking at it with their fovea). The reason for such training is that the healthy fovea is far greater in its resolving power than any other part of the retina.

</doc>
<doc id="49299" url="https://en.wikipedia.org/wiki?curid=49299" title="Nellie Tayloe Ross">
Nellie Tayloe Ross

Nellie Davis Tayloe Ross (November 29, 1876 – December 19, 1977) was an American politician, the 14th Governor of Wyoming from 1925 to 1927 and director of the United States Mint from 1933 to 1953. She was the first woman to be sworn in as governor of a U.S. state, and remains the only woman to have served as governor of Wyoming.
Ross was born in St. Joseph, Missouri to James Wynn Tayloe, a native of Tennessee, and Elizabeth Blair Green, who owned a plantation on the Missouri River. Her family moved to Miltonvale, Kansas in 1884, and she graduated from Miltonvale High School in 1892. She attended a teacher-training college for two years and taught kindergarten for four years. On September 11, 1902, Ross married William B. Ross, whom she had met when visiting relatives in Tennessee in 1900. William was governor of Wyoming from 1922 to his death on October 2, 1924. Ross succeeded him as governor when she won the special election, becoming the first female American governor on January 5, 1925. She was a staunch supporter of Prohibition during the 1920s. She lost re-election in 1926 but remained an active member of the Democratic Party.
In 1933, Ross became the first female Director of the United States Mint. Despite initial mistrust, she forged a strong bond with Mary Margaret O'Reilly, the Assistant Director of the Mint and one of the United States' highest-ranking female civil servants of her time. Ross served five terms as Director, retiring in 1953. During her later years, she wrote various women's magazines and traveled. Ross died in Washington, D.C., at the age of 101. At the time of her death, she was the oldest ex-governor in the United States.
Early years.
Born Nellie Davis Tayloe in St. Joseph, Missouri, Ross was the sixth child and first daughter of James Wynn Tayloe, a native of Stewart County, Tennessee, and his wife, Elizabeth Blair Green, who owned a plantation on the Missouri River. In 1884, when Ross was seven years of age, her family moved to Miltonvale in Cloud County in northern Kansas. This relocation happened after her father's old family home back in St. Joseph burned, and the sheriff was about to foreclose on the property.
After Ross graduated from Miltonvale High School in 1892, her family moved to Omaha, Nebraska. During this time, she taught private piano lessons and attended a teacher-training college for two years. She then taught kindergarten for four years. Two brothers sent Nellie on a trip to Europe in 1896.
While on a visit to her relatives in Dover, Tennessee, in 1900, Ross met William Bradford Ross, whom she married on September 11, 1902. William practiced law and planned to live in the American West. He moved to Cheyenne and established a law practice, bringing his wife to join him there. William became a leader in the Democratic Party in Wyoming. He ran for office several times unsuccessfully, losing to Republican candidates each time.
Career.
In 1922, William Ross was elected governor of Wyoming by appealing to progressive voters in both parties. However, after little more than a year and a half in office, he died on October 2, 1924, from complications from an appendectomy. The Democratic Party then nominated his widow, Nellie, to run for governor in a special election the following month.
Ross refused to campaign but easily won the race on November 4, 1924. On January 5, 1925, she became the first female governor in the history of the United States. As governor she continued her late husband's policies, which called for tax cuts, government assistance for poor farmers, banking reform, and laws protecting children, women workers, and miners. She urged Wyoming to ratify a pending federal amendment prohibiting child labor. Like her husband, she advocated the strengthening of prohibition laws.
Ross ran for re-election in 1926 but was narrowly defeated. She blamed her loss in part on her refusal to campaign for herself and her support for prohibition. Nevertheless, she remained active in the Democratic Party and campaigned for Al Smith in the 1928 presidential election though the two disagreed on prohibition. At the 1928 Democratic National Convention, she received 31 votes from ten states for vice president on the first ballot. She also gave a speech seconding Smith's nomination. After the convention, she served as vice chairman of the Democratic National Committee and as director of the DNC Women's Division.
U.S. President Franklin D. Roosevelt appointed Ross as director of the U.S. Mint on May 3, 1933, making her the first woman to hold that position. Ross and the Mint's Assistant Director Mary Margaret O'Reilly, "the Sweetheart of the Treasury" who had worked at the Mint since 1904, had mutual suspicions to overcome. Ross, who had endured poor relations with Eleanor Roosevelt and others on FDR's campaign, did not trust the career staff. O'Reilly saw another political appointee with no experience at the Mint Bureau replacing Robert J. Grant, who had been Denver Mint superintendent before his directorship. After a brief period, the two women came to appreciate each other's merits.
Ross and O'Reilly soon came to the usual division of labor between director and assistant: the director would handle public affairs and make policy decisions as needed, while the assistant dealt with the day-to-day business of the bureau. Ross undertook a heavy travel schedule, visiting Mint facilities, making speeches backing Roosevelt, and campaigning for Democratic candidates in Wyoming. This left O'Reilly running the Washington office as acting director. The two women carried on a businesslike but warm correspondence during these times, with O'Reilly writing to Ross (who had embarked on a tour of the mints) "I am so anxious to have your mind at ease about the office here Washington that I have resorted to rather frequent telegrams. They are so much more direct and up to date than letters ... my love to you and every good wish for the success of your visits to our beloved mint institutions." Teva J. Scheer, biographer of Ross, suggests that O'Reilly would have found Ross's reports from the field valuable; they showed how the Mint recovered from the initial years of the Depression, when relatively few coins were produced, to the mid-1930s, when strong demand for coinage led the bureau to run the mints with two or even three shifts.
In 1935, O'Reilly reached the mandatory federal retirement age of 70. Ross requested that President Roosevelt exempt O'Reilly from mandatory retirement because the assistant's knowledge of bureau affairs was so extensive and was so badly needed. A special order of President Roosevelt gave O'Reilly an extra year in the Mint Service. Though Ross supported the extension, she could not be seen as unable to do her job without O'Reilly's assistance, and hired Frank Leland Howard of the University of Virginia, who had a background in accounting, as O'Reilly's prospective replacement. Howard replaced O'Reilly when she retired on October 29, 1938, after two more extensions.
Ross' tenure saw the Mint in 1944 investigate how several 1933 double eagles, never officially released, had come onto the market. She is known for establishing the Franklin Half Dollar and starting the making of proof coins for public sale. Ross served five full terms until her retirement in 1953 and was succeeded by William H. Brett, whom President Dwight D. Eisenhower nominated in 1954.
Retirement and death.
After her retirement, Ross contributed articles to various women's magazines and traveled extensively. She made her last trip to Wyoming in 1972 at the age of ninety-six. Five years later, she died in Washington, D.C., at the age of 101; at the time of her death, she was the oldest ex-governor in the United States. She is interred in the family plot in Lakeview Cemetery in Cheyenne.

</doc>
<doc id="49300" url="https://en.wikipedia.org/wiki?curid=49300" title="Isabella of Angoulême">
Isabella of Angoulême

Isabella of Angoulême (, ; c. 1188 – 4 June 1246) was queen consort of England as the second wife of King John from 1200 until John's death in 1216. She was also reigning Countess of Angoulême from 1202 until 1246.
She had five children by the king, including his heir, later Henry III. In 1220, Isabella married Hugh X of Lusignan, Count of La Marche, by whom she had another nine children.
Some of her contemporaries, as well as later writers, claim that Isabella formed a conspiracy against King Louis IX of France in 1241, after being publicly snubbed by his mother, Blanche of Castile for whom she had a deep-seated hatred. In 1244, after the plot had failed, Isabella was accused of attempting to poison the king. To avoid arrest, she sought refuge in Fontevraud Abbey where she died two years later, but none of this can be confirmed.
Queen of England.
She was the only daughter and heir of Aymer Taillefer, Count of Angoulême, by Alice of Courtenay, who was sister of Peter II of Courtenay, Latin Emperor of Constantinople and granddaughter of King Louis VI of France.
Isabella became Countess of Angoulême in her own right on 16 June 1202, by which time she was already queen of England. Her marriage to King John took place on 24 August 1200, in Angoulême, a year after he annulled his first marriage to Isabel of Gloucester. She was crowned queen in an elaborate ceremony on 8 October at Westminster Abbey in London. Isabella was originally betrothed to Hugh IX le Brun, Count of Lusignan, son of the then Count of La Marche. As a result of John's temerity in taking her as his second wife, King Philip II of France confiscated all of their French lands, and armed conflict ensued.
At the time of her marriage to John, the blonde and blue-eyed 12-year-old Isabella was already renowned by some for her beauty and has sometimes been called the Helen of the Middle Ages by historians. Isabella was much younger than her husband and possessed a volatile temper similar to his own. King John was infatuated with his young, beautiful wife; however, his acquisition of her had as much, if not more to do with spiting his enemies, than romantic love. She was already engaged to Hugh IX le Brun, when she was taken by John. It had been said that he neglected his state affairs to spend time with Isabella, often remaining in bed with her until noon. However, these were rumors, ignited by John's enemies to discredit him as being a weak and grossly irresponsible ruler. Given that at the time they were made John was engaging in a desperate war with King Phillip of France to hold on to the remaining Plantagenet dukedoms. The common people began to term her a "siren" or "Messalina", which spoke volumes as to common opinion . Her mother-in-law, Eleanor of Aquitaine readily accepted her as John's wife.
On 1 October 1207 at Winchester Castle, Isabella gave birth to a son and heir who was named Henry after the King's father, Henry II. He was quickly followed by another son, Richard, and three daughters, Joan, Isabel, and Eleanor. All five children survived into adulthood, and would make illustrious marriages; all but Joan would produce offspring of their own.
Second marriage.
When King John died in October 1216, Isabella's first act was to arrange the speedy coronation of her nine-year-old son at the city of Gloucester on 28 October. As the royal crown had recently been lost in The Wash, along with the rest of King John's treasure, she supplied her own golden circlet to be used in lieu of a crown. The following July, less than a year after his crowning as King Henry III of England, she left him in the care of his regent, William Marshal, 1st Earl of Pembroke and returned to France to assume control of her inheritance of Angoulême.
In the spring of 1220, she married Hugh X of Lusignan, "le Brun", Seigneur de Luisignan, Count of La Marche, the son of her former fiancé, Hugh IX, to whom she had been betrothed before her marriage to King John. It had been previously arranged that her eldest daughter Joan should marry Hugh, and the little girl was being brought up at the Lusignan court in preparation for her marriage. Hugh, however, upon seeing Isabella, whose beauty had not diminished, preferred the girl's mother. Princess Joan was provided with another husband, King Alexander II of Scotland, whom she wed in 1221.
Isabella had married Hugh without the consent of the king's council in England, as was required of a queen dowager. That council had the power not only to assign to her any subsequent husband, but to decide whether she should be allowed to remarry at all. That Isabella flouted its authority moved the council to confiscate her dower lands and to stop the payment of her pension. Isabella and her husband retaliated by threatening to keep Princess Joan, who had been promised in marriage to the King of Scotland, in France. The council first responded by sending furious letters to the Pope, signed in the name of young King Henry, urging him to excommunicate Isabella and her husband, but then decided to come to terms with Isabella, to avoid conflict with the Scottish king, who was eager to receive his bride. Isabella was granted the stannaries in Devon, and the revenue of Aylesbury for a period of four years, in compensation for her confiscated dower lands in Normandy, as well as the £3,000 arrears for her pension.
Isabella had nine more children by Hugh X. Their eldest son Hugh XI of Lusignan succeeded his father as Count of La Marche and Count of Angoulême in 1249.
Isabella's children from her earlier marriage remained all their lives in England.
Rebellion and death.
Described by some contemporaries as "vain, capricious, and troublesome," Isabella could not reconcile herself with her less prominent position in France. Though Queen mother of England, Isabella was now mostly regarded as a mere Countess of La Marche and had to give precedence to other women. In 1241, when Isabella and Hugh were summoned to the French court to swear fealty to King Louis IX of France's brother, Alphonse, who had been invested as Count of Poitou, their mother, the Queen Dowager Blanche openly snubbed her. This so infuriated Isabella, who had a deep-seated hatred of Blanche for having fervently supported the French invasion of England during the First Barons' War in May 1216, that she began to actively conspire against King Louis. Isabella and her husband, along with other disgruntled nobles, including her son-in-law Raymond VII of Toulouse, sought to create an English-backed confederacy which united the provinces of the south and west against the French king. She encouraged her son Henry in his invasion of Normandy in 1230, but then did not provide him the support she had promised.
In 1244, after the confederacy had failed and Hugh had made peace with King Louis, two royal cooks were arrested for attempting to poison the King; upon questioning they confessed to having been in Isabella's pay. Before Isabella could be taken into custody, she fled to Fontevraud Abbey, where she died on 4 June 1246.
By her own prior arrangement, she was first buried in the Abbey's churchyard, as an act of repentance for her many misdeeds. On a visit to Fontevraud, her son King Henry III of England was shocked to find her buried outside the Abbey and ordered her immediately moved inside. She was finally placed beside Henry II and Eleanor of Aquitaine. Afterwards, most of her many Lusignan children, having few prospects in France, set sail for England and the court of Henry, their half-brother.
In popular culture.
She was played by actress Zena Walker in the TV series "The Adventures of Robin Hood" episode "Isabella" (1956), before her marriage to John, but not as a 12-year-old.
She was portrayed by actress Victoria Abril in the 1976 film "Robin and Marian".
She was played by actress Cory Pulman in the episode "The Pretender" (1986) of the TV series "Robin of Sherwood".
She was portrayed by actress Léa Seydoux in the 2010 film "Robin Hood".

</doc>
<doc id="49302" url="https://en.wikipedia.org/wiki?curid=49302" title="Pope Eleuterus">
Pope Eleuterus

Pope Eleuterus (died 189), also known as Eleutherius, was the Bishop of Rome from c. 174 to his death in 189. (The Vatican cites 171 or 177 to 185 or 193.) According to the "Liber Pontificalis", he was a Greek born in Nicopolis in Epirus, Greece. His contemporary Hegesippus wrote that he was a deacon of the Roman Church under Pope Anicetus (c. 154–164), and remained so under Pope Soter, whom he succeeded around 174.
Dietary law.
The 6th-century recension of "The Book of the Popes" known as the "Felician Catalog" includes additional commentary to the work's earlier entry on Eleuterus. One addition ascribes to Eleutherius a decree (or reïssuance of a decree) that no kind of food should be despised by Christians. Such a decree might have been issued against early continuations of Jewish dietary law and against similar laws practiced by the Gnostics and Montanists. It is also possible, however, that the editor of the passage attributed to Eleuterus a decree similar to another issued around the year 500 in order to give it greater authority.
British mission.
Another addition credited Eleuterus with receiving a letter from "Lucius, King of Britain" or "King of the Britons", declaring an intention to convert to Christianity. No earlier accounts of this mission have been found. It is now generally considered to be a pious forgery, although there remains disagreement over its original purpose. Haddan and Stubbs considered the passage "manifestly written in the time and tone" of St Prosper, secretary to Pope Leo the Great in the mid-5th century, and supportive of the missions of SS Germanus and Palladius. Duchesne dated the entry a little later to the pontificate of Boniface II around 530 and Mommsen to the early 7th century. Only the last would support the conjecture that it aimed to support the Gregorian mission to the Anglo-Saxons led by St Augustine, who encountered great difficulty with the native British Christians, as at the Synod of Chester. Indeed, the Celtic Christians invoked the antiquity of their church to generally "avoid" submission to Canterbury until the Norman conquest but it is noteworthy that no arguments invoking the mission to Lucius appear to have been made by either side during the synods among the Welsh and Saxon bishops.
The first British writer to mention the story was Bede and he seems to have taken it, not from native texts or traditions, but from "The Book of the Popes". Subsequently, it appeared in the 9th-century "History of the Britons" traditionally credited to Nennius: the account relates that a mission from the pope baptized "Lucius, the Britannic king, with all the petty kings of the whole Britannic people". The account, however, dates this baptism to  167 (a little before Eleuterus's pontificate) and credits it to Evaristus (reigned ). In the 12th century, more details began to be added to the story. Geoffrey of Monmouth's pseudohistorical "History of the Kings of Britain" goes into great detail concerning Lucius and names the pope's envoys to him as Fagan and Duvian. "The Book of Llandaf" placed the court of Lucius in southern Wales and names his emissaries to the pope as Elfan and Medwy.
An echo of this legend penetrated even to Switzerland. In a homily preached at Chur and preserved in an 8th- or 9th-century manuscript, St Timothy is represented as an apostle to Gaul, whence he went into Britain and baptized a king named Lucius, who himself became a missionary to Gaul and finally settled at Chur, where he preached the gospel with great success. In this way Lucius, the early missionary of the Swiss district of Chur, became identified with the alleged British king of the "Liber Pontificalis".
Harnack suggests that in the document which the compiler of the "Liber Pontificalis" drew his information the name found was not Britanio, but Britio. Now this is the name (Birtha- Britium) of the fortress of Edessa. The king in question is, therefore, Lucius Ælius Septimus Megas Abgar IX, of Edessa, a Christian king, as is well known. The original statement of the "Liber Pontificalis", in this hypothesis, had nothing to do with Britain. The reference was to Abgar IX of Edessa. But the compiler of the "Liber Pontificalis" changed Britio to Brittanio, and in this way made a British king of the Syrian Lucius.
Death.
According to the "Liber Pontificalis", Pope Eleutherius died on 24 May and was buried on the Vatican Hill ("in Vaticano") near the body of St. Peter. Later tradition has his body moved to the church of San Giovanni della Pigna, near the pantheon. In 1591, his remains were again moved to the church of Santa Susanna at the request of Camilla Peretti, the sister of Pope Sixtus V. His feast is celebrated on 26 May.

</doc>
<doc id="49304" url="https://en.wikipedia.org/wiki?curid=49304" title="Henry Morton Stanley">
Henry Morton Stanley

Henry Morton Stanley GCB (born John Rowlands; 28 January 1841 – 10 May 1904) was a Welsh journalist and explorer who was famous for his exploration of central Africa and his search for missionary and explorer David Livingstone. Upon finding Livingstone, Stanley reportedly asked, "Dr. Livingstone, I presume?" Stanley is also known for his search for the source of the Nile, his work in and development of the Congo Basin region in association with King Leopold II of the Belgians, and commanding the Emin Pasha Relief Expedition. He was knighted in 1899.
Early life.
Henry Stanley was born in 1841 as John Rowlands in Denbigh, Denbighshire, Wales. His mother Elizabeth Parry was 18 years old at the time of his birth. She abandoned him as a very young baby and cut off all communication. Stanley never knew his father, who died within a few weeks of his birth. There is some doubt as to his true parentage. As his parents were unmarried, his birth certificate describes him as a bastard, and the stigma of illegitimacy weighed heavily upon him all his life.
The boy John was given his father's surname of Rowlands and brought up by his maternal grandfather Moses Parry, a once-prosperous butcher who was living in reduced circumstances. He cared for the boy until he died, when John was five. Rowlands stayed with families of cousins and nieces for a short time, but he was eventually sent to the St. Asaph Union Workhouse for the Poor. The overcrowding and lack of supervision resulted in his being frequently abused by older boys. Historian Robert Aldrich has alleged that the headmaster of the workhouse raped or sexually assaulted Rowlands. When Rowlands was ten, his mother and two half-siblings stayed for a short while in this workhouse, but he did not recognize them until the headmaster told him who they were.
New country, new name.
Rowlands emigrated to the United States in 1859 at age 18. He disembarked at New Orleans and, according to his own declarations, became friends by accident with Henry Hope Stanley, a wealthy trader. He saw Stanley sitting on a chair outside his store and asked him if he had any job openings. He did so in the British style: "Do you need a boy, sir?" The childless man had indeed been wishing he had a son, and the inquiry led to a job and a close relationship between them. Out of admiration, John took Stanley's name. Later, he wrote that his adoptive parent died two years after their meeting, but in fact the elder Stanley did not die until 1878.
Stanley reluctantly joined in the American Civil War, first enrolling in the Confederate Army's 6th Arkansas Infantry Regiment and fighting in the Battle of Shiloh in 1862. After being taken prisoner, he was recruited at Camp Douglas, Illinois by its commander Colonel James A. Mulligan as a "Galvanized Yankee." He joined the Union Army on 4 June 1862 but was discharged 18 days later because of severe illness. After recovering, he served on several merchant ships before joining the US Navy in July 1864. He became a record keeper on board the "USS Minnesota", which led him into freelance journalism. Stanley and a junior colleague jumped ship on 10 February 1865 at a port in New Hampshire in search of greater adventures. Stanley was possibly the only man to serve in the Confederate Army, the Union Army, and the Union Navy.
Journalist.
Following the Civil War, Stanley became a journalist and organised an expedition to the Ottoman Empire that ended catastrophically when he was imprisoned. He eventually talked his way out of jail and received restitution for damaged expedition equipment.
Finding Livingstone.
Stanley travelled to Zanzibar in March 1871, later claiming that he outfitted an expedition with 192 porters. In his first despatch to the "New York Herald", however, he stated that his expedition numbered only 111. This was in line with figures in his diaries. James Gordon Bennett, Jr., publisher of the "New York Herald" and funder of the expedition, had delayed sending to Stanley the money he had promised, so Stanley borrowed money from the United States Consul.
During the expedition through the tropical forest, his thoroughbred stallion died within a few days after a bite from a tsetse fly, many of his porters deserted, and the rest were decimated by tropical diseases.
Stanley found Livingstone on 10 November 1871 in Ujiji, near Lake Tanganyika in present-day Tanzania. He may have greeted him with the now-famous line, "" It may also have been a fabrication, as Stanley tore out of his diary the pages relating to the encounter. Neither man mentioned it in any of the letters they wrote at this time. Livingstone's account of the encounter does not mention these words. The phrase is first quoted in a summary of Stanley's letters published by "The New York Times" on 2 July 1872. Stanley biographer Tim Jeal argued that the explorer invented it afterwards to help raise his standing because of "insecurity about his background".
The "Herald"'s own first account of the meeting, published 1 July 1872, reports: 
Preserving a calmness of exterior before the Arabs which was hard to simulate as he reached the group, Mr. Stanley said: – "Doctor Livingstone, I presume?" A smile lit up the features of the pale white man as he answered: "Yes, and I feel thankful that I am here to welcome you."
Stanley joined Livingstone in exploring the region, finding that there was no connection between Lake Tanganyika and the Nile. On his return, he wrote a book about his experiences: "How I Found Livingstone; travels, adventures, and discoveries in Central Africa".
Researching the Congo River.
In 1874, the "New York Herald" and Britain's "Daily Telegraph" financed Stanley on another expedition to Africa. His objective was to complete the exploration and mapping of the central African lakes and rivers, in the process circumnavigating Lakes Victoria and Tanganyika and locating the source of the Nile.
A related mission was to trace the course of the Congo River to the sea. During this expedition, Stanley used sectional boats and dug-out canoes to pass the large cataracts that separated the Congo into distinct tracts. The boats were taken apart and transported around the rapids before being rebuilt to travel on the next section of river. He reached the Portuguese outpost of Boma, around from the mouth of the Congo River, after 999 days on 9 August 1877. Muster lists and Stanley's diary (12 November 1874) show that he started with 228 people and reached Boma with 114 survivors, with he being the only European left out of three. In Stanley's "Through the Dark Continent" (1878) (which coined the term "Dark Continent" for Africa), Stanley said that his expedition had numbered 356.
Stanley attributed his success to his leading African porters, saying that his success was "all due to the pluck and intrinsic goodness of 20 men ... take the 20 out and I could not have proceeded beyond a few days journey".
Claiming the Congo for the Belgian king.
Stanley was approached by King Leopold II of the Belgians, the ambitious Belgian monarch who had organized a private holding company in 1876 disguised as an international scientific and philanthropic association, which he called the International African Association. The king spoke of his intention to introduce western civilization and bring religion to that part of Africa, but did not mention the fact that he wanted to claim the lands. At the end of his life, the king was embittered by the growing perception that his establishment of a Congo Free State was mitigated by its unscrupulous government.
Emin Pasha Relief Expedition.
In 1886, Stanley led the Emin Pasha Relief Expedition to "rescue" Emin Pasha, the governor of Equatoria in the southern Sudan. King Leopold II demanded that Stanley take the longer route via the Congo River, hoping to acquire more territory and perhaps even Equatoria After immense hardships and great loss of life, Stanley met Emin in 1888, charted the Ruwenzori Range and Lake Edward, and emerged from the interior with Emin and his surviving followers at the end of 1890. But this expedition tarnished Stanley's name because of the conduct of the other Europeans — British gentlemen and army officers. Army Major Edmund Musgrave Barttelot was shot by a carrier after behaving with extreme cruelty. James Sligo Jameson, heir to Irish whiskey manufacturer Jameson's, bought an 11-year-old girl and offered her to cannibals to document and sketch how she was cooked and eaten. Stanley found out only when Jameson had died of fever.
The spread of sleeping sickness across areas of central and eastern Africa that were previously free of the disease has been attributed to this expedition. But this hypothesis has been disputed. Sleeping sickness had been endemic in these regions for generations and then flared into epidemics as colonial trade increased trade throughout Africa during the ensuing decades.
Later years.
On his return to Europe, Stanley married Welsh artist Dorothy Tennant. They adopted a child named Denzil who donated around 300 items to the Stanley archives at the Royal Museum of Central Africa in Tervuren, Belgium in 1954. He died in 1959.
Stanley entered Parliament as a Liberal Unionist member for Lambeth North, serving from 1895 to 1900. He became Sir Henry Morton Stanley when he was made a Knight Grand Cross of the Order of the Bath in the 1899 Birthday Honours, in recognition of his service to the British Empire in Africa.
He died in London on 10 May 1904. At his funeral, he was eulogised by Daniel P. Virmar. His grave is in the churchyard of St. Michael and All Angels Church in Pirbright, Surrey, marked by a large piece of granite inscribed with the words "Henry Morton Stanley, Bula Matari, 1841–1904, Africa". Bula Matari translates as "Breaker of Rocks" or "Breakstones" in Kongo and was Stanley's name among locals in Congo. It can be translated as a term of endearment for, as the leader of Leopold's expedition, he commonly worked with the labourers breaking rocks with which they built the first modern road along the Congo River. It can also be translated in far less flattering terms. Author Adam Hochschild suggested that Stanley understood it as a heroic epithet, but there is evidence that Nsakala, the man who originally coined it, had meant it humorously. 
Charges of racism and cruelty.
Overview.
Stanley has been accused of indiscriminate cruelty against Africans. Some of the modern accusations can be explained away as journalistic exaggerations, although some of his contemporaries also brought the same charges, including men who served under him or had first hand information.
Stanley acknowledged that "any people have called me hard, but they are always those whose presence a field of work could best dispense with, and whose nobility is too nice to be stained with toil."
The authors of the book "The Congo: Plunder and Resistance" argued that Stanley had "a pathological fear of women, an inability to work with talented co-workers, and an obsequious love of the aristocratic rich", Stanley's intimate correspondence in the Royal Museum of Central Africa, however, between him and his two fiancées and between him and his wife implies that he enjoyed close relationships with at least a few women. Also, Stanley's closest male friends were a journalist and a former warehouseman, and his lecture agent stated that his client disliked grand social occasions and preferred being with old friends.
Having survived for ten years of his childhood in the workhouse at St Asaph, Stanley needed as a young man to be thought of as harder and more formidable than other explorers. This made him exaggerate punishments and hostile encounters. It was a serious error of judgement for which his reputation continues to pay a heavy price.
Professor Norman R. Bennett of Boston University, who edited the 1970 book "Stanley's Despatches to the New York Herald," said the following in his introduction to the book: "Stanley remains one of the most controversial of the major European explorers of Africa. His often turbulent career and the internal stresses of his personality help to explain this fact. Nonetheless, there is no apparent reason why, more than three-quarters of a century after his last venture, Stanley should continue to be singled out for his supposed excesses in Africa, while other European explorers, often responsible for far more loss of life than Stanley, receive sympathetic treatment." For example, Pierre Savorgnan de Brazza, who was revered in France, had shot Africans in self-defence, but unlike Stanley, never spoke of it. Samuel Baker killed far more Africans than Stanley did and in 1873 was mauled in the press for "cold blooded murder". David Livingstone shot dead several African Yao slave traders in 1861 when they attacked the mission at Magomero.
General opinion about Africans.
In "Through the Dark Continent", Stanley wrote that "the savage only respects force, power, boldness, and decision."
Yet in "How I Found Livingstone", he wrote that he was "prepared to admit any black man possessing the attributes of true manhood, or any good qualities ... to a brotherhood with myself." Stanley also hit Shaw, a racist white colleague on his Livingstone expedition. Stanley insulted and shouted at William Grant Stairs and Arthur Jephson for mistreating the Wangwana. He also described the history of Boma as "two centuries of pitiless persecution of black men by sordid whites". He also wrote about the superior beauty of black people in comparison with whites.
Stanley categorized some Africans as "lazy" in some of his writings.
Opinion about mixed African-Arab peoples.
In one of his books, Stanley said about mixed African-Arab people: "For the half-castes I have great contempt. They are neither black nor white, neither good nor bad, neither to be admired nor hated. They are all things, at all times... If I saw a miserable, half-starved negro, I was always sure to be told, he belonged to a half-caste. Cringing and hypocritical, cowardly and debased, treacherous and mean ... this syphilitic, blear-eyed, pallid-skinned, abortion of an Africanized Arab."
When Stanley first met a group of his Wangwana assistants, he was surprised that, "They were an exceedingly fine looking body of men, far more intelligent in appearance than I could ever have believed African barbarians could be."
These Wangwana of Zanzibar were of mixed Arabian and African ancestry - "Africanized Arabs" in Stanley's words. They became the backbone of all his major expeditions and were referred to as "his dear pets" by sceptical young officers on the Emin Pasha Expedition who resented their leader for favouring the Wangwana above themselves. "All are dear to me," Stanley told William Grant Stairs and Arthur Jephson, "who do their duty and the Zanzibaris have quite satisfied me on this and on previous expeditions." Stanley came to think of an individual Wangwana as "superior in proportion to his wages to ten Europeans."
Alleged cruel treatment of Africans.
Writer Tim Jeal has argued that during Stanley's 1871 expedition, he treated his indigenous porters well under the standards existing then. Three-quarters of his African assistants on his third expedition had enlisted with him on an earlier journey.
Richard Francis Burton, however, claimed that "Stanley shoots negroes as if they were monkeys". Burton wrote this in a letter to General Charles George Gordon, who replied that Stanley's killing in self-defence had been permissible. "These things may be done," he wrote, "but not advertised." On the subject of self-defence, Stanley wrote: "We went into the heart of Africa uninvited, therein lies our fault, but it was not so grave that our lives threatened should be forfeited."
Immediately after one of Stanley's expeditions in 1877, Reverend J. P. Farler met with African porters who had been part of the expedition and wrote, "Stanley's followers give dreadful accounts to their friends of the killing of inoffensive natives, stealing their ivory and goods, selling their captives, and so on. I do think a commission ought to inquire into these charges, because if they are true, it will do untold harm to the great cause of emancipating Africa. ... I cannot understand all the killing that Stanley has found necessary." 
In 1877, Augustus Sparhawk, a trader with a United States company on Zanzibar, alleged in writing that John Kirk, who detested Stanley for blackening his character, had bribed two of Stanley's assistants, Manwa Sera and Kacheche, to tell missionaries and others that Stanley had behaved brutally in Africa.
The Baptist missionary Thomas J. Comber wrote differently about Stanley saying that, "by constant daily exercise of his tact and influence over the people ... Stanley has succeeded in planting his station at Stanley Pool without a fight", despite being faced by Africans "who are very fond of fighting and can muster 3000 guns."
Stanley wrote with some measure of satisfaction when describing how Captain John Hanning Speke - the first European to visit Uganda - had punched in the teeth for disobedience Mbarak Bombay (a caravan leader also employed by Stanley), causing Stanley to claim that he would never allow Bombay to have the audacity to stand up for a boxing match with him. In the same paragraph, Stanley described how he several months later administered punishment to the African.
William Grant Stairs found Stanley during the Emina Pasha expedition to be cruel, secretive, and selfish. This, however, was after Stanley had come across a letter from Stairs to another officer, in which Stairs said that he (Stanley) was spying on his officers and meanly cheating them out of their fair share of food. As a result, the two men fell out. Unknown to Stanley, when he was away searching for Emin Pasha, Stairs and Thomas Heazle Parke had murdered at least a dozen pygmies (including women and children) whom they had caught stealing from the expedition's temporary vegetable garden at Fort Bodo near Lake Albert.
Stanley was admired by Arthur Jephson, whom William Bonny, the acerbic medical assistant, described as the "most honourable" officer on the expedition. Jephson wrote: "Stanley never fights where there is the smallest chance of making friends with the natives and he is wonderfully patient & long suffering with them."
John Rose Troup in his book about the Emin Pasha expedition said that he saw Stanley's self-serving and vindictive side. "In the forgoing letter he brings forward disgraceful charges, that really do not refer to me at all, although he blames me for what happened. The injustice of his accusations, made as they are without documentary or, as far as I can learn, any evidence, can hardly be made clear to the public, but they must be aware, when they read what has preceded this correspondence, that he has acted as no one in his position should have acted." Stanley had accused Troup and his colleagues in the Rear Column of failing to prevent the deranged Edmund Musgrave Barttelot from committing murders.
Possible inspiration for "Heart of Darkness".
The legacy of death and destruction in the Congo region, and the fact that Stanley had worked for King Leopold II of Belgium, is considered by author Sherry Norman to have made him an inspiration for Joseph Conrad's "Heart of Darkness."
Conrad, however, had spent six months of 1890 as a steamship captain on the Congo, years after Stanley had been there (1879-1884) and five years after Stanley had been recalled to Europe and ceased to be King Leopold's chief agent in Africa. By 1890, forced labour was being used to coerce Africans into collecting rubber. But when Stanley had been there, the inner tube for bicycle tyres had not yet been invented and there had been little demand for rubber.
Modern media.
His great-grandson Richard Stanley is a South African filmmaker and directs documentaries.
"Stanley and Livingstone", a popular film, was released in 1939 with Spencer Tracy as Stanley and Cedric Hardwicke as Livingstone.
The 1949 comedy film "Africa Screams" is the story of a dimwitted clerk named Stanley Livington, played by Lou Costello. He is mistaken for a famous African explorer and recruited to lead a treasure hunt.
In 1971, the BBC produced a six-part dramatised documentary series entitled "Search for the Nile". Much of the series was shot on location, with Stanley played by Keith Buckley.
Stanley appears as a character in Simon Gray's 1978 play "The Rear Column". The play tells the story of the men left behind to wait for Tippu Tib while Stanley went on to relieve Emin Pasha.
A Nintendo Entertainment System game based on his life was released in 1992 called "".
In 1997, the made-for-television film "Forbidden Territory: Stanley's Search for Livingstone" was produced by National Geographic. Stanley was portrayed by Aidan Quinn, and Livingstone was portrayed by Nigel Hawthorne.
In 2004, Welsh journalist Tim Butcher wrote his book "Blood River: A Journey Into Africa's Broken Heart". The book followed Stanley's journey through the Congo.
The 2009 History Channel series "Expedition Africa" documented a group of explorers attempting to traverse the route of Stanley's expedition in search of Livingstone.
In 2015, Oscar Hijuelos's novel "Twain & Stanley Enter Paradise" retold the story of Stanley's life through a focus on his friendship with Mark Twain.
Posthumous honours.
A hospital in St. Asaph, northern Wales is named after Stanley in honour of his birth in the area. It was formerly the workhouse in which he spent much of his early life. Memorials to Stanley have been erected in St. Asaph and in Denbigh (a statue of Stanley with an outstretched hand).
Taxa named in honour.
Taxa named in honour of Stanley include:

</doc>
<doc id="49307" url="https://en.wikipedia.org/wiki?curid=49307" title="Standing (law)">
Standing (law)

In law, standing or locus standi is the term for the ability of a party to demonstrate to the court sufficient connection to and harm from the law or action challenged to support that party's participation in the case. Standing exists from one of three causes:
In the United States, the current doctrine is that a person cannot bring a suit challenging the constitutionality of a law unless the plaintiff can demonstrate that he/she/it is or will "imminently" be harmed by the law. Otherwise, the court will rule that the plaintiff "lacks standing" to bring the suit, and will dismiss the case without considering the merits of the claim of unconstitutionality. To have a court declare a law unconstitutional, there must be a valid reason for the lawsuit. The party suing must have something to lose in order to sue unless it has automatic standing by action of law.
International courts.
The Council of Europe created the first international court before which individuals have automatic "locus standi".
Canada.
In Canadian administrative law, whether an individual has standing to bring an application for judicial review, or an appeal from the decision of a tribunal, is governed by the language of the particular statute under which the application or the appeal is brought. Some statutes provide for a narrow right of standing while others provide for a broader right of standing.
Frequently a litigant wishes to bring a civil action for a declaratory judgment against a public body or official. This is considered an aspect of administrative law, sometimes with a constitutional dimension, as when the litigant seeks to have legislation declared unconstitutional.
Public interest standing.
The Supreme Court of Canada developed the concept of public interest standing in three constitutional cases commonly called "the Standing trilogy": "Thorson v. Canada (Attorney General)", "Nova Scotia Board of Censors v. McNeil", and "Minister of Justice v. Borowski". The trilogy was summarized as follows in "Canadian Council of Churches v. Canada (Minister of Employment and Immigration)":
Public-interest standing is also available in non-constitutional cases, as the Court found in "Finlay v. Canada (Minister of Finance)".
United Kingdom.
In British administrative law, the applicant needs to have a sufficient interest in the matter to which the application relates. This sufficient interest requirement has been construed liberally by the courts. As Lord Diplock put it:
"t would...be a grave danger to escape "lacuna" in our system of public law if a pressure group...or even a single public spirited taxpayer, were prevented by outdated technical rules of "locus standi" from bringing the matter to the attention of the court to vindicate the rule of law and get the unlawful conduct stopped."
Australia.
Australia has a Common law understanding of "locus standi" or standing which is expressed in statutes such as the Administrative Decisions (Judicial Review) Act 1977 and common law decisions of the High Court of Australia especially the case "Australian Conservation Foundation v Commonwealth" (1980). The test for Standing is:
1. Do the party have special interest in the matter.<br>. Is that interest too distant?
There is no open standing unless statute allows it or represents needs of a specified class of people. The issue is one of remoteness.
Standing may apply to class of aggrieved people where, essentially the closeness of the plaintiff to the subject matter is the test.<br> Furthermore, a plaintiff must show that he or she has been specially affected in comparison with the public at large. 
Also, while there is no open standing per se, Prerogative writs like certiorari, prohibition, Quo warranto and habeas corpus have a low burden in establishing standing. and the various Attorneys Generals have a presumed standing in Administrative Law cases.
United States.
In United States law, the Supreme Court of the United States has stated, "In essence the question of standing is whether the litigant is entitled to have the court decide the merits of the dispute or of particular issues."
There are a number of requirements that a plaintiff must establish to have standing before a federal court. Some are based on the case or controversy requirement of the judicial power of Article Three of the United States Constitution, § 2, cl.1. As stated there, "The Judicial Power shall extend to all Cases . . . to Controversies . . ." The requirement that a plaintiff have standing to sue is a limit on the role of the judiciary and the law of Article III standing is built on the idea of separation of powers. Federal courts may exercise power only "in the last resort, and as a necessity".
The American doctrine of standing is assumed as having begun with the case of "Frothingham v. Mellon", 262 U.S. 447 (1923). But legal standing truly rests its first prudential origins in "Fairchild v. Hughes", (1922) which was authored by Justice Brandeis. In "Fairchild", a citizen sued the Secretary of State and the Attorney General to challenge the procedures by which the Nineteenth Amendment was ratified. Prior to it the doctrine was that all persons had a right to pursue a private prosecution of a public right. Since then the doctrine has been embedded in judicial rules and some statutes.
In 2011, in "Bond v. United States", the U.S. Supreme Court held that a criminal defendant has standing to challenge the federal statute he or she is charged with violating as being unconstitutional under the Tenth Amendment.
Standing requirements.
There are three standing requirements:
Prudential limitations.
Additionally, there are three major prudential (judicially created) standing principles. Congress can override these principles via statute:
Recent development of the doctrine.
In 1984, the Supreme Court reviewed and further outlined the standing requirements in a major ruling concerning the meaning of the three standing requirements of injury, causation, and redressability.
In the suit, parents of black public school children alleged that the Internal Revenue Service was not enforcing standards and procedures that would deny tax-exempt status to racially discriminatory private schools. The Court found that the plaintiffs did not have the standing necessary to bring suit. Although the Court established a significant injury for one of the claims, it found the causation of the injury (the nexus between the defendant’s actions and the plaintiff’s injuries) to be too attenuated. "The injury alleged was not fairly traceable to the Government conduct respondents challenge as unlawful".
In another major standing case, Lujan v. Defenders of Wildlife, 504 U.S. 555 (1992), the Supreme Court elaborated on the redressability requirement for standing. The case involved a challenge to a rule promulgated by the Secretary of the Interior interpreting §7 of the Endangered Species Act of 1973 (ESA). The rule rendered §7 of the ESA applicable only to actions within the United States or on the high seas. The Court found that the plaintiffs did not have the standing necessary to bring suit, because no injury had been established. The injury claimed by the plaintiffs was that damage would be caused to certain species of animals and that this in turn injures the plaintiffs by the reduced likelihood that the plaintiffs would see the species in the future. The court insisted though that the plaintiffs had to show how damage to the species would produce imminent injury to the plaintiffs. The Court found that the plaintiffs did not sustain this burden of proof. "The 'injury in fact' test requires more than an injury to a cognizable interest. It requires that the party seeking review be himself among the injured". The injury must be imminent and not hypothetical.
Beyond failing to show injury, the Court found that the plaintiffs failed to demonstrate the standing requirement of redressability. The Court pointed out that the respondents chose to challenge a more generalized level of Government action, "the invalidation of which would affect all overseas projects". This programmatic approach has "obvious difficulties insofar as proof of causation or redressability is concerned".'"
In a 2000 case, "Vermont Agency of Natural Resources v. United States ex rel. Stevens", 529 U.S. 765 (2000), the United States Supreme Court endorsed the "partial assignment" approach to qui tam relator standing to sue under the False Claims Act — allowing private individuals to sue on behalf of the U.S. government for injuries suffered solely by the government.
Taxpayer standing.
The initial case that established the doctrine of standing, "Frothingham v. Mellon", was a taxpayer standing case.
Taxpayer standing is the concept that any person who pays taxes should have standing to file a lawsuit against the taxing body if that body allocates funds in a way that the taxpayer feels is improper. The United States Supreme Court has held that taxpayer standing is not by itself a sufficient basis for standing against the United States government, unless the narrower Flast test is met. The Court has consistently found that the conduct of the federal government is too far removed from individual taxpayer returns for any injury to the taxpayer to be traced to the use of tax revenues, e.g., "United States v. Richardson."
In "DaimlerChrysler Corp. v. Cuno", the Court extended this analysis to state governments as well. However, the Supreme Court has also held that taxpayer standing is constitutionally sufficient to sue a municipal government in a federal court.
States are also protected against lawsuits by their sovereign immunity. Even where states waive their sovereign immunity, they may nonetheless have their own rules limiting standing against simple taxpayer standing against the state. Furthermore, states have the power to determine what will constitute standing for a litigant to be heard in a state court, and may deny access to the courts premised on taxpayer standing alone.
In Florida, a taxpayer has standing to sue if the state government is acting unconstitutionally with respect to public funds, or if government action is causing some special injury to the taxpayer that is not shared by taxpayers in general. In Virginia, the Supreme Court of Virginia has more or less adopted a similar rule. An individual taxpayer generally has standing to challenge an act of a city or county where they live, but does not have general standing to challenge state expenditures.
Standing to challenge statutes.
With limited exceptions, a party cannot have standing to challenge the constitutionality of a statute unless they will be subjected to the provisions of that statute. There are some exceptions, however, e.g. courts will accept First Amendment challenges to a statute on overbreadth grounds, where a person who is only partially affected by a statute can challenge parts that do not affect them on the grounds that laws that restrict speech have a chilling effect on other people's right to free speech.
The only other way someone can have standing to challenge the constitutionality of a statute is if the existence of the statute would otherwise deprive them of a right or a privilege even if the statute itself would not apply to them. The Virginia Supreme Court made this point clear in the case of "Martin v. Ziherl" 607 S.E.2d 367 (Va. 2005). Martin and Ziherl were girlfriend and boyfriend and engaged in unprotected sexual intercourse when Martin discovered that Ziherl had infected her with herpes, even though he knew he was infected and did not inform her of this. She sued him for damages, but because (at the time the case was filed) it was illegal to commit "fornication" (sexual intercourse between a man and a woman who are not married), Ziherl argued that Martin could not sue him because joint tortfeasors - those involved in committing a crime - cannot sue each other over acts occurring as a result of a criminal act ("Zysk v. Zysk", 404 S.E.2d 721 (Va. 1990)). Martin argued in rebuttal that because of the U.S. Supreme Court decision in "Lawrence v. Texas" (finding that state's sodomy law unconstitutional), Virginia's anti-fornication law was also unconstitutional for the reasons cited in Lawrence. Martin argued, therefore, she could, in fact, sue Ziherl for damages.
Lower courts decided that because the Commonwealth's Attorney doesn't prosecute fornication cases and no one had been prosecuted for fornication anywhere in Virginia in over 100 years, Martin had no risk of prosecution and thus lacked standing to challenge the statute. Martin appealed. Since Martin has something to lose - the ability to sue Ziherl for damages - if the statute is upheld, she had standing to challenge the constitutionality of the statute even though the possibility of her being prosecuted for violating it was zero. And since the U.S. Supreme Court in "Lawrence" has found that there is a privacy right in one's private, noncommercial sexual practices, the Virginia Supreme Court decided that the statute against fornication was unconstitutional. The finding gave Martin standing to sue Ziherl since the decision in "Zysk" is no longer applicable.
However, the only reason Martin had standing to challenge the statute was that she had something to lose if it stayed on the books.
Ballot measures.
In "Hollingsworth v. Perry," the Supreme Court ruled that being the proponents of a ballot measure is not by itself enough to confer legal standing. In that case, Proposition 8 had banned same-sex marriage in California, a ban that was ruled unconstitutional. The Supreme Court ruled that the proponents of Proposition 8 has no standing in court since they failed to show that they were harmed by the decision.
State law.
State law on standing differs substantially from federal law and varies considerably from state to state.
California.
On December 29, 2009, the California Court of Appeal for the Sixth District ruled that California Code of Civil Procedure Section 367 cannot be read as imposing a federal-style standing doctrine on California's code pleading system of civil procedure. In California, the fundamental inquiry is "always" whether the plaintiff has sufficiently pleaded a cause of action, not whether the plaintiff has some entitlement to judicial action separate from proof of the substantive merits of the claim advanced. The court acknowledged that the word "standing" is often sloppily used to refer to what is really "jus tertii", and held that "jus tertii" in state law is not the same thing as the federal standing doctrine.

</doc>
<doc id="49308" url="https://en.wikipedia.org/wiki?curid=49308" title="Ripeness">
Ripeness

In United States law, ripeness refers to the readiness of a case for litigation; "a claim is not ripe for adjudication if it rests upon contingent future events that may not occur as anticipated, or indeed may not occur at all." For example, if a law of ambiguous quality has been enacted but never applied, a case challenging that law lacks the ripeness necessary for a decision.
The goal is to prevent premature adjudication; if a dispute is insufficiently developed, any potential injury or stake is too speculative to warrant judicial action. Ripeness issues most usually arise when a plaintiff seeks anticipatory relief, such as an injunction. When drafting his complaint, a plaintiff may be able to avoid dismissal on ripeness grounds by requesting alternative relief in the form of a declaratory judgment, which in many states and jurisdictions allows a court to declare the rights of parties under the facts as proven without actually ordering that anything be done.
The Supreme Court fashioned a two-part test for assessing ripeness challenges to federal regulations. The case is often applied to constitutional challenges to federal and state statutes as well. The Court said in "Abbott Laboratories v. Gardner", :
In both "Abbott Laboratories" and its first companion case, "Toilet Goods Association v. Gardner", , the Court upheld pre-enforcement review of an administrative regulation. However, the Court denied such review in the second companion case because any harm from noncompliance with the FDA regulation at issue was too speculative in the Court's opinion to justify judicial review. Justice Harlan wrote for the Court in all three cases.
The ripeness doctrine should not be confused with the advisory opinion doctrine, another justiciability concept in American law.

</doc>
<doc id="49309" url="https://en.wikipedia.org/wiki?curid=49309" title="Mootness">
Mootness

In United States law, a matter is moot if further legal proceedings with regard to it can have no effect, or events have placed it beyond the reach of the law. Thereby the matter has been deprived of practical significance or rendered purely academic.
This is different from the British meaning of "moot", which means "debatable". The shift in usage was first observed in the United States. The U.S. development of this word stems from the practice of moot courts, in which hypothetical or fictional cases were argued as a part of legal education. These purely academic issues led the U.S. courts to describe cases where developing circumstances made any judgment ineffective as "moot". The doctrine can be compared to the ripeness doctrine, another judge-made rule, that holds that judges should not rule on cases based entirely on anticipated disputes or hypothetical facts. Similar doctrines prevent the federal courts of the United States from issuing advisory opinions.
U.S. federal courts.
In the U.S. federal judicial system, a moot case must be dismissed, there being a constitutional limitation on the jurisdiction of the federal courts. The reason for this is that Article Three of the United States Constitution limits the jurisdiction of all federal courts to "cases and controversies". Thus, a civil action or appeal in which the court's decision will not affect the rights of the parties is ordinarily beyond the power of the court to decide, provided it does not fall within one of the recognized exceptions.
A textbook example of such a case is the United States Supreme Court case "DeFunis v. Odegaard", . The plaintiff was a student who had been denied admission to law school, and had then been provisionally admitted during the pendency of the case. Because the student was slated to graduate within a few months at the time the decision was rendered, and there was no action the law school could take to prevent that, the Court determined that a decision on its part would have no effect on the student's rights. Therefore, the case was dismissed as moot.
However, there is disagreement as to both the source of the standards, and their application in the courts. Some courts and observers opine that cases "must" be dismissed because this is a constitutional bar, and there is no "case or controversy"; others have rejected the pure constitutional approach and adopted a so-called "prudential" view, where dismissal "may" depend upon a host of factors, whether the particular person has lost a viable interest in the case, or whether the issue itself survives outside the interests of the particular person, whether the circumstance are likely to recur, etc. In actual practice, the U.S. federal courts have been uneven in their decisions, which has led to the accusation that determinations are "ad hoc" and 'result-oriented.'
There are four major exceptions to this mootness rule. These are cases of "voluntary cessation" on the part of the defendant; questions that involve secondary or collateral legal consequences; questions that are "capable of "repetition," yet evading review"; and questions involving class actions where the named party ceases to represent the class.
Voluntary cessation.
Where a defendant is acting wrongfully, but ceases to engage in such conduct once a litigation has been threatened or commenced, the court will still not deem this correction to moot the case. Obviously, a party could stop acting improperly just long enough for the case to be dismissed and then resume the improper conduct. For example, in "Friends of the Earth, Inc. v. Laidlaw Environmental Services, Inc.", , the Supreme Court held that an industrial polluter, against whom various deterrent civil penalties were being pursued, could not claim that the case was moot, even though the polluter had ceased polluting and had closed the factory responsible for the pollution. The court noted that so long as the polluter still retained its license to operate such a factory, it could open similar operations elsewhere if not deterred by the penalties sought.
Another example occurs when a court dismisses as "moot" a legal challenge to an existing law, where the law being challenged is either amended or repealed through legislation before the court case could be settled. A recent instance of this occurred in "Moore v. Madigan," when Illinois Attorney General Lisa Madigan declined to appeal a ruling of the Seventh Circuit United States Court of Appeals striking down Illinois handgun carry ban to the United States Supreme Court, as Illinois subsequently passed a law legalizing concealed carry with a state-issued license, which rendered the case moot.
Secondary or collateral legal consequences.
"The obvious fact of life is that most criminal convictions do in fact entail adverse collateral legal consequences. The mere possibility that this will be the case is enough to preserve a criminal case from ending ignominiously in the limbo of mootness." Sibron v. New York. 
Capable of repetition, yet evading review.
A court will allow a case to go forward if it is the type for which persons will frequently be faced with a particular situation, but will likely cease to be in a position where the court can provide a remedy for them in the time that it takes for the justice system to address their situation. The most frequently cited example is the 1973 United States Supreme Court case of "Roe v. Wade", , which challenged a Texas law forbidding abortion in most circumstances. The state argued that the case was moot because plaintiff Roe was no longer pregnant by the time the case was heard. As Justice Blackmun wrote in the majority opinion:
The normal 266-day human gestation period is so short that the pregnancy will come to term before the usual appellate process is complete. If that termination makes a case moot, pregnancy litigation seldom will survive much beyond the trial stage, and appellate review will be effectively denied. Our law should not be that rigid.
Norma McCorvey, whose alias was Roe, became a pro-life advocate and attempted to have the decision of "Roe v. Wade" reversed and in "McCorvey v. Hill", 2004, the case failed to proceed based on being moot, without standing and out of time.
The Court cited "Southern Pacific Terminal Co. v. ICC", , which had held that a case was not moot when it presented an issue that was "capable of repetition, yet evading review". Perhaps in response to increasing workloads at all levels of the judiciary, the recent trend in the Supreme Court and other U.S. courts has been to construe this exception rather narrowly.
Many cases fall under the "capable of repetition" doctrine; however, because there is a review process available under most circumstances, the exception to declaring mootness did not apply to such cases. In "Memphis Light, Gas & Water Div. v. Craft", 436 U. S. 1, 8–9 (1978), the court noted that claims for damages save cases from mootness.
Class action representatives.
Where a class action lawsuit is brought, with one named plaintiff actually representing the interests of many others, the case will not become moot even if the named plaintiff ceases to belong to the class that is seeking a remedy. In "Sosna v. Iowa", , the plaintiff represented a class that was challenging an Iowa law that required persons to reside there for a year before seeking a divorce in Iowa's courts. The Supreme Court held that, although the plaintiff successfully divorced in another state, her attorneys could continue to competently advance the interests of other members of the class.
U.S. state courts.
The U.S. state courts are not subject to the Article III limitations on their jurisdiction, and some state courts are permitted by their local constitutions and laws to render opinions in moot cases where the establishment of a legal precedent is desirable. They may also establish exceptions to the doctrine. For instance, in some state courts the prosecution can lodge an appeal after a defendant is acquitted: although the appellate court cannot set aside a not-guilty verdict due to double jeopardy, it can issue a ruling as to whether a trial court's ruling on a particular issue during the trial was erroneous. This opinion will then be binding on future cases heard by the courts of that state.
Some U.S. states also accept certified questions from the federal courts or the courts of other states. Under these procedures, state courts can issue opinions, usually for the purpose of clarifying or updating state law, in cases not actually pending in those courts.
Outside of the U.S..
Although free from the U.S. Constitutional limitation, Canada has recognized that considerations of judicial economy and comity with the legislative and executive branch may justify a decision to dismiss an allegedly moot case, as deciding hypothetical controversies is tantamount to legislating. Considerations of the effectiveness of advocacy involved in the adversarial system and the possibility of recurrence of an alleged constitutional violation may sway the court. Additionally, the federal and provincial governments can ask for advisory opinions in hypothetical scenarios, termed reference questions, from their respective highest courts.
Moot point.
The phrase moot point refers to an issue that is irrelevant to a subject being discussed or (in British English) that is debatable. Due to the relatively uncommon usage of the word moot, this is sometimes rendered as the malapropism "mute point".

</doc>
<doc id="49315" url="https://en.wikipedia.org/wiki?curid=49315" title="Default (law)">
Default (law)

In law, a default is the failure to do something required by law or to appear
at a required time in legal proceedings.
In the United States, for example, when a party has failed to file meaningful response to pleadings within
the time allowed, with the result that only one side of a controversy has been presented
to the court, the party who has pleaded a claim for relief and received no response may request entry of default. In some jurisdictions the court may proceed to enter judgment immediately: others require that the plaintiff file a
notice of intent to take the default judgment and serve it on the unresponsive party.
If this notice is not opposed, or no adequate justification for the delay or lack
of response is presented, then the plaintiff is entitled to judgment in his favor. Such a judgment is referred to as a "default judgment" and, unless otherwise ordered, has the same effect as a judgment entered in a contested case.
It is possible to vacate or remove the default judgment, depending on the particular state's law.
Entry of default in the United States district courts is governed by Rule 55 of the Federal Rules of Civil Procedure. 

</doc>
<doc id="49316" url="https://en.wikipedia.org/wiki?curid=49316" title="Clare Martin">
Clare Martin

Clare Majella Martin (born 15 June 1952) is a former Australian journalist and politician. She was elected to the Northern Territory Legislative Assembly in a shock by-election win in 1995. She was appointed Opposition Leader in 1999, and won a surprise victory at the 2001 territory election, becoming the first Australian Labor Party (ALP) and first female Chief Minister of the Northern Territory. At the 2005 election, she led Territory Labor to the second-largest majority government in the history of the Territory, before resigning as Chief Minister on 26 November 2007.
Early life.
Martin was one of ten children. Her parents were strong Catholics and passionate Democratic Labor Party supporters. Her uncle, Kevin Cairns, was a Liberal minister and MP in the McMahon government, but the family was not inclined towards his conservative politics. Martin's ancestry includes the Coughlin family, which also had NSW's first female statistician and the noted test cricketer Victor Trumper. The family was originally from County Offaly, Ireland, until the Cromwell invasion, then left County Cork in the 1850s just after the Potato Famine.
After attending Loreto Normanhurst, Martin graduated from the University of Sydney in 1975 with a Bachelor of Arts degree, in which her major study was Music.
Pre-political career.
Having spent time in London and other overseas cities, she began working as a typist for the Australian Broadcasting Corporation in Sydney in 1978. In 1979, she became a trainee reporter. After several years, she began to take an interest in presenting, but was told that she would not be given a position in Sydney unless she had experience elsewhere . In February 1983, Martin was then offered a six-month position presenting a morning radio show in Darwin for the ABC Radio station 5DR.
She had little intention of staying there, and briefly returned to Canberra in May 1983, before being offered a job in Sydney. However, at the same time, Martin's partner was offered a partner's position at the law firm he had worked in Darwin. He liked living in Darwin and was keen to take up the position, so Martin agreed to decline the Sydney job and return to Darwin in May 1985 where she gained another position on an ABC Radio morning show.
In 1986, Martin made the move to television, as the presenter of "The 7.30 Report" until 1988. After returning from long service leave where she cared for her two young children, Martin returned to work in 1990 to work on ABC Radio's morning program.
Political career.
Martin had been interested in political journalism for some years, although she was not a member of any party, believing that party affiliation compromises journalistic integrity. In 1994, she was approached to contest the Darwin Legislative Assembly seat of Casuarina for the Australian Labor Party at the 1994 election. However, she was defeated by Country Liberal Party candidate Peter Adamson. She soon resigned from the party and returned to journalism, but when CLP Chief Minister Marshall Perron resigned from his Darwin seat of Fannie Bay, Martin opted to contest the ensuing by-election as the Labor candidate. Fannie Bay, like most Darwin electorates, had been a CLP stronghold; Perron held it with a majority of 8 percent. However, in a considerable upset, Martin went on to win the seat by 69 votes, becoming one of only two ALP MLAs in Darwin.
Martin worked hard to retain her seat at the 1997 election, and was successful, holding Fannie Bay despite a heavy defeat for the ALP. She subsequently served as Shadow Minister for Lands under then leader Maggie Hickey. When Hickey unexpectedly resigned in February 1999, Martin was in a position to succeed her, and was soon elected party leader, and hence Opposition Leader. She soon emerged as a vocal critic of the Burke government's policy of mandatory sentencing, and began preparing the ALP for the next election, which was then two years away.
Term as Chief Minister.
Martin faced her first electoral test as leader at the 2001 election. At the time, the Country Liberal Party had held office for 27 years, and Labor had never come particularly close to government. Indeed, it had never managed to win more than nine seats at any election. However, the ALP was coming off a particularly successful eighteen months, and Martin ran a skilled campaign. She was also able to take advantage of a number of gaffes made by Chief Minister Denis Burke, such as the decision to preference One Nation over the ALP – which lost the CLP a number of votes in crucial Darwin seats. The election also came during a bad time for the federal Coalition government, which was under fire for introducing a GST after previously vowing not to do so.
Despite this, most commentators were predicting the CLP would be returned for a ninth term in government, albeit with a reduced majority. However, in a shock result, Labor scored an eight-seat swing, achieving majority government by one seat. It did so on the strength of an unexpected Labor wave in Darwin. Labor had gone into the election holding only two seats in the capital—those of Martin and Paul Henderson—and had never held more than two seats in Darwin at any time. In the 2001 election, however, Labor took all but one seat in Darwin, including all seven seats in the northern part of the city. Darwin's northern suburbs are somewhat more diverse than the rest of the city. In the process, they ousted four sitting MLAs; Labor had not unseated a CLP incumbent since 1980. Although the CLP won a bare majority of the two-party vote, Labor's gains in Darwin were enough to make Martin the first ALP and first female Chief Minister in the history of the Northern Territory. Martin herself was reelected with a healthy swing of 9.2 percent in Fannie Bay, turning it into a safe Labor seat in one stroke.
As Chief Minister, Martin immediately set about making changes, repealing the territory's controversial mandatory sentencing laws, and introducing freedom of information legislation, which had been neglected during the CLP's 27-year rule. 
Aboriginal issues.
Although Martin appointed Aboriginal Territorians to her cabinet, she has been criticised for not improving the lot of her Aboriginal constituents, the majority of whom have a life expectancy well below that of white Australians. A respected commentator in "The Bulletin" suggested that she had gone slow on Aboriginal issues because she feared a white backlash that could have resulted in her government being toppled.
The life expectancy of the Northern Territory's Aboriginal citizens did not increase markedly during Martin's administration. Alcohol abuse continued to be a major issue in Aboriginal communities and third-world diseases like trachoma could be seen in remote Aboriginal townships. However, in 2006, Martin rejected accusations by John Howard and Federal Indigenous Affairs Minister, Mal Brough, that her government had been underfunding Aboriginal communities. A summit between the federal and territory governments was proposed by Mal Brough in May 2006, but this was snubbed by Martin.
Martin was critical of the Federal Government's intervention in Aboriginal communities as announced in 2007. She opposed certain aspects of the intervention such as removal of the permit system. In response, the Federal Government rejected the Territory's argument, saying it was essential to remove artificial barriers to Aboriginal townships that prevent the measures needed to improve living conditions for Indigenous children
Achievements.
In the longer term, she oversaw the completion of the Adelaide-Darwin railway, which had begun under the Burke government, and vowed to resurrect the stalled statehood movement. She also managed to markedly boost the ALP's standing amongst the electorate, as seen in the 2003 Katherine by-election, which saw a major swing to the party.
By 2005, the Northern Territory, under Martin's leadership, had achieved the following:
As Chief Minister, Martin led the ALP to the 2005 election, which was their first as an incumbent government in the Territory. Martin campaigned largely on law and order issues. It was predicted that the ALP would win a relatively narrow victory. However, in a result that had not been predicted by any commentators or even the most optimistic Labor observers, Martin led the ALP to a smashing victory. The final result gave 19 seats to the ALP, 4 to the opposition CLP and 2 to independents. The ALP won six seats from the CLP, four of which they had never won before in any election. Two of them were in Palmerston, an area where Labor had not even come close to winning. In the most unexpected victory of all, the ALP even managed to unseat the Opposition Leader and former Chief Minister, Burke, in his own Palmerston-area electorate. Labor won the second-largest majority government in the history of the Territory, bettered only by the CLP's near-sweep of the Legislative Assembly at the first elections, in 1974.
On 10 September 2007, Queensland Premier Peter Beattie announced he would leave politics that week. This left Martin as Labor's longest-serving current state or territory leader, and as the longest-serving state or territory head of government in Australia, until she herself announced her resignation on 26 November 2007.
Resignation.
On 26 November 2007, Clare Martin and her deputy Syd Stirling announced their resignations at a media conference in Darwin. Northern Territory education minister Paul Henderson was elected as the new leader and Chief Minister by the ALP caucus.
Post-political career.
In 2008, Martin became Chief Executive Officer of the Australian Council of Social Service, based in Sydney. In August 2010 she returned to the Northern Territory to become a Professorial Fellow in the Public and Social Policy Research Institute at Charles Darwin University.

</doc>
<doc id="49322" url="https://en.wikipedia.org/wiki?curid=49322" title="Rajmund Kanelba">
Rajmund Kanelba

Raymond Kanelba also "Rajmund Kanelba" (1897–1960) was a 20th-century Polish painter.
He was born in Warsaw and educated there as well as in Vienna and Paris. He was strongly influenced by the école de Paris but with rather realistic and anti-impressionist style. In 1926 his works were on display in Salon des Indépendants and Salon d'Automne and in 1952 he had a large exposition of his paintings in New York City.
Rajmund Kanelba lived most of his life in France but died in London.

</doc>
<doc id="49324" url="https://en.wikipedia.org/wiki?curid=49324" title="Unit interval">
Unit interval

In mathematics, the unit interval is the closed interval , that is, the set of all real numbers that are greater than or equal to 0 and less than or equal to 1. It is often denoted "" (capital letter I).
In addition to its role in real analysis, the unit interval is used to study homotopy theory in the field of topology.
In the literature, the term "unit interval" is sometimes applied to the other shapes that an interval from 0 to 1 could take: , , and . However, the notation "" is most commonly reserved for the closed interval .
Properties.
The unit interval is a complete metric space, homeomorphic to the extended real number line. As a topological space it is compact, contractible, path connected and locally path connected. The Hilbert cube is obtained by taking a topological product of countably many copies of the unit interval. 
In mathematical analysis, the unit interval is a one-dimensional analytical manifold whose boundary consists of the two points 0 and 1. Its standard orientation goes from 0 to 1. 
The unit interval is a totally ordered set and a complete lattice (every subset of the unit interval has a supremum and an infimum).
Cardinality.
The size or cardinality of a set is the number of elements it contains.
The unit interval is a subset of the real numbers formula_1. However, it has the same size as the whole set: the cardinality of the continuum. Since the real numbers can be used to represent points along an infinitely long line, this implies that a line segment of length 1, which is a part of that line, has the same number of points as the whole line. Moreover, it has the same number of points as a square of area 1, as a cube of volume 1, and even as an unbounded "n"-dimensional Euclidean space formula_2 (see Space filling curve).
The number of elements (either real numbers or points) in all the above-mentioned sets is uncountable, as it is strictly greater than the number of natural numbers.
Generalizations.
The interval [−1,1], with length two, demarcated by the positive and negative units, occurs frequently, such as in the range of the trigonometric functions sine and cosine and the hyperbolic function tanh. This interval may be used for the domain of inverse functions. For instance, when θ is restricted to [−π/2, π/2] then sin(θ) is in this interval and arcsine is defined there.
Sometimes, the term "unit interval" is used to refer to objects that play a role in various branches of mathematics analogous to the role that [0,1] plays in homotopy theory. For example, in the theory of quivers, the (analogue of the) unit interval is the graph whose vertex set is {0,1} and which contains a single edge "e" whose source is 0 and whose target is 1. One can then define a notion of homotopy between quiver homomorphisms analogous to the notion of homotopy between continuous maps.
Fuzzy logic.
In logic, the unit interval [0,1] can be interpreted as a generalization of the Boolean domain {0,1}, in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with formula_3 ; conjunction (AND) is replaced with multiplication (formula_4); and disjunction (OR) is defined, per De Morgan's laws, as formula_5 .
Interpreting these values as logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In these interpretations, a value is interpreted as the "degree" of truth – to what extent a proposition is true, or the probability that the proposition is true.

</doc>
<doc id="49325" url="https://en.wikipedia.org/wiki?curid=49325" title="Houyhnhnm">
Houyhnhnm

Houyhnhnms are a race of intelligent horses described in the last part of Jonathan Swift's satirical "Gulliver's Travels". The name is pronounced either or . (Swift apparently intended all words of the Houyhnhnm language to echo the neighing of horses.)
Description.
Gulliver's visit to the Land of the Houyhnhnms is described in Part IV of his "Travels", and its location illustrated on the map at the start of Part IV. 
The map shows Houyhnhnms Land to be south of Australia, it indicates Edels Land and Lewins Land to the north, and Nuyts Land to the north-east, on the mainland with the islands of St Francis and St Pieter further east, and Sweers, Maatsuyker and De Wit islands to the east. 
The map is somewhat careless with the scale, however; Edels Land to Lewins Land are shown adjacent, while in reality they are some 1000 km apart, while the sweep of the Great Australian Bight, from Cape Leeuwin, Australia's south-westerly point to the Maatsuyker Islands, off the southern tip of Tasmania, is over 3000 km.
Gulliver describes the land as "divided by long rows of trees, not regularly planted but naturally growing", with a "great plenty of grass, and several fields of oats". 
The Houyhnhnms are rational, equine beings and are masters of the land, contrasting strongly with the Yahoos, savage humanoid creatures who are no better than beasts of burden, or livestock. Whereas the Yahoos represent all that is bad about humans, Houyhnhnms have a settled, calm, reliable and rational society. Gulliver much prefers the Houyhnhnms' company to the Yahoos', even though the latter are biologically closer to him.
Interpretation.
Interpretation of the Houyhnhnms has been vexatious. One might possibly, for example, regard them as a veiled criticism by Swift of the British Empire's treatment of non-whites as lesser humans, or one could regard Gulliver's preference (and his immediate division of Houyhnhnms into color-based hierarchies) as absurd and the sign of his self-deception. In a modern context the story might be seen as presenting an early example of animal rights concerns, especially in Gulliver's account of how horses are cruelly treated in his society and the reversal of roles. The story is a possible inspiration for Pierre Boulle's novel "Planet of the Apes".
Book IV of "Gulliver's Travels" is the keystone, in some ways, of the entire work, and critics have traditionally answered the question whether Gulliver is insane (and thus just another victim of Swift's satire) by questioning whether or not the Houyhnhnms are truly admirable. Gulliver loves the land and is obedient to a race that is not like his own. The Houyhnhnm society is based upon reason, and only upon reason, and therefore the horses practice eugenics based on their analyses of benefit and cost. They have no religion and their sole morality is the defence of reason, and so they are not particularly moved by pity or a belief in the intrinsic value of life. Gulliver himself, in their company, builds the sails of his skiff from "Yahoo skins". The Houyhnhnms' lack of passion surfaces during the scheduled visit of "a friend and his family" to the home of Gulliver's master "upon some affair of importance". On the day of the visit, the mistress of his friend and her children arrive very late. she made no excuses "first for her husband" who had passed just that morning and she had to remain to make the proper arrangements for a "convenient place where his body should be laid". Gulliver remarked that "she behaved herself at our house as cheerfully as the rest". A further example of the lack of humanity and emotion in the Houyhnhnms is that their laws reason that each couple produce two children, one male and one female. In the event that a marriage produced two offspring of the same sex, the parents would take their children to the annual meeting and trade one with a couple who produced two children of the opposite sex. This latter part was viewed by some Swift scholars as his spoofing and or criticising the notion that the "ideal" family produces children of both sexes.
On one hand, the Houyhnhnms have an orderly and peaceful society. They have philosophy and a language that is entirely free of political and ethical nonsense. They have no word for a "lie" (and must substitute a circumlocution: "to say a thing which is not"). They also have a form of art that is derived from nature. Outside "Gulliver's Travels", Swift had expressed longstanding concern over the corruption of the English language, and he had proposed language reform. He had also, in "Battle of the Books" and in general in "A Tale of a Tub", expressed a preference for the Ancients (Classical authors) because their art was based directly upon nature, and not upon other art.
On the other hand, Swift was profoundly mistrustful of attempts at reason that resulted in either hubris (for example, the Projectors satirised in "A Tale of a Tub" or in Book III of "Gulliver's Travels") or immorality (such as the speaker of "A Modest Proposal", who offers an entirely logical and wholly immoral proposal for cannibalism). The Houyhnhnms embody both the good and the bad side of reason, for they have the pure language Swift wished for and the amorally rational approach to solving the problems of humanity (Yahoos); the extirpation of the Yahoo population by the horses is very like the speaker of "A Modest Proposal".
In the shipping lanes he is rescued by a Portuguese sea captain, a level-headed individual albeit full of concern for others, whose temperament at one level appears intermediate between the calm, rational Houyhnhnms of Houyhnhnmland and the norm of corrupt, European humanity, which Gulliver no longer distinguishes from Houyhnhnmland's wild Yahoos. Gulliver can speak with him, and though now disaffected from all humanity, he began to tolerate his company. Gulliver is returned to his English home and family, finds their smell and look intolerable and all his countrymen no better than "Yahoos", purchases and converses with two stabled horses, tolerating the stable boy, and assures the reader of his account's utter veracity.

</doc>
<doc id="49326" url="https://en.wikipedia.org/wiki?curid=49326" title="KStars">
KStars

KStars is a planetarium program using the KDE Platform. It can be used on most Unix-like computer operating systems, as well as on the Microsoft Windows platform using 'KDE for Windows'. It provides an accurate graphical representation of the night sky, from any location on Earth, at any date and time. The display includes up to 100 million stars (with additional addons), 13,000 deep sky objects, constellations from different cultures, all 8 planets, the Sun and Moon, and thousands of comets and asteroids. It has features to appeal to users of all levels, from informative hypertext articles about astronomy, to robust control of telescopes and CCD cameras, and logging of observations of specific objects.
KStars supports adjustable simulation speeds in order to view phenomena that happen over long timescales. For astronomical calculations, Astrocalculator can be used to predict conjunctions, and perform many common astronomical calculations. The following tools are included:
KStars has been packaged by many Linux/BSD distributions, including Red Hat Linux, OpenSUSE, Mandriva Linux, and Debian GNU/Linux. Some distributions package KStars as a separate application, some just provide a kdeedu package, which includes KStars. KStars is distributed with the KDE Software Compilation as part of the kdeedu "Edutainment" module.
KStars participated in Google Summer of Code in 2008, 2009, 2010, 2011 2012, and 2015. It has also participated in the first run of ESA's Summer of Code in Space in 2011.
It has been identified as one of the three best "Linux stargazing apps" in a Linux.com review.
The latest version of KStars is 2.4.0, released with KDE Applications 15.12. Released under the GNU General Public License, KStars is free software.

</doc>
<doc id="49329" url="https://en.wikipedia.org/wiki?curid=49329" title="Strait of Juan de Fuca">
Strait of Juan de Fuca

The Strait of Juan de Fuca (officially named Juan de Fuca Strait in Canada) is a large body of water about long that is the Salish Sea's outlet to the Pacific Ocean. The international boundary between Canada and the United States runs down the center of the Strait.
It was named in 1787 by the maritime fur trader Charles William Barkley, captain of the "Imperial Eagle", for Juan de Fuca, the Greek navigator who sailed in a Spanish expedition in 1592 to seek the fabled Strait of Anián. Barkley was the first non-indigenous person to find the strait, unless Juan de Fuca's story was true. The strait was explored in detail between 1789 and 1791 by Manuel Quimper, José María Narváez, Juan Carrasco, Gonzalo López de Haro, and Francisco de Eliza.
Definition.
The USGS defines the Strait of Juan de Fuca as a channel. It extends east from the Pacific Ocean between Vancouver Island, British Columbia, and the Olympic Peninsula, Washington, to Haro Strait, San Juan Channel, Rosario Strait, and Puget Sound. The Pacific Ocean boundary is formed by a line between Cape Flattery and Tatoosh Island, Washington, and Carmanah Point (Vancouver Island), British Columbia. Its northern boundary follows the shoreline of Vancouver Island from Carmanah Point to Gonzales Point, then follows a continuous line east to Seabird Point (Discovery Island), British Columbia, Cattle Point (San Juan Island), Washington, Iceberg Point (Lopez Island), Point Colville (Lopez Island), and then to Rosario Head (Fidalgo Island). The eastern boundary runs south from Rosario Head across Deception Pass to Whidbey Island, then along the western coast of Whidbey Island to Point Partridge, then across Admiralty Inlet to Point Wilson (Quimper Peninsula). The northern coast of the Olympic Peninsula forms the southern boundary of the strait. In the eastern entrance to the Strait, the Race Rocks Archipelago is located in the high current zone half way between Port Angeles, Washington, and Victoria, BC.
Climate.
Like the rest of the Salish Sea and surrounding regions, the climate of the Strait is disputed, with the Köppen system classifying it as Mediterranean, but most regional climatologists preferring oceanic. While the climate is mostly oceanic in nature, the dry summers result in the Mediterranean classification in the Köppen system. Rainfall ranges from over (temperate rainforest) conditions at the west end to as little as at the east end, near Sequim.
Because it is exposed to the generally westerly winds and waves of the Pacific, seas and weather in Juan de Fuca Strait are, on average, rougher than in the more protected waters inland, thereby resulting in a number of small-craft advisories. A weather station provides live data from Race Rocks at http://racerocks.ca/racerock/data/weatherlink/Current_Vantage_Pro.htm
Ferries.
An international vehicle ferry crosses the Strait from Port Angeles, Washington to Victoria, British Columbia several times each day, as do passenger ferries of the Washington State Ferry system, a seasonal private ferry connecting Port Angeles with Victoria and a private high-speed ferry between Victoria and Seattle.
Boundary dispute.
This strait remains the subject of a maritime boundary dispute between Canada and the United States. The dispute is only over the seaward boundary extending west from the mouth of the strait. The maritime boundary within the strait is not in dispute. Both governments have proposed a boundary based on the principle of equidistance, but with different basepoint selections, resulting in small differences in the line. Resolution of the issue should be simple, but has been hindered because it might influence other unresolved maritime boundary issues between Canada and the United States. In addition, the government of British Columbia has rejected both equidistant proposals, instead arguing that the Juan de Fuca submarine canyon is the appropriate "geomorphic and physiogeographic boundary". The proposed equidistant boundary currently marks the northern boundary of the Olympic Coast National Marine Sanctuary. British Columbia's position is based on the principle of natural prolongation which developed in international law. It poses a dilemma for the federal government of Canada. If Canada holds that the principle of natural prolongation applies to the Juan de Fuca Canyon on its Pacific Ocean coast, the assertion could undermine Canada's argument in the "Gulf of Maine" boundary dispute. In this Atlantic Ocean context, Canada favours an outcome based on the principle of equidistance.
Salish Sea.
In March 2008, the Chemainus First Nation proposed renaming the strait the "Salish Sea", an idea that reportedly met with approval by British Columbia's Aboriginal Relations Minister Mike de Jong, who pledged to put it before the B.C. cabinet for discussion. Making "Salish Sea" official required a formal application to the Geographical Names Board of Canada. A parallel American movement promoting the name had a different definition, combining of the Strait of Juan de Fuca and Puget Sound as well as the Strait of Georgia and related waters under the more general name "Salish Sea". This latter definition was made official in 2009 by geographic boards of Canada and the United States.
In October 2009, the Washington State Board of Geographic Names approved the Salish Sea toponym, not to replace the names of the Strait of Georgia, Puget Sound, and Strait of Juan de Fuca, but instead as a collective term for all three. The British Columbia Geographical Names Office passed a resolution only recommending that the name be adopted by the Geographical Names Board of Canada, should its US counterpart approve the name-change. The United States Board on Geographic Names approved the name on November 12, 2009.
Counties and regional districts.
Counties along the Strait of Juan de Fuca:
Regional districts along the Strait of Juan de Fuca:
Fauna.
Certain groups of seabirds called common murre migrate north by swimming. Some Pacific Coast murres paddle north to the sheltered bays of the Strait of Juan de Fuca to feed on herring and other small fish.
Other.
Some scenes from the movie "The Hunt for Red October" were filmed in the Strait of Juan de Fuca in 1989.

</doc>
<doc id="49330" url="https://en.wikipedia.org/wiki?curid=49330" title="Spellevator">
Spellevator

Spellevator is an educational computer game for the Apple II computer, published by MECC (now part of Mattel's The Learning Company). It was one of the first commercial games to use the ProDOS operating system.
Summary.
The player controls a dust bunny, which is chased by several vacuum cleaners with different movement patterns. The objective of the level is to grab all the letters and exit through the upper left corner. The player can pass through an unoccupied elevator (some vacuum cleaners use elevators also) by correctly answering a spelling or vocabulary question. Once one completes a level, the player can receive a bonus by correctly unscrambling the letters one grabbed into a word).
"Spellevator" had a utility on the disk's flipside that let a user create a word list and save it to any ProDOS formatted floppy disk. This way, teachers could customize the game to fit their own particular vocabulary lists.

</doc>
<doc id="49331" url="https://en.wikipedia.org/wiki?curid=49331" title="Patchwork">
Patchwork

Patchwork or "pieced work" is a form of needlework that involves sewing together pieces of fabric into a larger design. The larger design is usually based on repeat patterns built up with different fabric shapes (which can be different colors). These shapes are carefully measured and cut, basic geometric shapes making them easy to piece together. 
Uses.
Patchwork is most often used to make quilts, but it can also be used to make bags, wall-hangings, warm jackets, cushion covers, skirts, waistcoats and other items of clothing. Some textile artists work with patchwork, often combining it with embroidery and other forms of stitchery.
When used to make a quilt, this larger patchwork or pieced design becomes the "top" of a three-layered quilt, the middle layer being the batting, and the bottom layer the backing. To keep the batting from shifting, a patchwork or pieced quilt is often quilted by hand or machine using a running stitch in order to outline the individual shapes that make up the pieced top, or the quilting stitches may be random or highly ordered overall patterns that contrast with the patchwork composition.
History.
Evidence of patchwork—piecing small pieces of fabric together to create a larger piece and quilting layers of textile fabrics together—has been found throughout history. The earliest examples have been located in Egyptian tombs and also in early age of China about 5000 years ago. Further finds have been dated from the early Middle Ages, where layers of quilted fabric were used in the construction of armor—this kept the soldiers warm and protected. Japanese armor was made in a similar fashion.
Using this technique, quilts began to appear in households of the 11th to 13th centuries. As the European climate became colder around this time, the incidence of the use of bed quilts rose, and so developed the practice of embellishing a simple cloth through the creation of pattern and design, alongside the development of decorative quilting. The tradition of making quilts in this fashion was taken to America by the Pilgrims.
Americans.
Patchwork enjoyed a widespread revival during the Great Depression as a way to recycle worn clothing into warm quilts. Even very small and worn pieces of material are suitable for use in patchwork, although crafters today more often use new 100% cotton fabrics as the basis for their designs. In the US, patchwork declined after World War II, but was again revived during the American bicentennial. In the past, hand quilting was often done in a group around a frame. Instead of quilting, the layers are sometimes tied together at regular intervals with pieces of yarn, a practice known as tying or knotting, and which produces a "comforter".
Popularity.
The 2003 Quilting in America survey estimated that the total value of the American quilting industry was $2.7 billion. International quilting exhibitions attract thousands of visitors, while countless smaller exhibitions are held every weekend in local regions. Active cyber-quilting communities abound on the web; books and magazines on the subject are published in the hundreds every year; and there are many active local quilting guilds and shops in different countries. "Quilt Art" is established as a legitimate artistic medium, with quilted works of art selling for thousands of dollars to corporate buyers and galleries. Quilt historians and quilt appraisers are re-evaluating the heritage of traditional quilting and antique quilts, while superb examples of antique quilts are purchased for large sums by collectors and museums. The American Quilt Study Group is active in promotion of research on the history of quilting.
Asia.
In Indian stitching blanket using different small pieces of cloth is an art. It is popularly known as Kaudhi in Karnataka. Such blankets are given as gifts to newborn babies in some parts of Karnataka. Lambani tribes wear skirts with such art
Structure.
There are three traditional structures used to construct a patchwork or pieced composition: 1) the block, 2) overall, and 3) strip piecing. Traditional patchwork has identifying names based on the arrangement of colors and shapes.
Blocks.
Patchwork blocks are pieced squares made up of colored shapes that repeat specific shapes to create patterns within the square or block, of, say, light and dark, or contrasting colors (motif (textile arts)). The blocks can all repeat the same pattern, or blocks can have several different patterns. The patchwork blocks are typically around 8–10" square (20 cm to 25 cm). They are sewn together in stacked rows to make a larger composition. Often strips of contrasting fabric forming a lattice separate the patchwork blocks from each other. Some common patchwork block names are Log Cabin, Drunkard's Path, Bear's Paw, Tulip, and Nine Patch.
A unique form of patchwork quilt is the crazy quilt. Crazy quilting was popular during the Victorian era (mid–late 19th century). The crazy quilt is made up of random shapes of luxurious fabric such as velvets, silks, and brocades and buttons, lace, and other embellishments left over from the gowns they had made for themselves. The patchwork pieces are stitched together forming "crazy" or non-repeat, asymmetric compositions. Fancy embroidery embellishes the seam lines between the individual, pieced shapes. The crazy quilt was a status symbol, as only well-to-do women had a staff to do all the household work, and had the time to sew their crazy quilt. Traditionally, the top was left without lining or batting. Many surviving crazy quilts still have the newspaper and other foundation papers used for piecing.
Overall.
Overall patchwork designs are incrementally pieced geometric shapes stitched together to form a larger random or composed design. The colored shapes can be randomly pieced or follow a strict order to create a specific effect, e.g. value (light to dark) progressions, or checkerboard effects. Names such as Hit or Miss, Clamshell, back-stitch, needle weave, criss-cross and Starburst identify some overall patchwork structures.
Strip piecing.
Strip piecing involves stitching together pieces of fabric in repeat patterns into long strips and then stitching the strips together lengthwise. The patchwork strips can be alternated with strips of contrasting colors. A typical strip patchwork quilt is the Flying Geese pattern.
Jelly Rolls and other pre-cuts.
Pre-cut fabrics come in many varieties and can be used to make various styles of quilts. Pre-cuts include "Jelly Rolls", which were brought onto the market by the fabric company Moda. Each Jelly Roll consists of 40 strips of 44" long fabric, all in various different styles and patterns.
Forms.
Specialised forms of patchwork include:
Trends.
Today, many things are quilted using a longarm quilting system. The system consists of a frame and a sewing machine. The patchwork, batting and backing are loaded onto the frame and in some systems each layer can be tensioned independently. No basting is usually necessary. The frames can be up to 14' long which is big enough for a king-size quilt to be tensioned ready for quilting. The sewing machine known as the longarm machine has an extended throat space, up to 36", and it can be moved on a two-axis rail system—left and right, forwards and backwards, enabling a 360-degree movement over the surface of the quilt.
Until recently, most longarm machines were hand-guided which meant the operator had to synchronise the speed of their hands with that of the machine motor. Fast hands and slow motor meant big stitches. Slow hands and fast motor meant small stitches. Since just after the turn of the century, most longarm machines are now sold with stitch-regulation, which means that the operator no longer has to synchronize hand speed with that of the motor. Electronics in the machine ensure the stitch length remains constant. More recently, fully computerized machines are being sold. Fully computerized machines have been available for over 12 years. They were invented by Paul Statler but have only recently become popular. These machines use specialised machine-driver software and CAD-type drawing packages to enable pattern digitisation and automatic quilting. An operator is still required to mind the machine and set the pattern onto the quilt.
It is thought that over 10,000 longarm quilting machines are in use today. In the US, there are many brands available and many places to obtain training and few distributors and trainers in other countries where business quilters are more likely to travel to the States for ongoing longarm training.

</doc>
<doc id="49332" url="https://en.wikipedia.org/wiki?curid=49332" title="Needlework">
Needlework

Needlework is a broad term for the handicrafts of decorative sewing and textile arts. Anything that uses a needle for construction can be called needlework. The definition may expand to include related textile crafts such as a crochet hook or tatting shuttles.
Similar abilities often transfer well between different varieties of needlework, such as fine motor skill and a knowledge of textile fibres. Some of the same tools may be used in several different varieties of needlework. For instance, a needle threader is useful in nearly all needlecrafts.

</doc>
<doc id="49333" url="https://en.wikipedia.org/wiki?curid=49333" title="Cultural bias">
Cultural bias

Cultural bias is the phenomenon of interpreting and judging phenomena by standards inherent to one's own culture. The phenomenon is sometimes considered a problem central to social and human sciences, such as economics, psychology, anthropology, and sociology. Some practitioners of the aforementioned fields have attempted to develop methods and theories to compensate for or a culture make assumptions about conventions, including conventions of language, notation, proof and evidence. They are then accused of mistaking these assumptions for laws of logic or nature. Numerous such biases exist, concerning cultural norms for color, location of body parts, mate selection, concepts of justice, linguistic and logical validity, acceptability of evidence, and taboos. Cultural bias extends on many more fields in the globalizing world. Ordinary people may tend to imagine other people as basically the same, not significantly more or less valuable, probably attached emotionally to different groups and different land. 
Examples.
People who read English often assume that it is natural to scan a visual field from left to right and from top to bottom. In the United States it is typical for the "on" position of a toggle switch to be "up", whereas in the UK, Australia, and New Zealand it is "down." Also, in these countries, North is the top of a map, up is usually the larger quantity and better, as well. As another example, Japanese do not place an X in a check-box to indicate acceptance—this indicates refusal.
These conventions are generally useful, as once one is used to light switches behaving a certain way one does not need to learn a per-light switch rule but just a general rule. Unfortunately, when people move between cultures or design something for a different group they often do not attend to which conventions remain and which change.
Linguistic and ethnic groups often do not share these notational assumptions. Notational and operative assumptions can change control systems if the users implement, from a different culture than the designers, funnel interpretations from their original world view. Safety-critical systems, according to Seidner (pp. 5–7), become responses to threats of control. Through the emergence of majority and minority categories in society, cultural biases ensue.

</doc>
<doc id="49338" url="https://en.wikipedia.org/wiki?curid=49338" title="Bipolar junction transistor">
Bipolar junction transistor

A bipolar junction transistor (bipolar transistor or BJT) is a type of transistor that uses both electron and hole charge carriers. In contrast, unipolar transistors, such as field-effect transistors, only use one kind of charge carrier. For their operation, BJTs use two junctions between two semiconductor types, n-type and p-type. 
BJTs are manufactured in two types, NPN and PNP, and are available as individual components, or fabricated in integrated circuits, often in large numbers. The basic function of a BJT is to amplify current. This allows BJTs to be used as amplifiers or switches, giving them wide applicability in electronic equipment, including computers, televisions, mobile phones, audio amplifiers, industrial control, and radio transmitters.
Note on current direction.
By convention, the direction of current on diagrams is shown as the direction that a positive charge would move. This is called "conventional current". However, current in metal conductors is due to the flow of electrons which, because they carry a negative charge, move in the opposite direction to conventional current. On the other hand, inside a bipolar transistor, currents can be composed of both positively charged holes and negatively charged electrons. In this article, current arrows are shown in the conventional direction, but labels for the movement of holes and electrons show their actual direction inside the transistor.
Function.
BJTs come in two types, or polarities, known as PNP and NPN based on the doping types of the three main terminal regions. An NPN transistor comprises two semiconductor junctions that share a thin p-doped anode region, and a PNP transistor comprises two semiconductor junctions that share a thin n-doped cathode region.
Charge flow in a BJT is due to diffusion of charge carriers across a junction between two regions of different charge concentrations. The regions of a BJT are called "emitter", "collector", and "base". A discrete transistor has three leads for connection to these regions. Typically, the emitter region is heavily doped compared to the other two layers, whereas the majority charge carrier concentrations in base and collector layers are about the same. By design, most of the BJT collector current is due to the flow of charges injected from a high-concentration emitter into the base where there are minority carriers that diffuse toward the collector, and so BJTs are classified as minority-carrier devices.
In typical operation, the base–emitter junction is forward biased, which means that the p-doped side of the junction is at a more positive potential than the n-doped side, and the base–collector junction is reverse biased. In an NPN transistor, when positive bias is applied to the base–emitter junction, the equilibrium is disturbed between the thermally generated carriers and the repelling electric field of the n-doped emitter depletion region. This allows thermally excited electrons to inject from the emitter into the base region. These electrons diffuse through the base from the region of high concentration near the emitter towards the region of low concentration near the collector. The electrons in the base are called "minority carriers" because the base is doped p-type, which makes holes the "majority carrier" in the base.
To minimize the percentage of carriers that recombine before reaching the collector–base junction, the transistor's base region must be thin enough that carriers can diffuse across it in much less time than the semiconductor's minority carrier lifetime. In particular, the thickness of the base must be much less than the of the electrons. The collector–base junction is reverse-biased, and so little electron injection occurs from the collector to the base, but electrons that diffuse through the base towards the collector are swept into the collector by the electric field in the depletion region of the collector–base junction. The thin "shared" base and asymmetric collector–emitter doping are what differentiates a bipolar transistor from two "separate" and oppositely biased diodes connected in series.
Voltage, current, and charge control.
The collector–emitter current can be viewed as being controlled by the base–emitter current (current control), or by the base–emitter voltage (voltage control). These views are related by the current–voltage relation of the base–emitter junction, which is just the usual exponential current–voltage curve of a p-n junction (diode).
The physical explanation for collector current is the concentration of minority carriers in the base region. Due to low level injection (in which there are much fewer excess carriers than normal majority carriers) the ambipolar transport rates (in which the excess majority and minority carriers flow at the same rate) is in effect determined by the excess minority carriers.
Detailed transistor models of transistor action, such as the Gummel–Poon model, account for the distribution of this charge explicitly to explain transistor behaviour more exactly. The charge-control view easily handles phototransistors, where minority carriers in the base region are created by the absorption of photons, and handles the dynamics of turn-off, or recovery time, which depends on charge in the base region recombining. However, because base charge is not a signal that is visible at the terminals, the current- and voltage-control views are generally used in circuit design and analysis.
In analog circuit design, the current-control view is sometimes used because it is approximately linear. That is, the collector current is approximately formula_1 times the base current. Some basic circuits can be designed by assuming that the emitter–base voltage is approximately constant, and that collector current is beta times the base current. However, to accurately and reliably design production BJT circuits, the voltage-control (for example, Ebers–Moll) model is required. The voltage-control model requires an exponential function to be taken into account, but when it is linearized such that the transistor can be modeled as a transconductance, as in the Ebers–Moll model, design for circuits such as differential amplifiers again becomes a mostly linear problem, so the voltage-control view is often preferred. For translinear circuits, in which the exponential I–V curve is key to the operation, the transistors are usually modeled as voltage-controlled current sources whose transconductance is proportional to their collector current. In general, transistor-level circuit design is performed using SPICE or a comparable analog circuit simulator, so model complexity is usually not of much concern to the designer.
Turn-on, turn-off, and storage delay.
The bipolar transistor exhibits a few delay characteristics when turning on and off. Most transistors, and especially power transistors, exhibit long base-storage times that limit maximum frequency of operation in switching applications. One method for reducing this storage time is by using a Baker clamp.
Transistor parameters: alpha (α) and beta (β).
The proportion of electrons able to cross the base and reach the collector is a measure of the BJT efficiency. The heavy doping of the emitter region and light doping of the base region causes many more electrons to be injected from the emitter into the base than holes to be injected from the base into the emitter.
The "common-emitter current gain" is represented by βF or the h-parameter hFE; it is approximately the ratio of the DC collector current to the DC base current in forward-active region. It is typically greater than 50 for small-signal transistors but can be smaller in transistors designed for high-power applications.
Another important parameter is the "common-base current gain", αF. The common-base current gain is approximately the gain of current from emitter to collector in the forward-active region. This ratio usually has a value close to unity; between 0.980 and 0.998. It is less than unity due to recombination of charge carriers as they cross the base region.
Alpha and beta are more precisely related by the following identities (NPN transistor):
Structure.
A BJT consists of three differently doped semiconductor regions: the "emitter" region, the "base" region and the "collector" region. These regions are, respectively, "p" type, "n" type and "p" type in a PNP transistor, and "n" type, "p" type and "n" type in an NPN transistor. Each semiconductor region is connected to a terminal, appropriately labeled: "emitter" (E), "base" (B) and "collector" (C).
The "base" is physically located between the "emitter" and the "collector" and is made from lightly doped, high resistivity material. The collector surrounds the emitter region, making it almost impossible for the electrons injected into the base region to escape without being collected, thus making the resulting value of α very close to unity, and so, giving the transistor a large β. A cross section view of a BJT indicates that the collector–base junction has a much larger area than the emitter–base junction.
The bipolar junction transistor, unlike other transistors, is usually not a symmetrical device. This means that interchanging the collector and the emitter makes the transistor leave the forward active mode and start to operate in reverse mode. Because the transistor's internal structure is usually optimized for forward-mode operation, interchanging the collector and the emitter makes the values of α and β in reverse operation much smaller than those in forward operation; often the α of the reverse mode is lower than 0.5. The lack of symmetry is primarily due to the doping ratios of the emitter and the collector. The emitter is heavily doped, while the collector is lightly doped, allowing a large reverse bias voltage to be applied before the collector–base junction breaks down. The collector–base junction is reverse biased in normal operation. The reason the emitter is heavily doped is to increase the emitter injection efficiency: the ratio of carriers injected by the emitter to those injected by the base. For high current gain, most of the carriers injected into the emitter–base junction must come from the emitter.
The low-performance "lateral" bipolar transistors sometimes used in CMOS processes are sometimes designed symmetrically, that is, with no difference between forward and backward operation.
Small changes in the voltage applied across the base–emitter terminals causes the current that flows between the "emitter" and the "collector" to change significantly. This effect can be used to amplify the input voltage or current. BJTs can be thought of as voltage-controlled current sources, but are more simply characterized as current-controlled current sources, or current amplifiers, due to the low impedance at the base.
Early transistors were made from germanium but most modern BJTs are made from silicon. A significant minority are also now made from gallium arsenide, especially for very high speed applications (see HBT, below).
NPN.
NPN is one of the two types of bipolar transistors, consisting of a layer of P-doped semiconductor (the "base") between two N-doped layers. A small current entering the base is amplified to produce a large collector and emitter current. That is, when there is a positive potential difference measured from the emitter of an NPN transistor to its base (i.e., when the base is high relative to the emitter) as well as positive potential difference measured from the base to the collector, the transistor becomes active. In this "on" state, current flows between the collector and emitter of the transistor. Most of the current is carried by electrons moving from emitter to collector as minority carriers in the P-type base region. To allow for greater current and faster operation, most bipolar transistors used today are NPN because electron mobility is higher than hole mobility.
A mnemonic device for the NPN transistor symbol is ""n"ot "p"ointing i"n"", based on the arrows in the symbol and the letters in the name.
PNP.
The other type of BJT is the PNP, consisting of a layer of N-doped semiconductor between two layers of P-doped material. A small current leaving the base is amplified in the collector output. That is, a PNP transistor is "on" when its base is pulled low relative to the emitter.
In a PNP transistor, emitter-base region is forward biased. so electric field and carriers will be generated. They should flow towards the base junction, but the base part is very thin and has low conductivity. the reverse biased collector base part has generated holes. so due to the electric field, carriers or electrons get pulled by the holes.
The arrows in the NPN and PNP transistor symbols are on the emitter legs and point in the direction of the conventional current flow when the device is in forward active mode.
A mnemonic device for the PNP transistor symbol is ""p"ointing i"n" ("p"roudly/"p"ermanently)", based on the arrows in the symbol and the letters in the name.
Heterojunction bipolar transistor.
The heterojunction bipolar transistor (HBT) is an improvement of the BJT that can handle signals of very high frequencies up to several hundred GHz. It is common in modern ultrafast circuits, mostly RF systems.
Heterojunction transistors have different semiconductors for the elements of the transistor. Usually the emitter is composed of a larger bandgap material than the base. The figure shows that this difference in bandgap allows the barrier for holes to inject backward from the base into the emitter, denoted in the figure as Δφp, to be made large, while the barrier for electrons to inject into the base Δφn is made low. This barrier arrangement helps reduce minority carrier injection from the base when the emitter-base junction is under forward bias, and thus reduces base current and increases emitter injection efficiency.
The improved injection of carriers into the base allows the base to have a higher doping level, resulting in lower resistance to access the base electrode. In the more traditional BJT, also referred to as homojunction BJT, the efficiency of carrier injection from the emitter to the base is primarily determined by the doping ratio between the emitter and base, which means the base must be lightly doped to obtain high injection efficiency, making its resistance relatively high. In addition, higher doping in the base can improve figures of merit like the Early voltage by lessening base narrowing.
The grading of composition in the base, for example, by progressively increasing the amount of germanium in a SiGe transistor, causes a gradient in bandgap in the neutral base, denoted in the figure by ΔφG, providing a "built-in" field that assists electron transport across the base. That drift component of transport aids the normal diffusive transport, increasing the frequency response of the transistor by shortening the transit time across the base.
Two commonly used HBTs are silicon–germanium and aluminum gallium arsenide, though a wide variety of semiconductors may be used for the HBT structure. HBT structures are usually grown by epitaxy techniques like MOCVD and MBE.
Regions of operation.
Bipolar transistors have five distinct regions of operation, defined by BJT junction biases.
The modes of operation can be described in terms of the applied voltages (this description applies to NPN transistors; polarities are reversed for PNP transistors):
In terms of junction biasing: ("reverse biased base–collector junction" means V < 0 for NPN, opposite for PNP)
Although these regions are well defined for sufficiently large applied voltage, they overlap somewhat for small (less than a few hundred millivolts) biases. For example, in the typical grounded-emitter configuration of an NPN BJT used as a pulldown switch in digital logic, the "off" state never involves a reverse-biased junction because the base voltage never goes below ground; nevertheless the forward bias is close enough to zero that essentially no current flows, so this end of the forward active region can be regarded as the cutoff region.
Active-mode NPN transistors in circuits.
The diagram shows a schematic representation of an NPN transistor connected to two voltage sources. To make the transistor conduct appreciable current (on the order of 1 mA) from C to E, "V"BE must be above a minimum value sometimes referred to as the cut-in voltage. The cut-in voltage is usually about 650 mV for silicon BJTs at room temperature but can be different depending on the type of transistor and its biasing. This applied voltage causes the lower P-N junction to 'turn on', allowing a flow of electrons from the emitter into the base. In active mode, the electric field existing between base and collector (caused by "V"CE) will cause the majority of these electrons to cross the upper P-N junction into the collector to form the collector current "I"C. The remainder of the electrons recombine with holes, the majority carriers in the base, making a current through the base connection to form the base current, "I"B. As shown in the diagram, the emitter current, "I"E, is the total transistor current, which is the sum of the other terminal currents, (i.e., "I"E = "I"B + "I"C).
In the diagram, the arrows representing current point in the direction of conventional current – the flow of electrons is in the opposite direction of the arrows because electrons carry negative electric charge. In active mode, the ratio of the collector current to the base current is called the "DC current gain". This gain is usually 100 or more, but robust circuit designs do not depend on the exact value (for example see op-amp). The value of this gain for DC signals is referred to as formula_4, and the value of this gain for small signals is referred to as formula_5. That is, when a small change in the currents occurs, and sufficient time has passed for the new condition to reach a steady state formula_5 is the ratio of the change in collector current to the change in base current. The symbol formula_7 is used for both formula_4 and formula_5.
The emitter current is related to formula_10 exponentially. At room temperature, an increase in formula_10 by approximately 60 mV increases the emitter current by a factor of 10. Because the base current is approximately proportional to the collector and emitter currents, they vary in the same way.
Active-mode PNP transistors in circuits.
The diagram shows a schematic representation of a PNP transistor connected to two voltage sources. To make the transistor conduct appreciable current (on the order of 1 mA) from E to C, formula_12 must be above a minimum value sometimes referred to as the cut-in voltage. The cut-in voltage is usually about 650 mV for silicon BJTs at room temperature but can be different depending on the type of transistor and its biasing. This applied voltage causes the upper P-N junction to 'turn-on' allowing a flow of holes from the emitter into the base. In active mode, the electric field existing between the emitter and the collector (caused by formula_13) causes the majority of these holes to cross the lower p-n junction into the collector to form the collector current formula_14. The remainder of the holes recombine with electrons, the majority carriers in the base, making a current through the base connection to form the base current, formula_15. As shown in the diagram, the emitter current, formula_16, is the total transistor current, which is the sum of the other terminal currents (i.e., "I"E = "I"B + "I"C).
In the diagram, the arrows representing current point in the direction of conventional current – the flow of holes is in the same direction of the arrows because holes carry positive electric charge. In active mode, the ratio of the collector current to the base current is called the "DC current gain". This gain is usually 100 or more, but robust circuit designs do not depend on the exact value. The value of this gain for DC signals is referred to as formula_4, and the value of this gain for AC signals is referred to as formula_5. However, when there is no particular frequency range of interest, the symbol formula_7 is used.
It should also be noted that the emitter current is related to formula_12 exponentially. At room temperature, an increase in formula_12 by approximately 60 mV increases the emitter current by a factor of 10. Because the base current is approximately proportional to the collector and emitter currents, they vary in the same way.
History.
The bipolar point-contact transistor was invented in December 1947 at the Bell Telephone Laboratories by John Bardeen and Walter Brattain under the direction of William Shockley. The junction version known as the bipolar junction transistor, invented by Shockley in 1948, enjoyed three decades as the device of choice in the design of discrete and integrated circuits. Nowadays, the use of the BJT has declined in favor of CMOS technology in the design of digital integrated circuits. The incidental low performance BJTs inherent in CMOS ICs, however, are often utilized as bandgap voltage reference, silicon bandgap temperature sensor and to handle electrostatic discharge.
Germanium transistors.
The germanium transistor was more common in the 1950s and 1960s, and while it exhibits a lower "cut off" voltage, typically around 0.2 V, making it more suitable for some applications, it also has a greater tendency to exhibit thermal runaway.
Early manufacturing techniques.
Various methods of manufacturing bipolar transistors were developed.
Theory and modeling.
Transistors can be thought of as two diodes (P–N junctions) sharing a common region that minority carriers can move through. A PNP BJT will function like two diodes that share an N-type cathode region, and the NPN like two diodes sharing a P-type anode region. Connecting two diodes with wires will not make a transistor, since minority carriers will not be able to get from one P–N junction to the other through the wire.
Both types of BJT function by letting a small current input to the base control an amplified output from the collector. The result is that the transistor makes a good switch that is controlled by its base input. The BJT also makes a good amplifier, since it can multiply a weak input signal to about 100 times its original strength. Networks of transistors are used to make powerful amplifiers with many different applications. In the discussion below, focus is on the NPN bipolar transistor. In the NPN transistor in what is called active mode, the base–emitter voltage formula_10 and collector–base voltage formula_23 are positive, forward biasing the emitter–base junction and reverse-biasing the collector–base junction. In the active mode of operation, electrons are injected from the forward biased n-type emitter region into the p-type base where they diffuse as minority carriers to the reverse-biased n-type collector and are swept away by the electric field in the reverse-biased collector–base junction. For a figure describing forward and reverse bias, see semiconductor diodes.
Large-signal models.
In 1954, Jewell James Ebers and John L. Moll introduced their mathematical model of transistor currents:
Ebers–Moll model.
The DC emitter and collector currents in active mode are well modeled by an approximation to the Ebers–Moll model:
The base internal current is mainly by diffusion (see Fick's law) and
where
The formula_34 and forward formula_7 parameters are as described previously. A reverse formula_7 is sometimes included in the model.
The unapproximated Ebers–Moll equations used to describe the three currents in any operating region are given below. These equations are based on the transport model for a bipolar junction transistor.
where
Base-width modulation.
As the collector–base voltage (formula_47) varies, the collector–base depletion region varies in size. An increase in the collector–base voltage, for example, causes a greater reverse bias across the collector–base junction, increasing the collector–base depletion region width, and decreasing the width of the base. This variation in base width often is called the "Early effect" after its discoverer James M. Early.
Narrowing of the base width has two consequences:
Both factors increase the collector or "output" current of the transistor in response to an increase in the collector–base voltage.
In the forward-active region, the Early effect modifies the collector current (formula_38) and the forward common emitter current gain (formula_1) as given by:
where:
Punchthrough.
When the base–collector voltage reaches a certain (device specific) value, the base–collector depletion region boundary meets the base–emitter depletion region boundary. When in this state the transistor effectively has no base. The device thus loses all gain when in this state.
Gummel–Poon charge-control model.
The Gummel–Poon model is a detailed charge-controlled model of BJT dynamics, which has been adopted and elaborated by others to explain transistor dynamics in greater detail than the terminal-based models typically do [http://ece-www.colorado.edu/~bart/book/book/chapter5/ch5_6.htm#5_6_2]. This model also includes the dependence of transistor formula_7-values upon the direct current levels in the transistor, which are assumed current-independent in the Ebers–Moll model.
Small-signal models.
hybrid-pi model.
The hybrid-pi model is a popular circuit model used for analyzing the small signal behavior of bipolar junction and field effect transistors. Sometimes it is also called "Giacoletto model" because it was introduced by L.J. Giacoletto in 1969. The model can be quite accurate for low-frequency circuits and can easily be adapted for higher frequency circuits with the addition of appropriate inter-electrode capacitances and other parasitic elements.
h-parameter model.
Another model commonly used to analyze BJT circuits is the "h-parameter" model, closely related to the hybrid-pi model and the y-parameter two-port, but using input current and output voltage as independent variables, rather than input and output voltages. This two-port network is particularly suited to BJTs as it lends itself easily to the analysis of circuit behaviour, and may be used to develop further accurate models. As shown, the term, "x", in the model represents a different BJT lead depending on the topology used. For common-emitter mode the various symbols take on the specific values as:
and the h-parameters are given by:
As shown, the h-parameters have lower-case subscripts and hence signify AC conditions or analyses. For DC conditions they are specified in upper-case. For the CE topology, an approximate h-parameter model is commonly used which further simplifies the circuit analysis. For this the "h"oe and "h"re parameters are neglected (that is, they are set to infinity and zero, respectively). It should also be noted that the h-parameter model as shown is suited to low-frequency, small-signal analysis. For high-frequency analyses the inter-electrode capacitances that are important at high frequencies must be added.
Etymology of hFE.
The 'h' refers to its being an h-parameter, a set of parameters named for their origin in a "hybrid equivalent circuit" model. 'F' is from" forward current amplification" also called the current gain. 'E' refers to the transistor operating in a "common emitter" (CE) configuration. Capital letters used in the subscript indicate that hFE refers to a direct current circuit.
Industry models.
The Gummel Poon SPICE model is often used, but it suffers from several limitations. These have been addressed in various more advanced models: Mextram, VBIC, HICUM, Modella.
Applications.
The BJT remains a device that excels in some applications, such as discrete circuit design, due to the very wide selection of BJT types available, and because of its high transconductance and output resistance compared to MOSFETs.
The BJT is also the choice for demanding analog circuits, especially for very-high-frequency applications, such as radio-frequency circuits for wireless systems. 
High speed digital logic.
Emitter-coupled logic (ECL) use BJTs.
Bipolar transistors can be combined with MOSFETs in an integrated circuit by using a BiCMOS process of wafer fabrication to create circuits that take advantage of the application strengths of both types of transistor.
Amplifiers.
The α and β characterizes the current gain of the BJT. It is this gain that allow BJTs to be used as the building blocks of electronic amplifiers. The three main BJT amplifier topologies are :
Temperature sensors.
Because of the known temperature and current dependence of the forward-biased base–emitter junction voltage, the BJT can be used to measure temperature by subtracting two voltages at two different bias currents in a known ratio [http://www.maxim-ic.com/appnotes.cfm/appnote_number/689].
Logarithmic converters.
Because base–emitter voltage varies as the log of the base–emitter and collector–emitter currents, a BJT can also be used to compute logarithms and anti-logarithms. A diode can also perform these nonlinear functions but the transistor provides more circuit flexibility.
Vulnerabilities.
Exposure of the transistor to ionizing radiation causes radiation damage. Radiation causes a buildup of 'defects' in the base region that act as recombination centers. The resulting reduction in minority carrier lifetime causes gradual loss of gain of the transistor.
Power BJTs are subject to a failure mode called secondary breakdown, in which excessive current and normal imperfections in the silicon die cause portions of the silicon inside the device to become disproportionately hotter than the others. The doped silicon has a negative temperature coefficient, meaning that it conducts more current at higher temperatures. Thus, the hottest part of the die conducts the most current, causing its conductivity to increase, which then causes it to become progressively hotter again, until the device fails internally. The thermal runaway process associated with secondary breakdown, once triggered, occurs almost instantly and may catastrophically damage the transistor package.
If the emitter-base junction is reverse biased into avalanche or Zener mode and current flows for a short period of time, the current gain of the BJT will be permanently degraded.

</doc>
<doc id="49340" url="https://en.wikipedia.org/wiki?curid=49340" title="Doping">
Doping

Doping may refer to:

</doc>
<doc id="49351" url="https://en.wikipedia.org/wiki?curid=49351" title="Trucker's hitch">
Trucker's hitch

The trucker's hitch is a compound knot commonly used for securing loads on trucks or trailers. This general arrangement, using loops and turns in the rope itself to form a crude block and tackle, has long been used to tension lines and is known by multiple names. Knot author Geoffrey Budworth claims the knot can be traced back to the days when carters and hawkers used horse-drawn conveyances to move their wares from place to place.
Variations.
The portion of the trucker's hitch which differs in the following variations is the method used to form the loop which the working end slides through to produce the mechanical advantage. The different methods of forming the loop affect the ease and speed of tying and releasing and the stability of the final product.
The variations are presented in order of increasing stability.
Sheepshank style loop.
This version of the knot uses a sheepshank-like construction, in this kind of application also known as a "bell ringer's knot", to form the loop. It is quicker to make than a fixed loop, but is less dependable. It is avoided in critical applications (such as securing a load on a truck) as it can fall apart under too little load or too much load, and can capsize if not dressed properly. However, this knot may be made secure by adding a Half Hitch by using the top bight of the Sheepshank. This form of the trucker's hitch is least likely to jam, coming apart easily once tension is released. Different sources show slight variations in the way the sheepshank portion is formed and dressed.
Slipped overhand loop.
The loop formed in this version is a simple Slipped Overhand Loop or a variation using multiple turns of rope to form the eye of the loop. If extra loops are used to form the eye it tends to ease untying. In order to prevent the closing of the loop under load, the loop must be formed by the working end of the rope (which will later pass through the loop). If the standing end goes through the loop, it will close under load.
Fixed loop.
The most reliable common variation uses a fixed loop, such as an alpine butterfly loop, artillery loop, figure-eight loop, or another of many suitable loop knots. If a fixed loop is used repeatedly for tying the trucker's hitch in the same portion of rope, excessive wear or other damage may be suffered by the portion of the loop which working end slides against.
Finishing the hitch.
In tightening the trucker's hitch, tension can be effectively increased by repeatedly pulling sideways while preventing the tail end from slipping through the loop, and then cinching the knot tighter as the sideways force is released. This is called "sweating a line".
Once tight, the trucker's hitch is often secured with a half hitch, usually slipped for easy releasing and to avoid the necessity of access to the end of the rope, though a more secure finish, such as two half-hitches, may be called for. Under large loads, the finishing half hitch can jam, especially if it is not slipped; the difficulty of releasing it can be compounded by the fact that the knot is typically still under tension when it is untied.
Mechanical advantage and friction.
All common variations of the trucker's hitch use a loop in the standing part of the rope and the anchor point as makeshift pulleys in order to "theoretically" obtain a 3 to 1 mechanical advantage while pulling on the working end.
There is sometimes confusion about how much theoretical mechanical advantage is provided by the trucker's hitch. If the trucker's hitch were to be used as in the pulley diagram at right, to lift a weight off the floor, the theoretical mechanical advantage would be only 2:1. However in the common use of the trucker's hitch, a static hook, ring, or rail, serves as the lower pulley, and the rope across the top of the load is the portion being tensioned. Thus, the standing part of the rope is represented by the top anchor point in the diagram, and the theoretical ratio is indeed 3:1 when the working end is tensioned. That is, in a frictionless system, every unit of force exerted on the working end would produce 3 units in the standing part of the rope over the load. In the typical use of the trucker's hitch, where it is used to tighten a rope over a load, when the end is secured to the loop of the Truckers hitch and let go, the tension in the two segments of rope around the ring will rise 50%, unless the rope slackens when it is being tied off, in which case the tension may drop to any value or even zero if enough slack is allowed. But when the trucker's hitch is used as in the diagram, after tying off, the load on the attachment point above the top pulley will drop to 400 lb and the tension in the two lines going to the lower pulley will not change.
Theoretical considerations aside, in real world use the mechanical advantage of the trucker's hitch is significantly less than the ideal case due to the effects of friction. Friction has been reported to reduce the mechanical advantage from 3 to 1, to well less than 2 to 1 in many cases. One advantage of the friction within the trucker's hitch, compared to a hypothetical pulley-based system, is that it allows the hitch to be held taut with less force while the working end is secured.
Cultural references.
The trucker's hitch knot is portrayed by comedy duo Ylvis in their 2014 song with the same name. The lyrics and the video demonstrate how to tie the knot.

</doc>
<doc id="49352" url="https://en.wikipedia.org/wiki?curid=49352" title="Hangman's knot">
Hangman's knot

The hangman's knot or hangman's noose (also known as a collar during the Elizabethan era) is a well-known knot most often associated with its use in hanging a person. For a hanging, the knot of the rope is typically placed under or just behind the left ear. When the condemned drops to the end of the rope, the force is supposed to break the neck. The knot is non-jamming but tends to resist attempts to loosen it. 
Number of coils.
Each additional coil adds friction to the knot, which makes the noose harder to pull closed or open. The number of coils should therefore be adjusted depending on the intended use, the type and thickness of rope, and environmental conditions such as wet or greasy rope. Six to eight loops are normal when using natural ropes. One coil makes it equivalent to the simple slip knot.
The number thirteen was thought to be unlucky. Consequently, thirteen coils were found in a hangman’s noose, a foreboding sign for those convicted to be hanged.
Woody Guthrie sings of the hangman using thirteen coils:
<poem>Did you ever see a hangman tie a hangknot?
I've seen it many a time and he winds, he winds,
After thirteen times he's got a hangknot.</poem>
Other uses.
A variation of this knot is used in fishing and is called the Uni-knot. It is used to tie fishing line to terminal tackle, join two pieces of line, or for snelling hooks. It is especially useful when used with slick braided line as more coils can be added to increase the friction of the knot and will not let the knot pull out. It is also useful in that the knot can be pulled down tight to the lure or it can be left with a larger loop that gives the lure more freedom of movement. The hangman's noose can also be used in boating to secure an eyelet on a rope or sheet without splicing it.

</doc>
<doc id="49355" url="https://en.wikipedia.org/wiki?curid=49355" title="Thief knot">
Thief knot

The Thief knot resembles the reef knot (square knot) except that the free, or bitter ends are on opposite sides. It is said that sailors would secure their belongings in a using the thief knot, often with the ends hidden. If another sailor went through the bag, the odds were high the thief would tie the bag back using the more common reef knot, revealing the tampering, hence the name. It is difficult to tie by mistake, unlike the granny knot. 
The thief knot is much less secure than the already insecure reef knot. It unties itself if the lines are pulled when the same action would seize a reef knot. 

</doc>
<doc id="49364" url="https://en.wikipedia.org/wiki?curid=49364" title="Turner syndrome">
Turner syndrome

Turner syndrome (TS) also known as Ullrich–Turner syndrome, gonadal dysgenesis, and 45,X, is a condition in which a female is partly or completely missing an X chromosome. Signs and symptoms vary among those affected. --> Often, a short and webbed neck, low-set ears, low hairline at the back of the neck, short stature, and swollen hands and feet are seen at birth. --> Typically they are without menstrual periods, do not develop breasts, and are unable to have children. --> Heart defects, diabetes, and low thyroid hormone occur more frequently. --> Most people with TS have normal intelligence. --> Many, however, have troubles with spatial visualization such as that needed for mathematics. Vision and hearing problems occur more often.
Turner syndrome is not usually inherited from a person's parents. No environmental risks are known and the mother's age does not play a role. Turner syndrome is due to a chromosomal abnormality in which all or part of one of the X chromosomes is missing or altered. --> While most people have 46 chromosomes, people with TS usually have 45. The chromosomal abnormality may be present in just some cells in which case it is known as TS with mosaicism. In these cases, the symptoms are usually fewer and possibly none occur at all. Diagnosis is based on physical signs and genetic testing.
No cure for Turner syndrome is known. Treatment, however, may help with symptoms. --> Human growth hormone injections during childhood may increase adult height. --> Estrogen replacement therapy can promote development of the breasts and hips. --> Medical care is often required to manage other health problems with which TS is associated.
Turner syndrome occurs in between one in 2000 and one in 5000 females at birth. All regions of the world and cultures are affected about equally. People with TS have a shorter life expectancy, mostly due to heart problems and diabetes. Henry Turner first described the condition in 1938. --> In 1964, it was determined to be due to a chromosomal abnormality.
Signs and symptoms.
Of the following common symptoms of Turner syndrome, an individual may have any combination of symptoms and is unlikely to have all symptoms.
Other features may include a small lower jaw (micrognathia), cubitus valgus, soft upturned nails, palmar crease, and drooping eyelids. Less common are pigmented moles, hearing loss, and a high-arch palate (narrow maxilla). Turner syndrome manifests itself differently in each female affected by the condition; therefore, no two individuals share the same features.
While most of the physical findings are harmless, significant medical problems can be associated with the syndrome.
Prenatal.
Despite the excellent postnatal prognosis, 99% of Turner-syndrome conceptions are thought to end in spontaneous abortion or stillbirth, and as many as 15% of all spontaneous abortions have the 45,X karyotype. Among cases that are detected by routine amniocentesis or chorionic villus sampling, one study found that the prevalence of Turner syndrome among tested pregnancies was 5.58 and 13.3 times higher, respectively, than among live neonates in a similar population.
Cardiovascular.
Prevalence of cardiovascular malformations.
The prevalence of cardiovascular malformations among patients with Turner syndrome ranges from 17% to 45%. The variations found in the different studies are mainly attributable to variations in noninvasive methods used for screening and the types of lesions that they can characterize. However, it could be simply attributable to the small number of subjects in most studies.
Different karyotypes may have differing prevalence of cardiovascular malformations. Two studies found a prevalence of cardiovascular malformations of 30% and 38% in a group of pure 45,X monosomy. Considering other karyotype groups, though, they reported a prevalence of 24.3% and 11% in patients with mosaic X monosomy, and a prevalence of 11% in patients with X chromosomal structural abnormalities.
The higher prevalence in the group of pure 45,X monosomy is primarily due to a significant difference in the prevalence of aortic valve abnormalities and coarctation of the aorta, the two most common cardiovascular malformations.
Congenital heart disease.
The most commonly observed are congenital obstructive lesions of the left side of the heart, leading to reduced flow on this side of the heart. This includes bicuspid aortic valve and coarctation (narrowing) of the aorta. Sybert, 1998 found that more than 50% of the cardiovascular malformations observed in her study of individuals with Turner syndrome were bicuspid aortic valves or coarctation of the aorta, alone or in combination.
Other congenital cardiovascular malformations, such as partial anomalous venous drainage and aortic valve stenosis or aortic regurgitation, are also more common in Turner syndrome than in the general population. Hypoplastic left heart syndrome represents the most severe reduction in left-sided structures
Bicuspid aortic valve.
Up to 15% of adults with Turner syndrome have bicuspid aortic valves, meaning that there are only two, instead of three, parts to the valves in the main blood vessel leading from the heart. Since bicuspid valves are capable of regulating blood flow properly, this condition may go undetected without regular screening. However, bicuspid valves are more likely to deteriorate and later fail. Calcification also occurs in the valves, which may lead to a progressive valvular dysfunction as evidenced by aortic stenosis or regurgitation.
With a prevalence from 12.5% to 17.5% (Dawson-Falk et al., 1992), bicuspid aortic valve is the most common congenital malformation affecting the heart in this syndrome. It is usually isolated but it may be seen in combination with other anomalies, particularly coarctation of the aorta.
Coarctation of the aorta.
Between 5% and 10% of those born with Turner syndrome have coarctation of the aorta, a congenital narrowing of the descending aorta, usually just distal to the origin of the left subclavian artery (the artery that branches off the arch of the aorta to the left arm) and opposite to the duct (and so termed "juxtaductal"). Estimates of the prevalence of this malformation in patients with Turner syndrome ranges from 6.9% to 12.5% . A coarctation of the aorta in a female is suggestive of Turner syndrome, and suggests the need for further tests, such as a karyotype.
Partial anomalous venous drainage.
This abnormality is a relatively rare congenital heart disease in the general population. The prevalence of this abnormality also is low (around 2.9%) in Turner syndrome. However, its relative risk is 320 in comparison with the general population. Strangely, Turner syndrome seems to be associated with unusual forms of partial anomalous venous drainage.
In the management of a patient with Turner syndrome it is essential to keep in mind that these left-sided cardiovascular malformations in Turner syndrome result in an increased susceptibility to bacterial endocarditis. Therefore, prophylactic antibiotics should be considered when procedures with high risk endocarditis are performed, such as dental cleaning.
Turner syndrome is often associated with persistent hypertension, sometimes in childhood. In the majority of Turner syndrome patients with hypertension, there is no specific cause. In the remainder, it is usually associated with cardiovascular or kidney abnormalities, including coarctation of the aorta.
Aortic dilation, dissection, and rupture.
Two studies have suggested aortic dilatation in Turner syndrome, typically involving the root of the ascending aorta and occasionally extending through the aortic arch to the descending aorta, or at the site of previous coarctation of the aorta repair.
Sybert, 1998 points out that it remains unproven that aortic root diameters that are relatively large for body surface area but still well within normal limits imply a risk for progressive dilatation.
Prevalence of aortic abnormalities.
The prevalence of aortic root dilatation ranges from 8.8% to 42% in patients with Turner syndrome. Even if not every aortic root dilatation necessarily goes on to an aortic dissection (circumferential or transverse tear of the intima), complications such as dissection, aortic rupture resulting in death may occur. The natural history of aortic root dilatation is still unknown, but it is a fact that it is linked to aortic dissection and rupture, which has a high mortality rate.
Aortic dissection affects 1% to 2% of patients with Turner syndrome. As a result, any aortic root dilatation should be seriously taken into account as it could become a fatal aortic dissection. Routine surveillance is highly recommended.
Risk factors for aortic rupture.
It is well established that cardiovascular malformations (typically bicuspid aortic valve, coarctation of the aorta and some other left-sided cardiac malformations) and hypertension predispose to aortic dilatation and dissection in the general population. At the same time it has been shown that these risk factors are common in Turner syndrome. Indeed, these same risk factors are found in more than 90% of patients with Turner syndrome who develop aortic dilatation. Only a small number of patients (around 10%) have no apparent predisposing risk factors. It is important to note that the risk of hypertension is increased 3-fold in patients with Turner syndrome. Because of its relation to aortic dissection blood pressure needs to be regularly monitored and hypertension should be treated aggressively with an aim to keep blood pressure below 140/80 mmHg. It has to be noted that as with the other cardiovascular malformations, complications of aortic dilatation is commonly associated with 45,X karyotype.
Pathogenesis of aortic dissection and rupture.
The exact role that all these risk factors play in the process leading to such fatal complications is still quite unclear. Aortic root dilatation is thought to be due to a mesenchymal defect as pathological evidence of cystic medial necrosis has been found by several studies. The association between a similar defect and aortic dilatation is well established in such conditions such as Marfan syndrome. Also, abnormalities in other mesenchymal tissues (bone matrix and lymphatic vessels) suggests a similar primary mesenchymal defect in patients with Turner syndrome. However, there is no evidence to suggest that patients with Turner syndrome have a significantly higher risk of aortic dilatation and dissection in absence of predisposing factors. So the risk of aortic dissection in Turner syndrome appears to be a consequence of structural cardiovascular malformations and hemodynamic risk factors rather than a reflection of an inherent abnormality in connective tissue (Sybert, 1998). The natural history of aortic root dilatation is unknown, but because of its lethal potential, this aortic abnormality needs to be carefully followed.
Skeletal.
Normal skeletal development is inhibited due to a large variety of factors, mostly hormonal. The average height of a woman with Turner syndrome, in the absence of growth hormone treatment, is (140 cm). Patients with Turner's mosaicism can reach normal average height.
The fourth metacarpal bone (fourth toe and ring finger) may be unusually short, as may the fifth.
Due to inadequate production of estrogen, many of those with Turner syndrome develop osteoporosis. This can decrease height further, as well as exacerbate the curvature of the spine, possibly leading to scoliosis. It is also associated with an increased risk of bone fractures.
Kidney.
Approximately one-third of all women with Turner syndrome have one of three kidney abnormalities:
Some of these conditions can be corrected surgically. Even with these abnormalities, the kidneys of most women with Turner syndrome function normally. However, as noted above, kidney problems may be associated with hypertension.
Thyroid.
Approximately one-third of all women with Turner syndrome have a thyroid disorder. Usually it is hypothyroidism, specifically Hashimoto's thyroiditis. If detected, it can be easily treated with thyroid hormone supplements.
Diabetes.
Women with Turner syndrome are at a moderately increased risk of developing type 1 diabetes in childhood and a substantially increased risk of developing type 2 diabetes by adult years. The risk of developing type 2 diabetes can be substantially reduced by maintaining a healthy weight.
Cognitive.
Turner syndrome does not typically cause intellectual disability or impair cognition. However, learning difficulties are common among women with Turner syndrome, particularly a specific difficulty in perceiving spatial relationships, such as nonverbal learning disorder. This may also manifest itself as a difficulty with motor control or with mathematics. While it is not correctable, in most cases it does not cause difficulty in daily living. Most Turner syndrome patients are employed as adults and lead productive lives.
Also, a rare variety of Turner syndrome, known as "Ring-X Turner syndrome", has about a 60% association with intellectual disability. This variety accounts for around 2–4% of all Turner syndrome cases.
Reproductive.
Women with Turner syndrome are almost universally infertile. While some women with Turner syndrome have successfully become pregnant and carried their pregnancies to term, this is very rare and is generally limited to those women whose karyotypes are not 45,X. Even when such pregnancies do occur, there is a higher than average risk of miscarriage or birth defects, including Turner Syndrome or Down Syndrome. Some women with Turner syndrome who are unable to conceive without medical intervention may be able to use IVF or other fertility treatments.
Usually estrogen replacement therapy is used to spur growth of secondary sexual characteristics at the time when puberty should onset. While very few women with Turner Syndrome menstruate spontaneously, estrogen therapy requires a regular shedding of the uterine lining ("withdrawal bleeding") to prevent its overgrowth. Withdrawal bleeding can be induced monthly, like menstruation, or less often, usually every three months, if the patient desires. Estrogen therapy does not make a woman with nonfunctional ovaries fertile, but it plays an important role in assisted reproduction; the health of the uterus must be maintained with estrogen if an eligible woman with Turner Syndrome wishes to use IVF (using donated oocytes).
Turner syndrome is a cause of primary amenorrhea, premature ovarian failure (hypergonadotropic hypogonadism), streak gonads and infertility. Failure to develop secondary sex characteristics (sexual infantilism) is typical.
Especially in mosaic cases of Turner syndrome that contains Y-chromosome (e.g. 45,X/46,XY) due to the risk of development of ovarian malignancy (most common is gonadoblastoma) gonadectomy is recommended.
Turner syndrome is characterized by primary amenorrhoea, premature ovarian failure, streak gonads and infertility. However, technology (especially oocyte donation) provides the opportunity of pregnancy in these patients.
As more women with Turner syndrome complete pregnancy thanks to modern techniques to treat infertility, it has to be noted that pregnancy may be a risk of cardiovascular complications for the mother. Indeed, several studies had suggested an increased risk for aortic dissection in pregnancy. Three deaths have even been reported. The influence of estrogen has been examined but remains unclear. It seems that the high risk of aortic dissection during pregnancy in women with Turner syndrome may be due to the increased hemodynamic load rather than the high estrogen rate. Of course these findings are important and need to be remembered while following a pregnant patient with Turner syndrome.
Cause.
Turner syndrome is caused by the absence of one complete or partial copy of the X chromosome in some or all the cells. The abnormal cells may have only one X (monosomy) (45,X) or they may be affected by one of several types of partial monosomy like a deletion of the short p arm of one X chromosome (46,X,del(Xp)) or the presence of an isochromosome with two q arms (46,X,i(Xq)) In mosaic individuals, cells with X monosomy (45,X) may occur along with cells that are normal (46,XX), cells that have partial monosomies, or cells that have a Y chromosome (46,XY). The presence of mosaicism is estimated to be relatively common in affected individuals (67–90%).
Inheritance.
In the majority of cases where monosomy occurs, the X chromosome comes from the mother. This may be due to a nondisjunction in the father. Meiotic errors that lead to the production of X with p arm deletions or abnormal Y chromosomes are also mostly found in the father. Isochromosome X or ring chromosome X on the other hand are formed equally often by both parents. Overall, the functional X chromosome usually comes from the mother.
In most cases, Turner syndrome is a sporadic event, and for the parents of an individual with Turner syndrome the risk of recurrence is not increased for subsequent pregnancies. Rare exceptions may include the presence of a balanced translocation of the X chromosome in a parent, or where the mother has 45,X mosaicism restricted to her germ cells.
Diagnosis.
Prenatal.
Turner syndrome may be diagnosed by amniocentesis or chorionic villus sampling during pregnancy.
Usually, fetuses with Turner syndrome can be identified by abnormal ultrasound findings ("i.e.", heart defect, kidney abnormality, cystic hygroma, ascites). In a study of 19 European registries, 67.2% of prenatally diagnosed cases of Turner Syndrome were detected by abnormalities on ultrasound. 69.1% of cases had one anomaly present, and 30.9% had two or more anomalies.
An increased risk of Turner syndrome may also be indicated by abnormal triple or quadruple maternal serum screen. The fetuses diagnosed through positive maternal serum screening are more often found to
have a mosaic karyotype than those diagnosed based on ultrasonographic abnormalities, and
conversely those with mosaic karyotypes are less likely to have associated ultrasound abnormalities.
Although the recurrence risk is not increased, genetic counseling is often recommended for families who have had a pregnancy or child with Turner syndrome.
Postnatal.
Turner syndrome can be diagnosed postnatally at any age. Often, it is diagnosed at birth due to heart problems, an unusually wide neck or swelling of the hands and feet. However, it is also common for it to go undiagnosed for several years, typically until the girl reaches the age of puberty/adolescence and she fails to develop properly (the changes associated with puberty do not occur). In childhood, a short stature can be indicative of Turner syndrome.
A test, called a karyotype or a chromosome analysis, analyzes the chromosomal composition of the individual. This is the test of choice to diagnose Turner syndrome.
Treatment.
As a chromosomal condition, there is no cure for Turner syndrome. However, much can be done to minimize the symptoms. For example:
Epidemiology.
Approximately 99 percent of all fetuses with Turner syndrome result in spontaneous termination during the first trimester. Turner syndrome accounts for about 10 percent of the total number of spontaneous abortions in the United States. The incidence of Turner syndrome in live female births is believed to be around 1 in 2000.
History.
The syndrome is named after Henry Turner, an endocrinologist from Illinois, who described it in 1938. In Europe, it is often called Ullrich–Turner syndrome or even Bonnevie–Ullrich–Turner syndrome to acknowledge that earlier cases had also been described by European doctors.
The first published report of a female with a 45,X karyotype was in 1959 by Dr. Charles Ford and colleagues in Harwell, Oxfordshire and Guy's Hospital in London. It was found in a 14-year-old girl with signs of Turner syndrome.

</doc>
<doc id="49365" url="https://en.wikipedia.org/wiki?curid=49365" title="LGM-30 Minuteman">
LGM-30 Minuteman

The LGM-30 Minuteman is a U.S. land-based intercontinental ballistic missile (ICBM), in service with the Air Force Global Strike Command. As of 2014, the LGM-30G Minuteman III version is the only land-based ICBM in service in the United States.
Development of the Minuteman began in the mid-1950s as the outgrowth of basic research into solid fuel rocket motors which indicated an ICBM based on solids was possible. Such a missile could stand ready for extended periods of time with little maintenance, and then launch on command. In comparison, existing U.S. missile designs using liquid fuels required a lengthy fueling process immediately before launch, which left them open to the possibility of surprise attack. This potential for immediate launch gave the missile its name; like the Revolutionary War's Minutemen, the Minuteman was designed to be launched on a moment's notice.
Minuteman entered service in 1962 as a weapon tasked primarily with the deterrence role, threatening Soviet cities with a counterattack if the U.S. was attacked. However, with the development of the U.S. Navy's Polaris which addressed the same role, the Air Force began to modify Minuteman into a weapon with much greater accuracy with the specific intent of allowing it to attack hardened military targets, including Soviet missile silos. The Minuteman-II entered service in 1965 with a host of upgrades to improve its accuracy and survivability in the face of an anti-ballistic missile (ABM) system the Soviets were known to be developing. Minuteman-III followed in 1970, using three smaller warheads instead of one large one, which made it very difficult to attack by an anti-ballistic missile system which would have to hit all three widely separated warheads to be effective. Minuteman-III was the first multiple independently targetable reentry vehicle (MIRV) ICBM to be deployed. Each missile can carry up to three nuclear warheads, which have a yield in the range of 300 to 500 kilotons.
Peaking at 1,000 missiles in the 1970s, the current U.S. force consists of 450 Minuteman-III missiles in missile silos around Malmstrom AFB, Montana; Minot AFB, North Dakota; and F.E. Warren AFB, Wyoming. By 2018 this will be reduced to 400 armed missiles, with 50 unarmed missiles in reserve, and four non-deployed test launchers to comply with the New START treaty. The Air Force plans to keep the missile in service until at least 2030. It is one component of the U.S. nuclear triad—the other two parts of the triad being the Trident submarine-launched ballistic missile (SLBM), and nuclear weapons carried by long-range strategic bombers.
History.
Edward Hall and solid fuels.
Minuteman owes its existence largely to the efforts of then Air Force Colonel Edward N. Hall. In 1956, Hall was put in charge of the solid fuel propulsion division of General Schriever's Western Development Division, which had originally been formed to lead development of the Atlas and Titan ICBMs. Solid fuels were already commonly used in rockets, but strictly for short-range uses. Hall's superiors were interested in short and medium range missiles with solids, especially for use in Europe, but Hall was convinced that they could be used for a true ICBM with range.
To achieve the required energy, Hall began funding research at Boeing and Thiokol into the use of ammonium perchlorate composite propellant. Adapting a concept developed in the UK, they cast the fuel into large cylinders with a star-shaped hole running along the inner axis. This allowed the fuel to burn along the entire length of the cylinder, rather than just the end as in earlier designs. The increased burn rate meant increased thrust. This also meant the heat was spread across the entire motor, instead of the end, and because it burned from the inside out it did not reach the wall of the missile fuselage until the fuel was finished burning.
Guidance of an ICBM is based not only on the direction the missile is travelling, but the precise instant that thrust is cut off. Too much thrust and the warhead will overshoot its target, too little and it will fall short. Solids are normally very hard to predict in terms of burning time and their instantaneous thrust during the burn, which made them questionable for the sort of accuracy required to hit a target at intercontinental range. This appeared at first to be an insurmountable problem, but in the end was solved in almost trivial fashion. A series of ports were added inside the rocket nozzle that were opened when the guidance systems called for engine cut-off. The reduction in pressure was so abrupt that the last burning fuel ejected itself and the flame was snuffed out.
Rapid success in the development program, combined with Edward Teller's promise of much lighter nuclear warheads during Project Nobska, led the Navy to abandon their work with the US Army's liquid fuel Jupiter missile and begin development of a solid fuel missile of their own. They felt that liquid fuels were too dangerous to use onboard ships, and especially submarines. Aerojet's work with Hall would be adapted for their Polaris missile starting in December 1956.
Missile farm concept.
The Air Force, however, saw no pressing need for a solid fuel ICBM. Atlas and Titan were progressing, and "storable" liquids were being developed that would allow the missiles to be left in a ready-to-shoot form for extended periods. But Hall saw solid fuels not only as a way to improve launch times or safety, but part of a radical plan to greatly reduce the cost of ICBMs so that thousands could be built. He was aware that new computerized assembly lines would allow continual production, and that similar equipment would allow a small team to oversee operations for dozens or hundreds of missiles. A solid fuel design would be much simpler to build, and easier to maintain in service.
His ultimate plan was to build a number of integrated missile "farms" that included factories, missile silos, transport and even recycling. Each farm would support between 1,000 and 1,500 missiles being produced in a continual low rate cycle. Systems in the missiles would detect failures, at which point it would be removed and recycled, while a newly built missile was put into the silo. The missile design itself was based purely on lowest possible cost, reducing its size and complexity because "the basis of the weapon's merit was its low cost per completed mission; all other factors - accuracy, vulnerability and reliability - were secondary."
Hall's plan did not go unopposed, especially by the more established names in the ICBM field. Ramo-Wooldridge pressed for a system with higher accuracy, but Hall countered that the missile's role was to attack Soviet cities, and that "a force which provides numerical superiority over the enemy will provide a much stronger deterrent than a numerically inferior force of greater accuracy." Hall was known for his "friction with others" and in 1958 Schriever removed him from the Minuteman project and sent him to the UK to oversee deployment of the Thor ICBM. On his return to the US in 1959, Hall retired from the Air Force, but received his second Legion of Merit in 1960 for his work on solid fuels.
Although he was removed from the Minuteman project, Hall's work on cost reduction had already produced a new design of diameter, much smaller than the Atlas and Titan at , which would mean much smaller and cheaper silos. Hall's goal of dramatic cost reduction was a success, although many of the other concepts of his missile farm were abandoned.
Guidance system.
Previous long-range missiles were liquid fueled and required considerable time, 30 minutes to an hour or more, to be fueled. During this time other crewmembers would be spinning up the inertial guidance system, setting its initial position, and programming in the target coordinates. This normally took about as long as the fueling process, so it was not considered a problem that needed to be solved. Minuteman was designed from the outset to be launched in minutes. While the use of solid fuel eliminated the delays fueling up, it did nothing for the delays in erecting and aligning the guidance system. For quick launch, the guidance system would have to be kept running and aligned at all times, a serious problem for the mechanical systems of the era, especially the gyroscopes which used ball bearings.
After considerable deliberation, a design by Autonetics using air bearings was selected, after they pointed out that their experimental set had been running continually from 1952 to 1957. Autonetics further advanced the state of the art by building their bearing not in the form of a single spindle but a ball. This allowed the gyros to precess in two directions instead of along a single axis, meaning that only two gyros instead of three would be needed for the inertial platform.
The last major advance in the Minuteman development was the decision to use a general purpose digital computer in place of the analog or custom designed digital computers of earlier missile designs. This was not chosen to improve the guidance accuracy "per se", but a side effect of wishing to reduce the total number of parts in the missile. Previous missile designs had an autopilot that kept the missile flying in a straight line, and a separate guidance system that provided inputs to the autopilot to adjust its trajectory. Using a single more powerful computer would eliminate the need for two separate units.
Since the guidance computer would otherwise be doing nothing while the missile sat in the silo, using a general purpose computer and simply running a different program on it allowed it to handle the monitoring of the various sensors and test equipment. With older designs this had been handled externally, requiring miles of extra wiring and many connectors. In order to store multiple programs, the computer was built in the form of a drum machine but used a hard disk in place of the drum.
Building a computer with the required performance, size and weight demanded the use of transistors, which were at that time very expensive and not very reliable. Earlier efforts to use transistorized computers for guidance, BINAC and the system on the SM-64 Navaho, had failed to work and were abandoned. The Air Force and Autonetics spent millions on a program to improve transistor and component reliability 100 times, leading to the "Minuteman high-rel parts" specifications. The techniques developed during this program were equally useful for improving all transistor construction, and greatly reduced the failure rate of transistor production lines in general. This improved yield, which had the effect of greatly lowering production costs, and had enormous spin-off effects in the electronics industry.
The use of a general purpose computer would have long-lasting effects on the Minuteman program, and the US's nuclear stance in general. Earlier ICBMs using custom wired computers were capable of attacking a single target, because the precise trajectory information was hard coded directly in the system's logic. With Minuteman, the targeting could be easily changed by loading new trajectory information into the computer's memory, a task that could be completed in a few hours.
Missile gap.
In 1957 a series of intelligence reports suggested the Soviets were far ahead in the missile race and would be able to overwhelm the US by the early 1960s. It was later demonstrated that this "missile gap" was just as fictional as the "bomber gap" of a few years earlier, but through the late 1950s it was a serious concern. If the Soviets were building missiles in the numbers being predicted by the CIA and others within the defense establishment, by as early as 1961 they would have enough to attack all SAC and ICBM bases in the US in a single strike. The Air Force, concerned about the survivability of its striking force in the short term, began the WS-199 program, initially to develop a highly survivable strategic missile, and in the meantime, pushed Minuteman for crash development starting in September 1958.
Advanced surveying of the potential silo sites had already begun in late 1957. Fears of a Soviet anti-ballistic missile system, which was known to be under development at Sary Shagan, led to calls for the adoption of a maneuvering reentry vehicle (MARV). These greatly complicate the problem of shooting down a warhead, and a number of designs were tested under WS-199, notably the Alpha Draco and Boost Glide Reentry Vehicle programs of 1957. These used long and skinny arrow-like shapes that provided aerodynamic lift in the high atmosphere, and could be fitted to existing missiles like Minuteman. However, their shape required more room on the front of the missile than a traditional RV. To address this, the Minuteman silos were revised to be built deeper. Although Minuteman would not deploy a boost-glide warhead, the extra space proved invaluable in the future as it allowed the missile to be extended and carry more fuel and payload.
The Puzzle of Polaris.
During Minuteman's early development, the Air Force maintained the policy that the manned strategic bomber was the primary weapon of nuclear war. Blind bombing accuracy on the order of was expected, and the weapons were sized to ensure even the hardest targets would be destroyed as long as the weapon fell within this range. The USAF had enough bombers to attack every military and industrial target in the USSR and was confident that its bombers would survive in great enough numbers that such a strike would utterly destroy the country.
Soviet ICBMs upset this equation to a degree. Their accuracy was known to be low, on the order of , but they carried large warheads that would be useful against Strategic Air Command's bombers, which parked in the open. Since there was no system to detect the ICBMs being launched, the possibility was raised that the Soviets could launch a sneak attack with a few dozen missiles that would take out a significant portion of SACs bomber fleet. In this environment, the Air Force saw their own ICBMs not as a primary weapon of war, but as a way to ensure that the Soviets would not risk a sneak attack. ICBMs, especially newer models that were housed in silos, could be expected to survive an attack by a single Soviet missile. In any conceivable scenario where both sides had similar numbers of ICBMs, the US force would survive a sneak attack in sufficient numbers to ensure destruction of all major Soviet cities in return. In such an environment, the Soviets would not risk an attack.
Considering this "countervalue" attack concept, strategic planners calculated that an attack of "400 equivalent megatons" aimed at the largest Soviet cities would promptly kill 30% of their population and destroy 50% of their industry. Larger attacks raised these numbers only slightly. This suggested that there was a "finite deterrent" level around 400 megatons that would be enough to prevent a Soviet attack no matter how many missiles they had of their own. All that had to be ensured was that the US missiles survived, which seemed likely given the low accuracy of the Soviet weapons. Reversing the problem, the addition of ICBMs to the US force did not eliminate the need, or desire, to attack Soviet military targets, and the Air Force maintained that bombers were the only suitable platform in that role.
This nevertheless presented a serious problem for the Air Force. While still pressing for development of newer bombers, at that time represented by the supersonic B-70, it appeared the countervalue role was served perfectly well by the Navy's Polaris. Polaris had enough range that the submarines could roam open areas of the ocean, and would be essentially invulnerable to attack no matter how many missiles the Soviets had, or how accurate they were. Based on the same 400 equivalent megatons calculation, they set about building a fleet of 41 submarines carrying 16 missiles each, giving the Navy a finite deterrent that was unassailable.
A February 1960 memo by RAND, entitled "The Puzzle of Polaris", was passed around among high-ranking Air Force officials. It suggested that Polaris negated any need for Air Force ICBMs if they were also being aimed at Soviet cities. This would have long-lasting effects on the future of the Minuteman program, which, by 1961, was firmly evolving towards a counterforce capability.
Kennedy and Minuteman.
Minuteman was entering final testing just as John F. Kennedy was entering the White House. His new Secretary of Defense, Robert McNamara, was tasked with the seemingly impossible mission of producing the world's best defense while at the same time limiting spending. McNamara began to apply cost/benefit analysis to the problem, and Minuteman's low production cost made its selection as the basis for a US buildout a foregone conclusion. Atlas and Titan were soon scrapped, and the storable liquid fueled Titan II deployment was severely curtailed. Perhaps equally foregone, McNamara also cancelled the B-70 bomber project.
Minuteman's low cost also had spin-off effects on non-ICBM programs. Another way to prevent a sneak attack was provided by the Army's Nike Zeus, an interceptor missile that was capable of shooting down the Soviet warheads. This had initially been proposed as a way to defend the SAC bomber fleet. The Army argued that upgraded Soviet missiles might be able to attack US missiles in their silos, and Zeus would be able to blunt such an attack. Zeus was expensive, however, and the Air Force pointed out that it was more cost-effective to build another Minuteman missile than the Zeus system needed to protect it. Given the large size and complexity of the Soviet liquid-fueled missiles, an ICBM building race was one the Soviets could not afford. Zeus was cancelled in 1963.
Minuteman and counterforce.
Minuteman's selection as the primary Air Force ICBM was initially based on the same logic as their earlier missiles, that the weapon was primarily one designed to ride out any potential Soviet attack and ensure they would be hit in return. But Minuteman had a combination of features that led to its rapid evolution into the US's primary weapon of nuclear war.
Primary among these qualities was its digital computer. This could be updated in the field with new targets and better information about the flight paths with relative ease, gaining accuracy for little cost. One of the unavoidable effects on the warhead's trajectory was the mass of the Earth, which is not even, and contains many mass concentrations that pull on the warhead. Through the 1960s, the Defense Mapping Agency (now part of National Geospatial-Intelligence Agency) mapped these with increasing accuracy, feeding that information back into the Minuteman fleet. The Minuteman was deployed with a circular error probable (CEP) of about , but this had improved to about by 1965. This was accomplished without any mechanical changes to the missile or its navigation system.
At those levels, the ICBM begins to approach the manned bomber in terms of accuracy; a small upgrade, roughly doubling the accuracy of the INS, would give it the same CEP as the manned bomber. Autonetics began such development even before the original Minuteman entered fleet service, and the Minuteman-II had a CEP of . Additionally, the computers were upgraded with more memory, allowing them to store information for eight targets, which the missile crews could select among almost instantly, greatly increasing their flexibility. From that point, Minuteman became the US's primary deterrent weapon, until its performance was matched by the Navy's Trident missile of the 1980s.
Questions about the need for the manned bomber were quickly raised. The Air Force began to offer a number of reasons why the bomber offered value, in spite of costing more money to buy and being much more expensive to operate and maintain. Newer bombers with better survivability, like the B-70, cost many times that of the Minuteman, and in spite of great efforts through the 1960s, it became increasingly vulnerable. The B-1 of the early 1970s eventually emerged with a price tag around $200 million ($ million today) while the Minuteman-III's built during the 1970s cost only $7 million ($ million today).
The Air Force countered that having a variety of platforms complicated the defense; if the Soviets built an effective anti-ballistic missile system of some sort, the ICBM and SLBM fleet might be rendered useless, while the bombers would remain. This became the nuclear triad concept, which survives into the 2000s. Although this argument was successful, the numbers of manned bombers has been repeatedly cut and the deterrent role increasingly passed to missiles.
Minuteman-I (LGM-30A/B or SM-80/HSM-80A).
Deployment.
The LGM-30A Minuteman-I was first test-fired on 1 February 1961, and entered into the Strategic Air Command's arsenal in 1962, at Malmstrom Air Force Base, Montana; the "improved" LGM-30B became operational at Ellsworth Air Force Base, South Dakota, Minot Air Force Base, North Dakota, F.E. Warren Air Force Base, Wyoming, and Whiteman Air Force Base, Missouri in 1963. All 800 Minuteman-I missiles were delivered by June 1965. Each of the bases had 150 missiles emplaced. F.E. Warren AFB had 200 of the Minuteman-IB missiles. Malmstrom AFB had 150 of the Minuteman-I and about five years later added 50 of the Minuteman-II similar to those installed at Grand Forks AFB, ND.
Guidance.
The Minuteman-I Autonetics D-17 flight computer used a rotating air bearing magnetic disk holding 2,560 "cold-stored" words in 20 tracks (write heads disabled after program fill) of 24 bits each and one alterable track of 128 words. The time for a D-17 disk revolution was 10 ms. The D-17 also used a number of short loops for faster access of intermediate results storage. The D-17 computational minor cycle was three disk revolutions or 30 ms. During that time all recurring computations were performed. For ground operations the inertial platform was aligned and gyro correction rates updated. During flight, filtered command outputs were sent by each minor cycle to the engine nozzles. Unlike modern computers, which use descendants of that technology for secondary storage on hard disk, the disk was the active computer memory. The disk storage was considered hardened to radiation from nearby nuclear explosions, making it an ideal storage medium. To improve computational speed, the D-17 borrowed an instruction look-ahead feature from the Autonetics-built Field Artillery Data Computer (M18 FADAC) that permitted simple instruction execution every word time.
The D-17B and the D-37C guidance and control computers were integral components of the Minuteman-I and Minuteman-II missiles, respectively, which formed a part of the United States ICBM arsenal. The Minuteman-III missiles, which use D-37D computers, complete the 1000 missile deployment of this system. The initial cost of these computers ranged from about $139,000 (D-37C) to $250,000 (D-17B).
Minuteman-II (LGM-30F).
The LGM-30F Minuteman-II was an improved version of the Minuteman-I missile. Development on the Minuteman-II began in 1962 as the Minuteman-I entered the Strategic Air Command's nuclear force. Minuteman-II production and deployment began in 1965 and completed in 1967. It had an increased range, greater throw weight and guidance system with better azimuthal coverage, providing military planners with better accuracy and a wider range of targets. Some missiles also carried penetration aids, allowing higher probability of kill against Moscow's anti-ballistic missile system. The payload consisted of a single Mk-11C reentry vehicle containing a W56 nuclear warhead with a yield of 1.2 megatons of TNT (5 PJ).
The major new features provided by Minuteman-II were:
System modernization was concentrated on launch facilities and command and control facilities. This provided decreased reaction time and increased survivability when under nuclear attack. Final changes to the system were performed to increase compatibility with the expected LGM-118A Peacekeeper. These newer missiles were later deployed into modified Minuteman silos.
The Minuteman-II program was the first mass-produced system to use a computer constructed from integrated circuits (the Autonetics D-37C). The Minuteman-II integrated circuits were diode-transistor logic and diode logic made by Texas Instruments. The other major customer of early integrated circuits was the Apollo Guidance Computer, which had similar weight and ruggedness constraints. The Apollo integrated circuits were resistor-transistor logic made by Fairchild Semiconductor. The Minuteman-II flight computer continued to use rotating magnetic disks for primary storage. The Minuteman-II included diodes by Microsemiconductor.
Minuteman-III (LGM-30G): the current model.
The LGM-30G Minuteman-III program started in 1966, and included several improvements over the previous versions. It was first deployed in 1970. Most modifications related to the final stage and reentry system (RS). The final (third) stage was improved with a new fluid-injected motor, giving finer control than the previous four-nozzle system.
Performance improvements realized in Minuteman-III include increased flexibility in reentry vehicle (RV) and penetration aids deployment, increased survivability after a nuclear attack, and increased payload capacity. The missile retains a gimballed inertial guidance system.
Minuteman-III originally contained the following distinguishing features:
[[File:Minuteman III MIRV path.svg|thumb|Minuteman-III MIRV launch sequence:<br>
1. The missile launches out of its silo by firing its 1st-stage boost motor ("A").<br>
2. About 60 seconds after launch, the 1st stage drops off and the 2nd-stage motor ("B") ignites. The missile shroud ("E") is ejected.<br>
3. About 120 seconds after launch, the 3rd-stage motor ("C") ignites and separates from the 2nd stage.<br>
4. About 180 seconds after launch, 3rd-stage thrust terminates and the Post-Boost Vehicle ("D") separates from the rocket.<br>
5. The Post-Boost Vehicle maneuvers itself and prepares for re-entry vehicle (RV) deployment.<br>
6. The RVs, as well as decoys and chaff, are deployed during backaway.<br>
7. The RVs and chaff re-enter the atmosphere at high speeds and are armed in flight.<br>
8. The nuclear warheads initiate, either as air bursts or ground bursts.]]
The existing Minuteman-III missiles have been further improved over the decades in service, with more than $7 billion spent in the last decade to upgrade the 450 missiles.
Guidance Replacement Program (GRP).
The Guidance Replacement Program (GRP) replaces the NS20A Missile Guidance Set with the NS50A Missile Guidance Set. The newer system extends the service life of the Minuteman missile beyond the year 2030 by replacing aging parts and assemblies with current, high reliability technology while maintaining the current accuracy performance. The replacement program was completed 25 February 2008.
Propulsion Replacement Program (PRP).
Beginning in 1998 and continuing through 2009, the Propulsion Replacement Program extends the life and maintains the performance by replacing the old solid propellant boosters (downstages).
Single Reentry Vehicle (SRV).
The Single Reentry Vehicle (SRV) modification enabled the United States ICBM force to abide by the now-vacated START II treaty requirements by reconfiguring Minuteman-III missiles from three reentry vehicles down to one. Though it was eventually ratified by both parties, START II never entered into force and was essentially superseded by follow-on agreements such as SORT and New START, which do not limit MIRV capability.
Safety Enhanced Reentry Vehicle (SERV).
Beginning in 2005, Mk-21/W87 RVs from the deactivated Peacekeeper missile will be placed on the Minuteman-III force under the Safety Enhanced Reentry Vehicle (SERV) program. The older W78 does not have many of the safety features of the newer W87, such as insensitive high explosive, as well as more advanced safety devices. In addition to implementing these safety features in at least a portion of the future Minuteman-III force, the decision to transfer W87s onto the missile is based on two features that will improve the targeting capabilities of the weapon: more fuzing options which will allow for greater targeting flexibility and the most accurate reentry vehicle available which provides a greater probability of damage to the designated targets.
Current and future deployment.
The Minuteman-III missile entered service in 1970, with weapon systems upgrades included during the production run from 1970 to 1978 to increase accuracy and payload capacity. , the USAF plans to operate it until 2030.
The LGM-118A Peacekeeper (MX) ICBM, which was to have replaced the Minuteman, was retired in 2005 as part of START II.
A total of 450 LGM-30G missiles are emplaced at F.E. Warren Air Force Base, Wyoming (90th Missile Wing), Minot Air Force Base, North Dakota (91st Missile Wing), and Malmstrom Air Force Base, Montana (341st Missile Wing). All Minuteman-I and Minuteman-II missiles have been retired. The United States prefers to keep its MIRV deterrents on submarine-launched Trident Nuclear Missiles. Fifty of these will be put into "warm" unarmed status, taking up half of the 100 slots in America's allowable nuclear reserve.
Testing.
Minuteman-III missiles are regularly tested with launches from Vandenberg Air Force Base in order to validate the effectiveness, readiness, and accuracy of the weapon system, as well as to support the system's primary purpose, nuclear deterrence. The safety features installed on the Minuteman-III for each test launch allow the flight controllers to terminate the flight at any time if the systems indicate that its course may take it unsafely over inhabited areas. Since these flights are for test purposes only, even terminated flights can send back valuable information to correct a potential problem with the system.
The 576th Flight Test Squadron is responsible for planning, preparing, conducting, and assessing all ICBM ground and flight tests.
Advanced Maneuverable Reentry Vehicle.
When defending hardened targets, it is possible for a defensive ABM system to accurately track incoming warheads and choose to ignore those that will fall outside the lethal range of the target. This can, depending on the accuracy of the warheads, greatly reduce the number of defensive missiles that have to be fired in response to an attack. The simplest way to counter this possibility is to make a reentry vehicle that can maneuver, approaching its target along a trajectory that looks like it is going to miss, and then correcting at the last possible moment, leaving too little time for the defensive missile to launch. This concept is known as a maneuverable reentry vehicle, or MARV.
The "Advanced Maneuverable Reentry Vehicle" (AMaRV) was a prototype MARV built by McDonnell-Douglas Corp. Four AMaRVs were made and represented a significant leap in Reentry Vehicle sophistication. Three of the AMaRVs were launched by surplus Minuteman-1s on 20 December 1979, 8 October 1980 and 4 October 1981. AMaRV had an entry mass of approximately 470 kg, a nose radius of 2.34 cm, a forward frustum half-angle of 10.4°, an inter-frustum radius of 14.6 cm, aft frustum half angle of 6°, and an axial length of 2.079 meters. No accurate diagram or picture of AMaRV has ever appeared in the open literature. However, a schematic sketch of an AMaRV-like vehicle along with trajectory plots showing hairpin turns has been published.
AMaRV's attitude was controlled through a split body flap (also called a "split-windward flap") along with two yaw flaps mounted on the vehicle's sides. Hydraulic actuation was used for controlling the flaps. AMaRV was guided by a fully autonomous navigation system designed for evading anti-ballistic missile (ABM) interception.
Influences.
The Minuteman Missile National Historic Site in South Dakota preserves a Launch Control Facility (D-01) and a launch facility (D-09) under the control of the National Park Service. The North Dakota State Historical Society maintains the Ronald Reagan Minuteman Missile Site, preserving a Missile Alert Facility, Launch Control Center and Launch Facility in the WS-133B "Deuce" configuration, near Cooperstown, North Dakota.
Appearances in media.
Footage of Minuteman-III ICBM test launches have been featured in several theatrical films and television movies where missile launch footage is needed. The Department of Defense film released for use was mainly drawn from Vandenberg Air Force Base test shots in 1966, including from a "salvo launch" (more than one ICBM launched simultaneously).
Theatrically released films using the footage include (most notably), the 1978 film "Superman" (which features the "twin shot"), and more extensively, the 1977 nuclear war film "Damnation Alley." The made for TV film "The Day After" also features the same footage, although the first stage of flight is completed via special effects. "Terminator 3" uses computer generated images of Minuteman missiles launching from the Plains on "Judgment Day". Minutemen also feature in "Eagle Strike", by Anthony Horowitz, in which fictional power-crazed multimillionaire Damian Cray orders their release from Air Force One. In the film "WarGames" a failed Minuteman launch simulation exercise caused by a conflicted launch control officer is the impetus for the conversion of the missiles to full automatic control by the computer system that Matthew Broderick's character later hacks into.
Other roles.
Mobile Minuteman.
Mobile Minuteman was a program for rail-based ICBMs to help increase survivability and for which the USAF released details on 12 October 1959. The Operation Big Star performance test was from 20 June to 27 August 1960 at Hill Air Force Base, and the 4062nd Strategic Missile Wing (Mobile) was organized 1 December 1960 for 3 planned missile train squadrons, each with 10 trains carrying 3 missiles per train. During the Kennedy/McNamara cutbacks, the DoD announced "that it has abandoned the plan for a mobile Minuteman ICBM. The concept called for 600 to be placed in service—450 in silos and 150 on special trains, each train carrying 5 missiles." After Kennedy announced on 18 March 1961, that the 3 squadrons were to be replaced with "fixed-base squadrons", Strategic Air Command discontinued the 4062nd Strategic Missile Wing on 20 February 1962.
Air Launched ICBM.
Air Launched ICBM was a STRAT-X proposal in which SAMSO successfully conducted an Air Mobile Feasibility Test that airdropped a Minuteman 1b from a C-5A Galaxy aircraft from over the Pacific Ocean. The missile fired at , and the 10-second engine burn carried the missile to 20,000 feet again before it dropped into the ocean. Operational deployment was discarded due to engineering and security difficulties, and the capability was a negotiating point in the Strategic Arms Limitation Talks.
Emergency Rocket Communications System (ERCS).
An additional part of the National Command Authority communication relay system was called the Emergency Rocket Communication System (ERCS). Specially designed rockets called BLUE SCOUT carried radio-transmitting payloads high above the continental United States, to relay messages to units within line-of-sight. In the event of a nuclear attack, ERCS payloads would relay pre-programmed messages giving the "go-order" to SAC units. BLUE SCOUT launch sites were located at Wisner, West Point and Tekamah, Nebraska. These locations were vital for ERCS effectiveness due to their centralized position in the US, within range of all missile complexes. Later ERCS configurations were placed on the top of modified Minuteman-II ICBMs (LGM-30Fs) under the control of the 510th Strategic Missile Squadron located at Whiteman Air Force Base, Missouri.
The Minuteman ERCS may have been assigned the designation LEM-70A.
Satellite launching role.
The U.S. Air Force has considered using some decommissioned Minuteman missiles in a satellite launching role. These missiles would be stored in silos, for launch upon short notice. The payload would be variable, and would have the ability to be replaced quickly. This would allow a surge capability in times of emergency.
During the 1980s, surplus Minuteman missiles were used to power the Conestoga rocket produced by Space Services Inc. of America. It was the first privately developed rocket, but only saw three flights and was discontinued due to a lack of business. More recently, converted Minuteman missiles have been used to power the Minotaur line of rockets produced by Orbital Sciences.
Ground and air launch targets.
L-3 Communications is currently using SR-19 SRBs, Minuteman-II Second Stage Solid Rocket Boosters, as delivery vehicles for a range of different re-entry vehicles as targets for the THAAD and ASIP interceptor missile programs as well as radar testing.
Operator.
Operational units.
The basic tactical unit of a Minuteman wing is the squadron, consisting of five flights. Each flight consists of ten unmanned launch facilities (LFs) which are remotely controlled by a manned launch control center (LCC). The five flights are interconnected and status from any LF may be monitored by any of the five LCCs. Each LF is located at least three nautical miles (5.6 km) from any LCC. Control does not extend outside the squadron (thus the 319th Missile Squadron's five LCCs cannot control the 320th Missile Squadron's 50 LFs even though they are part of the same Space Launch Wing). Each Minuteman wing is assisted logistically by a nearby Missile Support Base (MSB).

</doc>
<doc id="49367" url="https://en.wikipedia.org/wiki?curid=49367" title="Laurent-Désiré Kabila">
Laurent-Désiré Kabila

Laurent-Désiré Kabila () (November 27, 1939 – January 16, 2001), or simply Laurent Kabila, was President of the Democratic Republic of the Congo from May 17, 1997, when he overthrew Mobutu Sese Seko, until his assassination by one of his bodyguards on January 16, 2001. He was succeeded eight days later by his son Joseph.
Early life.
Kabila was born to a member of the Luba tribe in Baudoinville, Katangu Province, (now Moba, Tanganyika District) in the Belgian Congo. His father was a Luba and his mother was a Lunda. He studied political philosophy in France, and in Yugoslavia at the University of Belgrade. Later he attended the University of Dar es Salaam in Tanzania.
Political activities.
Congo crisis.
When the Congo gained independence from Belgium on June 30, 1960 and the Congo crisis began, Kabila had a role as a "deputy commander" in the Jeunesses Balubakat, the youth wing of the Patrice Lumumba-aligned General Association of the Baluba People of Katanga (Balubakat), actively fighting the secessionist forces of Moise Tshombe. Within months, Joseph Mobutu overthrew Lumumba, and in 1962 Kabila was appointed to the provincial assembly for North Katanga and was chief of cabinet for Minister of Information Ferdinand Tumba.
Kabila established himself as a supporter of hard-line Lumumbist Prosper Mwamba Ilunga. When the Lumumbists formed the Conseil National de Libération, he was sent to eastern Congo to help organize a revolution, in particular in the Kivu and North Katanga provinces. In 1965, Kabila set up a cross-border rebel operation from Kigoma, Tanzania, across Lake Tanganyika.
Che Guevara.
Che Guevara assisted Kabila for a short time in 1965. Guevara had appeared in the Congo with approximately 100 men who planned to bring about a Cuban-style revolution. Guevara judged Kabila (then 26) as "not the man of the hour" he had alluded to, being too distracted. This, in Guevara's opinion, accounted for Kabila showing up days late at times to provide supplies, aid, or backup to Guevara's men. The lack of cooperation between Kabila and Guevara contributed to the suppression of the revolt that same year.
In Guevara's view, of all of the people he met during his campaign in Congo, only Kabila had "genuine qualities of a mass leader"; but Guevara castigated Kabila for a lack of "revolutionary seriousness". After the failure of the rebellion, Kabila turned to smuggling gold and timber on Lake Tanganyika. He also ran a bar and brothel in Tanzania.
Marxist mini-state (1967–1988).
In 1967, Kabila and his remnant of supporters moved their operation into the mountainous Fizi – Baraka area of South Kivu in the Congo, and founded the People's Revolutionary Party (PRP). With the support of the People's Republic of China, the PRP created a secessionist Marxist state in South Kivu province, west of Lake Tanganyika.
The PRP state came to an end in 1988 and Kabila disappeared and was widely believed to be dead. While in Kampala, Kabila reportedly met Yoweri Museveni, the future president of Uganda. Museveni and former Tanzanian President Julius Nyerere later introduced Kabila to Paul Kagame, who would become president of Rwanda. These personal contacts became vital in mid-1990s, when Uganda and Rwanda sought a Congolese face for their intervention in Zaire.
First Congo War.
Kabila returned in October 1996, leading ethnic Tutsis from South Kivu against Hutu forces, marking the beginning of the First Congo War. With support from Uganda, Rwanda, and Burundi, Kabila pushed his forces into a full-scale rebellion against Mobutu as the Alliance of Democratic Forces for the Liberation of Congo-Zaire (ADFL). He used children in the conflict and it was estimated that up to 10 000 children served under him.
By mid-1997, the ADFL had almost completely overrun the country and the remains of Mobutu's army. Only the country's decrepit infrastructure slowed Kabila's forces down; in many areas, the only means of transit were irregularly used dirt paths. Following failed peace talks held on board the South African ship SAS "Outeniqua", Mobutu fled into exile on 16 May.
The next day, from his base in Lubumbashi, Kabila proclaimed himself president. Kabila suspended the Constitution, and changed the name of the country from Zaire to the Democratic Republic of the Congo—the country's official name from 1964 to 1971. He made his grand entrance into Kinshasa on 20 May and was sworn in on 31 May, officially commencing his term as president.
Presidency (1997–2001).
Kabila had been a committed Marxist, but his policies at this point were a mix of capitalism and collectivism. He declared that elections would not be held for two years, since it would take him at least that long to restore order. While some in the West hailed Kabila as representing a "new breed" of African leadership, critics charged that Kabila's policies differed little from his predecessor's, being characterised by authoritarianism, corruption, and human rights abuses. As early as late 1997, Kabila was being denounced as "another Mobutu."
Kabila was also accused of self-aggrandizing tendencies, including trying to set up a personality cult, with the help of Mobutu's former minister of information, Dominique Sakombi Inongo. Sakombi Inongo branded Kabila as "the Mzee," and posters reading "Here is the man we needed" () appeared all over the country.
By 1998, Kabila's former allies in Uganda and Rwanda had turned against him and backed a new rebellion of the Rally for Congolese Democracy (RCD), the Second Congo War. Kabila found new allies in Angola, Namibia, and Zimbabwe, and managed to hold on in the south and west of the country and by July 1999, peace talks led to the withdrawal of most foreign forces.
Assassination.
In the afternoon of January 16, 2001 Kabila was shot by a child soldier from his bodyguard, Rashidi Muzele, who was killed as he attempted to flee the scene at Kabila's "Palais de Marbre" residence. According to a Rwandan former intelligence chief and allegations made by Democratic Republic of Congo's officials, his assassination was committed by some of his bodyguards and masterminded by Rwanda. According to the documentary film "Murder in Kinshasa", made by Marlène Rabaud and Arnaud Zajtman, a Lebanese diamond dealer allegedly organized the logistics of the assassination. 
Eleven Lebanese nationals were executed in the evening of the assassination as part of a punitive campaign by the DRC's authorities who managed to keep power, despite the assassination of their President. The exact circumstances are still disputed. Kabila reportedly died on the spot, according to DRC's then health minister Dr Mashako Mamba, who was in the next door office when Kabila was shot and arrived immediately after the assassination. The government claimed that Kabila was still alive, however, when he was flown to a hospital in Zimbabwe after he was shot so that DRC authorities could organize the tense succession.
The Congolese government announced that he had died of his wounds on January 18. One week later, his body was returned to Congo for a state funeral and his son, Joseph Kabila, became president eight days later. By doing so, DRC officials were accomplishing the "verbal testimony" of the deceased President. Then Justice Minister Mwenze Kongolo and Laurent-Désiré Kabila's aide de camp Eddy Kapend have reported that Laurent Kabila had told them that his son Joseph, then number two of the army, should take over, if Laurent-Désiré Kabila was to pass away.
Aftermath.
The investigation into Kabila's assassination led to 135 people – including 4 children – being tried before a special military tribunal. The alleged ringleader, Colonel Eddy Kapend (one of Kabila's cousins), and 25 others were sentenced to death in January 2003, but not executed. Of the other defendants 64 were jailed, with sentences from six months to life, and 45 were exonerated. Some individuals were also accused of being involved in a plot to overthrow his son. Among them was Kabila's special advisor Emmanuel Dungia, former ambassador to South Africa. Many people believe the trial was flawed and the convicted defendants are innocent.

</doc>
<doc id="49369" url="https://en.wikipedia.org/wiki?curid=49369" title="1928 Winter Olympics">
1928 Winter Olympics

The 1928 Winter Olympics, officially known as the II Olympic Winter Games (French: Les "IIes Jeux olympiques d'hiver") (German: "Olympische Winterspiele 1928") (Italian: "II Giochi olimpici invernali") (Romansch: "Gieus olimpics d'enviern 1928"), were a winter multi-sport event which was celebrated February 11–19, 1928 in St. Moritz, Switzerland. The 1928 Games were the first true "Winter Olympics" held on its own as they were not in conjunction with a "Summer Olympics". The preceding 1924 Games were retroactively renamed the inaugural Winter Olympics, though they had been in fact part of the 1924 Summer Olympics. All preceding Winter Events of the Olympic Games were the winter sports part of the schedule of the Summer Games, and not held as a separate Winter Games. These games also replaced the now redundant Nordic Games, that were held quadrennially since early in the century.
Fluctuating weather conditions made these Olympics memorable. The opening ceremony was held in a blizzard. In contrast, warm weather conditions plagued the Olympics for the remainder of the Games, requiring cancellations of one event with temperatures as high as 25 °C (77 °F). (See further description at the Wikipedia main article on Winter Olympic Games.)
Events.
Medals were awarded in 14 events contested in 4 sports (8 disciplines).
Participating nations.
Athletes from 25 nations competed at these Games, up from 16 in 1924. Nations making their first appearance at the Winter Olympic Games were Argentina (first participation of a delegation coming from a country belonging to the Southern Hemisphere), Estonia, Germany, Japan, Lithuania, Luxembourg, Mexico, the Netherlands, and Romania.

</doc>
<doc id="49370" url="https://en.wikipedia.org/wiki?curid=49370" title="Anthony Burgess">
Anthony Burgess

John Anthony Burgess Wilson, (; 25 February 1917 – 22 November 1993) – who published under the pen name Anthony Burgess – was an English writer and composer. From relatively modest beginnings in a Catholic family in Manchester, he eventually became one of the best known English literary figures of the latter half of the twentieth century.
Although Burgess was predominantly a comic writer, his dystopian satire "A Clockwork Orange" remains his best known novel. In 1971 it was adapted into a highly controversial film by Stanley Kubrick, which Burgess said was chiefly responsible for the popularity of the book. Burgess produced numerous other novels, including the Enderby quartet, and "Earthly Powers", regarded by most critics as his greatest novel. He wrote librettos and screenplays, including for the 1977 TV mini-series "Jesus of Nazareth". He worked as a literary critic, including for "The Observer" and "The Guardian", and wrote studies of classic writers, notably James Joyce. A versatile linguist, Burgess lectured in phonetics, and translated "Cyrano de Bergerac", "Oedipus the King" and the opera "Carmen", among others.
Burgess also composed over 250 musical works; he sometimes claimed to consider himself as much a composer as an author, although he enjoyed considerably more success in writing.
Biography.
Early life.
Burgess was born at 91 Carisbrook Street in Harpurhey, a suburb of Manchester, to Catholic parents (his mother was a convert), Joseph and Elizabeth Wilson. He described his background as lower middle class; growing up during the Great Depression, the Wilsons were fairly well off, as the demand for their tobacco and alcohol wares remained constant . He was known in childhood as Jack, Little Jack, and Johnny Eagle. At his confirmation, the name Anthony was added and he became John Anthony Burgess Wilson. He began using the pen name Anthony Burgess upon the publication of his 1956 novel "Time for a Tiger".
His mother Elizabeth (née Burgess) died at the age of 30 at home on 19 November 1918, during the 1918 flu pandemic. The causes listed on her death certificate were influenza, acute pneumonia, and cardiac failure. His sister Muriel had died four days earlier on 15 November from influenza, broncho-pneumonia, and cardiac failure, aged eight. Burgess believed he was resented by his father, Joseph Wilson, for having survived, when his mother and sister did not.
After the death of his mother, Burgess was raised by his maternal aunt, Ann Bromley, in Crumpsall with her two daughters. During this time, Burgess's father worked as a bookkeeper for a beef market by day, and in the evening played piano at a public house in Miles Platting. After his father married the landlady of this pub, Margaret Dwyer, in 1922, Burgess was raised by his father and stepmother. By 1924 the couple had established a tobacconist and off-licence business with four properties. On 18 April 1938, Joseph Wilson died from cardiac failure, pleurisy, and influenza at the age of 55, leaving no inheritance despite his apparent business success. Burgess' stepmother died of a heart attack in 1940.
Burgess has said of his largely solitary childhood: "I was either distractedly persecuted or ignored. I was one despised ... Ragged boys in gangs would pounce on the well-dressed like myself." He attended St. Edmund's Elementary School before moving on to Bishop Bilsborrow Memorial Elementary School, both Catholic schools, in Moss Side. He later reflected: "When I went to school I was able to read. At the Manchester elementary school I attended, most of the children could not read, so I was ... a little apart, rather different from the rest." Good grades resulted in a place at Xaverian College (1928–1937). As a young child he did not care about music, until he heard on his home-built radio "a quite incredible flute solo", which he characterised as "sinuous, exotic, erotic," and became spellbound. Eight minutes later the announcer told him he had been listening to "Prélude à l'après-midi d'un faune" by Claude Debussy. He referred to this as a "psychedelic moment ... a recognition of verbally inexpressible spiritual realities." When Burgess announced to his family that he wanted to be a composer, they objected as "there was no money in it." Music was not taught at his school, but at about age 14 he taught himself to play the piano. Burgess had originally hoped to study music at university, but the music department at the Victoria University of Manchester turned down his application because of poor grades in physics. So instead he studied English language and literature there between 1937 and 1940, graduating with a Bachelor of Arts. His thesis concerned Marlowe's "Doctor Faustus", and he graduated with an upper second-class honours, which he found disappointing. When grading one of Burgess's term papers, the historian A.J.P. Taylor, wrote: "Bright ideas insufficient to conceal lack of knowledge."
Burgess met Llewela "Lynne" Isherwood Jones at the University where she was studying economics, politics and modern history, graduating in 1942 with an upper second-class. She reportedly claimed to be a distant relative of Christopher Isherwood, although the Lewis and Biswell biographies dispute this. Burgess and Jones were married on 22 January 1942.
Military service.
Burgess spent six weeks in 1940 as an army recruit in Eskbank before becoming a Nursing Orderly Class 3 in the Royal Army Medical Corps. During his service he was unpopular and was involved in incidents such as knocking off a corporal's cap and polishing the floor of a corridor to make people slip. In 1941 Burgess was pursued by military police of the British Armed Forces for desertion after overstaying his leave from Morpeth military base with his future bride Lynne. In 1942 he asked to be transferred to the Army Educational Corps and despite his loathing of authority he was promoted to sergeant. During the blackout his pregnant wife Lynne was beaten and raped by four American deserters in her home and perhaps as a result she lost the child. Burgess, stationed at the time in Gibraltar, was denied leave to see her.
At his stationing in Gibraltar, which he later wrote about in "A Vision of Battlements", he worked as a training college lecturer in speech and drama, teaching alongside Ann McGlinn in German, French and Spanish. McGlinn's communist ideology would have a major influence on his later novel "A Clockwork Orange". Burgess played a key role in "The British Way and Purpose" programme, designed to reintroduce members of the forces to the peacetime socialism of the post-war years in Britain. He was an instructor for the Central Advisory Council for Forces Education of the Ministry of Education. Burgess' flair for languages was noticed by army intelligence and he took part in debriefings of Dutch expatriates and Free French who found refuge in Gibraltar during the war. In the neighbouring Spanish town of La Línea de la Concepción he was arrested for insulting General Franco but released from custody shortly after the incident.
Early teaching career.
Burgess left the army in 1946 with the rank of sergeant-major and was for the next four years a lecturer in speech and drama at the Mid-West School of Education near Wolverhampton and at the Bamber Bridge Emergency Teacher Training College near Preston. Burgess taught in the extramural department of Birmingham University (1946–50).
In late 1950 he began working as a secondary school teacher at Banbury Grammar School (now Banbury School) teaching English literature. In addition to his teaching duties he supervised sports and ran the school's drama society. He organised a number of amateur theatrical events in his spare time. These involved local people and students and included productions of T. S. Eliot's "Sweeney Agonistes". Reports from his former students and colleagues indicate that he cared deeply about teaching.
With financial assistance provided by Lynne's father the couple were able to put a down payment on a cottage in the village of Adderbury, close to Banbury. He named the cottage "Little Gidding" after one of Eliot's "Four Quartets". Burgess cut his journalistic teeth in Adderbury, writing several articles for the local newspaper, the "Banbury Guardian".
Malaya.
In 1954, Burgess joined the British Colonial Service as a teacher and education officer in Malaya, initially stationed at Kuala Kangsar in Perak, in what were then known as the Federated Malay States. Here he taught at the "Malay College" (now Malay College Kuala Kangsar – MCKK), modeled on English public school lines. In addition to his teaching duties, he was a housemaster in charge of students of the preparatory school, who were housed at a Victorian mansion known as "King's Pavilion". A variety of the music he wrote there was influenced by the country, notably Sinfoni Melayu for orchestra and brass band, which included cries of Merdeka (independence) from the audience. No score, however, is extant.
Burgess and his wife had occupied a noisy apartment where privacy was minimal, and this caused resentment. Following a dispute with the Malay College's principal about this, Burgess was reposted to the Malay Teachers' Training College at Kota Bharu, Kelantan. Burgess attained fluency in Malay, spoken and written, achieving distinction in the examinations in the language set by the Colonial Office. He was rewarded with a salary increase for his proficiency in the language.
He devoted some of his free time in Malaya to creative writing "as a sort of gentlemanly hobby, because I knew there wasn't any money in it," and published his first novels: "Time for a Tiger", "The Enemy in the Blanket" and "Beds in the East". These became known as "The Malayan Trilogy" and were later published in one volume as "The Long Day Wanes".
Brunei.
After a brief period of leave in Britain during 1958, Burgess took up a further Eastern post, this time at the Sultan Omar Ali Saifuddin College in Bandar Seri Begawan, Brunei. Brunei had been a British protectorate since 1888, and was not to achieve independence until 1984. In the sultanate, Burgess sketched the novel that, when it was published in 1961, was to be entitled "Devil of a State" and, although it dealt with Brunei, for libel reasons the action had to be transposed to an imaginary East African territory similar to Zanzibar, named Dunia. In his autobiography "Little Wilson and Big God" (1987) Burgess wrote:""This novel was, is, about Brunei, which was renamed Naraka, Malay-Sanskrit for 'hell.' Little invention was needed to contrive a large cast of unbelievable characters and a number of interwoven plots. Though completed in 1958, the work was not published until 1961, for what it was worth it was made a choice of the book society. Heinemann, my publisher, was doubtful about publishing it: it might be libellous. I had to change the setting from Brunei to an East African one. Heinemann was right to be timorous. In early 1958, "The Enemy in the Blanket" appeared and at once provoked a libel suit.""
About this time Burgess collapsed in a Brunei classroom while teaching history and was diagnosed as having an inoperable brain tumour. Burgess was given just a year to live, prompting him to write several novels to get money to provide for his widow. He gave a different account, however, to Jeremy Isaacs in a "Face to Face" interview on the BBC "The Late Show" (21 March 1989). He said "Looking back now I see that I was driven out of the Colonial Service. I think possibly for political reasons that were disguised as clinical reasons." He alluded to this in an interview with Don Swaim, explaining that his wife Lynne had said something "obscene" to the British Queen's consort, the Duke of Edinburgh, during an official visit, and the colonial authorities turned against him. He had already earned their displeasure, he told Swaim, by writing articles in the newspaper in support of the revolutionary opposition party the Parti Rakyat Brunei, and for his friendship with its leader Dr. Azahari. Burgess' biographers attribute the incident to the author's notorious mythomania. Geoffrey Grigson writes, He was, however, suffering from the effects of prolonged heavy drinking (and associated poor nutrition), of the often oppressive south-east Asian climate, of chronic constipation, and of overwork and professional disappointment. As he put it, the scions of the sultans and of the élite in Brunei "did not wish to be taught", because the free-flowing abundance of oil guaranteed their income and privileged status. He may also have wished for a pretext to abandon teaching and get going full-time as a writer, having made a late start.
Repatriate years.
Burgess was invalided home in 1959 and relieved of his position in Brunei. He spent some time in the neurological ward of a London hospital (see "The Doctor is Sick") where he underwent cerebral tests that found no illness. On discharge, benefiting from a sum of money which Lynne Burgess had inherited from her father, together with their savings built up over six years in the East, he decided to become a full-time writer. The couple lived first in an apartment in Hove, near Brighton. They later moved to a semi-detached house called "Applegarth" in Etchingham, approximately a mile from the Jacobean house where Rudyard Kipling had lived in Burwash, and one mile from the Robertsbridge home of Malcolm Muggeridge. Upon the death of Burgess's father-in-law, the couple used their inheritance to decamp to a terraced town house in Chiswick. This provided convenient access to the White City BBC television studios where he later became a frequent guest. During these years Burgess became a regular drinking partner of the novelist William S. Burroughs. Their meetings took place in London and Tangiers.
A sea voyage the couple took with the Baltic Line from Tilbury to Leningrad in June 1961 resulted in the novel "Honey for the Bears". He wrote in his autobiographical "You've Had Your Time" (1990), that in re-learning Russian at this time, he found inspiration for the Russian-based slang Nadsat that he created for "A Clockwork Orange", going on to note "I would resist to the limit any publisher's demand that a glossary be provided."
Liliana Macellari, an Italian translator twelve years younger than Burgess, came across his novels "Inside Mr. Enderby" and "A Clockwork Orange", while writing about English fiction. The two first met in 1963 over lunch in Chiswick and began an affair. In 1964, Liana gave birth to Burgess' son, Paolo Andrea. The affair was hidden from Burgess's now-alcoholic wife, whom he refused to leave for fear of offending his cousin (by Burgess's stepmother, Margaret Dwyer Wilson), George Patrick Dwyer, then the Roman Catholic Bishop of Leeds.
Lynne Burgess died from cirrhosis of the liver, on 20 March 1968. Six months later, in September 1968, Burgess married Liana, acknowledging her four-year-old boy as his own, although the birth certificate listed Roy Halliday, Liana's former partner, as the father. Paolo Andrea (also known as Andrew Burgess Wilson) died in London in 2002, aged 37. Liana died in 2007.
Tax exile.
Burgess was a Conservative (though, as he clarified in an interview with "The Paris Review", his political views could be considered "a kind of anarchism" since his ideal of a "Catholic Jacobite imperial monarch" wasn't practicable), a (lapsed) Catholic and Monarchist, harbouring a distaste for all republics. He believed that socialism for the most part was "ridiculous" but did "concede that socialized medicine is a priority in any civilized country today." To avoid the 90% tax the family would have incurred because of their high income, they left Britain and toured Europe in a Bedford Dormobile motor-home. During their travels through France and across the Alps, Burgess wrote in the back of the van as Liana drove. In this period, he wrote novels and produced film scripts for Lew Grade and Franco Zeffirelli. His first place of residence after leaving England was Lija, Malta (1968–70). The negative reaction from a lecture that Burgess delivered to an audience of Catholic priests in Malta precipitated a move by the couple to Italy. The Burgesses maintained a flat in Rome, a country house in Bracciano, and a property in Montalbuccio. On hearing rumours of a mafia plot to kidnap Paolo-Andrea while the family was staying in Rome, Burgess decided to move to Monaco in 1975. Burgess was also motivated to move to the tax haven of Monaco as the country did not level income tax and widows were exempt from death duties, a form of taxation on their husband's estates.
The couple also had a villa in Provence, in Callian, Var, France, and an apartment just off Baker Street, London. 
Burgess lived for two years in the United States, working as a visiting professor at Princeton University with the creative writing program (1970) and as a distinguished professor at the City College of New York (1972). At City College he was a close colleague and friend of Joseph Heller. He went on to teach creative writing at Columbia University and was writer-in-residence at the University of North Carolina at Chapel Hill (1969) and at the University at Buffalo (1976). He lectured on the novel at the University of Iowa in 1975. Eventually he settled in Monaco in 1976, where he was active in the local community, becoming a co-founder in 1984 of the Princess Grace Irish Library, a centre for Irish cultural studies.
Although Burgess lived not far from Graham Greene, whose house was in Antibes, Greene became aggrieved shortly before his death by comments in newspaper articles by Burgess, and broke off all contact. Gore Vidal revealed in his 2006 memoir "Point to Point Navigation" that Greene disapproved of Burgess's appearance on various European television stations to discuss his (Burgess') books. Vidal recounts that Greene apparently regarded a willingness to appear on television as something that ought to be beneath a writer's dignity. "He talks about his books", Vidal quotes an exasperated Greene as saying.
During this time, Burgess spent much time at his chalet two kilometres outside Lugano, Switzerland.
Death.
Burgess wrote: "I shall die somewhere in the Mediterranean lands, with an inaccurate obituary in the Nice-Matin, unmourned, soon forgotten." In fact he died in the country of his birth. He returned to Twickenham, an outer suburb of London, where he owned a house, to await death. Burgess died on 22 November 1993 from lung cancer, at the Hospital of St John & St Elizabeth in London. His ashes were inurned at the Monaco Cemetery. The epitaph on Burgess's marble memorial stone, reads "Abba Abba." The phrase has several connotations. It means "Father, father" in Aramaic, Arabic, Hebrew and other Semitic languages. It is Burgess's initials forwards and backwards; part of the rhyme scheme for the Petrarchan sonnet; and the title of Burgess's 22nd novel, concerning the death of Keats. Eulogies at his memorial service at St Paul's, Covent Garden, London in 1994 were delivered by the journalist Auberon Waugh and the novelist William Boyd. "The Times" obituary heralded the author as "a great moralist." His estate was worth $3 million, and left a large European property portfolio of houses and apartments.
Life in music.
An accomplished musician, Burgess composed regularly throughout his life, and once said, "I wish people would think of me as a musician who writes novels, instead of a novelist who writes music on the side." Several of his pieces were broadcast during his lifetime on BBC Radio. His Symphony No. 3 in C was premiered by the University of Iowa orchestra in Iowa City in 1975. Burgess described his "Sinfoni Melayu" as an attempt to "combine the musical elements of the country into a synthetic language which called on native drums and xylophones." The structure of "Napoleon Symphony: A Novel in Four Movements" (1974) was modelled on Beethoven's Eroica symphony, while "Mozart and the Wolf Gang" (1991) mirrors the sound and rhythm of Mozartian composition, among other things attempting a fictional representation of Symphony No.40. Beethoven's Symphony No. 9 features prominently in "A Clockwork Orange" (and in Stanley Kubrick's film version of the novel). Many of his unpublished compositions are listed in "This Man and Music". He wrote a good deal of music for recorder as his son played the instrument. Several of his pieces for recorder and piano including the Sonata No. 1, Sonatina and 'Tre Pezzetti' have been included on a major CD release from recorder player John Turner and pianist Harvey Davies; the double album also includes related music from 15 other composers and is titled 'Anthony Burgess – The Man and his Music' (Metier records, release September 2013).
Burgess produced a translation of Bizet's "Carmen" which was performed by the English National Opera, and wrote for the 1973 Broadway musical "Cyrano", using his own adaptation of the original Rostand play as his basis. He created "Blooms of Dublin" in 1982, an operetta based on James Joyce's "Ulysses" (televised for the BBC) and wrote a libretto for Weber's "Oberon", performed by the Edinburgh-based Scottish Opera.
On the BBC's "Desert Island Discs" radio programme in 1966, Burgess chose as his favourite music Purcell's "Rejoice in the Lord Alway"; Bach's "Goldberg Variations" No. 13; Elgar's Symphony No. 1 in A-flat major; Wagner's "Walter's Trial Song" from "Die Meistersinger von Nürnberg"; Debussy's "Fêtes" from "Nocturnes"; Lambert's "The Rio Grande"; Walton's Symphony No. 1 in B-flat minor; and Vaughan Williams' "On Wenlock Edge".
Linguistics.
"Burgess's linguistic training", wrote Raymond Chapman and Tom McArthur in "The Oxford Companion to the English Language", "is shown in dialogue enriched by distinctive pronunciations and the niceties of register." During his years in Malaya, and after he had mastered Jawi, the Arabic script adapted for Malay, Burgess taught himself the Persian language, after which he produced a translation of Eliot's "The Waste Land" into Persian (unpublished). He worked on an anthology of the best of English literature translated into Malay, which failed to achieve publication. Burgess's published translations include two different versions of "Cyrano de Bergerac", "Oedipus the King" and "Carmen".
Burgess's interest in language was reflected in the invented, Anglo-Russian teen slang of "A Clockwork Orange" (Nadsat), and in the movie "Quest for Fire" (1981), for which he invented a prehistoric language ("Ulam") for the characters. His interest is reflected in his characters. In "The Doctor is Sick", Dr Edwin Spindrift is a lecturer in linguistics who escapes from a hospital ward which is peopled, as the critic Saul Maloff put it in a review, with "brain cases who happily exemplify varieties of English speech." Burgess, who had lectured on phonetics at the University of Birmingham in the late 1940s, investigates the field of linguistics in "Language Made Plain" and "A Mouthful of Air".
The depth of Burgess's multilingual proficiency came under discussion in Roger Lewis's . Lewis claimed that during production in Malaysia of the BBC documentary "A Kind of Failure" (1982), Burgess's supposedly fluent Malay was not understood by waitresses at a restaurant where they were filming. It was claimed that the documentary's director deliberately kept these moments intact in the film to expose Burgess's linguistic pretensions. A letter from David Wallace that appeared in the magazine of the London "Independent on Sunday" newspaper on 25 November 2002 shed light on the affair. Wallace's letter read, in part:
Lewis may not have been fully aware of the fact that a quarter of Malaysia's population is made up of Hokkien- and Cantonese-speaking Chinese. However, Malay had been installed as the National Language with the passing of the Language Act of 1967. By 1982 all national primary and secondary schools in Malaysia would have been teaching with Bahasa Melayu as a base language (see Harold Crouch, "Government and Society in Malaysia", Ithaca and London: Cornell University Press, 1996).
Works.
Novels.
His Malayan trilogy "The Long Day Wanes" was Burgess's first published fiction. Its three books are "Time for a Tiger," "The Enemy in the Blanket" and "Beds in the East." It was Burgess's ambition to become "the true fictional expert on Malaya." In these works, Burgess was working in the tradition established by Kipling for British India, and Conrad and Maugham for Southeast Asia. Burgess operated more in the mode of Orwell, who had a good command of Urdu and Burmese (necessary for Orwell's work as a police officer) and Kipling, who spoke Hindi (having learnt it as a child). Like many of his fellow English expatriates in Asia, Burgess had excellent spoken and written command of his operative language(s), both as a novelist and speaker, including Malay.
Burgess's repatriate years (c. 1960–69) produced "Enderby" and "The Right to an Answer," which touches on the theme of death and dying, and "One Hand Clapping," a satire on the vacuity of popular culture. "The Worm and the Ring" (1961) had to be withdrawn from circulation under the threat of libel action from one of Burgess's former colleagues, a school secretary.
His dystopian novel "A Clockwork Orange" was published in 1962. It was inspired initially by an incident during the Second World War in which his wife Lynne was robbed, assaulted and violated by deserters from the US Army in London during the blackout. The event may have contributed to her subsequent miscarriage. The book was an examination of free will and morality. The young anti-hero, Alex, captured after a short career of violence and mayhem, undergoes a course of aversion therapy treatment to curb his violent tendencies. This results in making him defenceless against other people and unable to enjoy some of his favourite music that, besides violence, had been an intense pleasure for him. In the non-fiction book "Flame into Being" (1985) Burgess described "A Clockwork Orange" as "a jeu d'esprit knocked off for money in three weeks, it became known as the raw material for a film which seemed to glorify sex and violence." He added "the film made it easy for readers of the book to misunderstand what it was about, and the misunderstanding will pursue me till I die." Near the time of publication the final chapter was cut from the American edition of the book. Burgess had written "A Clockwork Orange" with twenty-one chapters, meaning to match the age of majority. "21 is the symbol of human maturity, or used to be, since at 21 you got to vote and assumed adult responsibility," Burgess wrote in a foreword for a 1986 edition. Needing money and thinking that the publisher was "being charitable in accepting the work at all," Burgess accepted the deal and allowed "A Clockwork Orange" to be published in the US with the twenty-first chapter omitted. Stanley Kubrick's film adaptation of "A Clockwork Orange" was based on the American edition, and thus helped to perpetuate the loss of the last chapter.
In Martin Seymour-Smith's "Novels and Novelists: A Guide to the World of Fiction," Burgess related that he would often prepare a synopsis with a name-list before beginning a project. Seymour-Smith wrote: "Burgess believes overplanning is fatal to creativity and regards his unconscious mind and the act of writing itself as indispensable guides. He does not produce a draft of a whole novel but prefers to get one page finished before he goes on to the next, which involves a good deal of revision and correction."
"" is a fictional recreation of Shakespeare's love-life and an examination of the supposedly partly syphilitic sources of the bard's imaginative vision. The novel, which drew on Edgar I. Fripp's 1938 biography "Shakespeare, Man and artist," won critical acclaim and placed Burgess among the first rank novelists of his generation. "M/F" (1971) was listed by the writer himself as one of the works of which he was most proud. "Beard's Roman Women" was revealing on a personal level, dealing with the death of his first wife, his bereavement, and the affair that led to his second marriage. In "Napoleon Symphony," Burgess brought Bonaparte to life by shaping the novel's structure to Beethoven's "Eroica" symphony. The novel contains a portrait of an Arab and Muslim society under occupation by a Christian western power (Egypt by Catholic France). In the 1980s, religious themes began to feature heavily ("The Kingdom of the Wicked," "Man of Nazareth," "Earthly Powers"). Though Burgess lapsed from Catholicism early in his youth, the influence of the Catholic "training" and worldview remained strong in his work all his life. This is notable in the discussion of free will in "A Clockwork Orange," and in the apocalyptic vision of devastating changes in the Catholic Church – due to what can be understood as Satanic influence – in "Earthly Powers" (1980).
Burgess kept working through his final illness and was writing on his deathbed. The late novel "Any Old Iron" is a generational saga of two families, one Russian-Welsh, the other Jewish, encompassing the sinking of the Titanic, World War I, the Russian Revolution, the Spanish Civil War, World War II, the early years of the State of Israel, and the rediscovery of Excalibur. "A Dead Man in Deptford," about Christopher Marlowe, is a companion novel to "." The verse novel "" was published posthumously.
Critical studies.
Burgess started his career as a critic. His "English Literature, A Survey for Students", was aimed at newcomers to the subject. He followed this with "The Novel To-day" (Longmans, 1963) and "The Novel Now: A Student's Guide to Contemporary Fiction" (New York: W.W. Norton and Company, 1967). He wrote the Joyce studies "Here Comes Everybody: An Introduction to James Joyce for the Ordinary Reader" (also published as "Re Joyce") and "Joysprick: An Introduction to the Language of James Joyce". Also published was "A Shorter 'Finnegans Wake",' Burgess's abridgement. His 1970 "Encyclopædia Britannica" entry on the novel (under "Novel, the") is regarded as a classic of the genre. Burgess wrote full-length critical studies of William Shakespeare, Ernest Hemingway and D. H. Lawrence, as well as "Ninety-nine Novels: The Best in English since 1939".
Screenwriting.
Burgess wrote the screenplays for "Moses the Lawgiver" (Gianfranco De Bosio 1974), "Jesus of Nazareth" (Franco Zeffirelli 1977), and "A.D." (Stuart Cooper, 1985). Burgess was co-writer of the script for the TV series "Sherlock Holmes and Doctor Watson" (1980). The film treatments he produced include "Amundsen", "Attila", "The Black Prince", "Cyrus the Great", "Dawn Chorus", "The Dirty Tricks of Bertoldo", "Eternal Life", "Onassis", "Puma", "Samson and Delilah", "Schreber", "The Sexual Habits of the English Middle Class", "Shah", "That Man Freud" and "Uncle Ludwig". Burgess devised a Stone Age language for "La Guerre du Feu" ("Quest for Fire"; Jean-Jacques Annaud, 1981).
Burgess penned many unpublished scripts, including "Will!" or "The Bawdy Bard" about Shakespeare, based on the novel "Nothing Like The Sun". Encouraged by the success of "" (a parody of James Bond adventures), Burgess wrote a screenplay for "The Spy Who Loved Me", also rejected, although the huge submarine silo seen in the finished film was reportedly Burgess's inspiration.

</doc>
<doc id="49373" url="https://en.wikipedia.org/wiki?curid=49373" title="Grid computing">
Grid computing

Grid computing is the collection of computer resources from multiple locations to reach a common goal. The grid can be thought of as a distributed system with non-interactive workloads that involve a large number of files. Grid computing is distinguished from conventional high performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, “distributed” or “grid” computing, can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus.
Overview.
Grid computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may then disappear just as quickly.
One of the main strategies of grid computing is to use middleware to divide and apportion pieces of a program among several computers, sometimes up to many thousands. Grid computing involves computation in a distributed fashion, which may also involve the aggregation of large-scale clusters.
The size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks. "The notion of a confined grid may also be known as an intra-nodes cooperation whilst the notion of a larger, wider grid may thus refer to an inter-nodes cooperation".
Grids are a form of distributed computing whereby a “super virtual computer” is composed of many networked loosely coupled computers acting together to perform very large tasks. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.
Coordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the Grid context.
Comparison of grids and conventional supercomputers.
“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors. The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.
There are also some differences in programming and deployment. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a “thin” layer of “grid” infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine, and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.
Design considerations and variations.
One feature of distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.
One disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dialup Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in expected time.
The impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust “client” nodes must place in the central system such as placing applications in virtual machines.
Public systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this trade off, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). There are diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.
In fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.
Market segmentation of the grid computing market.
For the segmentation of the grid computing market, two perspectives need to be considered: the provider side and the user side:
The provider side.
The overall grid market comprises several specific markets. These are the grid middleware market, the market for grid-enabled applications, the utility computing market, and the software-as-a-service (SaaS) market.
Grid middleware is a specific software product, which enables the sharing of heterogeneous resources, and Virtual Organizations. It is installed and integrated into the existing infrastructure of the involved company or companies, and provides a special layer placed among the heterogeneous infrastructure and the specific user applications. Major grid middlewares are Globus Toolkit, gLite, and UNICORE.
Utility computing is referred to as the provision of grid computing and applications as service either as an open grid utility or as a hosting solution for one organization or a VO. Major players in the utility computing market are Sun Microsystems, IBM, and HP.
Grid-enabled applications are specific software applications that can utilize grid infrastructure. This is made possible by the use of grid middleware, as pointed out above.
Software as a service (SaaS) is “software that is owned, delivered and managed remotely by one or more providers.” (Gartner 2007) Additionally, SaaS applications are based on a single set of common code and data definitions. They are consumed in a one-to-many model, and SaaS uses a Pay As You Go (PAYG) model or a subscription model that is based on usage. Providers of SaaS do not necessarily own the computing resources themselves, which are required to run their SaaS. Therefore, SaaS providers may draw upon the utility computing market. The utility computing market provides computing resources for SaaS providers.
The user side.
For companies on the demand or user side of the grid computing market, the different segments have significant implications for their IT deployment strategy. The IT deployment strategy as well as the type of IT investments made are relevant aspects for potential grid users and play an important role for grid adoption.
CPU scavenging.
CPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the unused resources in a network of participants (whether worldwide or internal to an organization). Typically this technique uses desktop computer instruction cycles that would otherwise be wasted at night, during lunch, or even in the scattered seconds throughout the day when the computer is waiting for user input on relatively fast devices. In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.
Many volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go "offline" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.
Creating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations. Like other full-featured batch systems, HTCondor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. It can be used to manage workload on a dedicated cluster of computers as well or it can seamlessly integrate both dedicated resources (rack-mounted clusters) and non-dedicated desktop machines (cycle scavenging) into one computing environment.
History.
The term "grid computing" originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, "The Grid: Blueprint for a new computing infrastructure" (1999).
CPU scavenging and volunteer computing were popularized beginning in 1997 by distributed.net and later in 1999 by SETI@home to harness the power of networked PCs worldwide, in order to solve CPU-intensive research problems.
The ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster, Carl Kesselman, and Steve Tuecke, widely regarded as the "fathers of the grid". They led the effort to create the Globus Toolkit incorporating not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.
In 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid). Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.
Progress in Grid computing.
In November 2006, Seidel received the Sidney Fernbach Award at the Supercomputing Conference in Tampa, Florida."For outstanding contributions to the development of software for HPC and Grid computing to enable the collaborative numerical investigation of complex problems in physics; in particular, modeling black hole collisions." This award, which is one of the highest honors in computing, was awarded for his achievements in numerical relativity.
Projects and applications.
Grid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling. Grids offer a way of using the information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.
Grid computing is being applied by the National Science Foundation's National Technology Grid, NASA's Information Power Grid, Pratt & Whitney, Bristol-Myers Squibb Co., and American Express.
One cycle-scavenging network is SETI@home, which was using more than 3 million computers to achieve 23.37 sustained teraflops (979 lifetime teraflops) .
As of August 2009 Folding@home achieves more than 4 petaflops on over 350,000 machines.
The European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is “to establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies”. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration, but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by IT-Tude.com.
The Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the LHC Computing Grid (LCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within LCG can be found online as can real time monitoring of the EGEE infrastructure. The relevant software and documentation is also publicly accessible. There is speculation that dedicated fiber optic links, such as those installed by CERN to address the LCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection. The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.
The distributed.net project was started in 1997.
The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.
In 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.
As of 2011, over 6.2 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid, which tops the processing power of the current fastest supercomputer system (China's Tianhe-I).
Definitions.
Today there are many definitions of "grid computing":

</doc>
<doc id="49374" url="https://en.wikipedia.org/wiki?curid=49374" title="978">
978

__NOTOC__
Year 978 (CMLXXVIII) was a common year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49375" url="https://en.wikipedia.org/wiki?curid=49375" title="Larynx">
Larynx

The larynx (plural larynges; from the Greek λάρυγξ "lárynx"), commonly called the voice box, is an organ in the neck of amphibians, reptiles, and mammals involved in breathing, sound production, and protecting the trachea against food aspiration. It manipulates pitch and volume. The larynx houses the vocal folds (vocal cords), which are essential for phonation. The vocal folds are situated just below where the tract of the pharynx splits into the trachea and the esophagus.
Structure.
Cartilages.
There are nine cartilages, three unpaired and three paired, that support the mammalian larynx and form its skeleton.
Unpaired cartilages:
Paired cartilages:
Muscles.
The muscles of the larynx are divided into "intrinsic" and "extrinsic" muscles.
The intrinsic muscles are divided into respiratory and the phonatory muscles (the muscles of phonation). The respiratory muscles move the vocal cords apart and serve breathing. The phonatory muscles move the vocal cords together and serve the production of voice. The extrinsic, passing between the larynx and parts around; and intrinsic, confined entirely. The main respiratory muscles are the posterior cricoarytenoid muscles. The phonatory muscles are divided into adductors (lateral cricoarytenoid muscles, arytenoid muscles) and tensors (cricothyroid muscles, thyroarytenoid muscles).
Intrinsic.
The intrinsic laryngeal muscles are responsible for controlling sound production. 
Notably, the only muscle capable of separating the vocal cords for normal breathing is the posterior cricoarytenoid. If this muscle is incapacitated on both sides, the inability to pull the vocal folds apart (abduct) will cause difficulty breathing. Bilateral injury to the recurrent laryngeal nerve would cause this condition. It is also worth noting that all muscles are innervated by the recurrent laryngeal branch of the vagus except the cricothyroid muscle, which is innervated by the external laryngeal branch of the superior laryngeal nerve (a branch of the vagus).
Extrinsic.
The extrinsic laryngeal muscles support and position the larynx within the trachea.
Innervation.
The larynx is innervated by branches of the vagus nerve on each side. Sensory innervation to the glottis and laryngeal vestibule is by the internal branch of the superior laryngeal nerve. The external branch of the superior laryngeal nerve innervates the cricothyroid muscle. Motor innervation to all other muscles of the larynx and sensory innervation to the subglottis is by the recurrent laryngeal nerve. While the sensory input described above is (general) visceral sensation (diffuse, poorly localized), the vocal fold also receives general somatic sensory innervation (proprioceptive and touch) by the superior laryngeal nerve.
Injury to the external laryngeal nerve causes weakened phonation because the vocal folds cannot be tightened. Injury to one of the recurrent laryngeal nerves produces hoarseness, if both are damaged the voice may or may not be preserved, but breathing becomes difficult.
Development.
In adult humans, the larynx is found in the anterior neck at the level of the C3–C6 vertebrae. It connects the inferior part of the pharynx (hypopharynx) with the trachea. The laryngeal skeleton consists of nine cartilages: three single (epiglottic, thyroid and cricoid) and three paired (arytenoid, corniculate, and cuneiform). The hyoid bone is not part of the larynx, though the larynx is suspended from the hyoid. The larynx extends vertically from the tip of the epiglottis to the inferior border of the cricoid cartilage. Its interior can be divided in supraglottis, glottis and subglottis.
In newborn infants, the larynx is initially at the level of the C2–C3 vertebrae, and is further forward and higher relative to its position in the adult body. The larynx descends as the child grows.
Function.
Sound generation.
Sound is generated in the larynx, and that is where pitch and volume are manipulated. The strength of expiration from the lungs also contributes to loudness.
Manipulation of the larynx is used to generate a source sound with a particular fundamental frequency, or pitch. This source sound is altered as it travels through the vocal tract, configured differently based on the position of the tongue, lips, mouth, and pharynx. The process of altering a source sound as it passes through the filter of the vocal tract creates the many different vowel and consonant sounds of the world's languages as well as tone, certain realizations of stress and other types of linguistic prosody. The larynx also has a similar function to the lungs in creating pressure differences required for sound production; a constricted larynx can be raised or lowered affecting the volume of the oral cavity as necessary in glottalic consonants.
The vocal folds can be held close together (by adducting the arytenoid cartilages) so that they vibrate (see phonation). The muscles attached to the arytenoid cartilages control the degree of opening. Vocal fold length and tension can be controlled by rocking the thyroid cartilage forward and backward on the cricoid cartilage (either directly by contracting the cricothyroids or indirectly by changing the vertical position of the larynx), by manipulating the tension of the muscles within the vocal folds, and by moving the arytenoids forward or backward. This causes the pitch produced during phonation to rise or fall. In most males the vocal folds are longer and with a greater mass than most females' vocal folds, producing a lower pitch.
The vocal apparatus consists of two pairs of mucosal folds. These folds are false vocal folds (vestibular folds) and true vocal folds (folds). The false vocal folds are covered by respiratory epithelium, while the true vocal folds are covered by stratified squamous epithelium. The false vocal folds are not responsible for sound production, but rather for resonance. The exceptions to this are found in Tibetan Chant and Kargyraa, a style of Tuvan throat singing. Both make use of the false vocal folds to create an undertone. These false vocal folds do not contain muscle, while the true vocal folds do have skeletal muscle.
Other.
The most important role of the larynx is its protecting function; the prevention of foreign objects from entering the lungs by coughing and other reflexive actions. A cough is initiated by a deep inhalation through the vocal folds, followed by the elevation of the larynx and the tight adduction (closing) of the vocal folds. The forced expiration that follows, assisted by tissue recoil and the muscles of expiration, blows the vocal folds apart, and the high pressure expels the irritating object out of the throat. Throat clearing is less violent than coughing, but is a similar increased respiratory effort countered by the tightening of the laryngeal musculature. Both coughing and throat clearing are predictable and necessary actions because they clear the respiratory passageway, but both place the vocal folds under significant strain.
Another important role of the larynx is abdominal fixation, a kind of Valsalva maneuver in which the lungs are filled with air in order to stiffen the thorax so that forces applied for lifting can be translated down to the legs. This is achieved by a deep inhalation followed by the adduction of the vocal folds. Grunting while lifting heavy objects is the result of some air escaping through the adducted vocal folds ready for phonation.
Abduction of the vocal folds is important during physical exertion. The vocal folds are separated by about during normal respiration, but this width is doubled during forced respiration.
During swallowing, the backward motion of the tongue forces the epiglottis over the glottis' opening to prevent swallowed material from entering the larynx which leads to the lungs; the larynx is also pulled upwards to assist this process. Stimulation of the larynx by ingested matter produces a strong cough reflex to protect the lungs.
In addition, intrinsic laryngeal muscles (ILM) are spared from muscle wasting disorders, such as Duchenne muscular dystrophy, may facilitate the development of novel strategies for the prevention and treatment of muscle wasting in a variety of clinical scenarios. ILM have a calcium regulation system profile suggestive of a better ability to handle calcium changes in comparison to other muscles, and this may provide a mechanistic insight for their unique pathophysiological properties 
Clinical significance.
Disorders.
There are several things that can cause a larynx to not function properly. Some symptoms are hoarseness, loss of voice, pain in the throat or ears, and breathing difficulties. Larynx transplant is a rare procedure. The world's first successful operation took place in 1998 at the Cleveland Clinic, and the second took place in October 2010 at the University of California Davis Medical Center in Sacramento.
Other animals.
[[File:Kehlkopf Pferd.jpg|thumb|Cut through the larynx of a horse
(frontal section, posterior view)1 hyoid bone; 2 epiglottis; 3 vestibular fold; 4 vocal fold; 5 ventricularis muscle; 6 ventricle of larynx; 7 vocalis muscle; 8 Thyroid Cartilage; 9 Cricoid Cartilage; 10 infraglottic cavity; 11 first tracheal cartilage; 12 trachea]]
Pioneering work on the structure and evolution of the larynx was carried out in the 1920s by the British comparative anatomist Victor Negus, culminating in his monumental work "The Mechanism of the Larynx" (1929). Negus, however, pointed out that the descent of the larynx reflected the reshaping and descent of the human tongue into the pharynx. This process is not complete until age six to eight years. Some researchers, such as Philip Lieberman, Dennis Klatt, Brant de Boer and Kenneth Stevens using computer-modeling techniques have suggested that the species-specific human tongue allows the vocal tract (the airway above the larynx) to assume the shapes necessary to produce speech sounds that enhance the robustness of human speech. Sounds such as the vowels of the words see and do, and [u, (in phonetic notation) have been shown to be less subject to confusion in classic studies such as the 1950 Peterson and Barney investigation of the possibilities for computerized speech recognition.
In contrast, though other species have low larynges their tongues remains anchored in their mouths and their vocal tracts cannot produce the range of speech sounds of humans. The ability to lower the larynx transiently in some species extends the length of their vocal tract, which as Fitch showed creates the acoustic illusion that they are larger. Research at Haskins Laboratories in the 1960s showed that speech allows humans to achieve a vocal communication rate that exceeds the fusion frequency of the auditory system by fusing sounds together into syllables and words. The additional speech sounds that the human tongue enables us to produce, particularly , allow humans to unconsciously infer the length of the vocal tract of the person who is talking, a critical element in recovering the phonemes that make up a word.
Non-mammals.
Most tetrapod species possess a larynx, but its structure is typically simpler than that found in mammals. The cartilages surrounding the larynx are apparently a remnant of the original gill arches in fish, and are a common feature, but not all are always present. For example, the thyroid cartilage is found only in mammals. Similarly, only mammals possess a true epiglottis, although a flap of non-cartilagenous mucosa is found in a similar position in many other groups. In modern amphibians, the laryngeal skeleton is considerably reduced; frogs have only the cricoid and arytenoid cartilages, while salamanders possess only the arytenoids.
Vocal folds are found only in mammals, and a few lizards. As a result, many reptiles and amphibians are essentially voiceless; frogs use ridges in the trachea to modulate sound, while birds have a separate sound-producing organ, the syrinx.
History.
Roman physician Galen first described the larynx, describing it as the "first and supremely most important instrument of the voice"

</doc>
<doc id="49378" url="https://en.wikipedia.org/wiki?curid=49378" title="924">
924

__NOTOC__
Year 924 (CMXXIV) was a leap year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49379" url="https://en.wikipedia.org/wiki?curid=49379" title="925">
925

__NOTOC__
Year 925 (CMXXV) was a common year starting on Saturday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49380" url="https://en.wikipedia.org/wiki?curid=49380" title="929">
929

__NOTOC__
Year 929 (CMXXIX) was a common year starting on Thursday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49382" url="https://en.wikipedia.org/wiki?curid=49382" title="928">
928

__NOTOC__
Year 928 (CMXXVIII) was a leap year starting on Tuesday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By topic.
Religion.
</onlyinclude>

</doc>
<doc id="49383" url="https://en.wikipedia.org/wiki?curid=49383" title="576">
576

__NOTOC__
Year 576 (DLXXVI) was a leap year starting on Wednesday (link will display the full calendar) of the Julian calendar. The denomination 576 for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Asia.
</onlyinclude>

</doc>
<doc id="49386" url="https://en.wikipedia.org/wiki?curid=49386" title="927">
927

__NOTOC__
Year 927 (CMXXVII) was a common year starting on Monday (link 'will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>
By place.
Religion.
</onlyinclude>

</doc>
<doc id="49387" url="https://en.wikipedia.org/wiki?curid=49387" title="Deep Blue (chess computer)">
Deep Blue (chess computer)

Deep Blue was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls.
Deep Blue won its first game against a world champion on February 10, 1996, when it defeated Garry Kasparov in game one of a six-game match. However, Kasparov won three and drew two of the following five games, defeating Deep Blue by a score of 4–2. Deep Blue was then heavily upgraded, and played Kasparov again in May 1997. Deep Blue won game six, therefore winning the six-game rematch 3½–2½ and becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls. Kasparov accused IBM of cheating and demanded a rematch. IBM refused and retired Deep Blue.
Development for Deep Blue began in 1985 with the ChipTest project at Carnegie Mellon University. This project eventually evolved into Deep Thought, at which point the development team was hired by IBM. The project evolved once more with the new name Deep Blue in 1989. Grandmaster Joel Benjamin was also signed on to the development team by IBM.
Origins.
The project was started as ChipTest at Carnegie Mellon University by Feng-hsiung Hsu, followed by its successor, Deep Thought. After their graduation from Carnegie Mellon, Hsu, Thomas Anantharaman, and Murray Campbell from the Deep Thought team were hired by IBM Research to continue their quest to build a chess machine that could defeat the world champion. Hsu and Campbell joined IBM in autumn 1989, with Anantharaman following later. Anantharaman subsequently left IBM for Wall Street and Arthur Joseph Hoane joined the team to perform programming tasks. Jerry Brody, a long-time employee of IBM Research, was recruited for the team in 1990.
The team was managed first by Randy Moulic, followed by Chung-Jen (C J) Tan.
After Deep Thought's 1989 match against Kasparov, IBM held a contest to rename the chess machine and it became "Deep Blue", a play on IBM's nickname, "Big Blue". After a scaled-down version of Deep Blue, Deep Blue Jr., played Grandmaster Joel Benjamin, Hsu and Campbell decided that Benjamin was the expert they were looking for to develop Deep Blue's opening book, and Benjamin was signed by IBM Research to assist with the preparations for Deep Blue's matches against Garry Kasparov.
In 1995 "Deep Blue prototype" (actually Deep Thought II, renamed for PR reasons) played in the 8th World Computer Chess Championship. Deep Blue prototype played the computer program Wchess to a draw while Wchess was running on a personal computer. In round 5 Deep Blue prototype had the white pieces and lost to the computer program Fritz 3 in 39 moves while Fritz was running on an Intel Pentium 90 MHz personal computer. In the end of the championship Deep Blue prototype was tied for second place with the computer program Junior while Junior was running on a personal computer.
Deep Blue versus Kasparov.
Deep Blue and Kasparov played each other on two occasions. The first match began on February 10, 1996, in which Deep Blue became the first machine to win a chess game against a reigning world champion (Garry Kasparov) under regular time controls. However, Kasparov won three and drew two of the following five games, beating Deep Blue by a score of 4–2 (wins count 1 point, draws count ½ point). The match concluded on February 17, 1996.
Deep Blue was then heavily upgraded (unofficially nicknamed "Deeper Blue") and played Kasparov again in May 1997, winning the six-game rematch 3½–2½, ending on May 11. Deep Blue won the deciding game six after Kasparov made a mistake in the opening, becoming the first computer system to defeat a reigning world champion in a match under standard chess tournament time controls.
The system derived its playing strength mainly from brute force computing power. It was a massively parallel, RS/6000 SP Thin P2SC-based system with 30 nodes, with each node containing a 120 MHz P2SC microprocessor, enhanced with 480 special purpose VLSI chess chips. Its chess playing program was written in C and ran under the AIX operating system. It was capable of evaluating 200 million positions per second, twice as fast as the 1996 version. In June 1997, Deep Blue was the 259th most powerful supercomputer according to the TOP500 list, achieving 11.38 GFLOPS on the High-Performance LINPACK benchmark.
The Deep Blue chess computer that defeated Kasparov in 1997 would typically search to a depth of between six and eight moves to a maximum of twenty or even more moves in some situations. David Levy and Monty Newborn estimate that one additional ply (half-move) increases the playing strength 50 to 70 Elo points.
Deep Blue's evaluation function was initially written in a generalized form, with many to-be-determined parameters (e.g. how important is a safe king position compared to a space advantage in the center, etc.). The optimal values for these parameters were then determined by the system itself, by analyzing thousands of master games. The evaluation function had been split into 8,000 parts, many of them designed for special positions. In the opening book there were over 4,000 positions and 700,000 grandmaster games. The endgame database contained many six piece endgames and five or fewer piece positions. Before the second match, the chess knowledge of the program was fine tuned by grandmaster Joel Benjamin. The opening library was provided by grandmasters Miguel Illescas, John Fedorowicz, and Nick de Firmian. When Kasparov requested that he be allowed to study other games that Deep Blue had played so as to better understand his opponent, IBM refused. However, Kasparov did study many popular PC computer games to become familiar with computer game play in general.
Writer Nate Silver suggests that a bug in Deep Blue's software led to a seemingly random move (the 44th in the first game) which Kasparov misattributed to "superior intelligence". Subsequently, Kasparov experienced a drop in performance due to anxiety in the following game.
Aftermath.
After the loss, Kasparov said that he sometimes saw deep intelligence and creativity in the machine's moves, suggesting that during the second game, human chess players had intervened on behalf of the machine, which would be a violation of the rules. IBM denied that it cheated, saying the only human intervention occurred between games. The rules provided for the developers to modify the program between games, an opportunity they said they used to shore up weaknesses in the computer's play that were revealed during the course of the match. Kasparov requested printouts of the machine's log files but IBM refused, although the company later published the logs on the Internet. Kasparov demanded a rematch, but IBM refused and dismantled Deep Blue. Owing to an insufficient sample of games between Deep Blue and officially rated chess players, a chess rating for Deep Blue was not established.
In 2003 a documentary film was made that explored these claims. Entitled "", the film interviewed some people who suggest that Deep Blue's victory was a ploy by IBM to boost its stock value.
One of the cultural impacts of Deep Blue was the creation of a new game called Arimaa designed to be much more difficult for computers than chess.
One of the two racks that made up Deep Blue is on display at the National Museum of American History in their exhibit about the Information Age ; the other rack appears at the Computer History Museum in the "Artificial Intelligence and Robotics" gallery of the Revolution exhibit. (Reports that Deep Blue was sold to United Airlines appear to originate from confusion between Deep Blue itself and other RS6000/SP2 systems.)
Feng-hsiung Hsu later claimed in his book "Behind Deep Blue" that he had the rights to use the Deep Blue design to build a bigger machine independently of IBM to take Kasparov's rematch offer, but Kasparov refused a rematch.
Deep Blue, with its capability of evaluating 200 million positions per second, was the fastest computer that ever faced a world chess champion. Today, in computer chess research and matches of world class players against computers, the focus of play has often shifted to software chess programs, rather than using dedicated chess hardware. Modern chess programs like Houdini, Rybka, Deep Fritz, or Deep Junior are more efficient than the programs during Deep Blue's era. In a November 2006 match between Deep Fritz and world chess champion Vladimir Kramnik, the program ran on a personal computer containing two Intel Core 2 Duo CPUs, capable of evaluating only 8 million positions per second, but searching to an average depth of 17 to 18 plies in the middlegame thanks to heuristics; it won 4-2.
References.
Notes
Bibliography

</doc>
<doc id="49388" url="https://en.wikipedia.org/wiki?curid=49388" title="932">
932

__NOTOC__
Year 932 (CMXXXII) was a leap year starting on Sunday (link will display the full calendar) of the Julian calendar.
Events.
<onlyinclude>

</doc>
<doc id="49390" url="https://en.wikipedia.org/wiki?curid=49390" title="Cello (web browser)">
Cello (web browser)

Cello was an early graphical web browser for Windows 3.1, developed by Thomas R. Bruce of the Legal Information Institute at Cornell Law School, and released as shareware in 1993. While other browsers ran on various Unix machines, Cello was the first web browser for Microsoft Windows, using the winsock system to access the Internet. In addition to the basic Windows, Cello worked on Windows NT 3.5 and with small modifications on OS/2.
Cello was created because of a demand for Web access by lawyers, who were more likely to use Microsoft Windows than the Unix operating systems supporting earlier Web browsers, including the first release of Mosaic. The lack of a Windows browser meant many legal experts were unable to access legal information made available in hypertext on the World Wide Web. Cello was popular during 1993/1994, but fell out of favor following the release of Mosaic for Windows and Netscape, after which Cello development was abandoned.
Cello was first publicly released on 8 June 1993. A version 2.0 was announced, but development was abandoned. Version 1.01a, 16 April 1994, was the last public release. Since then, the Legal Information Institute at Cornell Law School has licensed the Cello 2.0 source code, which has been used to develop commercial software.
The browser is no longer available from its original homepage. However, it can still be downloaded from mirror sites.
Development and history.
The development of Cello started in 1992, with beta versions planned for June 1993 and a release for July 1993. It was publicly announced on 12 April 1993.
The Legal Information Institute at Cornell Law School created the first law site on the
Internet in 1992 and the first legal website in 1993. However, at the time, there were no web browsers for the Microsoft Windows operating system, which was used by most lawyers. Thus, to allow lawyers to use their website, the Legal Information Institute developed the first Windows-based Web browser. This was made possible by a grant from the National Center for Automated Information Research.
Although other browsers at the time were based on CERN's WWW libraries called libwww, PCs of the time were not powerful enough to run the UNIX-oriented code. As a result, Thomas Bruce had to rewrite most of the WWW libraries to work on Microsoft Windows. It should also be noted that unlike most commercial browsers at that time, Cello didn't utilize any of Mosaic's source code and thus had a different look and feel.
Steven Sinofsky, president of the Windows division at Microsoft wrote in a June 1994 email: "We do not currently plan on any other client software the upcoming release of Windows 95, especially something like Mosaic or Cello." Nevertheless, on 11 January 1995, Microsoft announced that it had licensed the Mosaic technology from Spyglass, which it would use to create Internet Explorer. On 15 August 1995, Microsoft debuted its own web browser Internet Explorer 1 for Windows 95. While it did not ship with the original release of Windows 95, it shipped with Microsoft Plus! for Windows 95.
Usage.
When released in 1993, Cello was the only browser for the Microsoft Windows platform. Shortly after launch, Cello was being downloaded at a rate of 500 copies per day. As such, it achieved a fair amount of use and recognition within the legal community, including a number of PC users with between 150,000 to 200,000 users. In 1994, most websites were visited using either the Cello browser or the Mosaic browser. Despite having fewer features than Mosaic, Cello continued to be used due to its simpler interface and lower system requirements. Cello was praised for being easy to install, because it wasn't necessary to install Win32s or a TCP/IP stack for Windows 3.1. Following the release of Windows 95, which offered a much better TCP/IP interface, Cello fell into disuse and was abandoned.
By 1995, Cello, like the Mosaic browser, was overshadowed by two newer browsers: Netscape and Internet Explorer and fell into disuse. By 1999, Cello was considered to be a "historical" browser.
Cello is considered to be one of the early casualties of the Browser wars.
Features.
Cello had the following features:
Unlike Mosaic, Cello did not have toolbar buttons, and instead commands were accessed through pull-down menus.
Cello supported the following protocols: HTTP 1.0, Gopher (not Gopher+), read-only FTP, SMTP mailing, Telnet, Usenet, CSO/ph/qi directly and WAIS, HyTelnet, TechInfo, Archie, X.500, TN3270 and a number of others through public gateways.
Cello supported the following FTP servers: most Unix servers(including SunOS, System V, and Linux),IBM VM, IBM VM, VMS systems, Windows NT, QVTNet, NCSA/CUTCP/Rutgers PC servers,FTP Software PC server, HellSoft NLM for Novell.
Cello works best with a direct Ethernet connection, but it also supports SLIP and PPP dialup connections through the use of asynchronous sockets. Cello has an integrated TCP/IP runtime stack.
Release history.
The following versions were released:
Although Cello 2.0 had been announced, development ceased before a public release.
IBM released a fix for their TCP/IP V2.0 stack so that Cello would work with OS/2 WinOS/2 on 9 February 1994.
Browser Comparison Table.
The following table shows how Cello compared to browsers of its time.
Technical.
The user agent for Cello is: codice_1 so the latest one is codice_2
DDE support.
Cello featured DDE support. OLE support and DDE client support were planned, but never released.
An example of how to invoke Cello from a Microsoft Word macro.
System requirements.
Cello has the following system requirements:
Criticism.
Cello was not very stable and its development halted early.
Cello did not render graphics well and required that the user reload the webpage when resizing the window. Like most browsers at the time, Cello also did not support any web security protocols. It was also said that Cello rendered html "crudely" and pages would appear jaggedly.
Cello also had sub-par performance in accessing the Internet and processing hypermedia documents.

</doc>
<doc id="49392" url="https://en.wikipedia.org/wiki?curid=49392" title="Affirmative action">
Affirmative action

Affirmative action (known as employment equity in Canada, reservation in India and Nepal, and positive discrimination in the UK) is the policy of favoring members of a disadvantaged group who currently suffer or historically have suffered from discrimination within a culture. Often, these people are disadvantaged for historical reasons, such as oppression or slavery. Historically and internationally, support for affirmative action has sought to achieve goals such as bridging inequalities in employment and pay, increasing access to education, promoting diversity, and redressing apparent past wrongs, harms, or hindrances. The nature of affirmative action policies varies from region to region. Some countries, such as India, use a quota system, whereby a certain percentage of jobs or school vacancies must be reserved for members of a certain group. In some other regions, specific quotas do not exist; instead, members of minorities are given preference in selection processes.
The term "affirmative action" was first used in the United States in "Executive Order No.10925", signed by President John F. Kennedy on 6 March 1961, which included a provision that government contractors "take "affirmative action" to ensure that applicants are employed, and employees are treated during employment, without regard to their race, creed, color, or national origin." In 1967, gender was added to the anti-discrimination list. In 1989, the International Convention on the Elimination of All Forms of Racial Discrimination stipulated (in Article 2.2) that affirmative action programs may be required of countries that ratified the convention, in order to rectify systematic discrimination. It also states that such programs "shall in no case entail as a consequence the maintenance of unequal or separate rights for different racial groups after the objectives for which they were taken have been achieved."
In the United States, affirmative action has been the subject of numerous court cases, and in 2003, a Supreme Court decision ("Grutter v. Bollinger", 539 US 244) permitted educational institutions to consider race as a factor when admitting students. Several countries, such as India, reserve political positions for members of disadvantaged groups. In other countries, such as the United Kingdom, affirmative action is rendered illegal because it does not treat all races equally. This approach to equal treatment is described as being "color blind." In such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes described as "positive action."
Opponents of affirmative action such as George Sher believe that affirmative action devalues the accomplishments of people who are chosen based on the social group to which they belong rather than their qualifications, thus rendering affirmative action counterproductive. Opponents, who sometimes say that affirmative action is reverse discrimination, further claim that affirmative action has undesirable side-effects in addition to failing to achieve its goals. For example, the idea of "mismatching" suggests that affirmative action might place students into colleges that are too difficult for them, increasing their chances of dropping out.
Origins.
The term "affirmative action" was first used in the United States in "Executive Order No.10925", signed by President John F. Kennedy on 6 March 1961, which included a provision that government contractors "take "affirmative action" to ensure that applicants are employed, and employees are treated during employment, without regard to their race, creed, color, or national origin." It was used to promote actions that achieve non-discrimination. In 1965, President Lyndon B. Johnson issued Executive Order 11246 which required government employers to take "affirmative action" to "hire without regard to race, religion and national origin". This prevented employers from discriminating against members of disadvantaged groups.
In 1967, gender was added to the anti-discrimination list.
Affirmative action is intended to promote the opportunities of defined minority groups within a society to give them equal access to that of the majority population.
It is often instituted for government and educational settings to ensure that certain designated "minority groups" within a society are able to participate in all provided opportunities including promotional, educational, and training opportunities.
The stated justification for affirmative action by its proponents is that it helps to compensate for past discrimination, persecution or exploitation by the ruling class of a culture, and to address existing discrimination.
Women.
Several different studies investigated the effect of affirmative action on women. Kurtulus (2012) in her review of affirmative action and the occupational advancement of minorities and women during 1973-2003 showed that the effect of affirmative action on advancing black, Hispanic, and white women into management, professional, and technical occupations occurred primarily during the 1970s and early 1980s. During this period, contractors grew their shares of these groups more rapidly than noncontractors because of the implementation of affirmative action. But the positive effect of affirmative action vanished entirely in the late 1980s, which Kurtulus says may be due to the slowdown into advanced occupation for women and minorities because of the political shift of affirmative action that started by President Reagan. Becoming a federal contractor increased white women's share of professional occupations by 0.183 percentage points, or 7.3 percent, on average during these three decades, and increased black women's share by 0.052 percentage points (or by 3.9 percent). Becoming a federal contractor also increased Hispanic women's and black men's share of technical occupations on average by 0.058 percent and 0.109 percentage points respectively (or by 7.7 and 4.2 percent). These represent a substantial contribution of affirmative action to overall trends in the occupational advancement of women and minorities over the three decades under the study. A further study by Kim and Kim (2014) considered the impact of four primary factors on support for affirmative action programs for women: gender; political factors; psychological factors; and social structure. They found that, "Affirmative action both corrects existing unfair treatment and gives women equal opportunity in the future."
Quotas.
Law regarding quotas and affirmative action varies widely from nation to nation. Caste based quotas are used in Reservation in India. However, they are illegal in the United States, where no employer, university, or other entity may create a set number required for each race.
In 2012, the European Union Commission approved a plan for women to constitute 40% of non-executive board directorships in large listed companies in Europe by 2020. In Sweden, the Supreme Court has ruled that "affirmative action" ethnic quotas in universities are discrimination and hence unlawful. It said that the requirements for the intake should be the same for all. The Justice Chancellor said that the decision left no room for uncertainty.
National approaches.
In some countries that have laws on racial equality, affirmative action is rendered illegal because it does not treat all races equally. This approach of equal treatment is sometimes described as being "color blind", in hopes that it is effective against discrimination without engaging in reverse discrimination.
In such countries, the focus tends to be on ensuring equal opportunity and, for example, targeted advertising campaigns to encourage ethnic minority candidates to join the police force. This is sometimes described as "positive action."
Africa.
South Africa.
Apartheid.
The Apartheid government, as a matter of state policy, favoured white-owned, especially Afrikaner-owned companies. The aforementioned policies achieved the desired results, but in the process they marginalised and excluded black people. Skilled jobs were also reserved for white people, and blacks were largely used as unskilled labour, enforced by legislation including the Mines and Works Act, the Job Reservations Act, the Native Building Workers Act, the Apprenticeship Act and the Bantu Education Act, creating and extending the "colour bar" in South African labour. For example, in early 20th century South Africa mine owners preferred hiring black workers because they were cheaper. Then the whites successfully persuaded the government to enact laws that highly restricted the blacks' employment opportunities.
Since the 1960s the Apartheid laws had been weakened. Consequently, from 1975 to 1990 the real wages of black manufacturing workers rose by 50%, that of whites by 1%.
The economic and politically structured society during the apartheid ultimately caused disparities in employment, occupation and income within labour markets, which provided advantages to certain groups and characteristics of people. This in due course was the motivation to introduce affirmative action in South Africa, following the end of Apartheid.
Post-apartheid - the Employment Equity Act.
Following the transition to democracy in 1994, the African National Congress-led government chose to implement affirmative action legislation to correct previous imbalances (a policy known as employment equity). As such, all employers were compelled by law to employ previously disenfranchised groups (blacks, Indians, Chinese and Coloureds). A related, but distinct concept is Black Economic Empowerment.
The Employment Equity Act and the Broad Based Black Economic Empowerment Act aim to promote and achieve equality in the workplace (in South Africa termed "equity"), by advancing people from designated groups. The designated groups who are to be advanced include all people of colour, women (including white women) and people with disabilities (including whites). Employment Equity legislation requires companies employing more than 50 people to design and implement plans to improve the representativity of workforce demographics, and report them to the Department of Labour.
Employment Equity also forms part of a company's Black Economic Empowerment scorecard: in a relatively complex scoring system, which allows for some flexibility in the manner in which each company meets its legal commitments, each company is required to meet minimum requirements in terms of representation by previously disadvantaged groups. The matters covered include equity ownership, representation at employee and management level (up to board of director level), procurement from black-owned businesses and social investment programs, amongst others.
The policies of Employment Equity and, particularly, Black Economic empowerment have been criticised both by those who view them as discriminatory against white people, and by those who view them as ineffectual.
These laws cause disproportionally high costs for small companies and reduce economic growth and employment. The laws may give the black middle-class some advantage but can make the worse-off blacks even poorer. Moreover, the Supreme Court has ruled that in principle blacks may be favored, but in practice this should not lead to unfair discrimination against the others. Yet it is impossible to favor somebody without discriminating against others.
Affirmative Action Purpose.
As mentioned previously affirmative action was introduced through the Employment Equality Act, 55 in 1998, 4 years after the end of Apartheid. This act was passed to promote the constitutional right of equality and exercise true democracy. This idea was to eliminate unfair discrimination in employment, to ensure the implementation of employment equity to redress the effects of discrimination, to achieve a diverse workforce broadly representative of our people, to promote economic development and efficiency in the workforce and to give effects to the obligations of the Republic as a member of the International Labour Organisation.
Many embraced the Act; however some concluded that the act contradicted itself. The act eliminates unfair discrimination in certain sectors of the national labour market by imposing similar constraints on another.
With the introduction of Affirmative Action, Black Economic Empowerment (BEE) rose additionally in South Africa. The BEE was not a moral initiative to redress the wrongs of the past but to promote growth and strategies that aim to realize a country's full potential. The idea was targeting the weakest link in economics, which was inequality and which would help develop the economy. This is evident in the statement by the Department of Trade and Industry, "As such, this strategy stresses a BEE process that is associated with growth, development and enterprise development, and not merely the redistribution of existing wealth". Similarities between the BEE and affirmative action are apparent; however there is a difference. BEE focuses more on employment equality rather than taking wealth away from the skilled white labourers.
The main goal of Affirmative Action is for a country to reach its full potential. This occurrence would result in a completely diverse workforce in economic and social sectors. Thus broadening the economic base and therefore stimulating economic growth.
Outcomes.
Once applied within the country, many different outcomes arose, some positive and some negative. This depended on the approach and the view of The Employment Equality Act and Affirmative Action.
Positive:
Pre Democracy, the Apartheid discriminated against non-white races, so with affirmative action, the country started to redress past discriminations. Affirmative Action also focused on combating structural racism and racial inequality, hoping to maximize diversity in all levels of society and sectors. Achieving this would elevate the status of the perpetual underclass and to restore equal access to the benefits of society.
Negative:
Though Affirmative Action had its positives, negatives arose. A quota system was implemented, which aimed to achieve targets of diversity in a work force. This target affected the hiring and level of skill in the work force, ultimately affecting the free market. Affirmative action created marginalization for coloured and Indian races in South Africa, as well as developing and aiding the middle and elite classes, leaving the lower class behind. This created a bigger gap between the lower and middle class, which led to class struggles and a greater segregation. Entitlement began to arise with the growth of the middle and elite classes, as well as race entitlement. Many believe that affirmative action is discrimination in reverse. With all these negatives, much talent started to leave the country. Many negative consequences of affirmative action, specifically the quota system, drive skilled labour away, resulting in bad economic growth. This is due to very few international companies wanting to invest in South Africa.
With these negative and positive outcomes of Affirmative Action it is evident that the concept of affirmative action is a continuous and learning idea.
Asia.
China.
There is affirmative action in education for minority nationalities. This may equate to lowering minimum requirements for the National University Entrance Examination, which is a mandatory exam for all students to enter university. Some universities set quotas for minority (non-Han) student intake. Further, minority students enrolled in ethnic minority-oriented specialties (e.g. language and literature programs) are provided with scholarships and/or pay no tuition, and are granted a monthly stipend.
Israel.
A class-based affirmative action policy was incorporated into the admission practices of the four most selective universities in Israel during the early to mid-2000s. In evaluating the eligibility of applicants, neither their financial status nor their national or ethnic origins are considered. The emphasis, rather, is on structural disadvantages, especially neighborhood socioeconomic status and high school rigor, although several individual hardships are also weighed. This policy made the four institutions, especially the echelons at the most selective departments, more diverse than they otherwise would have been. The rise in geographic, economic and demographic diversity of a student population suggests that the plan's focus on structural determinants of disadvantage yields broad diversity dividends.
Israeli citizens who are; Women, Arabs, Blacks or people with disabilities are entitled to Affirmative Action in the civil service employment. Also Israeli citizens who are Arabs, Blacks or people with disabilities are entitled for Affirmative Actions are entitled for full University tuition scholarships by the state.
Izraeli in her study of gender Politics in Israel showed that the paradox of affirmative action for women directors is that the legitimation for legislating their inclusion on boards also resulted in the exclusion of women's interested as a legitimate issue on the boards' agendas. "The new culture of the men's club is seductive token women are under the pressure to become "social males" and prove that their competence as directors, meaning that they are not significantly different from men. In the negotiation for status as worthy peers, emphasizing gender signals that a woman is an "imposter," someone who does not rightfully belong in the position she is claiming to fill." And once affirmative action for women is fulfilled, and then affirmative action shares the element, as Izareli put it, the "group equality discourse," making it easier for other groups to claim for a fairer distribution of resources. This suggests that affirmative action can have applications for different groups in Israel.
India.
Reservation in India is a form of affirmative action designed to improve the well-being of backward and under-represented communities defined primarily by their caste.
Sri Lanka.
In 1971 the Standardization policy of Sri Lankan universities was introduced as an affirmative action program for students from areas which had lower rates of education than other areas due to missionary activity in the north and east, which essentially were the Tamil areas. Successive governments cultivated a historical myth after the colonial powers had left that the British had practised communal favouritism towards Christians and the minority Tamil community for the entire 200 years they had controlled Sri Lanka. However, the Sinhalese in fact benefitted from trade and plantation cultivations over the rest of the other groups and their language and culture as well as the religion of Buddhism was fostered and made into mediums for schools over the Tamil language, which did not have the same treatment and Tamils learned English instead as there was no medium for Tamil until near independence. Tamils' knowledge of English and education came from the very American missionary activity by overseas Christians that the British were concerned will anger the Sinhalese and destroy their trading relationships, so they sent them to the Tamil areas instead to teach, thinking it would have no consequences and due to their small numbers. The British sending the missionaries to the north and east was for the protection of the Sinhalese and in fact showed favouritism to the majority group instead of the minorities to maintain trading relationships and benefits from them. The Tamils, out of this random benefit from learning English and basic education excelled and flourished and were able to take many civil service jobs to the chagrin of the Sinhalese. The myth of Divide and Rule is untrue. The 'policy of standardisation' was typical of affirmative action policies, in that it required drastically lower standards for Sinhalese students than for the more academic Tamils who had to get about ten more marks to enter into universities. The policy, were it not implemented would have prevented the civil wars ahead as the policies had no basis and in fact is an example of discrimination against the Tamil ethnic group.
Malaysia.
The Malaysian New Economic Policy or NEP serves as a form of affirmative action. Malaysia provides affirmative action to the majority because in general, the Malays have lower income than the Chinese who have traditionally been involved in businesses and industries. Malaysia is a multi-ethnic country, with Malays making up the majority of close to 52% of the population. About 23% of the population are Malaysians of Chinese descent, while Malaysians of Indian descent comprise about 7% of the population. During more than 100 years of British colonization, the Malays were discriminated against employment because the British preferred to bring in influx of migrant workers from China and India.
("See also Bumiputra") The mean income for Malays, Chinese and Indians in 1957/58 were 134, 288 and 228 respectively. In 1967/68 it was 154, 329 and 245, and in 1970 it was 170, 390 and 300. Mean income disparity ratio for Chinese/Malays rose from 2.1 in 1957/58 to 2.3 in 1970, whereas for Indians/Malays the disparity ratio also rose from 1.7 to 1.8 in the same period. The Malays viewed Independence as restoring their proper place in their own country's socioeconomic order while the non-Malays were opposing government efforts to advance Malay political primacy and economic welfare.
Europe.
Finland.
In certain university education programs, including legal and medical education, there are quotas for persons who reach a certain standard of skills in the Swedish language; for students admitted in these quotas, the education is partially arranged in Swedish. The purpose of the quotas is to guarantee that a sufficient number of professionals with skills in Swedish are educated for nationwide needs. The quota system has met with criticism from the Finnish speaking majority, some of whom consider the system unfair. In addition to these linguistic quotas, women may get preferential treatment in recruitment for certain public sector jobs if there is a gender imbalance in the field.
France.
No distinctions based on race, religion or sex are allowed under the 1958 French Constitution. Since the 1980s, a French version of affirmative action based on neighborhood is in place for primary and secondary education. Some schools, in neighborhoods labeled "Priority Education Zones", are granted more funds than the others. Students from these schools also benefit from special policies in certain institutions (such as Sciences Po).
The French Ministry of Defence tried in 1990 to give more easily higher ranks and driving licenses to young French soldiers with North-African ancestry. After a strong protest by a young French lieutenant in the Ministry of Defence newspaper ("Armées d'aujourd'hui"), this driving license and rank project was cancelled. After the Sarkozy election, a new attempt in favour of Arabian-French students was made but Sarkozy did not gain enough political support to change the French constitution. However, highly ranked French schools do implement affirmative action in that they are obligated to take a certain number of students from impoverished families. <br> Additionally, following the Norwegian example, after 27 January 2014, women must represent at least 20% of board members in all stock exchange listed or state owned companies. After 27 January 2017, the proportion will increase to 40%. All male director nominations will be invalid as long as the condition is not met, and financial penalties may apply for other directors.
Germany.
Article 3 of the German Basic Law provides for equal rights of all people regardless of sex, race or social background. There are programs stating that if men and women have equal qualifications, women have to be preferred for a job; moreover, the disabled should be preferred to non-disabled people. This is typical for all positions in state and university service , typically using the phrase "We try to increase diversity in this line of work". In recent years, there has been a long public debate about whether to issue programs that would grant women a privileged access to jobs in order to fight discrimination. Germany's "Left Party" brought up the discussion about affirmative action in Germany's school system. According to Stefan Zillich, quotas should be "a possibility" to help working class children who did not do well in school gain access to a "Gymnasium" (University-preparatory school). Headmasters of "Gymnasien" have objected, saying that this type of policy would "be a disservice" to poor children.
Norway.
In all public stock companies (ASA) boards, either gender should be represented by at least 40%. This affects roughly 400 companies of over 300,000 in total.
Seierstad & Opsahl in their study of the effects of affirmative action on presence, prominence, and social capital of women directors in Norway found that there are few boards chaired by a woman, from the beginning of the implementation of affirmative action policy period to August 2009, the proportion of boards led by a woman has increased from 3.4% to 4.3%. This suggests that the law has had a marginal effect on the sex of the chair and the boards remain internally segregated. Although at the beginning of our observation period, only 7 of 91 prominent directors were women. The gender balance among prominent directors has changed considerable through the period, and at the end of the period, 107 women and 117 men were prominent directors. Interestingly, by applying more restrictive definitions of prominence, the proportion of directors who are women generally increases. If only considering directors with at least three directorships, 61.4% of them are women. When considering directors with seven or more directorships, all of them are women. Thus, affirmative action increase the female population in the director position.
Romania.
Romani people are allocated quotas for access to public schools and state universities. There is evidence that some ethnic Romanians exploit the system so they can be themselves admitted to universities, which has drawn criticism from Roma representatives.
Russia.
Quota systems existed in the USSR for various social groups including ethnic minorities (as a compensation of their "cultural backwardness"), women and factory workers.
Quotas for access to university education, offices in the Soviet system and the Communist Party existed: for example, the position of First Secretary of a Soviet Republic's (or Autonomous Republic's) Party Committee was always filled by a representative of this republic's "titular ethnicity".
Modern Russia retains this system partially. Some quotas (such as those for factory workers) are abolished, however, the quotas for women and ethnic minorities remain.
Slovakia.
The Constitutional Court declared in October 2005 that affirmative action i.e. "providing advantages for people of an ethnic or racial minority group" as being against its Constitution.
United Kingdom.
In the UK, any discrimination, quotas or favouritism due to sex, race and ethnicity among other "protected characteristics" is generally illegal for any reason in education, employment, during commercial transactions, in a private club or association, and while using public services. The Equality Act 2010 established the principles of equality and their implementation in the UK.
Specific exemptions include:
North America.
Canada.
The equality section of the Canadian Charter of Rights and Freedoms explicitly permits affirmative action type legislation, although the Charter does not "require" legislation that gives preferential treatment. Subsection 2 of Section 15 states that the equality provisions do "not preclude any law, program or activity that has as its object the amelioration of conditions of disadvantaged individuals or groups including those that are disadvantaged because of race, national or ethnic origin, colour, religion, sex, age or mental or physical disability."
The Canadian Employment Equity Act requires employers in federally-regulated industries to give preferential treatment to four designated groups: Women, people with disabilities, aboriginal people, and visible minorities. In most Canadian Universities, people of Aboriginal background normally have lower entrance requirements and are eligible to receive exclusive scholarships. Some provinces and territories also have affirmative action-type policies. For example, in Northwest Territories in the Canadian north, aboriginal people are given preference for jobs and education and are considered to have P1 status. Non-aboriginal people who were born in the NWT or have resided half of their life there are considered a P2, as well as women and people with disabilities.
United States.
The concept of affirmative action was introduced in the early 1960s in the United States, as a way to combat racial discrimination in the hiring process and, in 1967, the concept was expanded to include sex. Affirmative action was first created from Executive Order 10925, which was signed by President John F. Kennedy on 6 March 1961 and required that government employers "not discriminate against any employee or applicant for employment because of race, creed, color, or national origin" and "take affirmative action to ensure that applicants are employed, and that employees are treated during employment, without regard to their race, creed, color, or national origin".
On 24 September 1965, President Lyndon B. Johnson signed Executive Order 11246, thereby replacing Executive Order 10925 and affirming Federal Government's commitment "to promote the full realization of equal employment opportunity through a positive, continuing program in each executive department and agency". Affirmative action was extended to women by Executive Order 11375 which amended Executive Order 11246 on 13 October 1967, by adding "sex" to the list of protected categories. In the U.S. affirmative action's original purpose was to pressure institutions into compliance with the nondiscrimination mandate of the Civil Rights Act of 1964. The Civil Rights Acts do not cover veterans, people with disabilities, or people over 40. These groups are protected from discrimination under different laws.
Affirmative action has been the subject of numerous court cases, and has been questioned upon its constitutional legitimacy. In 2003, a Supreme Court decision regarding affirmative action in higher education ("Grutter v. Bollinger", 539 US 244 – Supreme Court 2003) permitted educational institutions to consider race as a factor when admitting students. Alternatively, some colleges use financial criteria to attract racial groups that have typically been under-represented and typically have lower living conditions. Some states such as California (California Civil Rights Initiative), Michigan (Michigan Civil Rights Initiative), and Washington (Initiative 200) have passed constitutional amendments banning public institutions, including public schools, from practicing affirmative action within their respective states. Conservative activists have alleged that colleges quietly use illegal quotas to increase the number of minorities and have launched numerous lawsuits to stop them.
Oceania.
New Zealand.
Individuals of Māori or other Polynesian descent are often afforded improved access to university courses, or have scholarships earmarked specifically for them. Affirmative action is provided for under section 73 of the Human Rights Act 1993 and section 19(2) of the New Zealand Bill of Rights Act 1990.
South America.
Brazil.
Some Brazilian Universities (State and Federal) have created systems of preferred admissions (quotas) for racial minorities (blacks and native Brazilians), the poor and people with disabilities. There are also quotas of up to 20% of vacancies reserved for people with disabilities in the civil public services. The Democrats party, accusing the board of directors of the University of Brasília of "Nazism", appealed to the Supreme Federal Court against the constitutionality of the quotas the University reserves for minorities. The Supreme Court unanimously approved their constitutionality on 26 April 2012.
International organizations.
United Nations.
The International Convention on the Elimination of All Forms of Racial Discrimination stipulates (in Article 2.2) that affirmative action programs may be required of countries that ratified the convention, in order to rectify systematic discrimination. It states, however, that such programs "shall in no case entail as a consequence the maintenance of unequal or separate rights for different racial groups after the objectives for which they were taken have been achieved."
The United Nations Human Rights Committee states that "the principle of equality sometimes requires States parties to take affirmative action in order to diminish or eliminate conditions which cause or help to perpetuate discrimination prohibited by the Covenant. For example, in a State where the general conditions of a certain part of the population prevent or impair their enjoyment of human rights, the State should take specific action to correct those conditions. Such action may involve granting for a time to the part of the population concerned certain preferential treatment in specific matters as compared with the rest of the population. However, as long as such action is needed to correct discrimination, in fact, it is a case of legitimate differentiation under the Covenant."
Support.
The principle of affirmative action is to promote societal equality through the preferential treatment of socioeconomically disadvantaged people. Often, these people are disadvantaged for historical reasons, such as oppression or slavery.
Historically and internationally, support for affirmative action has sought to achieve a range of goals: bridging inequalities in employment and pay; increasing access to education; enriching state, institutional, and professional leadership with the full spectrum of society; redressing apparent past wrongs, harms, or hindrances, in particular addressing the apparent social imbalance left in the wake of slavery and slave laws.
Polls.
According to a poll taken by "USA Today" in 2005, majority of Americans support affirmative action for women, while views on minority groups were more split. Men are only slightly more likely to support affirmative action for women; though a majority of both do. However, a slight majority of Americans do believe that affirmative action goes beyond ensuring access and goes into the realm of preferential treatment. More recently, a Quinnipiac poll from June 2009 finds that 55% of Americans feel that affirmative action in general should be discontinued, though 55% support it for people with disabilities. A Gallup poll from 2005 showed that 72% of black Americans and 44% of white Americans supported racial affirmative action (with 21% and 49% opposing), with support and opposition among Hispanics falling between those of blacks and whites. Support among blacks, unlike among whites, had almost no correlation with political affiliation.
A 2009 Quinnipiac University Polling Institute survey found 65% of American voters opposed the application of affirmative action to gay people, with 27% indicating they supported it.
A Leger poll taken in 2010 found 59% of Canadians opposed considering race, gender, or ethnicity when hiring for government jobs.
Criticism.
Opponents of affirmative action such as George Sher believe that affirmative action devalues the accomplishments of people who are chosen based on the social group to which they belong rather than their qualifications, thus rendering affirmative action counterproductive. Opponents, who sometimes say that affirmative action is reverse discrimination, further claim that affirmative action has undesirable side-effects in addition to failing to achieve its goals. They argue that it hinders reconciliation, replaces old wrongs with new wrongs, undermines the achievements of minorities, and encourages individuals to identify themselves as disadvantaged, even if they are not. It may increase racial tension and benefit the more privileged people within minority groups at the expense of the least fortunate within majority groups (such as lower-class white people).
Opponents claim that cases such as "Fisher v. University of Texas" are few of the many examples that show how reverse discrimination can take place. In 2008, Abigail Fisher, who is a native to Texas, sued the University of Texas at Austin, claiming that she was denied admission to the university because she was "white". The students that are of top 10% in the applicants of the University of Texas are admitted and there are students that compete to barely make it in on the threshold, such as Abigail Fisher. In such cases, race becomes an important factor in deciding who gets admitted to the university, and Fisher argued that discriminating and accepting students according to their race is a violation of the Equal Protection Clause of the Fourteenth Amendment, which ensures equal protection of the law and the citizen's privilege as a citizen of United States. The constitutionality of affirmative action in college admissions is now before the Supreme Court in the 2013 landmark case "Fisher v. University of Texas".
American economist, social and political commentator, Dr. Thomas Sowell identified some negative results of race-based affirmative action in his book, "Affirmative Action Around the World: An Empirical Study". Sowell writes that affirmative action policies encourage non-preferred groups to designate themselves as members of preferred groups (i.e., primary beneficiaries of affirmative action) to take advantage of group preference policies; that they tend to benefit primarily the most fortunate among the preferred group (e.g., upper and middle class blacks), often to the detriment of the least fortunate among the non-preferred groups (e.g., poor whites or Asians); that they reduce the incentives of both the preferred and non-preferred to perform at their best – the former because doing so is unnecessary and the latter because it can prove futile – thereby resulting in net losses for society as a whole; and that they increase animosity toward preferred groups.
Mismatching.
Mismatching is the term given to the negative effect that affirmative action has when it places a student into a college that is too difficult for him or her. For example, according to the theory, in the absence of affirmative action, a student will be admitted to a college that matches his or her academic ability and have a good chance of graduating. However, according to the mismatching theory, affirmative action often places a student into a college that is too difficult, and this increases the student's chance of dropping out. Thus, according to the theory, affirmative action hurts its intended beneficiaries, because it increases their dropout rate.
Evidence in support of the mismatching theory was presented by Gail Heriot, a professor of law at the University of San Diego and a member of the U.S. Commission on Civil Rights, in an 24 August 2007 article published in the "Wall Street Journal". The article reported on a 2004 study that was conducted by UCLA law professor Richard Sander and published in the "Stanford Law Review". The study concluded that there were 7.9% fewer black attorneys than there would have been if there had been no affirmative action. The study was titled, "A Systemic Analysis of Affirmative Action in American Law Schools." The article also states that because of mismatching, blacks are more likely to drop out of law school and fail bar exams.
Sander's paper on mismatching has been criticized by several law professors, including Ian Ayres and Richard Brooks from Yale who argue that eliminating affirmative action would actually reduce the number of black lawyers by 12.7%.
A 2011 study proposed that mismatch can only occur when a selective school possesses private information that, had this information been disclosed, would have changed the student's choice of school. The study found that this is in fact the case for Duke University, and that this information predicts the student's academic performance after beginning college.

</doc>
<doc id="49393" url="https://en.wikipedia.org/wiki?curid=49393" title="Office of National Assessments">
Office of National Assessments

The Office of National Assessments (ONA) is an Australian intelligence agency. ONA was established by the Office of National Assessments Act 1977 as an independent body directly accountable to the Prime Minister of Australia. ONA provides all-source assessments on international political, strategic and economic developments to the Prime Minister and senior ministers in the National Security Committee of Cabinet. It also coordinates and evaluates the work and performance of Australia’s foreign intelligence agencies. ONA is not an intelligence collection agency.
ONA's financial framework is governed by the Financial Management and Accountability Act 1997 (FMA Act).
Locations.
In October 2011, ONA moved into the Robert Marsden Hope Building, a refurbished building in the Parliamentary Triangle. The building is named for Justice Hope, who led two Royal Commissions into Australia's intelligence and security agencies and operations, the first of which led to the creation of ONA. Before its move, ONA had been a sub-tenant in the Central Office building of the Australian Security Intelligence Organisation (ASIO) in Russell, Canberra.
The Director-General of ONA is an independent statutory officer who is not subject to external direction on the content of ONA assessments. ONA has about 150 staff, including 100 analysts. The current Director-General of ONA is Richard Maude, who was appointed to a five-year term at the agency in April 2013.
Role.
The Office of National Assessments is not a producer of intelligence; it draws on intelligence collected by DIO, ASIS, ASIO, DIGO and DSD, and combines it with open source research to produce assessments on key issues for the Prime Minister and senior ministers. The assessments are designed to assist the Australian Government in strategic decision making and ensure that government is fully briefed on emerging threats both in the region and globally. 
In the media.
Although not a secret organisation, ONA usually attracts little attention. However, a striking exception occurred in 2001 when the former Prime Minister, John Howard, publicly relied upon an ONA assessment to support his claims about asylum seekers on the MV "Tampa", in an incident which became known as the "Tampa affair". The ONA assessment was later leaked to the public in its entirety, showing that the assessment was ultimately based on nothing more than press releases from various government ministers.
In 2003, in the lead-up to the 2003 invasion of Iraq, an ONA intelligence officer named Andrew Wilkie resigned from the agency, citing ethical concerns in relation to selective and exaggerated use of intelligence by the Australian Government on the matter of Iraq and weapons of mass destruction.
Flood report.
ONA has experienced substantial growth since the release of the report into intelligence agencies by Philip Flood which recommended a doubling of the agency's budget and staffing resources and formalisation of the agency's role as a coordinator and evaluator of the other Australian foreign intelligence agencies. The only ONA specific recommendation not implemented from the Flood report was the renaming of ONA to the Australian Foreign Intelligence Assessment Agency (AFIAA).
Branches.
The ONA is divided into branches which cover geographic or thematic areas. It does not have a graduate intake, but recruits from other intelligence agencies, academia, Defence and DFAT for its analysts.

</doc>
<doc id="49396" url="https://en.wikipedia.org/wiki?curid=49396" title="Volunteer (botany)">
Volunteer (botany)

In gardening and agronomic terminology, a volunteer is a plant that grows on its own, rather than being deliberately planted by a farmer or gardener. Volunteers often grow from seeds that float in on the wind, are dropped by birds, or are inadvertently mixed into compost. Unlike weeds, which are unwanted plants, a volunteer may be encouraged by gardeners once it appears, being watered, fertilized, or otherwise cared for. The action of such plants – to sprout or grow in this fashion – may also be described as volunteering.
Volunteers that grow from the seeds of specific cultivars are not reliably identical or similar to their parent, and often differ significantly from it. Such open pollinated plants, if they show desirable characteristics, may be selected to become new cultivars.
Agriculture.
In agricultural rotations, self-set plants from the previous year's crop may become established as weeds in the current crop. For example, volunteer winter wheat will germinate to quite high levels in a following oilseed rape crop, usually requiring chemical control measures. In agricultural research, high purity of a harvested crop is usually desirable. To achieve this, typically a group of temporary workers will walk the crop rows looking for volunteer plants, or "rogue" plants in an exercise often referred to as "roguing."

</doc>
<doc id="49397" url="https://en.wikipedia.org/wiki?curid=49397" title="Battle of Chosin Reservoir">
Battle of Chosin Reservoir

The Battle of Chosin Reservoir, also known as the Chosin Reservoir Campaign or the Changjin Lake Campaign (; ), was a decisive battle in the Korean War. "Chosin" is the Japanese pronunciation of the Korean name, "Changjin". The UN forces relied on Japanese language maps dating from their occupation of Korea which had only ended five years earlier at the conclusion of World War II. Shortly after the People's Republic of China entered the conflict, the People's Volunteer Army 9th Army infiltrated the northeastern part of North Korea.
On 27 November, the Chinese 9th Army surprised the US X Corps commanded by Major General Edward Almond at the Chosin Reservoir area. A brutal 17 day battle in freezing weather soon followed. In the period between 27 November and 13 December 1950, 30,000 United Nations troops (later nicknamed "The Chosin Few") under the field command of Major General Oliver P. Smith were encircled and attacked by approximately 120,000 Chinese troops under the command of Song Shi-Lun, who had been ordered by Mao Zedong to destroy the UN forces. The UN forces were nonetheless able to make a fighting withdrawal and broke out of the encirclement while inflicting crippling losses on the Chinese. While the battle resulted in the Chinese pushing the UN out of North Korea, it was a Pyrrhic victory. The evacuation of the X Corps from the port of Hungnam marked the complete withdrawal of UN troops from North Korea.
Background.
By mid-1950 after the successful landing at Inchon by the US X Corps and the subsequent destruction of the Korean People's Army, the Korean War appeared to be all but over. United Nations (UN) forces advanced rapidly into North Korea with the intention of reuniting North and South Korea before the end of 1950. North Korea is divided through the center by the impassable Taebaek Mountains, which separated the UN forces into two groups. The US Eighth Army advanced north through the western coast of the Korean Peninsula, while the Republic of Korea (ROK) I Corps and the US X Corps advanced north on the eastern coast.
At the same time the People's Republic of China entered the conflict after issuing several warnings to the United Nations. On 19 October 1950, large formations of Chinese troops, dubbed the People's Volunteer Army (PVA), secretly crossed the border and into North Korea. One of the first Chinese units to reach the Chosin Reservoir area was the PVA 42nd Corps, and it was tasked with stopping the eastern UN advances. On 25 October, the advancing ROK I Corps made contact with the Chinese and halted at Funchilin Pass, south of the Chosin Reservoir. After the landing at Wonsan, the US 1st Marine Division of the X Corps engaged the defending PVA 124th Division on 2 November, and the ensuing battle caused heavy casualties among the Chinese. On 6 November, the PVA 42nd Corps ordered a retreat to the north with the intention of luring the UN forces into the Chosin Reservoir. By 24 November, the 1st Marine Division occupied both Sinhung-ni on the eastern side of the reservoir, and Yudami-ni on the west side of the reservoir.
Faced with the sudden attacks by Chinese forces in the Eighth Army sector, General Douglas MacArthur ordered the Eighth Army to launch the Home-by-Christmas Offensive. To support the offensive, MacArthur ordered the X Corps to attack west from the Chosin Reservoir and to cut the vital Manpojin—Kanggye—Huichon supply line. As a response, Major General Edward M. Almond, commander of the US X Corps, formulated a plan on 21 November. It called for the US 1st Marine Division to advance west through Yudami-ni, while the US 7th Infantry Division would provide a regimental combat team to protect the right flank at Sinhung-ni. The US 3rd Infantry Division would also protect the left flank while providing security in the rear area. By then the X Corps was stretched thin along a front.
Surprised by the Marine landing at Wonsan, China's Chairman Mao Zedong called for the immediate destruction of the ROK Capital Division, ROK 3rd Infantry Division, US 1st Marine Division, and US 7th Infantry Division in a telegraph to Commander Song Shi-Lun of the PVA 9th Army on 31 October. Under Mao's urgent orders, the 9th Army was rushed into North Korea on 10 November. Undetected by UN intelligence, the 9th Army quietly entered the Chosin Reservoir area on 17 November, with the 20th Corps of the 9th Army relieving the 42nd Corps near Yudami-ni.
Prelude.
Location, terrain and weather.
Chosin Reservoir is a man-made lake located in the northeast of the Korean peninsula. The name Chosin is the Japanese pronunciation of the Korean place name Changjin, and the name stuck due to the outdated Japanese maps used by UN forces. The battle's main focus was around the long road that connects Hungnam and Chosin Reservoir, which served as the only retreat route for the UN forces. Through these roads, Yudami-ni and Sinhung-ni, located at the west and east side of the reservoir respectively, are connected at Hagaru-ri. From there, the road passes through Koto-ri and eventually leads to the port of Hungnam. The area around the Chosin Reservoir was sparsely populated.
The battle was fought over some of the roughest terrain during some of the harshest winter weather conditions of the Korean War. The road was created by cutting through the hilly terrain of Korea, with steep climbs and drops. Dominant peaks, such as the Funchilin Pass and the Toktong Pass, overlook the entire length of the road. The road's quality was poor, and in some places it was reduced to a one lane gravel trail. On 14 November, a cold front from Siberia descended over the Chosin Reservoir, and the temperature plunged to as low as . The cold weather was accompanied by frozen ground, creating considerable danger of frostbite casualties, icy roads, and weapon malfunctions. Medical supplies froze; morphine syrettes had to be defrosted in a medic's mouth before they could be injected; frozen blood plasma was useless on the battlefield. Even cutting off clothing to deal with a wound risked gangrene and frostbite. Batteries used for the Jeeps and radios did not function properly in the temperature and quickly ran down. The lubrication in the guns gelled and rendered them useless in battle. Likewise, the springs on the firing pins would not strike hard enough to fire the round, or would jam.
Forces and strategies.
Although the 1st Marine Division landed at Wonsan as part of Almond's US X Corps, Almond and Major General Oliver P. Smith of the 1st Marine Division shared a mutual loathing of each other that dated back to a meeting before the landing at Inchon, during which Almond had spoken of how easy amphibious landings are even though he had never been involved in one. Smith believed that there were large numbers of Chinese forces in North Korea despite the fact that higher headquarters in Tokyo had said otherwise, while Almond felt Smith was overly cautious. The mutual distrust between the two commanders made Smith slow the 1st Marine Division's advance towards the Chosin Reservoir against Almond's instructions. Along the way Smith established supply points and airfields at Hagaru-ri and Koto-ri.
While the US X Corps was pushing towards the reservoir, the Chinese formulated their strategy based on their experiences in the Chinese Civil War. Working from the assumption that only a light UN presence would be at the reservoir, the 9th Army was to first destroy the UN garrisons at Yudami-ni and Sinhung-ni, then push towards Hagaru-ri. Believing that the bulk of the US X Corps would scramble to rescue the destroyed units, the 9th Army would then block and trap the main UN forces on the road between Hagaru-ri and Hungnam. The 9th Army initially committed eight divisions for the battle, with most of the forces concentrated at Yudami-ni and Sinhung-ni.
The flaw in the Chinese plan was a lack of accurate intelligence on the UN forces. Although the US X Corps was stretched thin over northeast Korea, the slow Marine advance allowed the bulk of the US 1st Marine Division, including the 5th, 7th and 11th Marines, to be concentrated at Yudami-ni. Conversely, the strategically important Hagaru-ri, which contained an airfield and a supply dump, was not a priority for the Chinese despite being lightly defended by the 1st and the 7th Marines. Only the Regimental Combat Team 31, an understrength and hastily formed regimental combat team of the US 7th Infantry Division, was thinly spread along the eastern bank of the reservoir. Those units would later take the brunt of the Chinese assaults. As for the UN strength, the 1st Marine Division had an effective strength of 25,473 men at the start of the battle, and it was further reinforced by the British 41 Royal Marine Commando and the equivalent strength of two regiments from the 3rd and the 7th Infantry Divisions. Thus the UN forces had an approximate strength of 30,000 during the course of the battle. The UN forces at Chosin were also supported by one of the greatest concentrations of air power during the Korean War, in which the 1st Marine Air Wing stationed at Yonpo Airfield and five aircraft carriers from the US Navy Task Force 77 were able to launch 230 sorties daily to provide close air support during the battle, while the US Air Force Far East Combat Cargo Command in Japan reached the capacity of airdropping 250 tons of supplies per day to resupply the trapped UN forces.
Although the 9th Army was one of China's elite formations composed of veterans and former POWs from the Huaihai Campaign, several deficiencies hampered its ability during the battle. Initially the 9th Army was intended to be outfitted in Manchuria during November, but Mao suddenly ordered it into Korea before that could happen. As the result, the 9th Army received almost no winter gear for the harsh Korean winter. Similarly, poor logistics forced the 9th Army to abandon heavy artillery, while working with little food and ammunition. The food shortage forced the 9th Army to initially station a third of its strength away from the Chosin Reservoir as reserve, and starvation and exposure soon broke out among the Chinese units as foraging was not an option at the sparsely populated reservoir. By the end of the battle, more Chinese troops died from the cold than from combat and air raids.
As for the Chinese strength, it is normally assumed that the Chinese had 120,000 troops for the battle, due to the fact that the 9th Army were composed of 12 divisions with a strength of 10,000+ men per division. Before arriving in Korea, the 9th Army was reinforced. Each of its three Corps now had four divisions instead of the regular three. Infantry from two formerly liberated (surrendered) Nationalist divisions were absorbed to bring each infantry company up to strength. Some companies had ~150 men, but some companies were reinforced with more than 200 men. On average, each division of the 9th Army had more infantry troops than other Chinese divisions.
Eventually all 12 Chinese division of the 9th Army were deployed. Eight divisions of the PVA 20th and 27th Corps served as the main attacking force. Four divisions of the PVA 26th Corps initially were held back in reserve, and deployed after 20th and 27the Corps exhausted all available strength.
Battle.
On the night of 27 November, the PVA 20th and 27th Corps of the 9th Army launched multiple attacks and ambushes along the road between the Chosin Reservoir and Koto-ri. At Yudam-ni, the 5th, 7th and 11th Marines were surrounded and attacked by the PVA 79th and 89th Divisions, with the 59th Division attacking the road between Yudam-ni and Hagaru-ri to cut off communication. Similarly, RCT-31 was isolated and ambushed at Sinhung-ni by the PVA 80th, 81st and 94th Divisions. At Hagaru-ri, the 1st Marine Division command headquarters was attacked by the PVA 58th Division. Finally, the PVA 60th Division surrounded elements of the 1st Marines at Koto-ri from the north. Caught by complete surprise, the UN forces were cut off at Yudam-ni, Sinhung-ni, Hagaru-ri and Koto-ri by 28 November.
Actions at Yudam-ni.
Acting on Almond's instruction, Smith ordered the 5th Marines to attack west toward Mupyong-ni on 27 November. The attack was soon stalled by the PVA 89th Division and forced the Marines to dig in on the ridges surrounding Yudam-ni. As night came, three Chinese regiments of the 79th Division attacked the ridges on the north and northwest of Yudam-ni, hoping to annihilate the garrison in one stroke. Close range fighting soon developed as the attackers infiltrated Marine positions, but the 5th and 7th Marines held the line while inflicting heavy casualties to the Chinese. As day broke on 28 November, the Chinese forces and the American defenders were locked in a stalemate around the Yudam-ni perimeter.
While the battle was underway at Yudam-ni, the PVA 59th Division blocked the road between Yudam-ni and Hagaru-ri by attacking the defending Charlie and Fox Companies of the 7th Marines. The successful assault forced Charlie Company to retreat into Yudam-ni which left Fox Company trapped in Toktong Pass, a vital pass that controlled the road. On 29 November, several efforts by the 7th Marines failed to rescue Fox Company despite inflicting heavy casualties on the Chinese. Aided by artillery from Hagaru-ri and Marine Corsair fighters, Fox Company managed to hold out for five days while enduring constant attacks by the PVA 59th Division.
After the heavy losses suffered by the PVA 79th Division at Yudam-ni, 9th Army headquarters realized that the bulk of the 1st Marine Division was stationed at Yudam-ni, with a garrison strength that was double the initial estimate. Believing that any further assaults would be futile, Song Shi-Lun ordered the 9th Army to switch their main attacks toward Sinhung-ni and Hagaru-ri, leaving Yudam-ni alone from 28 November to 30 November. At the same time, the US Eighth Army on the Korean western front was forced into full retreat at the Battle of the Ch'ongch'on River, and MacArthur ordered Almond to withdraw the US X Corps to the port of Hungnam. Acting on the instruction of Almond and Smith, Lieutenant Colonel Raymond L. Murray and Colonel Homer L. Litzenberg, commanders of the 5th and 7th Marines, respectively, issued a joint order to break out from Yudam-ni to Hagaru-ri on 30 November. Faced with tough fighting between the blocking Chinese divisions and the withdrawing Marines, Smith remarked: "Retreat, hell! We're not retreating, we're just advancing in a different direction."
For the breakout, the Marines formed into a convoy with a single M4A3 Sherman tank as the lead. The plan was to have 3rd Battalion, 5th Marines (3/5) as the vanguard of the convoy, with three battalions covering the rear. At the same time, 1st Battalion, 7th Marines (1/7) would attack towards Fox Company in order to open the road at Toktong Pass. To start the breakout, 3rd Battalion, 7th Marines (3/7) had to first attack south and capture Hill 1542 and Hill 1419 in order to cover the road from Chinese attacks. The breakout was carried out under the air cover of the 1st Marine Air Wing.
On the morning of 1 December, 3rd Battalion, 7th Marines (3/7) engaged the PVA 175th Regiment of the 59th Division at Hill 1542 and Hill 1419. The tenacious Chinese defenders soon forced the Marines to dig in on the slopes between the road and the peaks when the convoy passed 3/7's position by the afternoon. With Hagaru-ri still not captured, the PVA High Command scrambled the 79th Division to resume attacks on Yudam-ni while the 89th Division rushed south towards Koto-ri. The Chinese struck at night, and the ferocious fighting forced the rear covering forces to call in night fighters to suppress the attacks. The fighting lasted well into the morning of 2 December until all the Marines managed to withdraw from Yudam-ni.
At the same time, 1st Battalion, 7th Marines (1/7) also tried to break the Chinese blockade at Hill 1419 on 1 December. Despite being badly reduced by combat, hunger and frostbite, the PVA 59th Division sent in its last five platoons and refused to yield. As night approached, 1/7 finally captured the peak and started to march through the hills on the east side of the road. Relying on the element of surprise, they managed to destroy several Chinese positions along the road. On the morning of 2 December, a joint attack by Fox Company and 1/7 secured the Toktong Pass, thus opening the road between Yudam-ni and Hagaru-ri.
Although the road had been opened between Yudam-ni and Hagaru-ri, the convoy still had to fight through the numerous Chinese positions on the hills overlooking the road. On the first night of the retreat, the Chinese struck the convoy in force and inflicted heavy casualties upon 3rd Battalion, 5th Marines (3/5). Although strong air cover suppressed most of the Chinese forces for the rest of the march, the cold weather, harassing fire, raiding parties, and road blocks slowed the retreat to a crawl while inflicting numerous casualties. Despite those difficulties, the convoy reached Hagaru-ri in an orderly fashion on the afternoon of 3 December, with the withdrawal completed on 4 December.
East of the reservoir.
Regimental Combat Team 31 (RCT-31), later known as "Task Force Faith", was a hastily formed regimental combat team from the 7th Infantry Division that guarded the right flank of the Marine advance towards Mupyong-ni. Before the battle, RCT-31 was spread thin with main elements separated on the hills north of Sinhung-ni, the inlet west of Sinhung-ni, and the town of Hudong-ni south of Sinhung-ni. Although the Chinese believed RCT-31 to be a reinforced regiment, the task force was actually under strength with one battalion missing, due to the bulk of the 7th Infantry Division being scattered over northeast Korea.
On the night of 27 November, three regiments from the 80th Division attacked the northern hills and the inlets, completely surprising the defenders. The ensuing battle inflicted heavy casualties on the 1st Battalion, 32nd Infantry to the north of Sinhung-ni, while the 57th Field Artillery Battalion and the 3rd Battalion, 31st Infantry were almost overrun at the inlet. The Chinese also sent the 242nd Regiment of the 81st Division towards Hill 1221, an undefended hill that controlled the road between Sinhung-ni and Hudong-ni. As the night's fighting ended, RCT-31 was separated into three elements.
Believing that the defenders were completely destroyed at the inlet, the Chinese stopped their attacks and proceeded to loot the US positions for food and clothing. As the morning came on 28 November, the 3rd Battalion, 31st Infantry counterattacked the PVA 239th Regiment at the inlet, sending the surprised Chinese back in a complete rout. In the afternoon, Almond flew into the perimeter of RCT-31, convinced that RCT-31 was strong enough to begin its attack north and deal with whatever "remnants" of Chinese forces that were in their way. Almond ordered Colonel Allan D. Maclean, the commander of RCT-31, to resume the offensive north while presenting Silver Stars to three of Maclean's officers. In disgust, Lieutenant Colonel Don C. Faith, Jr., the commander of the 1st Battalion, 32nd Infantry, threw his medal into the snow.
On the night of 28 November, the PVA 80th Division attacked again with four regiments. At the inlet, the Chinese assault became a disaster as communications broke down while devastating fire from the anti-aircraft (AA) guns attached to the 57th Field Artillery Battalion swept the Chinese ranks. In the aftermath of the fighting, the PVA 238th and the 239th Regiment together had less than 600 soldiers. The attacks by PVA 240th Regiment, on the other hand, forced Maclean to order a retreat from the northern hills towards the inlet. On 29 November, the 1st Battalion managed to break through the Chinese blockade and reached the inlet, but Maclean disappeared as he mistook some Chinese soldiers as American. The Chinese finally stopped their attacks on the night of 29 November while waiting for fresh reinforcements.
While RCT-31 was under siege, Almond finally instructed the 1st Marine Division to rescue RCT-31 by breaking out of Yudam-ni—an impossible order for Smith to implement. Only the 31st Tank Company tried to rescue RCT-31 by attacking Hill 1221, but without infantry support, the two armored attacks on 28 and 29 November were stalled by slippery roads, rough terrain, and close infantry assaults. By 30 November the US forces evacuated Hudong-ni in order to defend Hagaru-ri, leaving the rest of RCT-31 completely stranded.
On 30 November, Major General David G. Barr, the commander of the 7th Infantry Division, flew into the Sinhung-ni inlet and met with Faith, who by now had assumed command of RCT-31. Faith expressed the difficulties for a breakout, particularly the 500 wounded that RCT-31 had to carry. On the same day, parts of the PVA 94th Division and the rest of 81st Divisionarrived as reinforcements for the 80th Division. By midnight, six Chinese regiments renewed their attacks and Zhan Danan, the commander of the 80th Division, ordered the complete destruction of RCT-31 before dawn. Again, the 57th Battalion's AA guns held the Chinese at bay, but the shell supplies were running desperately low. On the day of 1 December, Faith finally ordered RCT-31 to breakout from Sinhung-ni and withdraw to Hagaru-ri.
The breakout began as soon as the weather allowed the 1st Marine Air Wing to provide air cover on 1 December. As the soldiers formed a convoy and tried to leave the perimeter, the PVA 241st Regiment immediately swarmed over the American forces, with three other regiments closing in. Left with no choice, the covering aircraft dropped napalm right in front of RCT-31, causing casualties among both Chinese and US troops. The resulting firestorm wiped out the blocking Chinese company, allowing the convoy to advance. As the front of RCT-31 made their way forward, heavy small arms fire caused many members of the rear guard to seek shelter below the road instead of protecting the trucks. Chinese fire also killed or wounded those already in the trucks as well as the drivers, who viewed the job as a form of suicide. Slowly, the convoy approached a roadblock under Hill 1221 in the late afternoon. Several parties tried to clear Hill 1221, but after taking part of the hill, the leaderless soldiers continued out onto the frozen reservoir instead of returning to the column. As Faith led an assault on the roadblock, he was hit by a Chinese grenade and subsequently died of his wounds. The convoy managed to fight past the first road block, but as it reached the second at Hudong-ni, RCT-31 disintegrated under Chinese attacks. About 1,050 soldiers out of the original 2,500 managed to reach Hagaru-ri, and only 385 survivors were deemed able-bodied. The remnants of RCT-31 were formed into a provisional army battalion for the rest of the battle.
Actions at Hagaru-ri.
To support the Marine attack towards Mupyong-ni, Hagaru-ri became an important supply dump with an airfield under construction. Smith and 1st Marine Division headquarters were also located at Hagaru-ri. With the bulk of the 1st Marine Division gathered at Yudam-ni, Hagaru-ri was lightly defended by two battalions from the 1st and 7th Marines, the rest of the garrison being composed of engineers and rear support units from both the Army and the Marine Corps.
The original Chinese plan called for the 58th Division to attack Hagaru-ri on the night of 27 November, but the division became lost in the countryside due to the outdated Japanese maps it used. It was not until the dawn of 28 November that the 58th Division arrived at Hagaru-ri. Meanwhile, from the fighting and ambushes that had occurred the previous night, the garrison at Hagaru-ri noticed the Chinese forces around them. Lieutenant Colonel Thomas L. Ridge, commander of 3rd Battalion, 1st Marines (3/1), predicted the Chinese attack would come on the night of 28 November. Almost everyone, including rear support units with little combat training, was pressed into the front line due to the manpower shortage, and the entire perimeter was on full alert by 21:30.
It was not long before the PVA 173rd Regiment attacked the western and the southern perimeter, while the 172nd Regiment struck the hills on the northern perimeter. Despite the preparations, the understrength garrison was overwhelmed, with the Chinese opening several gaps in the defenses and reaching the rear areas. The resulting chaos, however, caused a breakdown in discipline among the Chinese soldiers, who began looting food and clothing instead of exploiting the situation. The defending Americans managed to destroy the Chinese forces in counterattacks, while a breakdown of communications between the Chinese regiments allowed the gaps to close. When the fighting stopped, the Chinese had only gained the East Hill on the northern perimeter. Another attack was planned for the night of 29 November, but air raids by VMF-542 broke up the Chinese formations before it could be carried out.
Given the critical manpower shortage at Hagaru-ri, on November 29, Smith ordered Colonel Lewis "Chesty" Puller of the First Marine Regiment to assemble a task force to be sent north from Koto-ri to open the road south of Hagaru-ri. In response, a task force was formed with 921 troops from the 41 Royal Marine Commando, G Company of the 1st Marines and B Company of the 31st Infantry. The task force was dubbed "Task Force Drysdale" after its commander Lieutenant Colonel Douglas B. Drysdale, who also commanded 41 Commando. On the afternoon of 29 November, Task Force Drysdale pushed north from Koto-ri while under constant attack from the PVA 60th Division. The task force's harrowing experience later earned the road the nickname "Hell Fire Valley". As the Chinese attacks dragged on, the task force became disorganized, and a destroyed truck in the convoy later split the task force into two segments. Although the lead segment of the task force fought its way into Hagaru-ri on the night of 29 November, the rear segment was destroyed. Despite suffering 159 wounded and 162 dead and missing, the task force managed to bring in 300 badly needed infantrymen for the defense at Hagaru-ri.
As more reinforcements arrived from Hudong-ni on 30 November, the garrisons attempted to recapture the East Hill. All efforts failed despite the destruction of a Chinese company. When darkness settled, the PVA 58th Division gathered its remaining 1,500 soldiers in a last-ditch attempt to capture Hagaru-ri. The reinforced defenders annihilated most of the attacking forces, with only the defences around the East Hill giving way. As the Chinese tried to advance from the East Hill, they were cut down by the 31st Tank Company.
By 1 December, the PVA 58th Division was virtually destroyed, with the remainder waiting for reinforcements from the 26th Corps of the 9th Army. But much to the frustration of Song Shi-Lun, the 26th Corps did not arrive before the Marines broke out of Yudam-ni. The airfield was opened to traffic on 1 December, allowing UN forces to bring in reinforcements and to evacuate the dead and the wounded. With the Marines at Yudam-ni completing their withdrawal on 4 December, the trapped UN forces could finally start their breakout towards the port of Hungnam.
Breakout.
After a short period of rest, the breakout began on 6 December with the 7th Marines as the vanguard of the retreating column while the 5th Marines covered the rear. At the same time, the much-delayed PVA 26th Corps arrived at Hagaru-ri with its 76th and 77th Division relieving the 58th and 60th Divisions. As the 7th Marines pushed aside the PVA 76th Division south of Hagaru-ri, the 5th Marines took over the Hagaru-ri perimeter and recaptured the East Hill from the 76th Division. In a last effort to stop the breakout, the customary Chinese night attack returned with the 76th and 77th Division striking the Hagaru-ri perimeter from all directions. The Marines repulsed the Chinese attacks, inflicting heavy casualties.
Meanwhile, the 7th Marines opened the road between Hagaru-ri and Koto-ri by capturing the high ground surrounding the road. But as soon as the Marines pulled out, the 77th Division returned to the peaks and attacked the column. Chaotic fighting broke out within the column and the retreat was slowed to a crawl. The Marine night fighters, however, returned to subdue the Chinese forces, and the fighting destroyed most of the blocking troops. On 7 December, the rest of the column managed to reach Koto-ri with little difficulty with the last elements arrived at Koto-ri that night.
After the failure of the 26th Corps at Hagaru-ri, the PVA High Command ordered the 26th and the 27th Corps to chase the escaping UN force with the 20th Corps blocking the escape route. But with most of the 20th Corps destroyed at Yudam-ni and Hagaru-ri, the only forces between Koto-ri and Hungnam were the remnants of the 58th and the 60th Divisions. In desperation, Song Shi-Lun ordered these troops to dig in at Funchilin Pass while blowing up the vital treadway bridge, hoping the terrain and obstacles would allow the 26th and the 27th Corps to catch up with the retreating UN forces. The PVA 180th Regiment that occupied Hill 1081 blew up the original concrete bridge and two improvised replacements in succession, believing the bridge was rendered irreparable. In response, 1st Battalion, 1st Marines (1/1) attacked Hill 1081 from the south, and the hill was captured on 9 December after the defenders fought to the last man. At the same time, the 7th Marines and RCT-31 attacked the treadway bridge from the north, only to encounter defenders that were already frozen in their foxholes.
With the path to Hungnam blocked at Funchilin Pass, eight C-119 Flying Boxcars flown by the US 314th Troop Carrier Wing were used to drop portable bridge sections by parachute. The bridge, consisting of eight separate long, sections, was dropped one section at a time, using a parachute on each section. Four of these sections, together with additional wooden extensions were successfully reassembled into a replacement bridge by Marine Corps combat engineers and the US Army 58th Engineer Treadway Bridge Company on 9 December, enabling UN forces to proceed. Outmaneuvered, the PVA 58th and 60th Divisions still tried to slow the UN advance with ambushes and raids, but after weeks of non-stop fighting, the two Chinese divisions combined had only 200 soldiers left. The last UN forces left Funchilin Pass by 11 December.
One of the last engagements during the withdrawal was an ambush at Sudong by the pursuing PVA 89th Division, which Task Force Dog of the 3rd Infantry Division repulsed with little difficulty. The trapped UN forces finally reached the Hungnam perimeter by 21:00 on 11 December.
Evacuation at Hungnam.
By the time the UN forces arrived at Hungnam, MacArthur had already ordered the evacuation of the US X Corps on 8 December in order to reinforce the US Eighth Army, which by then was badly depleted and retreating rapidly towards the 38th parallel. Following his orders, the ROK I Corps, the ROK 1st Marine Regiment, the US 3rd Infantry Division and the US 7th Infantry Division had also set up defensive positions around the port. Some skirmishes broke out between the defending US 7th, 17th and 65th Infantry and the pursuing PVA 27th Corps, but against the strong naval fire support provided by US Navy Task Force 90, the badly mauled 9th Army was in no shape to approach the Hungnam perimeter.
In what US historians called the "greatest evacuation movement by sea in US military history", a 193-ship armada assembled at the port and evacuated not only the UN troops, but also their heavy equipment and roughly a third of the Korean refugees. One Victory ship, the "SS Meredith Victory" evacuated 14,000 refugees. The last UN unit left at 14:36 on 24 December, and the port was destroyed to deny its use to the Chinese and North Korean forces. The PVA 27th Corps entered Hungnam on the morning of 25 December.
Aftermath.
While the US X Corps was being evacuated from the eastern front, the US Eighth Army had already retreated to the 38th parallel on the western front in the aftermath of the Battle of the Ch'ongch'on River. With the entire UN front collapsing, the race to the Yalu was ended with the communist forces of China recapturing much of North Korea. The Korean War would drag on for another two and a half years before the armistice was signed on 27 July 1953. Besides the loss of North Korea, the US X Corps and the ROK I Corps later reported a total of 10,495 battle casualties, of which 4,385 were from the US Marines, 3,163 were from the US Army, 2,812 were from South Koreans attached to American formations and 78 were from the British Royal Marines. Outside of the combat losses, the 1st Marine Division also reported 7,338 non-battle casualties due to the cold weather.
Despite the losses, the US X Corps preserved much of its strength. About 105,000 soldiers, 98,000 civilians, 17,500 vehicles, and 350,000 tons of supplies were shipped from Hungnam to Pusan, and they would later rejoin the war effort in Korea. Smith was credited for saving the US X Corps from destruction, while the 1st Marine Division, the 41 Royal Marine Commando and the Army's RCT-31 were awarded the Presidential Unit Citation for their tenacity during the battle. Fourteen Marines, two Soldiers and one Navy pilot received the Medal of Honor, and all of the UN troops that served at Chosin were later honored with the nickname "The Chosin Few". On 15 September 2010, the Veterans of the Korean War Chosin Reservoir Battle memorial was unveiled by the United States Marine Corps Commandant General James T. Conway at Camp Pendleton.
China was also catapulted into the status of a major military power following the victory at Chosin, but the victory came with a staggering cost. With the escape of the US X Corps and the ROK I Corps, Mao's vision for Chosin was not realized, and the failure caused Song Shi-Lun to offer his resignation. At the same time, heavy casualties caused by both combat and poor logistical support destroyed much of the eight elite divisions under the 20th and the 27th Corps. Of those eight divisions, two divisions were forced to disband, and not until March 1951 did the 9th Army return to its normal strength and become combat effective. With the absence of nearly 40 percent of the Chinese forces in Korea in early 1951, the heavy Chinese losses at Chosin ultimately enabled the UN forces to maintain a foothold in Korea.
Operation Glory.
During the battle, UN casualties were buried at temporary grave sites along the road. Operation Glory took place from July to November 1954, during which the dead of each side were exchanged. The remains of 4,167 US Soldiers and Marines were exchanged for 13,528 North Korean and Chinese dead. In addition, 546 civilians who died in UN prisoner of war camps were turned over to the South Korean government. After Operation Glory, 416 Korean War "unknowns" were buried in the National Memorial Cemetery of the Pacific. According to a Defense Prisoner of War/Missing Personnel Office (DPMO) white paper, 1,394 names were also transmitted during "Operation Glory" from the Chinese and North Koreans, of which 858 proved to be correct. The 4,167 returned remains were found to be 4,219 individuals, of whom 2,944 were found to be Americans, with all but 416 identified by name. Of the 239 Korean War unaccounted for, 186 are not associated with the Punchbowl unknowns. From 1990 to 1994 North Korea excavated and returned more than 208 sets of remains which possibly include 200 to 400 US servicemen, but very few have been identified due to the co-mingling of remains. From 2001 to 2005, more remains were recovered from the Chosin Battle site, and around 220 were recovered near the Chinese border between 1996 to 2006.
Notes.
Footnotes
Citations

</doc>
<doc id="49399" url="https://en.wikipedia.org/wiki?curid=49399" title="XY sex-determination system">
XY sex-determination system

The XY sex-determination system is the sex-determination system found in humans, most other mammals, some insects ("Drosophila"), and some plants ("Ginkgo"). In this system, the sex of an individual is determined by a pair of sex chromosomes (gonosomes). Females have two of the same kind of sex chromosome (XX), and are called the homogametic sex. Males have two distinct sex chromosomes (XY), and are called the heterogametic sex.
This system is in contrast with the ZW sex-determination system found in birds, some insects, many reptiles, and other animals, in which the heterogametic sex is female.
A temperature-dependent sex determination system is found in some reptiles.
Mechanisms.
All animals have a set of DNA coding for genes present on chromosomes. In humans, most mammals, and some other species, two of the chromosomes, called the X chromosome and Y chromosome, code for sex. In these species, one or more genes are present on their Y-chromosome that determine maleness. In this process, an X chromosome and a Y chromosome act to determine the sex of offspring, often due to genes located on the Y chromosome that code for maleness. Offspring have two sex chromosomes: an offspring with two X chromosomes will develop female characteristics, and an offspring with an X and a Y chromosome will develop male characteristics.
Humans.
In humans, a single gene ("SRY") present on the Y chromosome acts as a signal to set the developmental pathway towards maleness. Presence of this gene starts off the process of virilization. This and other factors result in the sex differences in humans. The cells in females, with two X chromosomes, undergo X-inactivation, in which one of the two X chromosomes is inactivated. The inactivated X chromosome remains within a cell as a Barr body.
Humans, as well as some other organisms, can have a chromosomal arrangement that is contrary to their phenotypic sex; for example, XX males or XY females (see androgen insensitivity syndrome). Additionally, an abnormal number of sex chromosomes (aneuploidy) may be present, such as Turner's syndrome, in which a single X chromosome is present, and Klinefelter's syndrome, in which two X chromosomes and a Y chromosome are present, XYY syndrome and XXYY syndrome. Other less common chromosomal arrangements include: triple X syndrome, 48, XXXX, and 49, XXXXX.
Other animals.
XY system in mammals: Sex is determined by presence of Y. "Female" is the default sex; due to the absence of the Y. In the 1930s, Alfred Jost determined that the presence of testosterone was required for Wolffian duct development in the male rabbit.
SRY is an intronless sex-determining gene on the Y chromosome in the therians (placental mammals and marsupials). Non-human mammals use several genes on the Y-chromosome. Not all male-specific genes are located on the Y-chromosome. Other species (including most "Drosophila" species) use the presence of two X chromosomes to determine femaleness. One X chromosome gives putative maleness. The presence of Y-chromosome genes is required for normal male development.
Other systems.
Birds and many insects have a similar system of sex determination ("ZW sex-determination system"), in which it is the females that are heterogametic (ZW), while males are homogametic (ZZ).
Many insects of the order Hymenoptera instead have a system (the "haplo-diploid sex-determination system"), where the males are haploid individuals (which just one chromosome of each type), while the females are diploid (with chromosomes appearing in pairs). Some other insects have the "X0 sex-determination system", where just one chromosome type appears in pairs for the female but alone in the males, while all other chromosomes appear in pairs in both sexes.
Influences.
Genetic.
For a long time, biologists believed that the female form was the default template for the mammalian fetuses of both sexes. After the discovery of the testis-determining gene SRY, many scientists shifted to the theory that the genetic mechanism that determines a fetus to develop into a male form was initiated by the SRY gene, which was thought to be responsible for the production of testosterone and its overall effects on body and brain development. This perspective still shared the classical way of thinking; that in order to produce two sexes, nature has developed a default female pathway and an active pathway by which male genes would initiate the process of determining a male sex, as something that is developed in addition to and based on the default female form. This view is no longer considered accurate by most scientists who study the genetics of sex. In an interview for the "Rediscovering Biology" website, researcher Eric Vilain described how the paradigm changed since the discovery of the SRY gene:
In mammals, including humans, the SRY gene is responsible with triggering the development of non-differentiated gonads into testes, rather than ovaries. However, there are cases in which testes can develop in the absence of an SRY gene (see sex reversal). In these cases, the SOX9 gene, involved in the development of testes, can induce their development without the aid of SRY. In the absence of SRY and SOX9, no testes can develop and the path is clear for the development of ovaries. Even so, the absence of the SRY gene or the silencing of the SOX9 gene are not enough to trigger sexual differentiation of a fetus in the female direction. A recent finding indicates that ovary development and maintenance is an active process, regulated by the expression of a "pro-female" gene, FOXL2. In an interview for the "TimesOnline" edition, study co-author Robin Lovell-Badge explained the significance of the discovery:
Implications.
Looking into the genetic determinants of human sex can have wide-ranging consequences. Scientists have been studying different sex determination systems in fruit flies and animal models to attempt an understanding of how the genetics of sexual differentiation can influence biological processes like reproduction, ageing and disease.
Maternal.
In humans and many other species of animals, the father determines the sex of the child. In the XY sex-determination system, the female-provided ovum contributes an X chromosome and the male-provided sperm contributes either an X chromosome or a Y chromosome, resulting in female (XX) or male (XY) offspring, respectively.
Hormone levels in the male parent affect the sex ratio of sperm in humans. Maternal influences also impact which sperm are more likely to achieve conception.
Human ova, like those of other mammals, are covered with a thick translucent layer called the zona pellucida, which the sperm must penetrate to fertilize the egg. Once viewed simply as an impediment to fertilization, recent research indicates the zona pellucida may instead function as a sophisticated biological security system that chemically controls the entry of the sperm into the egg and protects the fertilized egg from additional sperm.
Recent research indicates that human ova may produce a chemical which appears to attract sperm and influence their swimming motion. However, not all sperm are positively impacted; some appear to remain uninfluenced and some actually move away from the egg.
Maternal influences may also be possible that affect sex determination in such a way as to produce fraternal twins equally weighted between one male and one female.
The time at which insemination occurs during the oestrus cycle has been found to affect the sex ratio of the offspring of humans, cattle, hamsters, and other mammals. Hormonal and pH conditions within the female reproductive tract vary with time, and this affects the sex ratio of the sperm that reach the egg.
Sex-specific mortality of embryos also occurs.
History.
Ancient ideas on sex determination.
Since ancient times, people have believed that the sex of an infant is determined by how much heat a man's sperm had during insemination. Aristotle wrote that:
Aristotle claimed that the male principle was the driver behind sex determination, such that if the male principle was insufficiently expressed during reproduction, the fetus would develop as a female. In contrast, modern genetics has developed a view on sex determination in which no one single factor is responsible for determining sex; a number of pro-male, anti-male and pro-female genes being responsible, though the largest factor is whether the male's gamete carries an X or Y chromosome.
Beginnings of genetics of sex determination.
Edmund Beecher Wilson and Nettie Stevens are credited with discovering, in 1905, the chromosomal XY sex-determination system; the fact that males have XY sex chromosomes and females have XX sex chromosomes. 
The first clues to the existence of a factor that determines the development of testis in mammals came from experiments carried out by Alfred Jost, who castrated embryonic rabbits in utero and noticed that they all developed as female. 
In 1959, C. E. Ford and his team, in the wake of Jost's experiments, discovered that the Y chromosome was needed for a fetus to develop as male when they examined patients with Turner's syndrome, who grew up as phenotypic females, and found them to be X0 (hemizygous for X and no Y). At the same time, Jacob & Strong described a case of a patient with Klienfelter's syndrome (XXY), which implicated the presence of a Y chromosome in development of maleness.
All these observations lead to a consensus that a dominant gene that determines testis development (TDF) must exist on the human Y chromosome. The search for this testis-determining factor (TDF) led a team of scientists in 1990 to discover a region of the Y chromosome that is necessary for the male sex determination, which was named SRY (Sex-determining Region of the Y chromosome).

</doc>
<doc id="49400" url="https://en.wikipedia.org/wiki?curid=49400" title="Window">
Window

A window is an opening in a wall, door, roof or vehicle that allows the passage of light and, if not closed or sealed, air and sound.
Modern windows are usually glazed or covered in some other transparent or translucent material. Windows are held in place by frames. Many glazed windows may be opened, to allow ventilation, or closed, to exclude inclement weather. Windows often have a latch or similar mechanism to lock the window shut.
Types include the eyebrow window, fixed windows, single-hung and double-hung sash windows, horizontal sliding sash windows, casement windows, awning windows, hopper windows, tilt and slide windows (often door-sized), tilt and turn windows, transom windows, sidelight windows, jalousie or louvered windows, clerestory windows, skylights, roof windows, roof lanterns, bay windows, oriel windows, thermal, or Diocletian, windows, picture windows, emergency exit windows, stained glass windows, French windows, and double- and triple paned windows.
The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt, in Alexandria ca. 100 AD. Paper windows were economical and widely used in ancient China, Korea and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century. Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were perfected.
Etymology.
The English language-word "window" originates from the Old Norse 'vindauga', from 'vindr – wind' and 'auga – eye', i.e., "wind eye". In Norwegian Nynorsk and Icelandic the Old Norse form has survived to this day (in Icelandic only as a less used synonym to "gluggi"), in Swedish the word "vindöga" remains as a term for a hole through the roof of a hut, and in the Danish language 'vindue' and Norwegian Bokmål 'vindu', the direct link to 'eye' is lost, just like for 'window'. The Danish (but not the Bokmål) word is pronounced fairly similarly to "window".
"Window" is first recorded in the early 13th century, and originally referred to an unglazed hole in a roof. "Window" replaced the Old English "eagþyrl", which literally means 'eye-hole,' and 'eagduru' 'eye-door'. Many Germanic languages however adopted the Latin word 'fenestra' to describe a window with glass, such as standard Swedish 'fönster', or German 'Fenster'. The use of "window" in English is probably because of the Scandinavian influence on the English language by means of loanwords during the Viking Age. In English the word "fenester" was used as a parallel until the mid-18th century. "Fenestration" is still used to describe the arrangement of windows within a façade, as well as "defenestration", meaning to throw something out of a window.
History.
The earliest windows were just holes in a wall. Later, windows were covered with animal hide, cloth, or wood. Shutters that could be opened and closed came next. Over time, windows were built that both protected the inhabitants from the elements and transmitted light, using multiple small pieces of translucent material (such as flattened pieces of translucent animal horn, thin slices of marble, or pieces of glass) set in frameworks of wood, iron or lead. In the Far East, paper was used to fill windows.
The Romans were the first known to use glass for windows, a technology likely first produced in Roman Egypt. Namely, in Alexandria ca. 100 AD cast glass windows, albeit with poor optical properties, began to appear, but these were small thick productions, little more than blown glass jars (cylindrical shapes) flattened out into sheets with circular striation patterns throughout. It would be over a millennium before a window glass became transparent enough to see through clearly, as we think of it now.
Over the centuries techniques were developed to shear through one side of a blown glass cylinder and produce thinner rectangular window panes from the same amount of glass material. This gave rise to tall narrow windows, usually separated by a vertical support called a mullion. Mullioned glass windows were the windows of choice among European well-to-do, whereas paper windows were economical and widely used in ancient China, Korea and Japan. In England, glass became common in the windows of ordinary homes only in the early 17th century whereas windows made up of panes of flattened animal horn were used as early as the 14th century.
Modern-style floor-to-ceiling windows became possible only after the industrial plate glass making processes were perfected. Modern windows are usually filled with glass, although a few are transparent plastic.
Types.
Eyebrow.
The term eyebrow window is used in two ways: a curved top window in a wall or in an eyebrow dormer; and a row of small windows usually under the front eaves such as the James-Lorah House in Pennsylvania.
Fixed.
A window that cannot be opened, whose function is limited to allowing light to enter (unlike an unfixed window, which can open and close). Clerestory windows are often fixed. Transom windows may be fixed or operable. This type of window is used in situations where light or vision alone is needed as no ventilation is possible windows without the use of trickle vents or overglass vents.
Single-hung sash.
One sash is movable (usually the bottom one) and the other fixed. This is the earlier form of sliding sash window, and is also cheaper.
Double-hung sash.
A sash window is the traditional style of window in the United Kingdom, and many other places that were formerly colonized by the UK, with two parts (sashes) that overlap slightly and slide up and down inside the frame. The two parts are not necessarily the same size. Currently most new double-hung sash windows use spring balances to support the sashes, but traditionally, counterweights held in boxes on either side of the window were used. These were and are attached to the sashes using pulleys of either braided cord or, later, purpose-made chain. Three types of spring balances are called a tape or clock spring balance; channel or block-and-tackle balance; and a spiral or tube balance.
Double-hung sash windows were traditionally often fitted with shutters. Sash windows can be fitted with simplex hinges that let the window be locked into hinges on one side, while the rope on the other side is detached—so the window can be opened for fire escape or cleaning.
Horizontal sliding sash.
Has two or more sashes that overlap slightly but slide horizontally within the frame. In the UK, these are sometimes called "Yorkshire" sash windows, presumably because of their traditional use in that county.
Casement.
A window with a hinged sash that swings in or out like a door comprising either a side-hung, top-hung (also called "awning window"; see below), or occasionally bottom-hung sash or a combination of these types, sometimes with fixed panels on one or more sides of the sash. In the USA, these are usually opened using a crank, but in parts of Europe they tend to use projection friction stays and espagnolette locking. Formerly, plain hinges were used with a casement stay. Handing applies to casement windows to determine direction of swing; a casement window may be left-handed, right-handed, or double. The casement window is the dominant type now found in modern buildings in the UK and many other parts of Europe.
Awning.
An awning window is a casement window that is hung horizontally, hinged on top, so that it swings outward like an awning. In addition to being used independently, they can be stacked, several in one opening, or combined with fixed glass. They are particularly useful for ventilation.
Hopper.
A hopper window is a bottom-pivoting casement window that opens by tilting vertically, typically to the inside.
Pivot.
A window hung on one hinge on each of two opposite sides which allows the window to revolve when opened. The hinges may be mounted top and bottom (Vertically Pivoted) or at each jamb (Horizontally Pivoted). The window will usually open initially to a restricted position for ventilation and, once released, fully reverse and lock again for safe cleaning from inside. Modern pivot hinges incorporate a friction device to hold the window open against its own weight and may have restriction and reversed locking built in. In the UK, where this type of window is most common, they were extensively installed in high-rise social housing.
Tilt and slide.
A window (more usually a door-sized window) where the sash tilts inwards at the top and then slides horizontally behind the fixed pane.
Tilt and turn.
A "tilt and turn" window can both tilt inwards at the top or open inwards from hinges at the side. This is the most common type of window in Germany, its country of origin. It is also widespread in many other European countries. In Europe it is usual for these to be of the "turn first" type. i.e. when the handle is turned to 90 degrees the window opens in the side hung mode. With the handle turned to 180 degrees the window opens in bottom hung mode. Most usually in the UK the windows will be "tilt first" i.e. bottom hung at 90 degrees for ventilation and side hung at 180 degrees for cleaning the outer face of the glass from inside the building.
Transom.
A window above a door; in an exterior door the "transom window is often fixed, in an interior door it can open either by hinges at top or bottom, or rotate on hinges. It provided ventilation before forced air heating and cooling. A fan-shaped transom is known as a fanlight, especially in the British Isles.
Side light.
Windows beside a door or window are called "side"-, "wing"-, and "margen-lights" and "flanking windows".
Jalousie window.
Also known as a louvered window, the jalousie window consists of parallel slats of glass or acrylic that open and close like a Venetian blind, usually using a crank or a lever. They are used extensively in tropical architecture. A jalousie door is a door with a jalousie window.
Clerestory.
A window set in a roof structure or high in a wall, used for daylighting.
Skylight.
A flat or slope window used for daylighting, built into a roof structure that is out of reach.
Roof.
A sloped window used for daylighting, built into a roof structure. It is one of the few windows that could be used as an exit. Larger roof windows meet building codes for emergency evacuation.
Roof lantern.
A roof lantern is a multi-paned glass structure, resembling a small building, built on a roof for day or moon light. Sometimes includes an additional clerestory. May also be called a cupola.
Bay.
A multi-panel window, with at least three panels set at different angles to create a protrusion from the wall line.
Oriel.
This form of bay window most often appears in Tudor-style houses and monasteries. It projects from the wall and does not extend to the ground. Originally a form of porch, they are often supported by brackets or corbels.
Thermal.
Thermal, or Diocletian, windows are large semicircular windows (or niches) which are usually divided into three lights (window compartments) by two mullions. The central compartment is often wider than the two side lights on either side of it.
Picture.
A picture window is a large fixed window in a wall, typically without glazing bars, or glazed with only perfunctory glazing bars near the edge of the window. Picture windows provide an unimpeded view, as if framing a picture.
Multi-lite.
A window glazed with small panes of glass separated by wooden or lead "glazing bars", or "muntins", arranged in a decorative "glazing pattern" often dictated by the building's architectural style. Due to the historic unavailability of large panes of glass, the multi-lit (or "lattice window") was the most common window style until the beginning of the 20th century, and is still used in traditional architecture.
Emergency exit/egress.
A window big enough and low enough so that occupants can escape through the opening in an emergency, such as a fire. In many countries, exact specifications for emergency windows in bedrooms are given in many building codes. Specifications for such windows may also allow for the entrance of emergency rescuers. Vehicles, such as buses and aircraft, frequently have emergency exit windows as well.
Stained glass.
A window composed of pieces of colored glass, transparent, translucent or opaque, frequently portraying persons or scenes. Typically the glass in these windows is separated by lead glazing bars. Stained glass windows were popular in Victorian houses and some Wrightian houses, and are especially common in churches.
French.
A French window (when hinged "French door") is a large door-sized lattice light, typically set in pairs or multiples thereof. Known as "porte-fenêtre" in France and "portafinestra" in Italy, they often overlook a terrace and are commonly used in modern houses.
Double-paned.
Double-paned windows have two parallel panes (slabs of glass) with a separation of typically about 1 cm; this space is permanently sealed and filled at the time of manufacture with dry air or other dry nonreactive gas. Such windows provide a marked improvement in thermal insulation (and usually in acoustic insulation as well) and are resistant to fogging and frosting caused by temperature differential. They are widely used for residential and commercial construction in intemperate climates. Triple-paned windows have been commercially manufactured and marketed with claims of additional benefit but have not become common.
Terms.
EN 12519 is the European norm that describes windows terms officially used in EU Member States.
The main terms are:
Labeling.
The United States NFRC Window Label lists the following terms:
The European harmonised standard hEN 14351-1, which deals with doors and windows, defines 23 characteristics (divided into "essential" and "non "essential". Two other, preliminary European Norms that are under development deal with internal pedestrian doors (prEN 14351-2), smoke and fire resisting doors, and openable windows (prEN 16034).
Construction.
Windows can be a significant source of heat transfer. Therefore, insulated glazing units consist of two or more panes to reduce the transfer of heat.
Grids or muntins.
These are the pieces of framing that separate a larger window into smaller panes. In older windows, large panes of glass were quite expensive, so muntins let smaller panes fill a larger space. In modern windows, light-colored muntins still provide a useful function by reflecting some of the light going through the window, making the window itself a source of diffuse light (instead of just the surfaces and objects illuminated within the room). By increasing the indirect illumination of surfaces near the window, muntins tend to brighten the area immediately around a window and reduce the contrast of shadows within the room.
Frame and sash construction.
Frames and sashes can be made of the following materials:
Composites may combine materials to obtain aesthetics of one material with the functional benefits of another.
A special class of PVC window frames, uPVC window frames, became widespread since the late 20th century, particularly in Europe: there were 83.5 million installed by 1998 with numbers still growing as of 2012.
Glazing and filling.
Low-emissivity coated panes reduce heat transfer by radiation, which, depending on which surface is coated, helps prevent heat loss (in cold climates) or heat gains (in warm climates).
High thermal resistance can be obtained by evacuating or filling the insulated glazing units with gases such as argon or krypton, which reduces conductive heat transfer due to their low thermal conductivity. Performance of such units depends on good window seals and meticulous frame construction to prevent entry of air and loss of efficiency.
Modern double-pane and triple-pane windows often include one or more low-e coatings to reduce the window's U-factor (its insulation value, specifically its rate of heat loss). In general, soft-coat low-e coatings tend to result in a lower solar heat gain coefficient (SHGC) than hard-coat low-e coatings.
Modern windows are usually glazed with one large sheet of glass per sash, while windows in the past were glazed with multiple panes separated by "glazing bars", or "muntins", due to the unavailability of large sheets of glass. Today, glazing bars tend to be decorative, separating windows into small panes of glass even though larger panes of glass are available, generally in a pattern dictated by the architectural style at use. Glazing bars are typically wooden, but occasionally lead glazing bars soldered in place are used for more intricate glazing patterns.
Other construction details.
Many windows have movable window coverings such as blinds or curtains to keep out light, provide additional insulation, or ensure privacy.
Windows allow natural light to enter, but too much can have negative effects such as glare and heat gain. Additionally, while windows let the user see outside, there must be a way to maintain privacy on in the inside. Window coverings are practical accommodations for these issues.
Windows and the sun.
Sun incidence angle.
Historically, windows are designed with surfaces parallel to vertical building walls. Such a design allows considerable solar light and heat penetration due to the most commonly occurring incidence of sun angles. In passive solar building design, an extended eave is typically used to control the amount of solar light and heat entering the window(s).
An alternative method is to calculate an optimum window mounting angle that accounts for summer sun load minimization, with consideration of actual latitude of the building. This process has been implemented, for example, in the Dakin Building in Brisbane, California—in which most of the fenestration is designed to reflect summer heat load and help prevent summer interior over-illumination and glare, by canting windows to nearly a 45 degree angle.
Solar window.
Photovoltaic windows not only provide a clear view and illuminate rooms, but also convert sunlight to electricity for the building. In most cases, translucent photovoltaic cells are used.
Passive solar.
Passive solar windows allow light and solar energy into a building while minimizing air leakage and heat loss. Properly positioning these windows in relation to sun, wind, and landscape—while properly shading them to limit excess heat gain in summer and shoulder seasons, and providing thermal mass to absorb energy during the day and release it when temperatures cool at night—increases comfort and energy efficiency. Properly designed in climates with adequate solar gain, these can even be a building's primary heating system.
Window coverings.
A window covering is a shade or screen that provides multiple functions. For example, some window coverings control solar heat gain and glare. There are external shading devices and internal shading devices. Low-e window film is a low-cost alternative to window replacement to transform existing poorly-insulating windows into energy-efficient windows. For high-rise buildings, smart glass can provide an alternative.

</doc>
<doc id="49401" url="https://en.wikipedia.org/wiki?curid=49401" title="Hall">
Hall

In architecture, a hall is a relatively large space enclosed by a roof and walls. In the Iron Age, a mead hall was such a simple building and was the residence of a lord and his retainers. Later, rooms were partitioned from it, and the space next to the front door became the entrance hall. Today, the (entrance) hall of a house is the space next to the front door or vestibule leading to the rooms directly and/or indirectly. Where the hall inside the front door of a house is elongated, it may be called a passage, corridor (from Spanish "corredor" used in El Escorial and 100 years later in Castle Howard) or hallway.
Other meanings.
The term "hall" is often used to designate a British or Irish country house such as a hall house, or specifically a Wealden hall house, and manor houses.
In later medieval Europe, the main room of a castle or manor house was the great hall. In a medieval building, the hall was where the fire was kept. With time, its functions as dormitory, kitchen, parlour and so on were divided off to separate rooms or, in the case of the kitchen, a separate building.
The Hall and parlor house was found in England and was a fundamental, historical floor plan in parts of the United States from 1620 to 1860.
Many buildings at colleges and universities are formally titled "_______ Hall", typically being named after the person who endowed it, for example, King's Hall, Cambridge. Others, such as Lady Margaret Hall, Oxford, commemorate respected people. Between these in age, Nassau Hall at Princeton University began as the single building of the then college. In medieval origin, these were the halls in which the members of the university lived together during term time. In many cases, some aspect of this community remains.
At colleges in the universities of Oxford and Cambridge, Hall is the dining hall for students, with High Table at one end for fellows. Typically, at "Formal Hall", gowns are worn for dinner during the evening, whereas for "informal Hall" they are not.
A hall is also a building consisting largely of a principal room, that is rented out for meetings and social affairs. It may be privately or government-owned, such as a function hall owned by one company used for weddings and cotillions (organized and run by the same company on a contractual basis) or a community hall available for rent to anyone, such as a British village hall.
In religious architecture, as in Islamic architecture, the prayer hall is a large room dedicated to the practice of the worship. (example : the prayer hall of the Great Mosque of Kairouan in Tunisia). A hall church is a church with nave and side aisles of approximately equal height. Many churches have an associated church hall used for meetings and other events.
Following a line of similar development, in office buildings and larger buildings (theatres, cinemas etc.), the entrance hall is generally known as the foyer (the French for fireplace). The atrium, a name sometimes used in public buildings for the entrance hall, was the central courtyard of a Roman house.
Types.
In architecture, the head "double-loaded" describe corridors that connects to rooms on both sides. Conversely, a single-loaded corridor only has rooms on one side (and possible windows on the other). A blind corridor doesn't lead anywhere.

</doc>
<doc id="49402" url="https://en.wikipedia.org/wiki?curid=49402" title="Closet">
Closet

A closet (especially in North American usage) is an enclosed space, a cabinet, or a cupboard in a house or building used for general storage or hanging or storing clothes.
Modern closets can be built into the walls of the house during construction so that they take up no apparent space in the bedroom, or they can be large, free-standing pieces of furniture designed for clothing storage, in which case they are often called wardrobes or armoires. Closets are often built under stairs, thereby using awkward space that would otherwise go unused.
In current British and Pakistan usage, a "wardrobe" can also be built-in, and the words "cupboard" or walk-in-wardrobe can be used to refer to a closet. In Elizabethan and Middle English, "closet" referred to a larger room in which a person could sit and read in private, but now refers to a small room in general.
In Indian usage, a closet often refers to a toilet. This probably originated from the word "water closet", which refers to a flush toilet.
Types.
Broom closet: A closet with top to bottom space used for storing brooms, mops, vacuums, cleaning supplies, buckets, etc.
Coat closet: A closet located near the front door. Usually used to store coats, jackets, hoodies, sweatshirts, gloves, hats, scarfs, and boots/shoes. This kind of closet sometimes have shelving. It only has a rod and some buttom space used for clothes stored in boxes or drawers.
Linen closet: A tall, narrow closet with shelves in a bathroom used for storing towels,sheets,washcloths, and toiletries.
Utility closet: A utility closet is a closet most commonly used to house appliances and cleaning supplies. Most of the time, you may find a hot water heater and possibly the furnace. The closet may have shelves for storing appliances on top where they are out of the way.
Walk-in closet: A walk-in closet is a closet where someone walks in to store things. They may have lighting, walls, and a floor from other spaces. The walk-in closet can have hinged, bi-fold, or sliding doors.
Wall closet: A wall closet is a closet in a bedroom that is built into the wall. It may be closed by curtains or folding doors, which clothes can be stored folded on shelves.
Wardrobe: A wardrobe is a small closet used for storing clothes.
Pantry: A pantry is a closet or cabinet in a kitchen used for storing food, dishes, linens, and provisions. The closet may have shelves for putting food on.
Closet tax question in colonial America.
Though some sources claim that colonial American houses often lacked closets because of a "closet tax" imposed by the British crown, others argue that closets were absent in most houses simply because their residents had few possessions.
Closet organizers.
Closet organizers are integrated shelving systems. Different materials have advantages and disadvantages:

</doc>
