<doc id="52630" url="https://en.wikipedia.org/wiki?curid=52630" title="History of Russia (1991–present)">
History of Russia (1991–present)

The history of Russia from 1991 to the present began with the dissolution of the Soviet Union on 26 December 1991, and the establishment of the Russian Federation.
The Russian Federation was the largest of the fifteen republics that made up the Soviet Union, accounting for over 60% of the gross domestic product (GDP) and over 50% of the Soviet population. Russians also dominated the Soviet military and the Communist Party (CPSU). Thus, the Russian Federation was widely accepted as the Soviet Union's successor state in diplomatic affairs and it assumed the USSR's permanent membership and veto in the UN Security Council (see "Russia and the United Nations").
Despite this acceptance, the Russian Federation lacked the military and political power of the former Soviet Union. Russia managed to make the other former Soviet republics voluntarily disarm themselves of nuclear weapons and concentrated them under the command of the still effective rocket and space forces, but for the most part the Russian army and fleet were in near disarray by 1992. Prior to the dissolution of the Soviet Union, Boris Yeltsin had been elected President of Russia in June 1991 in the first direct presidential election in Russian history. In October 1991, as the USSR was on the verge of collapse, Yeltsin announced that Russia would proceed with radical market-oriented reform along the lines of Poland's "big bang", also known as "shock therapy".
Reforms.
"Shock Therapy".
The conversion of the world's largest state-controlled economy into a market-oriented economy would have been extraordinarily difficult regardless of the policies chosen. The policies chosen for this difficult transition were (1) liberalization, (2) stabilization, and (3) privatization. These policies were based on the neoliberal "Washington Consensus" of the International Monetary Fund (IMF), World Bank, and U.S. Treasury Department.
The programs of liberalization and stabilization were designed by Yeltsin's deputy prime minister Yegor Gaidar, a 35-year-old liberal economist inclined toward radical reform, and widely known as an advocate of "shock therapy".
The partial results of liberalization (lifting price controls) included worsening already apparent hyperinflation, initially due to monetary overhang and exacerbated after the central bank, an organ under parliament, which was skeptical of Yeltsin's reforms, was short of revenue and printed money to finance its debt. This resulted in the near bankruptcy of much of Russian industry.
The process of liberalization would create winners and losers, depending on how particular industries, classes, age groups, ethnic groups, regions, and other sectors of Russian society were positioned. Some would benefit by the opening of competition; others would suffer. Among the winners were the new class of entrepreneurs and black marketeers that had emerged under Mikhail Gorbachev's "perestroika". But liberalizing prices meant that the elderly and others on fixed incomes would suffer a severe drop in living standards, and people would see a lifetime of savings wiped out.
With inflation at double-digit rates per month as a result of printing, macroeconomic stabilization was enacted to curb this trend. Stabilization, also called structural adjustment, is a harsh austerity regime (tight monetary policy and fiscal policy) for the economy in which the government seeks to control inflation. Under the stabilization program, the government let most prices float, raised interest rates to record highs, raised heavy new taxes, sharply cut back on government subsidies to industry and construction, and made massive cuts in state welfare spending. These policies caused widespread hardship as many state enterprises found themselves without orders or financing. A deep credit crunch shut down many industries and brought about a protracted depression.
The rationale of the program was to squeeze the built-in inflationary pressure out of the economy so that producers would begin making sensible decisions about production, pricing and investment instead of chronically overusing resources—a problem that resulted in shortages of consumer goods in the Soviet Union in the 1980s. By letting the market rather than central planners determine prices, product mixes, output levels, and the like, the reformers intended to create an incentive structure in the economy where efficiency and risk would be rewarded and waste and carelessness were punished. Removing the causes of chronic inflation, the reform architects argued, was a precondition for all other reforms: Hyperinflation would wreck both democracy and economic progress, they argued; they also argued that only by stabilizing the state budget could the government proceed to dismantle the Soviet planned economy and create a new capitalist Russia.
Obstacles.
A major reason that Russia's transition has been so wrenching is that the country is remaking both its Soviet-era political and economic institutions at once. In addition, Russia is remaking itself as a new national state following the disintegration of the union.
The former Soviet Union was to deal with a number of unique obstacles during the post-Soviet transition. These obstacles may have left Russia on a far worse footing than other former Communist-led states to Russia's west that were also going through dual economic and political transitions, such as Poland, Hungary, and the Czech Republic, which have fared better since the collapse of the Eastern bloc between 1989 and 1991.
The first major problem facing Russia was the legacy of the Soviet Union's enormous commitment to the Cold War. In the late 1980s, the Soviet Union devoted a quarter of its gross economic output to the defense sector (at the time most Western analysts believed that this figure was 15 percent). At the time, the military-industrial complex employed at least one of every five adults in the Soviet Union. In some regions of Russia, at least half of the workforce was employed in defense plants. (The comparable U.S. figures were roughly one-sixteenth of gross national product and about one of every sixteen in the workforce.) The end of the Cold War and the cutback in military spending hit such plants very hard, and it was often impossible for them to quickly retool equipment, retrain workers, and find new markets to adjust to the new post–Cold War and post-Soviet era. In the process of conversion an enormous body of experience, qualified specialists and know-how has been lost, as the plants were sometimes switching from, for example, producing hi-tech military equipment to making kitchen utensils.
A second obstacle, partly related to the sheer vastness and geographical diversity of the Russian landmass, was the sizable number of "mono-industrial" regional economies (regions dominated by a single industrial employer) that Russia inherited from the Soviet Union. The concentration of production in a relatively small number of big state enterprises meant that many local governments were entirely dependent on the economic health of a single employer; when the Soviet Union collapsed and the economic ties between Soviet republics and even regions were severed, the production in the whole country dropped by more than fifty percent. Roughly half of Russia's cities had only one large industrial enterprise, and three fourths had no more than four. Consequently, the decrease in production caused tremendous unemployment and underemployment.
Thirdly, post-Soviet Russia did not inherit a system of state social security and welfare from the USSR. Instead the companies, mainly large industrial firms, were traditionally responsible for a broad range of social welfare functions—building and maintaining housing for their workforces, and managing health, recreational, educational, and similar facilities. The towns in contrast possessed neither the apparatus nor the funds for the provision of basic social services. Industrial employees were left heavily dependent on their firms. Thus, economic transformation created severe problems in maintaining social welfare since local governments were unable to assume finance or operational responsibility for these functions.
Finally, there is a human capital dimension to the failure of post-Soviet reforms in Russia. The former Soviet population was not necessarily uneducated. Literacy was nearly universal, and the educational level of the Soviet population was among the highest in the world with respect to science, engineering, and some technical disciplines. But the Soviets devoted little to what would be described as "liberal arts" in the West. The former Soviet Union's state enterprise managers were indeed highly skilled at coping with the demands on them under the Soviet system of planned production targets. But the incentive system built into state institutions and industries during the Soviet era encouraged skill in coping with state-run planned economy, but discouraged the risk-and-reward centered behavior of market capitalism. For example, the directors of Soviet state firms were rewarded for meeting output targets under difficult conditions, such as uncertainty about whether needed inputs would be delivered in time and in the right assortment. As noted, they were also responsible for a broad array of social welfare functions for their employees, their families, and the population of the towns and regions where they were located. Profitability and efficiency, however, were generally not the most prominent priorities for Soviet enterprise managers. Thus, almost no Soviet employees or managers had firsthand experience with decision-making in the conditions of a market economy.
Depression.
Russia's economy sank into deep depression by the mid-1990s, was hit further by the financial crash of 1998, and then began to recover in 1999–2000. According to Russian government statistics, the economic decline was far more severe than the Great Depression was in the United States in terms of Gross Domestic Product. It is about half as severe as the catastrophic drop borne out of the consequence of World War I, the fall of Tsarism, and the Russian Civil War.
Following the economic collapse of the early 1990s, Russia suffered from a sharp increase in the rates of poverty and economic inequality. Estimates by the World Bank based on both macroeconomic data and surveys of household incomes and expenditures indicate that whereas 1.5% of the population was living in poverty (defined as income below the equivalent of $25 per month) in the late Soviet era, by mid-1993 between 39% and 49% of the population was living in poverty. Per capita incomes fell by another 15% by 1998, according to government figures.
Public health indicators show a dramatic corresponding decline. In 1999, total population fell by about three-quarters of a million people. Meanwhile, life expectancy dropped for men from 64 years in 1990 to 57 years by 1994, while women's dropped from 74 to about 71. Both health factors and a sharp increase in deaths of the youth demographic from unnatural causes (such as murders, suicides and accidents) have significantly contributed to this trend. As of 2009, life expectancy is higher than at the nadir of the crisis in 1994, yet it still remains below the 1990 level, with men living to 59, and with women's life expectancy decreasing to 70.
Alcohol-related deaths skyrocketed 60% in the 1990s. Deaths from infectious and parasitic diseases shot up 100%, mainly because medicines were no longer affordable to the poor.
While Russia no longer suffered from the supply shortages of consumer goods that were so characteristic of the 1980s USSR ("see" Consumer goods in the Soviet Union), this was not only related to the opening of the Russian market to imports in the early 1990s but also to the impoverishment of the Russian people in the 1980s. Russians on fixed incomes (the vast majority of the workforce) saw their purchasing power drastically reduced, so while the stores might have been well stocked in the Yeltsin era, workers could now afford to buy little, if anything.
By 2011, the average income has risen to more than $700 per month, emblematic of the mild recovery in recent years thanks to a large extent to high oil prices. Growing income however, has not being evenly distributed. The social inequality has risen sharply during the 1990s with the Gini coefficient, for example, reaching 42% by the end of 2010. Russia's income disparities are now nearly as large as Brazil (which has long been among the world leader in inequality) while regional disparities in the level of poverty continue to trend upwards.
Backlash.
Structural reform lowered the standard of living for most groups of the population. Thus, reform created powerful political opposition. Democratization opened the political channels for venting these frustrations, thus translating into votes for anti-reform candidates, especially those of the Communist Party of the Russian Federation and its allies in the parliament. Russian voters, able to vote for opposition parties in the 1990s, often rejected economic reforms and yearned for the stability and personal security of the Soviet era. These were the groups that had enjoyed the benefits of Soviet-era state-controlled wages and prices, high state spending to subsidize priority sectors of the economy, protection from competition with foreign industries, and welfare entitlement programs.
During the Yeltsin years in the 1990s, these groups were well organized, voicing their opposition to reform through strong trade unions, associations of directors of state-owned firms, and political parties in the popularly elected parliament whose primary constituencies were among those vulnerable to reform. A constant theme of Russian history in the 1990s was the conflict between economic reformers and those hostile to the new capitalism.
Reform by decree.
On January 2, 1992, Yeltsin—acting as his own prime minister—enacted the most comprehensive components of economic reform by decree, thereby circumventing the Supreme Soviet of Russia and Congress of People's Deputies of Russia, which had been elected in March 1990, before the dissolution of the USSR. While this spared Yeltsin from the prospects of parliamentary bargaining and wrangling, it also eliminated any meaningful discussion of the right course of action for the country.
However, radical reform still faced some critical political barriers. The Soviet-era Central Bank was still subordinate to the conservative Supreme Soviet as opposed to the presidency. During the height of hyperinflation in 1992–1993, the Central Bank actually tried to derail reforms by actively printing money during a period of inflation. After all, the Russian government was short of revenue and was forced to print money to finance its debt. As a result, inflation exploded into hyperinflation, and the Russian economy continued in a serious slump.
Crisis.
Constitutional crisis.
The struggle for the center of power in post-Soviet Russia and for the nature of the economic reforms culminated in a political crisis and bloodshed in the fall of 1993. Yeltsin, who represented a course of radical privatization, was opposed by the parliament. Confronted with opposition to the presidential power of decree and threatened with impeachment, he "dissolved" the parliament on September 21, in contravention of the existing constitution, and ordered new elections and a referendum on a new constitution. The parliament then declared Yeltsin deposed and appointed Aleksandr Rutskoy acting president on September 22. Tensions built quickly, and matters came to a head after street riots on October 2–October 3. On October 4, Yeltsin ordered Special Forces and elite army units to storm the parliament building, the "White House" as it is called. With tanks thrown against the small-arms fire of the parliamentary defenders, the outcome was not in doubt. Rutskoy, Ruslan Khasbulatov, and the other parliamentary supporters surrendered and were immediately arrested and jailed. The official count was 187 dead, 437 wounded (with several men killed and wounded on the presidential side).
Thus the transitional period in post-Soviet Russian politics came to an end. A new constitution was approved by referendum in December 1993. Russia was given a strongly presidential system. Radical privatization went ahead. Although the old parliamentary leaders were released without trial on February 26, 1994, they would not play an open role in politics thereafter. Though its clashes with the executive would eventually resume, the remodeled Russian parliament had greatly circumscribed powers. ("For details on the constitution passed in 1993 see Constitution and government structure of Russia.")
First Chechen War.
In 1994, Yeltsin ordered 40,000 troops to prevent the separation of the southern region of Chechnya from Russia. Living south of Moscow, the predominantly Muslim Chechens for centuries had gloried in defying the Russians. Dzhokhar Dudayev, the Republic of Chechnya’s nationalist president, was driven to take his republic out of the Russian Federation, and had declared Chechnya's independence in 1991. Russia was quickly submerged in a quagmire like that of the U.S. in the Vietnam War. When the Russians attacked the Chechen capital of Grozny during the first weeks of January 1995, about 25,000 civilians died under week-long air raids and artillery fire in the sealed-off city. Massive use of artillery and air strikes remained the dominating strategy throughout the Russian campaign. Even so, Chechen insurgents seized thousands of Russian hostages, while inflicting humiliating losses on Russia's demoralized and ill-equipped troops. Russian troops had not secured the Chechen capital of Grozny by year's end.
The Russians finally managed to gain control of Grozny in February 1995 after heavy fighting. In August 1996, Yeltsin agreed to a ceasefire with Chechen leaders, and a peace treaty was formally signed in May 1997. However, the conflict resumed in 1999, thus rendering the 1997 peace accord meaningless. This time the rebellion was brutally crushed by Vladimir Putin.
Rise of the oligarchs.
The new capitalist opportunities presented by the opening of the Russian economy in the late 1980s and early 1990s affected many people's interests. As the Soviet system was being dismantled, well-placed bosses and technocrats in the Communist Party, KGB, and Komsomol (Soviet Youth League) were cashing in on their Soviet-era power and privileges. Some quietly liquidated the assets of their organization and secreted the proceeds in overseas accounts and investments. Others created banks and business in Russia, taking advantage of their insider positions to win exclusive government contracts and licenses and to acquire financial credits and supplies at artificially low, state-subsidized prices in order to transact business at high, market-value prices. Great fortunes were made almost overnight.
At the same time, a few young people, without much social status, saw opportunity in the economic and legal confusion of the transition. Between 1987 and 1992, trading of natural resources and foreign currencies, as well as imports of highly demanded consumer goods and then domestic production of their rudimentary substitutes, rapidly enabled these pioneering entrepreneurs to accumulate considerable wealth. In turn, the emerging cash-based, highly opaque markets provided a breeding ground for a large number of racket gangs.
By the mid-1990s, the best-connected former nomenklatura leaders accumulated considerable financial resources, while on the other hand, the most successful entrepreneurs became acquainted with government officials and public politicians. The privatization of state enterprises was a unique opportunity, because it gave many of those who had gained wealth in the early 1990s a chance to convert it into shares of privatized enterprises.
The Yeltsin government hoped to use privatization to spread ownership of shares in former state enterprises as widely as possible to create political support for his government and his reforms. The government used a system of free vouchers as a way to give mass privatization a jump-start. But it also allowed people to purchase shares of stock in privatized enterprises with cash. Even though initially each citizen received a voucher of equal face value, within months most of the vouchers converged in the hands of intermediaries who were ready to buy them for cash right away.
As the government ended the voucher privatization phase and launched cash privatization, it devised a program that it thought would simultaneously speed up privatization and yield the government a much-needed infusion of cash for its operating needs. Under the scheme, which quickly became known in the West as "loans for shares," the Yeltsin regime auctioned off substantial packages of stock shares in some of its most desirable enterprises, such as energy, telecommunications, and metallurgical firms, as collateral for bank loans.
In exchange for the loans, the state handed over assets worth many times as much. Under the terms of the deals, if the Yeltsin government did not repay the loans by September 1996, the lender acquired title to the stock and could then resell it or take an equity position in the enterprise. The first auctions were held in the fall of 1995. The auctions themselves were usually held in such a way so to limit the number of banks bidding for shares and thus to keep the auction prices extremely low. By summer 1996, major packages of shares in some of Russia's largest firms had been transferred to a small number of major banks, thus allowing a handful of powerful banks to acquire substantial ownership shares over major firms at shockingly low prices. These deals were effectively giveaways of valuable state assets to a few powerful, well-connected, and wealthy financial groups.
The concentration of immense financial and industrial power, which loans for shares had assisted, extended to the mass media. One of the most prominent of the financial barons Boris Berezovsky, who controlled major stakes in several banks and companies, exerted an extensive influence over state television programming for a while. Berezovsky and other ultra-wealthy, well-connected tycoons who controlled these great empires of finance, industry, energy, telecommunications, and media became known as the "Russian oligarchs". Along with Berezovsky, Mikhail Khodorkovsky, Roman Abramovich, Vladimir Potanin, Vladimir Bogdanov, Rem Viakhirev, Vagit Alekperov, Viktor Chernomyrdin, Viktor Vekselberg, and Mikhail Fridman emerged as Russia's most powerful and prominent oligarchs.
A tiny clique who used their connections built up during the last days of the Soviet years to appropriate Russia's vast resources during the rampant privatizations of the Yeltsin years, the oligarchs emerged as the most hated men in the nation. The Western world generally advocated a quick dismantling of the Soviet planned economy to make way for "free-market reforms," but later expressed disappointment over the newfound power and corruption of the "oligarchs."
Presidential election of 1996.
Campaigns.
Early in the campaign it had been thought that Yeltsin, who was in uncertain health (after recuperating from a series of heart attacks) and whose behavior was sometimes erratic, had little chance for reelection. When campaigning opened at the beginning of 1996, Yeltsin's popularity was close to zero. Meanwhile, the opposition Communist Party of the Russian Federation had already gained ground in parliamentary voting on December 17, 1995, and its candidate, Gennady Zyuganov, had a strong grassroots organization, especially in the rural areas and small towns, and appealed effectively to memories of the old days of Soviet prestige on the international stage and the socialist domestic order.
Panic struck the Yeltsin team when opinion polls suggested that the ailing president could not win; members of his entourage urged him to cancel presidential elections and effectively rule as dictator from then on. Instead, Yeltsin changed his campaign team, assigning a key role to his daughter, Tatyana Dyachenko, and appointing Anatoly Chubais campaign manager. Chubais, who was not just Yeltsin's campaign manager but also the architect of Russia's privatization program, set out to use his control of the privatization program as the key instrument of Yeltsin's reelection campaign.
The president's inner circle assumed that it had only a short time in which to act on privatization; it therefore needed to take steps that would have a large and immediate impact, making the reversal of reform prohibitively costly for their opponents. Chubais' solution was to co-opt potentially powerful interests, including enterprise directors and regional officials, in order to ensure Yeltsin's reelection.
The position of the enterprise directors to the program was essential to maintaining economic and social stability in the country. The managers represented one of the most powerful collective interests in the country; it was the enterprise managers who could ensure that labor did not erupt in a massive wave of strikes. The government, therefore, did not strenuously resist the tendency for voucher privatization to turn into "insider privatization," as it was termed, in which senior enterprise officials acquired the largest proportion of shares in privatized firms. Thus, Chubais allowed well-connected employees to acquire majority stakes in the enterprises. This proved to be the most widely used form of privatization in Russia. Three-quarters of privatized enterprises opted for this method, most often using vouchers. Real control thus wound up in the hands of the managers.
Support from the oligarchs was also crucial to Yeltsin's reelection campaign. The "loans for shares" giveaway took place in the run-up to the 1996 presidential election—at a point when it had appeared that Zyuganov might defeat Yeltsin. Yeltsin and his entourage gave the oligarchs an opportunity to scoop up some of Russia's most desirable assets in return for their help in his reelection effort. The oligarchs, in turn, reciprocated the favor.
In the spring of 1996, with Yeltsin's popularity at a low ebb, Chubais and Yeltsin recruited a team of six leading Russian financiers and media barons (all oligarchs) who bankrolled the Yeltsin campaign with $3 million and guaranteed coverage on television and in leading newspapers directly serving the president's campaign strategy. The media painted a picture of a fateful choice for Russia, between Yeltsin and a "return to totalitarianism." The oligarchs even played up the threat of civil war if a Communist were elected president.
In the outlying regions of the country, the Yeltsin campaign relied on its ties to other allies—the patron-client ties of the local governors, most of whom had been appointed by the president.
The Zyuganov campaign had a strong grass-roots organization, but it was simply no match to the financial resources and access to patronage that the Yeltsin campaign could marshal.
Yeltsin campaigned energetically, dispelling concerns about his health, exploiting all the advantages of incumbency to maintain a high media profile. To assuage voters' discontent, he made the claim that he would abandon some unpopular economic reforms and boost welfare spending, end the war in Chechnya, pay wage and pension arrears, and abolish military conscription (he did not live up to his promises after the election, except for ending the Chechen war, which was halted for 3 years). Yeltsin's campaign also got a boost from the announcement of a $10 billion loan to the Russian government from the International Monetary Fund.
Grigory Yavlinsky was the liberal alternative to Yeltsin and Zyuganov. He appealed to a well-educated middle class that saw Yeltsin as an incompetent alcoholic and Zyuganov as a Soviet-era throwback. Seeing Yavlinsky as a threat, Yeltsin's inner circle of supporters worked to bifurcate political discourse, thus excluding a middle ground—and convince voters that only Yeltsin could defeat the Communist "menace." The election became a two-man race, and Zyuganov, who lacked Yeltsin's resources and financial backing, watched haplessly as his strong initial lead was whittled away.
Elections.
Voter turnout in the first round of the polling on June 16 was 69.8%. According to returns announced on June 17, Yeltsin won 35% of the vote; Zyuganov won 32%; Aleksandr Lebed, a populist ex-general, a surprisingly high 14.5%; liberal candidate Grigory Yavlinsky 7.4%; far-right nationalist Vladimir Zhirinovsky 5.8%; and former Soviet president Mikhail Gorbachev 0.5%. With no candidate securing an absolute majority, Yeltsin and Zyuganov went into a second round of voting. In the meantime, Yeltsin co-opted a large segment of the electorate by appointing Lebed to the posts of national security adviser and secretary of the Security Council.
In the end, Yeltsin's election tactics paid off. In the run-off on July 3, with a turnout of 68.9%, Yeltsin won 53.8% of the vote and Zyuganov 40.3%, with the rest (5.9%) voting "against all". Moscow and Saint Petersburg (formerly Leningrad) together provided over half of the incumbent president's support, but he also did well in large cities in the Urals and in the north and northeast. Yeltsin lost to Zyuganov in Russia's southern industrial heartland. The southern stretch of the country became known as the "red belt", underscoring the resilience of the Communist Party in elections since the breakup of the Soviet Union.
Although Yeltsin promised that he would abandon his unpopular neoliberal austerity policies and increase public spending to help those suffering from the pain of capitalist reforms, within a month of his election, Yeltsin issued a decree canceling almost all of these promises.
Right after the election, Yeltsin's physical health and mental stability were increasingly precarious. Many of his executive functions thus devolved upon a group of advisers (most of whom had close links with the oligarchs).
Financial collapse.
The global recession of 1998, which started with the Asian financial crisis in July 1997, exacerbated Russia's economic crisis. Given the ensuing decline in world commodity prices, countries heavily dependent on the export of raw materials such as oil were among those most severely hit. Oil, natural gas, metals, and timber account for more than 80% of Russian exports, leaving the country vulnerable to swings in world prices. Oil is also a major source of government tax revenue. The sharp decline in the price of oil had severe consequences for Russia.
The pressures on the ruble, reflecting the weakness of the economy, resulted in a disastrous fall in the value of the currency. Massive tax evasion also continued, and the government found itself unable to service the massive loans it had received or even to pay its employees. The government stopped making timely payment of wages, pensions, and debts to suppliers; and when workers were paid, it was often with bartered goods rather than rubles. Coal miners were hard hit, and for several weeks in the summer they blocked sections of the Trans-Siberian railroad, effectively cutting the country in two. As time wore on, they added calls for the resignation of Yeltsin and his government to their wage demands.
A political crisis came to a head in March when Yeltsin suddenly dismissed Prime Minister Viktor Chernomyrdin and his entire cabinet on March 23. Yeltsin named a virtually unknown technocrat, Energy Minister Sergei Kiriyenko, aged 35, as acting prime minister. Russian observers expressed doubts about Kiriyenko's youth and inexperience. The Duma rejected his nomination twice. Only after a month-long standoff, during which Yeltsin threatened to dissolve the legislature, did the Duma confirm Kiriyenko on a third vote on April 24.
Kiriyenko appointed a new cabinet strongly committed to stemming the fall in value of Russia's currency. The oligarchs strongly supported Kiriyenko's efforts to maintain the exchange rate. A high exchange rate meant that they needed fewer rubles to buy imported goods, especially luxury items.
In an effort to prop up the currency and stem the flight of capital, Kiriyenko hiked interest rates to 150% in order to attract buyers for government bonds. But concerns about the financial crisis in Asia and the slump in world oil prices were already prompting investors to withdraw from Russia. By mid-1998, it was clear Russia would need help from IMF to maintain its exchange rate.
The Russian crisis caused alarm in the West. Pouring more money into the Russian economy would not be a long-term solution, but the U.S. in particular feared that Yeltsin's government would not survive a looming financial crisis without IMF help. U.S. President Bill Clinton's treasury secretary, Robert Rubin, also feared that a Russian collapse could create a panic on world money markets (and it indeed did help bring down one major US hedge fund Long-Term Capital Management). The IMF approved a $22.6 billion emergency loan on July 13.
Despite the bailout, Russia's monthly interest payments still well exceeded its monthly tax revenues. Realizing that this situation was unsustainable, investors continued to flee Russia despite the IMF bailout. Weeks later the financial crisis resumed and the value of the ruble resumed its fall, and the government fell into a self-perpetuating trap. To pay off the interest on the loans it had taken, it needed to raise still more cash, which it did through foreign borrowing. As lenders became increasingly certain that the government could not make good on its obligations, they demanded ever-higher interest rates, deepening the trap. Ultimately the bubble burst.
On August 17, Kiriyenko's government and the central bank were forced to suspend payment on Russia's foreign debt for 90 days, restructure the nation's entire debt, and devalue the ruble. The ruble went into free fall as Russians sought frantically to buy dollars. Western creditors lost heavily, and a large part of Russia's fledgling banking sector was destroyed, since many banks had substantial dollar borrowings. Foreign investment rushed out of the country, and financial crisis triggered an unprecedented flight of capital from Russia.
Political fallout.
The financial collapse produced a political crisis, as Yeltsin, with his domestic support evaporating, had to contend with an emboldened opposition in the parliament. A week later, on August 23, Yeltsin fired Kiryenko and declared his intention of returning Chernomyrdin to office as the country slipped deeper into economic turmoil. Powerful business interests, fearing another round of reforms that might cause leading concerns to fail, welcomed Kiriyenko's fall, as did the Communists.
Yeltsin, who began to lose his hold as his health deteriorated, wanted Chernomyrdin back, but the legislature refused to give its approval. After the Duma rejected Chernomyrdin's candidacy twice, Yeltsin, his power clearly on the wane, backed down. Instead, he nominated Foreign Minister Yevgeny Primakov, who on September 11 was overwhelmingly approved by the Duma.
Primakov's appointment restored political stability because he was seen as a compromise candidate able to heal the rifts between Russia's quarreling interest groups. There was popular enthusiasm for Primakov as well. Primakov promised to make the payment of wage and pension arrears his government’s first priority, and invited members of the leading parliamentary factions into his Cabinet.
Communists and trade unionists staged a nationwide strike on October 7, and called on President Yeltsin to resign. On October 9, Russia, which was also suffering from a bad harvest, appealed for international humanitarian aid, including food.
Recovery.
Russia bounced back from the August 1998 financial crash with surprising speed. Much of the reason for the recovery is that world oil prices rapidly rose during 1999–2000 (just as falling energy prices on the world market had deepened Russia's financial troubles), so that Russia ran a large trade surplus in 1999 and 2000. Another reason is that domestic industries such as food processing have benefited from the devaluation, which caused a steep increase in the prices of imported goods. Also, since Russia's economy was operating to such a large extent on barter and other non-monetary instruments of exchange, the financial collapse had far less of an impact on many producers than it would had the economy been dependent on a banking system. Finally, the economy has been helped by an infusion of cash; as enterprises were able to pay off arrears in back wages and taxes, it in turn allowed consumer demand for the goods and services of Russian industry to rise. For the first time in many years, unemployment in 2000 fell as enterprises added workers.
Nevertheless, the political and social equilibrium of the country remains tenuous to this day, and power remains a highly personalized commodity. The economy remains vulnerable to downturn if, for instance, world oil prices fall at a dramatic pace.
Succession crisis.
Yevgeny Primakov did not remain in his post long. Yeltsin grew suspicious that Primakov was gaining in strength and popularity and dismissed him in May 1999, after only eight months in office. Yeltsin then named Sergei Stepashin, who had formerly been head of the FSB (the successor agency to the KGB) and later been Interior Minister, to replace him. The Duma confirmed his appointment on the first ballot by a wide margin.
Stepashin's tenure was even shorter than Primakov's. In August 1999, Yeltsin once again abruptly dismissed the government and named Vladimir Putin as his candidate to head the new government. Like Stepashin, Putin had a background in the secret police, having made his career in the foreign intelligence service and later as head of the FSB. Yeltsin went so far as to declare that he saw Putin as his successor as president. The Duma narrowly voted to confirm Putin.
When appointed, Putin was a relatively unknown politician, but he quickly established himself both in public opinion and in Yeltsin's estimation as a trusted head of government, largely due to the Second Chechen War. Just days after Yeltsin named Putin as a candidate for prime minister, Chechen forces engaged the Russian army in Dagestan, a Russian autonomy near Chechnya. In the next month, several hundred people died in apartment building bombings in Moscow and other cities, bombings Russian authorities attributed to Chechen rebels. In response, the Russian army entered Chechnya in late September 1999, starting the Second Chechen War. The Russian public at the time, angry over the terrorist bombings, widely supported the war. The support translated into growing popularity for Putin, who had taken decisive action in Chechnya.
After the success of political forces close to Putin in the December 1999 parliamentary elections, Yeltsin evidentially felt confident enough in Putin that he resigned from the presidency on December 31, six months before his term was due to expire. This made Putin acting president and gave Putin ample opportunity to position himself as frontrunner for the Russian presidential election held on March 26, 2000, which he won. The Chechen War figured prominently in the campaign. In February 2000, Russian troops entered Grozny, the Chechen capital, and a week before the election, Putin flew to Chechnya on a fighter jet, claiming victory.
Vladimir Putin.
In August 2000, the Russian submarine K-141 Kursk suffered an explosion, causing the submarine to sink in the shallow area of the Barents Sea. Russia organized a vigorous but hectic attempt to save the crew, and the entire futile effort was surrounded by unexplained secrecy. This, as well as the slow initial reaction to the event and especially to the offers of foreign aid in saving the crew, brought much criticism on the government and personally on President Putin.
On October 23, 2002, Chechen separatists took over a Moscow theater. Over 700 people inside were taken hostage in what has been called the Moscow theater hostage crisis. The separatists demanded the immediate withdrawal of Russian forces from Chechnya and threatened to blow up the building if authorities attempted to enter. Three days later, Russian commandos stormed the building after the hostages had been subdued with a sleeping gas, shooting the unconscious militants, and killing over 100 civilian hostages with the sleeping gas in the process.
In the aftermath of the theater siege, Putin began renewed efforts to eliminate the Chechen insurrection. ("For additional details on the war in Chechnya under Putin, see Second Chechen War.") The government canceled scheduled troop withdrawals, surrounded Chechen refugee camps with soldiers, and increased the frequency of assaults on separatist positions.
Chechen militants responded in kind, stepping up guerrilla operations and rocket attacks on federal helicopters. Several high-profile attacks have taken place. In May 2004, Chechen separatists assassinated Akhmad Kadyrov, the pro-Russia Chechen leader who became the president of Chechnya 8 months earlier after an election conducted by Russian authorities. On August 24, 2004, two Russian aircraft were bombed. This was followed by the Beslan school hostage crisis in which Chechen separatists took 1,300 hostages. The initially high public support for the war in Chechnya has declined.
Putin has confronted several very influential oligarchs (Vladimir Gusinsky, Boris Berezovsky and Mikhail Khodorkovsky, in particular) who attained large stakes of state assets, allegedly through illegal schemes, during the privatization process. Gusinsky and Berezovsky have been forced to leave Russia and give up parts of their assets. Khodorkovsky was jailed in Russia and has lost his YUKOS company, formerly the largest oil producer in Russia. Putin's stand against oligarchs is generally popular with the Russian people, even though the jailing of Khodorkovsky is mainly seen as part of a takeover operation by government officials, according to another Levada-Center poll.
These confrontations have also lead to Putin establishing control over Russian media outlets previously owned by the oligarchs. In 2001 and 2002, TV channels NTV (previously owned by Gusinsky), TV6 and TVS (owned by Berezovsky) were all taken over by media groups loyal to Putin. Similar takeovers have also occurred with print media.
Putin's popularity, which stems from his reputation as a strong leader, stands in contrast to the unpopularity of his predecessor, but it hinges on a continuation of economic recovery. Putin came into office at an ideal time: after the devaluation of the ruble in 1998, which boosted demand for domestic goods, and while world oil prices were rising. Indeed, during the seven years of his presidency, real GDP grew on average 6.7% a year, average income increased 11% annually in real terms, and a consistently positive balance of the federal budget enabled the government to cut 70% of the external debt (according to the Institute for Complex Strategic Studies). Thus, many credit him with the recovery, but his ability to withstand a sudden economic downturn has been untested. Putin won the Russian presidential election in 2004 without any significant competition.
Some researchers assert that most Russians today have come to regret the collapse of the Soviet Union in 1991. On repeated occasions, even Vladimir Putin—Boris Yeltsin's handpicked successor — stated that the fall of Soviet rule had led to few gains and many problems for most Russian citizens. In a campaign speech in February 2004, for example, Putin called the dismantlement of the Soviet Union a "national tragedy on an enormous scale," from which "only the elites and nationalists of the republics gained." He added, "I think that ordinary citizens of the former Soviet Union and the post-Soviet space gained nothing from this. On the contrary, people have faced a huge number of problems."
Putin's international prestige suffered a major blow in the West during the disputed 2004 Ukrainian presidential election. Putin had twice visited Ukraine before the election to show his support for the pro-Russian Viktor Yanukovych against opposition leader Viktor Yushchenko, a pro-Western liberal economist. He congratulated Yanukovych, followed shortly afterwards by Belorussian president Alexander Lukashenko, on his victory before election results were even made official and made statements opposing the rerun of the disputed second round of elections, won by Yanukovych, amid allegations of large-scale voting fraud. The second round was ultimately rerun; Yushchenko won the round and was eventually declared the winner on January 10, 2005. In the West, the reaction to Russia's handling of, or perhaps interference in, the Ukrainian election evoked echoes of the Cold War, but relations with the U.S. have remained stable.
In 2005, the Russian government replaced the broad in-kind Soviet-era benefits, such as free transportation and subsidies for heating and other utilities for socially vulnerable groups by cash payments. The reform, known as monetization, has been unpopular and caused a wave of demonstrations in various Russian cities, with thousands of retirees protesting against the loss of their benefits. This was the first time such wave of protests took place during the Putin administration. The reform has hurt the popularity of the Russian government, but Putin personally was still popular, with a 77% approval rating.
In 2008, Kosovo's declaration of independence saw a marked deterioration in Russia's relationship with the West. It also saw South Ossetia war against Georgia, that followed the Georgia's attempt to take over the breakaway region of South Ossetia. Russian troops entered South Ossetia and forced Georgian troops back, establishing their control on this territory. In the fall of 2008, Russia unilaterally recognized the independence of South Ossetia and Abkhazia.
2014 annexation of Crimea.
On 22 February 2014, the Yanukovych government of Ukraine collapsed as a result of the 2014 Ukrainian revolution. A new government was formed to replace it with new elections scheduled for May 2014. On 1 March, from exile, Viktor Yanukovych requested that Russia use military forces "to establish legitimacy, peace, law and order, stability and defending the people of Ukraine". On the same day, Vladimir Putin requested and received authorization from the Russian Parliament to deploy Russian troops to Ukraine in response to the crisis and gained complete control over Crimean Peninsula within a day.
On 6 March 2014, the Crimean Parliament voted to "enter into the Russian Federation with the rights of a subject of the Russian Federation" and later held a referendum asking the people of these regions whether they wanted to join Russia as a federal subject, or if they wanted to restore the 1992 Crimean constitution and Crimea's status as a part of Ukraine. Though passed with an overwhelming majority, the results are contested by ones and approved by others. Crimea and Sevastopol formally declared independence as the Republic of Crimea and requested that they be admitted as constituents of the Russian Federation. On 18 March 2014, Russia and Crimea signed a treaty of accession of the Republic of Crimea and Sevastopol in the Russian Federation, though the United Nations General Assembly voted in favor of a non-binding statement to oppose Russia's annexation of the peninsula.
Relations with the West.
In the early period after Russia became independent, Russian foreign policy repudiated Marxism–Leninism as a putative guide to action, emphasizing cooperation with the West in solving regional and global problems, and soliciting economic and humanitarian aid from the West in support of internal economic reforms.
However, although Russia's leaders now described the West as its natural ally, they grappled with defining new relations with the East European states, the new states formed upon the disintegration of Yugoslavia, and Eastern Europe. Russia opposed the expansion of NATO into the former Soviet bloc nations of the Czech Republic, Poland, and Hungary in 1997 and, particularly, the second NATO expansion into the Baltic states in 2004. In 1999, Russia opposed the NATO bombing of Yugoslavia for more than two months ("see" Kosovo War), but later joined NATO peace-keeping forces in the Balkans in June 1999.
Relations with the West have also been stained by Russia's relationship with Belarus. Belarusian President Alexander Lukashenko, an authoritarian leader, has shown much interest in aligning his country with Russia, and no interest in deepening ties with the ever-expanding NATO or implementing Western-backed neoliberal economic reforms. A union agreement between Russia and Belarus was formed on April 2, 1996. The agreement was tightened, becoming the Union of Russia and Belarus on April 3, 1997. Further strengthening of the union occurred on December 25, 1998, and in 1999.
Under Putin, Russia has sought to strengthen ties with the People's Republic of China by signing the Treaty of Good-Neighborliness and Friendly Cooperation as well building the Trans-Siberian oil pipeline geared toward growing Chinese energy needs. He also made a number of appearances in the media with President of the United States George W. Bush in which the two described each other as "friends".
External links.
Mark Hollingsworth & Stewart Lansley, Londongrad: From Russia With Cash, 2009, 4th Estate

</doc>
<doc id="52631" url="https://en.wikipedia.org/wiki?curid=52631" title="Tobacco industry">
Tobacco industry

The tobacco industry comprises those persons and companies engaged in the growth, preparation for sale, shipment, advertisement, and distribution of tobacco and tobacco-related products. It is a global industry; tobacco can grow in any warm, moist environment, which means it can be farmed on all continents except Antarctica.
Tobacco, one of the most widely used addictive substances in the world, is a plant native to the Americas and historically one of the half-dozen most important crops grown by American farmers. More specifically, tobacco refers to any of various plants of the genus "Nicotiana" (especially "N. tabacum") native to tropical America and widely cultivated for their leaves, which are dried and processed chiefly for smoking in pipes, cigarettes, and cigars; it is also cut to form chewing tobacco or ground to make snuff or dipping tobacco, as well as other less common preparations. From 1617 to 1793 tobacco was the most valuable staple export from the English American mainland colonies and the United States. Until the 1960s, the United States not only grew but also manufactured and exported more tobacco than any other country.
Tobacco is an agricultural commodity product, similar in economic terms to agricultural foodstuffs: the price is in part determined by crop yields, which vary depending on local weather conditions. The price also varies by specific species or cultivar grown, the total quantity on the market ready for sale, the area where it is grown, the health of the plants, and other characteristics individual to product quality.
Since 1964 conclusive medical evidence of the deadly effects of tobacco consumption has led to a sharp decline in official support for producers and manufacturers of tobacco, although it contributes to the agricultural, fiscal, manufacturing, and exporting sectors of the economy. Laws around the world now often have some restrictions on smoking, but almost 6 trillion cigarettes are still produced each year, representing over a 12% increase since the year 2000. China accounts for over 40% of current world production. Tobacco is often heavily taxed to gain revenues for governments and as an incentive for people not to smoke.
History.
For a history of how tobacco has been grown and marketed, see tobacco, smoking and articles on similar topics.
Position.
The phrase "tobacco industry" generally refers to the companies involved in the manufacture of cigarettes, cigars, snuff, chewing tobacco and pipe tobacco. China National Tobacco Co. has become the largest tobacco company in the world by volume. Following extensive merger and acquisition activity in the 1990s and 2000s, five firms dominate international markets - in alphabetical order: 
Altria, formerly called the Philip Morris Cos. (Philip Morris Companies Inc.), still owns the Philip Morris tobacco business in the United States, but Philip Morris International has been fully independent since 2008. In most countries these companies either have long-established dominance, or have purchased the major domestic producer or producers (often a former state monopoly). Until 2014 the United States had one other substantial independent firm, Lorillard, which Reynolds American, Inc. acquired. India has its own major player, ITC Limited (25.4%-owned by British American Tobacco). A small number of state monopolies survive, as well as some small independent firms.
Tobacco advertising is becoming increasingly restricted by the governments of countries around the world citing health issues as a reason to restrict tobaccos appeal
Industry outlook in the United States.
The tobacco industry in the United States has suffered greatly since the mid-1990s, when it was successfully sued by several U.S. states. The suits claimed that tobacco causes cancer, that companies in the industry knew this, and that they deliberately understated the significance of their findings, contributing to the illness and death of many citizens in those states.
The industry was found to have decades of internal memos confirming in detail that tobacco (which contains nicotine) is both addictive and carcinogenic (cancer-causing).
The suit resulted in a large cash settlement being paid by a group of tobacco companies to the states that sued. Further, since the suit was settled, other individuals have come forth, in class action lawsuits, claiming individual damages. New suits of this nature will probably continue for a long time.
Since the settlement is a heavy tax on the profits of the tobacco industry in the US, regressive against smokers, and further settlements being made only add to the financial burden of these companies, it is debatable if the industry has a money-producing long term outlook.
The tobacco industry has historically been largely successful in this litigation process, with the majority of cases being won by the industry. During the first 42 years of tobacco litigation (between 1954 and 1996) the industry maintained a clean record in litigation thanks to tactics described in a R.J. Reynolds Tobacco Company internal memo as "the way we won these cases, to paraphrase Gen. Patton, is not by spending all of Reynolds' money, but by making the other son of a bitch spend all of his." Between 1995 and 2005 only 59% of cases were won by the tobacco industry either outright or on appeal in the US, but the continued success of the industry's efforts to win these cases is questionable. In Florida, the industry has lost 77 of the 116 "Engle progeny" cases that have gone to trial. The U.S. Supreme Court has
also denied the industry's major grounds for appeal of Engle cases.
In June 2009, U.S. President Barack Obama signed into law the Family Smoking Prevention and Tobacco Control Act which has been called a "sweeping anti-smoking" bill. Amongst other restrictions, this Act banned the use of any constituent, additive, herb or spice that adds a "characterizing flavor" to the tobacco product or smoke (Section 907)(a)(1)(A). The aim of this ban is to prevent children and teenagers from becoming addicted to cigarettes at a young age with the U.S. Department of Health and Human Services citing that "studies have shown that 17 year old
smokers are three times as likely to use flavored cigarettes as are smokers over the age of 25". This ban however does not apply to menthol cigarettes, which are exempt from the bill.
Lawsuits against the tobacco industry are primarily restricted to the United States due to differences in legal systems in other countries. Many businesses class ongoing lawsuits as a cost of doing business in the US and feel their revenue will be only marginally affected by the activities.
Conflicting points of view.
There are two entrenched interests that have opinions about the tobacco industry: (a) participants in the industry, and (b) people affected by the deaths attributable to tobacco use. These interests conflict as they involve large amounts of money, long-held (historically) belief systems, and the premature deaths of loved family members.
Participants in the industry argue that commercial tobacco production is a vital part of the American and world economy. They state that thousands of farmers in the United States, alone, make their living from raising tobacco leaves for use by the industry. They cite the fact that the tobacco industry contributes billions of dollars in tax revenue to the state and federal government every year.
Tobacco control.
On May 11, 2004, the U.S. became the 108th country to sign the World Health Organization's Global Treaty on Tobacco Control. This treaty places broad restrictions on the sale, advertising, shipment, and taxation of tobacco products. The U.S. has not yet ratified this treaty in its senate and does not yet have a schedule for doing so.
Most recently, there has been discussion within the tobacco control community of transforming the tobacco industry through the replacement of tobacco corporations by other types of business organizations that can be established to provide tobacco to the market while not attempting to increase market demand.
On February 20, 2007, the US Supreme Court ruled that the Altria Group (formerly Philip Morris) did not have to pay $79.5 million in punitive damages awarded to Mayola Williams in a 1999 Oregon court ruling, when she sued Phillip Morris for responsibility in the cancer death of her husband, Jesse Williams. The Supreme Court's decision overturns a ruling made by the Oregon Supreme Court that upheld the award.
On April 3, 2008, The U.S. Court of Appeals for the Second Circuit threw out a $800 billion class-action lawsuit filed on behalf of a group or class of people who smoked light cigarettes. The plaintiffs' lawyers were confident that they would be able to win this suit due to the success of the Schwab case wherein tobacco companies were found guilty of fraud-like charges because they were selling the idea that light cigarettes were safer than regular cigarettes. The ruling by the three-judge panel will not allow the suit to be pursued as a class, but instead need proof for why individual smokers chose light cigarettes over regular cigarettes.
Working to change smokeless tobacco control.
As of 2007, British American Tobacco, Reynolds American, Imperial Tobacco and Philip Morris are lobbying the European Union to lift a ban on smokeless alternatives to cigarettes. This was imposed in Britain in 1990 after the US Smokeless Tobacco company attempted to bring pouches of snuff (i.e. ground tobacco) for oral use, called Skoal Bandits, to market. The move to lift the ban is supported by antismoking groups and the Royal College of Physicians, as the oral snuff which the industry is attempting to introduce only verifiably increases the user's risk of pancreatic cancer, but not of oral or lung cancer. Indications of an increase in oral cancer are present in some studies, but have only very rarely been statistically significant.
Production by country or region.
The United Nations Food and Agriculture Organization estimates the following production of unprocessed tobacco by country/region in 2000. (Figures are in thousands of tonnes.)
Tobacco industry in popular culture.
The tobacco industry has had a long relationship with the entertainment industry. In silent era movies, back-lit smoke was often used by filmmakers to create sense of mystery and sensuality in a scene. Later, cigarettes were deliberately placed in the hands of Hollywood stars as an early phase of product placement, until health regulating bodies tightened rules on tobacco advertisement and anti-smoking groups pressured actors and studio executives against such tactics. Big Tobacco has since been the subject focus of films such as the docudrama "The Insider" (1999) and "Thank You For Smoking" (2005).
These issues have also constituted a recurring storyline in the AMC series "Mad Men", from season 1 beginning with the pilot episode ("Smoke Gets In Your Eyes") through season 7's midseason finale, "Waterloo". In another television show, the Netflix original program "Orange Is the New Black" season 2, episode 2 ("Looks Blue, Tastes Red"), "Black Cindy bring some Naderism to Lichtfield (a women's federal penitentiary) when talking about Philip Morris: 'Nah they ain't so bad. The people can decide for themselves if they wanna smoke. The real evil is them companies killing us without our consent. Monsanto. Rio Tinto. Big Pharma. BP. Halliburton. I've been reading there's some dark shit goin' down. Not that those motherfuckers are ever gonna hire us! The real criminals? They don't bother with small timers.' ".

</doc>
<doc id="52634" url="https://en.wikipedia.org/wiki?curid=52634" title="Baking">
Baking

Baking is a method of cooking food that uses prolonged dry heat, normally in an oven, but also in hot ashes, or on hot stones. The most common baked item is bread but many other types of foods are baked. Heat is gradually transferred "from the surface of cakes, cookies, and breads to their centre. As heat travels through it transforms batters and doughs into baked goods with a firm dry crust and a softer centre". Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously, or one after the other. Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit.
Because of historical social and familial roles, baking has traditionally been performed at home by women for domestic consumption and by men in bakeries and restaurants for local consumption. When production was industrialized, baking was automated by machines in large factories. The art of baking remains a fundamental skill and is important for nutrition, as baked goods, especially breads, are a common but important food, both from an economic and cultural point of view. A person who prepares baked goods as a profession is called a baker.
Foods and techniques.
All types of food can be baked, but some require another special care and protection from direct heat. Various techniques have been developed to provide this protection.
In addition to bread, baking is used to prepare cakes, pastries, pies, tarts, quiches, cookies, scones, crackers, pretzels, and more. These popular items are known collectively as "baked goods," and are often sold at a bakery, which is a store that carries only baked goods, or at markets, grocery stores, or through other venues.
Meat, including cured meats, such as ham can also be baked, but baking is usually reserved for meatloaf, smaller cuts of whole meats, or whole meats that contain stuffing or coating such as bread crumbs or buttermilk batter. Some foods are surrounded with moisture during baking by placing a small amount of liquid (such as water or broth) in the bottom of a closed pan, and letting it steam up around the food, a method commonly known as braising or slow baking. Larger cuts prepared without stuffing or coating are more often roasted, which is a similar process, using higher temperatures and shorter cooking times. Roasting, however, is only suitable for finer cuts of meat, so other methods have been developed to make tougher meat cuts palatable after baking. One of these is the method known as "en croûte" (French for "in a crust"), which protects the food from direct heat and seals the natural juices inside. Meat, poultry, game, fish or vegetables can be prepared by baking "en croûte". Well-known examples include Beef Wellington, where the beef is encased in pastry before baking; pâté en croûte, where the terrine is encased in pastry before baking; and the Vietnamese variant, a meat-filled pastry called pâté chaud. The "en croûte" method also allows meat to be baked by burying it in the embers of a fire – a favourite method of cooking venison. In this case, the protective casing (or crust) is made from a paste of flour and water and is discarded before eating. Salt can also be used to make a protective crust that is not eaten. Another method of protecting food from the heat while it is baking, is to cook it "en papillote" (French for "in parchment"). In this method, the food is covered by baking paper (or aluminium foil) to protect it while it is being baked. The cooked parcel of food is sometimes served unopened, allowing diners to discover the contents for themselves which adds an element of surprise.
Eggs can also be used in baking to produce savoury or sweet dishes. In combination with dairy products especially cheese, they are often prepared as a dessert. For example, although a baked custard can be made using starch (in the form of flour, cornflour, arrowroot, or potato flour), the flavour of the dish is much more delicate if eggs are used as the thickening agent. Baked custards, such as crème caramel, are among the items that need protection from an oven's direct heat, and the "bain-marie" method serves this purpose. The cooking container is half submerged in water in another, larger one, so that the heat in the oven is more gently applied during the baking process. Baking a successful soufflé requires that the baking process be carefully controlled. The oven temperature must be absolutely even and the oven space not shared with another dish. These factors, along with the theatrical effect of an air-filled dessert, have given this baked food a reputation for being a culinary achievement. Similarly, a good baking technique (and a good oven) are also needed to create a baked Alaska because of the difficulty of baking hot meringue and cold ice cream at the same time.
Baking can also be used to prepare various other foods such as pizzas, baked potatoes, baked apples, baked beans, some casseroles and pasta dishes such as lasagne.
Baking in ancient times.
The first evidence of baking occurred when humans took wild grass grains, soaked them in water, and mixed everything together, mashing it into a kind of broth-like paste. The paste was cooked by pouring it onto a flat, hot rock, resulting in a bread-like substance. Later, when humans mastered fire, the paste was roasted on hot embers, which made bread-making easier, as it could now be made any time fire was created. The world's oldest oven was discovered in Croatia in 2014 dating back 6500 years ago. The Ancient Egyptians baked bread using yeast, which they had previously been using to brew beer. Bread baking began in Ancient Greece around 600 BC, leading to the invention of enclosed ovens. "Ovens and worktables have been discovered in archaeological digs from Turkey (Hacilar) to Palestine (Jericho) and date back to 5600 BC."
Baking flourished during the Roman Empire. Beginning around 300 BC, the pastry cook became an occupation for Romans (known as the pastillarium) and became a respected profession because pastries were considered decadent, and Romans loved festivity and celebration. Thus, pastries were often cooked especially for large banquets, and any pastry cook who could invent new types of tasty treats was highly prized. Around 1 AD, there were more than three hundred pastry chefs in Rome, and Cato wrote about how they created all sorts of diverse foods and flourished professionally and socially because of their creations. Cato speaks of an enormous number of breads including; libum (sacrificial cakes made with flour), placenta (groats and cress), spira (our modern day flour pretzels), scibilata (tortes), savaillum (sweet cake), and globus apherica (fritters). A great selection of these, with many different variations, different ingredients, and varied patterns, were often found at banquets and dining halls. The Romans baked bread in an oven with its own chimney, and had mills to grind grain into flour. A bakers' guild was established in 168 BC in Rome.
Commercial baking.
Eventually, the Roman art of baking became known throughout Europe and eventually spread to eastern parts of Asia. 
By the 13th century in London, commercial trading, including baking, had many regulations attached. In the case of food, they were designed to create a system "so there was little possibility of false measures, adulterated food or shoddy manufactures." There were by that time twenty regulations applying to bakers alone, including that every baker had to have "the impression of his seal" upon each loaf of bread.
Beginning in the 19th century, alternative leavening agents became more common, such as baking soda. Bakers often baked goods at home and then sold them in the streets. This scene was so common that Rembrandt, among others, painted a pastry chef selling pancakes in the streets of Germany, with children clamoring for a sample. In London, pastry chefs sold their goods from handcarts. This developed into a delivery system of baked goods to households and greatly increased demand as a result. In Paris, the first open-air café of baked goods was developed, and baking became an established art throughout the entire world.
Baking eventually developed into a commercial industry using automated machinery which enabled more goods to be produced for widespread distribution. In the United States, the baking industry "was built on marketing methods used during feudal times and production techniques developed by the Romans." Some makers of snacks such as potato chips or crisps have produced baked versions of their snack products as an alternative to the usual cooking method of deep-frying in an attempt to reduce their calorie or fat content. Baking has opened up doors to businesses such as cake shops and factories where the baking process is done with larger amounts in large, open furnaces.
The aroma and texture of baked goods as they come out of the oven are strongly appealing but is a quality that is quickly lost. Since the flavour and appeal largely depend on freshness, commercial producers have to compensate by using food additives as well as imaginative labeling. As more and more baked goods are purchased from commercial suppliers, producers try to capture that original appeal by adding the label "home-baked." Such attempts seek to make an emotional link to the remembered freshness of baked goods as well as to attach positive associations the purchaser has with the idea of "home" to the bought product. Freshness is such an important quality that restaurants, although they are commercial (and not domestic) preparers of food, bake their own products. For example, scones at The Ritz London Hotel "are not baked until early afternoon on the day they are to be served, to make sure they are as fresh as possible."
Equipment.
Baking needs an enclosed space for heating – typically in an oven. The fuel can be supplied by wood, coal, gas, or electricity. Adding and removing items from an oven may be done by hand with an oven mitt or by a peel, a long handled tool specifically used for that purpose.
Many commercial ovens are equipped with two heating elements: one for baking, using convection and thermal conduction to heat the food, and one for broiling or grilling, heating mainly by radiation. Another piece of equipment still used for baking is the Dutch oven. "Also called a bake kettle, bastable, bread oven, fire pan, bake oven kail pot, tin kitchen, roasting kitchen, "doufeu" (French: "gentle fire") or "feu de compagne" (French: "country oven") originally replaced the cooking jack as the latest fireside cooking technology," combining "the convenience of pot-oven and hangover oven." 
Asian cultures have adopted steam baskets to produce the effect of baking while reducing the amount of fat needed.
Process.
There are eleven events that occur concurrently during baking, and some of them, such as starch glutenization, would not occur at room temperature.
The dry heat of baking changes the form of starches in the food and causes its outer surfaces to brown, giving it an attractive appearance and taste. The browning is caused by caramelization of sugars and the Maillard reaction. Maillard browning occurs when "sugars break down in the presence of proteins". Because foods contain many different types of sugars and proteins, Maillard browning contributes to the flavour of a wide range of foods, including nuts, roast beef and baked bread." The moisture is never entirely "sealed in"; over time, an item being baked will become dry. This is often an advantage, especially in situations where drying is the desired outcome, like drying herbs or roasting certain types of vegetables.
The baking process does not require any fat to be used to cook in an oven. When baking, consideration must be given to the amount of fat that is contained in the food item. Higher levels of fat such as margarine, butter, lard, or vegetable shortening will cause an item to spread out during the baking process.
With the passage of time, breads harden and become stale. This is not primarily due to moisture being lost from the baked products, but more a reorganization of the way in which the water and starch are associated over time. This process is similar to recrystallization and is promoted by storage at cool temperatures, such as in a domestic refrigerator or freezer.
Cultural and religious significance.
Baking, especially of bread, holds special significance for many cultures. It is such a fundamental part of everyday food consumption that the children's nursery rhyme "Pat-a-cake, pat-a-cake, baker's man" takes baking as its subject. Baked goods are normally served at all kinds of party and special attention is given to their quality at formal events. They are also one of the main components of a tea party, including at nursery teas and high teas, a tradition which started in Victorian Britain, reportedly when Anna Russell, Duchess of Bedford "grew tired of the sinking feeling which afflicted her every afternoon round 4 o'clock ... In 1840, she plucked up courage and asked for a tray of tea, bread and butter, and cake to be brought to her room. Once she had formed the habit she found she could not break it, so spread it among her friends instead. As the century progressed, afternoon tea became increasingly elaborate."
Benedictine Sisters of the Benedectine Monastery of Caltanissetta producing the crocette, they used to be prepared for the Holy Crucifix festivity. This was situated next to the Church of the Holy Cross, from which the sweets take the name.
For Jews, Matzo is a baked product of considerable religious and ritual significance. Baked matzah bread can be ground up and used in other dishes, such as Gefilte fish, and baked again. For Christians, bread has to be baked to be used as an essential component of the sacrament of the Eucharist. In the Eastern Christian tradition, baked bread in the form of birds is given to children to carry to the fields in a spring ceremony that celebrates the Forty Martyrs of Sebaste.

</doc>
<doc id="52636" url="https://en.wikipedia.org/wiki?curid=52636" title="Boiling">
Boiling

Boiling is the rapid vaporization of a liquid, which occurs when a liquid is heated to its boiling point, the temperature at which the vapor pressure of the liquid is equal to the pressure exerted on the liquid by the surrounding atmosphere. There are two main types of boiling; nucleate boiling where small bubbles of vapor form at discrete points, and critical heat flux boiling where the boiling surface is heated above a certain critical temperature and a film of vapor forms on the surface. Transition boiling is an intermediate, unstable form of boiling with elements of both types. The boiling point of water is 100 °C or 212 °F, but is lower with the decreased atmospheric pressure found at higher altitudes.
Boiling water is used as a method of making it potable by killing microbes that may be present. The sensitivity of different micro-organisms to heat varies, but if water is held at 70 °C (158 °F) for ten minutes, many organisms are killed, but some are more resistant to heat and require one minute at the boiling point of water. "Clostridium" spores can survive this treatment, but as the infection caused by this microbe is not water-borne, this is not a problem.
Boiling is also used in cooking. Foods suitable for boiling include vegetables, starchy foods such as rice, noodles and potatoes, eggs, meats, sauces, stocks and soups. As a cooking method it is simple and suitable for large scale cookery. Tough meats or poultry can be given a long, slow cooking and a nutritious stock is produced. Disadvantages include loss of water-soluble vitamins and minerals. Commercially prepared foodstuffs are sometimes packed in polythene sachets and sold as "boil-in-the-bag" products.
Types.
Nucleate.
"Nucleate boiling" is characterized by the growth of bubbles or pops on a heated surface, which rise from discrete points on a surface, whose temperature is only slightly above the liquid’s. In general, the number of nucleation sites are increased by an increasing surface temperature.
An irregular surface of the boiling vessel (i.e., increased surface roughness) or additives to the fluid (i.e., surfactants and/or nanoparticles) can create additional nucleation sites, while an exceptionally smooth surface, such as plastic, lends itself to superheating. Under these conditions, a heated liquid may show boiling delay and the temperature may go somewhat above the boiling point without boiling.
Critical heat flux.
As the boiling surface is heated above a critical temperature, a film of vapor forms on the surface. Since this vapor film is much less capable of carrying heat away from the surface, the temperature rises very rapidly beyond this point into the transition boiling regime. The point at which this occurs is dependent on the characteristics of boiling fluid and the heating surface in question.
Transition.
"Transition boiling" may be defined as the unstable boiling, which occurs at surface temperatures between the maximum attainable in nucleate and the minimum attainable in film boiling.
The formation of bubbles in a heated liquid is a complex physical process which often involves cavitation and acoustic effects, such as the broad-spectrum hiss one hears in a kettle not yet heated to the point where bubbles boil to the surface.
Film.
If a surface heating the liquid is significantly hotter than the liquid then film boiling will occur, where a thin layer of vapor, which has low thermal conductivity, insulates the surface. This condition of a vapor film insulating the surface from the liquid characterizes "film boiling".
In distillation, boiling is used in separating mixtures. This is possible because the vapor rising from a boiling fluid generally has a ratio of components different from that in the liquid.
Uses.
For making water potable.
As a method of disinfecting water, bringing it to its boiling point at , is the oldest and most effective way since it does not affect the taste, it's effective despite contaminants or particles present in it, and is a single step process which eliminates most microbes responsible for causing intestine related diseases. In places having a proper water purification system, it is recommended only as an emergency treatment method or for obtaining potable water in the wilderness or in rural areas, as it cannot remove chemical toxins or impurities.
The elimination of micro-organisms by boiling follows first-order kinetics—at high temperatures it is achieved in less time and at lower temperatures, in more time. The heat sensitivity of micro-organisms varies, at , Giardia species (causes Giardiasis) can take ten minutes for complete inactivation, most intestine affecting microbes and "E. coli" (gastroenteritis) take less than a minute; at boiling point, "Vibrio cholerae" (cholera) takes ten seconds and hepatitis A virus (causes the symptom of jaundice), one minute. Boiling does not ensure the elimination of all micro-organisms; the bacterial spores Clostridium can survive at but are not water-borne or intestine affecting. Thus for human health, complete sterilization of water is not required.
The traditional advice of boiling water for ten minutes is mainly for additional safety, since microbes start getting eliminated at temperatures greater than and bringing it to its boiling point is also a useful indication that can be seen without the help of a thermometer, and by this time, the water is disinfected. Though the boiling point decreases with increasing altitude, it is not enough to affect the disinfecting process.
In cooking.
"Boiling" is the method of cooking food in boiling water, or other water-based liquids such as stock or milk. Simmering is gentle boiling, while in poaching the cooking liquid moves but scarcely bubbles.
The boiling point of water is typically considered to be 100 °C or 212 °F. Pressure and a change in composition of the liquid may alter the boiling point of the liquid. For this reason, high elevation cooking generally takes longer since boiling point is a function of atmospheric pressure. In Denver, Colorado, USA, which is at an elevation of about one mile, water boils at approximately 95 °C or 203 °F. Depending on the type of food and the elevation, the boiling water may not be hot enough to cook the food properly. Similarly, increasing the pressure as in a pressure cooker raises the temperature of the contents above the open air boiling point.
Adding a water soluble substance, such as salt or sugar also increases the boiling point. This is called boiling-point elevation. At palatable concentrations of salt, the effect is very small, and the boiling point elevation is difficult to notice. However, while making thick sugar syrup, such as for Gulab Jamun, one will notice boiling point elevation. Due to variations in composition and pressure, the boiling point of water is almost never exactly 100 °C, but rather close enough for cooking.
Foods suitable for boiling include vegetables, starchy foods such as rice, noodles and potatoes, eggs, meats, sauces, stocks and soups.
Boiling has several advantages. It is safe and simple, and it is appropriate for large-scale cookery. Older, tougher, cheaper cuts of meat and poultry can be made digestible. Nutritious, well flavored stock is produced. Also, maximum color and nutritive value is retained when cooking green vegetables, provided boiling time is kept to the minimum.
On the other hand, there are several disadvantages. There is a loss of soluble vitamins from foods to the water (if the water is discarded). Boiling can also be a slow method of cooking food.
Boiling can be done in several ways: The food can be placed into already rapidly boiling water and left to cook, the heat can be turned down and the food can be simmered; or the food can also be placed into the pot, and cold water may be added to the pot. This may then be boiled until the food is satisfactory.
Water on the outside of a pot, i.e., a wet pot, increases the time it takes the pot of water to boil. The pot will heat at a normal rate once all excess water on the outside of the pot evaporates.
Boiling is also often used to remove salt from certain foodstuffs, such as bacon, if a less saline product is required.
Boil-in-the-bag.
Also known as "boil-in-bag", this involves heating or cooking ready-made foods sealed in a thick plastic bag. The bag containing the food, often frozen, is submerged in boiling water for a prescribed time. The resulting dishes can be prepared with greater convenience as no pots or pans are dirtied in the process. Such meals are available for camping as well as home dining.
Contrast with Evaporation
At any given temperature, all the molecules in a liquid do not have the same kinetic energy. Some high energy particles on the liquid surface may have enough energy to escape the inter molecular forces of attraction of the liquid and become a gas. This is called evaporation. 
Evaporation only happens on the surface while boiling happens throughout the liquid. 

</doc>
<doc id="52638" url="https://en.wikipedia.org/wiki?curid=52638" title="Altria">
Altria

Altria Group, Inc. (renamed from Philip Morris Companies Inc. on January 27, 2003) is one of the world's largest tobacco and cigarette corporations. It is a multinational corporation based in Henrico County, Virginia, United States.
Altria is the parent company of Philip Morris USA, John Middleton, Inc., U.S. Smokeless Tobacco Company, Inc., Philip Morris Capital Corporation, and Chateau Ste. Michelle Wine Estates. Philip Morris International was spun off in 2008. Altria maintains a 28.7% stake in the UK-based brewer SABMiller plc. It is a component of the S&P 500 and was a component of the Dow Jones Industrial Average until February 19, 2008. On January 6, 2009, Altria acquired UST Inc., a smokeless tobacco manufacturer, which also owned wine producer Ste Michelle Wine Estates, and is now a subsidiary of Altria.
History.
Altria emerged from Philip Morris. The onset of "rebranding" of Philip Morris Companies to Altria took place in 2003 (Philip Morris would later split, with PM USA remaining Altria's primary and only consistently held asset). Altria was created because Philip Morris wished to emphasize that its business portfolio had come to consist of more than Philip Morris USA and Philip Morris International; at the time, it owned an 84% stake in Kraft, although that business has since been spun off. The name "Altria" is claimed to come from the Latin word for "high" and was part of a trend of companies rebranding to names that previously did not exist, Accenture (previously 
Andersen Consulting) and Verizon being notable examples, though linguist Steven Pinker suggests that in fact the name is an "egregious example" of phonesthesia — with the company attempting to "switch its image from bad people who sell addictive carcinogens to a place or state marked by altruism and other lofty values".
The company's branding consultants, the Wirthlin Group, said: “The name change alternative offers the possibility of masking the negatives associated with the tobacco business,” thus enabling the company to improve its image and raise its profile without sacrificing tobacco profits.
Philip Morris executives thought a name change would insulate the larger corporation and its other operating companies from the political pressures on tobacco.
The rebranding took place amidst social, legal and financially troubled circumstances. In 2003 Altria was ranked "Fortune" number 11, and has steadily declined since. In 2010 Altria Group (MO) ranked at "Fortune" number 137, whereas its former asset, Philip Morris International, was ranked 94th.
On March 30, 2007, Altria's 88.1% stake in Kraft Foods Inc was spun off, through a distribution of the remaining stake of shares (88.1%) to Altria shareholders. That same year, Altria began selling all its shares of Philip Morris International to Altria stockholders, a spin off that was completed on March 28, 2008. Again in 2007 the company began the acquisition of cigar manufacturer John Middleton Co. from Bradford Holdings, Inc., which was complete in 2008. After Philip Morris International spun off, the former international subsidiaries halted the purchase of tobacco from America, which was a major factor in the closing of a newly renovated plant in North Carolina, an approximately 50% reduction in manufacturing, large-scale layoffs, and induced early retirements.
In 2008, Altria officially moved its headquarters from New York City to to Richmond, Virginia after Philip Morris sold its downtown offices in New York City a decade earlier. With a few exceptions, all manufacturing, commercial, and executive employees had long been based in and around Richmond. Currently the company is headquartered in an unincorporated area within Henrico County, less than five miles west of the city limits of Richmond and less than ten miles from its downtown Richmond campus.
Aside from the Philip Morris/Altria headquarters, some of their other buildings included the Philip Morris Center for Research and Technology in downtown Richmond, their manufacturing center in South Richmond, and the adjacent operations center which began shutting down in 2007-2008, as a result of the loss of demand from PMI member companies. The layoffs beginning in 2007 affected thousands of Altria, Altria Client Services, Philip Morris USA, and contracted employees in Richmond and North Carolina.
In 2009, Altria finalized its purchase of UST Inc., whose products included smokeless tobacco (made by U.S. Smokeless Tobacco Company) and wine (made by Ste. Michelle Wine Estates). This ended a short era of competition between the new Marlboro smokeless tobacco products such as snus, and those produced by UST Inc.
In 2015 the company was criticized for a number of active and possible lawsuits with countries such as Uruguay, Australia and Ireland over proposed changes to their cigarette packaging and anti-smoking laws.
Holdings.
Altria Group, Inc. owns 100 percent of Philip Morris USA, John Middleton, Inc. and Philip Morris Capital Corporation. It also owns 28.7% SABMiller PLC, one of the world's largest brewing companies, where it has 3 seats on the 11-person board of directors.
Before the recent restructuring, the net revenue (and operating income) of Altria Group, Inc. came predominantly from its tobacco business, as is shown in the following table. Altria's share of SABMiller's revenue and profits is not included in the table below because its holding are too small to be consolidated in the group accounts.
Brands.
Tobacco.
The corporation's brands include:
Corporate governance.
Board of directors.
Members of the board of directors of Altria Group as/of February 2013 were:
Headquarters.
Prior to being based in Virginia, Philip Morris had its headquarters in Midtown Manhattan, New York City. In 2003, Philip Morris announced that it would move its headquarters to Virginia. The company said that it planned to keep around 750 employees in its former headquarters. Brendan McCormick, a spokesperson for Philip Morris, said that the company estimated that the move would save the company over $60 million each year. The company now has its head offices in an unincorporated area of Henrico County, Virginia, in Richmond. In addition, the company has a 450,000-square-foot, $350 million Center for Research and Technology located in downtown Richmond at the Virginia BioTechnology Research Park that employs approximately 600 scientists, engineers and support staff.
Political influence.
According to the Center for Public Integrity, Altria spent around $101 million on lobbying the United States government between 1998 and 2004, making it the second most active organization in the nation.
Altria also funded The Advancement of Sound Science Coalition which lobbied against the scientific consensus on anthropogenic climate change.
Daniel Smith, representing Altria, sits on the Private Enterprise Board of the American Legislative Exchange Council (ALEC).

</doc>
<doc id="52641" url="https://en.wikipedia.org/wiki?curid=52641" title="William McMahon">
William McMahon

Sir William "Billy" McMahon, (23 February 190831 March 1988), was an Australian politician who was the Leader of the Liberal Party and the 20th Prime Minister of Australia from 10 March 1971 to 5 December 1972. McMahon was a member of the Australian House of Representatives for the seat of Lowe from his election in 1949 until his resignation in 1982. He rose to power at a bad time for the Coalition after over two decades in power, and he led his government to a loss to the Labor Party led by Gough Whitlam. He was the longest continuously serving government minister in Australian history - serving 21 years and 6 months - and held the longest tenure as Prime Minister without leading his party to victory at an election, being Prime Minister for 1 year and 270 days.
McMahon was born in Sydney, Australia, to an Australian mother and an Irish-Australian father, and was one of four children. When his mother died in 1917, when he was 9, McMahon was brought up by relatives and guardians, the most prominent among them his maternal uncle, who became Lord Mayor of Sydney in 1932. McMahon's father died when he was 18. McMahon was educated at Abbotsholme College, Killara, and at Sydney Grammar School and attended the University of Sydney, where he graduated with a Bachelor of Laws and returned to study economics, a factor that made him an apt Treasurer, but was a factor in the downfall of his premiership. While at university, McMahon competed in boxing and took interest in theatre, music and art.
After first graduating, McMahon worked as a solicitor, before serving in the Army during the Second World War. He was commissioned in the Citizens Military Force (now Australian Army Reserve) and later transferred to the Australian Imperial Force. He achieved the rank of captain in 1942 and was promoted to major in 1943, before he was classified medically unfit for overseas service. He was confined to staff work in Australia, where he was quartermaster for the Australian II Corps and the Australian Second Army.
After a tour of Europe to observe problems created by the Second World War, McMahon returned to the University of Sydney to complete his Bachelor of Economics degree, and was elected to Parliament in 1949, representing the seat of Lowe in the House of Representatives. McMahon became a minister in 1951. He became Deputy Leader of the Liberal Party in 1966. In 1971, when Prime Minister John Gorton resigned after a leadership vote ended in a tie, McMahon became leader, thus becoming Prime Minister himself.
The McMahon Government was formed at a turbulent time for the Coalition, and in the 1972 federal election, McMahon led his party to defeat. McMahon remained a member of Parliament until 1982, when he resigned.
Early life.
William McMahon was born in Sydney, the son of Mary Walder McMahon and William McMahon, a lawyer. His uncle was Samuel Walder, Lord Mayor of Sydney. His father was of Irish descent. McMahon's mother died when he was 9 and his father when he was 18. He was educated at Sydney Grammar School and at the University of Sydney, where he graduated in law. He practised in Sydney with Allen, Allen & Hemsley (now Allens Arthur Robinson), the oldest law firm in Australia. In 1940, he joined the Army, but because of a hearing loss, he was confined to staff work. After the Second World War, he travelled in Europe and completed an economics degree at the University of Sydney.
Politics.
McMahon was elected to the House of Representatives for the Sydney seat of Lowe in the 1949 federal election, one of the flood of new Liberal MPs known as the "forty-niners". He was capable and ambitious, and in 1951 Prime Minister Robert Menzies made him Minister for Air and Minister for the Navy. McMahon served in Cabinet in one capacity or another for the next 21 years. At various times under Menzies, he was Minister for Social Services, Primary Industry and Labour and National Service. He was also Vice-President of the Executive Council. In 1966, when Harold Holt became Prime Minister, McMahon succeeded him as Treasurer and as Deputy Leader of the Liberal Party. Despite his steady advance, McMahon remained unpopular with his colleagues. He was highly capable, but seen as too ambitious and a schemer.
After making an extensive tour of Europe to observe the problems created by World War II, McMahon returned to the University of Sydney (B.Ec., 1949). In 1948 (Sir) Jack Cassidy sought preselection for the new Federal seat of Lowe and asked McMahon to speak at Strathfield on his behalf. So impressed were the Liberal Party women whom he addressed that they encouraged him to stand for preselection himself. Elected in December 1949 as the Liberal member, he was to hold the seat for thirty-two years, although he never lived in the electorate.
McMahon’s maiden speech on 2 March 1950 displayed not only his attributes—proficiency in economics and robust preparation—but also an inclination to show off and exaggerate, and weak attempts at humour. Its theme was that the coalition parties had a greater prospect of maintaining full employment than the Australian Labor Party whose ‘lack of warmth for private enterprise’ and tendency to increase the size of the public service channelled employment into non-productive spheres.
After the 1951 election McMahon became minister for the navy and minister for air. He visited troops in Korea and approved Sir James Hardman’s reorganisation of the Royal Australian Air Force along functional command lines. Appointed minister for social services in 1954, he supported the building of more rehabilitation facilities to enable disabled people to enter the workforce. The minister for trade, (Sir) John McEwen, lobbied the prime minister, (Sir) Robert Menzies, to promote McMahon and on 11 January 1956 he was elevated to cabinet as minister for primary industry. With no experience in agriculture, McMahon was expected to comply with decisions made by McEwen. Instead, by working hard and mastering his brief, he often brought matters to cabinet without McEwen’s knowledge and argued against his senior minister.
In his longest held portfolio, as minister for labour and national service (1958–66), McMahon introduced the National Service Act (1964) that authorised conscription for army service. Australia was soon to send troops to fight in South Vietnam and the Borneo State of Malaysia. The government also wished to increase army manpower in case of wider conflicts involving the country’s commitments under the South-East Asia Treaty Organization and the Australia, New Zealand, United States Security Treaty. He pursued the Communist-dominated Waterside Workers Federation, established an inquiry into waterfront efficiency and employment, legislated to strip the WWF of its authority over recruitment and made deregistration of the union theoretically possible. From 1964 to 1966 he was Vice-President of the Executive Council.
When Harold Holt replaced Menzies as prime minister on 26 January 1966, McMahon defeated (Sir) Paul Hasluck for the deputy leadership. As deputy, he was also treasurer (1966–69)—the post he had always wanted. He developed good relationships with his department—which contained a number of highly skilled economists—and was appointed a governor (1966–69) of the International Monetary Fund and chairman (1968–69) of the board of governors of the Asian Development Bank. Extensive knowledge of his portfolio, his understanding of economics, his inquisition of public servants and his desire to keep control of expenditure often made him unpopular, but these qualities boosted his reputation as a treasurer. He introduced four budgets, gradually reducing the deficit from $644 million in 1967-68 to $30 million in 1969-70. They were characterised by significant increased spending on defence, drought assistance, pension benefits and grants to the States, and by new Commonwealth programs for the health, education and housing of Aborigines, and for school libraries. Funding came from increased company and sales tax rates, radio and television licence fees, air navigation charges and overseas borrowings. Together with (Sir) John Gorton, he tried to resist State demands for extra revenue. Relations between the Treasury and the Department of Trade were strained even when Holt was treasurer. When McMahon became treasurer his relationship with McEwen deteriorated further. They clashed over industry protection, McMahon’s opposition to the establishment of the Australian Industry Development Corporation and his (ultimately vindicated) decision not to devalue the Australian dollar. McEwen accused McMahon of being behind the Basic Industries Group, a pro-free-trade agricultural lobby that funded Western Australian and Victorian Liberals to stand against Country Party members. The governor-general, R. G. (Lord) Casey, met with McMahon to encourage him to heal relations with McEwen, but there were persistent tensions that the affable Holt found difficult to manage.
When Holt drowned in December 1967, McMahon was assumed to be his probable successor. However, John McEwen, interim Prime Minister and leader of the Country Party, announced that he and his party would not serve in a government led by McMahon. McEwen did not state his reasons publicly, but privately he told McMahon he did not trust him. McEwen, an arch-protectionist, correctly suspected that McMahon favoured policies of free trade and deregulation.
McMahon therefore withdrew, and Senator John Gorton won the party room ballot for party leader and Prime Minister. McMahon became Foreign Minister and waited for his chance at a comeback. The Coalition was nearly defeated at the 1969 federal election. After the election, McMahon challenged Gorton for the leadership, but failed in part because of McEwen's continued opposition.
In January 1971, McEwen retired as Country Party leader and Deputy Prime Minister. His successor, Doug Anthony, discontinued the veto against McMahon. In March 1971, the Defence Minister, Malcolm Fraser, resigned from Cabinet and denounced Gorton, who then announced a leadership spill. The ensuing party room vote was tied, and under the party rules of the time this meant the motion was lost and Gorton could have theoretically remained as leader and Prime Minister. Nevertheless, Gorton declared that a tie vote meant he no longer had the confidence of the party, and voluntarily resigned the leadership. McMahon was then elected leader (and Prime Minister), and Gorton was elected deputy Liberal leader.
Prime Minister.
McMahon came into office at a bad time for the Coalition, which was increasingly seen as tired and unfocused after 22 years in power. His first problem was Gorton. Since Gorton had been elected as Liberal deputy leader, McMahon was all but forced to name him Defence Minister. This farcical situation came to a head when Gorton published two articles detailing the problems he had had with ministers leaking information from cabinet. McMahon forced Gorton's resignation. Billy Snedden was chosen as the new deputy Liberal leader.
McMahon found himself dealing with a resurgent Labor Party under Gough Whitlam. Labor had come within four seats of winning government in 1969, and since then had positioned itself as a credible government-in-waiting. Over the next year-and-a-half, McMahon was unable to get the better of Whitlam. McMahon was no match in parliamentary debates for Whitlam, a witty and powerful orator. He frequently found himself on the defensive as Whitlam attacked the increasingly unpopular Vietnam War and advocated radical new policies such as universal health insurance. In a typical instance, McMahon attacked Whitlam for his demands that Australia recognise the People's Republic of China, only to have to back down when U.S President Richard Nixon announced his visit to China. He was not helped by rising inflation, which hurt his reputation as a sound economic manager. Additionally, the Liberal Party was showing severe schisms, which came at an especially bad time since McMahon had, at most, two years before the next election. His voice and appearance also came across badly on television.
In June 1971, McMahon cancelled Gorton's planned nuclear power program, which had included a reactor capable of generating weapons-grade plutonium. He considered it inconsistent with the goals of the Nuclear Non-Proliferation Treaty, signed under Gorton in 1970 and ratified under Whitlam in 1973.
McMahon went into 1972 facing a statutory general election. By then, Labor had established a clear lead in the polls and McMahon's approval ratings had dwindled to 28 percent. The press had turned on him so violently that the British psephologist David Butler recalled on a visit to Australia that he could not recall a prime minister in any country being "so comprehensively panned" as McMahon. By then, it was widely perceived that McMahon simply "did not look or sound like a Prime Minister". He waited for as long as he could, but finally called a federal election for 2 December. During the campaign, McMahon was abandoned by some of his own ministers, unheard of in a Westminster system. The Coalition was swept from power on an eight-seat swing. Late on election night, with the result beyond doubt, McMahon conceded defeat, ending the longest unbroken run in government in Australian history.
McMahon had been a minister continuously for 21 years and 6 months, a record in the Australian Government that has never been threatened. Only Sir George Pearce and Sir John McEwen had longer overall ministerial service, but their terms were not continuous.
Political journalist Laurie Oakes described McMahon as "devious, nasty, dishonest - he lied all the time and stole things" before describing an incident where McMahon attempted to steal a tape recorder from his radio station by claiming ownership of the device despite it having the radio station's name engraved on it. He concludes with "totally unworthy individual and the fact that he was Prime Minister of this country was a disgrace".
Later political career.
McMahon served in the Shadow Cabinet under his successor Billy Snedden, but was dropped after the 1974 election. He retained his seat in Parliament in the 1975, 1977 and 1980 elections.
McMahon became Joint Father of the House of Representatives with Clyde Cameron in 1977, and sole Father in 1980 when Cameron retired. On the retirement of Senator Justin O'Byrne in 1981, he became Father of the Parliament. He resigned from Parliament in 1982 causing the Lowe by-election, 1982 which was won by Labor for the first time in the seats' 31-year history. Traditionally a Labor leaning electorate, the swing against the soon to be defeated Fraser Liberal Government was in stark contrast to McMahon's ability to hold the seat for such a sustained period.
Personal life.
In 1965, aged 57, McMahon married Sonia Rachel Hopkins, who was then aged 32. McMahon had proposed six months after the pair first met. The wedding was held three months later at St Mark's Church, Darling Point, followed by a reception for 400 people at the Royal Sydney Golf Club.
McMahon had three children; Melinda, Julian and Deborah. Julian is an actor and model while Melinda and Deborah, who is openly gay and suffers from schizophrenia, lead largely private lives.
Throughout his life there were also frequent rumours that he was homosexual. The suggestion was repeatedly denied by Lady McMahon; one occasion in the 1970s resulted in an infamous tabloid headline "My Billy's No Poofter - Sonia Tells".
McMahon died of cancer in the Sydney suburb of Potts Point on 31 March 1988 aged 80. He was cremated after a private ceremony. A memorial service was held at St Andrew’s Cathedral on 8 August 1988.
Sonia McMahon died, aged 77, on 2 April 2010, 22 years after her husband's death.
Honours.
McMahon was appointed a Privy Counsellor in 1966, a Companion of Honour in the New Year's Day Honours of 1972 and a Knight Grand Cross of the Order of St Michael and St George in the Queen's Birthday Honours of 1977.
Following the 2009 redistribution of New South Wales federal electorates, the Division of Prospect was renamed the Division of McMahon starting at the 2010 federal election.

</doc>
<doc id="52642" url="https://en.wikipedia.org/wiki?curid=52642" title="Van de Graaff generator">
Van de Graaff generator

A Van de Graaff generator is an electrostatic generator which uses a moving belt to accumulate electric charge on a hollow metal globe on the top of an insulated column, creating very high electric potentials. It produces very high voltage direct current (DC) electricity at low current levels. It was invented by American physicist Robert J. Van de Graaff in 1929. The potential difference achieved in modern Van de Graaff generators can reach 5 megavolts. A tabletop version can produce on the order of 100,000 volts and can store enough energy to produce a visible spark. Small Van de Graaff machines are produced for entertainment, and in physics education to teach electrostatics; larger ones are displayed in science museums.
The Van de Graaff generator was developed as a particle accelerator in physics research, its high potential is used to accelerate subatomic particles to high speeds in an evacuated tube. It was the most powerful type of accelerator in the 1930s until the cyclotron was developed. Today it is still used as an accelerator to generate energetic particle and x-ray beams in fields such as nuclear medicine. In order to double the voltage, two generators are often used together, one generating positive and the other negative potential; this is called a tandem Van de Graaff accelerator. For example, the Brookhaven National Laboratory Tandem Van de Graaff achieves about 30 million volts of potential difference.
The voltage produced by an open-air Van de Graaff machine is limited by arcing and corona discharge to about 5 megavolts. Most modern industrial machines are enclosed in a pressurized tank of insulating gas; these can achieve potentials up to about 25 megavolts.
Description.
A simple Van de Graaff generator consists of a belt of rubber (or a similar flexible dielectric material) running over two rollers of differing material, one of which is surrounded by a hollow metal sphere. Two electrodes, (2) and (7), in the form of comb-shaped rows of sharp metal points, are positioned near the bottom of the lower roller and inside the sphere, over the upper roller. Comb (2) is connected to the sphere, and comb (7) to ground. The method of charging is based on the triboelectric effect, wherein simple contact of dissimilar materials causes the transfer of some electrons from one material to the other. For example, (see the diagram), the rubber of the belt will become negatively charged while the acrylic glass of the upper roller will become positively charged. The belt carries away negative charge on its inner surface while the upper roller accumulates positive charge. Next, the strong electric field surrounding the positive upper roller (3) induces a very high electric field near the points of the nearby comb (2). At the points, the field becomes high enough to ionize air molecules, and the electrons are attracted to the outside of the belt while positive ions go to the comb. At the comb (2) they are neutralized by electrons that were on the comb, thus leaving the comb and the attached outer shell (1) with fewer net electrons. By the principle illustrated in the Faraday ice pail experiment, i.e. by Gauss's Law, the excess positive charge is accumulated on the outer surface of the outer shell (1), leaving no field inside the shell. Electrostatic induction by this method continues, building up very large amounts of charge on the shell.
In the example, the lower roller (6) is metal, which picks negative charge off the inner surface of the belt. The lower comb (7) develops a high electric field at its points that also becomes large enough to ionize air molecules. In this case the electrons are attracted to the comb and positive air ions neutralize negative charge on the outer surface of the belt, or become attached to the belt. The exact balance of charges on the up-going versus down-going sides of the belt will depend on the combination of the materials used. In the example, the upward-moving belt must be more positive than the downward-moving belt. As the belt continues to move, a constant 'charging current' travels via the belt, and the sphere continues to accumulate positive charge until the rate that charge is being lost (through leakage and corona discharges) equals the charging current. The larger the sphere and the farther it is from ground, the higher will be its peak potential. In the example, the wand with metal sphere (8) is connected to ground, as is the lower comb (7); electrons are drawn up from ground due to the attraction by the positive sphere, and when the electric field is large enough (see below) the air breaks down in the form of an electrical discharge spark (9). Since the material in the belt and rollers can be selected, the accumulated charge on the hollow metal sphere can either be made positive (electron deficient) or negative (excess electrons).
The triboelectric type of generator described above is easier to build for science fair or homemade projects, since it does not require a high-voltage source. Higher potentials can be reached with alternative designs (not discussed here) in which high voltage sources are used at the upper and/or lower positions of the belt to more efficiently transfer charge onto and off the belt.
A Van de Graaff generator terminal does not need to be sphere-shaped to work, and in fact, the optimum shape is a sphere with an inward curve around the hole where the belt enters. A rounded terminal minimizes the electric field around it, allowing greater potentials to be achieved without ionization of the surrounding air, or other dielectric gas. Outside the sphere, the electric field becomes very strong and applying charges directly from the outside would soon be prevented by the field. Since electrically charged conductors have no electric field inside, charges can be added continuously from the inside without raising them to the full potential of the outer shell. Since a Van de Graaff generator can supply the same small current at almost any level of electrical potential, it is an example of a nearly ideal current source.
The maximum achievable potential is approximately equal to the sphere's radius (R) multiplied by the electric field (Emax) where corona discharges begin to form within the surrounding gas. For air at STP the breakdown field is about 30 kV/cm. Therefore, a polished spherical electrode 30 cm in diameter could be expected to develop a maximum voltage (R Emax) of about 450 kV. This explains why Van de Graaff generators are often made with the largest possible diameter.
History.
The concept of an electrostatic generator in which charge is mechanically transported in small amounts into the interior of a high voltage electrode goes back to the Kelvin water dropper, invented in 1867 by William Thomson (Lord Kelvin), in which charged drops of water fall into a bucket with the same polarity charge, adding to the charge. In this machine the gravitational force moves the drops against the opposing electrostatic field of the bucket. Kelvin himself first suggested using a belt to carry the charge instead of water. The first electrostatic machine that used an endless belt to transport charge was constructed in 1872 by Augusto Righi. It used an india rubber belt with wire rings along its length as charge carriers, which passed into a spherical metal electrode. The charge was applied to the belt from the grounded lower roller by electrostatic induction using a charged plate. Gray also invented a belt machine. Another more complicated belt machine was invented in 1903 by Juan Burboa A more immediate inspiration for Van de Graaff was a generator W. F. G. Swann was developing in the 1920s in which charge was transported to an electrode by falling metal balls, thus returning to the principle of the Kelvin water dropper.
The reason that the charge extracted from the belt moves to the outside of the sphere electrode even though it already has a high charge of the same polarity is explained by the Faraday ice pail experiment.
The Van de Graaff generator was developed, starting in 1929, by physicist Robert J. Van de Graaff at Princeton University on a fellowship, with help from colleague Nicholas Burke. The first model was demonstrated in October 1929. The first machine used an ordinary tin can, a small motor, and a silk ribbon bought at a five-and-dime store. Whereupon he went to the head of the physics department requesting a hundred dollars to make an improved version. He did get the money, with some difficulty. By 1931 he could report achieving 1.5 million volts, saying "The machine is simple, inexpensive, and portable. An ordinary lamp socket furnishes the only power needed." According to a patent application, it had two 60-cm-diameter charge-accumulation spheres mounted on borosilicate glass columns 180 cm high; the apparatus cost only $90 in 1931.
Van de Graaff applied for a second patent in December 1931, which was assigned to MIT in exchange for a share of net income. The patent was later granted.
In 1933, Van de Graaff built a 40-foot (12-m) model at MIT's Round Hill facility, the use of which was donated by Colonel Edward H. R. Green.
In 1937, the Westinghouse Electric company built a Van de Graaff generator capable of generating 5 MeV in Forest Hills, Pennsylvania. It marked the beginning of nuclear research for civilian applications. It was decommissioned in 1958 and was demolished in 2015.
A more recent development is the tandem Van de Graaff accelerator, containing one or more Van de Graaff generators, in which negatively charged ions are accelerated through one potential difference before being stripped of two or more electrons, inside a high voltage terminal, and accelerated again. An example of a three-stage operation has been built in Oxford Nuclear Laboratory in 1964 of a 10 MV single-ended "injector" and a 6 MV EN tandem.
One of Van de Graaff's accelerators used two charged domes of sufficient size that each of the domes had laboratories inside - one to provide the source of the accelerated beam, and the other to analyze the actual experiment. The power for the equipment inside the domes came from generators that ran off the belt, and several sessions came to a rather gruesome end when a pigeon would try to fly between the two domes, causing them to discharge. (The accelerator was set up in an airplane hangar.)
By the 1970s, up to 14 million volts could be achieved at the terminal of a tandem that used a tank of high-pressure sulfur hexafluoride (SF6) gas to prevent sparking by trapping electrons. This allowed the generation of heavy ion beams of several tens of megaelectronvolts, sufficient to study light ion direct nuclear reactions. The highest potential sustained by a Van de Graaff accelerator is 25.5 MV, achieved by the tandem at the Holifield Radioactive Ion Beam Facility at Oak Ridge National Laboratory.
A further development is the pelletron, where the rubber or fabric belt is replaced by a chain of short conductive rods connected by insulating links, and the air-ionizing electrodes are replaced by a grounded roller and inductive charging electrode. The chain can be operated at much higher velocity than a belt, and both the voltage and currents attainable are much higher than with a conventional Van de Graaff generator. The 14 UD Heavy Ion Accelerator at The Australian National University houses a 15-million-volt pelletron. Its chains are more than 20 meters long and can travel faster than .
The Nuclear Structure Facility (NSF) at Daresbury Laboratory was proposed in the 1970s, commissioned in 1981, and opened for experiments in 1983. It consisted of a tandem Van de Graaff generator operating routinely at 20 MV, housed in a distinctive building 70 metres high. During its lifetime, it accelerated 80 different ion beams for experimental use, ranging from protons to uranium. A particular feature was the ability to accelerate rare isotopic and radioactive beams. Perhaps the most important discovery made on the NSF was that of super-deformed nuclei. These nuclei, when formed from the fusion of lighter elements, rotate very rapidly. The pattern of gamma rays emitted as they slow down provided detailed information about the inner structure of the nucleus. Following financial cutbacks, the NSF closed in 1993.
Van de Graaff generators on display.
The largest air-insulated Van de Graaff generator in the world, built by Dr. Van de Graaff in the 1930s, is now on permanent display at Boston's Museum of Science. With two conjoined aluminium spheres standing on columns tall, this generator can often reach 2 MV (2 million volts). Shows using the Van de Graaff generator and several Tesla coils are conducted two to three times a day. Many science museums, such as the American Museum of Science and Energy, have small-scale Van de Graaff generators on display, and exploit their static-producing qualities to create "lightning" or make people's hair stand up. Van de Graaff generators are also used in schools and in science shows.
Comparison with other high-voltage generators.
Other classical electrostatic machines like a Wimshurst machine or a Bonetti machine can easily produce more current than a Van de Graaff generator for experiments with electrostatics, and have positive and negative outputs. In these generators, however, corona discharge from exposed metal parts at high potentials and poorer insulation result in smaller voltages. In an electrostatic generator, the rate of charge transported (current) to the high voltage electrode is very small, so the maximum voltage is reached when the leakage current from the electrode equals the rate of charge transport. In the Van de Graaff generator, the belt allows the transport of charge into the interior of a large hollow spherical electrode. This is the ideal shape to minimize leakage and corona discharge, so the Van de Graaff generator can produce the highest voltage. This is why the Van de Graaff design has been used for all electrostatic particle accelerators.

</doc>
<doc id="52644" url="https://en.wikipedia.org/wiki?curid=52644" title="Cysteine">
Cysteine

Cysteine (abbreviated as Cys or C) is a semi-essential proteinogenic amino acid with the formula HO2CCH(NH2)CH2SH>. It is encoded by the codons UGU and UGC. The thiol side chain in cysteine often participates in enzymatic reactions, as a nucleophile. The thiol is susceptible to oxidization to give the disulfide derivative cystine, which serves an important structural role in many proteins. When used as a food additive, it has the E number E920.
Sources.
Dietary sources.
Although classified as a non-essential amino acid, in rare cases, cysteine may be essential for infants, the elderly, and individuals with certain metabolic disease or who suffer from malabsorption syndromes. Cysteine can usually be synthesized by the human body under normal physiological conditions if a sufficient quantity of methionine is available. Cysteine is catabolized in the gastrointestinal tract and blood plasma. In contrast, cystine travels safely through the GI tract and blood plasma and is promptly reduced to the two cysteine molecules upon cell entry.
Cysteine is found in most high-protein foods, including:
Like other amino acids, cysteine has an amphoteric character. 
Industrial sources.
The majority of L-cysteine is obtained industrially by hydrolysis of animal materials, such as poultry feathers or hog hair. Despite widespread belief otherwise, there is little evidence that human hair is used as a source material and its use is explicitly banned in the European Union. Synthetically produced L-cysteine, compliant with Jewish kosher and Muslim halal laws, is also available, albeit at a higher price. The synthetic route involves fermentation using a mutant of "E. coli". Degussa introduced a route from substituted thiazolines. Following this technology, L-cysteine is produced by the hydrolysis of racemic 2-amino-Δ2-thiazoline-4-carboxylic acid using "Pseudomonas thiazolinophilum". 
Biosynthesis.
In animals, biosynthesis begins with the amino acid serine. The sulfur is derived from methionine, which is converted to homocysteine through the intermediate S-adenosylmethionine. Cystathionine beta-synthase then combines homocysteine and serine to form the asymmetrical thioether cystathionine. The enzyme cystathionine gamma-lyase converts the cystathionine into cysteine and alpha-ketobutyrate. In plants and bacteria, cysteine biosynthesis also starts from serine, which is converted to "O"-acetylserine by the enzyme serine transacetylase. The enzyme O-acetylserine (thiol)-lyase, using sulfide sources, converts this ester into cysteine, releasing acetate.
Biological functions.
The cysteine thiol group is nucleophilic and easily oxidized. The reactivity is enhanced when the thiol is ionized, and cysteine residues in proteins have pKa values close to neutrality, so are often in their reactive thiolate form in the cell. Because of its high reactivity, the thiol group of cysteine has numerous biological functions.
Precursor to the antioxidant glutathione.
Due to the ability of thiols to undergo redox reactions, cysteine has antioxidant properties. Cysteine's antioxidant properties are typically expressed in the tripeptide glutathione, which occurs in humans as well as other organisms. The systemic availability of oral glutathione (GSH) is negligible; so it must be biosynthesized from its constituent amino acids, cysteine, glycine, and glutamic acid. Glutamic acid and glycine are readily available in most Western diets, but the availability of cysteine can be the limiting substrate.
Precursor to iron-sulfur clusters.
Cysteine is an important source of sulfide in human metabolism. The sulfide in iron-sulfur clusters and in nitrogenase is extracted from cysteine, which is converted to alanine in the process.
Metal ion binding.
Beyond the iron-sulfur proteins, many other metal cofactors in enzymes are bound to the thiolate substituent of cysteinyl residues. Examples include zinc in zinc fingers and alcohol dehydrogenase, copper in the blue copper proteins, iron in cytochrome P450, and nickel in the -hydrogenases. The thiol group also has a high affinity for heavy metals, so that proteins containing cysteine, such as metallothionein, will bind metals such as mercury, lead, and cadmium tightly.
Roles in protein structure.
In the translation of messenger RNA molecules to produce polypeptides, cysteine is coded for by the UGU and UGC codons.
Cysteine has traditionally been considered to be a hydrophilic amino acid, based largely on the chemical parallel between its thiol group and the hydroxyl groups in the side-chains of other polar amino acids. However, the cysteine side chain has been shown to stabilize hydrophobic interactions in micelles to a greater degree than the side chain in the non-polar amino acid glycine, and the polar amino acid serine. In a statistical analysis of the frequency with which amino acids appear in different chemical environments in the structures of proteins, free cysteine residues were found to associate with hydrophobic regions of proteins. Their hydrophobic tendency was equivalent to that of known non-polar amino acids such as methionine and tyrosine (tyrosine is polar aromatic but also hydrophobic), and was much greater than that of known polar amino acids such as serine and threonine. Hydrophobicity scales, which rank amino acids from most hydrophobic to most hydrophilic, consistently place cysteine towards the hydrophobic end of the spectrum, even when they are based on methods that are not influenced by the tendency of cysteines to form disulfide bonds in proteins. Therefore, cysteine is now often grouped among the hydrophobic amino acids, though it is sometimes also classified as slightly polar, or polar.
While free cysteine residues do occur in proteins, most are covalently bonded to other cysteine residues to form disulfide bonds. Disulfide bonds play an important role in the folding and stability of some proteins, usually proteins secreted to the extracellular medium. Since most cellular compartments are reducing environments, disulfide bonds are generally unstable in the cytosol with some exceptions as noted below.
Disulfide bonds in proteins are formed by oxidation of the thiol groups of cysteine residues. The other sulfur-containing amino acid, methionine, cannot form disulfide bonds. More aggressive oxidants convert cysteine to the corresponding sulfinic acid and sulfonic acid. Cysteine residues play a valuable role by crosslinking proteins, which increases the rigidity of proteins and also functions to confer proteolytic resistance (since protein export is a costly process, minimizing its necessity is advantageous). Inside the cell, disulfide bridges between cysteine residues within a polypeptide support the protein's tertiary structure. Insulin is an example of a protein with cystine crosslinking, wherein two separate peptide chains are connected by a pair of disulfide bonds.
Protein disulfide isomerases catalyze the proper formation of disulfide bonds; the cell transfers dehydroascorbic acid to the endoplasmic reticulum, which oxidises the environment. In this environment, cysteines are, in general, oxidized to cystine and are no longer functional as a nucleophiles.
Aside from its oxidation to cystine, cysteine participates in numerous posttranslational modifications. The nucleophilic thiol group allows cysteine to conjugate to other groups, e.g., in prenylation. Ubiquitin ligases transfer ubiquitin to its pendant, proteins, and caspases, which engage in proteolysis in the apoptotic cycle. Inteins often function with the help of a catalytic cysteine. These roles are typically limited to the intracellular milieu, where the environment is reducing, and cysteine is not oxidized to cystine.
Applications.
Cysteine, mainly the L-enantiomer, is a precursor in the food, pharmaceutical, and personal-care industries. One of the largest applications is the production of flavors. For example, the reaction of cysteine with sugars in a Maillard reaction yields meat flavors. L-Cysteine is also used as a processing aid for baking.
In the field of personal care, cysteine is used for permanent wave applications, predominantly in Asia. Again, the cysteine is used for breaking up the disulfide bonds in the hair's keratin.
Cysteine is a very popular target for site-directed labeling experiments to investigate biomolecular structure and dynamics. Maleimides will selectively attach to cysteine using a covalent Michael addition. Site-directed spin labeling for EPR or paramagnetic relaxation enhanced NMR also uses cysteine extensively.
In a 1994 report released by five top cigarette companies, cysteine is one of the 599 additives to cigarettes. Like most cigarette additives, however, its use or purpose is unknown. Its inclusion in cigarettes could offer two benefits: acting as an expectorant, since smoking increases mucus production in the lungs; or increasing the beneficial antioxidant glutathione (which is diminished in smokers).
Reducing toxic effects of alcohol.
Cysteine has been proposed as a preventative or antidote for some of the negative effects of alcohol, including liver damage and hangover. It counteracts the poisonous effects of acetaldehyde. Cysteine supports the next step in metabolism, which turns acetaldehyde into the relatively harmless acetic acid. In a rat study, test animals received an dose of acetaldehyde. Those that received cysteine had an 80% survival rate; when both cysteine and thiamine were administered, all animals survived. No direct evidence indicates its effectiveness in humans who consume alcohol at low levels.
"N"-Acetylcysteine.
"N"-Acetyl-L-cysteine is a derivative of cysteine wherein an acetyl group is attached to the nitrogen atom. This compound is sold as a dietary supplement, and used as an antidote in cases of acetaminophen overdose, and obsessive compulsive disorders such as trichotillomania.
Sheep.
Cysteine is required by sheep to produce wool: It is an essential amino acid that must be taken in from their feed. As a consequence, during drought conditions, sheep produce less wool; however, transgenic sheep that can make their own cysteine have been developed.
Dietary Restrictions.
The presence of L-Cysteine is often a point of contention for people following dietary restrictions such as Kosher, Halal, Vegan or Vegetarian as it may be sourced from various human or animal sources. As a result an increasing amount of L-Cysteine is produced via a microbial or other synthetic processes.

</doc>
<doc id="52647" url="https://en.wikipedia.org/wiki?curid=52647" title="Liberty (disambiguation)">
Liberty (disambiguation)

Liberty is the quality individuals have to control their own actions.
Liberty may also refer to:

</doc>
<doc id="52648" url="https://en.wikipedia.org/wiki?curid=52648" title="Camera">
Camera

A camera is an optical instrument for recording or capturing images, which may be stored locally, transmitted to another location, or both. The images may be individual still photographs or sequences of images constituting videos or movies.The camera is a remote sensing device as it senses subjects without physical contact. The word "camera" comes from "camera obscura", which means "dark chamber" and is the Latin name of the original device for projecting an image of external reality onto a flat surface. The modern photographic camera evolved from the camera obscura. The functioning of the camera is very similar to the functioning of the human eye.
Functional description.
A camera may work with the light of the visible spectrum or with other portions of the electromagnetic spectrum. A still camera is an optical device which creates a single image of an object or scene, and records it on an electronic sensor or photographic film. All cameras use the same basic design: light enters an enclosed box through a converging lens/convex lens and an image is recorded on a light-sensitive medium(mainly a transition metal-hallide). A shutter mechanism controls the length of time that light can enter the camera. Most photographic cameras have functions that allow a person to view the scene to be recorded, allow for a desired part of the scene to be in focus, and to control the exposure so that it is not too bright or too dim. A display, often a liquid crystal display (LCD), permits the user to view scene to be recorded and settings such as ISO speed, exposure, and shutter speed.
A movie camera or a video camera operates similarly to a still camera, except it records a series of static images in rapid succession, commonly at a rate of 24 frames per second. When the images are combined and displayed in order, the illusion of motion is achieved.
Thanks to the help of modern science, A group of photoscientists of MIT has successfully created a camera having frame rate of 1 trillion per second, able to see the light emerging and reflecting or refracting on a opaque or translucent media.
History.
The forerunner to the photographic camera was the "camera obscura". In the fifth century B.C., the Chinese philosopher Mo Ti noted that a pinhole can form an inverted and focused image, when light passes through the hole and into a dark area. Mo Ti is the first recorded person to have exploited this phenomenon to trace the inverted image to create a picture. Writing in the fourth century B.C., Aristotle also mentioned this principle. He described observing a partial solar eclipse in 330 B.C. by seeing the image of the Sun projected through the small spaces between the leaves of a tree. In the tenth century, the Arabic scholar Ibn al-Haytham (Alhazen) also wrote about observing a solar eclipse through a pinhole, and he described how a sharper image could be produced by making the opening of the pinhole smaller. English philosopher Roger Bacon wrote about these optical principles in his 1267 treatise "Perspectiva". By the fifteenth century, artists and scientists were using this phenomenon to make observations. Originally, an observer had to enter an actual room, in which a pinhole was made on one wall. On the opposite wall, the observer would view the inverted image of the outside. The name "camera obscura", Latin for "dark room", derives from this early implementation of the optical phenomenon. The term was first coined by mathematician and astronomer Johannes Kepler in his "Ad Vitellionem paralipomena" of 1604.
The Italian scientist Giambattista della Porta described the camera obscura in detail in his 1558 work "Magia Naturalis", and specifically suggested that an artist could project a camera obscura's images onto paper, and trace the outlines. The camera obscura was popular as an aid for drawing and painting from the 1600s to the 1800s. Portable set-ups were devised in the 17th century. For example, Kepler had built a portable tent, and outfitted the camera obscura with a lens by 1620. This set-up remained popular up to the early 1800s. The scientist Robert Hooke presented a paper in 1694 to the Royal Society, in which he described a portable camera obscura. It was a cone-shaped box which fit onto the head and shoulders of its user. A hand-held device with a mirror reflex mechanism was first proposed by Johann Zahn in 1685, a design that would later be used in photographic cameras.
Before the development of the photographic camera, it had been known for hundreds of years that some substances, such as silver salts, darkened when exposed to sunlight. In a series of experiments, published in 1727, the German scientist Johann Heinrich Schulze demonstrated that the darkening of the salts was due to light alone, and not influenced by heat or exposure to air. The Swedish chemist Carl Wilhelm Scheele showed in 1777 that silver chloride was especially susceptible to darkening from light exposure, and that once darkened, it becomes insoluble in an ammonia solution. The first person to use this chemistry to create images was Thomas Wedgwood. To create images, Wedgwood placed items, such as leaves and insect wings, on ceramic pots coated with silver nitrate, and exposed the set-up to light. These images weren't permanent, however, as Wedgwood didn't employ a fixing mechanism. He ultimately failed at his goal of using the process to create fixed images created by a camera obscura.
The first permanent photograph of a camera image was made in 1826 by Joseph Nicéphore Niépce using a sliding wooden box camera made by Charles and Vincent Chevalier in Paris. Niépce had been experimenting with ways to fix the images of a camera obscura since 1816. The photograph Niépce succeeded in creating shows the view from his window. It was made using an 8-hour exposure on pewter coated with bitumen. Niépce called his process "heliography". Niépce corresponded with the inventor Louis-Jacques-Mande Daguerre, and the pair entered into a partnership to improve the heliographic process. Niépce had experimented further with other chemicals, to improve contrast in his heliographs. Daguerre contributed an improved camera obscura design, but the partnership ended when Niépce died in 1833. Daguerre succeeded in developing a high-contrast and extremely sharp image by exposing on a plate coated with silver iodide, and exposing this plate again to mercury vapor. By 1837, he was able to fix the images with a common salt solution. He called this process "Daguerreotype", and tried unsuccessfully for a couple years to commercialize it. Eventually, with help of the scientist and politician François Arago, the French government acquired Daguerre's process for public release. In exchange, pensions were provided to Daguerre as well as Niépce's son, Isidore.
In the 1830s, the English scientist Henry Fox Talbot independently invented a process to fix camera images using silver salts. Although dismayed that Daguerre had beaten him to the announcement of photography, on January 31, 1839 he submitted a pamphlet to the Royal Institution entitled "Some Account of the Art of Photogenic Drawing", which was the first published description of photography. Within two years, Talbot developed a two-step process for creating photographs on paper, which he called calotypes. The calotyping process was the first to utilize negative prints, which reverse all values in the photograph - black shows up as white and vice versa. Negative prints allow, in principle, unlimited duplicates of the positive print to be made. Calotyping also introduced the ability for a printmaker to alter the resulting image through retouching. Calotypes were never as popular or widespread as daguerreotypes, owing mainly to the fact that the latter produced sharper details. However, because daguerreotypes only produce a direct positive print, no duplicates can be made. It is the two-step negative/positive process that formed the basis for modern photography.
The first photographic camera developed for commercial manufacture was a daguerreotype camera, built by Alphonse Giroux in 1839. Giroux signed a contract with Daguerre and Isidore Niépce to produce the cameras in France, with each device and accessories costing 400 francs. The camera was a double-box design, with a landscape lens fitted to the outer box, and a holder for a ground glass focusing screen and image plate on the inner box. By sliding the inner box, objects at various distances could be brought to as sharp a focus as desired. After a satisfactory image had been focused on the screen, the screen was replaced with a sensitized plate. A knurled wheel controlled a copper flap in front of the lens, which functioned as a shutter. The early daguerreotype cameras required long exposure times, which in 1839 could be from 5 to 30 minutes.
After the introduction of the Giroux daguerreotype camera, other manufacturers quickly produced improved variations. Charles Chevalier, who had earlier provided Niépce with lenses, created in 1841 a double-box camera using a half-sized plate for imaging. Chevalier’s camera had a hinged bed, allowing for half of the bed to fold onto the back of the nested box. In addition to having increased portability, the camera had a faster lens, bringing exposure times down to 3 minutes, and a prism at the front of the lens, which allowed the image to be laterally correct. Another French design emerged in 1841, created by Marc Antoine Gaudin. The Nouvel Appareil Gaudin camera had a metal disc with three differently-sized holes mounted on the front of the lens. Rotating to a different hole effectively provided variable f-stops, letting in different amount of light into the camera. Instead of using nested boxes to focus, the Gaudin camera used nested brass tubes. In Germany, Peter Friedrich Voigtländer designed an all-metal camera with a conical shape that produced circular pictures of about 3 inches in diameter. The distinguishing characteristic of the Voigtländer camera was its use of a lens designed by Josef Max Petzval. The f/3.5 Petzval lens was nearly 30 times faster than any other lens of the period, and was the first to be made specifically for portraiture. Its design was the most widely used for portraits until Carl Zeiss introduced the anastigmat lens in 1889.
Within a decade of being introduced in America, 3 general forms of camera were in popular use: the American- or chamfered-box camera, the Robert’s-type camera or “Boston box”, and the Lewis-type camera. The American-box camera had beveled edges at the front and rear, and an opening in the rear where the formed image could be viewed on ground glass. The top of the camera had hinged doors for placing photographic plates. Inside there was one available slot for distant objects, and another slot in the back for close-ups. The lens was focused either by sliding or with a rack and pinion mechanism. The Robert’s-type cameras were similar to the American-box, except for having a knob-fronted worm gear on the front of the camera, which moved the back box for focusing. Many Robert’s-type cameras allowed focusing directly on the lens mount. The third popular daguerreotype camera in America was the Lewis-type, introduced in 1851, which utilized a bellows for focusing. The main body of the Lewis-type camera was mounted on the front box, but the rear section was slotted into the bed for easy sliding. Once focused, a set screw was tightened to hold the rear section in place. Having the bellows in the middle of the body facilitated making a second, in-camera copy of the original image.
Daguerreotype cameras formed images on silvered copper plates. The earliest daguerreotype cameras required several minutes to half an hour to expose images on the plates. By 1840, exposure times were reduced to just a few seconds owing to improvements in the chemical preparation and development processes, and to advances in lens design. American daguerreotypists introduced manufactured plates in mass production, and plate sizes became internationally standardized: whole plate (6.5 x 8.5 inches), three-quarter plate (5.5 x 7 1/8 inches), half plate (4.5 x 5.5 inches), quarter plate (3.25 x 4.25 inches), sixth plate (2.75 x 3.25 inches), and ninth plate (2 x 2.5 inches). Plates were often cut to fit cases and jewelry with circular and oval shapes. Larger plates were produced, with sizes such as 9 x 13 inches (“double-whole” plate), or 13.5 x 16.5 inches (Southworth & Hawes’ plate).
The collodion wet plate process that gradually replaced the daguerreotype during the 1850s required photographers to coat and sensitize thin glass or iron plates shortly before use and expose them in the camera while still wet. Early wet plate cameras were very simple and little different from Daguerreotype cameras, but more sophisticated designs eventually appeared. The Dubroni of 1864 allowed the sensitizing and developing of the plates to be carried out inside the camera itself rather than in a separate darkroom. Other cameras were fitted with multiple lenses for photographing several small portraits on a single larger plate, useful when making cartes de visite. It was during the wet plate era that the use of bellows for focusing became widespread, making the bulkier and less easily adjusted nested box design obsolete.
For many years, exposure times were long enough that the photographer simply removed the lens cap, counted off the number of seconds (or minutes) estimated to be required by the lighting conditions, then replaced the cap. As more sensitive photographic materials became available, cameras began to incorporate mechanical shutter mechanisms that allowed very short and accurately timed exposures to be made.
The use of photographic film was pioneered by George Eastman, who started manufacturing paper film in 1885 before switching to celluloid in 1889. His first camera, which he called the "Kodak," was first offered for sale in 1888. It was a very simple box camera with a fixed-focus lens and single shutter speed, which along with its relatively low price appealed to the average consumer. The Kodak came pre-loaded with enough film for 100 exposures and needed to be sent back to the factory for processing and reloading when the roll was finished. By the end of the 19th century Eastman had expanded his lineup to several models including both box and folding cameras.
Films also made possible capture of motion (cinematography) establishing the movie industry by end of 19th century.
The first camera using digital electronics to capture and store images was developed by Kodak engineer Steven Sasson in 1975. He used a charge-coupled device (CCD) provided by Fairchild Semiconductor, which provided only 0.01 megapixels to capture images. Sasson combined the CCD device with movie camera parts to create a digital camera that saved black and white images onto a cassette tape. The images were then read from the cassette and viewed on a TV monitor. Later, cassette tapes were replaced by flash memory.
Gradually in the 2000s and 2010s, digital cameras became the dominant type of camera across consumer, television and movies.
Mechanics.
Image capture.
Traditional cameras capture light onto photographic plate or photographic film. Video and digital cameras use an electronic image sensor, usually a charge coupled device (CCD) or a CMOS sensor to capture images which can be transferred or stored in a memory card or other storage inside the camera for later playback or processing.
Cameras that capture many images in sequence are known as movie cameras or as ciné cameras in Europe; those designed for single images are still cameras.
However these categories overlap as still cameras are often used to capture moving images in special effects work and many modern cameras can quickly switch between still and motion recording modes.
Lens.
The lens of a camera captures the light from the subject and brings it to a focus on the sensor. The design and manufacture of the lens is critical to the quality of the photograph being taken. The technological revolution in camera design in the 19th century revolutionized optical glass manufacture and lens design with great benefits for modern lens manufacture in a wide range of optical instruments from reading glasses to microscopes. Pioneers included Zeiss and Leitz.
Camera lenses are made in a wide range of focal lengths. They range from extreme wide angle, and standard, medium telephoto. Each lens is best suited to a certain type of photography. The extreme wide angle may be preferred for architecture because it has the capacity to capture a wide view of a building. The normal lens, because it often has a wide aperture, is often used for street and documentary photography. The telephoto lens is useful for sports and wildlife but it is more susceptible to camera shake.
Focus.
Due to the optical properties of photographic lenses, only objects within a limited range of distances from the camera will be reproduced clearly. The process of adjusting this range is known as changing the camera's focus. There are various ways of focusing a camera accurately. The simplest cameras have fixed focus and use a small aperture and wide-angle lens to ensure that everything within a certain range of distance from the lens, usually around 3 metres (10 ft) to infinity, is in reasonable focus. Fixed focus cameras are usually inexpensive types, such as single-use cameras. The camera can also have a limited focusing range or scale-focus that is indicated on the camera body. The user will guess or calculate the distance to the subject and adjust the focus accordingly. On some cameras this is indicated by symbols (head-and-shoulders; two people standing upright; one tree; mountains).
Rangefinder cameras allow the distance to objects to be measured by means of a coupled parallax unit on top of the camera, allowing the focus to be set with accuracy. Single-lens reflex cameras allow the photographer to determine the focus and composition visually using the objective lens and a moving mirror to project the image onto a ground glass or plastic micro-prism screen. Twin-lens reflex cameras use an objective lens and a focusing lens unit (usually identical to the objective lens.) in a parallel body for composition and focusing. View cameras use a ground glass screen which is removed and replaced by either a photographic plate or a reusable holder containing sheet film before exposure. Modern cameras often offer autofocus systems to focus the camera automatically by a variety of methods.
Some experimental cameras, for example the planar Fourier capture array (PFCA), do not require focusing to allow them to take pictures. In conventional digital photography, lenses or mirrors map all of the light originating from a single point of an in-focus object to a single point at the sensor plane. Each pixel thus relates an independent piece of information about the far-away scene. In contrast, a PFCA does not have a lens or mirror, but each pixel has an idiosyncratic pair of diffraction gratings above it, allowing each pixel to likewise relate an independent piece of information (specifically, one component of the 2D Fourier transform) about the far-away scene. Together, complete scene information is captured and images can be reconstructed by computation.
Some cameras have post focusing. Post focusing means take the pictures first and then focusing later at the personal computer. The camera uses many tiny lenses on the sensor to capture light from every camera angle of a scene and is called plenoptics technology. A current plenoptic camera design has 40,000 lenses working together to grab the optimal picture.
Exposure control.
The size of the aperture and the brightness of the scene controls the amount of light that enters the camera during a period of time, and the shutter controls the length of time that the light hits the recording surface. Equivalent exposures can be made using a large aperture size with a fast shutter speed and a small aperture with a slow shutter.
Shutters.
Although a range of different shutter devices have been used during the development of the camera only two types have been widely used and remain in use today.
The Leaf shutter or more precisely the in-lens shutter is a shutter contained within the lens structure, often close to the diaphragm consisting of a number of metal leaves which are maintained under spring tension and which are opened and then closed when the shutter is released. The exposure time is determined by the interval between opening and closing. In this shutter design, the whole film frame is exposed at one time. This makes flash synchronisation much simpler as the flash only needs to fire once the shutter is fully open. Disadvantages of such shutters are their inability to reliably produce very fast shutter speeds ( faster than 1/500th second or so) and the additional cost and weight of having to include a shutter mechanism for every lens.
The focal-plane shutter operates as close to the film plane as possible and consists of cloth curtains that are pulled across the film plane with a carefully determined gap between the two curtains (typically running horizontally) or consisting of a series of metal plates (typically moving vertically) just in front of the film plane. The focal-plane shutter is primarily associated with the single lens reflex type of cameras, since covering the film rather than blocking light passing through the lens allows the photographer to view through the lens at all times "except" during the exposure itself. Covering the film also facilitates removing the lens from a loaded camera (many SLRs have interchangeable lenses).
Complexities.
Professional medium format SLR (single-lens-reflex) cameras (typically using 120/220 roll film) use a hybrid solution, since such a large focal-plane shutter would be difficult to make and/or may run slowly. A manually inserted blade known as a dark slide allows the film to be covered when changing lenses or film backs. A blind inside the camera covers the film prior to and after the exposure (but is not designed to be able to give accurately controlled exposure times) and a leaf shutter that is normally "open" is installed in the lens. To take a picture, the leaf shutter closes, the blind opens, the leaf shutter opens then closes again, and finally the blind closes and the leaf shutter re-opens (the last step may only occur when the shutter is re-cocked).
Using a focal-plane shutter, exposing the whole film plane can take much longer than the exposure time. The exposure time does not depend on the time taken to make the exposure over all, only on the difference between the time a specific point on the film is uncovered and then covered up again. For example, an exposure of 1/1000 second may be achieved by the shutter curtains moving across the film plane in 1/50th of a second but with the two curtains only separated by 1/20th of the frame width. In fact in practice the curtains do not run at a constant speed as they would in an ideal design, obtaining an even exposure time depends mainly on being able to make the two curtains accelerate in a similar manner.
When photographing rapidly moving objects, the use of a focal-plane shutter can produce some unexpected effects, since the film closest to the start position of the curtains is exposed earlier than the film closest to the end position. Typically this can result in a moving object leaving a slanting image. The direction of the slant depends on the direction the shutter curtains run in (noting also that as in all cameras the image is inverted and reversed by the lens, i.e. "top-left" is at the bottom right of the sensor as seen by a photographer behind the camera).
Focal-plane shutters are also difficult to synchronise with flash bulbs and electronic flash and it is often only possible to use flash at shutter speeds where the curtain that opens to reveal the film completes its run and the film is fully uncovered, before the second curtain starts to travel and cover it up again. Typically 35mm film SLRs could sync flash at only up to 1/60th second if the camera has horizontal run cloth curtains, and 1/125th if using a vertical run metal shutter.
Formats.
A wide range of film and plate formats have been used by cameras. In the early history plate sizes were often specific for the make and model of camera although there quickly developed some standardisation for the more popular cameras. The introduction of roll film drove the standardization process still further so that by the 1950s only a few standard roll films were in use. These included 120 film providing 8, 12 or 16 exposures, 220 film providing 16 or 24 exposures, 127 film providing 8 or 12 exposures (principally in Brownie cameras) and 135 (35 mm film) providing 12, 20 or 36 exposures – or up to 72 exposures in the half-frame format or in bulk cassettes for the Leica Camera range.
For cine cameras, film 35 mm wide and perforated with sprocket holes was established as the standard format in the 1890s. It was used for nearly all film-based professional motion picture production. For amateur use, several smaller and therefore less expensive formats were introduced. 17.5 mm film, created by splitting 35 mm film, was one early amateur format, but 9.5 mm film, introduced in Europe in 1922, and 16 mm film, introduced in the US in 1923, soon became the standards for "home movies" in their respective hemispheres. In 1932, the even more economical 8 mm format was created by doubling the number of perforations in 16 mm film, then splitting it, usually after exposure and processing. The Super 8 format, still 8 mm wide but with smaller perforations to make room for substantially larger film frames, was introduced in 1965.
Camera accessories.
Accessories for cameras are mainly for care, protection, special effects and functions.
Camera designs.
Plate camera.
The earliest cameras produced in significant numbers used sensitised glass plates were "plate cameras". Light entered a lens mounted on a lens board which was separated from the plate by an extendible bellows.There were simple box cameras for glass plates but also single-lens reflex cameras with interchangeable lenses and even for color photography (Autochrome Lumière). Many of these cameras had controls to raise or lower the lens and to tilt it forwards or backwards to control perspective.
Focussing of these plate cameras was by the use of a ground glass screen at the point of focus. Because lens design only allowed rather small aperture lenses, the image on the ground glass screen was faint and most photographers had a dark cloth to cover their heads to allow focussing and composition to be carried out more easily. When focus and composition were satisfactory, the ground glass screen was removed and a sensitised plate put in its place protected by a dark slide. To make the exposure, the dark slide was carefully slid out and the shutter opened and then closed and the dark slide replaced.
Glass plates were later replaced by sheet film in a dark slide for sheet film; adaptor sleeves were made to allow sheet film to be used in plate holders. In addition to the ground glass, a simple optical viewfinder was often fitted. Cameras which take single exposures on sheet film and are functionally identical to plate cameras were used for static, high-image-quality work; much longer in 20th century, see Large-format camera, below.
Folding camera.
The introduction of films enabled the existing designs for plate cameras to be made much smaller and for the base-plate to be hinged so that it could be folded up compressing the bellows. These designs were very compact and small models were dubbed "vest pocket" cameras. Folding rollfilm cameras were preceded by folding plate cameras, more compact than other designs.
Box camera.
Box cameras were introduced as a budget level camera and had few if any controls. The original box Brownie models had a small reflex viewfinder mounted on the top of the camera and had no aperture or focusing controls and just a simple shutter. Later models such as the Brownie 127 had larger direct view optical viewfinders together with a curved film path to reduce the impact of deficiencies in the lens.
Rangefinder camera.
As camera a lens technology developed and wide aperture lenses became more common, rangefinder cameras were introduced to make focussing more precise. Early rangefinders had two separate viewfinder windows, one of which is linked to the focusing mechanisms and moved right or left as the focusing ring is turned. The two separate images are brought together on a ground glass viewing screen. When vertical lines in the object being photographed meet exactly in the combined image, the object is in focus. A normal composition viewfinder is also provided. Later the viewfinder and rangefinder were combined. Many rangefinder cameras had interchangeable lenses, each lens requiring its own range- and viewfinder linkages.
Rangefinder cameras were produced in half- and full-frame 35 mm and rollfilm (medium format).
Instant picture camera.
After exposure every photograph is taken through pinch rollers inside of the instant camera. Thereby the developer paste contained in the paper 'sandwich' distributes on the image. After a minute, the cover sheet just needs to be removed and one gets a single original positive image with a fixed format. With some systems it was also possible to create an instant image negative, from which then could be made copies in the photo lab. The ultimate development was the SX-70 system of Polaroid, in which a row of ten shots - engine driven - could be made without having to remove any cover sheets from the picture. There were instant cameras for a variety of formats, as well as cartridges with instant film for normal system cameras.
Single-lens reflex.
In the single-lens reflex camera, the photographer sees the scene through the camera lens. This avoids the problem of parallax which occurs when the viewfinder or viewing lens is separated from the taking lens. Single-lens reflex cameras have been made in several formats including sheet film 5x7" and 4x5", roll film 220/120 taking 8,10, 12 or 16 photographs on a 120 roll and twice that number of a 220 film. These correspond to 6x9, 6x7, 6x6 and 6x4.5 respectively (all dimensions in cm). Notable manufacturers of large format and roll film SLR cameras include Bronica, Graflex, Hasselblad, Mamiya, and Pentax. However the most common format of SLR cameras has been 35 mm and subsequently the migration to digital SLR cameras, using almost identical sized bodies and sometimes using the same lens systems.
Almost all SLR cameras use a front surfaced mirror in the optical path to direct the light from the lens via a viewing screen and pentaprism to the eyepiece. At the time of exposure the mirror is flipped up out of the light path before the shutter opens. Some early cameras experimented with other methods of providing through-the-lens viewing, including the use of a semi-transparent pellicle as in the Canon "Pellix" and others with a small periscope such as in the Corfield Periflex series.
Twin-lens reflex.
Twin-lens reflex cameras used a pair of nearly identical lenses, one to form the image and one as a viewfinder. The lenses were arranged with the viewing lens immediately above the taking lens. The viewing lens projects an image onto a viewing screen which can be seen from above. Some manufacturers such as Mamiya also provided a reflex head to attach to the viewing screen to allow the camera to be held to the eye when in use. The advantage of a TLR was that it could be easily focussed using the viewing screen and that under most circumstances the view seen in the viewing screen was identical to that recorded on film. At close distances however, parallax errors were encountered and some cameras also included an indicator to show what part of the composition would be excluded.
Some TLR had interchangeable lenses but as these had to be paired lenses they were relatively heavy and did not provide the range of focal lengths that the SLR could support. Most TLRs used 120 or 220 film; some used the smaller 127 film.
Large-format camera.
The large-format camera, taking sheet film, is a direct successor of the early plate cameras and remained in use for high quality photography and for technical, architectural and industrial photography. There are three common types, the view camera with its monorail and field camera variants, and the press camera. They have an extensible bellows with the lens and shutter mounted on a lens plate at the front. Backs taking rollfilm, and later digital backs are available in addition to the standard dark slide back. These cameras have a wide range of movements allowing very close control of focus and perspective. Composition and focusing is done on view cameras by viewing a ground-glass screen which is replaced by the film to make the exposure; they are suitable for static subjects only, and are slow to use.
Medium-format camera.
Medium-format cameras have a film size between the large-format cameras and smaller 35mm cameras. Typically these systems use 120 or 220 rollfilm. The most common image sizes are 6×4.5 cm, 6×6 cm and 6×7 cm; the older 6×9 cm is rarely used. The designs of this kind of camera show greater variation than their larger brethren, ranging from monorail systems through the classic Hasselblad model with separate backs, to smaller rangefinder cameras. There are even compact amateur cameras available in this format.
Subminiature camera.
Cameras taking film significantly smaller than 35 mm were made. Subminiature cameras were first produced in the nineteenth century. The expensive 8×11 mm Minox, the only type of camera produced by the company from 1937 to 1976, became very widely known and was often used for espionage (the Minox company later also produced larger cameras). Later inexpensive subminiatures were made for general use, some using rewound 16 mm cine film. Image quality with these small film sizes was limited.
Movie camera.
A ciné camera or movie camera takes a rapid sequence of photographs on image sensor or strips of film. In contrast to a still camera, which captures a single snapshot at a time, the ciné camera takes a series of images, each called a "frame" through the use of an intermittent mechanism.
The frames are later played back in a ciné projector at a specific speed, called the "frame rate" (number of frames per second). While viewing, a person's eyes and brain merge the separate pictures to create the illusion of motion. The first ciné camera was built around 1888 and by 1890 several types were being manufactured. The standard film size for ciné cameras was quickly established as 35mm film and this remained in use until transition to digital cinematography. Other professional standard formats include 70 mm film and 16mm film whilst amateurs film makers used 9.5 mm film, 8mm film or Standard 8 and Super 8 before the move into digital format.
The size and complexity of ciné cameras varies greatly depending on the uses required of the camera. Some professional equipment is very large and too heavy to be hand held whilst some amateur cameras were designed to be very small and light for single-handed operation.
Camcorders.
A camcorder is an electronic device combining a video camera and a video recorder. Although marketing materials may use the colloquial term "camcorder", the name on the package and manual is often "video camera recorder". Most devices capable of recording video are camera phones and digital cameras primarily intended for still pictures; the term "camcorder" is used to describe a portable, self-contained device, with video capture and recording its primary function.
Professional video camera.
A professional video camera (often called a television camera even though the use has spread beyond television) is a high-end device for creating electronic moving images (as opposed to a movie camera, that earlier recorded the images on film). Originally developed for use in television studios, they are now also used for music videos, direct-to-video movies, corporate and educational videos, marriage videos etc.
These cameras earlier used vacuum tubes and later electronic sensors.
Digital camera.
A digital camera (or digicam) is a camera that encodes digital images and videos digitally and stores them for later reproduction. Most cameras sold today are digital, and digital cameras are incorporated into many devices ranging from mobile phones (called camera phones) to vehicles.
Digital and film cameras share an optical system, typically using a lens with a variable diaphragm to focus light onto an image pickup device. The diaphragm and shutter admit the correct amount of light to the imager, just as with film but the image pickup device is electronic rather than chemical. However, unlike film cameras, digital cameras can display images on a screen immediately after being recorded, and store and delete images from memory. Most digital cameras can also record moving videos with sound. Some digital cameras can crop and stitch pictures and perform other elementary image editing.
Consumers adopted digital cameras in 1990s. Professional video cameras transitioned to digital around the 2000s-2010s. Finally movie cameras transitioned to digital in the 2010s.

</doc>
<doc id="52649" url="https://en.wikipedia.org/wiki?curid=52649" title="Acetylcholine">
Acetylcholine

Acetylcholine is an organic chemical that functions in the brain and body of many types of animals, including humans, as a neurotransmitter—a chemical released by nerve cells to send signals to other cells. Its name is derived from its chemical structure: it is an ester of acetic acid and choline. Parts in the body that use or are affected by acetylcholine are referred to as cholinergic. Substances that interfere with acetylcholine activity are called anticholinergics. 
Acetylcholine is the neurotransmitter used at the neuromuscular junction—in other words, it is the chemical that motor neurons of the nervous system release in order to activate muscles. This property means that drugs that affect cholinergic systems can have very dangerous effects ranging from paralysis to convulsions. Acetylcholine is also used as a neurotransmitter in the autonomic nervous system, both as an internal transmitter for the sympathetic nervous system and as the final product released by the parasympathetic nervous system.
Inside the brain acetylcholine functions as a neuromodulator—a chemical that alters the way other brain structures process information rather than a chemical used to transmit information from point to point. The brain contains a number of cholinergic areas, each with distinct functions. They play an important role in arousal, attention, and motivation.
Partly because of its muscle-activating function, but also because of its functions in the autonomic nervous system and brain, a large number of important drugs exert their effects by altering cholinergic transmission. Numerous venoms and toxins produced by plants, animals, and bacteria, as well as chemical nerve agents such as Sarin, cause harm by inactivating or hyperactivating muscles via their influences on the neuromuscular junction. Drugs that act on muscarinic acetylcholine receptors, such as atropine, can be poisonous in large quantities, but in smaller doses they are commonly used to treat certain heart conditions and eye problems. Scopolamine, which acts mainly on muscarinic receptors in the brain, can cause delirium and amnesia. The addictive qualities of nicotine derive from its effects on nicotinic acetylcholine receptors in the brain.
Functions.
Acetylcholine has functions both in the peripheral nervous system (PNS) and in the central nervous system (CNS) as a neuromodulator. In the peripheral nervous system, acetylcholine activates muscles, and is a major neurotransmitter in the autonomic nervous system.
Cellular effects.
Like many other biologically active substances, acetylcholine exerts its effects by binding to and activating receptors located on the surface of cells. There are two main classes of acetylcholine receptor, nicotinic and muscarinic. They are named for chemicals that can selectively activate each type of receptor without activating the other: muscarine is a compound found in the mushroom "Amanita muscaria"; nicotine is found in tobacco.
Nicotinic acetylcholine receptors are ligand-gated ion channels permeable to sodium, potassium, and calcium ions. In other words, they are ion channels embedded in cell membranes, capable of switching from a closed to open state when acetylcholine binds to them; in the open state they allow ions to pass through. Nicotinic receptors come in two main types, known as muscle-type and neuronal-type. The muscle-type can be selectively blocked by curare, the neuronal-type by hexamethonium. The main location of muscle-type receptors is on muscle cells, as described in more detail below. Neuronal-type receptors are located in autonomic ganglia (both sympathetic and parasympathetic), and in the central nervous system.
Muscarinic acetylcholine receptors have a more complex mechanism, and affect target cells over a longer time frame. In mammals, five subtypes of muscarinic receptors have been identified, labeled M1 through M5. All of them function as G protein-coupled receptors, meaning that they exert their effects via a second messenger system. The M1, M3, and M5 subtypes are Gq-coupled; they increase intracellular levels of IP3 and calcium by activating phospholipase C. Their effect on target cells is usually excitatory. The M2 and M4 subtypes are Gi/Go-coupled; they decrease intracellular levels of cAMP by inhibiting adenylate cyclase. Their effect on target cells is usually inhibitory. Muscarinic acetylcholine receptors are found in both the central nervous system and the peripheral nervous system of the heart, lungs, upper gastrointestinal tract, and sweat glands.
Neuromuscular junction.
Acetylcholine is the substance the nervous system uses to activate skeletal muscles, a kind of striated muscle. These are the muscles used for all types of voluntary movement, in contrast to smooth muscle tissue, which is involved in a range of involuntary activities such as movement of food through the gastrointestinal tract and constriction of blood vessels. Skeletal muscles are directly controlled by motor neurons located in the spinal cord or, in a few cases, the brainstem. These motor neurons send their axons through motor nerves, from which they emerge to connect to muscle fibers at a special type of synapse called the neuromuscular junction. 
When a motor neuron generates an action potential, it travels rapidly along the nerve until it reaches the neuromuscular junction, where it initiates an electrochemical process that causes acetylcholine to be released into the space between the presynaptic terminal and the muscle fiber. The acetylcholine molecules then bind to nicotinic ion-channel receptors on the muscle cell membrane, causing the ion channels to open. Sodium ions then flow into the muscle cell, initiating a sequence of steps that finally produce muscle contraction.
Autonomic Nervous System.
The autonomic nervous system controls a wide range of involuntary and unconscious body functions. Its main branches are the sympathetic nervous system and parasympathetic nervous system. Broadly speaking, the function of the sympathetic nervous system is to mobilize the body for action: the slogan often used for it is fight-or-flight. The function of the parasympathetic nervous system is to put the body in a state conducive to rest, regeneration, digestion, and reproduction: it is sometimes described using the slogans "rest and digest" or "feed and breed". Both branches use acetylcholine, but in different ways.
At a schematic level, the sympathetic and parasympathetic nervous systems are both organized in essentially the same way: preganglionic neurons in the central nervous system send projections to neurons located in autonomic ganglia; these neurons then send output projections to virtually every tissue of the body. In both branches the internal connections—the projections from the central nervous system to the autonomic ganglia—use acetylcholine as neurotransmitter, and the receptors it activates are of the nicotinic type. In the parasympathetic nervous system the output connections—the projections from ganglion neurons to tissues that don't belong to the nervous system—also release acetylcholine, acting on muscarinic receptors. In the sympathetic nervous system the output connections mainly release noradrenaline, although acetylcholine is released at a few points, such as the sudomotor innervation of the sweat glands.
Direct Vascular Effects.
Acetylcholine in the serum exerts a direct effect on vascular tone by binding to Muscarinic receptors present on vascular endothelium. These cells respond by increasing production of Nitric Oxide, which signals the surrounding smooth muscle to relax, leading to vasodilation.
Central nervous system.
In the central nervous system, ACh has a variety of effects as a neuromodulator upon plasticity, arousal and reward. ACh has an important role in the enhancement of sensory perceptions when we wake up and in sustaining attention.
Damage to the cholinergic (acetylcholine-producing) system in the brain has been shown to be plausibly associated with the memory deficits associated with Alzheimer's disease.
ACh has also been shown to promote REM sleep.
In the brainstem acetylcholine originates from the Pedunculopontine nucleus and laterodorsal tegmental nucleus collectively known as the mesopontine tegmentum area or pontomesencephalotegmental complex. In the basal forebrain, it originates from the basal optic nucleus of Meynert and medial septal nucleus:
In addition, ACh acts as an important internal transmitter in the striatum, which is part of the basal ganglia. It is released by cholinergic interneurons. In humans, non-human primates and rodents, these interneurons respond to salient environmental stimuli with stereotyped responses that are temporally aligned with the responses of dopaminergic neurons of the substantia nigra.
Decision making.
One well-supported function of acetylcholine (ACh) in cortex is increased responsiveness to sensory stimuli, a form of attention. An additional suggested function of ACh in cortex is suppression of intracortical information transmission. Some forms of learning and plasticity in cortex appear dependent on the presence of acetylcholine. ACh has been implicated in the reporting of expected uncertainty in the environment based both on the suggested functions listed above and results recorded while subjects perform a behavioral cuing task.
Diseases and disorders.
Myasthenia gravis.
The disease myasthenia gravis, characterized by muscle weakness and fatigue, occurs when the body inappropriately produces antibodies against acetylcholine nicotinic receptors, and thus inhibits proper acetylcholine signal transmission. Over time, the motor end plate is destroyed. Drugs that competitively inhibit acetylcholinesterase (e.g., neostigmine, physostigmine, or primarily pyridostigmine) are effective in treating this disorder. They allow endogenously released acetylcholine more time to interact with its respective receptor before being inactivated by acetylcholinesterase in the synaptic cleft (the space between nerve and muscle).
Pharmacology.
Blocking, hindering or mimicking the action of acetylcholine has many uses in medicine. Drugs acting on the acetylcholine system are either agonists to the receptors, stimulating the system, or antagonists, inhibiting it. Acetylcholine receptor agonists and antagonists can either have an effect directly on the receptors or exert their effects indirectly, e.g., by affecting the enzyme acetylcholinesterase, which degrades the receptor ligand. Agonists increase the level of receptor activation, antagonists reduce it.
Acetylcholine itself does not have therapeutic value as a drug for intravenous administration because of its multi-faceted action and rapid inactivation by cholinesterase. However, it is used in the form of eye drops to cause constriction of the pupil during cataract surgery, which facilitates quick post-operational recovery.
Nicotine.
Imitates ACh, activating ACh receptors
Atropine.
Atropine is a non-selective competitive antagonist with Acetylcholine at muscarinic receptors.
Cholinesterase inhibitors.
Many ACh receptor agonists work indirectly by inhibiting the enzyme acetylcholinesterase. The resulting accumulation of acetylcholine causes continuous stimulation of the muscles, glands, and central nervous system, which can result in fatal convulsions if the dose is high.
They are examples of enzyme inhibitors, and increase the action of acetylcholine by delaying its degradation; some have been used as nerve agents (Sarin and VX nerve gas) or pesticides (organophosphates and the carbamates). Many toxins and venoms produced by plants and animals also contain cholinesterase inhibitors. In clinical use, they are administered in low doses to reverse the action of muscle relaxants, to treat myasthenia gravis, and to treat symptoms of Alzheimer's disease (rivastigmine, which increases cholinergic activity in the brain).
Synthesis inhibitors.
Organic mercurial compounds, such as methylmercury, have a high affinity for sulfhydryl groups, which causes dysfunction of the enzyme choline acetyltransferase. This inhibition may lead to acetylcholine deficiency, and can have consequences on motor function.
Release inhibitors.
Botulin acts by suppressing the release of acetylcholine, whereas the venom from a black widow spider (alpha-latrotoxin) has the reverse effect. ACh inhibition causes paralysis. When bitten by a black widow spider, one experiences the wastage of ACh supplies and the muscles begin to contract. If and when the supply is depleted, paralysis occurs.
Biochemical mechanisms.
Acetylcholine is synthesized in certain neurons by the enzyme choline acetyltransferase from the compounds choline and acetyl-CoA. Cholinergic neurons are capable of producing ACh. An example of a central cholinergic area is the nucleus basalis of Meynert in the basal forebrain.
The enzyme acetylcholinesterase converts acetylcholine into the inactive metabolites choline and acetate. This enzyme is abundant in the synaptic cleft, and its role in rapidly clearing free acetylcholine from the synapse is essential for proper muscle function. Certain neurotoxins work by inhibiting acetylcholinesterase, thus leading to excess acetylcholine at the neuromuscular junction, causing paralysis of the muscles needed for breathing and stopping the beating of the heart.
Chemistry.
Acetylcholine is a choline molecule that has been acetylated at the oxygen atom. Because of the presence of a highly polar, charged ammonium group, acetylcholine does not penetrate lipid membranes. Because of this, when the drug is introduced externally, it remains in the extracellular space and does not pass through the blood–brain barrier. A synonym of this drug is miochol.
History.
Acetylcholine (ACh) was first identified in 1915 by Henry Hallett Dale for its actions on heart tissue. It was confirmed as a neurotransmitter by Otto Loewi, who initially gave it the name Vagusstoff because it was released from the vagus nerve. Both received the 1936 Nobel Prize in Physiology or Medicine for their work. Acetylcholine was also the first neurotransmitter to be identified.

</doc>
<doc id="52652" url="https://en.wikipedia.org/wiki?curid=52652" title="NIH (disambiguation)">
NIH (disambiguation)

NIH may refer to:

</doc>
<doc id="52653" url="https://en.wikipedia.org/wiki?curid=52653" title="Squash (sport)">
Squash (sport)

Squash is a racket sport played by two (singles) or four players (doubles) in a four-walled court with a small, hollow rubber ball. The players must alternate in striking the ball with their racket and hit the ball onto the playable surfaces of the four walls of the court.
The game was formerly called squash rackets, a reference to the "squashable" soft ball used in the game (compared with the harder ball used in its sister game rackets).
Squash is recognized by the IOC and supporters are lobbying for its incorporation in a future Olympic program.
History.
The use of stringed rackets is shared with tennis, which dates from the late sixteenth century, though is more directly descended from the game of rackets from England. In "rackets", instead of hitting over a net as in sports such as tennis, players hit a squeezable ball against walls.
Squash was invented in Harrow School out of the older game rackets around 1830 before the game spread to other schools, eventually becoming an international sport. The first courts built at this school were rather dangerous because they were near water pipes, buttresses, chimneys, and ledges. The school soon built four outside courts. Natural rubber was the material of choice for the ball. Students modified their rackets to have a smaller reach to play in these cramped conditions.
The rackets have changed in a similar way to those used in tennis. Squash rackets used to be made out of laminated timber. In the 1980s, construction shifted to lighter materials (such as aluminium and graphite) with small additions of components like Kevlar, boron and titanium. Natural "gut" strings were also replaced with synthetic strings.
In the 19th century the game increased in popularity with various schools, clubs and even private citizens building squash courts, but with no set dimensions. The first squash court in North America appeared at St. Paul's School in Concord, New Hampshire in 1884. In 1904 in Philadelphia, Pennsylvania, the earliest national association of squash in the world was formed as the United States Squash rackets Association, (USSRA), now known as U.S. Squash. In April 1907 the Tennis, rackets & Fives Association set up a sub committee to set standards for squash. Then the sport soon formed, combining the three sports together called “Squash”. In 1912, the RMS "Titanic" had a squash court in first class. The 1st-Class Squash Court was situated on G-Deck and the Spectators Viewing Gallery was on the deck above on F-Deck. To use the Court cost 50 cents in 1912. Passengers could use the court for 1 hour unless others were waiting. It was not until 1923 that the Royal Automobile Club hosted a meeting to further discuss the rules and regulations and another five years elapsed before the Squash rackets Association was formed to set standards for squash in Great Britain.
Playing equipment.
Standard rackets are governed by the rules of the game. Traditionally they were made of laminated wood (typically ash), with a small strung area using natural gut strings. After a rule change in the mid-1980s, they are now almost always made of composite materials or metals (graphite, Kevlar, titanium, boron) with synthetic strings. Modern rackets have maximum dimensions of 686 mm (27.0 in) long and 215 mm (8.5 in) wide, with a maximum strung area of 500 square centimetres (90 sq in). The permitted maximum weight is , but most have a weight between 90 and 150 grams (3–5.3 oz.).
Squash balls are between 39.5 and 40.5 mm in diameter, and have a weight of 23 to 25 grams. They are made with two pieces of rubber compound, glued together to form a hollow sphere and buffed to a matte finish. Different balls are provided for varying temperature and atmospheric conditions and standards of play: more experienced players use slow balls that have less bounce than those used by less experienced players (slower balls tend to "die" in court corners, rather than "standing up" to allow easier shots). Depending on its specific rubber composition, a squash ball has the property that it bounces more at higher temperatures. Squash balls must be hit dozens of times to warm them up at the beginning of a session; cold squash balls have very little bounce.
Small colored dots on the ball indicate its dynamic level (bounciness), and thus the standard of play for which it is suited. The recognized speed colors indicating the degree of dynamism are:
Some ball manufacturers such as Dunlop use a different method of grading balls based on experience. They still have the equivalent dot rating, but are named to help choose a ball that is appropriate for one's skill level. The four different ball types are Intro (Blue dot, 140% of Pro bounce), Progress (Red dot, 120% of Pro bounce), Competition (single yellow dot, 110% of Pro bounce) and Pro (double yellow dot).
The "double-yellow dot" ball, introduced in 2000, is the competition standard, replacing the earlier "yellow-dot" ball. There is also an "orange dot" ball for use at high altitudes.
Players wear comfortable sports clothing. In competition, men usually wear shorts and a T-shirt, tank top or a polo shirt. Women normally wear a skirt or skort and a T-shirt or a tank top, or a sports dress. The National Institutes of Health recommends wearing goggles with polycarbonate lenses.
Many squash venues mandate the use of eye protection and some association rules require that all juniors and doubles players must wear eye protection.
The court.
The squash court is a playing surface surrounded by four walls. The court surface contains a front line separating the front and back of the court and a half court line, separating the left and right hand sides of the back portion of the court, creating three 'boxes': the front half, the back left quarter and the back right quarter. Both the back two boxes contain smaller service boxes. The floor-markings on a squash court are only relevant during serves.
There are four walls to a squash court. The front wall, on which three parallel lines are marked, has the largest playing surface, whilst the back wall, which typically contains the entrance to the court, has the smallest. The out line runs along the top of the front wall, descending along the side walls to the back wall. There are no other markings on the side or back walls. Shots struck above or touching the out line, on any wall, are out. The bottom line of the front wall marks the top of the 'tin', a half metre-high metal area which if struck means that the ball is out. In this way the tin can be seen as analogous to the net in other racket sports such as tennis. The middle line of the front wall is the service line and is only relevant during serves.
Game play.
Service.
The players spin a racket to decide who serves first. This player starts the first rally by electing to serve from either the left or right service box. For a legal serve, one of the server's feet must be touching the service box, not touching any part of the service box lines, as the player strikes the ball. After being struck by the racket, the ball must strike the front wall above the service line and below the out line and land in the opposite back quarter court. The receiving player can choose to volley a serve after it has hit the front wall. If the server wins the point, the two players switch sides for the following point.
Play.
After the serve, the players take turns hitting the ball against the front wall, above the tin and below the out line. The ball may strike the side or back walls at any time, as long as it hits below the out line. It must not hit the floor after hitting the racket and before hitting the front wall. A ball landing on either the out line or the line along the top of the tin is considered to be out. After the ball hits the front wall, it is allowed to bounce once on the floor (and any number of times against the side or back walls) before a player must return it. Players may move anywhere around the court but accidental or deliberate obstruction of the other player's movements is forbidden. Players typically return to the centre of the court after making a shot.
Scoring systems.
Squash scoring systems have evolved over time. The original scoring
system is known as English scoring, also called hand-out scoring.
Under this system, if the server wins a rally, they receive a point,
while if the returner wins rally, only the service changes (i.e.,
the ball goes "hand-out") and no point is given. The first player to
reach 9 points wins the game. However, if the score reaches 8–8, the
player who was first to reach 8 decides whether the game will be
played to 9, as before (called "set one"), or to 10 (called "set
two"). At one time this scoring system was preferred in Britain, and
also among countries with traditional British ties, such as Australia,
Canada, Pakistan, South Africa, India and Sri Lanka.
The current official scoring system for all levels of professional and
amateur squash is called point-a-rally scoring (PARS). In PARS, the
winner of a rally always receives a point, regardless of whether they
were the server or returner. Games are played to 11, but in contrast
to English scoring, players must win by two clear points. That is, if
the score reaches 10–10, play continues until one player wins by two
points. PARS to 11 is now used on the men's and women's professional tour, and
the tin height has been lowered by two inches (to 17 inches) for all PSA events (men's and women's).
Another scoring system is American scoring. The rules of American scoring are identical to PARS, apart from games are played to 15. This system is not widely used because games were considered to last too long and the winner would usually be the fitter player, not necessarily the better player.
Competition matches are usually played to "best-of-five" (i.e. the
first player to win three games)
Types of shots played.
In squash, there are many types of shots played that lead to interesting games and strategy.
Strategy and tactics.
A key strategy in squash is known as "dominating the T" (the intersection of the red lines near the centre of the court where the player is in the best position to retrieve the opponent's next shot). Skilled players will return a shot, and then move back toward the "T" before playing the next shot. From this position, the player can quickly access any part of the court to retrieve the opponent's next shot with a minimum of movement.
A common strategy is to hit the ball straight up the side walls to the back corners; this is the basic squash shot, referred to as a "rail," straight drive, wall, or "length." After hitting this shot, the player will then move to the centre of the court near the "T" to be well placed to retrieve the opponent's return. Attacking with soft or "short" shots to the front corners (referred to as "drop shots") causes the opponent to cover more of the court and may result in an outright winner. Boasts or angle shots are deliberately struck off one of the side walls before the ball reaches the front. They are used for deception and again to cause the opponent to cover more of the court.
Rallies between experienced players may involve 30 or more shots and therefore a very high premium is placed on fitness, both aerobic and anaerobic. As players become more skilled and, in particular, better able to retrieve shots, points often become a war of attrition. At higher levels of the game, the fitter player has a major advantage.
Ability to change the direction of ball at the last instant is also important to unbalance the opponent. Expert players can anticipate the opponent's shot a few tenths of a second before the average player, giving them a chance to react sooner.
Depending on the style of play, it is common to refer to squash players as
Interference and obstruction.
Interference and obstruction are an inevitable aspect of this sport, since two players are confined within a shared space. Generally, the rules entitle players to a direct straight line access to the ball, room for a reasonable swing and an unobstructed shot to any part of the front wall. When interference occurs, a player may appeal for a "let" and the referee (or the players themselves if there is no official) then interprets the extent of the interference. The referee may elect to allow a let and the players then replay the point, or award a "stroke" to the appealing player (meaning that he is declared the winner of that point) depending on the degree of interference, whether the interfering player made an adequate effort to avoid interfering, and whether the player interfered with was likely to have hit a winning shot had the interference not occurred. An exception to all of this occurs when the interfering player is directly in the path of the other player's swing, effectively preventing the swing, in which case a stroke is always awarded.
When it is deemed that there has been little or no interference, or that it is impossible to say one way or the other, the rules provide that no let is to be allowed, in the interests of continuity of play and the discouraging of spurious appeals for lets. Because of the subjectivity in interpreting the nature and magnitude of interference, the awarding (or withholding) of lets and strokes is often controversial.
When a player's shot hits their opponent prior to hitting the front wall, interference has occurred. If the ball was travelling towards the side wall when it hit the opponent, or if it had already hit the side wall and was travelling directly to the front wall, it is usually a let. However, it is a stroke to the player who hit the ball if the ball was travelling straight to the front wall when the ball hit the opponent, without having first hit the side wall. Generally after a player has been hit by the ball, both players stand still; if the struck player is standing directly in front of the player who hit the ball he loses the stroke, if he is not straight in front, a let is played. If it is deemed that the player who is striking the ball is deliberately trying to hit his opponent, he will lose the stroke. An exception to all of this occurs when the player hitting the ball has "turned", i.e., let the ball pass him on one side, but then hit it on the other side as it came off the back wall. In these cases, the stroke goes to the player who was hit by the ball.
Referee.
 The referee is usually a certified position issued by the club or assigned squash league. The referee has dominant power over the squash players. Any conflict or interference is dealt with by the referee. The referee may also issue to take away points or games due to improper etiquette regarding conduct or rules. Refer to “Interference and Obstruction” for more detail. In addition the referee is usually responsible for the scoring of games.
Nowadays, three referees are usually used in professional tournaments. The Central referee has responsibility to call the score and make decisions with the two side referees.
Cultural, social, and health aspects.
There are several variations of squash played across the world. In the U.S. hardball singles and doubles are played with a much harder ball and different size courts (as noted above). Hardball singles has lost much of its popularity in North America (in favour of the International version), but the hardball doubles game is still active. There is also a doubles version of squash played with the standard ball, sometimes on a wider court, and a more tennis-like variation known as squash tennis.
The relatively small court and low-bouncing ball makes scoring points harder and rallies usually longer than in its American cousin, racketball, as the ball may be played to all four corners of the court. Since every ball must strike the front wall above the tin (unlike racketball), the ball cannot be easily "killed". Another difference between squash and racketball is the service game. Racketball allows for the entire back court (from 20-feet to 40-feet) to be used as a service return area; this makes returning serves much more challenging in racketball than squash. Racketball serves routinely exceed 140 mph (225 km/h) and are a crucial component of the game, similar to tennis.
Squash provides an excellent cardiovascular workout. In one hour of squash, a player may expend approximately 600 to 1000 food calories (3,000 to 4,000 kJ). The sport also provides a good upper and lower body workout by exercising both the legs in running around the court and the arms and torso in swinging the racket. In 2003, "Forbes" rated squash as the number one healthiest sport to play. However, some studies have implicated squash as a cause of possible fatal cardiac arrhythmia and argued that squash is an inappropriate form of exercise for older men with heart disease.
Squash around the world.
According to the World Squash Federation, as of June 2009, there were 49908 squash courts in the world, with 188 countries and territories having at least one court. England had the greatest number at 8,500. The other countries with more than 1,000 courts, in descending order by number were Germany, Egypt, the United States of America, Australia, South Africa, Canada, Malaysia, France, the Netherlands, and Spain. Today, The United States has the fastest growing squash participation. There are an estimated 20 million squash players world-wide.
As of June 2009, there were players from nineteen countries in the top fifty of the men's world rankings, with England and Egypt leading with eleven each. The women's world rankings featured players from sixteen countries, led by England with eleven.
The men's and women's Professional Squash Association tour, men's rankings and women's rankings are run by the Professional Squash Association (PSA).
The Professional Squash Tour is a tour based in the United States.
Wider acceptance.
Squash has been featured regularly at the multi-sport events of the Commonwealth Games and Asian Games since 1998. Squash is also a regular sport at the Pan American Games since 1995. Squash players and associations have lobbied for many years for the sport to be accepted into the Olympic Games, with no success to date. Squash narrowly missed being instated for the 2012 London Games and the 2016 Rio de Janeiro Games (missed out again as the IOC assembly decided to add golf and rugby sevens to the Olympic programme). Squash also missed out as an event in the 2020 Olympic Games. At the 125th IOC Session in Buenos Aires, the IOC voted for Wrestling instead of Squash or Baseball/Softball.
A new bid is expected to be launched for the 2024 Olympics, when the time comes.
Players, records and rankings.
The (British) Squash Rackets Association (now known as England Squash & Racketball) conducted its first British Open championship for men in December 1930, using a "challenge" system. Charles Read was designated champion in 1930, but was beaten in home and away matches by Don Butcher, who was then recorded as the champion for 1931. The championship continues to this day, but has been conducted with a "knockout" format since 1947.
The women's championship started in 1921, and has been dominated by relatively few players: Joyce Cave, Nancy Cave, Cecily Fenwick (England) in the 1920s; Margot Lumb & Susan Noel (England) 1930s; Janet Morgan (England) 1950s; Heather McKay (Australia) 1960s and 1970s; Vicki Cardwell (Australia) and Susan Devoy (New Zealand) 1980s; Michelle Martin and Sarah Fitz-Gerald (Australia) 1990s and Nicol David (Malaysia) 2000s.
The Men's British Open has similarly been dominated by relatively few players: F.D. Amr Bey (Egypt) in the 1930s; Mahmoud Karim (Egypt) in the 1940s; brothers Hashim Khan and Azam Khan (Pakistan) in the 1950s and 1960s; Jonah Barrington (Great Britain and Ireland) and Geoff Hunt (Australia) in the 1960s and 1970s, Jahangir Khan (Pakistan) 1980s ; Jansher Khan (Pakistan) in the 1990s and more recently, David Palmer and Nick Matthew.<br>
The World Open (squash) was inaugurated in 1976 and serves as the main competition today. Jansher Khan holds the record of winning eight World titles followed by Jahangir Khan with six, Geoff Hunt & Amr Shabana four, Nick Matthew & Ramy Ashour three. The women's record is held by Nicol David with eight wins followed by Sarah Fitzgerald five, Susan Devoy four, and Michelle Martin three.
Heather McKay remained undefeated in competitive matches for 19 years (between 1962 and 1981) and won sixteen consecutive British Open titles between 1962 and 1977.
Previous world number one Peter Nicol stated that he believed squash had a "very realistic chance" of being added to the list of Olympic sports for the 2016 Olympic Games, but it ultimately lost out to golf and rugby sevens.
Current rankings.
The Professional Squash Association (PSA) publishes monthly rankings of professional players: Dunlop PSA World Rankings.

</doc>
<doc id="52659" url="https://en.wikipedia.org/wiki?curid=52659" title="Albert Brudzewski">
Albert Brudzewski

Albert Brudzewski, "also" Albert Blar (of Brudzewo), Albert of Brudzewo or Wojciech Brudzewski (in Latin, "Albertus de Brudzewo"; c.1445–c.1497) was a Polish astronomer, mathematician, philosopher and diplomat.
Life.
Albert (in Polish, "Wojciech"), who would sign himself ""de Brudzewo"" ("of Brudzewo"), was born about 1445, in Brudzewo, near Kalisz. Scant information exists about his early life. It is only known that as a 23-year-old he matriculated at the Kraków Academy (now Jagiellonian University), where he remained through nearly all his life, teaching there for two decades. He served as the Academy's dean, as "procurator" (administrator of its property), and as head of the "Bursa Hungarorum" ("Hungarians' Dormitory").
Albert is remembered as a remarkable teacher. At the Kraków Academy he impressed students by his extraordinary knowledge of literature, and taught mathematics and astronomy. When in 1490 he became a bachelor of theology, he also lectured on Aristotle's philosophy. These lectures were attended by Nicolaus Copernicus, who enrolled at the Academy in 1491. A major accomplishment of Albert's was his modernization of the teaching of astronomy by introducing the most up-to-date texts.
Albert was well versed in Georg von Peuerbach's "Theory of the Planets" and Regiomontanus' "Astronomical Tables". He was skeptical of the geocentric system. He was the first to state that the Moon moves in an ellipse and always shows its same side to the Earth.
He drew up tables for calculating the positions of heavenly bodies. In 1482 he wrote a "Commentum planetarium in theoricas Georgii Purbachii"—a commentary on Georg von Peuerbach's text, "New Theories of the Planets"—published in Milan by his pupil, Jan Otto de Kraceusae.
Besides Copernicus, Albert's students included the mathematician Bernard Wapowski and the German poet and Renaissance humanist, Conrad Celtis, who in Kraków established the first Central European literary society, "Sodalitas Litterana Vistulana".
In 1495, at the behest of Cardinal Fryderyk Jagiellończyk (Frederick Jagiellon), Brudzewski moved to Vilnius as secretary to Grand Duke of Lithuania Aleksander Jagiellon, who would later become King Alexander of Poland. He served the Grand Duke as a diplomat; one of his most important missions involved negotiations with Muscovy's Tsar Ivan the Terrible. It was in Vilnius that Albert wrote his treatise, "Conciliator", whose original has yet to be found.
Albert of Brudzewo died in Vilnius. The exact date of his death is not known; some sources state that he died at the age of fifty.

</doc>
<doc id="52662" url="https://en.wikipedia.org/wiki?curid=52662" title="65 BC">
65 BC

__NOTOC__
Year 65 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Cotta and Torquatus (or, less frequently, year 689 "Ab urbe condita"). The denomination 65 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Roman Republic.
</onlyinclude>

</doc>
<doc id="52663" url="https://en.wikipedia.org/wiki?curid=52663" title="64 BC">
64 BC

__NOTOC__
Year 64 BC was a year of the pre-Julian Roman calendar. At the time it was known as the Year of the Consulship of Caesar and Figulus (or, less frequently, year 690 "Ab urbe condita"). The denomination 64 BC for this year has been used since the early medieval period, when the Anno Domini calendar era became the prevalent method in Europe for naming years.
Events.
<onlyinclude>
By place.
Syria.
</onlyinclude>

</doc>
<doc id="52664" url="https://en.wikipedia.org/wiki?curid=52664" title="Racket (sports equipment)">
Racket (sports equipment)

A Racket is a sports implement consisting of a handled frame with an open hoop across which a network of strings or catgut is stretched tightly. It is used for striking a ball or shuttlecock in games such as squash, tennis, racquetball, and badminton. Collectively, these games are known as racket sports. This predecessor to the modern game of squash, rackets, is played with wooden rackets. While squash equipment has evolved in the intervening century, rackets equipment has changed little.
The frame of rackets for all sports was traditionally made of laminated wood and the strings of animal intestine known as catgut. The traditional racket size was limited by the strength and weight of the wooden frame which had to be strong enough to hold the strings and stiff enough to hit the ball or shuttle. Manufacturers started adding non-wood laminates to wood rackets to improve stiffness. Non-wood rackets were made first of steel, then of aluminium, and then carbon fiber composites. Wood is still used for real tennis, rackets, and xare. Most rackets are now made of composite materials including carbon fibre or fiberglass, metals such as titanium alloys, or ceramics.
Cat gut has partially been replaced by synthetic materials including nylon, polyamide, and other polymers. Rackets are restrung when necessary, which may be after every match for a professional or never for a social player.
Spelling.
"Racket" is the standard spelling of the word. "Racquet" is an alternative spelling used more commonly in certain sports (squash, racquetball, badminton) and less commonly in others (tennis). "Racket" is the preferred spelling in tennis. While some writers, especially those outside North America, prefer the French-influenced "racquet", "racket" is the predominant spelling by a large margin. Similarly, while some believe that "racket" came about as a misspelling of "racquet", "racket" is in fact the older spelling: it has been in use since the 16th century, with "racquet" only showing up later in the 19th century as a variant of "racket".
Etymology.
The origin of the term "racket" is unclear. According to a popular belief first published by Malcolm Whitman in 1932, the expression comes from the Arabic term "rahat al-yad", meaning "palm of hand". Modern research however reveals this thesis in a highly questionable light. Instead, the term is more likely to be derived from the Flemish word "raketsen" which is itself derived from Middle French "rachasser", meaning "to strike (the ball) back".
Badminton.
Badminton rackets are light, with top quality rackets weighing between about 80 and 100 grams (with strings). Modern rackets are composed of carbon fibre composite (graphite reinforced plastic), which may be augmented by a variety of materials. Carbon fibre has an excellent strength to weight ratio, is stiff, and gives excellent kinetic energy transfer. Before the adoption of carbon fibre composite, rackets were made of wood to their excessive weight and cost.
There is a wide variety of racket designs, although the racket size and shape are limited by the Laws. Different rackets have playing characteristics that appeal to different players. The traditional oval head shape is still available, but an isometric head shape is increasingly common in new rackets.
Rackets.
This predecessor to the modern game of squash, rackets, is played with wooden rackets. While squash equipment has evolved in the intervening century, rackets equipment has changed little.
Racquetball.
According to the current racquetball rules there are no limitations on the weight of a racquetball racket.
Racquetball rackets, unlike many other types, generally have little or no neck, the grip connecting directly to the head. They also tend to have head shapes that are notably wider at the top, with some older rackets looking almost triangular or teardrop shaped.
Real tennis.
In real tennis, 27-inch (686-mm) long rackets are made of wood and very tight strings to cope with the game's heavy balls. The racket heads are bent slightly to make striking balls close to the floor or in corners easier.
Squash.
Standard squash rackets are governed by the rules of the game. Traditionally they were made of laminated timber (typically Ash), with a small strung area using natural gut strings. After a rule change in the mid-1980s, they are now almost always made of composite materials such as carbon fiber or metals (graphite, Kevlar, titanium, and/or boron) with synthetic strings. Modern rackets are 70 cm long, with a maximum strung area of 500 square centimetres (approximately 75 square inches) and a mass between 90 and 200 grams (4–7 ounces).
Table tennis.
Table tennis uses a table tennis racket made from laminated wood covered with rubber on one or two sides depending on the grip of the player. Unlike a conventional racket, it does not contain strings strung across an open frame. This is called either a paddle, racket, or a bat, with usage differing by region. In the USA the term "paddle" is common, in Europe the term is "bat", and the official ITTF term is "racket."
Tennis.
The parts of a tennis racket are the head, rim, face, neck, butt/butt cap, handle and strings.
Modern tennis rackets vary in length, weight, and head size. is normally a junior's length, while are for stronger and taller adult players. Some are also available at lengths of . Weights of a racket also vary between unstrung and strung. Rackets originally flared outward at the bottom of the handle to prevent slipping. The rounded bottom was called a bark bottom after its inventor Matthew Barker. But by 1947, this style became superfluous. Head size also plays a role in a racket's qualities. A larger head size generally means more power and a larger "sweet spot" that is more forgiving on off-center hits. A smaller head size offers more precise control. Current racket head sizes vary between , with most players adopting one from . Rackets that are mid way in weight, size and player ability, are often called ""tweener rackets."" 
Vibration dampers (also commonly known as "gummies") may be interlaced in the proximal part of the string array for improved feel. 
Throughout most of tennis' history, rackets were made of laminated wood, with heads of around . In the late 1960s, Wilson produced the T2000 steel racket with wire wound around the frame to make string loops. It was popularized by the top American player Jimmy Connors. In 1968 Spalding launched an aluminum racket, called The Smasher. Aluminium, though lighter and more flexible than steel, was less accurate than wood. Because of this, most of the top players still preferred to use wooden frames, and a decade later they were still in use. 
By 1975, aluminum construction improvements allowed for the introduction of the first "oversized" racket, which was manufactured by Weed. Prince popularized the oversize racket, which had a head size of approximately and opened the door for the introduction of rackets having other non-standard head sizes such as mid-size (90 square inches) and mid-plus size (95 square inches).
In the early 1980s, "graphite" (carbon fibre) composites were introduced, and other materials were added to the composite, including ceramics, glass-fibre, boron, and titanium. The Dunlop Max200G used by John McEnroe from 1983 was an early graphite racket, along with the very popular Prince original graphite. Composite rackets are the contemporary standard, the last wooden racquet appeared at Wimbledon in 1987.
Longer rackets were introduced by Dunlop to give additional reach for shots such as the serve and volley where shorter players may be at a disadvantage. Mid-size or mid-plus rackets are the general standard for professional players.
Stringing (material, pattern, tension) is an important factor in the performance of a tennis racket. A few elite players use natural gut, but the vast majority of strings are a nylon or polyester synthetic. Some (American champion Pete Sampras is a prominent example) consider the natural string to be more responsive, providing a better "feel", but synthetic is favored for its much superior durability, consistency, as well as much lower cost. String pattern (the vertical/horizontal grid) is a function of the racket head size and design. A tighter pattern is considered to deliver more precise control; a more "open" pattern to offer greater potential for power and spin. Modern rackets are marked with a recommended string tension range. The basic rule is that a lower tension creates more power (from a "trampoline" effect) and a higher string tension creates more control (the less 'trampoline effect' the more predictable the power and angle of the departure from the string bed.)

</doc>
<doc id="52665" url="https://en.wikipedia.org/wiki?curid=52665" title="Newcastle, New South Wales">
Newcastle, New South Wales

The Newcastle metropolitan area is the second most populated area in the Australian state of New South Wales and includes most of the Newcastle and Lake Macquarie local government areas. It is the hub of the Greater Newcastle area which includes most parts of the local government areas of City of Newcastle, City of Lake Macquarie, City of Cessnock, City of Maitland and Port Stephens Council.
History.
Pre-European settlement.
Newcastle and the lower Hunter Region were traditionally occupied by the Awabakal and Worimi Aboriginal People, who called the area Malubimba.
Founding and settlement by Europeans.
In September 1797 Lieutenant John Shortland became the first European to explore the area. His discovery of the area was largely accidental; as he had been sent in search of a number of convicts who had seized as she was sailing from Sydney Cove. While returning, Lt. Shortland entered what he later described as "a very fine river", which he named after New South Wales' Governor John Hunter. He returned with reports of the deep-water port and the area's abundant coal. Over the next two years, coal mined from the area was the New South Wales colony's first export.
Newcastle gained a reputation as a "hellhole" as it was a place where the most dangerous convicts were sent to dig in the coal mines as harsh punishment for their crimes. By the start of the 19th century the mouth of the Hunter River was being visited by diverse groups of men, including coal diggers, timber-cutters, and more escaped convicts. Philip Gidley King, the Governor of New South Wales from 1800, decided on a more positive approach to exploit the now obvious natural resources of the Hunter Valley. In 1801, a convict camp called King's Town (named after Governor King) was established to mine coal and cut timber. In the same year, the first shipment of coal was dispatched to Sydney. This settlement closed less than a year later.
A settlement was again attempted in 1804, as a place of secondary punishment for unruly convicts. The settlement was named Coal River, also Kingstown and then renamed Newcastle, after England's famous coal port. The name first appeared by the commission issued by Governor King on 15 March 1804 to Lieutenant Charles Menzies of the marine detachment on , then at Port Jackson, appointing him superintendent of the new settlement. The new settlement, comprising convicts and a military guard, arrived at the Hunter River on 27 March 1804 in three ships: , the "Resource" and the "James". The convicts were rebels from the 1804 Castle Hill convict rebellion. The link with Newcastle upon Tyne, UK, its namesake and also whence many of the 19th century coal miners came, is still obvious in some of the place-names – such as Jesmond, Hexham, Wickham, Wallsend and Gateshead. Morpeth, New South Wales is a similar distance north of Newcastle as Morpeth, Northumberland is north of Newcastle upon Tyne.
Under Captain James Wallis, commandant from 1815 to 1818, the convicts' conditions improved, and a building boom began. Captain Wallis laid out the streets of the town, built the first church of the site of the present Christ Church Anglican Cathedral, erected the old gaol on the seashore, and began work on the breakwater which now joins Nobbys Head to the mainland. The quality of these first buildings was poor, and only (a much reinforced) breakwater survives. During this period, in 1816, the oldest public school in Australia was built in East Newcastle.
Newcastle remained a penal settlement until 1822, when the settlement was opened up to farming. As a penal colony, the military rule was harsh, especially at Limeburners' Bay, on the inner side of Stockton peninsula. There, convicts were sent to burn oyster shells for making lime. Military rule in Newcastle ended in 1823. Prisoner numbers were reduced to 100 (most of these were employed on the building of the breakwater), and the remaining 900 were sent to Port Macquarie.
Civilian government and onwards.
After removal of the last convicts in 1823, the town was freed from the infamous influence of the penal law. It began to acquire the aspect of a typical Australian pioneer settlement, and a steady flow of free settlers poured into the hinterland.
The formation during the nineteenth century of the Newcastle and Hunter River Steamship Company saw the establishment of regular steamship services from Morpeth and Newcastle with Sydney. The company had a fleet of freighters as well as several fast passenger vessels, including the PS "Newcastle" and the PS "Namoi". The "Namoi" had first-class cabins with the latest facilities.
Because of the coal supply, small ships plied between Newcastle and Sydney, Brisbane, Melbourne and Adelaide, carrying coal to gas works and bunkers for shipping, and railways. These were commonly known as "sixty-milers", referring to the nautical journey between Newcastle and Sydney. These ships continued in service until recent times.
1920s to present.
During World War II, Newcastle was an important industrial centre for the Australian war effort. Consequently, it was considered to be a potential Japanese target during the Second World War. In the early hours of 8 June 1942, the Japanese submarine briefly shelled Newcastle. Among the areas hit within the city were dockyards, the steel works, Parnell Place in the city's now affluent East End, the breakwall and Art Deco ocean baths. There were no casualties in the attack and damage was minimal.
The Port of Newcastle remains the economic and trade centre for the resource rich Hunter Valley and for much of the north and northwest of New South Wales. Newcastle is the world's largest coal export port and Australia's oldest and second largest tonnage throughput port, with over 3,000 shipping movements handling cargo of 95.8 Mt per annum, of which coal exports represented 90.8 Mt in 2008–09. The volume of coal exported, and attempts to increase coal exports, are opposed by environmental groups.
Newcastle has a small shipbuilding industry, which has declined since the 1970s. In recent years the only major ship-construction contract awarded to the area was the construction of the Huon class minehunters. The era of extensive heavy industry passed when the steel works closed in 1999. Many of the remaining manufacturing industries have located themselves well away from the city itself.
Newcastle has one of the oldest theatre districts in Australia. Victoria Theatre on Perkins Street is the oldest purpose-built theatre in the country. The theatre district that occupied the area around what is now the Hunter Street Mall vanished during the 1940s. The old city centre has seen some new apartments and hotels built in recent years, but the rate of commercial and retail occupation remains low while alternate suburban centres have become more important. The CBD itself is shifting to the west, towards the major urban renewal area known as "Honeysuckle". This renewal, to run for another 10 years, is a major part of arresting the shift of business and residents to the suburbs. Commercial renewal has been accompanied by cultural renaissance. There is a vibrant arts scene in the city including a highly regarded art gallery, and an active Hunter Writers' Centre Recent fictional representations (for example Antoinette Eklund's 'Steel River') present a new vision of the city, using the city's historic past as a backdrop for contemporary fiction.
The old central business district, located at Newcastle's eastern end, still has a considerable number of historic buildings, dominated by Christ Church Cathedral, seat of the Anglican Bishop of Newcastle. Other noteworthy buildings include Fort Scratchley, the Ocean Baths, the old Customs House, the 1920s City Hall, the 1890s Longworth Institute (once regarded as the finest building in the colony) and the 1930s art deco University House (formerly NESCA House, seen in the film "Superman Returns").
Economy.
19th and early 20th centuries.
Coal mining began in earnest on 3 May 1833 when the Australian Agricultural Company received land grants at Newcastle plus a 31-year monopoly on that town's coal traffic. Other collieries were within a radius of the town. Principal coal mines were located at Stockton, Tighes Hill, Carrington and the Newcastle Coal and Copper Company's collieries at Merewether (includes the Glebe), Wallsend and the Waratah collieries. All operations had closed by the early 1960s.
On 10 December 1831 the Australian Agricultural Company officially opened Australia's first railway, at the intersection of Brown & Church Streets, Newcastle, New South Wales. Privately owned and operated to service the "A Pit" coal mine, it was a cast-iron fishbelly rail on an inclined plane as a gravitational railway
In the 1850s, a major copper smelting works was established at Burwood, near Merewether. An engraving of this appeared in the "Illustrated London News" on 11 February 1854. The English and Australian Copper Company built another substantial works at Broadmeadow circa 1890, and in that decade a zinc smelter was built inland, by Cockle Creek.
The largest factory of its kind in the Southern Hemisphere was constructed in 1885, on a site between the suburbs of Tighes Hill and Port Waratah, by Charles Upfold, from London, for his Sydney Soap and Candle Company, to replace a smaller factory in Wickham. Their soap products won 17 medals at International Exhibitions. At the Sydney International Exhibition they won a bronze medal "against all-comers from every part of the world", the only first prize awarded for soap and candles. Following World War I the company was sold to Messrs Lever & Kitchen (today Unilever), and the factory closed in the mid-1930s.
In 1911, BHP chose the city as the site for its steelworks due to the abundance of coal. The land put aside was prime real estate, on the southern edge of the harbour. In 1915, the BHP steelworks opened, beginning a period of some 80 years dominating the steel works and heavy industry. As Mayfield and the suburbs surrounding the steelworks declined in popularity because of pollution, the steelworks thrived, becoming the region's largest employer.
Economic decline, increase in unemployment and return to stability.
In 1999, the steelworks closed after 84 years operation and had employed about 50,000 in its existence, many for decades. The closure of the BHP steelworks occurred at a time of strong economic expansion in Australia. At the time of the closure and since the closure Newcastle experienced a significant amount of economic diversification which has strengthened the local economy. Despite this, the closure caused a deterioration of the employment situation in Newcastle where the unemployment rate rose rapidly to almost 12% from under 9% at the previous trough just prior to the closure.
Since 2003, Australia experienced the effects of the 2000s commodities boom as commodities prices for major export good such as coal and iron ore rose significantly. This provided a large incentive for investment in the Newcastle and Hunter region due to its status as a major coal mining and export hub to Asian markets. Large projects related to the coal industry helped to propel the Newcastle unemployment rate to 20 year lows and allow the Newcastle region to weather the effects of the late 2000s recession better than NSW as a whole. As of 2009 the two largest single employers are the Hunter New England Area Health Service and the University of Newcastle. The National Stock Exchange of Australia (formerly Newcastle Stock Exchange) is based in the city.
Disasters.
1989 Newcastle earthquake.
On 28 December 1989, Newcastle experienced an earthquake measuring 5.6 on the Richter scale, which killed 13 people, injured 162 and destroyed or severely damaged a number of prominent buildings. Some had to be demolished, including the large George Hotel in Scott Street (city), the Century Theatre at Broadmeadow, the Hunter Theatre (formerly 'The Star') and the majority of The Junction school at Merewether. Part of the Newcastle Workers' Club, a popular venue, was destroyed and later replaced by a new structure. The following economic recession of the early 1990s meant that the city took several years to recover.
However, Beaumont St Hamilton, where many buildings sustained major damage, became a thriving cosmopolitan restaurant strip after the earthquake and is still going strong today. The earthquake helped to rekindle business in this suburban strip.
June 2007 Hunter Region and Central Coast storms.
On 8 June 2007 the Hunter and Central Coast regions were battered by the worst series of storms to hit New South Wales in 30 years. This resulted in extensive flooding and nine deaths. Thousands of homes were flooded and many were destroyed. The Hunter and Central Coast regions were declared natural disaster areas by the state Premier, Mr Morris Iemma, on 8 June 2007 . Further flooding was predicted by the Bureau of Meteorology but was less severe than predicted.
During the early stages of the storms, the bulk carrier ship ran aground at Nobby's Beach after failing to heed warnings to move offshore. The "Pasha Bulker" was finally refloated on the third salvage attempt on 2 July 2007 despite earlier fears that the ship would break up. After initially entering the port for minor repairs, it departed under tow on 26 July 2007 for major repairs in Asia.
Maritime.
On 12 July 1866 a paddle steamer the , on its way to Brisbane, Queensland from Newcastle carrying 60 passengers, was caught in a storm as it made its way out of the harbour. 60 lives were lost with only one survivor, Frederick Hedges, who was plucked from the water by the sole survivor of the Dunbar that had sunk in Sydney Harbour.
The most tragic maritime accident of the 20th century in Newcastle occurred on 9 August 1934 when the Stockton-bound ferry "Bluebell" collided with the coastal freighter, "Waraneen", and sank in the middle of the Hunter River. The Bluebell Collision claimed three lives and fifteen passengers were admitted to the Newcastle Hospital, with two suffering severely from the effects of immersion. It was later found that the ferry captain was at fault.
These are only two events in Newcastle's very long history of shipwrecks including the 1974 beaching of the , and the 2007 beaching of the .
Aviation.
On 16 August 1966, an RAAF CAC Sabre crashed into the inner city suburb of The Junction. The pilot, Flying Officer Warren William Goddard, experienced engine troubles and unsuccessfully tried to get the plane over the Pacific Ocean. The Junction is a highly populated suburb of Newcastle and most of the plane wreckage landed in the shopping area of the suburb. In 2007 a memorial plaque was unveiled for the killed pilot.
Geography.
Newcastle is on the southern bank of the Hunter River mouth. The northern side is dominated by sand dunes, swamps and multiple river channels. A "green belt" protecting plant and wildlife flanks the city from the west (Watagan mountains) around to the north where it meets the coast just north of Stockton. Urban development is mainly restricted to the hilly southern bank. The small town of Stockton sits opposite central Newcastle at the river mouth and is linked by ferry. Road access between Stockton and central Newcastle is via the Stockton Bridge, a distance of . Much of the city is undercut by the coal measures of the Sydney sedimentary basin, and what were once numerous coal-mining villages located in the hills and valleys around the port have merged into a single urban area extending southwards to Lake Macquarie.
Climate.
Newcastle has a borderline oceanic/humid subtropical climate like much of central and northern New South Wales. Summers tend to be warm and winters are generally mild. Precipitation is heaviest in late autumn and early winter.
Demographics.
The metropolitan area of Newcastle is the second most populous area in New South Wales, and includes most of the Newcastle and Lake Macquarie local government areas as well as Fern Bay, a southern suburb of Port Stephens Council. At the 2011 census it had a population of 308,308. As of 30 June 2009 the population of the city of Newcastle itself was estimated to be 154,777 while Lake Macquarie was actually larger with a population of 199,277.
Newcastle is often quoted as being the seventh largest city in Australia. This is misleading as the area represented extends well beyond both the City of Newcastle and the Newcastle metropolitan area. The area, officially the Newcastle Statistical District, is referred to as Greater Newcastle or the Lower Hunter Region, which includes most parts of the Newcastle, Lake Macquarie, Cessnock, Maitland and Port Stephens local government areas and, as of 30 June 2009, has an estimated population of 540,796. Despite their proximity, all of the LGAs in the region maintain their own individual identities, separate from Newcastle. Newcastle remains the regional hub for most services.
The demonym for the people of Newcastle is "Novocastrian".
Domestic architecture.
Buildings.
A heritage area to the east of the Central Business District, centred on Christ Church Cathedral, has many Victorian terrace houses.
Structures.
Notable structures in Newcastle include the Queens Wharf Tower and the recently completed ANZAC Walk.
Education.
Primary and secondary schools.
The oldest state school in the area is Newcastle East Public School, a primary school established in 1816. Newcastle East Public School is the oldest continuously operating school in Australia, and will celebrate its bicentenary in 2016. Newcastle High School, which was formed by the merger of three schools, traces its lineage to a secondary school section initially founded on the grounds of Newcastle East Public School. There are three selective state schools in the area. Hunter School of the Performing Arts is a fully selective K-12 school and only takes students by audition. Merewether High School is a fully selective high school in the suburb of Broadmeadow. Hunter Sports High School is a partially selective sporting high school. The school accepts around half its students from the local area and around half by audition.
The two main independent schools in Newcastle are Newcastle Grammar School and St Philip's Christian College, both coeducational K-12 schools.
Tertiary and further education.
The city's main provider of tertiary education is the University of Newcastle. It was established in 1951 as a satellite campus of the University of New South Wales and obtained autonomy in 1965. The University now offers over 150 undergraduate and graduate courses to a student population of more than 32,000, including 7,300 international students from more than 80 countries. The main campus is in the suburb of Callaghan about from the CBD.
There are three campuses of the Hunter Institute of TAFE, one located in the Newcastle CBD, one in the suburb of Hamilton East and the other located in the suburb of Tighes Hill. The Tighes Hill campus is the network's largest campus and offers courses in business, hospitality and various trades.
Culture.
Festivals.
Newcastle holds a variety of cultural events and festivals.
The Newcastle Regional Show is held in the Newcastle Showground annually. There are a mixture of typical regional show elements such as woodchopping displays, showbags, rides and stalls and usually fireworks to complement the events in the main arena.</ref>
The Mattara festival, founded in 1961, is the official festival of Newcastle with a more traditional 'country fair' type program that combines a parade, rides, sporting events, band competitions and portrait and landscape painting exhibitions.
The Newcastle Jazz Festival is held across three days in August, and attracts performers and audiences from all over Australia.</ref>
The Shoot Out 24 Hour Filmmaking Festival, first started in Newcastle in 1999. This is the film festival where film-makers come together in one place to make a short film in 24 hours. It is run annually in July.</ref>
This Is Not Art is a national festival of new media and arts held in Newcastle each year over the October long weekend. Since its humble beginnings in 1998, it has become one of the leading arts festivals in Australia dedicated to the work and ideas of communities not included in other major Australian arts festivals. The umbrella program includes the independent festivals Electrofringe, the National Young Writers' Festival, Critical Animals, Sound Summit, Crack Theatre Festival and other projects that vary from year to year.
The Newcastle Entertainment Centre, located inside the Newcastle Showground is a popular venue for regular events including wrestling, concerts and monster truck shows.
Music.
Newcastle has an active youth music culture, as well as a Conservatorium of Music which is part of the University of Newcastle. It continues to support local bands and has a large underground music scene. The members of Silverchair, the highly successful Australian band, hail from Newcastle, as does the Australian band The Screaming Jets. It has a fertile punk rock and hardcore scene, which has spawned successful local acts and national acts. 
Visual arts and galleries.
Notable modernist artists associated with Newcastle are seascape sketcher Shay Docking (1928–1998), the cubist influenced abstract painter William Rose (1929–1999), landscape painter John Olsen, who was born in Newcastle in 1928, still- life painter Margaret Olley, portraitist William Dobell and figurative painter John Montefiore lived at Lake Macquarie to the south of the city. Art collector William Bowmore resided in Newcastle and collected Brett Whiteley paintings as well as owning a large collection of international art and artefacts. The Von Bertouch Galleries was a commercial gallery founded by Anne Von Bertouch and exhibited nationally and locally well known artists .
The Newcastle Art Gallery is home to one of Australia's most substantial public art collections outside a major capital city, and its extensive collection of works by contemporary and historical Australian visual artists presents an overview of Australian art. Due to an ongoing space issue, the gallery is planning a major redevelopment.
Theatre.
Newcastle has a variety of smaller theatres, but the main theatre in the CBD is now the "Civic", at Wheeler Place, (seating capacity about 1500), one of Australia's great historic theatres built during 1929 in Art Deco style. It hosts a wide range of musicals, plays, concerts, dance and other events each year. Newcastle previously boasted several large theatres, among them the oldest purpose-built theatre in Australia, the Victoria Theatre on Perkins Street (built 1876, capacity 1750), saw touring international opera companies such as the D'Oyly Carte Opera Company, and other troupes, and played host to some of the greatest stars of the age, such as Dame Nellie Melba, Gladys Moncrieff, and Richard Tauber, (it is now closed and derelict); the "Century", Nineways, Broadmeadow, (built 1941, capacity 1800) although largely used as a cinema was a popular Symphony orchestra venue (demolished 1990 after being severely damaged by the 1989 earthquake); the "Hunter" (capacity 1000) at The Junction, had advanced modern stage facilities, but was eventually sold and demolished to make way for a motel that was destroyed by the 1989 earthquake. The decline in theatres and cinemas from the 1960s onwards was blamed on television.
Newcastle has also been home to noted Australian actors, comedians and entertainers, including Sarah Wynter, John Doyle (part of comic act Roy and HG), Susie Porter, Celia Ireland, Yahoo Serious and Jonathan Biggins. The cast of the Tap Dogs show also come from Newcastle.
Media arts.
Newcastle is home to the Octapod Association, a New Media Arts collective established in 1996. Octapod presents the annual This Is Not Art Festival and is also home to the Podspace Gallery.
Museums.
The Newcastle Museum was founded in 1988 in the former headquarters of the Great Northern Railway and stewards local history, culture, industry, and science. It features permanent exhibitions relating to coal mining and steel production, aboriginal history and the area's history, as well as a hands-on science center.
Sport.
Cricket.
Newcastle Number 1 Sports Ground was for many years a stopover on the tour itinerary for visiting international teams as they faced the Northern New South Wales XI. In 1981–82 the ground was allocated a Sheffield Shield match when the SCG was unavailable, and healthy crowds saw No.1 then become host to at least one first-class fixture featuring the New South Wales Blues each year.
A bid for Newcastle to establish a 2012 team in the national Twenty20 competition the Big Bash League, with games played at either Hunter Stadium or No.1 Sports Ground was unsuccessful.
Horse racing.
Newcastle Jockey Club Limited races 35 times annually at Broadmeadow, a spacious turf track with a home straight. It is the venue for three Group 3 races. In March is the 1400 metre Newcastle Newmarket Handicap and in September the 1400 metre Cameron Handicap and the 2300 metre Newcastle Gold Cup.
Ice hockey and skating.
The Newcastle North Stars are Newcastle's representatives in the Australian Ice Hockey League championships. Originally based in Newcastle West in the 1970-80s, the North Stars now play out of the Hunter Ice Skating Stadium in Warners Bay.
Netball.
The Hunter Jaegers (Commonwealth Bank Trophy – Netball) were based at the Newcastle Entertainment Centre. They became defunct in 2007 after merging with the Sydney Swifts to become the NSW Swifts. Officially opened in June 1992, the Entertainment Centre offers 5,000 square metres of clear span floor space and is capable of catering for capacities from 2,000 to 6,500 for entertainment style events. The Centre was built to house the now defunct Newcastle Falcons National Basketball League team and was also home to the Hunter Pirates before a lack of sponsorship forced them to relocate to Singapore after the 2005–06 season, where they were renamed the Singapore Slingers. The Slingers played one home game at the Centre during the 2006–07 season.
Football.
Several different football codes are popular sports in Newcastle, with at least one having been played since the mid-1800s.
Rugby League.
Newcastle sports teams playing in national competitions include the Newcastle Knights, a team that plays in Australia's premier rugby league competition, the National Rugby League. The Knights play at Hunter Stadium, situated in the suburb of New Lambton. After a recent upgrade, the stadium now has capacity for almost 27,000 spectators. In May 2008, the NSW state government agreed to provide a further $20 million for further upgrades to increase the crowd capacity to 40,000 by end of 2010. The stadium is the only sports venue of its class in New South Wales that is north of Sydney.
The Newcastle Rugby League holds local club competition and has done so since the early 1900s. Touring domestic and international teams would play against Newcastle's representative team which was made up of players from this League.
Rugby Union.
Rugby Union is a football code that has been played in Newcastle since at least 1869, with the Newcastle Football Club formed in 1877. Newcastle and Hunter Rugby Union is the main body overseeing the sport in the region.
Soccer (Association Football).
The Newcastle United Jets Football Club, which plays in Australia's highest level soccer competition, the A-League, also play at Hunter Stadium. The Newcastle United Jets won the A-League competition in their third season, defeating local rivals the Central Coast Mariners FC in the grand final. The Jets are playing in the 2015/16 A-League season, and their eleventh in the Australian National Competition.
The city also played host to 4 games of the 2015 AFC Asian Cup, including the semi-final between Australia and the United Arab Emirates, as well as the 3rd place playoff between the United Arab Emirates and Iraq.
Water sports.
Newcastle has an abundance of beaches and surf breaks for which the city is internationally well known. Newcastle hosts the annual surfing contest 'Surfest' on the world professional surfing tour. Four time world champion surfer Mark Richards grew up surfing at Newcastle's Merewether Beach, and is a local icon, appearing at many local functions, and supporting local charities. Nobbys Beach is a very popular kitesurfing spot, especially during the warm summer months when there are northeasterly sea breezes.
Basketball.
Newcastle has had two teams in the National Basketball League, the Newcastle Falcons and The Hunter Pirates. Both teams folded due to financial difficulties. The Newcastle Hunters currently play in the Waratah Australian Basketball League (WABL), the highest level competition in NSW. The team plays out of the Broadmeadow Basketball Stadium.
Media.
Newcastle is served by a daily tabloid, "The Herald" (formerly "The Newcastle Morning Herald and Miners' Advocate" and then "The Newcastle Herald"), several weeklies including the "Newcastle Star", "The Post" and the bi-monthly "The Hunter Advocate".
Other alternative media in the city include the university's student publications Opus and Yak Magazine, Newcastle Mirage (a local arts and culture zine) and Urchin (a zine published by the media and arts organisation Octapod).
The city is also served by several local radio stations, including those owned by the Australian Broadcasting Corporation and SBS.
Newcastle is also served by 5 television networks, three commercial and two national services:
NBN Television produces an evening news bulletin combining local, state, national and international news screening nightly at 6.00 pm, NBN does not air Nine News Sydney at 6, but shows stories from the bulletin. Both Prime7 and Southern Cross Ten produce shorter bulletins as require by broadcasting legislation.
Subscription Television service Foxtel is also available via satellite.
Transport.
Like most major cities, the Newcastle metropolitan area has an extensive system of both road links and road based public transport services (bus, taxi etc.) which cover most areas of both Newcastle and Lake Macquarie and which extend beyond the metropolitan area itself. Rail transport, however, is accessible to only a relatively small percentage of the population along the major rail transport routes and ferry services are restricted to those commuting between Newcastle and Stockton. Within the metropolitan area the car remains the dominant form of transportation. At the time of the 2001 Census, less than 4% of the population caught public transport, of which around 2.5% travelled by bus and 1% used the train or ferry to commute to work. On the other hand, over 72% of the population travelled by car to and from work. Newcastle, like all major Australian urban centres, had a tram system, but it was closed in 1950. In 2014 it was announced that trams would return to the city as a modern light rail system.
Road.
Newcastle is connected to surrounding cities by the Pacific Motorway (South), Hunter Expressway (West), New England Highway (West) and the Pacific Highway (North and South). Hunter Street, the main shopping street in the Newcastle CBD, is the major link to the Pacific Highway from the CBD.
Bus.
Bus services within Newcastle are operated by Newcastle Buses & Ferries, a subsidiary of the State Transit Authority of New South Wales. Trips within a designated area of the Newcastle CBD on State Transit-operated bus services are fare-free under the Newcastle Alliance's Free City Buses programme.
The network radiates from a bus terminal near Newcastle railway station, on the waterfront of Newcastle's CBD. Major interchanges are located at the University of Newcastle, Wallsend, Glendale, Warners Bay, Belmont, Charlestown, Westfield Kotara and Broadmeadow Station.
Rail.
The Newcastle area is serviced by two NSW TrainLink intercity lines providing local and regional commuter services from Hamilton after the closure of the Newcastle line. The Central Coast & Newcastle Line has twice-hourly train services to Sydney and the Central Coast. The Hunter Line has twice-hourly services to Maitland and less frequently to Scone and Dungog. Two long distance lines operate through the Newcastle area using Broadmeadow Station. These provide services to Moree, Armidale, Brisbane and Sydney.
Newcastle once had rail passenger services to Belmont and Toronto, on Lake Macquarie, Wallsend, Kurri Kurri and several towns and villages between Maitland and Cessnock, but these lines have been closed. In the late 1990s there was intense debate about the future of the rail line into central Newcastle. The New South Wales government had planned to cut the line at Broadmeadow station, ending rail services into Newcastle station in the city centre to allow better connections between the city and the waterfront precinct. This proposal was dropped in 2006.
The proposal to close the line was reactivated and in December 2014, the Newcastle line was curtailed to Hamilton. A new Wickham Transport Interchange will be built from where the Newcastle Light Rail line will operate.
Water.
The Port of Newcastle is crucial to the economic life of Newcastle and the Hunter Valley region beyond. Over 90 million tonnes of coal is shipped through the facility each year – making it the largest coal exporting port in the world. The Port of Newcastle claims to be Australia's first port. Coal was first exported from the harbour in 1799.
Newcastle Buses & Ferries operates a ferry service across the Hunter River between Newcastle's CBD and Stockton.
Air.
Newcastle Airport is located north of the Newcastle CBD ( by road). The airport, which is a joint venture between Newcastle City Council and Port Stephens Council, has experienced rapid growth since 2000 as a result of an increase in low cost airline operations. The airport is located at RAAF Base Williamtown, a Royal Australian Air Force base on land leased from the Department of Defence.</ref>
Newcastle Heliport operates alongside the lower section of Newcastle Harbour.
The suburb of Broadmeadow is home to the base of the Westpac Life Saver Rescue Helicopter Service. The Helicopter service is one of the longest running services of this type in the world. Two helicopters operate out of this base and operate 24 hours a day.
The closure of Belmont Airport, commonly referred to as Aeropelican, in the Lake Macquarie suburb of Marks Point has caused Williamtown to become Newcastle's only major airport and residents in the south of the Newcastle metropolitan area must commute up to by car to reach Williamtown.

</doc>
<doc id="52666" url="https://en.wikipedia.org/wiki?curid=52666" title="Joseph Lyons">
Joseph Lyons

Joseph Aloysius Lyons, (15 September 1879 – 7 April 1939) was an Australian politician. He was Labor Premier of Tasmania from 1923 to 1928 and a Minister in the James Scullin government from 1929 until his resignation from the Labor Party in March 1931. He subsequently led the United Australia Party and was the tenth Prime Minister of Australia from January 1932 until his death.
Early life.
Lyons was born in Stanley, Tasmania, the grandson of Irish immigrants. His father, Michael Lyons, was a successful farmer who afterwards engaged in a butchery and bakery business, but lost this on account of bad health, and subsequently was forced to work as a labourer. His mother did much to keep the family of eight children together, but Joseph had to leave school at nine to work as a messenger and printer's devil. But with the assistance of two aunts, he was able to resume his education at the Philip Smith Teachers' Training College, Hobart, and became a teacher.[http://trove.nla.gov.au/ndp/del/article/30088720] He also became an active trade unionist and was an early member of the Australian Labor Party in Tasmania.
State politics.
In 1909 Lyons was elected to the Tasmanian House of Assembly for Wilmot in central Tasmania. From 1914 to 1916 he was Treasurer (finance minister) and Minister for Education and Railways in John Earle's state Labor government. As Education Minister he oversaw a number of reforms, including abolition of fees for state schools, improving teachers' pay and conditions, and founding Tasmania's first state high schools.
In 1913, a participant in the Labor discussion groups, Eliza Burnell, introduced him to her 15-year-old daughter, Enid Burnell, a trainee teacher and they were married two years later. She was a strong-minded woman who exercised great influence over Lyons, while raising their eleven children.
When the ALP split over conscription during the First World War in 1916, Earle, a pro-conscriptionist, followed Prime Minister Billy Hughes out of the Labor party. Like most Australians of Irish Catholic background, Lyons was an anti-conscriptionist and stayed in the Labor Party, becoming its new leader in Tasmania.
He led the Labor opposition in the Tasmanian Parliament until 1923 when he became Premier, leading a minority ALP government. He held office until 1928, also serving as Treasurer during the whole period of his premiership. Lyons' government was cautious and pragmatic, establishing good relations with business and the conservative government in Canberra, but attracting some criticism from unionists within his own party. Labor narrowly lost the 1928 state election to the Nationalist Party.
Federal politics.
At the 1929 election, Lyons ran for the federal seat of Wilmot, covering the same territory as his state seat. He was swept into office in Labor's landslide victory under James Scullin. He was appointed Postmaster-General and Minister for Works and Railways.
When the Depression struck in 1930, the Scullin government split over its response. Lyons became the leading advocate within the government of orthodox finance and deflationary economic policies, and an opponent of the inflationary, proto-Keynesian policies of Treasurer Ted Theodore. Theodore was forced to resign over accusations of corruption in June 1930, and Scullin took over the Treasury portfolio in addition to the Prime Ministership. Lyons served as acting Treasurer from August 1930 to January 1931 while Scullin was in Britain for the Imperial Conference. In October 1930 Lyons announced his plan for recovery, insisting on the need to maintain a balanced budget and cut public spending and salaries, although also advising lower interest rates and the provision of greater credit for industry.
His conservative economic approach won him support among business, but angered many in the Labor caucus, who wanted to expand the deficit to stimulate the economy, and were horrified at the prospect of cuts in salaries and government spending. Alienated by their attacks, Lyons began to consider suggestions from a group of his new business supporters, including influential members of the Melbourne Establishment, that he leave the government to take over the leadership of the conservative opposition.
Resignation from the Labor Party.
Although the UAP was essentially an enlarged Nationalist Party, Lyons was chosen as leader of the party. He thus became Leader of the Opposition, with former Nationalist leader John Latham as his deputy The UAP realised that Lyons, an affable family man with the common touch, was a far more electorally appealing figure than the aloof Latham. Additionally, his Labor background and his Catholicism would allow him to win traditional Labor constituencies (working-class voters and Irish Catholics) over to what was essentially an upper- and middle-class conservative party.
In March, at about the same time as Lyons led his group of defectors from the right of the Labor Party across the floor, 5 left-wing NSW Labor MPs, supporters of New South Wales Premier Jack Lang, also split from the official Labor Party over the government's economic policies (for Lyons they had been too radical, for the Langites they were not radical enough), forming a "Lang Labor" group on the cross-benches and costing the government its majority in the House of Representatives. Late in the year, the Langite MPs supported a UAP no-confidence motion and brought the government down, forcing an early election.
At the 1931 election, Lyons and the UAP offered stable, orthodox financial policies in response to what they branded as Scullin's poor stewardship of the economy. While Labor remained split between the official party and the Langites, the UAP projected an image of putting national unity above class conflict. The result was a huge victory for the UAP, which took 34 seats against 18 seats for the two wings of the Labor Party combined. The new government was sworn in January 1932. Lyons became the third former Labor MP (after Hughes and Joseph Cook) to become a non-Labor Prime Minister.
Prime Minister.
The UAP fought the election in the traditional non-Labor Coalition with the Country Party (then led by Sir Earle Page). However, the massive swing to the UAP left it only four seats short of a majority in its own right, and Lyons' position was strong enough that he was able to govern alone during his first term. After the 1934 election resulted in the government's loss of eight seats, Lyons was forced to invite the Country Party into his government. Until 1935 Lyons served as treasurer as well as prime minister. In office, Lyons followed the same conservative financial policy he had advocated during the Scullin government, cutting public spending and debt. He benefited politically from the gradual worldwide recovery that took place after 1932.
As far as foreign policy was concerned, Lyons was a firm though by no means servile ally of Britain, and also supported the League of Nations. His government tended to support the conciliation of the dictatorships of Germany, Italy, and Japan to avoid another world war, but he still increased military spending. As a result, he ensured an expansion of the armed forces, the opening of an aircraft factory, and the planning of new munitions factories and shipyards.
At the 1934 election the ambitious and talented Robert Menzies entered Parliament, and was immediately seen as Lyons's successor, although he denied that he was seeking to displace Lyons.
The government won a third term at the 1937 election, with 44 of 74 seats and 50.6 percent of the two-party preferred vote against a reunited Labor Party led by John Curtin. As the international situation darkened in the late 1930s, Lyons, a long-term pacifist, became increasingly despondent. Most politicians expected that he would soon be replaced by Menzies, who resigned from Cabinet in protest at the government's inaction on the national insurance scheme.
Death.
On 7 April 1939, in Sydney, Darlinghurst Lyons died suddenly of a heart attack – the first Australian prime minister to die in office, he was buried in Devonport, Tasmania. He was 59 years old.
Aftermath and legacy.
Lyons was one of the most genuinely popular men to hold the office of prime minister, and his death caused widespread grief. His genial, laid-back appearance often led to cartoon portrayal as a sleepy koala. A devout Catholic, he was the second Catholic to become prime minister, after his immediate predecessor Scullin, and the only non-Labor Catholic prime minister until Tony Abbott.
Lyons is the only person in Australian history to have been prime minister, Premier of a State, and Leader of the Opposition in both the Federal Parliament and a State Parliament (although George Reid had served as premier of a colony before Federation). Lyons is also the only prime minister to come from Tasmania. At the time of his death, he was the second longest-serving prime minister in Australia's history, behind only Hughes.
Lyons was the only Australian prime minister to be in office during the reigns of three monarchs (George V, Edward VIII, and George VI), and the only Australian prime minister who was in office continuously throughout a monarch's entire reign (albeit a very short one, the 11 month-reign of Edward VIII).
Dame Enid Lyons later went into politics in her own right, in 1943 becoming the first woman to sit in the House of Representatives, and later the first woman Cabinet Minister in the Menzies' Liberal government. Two of their sons later became involved in Tasmanian state politics in the Liberal Party: Kevin Lyons was Deputy Premier between 1969 and 1972 and Brendan Lyons served in the ministry of Robin Gray during the 1980s.
Honours.
In 1984, Lyons' old seat of Wilmot was renamed the Division of Lyons jointly in honour of Lyons and his widow Dame Enid Lyons. The state seat of Wilmot was also renamed Lyons for the same reason.
The Canberra suburb of Lyons, Australian Capital Territory is named in honour of Joseph Lyons.
In 1975 he was honoured on a postage stamp bearing his portrait issued by Australia Post.

</doc>
<doc id="52667" url="https://en.wikipedia.org/wiki?curid=52667" title="Ben Chifley">
Ben Chifley

Joseph Benedict "Ben" Chifley (; 22 September 1885 – 13 June 1951) was an Australian politician who was the 16th Prime Minister of Australia from 1945 to 1949. He became Leader of the Labor Party on the death of John Curtin, and went on to retain a majority in both Houses of the Australian Parliament at the 1946 election, before his government was defeated at the 1949 election. The radical reforming nature of the Chifley Government was such that, between 1946 and 1949, the Australian Parliament passed 299 Acts, a record up until then, and well beyond the previous record of the Labor Government of Andrew Fisher, which passed 113 Acts from 1910 to 1913.
Amongst the Chifley Labor Government's legislation was the post-war immigration scheme, the establishment of Australian citizenship, the Snowy Mountains Scheme, over-viewing the foundation of airlines Qantas and TAA, improvements in social services, the creation of the Commonwealth Employment Service, the introduction of federal funds to the States for public housing construction, the establishment of a Universities Commission for the expansion of university education, the introduction of a Pharmaceutical Benefits Scheme (PBS) and free hospital ward treatment, the reorganisation and enlargement of the CSIRO, the establishment of a civilian rehabilitation service, the founding of the Australian Security Intelligence Organisation (ASIO), and the establishment of the Australian National University.
One of the few successful referendums to modify the Australian Constitution, the 1946 Social Services referendum, took place during Chifley's term.
Early life.
Born on 22 September 1885 in Bathurst, New South Wales, Chifley was the son of a blacksmith of Irish Roman Catholic descent. Chifley was raised mostly by his grandfather for nine years. Since his grandfather lost his savings in the bank crash of 1892/93, he had acquired his lifelong dislike of the private banks early. He was educated at Roman Catholic schools in Bathurst, and joined the New South Wales Railways at age 15. Chifley became an engine driver. He was one of the founders of the AFULE (the Australian Federated Union of Locomotive Enginemen) and an active member of the Labor Party. In 1914 he married Elizabeth McKenzie, a staunch Presbyterian. The couple exchanged wedding vows in a Presbyterian church. Chifley remained a practising Catholic, but his marriage to a non-Catholic ignited criticism in certain Roman Catholic circles. In 1917 he was one of the leaders of a prolonged strike, which resulted in his being dismissed. He was reinstated by Jack Lang's New South Wales Labor government. Chifley represented his union before industrial tribunals and taught himself industrial law.
Parliament.
In 1928, at his second try, Chifley won the seat of Macquarie in the House of Representatives, which covered Bathurst, Lithgow, and the Blue Mountains. He was in general a supporter of the James Scullin government's economic policies, and in 1931 he became Minister for Defence. At the 1931 general election, the Scullin government was defeated in a landslide and Chifley lost his seat on a 16-point swing to the UAP's John Lawson. During the Depression he survived on his wife's family's money and his part-ownership of the Bathurst newspaper the "National Advocate".
In 1935 the Lyons government appointed him a member of the Royal Commission on Banking, a subject on which he had become an expert. He submitted a minority report advocating that the private banks be nationalised.
After an unsuccessful effort to win back Macquarie in 1934, Chifley finally won his seat back in 1940 on a swing of 10 percent, and the following year he became Treasurer (finance minister) in John Curtin's Labor government. Although Frank Forde was nominally the number-two-man in the government, Chifley became the minister Curtin most relied on. He controlled most domestic policy while Curtin was preoccupied with the war effort. He presided over the massive increases in government expenditure and taxation that accompanied the war, and imposed a regime of economic regulation that made him very unpopular with business and the press.
Prime minister.
When Curtin died in July 1945, Forde became Prime Minister for eight days. Chifley defeated him in the leadership ballot, replacing him as Prime Minister and Curtin as Labor leader. Once the war ended a month later, normal political life resumed, and Chifley faced Robert Menzies and his new Liberal Party in the 1946 election, which Chifley won with 54 percent of the two-party-preferred vote. It was the first time that a Labor government had been elected to a second full term. In the post-war years, Chifley maintained wartime economic controls, including the highly unpopular petrol rationing. He did this partly to help Britain in its postwar economic difficulties.
Legislative achievements.
Feeling secure in power, Chifley decided it was time to advance towards Labor's objective of democratic socialism. According to a biographer of Chifley, his government embarked upon greater "general intervention and planning in economic and social affairs", with its policies directed towards better conditions in the workplace, full employment, and an improvement in the "equalisation of wealth, income and opportunity". Chifley was successful in transforming the wartime economy into a peacetime economy, and undertook a number of social welfare initiatives, as characterised by fairer pensions and unemployment and sickness benefits, the construction of new universities and technical colleges, and the building of 200,000 houses between 1945 and 1949.
Among other measures, the Chifley government passed legislation to establish a universal public health system modeled on the British National Health Service, including a free formulary of essential medicines. This was successfully opposed in the Australian High Court by the British Medical Association (precursor of the Australian Medical Association). Chifley then organised one of the few successful constitutional referenda to insert a new section 51xxiiiA which permitted federal legislation over pharmaceutical benefits, together with family allowances, benefits to students and hospital benefits, child endowment, widows' pensions, unemployment benefits, and maternity allowances. The subsequent federal legislation in relation to pharmaceutical benefits was deemed constitutional by the High Court. This paved the way for the Pharmaceutical Benefits Scheme, an important component of Australia's modern public health system.
The 1946 referendum made possible many of the Chifley Labor government's other legislative initiatives in social welfare and social provision, including the following:
The achievements of both Chifley's government and those of the previous Curtin Government in expanding Australia's social welfare services (as characterised by a tenfold increase in commonwealth expenditure on social provision between 1941 and 1949) were brought together under the Social Services Consolidation Act of 1947, which consolidated the various social services benefits, liberalised some existing social security provisions, and increased the rates of various benefits.
Among the government's other legislative achievements were:
The radical reforming nature of Chifley's government was such that between 1946 and 1949, the Australian Parliament enacted 299 bills, a record at that time. Chifley and his ministers were able to ensure that Australia's wartime economy was managed effectively and that post-war debts were minimised. In addition, ex-service personnel were eased back into civilian life (avoiding the hardship and dislocation that had occurred after the end of the First World War), while a series of liberal measures were carried out which bore fruit during the economic boom of the Fifties and Sixties. As noted by one historian, Chifley's government "balanced economic development and welfare support with restraint and regulation and provided the framework for Australia's post-war economic prosperity."
Bank nationalisation and 1949 coal strike.
In 1947, Chifley announced the government's intention to nationalise the banks. This provoked massive opposition from the press, and middle-class opinion turned against Labor. The High Court eventually found Chifley's legislation to be unconstitutional. Chifley's government did, however, succeed in passing the Banking and Commonwealth Bank Acts of 1945, which gave the government control over monetary policy and established the Commonwealth Bank as Australia's national bank.
A prolonged and bitter strike in the coal industry began in June 1949 and caused unemployment and hardship. Chifley saw the strike as a move by the Communist Party to challenge Labor's place as the party of the working class, and he sent in the army to break the strike. Despite this, Menzies exploited the rising Cold War hysteria to portray Labor as soft on Communism. These events, together with a perception that Chifley and Labor had grown increasingly arrogant in office, led to the Liberal election victory at the 1949 election. While Labor won an additional four seats in a House of Representatives that had been expanded from 74 seats to 121 seats, Menzies and the Coalition won an additional 48.
Opposition again.
Chifley was now aged 64 and in poor health (like Curtin, he was a lifelong smoker), but he refused to retire from politics. Labor had retained control of the Senate, and Chifley, now Leader of the Opposition, took advantage of this to bring misery to the Menzies government at every turn.
Menzies responded by introducing a bill to ban the Communist Party of Australia in 1950. He expected Chifley to reject it and give him an excuse to call a double dissolution election. Menzies apparently hoped to repeat his "soft-on-Communism" theme to win a majority in both chambers. However, Chifley let the bill pass after a redraft (it was ultimately thrown out by the High Court). However, when Chifley rejected Menzies' Commonwealth Banking Bill a few months later, Menzies called a double dissolution election for April 1951. Although Chifley managed to lead Labor to a five-seat swing in the House, Labor lost six seats in the Senate, giving the Coalition control of both chambers.
Death.
A few weeks later, Chifley suffered a heart attack in his room at the Hotel Kurrajong in Canberra (he had lived there throughout his political career, having refused to reside at The Lodge while Prime Minister).
Chifley at first made light of the sudden heart attack and attempted to dissuade his secretary and confidante, Phyllis Donnelly, who was making him a cup of tea, from calling a doctor. As his condition deteriorated, however, Donnelly called Dr. John Holt, who ordered Chifley's immediate removal to hospital. Chifley died in an ambulance on the way to the Canberra Community Hospital. He was pronounced dead at 10:45 pm. Prime Minister Menzies heard of Chifley's demise while attending a parliamentary ball at King's Hall in Parliament House to celebrate the 50th Jubilee of Federation (Chifley was invited but had declined to attend). Menzies was deeply distressed and abandoned his normally impassive demeanour to announce in a halting subdued voice:It is my very sorrowful duty during this celebration tonight to tell you that Mr. Chifley has died. I don't want to try to talk about him now because, although we were political opponents, he was a friend of mine and yours, and a fine Australian. You will all agree that in the circumstances the festivities should end. It doesn't matter about party politics on an occasion such as this. Oddly enough, in Parliament we get on very well. We sometimes find we have the warmest friendships among people whose politics are not ours. Mr Chifley served this country magnificently for years.
Legacy.
More than 30 years after his death, Chifley's name still aroused partisan passions. In 1987 the New South Wales Labor government decided to name the planned new university in Sydney's western suburbs Chifley University. When, in 1989, a new Liberal government renamed it the University of Western Sydney, controversy broke out. According to a debate on the topic, held in 1997 after the Labor Party had regained government, the decision to rename Chifley University reflected a desire to attach the name of Western Sydney to institutions of lasting significance, and that idea ultimately received the support of Bob Carr, later the Premier of New South Wales.
Honours.
Places and institutions that have been named after Chifley include:
In 1975 he was honoured on a postage stamp bearing his portrait issued by Australia Post.
One of the locomotives driven by Chifley, 5112, is preserved on a plinth at the eastern end of Bathurst Railway Station.

</doc>
<doc id="52669" url="https://en.wikipedia.org/wiki?curid=52669" title="John McEwen">
John McEwen

Sir John McEwen, (29 March 190020 November 1980) was an Australian politician and the 18th Prime Minister of Australia. He was the last member of the Country Party to serve as Prime Minister. He was nicknamed "Black Jack" by Robert Menzies due to his dark 'beetle-browed' appearance and temper.
Early life.
McEwen was born at Chiltern, Victoria to David James McEwen, a pharmacist from Ireland, and his second wife, Amy Ellen (née Porter; died 1901). His father died in 1907 and consequently McEwen was raised by his grandmother with her sister. He was educated at state schools and at 15 became a junior public service clerk. He enlisted in the Army immediately upon turning 18 but the First World War ended while he was still in training. He commenced dairy farming at Tongala (Victoria), near Shepparton, and then changed to sheep and cattle farming in nearby Stanhope.
Political career.
McEwen was active in farmer organisations and in the Country Party. In 1934 he was elected to the House of Representatives for the electorate of Echuca. That seat was abolished in 1937, and McEwen followed most of his constituents into Indi. He changed seats again in 1949, when Murray was carved out of the northwestern portion of Indi and McEwen transferred there. Between 1937 and 1941 he was successively Minister for the Interior, Minister for External Affairs and simultaneously Minister for Air and Minister for Civil Aviation. In 1940 when Archie Cameron resigned as Country Party leader he contested the leadership ballot against Sir Earle Page: the ballot was tied and Arthur Fadden was chosen as the independent.
When the conservatives returned to office in 1949 under Robert Menzies after eight years in opposition, McEwen became Minister for Commerce and Agriculture, switching to Minister for Trade and Industry in 1956. He pursued what became known as "McEwenism" – a policy of high tariff protection for the manufacturing industry, so that industry would not challenge the continuing high tariffs on imported raw materials, which benefitted farmers but pushed up industry's costs. This policy was a part (some argue the foundation) of what became known as the "Australian Settlement' which promoted high wages, industrial development, government intervention in industry (both as an owner- Australian governments traditionally owned banks and insurance companies and the railways and through policies designed to assist particular industries) and decentralisation. In 1958 Fadden retired and McEwen succeeded him as Country Party leader.
When Menzies retired in 1966, McEwen became the longest-serving figure in the government, and he had an effective veto over government policy. When Menzies' successor, Harold Holt, was officially presumed dead on 19 December 1967, the Governor-General Lord Casey sent for McEwen and he was sworn in as Prime Minister, on the understanding that his commission would continue only so long as it took for the Liberals to elect a new leader. He retained all the ministers appointed by Harold Holt and had them sworn in as the McEwen Ministry. Approaching 68, McEwen was the oldest person ever to be appointed Prime Minister of Australia, although not the oldest to serve; Menzies left office one month and six days after his 71st birthday.
McEwen had been encouraged to remain Prime Minister on a more permanent basis but to do so would have required him to defect to the Liberals, an option he had never contemplated.
It had long been presumed that the Treasurer and Liberal deputy leader, William McMahon, would succeed Holt as Liberal leader. However, McEwen sparked a leadership crisis when he announced that he and his Country Party colleagues would not serve under McMahon. McEwen is reported to have despised McMahon personally. But more importantly, McEwen was bitterly opposed to McMahon on political grounds, because McMahon was allied with free trade advocates in the conservative parties and favoured sweeping tariff reforms, a position that was vehemently opposed by McEwen, his Country Party colleagues and their rural constituents.
Another key factor in McEwen's antipathy towards McMahon was hinted at soon after the crisis by the veteran political journalist Alan Reid. According to Reid, McEwen was aware that McMahon was habitually breaching Cabinet confidentiality and regularly leaking information to favoured journalists and lobbyists, including Maxwell Newton, who had been hired as a "consultant" by Japanese trade interests.
McEwen's opposition forced McMahon to withdraw from the leadership ballot and opened the way for the successful campaign to promote the Minister for Education and Science, Senator John Gorton, to the Prime Ministership with the support of a group led by Defence Minister Malcolm Fraser. Gorton replaced McEwen as Prime Minister on 10 January 1968. It was the second time the Country Party had effectively vetoed its senior partner's choice for the leadership; in 1923 Earle Page had demanded that the Nationalist Party, one of the forerunners of the Liberals, remove Billy Hughes as leader before he would even consider coalition talks. Gorton created the formal title Deputy Prime Minister for McEwen, confirming his status as the second-ranking member of the government. Prior to then, the title had been used informally for whoever was recognised as the second-ranking member of the government.
McEwen retired from politics in 1971. His successor, Doug Anthony, said that McEwen's previous objections to McMahon no longer held, finally freeing the Liberals to replace Gorton with McMahon within two months. At the time of his resignation, McEwen had served 36 years and 5 months, including 34 years as either a minister or opposition frontbencher. He was the last serving parliamentarian from the Great Depression era, the last parliamentary survivor of the Lyons government. By the time of his death, Malcolm Fraser's government was abandoning McEwenite trade policies.
Honours.
McEwen was awarded the Companion of Honour (CH) in 1969. He was knighted in 1971 after his retirement from politics, becoming a Knight Grand Cross of the Order of St Michael and St George (GCMG). The Japanese government conferred on him the Grand Cordon, Order of the Rising Sun in 1973.
Personal.
On 21 September 1921 he married Anne Mills McLeod, known as Annie; they had no children. In 1966, she was made a Dame Commander of the Order of the British Empire (DBE). After a long illness Dame Anne McEwen died on 10 February 1967.
At the time of becoming Prime Minister in December of that year, McEwen was a widower, being the first Australian Prime Minister unmarried during his term of office. (The next such case was Julia Gillard, Prime Minister 2010–13, who has a domestic partner but is unmarried.)
On 26 July 1968, McEwen married Mary Eileen Byrne, his personal secretary; he was aged 68, she was 46. In retirement he distanced himself from politics, undertook some consulting work, and travelled to Japan and South Africa. He had no children by any of his marriages. 
McEwen suffered dermatitis all his life and ended up committed suicide by starving himself to death on the 20th of November 1980, at the age of 80. He was survived by his second wife, and was cremated. His estate was sworn for probate at $2,180,479. He was also receiving a small pension from the Department of Social Security at the time of his death. He was the last surviving member of Joseph Lyons' Cabinet.

</doc>
<doc id="52671" url="https://en.wikipedia.org/wiki?curid=52671" title="Psychosomatic medicine">
Psychosomatic medicine

Psychosomatic medicine is an interdisciplinary medical field exploring the relationships among social, psychological, and behavioral factors on bodily processes and quality of life in humans and animals.
The academic forebear of the modern field of behavioral medicine and a part of the practice of consultation-liaison psychiatry, psychosomatic medicine integrates interdisciplinary evaluation and management involving diverse specialties including psychiatry, psychology, neurology, internal medicine, surgery, allergy, dermatology and psychoneuroimmunology. Clinical situations where mental processes act as a major factor affecting medical outcomes are areas where psychosomatic medicine has competence.
History of psychosomatics.
In the medieval Islamic world the Persian psychologist-physicians Ahmed ibn Sahl al-Balkhi (d. 934) and Haly Abbas (d. 994) developed an early understanding of illness that was due to the interaction of the mind and the body. They realized how a patient's physiology and psychology can have an effect on one another. They found correlations between patients who were physically and mentally healthy and between those who were physically and mentally ill.
In the beginnings of the 20th century, Franz Alexander led the movement looking for the dynamic interrelation between mind and body. Sigmund Freud pursued a deep interest in psychosomatic illnesses following his correspondence with Georg Groddeck who was, at the time, researching the possibility of treating physical disorders through psychological processes.
Since the 1970s, due to the work of Thure von Uexküll and his colleagues in Germany and elsewhere, biosemiotic theory has been used as a theoretical basis for psychosomatic medicine. Particularly, the umwelt concept and the theory of organism by Jakob von Uexküll has been found useful as an approach to describe psychosomatic phenomena.
Psychosomatic disorders.
Some physical diseases are believed to have a mental component derived from the stresses and strains of everyday living. This has been suggested, for example, of lower back pain and high blood pressure, which some researchers have suggested may be related to stresses in everyday life. However, within a psychosomatic framework, mental and emotional states are seen as capable of significantly influencing the course of any physical illness. Psychiatry traditionally distinguishes between psychosomatic disorders, disorders in which mental factors play a significant role in the development, expression, or resolution of a physical illness, and somatoform disorders, disorders in which mental factors are the sole cause of a physical illness.
It is sometimes difficult to establish whether an illness causation is primarily physical, mental, or whether it has a psychosomatic component. For instance, while peptic ulcer was once thought of as being purely caused by stress, later research revealed that "Helicobacter pylori" caused 80% of ulcers. Nevertheless, most people living with "Helicobacter pylori" do not develop ulcers, and 30% of patients with ulcers have no "H. pylori" infection. Therefore, mental stress could still play some role. Similarly, in irritable bowel syndrome (IBS), recent research has shown significant differences in the behavior of the gut in IBS patients. On the other hand, there are no actual structural changes in the gut. Again, stress and emotions might still play a role.
The strongest perspective on psychosomatic disorders is that attempting to distinguish between purely physical and mixed psychosomatic disorders is increasingly obsolete as almost all physical illness have mental factors that determine their onset, presentation, maintenance, susceptibility to treatment, and resolution. According to this view, even the course of serious illnesses, such as cancer, can potentially be influenced by a person's thoughts, feelings and general state of mental health.
Addressing such factors is the remit of the applied field of behavioral medicine. In modern society, psychosomatic aspects of illness are often attributed to stress making the remediation of stress one important factor in the development, treatment, and prevention of psychosomatic illness.
Connotations of the term "psychosomatic illness".
In the field of psychosomatic medicine, the phrase "psychosomatic illness" is used more narrowly than it is within the general population. For example, in lay language, the term often encompasses illnesses with no physical basis at all, and even illnesses that are faked (malingering). In contrast, in contemporary psychosomatic medicine, the term is normally restricted to those illnesses that do have a clear physical basis, but where it is believed that psychological and mental factors also play a role. Some researchers within the field believe that this overly broad interpretation of the term may have caused the discipline to fall into disrepute clinically. For this reason, among others, the field of behavioral medicine has taken over much of the remit of psychosomatic medicine in practice and there exist large areas of overlap in the scientific research.
Criticism.
The idea that a person's mental state can influence the course and severity of even the most severe physical diseases has led to some very strong claims. For example, it has been suggested that patients with intractable cancer may be able to survive longer if provided with psychotherapy to improve their outlook. Early studies provided some support for this view. However, a major review published in 2007, which evaluated the evidence for these benefits, concluded that no studies meeting the minimum quality standards required in this field have demonstrated such an benefit. The review further argues that these unsubstantiated claims that "positive outlook" or "fighting spirit" can help slow cancer may be harmful to the patients themselves. Patients may come to believe that their poor progress results from "not having the right attitude", when in fact it may be no fault of their own.
Treatment.
Psychosomatic medicine is considered a subspecialty of the fields of psychiatry and neurology. Medical treatments and psychotherapy are used to treat psychosomatic disorders.

</doc>
<doc id="52672" url="https://en.wikipedia.org/wiki?curid=52672" title="Deejay (Jamaican)">
Deejay (Jamaican)

Deejay (alternatively spelled DJ) is a term in Jamaican music for a reggae or dancehall musician who sings and "toasts" to an instrumental riddim.
Deejays are not to be confused with disc jockeys from other music genres like hip-hop, where they select and play music. Dancehall/reggae DJs who select riddims to play are called selectors. Deejays who are more likely to sing are sometimes called singjays.
The term deejay came about as a result of the act of some selectors (as they were called) of the 1960s and 1970s such as U-Roy or King Stitt toasting to the version side of popular records of the time. The version came about when the record company produced the 45 record with the song, the flip side of which had the instrumental version of the song. This gave the deejays the chance to make up on-the-fly lyrics to the instrumental music. This occurrence gave rise to deejay toasting and the term has been used in that context ever since.
Toasting.
Toasting, chatting (rap in other parts of the Anglo Caribbean), or deejaying is the act of talking or chanting, usually in a monotone melody, over a rhythm or beat by a deejay. Traditionally, the method of toasting originated from the griots of Caribbean calypso and mento traditions. The lyrics can either be improvised or pre-written.
Toasting has been used in various African traditions, such as griots chanting over a drum beat, as well as in the United States and Jamaican music forms, such as ska, reggae, dancehall, and dub. Toasting is also often used in soca and bouyon music. The African American oral tradition of toasting, a mix of talking and chanting, influenced the development of MCing in US hip hop music. The combination of singing and toasting is known as singjaying.
In the late 1950s deejay toasting was developed by Count Machuki. He conceived the idea from listening to disc jockeys on American radio stations. He would do African American jive over the music while selecting and playing R&B music. Deejays like Count Machuki working for producers would play the latest hits on traveling sound systems at parties and add their toasts or vocals to the music. These toasts consisted of comedy, boastful commentaries, half-sung rhymes, rhythmic chants, squeals, screams and rhymed storytelling.
Osbourne Ruddock (aka King Tubby) was a Jamaican sound recording engineer who created vocal-less rhythm backing tracks that were used by DJs doing toasting by creating one-off vinyl discs (also known as dub plates) of songs without the vocals and adding echo and sound effects.
Late 1960s toasting deejays included U-Roy and Dennis Alcapone, the latter known for mixing gangster talk with humor in his toasting. In the early 1970s, toasting deejays included I-Roy (his nickname is a homage to U-Roy) and Dillinger, the latter known for his humorous toasting style. In the early 1970s Big Youth became very popular and had three very successful albums, "Screeming Taget", "Dreadlocks Dread" and "Natty Cultural Dread". In the late 1970s, Trinity became a popular toasting deejay.
The 1980s saw the first deejay toasting duo, Michigan & Smiley, and the development of toasting outside of Jamaica. In England, Pato Banton explored his Caribbean roots, humorous and political toasting while Ranking Roger of the Second Wave or Two-Tone ska revival band the Beat from the 1980s did Jamaican toasting over music that blended ska, pop, and some punk influences.
The rhythmic rhyming of vocals of African American toasting influenced the development of toasting in Jamaica and development of the Dancehall style (e.g. hip-hop pioneer and Jamaican expatriate DJ Kool Herc and Phife Dawg of A Tribe Called Quest). Jamaican deejay toasting also influenced various types of dance music, such as jungle music and UK garage. Dancehall artists that have achieved pop hits with toasting-influenced vocals include Shabba Ranks, Shaggy, Lady Saw, Sean Paul, Terror Fabulous and Damian Marley.

</doc>
<doc id="52675" url="https://en.wikipedia.org/wiki?curid=52675" title="Strouhal number">
Strouhal number

In dimensional analysis, the Strouhal number (St) is a dimensionless number describing oscillating flow mechanisms. The parameter is named after Vincenc Strouhal, a Czech physicist who experimented in 1878 with wires experiencing vortex shedding and singing in the wind. The Strouhal number is an integral part of the fundamentals of fluid mechanics.
The Strouhal number is often given as;
where "f" is the frequency of vortex shedding, "L" is the characteristic length (for example hydraulic diameter, or chord length) and "U" is the flow velocity. In certain cases like heaving (plunging) flight, this characteristic length is the amplitude of oscillation. This selection of characteristic length can be used to present a distinction between Strouhal number and Reduced Frequency. 
where "k" is the reduced frequency and "a" is amplitude of the heaving oscillation. 
For large Strouhal numbers (order of 1), viscosity dominates fluid flow, resulting in a collective oscillating movement of the fluid "plug". For low Strouhal numbers (order of 10−4 and below), the high-speed, quasi steady state portion of the movement dominates the oscillation. Oscillation at intermediate Strouhal numbers is characterized by the buildup and rapidly subsequent shedding of vortices.
For spheres in uniform flow in the Reynolds number range of 800 < Re < 200,000 there co-exist two values of the Strouhal number. The lower frequency is attributed to the large-scale instability of the wake and is independent of the Reynolds number Re and is approximately equal to 0.2. The higher frequency Strouhal number is caused by small-scale instabilities from the separation of the shear layer.
Applications.
In metrology, specifically axial-flow turbine meters, the Strouhal number is used in combination with the Roshko number to give a correlation between flow rate and frequency. The advantage of this method over the freq/viscosity versus K-factor method is that it takes into account temperature effects on the meter. 
f = meter frequency,
U = flow rate,
C = linear coefficient of expansion for the meter housing material
This relationship leaves Strouhal dimensionless, although a dimensionless approximation is often used for C3, resulting in units of pulses/volume (same as K-factor).
In animal flight or swimming, propulsive efficiency is high over a narrow range of Strouhal constants, generally peaking in the 0.2 < St < 0.4 range. This range is used in the swimming of dolphins, sharks, and bony fish, and in the cruising flight of birds, bats and insects. However, in other forms of flight other values are found. Intuitively the ratio measures the steepness of the strokes, viewed from the side (e.g., assuming movement through a stationary fluid) – "f" is the stroke frequency, "L" is the amplitude, so the numerator "fL" is half the vertical speed of the wing tip, while the denominator "V" is the horizontal speed. Thus the graph of the wing tip forms an approximate sinusoid with aspect (maximum slope) twice the Strouhal constant.

</doc>
<doc id="52682" url="https://en.wikipedia.org/wiki?curid=52682" title="Greek literature">
Greek literature

Greek literature refers to writings composed in areas of Greek influence, throughout the whole period in which the Greek-speaking people have existed. Ancient Greek literature refers to literature written in Ancient Greek from the oldest surviving written works in the Greek language until approximately the fifth century AD and the rise of the Byzantine Empire. At the beginning of Greek literature are works of Homer, the "Iliad" and the "Odyssey". In the classical period many of the genres of western literature became more prominent. The two major lyrical poets were Sappho and Pindar. The Classical era also saw the dawn of drama. Two of the most influential historians were Herodotus and Thucydides. In the 4th century BC, three philosophers are notable: Socrates, Plato, and Aristotle. Later Greek poetry flourished primarily in the 3rd century BC. During the Roman era, the physician Galen, in the history of ancient science, is the most significant person in medicine after Hippocrates.
Byzantine literature refers to literature of the Byzantine Empire written in Atticizing, Medieval and early Modern Greek. Chronicles, distinct from historic, arose in this period. Encyclopedias also flourished in this period. Modern Greek literature refers to literature written in common Modern Greek, emerging from late Byzantine times in the 11th century AD. The Cretan Renaissance poem "Erotokritos" is the masterpiece of this early period of modern Greek literature. Much later, Diafotismos was an ideological, philological, linguistic and philosophical movement among 18th century Greeks that translate the ideas and values of European Enlightenment into the Greek world. Adamantios Korais and Rigas Feraios are two of the most notable figures. 
The years before the Greek Independence, the Ionian islands became the center of the Heptanese School (literature). Notable representatives were Andreas Laskaratos, Andreas Kalvos, Aristotelis Valaoritis and Dionysios Solomos.
Later the intellectual center was transferred in Athens. A major figure of this new era was Kostis Palamas (1859 – 1943), considered "national poet" of Greece. Modern Greek literature is usually (but not exclusively) written in polytonic orthography. Modern Greek literature is represented by many writers, poets and novelists. George Seferis and Odysseas Elytis have been awarded the Nobel Prize in Literature.
Ancient Greek literature (before AD 350).
Ancient Greek literature refers to literature written in Ancient Greek from the oldest surviving written works in the Greek language until approximately the fifth century AD and the rise of the Byzantine Empire. The Greek language arose from the proto-Indo-European language, though roughly one-third of its words cannot be derived from various reconstructions of the tongue. A number of alphabets and syllabaries had been used to render Greek, but surviving Greek literature was written in a Phoenician-derived alphabet that arose primarily in Greek Ionia and was fully adopted by Athens by the fifth century BC.
Preclassical.
At the beginning of Greek literature stand the two monumental works of Homer, the "Iliad" and the "Odyssey". Though dates of composition vary, these works were fixed around 800 BC or after. The other great poet of the preclassical period was Hesiod. His two surviving works are "Works and Days" and "Theogony". Some ancients thought Homer and Hesiod roughly contemporaneous, even rivals in contests, but modern scholarship raises doubts on these issues.
Classical.
In the classical period many of the genres of western literature became more prominent. Lyrical poetry, odes, pastorals, elegies, epigrams; dramatic presentations of comedy and tragedy; histories, rhetorical treatises, philosophical dialectics, and philosophical treatises all arose in this period. As the genres evolved, various expectations arose, such that a particular poetic genre came to require the Doric or Lesbos dialect.
The two major lyrical poets were Sappho and Pindar. The Classical era also saw the dawn of drama. Of the hundreds of tragedies written and performed during the classical age, only a limited number of plays by three authors have survived: Aeschylus, Sophocles, and Euripides.
Like tragedy, the comedy arose from a ritual in honor of Dionysus, but in this case the plays were full of frank obscenity, abuse, and insult. The surviving plays by Aristophanes are a treasure trove of comic presentation.
Two of the most influential historians who had yet lived flourished during Greece's classical age: Herodotus and Thucydides. A third historian, Xenophon, began his "Hellenica" where Thucydides ended his work about 411 BCE and carried his history to 362 BCE.
The greatest prose achievement of the 4th century BCE was in philosophy. Among the tide of Greek philosophy, three names tower above the rest: Socrates —even though he did not write anything himself, Plato, and Aristotle.
Hellenistic.
By 338 BC many of the key Greek cities had been conquered by Philip II of Macedon. Philip II's son Alexander extended his father's conquests greatly. The Greek colony of Alexandria in northern Egypt became, from the 3rd century BC, the outstanding center of Greek culture.
Later Greek poetry flourished primarily in the 3rd century BC. The chief poets were Theocritus, Callimachus, and Apollonius of Rhodes. Theocritus, who lived from about 310 to 250 BC, was the creator of pastoral poetry, a type that the Roman Virgil mastered in his Eclogues.
Drama was represented by the New Comedy, of which Menander was the principal exponent.
One of the most valuable contributions of the Hellenistic period was the translation of the Old Testament into Greek. The work was done at Alexandria and completed by the end of the 2nd century BC. The name Septuagint is from Latin septuaginta "seventy," from the tradition that there were 72 scholars who did the work.
Roman Age.
Significant historians of the period were Timaeus, Polybius, Diodorus Siculus, Dionysius of Halicarnassus, Appian of Alexandria, Arrian, and Plutarch. The period of time they cover extended from late in the 4th century BC to the 2nd century AD. Eratosthenes of Alexandria, who died about 194 BC, wrote on astronomy and geography, but his work is known mainly from later summaries. The philosopher and historian Strabo is mostly known for his main work "Geographica". The physician Galen, in the history of ancient science, is the most significant person in medicine after Hippocrates, who had laid the foundation of medicine in the 5th century BC.
The popular novel "Daphnis and Chloe" is the only known work of the 2nd century AD Greek novelist and romancer Longus.
The New Testament, written by various authors in varying qualities of Koine Greek hails from this period (1st to early 2nd century AD), the most important works being the Gospels and the Epistles of Saint Paul. Patristic literature was written in the Hellenistic Greek of this period. Syria and Alexandria, especially, flourished.
Byzantine literature (AD 290–1453).
Byzantine literature refers to literature of the Byzantine Empire written in Atticizing, Medieval and early Modern Greek.
If Byzantine literature is the expression of the intellectual life of the Byzantine Greeks during the Christian Middle Ages, then it is a multiform organism, combining Greek and Christian civilization on the common foundation of the Roman political system, set in the intellectual and ethnographic atmosphere of the Near East. Byzantine literature partakes of four different cultural elements: the Greek, the Christian, the Roman, and the Oriental, the character of which commingling with the rest. To Hellenistic intellectual culture and Roman governmental organization are added the emotional life of Christianity and the world of Oriental imagination, the last enveloping all the other three.
Aside from personal correspondence, literature of this period was primarily written in the Atticizing style. Some early literature of this period was written in Latin; some of the works from the Latin Empire were written in French.
Chronicles, distinct from historic, arose in this period. Encyclopedias also flourished in this period.
Modern Greek literature (post-1453).
Modern Greek literature refers to literature written in common Modern Greek, emerging from late Byzantine times in the 11th century AD. During this period, spoken Greek became more prevalent in the written tradition, as demotic Greek came to be used more and more over the Attic idiom and the katharevousa reforms.
The migration of Byzantine scholars and other émigrés from southern Italy and Byzantium during the decline of the Byzantine Empire (1203–1453) and mainly after the fall of Constantinople in 1453 until the 16th century, is considered by some scholars as key to the revival of Greek and Roman studies and subsequently in the development of the Renaissance humanism and science. These emigres were grammarians, humanists, poets, writers, printers, lecturers, musicians, astronomers, architects, academics, artists, scribes, philosophers, scientists, politicians and theologians. They brought to Western Europe the far greater preserved and accumulated knowledge of their own civilization.
The Cretan Renaissance poem "Erotokritos" is undoubtedly the masterpiece of this early period of modern Greek literature, and represents one of its supreme achievements. It is a verse romance written around 1600 by Vitsentzos Kornaros (1553–1613). The other major representative of the Cretan literature was Georgios Chortatzis and his most notable work was "Erofili". Other plays include "The Sacrifice of Abraham" by Kornaros, "Panoria" and "Katsourbos" by Chortatzis, "King Rodolinos" by Andreas Troilos, "Stathis (comedy)" and "Voskopoula" by unknown artists.
Much later, Diafotismos was an ideological, philological, linguistic and philosophical movement among 18th century Greeks that translate the ideas and values of European Enlightenment into the Greek world. Adamantios Korais and Rigas Feraios are two of the most notable figures. In 1819, "Korakistika", written by Iakovakis Rizos Neroulos, was a lampoon against the Greek intellectual Adamantios Korais and his linguistic views, who favoured the use of a more conservative form of the Greek language, closer to the ancient.
The years before the Greek Independence, the Ionian islands became the center of the Heptanese School (literature). Its main characteristics was the Italian influence, romanticism, nationalism and use of Demotic Greek. Notable representatives were Andreas Laskaratos, Andreas Kalvos, Aristotelis Valaoritis and Dionysios Solomos.
Later the independence the intellectual center was transferred in Athens. A major figure of this new era was Kostis Palamas, considered "national poet" of Greece. He was the central figure of the Greek literary generation of the 1880s and one of the cofounders of the so-called New Athenian School (or Palamian School). Its main characteristic was the use of Demotic Greek. He was also the writer of the Olympic Hymn.
Modern Greek literature is usually (but not exclusively) written in polytonic orthography, though the monotonic orthography was made official in 1981 by Andreas Papandreou government. Modern Greek literature is represented by many writers, poets and novelists. Major representatives are Angelos Sikelianos, Emmanuel Rhoides, Athanasios Christopoulos, Kostis Palamas, Penelope Delta, Yannis Ritsos, Alexandros Papadiamantis, Nikos Kazantzakis, Andreas Embeirikos, Kostas Karyotakis, Gregorios Xenopoulos, Constantine P. Cavafy, Demetrius Vikelas, while George Seferis and Odysseas Elytis have been awarded the Nobel Prize in Literature.
Other writers include Manolis Anagnostakis, Nicolas Calas, Kiki Dimoula, Maro Douka, Nikos Engonopoulos, Nikos Gatsos, Iakovos Kambanelis, Nikos Kavvadias, Andreas Karkavitsas, Kostas Krystallis, Dimitris Lyacos, Petros Markaris, Jean Moréas, Stratis Myrivilis, Dimitris Psathas, Ioannis Psycharis, Alexandros Rizos Rangavis, Miltos Sahtouris, Antonis Samarakis, Giannis Skarimpas, Dido Sotiriou, Alexandros Soutsos, Panagiotis Soutsos, Angelos Terzakis, Kostas Varnalis, Vassilis Vassilikos, Elias Venezis, Nikephoros Vrettakos.

</doc>
<doc id="52684" url="https://en.wikipedia.org/wiki?curid=52684" title="Ancient Greek architecture">
Ancient Greek architecture

The architecture of ancient Greece is the architecture produced by the Greek-speaking people ("Hellenic" people) whose culture flourished on the Greek mainland and the Peloponnese, the Aegean Islands, and in colonies in Anatolia and Italy for a period from about 900 BC until the 1st century AD, with the earliest remaining architectural works dating from around 600 BC.
Ancient Greek architecture is best known from its temples, many of which are found throughout the region, mostly as ruins but many substantially intact. The second important type of building that survives all over the Hellenic world is the open-air theatre, with the earliest dating from around 350 BC. Other architectural forms that are still in evidence are the processional gateway ("propylon"), the public square ("agora") surrounded by storied colonnade ("stoa"), the town council building ("bouleuterion"), the public monument, the monumental tomb ("mausoleum") and the "stadium".
Ancient Greek architecture is distinguished by its highly formalised characteristics, both of structure and decoration. This is particularly so in the case of temples where each building appears to have been conceived as a sculptural entity within the landscape, most often raised on high ground so that the elegance of its proportions and the effects of light on its surfaces might be viewed from all angles. Nikolaus Pevsner refers to "the plastic shape of the temple...placed before us with a physical presence more intense, more alive than that of any later building".
The formal vocabulary of ancient Greek architecture, in particular the division of architectural style into three defined orders: the Doric Order, the Ionic Order and the Corinthian Order, was to have profound effect on Western architecture of later periods. The architecture of ancient Rome grew out of that of Greece and maintained its influence in Italy unbroken until the present day. From the Renaissance, revivals of Classicism have kept alive not only the precise forms and ordered details of Greek architecture, but also its concept of architectural beauty based on balance and proportion. The successive styles of Neoclassical architecture and Greek Revival architecture followed and adapted Ancient Greek styles closely. 
Influences.
Geography.
The mainland and islands of Greece are rocky, with deeply indented coastline, and rugged mountain ranges with few substantial forests. The most freely available building material is stone. Limestone was readily available and easily worked. There is an abundance of high quality white marble both on the mainland and islands, particularly Paros and Naxos. This finely grained material was a major contributing factor to precision of detail, both architectural and sculptural, that adorned ancient Greek architecture. Deposits of high quality potter's clay were found throughout Greece and the Islands, with major deposits near Athens. It was used not only for pottery vessels, but also roof tiles and architectural decoration.
The climate of Greece is maritime, with both the coldness of winter and the heat of summer tempered by sea breezes. This led to a lifestyle where many activities took place outdoors. Hence temples were placed on hilltops, their exteriors designed as a visual focus of gatherings and processions, while theatres were often an enhancement of a naturally occurring sloping site where people could sit, rather than a containing structure. Colonnades encircling buildings, or surrounding courtyards provided shelter from the sun and from sudden winter storms.
The light of Greece may be another important factor in the development of the particular character of ancient Greek architecture. The light is often extremely bright, with both the sky and the sea vividly blue. The clear light and sharp shadows give a precision to the details of landscape, pale rocky outcrops and seashore. This clarity is alternated with periods of haze that varies in colour to the light on it. In this characteristic environment, the ancient Greek architects constructed buildings that were marked by precision of detail. The gleaming marble surfaces were smooth, curved, fluted, or ornately sculpted to reflect the sun, cast graded shadows and change in colour with the ever-changing light of day.
History.
Historians divide ancient Greek civilization into two eras, the Hellenic period (from around 900 BC to the death of Alexander the Great in 323 BC), and the Hellenistic period (323 BC to 30 AD). During the earlier Hellenic period, substantial works of architecture began to appear around 600 BC. During the later (Hellenistic) period, Greek culture spread widely, initially as a result of Alexander's conquest of other lands, and later as a result of the rise of the Roman Empire, which adopted much of Greek culture.
Before the Hellenic era, two major cultures had dominated the region: the Minoan (c. 2800–1100 BC), and the Mycenaean (c. 1500–1100 BC). Minoan is the name given by modern historians to the culture of the people of ancient Crete, known for its elaborate and richly decorated palaces, and for its pottery painted with floral and marine motifs. The Mycenaean culture, which flourished on the Peloponnesus, was quite different in character. Its people built citadels, fortifications and tombs rather than palaces, and decorated their pottery with bands of marching soldiers rather than octopus and seaweed. Both these civilizations came to an end around 1100 BC, that of Crete possibly because of volcanic devastation, and that of Mycenae because of an invasion by the Dorian people who lived on the Greek mainland. Following these events, there was a period from which few signs of culture remain. This period is thus often referred to as a Dark Age.
Art.
The art history of the Hellenic era is generally subdivided into four periods: the Protogeometric (1100–900 BC), the Geometric (900–700 BC), the Archaic (700 – 500 BC) and the Classical (500 – 323 BC) with sculpture being further divided into Severe Classical, High Classical and Late Classical. The first signs of the particular artistic character that defines ancient Greek architecture are to be seen in the pottery of the Dorian Greeks from the 10th century BC. Already at this period it is created with a sense of proportion, symmetry and balance not apparent in similar pottery from Crete and Mycenae. The decoration is precisely geometric, and ordered neatly into zones on defined areas of each vessel. These qualities were to manifest themselves not only through a millennium of Greek pottery making, but also in the architecture that was to emerge in the 6th century. The major development that occurred was in the growing use of the human figure as the major decorative motif, and the increasing surety with which humanity, its mythology, activities and passions were depicted.
The development in the depiction of the human form in pottery was accompanied by a similar development in sculpture. The tiny stylised bronzes of the Geometric period gave way to life-sized highly formalised monolithic representation in the Archaic period. The Classical period was marked by a rapid development towards idealised but increasingly lifelike depictions of gods in human form. This development had a direct effect on the sculptural decoration of temples, as many of the greatest extant works of ancient Greek sculpture once adorned temples, and many of the largest recorded statues of the age, such as the lost chryselephantine statues of Zeus at the Temple of Zeus at Olympia and Athena at the Parthenon, Athens, both over 40 feet high, were once housed in them.
Religion and philosophy.
The religion of ancient Greece was a form of nature worship that grew out of the beliefs of earlier cultures. However, unlike earlier cultures, man was no longer perceived as being threatened by nature, but as its sublime product. The natural elements were personified as gods of completely human form, and very human behaviour.
The home of the gods was thought to be Olympus, the highest mountain in Greece. The most important deities were: Zeus, the supreme god and ruler of the sky; Hera, his wife and goddess of marriage; Athena, goddess of wisdom; Poseidon, god of the sea; Demeter, goddess of the earth; Apollo, god of the sun, law, reason, music and poetry; Artemis, goddess of the moon, the hunt and the wilderness; Aphrodite, goddess of love; Ares, God of war; Hermes, god of commerce and medicine, Hephaestus, god of fire and metalwork, and Dionysus, god of wine and fruit-bearing plants. Worship, like many other activities, was done in community, in the open. However, by 600 BC, the gods were often represented by large statues and it was necessary to provide a building in which each of these could be housed. This led to the development of temples.
The ancient Greeks perceived order in the universe, and in turn, applied order and reason to their creations. Their humanist philosophy put mankind at the centre of things, and promoted well-ordered societies and the development of democracy. At the same time, the respect for human intellect demanded reason, and promoted a passion for enquiry, logic, challenge, and problem solving. The architecture of the ancient Greeks, and in particular, temple architecture, responds to these challenges with a passion for beauty, and for order and symmetry which is the product of a continual search for perfection, rather than a simple application of a set of working rules.
Architectural character.
Early development.
There is a clear division between the architecture of the preceding Mycenaean culture and Minoan cultures and that of the ancient Greeks, the techniques and an understanding of their style being lost when these civilisations fell.
Mycenaean art is marked by its circular structures and tapered domes with flat-bedded, cantilevered courses. This architectural form did not carry over into the architecture of ancient Greece, but reappeared about 400 BC in the interior of large monumental tombs such as the Lion Tomb at Cnidos (c. 350 BC). Little is known of Mycenaean wooden or domestic architecture and any continuing traditions that may have flowed into the early buildings of the Dorian people.
The Minoan architecture of Crete, was of trabeated form like that of ancient Greece. It employed wooden columns with capitals, but the columns were of very different form to Doric columns, being narrow at the base and splaying upward. The earliest forms of columns in Greece seem to have developed independently. As with Minoan architecture, ancient Greek domestic architecture centred on open spaces or courtyards surrounded by colonnades. This form was adapted to the construction of hypostyle halls within the larger temples. The evolution that occurred in architecture was towards public building, first and foremost the temple, rather than towards grand domestic architecture such as had evolved in Crete.
Types of buildings.
Domestic buildings.
The Greek word for the family or household, oikos, is also the name for the house. Houses followed several different types. It is probable that many of the earliest houses were simple structures of two rooms, with an open porch or "pronaos" above which rose a low pitched gable or pediment. This form is thought to have contributed to temple architecture.
The construction of many houses employed walls of sun dried clay bricks or wooden framework filled with fibrous material such as straw or seaweed covered with clay or plaster, on a base of stone which protected the more vulnerable elements from damp. The roofs were probably of thatch with eaves which overhung the permeable walls. Many larger houses, such as those at Delos, were built of stone and plastered. The roofing material for substantial house was tile. Houses of the wealthy had mosaic floors and demonstrated the Classical style.
Many houses centred on a wide passage or "pasta" which ran the length of the house and opened at one side onto a small courtyard which admitted light and air. Larger houses had a fully developed peristyle courtyard at the centre, with the rooms arranged around it. Some houses had an upper floor which appears to have been reserved for the use of the women of the family.
City houses were built with adjoining walls and were divided into small blocks by narrow streets. Shops were sometimes located in the rooms towards the street. City houses were inward-facing, with major openings looking onto the central courtyard, rather than the street.
Public buildings.
The rectangular temple is the most common and best-known form of Greek public architecture. The temple did not serve the same function as a modern church, since the altar stood under the open sky in the temenos or sacred precinct, often directly before the temple. Temples served as the location of a cult image and as a storage place or strong room for the treasury associated with the cult of the god in question, and as a place for devotees of the god to leave their votive offerings, such as statues, helmets and weapons. Some Greek temples appear to have been oriented astronomically. The temple was generally part of a religious precinct known as the "acropolis". According to Aristotle, '"the site should be a spot seen far and wide, which gives good elevation to virtue and towers over the neighbourhood". Small circular temples, "tholos" were also constructed, as well as small temple-like buildings that served as treasuries for specific groups of donors.
During the late 5th and 4th centuries BC, town planning became an important consideration of Greek builders, with towns such as Paestum and Priene being laid out with a regular grid of paved streets and an "agora" or central market place surrounded by a colonnade or "stoa". The completely restored Stoa of Attalos can be seen in Athens. Towns were also equipped with a public fountain where water could be collected for household use. The development of regular town plans is associated with Hippodamus of Miletus, a pupil of Pythagoras.
Public buildings became "dignified and gracious structures", and were sited so that they related to each other architecturally. The propylon or porch, formed the entrance to temple sanctuaries and other significant sites with the best-surviving example being the Propylaea on the Acropolis of Athens. The bouleuterion was a large public building with a "hypostyle" hall that served as a court house and as a meeting place for the town council (boule). Remnants of bouleuterion survive at Athens, Olympia and Miletus, the latter having held up to 1200 people.
Every Greek town had an open-air theatre. These were used for both public meetings as well as dramatic performances. The theatre was usually set in a hillside outside the town, and had rows of tiered seating set in a semicircle around the central performance area, the "orchestra". Behind the orchestra was a low building called the "skênê", which served as a store-room, a dressing-room, and also as a backdrop to the action taking place in the orchestra. A number of Greek theatres survive almost intact, the best known being at Epidaurus, by the architect Polykleitos the Younger.
Greek towns of substantial size also had a palaestra or a gymnasium, the social centre for male citizens which included spectator areas, baths, toilets and club rooms. Other buildings associated with sports include the "hippodrome" for horse racing, of which only remnants have survived, and the "stadium" for foot racing, 600 feet in length, of which examples exist at Olympia, Delphi, Epidarus and Ephesus, while the Panathinaiko Stadium in Athens, which seats 45,000 people, was restored in the 19th century and was used in the 1896, 1906 and 2004 Olympic Games.
Structure.
Column and lintel.
[[File:Doric.JPG|thumb|upright=1.2|Parts of an Ancient Greek temple of the Doric Order:
1. Tympanum, 2. Acroterium, 3. Sima 4. Cornice 5. Mutules 7. Frieze 8. Triglyph 9. Metope 10. Regula 11. Gutta 12. Taenia 13. Architrave 14. Capital 15. Abacus 16. Echinus 17. Column 18. Fluting 19. Stylobate]]
The architecture of ancient Greece is of a trabeated or "post and lintel" form, i.e. it is composed of upright beams (posts) supporting horizontal beams (lintels). Although the existent buildings of the era are constructed in stone, it is clear that the origin of the style lies in simple wooden structures, with vertical posts supporting beams which carried a ridged roof. The posts and beams divided the walls into regular compartments which could be left as openings, or filled with sun dried bricks, lathes or straw and covered with clay daub or plaster. Alternately, the spaces might be filled with rubble. It is likely that many early houses and temples were constructed with an open porch or "pronaos" above which rose a low pitched gable or pediment.
The earliest temples, built to enshrine statues of deities, were probably of wooden construction, later replaced by the more durable stone temples many of which are still in evidence today. The signs of the original timber nature of the architecture were maintained in the stone buildings.
A few of these temples are very large, with several, such as the Temple of Zeus Olympus and the Olympians at Athens being well over 300 feet in length, but most were less than half this size. It appears that some of the large temples began as wooden constructions in which the columns were replaced piecemeal as stone became available. This, at least was the interpretation of the historian Pausanias looking at the Temple of Hera at Olympia in the 2nd century AD.
The stone columns are made of a series of solid stone cylinders or "drums" that rest on each other without mortar, but were sometimes centred with a bronze pin. The columns are wider at the base than at the top, tapering with an outward curve known as "entasis". Each column has a capital of two parts, the upper, on which rests the lintels, being square and called the "abacus". The part of the capital that rises from the column itself is called the "echinus". It differs according to the order, being plain in the Doric Order, fluted in the Ionic and foliate in the Corinthian. Doric and usually Ionic capitals are cut with vertical grooves known as "fluting". This fluting or grooving of the columns is a retention of an element of the original wooden architecture.
Entablature and pediment.
The columns of a temple support a structure that rises in two main stages, the entablature and the pediment.
The entablature is the major horizontal structural element supporting the roof and encircling the entire building. It is composed of three parts. Resting on the columns is the architrave made of a series of stone "lintels" that spanned the space between the columns, and meet each other at a joint directly above the centre of each column.
Above the architrave is a second horizontal stage called the "frieze". The frieze is one of the major decorative elements of the building and carries a sculptured relief. In the case of Ionic and Corinthian architecture, the relief decoration runs in a continuous band, but in the Doric Order, it is divided into sections called "metopes" which fill the spaces between vertical rectangular blocks called "triglyphs". The triglyphs are vertically grooved like the Doric columns, and retain the form of the wooden beams that would once have supported the roof.
The upper band of the entablature is called the "cornice", which is generally ornately decorated on its lower edge. The cornice retains the shape of the beams that would once have supported the wooden roof at each end of the building. At the front and rear of each temple, the entablature supports a triangular structure called the "pediment". The triangular space framed by the cornices is the location of the most significant sculptural decoration on the exterior of the building.
Masonry.
Every temple rested on a masonry base called the crepidoma, generally of three steps, of which the upper one which carried the columns was the "stylobate". Masonry walls were employed for temples from about 600 BC onwards. Masonry of all types was used for ancient Greek buildings, including rubble, but the finest ashlar masonry was usually employed for temple walls, in regular courses and large sizes to minimise the joints. The blocks were rough hewn and hauled from quarries to be cut and bedded very precisely, with mortar hardly ever being used. Blocks, particularly those of columns and parts of the building bearing loads were sometimes fixed in place or reinforced with iron clamps, dowels and rods of wood, bronze or iron fixed in lead to minimise corrosion.
Openings.
Door and window openings were spanned with a lintel, which in a stone building limited the possible width of the opening. The distance between columns was similarly affected by the nature of the lintel, columns on the exterior of buildings and carrying stone lintels being closer together than those on the interior, which carried wooden lintels. Door and window openings narrowed towards the top. Temples were constructed without windows, the light to the naos entering through the door. It has been suggested that some temples were lit from openings in the roof. A door of the Ionic Order at the Erechtheion (17 feet high and 7.5 feet wide at the top) retains many of its features intact, including mouldings, and an entablature supported on console brackets. (See Architectural Decoration, below)
Roof.
The widest span of a temple roof was across the cella, or internal space. In a large building, this space contains columns to support the roof, the architectural form being known as hypostyle. It appears that, although the architecture of ancient Greece was initially of wooden construction, the early builders did not have the concept of the diagonal truss as a stabilising member. This is evidenced by the nature of temple construction in the 6th century BC, where the rows of columns supporting the roof the cella rise higher than the outer walls, unnecessary if roof trusses are employed as an integral part of the wooden roof. The indication is that initially all the rafters were supported directly by the entablature, walls and hypostyle, rather than on a trussed wooden frame, which came into use in Greek architecture only in the 3rd century BC.
Ancient Greek buildings of timber, clay and plaster construction were probably roofed with thatch. With the rise of stone architecture came the appearance of fired ceramic roof tiles. These early roof tiles showed an S-shape, with the pan and cover tile forming one piece. They were much larger than modern roof tiles, being up to long, wide, thick and weighing around apiece. Only stone walls, which were replacing the earlier mudbrick and wood walls, were strong enough to support the weight of a tiled roof.
The earliest finds of roof tiles of the Archaic period in Greece are documented from a very restricted area around Corinth, where fired tiles began to replace thatched roofs at the temples of Apollo and Poseidon between 700 and 650 BC. Spreading rapidly, roof tiles were within fifty years in evidence for a large number of sites around the Eastern Mediterranean, including Mainland Greece, Western Asia Minor, Southern and Central Italy. Being more expensive and labour-intensive to produce than thatch, their introduction has been explained by the fact that their fireproof quality would have given desired protection to the costly temples. As a side-effect, it has been assumed that the new stone and tile construction also ushered in the end of overhanging eaves in Greek architecture, as they made the need for an extended roof as rain protection for the mudbrick walls obsolete.
Vaults and arches were not generally used, but begin to appear in tombs (in a "beehive" or cantilevered form such as used in Mycenaea) and occasionally, as an external feature, exedrae of voussoired construction from the 5th century BC. The dome and vault never became significant structural features, as they were to become in ancient Roman architecture.
Temple plans.
Most ancient Greek temples were rectangular, and were approximately twice as long as they were wide, with some notable exceptions such as the enormous Temple of Olympian Zeus, Athens with a length of nearly 2½ times its width. A number of surviving temple-like structures are circular, and are referred to as "tholos". The smallest temples are less than 25 metres (approx. 75 feet) in length, or in the case of the circular "tholos", in diameter. The great majority of temples are between 30–60 metres (approx. 100–200 feet) in length. A small group of Doric temples, including the Parthenon, are between 60–80 metres (approx. 200–260 feet) in length. The largest temples, mainly Ionic and Corinthian, but including the Doric Temple of the Olympian Zeus, Agrigento, were between 90–120 metres (approx. 300–390 feet) in length.
The temple rises from a stepped base or "stylobate", which elevates the structure above the ground on which it stands. Early examples, such as the Temple of Zeus at Olympus, have two steps, but the majority, like the Parthenon, have three, with the exceptional example of the Temple of Apollo at Didyma having six. The core of the building is a masonry-built "naos" within which is a cella, a windowless room originally housing the statue of the god. The cella generally has a porch or "pronaos" before it, and perhaps a second chamber or "antenaos" serving as a treasury or repository for trophies and gifts. The chambers were lit by a single large doorway, fitted with a wrought iron grill. Some rooms appear to have been illuminated by skylights.
On the stylobate, often completely surrounding the naos, stand rows of columns. Each temple is defined as being of a particular type, with two terms: one describing the number of columns across the entrance front, and the other defining their distribution.
Examples:
Proportion and optical illusion.
The ideal of proportion that was used by ancient Greek architects in designing temples was not a simple mathematical progression using a square module. The math involved a more complex geometrical progression, the so-called "Golden mean". The ratio is similar to that of the growth patterns of many spiral forms that occur in nature such as rams' horns, nautilus shells, fern fronds, and vine tendrils and which were a source of decorative motifs employed by ancient Greek architects as particularly in evidence in the volutes of capitals of the Ionic and Corinthian Orders.
The ancient Greek architects took a philosophic approach to the rules and proportions. The determining factor in the mathematics of any notable work of architecture was its ultimate appearance. The architects calculated for perspective, for the optical illusions that make edges of objects appear concave and for the fact that columns that are viewed against the sky look different from those adjacent that are viewed against a shadowed wall. Because of these factors, the architects adjusted the plans so that the major lines of any significant building are rarely straight.
The most obvious adjustment is to the profile of columns, which narrow from base to top. However, the narrowing is not regular, but gently curved so that each columns appears to have a slight swelling, called "entasis" below the middle. The "entasis" is never sufficiently pronounced as to make the swelling wider than the base; it is controlled by a slight reduction in the rate of decrease of diameter.
The Parthenon, the Temple to the Goddess Athena on the Acropolis in Athens, is the epitome of what Nikolaus Pevsner called "the most perfect example ever achieved of architecture finding its fulfilment in bodily beauty". Helen Gardner refers to its "unsurpassable excellence", to be surveyed, studied and emulated by architects of later ages. Yet, as Gardner points out, there is hardly a straight line in the building. Banister Fletcher calculated that the "stylobate" curves upward so that its centres at either end rise about 2.6 inches above the outer corners, and 4.3 inches on the longer sides. A slightly greater adjustment has been made to the entablature. The columns at the ends of the building are not vertical but are inclined towards the centre, with those at the corners being out of plumb by about 2.6 inches. These outer columns are both slightly wider than their neighbours and are slightly closer than any of the others.
Style.
Orders.
Ancient Greek architecture of the most formal type, for temples and other public buildings, is divided stylistically into three "orders", first described by the Roman architectural writer Vitruvius. These are: the Doric Order, the Ionic Order and the Corinthian Order, the names reflecting their regional origins within the Greek world. While the three orders are most easily recognizable by their capitals, the orders also governed the form, proportions, details and relationships of the columns, entablature, pediment and the stylobate. The different orders were applied to the whole range of buildings and monuments.
The Doric Order developed on mainland Greece and spread to Magna Graecia (Italy). It was firmly established and well-defined in its characteristics by the time of the building of the Temple of Hera at Olympia, c. 600 BC. The Ionic order co-existed with the Doric, being favoured by the Greek cites of Ionia, in Asia Minor and the Aegean Islands. It did not reach a clearly defined form until the mid 5th century BC. The early Ionic temples of Asia Minor were particularly ambitious in scale, such as the Temple of Artemis at Ephesus. The Corinthian Order was a highly decorative variant not developed until the Hellenistic period and retaining many characteristics of the Ionic. It was popularised by the Romans.
Doric Order.
The Doric order is recognised by its capital, of which the "echinus" is like a circular cushion rising from the top of the column to the square "abacus" on which rest the lintels. The echinus appears flat and splayed in early examples, deeper and with greater curve in later, more refined examples, and smaller and straight-sided in Hellenistc examples. A refinement of the Doric column is the entasis, a gentle convex swelling to the profile of the column, which prevents an optical illusion of concavity. This is more pronounced in earlier examples.
Doric columns are almost always cut with grooves, known as "fluting", which run the length of the column and are usually 20 in number, although sometimes fewer. The flutes meet at sharp edges called "arrises". At the top of the columns, slightly below the narrowest point, and crossing the terminating arrises, are three horizontal grooves known as the "hypotrachelion". Doric columns have no bases, until a few examples in the Hellenistic period.
The columns of an early Doric temple such as the Temple of Apollo at Syracuse, Sicily, may have a height to base diameter ratio of only 4:1 and a column height to entablature ratio of 2:1, with relatively crude details. A column height to diameter of 6:1 became more usual, while the column height to entablature ratio at the Parthenon is about 3:1. During the Hellenistic period, Doric conventions of solidity and masculinity dropped away, with the slender and unfluted columns reaching a height to diameter ratio of 7.5:1.
The Doric entablature is in three parts, the architrave, the frieze and the cornice. The architrave is composed of the stone lintels which span the space between the columns, with a joint occurring above the centre of each abacus. On this rests the frieze, one of the major areas of sculptural decoration. The frieze is divided into "triglyphs" and "metopes", the triglyphs, as stated elsewhere in this article, are a reminder of the timber history of the architectural style. Each triglyph has three vertical grooves, similar to the columnar fluting, and below them, seemingly connected, are guttae, small strips that appear to connect the triglyphs to the architrave below. A triglyph is located above the centre of each capital, and above the centre of each lintel. However, at the corners of the building, the triglyphs do not fall over the centre the column. The ancient architects took a pragmatic approach to the apparent "rules", simply extending the width of the last two metopes at each end of the building.
The cornice is a narrow jutting band of complex moulding which overhangs and protects the ornamented frieze, like the edge of an overhanging wooden-framed roof. It is decorated on the underside with projecting blocks, "mutules", further suggesting the wooden nature of the prototype. At either end of the building the pediment rises from the cornice, framed by moulding of similar form.
The pediment is decorated with figures that are in relief in the earlier examples, though almost freestanding by the time of the sculpture on the Parthenon. Early architectural sculptors found difficulty in creating satisfactory sculptural compositions in the tapering triangular space. By the Early Classical period, with the decoration of the Temple of Zeus at Olympia, (486-460 BC) the sculptors had solved the problem by having a standing central figure framed by rearing centaurs and fighting men who are falling, kneeling and lying in attitudes that fit the size and angle of each part of the space. The famous sculptor Phidias fills the space at the Parthenon (448-432 BC) with a complex array of draped and undraped figures of deities who appear in attitudes of sublime relaxation and elegance.
Ionic Order.
The Ionic Order is recognised by its voluted capital, in which a curved "echinus" of similar shape to that of the Doric Order, but decorated with stylised ornament, is surmounted by a horizontal band that scrolls under to either side, forming spirals or "volutes" similar to those of the nautilus shell or ram's horn. In plan, the capital is rectangular. It is designed to be viewed frontally but the capitals at the corners of buildings are modified with an additional scroll so as to appear regular on two adjoining faces. In the Hellenistic period, four-fronted Ionic capitals became common.
Like the Doric Order, the Ionic Order retains signs of having its origins in wooden architecture. The horizontal spread of a flat timber plate across the top of a column is a common device in wooden construction, giving a thin upright a wider area on which to bear the lintel, while at the same time reinforcing the load-bearing strength of the lintel itself. Likewise, the columns always have bases, a necessity in wooden architecture to spread the load and protect the base of a comparatively thin upright. The columns are fluted with narrow, shallow flutes that do not meet at a sharp edge but have a flat band or "fillet" between them. The usual number of flutes is twenty-four but there may be as many as forty-four. The base has two convex mouldings called "torus", and from the late Hellenic period stood on a square plinth similar to the "abacus".
The architrave of the Ionic Order is sometimes undecorated, but more often rises in three outwardly-stepped bands like overlapping timber planks. The frieze, which runs in a continuous band, is separated from the other members by rows of small projecting blocks. They are referred to as "dentils", meaning "teeth", but their origin is clearly in narrow wooden slats which supported the roof of a timber structure.
The Ionic Order is altogether lighter in appearance than the Doric, with the columns, including base and capital, having a 9:1 ratio with the diameter, while the whole entablature was also much narrower and less heavy than the Doric entablature. There was some variation in the distribution of decoration. Formalised bands of motifs such as alternating forms known as "egg and dart" were a feature of the Ionic entablatures, along with the bands of "dentils". The external frieze often contained a continuous band of figurative sculpture or ornament, but this was not always the case. Sometimes a decorative frieze occurred around the upper part of the "naos" rather than on the exterior of the building. These Ionic-style friezes around the "naos" are sometimes found on Doric buildings, notably the Parthenon. Some temples, like the Temple of Artemis at Ephesus, had friezes of figures around the lower drum of each column, separated from the fluted section by a bold moulding.
Caryatids, draped female figures used as supporting members to carry the entablature, were a feature of the Ionic order, occurring at several buildings including the Siphnian Treasury at Delphi in 525 BC and at the Erechtheion, about 410 BC.
Corinthian Order.
The Corinthian Order does not have its origin in wooden architecture. It grew directly out of the Ionic in the mid 5th century BC, and was initially of much the same style and proportion, but distinguished by its more ornate capitals. The capital was very much deeper than either the Doric or the Ionic capital, being shaped like a large "krater", a bell-shaped mixing bowl, and being ornamented with a double row of acanthus leaves above which rose voluted tendrils, supporting the corners of the abacus, which, no longer perfectly square, splayed above them. According to Vitruvius, the capital was invented by a bronze founder, Callimachus of Corinth, who took his inspiration from a basket of offerings that had been placed on a grave, with a flat tile on top to protect the goods. The basket had been placed on the root of an acanthus plant which had grown up around it. The ratio of the column height to diameter is generally 10:1, with the capital taking up more than 1/10 of the height. The ratio of capital height to diameter is generally about 1.16:1.
The Corinthian Order was initially used internally, as at the Temple of Apollo Epicurius at Bassae (c.450-425 BC). In 334 BC it appeared as an external feature on the Choragic Monument of Lysicrates in Athens, and then on a huge scale at the Temple of Zeus Olympia in Athens, (174 BC - AD 132). It was popularised by the Romans, who added a number of refinements and decorative details. During the Hellenistic period, Corinthian columns were sometimes built without fluting.
Decoration.
Architectural ornament.
Early wooden structures, particularly temples, were ornamented and in part protected by fired and painted clay revetments in the form of rectangular panels, and ornamental discs. Many fragments of these have outlived the buildings that they decorated and demonstrate a wealth of formal border designs of geometric scrolls, overlapping patterns and foliate motifs. With the introduction of stone-built temples, the revetments no longer served a protective purpose and sculptured decoration became more common.
The clay ornaments were limited to the roof of buildings, decorating the cornice, the corners and surmounting the pediment. At the corners of pediments they were called "acroteria" and along the sides of the building, "antefixes". Early decorative elements were generally semi-circular, but later of roughly triangular shape with moulded ornament, often palmate. Ionic cornices were often set with a row of lion's masks, with open mouths that ejected rainwater. From the Late Classical period, acroteria were sometimes sculptured figures.See "Architectural sculpture"
In the three orders of ancient Greek architecture, the sculptural decoration, be it a simple half round "astragal", a frieze of stylised foliage or the ornate sculpture of the pediment, is all essential to the architecture of which it is a part. In the Doric order, there is no variation in its placement. Reliefs never decorate walls in an arbitrary way. The sculpture is always located in several predetermined areas, the metopes and the pediment. In later Ionic architecture, there is greater diversity in the types and numbers of mouldings and decorations, particularly around doorways, where voluted brackets sometimes occur supporting an ornamental cornice over a door, such as that at the Erechtheion. A much applied narrow moulding is called "bead and reel" and is symmetrical, stemming from turned wooden prototypes. Wider mouldings include one with tongue-like or pointed leaf shapes, which are grooved and sometimes turned upward at the tip, and "egg and dart" moulding which alternates ovoid shapes with narrow pointy ones.
Architectural sculpture.
Architectural sculpture showed a development from early Archaic examples through Severe Classical, High Classical, Late Classical and Hellenistic. Remnants of Archaic architectural sculpture (700 - 500 BC) exist from the early 6th century BC with the earliest surviving pedimental sculpture being fragments of a Gorgon flanked by heraldic panthers from the centre of the pediment of the Artemis Temple of Corfu. A metope from a temple known as "Temple C" at Selinus, Sicily, shows, in a better preserved state, Perseus slaying the Gorgon Medusa. Both images parallel the stylised depiction of the Gorgons on the black figure name vase decorated by the Nessos painter (c. 600 BC), with the face and shoulders turned frontally, and the legs in a running or kneeling position. At this date images of terrifying monsters have predominance over the emphasis on the human figure that developed with Humanist philosophy.
The Severe Classical style (500 – 450 BC) is represented by the pedimental sculptures of the Temple of Zeus at Olympia, (470 – 456 BC). The eastern pediment shows a moment of stillness and "impending drama" before the beginning of a chariot race, the figures of Zeus and the competitors being severe and idealised representations of the human form. The western pediment has Apollo as the central figure, "majestic" and "remote", presiding over a battle of Lapiths and Centaurs, in strong contrast to that of the eastern pediment for its depiction of violent action, and described by D. E. Strong as the "most powerful piece of illustration" for a hundred years.
The shallow reliefs and three-dimensional sculpture which adorned the frieze and pediments, respectively, of the Parthenon, are the lifelike products of the High Classical style (450 – 400 BC) and were created under the direction of the sculptor Phidias. The pedimental sculpture represents the Gods of Olympus, while the frieze shows the Panathenaic procession and ceremonial events that took place every four years to honour the titular Goddess of Athens. The frieze and remaining figures of the eastern pediment show a profound understanding of the human body, and how it varies depending upon its position and the stresses that action and emotion place upon it. Benjamin Robert Haydon described the reclining figure of Dionysus as "...the most heroic style of art, combined with all the essential detail of actual life".
The names of many famous sculptors are known from the Late Classical period (400 – 323 BC), including Timotheos, Praxiteles, Leochares and Skopas, but their works are known mainly from Roman copies. Little architectural sculpture of the period remains intact. The Temple of Asclepius at Epidauros had sculpture by Timotheos working with the architect Theodotos. Fragments of the eastern pediment survive, showing the Sack of Troy. The scene appears to have filled the space with figures carefully arranged to fit the slope and shape available, as with earlier east pediment of the Temple of Zeus at Olympus. But the figures are more violent in action, the central space taken up, not with a commanding God, but with the dynamic figure of Neoptolemos as he seizes the aged king Priam and stabs him. The remaining fragments give the impression of a whole range of human emotions, fear, horror, cruelty and lust for conquest. The "acroteria" were sculptured by Timotheus, except for that at the centre of the east pediment which is the work of the architect. The palmate acroteria have been replaced here with small figures, the eastern pediment being surmounted by a winged Nike, poised against the wind.
Hellenistic architectural sculpture (323 – 31 BC) was to become more flamboyant, both in the rendering of expression and motion, which is often emphasised by flowing draperies, the Nike Samothrace which decorated a monument in the shape of a ship being a well known example. The Pergamon Altar (c. 180–160 BC) has a frieze (120 metres long by 2.3 metres high) of figures in very high relief. The frieze represents the battle for supremacy of Gods and Titans, and employs many dramatic devices: frenzy, pathos and triumph, to convey the sense of conflict.

</doc>
<doc id="52685" url="https://en.wikipedia.org/wiki?curid=52685" title="Ancient Roman architecture">
Ancient Roman architecture

Ancient Roman architecture adopted the external language of classical Greek architecture for the purposes of the ancient Romans, which were so different from Greek buildings as to create a new architectural style. The two styles are often considered one body of classical architecture. Roman architecture flourished in the Roman Republic and even more so under the Empire, when the great majority of surviving buildings were constructed. It used new materials, particularly concrete, and newer technologies such as the arch and the dome to make buildings that were typically strong and well-engineered. Large numbers remain in some form across the empire, sometimes complete and still in use.
Roman Architecture covers the period from the establishment of the Roman Republic in 509 BC to about the 4th century AD, after which it becomes reclassified as Late Antique or Byzantine architecture. Almost no substantial examples survive from before about 100 BC, and most of the major survivals are from the later empire, after about 100 AD. Roman architectural style continued to influence building in the former empire for many centuries, and the style used in Western Europe beginning about 1000 is called Romanesque architecture to reflect this dependence on basic Roman forms.
The Romans only began to achieve significant originality in architecture around the beginning of the Imperial period, after they had combined aspects of their original Etruscan architecture with others taken from Greece, including most elements of the style we now call classical architecture. They moved from trabeated construction mostly based on columns and lintels to one based on massive walls, punctuated by arches, and later domes, both of which greatly developed under the Romans. The classical orders now became largely decorative rather than structural, except in colonnades. Stylistic developments included the Tuscan and Composite orders; the first being a shortened, simplified variant on the Doric order and the Composite being a tall order with the floral decoration of the Corinthian and the scrolls of the Ionic. The period from roughly 40 BC to about 230 AD saw most of the greatest achievements, before the Crisis of the Third Century and later troubles reduced the wealth and organizing power of the central government.
The Romans produced massive public buildings and works of civil engineering, and were responsible for significant developments in housing and public hygiene, for example their public and private baths and latrines, under-floor heating in the form of the hypocaust, mica glazing (examples in Ostia Antica), and piped hot and cold water (examples in Pompeii and Ostia).
Overview.
Despite the technical developments of the Romans, which took their buildings far away from the basic Greek conception where columns were needed to support heavy beams and roofs, they were very reluctant to abandon the classical orders in formal public buildings, even though these had become essentially decorative. However, they did not feel entirely restricted by Greek aesthetic concerns, and treated the orders with considerable freedom.
Innovation started in the 3rd or 2nd century BC with the development of Roman concrete as a readily available adjunct to, or substitute for, stone and brick. More daring buildings soon followed, with great pillars supporting broad arches and domes. The freedom of concrete also inspired the colonnade screen, a row of purely decorative columns in front of a load-bearing wall. In smaller-scale architecture, concrete's strength freed the floor plan from rectangular cells to a more free-flowing environment.
Factors such as wealth and high population densities in cities forced the ancient Romans to discover new architectural solutions of their own. The use of vaults and arches, together with a sound knowledge of building materials, enabled them to achieve unprecedented successes in the construction of imposing infrastructure for public use. Examples include the aqueducts of Rome, the Baths of Diocletian and the Baths of Caracalla, the basilicas and Colosseum. These were reproduced at a smaller scale in most important towns and cities in the Empire. Some surviving structures are almost complete, such as the town walls of Lugo in Hispania Tarraconensis, now northern Spain. The administrative structure and wealth of the empire made possible very large projects even in locations remote from the main centres, as did the use of slave labour, both skilled and unskilled.
Especially under the empire, architecture often served a political function, demonstrating the power of the Roman state in general, and of specific individuals responsible for building. Roman architecture perhaps reached its peak in the reign of Hadrian, whose many achievements include rebuilding the Pantheon in its current form and leaving his mark on the landscape of northern Britain with Hadrian's Wall.
Origins.
While borrowing much from the preceding Etruscan architecture, such as the use of hydraulics and the construction of arches, Roman prestige architecture remained firmly under the spell of Ancient Greek architecture and the classical orders. This came initially from Magna Graecia, the Greek colonies in southern Italy, and indirectly from Greek influence on the Etruscans, but after the Roman conquest of Greece directly from the best classical and Hellenistic examples in the Greek world. The influence is evident in many ways; for example, in the introduction and use of the Triclinium in Roman villas as a place and manner of dining. Roman builders employed Greeks in many capacities, especially in the great boom in construction in the early Empire; some were slaves originating with the conquest.
Roman Architectural Revolution.
The Roman Architectural Revolution, also known as the "Concrete Revolution", was the widespread use in Roman architecture of the previously little-used architectural forms of the arch, vault, and dome. For the first time in history, their potential was fully exploited in the construction of a wide range of civil engineering structures, public buildings, and military facilities. These included amphitheatres, aqueducts, baths, bridges, circuses, dams, domes, harbours, and temples.
A crucial factor in this development, which saw a trend toward monumental architecture, was the invention of Roman concrete ("opus caementicium"), which led to the liberation of shapes from the dictates of the traditional materials of stone and brick.
These enabled the building of the many aqueducts throughout the empire, such as the Aqueduct of Segovia, the Pont du Gard, and the eleven aqueducts of Rome. The same concepts produced numerous bridges, some of which are still in daily use, for example the Puente Romano at Mérida in Spain, and the Pont Julien and the bridge at Vaison-la-Romaine, both in Provence, France.
The dome permitted construction of vaulted ceilings without crossbeams and made possible large covered public space such as public baths and basilicas, such as Hadrian's Pantheon, the Baths of Diocletian and the Baths of Caracalla, all in Rome.
The Romans first adopted the arch from the Etruscans, and implemented it in their own building. The use of arches that spring directly from the tops of columns was a Roman development, seen from the 1st century AD, that was very widely adopted in medieval Western, Byzantine and Islamic architecture.
Domes.
The Romans were the first builders in the history of architecture to realize the potential of domes for the creation of large and well-defined interior spaces. Domes were introduced in a number of Roman building types such as temples, thermae, palaces, mausolea and later also churches. Half-domes also became a favoured architectural element and were adopted as apses in Christian sacred architecture.
Monumental domes began to appear in the 1st century BC in Rome and the provinces around the Mediterranean Sea. Along with vaults, they gradually replaced the traditional post and lintel construction which makes use of the column and architrave. The construction of domes was greatly facilitated by the invention of concrete, a process which has been termed the "Roman Architectural Revolution". Their enormous dimensions remained unsurpassed until the introduction of structural steel frames in the late 19th century (see List of the world's largest domes).
Influence on later architecture.
Roman architecture supplied the basic vocabulary of Pre-Romanesque and Romanesque architecture, and spread across Christian Europe well beyond the old frontiers of the empire, to Ireland and Scandinavia for example. In the East, Byzantine architecture developed new styles of churches, but most other buildings remained very close to Late Roman forms. The same can be said in turn of Islamic architecture, where Roman forms long continued, especially in private buildings such as houses and the Turkish bath, and civil engineering such as fortifications and bridges.
In Europe the Italian Renaissance saw a conscious revival of correct classical styles, initially purely based on Roman examples. Vitruvius was respectfully reinterpreted by a series of architectural writers, and the Tuscan and Composite orders formalized for the first time, to give five rather than three orders. After the flamboyance of Baroque architecture, the Neoclassical architecture of the 18th century revived purer versions of classical style, and for the first time added direct influence from the Greek world.
Numerous local classical styles developed, such as Palladian architecture, Georgian architecture and Regency architecture in the English-speaking world, Federal architecture in the United States, and later Stripped Classicism and PWA Moderne.
Roman influences may be found around us today, in banks, government buildings, great houses, and even small houses, perhaps in the form of a porch with Doric columns and a pediment or in a fireplace or a mosaic shower floor derived from a Roman original, often from Pompeii or Herculaneum. The mighty pillars, domes and arches of Rome echo in the New World too, where in Washington DC we see them in the Capitol Building, the White House, the Lincoln Memorial and other government buildings. All across the US the seats of regional government were normally built in the grand traditions of Rome, with vast flights of stone steps sweeping up to towering pillared porticoes, with huge domes gilded or decorated inside with the same or similar themes that were popular in Rome.
In Britain, a similar enthusiasm has seen the construction of thousands of neo-Classical buildings over the last five centuries, both civic and domestic, and many of the grandest country houses and mansions are purely Classical in style, an obvious example being Buckingham Palace.
Materials.
Stone.
Marble is not found especially close to Rome, and was only rarely used there before Augustus, who famously boasted that he had found Rome made of brick and left it made of marble, though this was mainly as a facing for brick or concrete. The Temple of Hercules Victor of the late 2nd century BC is the earliest surviving exception in Rome. From Augustus' reign the quarries at Carrara were extensively developed for the capital, and other sources around the empire exploited, especially the prestigious Greek marbles like Parian. Travertine limestone was found much closer, around Tivoli, and was used from the end of the Republic; the Colosseum is mainly built of this stone, which has good load-bearing capacity, with a brick core. Other more or less local stones were used around the empire.
The Romans were extremely fond of luxury imported coloured marbles with fancy veining, and the interiors of the most important buildings were very often faced with slabs of these, which have usually now been removed even where the building survives. Imports from Greece for this purpose began in the 2nd century BC.
Roman brick.
The Romans made fired clay bricks from about the beginning of the Empire, replacing earlier sun-dried mud-brick. Roman brick was almost invariably of a lesser height than modern brick, but was made in a variety of different shapes and sizes. Shapes included square, rectangular, triangular and round, and the largest bricks found have measured over three feet in length. Ancient Roman bricks had a general size of 1½ Roman feet by 1 Roman foot, but common variations up to 15 inches existed. Other brick sizes in Ancient Rome included 24" x 12" x 4", and 15" x 8" x 10". Ancient Roman bricks found in France measured 8" x 8" x 3". The Constantine Basilica in Trier is constructed from Roman bricks 15" square by 1½" thick. There is often little obvious difference (particularly when only fragments survive) between Roman bricks used for walls on the one hand, and tiles used for roofing or flooring on the other, so archaeologists sometimes prefer to employ the generic term ceramic building material (or CBM).
The Romans perfected brick-making during the first century of their empire and used it ubiquitously, in public and private construction alike. The Romans took their brickmaking skills everywhere they went, introducing the craft to the local populations. The Roman legions, which operated their own kilns, introduced bricks to many parts of the empire; bricks are often stamped with the mark of the legion that supervised their production. The use of bricks in southern and western Germany, for example, can be traced back to traditions already described by the Roman architect Vitruvius. In the British Isles, the introduction of Roman brick by the ancient Romans was followed by a 600–700 year gap in major brick production.
Roman concrete.
Concrete quickly supplanted brick as the primary building material, and more daring buildings soon followed, with great pillars supporting broad arches and domes rather than dense lines of columns suspending flat architraves. The freedom of concrete also inspired the colonnade screen, a row of purely decorative columns in front of a load-bearing wall. In smaller-scale architecture, concrete's strength freed the floor plan from rectangular cells to a more free-flowing environment. Most of these developments are described by Vitruvius, writing in the first century AD in his work De Architectura.
Although concrete had been used on a minor scale in Mesopotamia, Roman architects perfected Roman concrete and used it in buildings where it could stand on its own and support a great deal of weight. The first use of concrete by the Romans was in the town of Cosa sometime after 273 BC. Ancient Roman concrete was a mixture of lime mortar, aggregate, pozzolana, water, and stones, and was stronger than previously-used concretes. The ancient builders placed these ingredients in wooden frames where they hardened and bonded to a facing of stones or (more frequently) bricks. The aggregates used were often much larger than in modern concrete, amounting to rubble.
When the framework was removed, the new wall was very strong, with a rough surface of bricks or stones. This surface could be smoothed and faced with an attractive stucco or thin panels of marble or other coloured stones called revetment. Concrete construction proved to be more flexible and less costly than building solid stone buildings. The materials were readily available and not difficult to transport. The wooden frames could be used more than once, allowing builders to work quickly and efficiently. Concrete is arguably the Roman contribution most relevant to modern architecture.
City design.
The ancient Romans employed regular orthogonal structures on which they molded their colonies. They probably were inspired by Greek and Hellenic examples, as well as by regularly planned cities that were built by the Etruscans in Italy. (see Marzabotto)
The Romans used a consolidated scheme for city planning, developed for military defense and civil convenience. The basic plan consisted of a central forum with city services, surrounded by a compact, rectilinear grid of streets, and wrapped in a wall for defense. To reduce travel times, two diagonal streets crossed the square grid, passing through the central square. A river usually flowed through the city, providing water, transport, and sewage disposal. Hundreds of towns and cities were built by the Romans throughout their empire. Many European towns, such as Turin, preserve the remains of these schemes, which show the very logical way the Romans designed their cities. They would lay out the streets at right angles, in the form of a square grid. All roads were equal in width and length, except for two, which were slightly wider than the others. One of these ran east–west, the other, north–south, and they intersected in the middle to form the center of the grid. All roads were made of carefully fitted flag stones and filled in with smaller, hard-packed rocks and pebbles. Bridges were constructed where needed. Each square marked off by four roads was called an "insula," the Roman equivalent of a modern city block.
Each insula was square, with the land within it divided. As the city developed, each insula would eventually be filled with buildings of various shapes and sizes and crisscrossed with back roads and alleys. Most insulae were given to the first settlers of a Roman city, but each person had to pay to construct his own house.
The city was surrounded by a wall to protect it from invaders and to mark the city limits. Areas outside city limits were left open as farmland. At the end of each main road was a large gateway with watchtowers. A portcullis covered the opening when the city was under siege, and additional watchtowers were constructed along the city walls. An aqueduct was built outside the city walls.
The development of Greek and Roman urbanization is relatively well-known, as there are relatively many written sources, and there has been much attention to the subject, since the Romans and Greeks are generally regarded as the main ancestors of modern Western culture. It should not be forgotten, though, that the Etruscans had many considerable towns and there were also other cultures with more or less urban settlements in Europe, primarily of Celtic origin.
Building types.
Amphitheatre.
The amphitheatre was, with the triumphal arch and basilica, the only major new type of building developed by the Romans. Some of the most impressive secular buildings are the amphitheatres, over 69 being known and many of which are well preserved, such as that at Arles, as well as its progenitor, the Colosseum in Rome. They were used for gladiatorial contests, public displays, public meetings and bullfights, the tradition of which still survives in Spain. Their typical shape, functions and name distinguish them from Roman theatres, which are more or less semicircular in shape; from the circuses (akin to hippodromes) whose much longer circuits were designed mainly for horse or chariot racing events; and from the smaller stadia, which were primarily designed for athletics and footraces.
The earliest Roman amphitheatres date from the middle of the first century BC, but most were built under Imperial rule, from the Augustan period (69 BC–69 AD) onwards. Imperial amphitheatres were built throughout the Roman empire; the largest could accommodate 40,000–60,000 spectators, and the most elaborate featured multi-storeyed, arcaded façades and were elaborately decorated with marble, stucco and statuary. After the end of gladiatorial games in the 5th century and of animal killings in the 6th, most amphitheatres fell into disrepair, and their materials were mined or recycled. Some were razed, and others converted into fortifications. A few continued as convenient open meeting places; in some of these, churches were sited.
Architecturally, they are typically an example of the Roman use of the classical orders to decorate large concrete walls pierced at intervals, where the columns have nothing to support. Aesthetically, however, the formula is successful.
Basilica.
The Roman basilica was a large public building where business or legal matters could be transacted. They were normally where the magistrates held court, and used for other official ceremonies, having many of the functions of the modern town hall. The first basilicas had no religious function at all. As early as the time of Augustus, a public basilica for transacting business had been part of any settlement that considered itself a city, used in the same way as the late medieval covered market houses of northern Europe, where the meeting room, for lack of urban space, was set "above" the arcades, however. Although their form was variable, basilicas often contained interior colonnades that divided the space, giving aisles or arcaded spaces on one or both sides, with an apse at one end (or less often at each end), where the magistrates sat, often on a slightly raised dais. The central aisle tended to be wide and was higher than the flanking aisles, so that light could penetrate through the clerestory windows.
The oldest known basilica, the Basilica Porcia, was built in Rome in 184 BC by Cato the Elder during the time he was Censor. Other early examples include the basilica at Pompeii (late 2nd century BC). After Christianity became the official religion, the basilica shape was found appropriate for the first large public churches, with the attraction of avoiding reminiscences of the Greco-Roman temple form.
Circus.
The Roman circus was a large open-air venue used for public events in the ancient Roman Empire. The circuses were similar to the ancient Greek hippodromes, although circuses served varying purposes and differed in design and construction. Along with theatres and amphitheatres, Circuses were one of the main entertainment sites of the time. Circuses were venues for chariot races, horse races, and performances that commemorated important events of the empire were performed there. For events that involved re-enactments of naval battles, the circus was flooded with water.
The performance space of the Roman circus was normally, despite its name, an oblong rectangle of two linear sections of race track, separated by a median strip running along the length of about two thirds the track, joined at one end with a semicircular section and at the other end with an undivided section of track closed (in most cases) by a distinctive starting gate known as the carceres, thereby creating a circuit for the races.
Forum.
A forum was a central public open space in a Roman municipium, or any civitas, primarily used as a marketplace, along with the buildings used for shops and the stoas used for open stalls. Other large public buildings were often sited at the edges or close by. Many forums were constructed at remote locations along a road by the magistrate responsible for the road, in which case the forum was the only settlement at the site and had its own name, such as Forum Popili or Forum Livi.
Every city had at least one forum of varying size. In addition to its standard function as a marketplace, a forum was a gathering place of great social significance, and often the scene of diverse activities, including political discussions and debates, rendezvous, meetings, etc. Much the best known example is the Roman Forum, the earliest of several in Rome.
In new Roman towns the forum was usually located at, or just off, the intersection of the main north-south and east-west streets (the cardo and decumanus). All forums would have a Temple of Jupiter at the north end, and would also contain other temples, as well as the basilica; a public weights and measures table, so customers at the market could ensure they were not being sold short measures; and would often have the baths nearby.
Horreum.
A horreum was a type of public warehouse used during the ancient Roman period. Although the Latin term is often used to refer to granaries, Roman horrea were used to store many other types of consumables; the giant Horrea Galbae in Rome were used not only to store grain but also olive oil, wine, foodstuffs, clothing and even marble. By the end of the imperial period, the city of Rome had nearly 300 horrea to supply its demands. The biggest were enormous, even by modern standards; the Horrea Galbae contained 140 rooms on the ground floor alone, covering an area of some 225,000 square feet (21,000 m²).
The first horrea were built in Rome towards the end of the 2nd century BC, with the first known public horreum being constructed by the ill-fated tribune, Gaius Gracchus in 123 BC. The word came to be applied to any place designated for the preservation of goods; thus it was often used refer to cellars ("horrea subterranea"), but it could also be applied to a place where artworks were stored, or even to a library. Some public horrea functioned somewhat like banks, where valuables could be stored, but the most important class of horrea were those where foodstuffs such as grain and olive oil were stored and distributed by the state.
Insula.
Multi-story apartment blocks called insulae catered to a range of residential needs. The cheapest rooms were at the top owing to the inability to escape in the event of a fire and the lack of piped water. Windows were mostly small, facing the street, with iron security bars. Insulae were often dangerous, unhealthy, and prone to fires because of overcrowding and haphazard cooking arrangements. There are examples in the Roman port town of Ostia, that date back to the reign of Trajan, but they seem to have been found only in Rome and a few other places. Elsewhere writers report them as something remarkable, but Livy and Vituvius refer to them in Rome. External walls were in "Opus Reticulatum" and interiors in "Opus Incertum", which would then be plastered and sometimes painted.
To lighten up the small dark rooms, tenants able to afford a degree of painted colourful murals on the walls. Examples have been found of jungle scenes with wild animals and exotic plants. Imitation windows (trompe l'oeil) were sometimes painted to make the rooms seem less confined.
Ancient Rome had elaborate and luxurious houses owned by the elite. The average house, or in cities apartment, of a commoner or plebe did not contain many luxuries. The domus, or single-family residence, was only for the well-off in Rome, with most having a layout of the closed unit, consisting of one or two rooms. Between 312 and 315 A.D. Rome had 1781 domus and 44,850 of insulae.
Insulae have been the subject of great debate for historians of Roman culture, defining the various meanings of the word. Insula was a word used to describe apartment buildings, or the apartments themselves, meaning apartment, or inhabitable room, demonstrating just how small apartments for Plebes were. Urban divisions were originally street blocks, and later began to divide into smaller divisions, the word insula referring to both blocks and smaller divisions. The insula contained cenacula, tabernae, storage rooms under the stairs, and lower floor shops. Another type of housing unit for Plebes was a cenaculum, an apartment, divided into three individual rooms: cubiculum, exedra, and medianum. Common Roman apartments were mainly masses of smaller and larger structures, many with narrow balconies that present mysteries as to their use, having no doors to access them, and they lacked the excessive decoration and display of wealth that aristocrats’ houses contained. Luxury in houses was not common, as the life of the average person did not consist of being in their houses, as they instead would go to public baths, and engage in other communal activities.
Lighthouses.
Many lighthouses were built around the Mediterranean and the coasts of the empire, including the Tower of Hercules at A Coruña in northern Spain, a structure which survives to this day. A smaller lighthouse at Dover, England also exists as a ruin about half the height of the original. The light would have been provided by a fire at the top of the structure.
Thermae.
All Roman cities had at least one Thermae, a popular facility for public bathing, exercising and socializing. Exercise might include wrestling and weight-lifting, as well as swimming. Bathing was an important part of the Roman day, where some hours might be spent, at a very low cost subsidized by the government. Wealthier Romans were often accompanied by one or more slaves, who performed any required tasks such as fetching refreshment, guarding valuables, providing towels, and at the end of the session, applying olive oil to their masters' bodies which was then scraped off with a strigil, a scraper made of wood or bone. Romans did not wash with soap and water as we do now.
Roman bath-houses were also provided for private villas, town houses and forts. They were normally supplied with water from an adjacent river or stream, or by aqueduct. The design of thermae is discussed by Vitruvius in De Architectura.
Temples.
Roman temples were among the most important and richest buildings in Roman culture, though only a few survive in any sort of complete state. Their construction and maintenance was a major part of ancient Roman religion, and all towns of any importance had at least one main temple, as well as smaller shrines. The main room "(cella)" housed the cult image of the deity to whom the temple was dedicated, and often a small altar for incense or libations. Behind the cella was a room or rooms used by temple attendants for storage of equipment and offerings.
Some remains of many Roman temples survive, above all in Rome itself, but the relatively few near-complete examples were nearly all converted to Christian churches (and sometimes subsequently to mosques), usually a considerable time after the initial triumph of Christianity under Constantine. The decline of Roman religion was relatively slow, and the temples themselves were not appropriated by the government until a decree of the Emperor Honorius in 415.
The form of the Roman temple was mainly derived from the Etruscan model, but using Greek styles. Roman temples emphasised the front of the building, which followed Greek temple models and typically consisted of wide steps leading to a portico with columns, a pronaos, and usually a triangular pediment above, which was filled with statuary in the most grand examples; this was as often in terracotta as stone, and no examples have survived except as fragments. However, unlike the Greek models, which generally gave equal treatment to all sides of the temple, which could be viewed and approached from all directions, the sides and rear of Roman temples might be largely undecorated (as in the Pantheon, Rome and Vic), inaccessible by steps (as in the Maison Carrée and Vic), and even back on to other buildings. As in the Maison Carrée, columns at the side might be half-columns, emerging from ("engaged with" in architectural terminology) the wall. The platform on which the temple sat was typically raised higher in Roman examples than Greek, with up ten or twelve or more steps rather than the three typical in Greek temples; the Temple of Claudius was raised twenty steps. These steps were normally only at the front, and typically not the whole width of that.
The Greek classical orders in all their details were closely followed in the façades of temples, as in other prestigious buildings. However the idealized proportions between the different elements set out by the only significant Roman writer on architecture to survive, Vitruvius, and subsequent Italian Renaissance writers, do not reflect actual Roman practice, which could be very variable, though always aiming at balance and harmony. Following a Hellenistic trend, the Corinthian order and its variant the Composite order were most common in surviving Roman temples, but for small temples like that at Alcántara, a simple Tuscan order could be used.
There was considerable local variation in style, as Roman architects often tried to incorporate elements the population expected in its sacred architecture. This was especially the case in Egypt and the Near East, where different traditions of large stone temples were already millennia old. The Romano-Celtic temple was a simple style for small temples found in the Western Empire, and by far the most common type in Roman Britain. It often lacked any of the distinctive classical features, and may have had considerable continuity with pre-Roman temples of the Celtic religion.
Theatres.
Roman theatres were built in all areas of the empire from Spain, to the Middle East. Because of the Romans' ability to influence local architecture, we see numerous theatres around the world with uniquely Roman attributes.
These buildings were semi-circular and possessed certain inherent architectural structures, with minor differences depending on the region in which they were constructed. The "scaenae frons" was a high back wall of the stage floor, supported by columns. The "proscaenium" was a wall that supported the front edge of the stage with ornately decorated niches off to the sides. The Hellenistic influence is seen through the use of the "proscaenium". The Roman theatre also had a "podium", which sometimes supported the columns of the "scaenae frons". The "scaenae" was originally not part of the building itself, constructed only to provide sufficient background for the actors. Eventually, it became a part of the edifice itself, made out of concrete. The theatre itself was divided into the stage (orchestra) and the seating section (auditorium). "Vomitoria" or entrances and exits were made available to the audience.
Villa.
A Roman villa was a country house built for the upper class, while a domus was a wealthy family's house in a town. The Empire contained many kinds of villas, not all of them lavishly appointed with mosaic floors and frescoes. In the provinces, any country house with some decorative features in the Roman style may be called a "villa" by modern scholars. Some were pleasure palaces such as those— like Hadrian's Villa at Tivoli— that were situated in the cool hills within easy reach of Rome or— like the Villa of the Papyri at Herculaneum— on picturesque sites overlooking the Bay of Naples. Some villas were more like the country houses of England or Poland, the visible seat of power of a local magnate, such as the famous palace rediscovered at Fishbourne in Sussex.
Suburban villas on the edge of cities were also known, such as the Middle and Late Republican villas that encroached on the Campus Martius, at that time on the edge of Rome, and which can be also seen outside the city walls of Pompeii, including the Villa of the Mysteries, famous for its frescos. These early suburban villas, such as the one at Rome's Auditorium site or at Grottarossa in Rome, demonstrate the antiquity and heritage of the "villa suburbana" in Central Italy. It is possible that these early, suburban villas were also in fact the seats of power (maybe even palaces) of regional strongmen or heads of important families ("gentes").
A third type of villa provided the organizational center of the large farming estates called latifundia; such villas might be lacking in luxuries. By the 4th century, "villa" could simply mean an agricultural estate or holding: Jerome translated the Gospel of Mark (xiv, 32) "chorion", describing the olive grove of Gethsemane, with "villa", without an inference that there were any dwellings there at all ("Catholic Encyclopedia" "Gethsemane").
With the colossal Diocletian's Palace, built in the countryside but later turned into a fortified city, a form of residential castle emerges, that anticipates the Middle Ages.
Watermills.
The initial invention of the watermill appears to have occurred in the hellenized eastern Mediterranean in the wake of the conquests of Alexander the Great and the rise of Hellenistic science and technology. In the subsequent Roman era, the use of water-power was diversified and different types of watermills were introduced. These include all three variants of the vertical water wheel as well as the horizontal water wheel. Apart from its main use in grinding flour, water-power was also applied to pounding grain, crushing ore, sawing stones and possibly fulling and bellows for iron furnaces.
Decorative structures.
Monoliths.
In architecture, a monolith is a structure which has been excavated as a unit from a surrounding matrix or outcropping of rock. Monoliths are found in all types of Roman buildings. They were either: quarried without being moved; or quarried and moved; or quarried, moved and lifted clear off the ground into their position (e.g. architraves); or quarried, moved and erected in an upright position (e.g. columns).
Transporting was done by land or water (or a combination of both), in the later case often by special-built ships such as obelisk carriers. For lifting operations, ancient cranes were employed since ca. 515 BC, such as in the construction of Trajan's Column.
Obelisks.
An obelisk is a tall, four-sided, narrow tapering monument which ends in a pyramid-like shape at the top. These were originally called "tekhenu" by the builders, the Ancient Egyptians. The Greeks who saw them used the Greek 'obeliskos' to describe them, and this word passed into Latin and then English. The Romans commissioned obelisks in an ancient Egyptian style. Examples include:
Roman gardens.
Roman gardens were influenced by Egyptian, Persian, and Greek gardening techniques. In Ancient Latium, a garden was part of every farm. According to Cato the Elder, every garden should be close to the house and should have flower beds and ornamental trees. Horace wrote that during his time flower gardens became a national indulgence.
Gardens were not reserved for the extremely wealthy. Excavations in Pompeii show that gardens attaching to residences were scaled down to meet the space constraints of the home of the average Roman. Modified versions of Roman garden designs were adopted in Roman settlements in Africa, Gaul, and Britannia. As town houses were replaced by tall "insula" (apartment buildings), these urban gardens were replaced by window boxes or roof gardens. 
Triumphal arch.
A triumphal arch is a monumental structure in the shape of an archway with one or more arched passageways, often designed to span a road. The origins of the Roman triumphal arch are unclear. There were precursors to the triumphal arch within the Roman world; in Italy, the Etruscans used elaborately decorated single bay arches as gates or portals to their cities. Surviving examples of Etruscan arches can still be seen at Perugia and Volterra. The two key elements of the triumphal arch – a round-topped arch and a square entablature – had long been in use as separate architectural elements in ancient Greece.
The innovation of the Romans was to these elements in a single free-standing structure. The columns became purely decorative elements on the outer face of arch, while the entablature, liberated from its role as a building support, became the frame for the civic and religious messages that the arch builders wished to convey. Little is known about how the Romans viewed triumphal arches. Pliny the Elder, writing in the first century AD, was the only ancient author to discuss them. He wrote that they were intended to "elevate above the ordinary world" an image of an honoured person usually depicted in the form of a statue with a quadriga.
The first recorded Roman triumphal arches were set up in the time of the Roman Republic. Generals who were granted a triumph were termed "triumphators" and would erect "fornices" or honorific arches bearing statues to commemorate their victories.
Roman triumphal practices changed significantly at the start of the imperial period when the first Roman Emperor Augustus decreed that only emperors would be granted triumphs. The triumphal arch changed from being a personal monument to being an essentially propagandistic one, serving to announce and promote the presence of the ruler and the laws of the state. Arches were not necessarily built as entrances, but – unlike many modern triumphal arches – they were often erected across roads and were intended to be passed through, not round.
Most Roman triumphal arches were built during the imperial period. By the fourth century AD there were 36 such arches in Rome, of which three have survived – the Arch of Titus (AD 81), the Arch of Septimius Severus (203-205) and the Arch of Constantine (312). Numerous arches were built elsewhere in the Roman Empire. The single arch was the most common, but many triple arches were also built, of which the Triumphal Arch of Orange ("circa" AD 21) is the earliest surviving example. From the 2nd century AD, many examples of the "arcus quadrifrons" – a square triumphal arch erected over a crossroads, with arched openings on all four sides – were built, especially in North Africa. Arch-building in Rome and Italy diminished after the time of Trajan (AD 98-117) but remained widespread in the provinces during the 2nd and 3rd centuries AD; they were often erected to commemorate imperial visits.
The ornamentation of an arch was intended to serve as a constant visual reminder of the triumph and "triumphator". The façade was ornamented with marble columns, and the piers and attics with decorative cornices. Sculpted panels depicted victories and achievements, the deeds of the "triumphator", the captured weapons of the enemy or the triumphal procession itself. The spandrels usually depicted flying Victories, while the attic was often inscribed with a dedicatory inscription naming and praising the "triumphator". The piers and internal passageways were also decorated with reliefs and free-standing sculptures. The vault was ornamented with coffers. Some triumphal arches were surmounted by a statue or a "currus triumphalis", a group of statues depicting the emperor or general in a quadriga.
Inscriptions on Roman triumphal arches were works of art in themselves, with very finely cut, sometimes gilded letters. The form of each letter and the spacing between them was carefully designed for maximum clarity and simplicity, without any decorative flourishes, emphasizing the Roman taste for restraint and order. This conception of what later became the art of typography remains of fundamental importance down to the present day.
Infrastructure.
Roads.
Roman roads were vital to the maintenance and development of the Roman state, and were built from about 500 BC through the expansion and consolidation of the Roman Republic and the Roman Empire. They provided efficient means for the overland movement of armies, officials and civilians, and the inland carriage of official communications and trade goods. At the peak of Rome's development, no fewer than 29 great military highways radiated from the capital, and the Late Empire's 113 provinces were interconnected by 372 great road links.
Roman road builders aimed at a regulation width (see Laws and standards above), but actual widths have been measured at between 3.6 ft (1.1 m) and more than . Today, the concrete has worn from the spaces around the stones, giving the impression of a very bumpy road, but the original practice was to produce a surface that was no doubt much closer to being flat.
Aqueduct.
The Romans constructed numerous aqueducts in order to bring water from distant sources into their cities and towns, supplying public baths, latrines, fountains and private households. Waste water was removed by complex sewage systems and released into nearby bodies of water, keeping the towns clean and free from effluent. Aqueducts also provided water for mining operations, milling, farms and gardens.
Aqueducts moved water through gravity alone, being constructed along a slight downward gradient within conduits of stone, brick or concrete. Most were buried beneath the ground, and followed its contours; obstructing peaks were circumvented or, less often, tunnelled through. Where valleys or lowlands intervened, the conduit was carried on bridgework, or its contents fed into high-pressure lead, ceramic or stone pipes and siphoned across. Most aqueduct systems included sedimentation tanks, sluices and distribution tanks to regulate the supply at need.
Rome's first aqueduct supplied a water-fountain sited at the city's cattle market. By the third century AD, the city had eleven aqueducts, sustaining a population of over a million in a water-extravagant economy; most of the water supplied the city's many public baths. Cities and municipalities throughout the Roman Empire emulated this model, and funded aqueducts as objects of public interest and civic pride, "an expensive yet necessary luxury to which all could, and did, aspire."
Most Roman aqueducts proved reliable, and durable; some were maintained into the early modern era, and a few are still partly in use. Methods of aqueduct surveying and construction are noted by Vitruvius in his work "De Architectura" (1st century BC). The general Frontinus gives more detail in his official report on the problems, uses and abuses of Imperial Rome's public water supply. Notable examples of aqueduct architecture include the supporting piers of the Aqueduct of Segovia, and the aqueduct-fed cisterns of Constantinople.
Bridges.
Roman bridges, built by ancient Romans, were the first large and lasting bridges built. Roman bridges were built with stone and had the arch as the basic structure (see arch bridge). Most utilized concrete as well, which the Romans were the first to use for bridges.
Roman arch bridges were usually semicircular, although a few were segmental (such as Alconétar Bridge). A segmental arch is an arch that is less than a semicircle. The advantages of the segmental arch bridge were that it allowed great amounts of flood water to pass under it, which would prevent the bridge from being swept away during floods and the bridge itself could be more lightweight. Generally, Roman bridges featured wedge-shaped primary arch stones (voussoirs) of the same in size and shape. The Romans built both single spans and lengthy multiple arch aqueducts, such as the Pont du Gard and Segovia Aqueduct. Their bridges featured from an early time onwards flood openings in the piers, e.g. in the Pons Fabricius in Rome (62 BC), one of the world's oldest major bridges still standing.
Roman engineers were the first and until the industrial revolution the only ones to construct bridges with concrete, which they called Opus caementicium. The outside was usually covered with brick or ashlar, as in the Alcántara bridge.
The Romans also introduced segmental arch bridges into bridge construction. The 330 m long Limyra Bridge in southwestern Turkey features 26 segmental arches with an average span-to-rise ratio of 5.3:1, giving the bridge an unusually flat profile unsurpassed for more than a millennium. Trajan's bridge over the Danube featured open-spandrel segmental arches made of wood (standing on 40 m high concrete piers). This was to be the longest arch bridge for a thousand years both in terms of overall and individual span length, while the longest extant Roman bridge is the 790 m long Puente Romano at Mérida.
Canals.
Roman canals were typically multi-purpose structures, intended for irrigation, drainage, land reclamation, flood control and navigation where feasible. Some navigational canals were recorded by ancient geographers and are still traceable by modern archaeology. Channels which served the needs of urban water supply are covered at the List of aqueducts in the Roman Empire.
Cisterns.
Freshwater reservoirs were commonly set up at the termini of aqueducts and their branch lines, supplying urban households, agricultural estates, imperial palaces, thermae or naval bases of the Roman navy.
Dams.
Roman dam construction began in earnest in the early imperial period. For the most part, it concentrated on the semi-arid fringe of the empire, namely the provinces of North Africa, the Near East, and Hispania. The relative abundance of Spanish dams below is due partly to more intensive field work there; for Italy only the Subiaco Dams, created by emperor Nero (54–68 AD) for recreational purposes, are attested. These dams are noteworthy, though, for their extraordinary height, which remained unsurpassed anywhere in the world until the Late Middle Ages.
The most frequent dam types were earth- or rock-filled embankment dams and masonry gravity dams. These served a wide array of purposes, such as irrigation, flood control, river diversion, soil-retention, or a combination of these functions. The impermeability of Roman dams was increased by the introduction of waterproof hydraulic mortar and especially "opus caementicium" in the Concrete Revolution. These materials also allowed for bigger structures to be built, like the Lake Homs Dam, possibly the largest water barrier today, and the sturdy Harbaqa Dam, both of which consist of a concrete core.
Roman builders were the first to realize the stabilizing effect of arches and buttresses, which they integrated into their dam designs. Previously unknown dam types introduced by the Romans include arch-gravity dams, arch dams,; buttress dams, and multiple-arch buttress dams.
Defensive walls.
The Romans generally fortified cities, rather than fortresses, but there are some fortified camps, such as the Saxon Shore forts like Porchester Castle in England. City walls were already significant in Etruscan architecture, and in the struggle for control of Italy under the early Republic many more were built, using different techniques. These included tightly-fitting massive irregular polygonal blocks, shaped to fit exactly in a way reminiscent of later Inca work. The Romans called a simple rampart wall an agger; at this date great height was not necessary. The Servian Wall around Rome was an ambitious project of the early 4th century BC. The wall was up to 10 metres (32.8 ft) in height in places, 3.6 metres (12 ft) wide at its base, 11 km (7 mi) long, and is believed to have had 16 main gates, though many of these are mentioned only from writings, with no other known remains. Some of it had a "fossa" or ditch in front, and an agger behind, and it was enough to deter Hannibal. Later the Aurelian Wall replaced it, enclosing an expanded city, and using more sophisticated designs, with small forts at intervals.
The Romans walled major cities and towns in areas they saw as vulnerable, and parts of many walls remain incorporated in later defences, as at Córdoba (2nd century BC), Chester (earth and wood in the 70s AD, stone from c. 100), and York (from 70s AD). Strategic walls across open country were far rarer, and Hadrian's Wall (from 122) and the Antonine Wall (from 142, abandoned only 8 years after completion) are the most significant examples, both on the Pictish frontier.
Architectural features.
Mosaics.
On his return from campaigns in Greece, the general Sulla brought back what is probably the most well-known element of the early imperial period, the mosaic, a decoration made of colourful chips of stone inserted into cement. This tiling method took the empire by storm in the late first century and the second century and in the Roman home joined the well known mural in decorating floors, walls, and grottoes with geometric and pictorial designs.
There were two main techniques in Greco-Roman mosaic: "opus vermiculatum" used tiny "tesserae", typically cubes of 4 millimeters or less, and was produced in workshops in relatively small panels which were transported to the site glued to some temporary support. The tiny "tesserae" allowed very fine detail, and an approach to the illusionism of painting. Often small panels called "emblemata" were inserted into walls or as the highlights of larger floor-mosaics in coarser work. The normal technique, however, was "opus tessellatum", using larger tesserae, which were laid on site. There was a distinct native Italian style using black on a white background, which was no doubt cheaper than fully coloured work.
A specific genre of Roman mosaic obtained the name "asaroton" (Greek "unswept floor"). It represented an optical illusion of the leftovers from a feast on the floor of reach houses.
Hypocaust.
A hypocaust was an ancient Roman system of underfloor heating, used to heat houses with hot air. The Roman architect Vitruvius, writing about the end of the 1st century B.C., attributes their invention to Sergius Orata. Many remains of Roman hypocausts have survived throughout Europe, western Asia, and northern Africa. The hypocaust was an invention which improved the hygiene and living conditions of citizens, and was a forerunner of modern central heating.
Hypocausts were used for heating hot baths (thermae), houses and other buildings, whether public or private. The floor was raised above the ground by pillars, called pilae stacks, with a layer of tiles, then a layer of concrete, then another of tiles on top; and spaces were left inside the walls so that hot air and smoke from the furnace would pass through these enclosed areas and out of flues in the roof, thereby heating but not polluting the interior of the room.
Roman roofs.
In Sicily truss roofs presumably appeared as early as 550 BC.Their potential was fully realized in the Roman period, which saw trussed roofs over 30 wide spanning the rectangular spaces of monumental public buildings such as temples, basilicas, and later churches. Such spans were three times as wide as the widest prop-and-lintel roofs and only surpassed by the largest Roman domes.
The largest truss roof by span of Ancient Rome covered the Aula Regia (throne room) built for emperor Domitian (81–96 AD) on the Palatine Hill, Rome. The timber truss roof had a width of 31.67 m, slightly surpassing the postulated limit of 30 m for Roman roof constructions. Tie-beam trusses allowed for much larger spans than the older prop-and-lintel system and even concrete vaulting. Nine out of the ten largest rectangular spaces in Roman architecture were bridged this way, the only exception being the groin vaulted Basilica of Maxentius.
Spiral stairs.
The spiral stair is a type of stairway which, due to its complex helical structure, was introduced relatively late into architecture. Although the oldest example dates back to the 5th century BC, it was only in the wake of the influential design of Trajan's Column that this space-saving new type permanently caught hold in Roman architecture.
Apart from the triumphal columns in the imperial cities of Rome and Constantinople, other types of buildings such as temples, thermae, basilicas and tombs were also fitted with spiral stairways. Their notable absence in the towers of the Aurelian Wall indicates that although used in medieval castles, they did not yet figure prominently in Roman military engineering. By late antiquity, separate stair towers were constructed adjacent to the main buildings, as in the Basilica of San Vitale.
The construction of spiral stairs passed on both to Christian and Islamic architecture.

</doc>
<doc id="52686" url="https://en.wikipedia.org/wiki?curid=52686" title="Romanesque architecture">
Romanesque architecture

Romanesque architecture is an architectural style of medieval Europe characterized by semi-circular arches. There is no consensus for the beginning date of the Romanesque style, with proposals ranging from the 6th to the late 10th century, this later date being the most commonly held. It developed in the 12th century into the Gothic style, marked by pointed arches. Examples of Romanesque architecture can be found across the continent, making it the first pan-European architectural style since Imperial Roman Architecture. The Romanesque style in England is traditionally referred to as Norman architecture.
Combining features of ancient Roman and Byzantine buildings and other local traditions, Romanesque architecture is known by its massive quality, thick walls, round arches, sturdy pillars, groin vaults, large towers and decorative arcading. Each building has clearly defined forms, frequently of very regular, symmetrical plan; the overall appearance is one of simplicity when compared with the Gothic buildings that were to follow. The style can be identified right across Europe, despite regional characteristics and different materials.
Many castles were built during this period, but they are greatly outnumbered by churches. The most significant are the great abbey churches, many of which are still standing, more or less complete and frequently in use. The enormous quantity of churches built in the Romanesque period was succeeded by the still busier period of Gothic architecture, which partly or entirely rebuilt most Romanesque churches in prosperous areas like England and Portugal. The largest groups of Romanesque survivors are in areas that were less prosperous in subsequent periods, including parts of southern France, northern Spain and rural Italy. Survivals of unfortified Romanesque secular houses and palaces, and the domestic quarters of monasteries are far rarer, but these used and adapted the features found in church buildings, on a domestic scale.
Definition.
According to the "Oxford English Dictionary", the word "Romanesque" means "descended from Roman" and was first used in English to designate what are now called Romance languages (first cited 1715). The French term """" was first used in the architectural sense by archaeologist Charles de Gerville in a letter of 18 December 1818 to Auguste Le Prévost to describe what Gerville sees as a "debased Roman architecture". In 1824 Gerville's friend Arcisse de Caumont adopted the label "" to describe the "degraded" European architecture from the 5th to the 13th centuries, in his "Essai sur l'architecture religieuse du moyen-âge, particulièrement en Normandie", at a time when the actual dates of many of the buildings so described had not been ascertained:
The first use in a published work is in William Gunn's "An Inquiry into the Origin and Influence of Gothic Architecture" (London 1819). The word was used by Gunn to describe the style that was identifiably Medieval and prefigured the Gothic, yet maintained the rounded Roman arch and thus appeared to be a continuation of the Roman tradition of building.
The term is now used for the more restricted period from the late 10th to 12th centuries. The term "Pre-romanesque" is sometimes applied to architecture in Germany of the Carolingian and Ottonian periods and Visigothic, Mozarab and Asturian constructions between the 8th and the 10th centuries in the Iberian Peninsula while "First Romanesque" is applied to buildings in north of Italy and Spain and parts of France that have Romanesque features but pre-date the influence of the Abbey of Cluny.
Scope.
Buildings of every type were constructed in the Romanesque style, with evidence remaining of simple domestic buildings, elegant town houses, grand palaces, commercial premises, civic buildings, castles, city walls, bridges, village churches, abbey churches, abbey complexes and large cathedrals. Of these types of buildings, domestic and commercial buildings are the most rare, with only a handful of survivors in the United Kingdom, several clusters in France, isolated buildings across Europe and by far the largest number, often unidentified and altered over the centuries, in Italy. Many castles exist, the foundations of which date from the Romanesque period. Most have been substantially altered, and many are in ruins.
By far the greatest number of surviving Romanesque buildings are churches. These range from tiny chapels to large cathedrals, and although many have been extended and altered in different styles, a large number remain either substantially intact or sympathetically restored, demonstrating the form, character and decoration of Romanesque church architecture.
History.
Origins.
Romanesque architecture was the first distinctive style to spread across Europe since the Roman Empire. With the decline of Rome, Roman building methods survived to an extent in Western Europe, where successive Merovingian, Carolingian and Ottonian architects continued to build large stone buildings such as monastery churches and palaces. In the more northern countries Roman building styles and techniques had never been adopted except for official buildings, while in Scandinavia they were unknown. Although the round arch continued in use, the engineering skills required to vault large spaces and build large domes were lost. There was a loss of stylistic continuity, particularly apparent in the decline of the formal vocabulary of the Classical Orders. In Rome several great Constantinian basilicas continued in use as an inspiration to later builders. Some traditions of Roman architecture also survived in Byzantine architecture with the 6th-century octagonal Byzantine Basilica of San Vitale in Ravenna being the inspiration for the greatest building of the Dark Ages in Europe, the Emperor Charlemagne's Palatine Chapel, Aachen, Germany, built around the year AD 800.
Dating shortly after the Palatine Chapel is a remarkable 9th-century Swiss manuscript known as the Plan of Saint Gall and showing a very detailed plan of a monastic complex, with all its various monastic buildings and their functions labelled. The largest building is the church, the plan of which is distinctly Germanic, having an apse at both ends, an arrangement not generally seen elsewhere. Another feature of the church is its regular proportion, the square plan of the crossing tower providing a module for the rest of the plan. These features can both be seen at the Proto-Romanesque St. Michael's Church, Hildesheim, 1001–1030.
Architecture of a Romanesque style also developed simultaneously in the north of Italy, parts of France and in the Iberian Peninsula in the 10th century and prior to the later influence of the Abbey of Cluny. The style, sometimes called First Romanesque or Lombard Romanesque, is characterised by thick walls, lack of sculpture and the presence of rhythmic ornamental arches known as a Lombard band.
Politics.
Charlemagne was crowned by the Pope in Old St. Peter's Basilica on Christmas Day in the year 800, with an aim to re-establishing the old Roman Empire. Charlemagne's political successors continued to rule much of Europe, with a gradual emergence of the separate political states that were eventually to become welded into nations, either by allegiance or defeat, the Kingdom of Germany giving rise to the Holy Roman Empire. The invasion of England by William, Duke of Normandy, in 1066, saw the building of both castles and churches that reinforced the Norman presence. Several significant churches that were built at this time were founded by rulers as seats of temporal and religious power, or places of coronation and burial. These include the Abbaye-Saint-Denis, Speyer Cathedral and Westminster Abbey (where little of the Norman church now remains).
At a time when the remaining architectural structures of the Roman Empire were falling into decay and much of its learning and technology lost, the building of masonry domes and the carving of decorative architectural details continued unabated, though greatly evolved in style since the fall of Rome, in the enduring Byzantine Empire. The domed churches of Constantinople and Eastern Europe were to greatly affect the architecture of certain towns, particularly through trade and through the Crusades. The most notable single building that demonstrates this is St Mark's Basilica, Venice, but there are many lesser-known examples, particularly in France, such as the church of Saint-Front, Périgueux and Angoulême Cathedral.
Much of Europe was affected by feudalism in which peasants held tenure from local rulers over the land that they farmed in exchange for military service. The result of this was that they could be called upon, not only for local and regional spats, but to follow their lord to travel across Europe to the Crusades, if they were required to do so. The Crusades, 1095–1270, brought about a very large movement of people and, with them, ideas and trade skills, particularly those involved in the building of fortifications and the metal working needed for the provision of arms, which was also applied to the fitting and decoration of buildings. The continual movement of people, rulers, nobles, bishops, abbots, craftsmen and peasants, was an important factor in creating a homogeneity in building methods and a recognizable "Romanesque style", despite regional differences.
Life became generally less secure after the Carolingian period. This resulted in the building of castles at strategic points, many of them being constructed as strongholds of the Normans, descendants of the Vikings who invaded northern France under Rollo in 911. Political struggles also resulted in the fortification of many towns, or the rebuilding and strengthening of walls that remained from the Roman period. One of the most notable surviving fortifications is that of the city of Carcassonne. The enclosure of towns brought about a lack of living space within the walls, and resulted in a style of town house that was tall and narrow, often surrounding communal courtyards, as at San Gimignano in Tuscany.
In Germany, the Holy Roman Emperors built a number of residences, fortified, but essentially palaces rather than castles, at strategic points and on trade routes. The Imperial Palace of Goslar (heavily restored in the 19th century) was built in the early 11th century by Otto III and Henry III, while the ruined Palace at Gelnhausen was received by Frederick Barbarossa prior to 1170. The movement of people and armies also brought about the building of bridges, some of which have survived, including the 12th-century bridge at Besalú, Catalonia, the 11th-century Puente de la Reina, Navarre and the Pont-Saint-Bénézet, Avignon.
Religion.
Across Europe, the late 11th and 12th centuries saw an unprecedented growth in the number of churches. A great number of these buildings, both large and small, remain, some almost intact and in others altered almost beyond recognition in later centuries. They include many very well known churches such as Santa Maria in Cosmedin in Rome, the Baptistery in Florence and San Zeno Maggiore in Verona. In France, the famous abbeys of Aux Dames and Les Hommes at Caen and Mont Saint-Michel date from this period, as well as the abbeys of the pilgrimage route to Santiago de Compostela. Many cathedrals owe their foundation to this date, with others beginning as abbey churches, and later becoming cathedrals. In England, of the cathedrals of ancient foundation, all were begun in this period with the exception of Salisbury, where the monks relocated from the Norman church at Old Sarum, and several, such as Canterbury, which were rebuilt on the site of Saxon churches. In Spain, the most famous church of the period is Santiago de Compostela. In Germany, the Rhine and its tributaries were the location of many Romanesque abbeys, notably Mainz, Worms, Speyer and Bamberg. In Cologne, then the largest city north of the Alps, a very important group of large city churches survives largely intact. As monasticism spread across Europe, Romanesque churches sprang up in Scotland, Scandinavia, Poland, Hungary, Sicily, Serbia and Tunisia. Several important Romanesque churches were built in the Crusader kingdoms.
Monasticism.
The system of monasticism in which the religious become members of an order, with common ties and a common rule, living in a mutually dependent community, rather than as a group of hermits living in proximity but essentially separate, was established by the monk Benedict in the 6th century. The Benedictine monasteries spread from Italy throughout Europe, being always by far the most numerous in England. They were followed by the Cluniac order, the Cistercians, Carthusians and Augustinian Canons. During the Crusades, the military orders of the Knights Hospitaller and the Knights Templar were founded.
The monasteries, which sometimes also functioned as cathedrals, and the cathedrals that had bodies of secular clergy often living in community, were a major source of power in Europe. Bishops and the abbots of important monasteries lived and functioned like princes. The monasteries were the major seats of learning of all sorts. Benedict had ordered that all the arts were to be taught and practiced in the monasteries. Within the monasteries books were transcribed by hand, and few people outside the monasteries could read or write.
In France, Burgundy was the centre of monasticism. The enormous and powerful monastery at Cluny was to have lasting effect on the layout of other monasteries and the design of their churches. Unfortunately, very little of the abbey church at Cluny remains; the "Cluny II" rebuilding of 963 onwards has completely vanished, but we have a good idea of the design of "Cluny III" from 1088 to 1130, which until the Renaissance remained the largest building in Europe. However, the church of St. Sernin at Toulouse, 1080–1120, has remained intact and demonstrates the regularity of Romanesque design with its modular form, its massive appearance and the repetition of the simple arched window motif.
Pilgrimage and Crusade.
One of the effects of the Crusades, which were intended to wrest the Holy Places of Palestine from Islamic control, was to excite a great deal of religious fervour, which in turn inspired great building programs. The Nobility of Europe, upon safe return, thanked God by the building of a new church or the enhancement of an old one. Likewise, those who did not return from the Crusades could be suitably commemorated by their family in a work of stone and mortar.
The Crusades resulted in the transfer of, among other things, a great number of Holy Relics of saints and apostles. Many churches, like Saint-Front, Périgueux, had their own home grown saint while others, most notably Santiago de Compostela, claimed the remains and the patronage of a powerful saint, in this case one of the Twelve Apostles. Santiago de Compostela, located near Galicia (present day Spain) became one of the most important pilgrimage destinations in Europe. Most of the pilgrims travelled the Way of St. James on foot, many of them barefooted as a sign of penance. They moved along one of the four main routes that passed through France, congregating for the journey at Jumièges, Paris, Vézelay, Cluny, Arles and St. Gall in Switzerland. They crossed two passes in the Pyrenees and converged into a single stream to traverse north-western Spain. Along the route they were urged on by those pilgrims returning from the journey. On each of the routes abbeys such as those at Moissac, Toulouse, Roncesvalles, Conques, Limoges and Burgos catered for the flow of people and grew wealthy from the passing trade. Saint-Benoît-du-Sault, in the Berry province, is typical of the churches that were founded on the pilgrim route.
Characteristics.
The general impression given by Romanesque architecture, in both ecclesiastical and secular buildings, is one of massive solidity and strength. In contrast with both the preceding Roman and later Gothic architecture, in which the load-bearing structural members are, or appear to be, columns, pilasters and arches, Romanesque architecture, in common with Byzantine architecture, relies upon its walls, or sections of walls called piers.
Romanesque architecture is often divided into two periods known as the "First Romanesque" style and the "Romanesque" style. The difference is chiefly a matter of the expertise with which the buildings were constructed. The First Romanesque employed rubble walls, smaller windows and unvaulted roofs. A greater refinement marks the Second Romanesque, along with increased use of the vault and dressed stone.
Walls.
The walls of Romanesque buildings are often of massive thickness with few and comparatively small openings. They are often double shells, filled with rubble.
The building material differs greatly across Europe, depending upon the local stone and building traditions. In Italy, Poland, much of Germany and parts of the Netherlands, brick is generally used. Other areas saw extensive use of limestone, granite and flint. The building stone was often used in comparatively small and irregular pieces, bedded in thick mortar. Smooth ashlar masonry was not a distinguishing feature of the style, particularly in the earlier part of the period, but occurred chiefly where easily worked limestone was available.
Buttresses.
Because of the massive nature of Romanesque walls, buttresses are not a highly significant feature, as they are in Gothic architecture. Romanesque buttresses are generally of flat square profile and do not project a great deal beyond the wall. In the case of aisled churches, barrel vaults, or half-barrel vaults over the aisles helped to buttress the nave, if it was vaulted.
In the cases where half-barrel vaults were used, they effectively became like flying buttresses. Often aisles extended through two storeys, rather than the one usual in Gothic architecture, so as to better support the weight of a vaulted nave. In the case of Durham Cathedral, flying buttresses have been employed, but are hidden inside the triforium gallery.
Arches and openings.
The arches used in Romanesque architecture are nearly always semicircular, for openings such as doors and windows, for vaults and for arcades. Wide doorways are usually surmounted by a semi-circular arch, except where a door with a lintel is set into a large arched recess and surmounted by a semi-circular "lunette" with decorative carving. These doors sometimes have a carved central jamb.
Narrow doors and small windows might be surmounted by a solid stone lintel. Larger openings are nearly always arched. A characteristic feature of Romanesque architecture, both ecclesiastic and domestic, is the pairing of two arched windows or arcade openings, separated by a pillar or colonette and often set within a larger arch. Ocular windows are common in Italy, particularly in the facade gable and are also seen in Germany. Later Romanesque churches may have wheel windows or rose windows with plate tracery.
There are a very small number of buildings in the Romanesque style, such as Autun Cathedral in France and Monreale Cathedral in Sicily in which pointed arches have been used extensively, apparently for stylistic reasons. It is believed that in these cases there is a direct imitation of Islamic architecture. At other late Romanesque churches such as Durham Cathedral, and Cefalù Cathedral, the pointed arch was introduced as a structural device in ribbed vaulting. Its increasing application was fundamental to the development of Gothic architecture.
Arcades.
An arcade is a row of arches, supported on piers or columns. They occur in the interior of large churches, separating the nave from the aisles, and in large secular interiors spaces, such as the great hall of a castle, supporting the timbers of a roof or upper floor. Arcades also occur in cloisters and atriums, enclosing an open space.
Arcades can occur in storeys or stages. While the arcade of a cloister is typically of a single stage, the arcade that divides the nave and aisles in a church is typically of two stages, with a third stage of window openings known as the clerestory rising above them. Arcading on a large scale generally fulfils a structural purpose, but it is also used, generally on a smaller scale, as a decorative feature, both internally and externally where it is frequently "blind arcading" with only a wall or a narrow passage behind it.
Piers.
In Romanesque architecture, piers were often employed to support arches. They were built of masonry and square or rectangular in section, generally having a horizontal moulding representing a capital at the springing of the arch. Sometimes piers have vertical shafts attached to them, and may also have horizontal mouldings at the level of the base.
Although basically rectangular, piers can often be of highly complex form, with half-segments of large hollow-core columns on the inner surface supporting the arch, or a clustered group of smaller shafts leading into the mouldings of the arch.
Piers that occur at the intersection of two large arches, such as those under the crossing of the nave and transept, are commonly cruciform in shape, each arch having its own supporting rectangular pier at right angles to the other.
Columns.
Columns are an important structural feature of Romanesque architecture. Colonnettes and attached shafts are also used structurally and for decoration. Monolithic columns cut from a single piece of stone were frequently used in Italy, as they had been in Roman and Early Christian architecture. They were also used, particularly in Germany, when they alternated between more massive piers. Arcades of columns cut from single pieces are also common in structures that do not bear massive weights of masonry, such as cloisters, where they are sometimes paired.
Salvaged columns.
In Italy, during this period, a great number of antique Roman columns were salvaged and reused in the interiors and on the porticos of churches. The most durable of these columns are of marble and have the stone horizontally bedded. The majority are vertically bedded and are sometimes of a variety of colours. They may have retained their original Roman capitals, generally of the Corinthian or Roman Composite style.
Some buildings, like Santa Maria in Cosmedin (illustrated above) and the atrium at San Clemente in Rome, may have an odd assortment of columns in which large capitals are placed on short columns and small capitals are placed on taller columns to even the height. Architectural compromises of this type are seen where materials have been salvaged from a number of buildings. Salvaged columns were also used to a lesser extent in France.
Drum columns.
In most parts of Europe, Romanesque columns were massive, as they supported thick upper walls with small windows, and sometimes heavy vaults. The most common method of construction was to build them out of stone cylinders called drums, as in the crypt at Speyer Cathedral.
Hollow core columns.
Where really massive columns were called for, such as those at Durham Cathedral, they were constructed of ashlar masonry and the hollow core was filled with rubble. These huge untapered columns are sometimes ornamented with incised decorations.
Alternation.
A common characteristic of Romanesque buildings, occurring both in churches and in the arcades that separate large interior spaces of castles, is the alternation of piers and columns.
The most simple form that this takes is to have a column between each adjoining pier. Sometimes the columns are in multiples of two or three. At St. Michael's, Hildesheim, an A B B A alternation occurs in the nave while an A B A alternation can be seen in the transepts.
At Jumièges there are tall drum columns between piers each of which has a half-column supporting the arch. There are many variations on this theme, most notably at Durham Cathedral where the mouldings and shafts of the piers are of exceptional richness and the huge masonry columns are deeply incised with geometric patterns.
Often the arrangement was made more complex by the complexity of the piers themselves, so that it was not piers and columns that alternated, but rather, piers of entirely different form from each other, such as those of Sant' Ambrogio, Milan, where the nature of the vault dictated that the alternate piers bore a great deal more weight than the intermediate ones and are thus very much larger.
Capitals.
The foliate Corinthian style provided the inspiration for many Romanesque capitals, and the accuracy with which they were carved depended very much on the availability of original models, those in Italian churches such as Pisa Cathedral or church of Sant'Alessandro in Lucca and southern France being much closer to the Classical than those in England.
The Corinthian capital is essentially round at the bottom where it sits on a circular column and square at the top, where it supports the wall or arch. This form of capital was maintained in the general proportions and outline of the Romanesque capital. This was achieved most simply by cutting a rectangular cube and taking the four lower corners off at an angle so that the block was square at the top, but octagonal at the bottom, as can be seen at St. Michael's Hildesheim.
This shape lent itself to a wide variety of superficial treatments, sometimes foliate in imitation of the source, but often figurative. In Northern Europe the foliate capitals generally bear far more resemblance to the intricacies of manuscript illumination than to Classical sources. In parts of France and Italy there are strong links to the pierced capitals of Byzantine architecture. It is in the figurative capitals that the greatest originality is shown. While some are dependent on manuscripts illustrations of Biblical scenes and depictions of beasts and monsters, others are lively scenes of the legends of local saints.
The capitals, while retaining the form of a square top and a round bottom, were often compressed into little more than a bulging cushion-shape. This is particularly the case on large masonry columns, or on large columns that alternate with piers as at Durham.(See illustrated above)
Vaults and roofs.
The majority of buildings have wooden roofs, generally of a simple "truss", "tie beam" or "king post" form. In the case of trussed rafter roofs, they are sometimes lined with wooden ceilings in three sections like those that survive at Ely and Peterborough cathedrals in England. In churches, typically the aisles are vaulted, but the nave is roofed with timber, as is the case at both Peterborough and Ely. In Italy where open wooden roofs are common, and tie beams frequently occur in conjunction with vaults, the timbers have often been decorated as at San Miniato al Monte, Florence.
Vaults of stone or brick took on several different forms and showed marked development during the period, evolving into the pointed ribbed arch characteristic of Gothic architecture.
Barrel vault.
The simplest type of vaulted roof is the barrel vault in which a single arched surface extends from wall to wall, the length of the space to be vaulted, for example, the nave of a church. An important example, which retains Medieval paintings, is the vault of Saint-Savin-sur-Gartempe, France, of the early 12th century. However, the barrel vault generally required the support of solid walls, or walls in which the windows were very small.
Groin vault.
Groin vaults occur in early Romanesque buildings, notably at Speyer Cathedral where the high vault of about 1060 is the first employment in Romanesque architecture of this type of vault for a wide nave. In later buildings employing ribbed vaultings, groin vaults are most frequently used for the less visible and smaller vaults, particularly in crypts and aisles. A groin vault is almost always square in plan and is constructed of two barrel vaults intersecting at right angles. Unlike a ribbed vault, the entire arch is a structural member. Groin vaults are frequently separated by transverse arched ribs of low profile as at Speyer and Santiago de Compostela. At Sainte Marie Madeleine, Vézelay, the ribs are square in section, strongly projecting and polychrome.
Ribbed vault.
Ribbed vaults came into general use in the 12th century. In ribbed vaults, not only are there ribs spanning the vaulted area transversely, but each vaulted bay has diagonal ribs, following the same course as the groins in a groin vault. However, whereas in a groin vault, the vault itself is the structural member, in a ribbed vault, it is the ribs that are the structural members, and the spaces between them can be filled with lighter, non-structural material.
Because Romanesque arches are nearly always semi-circular, the structural and design problem inherent in the ribbed vault is that the diagonal span is larger and therefore higher than the transverse span. The Romanesque builders used a number of solutions to this problem. One was to have the centre point where the diagonal ribs met as the highest point, with the infill of all the surfaces sloping upwards towards it, in a domical manner. This solution was employed in Italy at San Michele, Pavia, and Sant' Ambrogio, Milan.
The solution employed in England was to stilt the transverse ribs, maintaining a horizontal central line to the roof like that of a barrel vault. The diagonal ribs could also be depressed, a solution used on the sexpartite vaults at both the Saint-Étienne, (Abbaye-aux-Hommes) and Sainte-Trinité, (Abbaye-les-Dames) at Caen, France, in the late 11th and early 12th centuries.
Pointed arched vault.
The problems encountered in the structure and appearance of vaults was solved late in the Romanesque period with the introduction of pointed arched ribs which allowed the height of both diagonal and transverse ribs to be varied in proportion to each other. Pointed ribs made their first appearance in the transverse ribs of the vaults at Durham Cathedral in northern England, dating from 1128. Durham is a cathedral of massive Romanesque proportions and appearance, yet its builders introduced several structural features that were new to architectural design and were later to be hallmark features of the Gothic. Another Gothic structural feature employed at Durham is the flying buttress. However, these are hidden beneath the roofs of the aisles.
The earliest pointed vault in France is that of the narthex of La Madeleine, Vézelay, dating from 1130. They were subsequently employed with the development of the Gothic style at the east end of the Basilica of St Denis in Paris in 1140. An early ribbed vault in the Romanesque architecture of Sicily is that of the chancel at the Cathedral of Cefalù.
Domes.
Domes in Romanesque architecture are generally found within crossing towers at the intersection of a church's nave and transept, which conceal the domes externally. Called a "tiburio", this tower-like structure often has a blind arcade near the roof. Romanesque domes are typically octagonal in plan and use corner squinches to translate a square bay into a suitable octagonal base. Octagonal cloister vaults appear "in connection with basilicas almost throughout Europe" between 1050 and 1100. The precise form differs from region to region.
Ecclesiastical architecture.
Plan.
Many parish churches, abbey churches and cathedrals are in the Romanesque style, or were originally built in the Romanesque style and have subsequently undergone changes.
The simplest Romanesque churches are aisless halls with a projecting apse at the chancel end, or sometimes, particularly in England, a projecting rectangular chancel with a chancel arch that might be decorated with mouldings. More ambitious churches have aisles separated from the nave by arcades.
Abbey and cathedral churches generally follow the Latin Cross plan. In England, the extension eastward may be long, while in Italy it is often short or non-existent, the church being of T plan, sometimes with apses on the transept ends as well as to the east. In France the church of St Front, Périgueux, appears to have been modelled on St. Mark's Basilica, Venice, or the Byzantine Church of the Holy Apostles and is of a Greek cross plan with five domes. In the same region, Angoulême Cathedral is an aisless church of the Latin cross plan, more usual in France, but is also roofed with domes.
In Germany, Romanesque churches are often of distinctive form, having apses at both east and west ends, the main entrance being central to one side. It is probable that this form came about to accommodate a baptistery at the west end.
NOTE: The plans below do not show the buildings in their current states.
The Abbey Church of St. Gall, Switzerland, shows the plan that was to become common throughout Germanic Europe. It is a Latin Cross with a comparatively long nave and short transepts and eastern end, which is apsidal. The nave is aisled, but the chancel and transepts are not. It has an apsidal west end, which was to become a feature of Churches of Germany, such as Worms Cathedral. Speyer Cathedral, Germany, also has aisless transept and chancel. It has a markedly modular look. A typical Germanic characteristic is the presence of towers framing the chancel and the west end. There is marked emphasis on the western entrance, called "Westwerk", which is seen in several other churches. Each vault compartment covers two narrow bays of the nave
At Autun Cathedral, France, the pattern of the nave bays and aisles extends beyond the crossing and into the chancel, each aisle terminating in an apse. Each nave bay is separated at the vault by a transverse rib. Each transept projects to the width of two nave bays. The entrance has a narthex which screens the main portal. This type of entrance was to be elaborated in the Gothic period on the transepts at Chartres. Angoulême Cathedral, France, is one of several instances in which the Byzantine churches of Constantinople seem to have been influential in the design in which the main spaces are roofed by domes. This structure has necessitated the use of very thick walls, and massive piers from which the domes spring. There are radiating chapels around the apse, which is a typically French feature and was to evolve into the chevet.
As was typically the case in England, Ely Cathedral was a Benedictine monastery, serving both monastic and secular function. To facilitate this, the chancel or "presbytery" is longer than usually found in Europe, as are the aisled transepts which contained chapels. In England, emphasis was placed on the orientation of the chapels to the east. The very large piers at the crossing signify that there was once a tower. The western end having two round towers flanking a tall central tower was unique in Britain. Ely Cathedral was never vaulted and retains a wooden ceiling over the nave.
The cathedral of Santiago de Compostela shares many features with Ely, but is typically Spanish in its expansive appearance. Santiago held the body of St. James and was the most significant pilgrimage site in Europe. The narthex, the aisles, the large aisled transepts and numerous projecting chapels reflect this. The chancel is short, compared to that of Ely, and the altar set so as to provide clear view to a vast congregation simultaneously.
Modena Cathedral shows a typically Italian Romanesque plan, often architecturally termed a "basilica", because of its similarity in plan to a Roman basilicas.
Section.
In section, the typical aisled church or cathedral has a nave with a single aisle on either side. The nave and aisles are separated by an arcade carried on piers or on columns. The roof of the aisle and the outer walls help to buttress the upper walls and vault of the nave, if present. Above the aisle roof are a row of windows known as the clerestory, which give light to the nave. During the Romanesque period there was a development from this two-stage elevation to a three-stage elevation in which there is a gallery, known as a "triforium", between the arcade and the clerestory. This varies from a simple blind arcade decorating the walls, to a narrow arcaded passage, to a fully developed second story with a row of windows lighting the gallery.
Church and cathedral east ends.
The eastern end of a Romanesque church is almost always semi-circular, with either a high chancel surrounded by an ambulatory as in France, or a square end from which an apse projects as in Germany and Italy. Where square ends exist in English churches, they are probably influenced by Anglo Saxon churches. Peterborough and Norwich Cathedrals have retained round east ends in the French style. However, in France, simple churches without apses and with no decorative features were built by the Cistercians who also founded many houses in England, frequently in remote areas.
Church and cathedral façades and external decoration.
Romanesque church facades, generally to the west end of the building, are usually symmetrical, have a large central portal made significant by its mouldings or porch, and an arrangement of arched-topped windows. In Italy there is often a single central ocular or wheel window. The common decorative feature is arcading.
Smaller churches often have a single tower that is usually placed to the western end in France or England, either centrally or to one side, while larger churches and cathedrals often have two.
In France, Saint-Étienne, Caen, presents the model of a large French Romanesque facade. It is a symmetrical arrangement of nave flanked by two tall towers each with two buttresses of low flat profile that divide the facade into three vertical units. The lowest stage is marked by large doors, each set within an arch in each of the three vertical sections. The wider central section has two tiers of three identical windows, while in the outer sections there are two tiers of single windows, giving emphasis to the mass of the towers. The towers rise above the facade through three further tiers, the lowest of tall blind arcading, the next of arcading pierced by two narrow windows and the third of two large windows, divided into two lights by a colonnette.
This facade can be seen as the foundation for many other buildings, including both French and English Gothic churches. While the form is typical of northern France, its various components were common to many Romanesque churches of the period across Europe. Similar facades are found in Portugal. In England, Southwell Cathedral has maintained this form, despite the insertion of a huge Gothic window between the towers. Lincoln and Durham must once have looked like this. In Germany, Limburg Cathedral has a rich variety of openings and arcades in horizontal storeys of varying heights.
The churches of San Zeno Maggiore, Verona, and San Michele, Pavia, present two types of facade that are typical of Italian Romanesque, that which reveals the architectural form of the building, and that which screens it. At San Zeno, the components of nave and aisles are made clear by the vertical shafts that rise to the level of the central gable and by the varying roof levels. At San Miniato al Monte the definition of the architectural parts is made even clearer by the polychrome marble, a feature of many Italian Medieval facades, particularly in Tuscany.
At San Michele the vertical definition is present as at San Zeno, but the rooflines are screened behind a single large gable decorated with stepped arcading. At Santa Maria della Pieve, Arezzo, this screening is carried even further, as the roofline is horizontal and the arcading rises in many different levels while the colonettes that support them have a great diversity of decoration.
In the Rhineland and Netherlands the Carolingian form of west end known as the westwerk prevailed. Towers and apse of the western end are often incorporated into a multi-storey structure that bears little structural or visual relationship to the building behind it. These westwerks take a great variety of forms as may be seen at Maria Laach Abbey, St Gertrude, Nivelles, and St Serviatius, Maastricht.
Church towers.
Towers were an important feature of Romanesque churches and a great number of them are still standing. They take a variety of forms: square, circular and octagonal, and are positioned differently in relation to the church building in different countries. In northern France, two large towers, such as those at Caen, were to become an integral part of the facade of any large abbey or cathedral. In central and southern France this is more variable and large churches may have one tower or a central tower. Large churches of Spain and Portugal usually have two towers.
Many abbeys of France, such as that at Cluny, had many towers of varied forms. This is also common in Germany, where the apses were sometimes framed with circular towers and the crossing surmounted by an octagonal tower as at Worms Cathedral. Large paired towers of square plan could also occur on the transept ends, such as those at Tournai Cathedral in Belgium. In Germany, where four towers frequently occur, they often have spires that may be four or eight sided, or the distinctive "Rhenish helm" shape seen on the cathedrals of Limburg or Speyer. It is also common to see bell or onion-shaped spires of the Baroque period surmounting Romanesque towers in central and Eastern Europe.
In England, for large abbeys and cathedral buildings, three towers were favoured, with the central tower being the tallest. This was often not achieved, through the slow process of the building stages, and in many cases the upper parts of the tower were not completed until centuries later as at Durham and Lincoln. Large Norman towers exist at the cathedrals of Durham, Exeter, Southwell, Norwich and Tewkesbury Abbey. Such towers were often topped during the late Medieval period with a Gothic spire of wooden construction covered with lead, copper or shingles. In the case of Norwich Cathedral, the huge, ornate, 12th-century crossing-tower received a 15th-century masonry spire rising to a height of 320 feet and remaining to this day.
In Italy towers are almost always free standing and the position is often dictated by the landform of the site, rather than aesthetics. This is the case in nearly all Italian churches both large and small, except in Sicily where a number of churches were founded by the Norman rulers and are more French in appearance.
As a general rule, large Romanesque towers are square with corner buttresses of low profile, rising without diminishing through the various stages. Towers are usually marked into clearly defined stages by horizontal courses. As the towers rise, the number and size of openings increases as can be seen on the right tower of the transept of Tournai Cathedral where two narrow slits in the fourth level from the top becomes a single window, then two windows, then three windows at the uppermost level. This sort of arrangement is particularly noticeable on the towers of Italian churches, which are usually built of brick and may have no other ornament. Two fine examples occur at Lucca, at the church of San Frediano and at the "Duomo". It is also seen in Spain.
In Italy there are a number of large free-standing towers that are circular, the most famous of these being the Leaning Tower of Pisa. In other countries where circular towers occur, such as Germany, they are usually paired and often flank an apse. Circular towers are uncommon in England, but occur throughout the Early Medieval period in Ireland.
Polygonal towers were often used on crossings and occur in France, Germany, Italy and Spain such as that of the Old Cathedral, Salamanca, which is covered by a dome supported on a ribbed vault.
Smaller churches sometimes had bell-gables instead of towers, a feature which, according to some authors, is characteristic of the simplicity of much architecture in the Romanesque style.
Portals.
Romanesque churches generally have a single portal centrally placed on the west front, the focus of decoration for the facade of the building. Some churches such as Saint-Étienne, Caen, (11th century) and Pisa Cathedral (late 12th century) had three western portals, in the manner of Early Christian basilicas. Many churches, both large and small, had lateral entrances that were commonly used by worshippers.
Romanesque doorways have a character form, with the jambs having a series of receding planes, into each of which is set a circular shaft, all surmounted by a continuous abacus. The semi-circular arch which rises from the abacus has the same seried planes and circular mouldings as the jambs. There are typically four planes containing three shafts, but there may be as many as twelve shafts, symbolic of the apostles.
The opening of the portal may be arched, or may be set with a lintel supporting a tympanum, generally carved, but in Italy sometimes decorated with mosaic or fresco. A carved tympanum generally constitutes the major sculptural work of a Romanesque church. The subject of the carving on a major portal may be Christ in Majesty or the Last Judgement. Lateral doors may include other subjects such as the Birth of Christ. The portal may be protected by a porch, with simple open porches being typical of Italy, and more elaborate structures typical of France and Spain.
Interiors.
The structure of large churches differed regionally and developed across the centuries. The use of piers of rectangular plan to support arcades was common, as at Mainz Cathedral and St Gertrude Nivelle, and remained usual in smaller churches across Europe, with the arcades often taking the form of openings through the surface of a wall. In Italy, where there was a strong tradition of using marble columns, complete with capital, base and abacus, this remained prevalent, often reusing existent ancient columns, as at San Miniato al Monte. A number of 11th-century churches have naves distinguished by huge circular columns with no clerestory, or a very small one as at St Philibert, Tournus. In England stout columns of large diameter supported decorated arches, gallery and clerestory, as at the nave of Malmesbury Abbey (see "Piers and columns", above). By the early 12th century composite piers had evolved, in which the attached shafts swept upward to a ribbed vault or were continued into the mouldings of the arcade, as at Vézelay Abbey, Saint-Étienne, Caen, and Peterborough Cathedral.
The nature of the internal roofing varied greatly, from open timber roofs, and wooden ceilings of different types, which remained common in smaller churches, to simple barrel vaults and groin vaults and increasingly to the use of ribbed vaults in the late 11th and 12th centuries, which were to become a common feature of larger abbey churches and cathedrals. A number of Romanesque churches are roofed with a series of Domes. At Fontevrault Abbey the nave is covered by four domes, while at the Church of Saint Front, Périgueux, the church is of Greek cross plan, with a central dome surrounded by four smaller domes over the nave, chancel and transepts.
Internal decoration varied across Europe. Where wide expanses of wall existed, they were often plastered and painted. Wooden ceilings and timber beams were decorated. In Italy walls were sometimes faced with polychrome marble. Where buildings were constructed of stone that was suitable for carving, many decorative details occur, including ornate capitals and mouldings.
The apsidal east end was often a focus of decoration, with both architectonic forms such as arcading and pictorial features such as carved figures, murals and occasionally mosaics. Stained glass came into increasing use from the 11th century. In many churches the eastern end has been rebuilt in a later style. Of England's Norman cathedrals, no eastern end remains unchanged. In France the eastern terminals of the important abbeys of Caen, Vézelay and, most significantly, the Basilica of St Denis were completely rebuilt in the Gothic style. In Germany, major reconstructions of the 19th century sought to return many Romanesque buildings to their original form. Examples of simple Romanesque apses can be seen in the images of St Gertrude, Nivelles; St Philibert, Tournus, and San Miniato al Monte.
Other structures.
Among the structures associated with church buildings are crypts, porches, chapter houses, cloisters and baptisteries.
Crypts are often present as an underlying structure to a substantial church, and are generally a completely discrete space, but occasionally, as in some Italian churches, may be a sunken space under a raised chancel and open, via steps, to the body of the nave. Romanesque crypts have survived in many instances, such as Canterbury Cathedral, when the church itself has been rebuilt.
The usual construction of a Romanesque crypt is with many short stout columns carrying groin vaults, as at Worcester Cathedral.
Porches sometimes occur as part of the original design of a facade. This is very much the case in Italy, where they are usually only one bay deep and are supported on two columns, often resting on couchant lions, as at St Zeno, Verona.See above. Elsewhere, porches of various dates have been added to the facade or side entrance of existent churches and may be quite a substantial structure, with several bays of vaulting supported on an open or partially open arcade, and forming a sort of narthex as at the Church of St Maria, Laach.See above In Spain, Romanesque churches often have large lateral porches, like loggias.
Chapter houses often occur adjacent to monastic or cathedral churches. Few have survived intact from the Romanesque period. Early chapter houses were rectangular in shape, with the larger ones sometimes having groin or ribbed vaults supported on columns. Later Romanesque chapter houses sometimes had an apsidal eastern end. The chapter house at Durham Cathedral is a wide space with a ribbed vault, restored as originally constructed in 1130. The circular chapter house at Worcester Cathedral, built by Bishop Wulfstan (1062–95), was the first circular chapter house in Europe and was much imitated in England.
Cloisters are generally part of any monastic complex and also occur at cathedral and collegiate churches. They were essential to the communal way of life, a place for both working during daylight hours and relaxing during inclement weather. They usually abut the church building and are enclosed with windowless walls on the outside and an open arcade on the inside, looking over a courtyard or "cloister garth". They may be vaulted or have timber roofs. The arcades are often richly decorated and are home to some of the most fanciful carved capitals of the Romanesque period with those of Santo Domingo de Silos in Spain and the Abbey of St Pierre Moissac, being examples. Many Romanesque cloisters have survived in Spain, France, Italy and Germany, along with some of their associated buildings.
Baptisteries often occur in Italy as a free standing structure, associated with a cathedral. They are generally octagonal or circular and domed. The interior may be arcaded on several levels as at Pisa Cathedral. Other notable Romanesque baptisteries are that at Parma Cathedral remarkable for its galleried exterior, and the polychrome Baptistery of San Giovanni of Florence Cathedral, with vault mosaics of the 13th century including Christ in Majesty, possibly the work of the almost legendary Coppo di Marcovaldo.
Decoration.
Architectural embellishment.
Arcading is the single most significant decorative feature of Romanesque architecture. It occurs in a variety of forms, from the Lombard band, which is a row of small arches that appear to support a roofline or course, to shallow blind arcading that is often a feature of English architecture and is seen in great variety at Ely Cathedral, to the open dwarf gallery, first used at Speyer Cathedral and widely adopted in Italy as seen on both Pisa Cathedral and its famous Leaning Tower. Arcades could be used to great effect, both externally and internally, as exemplified by the church of Santa Maria della Pieve, in Arezzo.
Architectural sculpture.
The Romanesque period produced a profusion of sculptural ornamentation. This most frequently took a purely geometric form and was particularly applied to mouldings, both straight courses and the curved moldings of arches. In La Madeleine, Vezelay, for example, the polychrome ribs of the vault are all edged with narrow filets of pierced stone. Similar decoration occurs around the arches of the nave and along the horizontal course separating arcade and clerestory. Combined with the pierced carving of the capitals, this gives a delicacy and refinement to the interior.
In England, such decoration could be discrete, as at Hereford and Peterborough cathedrals, or have a sense of massive energy as at Durham where the diagonal ribs of the vaults are all outlined with chevrons, the mouldings of the nave arcade are carved with several layers of the same and the huge columns are deeply incised with a variety of geometric patterns creating an impression of directional movement. These features combine to create one of the richest and most dynamic interiors of the Romanesque period.
Although much sculptural ornament was sometimes applied to the interiors of churches, the focus of such decoration was generally the west front, and in particular, the portals. Chevrons and other geometric ornaments, referred to by 19th-century writers as "barbaric ornament", are most frequently found on the mouldings of the central door. Stylized foliage often appears, sometimes deeply carved and curling outward after the manner of the acanthus leaves on Corinthian capitals, but also carved in shallow relief and spiral patterns, imitating the intricacies of manuscript illuminations. In general, the style of ornament was more classical in Italy, such as that seen around the door of San Giusto in Lucca, and more "barbaric" in England, Germany and Scandinavia, such as that seen at Lincoln and Speyer Cathedrals. France produced a great range of ornament, with particularly fine interwoven and spiralling vines in the "manuscript" style occurring at Saint-Sernin, Toulouse.
Figurative sculpture.
With the fall of the Roman Empire, the tradition of carving large works in stone and sculpting figures in bronze died out. The best-known surviving large sculptural work of Proto-Romanesque Europe is the life-size wooden Crucifix commissioned by Archbishop Gero of Cologne in about 960–65. During the 11th and 12th centuries, figurative sculpture flourished in a distinctly Romanesque style that can be recognised across Europe, although the most spectacular sculptural projects are concentrated in South-Western France, Northern Spain and Italy.
Major figurative decoration occurs particularly around the portals of cathedrals and churches, ornamenting the tympanum, lintels, jambs and central posts. The tympanum is typically decorated with the imagery of Christ in Majesty with the symbols of the Four Evangelists, drawn directly from the gilt covers of medieval Gospel Books. This style of doorway occurs in many places and continued into the Gothic period. A rare survival in England is that of the "Prior's Door" at Ely Cathedral. In France, many have survived, with impressive examples at the Abbey of Saint-Pierre, Moissac, the Abbey of Sainte-Marie, Souillac, and Abbey of la Madaleine, Vézelay – all daughter houses of Cluny, with extensive other sculpture remaining in cloisters and other buildings. Nearby, Autun Cathedral has a Last Judgement of great rarity in that it has uniquely been signed by its creator Giselbertus (who was perhaps the patron rather than the sculptor). The same artist is thought to have worked at la Madaleine Vezelay which uniquely has two elaborately carved tympanum, the early inner one representing the Last Judgement and that on the outer portal of the narthex representing Jesus sending forth the Apostles to preach to the nations.
It is a feature of Romanesque art, both in manuscript illumination and sculptural decoration, that figures are contorted to fit the space that they occupy. Among the many examples that exist, one of the finest is the figure of the Prophet Jeremiah from the pillar of the portal of the Abbey of Saint-Pierre, Moissac, France, from about 1130. A significant motif of Romanesque design is the spiral, a form applied to both plant motifs and drapery in Romanesque sculpture. An outstanding example of its use in drapery is that of the central figure of Christ on the outer portal at La Madaleine, Vezelay.
Many of the smaller sculptural works, particularly capitals, are Biblical in subject and include scenes of Creation and the Fall of Man, episodes from the life of Christ and those Old Testament scenes that prefigure his Death and Resurrection, such as Jonah and the Whale and Daniel in the lions' den. Many Nativity scenes occur, the theme of the Three Kings being particularly popular. The cloisters of Santo Domingo de Silos Abbey in Northern Spain, and Moissac are fine examples surviving complete.
Murals.
The large wall surfaces and plain curving vaults of the Romanesque period lent themselves to mural decoration. Unfortunately, many of these early wall paintings have been destroyed by damp or the walls have been replastered and painted over. In most of Northern Europe such pictures were systematically destroyed in bouts of Reformation iconoclasm. In other countries they have suffered from war, neglect and changing fashion.
A classic scheme for the full painted decoration of a church, derived from earlier examples often in mosaic, had, as its focal point in the semi-dome of the apse, Christ in Majesty or Christ the Redeemer enthroned within a mandorla and framed by the four winged beasts, symbols of the Four Evangelists, comparing directly with examples from the gilt covers or the illuminations of Gospel Books of the period. If the Virgin Mary was the dedicatee of the church, she might replace Christ here. On the apse walls below would be saints and apostles, perhaps including narrative scenes, for example of the saint to whom the church was dedicated. On the sanctuary arch were figures of apostles, prophets or the twenty-four "elders of the Apocalypse", looking in towards a bust of Christ, or his symbol the Lamb, at the top of the arch. The north wall of the nave would contain narrative scenes from the Old Testament, and the south wall from the New Testament. On the rear west wall would be a Doom painting or Last Judgement, with an enthroned and judging Christ at the top.
One of the most intact schemes to exist is that at Saint-Savin-sur-Gartempe in France. (See picture above under "Vault") The long barrel vault of the nave provides an excellent surface for fresco, and is decorated with scenes of the Old Testament, showing the Creation, the Fall of Man and other stories including a lively depiction of Noah's Ark complete with a fearsome figurehead and numerous windows through with can be seen the Noah and his family on the upper deck, birds on the middle deck, while on the lower are the pairs of animals. Another scene shows with great vigour the swamping of Pharaoh's army by the Red Sea. The scheme extends to other parts of the church, with the martyrdom of the local saints shown in the crypt, and Apocalypse in the narthex and Christ in Majesty. The range of colours employed is limited to light blue-green, yellow ochre, reddish brown and black. Similar paintings exist in Serbia, Spain, Germany, Italy and elsewhere in France.
Stained glass.
The oldest-known fragments of medieval pictorial stained glass appear to date from the 10th century. The earliest intact figures are five prophet windows at Augsburg, dating from the late 11th century. The figures, though stiff and formalised, demonstrate considerable proficiency in design, both pictorially and in the functional use of the glass, indicating that their maker was well accustomed to the medium. At Canterbury and Chartres Cathedrals, a number of panels of the 12th century have survived, including, at Canterbury, a figure of Adam digging, and another of his son Seth from a series of Ancestors of Christ. Adam represents a highly naturalistic and lively portrayal, while in the figure of Seth, the robes have been used to great decorative effect, similar to the best stone carving of the period.
Many of the magnificent stained glass windows of France, including the famous windows of Chartres, date from the 13th century. Far fewer large windows remain intact from the 12th century. One such is the Crucifixion of Poitiers, a remarkable composition that rises through three stages, the lowest with a quatrefoil depicting the Martyrdom of St Peter, the largest central stage dominated by the crucifixion and the upper stage showing the Ascension of Christ in a mandorla. The figure of the crucified Christ is already showing the Gothic curve. The window is described by George Seddon as being of "unforgettable beauty".
Transitional style and the continued use of Romanesque forms.
During the 12th century, features that were to become typical of Gothic architecture began to appear. It is not uncommon, for example, for a part of building that has been constructed over a lengthy period extending into the 12th century, to have very similar arcading of both semi-circular and pointed shape, or windows that are identical in height and width, but in which the later ones are pointed. This can be seen on the towers of Tournai Cathedral and on the western towers and facade at Ely Cathedral. Other variations that appear to hover between Romanesque and Gothic occur, such as the facade designed by Abbot Suger at the Abbey of Saint-Denis, which retains much that is Romanesque in its appearance, and the Facade of Laon Cathedral, which, despite its Gothic form, has round arches.
Abbot Suger's innovative choir of the Abbey of Saint-Denis, 1140–44, led to the adoption of the Gothic style by Paris and its surrounding area, but other parts of France were slower to take it up, and provincial churches continued to be built in the heavy manner and rubble stone of the Romanesque, even when the openings were treated with the fashionable pointed arch.
In England, the Romanesque groundplan, which in that country commonly had a very long nave, continued to affect the style of building of cathedrals and those large abbey churches which were also to become cathedrals at the dissolution of the monasteries in the 16th century. Despite the fact that English cathedrals were built or rebuilt in many stages, substantial areas of Norman building can be seen in many of them, particularly in the nave arcades. In the case of Winchester Cathedral, the Gothic arches were literally carved out of the existent Norman piers. Other cathedrals have sections of their building which are clearly an intermediate stage between Norman and Gothic, such as the western towers of Ely Cathedral and part of the nave at Worcester Cathedral. The first truly Gothic building in England is the long eastern end of Canterbury Cathedral commenced in 1175.
In Italy, although many churches such as Florence Cathedral and Santa Maria Novella were built in the Gothic style, or utilising the pointed arch and window tracery, Romanesque features derived from the Roman architectural heritage, such as sturdy columns with capitals of a modified Corinthian form, continued to be used. The pointed vault was utilised where convenient, but it is commonly interspersed with semicircular arches and vaults wherever they conveniently fit. The facades of Gothic churches in Italy are not always easily distinguishable from the Romanesque.
Germany was not quick to adopt the Gothic style, and when it did so in the 1230s, the buildings were often modelled very directly upon French cathedrals, as Cologne Cathedral was modelled on Amiens. The smaller churches and abbeys continued to be constructed in a more provincial Romanesque manner, the date only being registered by the pointed window openings.
Romanesque castles, houses and other buildings.
The Romanesque period was a time of great development in the design and construction of defensive architecture. After churches and the monastic buildings with which they are often associated, castles are the most numerous type of building of the period. While most are in ruins through the action of war and politics, others, like William the Conqueror's White Tower within the Tower of London have remained almost intact.
In some regions, particularly Germany, large palaces were built for rulers and bishops. Local lords built great halls in the countryside, while rich merchants built grand town houses. In Italy, city councils constructed town halls, while wealthy cities of Northern Europe protected their trading interests with warehouses and commercial premises. All over Europe, dwellers of the town and country built houses to live in, some of which, sturdily constructed in stone, have remained to this day with sufficient of their form and details intact to give a picture of the style of domestic architecture that was in fashion at the time.
Examples of all these types of buildings can be found scattered across Europe, sometimes as isolated survivals like the two merchants' houses on opposite sides of Steep Hill in Lincoln, England, and sometimes giving form to a whole medieval city like San Gimignano in Tuscany, Italy. These buildings are the subject of a separate article.
Romanesque Revival.
During the 19th century, when Gothic Revival architecture was fashionable, buildings were occasionally designed in the Romanesque style. There are a number of Romanesque Revival churches, dating from as early as the 1830s and continuing into the 20th century where the massive and "brutal" quality of the Romanesque style was appreciated and designed in brick.
The Natural History Museum, London, designed by Alfred Waterhouse, 1879, on the other hand, is a Romanesque revival building that makes full use of the decorative potential of Romanesque arcading and architectural sculpture. The Romanesque appearance has been achieved while freely adapting an overall style to suit the function of the building. The columns of the foyer, for example, give an impression of incised geometric design similar to those of Durham Cathedral. However, the sources of the incised patterns are the trunks of palms, cycads and tropical tree ferns. The animal motifs, of which there are many, include rare and exotic species.
The type of modern buildings for which the Romanesque style was most frequently adapted was the warehouse, where a lack of large windows and an appearance of great strength and stability were desirable features. These buildings, generally of brick, frequently have flattened buttresses rising to wide arches at the upper levels after the manner of some Italian Romanesque facades. This style was adapted to suit commercial buildings by opening the spaces between the arches into large windows, the brick walls becoming a shell to a building that was essentially of modern steel-frame construction, the architect Henry Hobson Richardson giving his name to the style, Richardsonian Romanesque. Good examples of the style are Marshall Field's Wholesale Store, Chicago, by H.H. Richardson, 1885, and the Chadwick Lead Works in Boston, USA, by William Preston, 1887. The style also lent itself to the building of cloth mills, steelworks and powerstations.

</doc>
<doc id="52687" url="https://en.wikipedia.org/wiki?curid=52687" title="Classical architecture">
Classical architecture

Classical architecture usually denotes architecture which is more or less consciously derived from the principles of Greek and Roman architecture of classical antiquity, or sometimes even more specifically, from the works of Vitruvius. Different styles of classical architecture have arguably existed since the Carolingian Renaissance, and prominently since the Italian Renaissance. Although classical styles of architecture can vary greatly, they can in general all be said to draw on a common "vocabulary" of decorative and constructive elements. In much of the Western world, different classical architectural styles have dominated the history of architecture from the Renaissance until the second world war, though it continues to inform many architects to this day.
The term "classical architecture" also applies to any mode of architecture that has evolved to a highly refined state, such as classical Chinese architecture, or classical Mayan architecture. It can also refer to any architecture that employs classical aesthetic philosophy. The term might be used differently from "traditional" or "vernacular architecture", although it can share underlying axioms with it.
For contemporary buildings following authentic classical principles, the term New Classical Architecture may be used.
History.
Origins.
Classical architecture is derived from the architecture of ancient Greece and ancient Rome. With the collapse of the western part of the Roman empire, the architectural traditions of the Roman empire ceased to be practised in large parts of western Europe. In the Byzantine Empire, the ancient ways of building lived on but relatively soon developed into a distinct Byzantine style. The first conscious efforts to bring back the disused language of form of classical antiquity into Western architecture can be traced to the Carolingian Renaissance of the late 8th and 9th centuries. The gatehouse of Lorsch Abbey (c. 800), in present-day Germany thus displays a system of alternating attached columns and arches which could be an almost direct paraphrase of e.g., that of the Colosseum in Rome. Byzantine architecture, just as Romanesque and even to some extent Gothic architecture (with which classical architecture is often juxtaposed), can also incorporate classical elements and details but do not to the same degree reflect a conscious effort to draw upon the architectural traditions of antiquity; for example, they do not observe the idea of a systematic order of proportions for pillars. In general, therefore, they are not considered classical architectural styles in a strict sense.
Development.
During the Italian renaissance and with the demise of Gothic style, major efforts were made by architects such as Leon Battista Alberti, Sebastiano Serlio and Giacomo Barozzi da Vignola to revive the language of architecture of first and foremost ancient Rome. This was done in part through the study of the ancient Roman architectural treatise "De architectura" by Vitruvius, and to some extent by studying the actual remains of ancient Roman buildings in Italy. Nonetheless, the classical architecture of the Renaissance from the outset represents a highly specific interpretation of the classical ideas. In a building like the "Ospedale degli Innocenti" in Florence by Filippo Brunelleschi, one of the very earliest Renaissance buildings (built 1419–45), the treatment of the columns for example has no direct antecedent in ancient Roman architecture. During this time period, the study of ancient architecture developed into the architectural theory of classical architecture; somewhat over-simplified, one could say that classical architecture in its variety of forms ever since have been interpretations and elaborations of the architectural rules set down during antiquity.
Most of the styles originating in post-renaissance Europe can be described as classical architecture. This broad use of the term is employed by Sir John Summerson in "The Classical Language of Architecture". The elements of classical architecture have been applied in radically different architectural contexts than those for which they were developed, however. For example, Baroque or Rococo architecture are styles which, although classical at root, display an architectural language very much in their own right. During these periods, architectural theory still referred to classical ideas but rather less sincerely than during the Renaissance.
As a reaction to late baroque and rococo forms, architectural theorists from circa 1750 through what became known as Neoclassicism again consciously and earnestly attempted to emulate antiquity, supported by recent developments in Classical archaeology and a desire for an architecture based on clear rules and rationality. Claude Perrault, Marc-Antoine Laugier and Carlo Lodoli were among the first theorists of neoclassicism, while Étienne-Louis Boullée, Claude Nicolas Ledoux, Friedrich Gilly and John Soane were among the more radical and influential. Neoclassical architecture held a particularly strong position on the architectural scene c. 1750–1850. The competing neo-Gothic style however rose to popularity during the early 1800s, and the later part the 19th century was characterised by a variety of styles, some of them only slightly or not at all related to classicism (such as Art Nouveau), and eclecticism. Although classical architecture continued to play an important role and for periods of time at least locally dominated the architectural scene, as exemplified by the "Nordic Classicism" during the 1920s, classical architecture in its stricter form never regained its former dominance. With the advent of Modernism during the early 20th century, classical architecture arguably almost completely ceased to be practised.
Scope.
As noted above, classical styles of architecture dominated Western architecture for a very long time, roughly from the Renaissance until the advent of Modernism. That is to say, that classical antiquity at least in theory was considered the prime source of inspiration for architectural endeavours in the West for much of Modern history. Even so, because of liberal, personal or theoretically diverse interpretations of the antique heritage, classicism covers a broad range of styles, some even so to speak cross-referencing, like Neo-Palladian architecture, which draws its inspiration from the works of Italian Renaissance architect Andrea Palladio — who himself drew inspiration from ancient Roman architecture. Furthermore, it can even be argued (as noted above) that styles of architecture not typically considered classical, like Gothic, can be said to contain classical elements. Therefore, a simple delineation of the scope of classical architecture is difficult to make. The more or less defining characteristic can still be said to be a reference to ancient Greek or Roman architecture, and the architectural rules or theories that derived from that architecture.
Petrification.
In the grammar of architecture, the word "petrification" is often used when discussing the development of sacred structures, such as temples, mainly with reference to developments in the Greek world. During the Archaic and early Classical periods (about the 6th and early 5th centuries BC), the architectural forms of the earliest temples had solidified and the Doric emerged as the predominant element. A widely accepted theory in classical studies is that the earliest temple structures were of wood and the great forms, or elements of architectural style, were codified and rather permanent by the time we see the Archaic emergent and established. It was during this period, at different times and places in the Greek world, that the use of dressed and polished stone replaced the wood in these early temples, but the forms and shapes of the old wooden styles were retained, just as if the wooden structures had turned to stone, thus the designation "petrification" or sometimes "petrified carpentry" for this process.
This careful preservation of the primitive wooden appearance in the stone fabric of the newer buildings was scrupulously observed and this suggests that it may have been dictated by religion rather than aesthetics, although the exact reasons are now lost in the mists of antiquity. And not everyone within the great reach of Mediterranean civilization made this transition. The Etruscans in Italy were, from their earliest period, greatly influenced by their contact with Greek culture and religion, but they retained their wooden temples (with some exceptions) until their culture was completely absorbed into the Roman world, with the great wooden Temple of Jupiter on the Capitol in Rome itself being a good example. Nor was it the lack of knowledge of stone working on their part that prevented them from making the transition from timber to dressed stone.

</doc>
<doc id="52688" url="https://en.wikipedia.org/wiki?curid=52688" title="Baroque painting">
Baroque painting

Baroque painting is the painting associated with the Baroque cultural movement. The movement is often identified with Absolutism, the Counter Reformation and Catholic Revival, but the existence of important Baroque art and architecture in non-absolutist and Protestant states throughout Western Europe underscores its widespread popularity.
Baroque painting encompasses a great range of styles, as most important and major painting during the period beginning around 1600 and continuing throughout the 17th century, and into the early 18th century is identified today as Baroque painting. In its most typical manifestations, Baroque art is characterized by great drama, rich, deep colour, and intense light and dark shadows, but the classicism of French Baroque painters like Poussin and Dutch genre painters such as Vermeer are also covered by the term, at least in English. As opposed to Renaissance art, which usually showed the moment before an event took place, Baroque artists chose the most dramatic point, the moment when the action was occurring: Michelangelo, working in the High Renaissance, shows his David composed and still before he battles Goliath; Bernini's Baroque David is caught in the act of hurling the stone at the giant. Baroque art was meant to evoke emotion and passion instead of the calm rationality that had been prized during the Renaissance.
Among the greatest painters of the Baroque period are Velázquez, Caravaggio, Rembrandt, Rubens, Poussin, and Vermeer. Caravaggio is an heir of the humanist painting of the High Renaissance. His realistic approach to the human figure, painted directly from life and dramatically spotlit against a dark background, shocked his contemporaries and opened a new chapter in the history of painting. Baroque painting often dramatizes scenes using chiaroscuro light effects; this can be seen in works by Rembrandt, Vermeer, Le Nain and La Tour.
The Flemish painter Anthony van Dyck developed a graceful but imposing portrait style that was very influential, especially in England.
The prosperity of 17th century Holland led to an enormous production of art by large numbers of painters who were mostly highly specialized and painted only genre scenes, landscapes, Still-lifes, portraits or History paintings. Technical standards were very high, and Dutch Golden Age painting established a new repertoire of subjects that was very influential until the arrival of Modernism.
History.
The Council of Trent (1545–63), in which the Roman Catholic Church answered many questions of internal reform raised by both Protestants and by those who had remained inside the Catholic Church, addressed the representational arts in a short and somewhat oblique passage in its decrees. This was subsequently interpreted and expounded by a number of clerical authors like Molanus, who demanded that paintings and sculptures in church contexts should depict their subjects clearly and powerfully, and with decorum, without the stylistic airs of Mannerism.
This return toward a populist conception of the function of ecclesiastical art is seen by many art historians as driving the innovations of Caravaggio and the Carracci brothers, all of whom were working (and competing for commissions) in Rome around 1600, although unlike the Carracci, Caravaggio persistently was criticised for lack of decorum in his work.
However, although religious painting, history painting, allegories, and portraits were still considered the most noble subjects, landscape, still life, and genre scenes were also becoming more common in Catholic countries, and were the main genres in Protestant ones.
The term.
The term "Baroque" was initially used with a derogatory meaning, to underline the excesses of its emphasis. Others derive it from the mnemonic term "Baroco" denoting, in logical "Scholastica", a supposedly laboured form of syllogism.
In particular, the term was used to describe its eccentric redundancy and noisy abundance of details, which sharply contrasted the clear and sober rationality of the Renaissance. It was first rehabilitated by the Swiss-born art historian, Heinrich Wölfflin (1864–1945) in his "Renaissance und Barock" (1888); Wölfflin identified the Baroque as "movement imported into mass", an art antithetic to Renaissance art. He did not make the distinctions between Mannerism and Baroque that modern writers do, and he ignored the later phase, the academic Baroque that lasted into the 18th century. Writers in French and English did not begin to treat Baroque as a respectable study until Wölfflin's influence had made German scholarship pre-eminent.
A rather different art developed out of northern realist traditions in 17th century Dutch Golden Age painting, which had very little religious art, and little history painting, instead playing a crucial part in developing secular genres such as still life, genre paintings of everyday scenes, and landscape painting. While the Baroque nature of Rembrandt's art is clear, the label is less use for Vermeer and many other Dutch artists. Flemish Baroque painting shared a part in this trend, while also continuing to produce the traditional categories.

</doc>
