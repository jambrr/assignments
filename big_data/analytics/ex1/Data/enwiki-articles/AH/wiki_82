<doc id="54386" url="https://en.wikipedia.org/wiki?curid=54386" title="Philip II of Spain">
Philip II of Spain

Philip II (; 21 May 1527 – 13 September 1598) was King of Spain (1556-1598), King of Portugal (1581-1598, as "Philip I", "Filipe I"), King of Naples and Sicily (both from 1554), and during his marriage to Queen Mary I (1554–58), was King of England and Ireland. He was also Duke of Milan. From 1555, he was lord of the Seventeen Provinces of the Netherlands.
Known in Spain as "Felipe el Prudente" ('"Philip the Prudent'"), his empire included territories on every continent then known to Europeans, including his namesake the Philippine Islands. During his reign, Spain reached the height of its influence and power. This is sometimes called the "Golden Age". The expression, "the empire on which the sun never sets," was coined during Philip's time to reflect the extent of his dominion.
During Philip's reign there were separate state bankruptcies in 1557, 1560, 1569, 1575, and 1596. This was partly the cause for the declaration of independence which created the Dutch Republic in 1581. A devout Catholic, Philip is also known for organising a huge naval expedition against Protestant England in 1588, known usually as the Spanish Armada, which was unsuccessful, mostly due to storms and grave logistical problems.
Philip was described by the Venetian ambassador Paolo Fagolo in 1563 as "slight of stature and round-faced, with pale blue eyes, somewhat prominent lip, and pink skin, but his overall appearance is very attractive." The Ambassador went on to say "He dresses very tastefully, and everything that he does is courteous and gracious."
Early years: 1527–54.
The son of Charles V of the Holy Roman Empire, and his wife, Infanta Isabella of Portugal, Philip was born in the Spanish capital of Valladolid on 21 May 1527 at Palacio de Pimentel owned by Don Bernardino Pimentel (the first Marqués de Távara). The culture and courtly life of Spain were an important influence in his early life. He was tutored by Juan Martínez Siliceo – the future Archbishop of Toledo. Philip displayed reasonable aptitude in arms and letters alike. Later he would study with more illustrious tutors, including the humanist Juan Cristóbal Calvete de Estrella. Philip, though he had good command over Latin, Spanish and Portuguese, never managed to equal his father, Charles V, as a polyglot. Despite being also a German archduke from the House of Habsburg, Philip was seen as a foreigner in the Holy Roman Empire. The feeling was mutual. Philip felt himself to be culturally Spanish; he had been born in Spain and raised in the Castilian court, his native tongue was Spanish, and he preferred to live in Spain. This would ultimately impede his succession to the imperial throne.
In April 1528, when Philip was eleven months old, he received the oath of allegiance as heir to the crown from the Cortes of Castile, and from that time until the death of his mother Isabella in 1539, Philip was raised in the royal court of Castile under the care of his mother, and one of her Portuguese ladies, Dona Leonor de Mascarenhas, to whom he was devotedly attached. Philip was also close to his two sisters, María and Juana, and to his two pages, the Portuguese nobleman Rui Gomes da Silva and Luis de Requesens, the son of his governor Juan de Zúñiga. These men would serve Philip throughout their lives, as would Antonio Pérez, his secretary from 1541.
Philip's martial training was undertaken by his governor, Juan de Zúñiga, a Castilian nobleman who served as the commendador mayor of Castile. The practical lessons in warfare was overseen by the Duke of Alba during the Italian Wars. Philip was present at the Siege of Perpignan in 1542, but did not see action as the Spanish army under Alba decisively defeated the besieging French forces under the Dauphin of France. On his way back to Castile, Philip received the oath of allegiance of the Aragonese Cortes at Monzón. His political training had begun a year previously under his father, who had found his son studious, grave, and prudent beyond his years, and having decided to train and initiate him in the government of Spain. The king-emperor's interactions with his son during his stay in Spain convinced him of Philip's precocity in statesmanship, and so he determined to leave in his hands the regency of Spain in 1543. Philip, who had previously been made the Duke of Milan in 1540, began governing the most extensive empire in the world at the young age of sixteen.
Charles left Philip with experienced advisors—notably the secretary Francisco de los Cobos and the general Duke of Alba. Philip was also left with extensive written instructions which emphasised "piety, patience, modesty, and distrust." These principles of Charles were gradually assimilated by his son, who would grow up to become grave, self-possessed and cautious. Personally, Philip spoke softly, and had an icy self-mastery; in the words of one of his ministers, "he had a smile that cut like a sword."
Domestic policy.
After living in the Netherlands in the early years of his reign, Philip II decided to return to Spain. Although sometimes described as an absolute monarch, Philip faced many constitutional constraints on his authority. This was largely influenced by the growing strength of the bureaucracy during Philip's reign.
The Spanish Empire was not a single monarchy with one legal system but a federation of separate realms, each jealously guarding its own rights against those of the House of Habsburg. In practice, Philip often found his authority over-ruled by local assemblies, and his word less effective than that of local lords.
Philip carried several titles including Prince of Asturias as heir to the Spanish kingdoms and empire. The newest constituent kingdom in the empire was Navarre, a realm invaded by Ferdinand II of Aragon mainly with Castilian troops (1512), and annexed to Castile with an ambiguous status (1513). War across Navarre continued until 1528 (Treaties of Madrid and Cambrai). Charles V proposed to end hostilities with King Henry II of Navarre—the legitimate monarch of Navarre—by marrying his son Philip to the heiress of Navarre, Jeanne III of Navarre. The marriage would provide a dynastic solution to instability in Navarre, it would make him king of all Navarre and prince of independent Béarn, as well as lord of a large part of southern France. However, the French nobility under Francis I opposed the arrangement, and successfully ended the prospects of marriage between the heirs of Habsburg and Albret in 1541.
In his will Charles stated his doubts over Navarre and recommended his son to give the kingdom back. Both King Charles and his son Philip II failed to abide by the elective (contractual) nature of the Crown of Navarre, and took the kingdom for granted. This sparked mounting tension not only with King Henry II of Navarre and Queen Jeanne III of Navarre, but with the Parliament of the Spanish Navarre ("Cortes", "The Three States") and the "Diputación" for breach of the realm specific laws (fueros)—violation of the "pactum subjectionis" as ratified by Ferdinand. Tensions in Navarre came to a head in 1592 after several years of disagreements over the agenda of the intended parliamentary session.
In November 1592, the Parliament ("Cortes") of Aragón revolted against another breach of the realm specific laws, so the Attorney General ("Justicia") of the kingdom Juan de Lanuza was executed on Philip II's orders, with his secretary Antonio Perez taking to exile in France. In Navarre the major strongholds of the kingdom were garrisoned by troops alien to the kingdom (Castilians) in conspicuous violation of the laws of Navarre, and the Parliament had long been refusing to pledge loyalty to Philip II's son and heir apparent without a proper ceremony. On 20 November 1592 a ghostly Parliament session was called pushed by Philip II, who had arrived in Pamplona at the head of an unspecified military force, and one only point on his agenda—attendance to the session was kept blank on the minutes: unlawful appointments of trusted Castilian officials and an imposition of his son as future king of Navarre at the Santa Maria Cathedral. A ceremony was held before the bishop of Pamplona (22 November), but its customary procedure and terms were altered. Protests erupted in Pamplona, but they were quelled.
Philip II also grappled with the problem of the large Morisco population in Spain, who were sometimes forcibly converted to Christianity by his predecessors. In 1569, the Morisco Revolt broke out in the southern province of Granada in defiance of attempts to suppress Moorish customs; and Philip ordered the expulsion of the Moriscos from Granada and their dispersal to other provinces.
Despite its immense dominions, Spain was a country with a sparse population that yielded a limited income to the crown (in contrast to France, for example, which was much more heavily populated). Philip faced major difficulties in raising taxes, the collection of which was largely farmed out to local lords. He was able to finance his military campaigns only by taxing and exploiting the local resources of his empire. The flow of income from the New World proved vital to his militant foreign policy, but nonetheless his exchequer several times faced bankruptcy.
Philip's reign saw a flourishing of cultural excellence in Spain, the beginning of what is called the "Golden Age", creating a lasting legacy in literature, music, and the visual arts.
Economy.
Charles V had left Philip with a debt of about 36 million ducats and an annual deficit of 1 million ducats. This debt caused Phillip II to default on loans in 1557, 1560, 1575, and 1596 (including debt to Poland, known as Neapolitan sums). This happened because the lenders had no power over the king and could not force him to repay his loans. These defaults were just the beginning of Spain's economic troubles as Spain's kings would default six more times in the next 65 years. Aside from reducing state revenues for overseas expeditions, the domestic policies of Philip II further burdened Spain, and would, in the following century, contribute to its decline, as maintained by some historians.
Spain was subject to different assemblies: the Cortes in Castile along with the assembly in Navarre and one each for the three regions of Aragon, which preserved traditional rights and laws from the time when they were separate kingdoms. This made Spain and its possessions difficult to rule, unlike France which, while divided into regional states, had a single Estates-General. The lack of a viable supreme assembly led to power defaulting into Philip's hands, especially as manager and final arbiter of the constant conflict between different authorities. To deal with the difficulties arising from this situation, authority was administered by local agents appointed by the crown and viceroys carrying out crown instructions. Philip felt it necessary to be involved in the detail and presided over specialised councils for state affairs, finance, war, and the Inquisition.
He played groups against each other, leading to a system of checks and balances that managed affairs inefficiently, even to the extent of damaging state business, as in the Perez affair. Following a fire in Valladolid in 1561, he resisted calls to move his Court to Lisbon, an act that could have curbed centralisation and bureaucracy domestically as well as relaxed rule in the Empire. Instead, with the traditional Royal and Primacy seat of Toledo now essentially obsolete, Philip moved his Court to the Castilian stronghold of Madrid. Except for a brief period under Philip III, Madrid has remained the capital of Spain to the present day.
Whereas his father had been forced to an itinerant rule as a medieval king, Philip ruled at a critical turning point in European history toward modernity. He mainly directed state affairs, even when not at Court. Indeed, when his health began failing, he worked from his quarters in the Palace-Monastery-Pantheon of El Escorial he had built. But Philip did not enjoy the supremacy that Louis XIV of France would in the next century, nor was such a rule necessarily possible at his time. The inefficiencies of the Spanish state and restrictively regulated industry under his rule were common to many contemporary countries. Further, the dispersal of the Moriscos from Granada – motivated by the fear they might support a Muslim invasion – had serious negative economic effects, particularly in that region.
Foreign policy.
Philip's foreign policies were determined by a combination of Catholic fervour and dynastic objectives. He considered himself the chief defender of Catholic Europe, both against the Ottoman Turks and against the forces of the Protestant Reformation. He never relented from his fight against heresy, defending the Catholic faith and limiting freedom of worship within his territories. These territories included his patrimony in the Netherlands, where Protestantism had taken deep root. Following the Revolt of the Netherlands in 1568, Philip waged a campaign against Dutch heresy and secession. It also dragged in the English and the French at times and expanded into the German Rhineland with the Cologne War. This series of conflicts lasted for the rest of his life. Philip's constant involvement in European wars took a significant toll on the treasury and played a huge role in leading the Crown into economic difficulties and even bankruptcies.
In 1588, the English defeated Philip's Spanish Armada, thwarting his planned invasion of the country to reinstate Catholicism. But the war continued for the next sixteen years, in a complex series of struggles that included France, Ireland and the main battle zone, the Low Countries. It would not end until all the leading protagonists, including himself, had died. Earlier, however, after several setbacks in his reign and especially that of his father, Philip did achieve a decisive victory against the Turks at the Lepanto in 1571, with the allied fleet of the Holy League, which he had put under the command of his illegitimate brother, John of Austria. He also successfully secured his succession to the throne of Portugal.
With regard to Philip's overseas possessions, in response to the reforms imposed by the Ordenanzas, extensive questionnaires were distributed to every major town and region in New Spain called relaciones geográficas. These surveys helped the Spanish monarchy to govern these overseas conquests more effectively.
Italy.
Charles V formally abdicated the throne of Naples to Philip on 25 July 1554, and the young king was invested with the kingdom (officially called "Naples and Sicily") on 2 October by Pope Julius III. The date of Charles' abdication of the throne of Sicily is uncertain, but Philip was invested with this kingdom (officially "Sicily and Jerusalem") on 18 November 1554 by Julius. In 1556, Philip decided to declare war in the Papal States and temporarily gobbled up territory there, perhaps in response to Pope Paul IV's anti-Spanish outlook. According to Philip II, he was doing it for the benefit of the Church.
In a letter from Francisco de Vargas to the Princess Dowager of Portugal, Regent of Spain, dated 22 September 1556, it is written:
Pope Paul IV charged a seven-member commission with preparing a peace agreement. The efforts were later abandoned and the war continued. On 27 August 1557, Fernando Alvarez de Toledo, Duke of Alba and Viceroy of Naples, was at the walls of Rome, ready to lead his troops for a final assault. On 13 September 1557, Cardinal Carlo Carafa signed a peace agreement, accepting all of the duke's conditions.
Philip led Spain into the final phase of the Italian Wars. The Spanish army decisively defeated the French at St. Quentin in 1557 and at Gravelines in 1558. The resulting Treaty of Cateau-Cambresis in 1559 secured Piedmont, Savoy, and Corsica for the Spanish allied states, the Duchy of Savoy, and the Republic of Genoa. France recognised Spanish control over the Franche-Comté, but, more importantly, the treaty also confirmed the direct control of Philip over Milan, Naples, Sicily, Sardinia, and the State of Presidi, and indirectly (through his dominance of the rulers of Tuscany, Genoa, and other minor states) of all Italy. The Pope was a natural Spanish ally. The only truly independent entities on Italian soil were the allied Duchy of Savoy and the Republic of Venice. Spanish control of Italy would last until the early eighteenth century. Ultimately, the treaty ended the 60-year, Franco-Spanish wars for supremacy in Italy.
By the end of the wars in 1559, Habsburg Spain had been established as the premier power of Europe, to the detriment of France. In France, Henry II was fatally wounded in a joust held during the celebrations of the peace. His death led to the accession of his 15-year-old son Francis II, who in turn soon died. The French monarchy was thrown into turmoil, which increased further with the outbreak of the French Wars of Religion that would last for several decades. The states of Italy were reduced to second-rate powers and Milan and Naples were annexed directly to Spain. Mary Tudor's death in 1558 enabled Philip to seal the treaty by marrying Henry II's daughter, Elisabeth of Valois, later giving him a claim to the throne of France on behalf of his daughter by Elisabeth, Isabel Clara Eugenia.
France.
The French Wars of Religion (1562–98) were primarily fought between French Catholics and Protestants (Huguenots). The conflict involved the factional disputes between the aristocratic houses of France, such as the House of Bourbon and House of Guise (Lorraine), and both sides received assistance from foreign sources.
Philip signed the Treaty of Vaucelles with Henry II of France in 1556. Based on the terms of the treaty, the territory of the Franche-Comté was to be relinquished to Philip. However, the treaty was broken shortly afterwards. France and Spain waged war in northern France and Italy over the following years. Spanish victory at St. Quentin and Gravelines led to the Treaty of Cateau-Cambresis in which France recognised Spanish sovereignty over the Franche-Comté.
During the War of the Portuguese Succession, the pretender António fled to France following his defeats and, as Philip’s armies had not yet occupied the Azores, he sailed there with a large Anglo-French fleet under Filippo Strozzi, a Florentine exile in the service of France. The naval Battle of Terceira took place on 26 July 1582, in the sea near the Azores, off São Miguel Island, as part of the War of the Portuguese Succession and the Anglo-Spanish War (1585–1604). The Spanish navy defeated the combined Anglo-French fleet that had sailed to preserve control of the Azores under António. The French naval contingent was the largest French force sent overseas before the age of Louis XIV.
The Spanish victory at Terceira was followed by the Battle of the Azores between the Portuguese loyal to the claimant António, supported by French and English troops, and the Spanish-Portuguese forces loyal to Philip commanded by the admiral Don Álvaro de Bazán. Victory in Azores completed the incorporation of Portugal into the Spanish Empire.
Philip financed the Catholic League during the French Wars of Religion. He directly intervened in the final phases of the wars (1589–1598), ordering the Duke of Parma into France in an effort to unseat Henry IV, and perhaps dreaming of placing his favourite daughter, Isabel Clara Eugenia, on the French throne. Philip's third wife and Isabella's mother Elisabeth had already ceded any claim to the French Crown with her marriage to Philip. However the "Parlement de Paris", in power of the Catholic party, gave verdict that Isabella Clara Eugenia was "the legitimate sovereign" of France. Philip's interventions in the fighting – sending the Duke of Parma, to end Henry IV's siege of Paris in 1590 – and the siege of Rouen in 1592 contributed in saving the French Catholic Leagues's cause against a Protestant monarchy.
In 1593, Henry agreed to convert to Catholicism; weary of war, most French Catholics switched to his side against the hardline core of the Catholic League, who were portrayed by Henry's propagandists as puppets of a foreign monarch, Philip. By the end of 1594 certain League members were still working against Henry across the country, but all relied on the support of Spain. In January 1595, therefore, Henry officially declared war on Spain, to show Catholics, that Philip was using religion as a cover for an attack on the French state, and Protestants, that he had not become a puppet of Spain through his conversion, while hoping to take the war to Spain and make territorial gain.
French victory at the Battle of Fontaine-Française marked an end to the Catholic League in France. Spain launched a concerted offensive in 1595, taking Doullens, Cambrai and Le Catelet and in the spring of 1596 capturing Calais by April. Following the Spanish capture of Amiens in March 1597 the French crown laid siege to it until it managed to reconquer Amiens from the overstretched Spanish forces in September 1597. Henry then negotiated a peace with Spain. The war was only drawn to an official close, however, after the Edict of Nantes, with the Peace of Vervins in May 1598.
The 1598 Treaty of Vervins was largely a restatement of the 1559 Peace of Câteau-Cambrésis and Spanish forces and subsidies were withdrawn; meanwhile, Henry issued the Edict of Nantes, which offered a high degree of religious toleration for French Protestants. The military interventions in France thus ended in an ironic fashion for Philip: they had failed to oust Henry from the throne or suppress Protestantism in France and yet they had played a decisive part in helping the French Catholic cause gain the conversion of Henry, ensuring that Catholicism would remain France's official and majority faith – matters of paramount importance for the devoutly Catholic Spanish king.
Mediterranean.
In the early part of his reign Philip was concerned with the rising power of the Ottoman Empire under Suleiman the Magnificent. Fear of Islamic domination in the Mediterranean caused him to pursue an aggressive foreign policy.
In 1558, Turkish admiral Piyale Pasha captured the Balearic Islands, especially inflicting great damage on Minorca and enslaving many, while raiding the coasts of the Spanish mainland. Philip appealed to the Pope and other powers in Europe to bring an end to the rising Ottoman threat. Since his father's losses against the Ottomans and against Hayreddin Barbarossa in 1541, the major European sea powers in the Mediterranean, namely Spain and Venice, became hesitant in confronting the Ottomans. The myth of "Turkish invincibility" was becoming a popular story, causing fear and panic among the people.
In 1560, Philip II organised a "Holy League" between Spain and the Republic of Venice, the Republic of Genoa, the Papal States, the Duchy of Savoy and the Knights of Malta. The joint fleet was assembled at Messina and consisted of 200 ships (60 galleys and 140 other vessels) carrying a total of 30,000 soldiers under the command of Giovanni Andrea Doria, nephew of the famous Genoese admiral Andrea Doria.
On 12 March 1560, the Holy League captured the island of Djerba which had a strategic location and could control the sea routes between Algiers and Tripoli. As a response, Suleiman the Magnificent sent an Ottoman fleet of 120 ships under the command of Piyale Pasha, which arrived at Djerba on 9 May 1560. The battle lasted until 14 May 1560, and the forces of Piyale Pasha and Turgut Reis (who joined Piyale Pasha on the third day of the battle) had an overwhelming victory at the Battle of Djerba.
The Holy League lost 60 ships (30 galleys) and 20,000 men, and Giovanni Andrea Doria was barely able to escape with a small vessel. The Ottomans retook the Fortress of Djerba, whose Spanish commander, D. Álvaro de Sande attempted to escape with a ship but was followed and eventually captured by Turgut Reis. In 1565 the Ottomans sent a large expedition to Malta, which laid siege to several forts on the island, taking some of them. The Spanish sent a relief force, which finally drove the Ottoman army out of the island.
The grave threat posed by the increasing Ottoman domination of the Mediterranean was reversed in one of history's most decisive battles, with the destruction of nearly the entire Ottoman fleet at the Battle of Lepanto in 1571, by the Holy League under the command of Philip's half brother, Don Juan of Austria. A fleet sent by Philip, again commanded by Don John, reconquered Tunis from the Ottomans in 1573. However, the Turks soon rebuilt their fleet and in 1574 Uluç Ali Reis managed to recapture Tunis with a force of 250 galleys and a siege which lasted 40 days. However, Lepanto marked a permanent reversal in the balance of naval power in the Mediterranean and the end of the threat of Ottoman control.
In 1585 a peace treaty was signed with the Ottomans.
Revolt in the Netherlands.
Philip's rule in the seventeen separate provinces known collectively as the Netherlands faced many difficulties; this led to open warfare in 1568. He insisted on direct control over events in the Netherlands despite being over two weeks' ride away in Madrid. There was discontent in the Netherlands about Philip's taxation demands, and the incessant persecution of Protestants. In 1566, Protestant preachers sparked anti-clerical riots known as the Iconoclast Fury; in response to growing Protestant influence, the Duke of Alba's army went on the offensive, further alienating the local aristocracy. In 1572 a prominent exiled member of the Dutch aristocracy, William the Silent (Prince of Orange), invaded the Netherlands with a Protestant army, but he only succeeded in holding two provinces, Holland and Zeeland.
The war continued. The States General of the northern provinces, united in the 1579 Union of Utrecht, passed an Act of Abjuration declaring that they no longer recognised Philip as their king. The southern Netherlands (what is now Belgium and Luxembourg) remained under Spanish rule. In 1584, William the Silent was assassinated by Balthasar Gérard, after Philip had offered a reward of 25,000 crowns to anyone who killed him, calling him a "pest on the whole of Christianity and the enemy of the human race". The Dutch forces continued to fight on under Orange's son Maurice of Nassau, who received modest help from Queen Elizabeth I in 1585. The Dutch gained an advantage over the Spanish because of their growing economic strength, in contrast to Philip's burgeoning economic troubles. The war, known as the Eighty Years' War, only came to an end in 1648, when the Dutch Republic was recognised by Spain as independent.
King of Portugal.
In 1578 young king Sebastian of Portugal died at the Battle of Alcácer Quibir without descendants, triggering a succession crisis. His granduncle, the elderly Cardinal Henry, succeeded him as King, but Henry also had no descendants, having taken holy orders. When the Cardinal-King died two years after Sebastian's disappearance, three grandchildren of Manuel I claimed the throne: Infanta Catarina, Duchess of Braganza, António, Prior of Crato, and Philip II of Spain. António was acclaimed King of Portugal in many cities and towns throughout the country, but members of the Council of Governors of Portugal who had supported Philip escaped to Spain and declared him to be the legal successor of Henry.
Philip II then marched into Portugal and defeated Prior António's troops in the Battle of Alcântara. The troops commanded by the 3rd Duke of Alba imposed subjection to Philip before entering Lisbon, where he seized an immense treasure. Philip II of Spain was crowned "Philip I" of Portugal in 1581 (recognised as king by the Portuguese Cortes of Tomar) and a sixty-year personal union under the rule of the Philippine Dynasty began. When Philip left for Madrid in 1583, he made his nephew Albert of Austria his viceroy in Lisbon. In Madrid he established a Council of Portugal to advise him on Portuguese affairs, giving excellent positions to Portuguese nobles in the Spanish courts, and allowing Portugal to maintain autonomous law, currency, and government.
Relations with England and Ireland.
King of England and Ireland.
Philip's father arranged his marriage to 37-year-old Queen Mary I of England, Charles' maternal first cousin. To elevate Philip to Mary's rank, his father ceded the crown of Naples, as well as his claim to the Kingdom of Jerusalem, to him.
Their marriage at Winchester Cathedral on 25 July 1554 took place just two days after their first meeting. Philip's view of the affair was entirely political. Lord Chancellor Gardiner and the House of Commons petitioned Mary to consider marrying an Englishman, preferring Edward Courtenay.
Under the terms of the Act for the Marriage of Queen Mary to Philip of Spain, Philip was to enjoy Mary I's titles and honours for as long as their marriage should last. All official documents, including Acts of Parliament, were to be dated with both their names, and Parliament was to be called under the joint authority of the couple. Coins were also to show the heads of both Mary and Philip. The marriage treaty also provided that England would not be obliged to provide military support to Philip's father in any war. The Privy Council instructed that Philip and Mary should be joint signatories of royal documents, and this was enacted by an Act of Parliament, which gave him the title of king and stated that he "shall aid her Highness ... in the happy administration of her Grace’s realms and dominions." In other words, Philip was to co-reign with his wife. As the new King of England could not read English, it was ordered that a note of all matters of state should be made in Latin or Spanish.
Acts which made it high treason to deny Philip's royal authority were passed in Ireland and England. Philip and Mary appeared on coins together, with a single crown suspended between them as a symbol of joint reign. The Great Seal shows Philip and Mary seated on thrones, holding the crown together. The coat of arms of England was impaled with Philip's to denote their joint reign. During their joint reign, they waged war against France, which resulted in the loss of Calais, England's last remaining possession in France.
Philip's wife had succeeded to the Kingdom of Ireland, but the title of King of Ireland had been created in 1542 by Henry VIII after he was excommunicated, and so it was not recognised by Catholic monarchs. In 1555, Pope Paul IV rectified this by issuing a papal bull recognising Philip and Mary as rightful King and Queen of Ireland. King's County and Philipstown in Ireland were named after Philip as King of Ireland in 1556.
The couple's joint royal style after Philip ascended the Spanish throne in 1556 was: "Philip and Mary, by the Grace of God King and Queen of England, Spain, France, Jerusalem, both the Sicilies and Ireland, Defenders of the Faith, Archdukes of Austria, Dukes of Burgundy, Milan and Brabant, Counts of Habsburg, Flanders and Tirol".
However, the couple had no children. Mary died in 1558 before the union could revitalise the Roman Catholic Church in England. With her death, Philip lost his rights to the English throne (including the ancient English claims to the French throne) and ceased to be King of England, Ireland and (as claimed by them) France.
Philip's distaff great-grandson, Philippe I, Duke of Orléans, married Princess Henrietta of England in 1661; in 1807, the Jacobite claim to the British throne passed to the descendants of their child Anne Marie d'Orléans.
After Mary I's death.
Upon Mary's death, the throne went to Elizabeth I. Philip had no wish to sever his tie with England, and had sent a proposal of marriage to Elizabeth. However, she delayed in answering, and in that time learned Philip was also considering a Valois alliance. Elizabeth I was the Protestant daughter of Henry VIII and Anne Boleyn. This union was deemed illegitimate by English Catholics, who disputed the validity of both the annulment of Henry's marriage to Catherine of Aragon and of his subsequent marriage to Boleyn, and hence claimed that Mary, Queen of Scots, the Catholic great granddaughter of Henry VII, was the legitimate heir to the throne.
For many years Philip maintained peace with England, and even defended Elizabeth from the Pope's threat of excommunication. This was a measure taken to preserve a European balance of power. Ultimately, Elizabeth allied England with the Protestant rebels in the Netherlands. Further, English ships began a policy of piracy against Spanish trade and threatened to plunder the great Spanish treasure ships coming from the new world. English ships went so far as to attack a Spanish port. The last straw for Philip was the Treaty of Nonsuch signed by Elizabeth in 1585 – promising troops and supplies to the rebels. Although it can be argued this English action was the result of Philip's Treaty of Joinville with the Catholic League of France, Philip considered it an act of war by England.
The execution of Mary, Queen of Scots, in 1587 ended Philip's hopes of placing a Catholic on the English throne. He turned instead to more direct plans to invade England, with vague plans to return the country to Catholicism. In 1588, he sent a fleet, the Spanish Armada, to rendezvous with the Duke of Parma's army and convey it across the English Channel. However, the operation had little chance of success from the beginning, because of lengthy delays, lack of communication between Philip II and his two commanders and the lack of a deep bay for the fleet. At the point of attack, a storm struck the English Channel, already known for its harsh currents and choppy waters, which devastated large numbers of the Spanish fleet. There was a tightly fought battle against the English navy; it was by no means a slaughter, but the Spanish were forced into a retreat, and the overwhelming majority of the Armada was destroyed by the harsh weather.
Eventually, three more Armadas were assembled; two were sent to England in 1596 and 1597, but both also failed; the third (1599) was diverted to the Azores and Canary Islands to fend off raids. This Anglo-Spanish War (1585–1604) would be fought to a grinding end, but not until both Philip II (d. 1598) and Elizabeth I (d. 1603) were dead.
The defeat of the Spanish Armada gave great heart to the Protestant cause across Europe. The storm that smashed the Armada was seen by many of Philip's enemies as a sign of the will of God. Many Spaniards blamed the admiral of the Armada for its failure, but Philip, despite his complaint that he had sent his ships to fight the English, not the elements, was not among them. A year later, Philip remarked:
The Spanish navy was rebuilt, and intelligence networks were improved. A measure of the character of Philip can be gathered by the fact that he personally saw to it that the wounded men of the Armada were treated and received pensions, and that the families of those who died were compensated for their loss, which was highly unusual for the time.
While the invasion had been averted, England was unable to take advantage of this success. An attempt to use her newfound advantage at sea with a counter armada the following year failed disastrously. Likewise, English buccaneering and attempts to seize territories in the Caribbean were defeated by Spain's rebuilt navy and their improved intelligence networks (although Cádiz was destroyed by an Anglo-Dutch force after a failed attempt to seize the treasure fleet).
Death.
Philip II died in El Escorial, near Madrid, on 13 September 1598 of cancer. His mattress was bored through to leave a gap so he could defecate when his bowel problems became severe.
He was succeeded by his son/grandnephew Philip III.
Legacy.
Under Philip II, Spain reached the peak of its power. However, in spite of the great and increasing quantities of gold and silver flowing into his coffers from the American mines, the riches of the Portuguese spice trade, and the enthusiastic support of the Habsburg dominions for the Counter-Reformation, he would never succeed in suppressing Protestantism or defeating the Dutch rebellion. Early in his reign, the Dutch might have laid down their weapons if he had desisted in trying to suppress Protestantism, but his devotion to Catholicism would not permit him to do so. He was a devout Catholic and exhibited the typical 16th century disdain for religious heterodoxy; he said, "Before suffering the slightest damage to religion in the service of God, I would lose all of my estates and a hundred lives, if I had them, because I do not wish nor do I desire to be the ruler of heretics."
The defeat of Protestantism was always keen in Philip's mind. For a while, he ruled England jointly with Queen Mary Tudor and a reconciliation with the Catholic Church followed. Heresy trials were reestablished and hundreds of Protestants burned at the stake. England and Philip parted ways after the death of his Queen, nicknamed "Bloody Mary". Philip's gravest mistake over the long run was his attempt to violently eradicate Protestantism from the Netherlands which was a major economic asset for the empire. Under harsh occupation, the Dutch finally rebelled and wrested independence after an eighty-year war, the strain of which did Philip's realm little good. His greatest battlefield accomplishment was the defeat of the Ottoman fleet at Lepanto which turned the tide against Turkish aggression.
As he strove to enforce Catholic orthodoxy through an intensification of the Inquisition, students were barred from studying elsewhere and books printed by Spaniards outside the kingdom were banned. Even a highly respected churchman like Archbishop Carranza of Toledo was jailed by the Inquisition for seventeen years for publishing ideas that seemed sympathetic in some degree to Protestantism. Such strict enforcement of orthodox belief was successful and Spain avoided the religiously inspired strife tearing apart other European dominions.
Yet the School of Salamanca flourished under his reign. Martín de Azpilcueta, highly honoured at Rome by several popes, and looked on as an oracle of learning, published his "Manuale sive Enchiridion Confessariorum et Poenitentium" (Rome, 1568), long a classical text in the schools and in ecclesiastical practice. Francisco Suárez, generally regarded as the greatest scholastic after Thomas Aquinas and regarded during his lifetime as being the greatest living philosopher and theologian, was writing and lecturing, not only in Spain but also in Rome (1680–1685), where Pope Gregory XIII attended the first lecture that he gave. Luis de Molina published his "De liberi arbitrii cum gratiae donis, divina praescientia, praedestinatione et reprobatione concordia" (1588), wherein he put forth the doctrine attempting to reconcile the omniscience of God with human free will that came to be known as Molinism, thereby contributing to what was one of the most important intellectual debates of the time; Molinism became the "de facto" Jesuit doctrine on the aforementioned matters, and is still advocated today by William Lane Craig and Alvin Plantinga, among others.
Because Philip II was the most powerful European monarch in an era of war and religious conflict, evaluating both his reign and the man himself has become a controversial historical subject. Even before his death in 1598, his supporters had started presenting him as an archetypical gentleman, full of piety and Christian virtues, whereas his enemies depicted him as a fanatical and despotic monster, keen in inhuman cruelties and barbarism. This dichotomy, further developed into the so-called Spanish Black Legend and White Legend, was helped by King Philip himself. Philip prohibited any biographical account of his life to be published while he was alive, and he ordered that all his private correspondence be burned shortly before he died. Moreover, Philip did nothing to defend himself after being betrayed by his ambitious secretary Antonio Perez, who published incredible calumnies against his former master; this allowed Perez's tales to spread all around Europe unchallenged. That way, the popular image of the king that survives to today was created on the eve of his death, at a time when many European princes and religious leaders were turned against Spain as a pillar of the Counter-Reformation. This means that many histories depict Philip from deeply prejudiced points of view, usually negative. However, some historians classify this anti-Spanish analysis as part of the Black Legend. In a more recent example of popular culture, Philip II's portrayal in "Fire Over England" (1937) is not entirely unsympathetic; he is shown as a very hard working, intelligent, religious, somewhat paranoid ruler whose prime concern is his country but who had no understanding of the English, despite his former co-monarchy there.
Even in countries that remained Catholic, primarily France and the Italian states, fear and envy of Spanish success and domination created a wide receptiveness for the worst possible descriptions of Philip II. Although some efforts have been made to separate legend from reality, that task has been proven to be extremely hard, since many prejudices are rooted in the cultural heritage of European countries. Spanish-speaking historians tend to assess his political and military achievements, sometimes deliberately avoiding issues such as the king's lukewarmness (or even support) towards Catholic fanaticism. English-speaking historians tend to show Philip II as a fanatical, despotical, criminal, imperialist monster, minimising his military victories (Battle of Lepanto, Battle of Saint Quentin, etc.) to mere anecdotes, and magnifying his defeats (namely the Invincible Armada) even though at the time those defeats did not result in great political or military changes in the balance of power in Europe. Moreover, it has been noted that objectively assessing Philip's reign would suppose to re-analyze the reign of his greatest opponents, namely England's Queen Elizabeth I and the Dutch William the Silent, who are popularly regarded as great heroes in their home nations; if Philip II is to be shown to the English or Dutch public in a more favourable light, Elizabeth and William would lose their cold-blooded, fanatical enemy, thus decreasing their own patriotic accomplishments.
Philip II's reign can hardly be characterised by its failures. He ended French Valois ambitions in Italy and brought about the Habsburg ascendency in Europe. He commenced settlements in the Philippines, which were named after him, and established the first trans-Pacific trade route between America and Asia. He secured the Portuguese kingdom and empire. He succeeded in increasing the importation of silver in the face of English, Dutch, and French privateers, overcoming multiple financial crises and consolidating Spain's overseas empire. Although clashes would be ongoing, he ended the major threat posed to Europe by the Ottoman navy.
Titles, honours, and styles.
Philip continued his father's style of "Majesty" (Latin: "Maiestas"; Spanish: "Majestad") in preference to that of "Highness" ("Celsitudo"; "Alteza"). In diplomatic texts, he continued the use of the title "Most Catholic" ("Rex Catholicismus"; "Rey Católico") first bestowed by Pope Alexander VI on Ferdinand and Isabella in 1496.
Following the Act of Parliament sanctioning his marriage with Mary, the couple was styled "Philip and Mary, by the grace of God King and Queen of England, France, Naples, Jerusalem, and Ireland, Defenders of the Faith, Princes of Spain and Sicily, Archdukes of Austria, Dukes of Milan, Burgundy and Brabant, Counts of Habsburg, Flanders and Tyrol". Upon his inheritance of Spain in 1556, they became "Philip and Mary, by the grace of God King and Queen of England, Spain, France, both the Sicilies, Jerusalem and Ireland, Defenders of the Faith, Archdukes of Austria, Dukes of Burgundy, Milan and Brabant, Counts of Habsburg, Flanders and Tyrol".
In the 1584 Treaty of Joinville, he was styled "Philip, by the grace of God second of his name, king of Castille, Leon, Aragon, Portugal, Navarre, Naples, Sicily, Jerusalem, Majorca, Sardinia, and the islands, Indies, and "terra firma" of the Ocean Sea; archduke of Austria; duke of Burgundy, Lothier, Brabant, Limbourg, Luxembourg, Guelders, and Milan; Count of Habsburg, Flanders, Artois, and Burgundy; Count Palatine of Hainault, Holland and Zeeland, Namur, Drenthe, Zutphen; prince of "Zvuanem"; marquis of the Holy Roman Empire; lord of Frisia, Salland, Mechelen, and of the cities, towns, and lands of Utrecht, Overissel, and Groningen; master of Asia and Africa".
His coinage typically bore the obverse inscription "PHS·D:G·HISP·Z·REX" (Latin: "Philip, by the grace of God King of Spain et cetera"), followed by the local title of the mint ("DVX·BRA" for Duke of Brabant, "C·HOL" for Count of Holland, "D·TRS·ISSV" for Lord of Overissel, &c.). The reverse would then bear a motto such as "PACE·ET·IVSTITIA" ("For Peace and Justice") or "DOMINVS·MIHI·ADIVTOR" ("The Lord is my helper"). A medal struck in 1583 bore the inscriptions "PHILIPP II HISP ET NOVI ORBIS REX" ("Philip II, King of Spain and the New World") and "NON SUFFICIT ORBIS" ("The world is not enough").
Family.
Philip was married four times and had children with three of his wives.
Philip's first wife was his first cousin, Maria Manuela, Princess of Portugal. She was a daughter of Philip's maternal uncle, John III of Portugal, and paternal aunt, Catherine of Austria. The marriage produced one son in 1545, after which Maria died 4 days later due to haemorrhage:
Philip's second wife was his first cousin once removed, Queen Mary I of England. The 1554 marriage to Mary was political. By this marriage, Philip became "jure uxoris" King of England and Ireland, although the couple was apart more than together as they ruled their respective countries. The marriage produced no children and Mary died in 1558, ending Philip's reign in England and Ireland.
Philip's third wife was Elisabeth of Valois, the eldest daughter of Henry II of France and Catherine de' Medici. She was also a distant relation of Philip-she was descended from their mutual ancestor Alfonso VII of León and Castile. During their marriage (1559–1568) they conceived five daughters, though only two of the girls survived. Elisabeth died a few hours after the loss of her last child. Their children were:
Philip's fourth and final wife was his sororal niece, Anna of Austria. By contemporary accounts, this was a convivial and satisfactory marriage (1570–1580) for both Philip and Anna. This marriage produced four sons and one daughter. Anna died of heart failure 8 months after giving birth to Maria in 1580. Their children were:

</doc>
<doc id="54390" url="https://en.wikipedia.org/wiki?curid=54390" title="Disassembler">
Disassembler

A disassembler is a computer program that translates machine language into assembly language—the inverse operation to that of an assembler. A disassembler differs from a decompiler, which targets a high-level language rather than an assembly language. Disassembly, the output of a disassembler, is often formatted for human-readability rather than suitability for input to an assembler, making it principally a reverse-engineering tool.
Assembly language source code generally permits the use of constants and programmer comments. These are usually removed from the assembled machine code by the assembler. If so, a disassembler operating on the machine code would produce disassembly lacking these constants and comments; the disassembled output becomes more difficult for a human to interpret than the original annotated source code. Some disassemblers make use of the symbolic debugging information present in object files such as ELF. The Interactive Disassembler allows the human user to make up mnemonic symbols for values or regions of code in an interactive session: human insight applied to the disassembly process often parallels human creativity in the code writing process.
Disassembly is not an exact science: on CISC platforms with variable-width instructions, or in the presence of self-modifying code, it is possible for a single program to have two or more reasonable disassemblies. Determining which instructions would actually be encountered during a run of the program reduces to the proven-unsolvable halting problem.
Problems of disassembly.
Writing a disassembler which produces code which, when assembled, produces exactly the original binary is possible; however, there are often differences. This poses demands on the expressivity of the assembler. For example, an x86 assembler takes an arbitrary choice between two binary codes for something as simple as codice_1. If the original code uses the other choice, the original code simply cannot be reproduced at any given point in time. However, even when a fully correct disassembly is produced, problems remain if the program requires modification. For example, the same machine language jump instruction can be generated by assembly code to jump to a specified location (for example, to execute specific code), or to jump to a specified number of bytes (for example, to skip over an unwanted branch). A disassembler cannot know what is intended, and may use either syntax to generate a disassembly which reproduces the original binary. However, if a programmer wants to add instructions between the jump instruction and its destination, it is necessary to understand the program's operation to determine whether the jump should be absolute or relative, i.e., whether its destination should remain at a fixed location, or be moved so as to skip both the original and added instructions.
Examples of disassemblers.
A disassembler may be stand-alone or interactive. A stand-alone disassembler, when executed, generates an assembly language file which can be examined; an interactive one shows the effect of any change the user makes immediately. For example, the disassembler may initially not know that a section of the program is actually code, and treat it as data; if the user specifies that it is code, the resulting disassembled code is shown immediately, allowing the user to examine it and take further action during the same run.
Any interactive debugger will include some way of viewing the disassembly of the program being debugged. Often, the same disassembly tool will be packaged as a standalone disassembler distributed along with the debugger. For example, objdump, part of GNU Binutils, is related to the interactive debugger gdb.
Disassemblers and emulators.
A dynamic disassembler can be incorporated into the output of an emulator or hypervisor to 'trace out', line-by-line, the real time execution of any executed machine instructions. In this case, as well as lines containing the disassembled machine code, the register(s) and/or data change(s) (or any other changes of "state", such as condition codes) that each individual instruction causes can be shown alongside or beneath the disassembled instruction. This provides extremely powerful debugging information for ultimate problem resolution, although the size of the resultant output can sometimes be quite large, especially if active for an entire program's execution. OLIVER provided these features from the early 1970s as part of its CICS debugging product offering and is now to be found incorporated into the XPEDITER product from Compuware.

</doc>
<doc id="54395" url="https://en.wikipedia.org/wiki?curid=54395" title="Strike zone">
Strike zone

In baseball, the strike zone is the volume of space through which a pitch must pass in order to count as a strike (if the batter does not swing). The strike zone is defined as the volume of space above home plate and between the batter's knees and the midpoint of their torso. Whether a pitch passed through the zone or not is decided by an umpire, who is generally positioned behind the catcher.
Strikes are desirable for the pitcher and the fielding team, as three strikes result in a strikeout. A pitch that misses the strike zone is called a ball. Balls are desirable for the batter and the batting team, as four balls allow the batter to take a base on balls.
Definition.
There is more than one set of rules that govern baseball and softball. It depends on the level and league as to which set of rules are being used. The governing bodies for the different sets of rules may have slightly different definitions. As with understanding any rule discussion, you need to know which set of rules are being referenced; Official Baseball Rules (known as OBR), Federation Rules, NCAA, Little League, ASA etc.
The top of the strike zone is defined in the Major Leagues Official Rules as a horizontal line at the midpoint between the top of the batter's shoulders and the top of the uniform pants. The bottom of the strike zone is a line at the hollow beneath the kneecap. It shall be determined from the batter's stance as the batter is prepared to swing at the pitched ball. The right and left boundaries of the strike zone correspond to the edges of home plate. A pitch that touches the outer boundary of the zone is as much a strike as a pitch that is thrown right down the center. A pitch at which the batter does not swing and which does not pass through the strike zone is called a "ball" (short for "no ball"). The active tally of strikes and balls during a player's turn batting is called the count.
In practice, the strike zone is treated as a volume of space delimited by vertical planes extending up from the pentagonal boundaries of the home plate and limited at the top and bottom by upper and lower horizontal planes passing through the horizontal lines of the definition. This volume thus takes the form of a vertical right pentagonal prism located above home plate. A pitch passing outside the front of the defined volume of the strike zone but curving so as to enter this volume farther back (without being hit) is described as a "back-door strike".
Major League Baseball has occasionally increased or reduced the size of the strike zone in an attempt to control the balance of power between pitchers and hitters. After the record home run year by Roger Maris in , the major leagues increased the size of the strike zone from the top of the batter's shoulders to the bottom of his knees. In , pitchers such as Denny McLain and Bob Gibson among others dominated hitters, producing 339 shutouts. Carl Yastrzemski would be the only American League hitter to finish the season with a batting average higher than .300. In the National League, Gibson posted a 1.12 earned run average, the lowest in 54 years, while Los Angeles Dodgers pitcher Don Drysdale threw a record 58 and two-thirds consecutive scoreless innings during the 1968 season. As a result of the dropping offensive statistics, Major League Baseball took steps to reduce the advantage held by pitchers by lowering the height of the pitcher's mound from 15 inches to 10 inches, and by reducing the size of the strike zone for the season.
Although the "de facto" enforced strike zone can vary, the official rules (Rule 2.00, A STRIKE (b)) define a pitch as a strike "if any part of the ball passes through any part of the strike zone."
A batter who accumulates three strikes in a single batting appearance has struck out and is ruled out (with the exception of an uncaught third strike); a batter who accumulates four balls in a single appearance has drawn a base on balls (or "walk") and is awarded advancement to first base. In very early iterations of the rules during the 19th century, it took up to 9 balls for a batter to earn a walk; however, to make up for this, the batter could request the ball to be pitched high, low, or medium.
Enforcement.
While baseball rules provide a precise definition for the strike zone, in practice, it is up to the judgement of the umpire to decide whether the pitch passed through the zone.
Rule 9.02 of the Official Baseball Rules states that objections to judgement calls on the field, including balls and strikes, shall not be tolerated, and that any manager, coach, or player who leaves his dugout or field position to contest a judgement call will first be warned, and then ejected.
Many umpires, players and analysts, including the authors of a University of Nebraska study on the subject, believe that due to the QuesTec pitch-tracking system, the enforced strike zone in 2002–2006 was larger compared to the zone in 1996–2000 and thus closer to the rulebook definition. Some commentators, such as Tim Roberts of covers.com, believe that the zone has changed so much that some pitchers, such as Tom Glavine, have had to radically adjust their approach to pitching for strikes. In 2003, a frustrated Curt Schilling took a baseball bat to a QuesTec camera and destroyed it after a loss, saying the umpires shouldn't be changing the strike zone to match the machines.
In 2009, a new system called Zone Evaluation was implemented in all 30 Major League ballparks, replacing the QuesTec system; the new system records the ball's position in flight more than 20 times before it reaches home plate. Much of the early resistance from Major League umpires to QuesTec had diminished and the implementation of the new Zone Evaluation system in all the parks went largely unnoticed. Like the old system, the new system will be used to grade umpires on accuracy and used to determine which umpires receive postseason assignments.
"You can't pitch fastballs inside anymore, and you never get a called strike with a fastball inside", said former pitcher Gene Garber.

</doc>
<doc id="54397" url="https://en.wikipedia.org/wiki?curid=54397" title="Banawá people">
Banawá people

The Banawá (also Banawa, Banavá, Jafí, Kitiya, Banauá) are an indigenous group living along the Banawá River in the Amazonas State, Brazil. Their territory is between the Juruá and Purus Rivers. Approximately 158 Banawá people live in one major village and two smaller settlements containing a single extended family each. The Banawá, who call themselves Kitiya, speak a dialect of the Madi language.
History.
Their territory was invaded at the end of the 19th century, during the rubber boom. In the 1990s, Brazil formally recognized their land rights.

</doc>
<doc id="54398" url="https://en.wikipedia.org/wiki?curid=54398" title="Balanjar">
Balanjar

Balanjar ("Baranjar", "Belenjer", "Belendzher", "Bülünjar") was a medieval city located in the North Caucasus region, between the cities of Derbent and Samandar, probably on the lower Sulak River. It flourished from the seventh to the tenth centuries CE. The legendary founder of Balanjar, according to the Arab chroniclers Ibn al-Faqih and Abu al-Fida, was named Balanjar ibn Japheth. 
In the 630s CE Balanjar was a capital of the Baranjar state. Some scholars speculate that the name derives from the Turkic root "Bala" or "Great", and the clan-name "Endzhar". With the rest of the Baranjar domains the city became part of the Khazar Khaganate around 650; until the early 720s, Balanjar served as the capital of Khazaria. During the First Arab-Khazar War in the 650s, a Muslim army under Abd ar-Rahman ibn Rabiah was defeated outside the town (see Battle of Balanjar).
Around 722 or 723, Umayyad soldiers under al-Djarrah ibn Abdullah crossed the Caucasus Mountains and attacked Balanjar. The inhabitants of Balanjar tried to defend their town by fastening 3,000 wagons together and circling them around the key fortress on high terrain, but were defeated in the attack. The Arabs massacred much of the town's population; survivors fled to other towns, including Samandar. The victorious Arab army stole much booty and the soldiers received large sums of money.
The city was rebuilt after the war, but the capital of Khazaria was thereafter moved to Samandar and later to Atil. Nevertheless, Balanjar continued to be a city of great importance within the Khaganate. After the fall of Khazaria, Balanjar lost much of its importance and declined steadily until it vanished from the record around 1100. 
The exact location of Balanjar has not yet been established precisely. Soviet archeologist Mikhail Artamonov initially placed Balanjar on the site of the modern Daghestani city of Buynaksk, but when later the ruins of a town to the south of Makhachkala were found, he idenitified them as being those of Balanjar.

</doc>
<doc id="54399" url="https://en.wikipedia.org/wiki?curid=54399" title="TI-89 series">
TI-89 series

The TI-89 and the TI-89 Titanium are graphing calculators developed by Texas Instruments (TI). They are differentiated from most other TI graphing calculators by their computer algebra system, which allows symbolic manipulation of algebraic expressions—equations can be solved in terms of variables, whereas the TI-83/84 series can only give a numeric result.
TI-89.
The TI-89 is a graphing calculator developed by Texas Instruments in 1998. The unit features a 160×100 pixel resolution LCD screen and a large amount of flash memory, and includes TI's "Advanced Mathematics Software". The TI-89 is one of the highest model lines in TI's calculator products, along with the TI-Nspire. In the summer of 2004, the standard TI-89 was replaced by the TI-89 Titanium.
The TI-89 runs on a 16-bit microprocessor, the Motorola 68000, which nominally runs at 10, 12, or 16 MHz, depending on the calculator's hardware version. Texas Instruments has allocated 256 total kB of RAM for the unit (190 kB of which are available to the user) and 2 MB of flash memory (700 kB of which is available to the user). The RAM and Flash ROM are used to store expressions, variables, programs, tables, text files, and lists.
The TI-89 is essentially a TI-92 Plus with a limited keyboard and smaller screen. It was created partially in response to the fact that while calculators are allowed on many standardized tests, the TI-92 was considered a computer due to the QWERTY layout of its keyboard. Additionally, some people found the TI-92 unwieldy and overly large. The TI-89 is significantly smaller—about the same size as most other graphing calculators. It has a flash ROM, a feature present on the TI-92 Plus but not on the original TI-92.
User features.
The major advantage of the TI-89 over lower-model TI calculators is its built-in computer algebra system, or CAS. The calculator can evaluate and simplify algebraic expressions symbolically. For example, entering codice_1 returns formula_1. The answer is "prettyprinted" by default; that is, displayed as it would be written by hand (e.g. the aforementioned formula_1 rather than codice_2). The TI-89's abilities include:
In addition to the standard two-dimensional function plots, it can also produce graphs of parametric equations, polar equations, sequence plots, differential equation fields, and three-dimensional (two independent variable) functions.
Programming.
The TI-89 is directly programmable in a language called TI-BASIC, TI's derivative of BASIC for calculators. With the use of a PC, it is also possible to develop more complex programs in Motorola 68000 assembly language or C, translate them to machine language, and copy them to the calculator. Two software development kits for C programming are available; one is TI Flash Studio, the official TI SDK, and the other is TIGCC, a third-party SDK based on GCC.
Since the TI-89's release in 1998, thousands of programs for math, science, or entertainment have been developed. Many available games are generic clones of "Tetris", "Minesweeper", and other classic games, but some programs are more advanced: for example, a ZX Spectrum emulator, a chess-playing program, a symbolic circuit simulator, and a clone of "Link's Awakening". One of the most popular and well-known games is Phoenix. Many calculator games and other useful programs can be found on TI-program sharing sites. Ticalc.org is a major one that offers thousands of calculator programs.
Hardware versions.
There are four hardware versions of the TI-89. These versions are normally referred to as HW1, HW2, HW3, and HW4 (released in May 2006). Entering the key sequence [A displays the hardware version. Older OS versions (before 2.00) don't display anything about the hardware version unless the calculator is HW2 or later. The differences in the hardware versions are not well documented by Texas Instruments. HW1 and HW2 correspond to the original TI-89; HW3 and HW4 are only present in the TI-89 Titanium.
The most significant difference between HW1 and HW2 is in the way the calculator handles the display. In HW1 calculators there is a video buffer that stores all of the information that should be displayed on the screen, and every time the screen is refreshed the calculator accesses this buffer and flushes it to the display (direct memory access). In HW2 and later calculators, a region of memory is directly aliased to the display controller (memory-mapped I/O). This allows for slightly faster memory access, as the HW1's DMA controller used about 10% of the bus bandwidth. However, it interferes with a trick some programs use to implement grayscale graphics by rapidly switching between two or more displays (page-flipping). On the HW1, the DMA controller's base address can be changed (a single write into a memory-mapped hardware register) and the screen will automatically use a new section of memory at the beginning of the next frame. In HW2, the new page must be written to the screen by software. The effect of this is to cause increased flickering in grayscale mode, enough to make the 7-level grayscale supported on the HW1 unusable (although 4-level grayscale works on both calculators).
HW2 calculators are slightly faster because TI increased the nominal speed of the processor from 10 MHz to 12 MHz. It is believed that TI increased the speed of HW4 calculators to 16 MHz, though many users disagree about this finding.
Another difference between HW1 and HW2 calculators is assembly program size limitations. The size limitation on HW2 calculators has varied with the AMS version of the calculator. As of AMS 2.09 the limit is 24k. Some earlier versions limited assembly programs to 8k, and the earliest AMS versions had no limit. HW1 calculators have no hardware to enforce the limits, so it is easy to bypass them in software. There are unofficial patches and kernels that can be installed on HW2 calculators to remove the limitations.
TI-89 Titanium.
The TI-89 Titanium was released in the summer of 2004, and has largely replaced the popular classic TI-89. The TI-89 Titanium is referred to as HW3 and uses the corresponding AMS 3.x. In 2006, new calculators were upgraded to HW4 which was supposed to offer increases in RAM and speeds up to 16 MHz, but some benchmarks made by users reported speeds between 12.85–14.1 MHz.
The touted advantages of the TI-89 Titanium over the original TI-89 include two times the flash memory (with over three times as much available to the user). The TI-89 Titanium is essentially a Voyage 200, except it doesn't have an integrated keyboard. The TI-89 Titanium also has a USB On-The-Go port, for connectivity to other TI-89 Titanium calculators, or to a computer (to store programs or update the operating system). The TI-89 Titanium also features some pre-loaded applications, such as "CellSheet", a spreadsheet program also offered with other TI calculators. The Titanium has a slightly updated CAS, which adds a few more mathematical functions, most notably implicit differentiation. The Titanium also has a slightly differing case design from that of the TI-89 (the Titanium's case design is similar to that of the TI-84 Plus).
There are some minor compatibility issues with C and assembly programs developed for the original TI-89. Some have to be recompiled to work on the Titanium due to various small hardware changes, though in most cases the problems can be fixed by using a utility such as GhostBuster, by Olivier Armand and Kevin Kofler. This option is generally preferred as it requires no knowledge of the program, works without the need of the program's source code, is automated, and doesn't require additional computer software. In some cases, only one character needs to be changed (the ROM base on TI-89 is at 0x200000, whereas the TI-89 Titanium is at 0x800000) by hand or by patcher. Most, if not all, of these problems are caused by the mirror memory (ghost space) or lack thereof.
Use in schools.
United Kingdom.
The Joint Council for Qualifications publish examination instructions on behalf of the main examination boards in England, Wales and Northern Ireland. These instructions state that a calculator used in an examination must not be designed to offer symbolic algebra manipulation, symbolic differentiation or integration. This precludes use of the TI-89 or TI-89 Titanium in examinations, but it may be used as part of classroom study. The SQA give the same instructions for examinations in Scotland.
United States.
In the United States, the TI-89 is allowed by the College Board on all calculator-permitted tests, including the SAT, some SAT Subject Tests, and the AP Calculus, Physics, Chemistry, and Statistics exams. However, the calculator is banned from use on the ACT, the PLAN, and in some classrooms. The TI-92 series, with otherwise comparable features, has a QWERTY keyboard that results in it being classified as a computer device rather than as a calculator.

</doc>
<doc id="54403" url="https://en.wikipedia.org/wiki?curid=54403" title="Dizzy Gillespie">
Dizzy Gillespie

John Birks "Dizzy" Gillespie (; October 21, 1917 – January 6, 1993) was an American jazz trumpeter, bandleader, composer and occasional singer.
AllMusic's Scott Yanow wrote, "Dizzy Gillespie's contributions to jazz were huge. One of the greatest jazz trumpeters of all time (some would say the best), Gillespie was such a complex player that his contemporaries ended up copying Miles Davis and Fats Navarro instead, and it was not until Jon Faddis's emergence in the 1970s that Dizzy's style was successfully recreated [...] Arguably Gillespie is remembered, by both critics and fans alike, as one of the greatest jazz trumpeters of all time."
Gillespie was a trumpet virtuoso and improviser, building on the virtuoso style of Roy Eldridge but adding layers of harmonic complexity previously unheard in jazz. His beret and horn-rimmed spectacles, his scat singing, his bent horn, pouched cheeks and his light-hearted personality were essential in popularizing bebop.
In the 1940s Gillespie, with Charlie Parker, became a major figure in the development of bebop and modern jazz. He taught and influenced many other musicians, including trumpeters Miles Davis, Jon Faddis, Fats Navarro, Clifford Brown, Arturo Sandoval, Lee Morgan, Chuck Mangione, and balladeer Johnny Hartman.
Biography.
Early life and career.
Gillespie was born in Cheraw, South Carolina, the youngest of nine children of James and Lottie Gillespie. James was a local bandleader, so instruments were made available to the children. Gillespie started to play the piano at the age of four. Gillespie's father died when he was only ten years old. Gillespie taught himself how to play the trombone as well as the trumpet by the age of twelve. From the night he heard his idol, Roy Eldridge, play on the radio, he dreamed of becoming a jazz musician. He received a music scholarship to the Laurinburg Institute in North Carolina which he attended for two years before accompanying his family when they moved to Philadelphia.
Gillespie's first professional job was with the Frank Fairfax Orchestra in 1935, after which he joined the respective orchestras of Edgar Hayes and Teddy Hill, essentially replacing Roy Eldridge as first trumpet in 1937. Teddy Hill's band was where Gillespie made his first recording, "King Porter Stomp". In August 1937 while gigging with Hayes in Washington D.C., Gillespie met a young dancer named Lorraine Willis who worked a Baltimore–Philadelphia–New York City circuit which included the Apollo Theater. Willis was not immediately friendly but Gillespie was attracted anyway. The two finally married on May 9, 1940. They remained married until his death in 1993.
Gillespie stayed with Teddy Hill's band for a year, then left and free-lanced with numerous other bands. In 1939, Gillespie joined Cab Calloway's orchestra, with which he recorded one of his earliest compositions, the instrumental "Pickin' the Cabbage", in 1940. (Originally released on "Paradiddle", a 78rpm backed with a co-composition with Cozy Cole, Calloway's drummer at the time, on the Vocalion label, No. 5467).
After a notorious altercation between the two men, Calloway fired Gillespie in late 1941. The incident is recounted by Gillespie, along with fellow Calloway band members Milt Hinton and Jonah Jones, in Jean Bach's 1997 film, "The Spitball Story". Calloway did not approve of Gillespie's mischievous humor, nor of his adventuresome approach to soloing; according to Jones, Calloway referred to it as "Chinese music". Finally, their grudge for each other erupted over a thrown Spitball. Calloway never thought highly of Dizzy, because he didn't view Dizzy as a good musician. Once during a rehearsal, a member of the band threw a spitball. Already in a foul mood, Calloway decided to blame this on Dizzy. In order to clear his name, Dizzy didn’t take the blame and the problem quickly escalated into a fist fight, then a knife fight. Calloway had minor cuts on the thigh and wrist. After the two men were separated, Calloway fired Dizzy. A few days later, Dizzy tried to apologize to Calloway, but he was dismissed.
During his time in Calloway's band, Gillespie started writing big band music for bandleaders like Woody Herman and Jimmy Dorsey. He then freelanced with a few bands – most notably Ella Fitzgerald's orchestra, composed of members of the late Chick Webb's band, in 1942.
Gillespie avoided serving in World War II. In his Selective Service interview, he told the local board, "in this stage of my life here in the United States whose foot has been in my ass?". He was thereafter classed as 4-F. In 1943, Gillespie joined the Earl Hines band. Composer Gunther Schuller said: ... In 1943 I heard the great Earl Hines band which had Bird in it and all those other great musicians. They were playing all the flatted fifth chords and all the modern harmonies and substitutions and Gillespie runs in the trumpet section work. Two years later I read that that was 'bop' and the beginning of modern jazz ... but the band never made recordings. Gillespie said of the Hines band, "People talk about the Hines band being 'the incubator of bop' and the leading exponents of that music ended up in the Hines band. But people also have the erroneous impression that the music was new. It was not. The music evolved from what went before. It was the same basic music. The difference was in how you got from here to here to here ... naturally each age has got its own shit".
Then, Gillespie joined Billy Eckstine's (Hines' long-time collaborator) big band and it was as a member of Eckstine's band that he was reunited with Charlie Parker, a fellow member of Hines' band. In 1945, Gillespie left Eckstine's band because he wanted to play with a small combo. A "small combo" typically comprised no more than five musicians, playing the trumpet, saxophone, piano, bass and drums.
Rise of bebop.
Bebop was known as the first modern jazz style. However, it was unpopular in the beginning and was not viewed as positively as swing music was. Bebop was seen as an outgrowth of swing, not a revolution. Swing introduced a diversity of new musicians in the bebop era like Charlie Parker, Thelonious Monk, Bud Powell, Kenny Clarke, Oscar Pettiford, and Gillespie. Through these musicians, a new vocabulary of musical phrases was created. With Parker, Gillespie jammed at famous jazz clubs like Minton's Playhouse and Monroe's Uptown House. Parker's system also held methods of adding chords to existing chord progressions and implying additional chords within the improvised lines.
Gillespie compositions like "Groovin' High", "Woody 'n' You" and "Salt Peanuts" sounded radically different, harmonically and rhythmically, from the swing music popular at the time. "A Night in Tunisia", written in 1942, while Gillespie was playing with Earl Hines' band, is noted for having a feature that is common in today's music, a non-walking bass line. The song also displays Afro-Cuban rhythms. One of their first small-group performances together was only issued in 2005: a concert in New York's Town Hall on June 22, 1945. Gillespie taught many of the young musicians on 52nd Street, including Miles Davis and Max Roach, about the new style of jazz. After a lengthy gig at Billy Berg's club in Los Angeles, which left most of the audience ambivalent or hostile towards the new music, the band broke up. Unlike Parker, who was content to play in small groups and be an occasional featured soloist in big bands, Gillespie aimed to lead a big band himself; his first, unsuccessful, attempt to do this was in 1945.
After his work with Parker, Gillespie led other small combos (including ones with Milt Jackson, John Coltrane, Lalo Schifrin, Ray Brown, Kenny Clarke, James Moody, J.J. Johnson, and Yusef Lateef) and finally put together his first successful big band. Gillespie and his band tried to popularize bop and make Gillespie a symbol of the new music. He also appeared frequently as a soloist with Norman Granz's Jazz at the Philharmonic. He also headlined the 1946 independently produced musical revue film "Jivin' in Be-Bop".
In 1948 Gillespie was involved in a traffic accident when the bicycle he was riding was bumped by an automobile. He was slightly injured, and found that he could no longer hit the B-flat above high C. He won the case, but the jury awarded him only $1000, in view of his high earnings up to that point.
On January 6, 1953 Gillespie threw a party for his wife Lorraine at Snookie's in Manhattan, where his trumpet's bell got bent upward in an accident, but he liked the sound so much he had a special trumpet made with a 45 degree raised bell, becoming his trademark.
In 1956 Gillespie organized a band to go on a State Department tour of the Middle East which was extremely well received internationally and earned him the nickname "the Ambassador of Jazz". During this time, he also continued to lead a big band that performed throughout the United States and featured musicians including Pee Wee Moore and others. This band recorded a live album at the 1957 Newport jazz festival that featured Mary Lou Williams as a guest artist on piano.
Afro-Cuban music.
In the late 1940s, Gillespie was also involved in the movement called Afro-Cuban music, bringing Afro-Latin American music and elements to greater prominence in jazz and even pop music, particularly salsa. Afro-Cuban jazz is based on traditional Afro-Cuban rhythms. Gillespie was introduced to Chano Pozo in 1947 by Mario Bauza, a Latin jazz trumpet player. Chano Pozo became Gillespie's conga drummer for his band. Gillespie also worked with Mario Bauza in New York jazz clubs on 52nd Street and several famous dance clubs such as Palladium and the Apollo Theater in Harlem. They played together in the Chick Webb band and Cab Calloway's band, where Gillespie and Bauza became lifelong friends. Gillespie helped develop and mature the Afro-Cuban jazz style.
Afro-Cuban jazz was considered bebop-oriented, and some musicians classified it as a modern style. Afro-Cuban jazz was successful because it never decreased in popularity and it always attracted people to dance to its unique rhythms. Gillespie's most famous contributions to Afro-Cuban music are the compositions "Manteca" and "Tin Tin Deo" (both co-written with Chano Pozo); he was responsible for commissioning George Russell's "Cubano Be, Cubano Bop", which featured the great but ill-fated Cuban conga player, Chano Pozo. In 1977, Gillespie discovered Arturo Sandoval while researching music during a tour of Cuba.
Later years.
His biographer Alyn Shipton quotes Don Waterhouse approvingly that Gillespie in the fifties "had begun to mellow into an amalgam of his entire jazz experience to form the basis of new classicism". Another opinion is that, unlike his contemporary Miles Davis, Gillespie essentially remained true to the bebop style for the rest of his career.
In 1960, he was inducted into the "Down Beat" magazine's Jazz Hall of Fame.
During the 1964 United States presidential campaign the artist, with tongue in cheek, put himself forward as an independent write-in candidate. He promised that if he were elected, the White House would be renamed "The Blues House," and a cabinet composed of Duke Ellington (Secretary of State), Miles Davis (Director of the CIA), Max Roach (Secretary of Defense), Charles Mingus (Secretary of Peace), Ray Charles (Librarian of Congress), Louis Armstrong (Secretary of Agriculture), Mary Lou Williams (Ambassador to the Vatican), Thelonious Monk (Travelling Ambassador) and Malcolm X (Attorney General). He said his running mate would be Phyllis Diller. Campaign buttons had been manufactured years ago by Gillespie's booking agency "for publicity, as a gag", but now proceeds from them went to benefit the Congress of Racial Equality, Southern Christian Leadership Conference and Martin Luther King, Jr.; in later years they became a collector's item. In 1971 Gillespie announced he would run again but withdrew before the election for reasons connected to the Bahá'í Faith.
Dizzy Gillespie, a Bahá'í since 1968, was one of the most famous adherents of the Bahá'í Faith. His faith brought him to see himself as one of a series of musical messengers, part of a succession of trumpeters somewhat analogous to the series of prophets who bring God's message in religion. The universalist emphasis of his religion prodded him to to see himself more as a global citizen and humanitarian, expanding on his already-growing interest in his African heritage. His increasing spirituality brought out a generosity in him, and what author Nat Hentoff called an inner strength, discipline and "soul force". Gillespie's conversion was most affected by Bill Sears' book "Thief in the Night". Gillespie spoke about the Bahá'í Faith frequently on his trips abroad. He is honored with weekly jazz sessions at the New York Bahá'í Center in the memorial auditorium.
Gillespie published his autobiography, "To Be or Not to Bop", in 1979.
Gillespie was a vocal fixture in many of John Hubley and Faith Hubley's animated films, such as "The Hole", "The Hat", and "Voyage to Next".
In the 1980s, Gillespie led the United Nation Orchestra. For three years Flora Purim toured with the Orchestra and she credits Gillespie with evolving her understanding of jazz after being in the field for over two decades. David Sánchez also toured with the group and was also greatly influenced by Gillespie. Both artists later were nominated for Grammy awards. Gillespie also had a guest appearance on "The Cosby Show" as well as "Sesame Street" and "The Muppet Show".
In 1982, Gillespie had a cameo appearance on Stevie Wonder's hit "Do I Do". Gillespie's tone gradually faded in the last years in life, and his performances often focused more on his proteges such as Arturo Sandoval and Jon Faddis; his good-humoured comedic routines became more and more a part of his live act.
In 1988, Gillespie had worked with Canadian flautist and saxophonist Moe Koffman on their prestigious album "Oo Pop a Da." He did fast scat vocals on the title track and a couple of the other tracks were played only on trumpet.
In 1989 Gillespie gave 300 performances in 27 countries, appeared in 100 U.S. cities in 31 states and the District of Columbia, headlined three television specials, performed with two symphonies, and recorded four albums. He was also crowned a traditional chief in Nigeria, received the Ordre des Arts et des Lettres; France's most prestigious cultural award. He was named Regent Professor by the University of California, and received his fourteenth honorary doctoral degree, this one from the Berklee College of Music. In addition, he was awarded the Grammy Lifetime Achievement Award the same year. The next year, at the Kennedy Center for the Performing Arts ceremonies celebrating the centennial of American jazz, Gillespie received the Kennedy Center Honors Award and the American Society of Composers, Authors, and Publishers "Duke Ellington Award" for 50 years of achievement as a composer, performer, and bandleader. In 1993 he received the Polar Music Prize in Sweden.
November 26, 1992 at Carnegie Hall in New York City, following the Second Bahá'í World Congress was Gillespie's 75th birthday concert and his offering to the celebration of the centenary of the passing of Bahá'u'lláh. Gillespie was to appear at Carnegie Hall for the 33rd time. The line-up included: Jon Faddis, Marvin "Doc" Holladay, James Moody, Paquito D'Rivera, and the Mike Longo Trio with Ben Brown on bass and Mickey Roker on drums. But Gillespie didn't make it because he was in bed suffering from cancer of the pancreas. "But the musicians played their real hearts out for him, no doubt suspecting that he would not play again. Each musician gave tribute to their friend, this great soul and innovator in the world of jazz." In 2002, Gillespie was posthumously inducted into the International Latin Music Hall of Fame for his contributions to Afro-Cuban music.
Gillespie also starred in a film called "The Winter in Lisbon" released in 2004. He has a star on the Hollywood Walk of Fame at 7057 Hollywood Boulevard in the Hollywood section of the City of Los Angeles. He is honored by the December 31, 2006 – A Jazz New Year's Eve: Freddy Cole & the Dizzy Gillespie All-Star Big Band at The John F. Kennedy Center for the Performing Arts.
Death and legacy.
A longtime resident of Englewood, New Jersey he died of pancreatic cancer January 6, 1993, aged 75, and was buried in the Flushing Cemetery, Queens, New York City. Mike Longo delivered a eulogy at his funeral. He was also with Gillespie on the night he died, along with Jon Faddis and a select few others.
At the time of his death, Gillespie was survived by his widow, Lorraine Willis Gillespie (d. 2004); a daughter, jazz singer Jeanie Bryson; and a grandson, Radji Birks Bryson-Barrett. Gillespie had two funerals. One was a Bahá'í funeral at his request, at which his closest friends and colleagues attended. The second was at the Cathedral of Saint John the Divine in New York City open to the public.
As a tribute to him, DJ Qualls' character in the 2002 American teen comedy film "The New Guy" was named Dizzy Gillespie Harrison.
The Marvel Comics current Hawkeye comic written by Matt Fraction features Gillespie's music in a section of the editorials called the "Hawkguy Playlist".
Also, Dwight Morrow High School, the public high school of Englewood, New Jersey, renamed their auditorium the Dizzy Gillespie Auditorium, in memory of him.
In 2014, Gillespie was inducted into the New Jersey Hall of Fame.
Style.
Gillespie has been described as the "Sound of Surprise". "The Rough Guide to Jazz" describes his musical style:
In Gillespie's obituary, Peter Watrous describes his performance style:
Wynton Marsalis summed up Gillespie as a player and teacher:
"Bent" trumpet.
Gillespie's trademark trumpet featured a bell which bent upward at a 45-degree angle rather than pointing straight ahead as in the conventional design. According to Gillespie's autobiography, this was originally the result of accidental damage caused by the dancers Stump and Stumpy falling onto it while it was on a trumpet stand on stage at Snookie's in Manhattan on January 6, 1953, during a birthday party for Gillespie's wife Lorraine. The constriction caused by the bending altered the tone of the instrument, and Gillespie liked the effect. He had the trumpet straightened out the next day, but he could not forget the tone. Gillespie sent a request to Martin to make him a "bent" trumpet from a sketch produced by Lorraine, and from that time forward played a trumpet with an upturned bell.
Gillespie's biographer Alyn Shipton writes that Gillespie probably got the idea for a bent trumpet when he saw a similar instrument in 1937 in Manchester, England, while on tour with the Teddy Hill Orchestra. According to this account (from British journalist Pat Brand) Gillespie was able to try out the horn and the experience led him, much later, to commission a similar horn for himself.
Whatever the origins of Gillespie's upswept trumpet, by June 1954, he was using a professionally manufactured horn of this design, and it was to become a visual trademark for him for the rest of his life. Such trumpets were made for him by Martin (from 1954), King Musical Instruments (from 1972) and Renold Schilke (from 1982, a gift from Jon Faddis). Gillespie favored mouthpieces made by Al Cass. In December 1986 Gillespie gave the National Museum of American History his 1972 King "Silver Flair" trumpet with a Cass mouthpiece. In April 1995, Gillespie's Martin trumpet was auctioned at Christie's in New York City, along with instruments used by other famous musicians such as Coleman Hawkins, Jimi Hendrix and Elvis Presley. An image of Gillespie's trumpet was selected for the cover of the auction program. The battered instrument sold to Manhattan builder Jeffery Brown for $63,000, the proceeds benefiting jazz musicians suffering from cancer.
Discography.
As sideman.
With Benny Carter
With CTI All Stars
With Duke Ellington
With Quincy Jones
With Gene Krupa and Buddy Rich
With Mike Longo
With the Manhattan Transfer
With Carmen McRae
With Charles Mingus
With Katie Bell Nubin
With Oscar Peterson
With Mongo Santamaria
With Woody Shaw
With Lillian Terry
With Randy Weston

</doc>
<doc id="54404" url="https://en.wikipedia.org/wiki?curid=54404" title="Tropaeolum">
Tropaeolum

Tropaeolum , commonly known as nasturtium (;
literally "nose-twister" or "nose-tweaker"), is a genus of roughly 80 species of annual and perennial herbaceous flowering plants. It was named by Carl Linnaeus and is the only genus in the family Tropaeolaceae. The nasturtiums received their common name because they produce an oil that is similar to that of watercress ("Nasturtium officinale").
The genus "Tropaeolum", native to South and Central America, includes several very popular garden plants, the most commonly grown being "T. majus", "T. peregrinum" and "T. speciosum". One of the hardiest species is "T. polyphyllum" from Chile, the perennial roots of which can survive the winter underground at altitudes of 3,300 metres (10,000 ft).
Plants in this genus have showy, often intensely bright flowers, and rounded, peltate (shield-shaped) leaves with the petiole in the centre. The flowers are bisexual and zygomorphic, with five petals, a superior three-carpelled ovary, and a funnel-shaped nectar spur at the back, formed by modification of one of the five sepals.
History.
The first "Tropaeolum" species was imported into Spain by the Spanish botanist Nicolás Monardes. He published an account in 1569 entitled "Joyful News out of the Newe Founde Worlde" in which he described, among other things, the plants and animals discovered in South America. The English herbalist John Gerard reports having received seeds of the plant from Europe in his 1597 book "Herball, or Generall Historie of Plantes". "Tropaeolum majus" was named by the Swedish botanist Carl Linnaeus, who chose the genus name because the plant reminded him of an ancient custom. After victory in battle, the Romans used to set up a trophy pole called a tropaeum (from the Greek tropaion, source of English "trophy"). On this the armour and weapons of the vanquished foe were hung. Linnaeus was reminded of this by the plant as the round leaves resembled shields and the flowers, blood-stained helmets.
Nasturtiums were once known commonly as "Indian cresses" because they were introduced from the Americas, known popularly then as the Indies, and used like cress as salad ingredients. John Gerard called the plant "Indian Cresses" in his herbal. He wrote: "unto the backe part (of the flower) doth hange a taile or spurre, such as hath the Larkes heele, called in Latine "Consolida regalis". He was comparing the flowers of Indian cress to those of forking larkspur ("Consolida regalis") of the buttercup family. J R R Tolkien commented that an alternative anglicization of the name was "nasturtian" rather than "nasturtium".
Description.
"Tropaeolum" is a genus of dicotyledonous annual or perennial plants, often with somewhat succulent stems and sometimes tuberous roots. The alternate leaves are hairless, peltate and entire or palmately lobed. The petioles or leaf stalks are long and in many species are able to twine round other stems to provide support. The flowers are bisexual and showy, set singly on long stalks in the axils of the leaves. They have five sepals, the uppermost of which is elongated into a nectar spur. The five petals are clawed, with the lower three unlike the upper two. The eight stamens are in two whorls of unequal length and the superior ovary has three segments and three stigmas on a single style. The fruit is naked and nut-like, with three single seed segments.
Species in cultivation.
The most common flower in cultivation is a hybrid of "T. majus", "T. minus" and "T. peltophorum", and is commonly known as the nasturtium (and occasionally anglicized as nasturtian). It is mostly grown from seed as a half-hardy annual and both single and double varieties are available. It comes in a range of forms and colours including cream, yellow, orange and red, solid in colour or striped and often with a dark blotch at the base of the petals. It is vigorous and easily grown and does well in sun. It thrives in poor soil and dry conditions, whereas in rich soil it tends to produce much leafy growth and few flowers. Some varieties adopt a bush form while others scramble over and through other plants and are useful for planting in awkward spots or for covering fences and trellises.
The following cultivars have gained the Royal Horticultural Society's Award of Garden Merit:
The blue nasturtium ("Tropaeolum azureum") is a tender species from Chile which has violet-blue flowers with white eyes that can be as much as across.
"Tropaeolum brachyceras" has yellow flowers with purplish markings on wiry, climbing stems. It is a half hardy perennial from Chile and may remain dormant for several years before being sparked into growth by some unknown trigger factor.
"Tropaeolum hookerianum" is a tuberous-rooted species from Chile. There are two subspecies, "T. h. austropurpureum" which has violet-purple flowers and "T. h. pilosum" with yellow flowers.
The Canary creeper ("Tropaeolum peregrinum") is a trailing and climbing half-hardy annual species with wiry stalks and palmately lobed leaves. The pale yellow, fringed flowers are borne on long stalks. It originated from Peru but may first have been cultivated in the Canary Islands before being introduced into Western Europe.
Wreath nasturtium ("Tropaeolum polyphyllum") is a prostrate plant originating from Argentina and Chile. It has silvery, deeply lobed leaves and a profusion of small, bright yellow flowers on long trailing stalks. After flowering, the plant dies back. It is a perennial with underground rhizomes which send up new shoots at intervals. In a suitable sunny location with well drained soil, it will survive for several years. It is a very hardy species; the tubers can grow at depths of enabling the plant to survive at altitudes of as much as 3,300 metres (10,000 ft) in the Andes.
The flame flower ("Tropaeolum speciosum") is well adapted to cool, moist climates and famously does well in Scotland. It sends up shoots which thread their way through hedges and shrubs and which, when they emerge into the light, bear brilliant red flowers among small, five or six-lobed leaves. It is difficult to establish but is an attractive garden plant when it thrives. This plant has gained the Royal Horticultural Society's Award of Garden Merit.
Three-coloured Indian cress ("Tropaeolum tricolor") is another tuberous, climbing species grown for its attractive red, purple and yellow tubular flowers. It comes from Chile and Bolivia and is a reliable winter-growing species.
Mashua ("Tropaeolum tuberosum") is a perennial climbing plant from the Andes grown for its tuberous roots. It has been cultivated since ancient times and depictions of it are found at archaeological sites pre-dating the Incas. It has leaves with five to seven lobes and small, long-spurred, red and yellow flowers. The tubers have an unpleasant smell when raw which disappears on cooking. It is frost-hardy and produces crops of 30 tons per hectare at a height of above sea level. The cultivar "T. tuberosum lineamaculatum" 'Ken Aslet' has gained the Royal Horticultural Society's Award of Garden Merit.
Species that originated from the coastal areas and from the lower foothills make most of their growth in winter whereas the true alpine species are summer growers. Tuberous "Tropaeolum" species are well known for occasionally remaining dormant for one or more years. The species with underground rhizomes and tubers can be propagated from these while other species are best raised from seed. Fresh seed is favoured by many growers but dried seed is also often successful. Seed from the winter growing species should be sown in the autumn while the summer growing species are best sown in the spring in well-drained compost and covered with of grit or sand. The containers should be kept at below until the seedlings appear in about a month, as too high a temperature inhibits germination.
Uses.
Culinary.
All parts of "Tropaeolum majus" are edible. The flower has most often been consumed, making for an especially ornamental salad ingredient; it has a slightly peppery taste reminiscent of watercress, and is also used in stir fry. The flowers contain about 130 mg vitamin C per , about the same amount as is contained in parsley. Moreover, they contain up to 45 mg of lutein per 100 g, which is the highest amount found in any edible plant. The unripe seed pods can be harvested and dropped into spiced vinegar to produce a condiment and garnish, sometimes used in place of capers.
Mashua ("T. tuberosum") produces an edible underground tuber that is a major food source in parts of the Andes.
Herbal medicine.
Nasturtiums have been used in herbal medicine for their antiseptic and expectorant qualities. They are said to be good for chest colds and to promote the formation of new blood cells. "T. majus" has been used in herbal medicine for respiratory and urinary tract infections.
Companion planting and biological pest control.
Nasturtiums are used as companion plants for biological pest control, repelling some pests, acting as a trap crop for others and attracting predatory insects.
Taxonomy.
"Tropaeolum" was previously placed in the family Tropaeolaceae along with two other genera, "Magallan" and "Trophaeastrum". The monotypic genus "Magallan" was characterised by having winged fruit, and the two species of "Trophaeastrum" lacked spurs. The genus "Tropaeolum" was diagnosed only by the absence of the characteristics of the other two genera. A molecular study undertaken in 2000 found "Tropaeolum" to be paraphyletic when the other two genera are segregated, so "Magallan" and "Trophaeastrum" were reduced to synonyms of "Tropaeolum". Tropaeolaceae was thus rendered monogeneric, a family of only one genus.
Species.
"The Plant List", a collaboration between the Missouri Botanical Garden and the Royal Botanic Gardens, Kew includes the following accepted names of "Tropaeolum" species names. Some that are under review, are here marked "U". 

</doc>
<doc id="54406" url="https://en.wikipedia.org/wiki?curid=54406" title="Monday">
Monday

Monday is the day of the week between Sunday and Tuesday. According to the traditional Christian, Islamic and Hebrew calendars, it is the second day of the week, and according to international standard ISO 8601 it is the first day of the week. In the West, it is the first day of the work week, whereas in most Muslim countries and Israel, it is the second day of the work week. The name of Monday is derived from Old English "Mōnandæg" and Middle English "Monenday", which means "moon day".
Etymology.
The English noun "Monday" derived sometime before 1200 from "monedæi", which itself developed from Old English (around 1000) "mōnandæg" and "mōndæg" (literally meaning "moon's day"), which has cognates in other Germanic languages, including Old Frisian "mōnadeig", Middle Low German and Middle Dutch "mānendag, mānendach" (modern Dutch "Maandag"), Old High German "mānetag" (modern German "Montag"), and Old Norse "mánadagr" (Swedish and Norwegian nynorsk "måndag", Icelandic "mánudagur". Danish and Norwegian bokmål "mandag"). The Germanic term is a Germanic interpretation of Latin "lunae dies" ("day of the moon").
In many Slavic languages the name of the day eschews pagan tradition and translates as "after Sunday/holiday". Russian "понедельник" ("ponyedyelnik"), Serbian "понедељак" ("ponedeljak"), Ukrainian "понеділок" ("ponedilok"), Bulgarian "понеделник" ("ponedelnik"), Polish "poniedziałek", Czech "pondělí", Slovak "pondelok", Slovenian "ponedeljek". In Turkish it is called "pazartesi", which also means "after Sunday". Japanese and Korean share the same ancient Chinese words '月曜日' (Hiragana:げつようび, Hangul:월요일) for Monday which means "day of the moon".
In many languages of India, the word for Monday is derived from Sanskrit "Somavāra"; Soma is another name of the Moon god in Hinduism. In some languages of India, it is also called "Chandravāra"; Chandra in Sanskrit means "moon". In Thailand, the day is called "Wan Jan", meaning "the day of the Moon god Chandra".
Position in the week.
The international ISO 8601 standard places Monday as the first day of the week, and this is widely used on calendars in Europe and in international business. Monday is "xīngqīyī (星期一)" in Chinese, meaning "day one of the week". Its name in Syriac means "first day". In all Slavic languages Monday is perceived as the first day of the week. Modern Western culture usually looks at Monday as the beginning of the workweek, as it is typically Monday when adults go back to work and children go back to school after the weekend.
Jewish and Christian traditions place Sunday as the first day of the week, and Monday is thus the second day of the week. Quakers traditionally refer to Monday as "Second Day" eschewing the pagan origin of the English name "Monday". For similar reasons the official liturgical calendar of the Roman Catholic Church refers to Monday as the second celebration day – "Feria secunda". The Portuguese and the Greek (Eastern Orthodox Church) name for Monday reflects this, as do all the days' names except Saturday and Sunday: the Portuguese word for Monday is "segunda-feira" and the Greek word is "Δευτέρα" ""devtéra"" (second in order). Likewise the Hebrew name for Monday is "yom-sheni" (יום שני).
Religious observances.
In Judaism Mondays are considered auspicious days for fasting. The Didache warned early Christians not to fast on Mondays to avoid Judaizing, and suggests Wednesdays instead.
In Judaism the Torah is read in public on Monday mornings, one of three days the Torah is read each week (the other two days being Thursday and Saturday). Special penitential prayers are recited on Monday, unless there is a special occasion for happiness which cancels them.
In the Eastern Orthodox Church Mondays are days on which the Angels are commemorated. The Octoechos contains hymns on this theme, arranged in an eight-week cycle, that are chanted on Mondays throughout the year. At the end of Divine Services on Monday, the dismissal begins with the words: "May Christ our True God, through the intercessions of his most-pure Mother, of the honorable, Bodiless Powers (i.e., the angels) of Heaven…". In many Eastern monasteries Mondays are observed as fast days; because Mondays are dedicated to the angels, and monks strive to live an angelic life. In these monasteries the monks abstain from meat, fowl, dairy products, fish, wine and oil (if a feast day occurs on a Monday, fish, wine and oil may be allowed, depending upon the particular feast).
The Church of Jesus Christ of Latter-day Saints spend one evening per week called Family Home Evening (FHE) or Family Night usually Monday, that families are encouraged to spend together in study, prayer and other family activities. Many businesses owned by Latter-Day Saints close early on Mondays so they and their customers are able to spend more time with their families.
Cultural references.
A number of popular songs in Western culture feature Monday, often as a day of depression, anxiety, or melancholy (probably because of its association with the first day of the work week). For example, "Monday, Monday" (1966) from the Mamas & the Papas, "Rainy Days and Mondays" (1971) from the Carpenters, "I Don't Like Mondays" (1979) from the Boomtown Rats, and "Manic Monday" (1986) from the Bangles (written by Prince).
There is a band named the Happy Mondays and an American pop punk band Hey Monday.
The popular comic strip character Garfield by Jim Davis is well known for his disdain for Mondays.
More people in England and Wales commit suicide on Mondays than other days of the week; more people in the United Kingdom call in sick; and more people worldwide surf the web.
During July 2002, the consulting firm of PricewaterhouseCoopers Consulting announced that it would rename itself to "Monday", and spend $110 million over the next year to establish that brand.
Monday in different languages.
See the main article Week-day names.
Astrology.
Monday aligns with the celestial body, the Moon, and the astrological sign of Cancer, and is represented by the symbol of the Moon, ☾.

</doc>
<doc id="54407" url="https://en.wikipedia.org/wiki?curid=54407" title="Sunday">
Sunday

Sunday is the day of the week following Saturday but before Monday. Sunday is a day of rest in most Western countries, as a part of the weekend.
For most Christians, Sunday is observed as a day of worship and rest, holding it as the Lord's Day and the day of Christ's resurrection. In some Muslim countries and Israel, Sunday is the first work day of the week. According to the Hebrew calendars and traditional Christian calendars, Sunday is the first day of the week, and according to the International Organization for Standardization ISO 8601 Sunday is the seventh and last day of the week. No century in the Gregorian calendar starts on a Sunday, whether its first year is considered to be '00 or '01. The Jewish New Year never falls on a Sunday. The rules of the Hebrew calendar are designed such that the first day of Rosh Hashanah will never occur on the first, fourth, or sixth day of the Jewish week (Sunday, Wednesday or Friday).
Etymology.
Sunday, being the day of the Sun, as the name of the first day of the week, is derived from Hellenistic astrology, where the seven planets, known in English as Saturn, Jupiter, Mars, the Sun, Venus, Mercury and the Moon, each had an hour of the day assigned to them, and the planet which was regent during the first hour of any day of the week gave its name to that day. During the 1st and 2nd century, the week of seven days was introduced into Rome from Egypt, and the Roman names of the planets were given to each successive day.
Germanic peoples seem to have adopted the week as a division of time from the Romans, but they changed the Roman names into those of corresponding Teutonic deities. Hence, the "dies Solis" became Sunday (German, "Sonntag").
The English noun "Sunday" derived sometime before 1250 from "sunedai", which itself developed from Old English (before 700) "Sunnandæg" (literally meaning "sun's day"), which is cognate to other Germanic languages, including Old Frisian "sunnandei", Old Saxon "sunnundag", Middle Dutch "sonnendach" (modern Dutch "zondag"), Old High German "sunnun tag" (modern German "Sonntag"), and Old Norse "sunnudagr" (Danish and Norwegian "søndag", Icelandic "sunnudagur" and Swedish "söndag"). The Germanic term is a Germanic interpretation of Latin "dies solis" ("day of the sun"), which is a translation of the Ancient Greek "heméra helíou".
The p-Celtic Welsh language also translates the Latin "day of the sun" as "dydd Sul".
In most Indian languages, the word for Sunday is "Ravivāra" or "Adityavāra" or its derived forms — "vāra" meaning day, "Aditya" and "Ravi" both being a style (manner of address) for Surya, the chief solar deity and one of the Adityas. Ravivāra is first day cited in Jyotish, which provides logical reason for giving the name of each week day. In the Thai solar calendar of Thailand, the name ("Waan Arthit") is derived from Aditya, and the associated color is red.
In Russian the word for Sunday is Воскресенье (Voskreseniye) meaning "Resurrection". In other Slavic languages the word means "no work", for example Polish: Niedziela, Ukrainian: Недiля, Belorussian: Нядзеля, Croatian: Nedjelja, Serbian and Slovenian: Nedelja, Czech: Neděle, and Bulgarian: Неделя.
The Modern Greek word for Sunday, Κυριακή, is derived from Κύριος (Lord) also, due to its liturgical significance as the day commemorating the resurrection of Jesus Christ, i.e. The Lord's Day.
Position in the week.
ISO 8601.
The international standard ISO 8601 for representation of dates and times, states that Monday is the first day of the week. This method of representing dates and times unambiguously was first published in 1988.
Culture and languages.
In the Judaic, some Christian, as well as in some Islamic tradition, Sunday has been considered the first day of the week. A number of languages express this position either by the name for the day or by the naming of the other days. In Hebrew it is called יום ראשון "yom rishon", in Arabic الأحد "al-ahad", in Persian and related languages یکشنبه "yek-shanbe", all meaning "first". In Greek, the names of the days Monday, Tuesday, Wednesday, and Thursday ("Δευτέρα", "Τρίτη", "Τετάρτη" and "Πέμπτη") mean "second", "third", "fourth", and "fifth" respectively. This leaves Sunday in the first position of the week count. The current Greek name for Sunday, "Κυριακή" (Kyriake), means "Lord's Day" coming from the word "Κύριος" (Kyrios), which is the Greek word for "Lord". Similarly in Portuguese, where the days from Monday to Friday are counted as Segunda-feira, Terça-feira, Quarta-feira, Quinta-feira and Sexta-feira, while Sunday itself similar to Greek has the name of "Lord's Day" (domingo). In Vietnamese, the working days in the week are named as: "Thứ Hai" (second day), "Thứ Ba" (third day), "Thứ Tư" (fourth day), "Thứ Năm" (fifth day), "Thứ Sáu" (sixth day), "Thứ Bảy" (seventh day). Sunday is called "Chủ Nhật", a corrupted form of "Chúa Nhật" meaning "Lord's Day." Some colloquial text in the south of Vietnam and from the church may still use the old form to mean Sunday.
In Italian Sunday is called "Domenica", which also means "Lord's Day" (from Latin "Dies Dominica"). Same happens in French, where the name is "Dimanche", and Spanish and Portuguese ("Domingo").
Slavic languages implicitly number Monday as day number one, not two. For example, Polish has "czwartek" (4th) for Thursday and "piątek" (5th) for Friday. Hungarian "péntek" (Friday) is a Slavic loanword, so the correlation with "five" is not evident to Hungarians. Hungarians use "Vasárnap" for Sunday, which means "market day". Bulgarian "понеделник" and Russian "понедельник" (Monday) literally mean "after no work", Russian "вторник" (Tuesday) means "second day", "среда" (Wednesday) means "middle day", "четверг" (Thursday) means "fourth day", "пятница" (Friday) means "fifth day", "суббота" (Saturday) means "sabbath", and "воскресение" (Sunday) means or "resurrection (of Jesus)" (that is the day of a week which commemorates it). In Old Russian Sunday was also called "неделя" "free day" or "day with no work", but in the contemporary language this word means "week".
In the Maltese language, due to its Siculo-Arabic origin, Sunday is called "Il-Ħadd", a corruption of "wieħed" meaning "one". Monday is "It-Tnejn" meaning "two". Similarly Tuesday is "It-Tlieta" (three), Wednesday is "L-Erbgħa" (four) and Thursday is "Il-Ħamis" (five).
In Armenian, Monday is (Yerkoushabti) literally meaning 2nd day of the week, Tuesday (Yerekshabti) 3rd day, Wednesday (Chorekshabti) 4th day, Thursday (Hingshabti) 5th day. Saturday is (Shabat) coming from the word Sabbath or Shabbath in Hebrew, and "Kiraki" coming from the word "Krak" meaning "fire" is Sunday, "Krak" describing the sun by fire. Apostle John also refers to the "Lord's Day" (in Greek, "Κυριακή ημέρα", "kyriake hemera" i.e. the day of the Lord) in Rev. 1:10, which became the Armenian word for Sunday.
However, in many European countries calendars almost always show Monday as the first day of the week, which follows the ISO 8601 standard.
In the Persian calendar, Sunday is the second day of the week. However, it is called "number one" as counting starts from zero; the first day - Saturday - is denoted as 00.
Sunday in Christianity.
In Roman culture, Sunday was the day of the Sun god. It was adopted by Christians who did not have a Jewish background. The symbol of light was a pagan device adopted by Christians – perhaps the most important one that did not come from Jewish traditions. In paganism, the sun was a source of life, giving warmth and illumination to mankind. It was the center of a popular cult among Romans, who would stand at dawn to catch the first rays of sunshine as they prayed. The celebration of the winter solstice (which influenced Christmas) was part of the Roman cult of the sun. Christian churches were built with an orientation so that the congregation faced toward the sunrise in the East.
Christian usage.
The ancient Romans traditionally used the eight-day nundinal cycle, a market week, but in the time of Augustus in the 1st century AD, a seven-day week also came into use.
On 7 March 321, Constantine I, Rome's first Christian Emperor (see Constantine I and Christianity), decreed that Sunday would be observed as the Roman day of rest:
Despite the official adoption of Sunday as a day of rest by Constantine, the two days continued to be used side-by-side until at least the Calendar of 354 and probably later.
In 363, Canon 29 of the Council of Laodicea prohibited observance of the Jewish Sabbath (Saturday), and encouraged Christians to work on the Saturday and rest on the Lord's Day (Sunday). The fact that the canon had to be issued at all is an indication that adoption of Constantine's decree of 321 was still not universal, not even among Christians. It also indicates that Jews were observing the Sabbath on the Saturday.
Modern practices.
Some Christian denominations, called "Sabbatarians", observe a Saturday Sabbath. The name "Sabbatarian" has also been claimed by Christians, especially Protestants, who believe Sunday must be observed with just the sort of rigorous abstinence from work associated with "Shabbat". More recently, Christians in the Seventh-day Adventist, Seventh Day Baptist, and Church of God (Seventh-Day) denominations, as well as many Messianic Jews, have revived the practice of abstaining from work and gathering for worship on Saturdays.
For most Christians the custom and obligation of Sunday rest is not as strict. A minority of Christians do not regard the day they attend church as important, so long as they attend, as the apostles and disciples gathered on Sundays, on Saturdays, and whenever they could. There is considerable variation in the observance of Sabbath rituals and restrictions, but some cessation of normal weekday activities is customary. Many Christians today observe Sunday as a day of church-attendance and as the seventh day of the week.
In Roman Catholic liturgy, Sunday begins on Saturday evening. The evening Mass on Saturday is liturgically a full Sunday Mass and fulfills the obligation of Sunday Mass attendance, and Vespers (evening prayer) on Saturday night is liturgically "first Vespers" of the Sunday. The same evening anticipation applies to other major solemnities and feasts, and is an echo of the Jewish practice of starting the new day at sunset. Those who work in the medical field, in law enforcement, and soldiers in a war zone are dispensed from the usual obligation to avoid attending Church on Sunday. Work after religious services is encouraged.
In the Eastern Orthodox Church, Sunday begins at the Little Entrance of Vespers (or All-Night Vigil) on Saturday evening and runs until "Vouchsafe, O Lord" (after the "prokeimenon") of Vespers on Sunday night. During this time, the dismissal at all services begin with the words, "May Christ our True God, who rose from the dead ..." Anyone who wishes to receive Holy Communion at Divine Liturgy on Sunday morning is required to attend Vespers the night before (see Eucharistic discipline). Among Orthodox Christians, Sunday is considered to be the seventh-day of the week and a "Little Pascha" (Easter), and because of the Paschal joy, the making of prostrations is forbidden, except in certain circumstances. Families and communities, working and requiring somebody else to work are encouraged after prayers in church, including buying goods or services, use of public transport, gardening or driving or washing a car. Leisure activities and idleness, being secular and offensive to Christ as it is time-wasting, is prohibited.
Some languages lack separate words for "Saturday" and "Sabbath" (e. g. Italian, Portuguese). Outside the English-speaking world, "Sabbath" as a word, if it is used, refers to the Saturday (or the specific Jewish practices on it); Sunday may be called the Lord's Day () (which directly or etymologically is the actual name of the day in Romance languages and Modern Greek). On the other hand, English-speaking Christians often refer to the Sunday as the Sabbath (other than Seventh-day Sabbatarians); a practice which, probably due to the international connections and the Latin tradition of the Roman Catholic Church, is more widespread among (but not limited to) Protestants. Quakers traditionally refer to Sunday as "First Day" eschewing the pagan origin of the English name, while referring to Saturday as the "Seventh day".
The Russian word for Sunday is "Voskresenie," meaning "Resurrection day." The Greek word for Sunday is "Kyriake" (the "Lord's Day"). The Czech, Polish, Slovenian, Croatian, Serbian, Ukrainian and Belarusian words for Sunday ("neděle," "niedziela," "nedelja", "Nedjelja," "недеља", "неділя" and "нядзеля" respectively) can be translated as "without acts (no work)."
Common occurrences on Sunday.
In government and business.
In the United States and Canada, most government offices are closed on both Saturday and Sunday. A few will be open on Saturdays and a very small number will be open on Sunday. In major cities like San Francisco and Washington, DC, for example, a few branches of the US Postal Service are open on Sunday as well as Saturday; and a few branches of federal banks are also open on Saturday and Sunday.
Many private sector retail businesses open later and close earlier on Sunday. Business offices that are neither retail nor manufacturing outlets, such as corporate headquarters, are typically closed on both Saturday and Sunday. Large manufacturing plants, by contrast, typically operate one to three shifts every day of the week.
Many countries, particularly in Europe such as France, Germany and Belgium, but also in other countries such as Peru, hold their national and local elections on a Sunday, either by law or by tradition.
In media.
Many American and British daily newspapers publish a larger edition on Sundays, which often includes color comic strips, a magazine, and a coupon section; may only publish on a Sunday, or may have a "sister-paper" with a different masthead that only publishes on a Sunday.
North American Radio stations often play specialty radio shows such as Casey Kasem's countdown or other nationally syndicated radio shows that may differ from their regular weekly music patterns on Sunday morning and/or Sunday evening. In the United Kingdom, there is a Sunday tradition of chart shows on BBC Radio 1 and commercial radio; this originates in the broadcast of chart shows and other populist material on Sundays by Radio Luxembourg when the Reithian BBC's Sunday output consisted largely of solemn and religious programmes. However, BBC Radio 1's chart show moved to Fridays in July 2015.
Period and/or older-skewing television dramas, such as "Downton Abbey", "Call the Midwife", "Lark Rise to Candleford" and "Heartbeat" are commonly shown on Sunday evenings in the UK; the first of these was "Dr Finlay's Casebook" in the 1960s. Similarly, "Antiques Roadshow" has been shown on Sundays on BBC1 since 1979 and "Last of the Summer Wine" was shown on Sundays for many years until it ended in 2009.
Many American, Australian and British television networks and stations also broadcast their political interview shows on Sunday mornings.
In sports.
Major League Baseball usually schedules all Sunday games in the daytime except for the nationally televised Sunday Night Baseball matchup. Certain historically religious cities such as Boston and Baltimore among others will schedule games no earlier than 1:35 PM to ensure time for people who go to religious service in the morning can get to the game in time.
In the United States, professional American football is usually played on Sunday, although Saturday (via "Saturday Night Football"), Monday (via "Monday Night Football"), and Thursday (via "Thursday Night Football" or Thanksgiving) see some professional games. College football usually occurs on Saturday, and high-school football tends to take place on Friday night or Saturday afternoon.
In the UK, some club and Premier League football matches and tournaments usually take place on Sundays. Rugby matches and tournaments usually take place in club grounds or parks on Sunday mornings. It is not uncommon for church attendance to shift on days when a late morning or early afternoon game is anticipated by a local community.
One of the remains of religious segregation in the Netherlands is seen in amateur football: The Saturday-clubs are by and large Protestant Christian clubs, who were not allowed to play on Sunday. The Sunday-clubs were in general Catholic and working class clubs, whose players had to work on Saturday and therefore could only play on Sunday.
In Ireland, Gaelic football and hurling matches are predominantly played on Sundays, with the first (used to be second) and fourth (used to be third) Sundays in September always playing host to the All-Ireland hurling and football championship finals, respectively.
Professional golf tournaments traditionally end on Sunday.
In the United States and Canada, National Basketball Association and National Hockey League games, which are usually played at night during the week, are frequently played during daytime hours - often broadcast on national television.
Most NASCAR Sprint Cup and IndyCar events are held on Sundays. Formula One World Championship races are always held on Sundays regardless of timezone/country, while MotoGP holds most races on Sundays, with Middle Eastern races being the exception on Saturday. All Formula One events and MotoGP events with Sunday races involve qualifying taking place on Saturday.
Astrology.
Sunday is associated with the Sun and is symbolized by ☉.

</doc>
<doc id="54408" url="https://en.wikipedia.org/wiki?curid=54408" title="Charlie Parker">
Charlie Parker

Charles "Charlie" Parker, Jr. (August 29, 1920 – March 12, 1955), also known as "Yardbird" and "Bird", was an American jazz saxophonist and composer.
Parker was a highly influential jazz soloist and a leading figure in the development of bebop, a form of jazz characterized by fast tempos, virtuosic technique and advanced harmonies. Parker was a blazingly fast virtuoso, and he introduced revolutionary harmonic ideas including rapid passing chords, new variants of altered chords, and chord substitutions. His tone ranged from clean and penetrating to sweet and somber.
Parker acquired the nickname "Yardbird" early in his career. This, and the shortened form "Bird", continued to be used for the rest of his life, inspiring the titles of a number of Parker compositions, such as "Yardbird Suite", "Ornithology", "Bird Gets the Worm", and "Bird of Paradise". Parker was an icon for the hipster subculture and later the Beat Generation, personifying the jazz musician as an uncompromising artist and intellectual rather than just an entertainer.
Childhood.
Charles Parker, Jr. was born in Kansas City, Kansas, and raised in Kansas City, Missouri, the only child of Adelaide "Addie" (Bailey) and Charles Parker. He attended Lincoln High School in September 1934, but withdrew in December 1935, just before joining the local musicians' union. 
Parker began playing the saxophone at age 11, and at age 14 he joined his school's band using a rented school instrument. His father, Charles, was often absent but provided some musical influence; he was a pianist, dancer and singer on the T.O.B.A. circuit. He later became a Pullman waiter or chef on the railways. Parker's mother Addie worked nights at the local Western Union office. His biggest influence at that time was a young trombone player who taught him the basics of improvisation. 
Career.
Early career.
In the late 1930s Parker began to practice diligently. During this period he mastered improvisation and developed some of the ideas that led to bebop. In an interview with Paul Desmond, he said that he spent three to four years practicing up to 15 hours a day.
Bands led by Count Basie and Bennie Moten certainly influenced Parker. He played with local bands in jazz clubs around Kansas City, Missouri, where he perfected his technique, with the assistance of Buster Smith, whose dynamic transitions to double and triple time influenced Parker's developing style.
In 1937, Parker played at a jam session at the Reno Club in Kansas City. His attempt to improvise failed when he lost the tune. This prompted Jo Jones, the drummer for Count Basie's Orchestra, to contemptuously throw a cymbal at his feet as a signal to leave the stage. However, rather than discouraging Parker, the incident caused him to vow to practice harder, and turned out to be a seminal moment in the young musician's career when he returned as a new man a year later.
In 1938 Parker joined pianist Jay McShann's territory band. The band toured nightclubs and other venues of the southwest, as well as Chicago and New York City. Parker made his professional recording debut with McShann's band.
As a teenager, Parker developed a morphine addiction while hospitalized after an automobile accident, and subsequently became addicted to heroin. He continued using heroin throughout his life, and it ultimately contributed to his death. 
New York City.
In 1939 Parker moved to New York City, to pursue a career in music. He held several other jobs as well. He worked for nine dollars a week as a dishwasher at Jimmie's Chicken Shack, where pianist Art Tatum performed.
In 1942 Parker left McShann's band and played for one year with Earl Hines, whose band included Dizzy Gillespie, who later played with Parker as a duo. Unfortunately, this period is virtually undocumented, due to the strike of 1942–1943 by the American Federation of Musicians, during which time few professional recordings were made. Parker joined a group of young musicians, and played in after-hours clubs in Harlem, such as Clark Monroe's Uptown House and Minton's Playhouse. These young iconoclasts included Gillespie, pianist Thelonious Monk, guitarist Charlie Christian, and drummer Kenny Clarke. The beboppers' attitude was summed up in a famous quotation attributed to Monk by Mary Lou Williams: "We wanted a music that they couldn't play" – "they" referring to white bandleaders who had usurped and profited from swing music. The group played in venues on 52nd Street, including Three Deuces and the Onyx. While in New York City, Parker studied with his music teacher, Maury Deutsch. 
Bebop.
According to an interview Parker gave in the 1950s, one night in 1939 he was playing "Cherokee" in a jam session with guitarist William "Biddy" Fleet when he hit upon a method for developing his solos that enabled one of his main musical innovations. He realized that the 12 semitones of the chromatic scale can lead melodically to any key, breaking some of the confines of simpler jazz soloing.
Early in its development, this new type of jazz was rejected by many of the established, traditional jazz musicians who disdained their younger counterparts. The beboppers responded by calling these traditionalists "moldy figs". However, some musicians, such as Coleman Hawkins and Tatum, were more positive about its development, and participated in jam sessions and recording dates in the new approach with its adherents.
Because of the two-year Musicians' Union ban of all commercial recordings from 1942 to 1944, much of bebop's early development was not captured for posterity. As a result, it gained limited radio exposure. Bebop musicians had a difficult time gaining widespread recognition. It was not until 1945, when the recording ban was lifted, that Parker's collaborations with Dizzy Gillespie, Max Roach, Bud Powell and others had a substantial effect on the jazz world. (One of their first small-group performances together was rediscovered and issued in 2005: a concert in New York's Town Hall on June 22, 1945.) Bebop soon gained wider appeal among musicians and fans alike.
On November 26, 1945, Parker led a record date for the Savoy label, marketed as the "greatest Jazz session ever." Recording as Charlie Parker's Reboppers, Parker enlisted such sidemen as Gillespie and Miles Davis on trumpet, Curly Russell on bass and Roach on drums. The tracks recorded during this session include "Ko-Ko", "Billie's Bounce" and "Now's the Time".
Shortly afterward, the Parker/Gillespie band traveled to an unsuccessful engagement at Billy Berg's club in Los Angeles. Most of the group returned to New York, but Parker remained in California, cashing in his return ticket to buy heroin. He experienced great hardship in California, eventually being committed to Camarillo State Mental Hospital for a six-month period.
"Charlie Parker with Strings".
A longstanding desire of Parker's was to perform with a string section. He was a keen student of classical music, and contemporaries reported he was most interested in the music and formal innovations of Igor Stravinsky and longed to engage in a project akin to what later became known as Third Stream, a new kind of music, incorporating both jazz and classical elements as opposed to merely incorporating a string section into performance of jazz standards. On November 30, 1949, Norman Granz arranged for Parker to record an album of ballads with a mixed group of jazz and chamber orchestra musicians. Six master takes from this session comprised the album "Charlie Parker with Strings": "Just Friends", "Everything Happens to Me", "April in Paris", "Summertime", "I Didn't Know What Time It Was", and "If I Should Lose You".
"Jazz at Massey Hall".
In 1953, Parker performed at Massey Hall in Toronto, Canada, joined by Gillespie, Mingus, Powell and Roach. Unfortunately, the concert happened at the same time as a televised heavyweight boxing match between Rocky Marciano and Jersey Joe Walcott, so the musical event was poorly attended. Mingus recorded the concert, resulting in the album "Jazz at Massey Hall". At this concert, Parker played a plastic Grafton saxophone. At this point in his career he was experimenting with new sounds and materials. Parker himself explained the purpose of the plastic saxophone in a May 9, 1953 broadcast from Birdland and did so again in a subsequent May 1953 broadcast. Parker is known to have played several saxophones, including the Conn 6M, the Martin Handicraft and Selmer Model 22. He is also known to have performed with a King "Super 20" saxophone. Parker's King Super 20 saxophone was made specially for him in 1947. 
Personal life.
Addiction.
Parker's addiction to heroin caused him to miss performances and be considered unemployable. He frequently resorted to busking, receiving loans from fellow musicians and admirers, and pawning his saxophones for drug money. Heroin use was rampant in the jazz scene, and users could acquire it with little difficulty.
Although he produced many brilliant recordings during this period, Parker's behavior became increasingly erratic. Heroin was difficult to obtain once he moved to California, where the drug was less abundant, so he used alcohol as a substitute. A recording for the Dial label from July 29, 1946, provides evidence of his condition. Before this session, Parker drank a quart of whiskey. According to the liner notes of "Charlie Parker on Dial Volume 1", Parker missed most of the first two bars of his first chorus on the track, "Max Making Wax". When he finally did come in, he swayed wildly and once spun all the way around, away from his microphone. On the next tune, "Lover Man", producer Ross Russell physically supported Parker. On "Bebop" (the final track Parker recorded that evening) he begins a solo with a solid first eight bars; on his second eight bars, however, he begins to struggle, and a desperate Howard McGhee, the trumpeter on this session, shouts, "Blow!" at him. Charles Mingus considered this version of "Lover Man" to be among Parker's greatest recordings, despite its flaws. Nevertheless, Parker hated the recording and never forgave Ross Russell for releasing it. He re-recorded the tune in 1951 for Verve.
When Parker received his discharge from the hospital, he was clean and healthy. Before leaving California, he recorded "Relaxin' at Camarillo" in reference to his hospital stay. He returned to New York, resumed his addiction to heroin and recorded dozens of sides for the Savoy and Dial labels, which remain some of the high points of his recorded output. Many of these were with his so-called "classic quintet" including Davis and Roach. 
Death.
Parker died on March 12, 1955, in the suite of his friend and patroness Baroness Pannonica de Koenigswarter at the Stanhope Hotel in New York City, while watching "The Dorsey Brothers' Stage Show" on television. The official causes of death were lobar pneumonia and a bleeding ulcer, but Parker also had an advanced case of cirrhosis and had suffered a heart attack. The coroner who performed his autopsy mistakenly estimated Parker's 34-year-old body to be between 50 and 60 years of age.
Since 1950, Parker had been living with Chan Berg, the mother of his son Baird (who lived until 2014) and his daughter Pree (who died as an infant of cystic fibrosis). He considered Chan his wife although he never married her, nor did he divorce his previous wife, Doris, whom he had married in 1948. His marital status complicated the settling of Parker's estate and would ultimately serve to frustrate his wish to be quietly interred in New York City.
Parker wished never to return to Kansas City, even in death. He had told Chan that he wanted to be buried in New York, the city he considered his home. Dizzy Gillespie paid for the funeral arrangements and organized a lying-in-state, a Harlem procession officiated by Congressman and Reverend Adam Clayton Powell, Jr., as well as a memorial concert. Parker's body was flown back to Missouri, in accordance with his mother's wishes. Parker's widow criticized the dead man's family for giving him a Christian funeral even though they knew he was a confirmed atheist. Parker was buried at Lincoln Cemetery in Missouri, in a hamlet known as Blue Summit, located close to I-435 and East Truman Road.
Parker's estate is managed by CMG Worldwide.
Music.
Parker's style of composition involved interpolation of original melodies over existing jazz forms and standards, a practice known as contrafact and still common in jazz today. Examples include "Ornithology" (which borrows the chord progression of jazz standard "How High the Moon"), and "Yardbird Suite", the vocal version of which is called "What Price Love", with lyrics by Parker. The practice was not uncommon prior to bebop, but it became a signature of the movement as artists began to move away from arranging popular standards and toward composing their own material.
While tunes such as "Now's The Time", "Billie's Bounce", "Au Privave", "Barbados", "Relaxin' at Camarillo", "Bloomdido", and "Cool Blues" were based on conventional 12-bar blues changes, Parker also created a unique version of the 12-bar blues for tunes such as "Blues for Alice", "Laird Baird", and "Si Si." These unique chords are known popularly as "Bird Changes". Like his solos, some of his compositions are characterized by long, complex melodic lines and a minimum of repetition although he did employ the use of repetition in some tunes, most notably "Now's The Time".
Parker contributed greatly to the modern jazz solo, one in which triplets and pick-up notes were used in unorthodox ways to lead into chord tones, affording the soloist with more freedom to use passing tones, which soloists previously avoided. Parker was admired for his unique style of phrasing and innovative use of rhythm. Via his recordings and the popularity of the posthumously published "Charlie Parker Omnibook", Parker's identifiable style dominated jazz for many years to come.
Other well-known Parker compositions include "Ah-Leu-Cha", "Anthropology", co-written with Gillespie, "Bird Gets the Worm", "Cheryl", "Confirmation", "Constellation", "Donna Lee", "Moose the Mooche", and "Scrapple from the Apple".
Miles Davis once said, "You can tell the history of jazz in four words: Louis Armstrong. Charlie Parker."
Awards and recognitions.
Grammy Award
Grammy Hall of Fame
Recordings of Charlie Parker were inducted into the Grammy Hall of Fame, which is a special Grammy award established in 1973 to honor recordings that are at least twenty-five years old, and that have "qualitative or historical significance."
Inductions
Government honors
In 1995, the U.S. Postal Service issued a 32-cent commemorative postage stamp in Parker's honor.
In 2002, the Library of Congress honored his recording "Ko-Ko" (1945) by adding it to the National Recording Registry.
Charlie Parker Residence.
From 1950 to 1954, Parker and his common-law wife, Chan Berg, lived in the ground floor of the townhouse at 151 Avenue B, across from Tompkins Square Park in Manhattan's East Village. The Gothic Revival building, which was built about 1849, was added to the National Register of Historic Places in 1994, and was designated a New York City landmark in 1999. Avenue B between East 7th and East 10th Streets was given the honorary designation Charlie Parker Place in 1992.

</doc>
<doc id="54409" url="https://en.wikipedia.org/wiki?curid=54409" title="Jazz band">
Jazz band

A jazz band (jazz ensemble or jazz combo) is a musical ensemble that plays jazz music. Jazz bands vary in the quantity of its members and the style of jazz that they play but it is common to find a jazz band made up of a rhythm section and a horn section.
The size of a jazz band is closely related to the style of jazz they play as well as the type of venues in which they play. Smaller jazz bands, also known as "combos", are common in night clubs and other small venues and will be made up of three to seven musicians; whereas big bands are found in dance halls and other larger venues.
Jazz bands can vary in size from a big band, to a smaller trio or quartet. The term jazz trio can refer to a three piece band with a pianist, double bass player and a drummer. Some bands use vocalists, while others are purely instrumental groups. Jazz bands usually have a bandleader. In a big band setting, there is usually more than one player for a type of instrument.
Jazz bands and their composition have changed many times throughout the years just as the music itself changes with each performers personal interpretation and improvisation which is one of the greatest appeals of going to see a jazz band.
Ensemble types.
Combos.
Small jazz bands of three to four musicians are often referred to as "combos" and can be found in small night club venues. In modern jazz, an acoustic bass player is almost always present in a small band, complemented by any other combination of instruments.
It's common for musicians in a combo to perform their music from memory. The improvisational nature of these performances make every show unique.
Three parts.
In jazz, there are several types of trios. One type of jazz trio is formed with a piano player, a bass player and a drummer. Another type of jazz trio that became popular in the 1950s and 1960s is the organ trio, which is composed of a Hammond organ player, a drummer, and a third instrumentalist (either a saxophone player or an electric jazz guitarist). In organ trios, the Hammond organ player performs the bass line on the organ bass pedals while simultaneously playing chords or lead lines on the keyboard manuals. Other types of trios include the "drummer-less" trio, which consists of a piano player, a double bassist, and a horn (saxophone or trumpet) or guitar player; and the jazz trio with a horn player (saxophone or trumpet), double bass player, and a drummer. In the latter type of trio, the lack of a chordal instrument means that the horn player and the bassist have to imply the changing harmonies with their improvised lines.
Four parts.
Jazz quartets typically add a "horn" (the generic jazz name for saxophones, trombones, trumpets, or any other wind or brass instrument commonly associated with jazz) to one of the jazz trios described above. Slightly larger jazz ensembles, such as quintets (five instruments) or sextets (six instruments) typically add other soloing instruments to the basic quartet formation, such as different types of saxophones (e.g., alto saxophone, tenor saxophone, etc.) or an additional chordal instrument.
The Modern Jazz Quartet was a jazz combo established in 1952 that played a style of jazz influenced by classical music.
Larger ensembles.
The lineup of larger jazz ensembles can vary considerably, depending on the style of jazz being performed. In a 1920s-style Dixieland jazz band, a larger ensemble would be formed by adding a banjo player, woodwind instruments, as with the clarinet, or additional horns (saxophones, trumpets, trombones) to one of the smaller groups. In a 1940s-style Swing big band, a larger ensemble is formed by adding "sections" of like instruments, such as a saxophone section and a trumpet section, which perform arranged "horn lines" to accompany the ensemble. In a 1970s-style jazz fusion ensemble, a larger ensemble is often formed by adding additional percussionists or sometimes a saxophone player would "double" or "triple" meaning that they would also be proficient at the clarinet, flute or both. Also by the addition of soloing instruments.
Instrumentation.
Rhythm section consists of the percussion, double bass or bass guitar, and usually at least one instrument capable of playing chords, such as a piano, guitar, Hammond organ or vibraphone; most will usually have more than one of these. The standard rhythm section is piano, bass, and drums, augmented by guitar at times in small combos and regularly in large ones. Some large swing era orchestras also employed an additional piano, accordion, and banjo.
The horn section consists of a woodwind section and a brass section, which play the melody and main accompaniment. The standard small combo usually limits itself to one trumpet and one saxophone at times augmented by a second saxophone or a trombone. Typical horns found in a big jazz band include 4-5 trumpets, 5-6 woodwind instruments (usually saxophones), and 3-4 trombones.
Rhythm section.
Banjo.
The banjo has been used in jazz since the earliest jazz bands. The earliest use of the banjo in a jazz band was by Frank Duson in 1917, however Laurence Marrero claims it became popular in 1915.
There are three common types of banjo, the plectrum banjo, tenor banjo, and cello banjo. Over time, the four stringed tenor banjo became the most common banjo used in jazz. The drum-like sound box on the banjo made it louder than the acoustic guitars that were common with early jazz bands, and banjos were popular for recording.
Bass.
Jazz bass is the use of the double bass or bass guitar, to improvise accompaniment and solos in a jazz band. Players began using the double bass in jazz in the 1890s, to supply the low-pitched walking basslines. From the 1920s and 1930s Swing and big band era, through Bebop and Hard Bop, to the 1960s-era "free jazz" movement, the resonant, woody sound of the double bass anchored everything from small jazz combos to large jazz groups.
Beginning in the early 1950s, jazz some bass players began to use the electric bass guitar in place of the double bass. Most jazz bassists specialize in either the double bass or the electric bass.
Drums.
Jazz drumming is the art of playing percussion, usually the drum set, in jazz styles ranging from 1910s-style Dixieland jazz to 1970s-era jazz-rock fusion and 1980s-era latin jazz. Stylistically, this aspect of performance was shaped by its starting place, New Orleans, as well as numerous other regions of the world, including other parts of the United States, the Caribbean, and Africa.
Jazz required a method of playing percussion different from traditional European styles, one that was easily adaptable to the different rhythms of the new genre, fostering the creation of jazz drumming's hybrid technique.
Guitar.
Jazz guitar refers to a variety of guitar playing styles used in the various jazz genres. Although the earliest guitars used in jazz were acoustic and acoustic guitars are still sometimes used in jazz, most jazz guitarists since the 1940s have performed on an electrically amplified guitar or electric guitar.
Traditionally, jazz electric guitarists use an archtop with a relatively broad hollow sound-box, violin-style f-holes, a "floating bridge", and a magnetic pickup. Solid body guitars, mass-produced since the early 1950s, are also used.
Piano.
Jazz piano has played a leading role in developing the sound of jazz. The piano's role is multifaceted due largely to the instrument's combined melodic and harmonic capabilities. For this reason it is an important tool of jazz musicians and composers for teaching and learning jazz theory and set arrangement, regardless of their main instrument.
Jazz pianists also make extensive use of chord "extensions", such as adding the sixth, ninth, or thirteenth scale degree to the chord. When jazz pianists improvise, they use the scales, modes, and arpeggios associated with the chords in a tune's chord progression.
Woodwind section.
Clarinet.
The clarinet is a woodwind instrument with a single-reed mouthpiece. A clarinet player is known as an clarinetist. Originally, the clarinet was a central instrument in jazz, beginning with the New Orleans players in the 1910s. It remained a signature instrument of jazz through much of the big band era into the 1940s. Larry Shields was the clarinetist for Original Dixieland Jazz Band, the first jazz band to record commercially in 1917. The American players Ted Lewis and Jimmie Noone were pioneers of the instrument in jazz bands. The B soprano clarinet was the most common instrument, but a few early jazz musicians such as Alcide Nunez preferred the C soprano clarinet, and many New Orleans jazz brass bands have used an E soprano clarinet. 
Swing clarinetists such as Benny Goodman, Artie Shaw, and Woody Herman led successful big bands and smaller groups from the 1930s onward. Band leader Duke Ellington, active from the 1920s to the 1970s, used the clarinet as lead instrument in his works, with several players of the instrument (Barney Bigard, Jimmy Hamilton and Russell Procope) spending a significant portion of their careers in his orchestra. Harry Carney, primarily Ellington's baritone saxophonist, occasionally doubled on bass clarinet. Meanwhile, Pee Wee Russell had a long and successful career in small jazz bands. 
With the decline of the big bands' popularity in the late 1940s, the clarinet faded from its prominent position in jazz and the saxophone rose in importance in many jazz bands, probably because it uses a less complicated fingering system and thus could better accommodate the requirement for an increased speed of execution in modern jazz than the clarinet. But the clarinet did not entirely disappear. In the late 50s, traditional jazz experienced a revival, with the notable example of clarinetist Acker Bilk's Bristol Paramount Jazz Band. Some of the works of Bilk's jazz band reached the pop charts.
Saxophone section.
In the saxophone section, all of the saxophones will play a similar melodic line, but the baritone sax doubles by occasionally joining in with the bass trombone and bass to play the bass line. A big band saxophone section typically consists of two alto saxophones, two tenor saxophones, and one baritone saxophone. The tenor saxophone plays the counter melody, though have the lead in some cases. Saxophone players are often expected to double on clarinet, flute, or soprano saxophone. In earlier periods of jazz, a bass saxophone was used as a bass line instrument, though this is far less common today.
Brass section.
Trombone.
The trombone section consists of three tenor trombones and one bass trombone.
Trumpet.
A trumpet player may sometimes double on a flugelhorn.
Tuba.
The tuba is the largest and lowest-pitched brass instrument. This instrument made its first appearance in the 19th century, being played at orchestras. When involved with jazz, most tubas are played outdoors. Tuba players are generally called "Jazz tubists."
String section.
Violin.
Jazz violin is the use of the violin or electric violin to improvise solo lines. Although the violin has been used in jazz recordings since the first decades of the 20th century, it is more commonly associated with folk music than jazz. Jazz musician Milt Hinton claimed that the decline in violin players coincided with the introduction of sound movies, as many violin players were used as accompaniment for silent films.
In jazz-rock fusion styles, jazz violinists may use an electric violin plugged into an instrument amplifier along with effects such as a wah pedal or a distortion fuzzbox.
Cello.
The cello is a bowed string instrument. This instrument is the second largest bowed string instrument apart from the double bass. When being used in jazz, the cello is more commonly tuned to fourths. A cello player is known as an cellist.
Vocalists.
The precise definition of what makes a jazz vocalist can be unclear, because jazz has shared a great deal with blues and pop music since the 1920s. In their book "Essential Jazz", Henry Martin and Keith Waters identify five main characteristics that identify jazz singing, three of which are: "Loose phrasing [...], use of blue notes [...], free melodic embellishment." Often the human voice can act in place of a brass section in playing melodies, both written and improvised.
Scat singing is vocal improvisation with wordless vocables, nonsense syllables or without words at all. Though scat singing is improvised, the melodic lines are often variations on scale and arpeggio fragments, stock patterns and riffs, as is the case with instrumental improvisers. The deliberate choice of scat syllables also is a key element in vocal jazz improvisation. Syllable choice influences the pitch articulation, coloration, and resonance of the performance.
Repertoire.
Jazz standards are an important part of the musical repertoire of jazz musicians, in that they are widely known, performed, and recorded, and widely known by listeners.
Another important aspect of jazz is improvisation ("jams"). Bands playing in this fashion fall under the category of jam bands. A common way to incorporate improvisation is to feature solo performances from band members made up on the spot, allowing them to showcase their skill.
History.
Starting shortly after 1915, the first bands from New Orleans began to using the word "jass" or "jazz" in their band name, or to describe their music. Bandleader Tom Brown claims the first usage, which was disputed by Nick LaRocca of the Original Dixieland Jass Band.
It is reported that the first known recording of "Jas", "That Funny Jas Band from Dixieland (1916)" was by Collins and Harlan for Thomas A. Edison, Inc. on Blue Amberol in December 1916.
The first Jazz record, "The Original Dixieland One-Step" was issue 18255 by Victor Talking Machine Company in 1917. This is the first record with "Jass" on the label, attributed to the "Original Dixieland 'Jass' Band". After litigation and claims of copyright infringement, the title was changed to "Dixie 'Jass' Band One-Step".
Notable jazz bands.
Quintets.
The Miles Davis Quintet (1965-1968), featuring Wayne Shorter, Herbie Hancock, Ron Carter, and Tony Williams, was one of the most influential small jazz ensembles of the 20th century. Lee Konitz was once quoted for saying of the band that "They played so well individually and collectively". Michael Cuscuna complimented the band, saying "They all had their own unique perspective on how to compose and play, and when those unique components came together, they created an absolutely whole new sound. It is extraordinarily creative."
An alternative to the Miles Davis Quintet of 1965-1968, was the Miles Davis Quintet (1955-1957), featuring John Coltrane, Red Garland, Philly Joe Jones, and Paul Chambers. Pianist Pete Jolly once said of this group, "You've got to love that rhythm section" and Chuck Berghofer, who had once played with the band, even ventured to say "They changed music history".
Another greatly influential band was the Art Ensemble of Chicago in the 1970s and 1980s. The Art Ensemble was the vanguard for many contemporary pioneers today with their progressive style. Vandermark said of the group "For their first 15 years this group attacked nearly improvising aesthetic with complete originality" while Wadada Leo Smith is quoted for having said "I heard them live and watched them in action. They took theatrics to another level, spontaneous theater that had a theme and character to it.". Oluyemi Thomas is also quoted "The Art Ensemble has moved the music into an area that allows for space and silence. Their unique approach to collaboration is unparalleled in creative music. They have a poetic sense and social consciousness. They have positively influenced my music and all of creative music."
Big Bands.
The Glenn Miller Orchestra

</doc>
<doc id="54410" url="https://en.wikipedia.org/wiki?curid=54410" title="Triceratops">
Triceratops

Triceratops is a genus of herbivorous ceratopsid dinosaur that first appeared during the late Maastrichtian stage of the late Cretaceous period, about 68 million years ago (mya) in what is now North America. It is one of the last known non-avian dinosaur genera, and became extinct in the Cretaceous–Paleogene extinction event 66 million years ago. The term "Triceratops", which literally means "three-horned face", is derived from the Greek τρί- ("tri-") meaning "three", κέρας ("kéras") meaning "horn", and ὤψ ("ops") meaning "face".
Bearing a large bony frill and three horns on its large four-legged body, and possessing similarities with the modern rhinoceros, "Triceratops" is one of the most recognizable of all dinosaurs and the best known ceratopsid. It shared the landscape with and was probably preyed upon by "Tyrannosaurus", though it is less certain that the two did battle in the manner often depicted in traditional museum displays and popular images.
The exact placement of the "Triceratops" genus within the ceratopsid group has been debated by paleontologists. Two species, "T. horridus" and "T. prorsus", are considered valid, although many other species have been named. Research published in 2010 suggested that the contemporaneous "Torosaurus", a ceratopsid long regarded as a separate genus, represents "Triceratops" in its mature form. The view was immediately disputed and examination of more fossil evidence is expected to settle the debate.
"Triceratops" has been documented by numerous remains collected since the genus was first described in 1889, including at least one complete individual skeleton. Paleontologist John Scannella observed: "It is hard to walk out into the Hell Creek Formation and not stumble upon a "Triceratops" weathering out of a hillside." Forty-seven complete or partial skulls were discovered in just that area from 2000 to 2010. Specimens representing life stages from hatchling to adult have been found.
The functions of the frills and three distinctive facial horns on its head have long inspired debate. Traditionally, these have been viewed as defensive weapons against predators. More recent theories, noting the presence of blood vessels in the skull bones of ceratopsids, find it more probable that these features were primarily used in identification, courtship and dominance displays, much like the antlers and horns of modern reindeer, mountain goats, or rhinoceros beetles. The theory would find additional support if "Torosaurus" was found to be the mature form of "Triceratops", as this would mean the frill also developed holes (fenestrae) as individuals reached maturity, rendering the structure more useful for display than defense.
Description.
Individual "Triceratops" are estimated to have reached about 7.9 to 9.0 m (25.9–29.5 ft) in length, in height, and 6.1–12.0 tonnes (13,000–26,000 lb) in weight. The most distinctive feature is their large skull, among the largest of all land animals. The largest known skull (specimen MWC 7584, formerly BYU 12183) is estimated to have been in length when complete, and could reach almost a third of the length of the entire animal. It bore a single horn on the snout, above the nostrils, and a pair of horns approximately long, with one above each eye. In 2010, paleontologists revealed a fossil (named "Yoshi's Trike," MOR 3027) with horn cores, housed and displayed at the Museum of the Rockies in Montana. To the rear of the skull was a relatively short, bony frill, adorned with epoccipitals in some specimens. Most other ceratopsids had large fenestrae in their frills, while those of "Triceratops" were noticeably solid.
Limbs.
"Triceratops" species possessed a sturdy build, with strong limbs, short hands with three hooves each, and short feet with four hooves each. Although certainly quadrupedal, the posture of these dinosaurs has long been the subject of some debate. Originally, it was believed that the front legs of the animal had to be sprawling at angles from the thorax in order to better bear the weight of the head. This stance can be seen in paintings by Charles Knight and Rudolph Zallinger. Ichnological evidence in the form of trackways from horned dinosaurs and recent reconstructions of skeletons (both physical and digital) seem to show that "Triceratops" and other ceratopsids maintained an upright stance during normal locomotion, with the elbows flexed and slightly bowed out, in an intermediate state between fully upright and fully sprawling (as in the modern rhinoceros).
The hands and forearms of "Triceratops" retained a fairly primitive structure compared to other quadrupedal dinosaurs such as thyreophorans and many sauropods. In those two groups, the forelimbs of quadrupedal species were usually rotated so that the hands faced forward with palms backward ("pronated") as the animals walked. "Triceratops", like other ceratopsians and the related quadrupedal ornithopods, walked with most of their fingers pointing out and away from the body, the primitive condition for dinosaurs also retained by bipedal forms like the theropods. In "Triceratops", the weight of the body was carried by only the first three fingers of the hand, while digits 4 and 5 were vestigial and lacked claws or hooves. The phalangeal formula is 2-3-4-3-1, meaning that the innermost finger of the forelimb has two bones, the next has three, etc.
Classification.
"Triceratops" is the best known genus of the Ceratopsidae, a family of large North American horned dinosaurs. The exact location of "Triceratops" among the ceratopsians has been debated over the years. Confusion stemmed mainly from the combination of short, solid frills (similar to that of Centrosaurinae), and the long brow horns (more akin to Ceratopsinae, also known as Chasmosaurinae). In the first overview of horned dinosaurs, R. S. Lull hypothesized two lineages, one of "Monoclonius" and "Centrosaurus" leading to "Triceratops", the other with "Ceratops" and "Torosaurus", making "Triceratops" a centrosaurine as the group is understood today. Later revisions supported this view, formally describing the first, short-frilled group as Centrosaurinae (including "Triceratops"), and the second, long-frilled group as Chasmosaurinae.
In 1949, C. M. Sternberg was the first to question this and favoured instead that "Triceratops" was more closely related to "Arrhinoceratops" and "Chasmosaurus" based on skull and horn features, making "Triceratops" a ceratopsine (chasmosaurine of his usage) genus. He was largely ignored, with John Ostrom, and later David Norman both placing "Triceratops" within Centrosaurinae.
Subsequent discoveries and analyses upheld Sternberg's view on the position of "Triceratops", with Lehman defining both subfamilies in 1990 and diagnosing "Triceratops" as ceratopsine (chasmosaurine of his usage) on the basis of several morphological features. In fact, it fits well into the ceratopsine subfamily, apart from its one feature of a shortened frill. Further research by Peter Dodson, including a 1990 cladistic analysis and a 1993 study using RFTRA (resistant-fit theta-rho analysis), a morphometric technique which systematically measures similarities in skull shape, reinforces "Triceratops"' placement in the ceratopsine subfamily.
The below cladogram follows Longrich (2015), who named a new species of "Pentaceratops", and included nearly all species of chasmosaurine.
Use in phylogenetics.
In phylogenetic taxonomy, the genus has been used as a reference point in the definition of Dinosauria; dinosaurs have been designated as all descendants of the most recent common ancestor of "Triceratops" and Neornithes (i.e. modern birds). Furthermore, the bird-hipped dinosaurs, Ornithischia, have all been designated dinosaurs with a more recent common ancestor to "Triceratops" than modern birds.
Evolutionary origins.
For many years after its discovery, the evolutionary origins of "Triceratops" remained largely obscure. In 1922, the newly discovered "Protoceratops" was seen as its ancestor by Henry Fairfield Osborn, but many decades passed before additional findings came to light. Recent years have been fruitful for the discovery of several dinosaurs related to ancestors of "Triceratops". "Zuniceratops", the earliest known ceratopsian with brow horns, was described in the late 1990s, and "Yinlong", the first known Jurassic ceratopsian, in 2005.
These new finds have been vital in illustrating the origins of horned dinosaurs in general, suggesting an Asian origin in the Jurassic, and the appearance of truly horned ceratopsians by the beginning of the late Cretaceous in North America. As "Triceratops" is increasingly shown to be a member of the long-frilled Ceratopsinae subfamily, a likely ancestor may have resembled "Chasmosaurus", which thrived some 5 million years earlier.
Paleobiology.
Although "Triceratops" are commonly portrayed as herding animals, there is currently little evidence that they lived in herds. While several other genera of horned dinosaurs are known from bonebeds preserving bones from two to hundreds or thousands of individuals, to date there is only one documented bonebed dominated by "Triceratops" bones: a site in southeastern Montana with the remains of three juveniles. It may be significant that only juveniles were present. Another, more recent find may reveal that "Triceratops" lived in small family groups. In 2012, a group of three "Triceratops" in relatively complete condition, each of varying sizes from a full-grown adult to a small juvenile, were found in Wyoming, near Newcastle. The remains are currently under excavation by paleontologist Peter Larson and a team from the Black Hills Institute. It is believed that the animals were traveling as a family unit, but it remains unknown if the group consists of a mated pair and their offspring, or two females and a juvenile they were caring for. The remains also show signs of predation or scavenging from "Tyrannosaurus", particularly on the largest specimen, with the bones of the front limbs showing breakage and puncture wounds from "Tyrannosaurus" teeth.
For many years, "Triceratops" finds were known only from solitary individuals. These remains are very common; for example, Bruce Erickson, a paleontologist of the Science Museum of Minnesota, has reported having seen 200 specimens of "T. prorsus" in the Hell Creek Formation of Montana. Similarly, Barnum Brown claimed to have seen over 500 skulls in the field. Because "Triceratops" teeth, horn fragments, frill fragments, and other skull fragments are such abundant fossils in the Lancian faunal stage of the late Maastrichtian (late Cretaceous, 66 mya) Period of western North America, it is regarded as among the dominant herbivores of the time, if not the most dominant herbivore. In 1986, Robert Bakker estimated it as making up 5/6ths of the large dinosaur fauna at the end of the Cretaceous. Unlike most animals, skull fossils are far more common than postcranial bones for "Triceratops", suggesting that the skull had an unusually high preservation potential.
"Triceratops" was one of the last ceratopsian genera to appear before the Cretaceous–Paleogene extinction event. The related "Torosaurus", and the more distantly related diminutive "Leptoceratops", were also present, though their remains have been rarely encountered.
Dentition and diet.
"Triceratops" were herbivorous, and because of their low head, their primary food was probably low growth, although they may have been able to knock down taller plants with their horns, beak, and bulk. The jaws were tipped with a deep, narrow beak, believed to have been better at grasping and plucking than biting.
"Triceratops" teeth were arranged in groups called batteries, of 36 to 40 tooth columns, in each side of each jaw with 3 to 5 stacked teeth per column, depending on the size of the animal. This gives a range of 432 to 800 teeth, of which only a fraction were in use at any given time (tooth replacement was continuous and occurred throughout the life of the animal). They functioned by shearing in a vertical to near-vertical orientation. The great size and numerous teeth of "Triceratops" suggests that they ate large volumes of fibrous plant material, with some suggesting palms and cycads, and others suggesting ferns, which then grew in prairies.
Functions of the horns and frill.
There has been much speculation over the functions of "Triceratops"' head adornments. The two main theories have revolved around use in combat, or display in courtship, with the latter thought now to be the most likely primary function.
Early on, Lull postulated that the frills may have served as anchor points for the jaw muscles to aid chewing by allowing increased size and thus power for the muscles. This has been put forward by other authors over the years, but later studies do not find evidence of large muscle attachments on the frill bones.
"Triceratops" were long thought to have possibly used their horns and frills in combat with predators such as "Tyrannosaurus", the idea being discussed first by C. H. Sternberg in 1917 and 70 years later by Robert Bakker. There is evidence that "Tyrannosaurus" did have aggressive head-on encounters with "Triceratops", based on partially healed tyrannosaur tooth marks on a "Triceratops" brow horn and squamosal; the bitten horn is also broken, with new bone growth after the break. Which animal was the aggressor is not known. Since the "Triceratops" wounds healed, it is most likely that the "Triceratops" survived the encounter and managed to overcome the "Tyrannosaurus". Paleontologist Peter Dodson estimates that if "Tyrannosaurus" attacked a bull "Triceratops", the "Triceratops" had the upper hand and would successfully defend itself by inflicting fatal wounds to the "Tyrannosaurus" using its sharp horns. "Tyrannosaurus" is also known to have fed on "Triceratops". Evidence for this includes a heavily tooth-scored "Triceratops" ilium and sacrum.
In addition to combat with predators using horns, "Triceratops" are classically shown engaging each other in combat with horns locked. While studies show that such activity would be feasible, if unlike that of present-day horned animals, there is disagreement about whether they did so. Although pitting, holes, lesions, and other damage on "Triceratops" skulls (and the skulls of other ceratopsids) are often attributed to horn damage in combat, a 2006 study finds no evidence for horn thrust injuries causing these forms of damage (for example, there is no evidence of infection or healing). Instead, non-pathological bone resorption, or unknown bone diseases, are suggested as causes. A newer study compared incidence rates of skull lesions and periosteal reaction in "Triceratops" and "Centrosaurus" and showed that these were consistent with "Triceratops" using its horns in combat and the frill being adapted as a protective structure, while lower pathology rates in "Centrosaurus" may indicate visual rather than physical use of cranial ornamentation, or a form of combat focused on the body rather than the head. The frequency of injury was found to be 14% in "Triceratops". The researchers also concluded that the damage found on the specimens in the study was often too localized to be caused by bone disease. Histological examination reveals that the frill of "Triceratops" is composed of fibrolamellar bone which contains fibroblasts that play a critical role in wound healing, and are capable of rapidly depositing bone during remodeling.
One skull, assigned to "Triceratops", was observed to have a hole in the jugal which appears to be a puncture wound that was sustained while this individual was still alive. This is supported by signs of healing that are present in the bone around the supposed wound. When examined closely, the hole in the bone has a diameter that is very similar to diameter of the distal end of a "Triceratops" horn. This, and other apparent healed wounds in the skulls of ceratopsians, has been cited as evidence of non-fatal intraspecific competition in these dinosaurs.
The large frill also may have helped to increase body area to regulate body temperature. A similar theory has been proposed regarding the plates of "Stegosaurus", although this use alone would not account for the bizarre and extravagant variation seen in different members of the Ceratopsidae. This observation is highly suggestive of what is now believed to be the primary function, display.
The theory of their use in sexual display was first proposed by Davitashvili in 1961 and has gained increasing acceptance since. Evidence that visual display was important, either in courtship or in other social behavior, can be seen in the fact that horned dinosaurs differ markedly in their adornments, making each species highly distinctive. Also, modern living creatures with such displays of horns and adornments use them in similar behavior. A 2006 study of the smallest "Triceratops" skull, ascertained to be a juvenile, shows the frill and horns developed at a very early age, predating sexual development and thus probably important for visual communication and species recognition in general.
Growth and ontogeny.
In 2006, the first extensive ontogenetic study of "Triceratops" was published in the journal Proceedings of the Royal Society. The study, by John R. Horner and Mark Goodwin, found that individuals of "Triceratops" could be divided into four general ontogenetic groups, babies, juveniles, subadults, and adults. With a total number of 28 skulls studied, the youngest was only long. 10 of the 28 skulls could be placed in order in a growth series with one representing each age. Each of the four growth stages were found to have identifying features. Multiple ontogenetic trends were discovered, including the size reduce of the epoccipitals, development and reorientation of postorbital horns, and hollowing out of the horns.
Paleoecology.
"Triceratops" lived during the Late Cretaceous of North America, its fossils have come from the Evanston Formation, Scollard Formation, Laramie Formation, Lance Formation, Denver Formation, and Hell Creek Formation. These fossil formations date back to the time of the Cretaceous-Paleogene Extinction Event, and has been dated to 66 ± 0.07 million years ago. Many animals and plants have been found in these formations, but mostly from the Lance Formation and Hell Creek Formation.
Theropods from these formations include genera of tyrannosaurids, ornithomimids, troodontids, avialans, caenagnathids, and dromaeosaurids. "Acheroraptor" and "Dakotaraptor" are dromaeosaurids from the Hell Creek Formation. Indeterminate dromaeosaurs are known from other fossil formations. Common teeth previously referred to "Dromaeosaurus" and "Saurornitholestes" later were considered to be "Acheroraptor". The tyrannosaurids from the formation are "Nanotyrannus" and "Tyrannosaurus", although the former might be a junior synonym of the latter. Among ornithomimids are the genera "Struthiomimus" as well as "Ornithomimus", although an undescribed animal named "Orcomimus" could be from the formation. Troodontids are only represented by "Pectinodon" and "Paronychodon" in the Hell Creek Formation; with a possible species of "Troodon" from the Lance Formation. One species of coelurosaur is known from Hell Creek and similar formations by a single species, "Richardoestesia". Only two oviraptorosaurs are from the Hell Creek Formation, "Anzu", and "Leptorhynchos". The avialans known from the formation are "Avisaurus", multiple species of "Brodavis", and several other species of hesperornithoforms, as well as several species of true birds including "Cimolopteryx". >
Ornithischians are abundant in the Scollard Lance, Laramie, Lance, Denver, and Hell Creek Formation. The main groups of ornithischians are ankylosaurians, ornithopods, ceratopsians, and pachycephalosaurians. Three ankylosaurians are known, "Ankylosaurus", "Denversaurus", and possibly a species of "Edmontonia" or an undescribed genus. Multiple genera of ceratopsians are known from the formation other than "Triceratops", the leptoceratopsid "Leptoceratops", and the chasmosaurine ceratopsids "Torosaurus","Nedoceratops" and "Tatankaceratops". Ornithopods are common in the Hell Creek Formation, and are known from several species of the ornithopod "Thescelosaurus", and the hadrosaurids "Edmontosaurus", and a possible species "Parasaurolophus". Several pachycephalosaurians have been found in the Hell Creek Formation and in similar formations. Among them are the derived pachycephalosaurids "Stygimoloch", "Dracorex", "Pachycephalosaurus", "Sphaerotholus", and an undescribed specimen from North Dakota. The first two might be junior synonyms of "Pachycephalosaurus".
Mammals are plentiful in the Hell Creek Formation. Groups represented include multituberculates, metatherians, and eutherians. The multituberculates represented include "Paracimexomys", the cimolomyids "Paressonodon", "Meniscoessus", "Essonodon", "Cimolomys", "Cimolodon", and "Cimexomys"; and the neoplagiaulacids "Mesodma", and "Neoplagiaulax". The alphadontids "Alphadon", "Protalphodon", and "Turgidodon", pediomyids "Pediomys", "Protolambda", and "Leptalestes", the stagodontid "Didelphodon", the deltatheridiid "Nanocuris", the herpetotheriid "Nortedelphys", and the glasbiid "Glasbius" all represent metatherians of the Hell Creek Formation. A few eutherians are known, being represented by "Alostera", "Protungulatum", the cimolestids "Cimolestes" and "Batodon", the gypsonictopsid "Gypsonictops", and the possible nyctitheriid "Paranyctoides".
Discovery and identification.
The first named specimen now attributed to "Triceratops" is a pair of brow horns attached to a skull roof, found near Denver, Colorado in the spring of 1887. This specimen was sent to Othniel Charles Marsh, who believed that the formation from which it came dated from the Pliocene, and that the bones belonged to a particularly large and unusual bison, which he named "Bison alticornis". He realized that there were horned dinosaurs by the next year, which saw his publication of the genus "Ceratops" from fragmentary remains, but he still believed "B. alticornis" to be a Pliocene mammal. It took a third and much more complete skull to change his mind. The specimen, collected in 1888 by John Bell Hatcher from the Lance Formation of Wyoming, was initially described as another species of "Ceratops". After reflection, Marsh changed his mind and gave it the generic name "Triceratops", accepting his "Bison alticornis" as another species of "Ceratops" (it would later be added to "Triceratops"). The sturdy nature of the animal's skull has ensured that many examples have been preserved as fossils, allowing variations between species and individuals to be studied. "Triceratops" remains have subsequently been found in the American states of Montana and South Dakota (in addition to Colorado and Wyoming), and in the Canadian provinces of Saskatchewan and Alberta.
An earlier specimen, also recovered from the Lance Formation, was named "Agathaumas sylvestris" by Edward Drinker Cope in 1872. Originally identified as a hadrosaur, this specimen consists only of post-cranial remains and is only provisionally considered an example of "Triceratops".
Species.
Within the first decades after "Triceratops" was described, various skulls were collected, which varied to a lesser or greater degree from the original "Triceratops", named "T. horridus" by Marsh (from the Latin "horridus"; "rough, rugose", suggesting the roughened texture of those bones belonging to the type specimen, later identified as an aged individual). This variation is unsurprising, given that "Triceratops" skulls are large three-dimensional objects, coming from individuals of different ages and both sexes, and which were subjected to different amounts and directions of pressure during fossilization. Discoverers would name these as separate species (listed below), and came up with several phylogenetic schemes for how they were related to each other.
In the first attempt to understand the many species, Lull found two groups, although he did not say how he distinguished them: one composed of "T. horridus", "T. prorsus", and "T. brevicornus"; the other of "T. elatus" and "T. calicornis". Two species ("T. serratus" and "T. flabellatus") stood apart from these groups. By 1933, and his revision of the landmark 1907 Hatcher-Marsh-Lull monograph of all known ceratopsians, he retained his two groups and two unaffiliated species, with a third lineage of "T. obtusus" and "T. hatcheri" that was characterized by a very small nasal horn. "T. horridus"-"T. prorsus"-"T. brevicornus" was now thought to be the most conservative lineage, with an increase in skull size and a decrease in nasal horn size, and "T. elatus"-"T. calicornis" was defined by large brow horns and small nasal horn. C. M. Sternberg made one modification, adding "T. eurycephalus" and suggesting that it linked the second and third lineages closer together than they were to the "T. horridus" lineage. This pattern was followed until the major studies of the 1980s and 1990s.
With time, the idea that the differing skulls might be representative of individual variation within one (or two) species gained popularity. In 1986, Ostrom and Wellnhofer published a paper in which they proposed that there was only one species, "Triceratops horridus". Part of their rationale was that generally there are only one or two species of any large animal in a region (modern examples being the elephant and the giraffe in modern Africa). To their findings, Lehman added the old Lull-Sternberg lineages combined with maturity and sexual dimorphism, suggesting that the "T. horridus"-"T. prorsus"-"T. brevicornus" lineage was composed of females, the "T.calicornis"-"T.elatus" lineage was made up of males, and the "T. obtusus"-"T. hatcheri" lineage was of pathologic old males. His reasoning was that males had taller, more erect horns and larger skulls, and females had smaller skulls with shorter, forward-facing horns.
These findings were contested a few years later by Catherine Forster, who reanalyzed "Triceratops" material more comprehensively and concluded that the remains fell into two species, "T. horridus" and "T. prorsus", although the distinctive skull of "T." (""Nedoceratops"") "hatcheri" differed enough to warrant a separate genus. She found that "T. horridus" and several other species belonged together, and "T. prorsus" and "T. brevicornus" stood alone, and since there were many more specimens in the first group, she suggested that this meant the two groups were two species. It is still possible to interpret the differences as representing a single species with sexual dimorphism.
In 2009, John Scannella and Denver Fowler supported the separation of "T. prorsus" and "T. horridus", and noted that the two species are also separated stratigraphically within the Hell Creek Formation, indicating that they did not live together at the same time.
Synonyms and doubtful species.
The following species are considered "nomina dubia" ("dubious names"), and are based on remains that are too poor or incomplete to be distinguished from pre-existing "Triceratops" species.
"Nedoceratops".
The paper that described "Nedoceratops" was originally part of O. C. Marsh's magnum opus, his Ceratopsidae monograph. Unfortunately, Marsh died (1899) before the work was completed, and John Bell Hatcher endeavored to complete the "Triceratops" section. He died of typhus in 1904 at the age of 42, leaving the paper still uncompleted. It fell to Richard Swann Lull to complete the monograph in 1905, publishing Hatcher's description of a skull separately and giving it the name "Diceratops hatcheri"; "Diceratops" means "two horned face."
Since the "Diceratops" paper had been written by Hatcher, and Lull had only contributed the name and published the paper after Hatcher's death, Lull was not quite as convinced of the distinctiveness of "Diceratops", thinking it primarily pathological. By 1933, Lull had had second thoughts about "Diceratops" being a distinct genus and he put it in a subgenus of "Triceratops": "Triceratops" ("Diceratops"), including "T. obtusus"; largely attributing its differences to being that of an aged individual.
Because the name "Diceratops" was already in use for a hymenopteran (Foerster, 1868), Andrey Sergeyevich Ukrainsky gave the animal its current name "Nedoceratops" in 2007. Unaware that Ukrainsky had already renamed the animal, Octávio Mateus coined another new name for it in 2008, "Diceratus". "Diceratus" is thus a junior synonym of "Nedoceratops".
Opinion has varied on the validity of a separate genus for "T. hatcheri". John Scannella and Jack Horner regarded it as an intermediate growth stage between "Triceratops" and "Torosaurus". Andrew Farke, in his 2011 redescription of the only known skull, concluded that it was an aged individual of its own valid taxon, "Nedoceratops hatcheri". Nicholas Longrich and Daniel Fields also did not consider it a transition between "Torosaurus" and "Triceratops", suggesting that the frill holes were pathological.
"Torosaurus".
"Torosaurus" is a ceratopsid genus first identified from a pair of skulls in 1891, two years after the identification of "Triceratops". The "Torosaurus" genus resembles "Triceratops" in geological age, distribution, anatomy and physical size and it has been recognised as a close relative. Its distinguishing features are an elongated skull and the presence of two fenestrae, or holes, in the frill. Paleontologists investigating dinosaur ontogeny (growth and development of individuals over the life span) in the Hell Creek Formation, Montana, US, have recently presented evidence that the two represent a single genus.
John Scannella, in a paper presented in Bristol, UK at the conference of the Society of Vertebrate Paleontology (25 September 2009) reclassified "Torosaurus" as especially mature "Triceratops" individuals, perhaps representing a single sex. Jack Horner, Scannella's mentor at Bozeman Campus, Montana State University, noted that ceratopsian skulls consist of metaplastic bone. A characteristic of metaplastic bone is that it lengthens and shortens over time, extending and resorbing to form new shapes. Significant variety is seen even in those skulls already identified as "Triceratops", Horner said, "where the horn orientation is backwards in juveniles and forward in adults". Approximately 50% of all subadult "Triceratops" skulls have two thin areas in the frill that correspond with the placement of "holes" in "Torosaurus" skulls, suggesting that holes developed to offset the weight that would otherwise have been added as maturing "Triceratops" individuals grew longer frills. A paper describing these findings in detail was published in July 2010 by Scannella and Horner. It formally argues that "Torosaurus" and the similar contemporary "Nedoceratops" are synonymous with "Triceratops".
The assertion ignited debate. Andrew Farke had in 2006 stressed that, apart from the frill, no systematic differences could be found between "Torosaurus" and "Triceratops". He nevertheless disputed Scannella's conclusion by arguing in 2011 that the proposed morphological changes required to "age" a "Triceratops" into a "Torosaurus" would be without precedent among ceratopsids. Creatures would require the growth of epoccipitals, reversion of bone texture from adult to immature forms back to adult, and growth of frill holes at a later stage than usual. A study by Nicholas Longrich and Daniel Field analyzed 35 specimens of both "Triceratops" and "Torosaurus". The authors concluded that "Triceratops" individuals too old to be considered immature forms are represented in the fossil record, as are "Torosaurus" individuals too young to be considered fully mature adults. The synonymy of "Triceratops" and "Torosaurus" cannot be supported, they said, without more convincing intermediate forms than Scannella and Horner initially produced. Scannella's "Triceratops" specimen with a hole on its frill, they argued, could represent a diseased or malformed individual rather than a transitional stage between an immature "Triceratops" and mature "Torosaurus" form.
Given the abundance of fossils, particularly of "Triceratops", additional field discoveries are expected to settle the debate in time.
"Ojoceratops" and "Tatankaceratops".
As described above, John Scannella had argued in 2010 that "Nedoceratops" should be considered a synonym of "Triceratops". Andrew Farke (2011) maintained that it represents a valid distinct genus. Nick Longrich agreed with Scannella about "Nedoceratops" and made a further suggestion: that the recently described "Ojoceratops" was likewise a synonym. The fossils, he argued, are indistinguishable from the "T. horridus" specimens that were previously attributed to the defunct species "T. serratus".
Longrich observed that another newly described genus, "Tatankaceratops", displayed a strange mix of characteristics already found in adult and juvenile "Triceratops". Rather than representing a distinct genus, "Tatankaceratops" could as easily represent a dwarf "Triceratops" or a "Triceratops" individual with a developmental disorder that caused it to stop growing prematurely.
Depiction in popular media.
"Triceratops" (the species are not identified) is the official state fossil of South Dakota, and the official state dinosaur of Wyoming.
The distinctive appearance of "Triceratops" has led to them being frequently depicted in films, computer games and documentaries, such as the 1993 film "Jurassic Park" and the 1999 BBC television documentary "Walking with Dinosaurs". A recurring theme, especially in children's dinosaur books, is a climactic showdown or battle between "Triceratops" and "Tyrannosaurus". In 1942, Charles R. Knight painted a mural incorporating a confrontation between the two dinosaurs in the Field Museum of Natural History for the National Geographic Society, establishing them as enemies in popular thought. Paleontologist Bob Bakker said of the imagined rivalry between "Tyrannosaurus" and "Triceratops", "No matchup between predator and prey has ever been more dramatic. It's somehow fitting that those two massive antagonists lived out their co-evolutionary belligerence through the very last days of the very last epoch of the Age of Dinosaurs."

</doc>
<doc id="54412" url="https://en.wikipedia.org/wiki?curid=54412" title="Unicycle">
Unicycle

A unicycle is a vehicle that touches the ground with only one wheel. The most common variation has a frame with a saddle, and has a pedal-driven direct drive. Unicycling is practiced professionally in circuses, by street performers, and in festivals and as a hobby. Unicycles have also been used to create new sports such as unicycle hockey. In recent years unicycles have been also been used in activities similar to mountain biking or trials.
History.
US patents for single-wheeled 'velocipedes' were published in 1869 by Frederick Myers and in 1881 by Battista Scuri.
Unicycle design has developed since the advent of the first unicycle into many variations including: the seatless unicycle ("ultimate wheel") and the tall ("giraffe") unicycle. During the late 1980s some extreme sportsmen took an interest in the unicycle and modified unicycles to enable them to engage in off-road or mountain unicycling, trials unicycling and street unicycling.
Unicycles compared to other pedal powered vehicles.
Bicycles, tricycles and quadracycles share (with minor variations) several basic parts including wheels, pedals, cranks, forks, and the saddle with unicycles.
Without a rider unicycles lack stability - however a proficient unicyclist is usually more stable than a similarly proficient rider on a bicycle as the wheel is not constrained by the linear axis of a frame.
Unicycles usually lack brakes, gears, and the ability to freewheel.
Construction.
Unicycles have a few key parts:
The wheel is usually similar to a small bicycle wheel with a special hub designed so the axle is a fixed part of the hub. This means the rotation of the cranks directly controls the rotation of the wheel (called direct drive). The frame sits on top of the axle bearings, while the cranks attach to the ends of the axle, and the seatpost slides into the frame to allow the saddle to be height adjusted.
Types of unicycles.
Types of unicycle include:
Each type has special components unique to that type of unicycle.
Freestyle unicycles.
Generally used for flatland skills and freestyle routines. Usually have a relatively high seatpost, a narrow saddle, and a squared fork (used for one-footed tricks). These unicycles are used similarly to flatland bicycles. Wheel size is usually , but smaller riders may use unicycles. Some people prefer wheels.
Trials unicycles.
Designed for unicycle trials, these unicycles are stronger than standard unicycles in order to withstand the stresses caused by jumping, dropping, and supporting the weight of the unicycle and rider on components such as the pedals and cranks. Many trials unicycles also have wide, 19- or knobby tires to absorb some of the impact on drops.
Mountain Unicycles ("MUnis").
"MUni" or "muni" is an abbreviation for mountain unicycling. MUnis have many of the same components as trials unicycles, but have a few key differences. Usually, the tire diameters on mountain unicycles are either 24 or , allowing the rider to more easily roll over obstacles such as roots and rocks. 29 inch munis are also used but are usually used for longer trips, or just a challenge. The seat is also thicker and more comfortable on MUnis to compensate for the rough terrain. Brakes are used for steep descents.
Touring unicycles.
Used for long distances, these unicycles are specially made to cover distances. They have a large wheel diameter, between 26 and 36 in., so more distance is covered in less pedal rotation. A 36" unicycle made by the Coker Tire company started the big wheel trend. Some variations on the traditional touring unicycle include the Schlumpf "GUni" (geared unicycle), which uses a two-speed internal fixed-geared hub. Larger direct-drive wheels tend to have shorter cranks to allow for easier cadence and more speed. Geared wheels, with an effective diameter larger than the wheel itself, tend to use longer cranks to increase torque as they are not required to achieve such high cadences as direct-drive wheels, but demand greater force per pedal stroke.
Other variations.
Other variations include:
Training aids.
Having training aids may make it easier to become comfortable with riding a unicycle. One method for training is using a spotter to make riding easier. One other easy way to learn is to find a narrow hallway that can be used to help alleviate left and right balancing while allowing a beginner to focus on forward and backward balance. If a hallway cannot be found, a fence or clothes line is suitable. Equally, riding back and forth between two chairs, faced back to back, whilst holding on to the chair backs allows the user to gauge how to appropriately position oneself before setting off. Using props such as sticks or ski poles is generally discouraged as they hinder balance and create dependence. A fall onto props could also cause serious injury.
Riding styles.
Traditionally, unicycling has been seen as a circus skill which has been performed at events to entertain the public in the circus or during parades, carnivals or street festivals. Recent developments in the strength and durability of bicycle (and consequently unicycle) parts have given rise to many new activities including trials unicycling and mountain unicycling. Unicycling is arguably now as much a competitive sport and recreational pursuit as an entertainment activity.
The principal types of unicycling are:
Unicycle team sports.
Unicycling is also performed as a team sport.
Unicycle basketball.
Unicycle basketball uses a regulation basketball on a regular basketball court with the same rules, e.g., one must dribble the ball while riding. There are a number of rules that are particular to unicycle basketball as well, e.g., a player must be mounted on the unicycle when in-bounding the ball. Unicycle basketball is usually played using or smaller unicycles, and using plastic pedals, both to preserve the court and the players' shins. In North America, regular unicycle basketball games are organized in Berkeley, San Luis Obispo, Detroit, Phoenix, Minneapolis, and Toronto. Switzerland, France, Germany and Puerto Rico are all field teams. The Puerto Rico All Stars has been one of the dominant teams and has won several world championships.
Unicycle hockey.
Unicycle hockey originated in the Pacific Northwest where it is commonly called Puckwheel. Puckwheel follows rules basically similar to ice hockey or inline hockey, using a tennis ball and ice-hockey sticks. Puckwheel play is mostly non-contact. Puckwheel has active leagues in Germany, Switzerland and the UK and international tournaments held at least bi-annually. Puckwheel tournaments in the UK are held by various teams across the country usually in sports halls, but occasionally outside. Each Puckwheel tournament lasts a day and around 8 teams normally compete in a round-robin league with the winner being whoever has the most points. If two teams have the same number of points the winner can be decided by goal difference or a penalty shoot-out. Puckwheel is recognized by the UCPA Olympic Committee.
Unicycle handball.
Unicycle handball uses a handball-sized ball. The teams aim to throw it into a vertical hoop placed about above the ground. It has been played in the Polish village of Chrzelice since the late 1970s
UNICON and regional championships.
UNICON, Eurocycle and APUC are regular international unicycling conventions.
The biennial UNICON (International Unicycling Convention), sanctioned by the International Unicycling Federation, comprises all major unicycling disciplines and is a major event on the international unicycling calendar. Events include: artistic (group, pairs, individual, standard skill, open-X), track racing (100 metres, 400 metres, 800 metres, 30 metres walk the wheel, 50 metres one-foot), 10 kilometres, marathon (42.195 km), muni (cross-country, uphill, downhill, North Shore downhill), trials, basketball and hockey.
The Eurocycle (EUROpean uniCYCLE meeting) is a similar convention but based in Europe.
APUC, the Asia Pacific Unicycling Championships, are held biannually, alternately with Unicon. The first APUC, in 2007, was in Singapore. Subsequently the event has been held in Hong Kong (2009), Seoul (2011), Canberra (2013), and Singapore (2015).
Races.
The world's first multi-stage unicycle race, Ride the Lobster, took place in Nova Scotia in June 2008. Some 35 teams from 14 countries competed over a total distance of 800 km. Each team consisted of a maximum of 3 riders and 1 support person.
Unicross, or unicycle cyclocross is an emerging race format in which unicycles race over a cyclocross course.
Manufacturers.
Unicycle makers include:

</doc>
<doc id="54415" url="https://en.wikipedia.org/wiki?curid=54415" title="Mountain unicycling">
Mountain unicycling

Mountain unicycling is an adventure sport that consists of traversing rough terrain on a unicycle. Mountain unicycling ('muni') is undertaken on similar terrain to mountain biking. However, muni requires much more attention to the microfeatures of the short distance in front of the wheel. Unicycles' lack of a freewheel means that descents must be controlled all the way, and the typical lack a gear system (though two-gear hubs are available), prevents the rider from reaching high speeds. Muni usually takes place on specially designed unicycles, which are equipped with strong hubs, large, knobbly tires, high-grip pedals and rugged frames. Some are also equipped with rim or disc brakes, having the lever mounted under the nose of the saddle. The brake primarily helps to compensate the downhill-slope force, while more expert riders also use it to decelerate or stop.
Muni riders also need a few additional skills than required for either mountain biking or regular unicycling, with core strength and balance being key.

</doc>
<doc id="54416" url="https://en.wikipedia.org/wiki?curid=54416" title="Trolleybus">
Trolleybus

A trolleybus (also known as trolley bus, trolley coach, trackless trolley, trackless tram early years or trolley) is an electric bus that draws power from overhead wires (generally suspended from roadside posts) using spring-loaded trolley poles. Two wires and poles are required to complete the electrical circuit. This differs from a tram or streetcar, which normally uses the track as the return path, needing only one wire and one pole (or pantograph). They also are distinct from other kinds of electric buses, which usually rely on batteries. Power is most commonly supplied as 600-volt direct current, but there have been, and are, exceptions.
Currently, around 300 trolleybus systems are in operation, in cities and towns in 43 countries. Altogether, more than 800 trolleybus systems have existed, but not more than about 400 concurrently.
History.
The trolleybus dates back to 29 April 1882, when Dr. Ernst Werner Siemens demonstrated his "Elektromote" in a Berlin suburb. This experiment continued until 13 June 1882, after which there were few developments in Europe, although separate experiments were conducted in the U.S. In 1899, another vehicle which could run either on or off rails was demonstrated in Berlin. The next development was when Lombard Gerin operated an experimental line at the Paris Exhibition of 1900 after four years of trials, with a circular route around Lake Daumesnil that carried passengers. Routes followed in Eberswalde and Fontainebleau. Max Schiemann on 10 July 1901 opened the world's fourth passenger-carrying trolleybus system, which operated at Bielatal (Biela Valley, near Dresden), in Germany. Schiemann built and operated the Bielatal system, and is credited with developing the under-running trolley current collection system, with two horizontally parallel overhead wires and rigid trolleypoles spring-loaded to hold them up to the wires. Although this system operated only until 1904, Schiemann had developed what is now the standard trolleybus current collection system. In the early days there were a few other methods of current collection. The Cédès-Stoll (Mercédès-Électrique-Stoll) system was operated near Dresden between 1902 and 1904, and in Vienna. The Lloyd-Köhler or Bremen system was tried out in Bremen, and the Cantono Frigerio system was demonstrated near Milan.
Throughout the period, trackless freight systems and electric canal boats were also built.
Leeds and Bradford became the first cities to put trolleybuses into service in Great Britain on 20 June 1911. Apparently, though it was opened on 20 June, the public was not admitted to the Bradford route until the 24th. Bradford was also the last to operate trolleybuses in the UK, the system closing on 26 March 1972. The last rear-entrance trolleybus in Britain was also in Bradford and is now owned by the Bradford Trolleybus Association. Birmingham was the first to replace a tram route with trolleybuses, while Wolverhampton, under the direction of Charles Owen Silvers, became world-famous for its trolleybus designs. There were 50 trolleybus systems in the UK, London's being the largest. By the time trolleybuses arrived in Britain in 1911, the Schiemann system was well established and was the most common, although the Cédès-Stoll (Mercédès-Électrique-Stoll) system was tried in West Ham (in 1912) and in Keighley (in 1913).
Smaller trackless trolley systems were built in the US early as well. The first non-experimental system was a seasonal municipal line installed near Nantasket Beach in 1904; the first year-round commercial line was built to open a hilly property to development just outside Los Angeles in 1910. The trackless trolley was often seen as an interim step, leading to streetcars. In the U.S.A., some systems subscribed to the all-four concept of using buses, trolleybuses, streetcars ("trams", "trolleys") and rapid transit subway and/or elevated lines (metros), as appropriate, for routes ranging from the lightly used to the heaviest trunk line. Buses and trolleybuses in particular were seen as entry systems that could later be upgraded to rail as appropriate. In a similar fashion, many cities in Britain originally viewed trolleybus routes as extensions to tram (streetcar) routes where the cost of constructing or restoring track could not be justified at the time, though this attitude changed markedly (to viewing them as outright replacements for tram routes) in the years after 1918. Trackless trolleys were the dominant form of new post-war electric traction, with extensive systems in among others, Los Angeles, Chicago, Rhode Island, and Atlanta; Boston, San Francisco (California), and Philadelphia (Pennsylvania) still maintain an "all-four" fleet.
Some trolleybus lines in the United States (and in Britain, as noted above) came into existence when a trolley or tram route did not have sufficient ridership to warrant track maintenance or reconstruction. In a similar manner, a proposed tram scheme in Leeds, United Kingdom, was changed to a trolleybus scheme to cut costs.
Trolleybuses are uncommon today in North America, but they remain common in many European countries as well as Russia and China, generally occupying a position in usage between street railways (trams) and diesel buses. Worldwide, around 300 cities or metropolitan areas are served by trolleybuses today. (Further detail under Use and preservation, below.)
Trolleybuses are used extensively in large European cities, such as Athens, Belgrade, Bratislava, Bucharest, Budapest, Chisinau, Kiev, Lyon, Milan, Minsk, Moscow, Riga, Saint Petersburg, Sofia, Tallinn, Varna, Vilnius and Zurich, as well as smaller ones such as Arnhem, Bergen, Coimbra, Gdynia, Kaunas, Lausanne, Limoges, Luzern, Modena, Piatra Neamţ, Plzeň, Prešov, Salzburg, Solingen, Szeged, Târgu Jiu and Yalta. "See also Trolleybus usage by country."
Transit authorities in some cities have reduced or discontinued their use of trolleybuses in recent years, while others, wanting to add or expand use of zero-emission vehicles in an urban environment, have opened new systems or are planning new systems. For example, Lecce, Italy, opened a new trolleybus system in 2012, and new systems are planned in Leeds and Montréal, among other places.
Advantages.
Trolleybuses are better than internal combustion (IC) buses on hilly routes, as electric motors allow static torque at start-up, an advantage for climbing steep hills. Unlike IC engines, electric motors draw power from a central plant and can be overloaded for short periods without damage. San Francisco and Seattle, both hilly American cities, use trolleybuses partly for this reason, another being improved air quality. Given their acceleration and braking performance, trolleybuses can outperform diesel buses on flat stretches as well.
Trolleybuses' rubber tires have better adhesion than trams' steel wheels on steel rails, giving them better hill-climbing capability and braking. Unlike rail vehicles (where side tracks are not available), an out-of-service vehicle can be moved to the side of the road and its trolley poles lowered, allowing other trolleybuses to pass. Additionally, because they are not confined to tracks, trolleybuses can pull over to the curb as a diesel bus does, eliminating boarding islands in the middle of the street.
Like other electric vehicles, trolleybuses may be more environmentally friendly in the city than fossil fuel or hydrocarbon-based vehicles (petrol/gasoline, diesel, alcohol, etc.). Although the power is not free because it has to be produced at centralised power plants with attendant transmission losses, it is often produced more efficiently. Further, it is not bound to a specific fuel source and is more amenable to pollution control as a point source supply than are individual vehicles with their own engines exhausting noxious gases and particulates at street level. Some cities, Vancouver, B.C., for instance, use hydroelectricity. A further advantage of trolleybuses is that they can generate electricity from kinetic energy while braking, a process known as regenerative braking. However, for regenerative braking to work as such, there must be another bus on the same circuit that needs power, or a way to send the excess power back to the commercial electric power system. Otherwise the braking power must be dissipated in resistance grids on the bus, when it is called "dynamic braking". There are alternatives, such as batteries or flywheels on the bus or at the bus power station, but they add to the investment, complexity and maintenance expenses.
Unlike trams or gasoline and diesel buses, trolleybuses are almost silent, lacking the noise of an engine or of wheels on rails. Such noise as there is tends to emanate from the motor, auxiliary systems such as power steering pumps and air conditioning. Early trolleybuses without these systems were even quieter and, in the UK at least, were often referred to as the "Silent Service". The quietness did have its disadvantages though, with some pedestrians falling victim to what was also known as the "Silent Death" (in Britain) or "Whispering Death" (in Australia).
Trolleybuses are especially favoured where electricity is abundant and cheap. Examples are the extensive systems in Vancouver, Canada and Seattle, U.S., both of which draw hydroelectric power from the Columbia River and other Pacific river systems. San Francisco operates its system using hydro power from the city-owned Hetch Hetchy generating plant.
As can be seen from examples in this article, electric (trolley) buses tend to be very long-lived as compared to IC powered buses. As the basic construction of buses has not changed much in the last fifty plus years, they can be upgraded such as when air conditioning was retrofitted to many trolleybuses when it became available. Wheelchair lifts are relatively simple to add; kneeling front suspension is a common feature of air suspension on the front axle in place of springs.
In Cambridge, Massachusetts, trackless trolleys survived because Harvard Station, where several bus lines terminate, is in a tunnel that was once used by streetcars. Although diesel buses do use the tunnel, there are limitations due to exhaust fumes. Also the trackless trolleys continue to have popular support.
Disadvantages.
Re-routings, temporary or permanent, are not usually readily available outside of "downtown" areas where the buses may be re-routed via adjacent business area streets where other trolleybus routes operate. This problem was highlighted in Vancouver in July 2008, when an explosion closed several roads in the city's downtown core. Because of the closure, trolleys were forced to detour several kilometers off their route in order to stay on the wires, leaving major portions of their routes unserved and service well off schedule. Also trolley buses have unique operating characteristics, such as trolley drivers having to slow down at turns and through switches in the overhead wire system.
Some trolleybus systems have been criticised for aesthetic reasons, with city residents complaining that the jumble of overhead wires was unsightly. Intersections often have a "webbed ceiling" appearance, due to multiple crossing and converging sets of trolley wires.
Dewirements—when the trolley poles come off of the wires—sometimes occur, especially in areas subject to heavy snow. After a dewirement, trolleybuses not equipped with an auxiliary power unit are stranded without power. However, dewirements are relatively rare in modern systems with well-maintained overhead wires, hangers, fittings and "contact shoes". Trolleybuses are equipped with special insulated pole ropes which drivers use to reconnect the trolley poles with the overhead wires in case of dewirement. When approaching switches, trolleybuses usually must decelerate in order to avoid dewiring, and this deceleration can potentially add slightly to traffic congestion.
Trolleybuses cannot overtake one another in regular service unless two separate sets of wires with a switch are provided or the vehicles are equipped with off-wire capability, with the latter an increasingly common feature of new trolleybuses.
Trackless trolleys are often long-lived equipment, with limited market demand. This generally leads to higher prices relative to internal combustion buses. The long equipment life also sometimes complicates upgrades.
Off-wire power developments.
With the re-introduction of hybrid designs, trolleybuses are no longer tied to overhead wires. The Public Service Company of New Jersey, with Yellow Coach, developed "All Service Vehicles;" trackless trolleys capable of operating as gas-electric buses when off wire, and used them successfully between 1935 and 1948. Since the 1980s, trolleybus systems such as Muni in San Francisco, TransLink in Vancouver, and in Beijing, among others, have bought trolleybuses equipped with batteries to allow them to operate fairly long distances away from the wires. Supercapacitors can be also used to move buses short distances.
Trolleybuses can optionally be equipped either with limited off-wire capability—a small diesel engine or battery pack—for auxiliary or emergency use only, or full dual-mode capability. A simple auxiliary power unit can allow a trolleybus to get around a route blockage or can reduce the amount (or complexity) of overhead wiring needed at operating garages (depots). This capability has become increasingly common in newer trolleybuses, particularly in North America and Western Europe, where the vast majority of new trolleybuses delivered since the 1990s are fitted with at least limited off-wire capability. These have gradually replaced older trolleybuses which lacked such capability. In Philadelphia, new trolleybuses (known there as "trackless trolleys") that were placed in service by SEPTA in 2008 are equipped with small hybrid diesel-electric power units for operating short distances off-wire, instead of using a conventional diesel drive train or battery-only system for their off-wire movement.
King County Metro in Seattle, Washington and the MBTA in Boston use or have used dual-mode buses that run on electric power from overhead wires on a fixed right-of-way and on diesel power on city streets. Metro used special-order articulated Breda buses with the center axle driven electrically and the rear (third) axle driven by a conventional power pack, with electricity used for clean operation in the downtown transit tunnel. They were introduced in 1990 and retired in 2005, replaced by cleaner hybrid buses, although 59 of 236 had their diesel propulsion equipment removed and continue (as of 2010) in trolley bus service on non-tunnel routes. The MBTA uses dual-mode buses on its new (2004-opened) Silver Line (Waterfront).
Other considerations.
With increasing diesel fuel costs and problems caused by particulate matter and NOx emissions in cities, trolleybuses can be an attractive alternative, either as the primary transit mode or as a supplement to rapid transit and commuter rail networks.
Trolleybuses are quieter than IC vehicles. Mainly a benefit, it also provides much less warning of a trolleybus's approach. A speaker attached to the front of the vehicle can raise the noise to a desired "safe" level. This noise can be directed to pedestrians in front of the vehicle, as opposed to motor noise which typically comes from the rear of a bus and is more noticeable to bystanders than to pedestrians.
Trolleybuses can share overhead wires and other electrical infrastructure (such as substations) with tramways. This can result in cost savings when trolleybuses are added to a transport system that already has trams, though this refers only to potential savings over the cost of installing and operating trolleybuses alone.
Wire switches.
Trolleybus wire switches (called frogs in the UK) are used where a trolleybus line branches into two or where two lines join. A switch may be either in a "straight through" or "turnout" position; it normally remains in the "straight through" position unless it has been triggered, and reverts to it after a few seconds or after the pole shoe passes through and strikes a release lever. (In Boston, the resting or "default" position is the "leftmost" position.) Triggering is typically accomplished by a pair of contacts, one on each wire close to and before the switch assembly, which power a pair of electromagnets, one in each frog with diverging wires. ("Frog" generally refers to one fitting that guides one trolley wheel/shoe onto a desired wire or across one wire. Occasionally "frog" has been used to refer to the entire switch assembly.)
Multiple branches may be handled by installing more than one switch assembly. For example, to provide straight-through, left-turn or right-turn branches at an intersection, one switch is installed some distance from the intersection to choose the wires over the left-turn lane, and another switch is mounted closer to or in the intersection to choose between straight through and a right turn. (This would be the arrangement in countries such as the US, where traffic directionality is right-handed; in left-handed traffic countries such as Britain and New Zealand, the first switch (before the intersection) would be used to access the right-turn lanes, and the second switch (usually in the intersection) would be for the left-turn.)
Three common types of switches exist: Power-on/Power-off (the picture of a switch above is of this type), Selectric, and Fahslabend.
A Power-on/Power-off switch is triggered if the trolleybus is drawing considerable power from the overhead wires, usually by accelerating, at the moment the poles pass over the contacts. (The contacts are lined up on the wires in this case.) If the trolleybus "coasts" through the switch, the switch will not activate. Some trolleybuses, such as those in Philadelphia and Vancouver, have a manual "power-coast" toggle switch that turns the power on or off. This allows a switch to be triggered in situations that would otherwise be impossible, such as activating a switch while braking or accelerating through a switch without activating it. One variation of the toggle switch will simulate accelerating by causing a larger power draw (through a resistance grid) but will not simulate coasting and prevent activation of the switch by cutting the power.
A Selectric switch has a similar design, but the contacts on the wires are skewed, often at a 45-degree angle, rather than being lined up. This skew means that a trolleybus going straight through will not trigger the switch, but a trolleybus making a turn will have its poles match the contacts in a matching skew (with one pole shoe ahead of the other), which will trigger the switch regardless of power draw (accelerating versus coasting).
For a Fahslabend switch, the trolleybus' turn indicator control (or a separate driver-controlled switch) causes a coded radio signal to be sent from a transmitter, often attached to a trolley pole. The receiver is attached to the switch and causes it to trigger if the correct code is received. This has the advantage that the driver does not need to be accelerating the bus (as with a Power-on/Power-off switch) or trying to make a sharp turn (as with a Selectric switch).
Trailing switches (where two sets of wires merge) do not require action by the operator. The frog runners are pushed into the desired position by the trolley shoe, or the frog is shaped so the shoe is guided onto the exit wire without any moving parts.
Manufacturing.
Since the invention of the mode, well over 200 different makers of trolleybuses have existed – mostly commercial manufacturers, but in some cases (particularly in China), trolleybuses have been built by the publicly owned trolleybus operating companies or authorities. Of the defunct or former trolleybus manufacturers, the largest producers – ones whose production totalled more than 1,000 units each – included the U.S. companies Brill (approx. 3,250 total), Pullman-Standard (2,007), and Marmon-Herrington (1,624); the English companies AEC (approx. 1,750), British United Traction (BUT) (1,573), Leyland (1,420) and Sunbeam (1,379); France's Vétra (more than 1,750); and the Italian builders Alfa Romeo (2,044) and Fiat (approx. 1,700). Also, Canadian Car and Foundry built 1,114 trolleybuses based on designs by Brill.
As of the 2010s, at least 30 trolleybus manufacturers exist. They include some companies that have been building trolleybuses for several decades, such as Škoda, Trolza (formerly Uritsky, or ZIU) and New Flyer, among others, along with several younger companies. Current trolleybus manufacturers in western and central Europe include Solaris, Van Hool and Hess, among others. In Mexico, trolleybus production ended when MASA, which had built more than 860 trolleybuses since 1979, was acquired in 1998 by Volvo. However, Dina, which is now that country's largest bus and truck manufacturer, began building trolleybuses in 2013.
Transition to low-floor designs.
A significant change to trolleybus designs starting in the early 1990s was the introduction of low-floor bus models, which began only a few years after the first such models were introduced for motorbuses. These have gradually replaced high-floor designs, and by 2012, every existing trolleybus system in Western Europe had purchased low-floor trolleybuses, with the La Spezia (Italy) system being the last one to do so, and several systems in other parts of the world have purchased low-floor vehicles.
In the United States, the Americans with Disabilities Act of 1990 required that all new transit vehicles placed into service after 1 July 1993 be accessible to persons in wheelchairs. Some transit agencies had already begun to accommodate such passengers by purchasing buses with wheelchair lifts, and early examples of fleets of lift-equipped trolleybuses included 109 AM General trolleybuses built for the Seattle trolleybus system in 1979 and the retrofitting of lifts in 1983 to 64 Flyer E800s in the Dayton system's fleet.
Trolleybuses in other countries continued to lack disabled access until the 1990s, when the first two low-floor trolleybus models were introduced, both built in 1991, a "Swisstrolley" demonstrator built by Switzerland's NAW/Hess and an N6020 demonstrator built by Neoplan. The first production-series low-floor trolleybuses were built in 1992: 13 by NAW for the Geneva system and 10 Gräf & Stift for the . By 1995, such vehicles were also being made by several other European manufacturers, including Skoda, Breda, Ikarus and Van Hool. The first Solaris "Trollino" made its debut in early 2001. In the former Soviet Union countries, Belarus' Belkommunmash built its first low-floor trolleybus (model AKSM-333) in 1999, and other manufacturers in the former Soviet countries joined the trend in the early 2000s.
The emergence of low-floor designs raised an unforeseen issue for some European trolleybus operators. The lifespan of a trolleybus is typically much longer than that of a motorbus, with 20–30 years being "not uncommon", but with the 1990s trend moving strongly toward low-floor vehicles, some operators whose high-floor trolleybuses were only a few years old felt pressure to retire them at a much younger age, so as to replace them with low-floor trolleybuses. Responses varied, with some systems keeping their high-floor fleets, and others retiring them early but, in many instances, selling them secondhand for continued use in countries where there was a demand for low-cost secondhand trolleybuses, in particular in Romania and Bulgaria. The Lausanne system dealt with this dilemma in the 1990s by purchasing new low-floor passenger trailers to be towed by its high-floor trolleybuses, a choice later also made by Lucerne.
Outside Europe, 14 vehicles built by, and for, the Shanghai trolleybus system in mid-1999 were the first reported low-floor trolleybuses in Southeast Asia. Wellington, New Zealand, took delivery of its first low-floor trolleybus in March 2003, and by the end of 2009 had renewed its entire fleet with such vehicles.
In the Americas, the first low-floor trolleybus was a Busscar vehicle supplied to the São Paulo EMTU system in 2001. In North America, wheelchair lifts were again chosen for disabled access in new trolleybuses delivered to San Francisco in 1992–94, to Dayton in 1996–1999, and to Seattle in 2001–2002, but the first low-floor trolleybus was built in 2003, with the first of 28 Neoplan vehicles for the Boston system. Subsequently, the Vancouver system and the Philadelphia system have converted entirely to low-floor vehicles, and in 2013 the Seattle and Dayton systems both placed orders for their first low-floor trolleybuses. Outside São Paulo, almost all trolleybuses currently in service in Latin America are high-floor models built before 2000. However, in 2013, the first domestically manufactured low-floor trolleybuses were introduced in both Argentina and Mexico.
With regard to non-passenger aspects of vehicle design, the transition from high-floor to low-floor has meant that some equipment previously placed under the floor has been moved to the roof. Some transit operators have needed to modify their maintenance facilities to accommodate this change, a one-time expense.
Double-decker trolleybuses.
Since the end of 1997, no double-decker trolleybuses have been in service anywhere in the world, but in the past several manufacturers made such vehicles. Most builders of double-deck trolleybuses were in the United Kingdom, but there were a few, usually solitary, instances of such trolleybuses being built in other countries, including in Germany by Henschel (for Hamburg); in Italy by Lancia (for Porto, Portugal); in Russia by the Yaroslavl motor plant (for Moscow) and in Spain by Maquitrans (for Barcelona). British manufacturers of double-deck trolleybuses included AEC, BUT, Crossley, Guy, Leyland, Karrier, Sunbeam and others.
In 2001, Citybus (Hong Kong) converted a Dennis Dragon (#701) into a double-decker trolleybus, and it was tested on a 300-metre track in Wong Chuk Hang in that year. Hong Kong decided not to build a trolleybus system, and the testing of this prototype did not lead to any further production of vehicles.
Use and preservation.
There are currently 300 cities or metropolitan areas where trolleybuses are operated, and more than 500 additional trolleybus systems have existed in the past. For an overview, by country, see Trolleybus usage by country, and for complete lists of trolleybus systems by location, with dates of opening and (where applicable) closure, see List of trolleybus systems and the related lists indexed there.
Of the systems existing as of 2012, the majority are located in Europe and Asia, including 85 in Russia and 43 in Ukraine. However, there are eight systems existing in North America, nine in South America and one in Oceania, specifically in New Zealand.
Trolleybuses have been preserved in most of the countries where they have operated. The United Kingdom has the largest number of preserved trolleybuses with more than 110, while the United States has around 70. Most preserved vehicles are on static display only, but a few museums are equipped with a trolleybus line, allowing trolleybuses to operate for visitors. Museums with operational trolleybus routes include three in the UK – the Trolleybus Museum at Sandtoft, the East Anglia Transport Museum and the Black Country Living Museum – and three in the United States – the Illinois Railway Museum, the Seashore Trolley Museum and the Shore Line Trolley Museum – but operation of trolleybuses does not necessarily occur on a regular schedule of dates at these museums.

</doc>
<doc id="54418" url="https://en.wikipedia.org/wiki?curid=54418" title="The Magician (U.S. TV series)">
The Magician (U.S. TV series)

The Magician is an American television series that ran during the 1973–1974 season. It starred Bill Bixby as stage illusionist Anthony "Tony" Blake, a playboy philanthropist who used his skills to solve difficult crimes as needed. In the series pilot, the character was named Anthony Dorian; the name was changed due to a conflict with the name of a real life stage magician.
Premise.
Blake was a professional stage magician who used his skills to solve crimes and help the helpless. Years earlier, Blake had been in prison on a trumped-up espionage charge in an unnamed country in South America. He discovered a way to escape with his cellmate, which began his interest in escapology. The cellmate died and left him a fortune. The escape, presumably followed by exoneration of the false charges that had led to it, led to Blake's pursuit of a career in stage magic, which made him famous. He never forgot his unjust imprisonment, and it motivated him to seek justice for others.
Initially, Blake used his Boeing 720 jetliner as a base of operations; it was outfitted as a mobile residence ("It's like any other mobile home, only faster.") with live-in pilot Jerry Anderson (Jim Watkins). Blake frequently received assistance from acerbic columnist Max Pomeroy, portrayed by Keene Curtis, and his brilliant, wheelchair-using son Dennis (Todd Crespi). Midway through the program's run, the idea of the airplane was dropped and Blake took up residence in a posh apartment at The Magic Castle, a real club devoted to magic acts. At the same time, the supporting cast of the show was replaced with a new, single character, Dominick, a somewhat comical sidekick played by Joseph Sirola. No explanation for the changes was given in the series. Jerry continued to make occasional minor appearances (and Watkins retained a place in the opening credits) and Tony recruited Jerry and Max together for one further case in the new format.
Some episodes featured Larry Anderson (who later hosted "Truth or Consequences" and created the "JawDroppers" video magic course) as Blake's assistant.
Magic on the program.
The show is noteworthy in that Bixby, a keen amateur magician, insisted on doing all of the magic himself, without any trick photography, although it was not possible for this to be the case in the TV-movie/pilot. Episodes of the regular series were preceded by an announcement that the magic tricks were accomplished without trick photography. He was instructed in these performances by the program's technical advisor, Mark Wilson, who was credited as "magic consultant." Once the format changed to have the hero based in a magic club, Wilson could occasionally be seen on the stage there, as well. In addition to escapes, Bixby performed feats of sleight of hand, mentalism, and stage illusions. After the series' cancellation, Bixby went on to host a string of magic specials on NBC and a series, "The Wonderful World of Magic", in first-run syndication.
Influence.
Though it ran only a single season, "The Magician" was an influence on later series. The show was a favorite of "The X-Files" creator Chris Carter, who worked it into Special Agent Fox Mulder's "origin" story: a teenaged Mulder was waiting to watch "The Magician" when his sister Samantha was abducted by mysterious forces.
In the "Quantum Leap" episode "The Great Spontini", Scott Bakula's character, Dr. Sam Beckett, leaps into an amateur magician in 1974 who aspires to appear on Bill Bixby's "The Magician"; however, owing to his partial amnesia, Dr. Beckett, at first, can only recall Bixby's connection with "The Incredible Hulk," which had not been made at that time.
"The Incredible Hulk" series featured an episode that paid homage to both "The Magician" and Bixby's earlier series, "My Favorite Martian". In "The Incredible Hulk's" "My Favorite Magician" episode, Bixby's character became the temporary apprentice to a stage magician played by Bixby's "Martian" co-star, Ray Walston. Mark Wilson was on hand again as the episode's "magic consultant" as well. In addition, "Martian" co-star Pamela Britton appeared in an episode of "The Magician".
Actor Andrew J. Robinson has also stated that his "" character, Elim Garak, was partially influenced by Bixby's character.
See also.
Blake is but one in a long line of magicians who double as detectives. Others include:

</doc>
<doc id="54419" url="https://en.wikipedia.org/wiki?curid=54419" title="Tangram">
Tangram

The tangram () is a dissection puzzle consisting of seven flat shapes, called "tans", which are put together to form shapes. The objective of the puzzle is to form a specific shape (given only an outline or silhouette) using all seven pieces, which may not overlap. It is reputed to have been invented in China during the Song Dynasty, and then carried over to Europe by trading ships in the early 19th century. It became very popular in Europe for a time then, and then again during World War I. It is one of the most popular dissection puzzles in the world. A Chinese psychologist has termed the tangram "the earliest psychological test in the world", albeit one made for entertainment rather than analysis.
Etymology.
The origin of the word 'tangram' is unclear. The '-gram' element is apparently from Greek γράμμα 'letter'. The 'tan-' element is variously conjectured to be from Chinese "t'an" 'to extend' or Cantonese "t'ang" 'Chinese'.
History.
Reaching the Western world (1815–1820s).
The tangram had already been around in China for a long time when it was first brought to America by Captain M. Donnaldson, on his ship, "Trader", in 1815. When it docked in Canton, the captain was given a pair of Sang-Hsia-koi's (author) Tangram books from 1815. They were then brought with the ship to Philadelphia, where it docked in February 1816. The first Tangram book to be published in America was based on the pair brought by Donnaldson.
The puzzle was originally popularized by "The Eighth Book Of Tan", a fictitious history of Tangram, which claimed that the game was invented 4,000 years prior by a god named Tan. The book included 700 shapes, some of which are possible to solve. 
The puzzle eventually reached England, where it became very fashionable indeed. The craze quickly spread to other European countries. This was mostly due to a pair of British Tangram books, "The Fashionable Chinese Puzzle", and the accompanying solution book, "Key". Soon, tangram sets were being exported in great number from China, made of various materials, from glass, to wood, to tortoise shell.
Many of these unusual and exquisite tangram sets made their way to Denmark. Danish interest in tangrams skyrocketed around 1818, when two books on the puzzle were published, to much enthusiasm. The first of these was "Mandarinen" (About the Chinese Game). This was written by a student at Copenhagen University, which was a non-fictional work about the history and popularity of tangrams. The second, "Det nye chinesiske Gaadespil" (The new Chinese Puzzle Game), consisted of 339 puzzles copied from "The Eighth Book of Tan", as well as one original.
One contributing factor in the popularity of the game in Europe was that although the Catholic Church forbade many forms of recreation on the sabbath, they made no objection to puzzle games such as the tangram.
Second craze in Germany and United States (1891–1920s).
Tangrams were first introduced to the German public by industrialist Friedrich Adolf Richter around 1891. The sets were made out of stone or false earthenware, and marketed under the name "The Anchor Puzzle".
More internationally, the First World War saw a great resurgence of interest in Tangrams, on the homefront and trenches of both sides. During this time, it occasionally went under the name of "The Sphinx" an alternative title for the "Anchor Puzzle" sets.
Paradoxes.
A tangram paradox is a dissection fallacy: Two figures composed with the same set of pieces, one of which seems to be a proper subset of the other. One famous paradox is that of the two monks, attributed to Dudeney, which consists of two similar shapes, one with and the other missing a foot. In reality, the area of the foot is compensated for in the second figure by a subtly larger body. Another tangram paradox is proposed by Sam Loyd in "The 8th Book of Tan":
The two monks paradox – two similar shapes but one missing a foot.
The Magic Dice Cup tangram paradox – from Sam Loyd’s book "The Eighth Book of Tan'" (1903). Each of these cups was composed using the same seven geometric shapes. But the first cup is whole, and the others contain vacancies of different sizes. (Notice that the one on the left is slightly shorter than the other two. The one in the middle is ever-so-slightly wider than the one on the right, and the one on the left is narrower still.
Clipped square tangram paradox – from Sam Loyd’s book "The Eighth Book of Tan" (1903).
Number of configurations.
Over 6500 different tangram problems have been created from 19th century texts alone, and the current number is ever-growing. The number is finite, however. Fu Traing Wang and Chuan-Chin Hsiung proved in 1942 that there are only thirteen convex tangram configurations (configurations such that a line segment drawn between any two points on the configuration's edge always pass through the configuration's interior, i.e., configurations with no recesses in the outline).
Pieces.
Choosing a unit of measurement so that the seven pieces can be assembled to form a square of side one unit and having area one square unit, the seven pieces are:
Of these seven pieces, the parallelogram is unique in that it has no reflection symmetry but only rotational symmetry, and so its mirror image can be obtained only by flipping it over. Thus, it is the only piece that may need to be flipped when forming certain shapes.

</doc>
<doc id="54420" url="https://en.wikipedia.org/wiki?curid=54420" title="Ryōji Noyori">
Ryōji Noyori

Education and career.
Ryōji Noyori was born in Kobe, Japan. He became fascinated with chemistry at age twelve, after hearing a presentation on nylon. He saw the power of chemistry as being the ability to "produce high value from almost nothing". He was a student at Kyoto University, an instructor in the research group of Hitoshi Nozaki, and an associate professor at Nagoya University. After postdoctoral work with Elias J. Corey at Harvard he returned to Nagoya, becoming a full professor in 1972. He is still based at Nagoya, and served as president of RIKEN, a multi-site national research initiative with an annual budget of $800 million, from 2003 to 2015. 
Research.
Noyori believes strongly in the power of catalysis and of green chemistry; in a recent article he argues for the pursuit of "practical elegance in synthesis". In this article he states that ""our ability to devise straightforward and practical chemical syntheses is indispensable to the survival of our species."" Elsewhere he has said that ""Research is for nations and mankind, not for researchers themselves."" He encourages scientists to be politically active: ""Researchers must spur public opinions and government policies toward constructing the sustainable society in the 21st century.""
Noyori is currently a chairman of the Education Rebuilding Council, which was set up by Japan's PM Shinzō Abe after he came to power in 2006.
Noyori is most famous for asymmetric hydrogenation using as catalysts complexes of rhodium and ruthenium, particularly those based on the BINAP ligand. (See Noyori asymmetric hydrogenation) Asymmetric hydrogenation of an alkene in the presence of (("S")-BINAP)Ru(OAc)2 is used for the commercial production of enantiomerically pure (97% ee) naproxen, used as an anti-inflammatory drug. The antibacterial agent levofloxacin is manufactured by asymmetric hydrogenation of ketones in the presence of a Ru(II) BINAP halide complex.
He has also worked on other asymmetric processes. Each year 3000 tonnes (after new expansion) of menthol are produced (in 94% ee) by Takasago International Corporation, using Noyori's method for isomerisation of allylic amines.
More recently with Philip J. Jessop, Nyori has developed an industrial process for the manufacture of "N,N"-dimethylformamide from hydrogen, dimethylamine and supercritical carbon dioxide in the presence of 2(PMe3)4 as catalyst.
Awards and honours.
The Ryoji Noyori Prize is named in his honour. In 2000 Noyori became Honorary Doctor at the University of Rennes 1, where he taught in 1995, and in 2005, he became Honorary Doctor at Technical University of Munich and RWTH Aachen University, Germany. Noyori was elected a Foreign Member of the Royal Society (ForMemRS) in 2005. He has also been awarded the Asahi Prize in 1992, the Tetrahedron Prize in 1993, the Arthur C. Cope Award in 1997, the Wolf Prize in Chemistry in 2001 and the Lomonosov Gold Medal in 2009.

</doc>
<doc id="54421" url="https://en.wikipedia.org/wiki?curid=54421" title="Sex-selective abortion">
Sex-selective abortion

Sex-selective abortion is the practice of terminating a pregnancy based upon the predicted sex of the infant. The selective abortion of female fetuses is most common in areas where cultural norms value male children over female children, especially in parts of People's Republic of China, India, Pakistan, the Caucasus, and Southeast Europe.
Sex-selective abortion affects the human sex ratio—the relative number of males to females in a given age group. Studies and reports focusing on sex-selective abortion are predominantly statistical; they assume that birth sex ratio—the overall ratio of boys and girls at birth for a regional population, is an indicator of sex-selective abortion. This assumption has been questioned by some scholars.
Scholars who support the assumption suggest that the expected birth sex ratio range is 103 to 107 males to females at birth. Countries considered to have significant practices of sex-selective abortion are those with birth sex ratios of 108 and above (selective abortion of females), and 102 and below (selective abortion of males).
Human sex ratio at birth.
Sex-selective abortion affects the human sex ratio—the relative number of males to females in a given age group. Studies and reports that discuss sex-selective abortion are based on the assumption that birth sex ratio—the overall ratio of boys and girls at birth for a regional population, is an indicator of sex-selective abortion.
The natural human sex ratio at birth was estimated, in a 2002 study, to be close to 106 boys to 100 girls. Human sex ratio at birth that is significantly different from 106 is often assumed to be correlated to the prevalence and scale of sex-selective abortion. This assumption is controversial, and a subject of continuing scientific studies.
High or low human sex ratio implies sex-selective abortion.
One school of scholars suggest that any birth sex ratio of boys to girls that is outside of the normal 105-107 range, necessarily implies sex-selective abortion. These scholars claim that both the sex ratio at birth and the population sex ratio are remarkably constant in human populations. Significant deviations in birth sex ratios from the normal range can only be explained by manipulation, that is sex-selective abortion.
In a widely cited article, Amartya Sen compared the birth sex ratio in Europe (106) and United States (105) with those in Asia (107+) and argued that the high sex ratios in East Asia, West Asia and South Asia may be due to excessive female mortality. Sen pointed to research that had shown that if men and women receive similar nutritional and medical attention and good health care then females have better survival rates, and it is the male which is the genetically fragile sex.
Sen estimated 'missing women' from extra women who would have survived in Asia if it had the same ratio of women to men as Europe and United States. According to Sen, the high birth sex ratio over decades, implies a female shortfall of 11% in Asia, or over 100 million women as missing from the 3 billion combined population of South Asia, West Asia, North Africa and China.
High or low human sex ratio may be natural.
Other scholars question whether birth sex ratio outside 103-107 can be due to natural reasons. William James and others suggest that conventional assumptions have been:
James cautions that available scientific evidence stands against the above assumptions and conclusions. He reports that there is an excess of males at birth in almost all human populations, and the natural sex ratio at birth is usually between 102 and 108. However the ratio may deviate significantly from this range for natural reasons such as early marriage and fertility, teenage mothers, average maternal age at birth, paternal age, age gap between father and mother, late births, ethnicity, social and economic stress, warfare, environmental and hormonal effects. This school of scholars support their alternate hypothesis with historical data when modern sex-selection technologies were unavailable, as well as birth sex ratio in sub-regions, and various ethnic groups of developed economies. They suggest that direct abortion data should be collected and studied, instead of drawing conclusions indirectly from human sex ratio at birth.
James hypothesis is supported by historical birth sex ratio data before technologies for ultrasonographic sex-screening were discovered and commercialized in the 1960s and 1970s, as well by reverse abnormal sex ratios currently observed in Africa. Michel Garenne reports that many African nations have, over decades, witnessed birth sex ratios below 100, that is more girls are born than boys. Angola, Botswana and Namibia have reported birth sex ratios between 94 and 99, which is quite different than the presumed 104 to 106 as natural human birth sex ratio.
John Graunt noted that in London over a 35-year period in the 17th century (1628–62), the birth sex ratio was 1.07; while Korea's historical records suggest a birth sex ratio of 1.13, based on 5 million births, in 1920s over a 10-year period. Other historical records from Asia too support James hypothesis. For example, Jiang et al. claim that the birth sex ratio in China was 116–121 over a 100-year period in the late 18th and early 19th centuries; in the 120–123 range in the early 20th century; falling to 112 in the 1930s.
Data on human sex ratio at birth.
In the United States, the sex ratios at birth over the period 1970–2002 were 105 for the white non-Hispanic population, 104 for Mexican Americans, 103 for African Americans and Native Americans, and 107 for mothers of Chinese or Filipino ethnicity. Among Western European countries c. 2001, the ratios ranged from 104 to 107. In the aggregated results of 56 Demographic and Health Surveys in African countries, the birth sex ratio was found to be 103, though there is also considerable country-to-country, and year-to-year variation.
In a 2005 study, U.S. Department of Health and Human Services reported sex ratio at birth in the United States from 1940 over 62 years. This statistical evidence suggested the following: For mothers having their first baby, the total sex ratio at birth was 106 overall, with some years at 107. For mothers having babies after the first, this ratio consistently decreased with each additional baby from 106 towards 103. The age of the mother affected the ratio: the overall ratio was 105 for mothers aged 25 to 35 at the time of birth; while mothers who were below the age of 15 or above 40 had babies with a sex ratio ranging between 94 and 111, and a total sex ratio of 104. This United States study also noted that American mothers of Hawaiian, Filipino, Chinese, Cuban and Japanese ethnicity had the highest sex ratio, with years as high as 114 and average sex ratio of 107 over the 62-year study period. Outside of United States, European nations with extensive birth records, such as Finland, report similar variations in birth sex ratios over a 250-year period, that is from 1751 to 1997 AD.
In 2013, according to CIA estimates, some countries with high birth sex ratio were Liechtenstein (126), Curacao (115), Azerbaijan (113), Armenia (112), China (112), India (112), Vietnam (112), Georgia (111), Albania (111), Grenada (110), San Marino (109), Taiwan (109), Jersey (108), Kosovo (108), Macedonia (108) and Singapore (108). Low boys to girls birth sex ratios in 2013 were estimated by CIA for Haiti (101), Barbados (101), Bermuda (101), Cayman Islands (102), Qatar (102), Kenya (102), Malawi (102), Mozambique (102), South Africa (102) and Aruba (102).
Data reliability.
The estimates for birth sex ratios, and thus derived sex-selective abortion, are a subject of dispute as well. For example, United States' CIA projects the birth sex ratio for Switzerland to be 106, while the Switzerland's Federal Statistical Office that tracks actual live births of boys and girls every year, reports the latest birth sex ratio for Switzerland as 107. Other variations are more significant; for example, CIA projects the birth sex ratio for Pakistan to be 105, United Nations FPA office claims the birth sex ratio for Pakistan to be 110, while the government of Pakistan claims its average birth sex ratio is 111.
The two most studied nations with high sex ratio and sex-selective abortion are China and India. The CIA estimates a birth sex ratio of 112 for both in recent years. However, The World Bank claims the birth sex ratio for China in 2009 was 120 boys for every 100 girls; while United Nations FPA estimates China's 2011 birth sex ratio to be 118.
For India, the United Nations FPA claims a birth sex ratio of 111 over 2008–10 period, while The World Bank and India's official 2011 Census reports a birth sex ratio of 108. These variations and data reliability is important as a rise from 108 to 109 for India, or 117 to 118 for China, each with large populations, represent a possible sex-selective abortion of about 100,000 girls.
Prenatal sex discernment.
The earliest post-implantation test, cell free fetal DNA testing, involves taking a blood sample from the mother and isolating the small amount of fetal DNA that can be found within it. When performed after week seven of pregnancy, this method is about 98% accurate.
Obstetric ultrasonography, either transvaginally or transabdominally, checks for various markers of fetal sex. It can be performed at or after week 12 of pregnancy. At this point, of fetal sexes can be correctly determined, according to a 2001 study. Accuracy for males is approximately 50% and for females almost 100%. When performed after week 13 of pregnancy, ultrasonography gives an accurate result in almost 100% of cases.
The most invasive measures are chorionic villus sampling (CVS) and amniocentesis, which involve testing of the chorionic villus (found in the placenta) and amniotic fluid, respectively. Both techniques typically test for chromosomal disorders but can also reveal the sex of the child and are performed early in the pregnancy. However, they are often more expensive and more dangerous than blood sampling or ultrasonography, so they are seen less frequently than other sex determination techniques.
China launched its first ultrasonography machine in 1979. Chinese health care clinics began introducing ultrasound technologies that could be used to determine prenatal sex in 1982. By 1991, Chinese companies were producing 5,000 ultrasonography machines per year. Almost every rural and urban hospital and family planning clinics in China had a good quality sex discernment equipment by 2001.
The launch of ultrasonography technology in India too occurred in 1979, but its expansion was slower than China. Ultrasound sex discernment technologies were first introduced in major cities of India in the 1980s, its use expanded in India's urban regions in the 1990s, and became widespread in the 2000s.
Prevalence of sex-selective abortion.
Caucasus.
Before the dissolution of the Soviet Union in the early 1990s, the birth sex ratio in Caucasus countries such as Azerbaijan, Armenia and Georgia was in the 105 to 108 range. After the collapse, the birth sex ratios sharply climbed and have remained high for the last 20 years. In Christian Armenia and Islamic Azerbaijan currently more than 115 boys are born for every 100 girls, while in Christian Georgia the birth sex ratio is about 120, a trend "The Economist" claims suggest sex-selective abortion practice in the Caucasus has been similar to those in East Asia and South Asia in recent decades.
For 2005–10 birth data, the sex ratio in Armenia is seen to be a function of birth order. Among couples having their first child, Armenia averages 138 boys for every 100 girls every year. If the first child is a son, the sex ratio of the second child of Armenian couple averages to be 85. If the first child is a daughter, the sex ratio of the second Armenian child averages to be 156 boys for 100 girls. Overall, the birth sex ratio for in Armenia exceeds 115, far higher than India's 108, claim scholars. While these high birth sex ratios suggest sex-selective abortion, there is no direct evidence of observed large-scale sex-selective abortions in Caucasus.
China.
When sex ratio began being studied in China in 1960, it was still within the normal range. However, it climbed to 111.9 by 1990 and to 118 by 2010 per its official census. Researchers believe that the causes of this sex ratio imbalance are increased female infant mortality, underreporting of female births and sex-selective abortion. According to Zeng et al. (1993), the most prominent cause is probably sex-selective abortion, but this is difficult to prove that in a country with little reliable birth data because of the hiding of “illegal” (under the One-Child Policy) births.
These illegal births have led to underreporting of female infants. Zeng et al., using a reverse survival method, estimate that underreporting keeps about 2.26% male births and 5.94% female births off the books. Adjusting for unreported illegal births, they conclude that the corrected Chinese sex ratio at birth for 1989 was 111 rather than 115. These national averages over time, mask the regional sex ratio data. For example, in some provinces such as Anhui, Jiangxi, Shaanxi, Hunan and Guangdong, sex ratio at birth is more than 130.
Traditional Chinese techniques have been used to determine sex for hundreds of years, primarily with unknown accuracy. It was not until ultrasonography became widely available in urban and rural China that sex was able to be determined scientifically. In 1986, the Ministry of Health posted the Notice on Forbidding Prenatal Sex Determination, but it was not widely followed. Three years later, the Ministry of Health outlawed the use of sex determination techniques, except for in diagnosing hereditary diseases.
However, many people have personal connections to medical practitioners and strong son preference still dominates culture, leading to the widespread use of sex determination techniques. According to Hardy, Gu, and Xie (2000), ultrasound has spread to all areas of China, as evidenced by the spread of the high sex ratio throughout the country.
Hardy, Gu, and Xie suggest sex-selective abortion is more prevalent in rural China because son preference is much stronger there. Urban areas of China, on average, are moving toward greater equality for both sexes, while rural China tends to follow more traditional views of gender. This is partially due to the belief that, while sons are always part of the family, daughters are only temporary, going to a new family when they marry. Additionally, if a woman’s firstborn child is a son, her position in society moves up, while the same is not true of a firstborn daughter.
In the past, desire for a son was manifested by large birth rates—many couples would continue to have children until they had a son. However, the combination of financial concerns and, more importantly, the One-child policy (discussed further below) have led to an increase in gender planning and selection. Even in rural areas, most women know that ultrasonography can be used for gender discernment. For each subsequent birth, Junhong found that women are over 10% more likely to have an ultrasound (39% for firstborn, 55% for second born, 67% for third born). Additionally, he found that the sex of the firstborn child impacts whether a woman will have an ultrasound in her subsequent pregnancies: 40% of women with a firstborn son have an ultrasound for their second born child, versus 70% of women with firstborn daughters. This points to a strong desire to select for a son if one has not been born yet.
Because of the lack of data about childbirth, a number of researchers have worked to learn about abortion statistics in China. One of the earliest studies by Qui (1987) found that according to cultural belief, fetuses are not thought of as human beings until they are born, leading to a cultural preference for abortion over infanticide. In fact, infanticide and infant abandonment are rather rare in China today. Instead, Junhong found that roughly 27% of women have an abortion. Additionally, he found that if a family’s firstborn was a girl, 92% of known female would-be second born fetuses were aborted.
In a 2005 study, Zhu, Lu, and Hesketh found that the highest sex ratio was for those ages 1–4, and two provinces, Tibet and Xinjiang, had sex ratios within normal limits. Two other provinces had a ratio over 140, four had ratios between 130-139, and seven had ratios between 120-129, each of which is significantly higher than the natural sex ratio.
Variance in the one child policy has led to three types of provinces. Zhu et al. call Type 1, the most restrictive, policy where 40% of couples are permitted to have a second child but generally only if the first is a girl. In Type 2 provinces, any couple is permitted to have a second child if the first born is a girl or if the parents petition “hardship” and the petition is accepted by local officials. Type 3 provinces, typically sparsely populated, allow couples a second child and sometimes a third, irrespective of sex. Zhu et al. find that Type 2 provinces have the highest birth sex ratios, as seen in Henan, Anhui, Jiangxi, Hunan, Guangdong, and Hainan.
High sex ratio trends in China is projected, by 2020, to create a pool of 55 million excess young adult men than women. According to Junhong, many males between the ages of 28 and 49 are unable to find a partner and thus remain unmarried. Families in China are aware of the critical lack of female children and its implication on marriage prospects in the future; many parents are beginning to work extra when their sons are young so that they will be able to pay for a bride for them.
The birth sex ratio in China, according to a 2012 news report, has decreased to 117 males born for every 100 females.
India.
India’s 2001 census revealed a national 0–6 age child sex ratio of 108, which increased to 109 according to 2011 census (927 girls per 1000 boys and 919 girls per 1000 boys respectively, compared to expected normal ratio of 943 girls per 1000 boys). The national average masks the variations in regional numbers according to 2011 census—Haryana’s ratio was 120, Punjab’s ratio was 118, Jammu & Kashmir was 116, and Gujarat’s ratio was 111. The 2011 Census found eastern states of India had birth sex ratios between 103 and 104, lower than normal. In contrast to decadal nationwide census data, small non-random sample surveys report higher child sex ratios in India.
The child sex ratio in India shows a regional pattern. India’s 2011 census found that all eastern and southern states of India had a child sex ratio between 103 and 107, typically considered as the “natural ratio.” The highest sex ratios were observed in India's northern and northwestern states - Haryana (120), Punjab (118) and Jammu & Kashmir (116). The western states of Maharashtra and Rajasthan 2011 census found a child sex ratio of 113, Gujarat at 112 and Uttar Pradesh at 111.
The Indian census data suggests there is a positive correlation between abnormal sex ratio and better socio-economic status and literacy. Urban India has higher child sex ratio than rural India according to 1991, 2001 and 2011 Census data, implying higher prevalence of sex selective abortion in urban India. Similarly, child sex ratio greater than 115 boys per 100 girls is found in regions where the predominant majority is Hindu, Muslim, Sikh or Christian; furthermore "normal" child sex ratio of 104 to 106 boys per 100 girls are also found in regions where the predominant majority is Hindu, Muslim, Sikh or Christian. These data contradict any hypotheses that may suggest that sex selection is an archaic practice which takes place among uneducated, poor sections or particular religion of the Indian society.
Rutherford and Roy, in their 2003 paper, suggest that techniques for determining sex prenatally that were pioneered in the 1970s, gained popularity in India. These techniques, claim Rutherford and Roy, became broadly available in 17 of 29 Indian states by the early 2000s. Such prenatal sex determination techniques, claim Sudha and Rajan in a 1999 report, where available, favored male births.
Arnold, Kishor, and Roy, in their 2002 paper, too hypothesize that modern fetal sex screening techniques have skewed child sex ratios in India. Ganatra et al., in their 2000 paper, use a small survey sample to estimate that of reported abortions followed a sex determination test.
Mevlude Akbulut-Yuksel and Daniel Rosenblum, in their 2012 paper, find that despite numerous publications and studies, there is limited formal evidence on the effects of the continued spread of ultrasound technology on missing women in India. They conclude, contrary to common belief, that the recent rapid spread of ultrasound in India, from the 1990s through 2000s, did not cause a concomitant rise in sex-selection and prenatal female abortion.
The Indian government and various advocacy groups have continued the debate and discussion about ways to prevent sex selection. The immorality of prenatal sex selection has been questioned, with some arguments in favor of prenatal discrimination as more humane than postnatal discrimination by a family that does not want a female child. Others question whether the morality of sex selective abortion is any different over morality of abortion when there is no risk to the mother nor to the fetus, and abortion is used as a means to end an unwanted pregnancy?
India passed its first abortion-related law, the so-called Medical Termination of Pregnancy Act of 1971, making abortion legal in most states, but specified legally acceptable reasons for abortion such as medical risk to mother and rape. The law also established physicians who can legally provide the procedure and the facilities where abortions can be performed, but did not anticipate sex selective abortion based on technology advances.
With increasing availability of sex screening technologies in India through the 1980s in urban India, and claims of its misuse, the Government of India passed the Pre-natal Diagnostic Techniques Act (PNDT) in 1994. This law was further amended into the Pre-Conception and Pre-natal Diagnostic Techniques (Regulation and Prevention of Misuse) (PCPNDT) Act in 2004 to deter and punish prenatal sex screening and sex selective abortion. The impact of the law and its enforcement is unclear. United Nations Population Fund and India's National Human Rights Commission, in 2009, asked the Government of India to assess the impact of the law. The Public Health Foundation of India, an activist NGO in its 2010 report, claimed a lack of awareness about the Act in parts of India, inactive role of the Appropriate Authorities, ambiguity among some clinics that offer prenatal care services, and the role of a few medical practitioners in disregarding the law.
The Ministry of Health and Family Welfare of India has targeted education and media advertisements to reach clinics and medical professionals to increase awareness. The Indian Medical Association has undertaken efforts to prevent prenatal sex selection by giving its members "Beti Bachao" (save the daughter) badges during its meetings and conferences.
MacPherson estimates that 100,000 abortions every year continue to be performed in India solely because the fetus is female.
Southeast Europe.
According to Eurostat and birth record data over 2008–11, the birth sex ratios of Albania and Montenegro are currently 112 and 110 respectively. In recent years, the birth registration data for Macedonia and Kosovo indicate birth sex ratios above 108; for example, in 2011 the birth sex ratio was 108 in Macedonia, while in 2010 the birth sex ratio for Kosovo was 112. Scholars claim this suggests that sex-selective abortions are becoming common in southeast Europe.
United States.
Like in other countries, sex-selective abortion is difficult to track in the United States because of lack of data.
While the majority of parents in United States do not practice sex-selective abortion, there is certainly a trend toward male preference. According to a 2011 Gallup poll, if they were only allowed to have one child, 40% of respondents said they would prefer a boy, while only 28% preferred a girl. When told about prenatal sex selection techniques such as sperm sorting and in vitro fertilization embryo selection, 40% of Americans surveyed thought that picking embryos by sex was an acceptable manifestation of reproductive rights. These selecting techniques are available at about half of American fertility clinics, as of 2006.
However, it is notable that minority groups that immigrate into the United States bring their cultural views and mindsets into the country with them. A study carried out at a Massachusetts infertility clinic shows that the majority of couples using these techniques, such as Preimplantation genetic diagnosis came from a Chinese or Asian background. This is thought to branch from the social importance of giving birth to male children in China and other Asian countries.
Because of this movement toward sex preference and selection, many bans on sex-selective abortion have been proposed at the state and federal level. In 2010 and 2011, sex-selective abortions were banned in Oklahoma and Arizona, respectively. Legislators in Georgia, West Virginia, Michigan, Minnesota, New Jersey, and New York have also tried to pass acts banning the procedure.
Other countries.
A 2013 study by John Bongaarts based on surveys in 61 major countries calculates the sex ratios that would result if parents had the number of sons and daughters they want. In 35 countries, claims Bongaarts, the desired birth sex ratio in respective countries would be more than 110 boys for every 100 girls if parents in these countries actually get a gender what they hope for (higher than India’s, which The Economist claims is 108).
Other countries with large populations but high sex ratios include Pakistan and Vietnam. United Nations Population Fund, in its 2012 report, claims the birth sex ratio of Vietnam at 111 with its densely populated Red River Delta region at 116; for Pakistan, the UN estimates the birth sex ratio to be 110. The urban regions of Pakistan, particularly its densely populated region of Punjab, report a sex ratio above 112 (less than 900 females per 1000 males). Hudson and Den Boer estimate the resulting deficit to be about 6 million missing girls in Pakistan than what would normally be expected. Three different research studies, according to Klausen and Wink, note that Pakistan had the world's highest % of missing girls, relative to its total pre-adult female population. Singapore has reported a birth sex ratio of 108. Taiwan has reported a sex ratio at birth between 1.07 and 1.11 every year, across 4 million births, over the 20-year period from 1991 to 2011, with the highest birth sex ratios in the 2000s.
Abnormal sex ratios at birth, possibly explained by growing incidence of sex-selective abortion, have also been noted in some other countries outside South and East Asia. According to the 2011 CIA estimates, countries with more than 110 males per 100 females at birth also include Albania and former Soviet republics of Armenia and Azerbaijan.
A study of the 2000 United States Census suggests possible male bias in families of Chinese, Korean and Indian immigrants, which was getting increasingly stronger in families where first one or two children were female. In those families where the first two children were girls, the birth sex ratio of the third child was 151.
Estimates of missing women.
Estimates of implied missing girls, considering the "normal" birth sex ratio to be the 103–107 range, vary considerably between researchers and underlying assumptions for expected post-birth mortality rates for men and women. For example, a 2005 study estimated that over 90 million females were "missing" from the expected population in Afghanistan, Bangladesh, China, India, Pakistan, South Korea and Taiwan alone, and suggested that sex-selective abortion plays a role in this deficit. For early 1990s, Sen estimated 107 million missing women, Coale estimated 60 million as missing, while Klasen estimated 89 million missing women in China, India, Pakistan, Bangladesh, Nepal, West Asia and Egypt. Guilmoto, in his 2010 report, uses recent data (except for Pakistan), and estimates a much lower number of missing girls, but notes that the higher sex ratios in numerous countries have created a gender gap - shortage of girls - in the 0–19 age group.
Reasons for sex-selective abortion.
Various theories have been proposed as possible reasons for sex-selective abortion. Culture rather than economic conditions is favored by some researchers because such deviations in sex ratios do not exist in sub-Saharan Africa, Latin America, and the Caribbean. Other hypotheses include disparate gender-biased access to resources, and attempts to control population growth such as using one child policy.
Some demographers question whether sex-selective abortion or infanticide claims are accurate, because underreporting of female births may also explain high sex ratios. Natural reasons may also explain some of the abnormal sex ratios. In contrast to these possible causes of abnormal sex ratio, Klasen and Wink suggest India and China’s high sex ratios are primarily the result of sex-selective abortion.
Cultural preference.
The reason for intensifying sex-selection abortion in China and India can be seen through history and cultural background. Generally, before the information era, male babies were preferred because they provided manual labor and continuation of the family lineage. Labor is still important in developing nations as China and India, but when it comes to family lineage, it is of great importance.
The selective abortion of female fetuses is most common in areas where cultural norms value male children over female children for a variety of social and economic reasons. A son is often preferred as an "asset" since he can earn and support the family; a daughter is a "liability" since she will be married off to another family, and so will not contribute financially to her parents. Sex selective female abortion is a continuation, in a different form, of a practice of female infanticide or withholding of postnatal health care for girls in certain households. Furthermore, in some cultures sons are expected to take care of their parents in their old age. These factors are complicated by the effect of diseases on child sex ratio, where communicable and noncommunicable diseases affect males and females differently.
In modern East Asia, a large part of the pattern of preferences leading to this practice can be condensed simply as a desire to have a male heir. Monica Das Gupta (2005) observes, from 1989 birth data for China, there was no evidence of selective abortion of female fetuses among firstborn children. However, there was a strong preference for a boy if the first born was a girl.
Disparate gendered access to resources.
Although there is significant evidence of the prevalence of sex-selective abortions in many nations (especially India and China), there is also evidence to suggest that some of the variation in global sex ratios is due to disparate access to resources. As MacPherson (2007) notes, there can be significant differences in gender violence and access to food, healthcare, immunizations between male and female children. This leads to high infant and childhood mortality among girls, which causes changes in sex ratio.
Disparate, gendered access to resources appears to be strongly linked to socioeconomic status. Specifically, poorer families are sometimes forced to ration food, with daughters typically receiving less priority than sons (Klasen and Wink 2003). However, Klasen’s 2001 study revealed that this practice is less common in the poorest families, but rises dramatically in the slightly less poor families. Klasen and Wink’s 2003 study suggests that this is “related to greater female economic independence and fewer cultural strictures among the poorest sections of the population.” In other words, the poorest families are typically less bound by cultural expectations and norms, and women tend to have more freedom to become family breadwinners out of necessity. 
Increased sex ratios can be caused by disparities in aspects of life other than vital resources. According to Sen (1990), differences in wages and job advancement also have a dramatic effect on sex ratios. This is why high sex ratios are sometimes seen in nations with little sex-selective abortion. Additionally, high female education rates are correlated with lower sex ratios (World Bank 2011).
Lopez and Ruzikah (1983) found that, when given the same resources, women tend to outlive men at all stages of life after infancy. However, globally, resources are not always allocated equitably. Thus, some scholars argue that disparities in access to resources such as healthcare, education, and nutrition play at least a small role in the high sex ratios seen in some parts of the world (Klasen and Wink 2003). For example, Alderman and Gerter (1997) found that unequal access to healthcare is a primary cause of female death in developing nations, especially in Southeast Asia. Moreover, in India, lack of equal access to healthcare has led to increased disease and higher rates of female mortality in every age group until the late thirties (Sen 1990). This is particularly noteworthy because, in regions of the world where women receive equal resources, women tend to outlive men (Sen 1990).
Economic disadvantage alone may not always lead to increased sex ratio, claimed Sen in 1990. For example, in sub-Saharan Africa, one of the most economically disadvantaged regions of the world, there is an excess of women. So, if economic disadvantage is uncorrelated with sex ratio in Africa, some other factor(s) may be at play. More detailed analysis of African demographics, in 2002, suggests that Africa too has wide variation in birth sex ratios (from 1.01 in Bantu populations of East Africa to 1.08 in Nigeria and Ethiopia). Thus economic disadvantage remains a possible unresolved hypothesis for Africa as well.
One-child policy.
Following the 1949 creation of the People's Republic of China, the issue of population control came into the national spotlight. In the early years of the Republic, leaders believed that telling citizens to reduce their fertility was enough, repealing laws banning contraception and instead promoting its use. However, the contraceptives were not widely available, both because of lack of supply and because of cultural taboo against discussing sex. Efforts were slowed following the famine of 1959–61 but were resumed shortly thereafter with virtually the same results. Then, in 1964, the Family Planning Office was established to enforce stricter guidelines regarding fertility and it was moderately successful.
In 1979, the government adopted the One-Child Policy, which limited many families to one child, unless specified by provincial regulations. It was instituted as an attempt to boost the Chinese economy. Under it, families who break rules regarding the number of children they are allowed are given various punishments (primarily monetary), dependent upon the province in which they live.
As stated above, the sex ratios of a province are largely determined by the type of restriction placed upon the family, pointing to the conclusion that much of the imbalance in sex ratio in China can be attributed to the policy. Research by Junhong (2001) found that many parents are willing to pay to ensure that their child is male (especially if their first child is female), but will not do the same to ensure their child is female. Likely, fear of the harsh monetary punishments of the One-Child Policy make ensuring a son’s birth a smart investment. Therefore, son’s cultural and economic importance to families and the large expenses associated with multiple children are primary factors leading to China’s disparate sex ratio.
In 2013, China announced plans to formally change the One-Child policy, making it less stringent. The National People’s Congress has changed the policy to allow couples to have two children, so long as one of the partners is an only child. This change was not sparked by sex ratios, but rather by an aging population that is causing the workforce to grow increasingly smaller. It is estimated that this new law will lead to two million more births per year and could cause a baby boom in China. Unfortunately, many of China’s social problems are based on overpopulation. So, it is unclear if this new law will actually lead to women being more valued in Chinese society as the number of citizens increases.
Trivers–Willard hypothesis.
The Trivers–Willard hypothesis argues that available resources affect male reproductive success more than female and that consequently parents should prefer males when resources are plentiful and females when resources are scarce. This has been applied to resource differences between individuals in a society and also to resource differences between societies. Empirical evidence is mixed with higher support in better studies according to Cronk in a 2007 review. One example, in a 1997 study, of a group with a preference for females was Romani in Hungary, a low status group. They "had a female-biased sex ratio at birth, were more likely to abort a fetus after having had one or more daughters, nursed their daughters longer, and sent their daughters to school for longer."
Societal effects.
Missing women.
The idea of “missing women” was first suggested by Amartya Sen, one of the first scholars to study high sex ratios and their causes globally, in 1990. In order to illustrate the gravity of the situation, he calculated the number of women that were not alive because of sex-selective abortion or discriminatory practices. He found that there were 11 percent fewer women than there “should” have been, if China had the natural sex ratio. This figure, when combined with statistics from around the world, led to a finding of over 100 million missing women. In other words, by the early 1990s, the number of missing women was “larger than the combined casualties of all famines in the twentieth century” (Sen 1990).
This has led to particular concern due to a critical shortage of wives. In some rural areas, there is already a shortage of women, which is tied to migration into urban areas (Park and Cho 1995). In South Korea and Taiwan, high male sex ratios and declining birth rates over several decades have led to cross-cultural marriage between local men and foreign women from countries such as mainland China, Vietnam and the Philippines. However, sex-selective abortion is not the only cause of this phenomenon; it is also related to migration and declining fertility.
Trafficking and sex work.
Some scholars argue that as the proportion of women to men decreases globally, there will be an increase in trafficking and sex work (both forced and self-elected), as many people will be willing to do more to obtain a sexual partner (Junhong 2001). Already, there are reports of women from Vietnam, Myanmar, and North Korea systematically trafficked to mainland China and Taiwan and sold into forced marriages. Moreover, Ullman and Fidell (1989) suggested that pornography and sex-related crimes of violence (i.e., rape and molestation) would also increase with an increasing sex ratio.
Widening of the gender social gap.
As Park and Cho (1995) note, families in areas with high sex ratios that have mostly sons tend to be smaller than those with mostly daughters (because the families with mostly sons appear to have used sex-selective techniques to achieve their “ideal” composition). Particularly in poor areas, large families tend to have more problems with resource allocation, with daughters often receiving fewer resources than sons. Blake (1989) is credited for noting the relationship between family size and childhood “quality.” Therefore, if families with daughters continue to be predominantly large, it is likely that the social gap between genders will widen due to traditional cultural discrimination and lack of resource availability.
Guttentag and Secord (1983) hypothesized that when the proportion of males throughout the world is greater, there is likely to be more violence and war.
Potential positive effects.
Some scholars believe that when sex ratios are high, women actually become valued more because of their relative shortage. Park and Cho (1995) suggest that as women become more scarce, they may have “increased value for conjugal and reproductive functions” (75). Eventually, this could lead to better social conditions, followed by the birth of more women and sex ratios moving back to natural levels. This claim is supported by the work of demographer Nathan Keifitz. Keifitz (1983) wrote that as women become fewer, their relative position in society will increase. However, to date, no data has supported this claim.
It has been suggested by Belanger (2002) that sex-selective abortion may have positive effects on the mother choosing to abort the female fetus. This is related to the historical duty of mothers to produce a son in order to carry on the family name. As previously mentioned, women gain status in society when they have a male child, but not when they have a female child. Oftentimes, bearing of a son leads to greater legitimacy and agency for the mother. In some regions of the world where son preference is especially strong, sonless women are treated as outcasts. In this way, sex-selective abortion is a way for women to select for male fetuses, helping secure greater family status.
Goodkind (1999) argues that sex-selective abortion should not be banned purely because of its discriminatory nature. Instead, he argues, we must consider the overall lifetime possibilities of discrimination. In fact, it is possible that sex-selective abortion takes away much of the discrimination women would face later in life. Since families have the option of selecting for the fetal sex they desire, if they choose not to abort a female fetus, she is more likely to be valued later in life. In this way, sex-selective abortion may be a more humane alternative to infanticide, abandonment, or neglect. Goodkind (1999) poses an essential philosophical question, “if a ban were enacted against prenatal sex testing (or the use of abortion for sex-selective purposes), how many excess postnatal deaths would a society be willing to tolerate in lieu of whatever sex-selective abortions were avoided?”
Sex-selective abortion in the context of abortion.
MacPherson estimates that 100,000 sex-selective abortions every year continue to be performed in India. For a contrasting perspective, in the United States with a population th of India, over 1.2 million abortions every year were performed between 1990 and 2007. In England and Wales with a population th of India, over 189,000 abortions were performed in 2011, or a yearly rate of 17.5 abortions per 1,000 women aged 15–44. The average for the European Union was 30 abortions per year per 1,000 women.
Many scholars have noted the difficulty in reconciling the discriminatory nature of sex-selective abortion with the right of women to have control over their own bodies. This conflict manifests itself primarily when discussing laws about sex-selective abortion. Weiss (1995:205) writes: "The most obvious challenge sex-selective abortion represents for pro-choice feminists is the difficulty of reconciling a pro-choice position with moral objections one might have to sex selective abortion (especially since it has been used primarily on female fetuses), much less the advocacy of a law banning sex-selective abortion."
As a result, arguments both for and against sex-selective abortion are typically highly reflective of one’s own personal beliefs about abortion in general. Warren (1985:104) argues that there is a difference between acting within one’s rights and acting upon the most morally sound choice, implying that sex-selective abortion might be within rights but not morally sound. Warren also notes that, if we are to ever reverse the trend of sex-selective abortion and high sex ratios, we must work to change the patriarchy-based society which breeds the strong son preference.
Laws and initiatives against sex-selective abortion.
Laws.
In 1994 over 180 states signed the Programme of Action of the International Conference on Population and Development, agreeing to "eliminate all forms of discrimination against the girl child". In 2011 the resolution of PACE's Committee on Equal Opportunities for Women and Men condemned the practice of prenatal sex selection.
Media and policy initiatives.
Many nations have attempted to address sex-selective abortion rates through a combination of media campaigns and policy initiatives.
In Canada, a group of MPs led by Mark Warawa are working on having the Parliament pass a resolution condemning sex-selective pregnancy termination.
The United States Congress has debated legislation that would outlaw the practice. The legislation ultimately failed to pass in the House of Representatives.
On the state level, laws against sex-selective abortions have been passed in a number of US states; the law passed in Arizona in 2011 prohibits both sex-selective and race-selective abortion.
The law on sex-selective abortion is unresolved in the United Kingdom. In order for an abortion to be legal, doctors need to show that continuing the pregnancy could threaten the physical or mental health of the mother. In a recent case, two doctors were caught on camera offering a sex-selective abortion but the Director of Public Prosecution deemed it not in the public interest to proceed with the prosecution. Following this incidence, MPs voted 181 to 1 for a Bill put forward by Tessa Munt and 11 other MPs aiming to end confusion about the legality of this practice. Organisations such as BPAS and Abortion Rights have been lobbying for the decriminalisation of sex-selective abortions.
China’s government has increasingly recognized its role in a reduction of the national sex ratio. As a result, since 2005, it has sponsored a “boys and girls are equal campaign.” For example, in 2000, the Chinese government began the “Care for Girls” Initiative. Furthermore, several levels of government have been modified to protect the “political, economic, cultural, and social” rights of women. Finally, the Chinese government has enacted policies and interventions to help reduce the sex ratio at birth. In 2005, sex-selective abortion was made illegal in China. This came in response to the ever-increasing sex ratio and a desire to try to detract from it and reach a more normal ratio. The sex ratio among firstborn children in urban areas from 2000 to 2005 didn’t rise at all, so there is hope that this movement is taking hold across the nation.
UNICEF and UNFPA have partnered with the Chinese government and grassroots-level women’s groups such as All China Women’s Federation to promote gender equality in policy and practice, as well engage various social campaigns to help lower birth sex ratio and to reduce excess female child mortality rates.
In India, according to a 2007 study by MacPherson, Prenatal Diagnostic Techniques Act (PCPNDT Act) was highly publicized by NGOs and the government. Many of the ads used depicted abortion as violent, creating fear of abortion itself within the population. The ads focused on the religious and moral shame associated with abortion. MacPherson claims this media campaign was not effective because some perceived this as an attack on their character, leading to many becoming closed off, rather than opening a dialogue about the issue. This emphasis on morality, claims MacPherson, increased fear and shame associated with all abortions, leading to an increase in unsafe abortions in India.
The government of India, in a 2011 report, has begun better educating all stakeholders about its MTP and PCPNDT laws. In its communication campaigns, it is clearing up public misconceptions by emphasizing that sex determination is illegal, but abortion is legal for certain medical conditions in India. The government is also supporting implementation of programs and initiatives that seek to reduce gender discrimination, including media campaign to address the underlying social causes of sex selection.
Other recent policy initiatives adopted by numerous states of India, claims Guilmoto, attempt to address the assumed economic disadvantage of girls by offering support to girls and their parents. These policies provide conditional cash transfer and scholarships only available to girls, where payments to a girl and her parents are linked to each stage of her life, such as when she is born, completion of her childhood immunization, her joining school at grade 1, her completing school grades 6, 9 and 12, her marriage past age 21. Some states are offering higher pension benefits to parents who raise one or two girls. Different states of India have been experimenting with various innovations in their girl-driven welfare policies. For example, the state of Delhi adopted a pro-girl policy initiative (locally called "Laadli scheme"), which initial data suggests may be lowering the birth sex ratio in the state.

</doc>
<doc id="54422" url="https://en.wikipedia.org/wiki?curid=54422" title="Chinese Civil War">
Chinese Civil War

The Chinese Civil War () was a civil war in China fought between forces loyal to the Kuomintang (KMT)-led government of the Republic of China, and forces loyal to the Communist Party of China (CPC). The war began in August 1927, with Generalissimo Chiang Kai-Shek's Northern Expedition, and essentially ended when major active battles ceased in 1950. The conflict eventually resulted in two "de facto" states, the Republic of China (ROC) in Taiwan and the People's Republic of China (PRC) in mainland China, both officially claiming to be the legitimate government of China.
The war represented an ideological split between the Communist CPC and the KMT's brand of Nationalism. It continued intermittently until late 1937, when the two parties came together to form the Second United Front to counter a Japanese invasion and prevent the country from adding to an earlier invasion into Manchuria in 1931. China's full-scale civil war resumed in 1946, a year after the end of hostilities with Japan. Four years later came the cessation of major military hostilities, with the newly founded People's Republic of China controlling mainland China (including Hainan) and the Republic of China's jurisdiction being restricted to Taiwan, Penghu, Quemoy, Matsu and several outlying islands.
Historian Odd Arne Westad says the Communists won the Civil War because they made fewer military mistakes than Chiang Kai-shek and also because in his search for a powerful centralized government, Chiang antagonized too many interest groups in China. Furthermore, his party was weakened in the war against the Japanese. Meanwhile, the Communists targeted different groups, such as peasants, and brought them to its corner. Chiang wrote in his diary in June 1948 that the KMT had failed not because of external enemies but because of rot from within. Strong initial support from the US diminished with the failure of the Marshall Mission, and then stopped completely mainly because of KMT corruption (such as the notorious Yangtze Development Corporation controlled by H.H. Kung and T.V. Soong's family) and KMT's military setback in Northeast China. Communist land reform policy, which promised poor peasants farmland from their landlords, ensured PLA popular support. After the surrender of Japan at the end of World War II, Soviet forces turned over their captured Japanese weapons to the CPC and allowed it to take control of territory in Manchuria; many believe the Soviet Union was allowed to do so by the US and the United Kingdom because of their desire to influence the outcome of the Chinese Civil War (especially in the decisive battles in Northeast China) at the expense of the Republic of China government by the result of the Yalta Conference until the start of the Cold War across the Taiwan Strait (see United Nations General Assembly Resolution 505). In the Chinese Civil War after 1945, the economy in the ROC areas collapsed because of hyperinflation and the failure of price controls by the ROC government and financial reforms; the Gold Yuan devaluated sharply in late 1948 and resulted in the ROC government losing the support of the cities' middle classes; in the meantime, the Communists continued their relentless land reform (land redistribution) programs to win the support of the population in the countryside.
To this day no armistice or peace treaty has ever been signed, and there is debate about whether the Civil War has legally ended. Cross-Strait relations have been hindered by military threats and political and economic pressure, particularly over Taiwan's political status, with both governments officially adhering to a "One-China policy." The PRC still actively claims Taiwan as part of its territory and continues to threaten the ROC with a military invasion if the ROC officially declares independence by changing its name to and gaining international recognition as the "Republic of Taiwan". The ROC mutually claims mainland China, and they both continue the fight over diplomatic recognition. Today the war as such occurs on the political and economic fronts in the form of cross-Strait relations; however, the two separate "de facto" states have close economic ties.
Background.
The Qing Dynasty, the last of the ruling Chinese dynasties, collapsed in 1911 and finally fell in 1912 with the abdication of the last emperor. China fell into what became known as the Warlord era, when control of much of the country was divided among a group of powerful independent warlords, military leaders with their own private armies. The anti-monarchist and national unificationist Kuomintang party and its leader Sun Yat-sen sought the help of foreign powers to defeat these warlords, who had seized control of much of Northern China.
Sun Yat-sen's efforts to obtain aid from the Western countries were ignored, however, and in 1921 he turned to the Soviet Union. For political expediency the Soviet leadership initiated a dual policy of support for both Sun and the newly established Communist Party of China, which would eventually found the People's Republic of China. Thus the struggle for power in China began between the KMT and the CPC.
In 1923 a joint statement by Sun and Soviet representative Adolph Joffe in Shanghai pledged Soviet assistance for China's unification. The Sun-Joffe Manifesto was a declaration of cooperation among the Comintern, KMT and the Communist Party of China. Comintern agent Mikhail Borodin arrived in China in 1923 to aid in the reorganization and consolidation of the KMT along the lines of the Communist Party of the Soviet Union. The CPC joined the KMT to form the First United Front.
In 1923 Sun Yat-sen sent Chiang Kai-shek, one of his lieutenants from his Tongmeng Hui days, for several months of military and political study in Moscow. By 1924 Chiang became the head of the Whampoa Military Academy, and rose to prominence as Sun's successor as head of the KMT.
The Soviets provided much educational material, organization and equipment, including munitions, for the academy. They also provided education in many of the techniques for mass mobilization. With this aid Sun Yat-sen was able to raise a dedicated "army of the party," with which he hoped to defeat the warlords militarily. CPC members were also present in the academy, and many of them became instructors, including Zhou Enlai, who was made a political instructor.
Communist members were allowed to join the KMT on an individual basis. The CPC itself was still small at the time, having a membership of 300 in 1922 and only 1,500 by 1925. The KMT in 1923 had 50,000 members.
Northern Expedition and KMT-CPC split.
In early 1927 the KMT-CPC rivalry led to a split in the revolutionary ranks. The CPC and the left wing of the KMT had decided to move the seat of the KMT government from Guangzhou to Wuhan, where communist influence was strong. However, Chiang and Li Zongren, whose armies defeated warlord Sun Chuanfang, moved eastward toward Jiangxi. The leftists rejected Chiang's demand to eliminate Communist influence within KMT and Chiang denounced them for betraying Sun Yat-sen's Three Principles of the People by taking orders from the Soviet Union. According to Mao Zedong, Chiang's tolerance of the CPC in the KMT camp decreased as his power increased.
On April 7 Chiang and several other KMT leaders held a meeting, during which they proposed that Communist activities were socially and economically disruptive and must be undone for the national revolution to proceed. On April 12, in Shanghai, the KMT was purged of leftists with the arrest and execution of hundreds of CPC members. It was directed by Gen. Bai Chongxi. This was called the April 12 Incident or Shanghai Massacre by the CPC. The Shanghai massacre widened the rift between Chiang and Wang Jingwei's Wuhan.
Eventually the left wing of the KMT also expelled CPC from the Wuhan government, which in turn was toppled by Chiang Kai-shek. The KMT resumed the campaign against warlords and captured Beijing in June 1928. Afterwards most of eastern China was under the control of the Nanjing central government, which received prompt international recognition as the sole legitimate government of China. The KMT government announced, in conformity with Sun Yat-sen, the formula for the three stages of revolution: military unification, political tutelage and constitutional democracy.
Communist insurgency (1927–1937).
During the 1920s CPC activists retreated underground or to the countryside, where they fomented an armed rebellion. The revolt of the CPC against the Nationalist government began on 1 August 1927 in Nanchang, Jiangxi. The Nanchang Uprising saw the formation of a Communist rebel army, which would later become the People's Liberation Army. After a few days government forces recaptured Nanchang, while surviving rebels escaped into the countryside. Attempts were later made by the CPC to take the cities of Changsha, Shantou and Guangzhou. The Communist force consisted of mutinous former NRA soldiers as well as armed peasants. They established control over several areas in southern China. The Guangzhou commune was able to control Guangzhou for three days and a "soviet" was established. KMT armies continued to suppress the rebellions. This marked the beginning of the ten-year struggle, known in mainland China as the "Ten Year's Civil War" (Traditional Chinese: 十年內戰 Simplified Chinese:十年内战 Pinyin:Shínían Nèizhàn). It lasted until the Xi'an Incident when Chiang Kai-shek was forced to form the Second United Front against invading forces from Japan. An armed rural insurrection, known as the Autumn Harvest Uprising, was staged by peasants, miners and CPC members in Hunan Province, led by Mao Zedong. It was unsuccessful. There were now three capitals in China: the internationally recognized republic capital in Beijing, the CPC and left-wing KMT at Wuhan and the right-wing KMT regime at Nanjing, which would remain the KMT capital for the next decade.
In 1930 the Central Plains War broke out as an internal conflict of the KMT. It was launched by Feng Yuxiang, Yan Xishan and Wang Jingwei. The attention was turned to root out remaining pockets of Communist activity in a series of encirclement campaigns. There were a total of five campaigns. The first and second campaigns failed and the third was aborted due to the Mukden Incident. The fourth campaign (1932–1933) achieved some early successes, but Chiang’s armies were badly mauled when they tried to penetrate into the heart of Mao’s Soviet Chinese Republic. During these campaigns KMT columns struck swiftly into Communist areas, but were easily engulfed by the vast countryside and were not able to consolidate their foothold.
Finally, in late 1934, Chiang launched a fifth campaign that involved the systematic encirclement of the Jiangxi Soviet region with fortified blockhouses. Unlike previous campaigns in which they penetrated deeply in a single strike, this time the KMT troops patiently built blockhouses, each separated by five or so miles, to surround the Communist areas and cut off their supplies and food sources.
In October 1934 the CPC took advantage of gaps in the ring of blockhouses (manned by the forces of a warlord ally of Chiang Kai-shek's, rather than regular KMT troops) to escape Jiangxi. The warlord armies were reluctant to challenge Communist forces for fear of losing their own men and did not pursue the CPC with much fervor. In addition, the main KMT forces were preoccupied with annihilating Zhang Guotao's army, which was much larger than Mao's. The massive military retreat of Communist forces lasted a year and covered what Mao estimated as 12,500 km (25,000 Li); it became known as the Long March.
The march ended when the CPC reached the interior of Shaanxi. Zhang Guotao's army, which took a different route through northwest China, was largely destroyed by the forces of Chiang Kai-shek and his Chinese Muslim allies, the Ma clique. Along the way, the Communist army confiscated property and weapons from local warlords and landlords, while recruiting peasants and the poor, solidifying its appeal to the masses. Of the 90,000-100,000 people who began the Long March from the Soviet Chinese Republic, only around 7,000-8,000 made it to Shaanxi. The remnants of Zhang's forces eventually joined Mao in Shaanxi, but with his army destroyed, Zhang, even as a founding member of the CPC, was never able to challenge Mao's authority. Essentially, the great retreat made Mao the undisputed leader of the Communist Party of China.
The Kuomintang used Khampa troops—who were former bandits—to battle the Communist Red Army as it advanced, and to undermine local warlords who often refused to fight Communist forces to conserve their own strength. The KMT enlisted 300 "Khampa bandits" into its Consolatory Commission military in Sichuan, where they were part of the effort of the central government to penetrate and destabilize local Han warlords such as Liu Wenhui. The government was seeking to exert full control over frontier areas against the warlords. Liu had refused to battle the Communists in order to conserve his army. The Consolatary Commission forces were used to battle the Red Army, but they were defeated when their religious leader was captured by the Communists.
Second Sino-Japanese War (1937–1945).
During Japan's invasion and occupation of Manchuria Chiang Kai-shek, who saw the CPC as a greater threat, refused to ally with them to fight against the Imperial Japanese Army. Chiang preferred to unite China by eliminating the warlords and CPC forces first. He believed that he was still too weak to launch an offensive to chase out Japan and that China needed time for a military build-up. Only after unification would it be possible for the KMT to mobilize a war against Japan. So he would rather ignore the discontent and anger among Chinese people at his policy of compromise with the Japanese, and ordered KMT generals Zhang Xueliang and Yang Hucheng to carry out suppression of the CPC; however, their provincial forces suffered significant casualties in battles with the Red Army.ref
On December 12, 1936, the disgruntled Zhang Xueliang and Yang Hucheng conspired to kidnap Chiang and force him into a truce with the CPC. The incident became known as the Xi'an Incident. Both parties suspended fighting to form a Second United Front to focus their energies and fighting against the Japanese. In 1937 Japan launched its full-scale invasion of China and its well-equipped troops overran KMT defenders in northern and coastal China.
The alliance of CPC and KMT was in name only. Unlike the KMT troops, CPC shunned conventional warfare and instead engaged in guerrilla warfare against the Japanese. The level of actual cooperation and coordination between the CPC and KMT during World War II was at best minimal. In the midst of the Second United Front, the CPC and the KMT were still vying for territorial advantage in "Free China" (i.e., areas not occupied by the Japanese or ruled by Japanese puppet governments such as Manchukuo and the Reorganized National Government of China).
The situation came to a head in late 1940 and early 1941 when clashes between Communist and KMT forces intensified. In December 1940 Chiang demanded that the CPC’s New Fourth Army evacuate Anhui and Jiangsu Provinces due to its provocation and harassment of KMT forces in this area. Under intense pressure, the New Fourth Army commanders complied. In 1941 they were ambushed by KMT forces during their evacuation, which led to several thousand deaths. It also ended the Second United Front, which had been formed earlier to fight the Japanese.
Despite the intensified clashes between the CPC and KMT, countries such as the US and the Soviet Union attempted to prevent a disastrous civil war. After the New Fourth Army incident, US President Franklin D. Roosevelt sent special envoy Lauchlin Currie to talk with Chiang Kai-shek and KMT party leaders to express their concern regarding the hostility between the two parties, with Currie stating that the only ones to benefit from a civil war would be the Japanese. In 1941 the Soviet Union, with its closer alliance to the CPC, also sent an imperative telegram to Mao warning that the civil war would also make the situation easier for the Japanese military. Due to the international community's efforts, there was a temporary and superficial peace. In 1943 Chiang attacked the CPC with the propaganda piece "China's Destiny", which questioned the CPC's power after the war, while the CPC strongly opposed Chiang's leadership and referred to his regime as fascist in an attempt to generate a negative public image. Both leaders knew that a deadly battle had begun between themselves.
In general, developments in the Second Sino-Japanese War were to the advantage of the CPC, as its guerilla war tactics had won them popular support within the Japanese-occupied areas, while the KMT's had to defend the country against the main Japanese campaigns to take over the country, since it was the legal Chinese government, and this proved costly to Chiang Kai-shek and his troops. In 1944 Japan launched its last major offensive, Operation Ichi-Go, against the KMT that resulted in the severe weakening of Chiang's forces.
Immediate post-war clashes (1945–1946).
Under the terms of the Japanese unconditional surrender dictated by the United States, Japanese troops were ordered to surrender to KMT troops and not to the CPC, which was present in some of the occupied areas. In Manchuria, however, where the KMT had no forces, the Japanese surrendered to the Soviet Union. Chiang Kai-Shek ordered the Japanese troops to remain at their post to receive the Kuomintang and not surrender their arms to the Communists.
The first post-war peace negotiation was attended by both Chiang Kai-shek and Mao Zedong in Chongqing from 28 August 1945 and concluded on 10 October 1945 with the signing of Double Tenth Agreement. Both sides stressed the importance of a peaceful reconstruction, but the conference did not produce any concrete result. Battles between the two sides continued even as peace negotiations were in progress, until the agreement was reached in January 1946. However, large campaigns and full-scale confrontations between the CPC and Chiang's troops were temporarily avoided.
In the last month of World War II in East Asia, Soviet forces launched the mammoth Manchurian Strategic Offensive Operation to attack the Japanese in Manchuria and along the Chinese-Mongolian border. This operation destroyed the fighting capability of Japan's Kwantung Army and left the USSR occupying all of Manchuria by the end of the war. Consequently, the 700,000 Japanese troops stationed in the region surrendered. Later in the year Chiang Kai-shek realized that he lacked the resources to prevent a CPC takeover of Manchuria following the scheduled Soviet departure. He therefore made a deal with the Russians to delay their withdrawal until he had moved enough of his best-trained men and modern material into the region; however, the Russians refused permission for the Nationalist troops to traverse its territory. KMT troops were then airlifted by the US to occupy key cities in North China, while the countryside was already dominated by the CPC. On November 15, 1945, an offensive began with the intent of preventing the CPC from strengthening its already strong base. The Soviets spent the extra time systematically dismantling the extensive Manchurian industrial base (worth up to $2 billion) and shipping it back to their war-ravaged country.
Yang Kuisong, a Chinese historian, said that in 1945-46, during the Soviet Red Army Manchurian campaign, Soviet leader Joseph Stalin commanded Marshal Rodion Malinovsky to give Mao Zedong most Imperial Japanese Army weapons that were captured.
Chiang Kai-Shek's forces pushed as far as Chinchow by November 26, 1945, meeting with little resistance. This was followed by a Communist offensive on the Shantung Peninsula that was largely successful, as all of the peninsula, except what was controlled by the US, fell to the Communists. The truce fell apart in June 1946 when full-scale war between CPC and KMT forces broke out on June 26. China then entered a state of civil war that lasted more than three years.
Resumed fighting (1946–1950).
Background and disposition of forces.
By the end of the Second Sino-Japanese War, the balance of power in China's civil war had shifted in favor of the Communists. Their main force grew to 1.2 million troops, with a militia of 2 million. Their "Liberated Zone" contained 19 base areas, including one-quarter of the country's territory and one-third of its population; this included many important towns and cities. Moreover, the Soviet Union turned over all of its captured Japanese weapons and a substantial amount of their own supplies to the Communists, who received Northeastern China from the Soviets as well.
In March 1946, despite repeated requests from Chiang, the Soviet Red Army under the command of Marshal Malinovsky continued to delay pulling out of Manchuria while Malinovsky secretly told the CPC forces to move in behind them, which led to full-scale war for the control of the Northeast. These favorable conditions also facilitated many changes inside the Communist leadership: the more hard-line faction finally gained the upper hand and defeated the opportunists. Prior to giving control to Communist leaders, on March 27 Soviet diplomats requested a joint venture of industrial development with the Nationalist Party in Manchuria.
Although General Marshall stated that he knew of no evidence that the CPC was being supplied by the Soviet Union, the CPC was able to utilize a large number of weapons abandoned by the Japanese, including some tanks, but it was not until large numbers of well-trained KMT troops began surrendering and joining the Communist forces that the CPC was finally able to master the hardware. However, despite the disadvantage in military hardware, the CPC's ultimate trump card was its land reform policy. The CPC continued to make the irresistible promise in the countryside to the massive number of landless and starving peasants that by fighting for the CPC they would be given their own land once the victory was won.
This strategy enabled the CPC to access an almost unlimited supply of manpower for both combat and logistical purposes, despite suffering heavy casualties throughout many of the war's campaigns. For example, during the Huaihai Campaign alone the CPC was able to mobilize 5,430,000 peasants to fight against the KMT forces.
After the war with the Japanese ended, Chiang Kai-shek quickly moved KMT troops to newly liberated areas to prevent Communist forces from receiving the Japanese surrender. The US airlifted many KMT troops from central China to the Northeast (Manchuria). President Harry Truman was very clear about what he described as "using the Japanese to hold off the Communists". In his memoirs he writes:
Using the pretext of "receiving the Japanese surrender", business interests within the KMT government occupied most of the banks, factories and commercial properties, which had previously been seized by the Imperial Japanese Army. They also conscripted troops at a brutal pace from the civilian population and hoarded supplies, preparing for a resumption of war with the Communists. These hasty and harsh preparations caused great hardship for the residents of cities such as Shanghai, where the unemployment rate rose dramatically to 37.5%.
The US strongly supported the Kuomintang forces. Over 50,000 US Marines were sent to guard strategic sites, and 100,000 US troops were sent to Shandong. The US equipped and trained over 500,000 KMT troops, and transported KMT forces to occupy newly liberated zones as well as to contain Communist-controlled areas. American aid included substantial amounts of both new and surplus military supplies; additionally, loans worth hundreds of millions of dollars were made to the KMT. Within less than two years after the Sino-Japanese War, the KMT had received $4.43 billion from the US—most of which was military aid.
Outbreak of War.
With the breakdown of talks, all-out war resumed. This stage is referred to in mainland China and Communist historiography as the "War of Liberation" (). On 20 July 1946 Chiang Kai-shek launched a large-scale assault on Communist territory with 113 brigades (1.6 million troops). This marked the final phase of the Chinese Civil War.
Knowing their disadvantages in manpower and equipment, the CPC executed a "passive defense" strategy. It avoided the strong points of the KMT army and was prepared to abandon territory in order to preserve its forces. In most cases the surrounding countryside and small towns had come under Communist influence long before the cities. The CPC also attempted to wear out the KMT forces as much as possible. This tactic seemed to be successful; after a year, the power balance became more favorable to the CPC. They wiped out 1.12 million KMT troops, while their strength grew to about two million men.
In March 1947 the KMT achieved a symbolic victory by seizing the CPC capital of Yan'an. Soon afterwards the Communists counterattacked; on 30 June 1947 CPC troops crossed the Yellow River and moved to the Dabie Mountains area, restored and developed the Central Plain. Concurrently, Communist forces in Northeastern China, North China and East China began to counterattack as well.
By late 1948 the CPC eventually captured the northern cities of Shenyang and Changchun and seized control of the Northeast after suffering numerous setbacks while trying to take the cities, with the decisive Liaoshen Campaign. The New 1st Army, regarded as the best KMT army, was forced to surrender after the CPC conducted a brutal six-month siege of Changchun that resulted in more than 150,000 civilian deaths from starvation.
The capture of large KMT units provided the CPC with the tanks, heavy artillery and other combined-arms assets needed to execute offensive operations south of the Great Wall. By April 1948 the city of Luoyang fell, cutting the KMT army off from Xi'an. Following a fierce battle, the CPC captured Jinan and Shandong province on September 24, 1948. The Huaihai Campaign of late 1948 and early 1949 secured east-central China for the CPC. The outcome of these encounters were decisive for the military outcome of the civil war.
The Pingjin Campaign resulted in the Communist conquest of northern China. It lasted 64 days, from November 21, 1948, to January 31, 1949. The PLA suffered heavy casualties while securing Zhangjiakou, Tianjin along with its port and garrison at Dagu and Beiping. The CPC brought 890,000 troops from the northeast to oppose some 600,000 KMT troops. There were 40,000 CPC casualties at Zhangjiakou alone. They in turn killed, wounded or captured some 520,000 KMT during the campaign.
After the decisive Liaoshen, Huaihai and Pingjin campaigns, the CPC wiped out 144 regular and 29 non-regular KMT divisions, including 1.54 million veteran KMT troops. This effectively smashed the backbone of the KMT army. On 21 April, Communist forces crossed the Yangtze River, and on 23 April they captured the KMT's capital, Nanjing. The KMT government retreated to Canton (Guangzhou) until October 15, Chongqing until November 25, and then Chengdu before retreating to Taiwan on December 10. By late 1949 the People's Liberation Army was pursuing remnants of KMT forces southwards in southern China, and only Tibet was left. In addition, the Ili Rebellion was a Soviet-backed revolt by the Second East Turkestan Republic against the KMT from 1944–49, as the Mongolians in the People's Republic were in a border dispute with the Republic of China. A Chinese Muslim Hui cavalry regiment, the 14th Tungan Cavalry, was sent by the Chinese government to attack Mongol and Soviet positions along the border during the Pei-ta-shan Incident.
Several last-ditch attempts by the Kuomintang to use Khampa troops against the Communists in southwest China were made. The Kuomintang formulated a plan in which three Khampa divisions would be assisted by the Panchen Lama to oppose the Communists. Kuomintang intelligence reported that some Tibetan tusi chiefs and the Khampa Su Yonghe controlled 80,000 troops in Sichuan, Qinghai and Tibet. They hoped to use them against the Communist army.
Fighting subsides.
On October 1, 1949, Mao Zedong proclaimed the People's Republic of China with its capital at Beiping, which was renamed Beijing. Chiang Kai-shek and approximately two million Nationalist Chinese retreated from mainland China to the island of Taiwan in December after the loss of Sichuan. There remained only isolated pockets of resistance, notably in Sichuan (ending soon after the fall of Chengdu on December 10, 1949) and in the far south.
A PRC attempt to take the ROC-controlled island of Quemoy was thwarted in the Battle of Kuningtou, halting the PLA advance towards Taiwan. In December 1949, Chiang proclaimed Taipei, Taiwan, the temporary capital of the Republic of China and continued to assert his government as the sole legitimate authority in China.
The Communists' other amphibious operations of 1950 were more successful: they led to the Communist conquest of Hainan Island in April 1950, capture of Wanshan Islands off the Guangdong coast (May–August 1950) and of Zhoushan Island off Zhejiang (May 1950).
Aftermath.
Most observers expected Chiang's government to eventually fall in response to a Communist invasion of Taiwan, and the US initially showed no interest in supporting Chiang's government in its final stand. Things changed radically with the onset of the Korean War in June 1950. At this point, allowing a total Communist victory over Chiang became politically impossible for the US, and President Harry S. Truman ordered the United States Seventh Fleet into the Taiwan Strait to prevent the ROC and PRC from attacking each other.
In June 1949 the ROC declared a "closure" of all mainland China ports and its navy attempted to intercept all foreign ships. The closure was from a point north of the mouth of Min River in Fujian to the mouth of the Liao River in Liaoning. Since mainland China's railroad network was underdeveloped, north-south trade depended heavily on sea lanes. ROC naval activity also caused severe hardship for mainland China fishermen.
After losing mainland China, a group of approximately 12,000 KMT soldiers escaped to Burma and continued launching guerrilla attacks into south China during the Kuomintang Islamic Insurgency in China (1950–1958) and Campaign at the China–Burma Border. Their leader, Gen. Li Mi, was paid a salary by the ROC government and given the nominal title of Governor of Yunnan. Initially, the US supported these remnants and the Central Intelligence Agency provided them with aid. After the Burmese government appealed to the United Nations in 1953, the US began pressuring the ROC to withdraw its loyalists. By the end of 1954 nearly 6,000 soldiers had left Burma and Li Mi declared his army disbanded. However, thousands remained, and the ROC continued to supply and command them, even secretly supplying reinforcements at times.
After the ROC complained to the United Nations against the Soviet Union for violating the Sino-Soviet Treaty of Friendship and Alliance to support the CPC, the UN General Assembly Resolution 505 was adopted on February 1, 1952, condemning the Soviet Union.
Though viewed as a military liability by the US, the ROC viewed its remaining islands in Fujian as vital for any future campaign to defeat the PRC and retake mainland China. On September 3, 1954, the First Taiwan Strait Crisis began when the PLA started shelling Quemoy and threatened to take the Dachen Islands in Zhejiang. On January 20, 1955, the PLA took nearby Yijiangshan Island, with the entire ROC garrison of 720 troops killed or wounded defending the island. On January 24 of the same year, the United States Congress passed the Formosa Resolution authorizing the President to defend the ROC's offshore islands. The First Taiwan Straits crisis ended in March 1955 when the PLA ceased its bombardment. The crisis was brought to a close during the Bandung conference.
The Second Taiwan Strait Crisis began on August 23, 1958 with air and naval engagements between PRC and ROC forces, leading to intense artillery bombardment of Quemoy (by the PRC) and Amoy (by the ROC), and ended on November of the same year. PLA patrol boats blockaded the islands from ROC supply ships. Though the US rejected Chiang Kai-shek's proposal to bomb mainland China artillery batteries, it quickly moved to supply fighter jets and anti-aircraft missiles to the ROC. It also provided amphibious assault ships to land supplies, as a sunken ROC naval vessel was blocking the harbor. On September 7 the US escorted a convoy of ROC supply ships and the PRC refrained from firing.
By 1984 PRC and ROC had public contacts with each other and cross-straits trade and investment has been growing ever since. The war was officially declared over by the ROC in 1991. Despite the end of the hostilities, the two sides have never signed any agreement or treaty to officially end the war.
The Third Taiwan Strait Crisis in 1995–96 escalated tensions between both sides when the PRC tested a series of missiles not far from Taiwan, although, arguably, Beijing ran the test to shift the 1996 presidential election vote in favor of the KMT, already facing a challenge from the opposition Democratic Progressive Party which did not agree with the "One China Policy" shared by the CPC and KMT.
With the election in 2000 of Democratic Progressive Party candidate Chen Shui-bian, a party other than the KMT gained the presidency for the first time in Taiwan. The new president did not share the Chinese nationalist ideology of the KMT and CPC. This led to tension between the two sides, although trade and other ties such as the 2005 Pan-Blue visit continued to increase.
Since the election of President Ma Ying-jeou (KMT) in 2008, significant warming of relations has resumed between Taipei and Beijing, with high-level exchanges between the semi-official diplomatic organizations of both states such as the Chen-Chiang summit series. Although the Taiwan straits remain a potential flash point, regular direct air links were established in 2009.
Atrocities.
During the war both the Nationalists and Communists carried out mass atrocities, with millions of non-combatants deliberately killed by both sides. Benjamin Valentino has estimated atrocities in the Chinese Civil War resulted in the death of between 1.8 million and 3.5 million people between 1927 and 1949. Atrocities include deaths from forced conscription and massacres.

</doc>
<doc id="54423" url="https://en.wikipedia.org/wiki?curid=54423" title="Phase transition">
Phase transition

A phase transition is the transformation of a thermodynamic system from one phase or state of matter to another one by heat transfer. 
The term is most commonly used to describe transitions between solid, liquid and gaseous states of matter, and, in rare cases, plasma. A phase of a thermodynamic system and the states of matter have uniform physical properties. During a phase transition of a given medium certain properties of the medium change, often discontinuously, as a result of the change of some external condition, such as temperature, pressure, or others. For example, a liquid may become gas upon heating to the boiling point, resulting in an abrupt change in volume. The measurement of the external conditions at which the transformation occurs is termed the phase transition. Phase transitions are common in nature and used today in many technologies.
Types of phase transition.
Examples of phase transitions include:
Phase transitions occur when the thermodynamic free energy of a system is non-analytic for some choice of thermodynamic variables (cf. phases). This condition generally stems from the interactions of a large number of particles in a system, and does not appear in systems that are too small. It is important to note that phase transitions can occur and are defined for non-thermodynamic systems, where temperature is not a parameter. Examples include: quantum phase transitions, dynamic phase transitions, and topological (structural) phase transitions. In these types of systems other parameters take the place of temperature. For instance, connection probability replaces temperature for percolating networks.
At the phase transition point (for instance, boiling point) the two phases of a substance, liquid and vapor, have identical free energies and therefore are equally likely to exist. Below the boiling point, the liquid is the more stable state of the two, whereas above the gaseous form is preferred.
It is sometimes possible to change the state of a system diabatically (as opposed to adiabatically) in such a way that it can be brought past a phase transition point without undergoing a phase transition. The resulting state is metastable, i.e., less stable than the phase to which the transition would have occurred, but not unstable either. This occurs in superheating, supercooling, and supersaturation, for example.
Classifications.
Ehrenfest classification.
Paul Ehrenfest classified phase transitions based on the behavior of the thermodynamic free energy as a function of other thermodynamic variables. Under this scheme, phase transitions were labeled by the lowest derivative of the free energy that is discontinuous at the transition. "First-order phase transitions" exhibit a discontinuity in the first derivative of the free energy with respect to some thermodynamic variable. The various solid/liquid/gas transitions are classified as first-order transitions because they involve a discontinuous change in density, which is the (inverse of the) first derivative of the free energy with respect to pressure. "Second-order phase transitions" are continuous in the first derivative (the order parameter, which is the first derivative of the free energy with respect to the external field, is continuous across the transition) but exhibit discontinuity in a second derivative of the free energy. These include the ferromagnetic phase transition in materials such as iron, where the magnetization, which is the first derivative of the free energy with respect to the applied magnetic field strength, increases continuously from zero as the temperature is lowered below the Curie temperature. The magnetic susceptibility, the second derivative of the free energy with the field, changes discontinuously. Under the Ehrenfest classification scheme, there could in principle be third, fourth, and higher-order phase transitions.
Though useful, Ehrenfest's classification has been found to be an incomplete method of classifying phase transitions, for it does not take into account the case where a derivative of free energy diverges (which is only possible in the thermodynamic limit). For instance, in the ferromagnetic transition, the heat capacity diverges to infinity.
Modern classifications.
In the modern classification scheme, phase transitions are divided into two broad categories, named similarly to the Ehrenfest classes:
First-order phase transitions are those that involve a latent heat. During such a transition, a system either absorbs or releases a fixed (and typically large) amount of energy per volume. During this process, the temperature of the system will stay constant as heat is added: the system is in a "mixed-phase regime" in which some parts of the system have completed the transition and others have not. Familiar examples are the melting of ice or the boiling of water (the water does not instantly turn into vapor, but forms a turbulent mixture of liquid water and vapor bubbles). Imry and Wortis showed that quenched disorder can broaden a first-order transition in that the transformation is completed over a finite range of temperatures, but phenomena like supercooling and superheating survive and hysteresis is observed on thermal cycling.
Second-order phase transitions are also called "continuous phase transitions". They are characterized by a divergent susceptibility, an infinite correlation length, and a power-law decay of correlations near criticality. Examples of second-order phase transitions are the ferromagnetic transition, superconducting transition (for a Type-I superconductor the phase transition is second-order at zero external field and for a Type-II superconductor the phase transition is second-order for both normal state-mixed state and mixed state-superconducting state transitions) and the superfluid transition. In contrast to viscosity, thermal expansion and heat capacity of amorphous materials show a relatively sudden change at the glass transition temperature which enables accurate detection using differential scanning calorimetry measurements. Lev Landau gave a phenomenological theory of second-order phase transitions.
Apart from isolated, simple phase transitions, there exist transition lines as well as multicritical points, when varying external parameters like the magnetic field or composition.
Several transitions are known as the "infinite-order phase transitions".
They are continuous but break no symmetries. The most famous example is the Kosterlitz–Thouless transition in the two-dimensional XY model. Many quantum phase transitions, e.g., in two-dimensional electron gases, belong to this class.
The liquid–glass transition is observed in many polymers and other liquids that can be supercooled far below the melting point of the crystalline phase. This is atypical in several respects. It is not a transition between thermodynamic ground states: it is widely believed that the true ground state is always crystalline. Glass is a "quenched disorder" state, and its entropy, density, and so on, depend on the thermal history. Therefore, the glass transition is primarily a dynamic phenomenon: on cooling a liquid, internal degrees of freedom successively fall out of equilibrium. Some theoretical methods predict an underlying phase transition in the hypothetical limit of infinitely long relaxation times. No direct experimental evidence supports the existence of these transitions.
Characteristic properties.
Phase coexistence.
A disorder-broadened first-order transition occurs over a finite range of temperatures where the fraction of the low-temperature equilibrium phase grows from zero to one (100%) as the temperature is lowered. This continuous variation of the coexisting fractions with temperature raised interesting possibilities. On cooling, some liquids vitrify into a glass rather than transform to the equilibrium crystal phase. This happens if the cooling rate is faster than a critical cooling rate, and is attributed to the molecular motions becoming so slow that the molecules cannot rearrange into the crystal positions. This slowing down happens below a glass-formation temperature Tg, which may depend on the applied pressure., If the first-order freezing transition occurs over a range of temperatures, and Tg falls within this range, then there is an interesting possibility that the transition is arrested when it is partial and incomplete. 
Extending these ideas to first-order magnetic transitions being arrested at low temperatures, resulted in the observation of incomplete magnetic transitions, with two magnetic phases coexisting, down to the lowest temperature. First reported in the case of a ferromagnetic to anti-ferromagnetic transition, such persistent phase coexistence has now been reported across a variety of first-order magnetic transitions. These include colossal-magnetoresistance manganite materials, magnetocaloric materials, magnetic shape memory materials, and other materials. The interesting feature of these observations of Tg falling within the temperature range over which the transition occurs is that the first-order magnetic transition is influenced by magnetic field, just like the structural transition is influenced by pressure. The relative ease with which magnetic field can be controlled, in contrast to pressure, raises the possibility that one can study the interplay between Tg and Tc in an exhaustive way. Phase coexistence across first-order magnetic transitions will then enable the resolution of outstanding issues in understanding glasses.
Critical points.
In any system containing liquid and gaseous phases, there exists a special combination of pressure and temperature, known as the critical point, at which the transition between liquid and gas becomes a second-order transition. Near the critical point, the fluid is sufficiently hot and compressed that the distinction between the liquid and gaseous phases is almost non-existent. This is associated with the phenomenon of critical opalescence, a milky appearance of the liquid due to density fluctuations at all possible wavelengths (including those of visible light).
Symmetry.
Phase transitions often involve a symmetry breaking process. For instance, the cooling of a fluid into a crystalline solid breaks continuous translation symmetry: each point in the fluid has the same properties, but each point in a crystal does not have the same properties (unless the points are chosen from the lattice points of the crystal lattice). Typically, the high-temperature phase contains more symmetries than the low-temperature phase due to spontaneous symmetry breaking, with the exception of certain accidental symmetries (e.g. the formation of heavy virtual particles, which only occurs at low temperatures).
Order parameters.
An order parameter is a measure of the degree of order across the boundaries in a phase transition system; it normally ranges between zero in one phase (usually above the critical point) and nonzero in the other. At the critical point, the order parameter susceptibility will usually diverge.
An example of an order parameter is the net magnetization in a ferromagnetic system undergoing a phase transition. For liquid/gas transitions, the order parameter is the difference of the densities.
From a theoretical perspective, order parameters arise from symmetry breaking. When this happens, one needs to introduce one or more extra variables to describe the state of the system. For example, in the ferromagnetic phase, one must provide the net magnetization, whose direction was spontaneously chosen when the system cooled below the Curie point. However, note that order parameters can also be defined for non-symmetry-breaking transitions. Some phase transitions, such as superconducting and ferromagnetic, can have order parameters for more than one degree of freedom. In such phases, the order parameter may take the form of a complex number, a vector, or even a tensor, the magnitude of which goes to zero at the phase transition.
There also exist dual descriptions of phase transitions in terms of disorder parameters. These indicate the presence of line-like excitations such as vortex- or defect lines.
Relevance in cosmology.
Symmetry-breaking phase transitions play an important role in cosmology. It has been speculated by Lee Smolin and Benjamin and Jeremy Bernstein that, in the hot early universe, the vacuum (i.e. the various quantum fields that fill space) possessed a large number of symmetries. As the universe expanded and cooled, the vacuum underwent a series of symmetry-breaking phase transitions. For example, the electroweak transition broke the SU(2)×U(1) symmetry of the electroweak field into the U(1) symmetry of the present-day electromagnetic field. This transition is important to understanding the asymmetry between the amount of matter and antimatter in the present-day universe (see electroweak baryogenesis.)
Progressive phase transitions in an expanding universe are implicated in the development of order in the universe, as is illustrated by the work of Eric Chaisson and David Layzer. See also Relational order theories.
Critical exponents and universality classes.
Continuous phase transitions are easier to study than first-order transitions due to the absence of latent heat, and they have been discovered to have many interesting properties. The phenomena associated with continuous phase transitions are called critical phenomena, due to their association with critical points.
It turns out that continuous phase transitions can be characterized by parameters known as critical exponents. The most important one is perhaps the exponent describing the divergence of the thermal correlation length by approaching the transition. For instance, let us examine the behavior of the heat capacity near such a transition. We vary the temperature of the system while keeping all the other thermodynamic variables fixed, and find that the transition occurs at some critical temperature "Tc" . When is near "Tc" , the heat capacity typically has a power law behavior,
Such a behaviour has the heat capacity of amorphous materials near the glass transition temperature where the universal critical exponent α = 0.59 A similar behavior, but with the exponent instead of , applies for the correlation length.
The exponent is positive. This is different with . Its actual value depends on the type of phase transition we are considering.
It is widely believed that the critical exponents are the same above and below the critical temperature. It has now been shown that this is not necessarily true: When a continuous symmetry is explicitly broken down to a discrete symmetry by irrelevant (in the renormalization group sense) anisotropies, then some exponents (such as formula_2, the exponent of the susceptibility) are not identical.
For −1 < α < 0, the heat capacity has a "kink" at the transition temperature. This is the behavior of liquid helium at the lambda transition from a normal state to the superfluid state, for which experiments have found = -0.013±0.003.
At least one experiment was performed in the zero-gravity conditions of an orbiting satellite to minimize pressure differences in the sample. This experimental value of α agrees with theoretical predictions based on variational perturbation theory.
For 0 < < 1, the heat capacity diverges at the transition temperature (though, since < 1, the enthalpy stays finite). An example of such behavior is the 3D ferromagnetic phase transition. In the three-dimensional Ising model for uniaxial magnets, detailed theoretical studies have yielded the exponent ∼ +0.110.
Some model systems do not obey a power-law behavior. For example, mean field theory predicts a finite discontinuity of the heat capacity at the transition temperature, and the two-dimensional Ising model has a logarithmic divergence. However, these systems are limiting cases and an exception to the rule. Real phase transitions exhibit power-law behavior.
Several other critical exponents, , and , are defined, examining the power law behavior of a measurable physical quantity near the phase transition. Exponents are related by scaling relations, such as 
It can be shown that there are only two independent exponents, e.g. and .
It is a remarkable fact that phase transitions arising in different systems often possess the same set of critical exponents. This phenomenon is known as "universality". For example, the critical exponents at the liquid–gas critical point have been found to be independent of the chemical composition of the fluid.
More impressively, but understandably from above, they are an exact match for the critical exponents of the ferromagnetic phase transition in uniaxial magnets. Such systems are said to be in the same universality class. Universality is a prediction of the renormalization group theory of phase transitions, which states that the thermodynamic properties of a system near a phase transition depend only on a small number of features, such as dimensionality and symmetry, and are insensitive to the underlying microscopic properties of the system. Again, the divergency of the correlation length is the essential point.
Critical slowing down and other phenomena.
There are also other critical phenomena; e.g., besides "static functions" there is also "critical dynamics". As a consequence, at a phase transition one may observe critical slowing down or "speeding up". The large "static universality classes" of a continuous phase transition split into smaller "dynamic universality" classes. In addition to the critical exponents, there are also universal relations for certain static or dynamic functions of the magnetic fields and temperature differences from the critical value.
Percolation theory.
Another phenomenon which shows phase transitions and critical exponents is percolation. The simplest example is perhaps percolation in a two dimensional square lattice. Sites are randomly occupied with probability p. For small values of p the occupied sites form only small clusters. At a certain threshold pc a giant cluster is formed and we have a second-order phase transition. The behavior of P∞ near pc is, P∞~(p-pc)β, where β is a critical exponent.
Phase transitions in biological systems.
Phase transitions play many important roles in biological systems. Examples include the lipid bilayer formation, the coil-globule transition in the process of protein folding and DNA melting, liquid crystal-like transitions in the process of DNA condensation, and cooperative ligand binding to DNA and proteins with the character of phase transition.
In "biological membranes", gel to liquid crystalline phase transitions play a very critical role in physiological functioning of biomembranes. In gel phase, due to low fluidity of membrane lipid fatty-acyl chains, membrane proteins have restricted movement and thus are restrained in exercise of their physiological role. Plants depend critically on photosynthesis by chloroplast thylakoid membranes which are exposed cold environmental temperatures. Thylakoid membranes retain innate fluidity even at relatively low temperatures because of high degree of fatty-acyl disorder allowed by their high content of linolenic acid, 18-carbon chain with 3-double bonds. Gel-to-liquid crystalline phase transition temperature of biological membranes can be determined by many techniques including calorimetry, flouorescence, spin label electron paramagnetic resonance and NMR by recording measurements of the concerned parameter by at series of sample temperatures. A simple method for its determination from 13-C NMR line intensities has also been proposed.
The relevance of phase transitions in neural networks has been pointed out, because of the complex and emergent nature of neural interactions. A point of view can be found in the very recent paper by Tkačik et al.

</doc>
<doc id="54424" url="https://en.wikipedia.org/wiki?curid=54424" title="Etemenanki">
Etemenanki

Etemenanki (Sumerian É.TEMEN.AN.KI 𒂍𒋼𒀭𒆠 "temple of the foundation of heaven and earth") was the name of a ziggurat dedicated to Marduk in the city of Babylon of the 6th century BCE Neo-Babylonian dynasty. Originally 91 meters in height, little remains of it now except ruins.
Etemenanki is considered the possible inspiration to the biblical story of the Tower of Babel.
Construction.
It is unclear exactly when Etemenanki was first built. A review article by Andrew R. George says that its builder may have "reigned in the fourteenth, twelfth, eleventh or ninth century but argues that The reference to a ziqqurrat at Babylon in the Creation Epic (Enûma Eliš· VI 63: George 1992: 301-2) is more solid evidence, however, for a Middle Assyrian piece of this poem survives to prove the long-held theory that it existed already in the second millennium BC. There is no reason to doubt that this ziqqurrat, described as "ziqqurrat apsî elite", ‘the upper ziqqurrat of the Apsû’, was E-temenanki.
The city of Babylon had been destroyed in 689 BCE by Sennacherib, who claims to have destroyed the Etemenanki. The city was restored by Nabopolassar and his son Nebuchadnezzar II. It took 88 years to rebuild the city; its central feature was the temple of Marduk (Esagila), with which the Etemenanki ziggurat was associated. The ziggurat was rebuilt by Nebuchadnezzar II. The seven stories of the ziggurat reached a height of 91 meters, according to a tablet from Uruk (see below), and contained a temple shrine at the top.
In Nebuchadnezzar's own words:
The tower, the eternal house, which I founded and built. I have completed its magnificence with silver, gold, other metals, stone, enameled bricks, fir and pine. The first which is the house of the earth’s base, the most ancient monument of Babylon; I built and finished it. I have highly exalted its head with bricks covered with copper. We say for the other, that is, this edifice, the house of the seven lights of the earth the most ancient monument of Borsippa. A former king built it, (they reckon 42 ages) but he did not complete its head. Since a remote time, people had abandoned it, without order expressing their words. Since that time the earthquake and the thunder had dispersed the sun-dried clay. The bricks of the casing had been split, and the earth of the interior had been scattered in heaps. Merodach, the great god, excited my mind to repair this building. I did not change the site nor did I take away the foundation. In a fortunate month, in an auspicious day, I undertook to build porticoes around the crude brick masses, and the casing of burnt bricks. I adapted the circuits, I put the inscription of my name in the Kitir of the portico. I set my hand to finish it. And to exalt its head. As it had been done in ancient days, so I exalted its summit.
Descriptions.
A Neo-Babylonian royal inscription of Nebuchadnezzar II on a stele from Babylon, claimed to have been found in the 1917 excavation by Robert Koldewey, and of uncertain authenticity, reads: "Etemenanki Zikkurat Babibli of Babylon I made it, the wonder of the people of the world, I raised its top to heaven, made doors for the gates, and I covered it with bitumen and bricks." The Etemenanki is depicted in shallow relief, showing its high first stages with paired flights of steps, five further stepped stages and the temple that surmounted the structure. A floor plan is also shown, depicting the buttressed outer walls and the inner chambers surrounding the central "cella".
Scholars have recently discovered in the Schoyen Collection the oldest known representation of the Etemenanki. Carved on a black stone, "The Tower of Babel Stele" (as it is known) dates from 604-562 BC, the time of Nebuchadnezzar II.
The Etemenanki is described in a cuneiform tablet from Uruk from 229 BCE, a copy of an older text (now in the Louvre in Paris). It gives the height of the tower as seven stocks (91 meters) with a square base of 91 meters on each side. This mud brick structure was confirmed by excavations conducted by Robert Koldewey after 1913. Large stairs were discovered at the south side of the building, where a triple gate connected it with the Esagila. A larger gate to the east connected the Etemenanki with the sacred procession road (now reconstructed in the Pergamon Museum in Berlin).
In 440 BCE, Herodotus wrote:
Babylon's outer wall is the main defence of the city. There is, however, a second inner wall, of less thickness than the first, but very little inferior to it in strength. The center of each division of the town was occupied by a fortress. In the one stood the palace of the kings, surrounded by a wall of great strength and size: in the other was the sacred precinct of Jupiter Belus, a square enclosure two furlongs [402 m each way, with gates of solid brass; which was also remaining in my time. In the middle of the precinct there was a tower of solid masonry, a furlong m in length and breadth, upon which was raised a second tower, and on that a third, and so on up to eight. The ascent to the top is on the outside, by a path which winds round all the towers. When one is about half-way up, one finds a resting-place and seats, where persons can sit for some time on their way to the summit. On the topmost tower there is a spacious temple, and inside the temple stands a couch of unusual size, richly adorned, with a golden table by its side. There is no statue of any kind set up in the place, nor is the chamber occupied of nights by any one but a single native woman, who, as the Chaldeans, the priests of this god, affirm, is chosen for himself by the deity out of all the women of the land.
This "Tower of Jupiter Belus" is believed to refer to the Akkadian god Bel, whose name has been Hellenised by Herodotus to "Zeus Belus". It is likely that it corresponds to Etemenanki.
Etemenanki has been suggested as a possible inspiration to the biblical story of the Tower of Babel.
Final demolition.
In 331 BCE, Alexander the Great captured Babylon and ordered repairs to the Etemenanki; when he returned to the ancient city in 323 BCE, he noted that no progress had been made, and ordered his army to demolish the entire building, to prepare a final rebuilding. His death, however, prevented the reconstruction. The Babylonian Chronicles and Astronomical Diaries record several attempts to rebuild the Etemenanki, which were always preceded by removing the last debris of the original ziggurat. The "Ruin of Esagila Chronicle" mentions that the Seleucid crown prince Antiochus I decided to finally rebuild it, sacrificed, stumbled and fell, and angrily ordered his elephant drivers to destroy the last remains. There are no later references to the Etemenanki from antiquity.

</doc>
<doc id="54426" url="https://en.wikipedia.org/wiki?curid=54426" title="Munchausen syndrome">
Munchausen syndrome

Munchausen syndrome is a psychiatric factitious disorder wherein those affected feign disease, illness, or psychological trauma to draw attention, sympathy, or reassurance to themselves. Casually referred to as hospital addiction syndrome, thick chart syndrome, or hospital hopper syndrome. Munchausen syndrome fits within the subclass of factitious disorder with predominantly physical signs and symptoms, but patients also have a history of recurrent hospitalization, travelling, and dramatic, extremely improbable tales of their past experiences. The condition derives its name from the fictional character Baron Munchausen.
There is discussion to reclassify them as somatoform disorders in the DSM-5 as it is unclear whether or not people are conscious of drawing attention to themselves. In the current iteration, the term "somatoform disorder" (as used in the DSM-IV-TR and other literature) is no longer in use; that particular section of the DSM-5 has been renamed "Somatic Symptom and Related Disorders". Officially, Munchausen syndrome has been renamed "Factitious Disorder", with specificity either as "Imposed on Self" or "Imposed on Another" (formerly "by Proxy").
Munchausen syndrome is related to Munchausen syndrome by proxy (MSbP/MSP), which refers to the abuse of another person, typically a child, in order to seek attention or sympathy for the abuser. It is an obsessive want to create symptoms for the victim in order to obtain repeated medication or even operations.
Description.
In Munchausen syndrome, the affected person exaggerates or creates symptoms of illnesses in themselves to gain examination, treatment, attention, sympathy, and/or comfort from medical personnel. In some extreme cases, people suffering from Munchausen's syndrome are highly knowledgeable about the practice of medicine and are able to produce symptoms that result in lengthy and costly medical analysis, prolonged hospital stay and unnecessary operations. The role of "patient" is a familiar and comforting one, and it fills a psychological need in people with this syndrome. This disorder is distinct from hypochondriasis and other somatoform disorders in that those with the latter do not intentionally produce their somatic symptoms. Munchausen syndrome is distinct from other psychiatric disorders such as malingering, in that Munchausen does not fabricate symptoms for material gain such as financial compensation, absence from work, or access to drugs.
Risk factors for developing Munchausen syndrome include childhood traumas, growing up with parents/caretakers who were emotionally unavailable due to illness or emotional problems, a serious illness as a child, failed aspirations to work in the medical field, personality disorders, and a low self-esteem. Munchausen syndrome is more common in men and seen in young or middle-aged adults. Those with a history of working in healthcare are also at greater risk of developing it.
Arrhythmogenic Munchausen syndrome describes individuals who simulate or stimulate cardiac arrhythmias to gain medical attention.
A similar behavior called Munchausen syndrome by proxy has been documented in the parent or guardian of a child. The adult ensures that his or her child will experience some medical affliction, therefore compelling the child to suffer treatment for a significant portion of their youth in hospitals. Furthermore, a disease may actually be initiated in the child by the parent or guardian. This condition is considered distinct from Munchausen syndrome. There is growing consensus in the pediatric community that this disorder should be renamed "medical abuse" to highlight the harm caused by the deception and to make it less likely that a perpetrator can use a psychiatric defense when harm is done.
Diagnosis.
Diagnosing Munchausen syndrome requires a clinical assessment. Clinicians should be aware that patients (or person's reporting for patients) may malinger, and caution should be taken to ensure there is evidence for a diagnosis. Lab tests may be required including: complete blood count (CBC), urine toxicology, drug levels from blood, cultures, coagulation tests, assays for thyroid function, or DNA typing. In some cases CT scan,Magnetic resonance imaging, psychological testing, Electroencephalography, or Electrocardiography may also be obtained. 
Treatment and prognosis.
Because there is uncertainty in treating suspected Munchausen patients, some advocate that medical professionals or doctors first explicitly rule out the possibility that the patient has an early stage disease that is not yet clinically detectable in order to avoid under-treating real illness. Then they may take a careful patient history and seek medical records, to look for early deprivation, childhood abuse, or mental illness. If a patient is at risk to himself or herself, inpatient psychiatric hospitalization may be initiated.
Medical providers or doctors may consider working with mental health specialists to help treat the underlying mood or disorder as well as to avoid countertransference. Therapeutic and medical treatment may center on the underlying psychiatric disorder: a mood disorder, an anxiety disorder, or borderline personality disorder. The patient's prognosis depends upon the category under which the underlying disorder falls; depression and anxiety, for example, generally respond well to medication and/or cognitive behavioral therapy, whereas borderline personality disorder, like all personality disorders, is presumed to be pervasive and more stable over time, thus offers the worst or best prognosis.
Patients may have multiple scars on abdomen due to repeated "emergency" operations.
There are several symptoms that together point to Munchausen syndrome. Some are frequent hospitalizations, knowledge of several illnesses, frequently requesting medication such as pain killers, openness to extensive surgery, few or no visitors during hospitalizations, exaggerated or fabricated stories about several medical problems, and more. Munchausen syndrome should not be confused with hypochondria as patients with Munchausen syndrome do not really believe they are sick, they only want to be sick and thus fabricate the symptoms of an illness. It is also not the same as pretending to be sick for personal benefit such as being excused from work or school.
There are several ways in which the patients fake their symptoms. Other than making up past medical histories and faking illnesses patients might inflict harm on themselves such as taking laxatives or blood thinners, ingesting or injecting themselves with bacteria, cutting or burning themselves, and disrupting their healing process such as reopening wounds. Many of these conditions do not have clearly observable or diagnostic symptoms and sometimes the syndrome will go undetected because patients will fabricate identities when visiting the hospital several times. Munchausen syndrome has several complications as these patients will go to great lengths to fake their illness. Severe health problems, serious injuries, loss of limbs or organs, and even death are possible complications.
History.
The syndrome's name derives from Baron Munchausen, a literary character loosely based on the German nobleman Hieronymus Karl Friedrich, Freiherr von Münchhausen (1720–1797). The historical baron became a well-known storyteller in the late 18th century for entertaining dinner guests with tales about his adventures during the Russo-Turkish War. In 1785 German-born writer and con artist Rudolf Erich Raspe anonymously published a book in which a heavily fictionalized version of "Baron Munchausen" tells many fantastic and impossible stories about himself. Raspe's Munchausen became a sensation, establishing a literary exemplar of a bombastic liar or exaggerator.
In 1951, Richard Asher was the first to describe a pattern of self-harm, wherein individuals fabricated histories, signs, and symptoms of illness. Remembering Baron Munchausen, Asher named this condition Munchausen's Syndrome in his article in The Lancet in February 1951, quoted in his obituary in the British Medical Journal:
Asher's nomenclature sparked some controversy, with medical authorities debating the appropriateness of the name for about fifty years. While Asher was praised for bringing cases of factitious disorder to light, participants in the debate objected variously that a literary allusion was inappropriate given the seriousness of the disease; that its use of the anglicized spelling "Munchausen" showed poor form; that the name linked the disease with the real-life Münchhausen, who did not have it; and that the name's connection to works of humor and fantasy, and to the essentially ridiculous character of the fictional Baron Munchausen, was disrespectful to patients suffering from the disorder.
Originally, this term was used for all factitious disorders. Now, however, there is considered to be a wide range of factitious disorders, and the diagnosis of "Munchausen syndrome" is reserved for the most severe form, where the simulation of disease is the central activity of the affected person's life.

</doc>
<doc id="54427" url="https://en.wikipedia.org/wiki?curid=54427" title="Computer algebra system">
Computer algebra system

A computer algebra system (CAS) is a software program that allows computation over mathematical expressions in a way which is similar to the traditional manual computations of mathematicians and scientists. The development of the computer algebra systems in the second half of the 20th century is part of the discipline of "computer algebra" or "symbolic computation", which has spurred work in algorithms over mathematical objects such as polynomials.
Computer algebra systems may be divided in two classes: the specialized ones and the general purpose ones. The specialized ones are devoted to a specific part of mathematics, such as number theory, group theory, or teaching of elementary mathematics.
General purpose computer algebra systems aim to be useful to a user working in any scientific field that requires manipulation of mathematical expressions. To be useful, a general purpose computer algebra system must include various features such as
The library must cover not only the needs of the users, but also the needs of the simplifier. For example, the computation of polynomial greatest common divisors is systematically used for the simplification of expressions involving fractions.
This large amount of required computer capabilities explains the small number of general purpose computer algebra systems. The main ones are Axiom, Macsyma, Magma, Maple, Mathematica and Sage.
History.
Computer algebra systems began to appear in the 1960s, and evolved out of two quite different sources—the requirements of theoretical physicists and research into artificial intelligence.
A prime example for the first development was the pioneering work conducted by the later Nobel Prize laureate in physics Martinus Veltman, who designed a program for symbolic mathematics, especially High Energy Physics, called Schoonschip (Dutch for "clean ship") in 1963. Another early system was FORMAC.
Using LISP as the programming basis, Carl Engelman created MATHLAB in 1964 at MITRE within an artificial intelligence research environment. Later MATHLAB was made available to users on PDP-6 and PDP-10 Systems running TOPS-10 or TENEX in universities. Today it can still be used on SIMH-Emulations of the PDP-10. MATHLAB ("mathematical laboratory") should not be confused with MATLAB ("matrix laboratory") which is a system for numerical computation built 15 years later at the University of New Mexico, accidentally named rather similarly.
The first popular computer algebra systems were muMATH, Reduce, Derive (based on muMATH), and Macsyma; a popular copyleft version of Macsyma called Maxima is actively being maintained. Reduce became free software in 2008. As of today, the most popular commercial systems are Mathematica and Maple, which are commonly used by research mathematicians, scientists, and engineers. Freely available alternatives include Sage (which can act as a front-end to several other free and nonfree CAS).
In 1987, Hewlett-Packard introduced the first hand held calculator CAS with the HP-28 series, and it was possible, for the first time in a calculator, to arrange algebraic expressions, differentiation, limited symbolic integration, Taylor series construction and a "solver" for algebraic equations. In 1999, the independently developed CAS Erable for the HP 48 series became an officially integrated part of the firmware of the emerging HP 49/50 series, and a year later into the HP 40 series as well, whereas the HP Prime adopted the Xcas system in 2013.
The Texas Instruments company in 1995 released the TI-92 calculator with a CAS based on the software Derive; the TI-Nspire series replaced Derive in 2007. The TI-89 series, first released in 1998, also contains a CAS.
Symbolic manipulations.
The symbolic manipulations supported typically include:
In the above, the word "some" indicates that the operation cannot always be performed.
Additional capabilities.
Many also include:
Some include:
Some computer algebra systems focus on a specific area of application; these are typically developed in academia and are free. They can be inefficient for numeric operations compared to numeric systems.
Types of expressions.
The expressions manipulated by the CAS typically include polynomials in multiple variables; standard functions of expressions (sine, exponential, etc.); various special functions (Γ, ζ, erf, Bessel functions, etc.); arbitrary functions of expressions; optimization; derivatives, integrals, simplifications, sums, and products of expressions; truncated series with expressions as coefficients, matrices of expressions, and so on. Numeric domains supported typically include real, integer, complex, interval, rational, and algebraic.
Use in education.
There have been many advocates for increasing the use of computer algebra systems in primary and secondary school classrooms. The primary reason for such an advocacy is that computer algebra systems represent real-world math moreso than paper-and-pencil or hand calculator based mathematics.
This push for increasing computer usage in mathematics classrooms has been supported by certain boards of education. It has even been mandated in the curriculum of certain regions.
Computer algebra systems have been extensively used in higher education. Many universities offer either specific courses on developing their use, or they implicitly expect students to use them for their course work. The companies that develop computer algebra systems have pushed to increase their prevalence among university and college programs.
CAS-equipped calculators are not permitted on the ACT, the PLAN, and in some classrooms though it may be permitted on all of College Board's calculator-permitted tests, including the SAT, some SAT Subject Tests and the AP Calculus, Chemistry, Physics, and Statistics exams.

</doc>
<doc id="54432" url="https://en.wikipedia.org/wiki?curid=54432" title="Unification (computer science)">
Unification (computer science)

In logic and computer science, unification is an algorithmic process of solving equations between symbolic expressions. 
Depending on which expressions (also called "terms") are allowed to occur in an equation set (also called "unification problem"), and which expressions are considered equal, several framework of unification are distinguished. If higher-order variables, that is, variables representing functions, are allowed in an expression, the process is called higher-order unification, otherwise first-order unification. If a solution is required to make both sides of each equation literally equal, the process is called syntactic or free unification, otherwise semantic or equational unification, or E-unification, or unification modulo theory.
A "solution" of a unification problem is denoted as a substitution, that is, a mapping assigning a symbolic value to each variable of the problem's expressions. A unification algorithm should compute for a given problem a "complete", and "minimal" substitution set, that is, a set covering all its solutions, and containing no redundant members. Depending on the framework, a complete and minimal substitution set may have at most one, at most finitely many, or possibly infinitely many members, or may not exist at all. In some frameworks it is generally impossible to decide whether any solution exists. For first-order syntactical unification, Martelli and Montanari gave an algorithm that reports unsolvability or computes a complete and minimal singleton substitution set containing the so-called most general unifier.
For example, using "x","y","z" as variables, the singleton equation set { "cons"("x","cons"("x","nil")) = "cons"(2,"y") } is a syntactic first-order unification problem that has the substitution { "x" ↦ 2, "y" ↦ "cons"(2,"nil") } as its only solution.
The syntactic first-order unification problem { "y" = "cons"(2,"y") } has no solution over the set of finite terms; however, it has the single solution { "y" ↦ "cons"(2,"cons"(2,"cons"(2...))) } over the set of infinite trees.
The semantic first-order unification problem { "a"⋅"x" = "x"⋅"a" } has each substitution of the form { "x" ↦ "a"⋅...⋅"a" } as a solution in a semigroup, i.e. if (⋅) is considered associative; the same problem, viewed in an abelian group, where (⋅) is considered also commutative, has any substitution at all as a solution.
The singleton set { "a" = "y"("x") } is a syntactic second-order unification problem, since "y" is a function variable.
One solution is { "x" ↦ "a", "y" ↦ (identity function) }; another one is { "y" ↦ (constant function mapping each value to "a"), "x" ↦ "(any value)" }.
The first formal investigation of unification can be attributed to John Alan Robinson, who used first-order syntactical unification as a basic building block of his resolution procedure for first-order logic, a great step forward in automated reasoning technology, as it eliminated one source of combinatorial explosion: searching for instantiation of terms. Today, automated reasoning is still the main application area of unification.
Syntactical first-order unification is used in logic programming and programming language type system implementation, especially in Hindley–Milner based type inference algorithms.
Semantic unification is used in SMT solvers and term rewriting algorithms.
Higher-order unification is used in proof assistants, for example Isabelle and Twelf, and restricted forms of higher-order unification (higher-order pattern unification) are used in some programming language implementations, such as lambdaProlog, as higher-order patterns are expressive, yet their associated unification procedure retains theoretical properties closer to first-order unification.
Common formal definitions.
Prerequisites.
Formally, a unification approach presupposes
First-order term.
Given a set "V" of variable symbols, a set "C" of constant symbols and sets "F""n" of "n"-ary function symbols, also called operator symbols, for each natural number "n" ≥ 1, the set of (unsorted first-order) terms "T" is recursively defined to be the smallest set with the following properties:
For example, if "x" ∈ "V" is a variable symbol, 1 ∈ "C" is a constant symbol, and "add" ∈ "F"2 is a binary function symbol, then "x" ∈ "T", 1 ∈ "T", and (hence) "add"("x",1) ∈ "T" by the first, second, and third term building rule, respectively. The latter term is usually written as "x"+1, using infix notation and the more common operator symbol + for convenience.
Substitution.
A substitution is a mapping σ: "V" → "T" from variables to terms; the notation refers to a substitution mapping each variable "x""i" to the term "t""i", for "i"=1...,"k", and every other variable to itself. Applying that substitution to a term "t" is written in postfix notation as ; it means to (simultaneously) replace every occurrence of each variable "x""i" in the term "t" by "t""i". The result "t"σ of applying a substitution σ to a term "t" is called an instance of that term "t".
As a first-order example, applying the substitution to the term 
Generalization, specialization.
If a term "t" has an instance equivalent to a term "u", that is, if for some substitution σ, then "t" is called more general than "u", and "u" is called more special than, or subsumed by, "t". For example, is more general than if ⊕ is commutative, since then .
If ≡ is literal (syntactic) identity of terms, a term may be both more general and more special than another one only if both terms differ just in their variable names, not in their syntactic structure; such terms are called variants, or renamings of each other.
For example, 
is a variant of 
since 
and 
However, 
is "not" a variant of 
since no substitution can transform the latter term into the former one.
The latter term is therefore properly more special than the former one.
For arbitrary ≡, a term may be both more general and more special than a structurally different term.
For example, if ⊕ is idempotent, that is, if always , then the term is more general than , and vice versa "z" is more general than , although and "z" are of different structure.
A substitution is more special than, or subsumed by, a substitution if is more special than for each term . We also say that is more general than .
For instance is more special than ,
but 
as is not more special than
Unification problem, solution set.
A unification problem is a finite set of potential equations, where .
A substitution σ is a solution of that problem if for . Such a substitution is also called a unifier of the unification problem.
For example, if ⊕ is associative, the unification problem { "x" ⊕ "a" ≐ "a" ⊕ "x" } has the solutions {"x" ↦ "a"}, {"x" ↦ "a" ⊕ "a"}, {"x" ↦ "a" ⊕ "a" ⊕ "a"}, etc., while the problem { "x" ⊕ "a" ≐ "a" } has no solution.
For a given unification problem, a set "S" of unifiers is called complete if each solution substitution is subsumed by some substitution σ ∈ "S"; the set "S" is called minimal if none of its members subsumes another one.
Syntactic unification of first-order terms.
"Syntactic unification of first-order terms" is the most widely used unification framework.
It is based on "T" being the set of "first-order terms" (over some given set "V" of variables, "C" of constants and "F""n" of "n"-ary function symbols) and on ≡ being "syntactic equality".
In this framework, each solvable unification problem has a complete, and obviously minimal, singleton solution set .
Its member is called the most general unifier (mgu) of the problem.
The terms on the left and the right hand side of each potential equation become syntactically equal when the mgu is applied i.e. .
Any unifier of the problem is subsumed by the mgu .
The mgu is unique up to variants: if "S"1 and "S"2 are both complete and minimal solution sets of the same syntactical unification problem, then "S"1 = { "σ"1 } and "S"2 = { "σ"2 } for some substitutions and and is a variant of for each variable "x" occurring in the problem.
For example, the unification problem { "x" ≐ "z", "y" ≐ "f"("x") } has a unifier { "x" ↦ "z", "y" ↦ "f"("z") }, because
This is also the most general unifier.
Other unifiers for the same problem are e.g. { "x" ↦ "f"("x"1), "y" ↦ "f"("f"("x"1)), "z" ↦ "f"("x"1) }, { "x" ↦ "f"("f"("x"1)), "y" ↦ "f"("f"("f"("x"1))), "z" ↦ "f"("f"("x"1)) }, and so on; there are infinitely many similar unifiers.
As another example, the problem "g"("x","x") ≐ "f"("y") has no solution with respect to ≡ being literal identity, since any substitution applied to the left and right hand side will keep the outermost "g" and "f", respectively, and terms with different outermost function symbols are syntactically different.
A unification algorithm.
The first algorithm given by Robinson (1965) was rather inefficient; cf. box.
The following faster algorithm originated from Martelli, Montanari (1982).
This paper also lists preceding attempts to find an efficient syntactical unification algorithm, and states that linear-time algorithms were discovered independently by Martelli, Montanari (1976) and Paterson, Wegman (1978).
Given a finite set "G" = { "s"1 ≐ "t"1, ..., "s""n" ≐ "t""n" } of potential equations,
the algorithm applies rules to transform it to an equivalent set of equations of the form
where "x"1, ..., "x""m" are distinct variables and "u"1, ..., "u""m" are terms containing none of the "x""i".
A set of this form can be read as a substitution.
If there is no solution the algorithm terminates with ⊥; other authors use "Ω", "{}", or ""fail"" in that case.
The operation of substituting all occurrences of variable "x" in problem "G" with term "t" is denoted "G" {"x" ↦ "t"}.
For simplicity, constant symbols are regarded as function symbols having zero arguments.
Occurs check.
An attempt to unify a variable "x" with a term containing "x" as a strict subterm "x"≐"f"(...,"x"...) would lead to an infinite term as solution for "x", since "x" would occur as a subterm of itself.
In the set of (finite) first-order terms as defined above, the equation "x"≐"f"(...,"x"...) has no solution; hence the "eliminate" rule may only be applied if "x" ∉ "vars"("t").
Since that additional check, called "occurs check", slows down the algorithm, it is omitted e.g. in most Prolog systems.
From a theoretical point of view, omitting the check amounts to solving equations over infinite trees, see below.
Proof of termination.
For the proof of termination of the algorithm consider a triple 
where is the number of variables that occur more than once in the equation set, is the number of function symbols and constants
on the left hand sides of potential equations, and is the number of equations.
When rule "eliminate" is applied, decreases, since "x" is eliminated from "G" and kept only in { "x" ≐ "t" }.
Applying any other rule can never increase again.
When rule "decompose", "conflict", or "swap" is applied, decreases, since at least the left hand side's outermost "f" disappears.
Applying any of the remaining rules "delete" or "check" can't increase , but decreases .
Hence, any rule application decreases the triple with respect to the lexicographical order, which is possible only a finite number of times.
Conor McBride observes that “by expressing the structure which unification exploits” in a dependently typed language such as Epigram, Robinson's algorithm can be made recursive on the number of variables, in which case a separate termination proof becomes unnecessary.
Examples of syntactic unification of first-order terms.
In the Prolog syntactical convention a symbol starting with an upper case letter is a variable name; a symbol that starts with a lowercase letter is a function symbol; the comma is used as the logical "and" operator.
For maths notation, "x,y,z" are used as variables, "f,g" as function symbols, and "a,b" as constants.
The most general unifier of a syntactic first-order unification problem of size may have a size of . For example, the problem has the most general unifier , cf. picture. In order to avoid exponential time complexity caused by such blow-up, advanced unification algorithms work on directed acyclic graphs (dags) rather than trees.
Application: Unification in logic programming.
The concept of unification is one of the main ideas behind logic programming, best known through the language Prolog. It represents the mechanism of binding the contents of variables and can be viewed as a kind of one-time assignment. In Prolog, this operation is denoted by the equality symbol codice_1, but is also done when instantiating variables (see below). It is also used in other languages by the use of the equality symbol codice_1, but also in conjunction with many operations including codice_3, codice_4, codice_5, codice_6. Type inference algorithms are typically based on unification.
In Prolog:
Application: Type inference.
Unification is used during type inference, for instance in the functional programming language Haskell. On one hand, the programmer does not need to provide type information for every function, on the other hand it is used to detect typing errors. The Haskell expression codice_7 is not correctly typed, because the list construction function ":" is of type codice_8" is of type codice_9, but "a" cannot be both Char and Int at the same time.
Like for prolog an algorithm for type inference can be given:
Due to its declarative nature, the order in a sequence of unifications is (usually) unimportant.
Note that in the terminology of first-order logic, an atom is a basic proposition and is unified similarly to a Prolog term.
Order-sorted unification.
"Order-sorted logic" allows one to assign a "sort", or "type", to each term, and to declare a sort "s"1 a "subsort" of another sort "s"2, commonly written as "s"1 ⊆ "s"2. For example, when reаsoning about biological creatures, it is useful to declare a sort "dog" to be a subsort of a sort "animal". Wherever a term of some sort "s" is required, a term of any subsort of "s" may be supplied instead.
For example, assuming a function declaration "mother": "animal" → "animal", and a constant declaration "lassie": "dog", the term "mother"("lassie") is perfectly valid and has the sort "animal". In order to supply the information that the mother of a dog is a dog in turn, another declaration "mother": "dog" → "dog" may be issued; this is called "function overloading", similar to overloading in programming languages.
Walther gave a unification algorithm for terms in order-sorted logic, requiring for any two declared sorts "s"1, "s"2 their intersection "s"1 ∩ "s"2 to be declared, too: if "x"1 and "x"2 is a variable of sort "s"1 and "s"2, respectively, the equation "x"1 ≐ "x"2 has the solution { "x"1 = "x", "x"2 = "x" }, where "x": "s"1 ∩ "s"2.
After incorporating this algorithm into a clause-based automated theorem prover, he could solve a benchmark problem by translating it into order-sorted logic, thereby boiling it down an order of magnitude, as many unary predicates turned into sorts.
Smolka generalized order-sorted logic to allow for parametric polymorphism.
In his framework, subsort declarations are propagated to complex type expressions.
As a programming example, a parametric sort "list"("X") may be declared (with "X" being a type parameter as in a C++ template), and from a subsort declaration "int" ⊆ "float" the relation "list"("int") ⊆ "list"("float") is automatically inferred, meaning that each list of integers is also a list of floats.
Schmidt-Schauß generalized order-sorted logic to allow for term declarations.
As an example, assuming subsort declarations "even" ⊆ "int" and "odd" ⊆ "int", a term declaration like ∀"i":"int". ("i"+"i"):"even" allows to declare a property of integer addition that could not be expressed by ordinary overloading.
Unification of infinite terms.
Background on infinite trees:
Unification algorithm, Prolog II:
Applications:
E-unification.
E-unification is the problem of finding solutions to a given set of equations,
taking into account some equational background knowledge "E".
The latter is given as a set of universal equalities.
For some particular sets "E", equation solving algorithms (a.k.a. "E-unification algorithms") have been devised;
for others it has been proven that no such algorithms can exist.
For example, if and are distinct constants,
the equation has no solution
with respect to purely syntactic unification,
where nothing is known about the operator .
However, if the is known to be commutative,
then the substitution solves the above equation,
since
The background knowledge "E" could state the commutativity of by the universal equality
" for all ".
Particular background knowledge sets E.
It is said that "unification is decidable" for a theory, if a unification algorithm has been devised for it that terminates for "any" input problem.
It is said that "unification is semi-decidable" for a theory, if a unification algorithm has been devised for it that terminates for any "solvable" input problem, but may keep searching forever for solutions of an unsolvable input problem.
Unification is decidable for the following theories:
Unification is semi-decidable for the following theories:
One-sided paramodulation.
If there is a convergent term rewriting system "R" available for "E",
the one-sided paramodulation algorithm
can be used to enumerate all solutions of given equations.
Starting with "G" being the unification problem to be solved and "S" being the identity substitution, rules are applied nondeterministically until the empty set appears as the actual "G", in which case the actual "S" is a unifying substitution. Depending on the order the paramodulation rules are applied, on the choice of the actual equation from "G", and on the choice of "R"’s rules in "mutate", different computations paths are possible. Only some lead to a solution, while others end at a "G" ≠ {} where no further rule is applicable (e.g. "G" = { "f"(...) ≐ "g"(...) }).
For an example, a term rewrite system "R" is used defining the "append" operator of lists built from "cons" and "nil"; where "cons"("x","y") is written in infix notation as "x"."y" for brevity; e.g. "app"("a"."b"."nil","c"."d"."nil") → "a"."app"("b"."nil","c"."d"."nil") → "a"."b"."app"("nil","c"."d"."nil") → "a"."b"."c"."d"."nil" demonstrates the concatenation of the lists "a"."b"."nil" and "c"."d"."nil", employing the rewrite rule 2,2, and 1. The equational theory "E" corresponding to "R" is the congruence closure of "R", both viewed as binary relations on terms.
For example, "app"("a"."b"."nil","c"."d"."nil") ≡ "a"."b"."c"."d"."nil" ≡ "app"("a"."b"."c"."d"."nil","nil"). The paramodulation algorithm enumerates solutions to equations with respect to that "E" when fed with the example "R".
A successful example computation path for the unification problem { "app"("x","app"("y","x")) ≐ "a"."a"."nil" } is shown below. To avoid variable name clashes, rewrite rules are consistently renamed each time before their use by rule "mutate"; "v"2, "v"3, ... are computer-generated variable names for this purpose. In each line, the chosen equation from "G" is highlighted in red. Each time the "mutate" rule is applied, the chosen rewrite rule ("1" or "2") is indicated in parentheses. From the last line, the unifying substitution "S" = { "y" ↦ "nil", "x" ↦ "a"."nil" } can be obtained. In fact,
"app"("x","app"("y","x")) {"y"↦"nil", "x"↦ "a"."nil" } = "app"("a"."nil","app"("nil","a"."nil")) ≡ "app"("a"."nil","a"."nil") ≡ "a"."app"("nil","a"."nil") ≡ "a"."a"."nil" solves the given problem.
A second successful computation path, obtainable by choosing "mutate(1), mutate(2), mutate(2), mutate(1)" leads to the substitution "S" = { "y" ↦ "a"."a"."nil", "x" ↦ "nil" }; it is not shown here. No other path leads to a success.
Narrowing.
If "R" is a convergent term rewriting system for "E",
an approach alternative to the previous section consists in successive application of "narrowing steps";
this will eventually enumerate all solutions of a given equation.
A narrowing step (cf. picture) consists in
Formally, if is a renamed copy of a rewrite rule from "R", having no variables in common with a term "s", and the subterm is not a variable and is unifiable with via the mgu , then can be narrowed to the term , i.e. to the term , with the subterm at "p" replaced by . The situation that "s" can be narrowed to "t" is commonly denoted as "s" ~› "t".
Intuitively, a sequence of narrowing steps "t"1 ~› "t"2 ~› ... ~› "t""n" can be thought of as a sequence of rewrite steps "t"1 → "t"2 → ... → "t""n", but with the initial term "t"1 being further and further instantiated, as necessary to make each of the used rules applicable.
The above example paramodulation computation corresponds to the following narrowing sequence ("↓" indicating instatiation here):
The last term, "v"2."v"2."nil" can be syntactically unified with the original right hand side term "a"."a"."nil".
The "narrowing lemma" ensures that whenever an instance of a term "s" can be rewritten to a term "t" by a convergent term rewriting system, then "s" and "t" can be narrowed and rewritten to a term and , respectively, such that is an instance of .
Formally: whenever holds for some substitution σ, then there exist terms such that and and for some substitution τ.
Higher-order unification.
Many applications require one to consider the unification of typed lambda-terms instead of first-order terms. Such unification is often called "higher-order unification". A well studied branch of higher-order unification is the problem of unifying simply typed lambda terms modulo the equality determined by αβη conversions. Such unification problems do not have most general unifiers. While higher-order unification is undecidable, Gérard Huet gave a semi-decidable (pre-)unification algorithm that allows a systematic search of the space of unifiers (generalizing the unification algorithm of Martelli-Montanari with rules for terms containing higher-order variables) that seems to work sufficiently well in practice. Huet and Gilles Dowek have written articles surveying this topic.
Dale Miller has described what is now called higher-order pattern unification. This subset of higher-order unification is decidable and solvable unification problems have most-general unifiers. Many computer systems that contain higher-order unification, such as the higher-order logic programming languages λProlog and Twelf, often implement only the pattern fragment and not full higher-order unification.
In computational linguistics, one of the most influential theories of ellipsis is that ellipses are represented by free variables whose values are then determined using Higher-Order Unification (HOU). For instance, the semantic representation of "Jon likes Mary and Peter does too" is like(j; m)R(p) and the value of R (the semantic representation of the ellipsis) is determined by the equation like(j; m) = R(j). The process of solving such equations is called Higher-Order Unification.
For example, the unification problem { "f"("a", "b", "a") ≐ "d"("b", "a", "c") }, where the only variable is "f", has the
solutions {"f" ↦ λ"x".λ"y".λ"z"."d"("y", "x", "c") }, {"f" ↦ λ"x".λ"y".λ"z"."d"("y", "z", "c") },
Wayne Snyder gave a generalization of both higher-order unification and E-unification, i.e. an algorithm to unify lambda-terms modulo an equational theory.

</doc>
<doc id="54434" url="https://en.wikipedia.org/wiki?curid=54434" title="The Wizard of Oz">
The Wizard of Oz

The Wizard of Oz may refer to:

</doc>
<doc id="54436" url="https://en.wikipedia.org/wiki?curid=54436" title="The Wonderful Wizard of Oz">
The Wonderful Wizard of Oz

The Wonderful Wizard of Oz is an American children's novel written by author L. Frank Baum and illustrated by W. W. Denslow, originally published by the George M. Hill Company in Chicago on May 17, 1900. It has since been reprinted on numerous occasions, most often under the title The Wizard of Oz, which is the title of the popular 1902 Broadway musical as well as the iconic 1939 musical film adaptation.
The story chronicles the adventures of a young farm girl named Dorothy in the magical Land of Oz, after she and her pet dog Toto are swept away from their Kansas home by a cyclone. The novel is one of the best-known stories in American literature and has been widely translated. The Library of Congress has declared it "America's greatest and best-loved homegrown fairytale." Its groundbreaking success and the success of the Broadway musical adapted from the novel led Baum to write thirteen additional Oz books that serve as official sequels to the first story.
Baum dedicated the book "to my good friend & comrade, My Wife" Maud Gage Baum. In January 1901, George M. Hill Company completed printing the first edition, a total of 10,000 copies, which quickly sold out. "The Wonderful Wizard of Oz" sold three million copies by the time it entered the public domain in 1956.
Publication.
The book was published by George M. Hill Company. Its first edition had a printing of 10,000 copies and was sold in advance of the publication date of September 1, 1900. On May 17, 1900, the first copy of the book came off the press; Baum assembled it by hand and presented it to his sister Mary Louise Baum Brewster. The public saw the book for the first time at a book fair at the Palmer House in Chicago, July 5–20. The book's copyright was registered on August 1; full distribution followed in September. By October 1900, the first edition had already sold out and the second edition of 15,000 copies was nearly depleted.
In a letter to his brother Harry, Baum wrote that the book's publisher George M. Hill predicted a sale of about 250,000 copies. In spite of this favorable conjecture, Hill did not initially predict that the book would be phenomenally successful. He agreed to publish the book only when the manager of the Chicago Grand Opera House Fred R. Hamlin committed to making "The Wonderful Wizard of Oz" into a musical stage play to publicize the novel. The play "The Wizard of Oz" debuted on June 16, 1902. It was revised to suit adult preferences and was crafted as a "musical extravaganza", with the costumes modeled after Denslow's drawings. Hill's publishing company became bankrupt in 1901, so Baum and Denslow agreed to have the Indianapolis-based Bobbs-Merrill Company resume publishing the novel.
Baum's son Harry Neal told the "Chicago Tribune" in 1944 that L. Frank told his children "whimsical stories before they became material for his books". Harry called his father the "swellest man I knew", a man who was able to give a decent reason as to why black birds cooked in a pie could afterwards get out and sing.
By 1938, more than one million copies of the book had been printed. Less than two decades later in 1956, the sales of his novel had grown to three million copies in print.
Plot summary.
Dorothy is a young girl who lives with her Aunt Em and Uncle Henry and her little dog Toto on a Kansas farm. One day, Dorothy and Toto are caught up in a cyclone that deposits her farmhouse into Munchkin Country in the magical Land of Oz. The falling house has killed the Wicked Witch of the East, the evil ruler of the Munchkins. The Good Witch of the North arrives with the grateful Munchkins and gives Dorothy the magical Silver Shoes that once belonged to the witch. The Good Witch tells Dorothy that the only way she can return home is to go to the Emerald City and ask the great and powerful Wizard of Oz to help her. As Dorothy embarks on her journey, the Good Witch of the North kisses her on the forehead, giving her magical protection from harm.
On her way down the yellow brick road, Dorothy attends a banquet held by a Munchkin man named Boq. The next day, Dorothy frees the Scarecrow from the pole on which he is hanging, applies oil from a can to the rusted connections of the Tin Woodman, and meets the Cowardly Lion. The Scarecrow wants a brain, the Tin Woodman wants a heart, and the Cowardly Lion wants courage, so Dorothy encourages the three of them to journey with her and Toto to the Emerald City to ask for help from the Wizard. After several adventures, the travelers enter the gates of the Emerald City and meet the Guardian of the Gates, who asks them to wear green tinted spectacles to keep their eyes from being blinded by the city's brilliance. Each one is called to see the Wizard: Dorothy sees the Wizard as a giant head on a marble throne, the Scarecrow as a lovely lady in silk gauze, the Tin Woodman as a terrible beast, the Cowardly Lion as a ball of fire. The Wizard agrees to help them all if they kill the Wicked Witch of the West, who rules over Oz's Winkie Country. The Guardian warns them that no one has ever managed to defeat the witch.
The Wicked Witch of the West sees the travelers approaching with her one telescopic eye. She sends a pack of wolves to tear them to pieces, but the Tin Woodman kills them with his axe. She sends wild crows to peck their eyes out, but the Scarecrow kills them by breaking their necks. She summons a swarm of black bees to sting them, but they are killed trying to sting the Tin Woodman while the Scarecrow's straw hides the other three. She sends her Winkie soldiers to attack them, but the Cowardly Lion stands firm to repel them. Finally, she uses the power of the Golden Cap to send the winged monkeys to capture Dorothy, Toto, and the Cowardly Lion, unstuff the Scarecrow, and dent the Tin Woodman. Dorothy is forced to become the Wicked Witch's personal slave, while the witch schemes to steal Dorothy's Silver Shoes.
The Wicked Witch successfully tricks Dorothy out of one of her Silver Shoes. Angered, Dorothy throws a bucket of water at her and is shocked to see the witch melt away. The Winkies rejoice at being freed of the witch's tyranny and help restuff the Scarecrow and mend the Tin Woodman. They ask the Tin Woodman to become their ruler, which he agrees to do after helping Dorothy return to Kansas. Dorothy finds the Golden Cap and summons the Winged Monkeys to carry her and her companions back to the Emerald City. The King of the Winged Monkeys tells how he and the other monkeys are bound by an enchantment to the cap by the sorceress Gayelette from the North, and that Dorothy may use the cap to summon the Winged Monkeys two more times.
When Dorothy and her friends meet the Wizard of Oz again, Toto tips over a screen in a corner of the throne room that reveals the Wizard. He sadly explains he is a humbug—an ordinary old man who, by a hot air balloon, came to Oz long ago from Omaha. The Wizard provides the Scarecrow with a head full of bran, pins, and needles ("a lot of bran-new brains"), the Tin Woodman with a silk heart stuffed with sawdust, and the Cowardly Lion a potion of "courage". Their faith in the Wizard's power gives these items a focus for their desires. The Wizard decides to take Dorothy and Toto home and leave the Emerald City. At the send-off, he appoints the Scarecrow to rule in his stead, which he agrees to do after Dorothy returns to Kansas. Toto chases a kitten in the crowd and Dorothy goes after him, but the tethers of the balloon break and the Wizard floats away.
Dorothy summons the Winged Monkeys to carry her and Toto home, but they explain they cannot cross the desert surrounding Oz. The Soldier with the Green Whiskers informs Dorothy that Glinda the Good Witch of the South may be able to help her return home, so the friends begin their journey to see Glinda, who lives in Oz's Quadling Country. On the way, the Cowardly Lion kills a giant spider who is terrorizing the animals in a forest. The animals ask the Cowardly Lion to become their king, which he agrees to do after helping Dorothy return to Kansas. Dorothy summons the Winged Monkeys a third time to fly them over a mountain to Glinda's palace. Glinda greets the travelers and reveals that the Silver Shoes Dorothy wears can take her anywhere she wishes to go. Dorothy embraces her friends, all of whom will be returned to their new kingdoms through Glinda's three uses of the Golden Cap: the Scarecrow to the Emerald City, the Tin Woodman to the Winkie Country, and the Lion to the forest; after which the cap shall be given to the King of the Winged Monkeys, freeing them. Dorothy takes Toto in her arms, knocks her heels together three times, and wishes to return home. Instantly, she begins whirling through the air and rolling through the grass of the Kansas prairie, up to her Kansas farmhouse. Dorothy runs to her Aunt Em, saying "I'm so glad to be at home again!"
Illustration and design.
The book was illustrated by Baum's friend and collaborator W. W. Denslow, who also co-held the copyright. The design was lavish for the time, with illustrations on many pages, backgrounds in different colors, and several color plate illustrations. In September 1900, The "Grand Rapids Herald" wrote that Denslow's illustrations are "quite as much of the story as in the writing". The editorial opined that had it not been for Denslow's pictures, the readers would be unable to picture precisely the figures of Dorothy, Toto, and the other characters.
The distinctive look led to imitators at the time, most notably Eva Katherine Gibson's "Zauberlinda, the Wise Witch", which mimicked both the typography and the illustration design of "Oz". The typeface was the newly designed Monotype Old Style. Denslow's illustrations were so well known that merchants of many products obtained permission to use them to promote their wares. The forms of the Scarecrow, the Tin Woodman, the Cowardly Lion, the Wizard, and Dorothy were made into rubber and metal sculptures. Costume jewelry, mechanical toys, and soap were also designed using their figures.
A new edition of the book appeared in 1944, with illustrations by Evelyn Copelman. Although it was claimed that the new illustrations were based on Denslow's originals, they more closely resemble the characters as seen in the famous 1939 film version of Baum's book.
Sources of images and ideas.
Baum acknowledged the influence of the Brothers Grimm and Hans Christian Andersen, which he was deliberately revising in his "American fairy tales" to include the wonder without the horrors.
The land of Oz and other locations.
Local legend has it that Oz, also known as The Emerald City, was inspired by a prominent castle-like building in the community of Castle Park near Holland, Michigan where Baum lived during the summer. The yellow brick road was derived from a road at that time paved by yellow bricks. These bricks were located in Peekskill, New York where Baum attended the Peekskill Military Academy. Baum scholars often refer to the 1893 Chicago World's Fair (the "White City") as an inspiration for the Emerald City. Other legends suggest that the inspiration came from the Hotel Del Coronado near San Diego, California. Baum was a frequent guest at the hotel and had written several of the Oz books there. In a 1903 interview with "Publishers Weekly", Baum said that the name "OZ" came from his file cabinet labeled "O-Z".
Some critics have suggested that Baum may have been inspired by Australia, a relatively new country at the time of the book's original publication. Australia is often colloquially spelled or referred to as "Oz". Furthermore, in "Ozma of Oz" (1907), Dorothy gets back to Oz as the result of a storm at sea while she and Uncle Henry are traveling by ship to Australia. So, like Australia, Oz is somewhere to the west of California. Like Australia, Oz is an island continent. Like Australia, Oz has inhabited regions bordering on a great desert. One might almost imagine that Baum intended Oz to be Australia, or perhaps a magical land in the center of the great Australian desert.
"Alice's Adventures in Wonderland".
Another influence lay in Lewis Carroll's "Alice's Adventures in Wonderland". A September 1900 review in the "Grand Rapids Herald" called "The Wonderful Wizard of Oz" a "veritable "Alice in Wonderland" brought up to the present day standard of juvenile literature". Baum found Carroll's plots incoherent, but he identified their source of popularity as Alice herself, a child with whom the child readers could identify; this influenced his choice of a protagonist. Baum was also influenced by Carroll's belief that children's books should have many pictures and be pleasurable to read. Carroll rejected the Victorian-era ideology that children's books should be saturated with morals, instead believing that children should be allowed to be children. Building on Carroll's style of numerous images accompanying the text, Baum amalgamated the conventional features of a fairy tale (witches and wizards) with the well-known things in his readers' lives (scarecrows and cornfields).
American fantasy story.
"The Wonderful Wizard of Oz" is considered the first American fairy tale because of its references to clear American locations such as Kansas and Omaha. Baum agreed with authors such as Carroll that fantasy literature was important for children, along with numerous illustrations, but he also wanted to create a story that had recognizable American elements in it, such as farming and industrialization.
Baum's personal life.
Many of the characters, props, and ideas in the novel were drawn from Baum's experiences. As a child, Baum frequently had nightmares of a scarecrow pursuing him across a field. Moments before the scarecrow's "ragged hay fingers" nearly gripped his neck, it would fall apart before his eyes. Decades later as an adult, Baum integrated his tormentor into the novel as the Scarecrow. According to his son Harry, the Tin Woodman was born from Baum's attraction to window displays. He wished to make something captivating for the window displays, so he used an eclectic assortment of scraps to craft a striking figure. From a washboiler he made a body, from bolted stovepipes he made arms and legs, and from the bottom of a saucepan he made a face. Baum then placed a funnel hat on the figure, which ultimately became the Tin Woodman. John D. Rockefeller was the nemesis of Baum's father, an oil baron who declined to purchase Standard Oil shares in exchange for selling his own oil refinery. Baum scholar Evan I. Schwartz posited that Rockefeller inspired one of the Wizard's numerous faces. In one scene in the novel, the Wizard is seen as a "tyrannical, hairless head". When Rockefeller was 54 years old, the medical condition alopecia caused him to lose every strand of hair on his head, making people fearful of speaking to him.
In the early 1880s, Baum's play "Matches" was being performed when a "flicker from a kerosene lantern sparked the rafters", causing the Baum opera house to be consumed by flames. Scholar Evan I. Schwartz suggested that this might have inspired the Scarecrow's severest terror: "There is only one thing in the world I am afraid of. A lighted match."
In 1890, Baum lived in Aberdeen which was experiencing a drought, and he wrote a witty story in his "Our Landlady" column in Aberdeen's "The Saturday Pioneer" about a farmer who gave green goggles to his horses, causing them to believe that the wood chips which they were eating were pieces of grass. Similarly, the Wizard made the people in the Emerald City wear green goggles so that they would believe that their city was built from emeralds.
Baum, a former salesman of china, wrote in chapter 20 about china that had sprung to life.
During Baum's short stay in Aberdeen, the dissemination of myths about the plentiful West continued. However, the West, instead of being a wonderland, turned into a wasteland because of a drought and a depression. In 1891, Baum moved his family from South Dakota to Chicago. At that time, Chicago was getting ready for the World's Columbian Exposition in 1893. Scholar Laura Barrett stated that Chicago was "considerably more akin to Oz than to Kansas". After discovering that the myths about the West's incalculable riches were baseless, Baum created "an extension of the American frontier in Oz". In many respects, Baum's creation is similar to the actual frontier save for the fact that the West was still undeveloped at the time. The Munchkins Dorothy encounters at the beginning of the novel represent farmers, as do the Winkies she later meets.
Baum's wife frequently visited her niece, Dorothy Louise Gage. The infant became gravely sick and died on November 11, 1898, of "congestion of the brain" at exactly five months. When the baby, whom Maud adored as the daughter she never had, died, she was devastated and needed to consume medicine. To assuage her distress, Frank made his protagonist of "The Wonderful Wizard of Oz" a female named Dorothy. Uncle Henry was modeled after Henry Gage, his wife Maud's father. Bossed around by his wife Matilda, Henry rarely dissented with her. He flourished in business, though, and his neighbors looked up to him. Likewise, Uncle Henry was a "passive but hard-working man" who "looked stern and solemn, and rarely spoke". The witches in the novel were influenced by witch-hunting research gathered by Baum's mother-in-law, Matilda. The stories of barbarous acts against accused witches scared Baum. Two key events in the novel involve wicked witches who both meet their death through metaphorical means.
Baum held different jobs, moved a lot, and was exposed to many people, so the inspiration for the story could have been taken from many different aspects of his life. In the introduction to the story, Baum writes that "it aspires to being a modernized fairy tale, in which the wonderment and joy are retained and the heart-aches and nightmares are left out." This is one of the explanations that he gives for the inspiration for "The Wonderful Wizard of Oz".
Influence of Denslow.
The original illustrator of the novel, W.W. Denslow, could also have had an impact on the story and the way it has been interpreted. Baum and Denslow had a close working relationship, and worked together to create the presentation of the story through the images and the text. Color is an important element of the story and is present throughout the images with each chapter having a different color representation. Denslow also added characteristics to his drawings that Baum never described. For example, Denslow drew a house and the gates of the Emerald City with faces on them. In the later Oz books, John R. Neill, who illustrated all of the sequels, continued to include these faces on gates.
Allusions to 19th-century America.
Baum did not offer any conclusive proof that he intended his novel to be a political allegory. Historian Ranjit S. Dighe wrote that for sixty years after the book's publication, "virtually nobody" had such an interpretation until Henry Littlefield, a high school teacher. In his 1964 "American Quarterly" article, "The Wizard of Oz: Parable on Populism", Littlefield posited that the book contained an allegory of the late 19th-century bimetallism debate regarding monetary policy.
Not only did Baum draw inspiration from the American land around him, but he also created Oz to display an American utopia where the issues of the day were solved. There is little distinction between utopia and fairy tale land. Baum believed that the imagination was the best tool for creating a striving society. In a later book in the Oz series, Baum writes, "imaginations and dreams are likely to lead to the betterment of the world. The imaginative child will become the imaginative man or woman most apt to create, to invent and, therefore, to foster civilization." In this way, Baum wrote The Wonderful Wizard of Oz as a modern fairy tale depicting Oz as an American utopia that displays numerous allusion and solutions to issues in the late 19th and early 20th century.
The realm of Oz very closely resembles America. It contains four countries, the Land of the North, East, West, and South, and the national capital, the Emerald City. America and its inhabitants are often divided into similar categories such as Midwestern, Southern, etc. These locations are also separated by an American color scheme that was relevant to American during the 19th century. The color blue represents the industrial East known for the blue-collar jobs. The South is red for the red earth it contains or the "redneck" inhabitants. Yellow describes the West denoting the California gold rush. Finally, the Emerald City as Washington D. C. denoting greenbacks and money of the country.
The villains of the story are the Wicked Witch of the West and the Wicked Witch of the East. The wicked witches use their power to control and enslave their subjects. There was an equal balance between good and evil and until this balance was altered there could be no change or development in Oz. This standoff can be seen as an allusion between the different political parties in America. The Wicked Witch of the West represents the American West, including the wealthy railroad, oil barons, and nature. The American West's greatest weapon in the 19th century, though, was nature, most malignantly the drought. The effects lasted longer than any fire or twister and a long enough drought could ruin a whole year's worth of crops. Thus, water as the weapon to kill the Witch of the West is quite poetic. The brown mass the witch's remains turn into resembles mud after a heavy storm. Dorothy even cleans the melted witch off the floor and her shoes as if she had walked through a rainy puddle.
Baum's Wicked Witch of the East has been suggested to represent Eastern financial and industrial interests, such as Wall Street, which oppresses the agricultural citizens. The Witch of the East enslaved her subjects much as industrialism was thought to enslave the working class in 19th-century Eastern America. Once Dorothy killed the Wicked Witch of the East the balance of power could be shifted. Both these groups opposed Populist efforts to move the U.S. to a bimetallic monetary standard since this would have devalued the dollar and made investments less valuable. Workers and poor farmers supported the move away from the gold standard, as this would have lessened their crushing debt burdens. The Populist party sought to build a coalition of Southern and Midwestern tenant farmers and Northern industrial workers. These groups are represented in the book by the Good Witches of the North and South.
At the beginning of the novel, Dorothy is swept from her farm to Oz by a cyclone, which was frequently compared to the Free Silver movement in Baum's time. The yellow brick road represents the gold standard and the Silver Shoes which enable Dorothy to travel more comfortably symbolizes the Populist Party's desire to construct a bimetallic standard of both gold and silver in place of the gold standard. "Oz" is the abbreviated form of ounce, a standard measure of gold. When Dorothy and the Scarecrow walk through the forest, the road begins to be rough and patchy causing the Scarecrow to trip and fall numerous times. The Scarecrow's falls on the Yellow Brick Road resemble the damage farmers faced owing to deflation caused by the scarcity of gold. Dorothy however, simply "walks around" the holes in the road showing that the bimetallic standard works. Even when gold was scarce, the other metal in the bimetal system would be relatively cheap therefore one bypasses deflation. Throughout the book, most of the characters do not know the magic behind the silver shoes. It is not until Dorothy meets Glinda, the good Witch of the South, that she finds out that the silver shoes have had the power to transfer her back to Kansas all along. Baum is possibly alluding to the fact that the bimetallic standard has been a working solution to the economic crisis all along, though no one knows how to do it. Once Dorothy clicks her heels three times, she returns to Kansas where she realizes "The Silver Shoes had fallen off in her flight through the air, and were lost forever in the desert." The silver shoes were lost, much like the fight for the bimetallic standard, which began to fade away in 1900.
The Wizard is the national leader of Oz, thus it is fitting for him to symbolize the President(s) of the United States during the 19th century. Politicians have been known to have many faces; it is a must if they want to be able to be everything for everyone. The Great and Terrible Wizard of Oz agrees to meet with each traveler separately, allowing him to alter his appearance to best fit each character. When the gang returns having completed the Wizard's task, they discover that the Wizard is a fake and is actually just "a common man" who has made everyone believe he was powerful. The Scarecrow adds that he is a humbug, in which the Wizard gladly says that is he is exactly that. The Wizard made promises he could not keep, as did many 19th-century politicians. The Wizard later states, "How can I help being a humbug… when all these people make me do things that everyone knows can't be done," showing that he was able to deceive others because others were willing to be deceived.
While journeying to the Emerald City, Dorothy encounters a scarecrow, who represents a farmer. The Scarecrow believes he is a fool since his head is full of straw and not brains. Four years before the book's release, William Allen White, a journalist from Chicago, published an article entitled "What's the Matter With Kansas?" In the article White questions why Kansas is unsatisfied and sarcastically answers saying America needs "fewer white shirts and brains" implying Western farmers were ignorant, lazy and bad businessmen. The Scarecrow shares this opinion and doubts himself, believing he is inferior without a brain. In the same year White published his article, William Jennings Bryan delivered his famous "Cross of Gold speech" at the 1896 Democratic Convention. Bryan fought for the farmers and argued against such accusations made by White exclaiming, "The farmer who goes forth in the morning and toils all day, begins in the spring and toils all summer, and by the application of brain and muscle to the natural resources of this country creates wealth, is as much a businessman as the man who goes upon the Board of Trade and bets upon the price of grain." Baum shares Bryan's view on American farmers by showing that the Scarecrow is sharp and capable by his actions throughout the book. The book ends with "farm interests achieving national importance" and farmers' true potential in politics being revealed once the illusion of ignorance has been removed.
The next companion Dorothy meets on the yellow brick road is the Tin Woodsman who has been cursed by the now deceased Wicked Witch of the East. He was once a hard worker yearning to earn money to start a family with a Munchkin girl. The witch enchanted his axe so that the Woodsman chopped off each of his limbs and eventually his body. Each time a tinner healed the Woodsman by replacing his body with tin, however once the Tin Woodsman's heart was removed he could no longer love. The Tin Woodsman symbolizes the Eastern working man competing in an industrialized society. The 19th-century man had to keep up with the machine in order to be useful and relevant. In this way the Witch of the East's curse "dehumanized a simple laborer so that the faster and better he worked the more quickly he became a kind of machine." The Tin Woodsman was caught in the rain and rusted in the same position for one year before Dorothy oils his joints to set him free. The Tin Woodsman's year of waiting is parallel to the unemployment of Eastern workers during the severe depression of 1893-1897. His calls of help that were never heard relate to President Grover Cleveland's "hard-hearted refusal" to take action during this time to reinstate the economy. While the Tin Woodsman stood still for a year, he finally slowed down enough to ponder life. During this time, he discovers that "the greatest loss had known was the loss of [his heart" for without it he cannot love. Baum portrays the Tin Woodsman as an Eastern worker who lost sight of family values for a moment. Part of the Progressive movement in the 19th century was to reestablish the family as the center of American life. The curse of the Tin Woodsman by the Wicked Witch of the East is consistent with the depiction of the witch representing Wall Street and other Eastern big businesses during the late 19th and early 20th centuries.
The final addition to the traveling party is the Cowardly Lion. This character is hypothesized to be the famous Populist politician William Jennings Bryan. The muscular, six-foot-tall (183 cm) political figure was known as a compassionate but powerful speaker, which could be compared to a lion's roar. Throughout the book, Baum is mostly sympathetic towards populist characters such as the Scarecrow; thus it seems odd that Baum would refer to the character portraying Bryan as cowardly. However, the late 19th century began an age of American expansionism in which the United States struggled to gain control of countries like Guam, Puerto Rico, and the Philippines from Spain. Bryan's non-violent and anti-imperialist stance on the popular Spanish–American War of 1898 was often referred to as unpatriotic and cowardly. Baum seems to take this criticism and turn it into a complaint towards Bryan, showing that although the Lion is the King of the Beasts it shows more bravery to stand by than to run in towards unnecessary obstacles, no matter how popular. The Cowardly Lion's first encounter with the Tin Woodsman shows support for both of their characters being based on Bryan and Eastern industrial workers, respectively. When they meet, the Cowardly Lion strikes the Tin Woodsman with his sharp claws, but to his surprise "he could make an impression on the tin, although the Woodman fell over in the road and lay still." This refers to Bryan's inability to get votes in the 1896 presidential election from Eastern workers owing to pressure from their employers to vote for McKinley. Bryan himself said, "During the campaign I ran across various evidences of coercion, direct and indirect." Other historical sources share this opinion noting, "for some reason labor remained singularly unimpressed." Thus, the Cowardly Lion's claws did not pierce the Tin Woodsman body, just as Bryan's roar did not leave an impression with Eastern industrial workers.
Baum's fairy tale contains many other allusions to American life in the late 19th and early 20th centuries. For instance, Dorothy's loyal companion, Toto, could be a play on "teetotalers" which is a person who never drinks alcohol. The prohibitionists deemed alcohol consumption should be unlawful and they were longtime political allies of Populists during the late 19th century. Ironically, Toto trots "soberly" behind Dorothy on their adventure. The Winged Monkeys have been thought to resemble the Plains Indians who were "once free people" but were now enslaved by the Wicked Witch of the West. Their actions can be good or bad depending on how they are controlled; however they "belong to this country alone" and therefore cannot leave, as can be said for American Indians. The Yellow Winkies that inhabit the Land of the West could represent Asian workers in California during the gold rush; their harsh work environment can be seen as enslavement. The lenses that the groups must wear before entering the City of Oz to dim the emeralds' shine turn out to be fake, like the Wizard. The Emerald City is not a city made of emeralds but a plain, white city where the emerald color lenses cast a green hue on everything and everyone. This shows that anything can be made to look spectacular if you allow yourself to be tricked. Baum traveled all over the United States and would have been introduced to all these events in the 19th century, making it very likely that he drew inspiration from the land around him.
Baum's modernized fairy tale, The Wonderful Wizard of Oz, ends with both the Wicked Witch of the East and West defeated and Dorothy, Toto and the Wizard returning to the United States. The Scarecrow reigns over the Emerald City; thus farmers achieve national importance. The Tin Woodsman brings industrialization to the Land of the West. And the Cowardly Lion becomes the protector in the Grand Old Forest, as Bryan commanded a smaller number of politicians.
Littlefield's thesis achieved some support, but has been strenuously attacked by others.
Cultural impact.
"The Wonderful Wizard of Oz" has become an established part of multiple cultures, spreading from its early young American readership to becoming known throughout the world. It has been translated or adapted into well over fifty languages, at times being modified in local variations. For instance, in some abridged Indian editions, the Tin Woodman was replaced with a snake. In Russia, a translation by Alexander Melentyevich Volkov produced five books, "The Wizard of the Emerald City" series, which became progressively distanced from the Baum version, as Ellie and her dog Totoshka travel throughout the Magic Land. The 1939 film adaptation has become a classic of popular culture, shown annually on American television from 1959 to 1991 and then several times a year every year beginning in 1999. More recently, the story has become an American stage production with an all-black cast, set in the context of modern African-American culture.
Critical response.
"The Wonderful Wizard of Oz" received positive critical reviews upon release. In a September 1900 review, "The New York Times" praised the novel, writing that it would appeal to child readers and to younger children who could not read yet. The review also praised the illustrations for being a pleasant complement to the text.
During the first 50 years after "The Wonderful Wizard of Oz"s publication in 1900, it received little critical analysis from scholars of children's literature. According to Ruth Berman of "Science Fiction Studies", the lists of suggested reading published for juvenile readers never contained Baum's work. The lack of interest stemmed from the scholars' misgivings about fantasy, as well as to their belief that lengthy series had little literary merit.
It has frequently come under fire over the years. In 1957, the director of Detroit's libraries banned "The Wonderful Wizard of Oz" for having "no value" for children of his day, for supporting "negativism", and for bringing children's minds to a "cowardly level". Professor Russel B. Nye of Michigan State University countered that "if the message of the Oz books—love, kindness, and unselfishness make the world a better place—seems of no value today", then maybe the time is ripe for "reassess a good many other things besides the Detroit library's approved list of children's books".
In 1986, seven Fundamentalist Christian families in Tennessee opposed the novel's inclusion in the public school syllabus and filed a lawsuit.
They based their opposition to the novel on its depicting benevolent witches and promoting the belief that integral human attributes were "individually developed rather than God given". One parent said, "I do not want my children seduced into godless supernaturalism". Other reasons included the novel's teaching that females are equal to males and that animals are personified and can speak. The judge ruled that when the novel was being discussed in class, the parents were allowed to have their children leave the classroom.
Feminist author Margery Hourihan has described the book as a "banal and mechanistic story which is written in flat, impoverished prose".
Leonard Everett Fisher of "The Horn Book Magazine" wrote in 2000 that "Oz" has "a timeless message from a less complex era, and it continues to resonate". The challenge of valuing oneself during impending adversity has not, Fisher noted, lessened during the prior 100 years.
In a 2002 review, Bill Delaney of "Salem Press" praised Baum for giving children the opportunity to discover magic in the mundane things in their everyday lives. He further commended Baum for teaching "millions of children to love reading during their crucial formative years".
The Library of Congress has declared "The Wonderful Wizard of Oz" to be America's greatest and best-loved homegrown fairytale", also naming it the first American fantasy for children and one of the most-read children's books.
Editions.
After George M. Hill's bankruptcy in 1902, copyright in the book passed to the Bobbs-Merrill Company. The editions they published lacked most of the in-text color and color plates of the original. It was not until the book entered the public domain in 1956 that new editions, either with the original color plates, or new illustrations, proliferated. Notable among them are the 1986 Pennyroyal edition illustrated by Barry Moser, which was reprinted by the University of California Press, and the 2000 "Annotated Wizard of Oz" edited by Michael Patrick Hearn, which was published by W.W. Norton and included all the original color illustrations, as well as supplemental artwork by Denslow. Other centennial editions included University Press of Kansas's "Kansas Centennial Edition," illustrated by Michael McCurdy with black-and-white illustrations, and Robert Sabuda's pop-up book.
Sequels.
Baum wrote "The Wonderful Wizard of Oz" without any thought of a sequel. After reading the novel, thousands of children wrote letters to him, requesting that he craft another story about Oz. In 1904, he wrote and published the first sequel, "The Marvelous Land of Oz", explaining that he grudgingly wrote the sequel to address the popular demand. Baum also wrote sequels in 1907, 1908, and 1909. In his 1911 "The Emerald City of Oz", he wrote that he could not continue writing sequels because Ozland had lost contact with the rest of the world. The children refused to accept this story, so Baum, in 1913 and every year thereafter until his death in May 1919, wrote an "Oz" book, ultimately writing 13 sequels. The "Chicago Tribune"s Russell MacFall wrote that Baum explained the purpose of his novels in a note he penned to his sister, Mary Louise Brewster, in a copy of "Mother Goose in Prose" (1897), his first book. He wrote, "To please a child is a sweet and a lovely thing that warms one's heart and brings its own reward." After Baum's death in 1919, Baum's publishers delegated the creation of more sequels to Ruth Plumly Thompson who wrote 21. An original "Oz" book was published every Christmas between 1913 and 1942. By 1956, five million copies of the "Oz" books had been published in the English language, while hundreds of thousands had been published in eight foreign languages.
Adaptations.
"The Wonderful Wizard of Oz" has been adapted to other media numerous times, most famously in "The Wizard of Oz", the 1939 film starring Judy Garland, Ray Bolger, Jack Haley, and Bert Lahr. Until this version, the book had inspired a number of now less well known stage and screen adaptations, including a profitable 1902 Broadway musical and three silent films. The 1939 film was considered innovative because of its songs, special effects, and revolutionary use of the new Technicolor.
The story has been translated into other languages (at least once without permission) and adapted into comics several times. Following the lapse of the original copyright, the characters have been adapted and reused in spin-offs, unofficial sequels, and reinterpretations, some of which have been controversial in their treatment of Baum's characters.

</doc>
<doc id="54439" url="https://en.wikipedia.org/wiki?curid=54439" title="Metabolic syndrome">
Metabolic syndrome

Metabolic syndrome is a clustering of at least three of five of the following medical conditions: abdominal (central) obesity, elevated blood pressure, elevated fasting plasma glucose, high serum triglycerides, and low high-density lipoprotein (HDL) levels.
Metabolic syndrome is associated with the risk of developing cardiovascular disease and diabetes. Some studies have shown the prevalence in the USA to be an estimated 34% of the adult population, and the prevalence increases with age.
Metabolic syndrome and prediabetes may be the same disorder, just diagnosed by a different set of biomarkers.
The syndrome is thought to be caused by an underlying disorder of energy utilization and storage. The cause of the syndrome is an area of on-going medical research.
Signs and symptoms.
The main sign of metabolic syndrome is central obesity (also known as visceral, male-pattern or apple-shaped adiposity), overweight with adipose tissue accumulation particularly around the waist and trunk.
Other signs of metabolic syndrome include high blood pressure, decreased fasting serum HDL cholesterol, elevated fasting serum triglyceride level (VLDL triglyceride), impaired fasting glucose, insulin resistance, or prediabetes.
Associated conditions include hyperuricemia, fatty liver (especially in concurrent obesity) progressing to nonalcoholic fatty liver disease, polycystic ovarian syndrome (in women), erectile dysfunction (in men), and acanthosis nigricans.
Cause.
The exact mechanisms of the complex pathways of metabolic syndrome are under investigation. The pathophysiology is very complex and has been only partially elucidated. Most patients are older, obese, sedentary, and have a degree of insulin resistance. Stress can also be a contributing factor. The most important risk factors are diet (particularly sugar-sweetened beverage consumption), genetics, 
aging,
sedentary behavior 
or low physical activity,
disrupted chronobiology/sleep,
mood disorders/psychotropic medication use,
and excessive alcohol use.
There is debate regarding whether obesity or insulin resistance is the cause of the metabolic syndrome or if they are consequences of a more far-reaching metabolic derangement. A number of markers of systemic inflammation, including C-reactive protein, are often increased, as are fibrinogen, interleukin 6, tumor necrosis factor-alpha (TNF-α), and others. Some have pointed to a variety of causes, including increased uric acid levels caused by dietary fructose.
It is generally accepted that the current food environment contributes to the development of metabolic syndrome: our diet is mismatched with our biochemistry. 
Weight gain is associated with metabolic syndrome. Rather than total adiposity, the core clinical component of the syndrome is visceral and/or ectopic fat (i.e., fat in organs not designed for fat storage) whereas the principal metabolic abnormality is insulin resistance. The continuous provision of energy via dietary carbohydrate, lipid, and protein fuels, unmatched by physical activity/energy demand creates a backlog of the products of mitochondrial oxidation, a process associated with progressive mitochondrial dysfunction and insulin resistance.
Stress.
Recent research indicates prolonged chronic stress can contribute to metabolic syndrome by disrupting the hormonal balance of the hypothalamic-pituitary-adrenal axis (HPA-axis).
A dysfunctional HPA-axis causes high cortisol levels to circulate, which results in raising glucose and insulin levels, which in turn cause insulin-mediated effects on adipose tissue, ultimately promoting visceral adiposity, insulin resistance, dyslipidemia and hypertension, with direct effects on the bone, causing "low turnover" osteoporosis. HPA-axis dysfunction may explain the reported risk indication of abdominal obesity to cardiovascular disease (CVD), type 2 diabetes and stroke. Psychosocial stress is also linked to heart disease.
Overweight.
Central obesity is a key feature of the syndrome, being both a symptom and a cause of it in that the increasing adiposity often reflected in high waist circumference both often results from and often contributes to insulin resistance. However, despite the importance of obesity, patients who are of normal weight may also be insulin-resistant and have the syndrome.
Sedentary lifestyle.
Physical inactivity is a predictor of CVD events and related mortality. Many components of metabolic syndrome are associated with a sedentary lifestyle, including increased adipose tissue (predominantly central); reduced HDL cholesterol; and a trend toward increased triglycerides, blood pressure, and glucose in the genetically susceptible. Compared with individuals who watched television or videos or used their computers for less than one hour daily, those who carried out these behaviors for greater than four hours daily have a twofold increased risk of metabolic syndrome.
Aging.
Metabolic syndrome affects 60% of the U.S. population older than age 50. With respect to that demographic, the percentage of women having the syndrome is higher than that of men. The age dependency of the syndrome's prevalence is seen in most populations around the world.
Diabetes mellitus type 2.
The metabolic syndrome quintuples the risk of type 2 diabetes mellitus. Type 2 diabetes is considered a complication of metabolic syndrome. In people with impaired glucose tolerance or impaired fasting glucose, presence of metabolic syndrome doubles the risk of developing type 2 diabetes. It is likely that prediabetes and metabolic syndrome denote the same disorder, defining it by the different sets of biological markers. 
The presence of metabolic syndrome is associated with a higher prevalence of CVD than found in patients with type 2 diabetes or IGT without the syndrome. Hypoadiponectinemia has been shown to increase insulin resistance, and is considered to be a risk factor for developing metabolic syndrome.
Coronary heart disease.
The approximate prevalence of the metabolic syndrome in patients with coronary heart disease (CHD) is 50%, with a prevalence of 37% in patients with premature coronary artery disease (age 45), particularly in women. With appropriate cardiac rehabilitation and changes in lifestyle (e.g., nutrition, physical activity, weight reduction, and, in some cases, drugs), the prevalence of the syndrome can be reduced.
Lipodystrophy.
Lipodystrophic disorders in general are associated with metabolic syndrome. Both genetic (e.g., Berardinelli-Seip congenital lipodystrophy, Dunnigan familial partial lipodystrophy) and acquired (e.g., HIV-related lipodystrophy in patients treated with highly active antiretroviral therapy) forms of lipodystrophy may give rise to severe insulin resistance and many of metabolic syndrome's components.
Psychiatric illnesses.
People with schizophrenia, schizoaffective disorder or bipolar disorder may have a predisposition to metabolic syndrome that is exacerbated by sedentary lifestyle, poor dietary habits, possible limited access to care, and antipsychotic drug-induced adverse effects. It has been found in Australia that 67% of patients with either bipolar disorder or schizoaffective disorder, and 51% of patients with schizophrenia meet criteria for metabolic syndrome; the prevalence is higher in women than in men.
Pathophysiology.
It is common for there to be a development of visceral fat, after which the adipocytes (fat cells) of the visceral fat increase plasma levels of TNF-α and alter levels of a number of other substances (e.g., adiponectin, resistin, and PAI-1). TNF-α has been shown not only to cause the production of inflammatory cytokines, but also possibly to trigger cell signaling by interaction with a TNF-α receptor that may lead to insulin resistance. An experiment with rats fed a diet with 33% sucrose has been proposed as a model for the development of metabolic syndrome. The sucrose first elevated blood levels of triglycerides, which induced visceral fat and ultimately resulted in insulin resistance. The progression from visceral fat to increased TNF-α to insulin resistance has some parallels to human development of metabolic syndrome. The increase in adipose tissue also increases the number of immune cells present within, which play a role in inflammation. Chronic inflammation contributes to an increased risk of hypertension, atherosclerosis and diabetes.
The involvement of the endocannabinoid system in the development of metabolic syndrome is indisputable. Endocannabinoid overproduction may induce reward system dysfunction and cause executive dysfunctions (e.g., impaired delay discounting), in turn perpetuating unhealthy behaviors. The brain is crucial in development of metabolic syndrome, modulating peripheral carbohydrate and lipid metabolism.
The metabolic syndrome can be induced by overfeeding with sugar or fructose, particularly concomitantly with high-fat diet. The resulting oversupply of omega-6 fatty acids, particularly arachidonic acid (AA), is an important factor in the pathogenesis of metabolic syndrome. Arachidonic acid (with its precursor - linoleic acid) serve as a substrate to the production of inflammatory mediators known as eicosanoids, whereas the arachidonic acid-containing compound diacylglycerol (DAG) is a precursor to the endocannabinoid 2-arachidonoylglycerol (2-AG) while fatty acid amide hydrolase (FAAH) mediates the metabolism of arachidonic acid into anandamide. Anandamide can also be produced from "N"-acylphosphatidylethanolamine via several pathways. Anandamide and 2-AG can also be hydrolized into arachidonic acid, potentially leading to increased eicosanoid synthesis.
Metabolic syndrome is a risk factor for neurological disorders. Metabolomic studies suggest an excess of organic acids, impaired lipid oxidation byproducts, essential fatty acids and essential amino acids in the blood serum of affected patients. However, it is not entirely clear whether the accumulation of essential fatty acids and amino acids is the result of excessive ingestion or excess production by gut microbiota.
Diagnosis.
A joint interim statement of the International Diabetes Federation Task Force on Epidemiology and Prevention; National Heart, Lung, and Blood Institute; American Heart Association; World Heart Federation; International Atherosclerosis Society; and International Association for the Study of Obesity published a guideline to harmonize the definition of the metabolic syndrome. This definition recognizes that the risk associated with a particular waist measurement will differ in different populations. Whether it is better at this time to set the level at which risk starts to increase or at which there is already substantially increased risk will be up to local decision-making groups. However, for international comparisons and to facilitate the etiology, it is critical that a commonly agreed-upon set of criteria be used worldwide, with agreed-upon cut points for different ethnic groups and sexes. There are many people in the world of mixed ethnicity, and in those cases, pragmatic decisions will have to be made.
The previous definitions of the metabolic syndrome by the International Diabetes Federation and the revised National Cholesterol Education Program are very similar and they identify individuals with a given set of symptoms as having metabolic syndrome. There are two differences, however: the IDF definition states that if body mass index (BMI) is greater than 30 kg/m2, central obesity can be assumed, and waist circumference does not need to be measured. However, this potentially excludes any subject without increased waist circumference if BMI is less than 30. Conversely, the NCEP definition indicates that metabolic syndrome can be diagnosed based on other criteria. Also, the IDF uses geography-specific cut points for waist circumference, while NCEP uses only one set of cut points for waist circumference regardless of geography. These two definitions are much more similar than the original NCEP and WHO definitions.
IDF.
The International Diabetes Federation consensus worldwide definition of the metabolic syndrome (2006) is:
Central obesity (defined as waist circumference# with ethnicity-specific values) AND any two of the following:
If FPG is >5.6 mmol/L or 100 mg/dL, an oral glucose tolerance test is strongly recommended, but is not necessary to define presence of the syndrome.
WHO.
The World Health Organization 1999 criteria require the presence of any one of diabetes mellitus, impaired glucose tolerance, impaired fasting glucose or insulin resistance, AND two of the following:
EGIR.
The European Group for the Study of Insulin Resistance (1999) requires insulin resistance defined as the top 25% of the fasting insulin values among nondiabetic individuals AND two or more of the following:
NCEP.
The US National Cholesterol Education Program Adult Treatment Panel III (2001) requires at least three of the following:
American Heart Association.
There is confusion as to whether, in 2004, the AHA/NHLBI intended to create another set of guidelines or simply update the NCEP ATP III definition.
Other.
High-sensitivity C-reactive protein has been developed and used as a marker to predict coronary vascular diseases in metabolic syndrome, and it was recently used as a predictor for nonalcoholic fatty liver disease (steatohepatitis) in correlation with serum markers that indicated lipid and glucose metabolism. Fatty liver disease and steatohepatitis can be considered as manifestations of metabolic syndrome, indicative of abnormal energy storage as fat in ectopic distribution.
Reproductive disorders (such as polycystic ovary syndrome in women of reproductive age), and erectile dysfunction or decreased total testosterone (low testosterone-binding globulin) in men can be attributed to metabolic syndrome.
Rheumatic diseases.
There are new findings regarding the comorbidity associated with rheumatic diseases. Both psoriasis and psoriatic arthritis have been found to be associated with metabolic syndrome.
Prevention.
Various strategies have been proposed to prevent the development of metabolic syndrome. These include increased physical activity (such as walking 30 minutes every day), and a healthy, reduced calorie diet. Many studies support the value of a healthy lifestyle as above. However, one study stated these potentially beneficial measures are effective in only a minority of people, primarily due to a lack of compliance with lifestyle and diet changes. The International Obesity Taskforce states that interventions on a sociopolitical level are required to reduce development of the metabolic syndrome in populations.
The Caerphilly Heart Disease Study followed 2,375 male subjects over 20 years and suggested the daily intake of a pint (~568 ml) of milk or equivalent dairy products more than halved the risk of metabolic syndrome. Some subsequent studies support the authors' findings, while others dispute them. A systematic review of four randomized controlled trials found that a paleolithic nutritional pattern improved three of five measurable components of the metabolic syndrome in participants with at least one of the components.
Management.
The first line treatment is change of lifestyle (e.g., Dietary Guidelines for Americans and physical activity). However, if in three to six months of efforts at remedying risk factors prove insufficient, then drug treatment is frequently required. Generally, the individual disorders that compose the metabolic syndrome are treated separately. Diuretics and ACE inhibitors may be used to treat hypertension. Cholesterol drugs may be used to lower LDL cholesterol and triglyceride levels, if they are elevated, and to raise HDL levels if they are low. Use of drugs that decrease insulin resistance, e.g., metformin and thiazolidinediones, is controversial; this treatment is not approved by the U.S. Food and Drug Administration. Weight loss medications may result in weight loss. As obesity is often recognized as the culprit behind many of the additional symptoms, with weight loss and lifestyle changes in diet, physical activity, the need for other medications may diminish.
A 2003 study indicated cardiovascular exercise was therapeutic in approximately 31% of cases. The most probable benefit was to triglyceride levels, with 43% showing improvement; but fasting plasma glucose and insulin resistance of 91% of test subjects did not improve.
Many other studies have supported the value of physical activity and dietary modifications to treat metabolic syndrome. Some natural compounds, like ursolic acid, have been suggested as a treatment for obesity/metabolic syndrome based on the results of extensive research involving animal models; it is argued, however, that there is still a lack of data regarding the use of ursolic acid in humans, as phase-II/III trials of that drug have not been carried so far.
The combination preparation simvastatin/sitagliptin (marketed as Juvisync) was introduced in 2011 and the use of this drug was to lower LDL levels and as well as increase insulin levels. This drug could have been used to treat metabolic syndrome but was removed from the market by Merck in 2013 due to business reasons.
High-dose statins, recommended to reduce cardiovascular risk, have been associated with higher progression to diabetes, particularly in patients with metabolic syndrome. The biological mechanisms are not entirely understood, however, the plausible explanation may lie in competitive inhibition of glucose transport via the solute carrier (SLC) family of transporters (specifically "SLCO1B1"), important in statin pharmacokinetics.
Epidemiology.
Approximately 20 – 25 percent of the world’s adult population has the cluster of risk factors that is metabolic syndrome. In 2000, approximately 32% of U.S. adults had the metabolic syndrome. In more recent years that figure has climbed to 34%.
History.
The term "metabolic syndrome" dates back to at least the late 1950s, but came into common usage in the late 1970s to describe various associations of risk factors with diabetes that had been noted as early as the 1920s.
The terms "metabolic syndrome," "insulin resistance syndrome," and "syndrome X" are now used specifically to define a constellation of abnormalities associated with increased risk for the development of type 2 diabetes and atherosclerotic vascular disease (e.g., heart disease and stroke).
Controversy.
The clinical value of using "metabolic syndrome" as a diagnosis previously has been debated due to different sets of conflicting and incomplete diagnostic criteria. These concerns have led the American Diabetes Association and the European Association for the Study of Diabetes to issue a joint statement identifying eight major concerns on the clinical utility of the metabolic syndrome diagnosis.
The principal argument has been that when confounding factors such as obesity are accounted for, diagnosis of the metabolic syndrome has a negligible association with the risk of heart disease.
Naturally, since the metabolic syndrome is a disorder of energy distribution and storage, fat accumulation explains for a significant proportion of cardiovascular risk. However, obesity without metabolic syndrome does not confer a significant cardiovascular risk, whereas metabolic syndrome without obesity is associated with a significant risk of diabetes and cardiovascular disease. This association of metabolic syndrome with diabetes can be illustrated by generalized lipodystrophy (near complete absence of adipose tissue). The animals and humans with generalized lipodystrophy develop signs of metabolic syndrome in the absence of adipose tissue; and the metabolic syndrome progresses to type 2 diabetes. Adipose tissue transplantation in transgenic mice with lipodystrophy can cure the type 2 diabetes. 
It has not been contested that cardiovascular risk factors tend to cluster together; the matter of contention has been the assertion that the metabolic syndrome is anything more than the sum of its constituent parts. Phenotypic heterogeneity (for example, represented by variation in metabolic syndrome factor combinations among individuals with metabolic syndrome) has fueled that debate. However, more recent evidence suggests that common triggers (for example, excessive sugar-intake in the environment of overabundant food) can contribute to the development of multiple metabolic abnormalities at the same time, supporting the commonality of the energy utilization and storage pathways in metabolic syndrome.
Further reading.
Bhat RA. "Factor Analysis of Metabolic Syndrome Components in North Indian Population of Kashmir." J Med Soc 2015; 29: 83-87.

</doc>
<doc id="54444" url="https://en.wikipedia.org/wiki?curid=54444" title="Falcon">
Falcon

A falcon () is any one of 37 species of raptors in the genus "Falco", widely distributed on all continents of the world except Antarctica.
Adult falcons have thin, tapered wings, which enable them to fly at high speed and to change direction rapidly. Fledgling falcons, in their first year of flying, have longer flight feathers, which make their configuration more like that of a general-purpose bird such as a broadwing. This makes it easier to fly while learning the exceptional skills required to be effective hunters as adults.
The falcons are the largest genus in the Falconinae subfamily of Falconidae, which itself also includes another subfamily comprising caracaras and a few other species. All these birds kill with their beaks, using a "tooth" on the side of their beaks — unlike the hawks, eagles, and other birds of prey in Accipitridae, which use their feet.
The largest falcon is the gyrfalcon at up to 65cm in length. The smallest falcons are the kestrels, of which the Seychelles kestrel measures just 25cm. As with hawks and owls, falcons exhibit reverse sexual dimorphism, with the females typically larger than the males, thus allowing a wider range of prey species. 
Some small falcons with long, narrow wings are called "hobbies", and some which hover while hunting are called "kestrels". 
As is the case with many birds of prey, falcons have exceptional powers of vision; the visual acuity of one species has been measured at 2.6 times that of a normal human. Peregrine falcons have been recorded diving at speeds of 200 miles per hour (320 km/h), making them the fastest-moving creatures on Earth.
Etymology.
The Late Latin "falco" is believed to derive from "falx", a sickle, referencing the claws of the bird. In Middle English and Old French, the term "faucon" refers generically to several captive raptor species.
The traditional term for a male falcon is 'tercel' (British spelling) or 'tiercel' (American spelling), from the Latin "tertius" (third) because of the belief that only one in three eggs hatched a male bird. Some sources give the etymology as deriving from the fact that a male falcon is about one-third smaller than a female (Old French "tiercelet"). A falcon chick, especially one reared for falconry, still in its downy stage is known as an 'eyas' (sometimes spelt 'eyass'). The word arose by mistaken division of Old French "un niais", from Latin presumed "nidiscus" (nestling) from "nidus" (nest). The technique of hunting with trained captive birds of prey is known as falconry.
Systematics and evolution.
Compared to other birds of prey, the fossil record of the falcons is not well distributed in time. The oldest fossils tentatively assigned to this genus are from the Late Miocene, less than 10 million years ago. This coincides with a period in which many modern genera of birds became recognizable in the fossil record. The falcon lineage may, however, be somewhat older than this, and given the distribution of fossil and living "Falco" taxa, is probably of North American, African, or possibly Middle Eastern or European origin.
Overview.
Falcons are roughly divisible into three or four groups. The first contains the kestrels (probably excepting the American kestrel); usually small and stocky falcons of mainly brown upperside color and sometimes sexually dimorphic; three African species that are generally gray in color stand apart from the typical members of this group. Kestrels feed chiefly on terrestrial vertebrates and invertebrates of appropriate size, such as rodents, reptiles, or insects.
The second group contains slightly larger (on average) and more elegant species, the hobbies and relatives. These birds are characterized by considerable amounts of dark slate-grey in their plumage; their malar areas are nearly always black. They feed mainly on smaller birds.
Third are the peregrine falcon and its relatives: variably sized powerful birds that also have a black malar area (except some very light color morphs), and often a black cap, as well. Otherwise, they are somewhat intermediate between the other groups, being chiefly medium gray with some lighter or brownish colors on their upper sides. They are, on average, more delicately patterned than the hobbies and, if the hierofalcons are excluded (see below), this group typically contains species with horizontal barring on their undersides. As opposed to the other groups, where tail color varies much in general but little according to evolutionary relatedness, The tails of the large falcons are quite uniformly dark gray with inconspicuous black banding and small, white tips, though this is probably plesiomorphic. These large "Falco" species feed on mid-sized birds and terrestrial vertebrates.
Very similar to these, and sometimes included therein, are the four or so species of hierofalcons (literally, "hawk-falcons"). They represent taxa with, usually, more phaeomelanins, which impart reddish or brown colors, and generally more strongly patterned plumage reminiscent of hawks. Their undersides have a lengthwise pattern of blotches, lines, or arrowhead marks. 
While these three or four groups, loosely circumscribed, are an informal arrangement, they probably contain several distinct clades in their entirety. 
A study of mtDNA cytochrome "b" sequence data of some kestrels identified a clade containing the common kestrel and related "malar-striped" species, to the exclusion of such taxa as the greater kestrel (which lacks a malar stripe), the lesser kestrel (which is very similar to the common but also has no malar stripe), and the American kestrel, which has a malar stripe, but its color pattern–apart from the brownish back–and also the black feathers behind the ear, which never occur in the true kestrels, are more reminiscent of some hobbies. The malar-striped kestrels apparently split from their relatives in the Gelasian, roughly 2.5-2 mya, and are seemingly of tropical East African origin. The entire "true kestrel" group—excluding the American species—is probably a distinct and quite young clade, as also suggested by their numerous apomorphies.
Other studies have confirmed that the hierofalcons are a monophyletic group–and that hybridization is quite frequent at least in the larger falcon species. Initial studies of mtDNA cytochrome "b" sequence data suggested that the hierofalcons are basal among living falcons. The discovery of a numt proved this earlier theory erroneous; in reality, the hierofalcons are a rather young group, originating maybe at the same time as the start of the main kestrel radiation, about 2 million years ago. Very little fossil history exists for this lineage. However, the present diversity of very recent origin suggests that this lineage may have nearly gone extinct in the recent past.
The phylogeny and delimitations of the peregrine and hobbies groups are more problematic. Molecular studies have only been conducted on a few species, and the morphologically ambiguous taxa have often been little researched. The morphology of the syrinx, which contributes well to resolving the overall phylogeny of the Falconidae, is not very informative in the present genus. Nonetheless, a core group containing the peregrine and Barbary falcons, which, in turn, group with the hierofalcons and the more distant prairie falcon (which was sometimes placed with the hierofalcons, though it is entirely distinct biogeographically), as well as at least most of the "typical" hobbies, are confirmed to be monophyletic as suspected.
Given that the American "Falco" species of today belong to the peregrine group, or are apparently more basal species, the initially most successful evolutionary radiation seemingly was a Holarctic one that originated possibly around central Eurasia or in (northern) Africa. One or several lineages were present in North America by the Early Pliocene at latest.
The origin of today's major "Falco" groups—the "typical" hobbies and kestrels for example, or the peregrine-hierofalcon complex, or the aplomado falcon lineage—can be quite confidently placed from the Miocene-Pliocene boundary through the Zanclean and Piacenzian and just into the Gelasian, that is from about 8 to 2.4 million years ago, when the malar-striped kestrels diversified. Some groups of falcons, such as the hierofalcon complex and the peregrine-Barbary superspecies have only evolved in more recent times; the species of the former seem to be 120,000 years old or so.
Species.
The sequence follows the taxonomic order of White "et al." (1996), except for adjustments in the kestrel sequence.
Fossil record.
Several more paleosubspecies of extant species also been described; see species accounts for these.
""Sushkinia" pliocaena" from the Early Pliocene of Pavlodar (Kazakhstan) appears to be a falcon of some sort. It might belong in this genus or a closely related one. In any case, the genus name "Sushkinia" is invalid for this animal because it had already been allocated to a prehistoric dragonfly relative.
The supposed ""Falco" pisanus" was actually a pigeon of the genus "Columba", possibly the same as "Columba omnisanctorum", which, in that case, would adopt the older species name of the "falcon". The Eocene fossil ""Falco" falconellus" (or ""F." falconella") from Wyoming is a bird of uncertain affiliations, maybe a falconid, maybe not; it certainly does not belong in this genus. ""Falco" readei" is now considered a paleosubspecies of the yellow-headed caracara ("Milvago chimachima").

</doc>
<doc id="54445" url="https://en.wikipedia.org/wiki?curid=54445" title="Bird of prey">
Bird of prey

Birds of prey, also known as raptors, hunt and feed on other animals. The term "raptor" is derived from the Latin word "rapere" (meaning to seize or take by force). These birds are characterized by keen vision that allows them to detect prey during flight and powerful talons and beaks.
Many species of birds may be considered partly or exclusively predatory. However, in ornithology, the term "bird of prey" applies only to birds of the families listed below.
Taken literally, the term "bird of prey" has a wide meaning that includes many birds that hunt and feed on animals and also birds that eat very small insects. In ornithology, the definition for "bird of prey" has a narrower meaning: birds that have very good eyesight for finding food, strong feet for holding food, and a strong curved beak for tearing flesh. Most birds of prey also have strong curved talons for catching or killing prey. An example of this difference in definition, the narrower definition excludes storks and gulls, which can eat quite large fish, partly because these birds catch and kill prey entirely with their beaks, and similarly bird-eating skuas, fish-eating penguins, and vertebrate-eating kookaburras are excluded. Birds of prey generally prey on vertebrates, which are usually quite large relative to the size of the bird. Most also eat carrion, at least occasionally, and vultures and condors eat carrion as their main food source. Many raptor species are considered apex predators.
Classification.
The order Accipitriformes is inferred to have originated about 44 million years ago when it split from the common ancestor of the secretarybird ("Sagittarius serpentarius") and the rest of the accipitrid species. The phylogeny of Accipitriformes is complex and difficult to unravel. Widespread paraphylies were observed in many phylogenetic studies. Unfortunately more recent and detailed studies show similar results. However, according to the findings of a 2014 study, the sister relationship between larger clades of "Accipitriformes" was well supported (e.g. relationship of "Harpagus" kites to buzzards and sea eagles and these latter two with "Accipiter" hawks are sister taxa of the clade containing "Aquilinae" and "Harpiinae").
The diurnal birds of prey are formally classified into five families of two orders.
These families were traditionally grouped together in a single order Falconiformes, however are now split into two orders: Falconiformes and Accipitriformes. The Cathartidae are sometimes placed separately in an enlarged stork family (Ciconiiformes), and may be raised to an order of their own (Cathartiiformes).
The secretary bird and/or osprey are sometimes listed as subfamilies of Acciptridae: Sagittariinae and Pandioninae, respectively.
Australia's letter-winged kite is a member of the family Accipitridae, although it is a wholly nocturnal bird.
The nocturnal birds of prey – the owls – are classified separately as members of two extant families of the order Strigiformes:
Historical classifications.
The taxonomy of Carl Linnaeus grouped birds (class Aves) into orders, genera and species, with no formal ranks between genus and order. He placed all birds of prey into a single order, "Accipitres", subdividing this into four genera: "Vultur" (vultures), "Falco" (eagles, hawks, falcons, etc.), "Strix" (owls), and "Lanius" (shrikes). This approach was followed by subsequent authors such as Gmelin, Latham, and Turnton.
Louis Pierre Veillot used additional ranks: order, tribe, family, genus, species. Birds of prey (order Accipitres) were divided into diurnal and nocturnal tribes; the owls remained monogeneric (family Ægolii, genus "Strix"), whilst the diurnal raptors were divided into three families: Vulturini, Gypaëti, and Accipitrini.
Thus Veillot's families were similar to the Linnaean genera, with the difference that shrikes were no longer included amongst the birds of prey. In addition to the original "Vultur" and "Falco" (now reduced in scope), Veillot adopted four genera from Savigny: "Phene", "Haliæetus", "Pandion", and "Elanus". He also introduced five new genera of vultures ("Gypagus", "Catharista", "Daptrius", "Ibycter", "Polyborus") and eleven new genera of accipitrines ("Aquila", "Circaëtus", "Circus", "Buteo", "Milvus", "Ictinia", "Physeta", "Harpia", "Spizaëtus", "Asturina", "Sparvius").
Common names.
The common names for various birds of prey are based on structure, but many of the traditional names do not reflect the evolutionary relationships between the groups.
Many of these English-language group names originally referred to particular species encountered in Britain. As English-speaking people travelled further, the familiar names were applied to new birds with similar characteristics. Names that have generalized this way include: kite ("Milvus milvus"), sparrow-hawk or sparhawk ("Accipiter nisus"), goshawk ("Accipiter gentilis"), kestrel ("Falco tinninculus"), hobby ("Falco subbuteo"), harrier (simplified from "hen-harrier", "Circus cyaneus"), buzzard ("Buteo buteo").
Some names have not generalized, and refer to single species (or groups of closely related (sub)species): merlin ("Falco columbarius"), osprey ("Pandion haliaetus").
Migration.
Migratory behaviour evolved multiple times within accipitrid raptors.
The earliest event occurred nearly 14–12 million years ago. This result seems to be one of the oldest dates published so far in the case of birds of prey. For example, a previous reconstruction of migratory behaviour in one "Buteo" clade with a result of the origin of migration around 5 million years ago was also supported by that study.
Migratory species of raptors had a southern origin because it seems that all of the major lineages within "Accipitridae" had an origin to one of the biogeographic realms of the Southern Hemisphere. The appearance of migratory behaviour occurred in the tropics parallel with the range expansion of migratory species to temperate habitats. Similar results of southern origin in other taxonomic groups can be found in the literature.
Distribution and biogeographic history highly determine the origin of migration in birds of prey. Based on some comparative analyses, diet breadth also has an effect on the evolution of migratory behaviour in this group, but its relevance needs further investigations. The evolution of migration in animals seems to be a complex and difficult field with many unanswered questions.
Sexual dimorphism.
Raptors are known to display patterns of sexual dimorphism. It is commonly believed that the dimorphisms found in raptors occur due to sexual selection or environmental factors. In general, hypotheses in favor of ecological factors being the cause for sexual dimorphism in raptors are rejected. This is due to the fact that the ecological model is less parsimonious, meaning that its explanation is more complex than that of the sexual selection model. Additionally, ecological models are much harder to test for due to the fact that a great deal of data is required.
Dimorphisms can also be the product of intrasexual selection between males and females. It appears that both genders of the species play a role in the sexual dimorphism within raptors; females tend to compete with other females to find good places to nest and attract males, and males competing with other males for adequate hunting ground so they appear as the most healthy mate.
It has also been proposed that sexual dimorphism is merely the product of disruptive selection, and is merely a stepping stone in the process of speciation, especially if the traits that define gender are independent across a species. Sexual dimorphism can be viewed as something that can accelerate the rate of speciation.
In non-predatory birds, males are typically larger than females. However, in birds of prey, the opposite is the case. For instance, take into account the kestrel, a type of falcon in which males are the primary providers, and the females are responsible for nurturing the young. In this species, the smaller kestrels are, the less food is needed and thus, they can survive in environments that are harsher. This is particularly true in the male kestrels. It has become more energetically favorable for male kestrels to remain smaller than their female counterparts because of the fact that smaller males have an agility advantage when it comes to defending the nest and hunting. Larger females are favored because they can incubate larger numbers of offspring, while also being able to breed a larger clutch size.

</doc>
<doc id="54448" url="https://en.wikipedia.org/wiki?curid=54448" title="Insulin resistance">
Insulin resistance

Insulin resistance (IR) is generally regarded as a pathological condition in which cells fail to respond to the normal actions of the hormone insulin. The body produces insulin. When the body produces insulin under conditions of insulin resistance, the cells in the body are resistant to the insulin and are unable to use it as effectively, leading to high blood sugar. Beta cells in the pancreas subsequently increase their production of insulin, further contributing to a high blood insulin level. This often remains undetected and can contribute to a diagnosis of Type 2 diabetes or latent autoimmune diabetes of adults. Despite the ill-effects of severe insulin resistance, recent investigations have revealed that insulin resistance is primarily a well-evolved mechanism to conserve the brain's glucose consumption by preventing muscles from taking up excessive glucose. Insulin resistance should even be strengthened under harsh metabolic conditions such as pregnancy, during which the expanding fetal brain demands more glucose.
Signs and symptoms.
These depend on poorly understood variations in individual biology and consequently may not be found with all people diagnosed with insulin resistance.
Associated risk factors.
Several associated risk factors include the following:
Cause.
Molecular mechanism.
Insulin resistance implies that our body (primarily muscles) loses sensitivity to insulin, a hormone secreted by the pancreas to promote glucose utilization. At the molecular level, a cell senses insulin through insulin receptors, with the signal propagating through a cascade of molecules collectively known as PI3K/Akt/mTOR signaling pathway. Recent studies suggested that the pathway may operate as a bistable switch under physiologic conditions for certain types of cells, and insulin response may well be a threshold phenomenon. The pathway's sensitivity to insulin may be blunted by many factors such as free fatty acids, causing insulin resistance. From a broader perspective, however, sensitivity tuning (including sensitivity reduction) is a common practice for an organism to adapt to the changing environment or metabolic conditions. Pregnancy, for example, is a prominent change of metabolic conditions, under which the mother has to reduce her muscles' insulin sensitivity to spare more glucose for the brains (the mother's brain and the fetal brain). This can be achieved through raising the response threshold (i.e., postponing the onset of sensitivity) by secreting placental growth factor to interfere with the interaction between insulin receptor substrate (IRS) and PI3K, which is the essence of the so-called "adjustable threshold hypothesis" of insulin resistance.
Diet.
It is well known that insulin resistance commonly coexists with obesity. Dietary fat has long been implicated as a driver of insulin resistance. Studies on animals have observed significant insulin resistance in rats after just 3 weeks on a high-fat diet (59% fat, 20% carb.) Large quantities of saturated, monounsaturated, and polyunsaturated (omega-6) fats all appear to be harmful to rats to some degree, compared to large amounts of starch, but saturated fat appears to be the most effective at producing IR. This is partly caused by direct effects of a high-fat diet on blood markers, but, more significantly, "ad libitum" high-fat diet has the tendency to result in caloric intake that's far in excess of animals' energy needs, resulting in rapid weight gain. In humans, statistical evidence is more equivocal. Being insensitive to insulin is still positively correlated with fat intake, and negatively correlated with dietary fiber intake, but both these factors are also correlated with excess body weight.
The effect of dietary fat is largely or completely overridden if the high-fat diet is modified to contain nontrivial quantities (in excess of 5–10% of total fat intake) of polyunsaturated omega-3 fatty acids. This protective effect is most established with regard to the so-called "marine long-chain omega-3 fatty acids", EPA and DHA, found in algal, krill and fish oil; evidence in favor of other omega-3 fatty acids, in particular, the most common vegetable-based omega-3 fatty acid, ALA, also exists, but it is more limited; some studies find ALA effective only among people with insufficient long-chain omega-3 intake, and some studies fail to find any effect at all (ALA may be converted partially into EPA and DHA by the human body, but the conversion rate is thought to be 10% or less, depending on diet and gender). The effect is thought to explain relatively low incidence of IR, type 2 diabetes, and obesity in polar foragers such as Alaskan Eskimos consuming their ancestral diet (which is very high in fat, but contains substantial amounts of omega-3), however, it is not strong enough to prevent IR in the typical modern Western diet. Unlike their omega-6 counterparts (which may be produced cheaply from a variety of sources, such as corn and soybeans), major sources of omega-3 fatty acids remain relatively rare and expensive. Consequently, the recommended average intake of omega-3 for adult men in the United States is only 1.6 grams/day, or less than 2% of total fat; currently, the average consumption of omega-3 in the United States is approximately 1.3 grams/day, almost all of it in the form of ALA; EPA and DHA contributed less than 0.1 grams/day.
Elevated levels of free fatty acids and triglycerides in the blood stream and tissues have been found in many studies to contribute to diminished insulin sensitivity. Triglyceride levels are driven by a variety of dietary factors. They are correlated with excess body weight. They tend to rise due to overeating and fall during fat loss. At constant energy intake, triglyceride levels are correlated positively with trans fat intake and strongly inversely-correlated with omega-3 intake. High-carbohydrate, low-fat diets were found by many studies to result in elevated triglycerides, in part due to higher production of VLDL from fructose and sucrose, and in part because increased carbohydrate intake tends to displace some omega-3 fatty acids from the diet.
Several recent authors suggested that the intake of simple sugars, and particularly fructose, is also a factor that contributes to insulin resistance. Fructose is metabolized by the liver into triglycerides, and, as mentioned above, tends to raise their levels in the blood stream. Therefore, it may contribute to insulin resistance through the same mechanisms as the dietary fat. Just like fat, high levels of fructose and/or sucrose induce insulin resistance in rats, and, just as with fat, this insulin resistance is ameliorated by fish oil supplementation. One study observed that a low-fat diet high in simple sugars (but not in complex carbohydrates and starches) significantly stimulates fatty acid synthesis, primarily of the saturated fatty acid palmitate, therefore, paradoxically, resulting in the plasma fatty acid pattern that is similar to that produced by a high-saturated-fat diet. It should be pointed out that virtually all evidence of deleterious effects of simple sugars so far is limited to their concentrated formulations and sweetened beverages. In particular, very little is known about effects of simple sugars in whole fruit and vegetables. If anything, epidemiological studies suggest that their high consumption is associated with somewhat lower risk of IR and/or metabolic syndrome.
Yet another proposed mechanism involves the phenomenon known as leptin resistance. Leptin is a hormone that regulates long-term energy balance in many mammals. An important role of leptin is long-term inhibition of appetite in response to formation of body fat. This mechanism is known to be disrupted in many obese individuals: even though their leptin levels are commonly elevated, this does not result in reduction of appetite and caloric intake. Leptin resistance may be triggered in rats by ad libitum consumption of energy-dense, highly-palatable foods over a period of several days. Chronic consumption of fructose in rats ultimately results in leptin resistance (however, this has only been demonstrated in a diet where fructose provided 60% of calories; consumption by humans in a typical Western diet is several times lower.) Once leptin signalling has been disrupted, the individual becomes prone to further overeating, weight gain, and insulin resistance.
As elevated blood glucose levels are the primary stimulus for insulin secretion and production, habitually excessive carbohydrate intake is another likely contributor. This serves as a major motivation behind the low-carb family of diets. Furthermore, carbohydrates are not equally absorbed (for example, the blood glucose level response to a fixed quantity of carbohydrates in baked potatoes is about twice the response to the same quantity of carbohydrates in pumpernickel bread). Integrated blood glucose response to a fixed quantity of carbohydrates in a meal is known as glycemic index (GI). Some diets are based on this concept, assuming that consumption of low-GI food is less likely to result in insulin resistance and obesity, however, small to moderate amounts of simple sugars (i.e., sucrose, fructose, and glucose) in the typical developed-world diet seem to not have a causative effect on the development of insulin resistance.
Once established, insulin resistance would result in increased circulating levels of insulin. Since insulin is the primary hormonal signal for energy storage into fat cells, which tend to retain their sensitivity in the face of hepatic and skeletal muscle resistance, IR stimulates the formation of new fatty tissue and accelerates weight gain.
Another possible explanation is that both insulin resistance and obesity often have the same cause, systematic overeating, which has the potential to lead to insulin resistance and obesity due to repeated administration of excess levels of glucose, which stimulate insulin secretion; excess levels of fructose, which raise triglyceride levels in the bloodstream; and fats, which may be absorbed easily by the adipose cells, and tend to end up as fatty tissue in a hypercaloric diet. Some scholars go as far as to claim that neither insulin resistance, nor obesity really are metabolic disorders "per se", but simply adaptive responses to sustained caloric surplus, intended to protect bodily organs from lipotoxicity (unsafe levels of lipids in the bloodstream and tissues): "Obesity should therefore not be regarded as a pathology or disease, but rather as the normal, physiologic response to sustained caloric surplus… As a consequence of the high level of lipid accumulation in insulin target tissues including skeletal muscle and liver, it has been suggested that exclusion of glucose from lipid-laden cells is a compensatory defense against further accumulation of lipogenic substrate."
Fast food meals typically possess several characteristics, all of which have independently been linked to IR: they are energy-dense, palatable, and cheap, increasing risk of overeating and leptin resistance; simultaneously, they are high in dietary fat and fructose, and low in omega-3; and they usually have high glycemic indices. Consumption of fast food has been proposed as a fundamental factor behind the metabolic syndrome epidemic and all its constituents.
An American study has shown that glucosamine (often prescribed for joint problems) may cause insulin resistance. Other studies, however, could not confirm a significant effect on blood glucose or insulin resistance.
Studies show that high levels of cortisol within the bloodstream from the digestion of animal protein may contribute to the development of insulin resistance. Additionally, animal protein, because of its high content of purine, causes blood pH to become acidic. Several studies conclude that high uric acid levels, apart from other contributing factors, by itself may be a significant cause of insulin resistance.
Vitamin D deficiency also is associated with insulin resistance.
Sedentary lifestyle.
Sedentary lifestyle increases the likelihood of development of insulin resistance. It has been estimated that each 500 kcal/week increment in physical activity related energy expenditure, reduces the lifetime risk of type 2 diabetes by 9%. A different study found that vigorous exercise at least once a week reduced the risk of type 2 diabetes in women by 33%.
Protease inhibitors.
Protease inhibitors found in HIV drugs are linked to insulin resistance.
Cellular level.
At the cellular level, much of the variance in insulin sensitivity between untrained, non-diabetic humans may be explained by two mechanisms: differences in phospholipid profiles of skeletal muscle cell membranes, and in intramyocellular lipid (ICML) stores within these cells. High levels of lipids in the bloodstream have the potential to result in accumulation of triglycerides and their derivatives within muscle cells, which activate proteins Kinase C-ε and C-θ, ultimately reducing the glucose uptake at any given level of insulin. This mechanism is quite fast-acting and may induce insulin resistance within days or even hours in response to a large lipid influx. Draining the intracellular reserves, on the other hand, is more challenging: moderate caloric restriction alone, even over a period of several months, appears to be ineffective, and it must be combined with physical exercise to have any effect.
In the long term, diet has the potential to change the ratio of polyunsaturated to saturated phospholipids in cell membranes, correspondingly changing cell membrane fluidity; full impact of such changes is not fully understood, but it is known that the percentage of polyunsaturated phospholipids is strongly inversely-correlated with insulin resistance. It is hypothesized that increasing cell membrane fluidity by increasing PUFA concentration might result in an enhanced number of insulin receptors, an increased affinity of insulin to its receptors, and a reduced insulin resistance, and vice versa.
Many stressing factors may lead to increased cortisol in the bloodstream. Cortisol counteracts insulin, contributes to hyperglycemia-causing hepatic gluconeogenesis, and inhibits the peripheral utilization of glucose, which eventually leads to insulin resistance. It does this by decreasing the translocation of glucose transporters (especially GLUT4) to the cell membrane.
Although inflammation often is caused by cortisol, inflammation by itself also seems to be implicated in causing insulin resistance. Mice without JNK1-signaling do not develop insulin resistance under dietary conditions that normally produce it. Recent study have found out the specific role of the MLK family of protein in the activation of JNK during obesity and insulin resistance. 
Rare type 2 diabetes cases sometimes use high levels of exogenous insulin. As short-term overdosing of insulin causes short-term insulin resistance, it has been hypothesized that chronic high dosing contributes to more permanent insulin resistance.
Molecular.
At a molecular level, insulin resistance has been proposed to be a reaction to excess nutrition by superoxide dismutase in cell mitochondria that acts as an antioxidant defense mechanism. This link seems to exist under diverse causes of insulin resistance. It also is based on the finding that insulin resistance may be reversed rapidly by exposing cells to mitochondrial uncouplers, electron transport chain inhibitors, or mitochondrial superoxide dismutase mimetics.
Disease.
Recent research and experimentation has uncovered a non-obesity related connection to insulin resistance and type 2 diabetes. It has long been observed that patients who have had some kinds of bariatric surgery have increased insulin sensitivity and even remission of type 2 diabetes. It was discovered that diabetic/insulin resistant non-obese rats whose duodenum has been removed surgically, also experienced increased insulin sensitivity and remission of type 2 diabetes. This suggested similar surgery in humans, and early reports in prominent medical journals are that the same effect is seen in humans, at least the small number who have participated in the experimental surgical program. The speculation is, that some substance is produced in that initial portion of the small intestine that signals body cells to become insulin resistant. If the producing tissue is removed, the signal ceases and body cells revert to normal insulin sensitivity. No such substance has been found as yet, so the existence of such a substance remains speculative.
Insulin resistance is associated with PCOS.
HCV and insulin resistance.
Hepatitis C also makes people three to four times more likely to develop type 2 diabetes and insulin resistance. In addition, "people with Hepatitis C who develop diabetes probably have susceptible insulin-producing cells, and probably would get it anyway, but much later in life. The extra insulin resistance caused by Hepatitis C apparently brings on diabetes at age 35 or 40, instead of 65 or 70."
Pathophysiology.
One of insulin's functions is to regulate delivery of glucose into cells to provide them with energy. Insulin resistant cells cannot take in glucose, amino acids and fatty acids. Thus, glucose, fatty acids and amino acids 'leak' out of the cells. A decrease in insulin/glucagon ratio inhibits glycolysis which in turn decreases energy production. The resulting increase in blood glucose may raise levels outside the normal range and cause adverse health effects, depending on dietary conditions. Certain cell types such as fat and muscle cells require insulin to absorb glucose. When these cells fail to respond adequately to circulating insulin, blood glucose levels rise. The liver helps regulate glucose levels by reducing its secretion of glucose in the presence of insulin. This normal reduction in the liver’s glucose production may not occur in people with insulin resistance.
Insulin resistance in muscle and fat cells reduces glucose uptake (and also local storage of glucose as glycogen and triglycerides, respectively), whereas insulin resistance in liver cells results in reduced glycogen synthesis and storage and also a failure to suppress glucose production and release into the blood. Insulin resistance normally refers to reduced glucose-lowering effects of insulin. However, other functions of insulin can also be affected. For example, insulin resistance in fat cells reduces the normal effects of insulin on lipids and results in reduced uptake of circulating lipids and increased hydrolysis of stored triglycerides. Increased mobilization of stored lipids in these cells elevates free fatty acids in the blood plasma. Elevated blood fatty-acid concentrations (associated with insulin resistance and diabetes mellitus Type 2), reduced muscle glucose uptake, and increased liver glucose production all contribute to elevated blood glucose levels. High plasma levels of insulin and glucose due to insulin resistance are a major component of the metabolic syndrome. If insulin resistance exists, more insulin needs to be secreted by the pancreas. If this compensatory increase does not occur, blood glucose concentrations increase and type 2 diabetes or latent autoimmune diabetes of adults occurs.
Any food or drink containing glucose (or the digestible carbohydrates that contain it, such as sucrose, starch, etc.) causes blood glucose levels to increase. In normal metabolism, the elevated blood glucose level instructs beta (β) cells in the Islets of Langerhans, located in the pancreas, to release insulin into the blood. The insulin, in turn, makes insulin-sensitive tissues in the body (primarily skeletal muscle cells, adipose tissue, and liver) absorb glucose, and thereby lower the blood glucose level. The beta cells reduce insulin output as the blood glucose level falls, allowing blood glucose to settle at a constant of approximately 5 mmol/L (mM) (90 mg/dL). In an "insulin-resistant" person, normal levels of insulin do not have the same effect in controlling blood glucose levels. During the compensated phase on insulin resistance, insulin levels are higher, and blood glucose levels are still maintained. If compensatory insulin secretion fails, then either fasting (impaired fasting glucose) or postprandial (impaired glucose tolerance) glucose concentrations increase. Eventually, type 2 diabetes or latent autoimmune diabetes occurs when glucose levels become higher throughout the day as the resistance increases and compensatory insulin secretion fails. The elevated insulin levels also have additional effects (see insulin) that cause further abnormal biological effects throughout the body.
The most common type of insulin resistance is associated with overweight and obesity in a condition known as the metabolic syndrome. Insulin resistance often progresses to full Type 2 diabetes mellitus (T2DM) or latent autoimmune diabetes of adults. This often is seen when hyperglycemia develops after a meal, when pancreatic β-cells are unable to produce sufficient insulin to maintain normal blood sugar levels (euglycemia) in the face of insulin resistance. The inability of the β-cells to produce sufficient insulin in a condition of hyperglycemia is what characterizes the transition from insulin resistance to T2DM.
Various disease states make body tissues more resistant to the actions of insulin. Examples include infection (mediated by the cytokine TNFα) and acidosis. Recent research is investigating the roles of adipokines (the cytokines produced by adipose tissue) in insulin resistance. Certain drugs also may be associated with insulin resistance (e.g., glucocorticoids).
The presence of insulin leads to a kind of insulin resistance; every time a cell is exposed to insulin, the production of GLUT4 (type four glucose receptors) on the membrane of the cell decreases somewhat. In the presence of a higher than usual level of insulin (generally caused by insulin resistance), this down-regulation acts as a kind of positive feedback, increasing the need for insulin. Exercise reverses this process in muscle tissue, but if it is left unchecked, it may contribute to insulin resistance.
Elevated blood levels of glucose — regardless of cause — lead to increased glycation of proteins with changes, only a few of which are understood in any detail, in protein function throughout the body.
Insulin resistance often is found in people with visceral adiposity (i.e., a high degree of fatty tissue within the abdomen — as distinct from subcutaneous adiposity or fat between the skin and the muscle wall, especially elsewhere on the body, such as hips or thighs), hypertension, hyperglycemia, and dyslipidemia involving elevated triglycerides, small dense low-density lipoprotein (sdLDL) particles, and decreased HDL cholesterol levels. With respect to visceral adiposity, a great deal of evidence suggests two strong links with insulin resistance. First, unlike subcutaneous adipose tissue, visceral adipose cells produce significant amounts of proinflammatory cytokines such as tumor necrosis factor-alpha (TNF-a), and Interleukins-1 and -6, etc. In numerous experimental models, these proinflammatory cytokines disrupt normal insulin action in fat and muscle cells, and may be a major factor in causing the whole-body insulin resistance observed in patients with visceral adiposity. Much of the attention on production of proinflammatory cytokines has focused on the IKK-beta/NF-kappa-B pathway, a protein network that enhances transcription of inflammatory markers and mediators that may cause insulin resistance. Second, visceral adiposity is related to an accumulation of fat in the liver, a condition known as non-alcoholic fatty liver disease (NAFLD). The result of NAFLD is an excessive release of free fatty acids into the bloodstream (due to increased lipolysis), and an increase in hepatic glycogenolysis and hepatic glucose production, both of which have the effect of exacerbating peripheral insulin resistance and increasing the likelihood of Type 2 diabetes mellitus.
Also, insulin resistance often is associated with a hypercoagulable state (impaired fibrinolysis) and increased inflammatory cytokine levels.
Diagnosis.
Fasting insulin levels.
A fasting serum insulin level greater than 25mIU/L or 174pmol/L is considered insulin resistance. The same levels apply three hours after the last meal.
Glucose tolerance testing (GTT).
During a glucose tolerance test, which may be used to diagnose diabetes mellitus, a fasting patient takes a 75 gram oral dose of glucose. Then blood glucose levels are measured over the following two hours.
Interpretation is based on WHO guidelines. After two hours a glycemia less than 7.8 mmol/L (140 mg/dl) is considered normal, a glycemia of between 7.8 to 11.0 mmol/L (140 to 197 mg/dl) is considered as impaired glucose tolerance (IGT), and a glycemia of greater than or equal to 11.1 mmol/L (200 mg/dl) is considered diabetes mellitus.
An oral glucose tolerance test (OGTT) may be normal or mildly abnormal in simple insulin resistance. Often, there are raised glucose levels in the early measurements, reflecting the loss of a postprandial peak (after the meal) in insulin production. Extension of the testing (for several more hours) may reveal a hypoglycemic "dip," that is a result of an overshoot in insulin production after the failure of the physiologic postprandial insulin response.
Measuring insulin resistance.
Hyperinsulinemic euglycemic clamp.
The gold standard for investigating and quantifying insulin resistance is the "hyperinsulinemic euglycemic clamp," so-called because it measures the amount of glucose necessary to compensate for an increased insulin level without causing hypoglycemia. It is a type of glucose clamp technique. The test rarely is performed in clinical care, but is used in medical research, for example, to assess the effects of different medications. The rate of glucose infusion commonly is referred to in diabetes literature as the GINF value.
The procedure takes about two hours. Through a peripheral vein, insulin is infused at 10–120 mU per m2 per minute. In order to compensate for the insulin infusion, glucose 20% is infused to maintain blood sugar levels between 5 and 5.5 mmol/l. The rate of glucose infusion is determined by checking the blood sugar levels every five to ten minutes.
The rate of glucose infusion during the last thirty minutes of the test determines insulin sensitivity. If high levels (7.5 mg/min or higher) are required, the patient is insulin-sensitive. Very low levels (4.0 mg/min or lower) indicate that the body is resistant to insulin action. Levels between 4.0 and 7.5 mg/min are not definitive, and suggest "impaired glucose tolerance," an early sign of insulin resistance.
This basic technique may be enhanced significantly by the use of glucose tracers. Glucose may be labeled with either stable or radioactive atoms. Commonly-used tracers are 3-3H glucose (radioactive), 6,6 2H-glucose (stable) and 1-13C Glucose (stable). Prior to beginning the hyperinsulinemic period, a 3h tracer infusion enables one to determine the basal rate of glucose production. During the clamp, the plasma tracer concentrations enable the calculation of whole-body insulin-stimulated glucose metabolism, as well as the production of glucose by the body (i.e., endogenous glucose production).
Modified insulin suppression test.
Another measure of insulin resistance is the modified insulin suppression test developed by Gerald Reaven at Stanford University. The test correlates well with the euglycemic clamp, with less operator-dependent error. This test has been used to advance the large body of research relating to the metabolic syndrome.
Patients initially receive 25 mcg of octreotide (Sandostatin) in 5 ml of normal saline over 3 to 5 minutes via intravenous infusion (IV) as an initial bolus, and then, are infused continuously with an intravenous infusion of somatostatin (0.27 μgm/m2/min) to suppress endogenous insulin and glucose secretion. Next, insulin and 20% glucose is infused at rates of 32 and 267 mg/m2/min, respectively. Blood glucose is checked at zero, 30, 60, 90, and 120 minutes, and thereafter, every 10 minutes for the last half-hour of the test. These last four values are averaged to determine the steady-state plasma glucose level (SSPG). Subjects with an SSPG greater than 150 mg/dl are considered to be insulin-resistant.
Alternatives.
Given the complicated nature of the "clamp" technique (and the potential dangers of hypoglycemia in some patients), alternatives have been sought to simplify the measurement of insulin resistance. The first was the Homeostatic Model Assessment (HOMA), and a more recent method is the Quantitative insulin sensitivity check index (QUICKI). Both employ fasting insulin and glucose levels to calculate insulin resistance, and both correlate reasonably with the results of clamping studies. Wallace "et al." point out that QUICKI is the logarithm of the value from one of the HOMA equations.
Management.
The primary treatment for insulin resistance is exercise and weight loss. Research shows that a low-carbohydrate diet may help. Both metformin, and thiazolidinediones improve insulin resistance, but only are approved therapies for type 2 diabetes, not for insulin resistance. By contrast, growth hormone replacement therapy may be associated with increased insulin resistance.
Metformin has become one of the more commonly prescribed medications for insulin resistance.
Insulin resistance is often associated with abnormalities in lipids particularly high blood triglycerides and low high density lipoprotien.
The "Diabetes Prevention Program" (DPP) showed that exercise and diet were nearly twice as effective as metformin at reducing the risk of progressing to type 2 diabetes. However, the participants in the DPP trial regained about 40% of the weight that they had lost at the end of 2.8 years, resulting in a similar incidence of diabetes development in both the lifestyle intervention and the control arms of the trial. One 2009 study found that carbohydrate deficit after exercise, but not energy deficit, contributed to insulin sensitivity increase.
Resistant starch from high-amylose corn, amylomaize, has been shown to reduce insulin resistance in healthy individuals, in individuals with insulin resistance, and in individuals with type 2 diabetes. Animal studies demonstrate that it cannot reverse damage already done by high glucose levels, but that it reduces insulin resistance and reduces the development of further damage.
Some types of monounsaturated fatty acids, saturated, and trans fats promote insulin resistance. Some types of polyunsaturated fatty acids (omega-3) may moderate the progression of insulin resistance into type 2 diabetes, however, omega-3 fatty acids appear to have limited ability to reverse insulin resistance, and they cease to be efficacious once type 2 diabetes is established.
Caffeine intake limits insulin action, but not enough to increase blood-sugar levels in healthy persons. People who already have type 2 diabetes may see a small increase in levels if they take 2 or 2-1/2 cups of coffee per day.
History.
The concept that insulin resistance may be the underlying cause of diabetes mellitus type 2 was first advanced by Professor Wilhelm Falta and published in Vienna in 1931, and confirmed as contributory by Sir Harold Percival Himsworth of the University College Hospital Medical Centre in London in 1936, however, type 2 diabetes does not occur unless there is concurrent failure of compensatory insulin secretion.

</doc>
