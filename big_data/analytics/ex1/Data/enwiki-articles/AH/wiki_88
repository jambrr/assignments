<doc id="55014" url="https://en.wikipedia.org/wiki?curid=55014" title="Leslie Nielsen">
Leslie Nielsen

Leslie William Nielsen, OC (11 February 192628 November 2010) was a Canadian actor, comedian, and producer. He appeared in more than 100 films and 150 television programs, portraying more than 220 characters.
Nielsen was born in Regina, Saskatchewan. He enlisted in the Royal Canadian Air Force and later worked as a disc jockey before receiving a scholarship to study theatre at the Neighborhood Playhouse. Making his acting debut in 1948, he made more than 50 television appearances two years later. Nielsen made his film debut in 1956, with supporting roles in several drama, western, and romance films produced between the 1950s and the 1970s, with Nielsen crossing genres in both television and films.
Although his notable performances in the films "Forbidden Planet" and "The Poseidon Adventure" gave him standing as a serious actor, Nielsen later gained enduring recognition for his deadpan comedy roles during the 1980s and the early 1990s, after being cast against type for the Zucker, Abrahams, and Zucker comedy film "Airplane!". Nielsen specialized in his portrayal of characters oblivious to and complicit in their absurd surroundings, which gave him a reputation as a comedian. "Airplane!" marked Nielsen's turning point, which made him "the Olivier of spoofs" according to film critic Roger Ebert; his work on the film also led to further success in the genre with "The Naked Gun" film series, which are based on their earlier short-lived television series "Police Squad!", in which he also starred. Nielsen received a variety of awards and was inducted into the Canada and Hollywood Walks of Fame.
Early life.
Nielsen was born on 11 February 1926 in Regina, Saskatchewan. His mother, Mabel Elizabeth (née Davies), was a Welsh immigrant, and his father, Ingvard Eversen Nielsen, was a Danish-born constable in the Royal Canadian Mounted Police. Nielsen had two brothers; the elder, Erik Nielsen (1924–2008), was deputy prime minister of Canada from 1984 to 1986.
His half-uncle, Jean Hersholt, was an actor known for his portrayal of Dr. Christian in a radio series of that name and the subsequent television series and films. In a 1994 "Boston Globe" article, Nielsen explained, "I did learn very early that when I would mention my uncle, people would look at me as if I were the biggest liar in the world. Then I would take them home and show them 8-by-10 glossies, and things changed quite drastically. So I began to think that maybe this acting business was not a bad idea, much as I was very shy about it and certainly without courage regarding it. My uncle died not too long after I was in a position to know him. I regret that I had not a chance to know him better."
Nielsen lived for several years in Fort Norman (now Tulita), Northwest Territories where his father was with the Royal Canadian Mounted Police. His father was a troubled man who beat his wife and sons, and Leslie longed to escape. When he graduated from high school at 17, he joined the Royal Canadian Air Force even though he was legally deaf (he wore hearing aids most of his life). Following graduation from Victoria School of Performing and Visual Arts in Edmonton, Nielsen enlisted in the Royal Canadian Air Force and trained as an aerial gunner during World War II. He was too young to be fully trained or sent overseas. He worked briefly as a disc jockey at a Calgary, Alberta, radio station, before enrolling at the Lorne Greene Academy of Radio Arts, Toronto. While studying in Toronto, Nielsen received a scholarship to the Neighborhood Playhouse. He noted, "I couldn't refuse, but I must say when you come from the land of the snow goose, the moose and wool to New York, you're bringing every ton of hayseed and country bumpkin that you packed. As long as I didn't open my mouth, I felt a certain security. But I always thought I was going to be unmasked: 'OK, pack your stuff.' 'Well, what's the matter?' 'We've discovered you have no talent; we're shipping you back to Canada.'" He moved to New York City for his scholarship, studying theater and music at the Neighborhood Playhouse, while performing in summer stock theatre. Afterward, he attended the Actors Studio, until making his first television appearance in 1948 on an episode of "Studio One", alongside Charlton Heston, for which he was paid $75.
Career.
Early career.
Nielsen's career began in dramatic roles on television during "Television's Golden Age", appearing in almost 50 live programs in 1950 alone. He said there "was very little gold, we only got $75 or $100 per show." He narrated documentaries and commercials and most of his early work as a dramatic actor was uneventful. Hal Erickson of Allmovie noted that "much of Nielsen's early work was undistinguished; he was merely a handsome leading man in an industry overstocked with handsome leading men." In 1956 he made his feature film debut in the Michael Curtiz-directed musical film "The Vagabond King". In the "Seattle Post-Intelligencer", Nielsen remembered Curtiz as "a sadist, a charming sadist, but a sadist". Nielsen called this film "The Vagabond Turkey". Though the film was not a success, producer Nicholas Nayfack offered him an audition for the science fiction film "Forbidden Planet", resulting in Nielsen's taking a long contract with Metro-Goldwyn-Mayer (MGM).
"Forbidden Planet" became an instant success, and roles in other MGM films such as "Ransom!" (1956), "The Opposite Sex" (1956) and "Hot Summer Night" (1957) followed. In 1957 he won the lead role opposite Debbie Reynolds in the romantic comedy "Tammy and the Bachelor", which, as a "Chicago Tribune" critic wrote in 1998, made people consider Nielsen a dramatic actor and handsome romantic lead. However, dissatisfied with the films he was offered, calling the studios "a Tiffany, which had forgotten how to make silver", Nielsen left MGM after auditioning for Messala in the 1959 "Ben-Hur". Stephen Boyd got the role. After leaving the studios, Nielsen landed the lead role in the Disney miniseries "The Swamp Fox", as American Revolutionary War hero Francis Marion. In a 1988 interview he reflected on the series, saying, "That was a great experience, because the Disney people didn't do their shows like everyone else, knocking out an episode a week. ... We only had to do an episode a month, and the budgets were extremely high for TV at that time. We had location shooting rather than cheap studio backdrops, and very authentic costumes." Eight episodes were produced and aired between 1959 and 1961.
His television appearances include "Justice", "Alfred Hitchcock Presents", "Voyage to the Bottom of the Sea", "The Virginian", and "The Wild Wild West". In 1961, he was the lead in a Los Angeles police drama called "The New Breed". He guest-starred in a 1964 episode of "Daniel Boone" with Fess Parker in a minor but credited role. In 1968, he had a major role in the pilot for the police series "Hawaii Five-O", and appeared in one of the seventh-season episodes. In 1969, he had the leading role as a police officer in "".
In 1972, Nielsen appeared as the ship's captain in the "The Poseidon Adventure". He also starred in the William Girdler's 1977 action film, "Project: Kill". His last dramatic role before mainly comedy roles was the 1979 Canadian disaster film "City on Fire", in which he played a corrupt mayor. In 1980, he guest-starred as Sinclair on the CBS miniseries "The Chisholms".
"Airplane!" and "The Naked Gun".
Nielsen's supporting role of Dr. Rumack in Zucker, Abrahams and Zucker's 1980's "Airplane!" was a watershed in his career. The film, a parody of disaster films such as "Zero Hour!" and "Airport", was based on building a comedy around the actors known for dramatic roles. Other stars included Robert Stack, Peter Graves, and Lloyd Bridges. Nielsen's deadpan delivery contrasted with the absurdity surrounding him. When asked, "Surely you can't be serious?", he responded with a curt, "I am serious. And don't call me Shirley." In several interviews he reflected on the line: "I thought it was amusing, but it never occurred to me that it was going to become a trademark. It's such a surprise ... the thing comes out, people say, 'What did he say?!'" Nielsen said he was "...pleased and honored that had a chance to deliver that line." The comedic exchange was at #79 on the American Film Institute's AFI's 100 Years...100 Movie Quotes. The American Film Institute included the film in its list of the top ten comedy films of all time in 2008, and a 2007 survey in the United Kingdom judged it the second greatest comedy film of all time, while in 2012 "Empire" magazine voted it No. 1 in "The 50 Funniest Comedies Ever" poll. Critics praised the film, which also proved a long-term success with audiences. In 2010 "Airplane!" was selected for preservation in the National Film Registry by the Library of Congress.
The directors cast Nielsen for his ability to play like "a fish in water", saying "You could have cast funny people and done it with everybody winking, goofing off, and silly... we wanted people to be oblivious to the comedy." For Nielsen, "Airplane!" marked a shift from dramatic roles to deadpan comedy. When it was suggested his role in "Airplane!" was against type, Nielsen protested that he had "always been cast against type before", and that comedy was what he always wanted to do. The same directors cast Nielsen in a similar style, in their TV series "Police Squad!". The series introduced Nielsen as Frank Drebin, the stereotypical police officer modeled after serious characters in earlier police series.
"Police Squad"'s opening sequence was based on the 1950s show "M Squad", which starred Lee Marvin, which opened with footage of a police car roving through a dark urban setting with a big band playing a jazz song in the background. The voice-over and the show's organization into acts with an epilogue was homage to Quinn Martin police dramas including "The Fugitive", "The Streets of San Francisco", "Barnaby Jones", "The F.B.I.", and "Cannon". Nielsen portrayed a serious character whose one-liners appeared accidental next to the pratfalls and sight gags around him. Although the show lasted only six episodes Nielsen received an Emmy Award nomination for Outstanding Lead Actor in a Comedy Series.
Six years after cancellation of "Police Squad!", the film "" returned Nielsen to his role as Frank Drebin. It involved a ruthless drug king trying hypnosis to assassinate Queen Elizabeth II. Drebin, like the doctor in "Airplane!", seemed unaware of the absurdity around him even when contributing to it. Nielsen did many of his own stunts: "You have an idea of how you're going to do something, and it's your vision... unless you do it, it really doesn't stand a chance." This movie grossed over $78 million and was well received by critics. Ebert's 3½–star review (out of four) noted, "You laugh, and then you laugh at yourself for laughing."
' spawned two sequels: ' (1991) and "" (1994). "Naked Gun 2½" grossed more than the original, with $86.9 million, while "Naked Gun 33⅓" grossed $51.1 million. Nielsen remained open to a fourth "Naked Gun" film, although he doubted that it would be produced — "I don't think so", he said in 2005. "If there hasn't been one by now, I doubt it. I think it would be wonderful."
Nielsen briefly appeared on the World Wrestling Federation program in the summer of 1994 on "WWF Monday Night Raw"; capitalizing on Frank Drebin. Nielsen (and George Kennedy) were hired as sleuths to unravel the mystery of The Undertaker who had disappeared at January's Royal Rumble event. At SummerSlam 1994, in a "Naked Gun" parody, they were hot on the case (in fact, they were standing on a case). Although they did not find The Undertaker, the case had been closed (the literal case had been shut) and thus, they solved the mystery. In 1990, Nielsen appeared as a Frank Drebin character in advertisements in the United Kingdom for Red Rock Cider.
Non-comedic roles after "Airplane!" included "Prom Night" (1980) and "Creepshow" (1982), both horror films, and as a dramatic and unsympathetic character in the 1986 comedy "Soul Man". His last dramatic role was as Allen Green, a violent client of a prostitute killed in self-defense by Barbra Streisand's character, Claudia Draper, in Martin Ritt's courtroom drama "Nuts" (1987).
Later comedies.
Subsequent to "Airplane!" and "The Naked Gun", Nielsen portrayed similar styled roles in a number of other films. These mostly emulated the style of "The Naked Gun" with varying success and often targeted specific films: many were panned by critics and most performed poorly. "Repossessed" (1990) and ' (2001) were parodies of "The Exorcist" and ', respectively. Both attempted absurd comedy but were poorly received. Even a leading role in a Mel Brooks comic horror, "", failed to generate much box office excitement, although it did gain a following later release to video. Both 1996's "Spy Hard" and 1998's "Wrongfully Accused", a parody of James Bond films and "The Fugitive", were popular on video but not well received by critics.
His attempt at children's comedies met additional criticism. "Surf Ninjas" (1993) and "Mr. Magoo" (1997) had scathing reviews. Several critics were disappointed that Nielsen's role in "Surf Ninjas" was only "an extended cameo" and Chris Hicks recommended that viewers "avoid any comedy that features Leslie Nielsen outside of the "Naked Gun" series." Jeff Miller of the "Houston Chronicle" panned "Mr. Magoo", a live action remake of the 1950s cartoon, by saying, "I'm supposed to suggest how the film might be better but I can't think of anything to say other than to make the film again."
Nielsen's first major success since "The Naked Gun" came in a supporting role in "Scary Movie 3" (2003). His appearance as President Harris led to a second appearance in its sequel, "Scary Movie 4" (2006). This was the first time Nielsen had reprised a character since Frank Drebin. In one scene, Nielsen appeared almost nude, and one critic referred to the scene as putting "the 'scary' in "Scary Movie 4"."
Video, stage, and celebrity productions.
Nielsen also produced instructional golf videos, which were not presented in a serious style, beginning with 1993's "Bad Golf Made Easier". The videos combined comedy with golf techniques. The series spawned two additional sequels, "Bad Golf My Way" (1994) and "Stupid Little Golf Video" (1997). Nielsen also co-wrote a fictional autobiography titled "The Naked Truth". The book portrayed Nielsen as a popular actor with a long history of prestigious films.
In his eighties, Nielsen performed serious roles on screen and stage (such as his one-man theatre show "Darrow", in which he played Clarence Darrow), as well as providing voice-overs and appearances for commercials; cartoons like "Zeroman" where he had the leading role/voice; children's shows, such as "Pumper Pups", which he narrated, in addition to comedic film roles. The sibling relationship with his elder brother, the Honourable Erik Nielsen, a former Deputy Prime Minister of Canada, served as the premise of an HBO mockumentary entitled "The Canadian Conspiracy" in which Leslie Nielsen appeared, along with other prominent Canadian-born media personalities. He was a celebrity contestant on CBS's "Gameshow Marathon", where he played "The Price Is Right", "Let's Make a Deal", "Beat the Clock", and "Press Your Luck" for charity.
Final acting years.
Beginning in February 2007, Nielsen began playing a small role as a doctor in the humorous yet educational television show "Doctor*Ology". The show chronicles real-life medical techniques and technology, on the Discovery Channel. Nielsen said: "There are any number of things that you think about when you ponder if you hadn't been an actor, what would you be, and I've always said I'd like to be an astronaut or a doctor. I have such admiration for doctors. I just don't know how you go around to thank them enough for coming up with the world's most remarkable new discoveries."
In 2007, Nielsen starred in the drama "Music Within". In 2008, he portrayed a version of Uncle Ben for "Superhero Movie", a spoof of superhero films. He then appeared in the 2008 parody "An American Carol", which David Zucker directed, produced and co-wrote. He appeared in the 2009 parody "Stan Helsing". Nielsen portrayed the Doctor in the Spanish horror comedy "Spanish Movie", a spoof comedy like "Scary Movie", but making fun of popular Spanish films.
Nielsen appeared in more than 100 films and 1,500 television episodes, portraying more than 220 characters.
Personal life.
Nielsen married four times: nightclub singer Monica Boyar (1950–1956), Alisande Ullman (1958–1973), Brooks Oliver (1981–1983) and Barbaree Earl (2001–2010). Nielsen had two daughters from his second marriage, Maura and Thea Nielsen.
Nielsen often played golf. He joked, "I have no goals or ambition. I do, however, wish to work enough to maintain whatever celebrity status I have so that they will continue to invite me to golf tournaments." His interest in the sport led him to comedic instructional films.
Nielsen was a practical joker, and known for pranking people with a portable hand-controlled fart machine: "he always had that fart machine with him." His epitaph read: "Let 'er rip", a final reference to his favorite practical joke.
Nielsen was legally deaf and wore hearing aids for most of his life. Because of this impairment, he supported the Better Hearing Institute. Later in life, Nielsen had knee osteoarthritis. He participated in an educational video from The Arthritis Research Centre of Canada (ARC), demonstrating the physical examination of a patient with knee osteoarthritis.
Achievements.
Among his awards, in 1995 Nielsen received UCLA's Jack Benny Award. In 1988, he became the 1,884th personality to receive a star on the Hollywood Walk of Fame at 6541 Hollywood Blvd. In 2001 he was inducted into Canada's Walk of Fame. The following year he was made an Officer of the Order of Canada, although he was also a naturalized U.S. citizen. With his American status, he maintained his Canadian heritage: "There's no way you can be a Canadian and think you can lose it ... Canadians are a goodly group. They are very aware of caring and helping." On 19 May 2005, during the centennial gala of his birth province, Saskatchewan, Leslie Nielsen was introduced to HM Queen Elizabeth II.
In 1997, a Golden Palm Star on the Palm Springs, California, Walk of Stars was dedicated to him.
On 20 February 2002, Nielsen was named an honorary citizen of West Virginia and an Ambassador of Mountain State Goodwill. Nielsen visited the state many times to speak and visit friends. In 2003, in honor of Nielsen, Grant MacEwan College named its school of communications after him. Also in 2003, the Alliance of Canadian Cinema, Television and Radio Artists awarded him the ACTRA Award of Excellence.

</doc>
<doc id="55016" url="https://en.wikipedia.org/wiki?curid=55016" title="Ivo Andrić">
Ivo Andrić

Ivo Andrić (, ; 9 October 1892 – 13 March 1975) was a Yugoslav novelist, poet and short story writer who won the Nobel Prize in Literature in 1961. His writings dealt mainly with life in his native Bosnia under Ottoman rule.
Born in Travnik in Austrian-occupied Bosnia, Andrić attended high school in Sarajevo, where he became an active member of several South Slav national youth organizations. Following the assassination of Archduke Franz Ferdinand in June 1914, Andrić was arrested and imprisoned by the Austro-Hungarian police. As the authorities were unable to build a strong case against him, he was placed under house arrest for much of the war, only being released following a general amnesty for such cases in July 1917. After the war, he studied South Slavic history and literature at universities in Zagreb and Graz, eventually attaining his Ph.D in Graz in 1924. Between 1920 and 1941, he worked in the diplomatic service of the Kingdom of Yugoslavia. In 1939, he was appointed Yugoslav ambassador to Germany but his tenure ended in April 1941 with the German-led invasion of his country. Shortly after the invasion, Andrić returned to German-occupied Belgrade. He lived quietly in the city for the duration of World War II, writing some of his most important works, including "Na drini ćuprija" (The Bridge on the Drina). 
Following the war, Andrić was named to a number of ceremonial posts in Yugoslavia, which had come under communist rule at the war's end. In 1961, the Nobel Committee awarded him the Nobel Prize in Literature, selecting him over the likes of J.R.R. Tolkien, Robert Frost, John Steinbeck and E.M. Forster. The Committee cited "the epic force with which he ... traced themes and depicted human destinies drawn from his country's history". Afterwards, Andrić's works found an international audience, and were translated into a number of languages. In subsequent years, he received a number of awards in his native country. Andrić's health declined substantially in late 1974, and he died in Belgrade the following March. 
In the years following Andrić's death, the Belgrade apartment in which he spent much of World War II was converted into a museum and a nearby street corner was named in his honour. A number of cities in the former Yugoslavia have streets that bear his name. In 2012, filmmaker Emir Kusturica began construction of an ethno-town in eastern Bosnia that is named after Andrić. As Yugoslavia's only Nobel Prize-winning writer, Andrić was well known and respected in his native country during his lifetime. Since the breakup of Yugoslavia, Croatia's literary establishment has distanced itself from Andrić's oeuvre and his works have been disparaged by Bosniak literary critics for their supposed anti-Muslim bias. He is still highly regarded in Serbia for his contributions to Serbian literature.
Early life.
Family.
Ivo Andrić was born in Travnik on 9 October 1892 while his mother, Katarina ("" Pejić), was in the town visiting relatives. Andrić's parents were both Croats, and he was their only child. His father, Antun, was a struggling silversmith. He resorted to working as a school janitor in Sarajevo, where he lived with his wife and infant son. Antun died of tuberculosis, like most of his siblings, at the age of thirty-two. Andrić was only two years old. 
Widowed and penniless, Andrić's mother took him to Višegrad and placed him in the care of her sister-in-law Ana and brother-in-law Ivan Matković, an Austro-Hungarian police officer. The couple were financially stable but childless, so they agreed to look after the infant and brought him up as their own. In the meantime, Andrić's mother returned to Sarajevo seeking employment.
Andrić was raised in a country that had changed little since the Ottoman period despite being mandated to Austria-Hungary at the Congress of Berlin in 1878. Eastern and Western culture intermingled in Bosnia to a far greater extent than anywhere else in the Balkan peninsula. Having lived there from an early age, he came to cherish Višegrad, calling it "my real home". Though it was a small provincial town (or "kasaba"), it proved to be an enduring source of inspiration. Višegrad was multi-ethnic and multi-confessional, the predominant groups being Serbs and Bosnian Muslims. From an early age, Andrić closely observed the customs of the local people. These customs, and the particularities of life in eastern Bosnia, would later be detailed in his works. Andrić made his first friends in Višegrad, playing with them along the Drina River and the town's famous Mehmed Paša Sokolović Bridge.
Primary and secondary education.
At age six, Andrić began primary school. He later recounted that these were the happiest days of his life. At the age of ten, he received a three-year scholarship from a Croat cultural group called "Napredak" (Progress) to study in Sarajevo. In the autumn of 1902, he was registered at the Great Sarajevo Gymnasium (), the oldest secondary school in Bosnia. While in Sarajevo, Andrić lived with his mother, who worked in a rug factory. At the time, the city was overflowing with civil servants from all parts of Austria-Hungary, and thus many languages could be heard in its restaurants, cafés and on its streets. Culturally, the city boasted a strong Germanic element, and the curriculum in educational institutions was designed to reflect this. From a total of 83 teachers that worked at Andrić's school over a twenty-year period, only three were natives of Bosnia and Herzegovina. "The teaching program," biographer Celia Hawkesworth notes, "was devoted to producing dedicated supporters of the Monarchy." Andrić disapproved. "All that came ... at secondary school and university," he wrote, "was rough, crude, automatic, without concern, faith, humanity, warmth or love."
Andrić experienced difficulty in his studies, finding mathematics particularly challenging, and had to repeat the sixth grade. For a time, he lost his scholarship due to poor grades. Hawkesworth attributes Andrić's initial lack of academic success at least partly to his alienation from most of his teachers. Nonetheless, Andrić excelled in languages, particularly Latin, Greek and German. Although he initially showed substantial interest in natural sciences, he later began focusing on literature, likely under the influence of his two Croat instructors, writer and politician Đuro Šurmin and poet Tugomir Alaupović. Of all his teachers in Sarajevo, Andrić liked Alaupović best, and the two became lifelong friends. 
Andrić felt he was destined to become a writer. He began writing in secondary school, but received little encouragement from his mother. He recalled that when he showed her one of his first works, she replied: "Did you write this? What did you do that for?" Andrić published his first two poems in 1911 in a journal called "Bosanska vila" (Bosnian Fairy), which promoted Serbo-Croat unity. At the time, he was still a secondary school student. Prior to World War I his poems, essays, reviews, and translations appeared in journals such as "Vihor" (Whirlwind), "Savremenik" (The Contemporary), "Hrvatski pokret" (The Croatian Movement), and "Književne novine" (Literary News). One of Andrić’s favorite literary forms was lyrical reflective prose, and many of his essays and shorter pieces are prose poems. The historian Wayne S. Vucinich describes Andrić’s poetry from this period as "subjective and mostly melancholic". Andrić’s translations of August Strindberg, Walt Whitman, and a number of Slovene authors also appeared around this time.
Student activism.
In 1908, Austria-Hungary officially annexed Bosnia and Herzegovina, to the chagrin of South Slav nationalists like Andrić. In late 1911, Andrić was elected the first president of the Serbo-Croat Progressive Movement (; SHNO), a Sarajevo-based secret society that promoted unity and friendship between Serb and Croat youth and opposed the Austro-Hungarian occupation. Its members were vehemently criticized by both Serb and Croat nationalists, who dismissed them as "traitors to their nations". Unfazed, Andrić continued his anti-Habsburg activities. On 28 February 1912, he spoke before a crowd of 100 students at Sarajevo's railway station, urging them to continue their demonstrations. The Austro-Hungarian police later began harassing and prosecuting SHNO members. Ten were expelled from their schools or penalized in some other way, though Andrić himself escaped punishment. Andrić also joined the South Slav student movement known as Young Bosnia, and became one of its most prominent members.
In 1912, Andrić registered at the University of Zagreb, having received a scholarship from an educational foundation in Sarajevo. He enrolled in the department of mathematics and natural sciences because these were the only fields for which scholarships were offered, but was able to take some courses in Croatian literature. Andrić was well received by South Slav nationalists there, and regularly participated in on-campus demonstrations. This led to his being reprimanded by the university. After completing two semesters in Zagreb, Andrić transferred to the University of Vienna, where he resumed his studies. While in Vienna, he joined South Slav students in promoting the cause of Yugoslav unity and worked closely with two Yugoslav student societies, the Serbian cultural society "Zora" (Dawn) and the Croatian student club "Zvonimir", which shared his views on "integral Yugoslavism" (the eventual assimilation of all South Slav cultures into one).
Despite finding like-minded students in Vienna, the city's climate took a toll on Andrić's health. He contracted tuberculosis and became seriously ill, then asked to leave Vienna on medical grounds and continue his studies elsewhere, though Hawkesworth believes he may actually have been taking part in a protest of South Slav students that were boycotting German-speaking universities and transferring to Slavic ones. For a time, Andrić had considered transferring to Russia, but ultimately decided to complete his fourth semester at Jagiellonian University in Kraków. He transferred in early 1914, and continued to publish translations, poems and reviews.
World War I.
On 28 June 1914, Andrić learned of the assassination of Archduke Franz Ferdinand in Sarajevo. The assassin was Gavrilo Princip, a Young Bosnian and close friend of Andrić who had been one of the first to join the SHNO in 1911. Upon hearing the news, Andrić decided to leave Kraków and return to Bosnia. He travelled by train to Zagreb, and in mid-July, departed for the coastal city of Split with his friend, the poet and fellow South Slav nationalist Vladimir Čerina. Andrić and Čerina spent the rest of July at the latter's summer home. As the month progressed, the two began feeling increasingly uneasy about the escalating political crisis that followed the Archduke's assassination and eventually led to the outbreak of World War I. They then went to Rijeka. There, Čerina left Andrić without offering any explanation, only saying that he urgently needed to go to Italy. Several days later, Andrić learned that Čerina was being sought by the police.
By the time war was declared, Andrić had returned to Split feeling exhausted and ill. Given that most of his friends had already been arrested for nationalist activities, he was certain the same fate would befall him. Despite not being involved in the assassination plot, on 4 August, Andrić was arrested for "anti-state activities" and imprisoned in Split. From there, he was transferred to a prison in Šibenik, then to Rijeka and finally to Maribor, where he arrived on 19 August. Plagued by tuberculosis, Andrić passed the time reading, talking to his cellmates and learning languages.
By the following year, the case against Andrić was dropped due to lack of evidence, and he was released from prison on 20 March 1915. The authorities exiled him to the village of Ovčarevo, near Travnik. He arrived there on 22 March and was placed under the supervision of local Franciscan monks. Andrić soon befriended the friar Alojzije Perčinlić and began researching the history of the Catholic and Orthodox religious communities under Ottoman rule. Andrić lived in the parish headquarters, and the Franciscans gave him access to the monastery chronicles. In return, he assisted the parish priest and taught religious songs to pupils at the monastery school. Andrić's mother soon came to see her son, and offered to serve as a housekeeper for the parish priest. "Mother is very happy," Andrić wrote. "It has been three whole years since she saw me. And she can't grasp all that has happened to me in that time, nor the whole of my crazy, cursed existence. She cries, kisses me and laughs in turn. Like a mother."
Andrić was later transferred to a prison in Zenica, where Perčinlić regularly visited him. In March 1917, the Austro-Hungarian Army declared Andrić a political threat and exempted him from armed military service. He was thus registered with a non-combat unit until February of the following year. On 2 July 1917, Emperor Charles I declared a general amnesty for all political prisoners on the territory of Austria-Hungary. His freedom of movement restored, Andrić visited Višegrad, where he was reunited with several school friends. He remained in the town until July, when he was mobilized. Due to his poor health, Andrić was admitted to a Sarajevo hospital and thus avoided service. He was then transferred to the "Reservospital" in Zenica, where he was treated for several months before moving on to Zagreb. There, Andrić again fell seriously ill and sought treatment at the Sisters of Mercy hospital, which had become a gathering ground for many former political prisoners.
In January 1918, Andrić joined several other South Slav nationalist writers in editing a short-lived pan-Yugoslav periodical called "Književni jug" (Literary South). Here and in other periodicals, Andrić published book reviews, plays, verse, and translations. Over the course of several months in early 1918, Andrić's health began to deteriorate, and his friends believed he was nearing death. However, he recovered and spent the spring of 1918 in Krapina writing "Ex ponto", a book of prose poetry which was published that July. It was his first book.
Interwar period.
The end of World War I saw the disintegration of Austria-Hungary, which was replaced by a newly established South Slav state, the Kingdom of Serbs, Croats and Slovenes (renamed Yugoslavia in 1929). In late 1918, Andrić re-enrolled at the University of Zagreb and resumed his studies from where he had left them prior to his internment. By January 1919, he fell ill again and was back in hospital. Andrić's friend and fellow writer Ivo Vojnović became seriously worried for his life, and appealed to Andrić's old schoolteacher Tugomir Alaupović (who had just been appointed the new kingdom's Minister of Religious Affairs) to use his connections to help Andrić pay for treatment abroad. In February, Andrić wrote Alaupović and asked for help finding a government job in Belgrade. Eventually, Andrić chose to seek treatment in Split, where he stayed for the following six months. During his time on the Mediterranean coast, Andrić completed a second volume of prose poetry, titled "Nemiri" (Unrest), which was published the following year. By the time Andrić left, he had almost fully recovered, and quipped that he was cured by the "air, sun and figs." Troubled by news that his uncle was seriously ill, Andrić left Split in August and went to Višegrad. He returned to Zagreb two weeks later.
Early diplomatic career.
By 1919, Andrić had acquired his undergraduate degree in South Slavic history and literature at the University of Zagreb. Yet, he was perennially impoverished, and earned a meagre sum through his writing and editorial work. By mid-1919, he realized that he would be unable to financially support himself and his aging mother, aunt and uncle for much longer, and his appeals to Alaupović for help securing a government job became more frequent. In September 1919, Alaupović offered him a secretarial position at the Ministry of Religion, which Andrić accepted.
In late October, Andrić left for Belgrade. He became involved in the city's literary circles and soon acquired the distinction of being one of Belgrade's most popular young writers. Though the Belgrade press wrote positively of him, Andrić disliked being a public figure, and went into seclusion and distanced himself from his fellow writers. At the same time, he grew dissatisfied with his government job and wrote to Alaupović asking for a transfer to the Ministry of Foreign Affairs. On 20 February, Andrić's request was granted and he was assigned to the Foreign Ministry's mission at the Vatican. 
Andrić left Belgrade soon after, and reported for duty in late February. At this time, he published his first short story, "Put Alije Đerzeleza" (The Journey of Alija Đerzelez). He complained that the consulate was understaffed and that he did not have enough time to write. All evidence suggests he had a strong distaste for the ceremony and pomp that accompanied his work in the diplomatic service, but according to Hawkesworth, he endured it with "dignified good grace". Around this time, he began writing in the Ekavian dialect used in Serbia, and ceased writing in the Ijekavian dialect used in his native Bosnia. Andrić soon requested another assignment, and in November, he was transferred to Bucharest. Once again, his health deteriorated. Nevertheless, Andrić found his consular duties there did not require much effort, so he focused on writing, contributed articles to a Romanian journal and even had time to visit his family in Bosnia. In 1922, Andrić requested another reassignment. He was transferred to the consulate in Trieste, where he arrived on 9 December. The city's damp climate only caused Andrić's health to deteriorate further, and on his doctor's advice, he transferred to Graz in January 1923. He arrived in the city on 23 January, and was appointed vice-consul. Andrić soon enrolled at the University of Graz, resumed his schooling and began working on his doctoral dissertation in Slavic studies.
Advancement.
In August 1923, Andrić experienced an unexpected setback in his career. A law had been passed stipulating that all civil servants, especially those in the Foreign Ministry, had to have a doctoral degree. As Andrić had not completed his dissertation, he was informed that his employment would be terminated. Andrić's well-connected friends also intervened on his behalf and appealed to Foreign Minister Momčilo Ninčić, citing Andrić's diplomatic and linguistic abilities. In February 1924, the Foreign Ministry decided to retain Andrić as a day worker with the salary of a vice-consul. This gave him the opportunity to complete his Ph.D. On 24 May 1924, Andrić submitted his dissertation to a committee of examiners at the University of Graz, who gave it their approval. This allowed Andrić to take the examinations necessary for his Ph.D to be confirmed. He passed both his exams, and on 13 July, received his Ph.D. The committee of examiners recommended Andrić's dissertation be published. Andrić chose the title "Die Entwicklung des geistigen Lebens in Bosnien unter der Einwirkung der türkischen Herrschaft" (The Development of Spiritual Life in Bosnia Under the Influence of Turkish Rule). In it, he characterized the Ottoman occupation as a yoke that still loomed over Bosnia. "The effect of Turkish rule was absolutely negative," he wrote. "The Turks could bring no cultural content or sense of higher mission, even to those South Slavs who accepted Islam." 
Several days after receiving his Ph.D, Andrić wrote the Foreign Minister asking to be reinstated and submitted a copy of his dissertation, university documents and a medical certification that deemed him to be in good health. In September, the Foreign Ministry granted his request. Andrić stayed in Graz until 31 October, when he was assigned to the Foreign Ministry's Belgrade headquarters. During the two years he was in Belgrade, Andrić spent much of his time writing. His first collection of short stories was published in 1924, and he received a prize from the Serbian Royal Academy (of which he became a full-fledged member in February 1926). In October 1926, he was assigned to the consulate in Marseille and again appointed vice-consul. On 9 December 1926, he was transferred to the Yugoslav embassy in Paris. Andrić's time in France was marked by increasing loneliness and isolation. His uncle had died in 1924, his mother the following year, and upon arriving in France, he was informed that his aunt had died as well. "Apart from official contacts," he wrote Alaupović, "I have no company whatever." Andrić spent much of his time in the Paris archives poring over the reports of the French consulate in Travnik from 1809–14, material he would use in "Travnička hronika" (Travnik Chronicle), one of his future novels. 
In April 1928, Andrić was posted to Madrid as vice-consul. While there, he wrote essays on Simón Bolívar and Francisco Goya, and began work on the novel "Prokleta avlija" (The Damned Yard). In June 1929, he was named secretary of the Yugoslav legation to Belgium and Luxembourg in Brussels. On 1 January 1930, he was sent to Switzerland as part of Yugoslavia's permanent delegation to the League of Nations in Geneva, and was named deputy delegate the following year. In 1933, Andrić returned to Belgrade, and shortly thereafter France decorated him with the Legion of Honour. Two years later, he was named head of the political department of the Ministry of Foreign Affairs. On 5 November 1937, Andrić became assistant to Milan Stojadinović, Yugoslavia's Prime Minister and Foreign Minister.
World War II.
On 28 March 1939, Andrić was appointed Yugoslavia's ambassador to Germany. This appointment, Hawkesworth writes, demonstrates that he was highly regarded by the Yugoslav leadership. In 1934, Yugoslavia's King Alexander had been assassinated in Marseille. He was succeeded by his ten-year-old son Peter, and a regency council led by Peter's uncle Paul was established to rule in his place until he turned eighteen. Paul's government established closer economic and political ties with Germany. In March 1941, Yugoslavia signed the Tripartite Pact, pledging support for Germany and Italy. Though the negotiations had occurred behind Andrić's back, in his capacity as ambassador he was obliged to attend the document's signing in Berlin. Andrić was highly critical of the move, and on 17 March, he wrote to the Ministry of Foreign Affairs asking to be relieved of his duties. Ten days later, pro-Western Royal Yugoslav Air Force officers overthrew the regency and proclaimed Peter of age. This led to a breakdown in relations with Germany and prompted Adolf Hitler to order Yugoslavia's invasion. Given these circumstances, Andrić's position as ambassador to Germany was an extremely difficult one. Yet, he used the little influence he had to help Polish prisoners following the German invasion of Poland. 
Prior to their invasion of his country, the Germans offered Andrić the opportunity to evacuate to neutral Switzerland. He declined on the basis that his staff would not be allowed to go with him. On 6 April 1941, the Germans and their allies invaded Yugoslavia. The country capitulated on 17 April and was subsequently partitioned between the Axis powers. In early June, Andrić and his staff were taken back to German-occupied Belgrade, where some were jailed. Andrić was retired from the diplomatic service, but refused to receive his pension or cooperate in any way with the puppet government the Germans had installed in Serbia. He was spared jail, but the Germans kept him under close surveillance throughout the occupation. Because of his Croat heritage, they had offered him the chance to settle in Zagreb, then the capital of the fascist puppet state known as the Independent State of Croatia, but he declined. Andrić spent the following three years in a friend's Belgrade apartment in conditions that some biographers liken to house arrest. He directed most of his energies towards writing, and during this time completed two of his best known novels, "Na Drini ćuprija" (The Bridge on the Drina) and "Travnička hronika".
In late spring 1942, Andrić sent a message of sympathy to Draža Mihailović, the leader of the royalist Chetniks, one of two resistance movements vying for power in Axis-occupied Yugoslavia, the other being Josip Broz Tito's communist Partisans. In 1944, Andrić was forced to leave his friend's apartment during the Allied bombing of Belgrade and evacuate the city. As he joined a column of refugees, he became ashamed that he was fleeing by himself, in contrast to the masses of people accompanied by their children, spouses and infirm parents. "I looked myself up and down," he wrote, "and saw I was saving only myself and my overcoat." In the ensuing months, Andrić refused to leave the apartment, even during the heaviest bombing. That October, the Red Army and the Partisans drove the Germans out of Belgrade, and Tito proclaimed himself Yugoslavia's ruler.
Later life.
Political career and marriage.
Andrić's relationship with the communists was initially precarious given he had previously been an official in the royalist government. He returned to public life only once the Germans had been forced out of Belgrade. "Na Drini ćuprija" was published in March 1945. It was followed by "Travnička hronika" that September and "Gospođica" (The Young Lady) that November. The first of these, "Na Drini ćuprija", came to be regarded as Andrić's "magnum opus" and was proclaimed a classic of Yugoslav literature by the communist authorities. It chronicles the history of the Mehmed Paša Sokolović Bridge and the town of Višegrad from the bridge's construction in the 16th century until the outbreak of World War I. The second novel, "Travnička hronika", follows a French diplomat in Bosnia during the Napoleonic Wars. The third, "Gospođica", revolves around the life of a Sarajevan woman. In the post-war period, Andrić also published several short story collections, some travel memoirs, and a number of essays on writers such as Vuk Karadžić, Petar II Petrović-Njegoš, and Petar Kočić.
In November 1946, Andrić was elected vice-president of the Society for the Cultural Cooperation of Yugoslavia with the Soviet Union. The same month, he was named president of the Yugoslav Writers' Union. In 1947, he became a member of the People's Assembly of Bosnia and Herzegovina. In 1948, Andrić published a collection of short stories he had written during the war. His work came to influence writers such as Branko Ćopić, Vladan Desnica, Mihailo Lalić and Meša Selimović. In April 1950, Andrić became a deputy in the National Assembly of Yugoslavia. He held this position until 1953. In 1952, he had been decorated by the Presidium of the National Assembly for his services to the Yugoslav people. In 1954, Andrić published the novella "Prokleta avlija" (The Damned Yard), which tells of life in an Ottoman prison in Istanbul. That December, he was admitted into the League of Communists of Yugoslavia, the country's ruling party. According to Hawkesworth, it is unlikely he joined the party out of ideological conviction, but rather to "serve his country as fully as possible". 
On 27 September 1958, the 66-year-old Andrić married Milica Babić, a costume designer at the National Theatre of Serbia who was almost twenty years his junior. Earlier, he had announced it was "probably better" that a writer never marry. "He was perpetually persecuted by a kind of fear," a close friend recalled. "It seemed as though he had been born afraid, and that is why he married so late. He simply did not dare enter that area of life."
Nobel Prize, international recognition and death.
By the late 1950s, Andrić's works had been translated into a number of languages. On 26 October 1961, he was awarded the Nobel Prize in Literature by the Swedish Academy. Documents released 50 years later revealed that the Nobel Committee had selected Andrić over the likes of J.R.R. Tolkien, Robert Frost, John Steinbeck and E.M. Forster. The Committee cited "the epic force with which he has traced themes and depicted human destinies drawn from his country's history". Once the news was announced, Andrić's Belgrade apartment was swarmed by reporters, and he publicly thanked the Nobel Committee for selecting him as the winner of that year's prize. 
The Nobel Prize ensured Andrić received global recognition. The following March, he fell ill while on a trip to Cairo and had to return to Belgrade for an operation. He was thus obliged to cancel all promotional events in Europe and North America, but his works continued to be reprinted and translated into numerous languages. Judging by letters he had written at the time, Andrić felt burdened by the attention but did his best not to show it publicly. Upon receiving the Noble Prize, the number of awards and honours bestowed upon him multiplied. He received the Order of the Republic in 1962, as well as the 27 July Award of Bosnia-Herzegovina, the AVNOJ Award in 1967, and the Order of the Hero of Socialist Labour in 1972. Apart from being a member of the Yugoslav and Serbian Academy of Sciences and Arts, he was also a correspondent of their Bosnian and Slovenian counterparts. He was bestowed honorary doctorates by the universities of Belgrade, Sarajevo and Kraków.
Andrić's wife died on 16 March 1968. Nearing seventy-six at the time of her death, his health deteriorated steadily and he travelled little in his final years. He continued to write until 1974, when his health took another turn for the worse. In December 1974, he was admitted to a Belgrade hospital. He soon fell into a coma, and died in the Military Medical Academy at 1:15 a.m. on 13 March 1975, aged 82. Andrić's body was cremated, and on 24 April, the urn carrying his ashes was buried in the Alley of Distinguished Citizens at Belgrade's New Cemetery. The ceremony was attended by about 10,000 people.
Influences, style and themes.
Andrić was an avid reader in his youth. The young Andrić's literary interests varied greatly, ranging from the Greek and Latin Classics to the works of past and contemporary literary figures, including German and Austrian writers such as Johann Wolfgang von Goethe, Heinrich Heine, Friedrich Nietzsche, Franz Kafka, Rainer Maria Rilke and Thomas Mann, French writers Michel de Montaigne, Blaise Pascal, Gustave Flaubert, Victor Hugo and Guy de Maupassant, British writers Thomas Carlyle, Walter Scott and Joseph Conrad. Andrić also read the works of Miguel de Cervantes, Italian poet and philosopher Giacomo Leopardi, Russian writer Nikolay Chernyshevsky, Norwegian writer Henrik Ibsen, the American writers Walt Whitman and Henry James, and Czechoslovak philosopher Tomáš Garrigue Masaryk. Andrić was especially fond of Polish literature, and later stated that it had greatly influenced him. He held several Serb writers in high esteem, particularly Karadžić, Njegoš, Kočić and Aleksa Šantić. Andrić admired the Slovene poets Fran Levstik, Josip Murn and Oton Župančič, and translated some of their works. Kafka appears to have had a significant influence on Andrić's prose, and his philosophical outlook was informed strongly by the works of Danish philosopher Søren Kierkegaard. At one point in his youth, Andrić even took an interest in Chinese and Japanese literature, which he read in French- and German-language translations.
Much of Andrić's work was inspired by the traditions and peculiarities of life in Bosnia, and examines the complexity and cultural contrasts of the region's Muslim, Serb and Croat inhabitants. His two best known novels, "Na Drini ćuprija" and "Travnička hronika", subtly contrast Ottoman Bosnia's "oriental" propensities to the "Western atmosphere" first introduced by the French and later the Austro-Hungarians. His works contain many so-called Turkisms (), words of Turkish, Arabic or Persian origin that found their way into the languages of the South Slavs during Ottoman rule. According to Vucinich, Andrić uses these words to "express oriental nuances and subtleties that cannot be rendered as well in his own Serbo-Croatian".
In the opinion of literary historian Nicholas Moravcevich, Andrić's work "frequently betrays his profound sadness over the misery and waste inherent in the passing of time". As "Na Drini ćuprija" remains his most famous novel, it has received the most scholarly analysis of all his works. Most scholars have interpreted the eponymous bridge as a metonym for Yugoslavia, which was itself a bridge between the East and West during the Cold War. In his Nobel acceptance speech, Andrić described the country as one "which, at break-neck speed and at the cost of great sacrifices and prodigious efforts, is trying in all fields, including the field of culture, to make up for those things of which it has been deprived by a singularly turbulent and hostile past." In Andrić's view, the seemingly conflicting positions of Yugoslavia's disparate ethnic groups could be overcome by knowing one's history. This, he surmised, would help future generations avoid the mistakes of the past, and was in line with his cyclical view of time. Andrić expressed hope that these differences could be bridged and "histories demystified".
Legacy.
Shortly before his death, Andrić stated that he wished for all of his possessions to be preserved as an endowment to be used for "general cultural and humanitarian purposes". In March 1976, an administrative committee decided that the purpose of the endowment would be to promote the study of Andrić's work, as well as art and literature in general. It has since organized a number of international conferences, made grants to foreign scholars studying Andrić's works and offered financial aid to cover the publication costs of books covering Andrić's life and art. An annual yearbook, titled "Sveske Zadužbine Ive Andrića" (The Journals of the Ivo Andrić Endowment), is published by the organization. Andrić's will also stipulated that an award be given annually to the author of that year's best collection of short stories. 
The street that runs beside Novi dvor, now the seat of the President of Serbia, was posthumously named "Andrićev venac" (Andrić's Crescent) in his honour. It also includes a life-size statue of the writer; the flat where he spent his final years (now "Andrićev venac" 8.) has been turned into a museum. The museum was opened more than a year after Andrić's death and houses books, manuscripts, documents, photographs and some of his other belongings. Several of Serbia's other major cities, such as Novi Sad and Kragujevac, have streets named after him. Streets in a number of cities in Bosnia and Herzegovina, such as Sarajevo, Tuzla, and Višegrad, also carry his name.
The historian Bojan Aleskov considers Andrić one of the two central pillars of Serbian literature, the other being Njegoš. "The plasticity of his narrative," Moravcevich writes, "the depth of his psychological insight, and the universality of his symbolism remain unsurpassed in all of Serbian literature." Indeed, Andrić was the first and only Yugoslav writer ever to be awarded the Nobel Prize. Due to his Serb orientation, Andrić's oeuvre was shunned by Croatia's literary establishment following the breakup of Yugoslavia. His works were also dismissed by Bosniak (Bosnian Muslim) nationalists, who objected to his ostensibly negative portrayal of Muslim characters. Andrić's most vocal Bosniak detractors were Šukrija Kurtović, Adil Zulfikarpašić, Muhamed Filipović and Muhsin Rizvić, who accused him of everything from plagiarism to being a Serb nationalist, and made remarks insinuating that he was a homosexual. Some went so far as to call for his Nobel Prize to be taken away. A few of these scholars censured Andrić in the late 1950s, but most Bosniak criticism of his works appeared in the period immediately before the breakup of Yugoslavia and in the aftermath of the Bosnian War. In early 1992, a Bosniak nationalist in Višegrad destroyed a statue of Andrić with a sledgehammer. Twenty years later, in 2012, the award-winning filmmaker Emir Kusturica and Bosnian Serb President Milorad Dodik unveiled another statue of Andrić in Višegrad, this time as part of the construction of an ethno-town called Andrićgrad, sponsored by Kusturica and the Government of Republika Srpska. In June 2014, Andrićgrad was officially opened on the occasion of the 100th anniversary of the assassination of Franz Ferdinand.

</doc>
<doc id="55017" url="https://en.wikipedia.org/wiki?curid=55017" title="Fusion power">
Fusion power

Fusion power is the generation of energy by nuclear fusion. Fusion reactions are high energy reactions in which two lighter atomic nuclei fuse to form a heavier nucleus. When they combine a release of energy is expected in accordance with Einstein's formula formula_1. This major area of plasma physics research is concerned with harnessing this reaction as a source of large scale sustainable energy. There is no question of fusion's scientific feasibility, since stellar nucleosynthesis is the process in which stars transmute matter into energy emitted as radiation.
In almost all large scale commercial proposals, heat from neutron scattering in a controlled fusion reaction is used to operate a steam turbine that drives electrical generators, as in existing fossil fuel and nuclear fission power stations. Many different fusion concepts have come in and out of vogue over the years. The current leading designs are the tokamak and inertial confinement fusion (laser) approaches. , these technologies are not yet practically viable, as they are not "energetically" viable—i.e., it currently takes more energy to initiate and contain a fusion reaction than the reaction then produces.
There are also smaller-scale commercial proposals relying on other means of energy transfer, mostly forms of aneutronic fusion—but these are largely considered to be more remote than the large scale neutron scattering approaches.
Background.
Mechanism.
Fusion reactions occur when two (or more) atomic nuclei come close enough for the strong nuclear force pulling them together to exceed the electrostatic force pushing them apart, fusing them into heavier nuclei. For nuclei lighter than iron-56, the reaction is exothermic, releasing energy. For nuclei heavier than iron-56, it is endothermic, requiring an external source of energy. Hence, nuclei smaller than iron-56 are more likely to fuse while those heavier than iron-56 are more likely to break apart.
To fuse, nuclei must be brought close enough together for the strong force to act, which occurs only at very short distances. The electrostatic force keeping them apart acts over long distances, so a significant amount of kinetic energy is needed to overcome this "Coulomb barrier" before the reaction can take place. There are several ways of doing this, including speeding up atoms in a particle accelerator, or more commonly, heating them to very high temperatures.
Once an atom is heated above its ionization energy, its electrons are stripped away, leaving just the bare nucleus (the ion). The result is a hot cloud of ions and the electrons formerly attached to them. This cloud is known as a plasma. Because the charges are separated, plasmas are electrically conductive and magnetically controllable. Many fusion devices take advantage of this to control the particles as they are being heated.
Theoretically, any atoms can be fused if the pressure and temperature are high enough. Studies have been made of the conditions required to create fusion conditions for a variety of atoms. Power stations, however, are currently limited to only the lightest elements. Hydrogen is ideal for this purpose because of its small charge, making it the easiest atom to fuse, and producing helium.
Cross section.
A reaction's cross section, denoted σ, is the measure of how likely a fusion reaction will happen. It is a probability, and it depends on the velocity of the two nuclei when they strike one another. If the atoms move faster, fusion is more likely. If the atoms hit head on, fusion is more likely. Cross sections for many different fusion reactions were measured mainly in the 1970s using particle beams. A beam of ions of material A was fired at material B at different speeds, and the amount of neutrons coming off was measured. Neutrons are a key product of most fusion reactions.
In most cases, the nuclei are flying around in a hot cloud, with some distribution of velocities. If the plasma is thermalized, then the distribution looks like a bell curve, or maxwellian distribution. In this case, it is useful to take the average cross section over the velocity distribution. This is entered into the volumetric fusion rate:
where:
Lawson criterion.
This equation shows that energy varies with the temperature, density, speed of collision, and fuel used. This equation was central to John Lawsons' analysis of fusion power stations working with a hot plasma. Lawson assumed an energy balance, shown below.
Net Power = Efficiency * (Fusion - Radiation Loss - Conduction Loss)
Plasma clouds lose energy through conduction and radiation. Conduction is when ions, electrons or neutrals hit a surface and transfer a portion of their kinetic energy to the atoms of the surface. Radiation is when energy leaves the cloud as light. This can be in the visible, UV, IR, or X-ray light. Radiation increases as the temperature rises. To get net power from fusion, you must overcome these losses.
Triple Product: Density, temperature, time.
The Lawson criterion argues that a machine holding a hot thermalized and quasi-neutral plasma has to meet basic criteria to overcome the radiation losses, conduction losses and a power station efficiency of 30 percent. This became known as the "triple product": the plasma density and temperature and how long it is held in. For many years, fusion research has focused on achieving the highest triple product possible. This emphasis on formula_6 as a metric of success has hurt other considerations such as cost, size, complexity and efficiency. This has led to larger, more complicated and more expensive machines such as ITER and NIF.
Plasma behavior.
Plasma can be made by fully ionizing a gas. Plasma is a fluid which conducts electricity. In bulk, it is modeled using Magnetohydrodynamics which is a combination of the Navier-Stokes equations governing fluids and Maxwell's equations governing how magnetic and electric fields behave. Fusion exploits several plasma properties, including:
Self-Organization Plasma conducts electric and magnetic fields. This means that it can self-organize. Its motions can generate fields which can, in turn, self-contain it.
Diamagnetic Plasma Plasma can generate its own internal magnetic field. This can reject an externally applied magnetic field, making it diamagnetic.
Magnetic mirrors Plasma can be reflected when it moves from a low to high density magnetic field.
Energy capture.
There are several proposals for energy capture. The simplest is using a heat cycle to heat a fluid with fusion reactions. It has been proposed to use the neutrons generated by fusion to re-generate a spent fission fuel. In addition, direct energy conversion, has been developed (at LLNL in the 1980s) as a method to maintain a voltage using the products of a fusion reaction. This has demonstrated an energy capture efficiency of 48 percent.
Possible approaches.
Magnetic confinement fusion.
Tokamak: The tokamak is the most well-developed and well-funded approach to fusion energy. As of April 2012 there were an estimated 215 experimental tokamaks either planned, decommissioned or currently operating (35 tokamaks), worldwide. This method races hot plasma around in a magnetically confined ring, with an internal current. When completed, ITER will be the world's largest tokamak.
Spherical tokamak: A variation on the tokamak with a spherical shape.
Stellarator: These are twisted rings of hot plasma. The stellarator attempts to create a natural twist plasma path, using external magnets; while Tokamaks create those magnetic fields using an internal current. Stellarators were developed by Lyman Spitzer in 1950 and have four designs: Torsatron, Heliotron, Heliac and Helias. One example is Wendelstein 7-X, a German fusion device that produced its first plasma on December 10, 2015. Wendelstein 7-X, the world's largest stellarator-type fusion device, is not intended to produce energy, but will investigate the suitability of this type of device for a power station.
Levitated Dipole Experiment (LDX): These use a solid superconducting torus. This is magnetically levitated inside the reactor chamber. The superconductor forms an axisymmetric magnetic field that contains the plasma. The LDX was developed between MIT and Columbia University after 2000 by Jay Kesner and Michael E. Mauel.
Magnetic mirror: Developed by Richard F. Post and teams at LLNL in the 1960s. Magnetic mirrors reflected hot plasma back and forth in a line. Variations included the magnetic bottle and the biconic cusp. A series of well-funded, large, mirror machines were built by the US government in the 1970s and 1980s. Mirror research continues today.
Field-reversed configuration: This device traps plasma in a self-organized quasi-stable structure; where the particle motion makes an internal magnetic field which then traps itself.
Reversed field pinch: Here the plasma moves inside a ring. It has an internal magnetic field. As you move out from the center of this ring, the magnetic field reverses direction.
Inertial confinement fusion.
Direct drive: In this technique, lasers directly blast a pellet of fuel. The goal is to start ignition, a fusion chain reaction. Ignition was first suggested by John Nuckolls, in 1972. Notable direct drive experiments have been conducted at the Laboratory for Laser Energetics, Laser Mégajoule and the GEKKO XII facilities. Good implosions require fuel pellets with close to a perfect shape in order to generate a symmetrical inward shock wave and to produce the high-density plasma.
Fast ignition: This method uses two laser blasts. The first blast compresses the fusion fuel, while the second high energy pulse ignites it. Experiments have been conducted at the Laboratory for Laser Energetics using the Omega and Omega EP systems and at the GEKKO XII laser at the institute for laser engineering in Osaka Japan.
Indirect drive: In this technique, lasers blasts a structure around the pellet of fuel. This structure is known as a Hohlraum. As it disintegrates the pellet is bathed in a more uniform x-ray light, creating better compression. The largest system using this method is the National Ignition Facility.
Magneto-inertial fusion or Magnetized Liner Inertial Fusion: This combines a laser pulse with a magnetic pinch. The pinch community refers to it as Magnetized Liner Inertial Fusion while the ICF community refers to it as Magneto-inertial fusion.
Heavy Ion Beams There are also proposals to do inertial confinement fusion with ion beams instead of laser beams. The main difference is the mass of the beam has momentum, whereas lasers do not.
Magnetic or electric pinches.
Z-Pinch: This method sends a strong current (in the z-direction) through the plasma. The current generates a magnetic field that squeezes the plasma to fusion conditions. Pinches were the first method for man-made controlled fusion. Some examples include the Dense plasma focus and the Z machine at Sandia National Laboratories.
Theta-Pinch: This method sends a current inside a plasma, in the theta direction.
Screw Pinch: This method combines a theta and z-pinch for improved stabilization.
Inertial electrostatic confinement.
Fusor: This method uses an electric field to heat ions to fusion conditions. The machine typically uses two spherical cages, a cathode inside the anode, inside a vacuum. These machines are not considered a viable approach to net power because of their high conduction and radiation losses. They are simple enough to build that amateurs have fused atoms using them.
Polywell: This designs attempts to combine magnetic confinement with electrostatic fields, to avoid the conduction losses generated by the cage.
Other.
Magnetized target fusion: This method confines hot plasma using a magnetic field and squeezes it using inertia. Examples include LANL FRX-L machine, General Fusion and the plasma liner experiment.
Uncontrolled: Fusion has been initiated by man, using uncontrolled fission explosions to ignite the so-called Hydrogen Bomb. Early proposals for fusion power included using bombs to initiate reactions.
Beam fusion: A beam of high energy particles can be fired at another beam or target and fusion will occur. This was used in the 1970s and 1980s to study the cross sections of high energy fusion reactions.
Bubble fusion: This was a supposed fusion reaction that was supposed to occur inside extraordinarily large collapsing gas bubbles, created during acoustic liquid cavitation. This approach was discredited.
Cold fusion: This is a hypothetical type of nuclear reaction that would occur at, or near, room temperature. Cold fusion has gained a reputation as Pathological science.
Muon-catalyzed fusion: Muons allow atoms to get much closer and thus reduce the kinetic energy required to initiate fusion. Muons require more energy to produce than can be obtained from muon-catalysed fusion, making this approach impractical for the generation of power.
Common tools.
Heating.
Gas must be first heated to form a plasma. This then needs to be hot enough to start fusion reactions. A number of heating schemes have been explored:
Radiofrequency Heating A radio wave is applied to the plasma, causing it to oscillate. This is basically the same concept as a microwave oven. This is also known as electron cyclotron resonance heating or Dielectric heating.
Electrostatic Heating An electric field can do work on charged ions or electrons, heating them.
Neutral Beam Injection An external source of hydrogen is ionized and accelerated by an electric field to form a charged beam which is shone through a source of neutral hydrogen gas towards the plasma which itself is ionized and contained in the reactor by a magnetic field. Some of the intermediate hydrogen gas is accelerated towards the plasma by collisions with the charged beam while remaining neutral: this neutral beam is thus unaffected by the magnetic field and so shines through it into the plasma. Once inside the plasma the neutral beam transmits energy to the plasma by collisions as a result of which it becomes ionized and thus contained by the magnetic field thereby both heating and refuelling the reactor in one operation. The remainder of the charged beam is diverted by magnetic fields onto cooled beam dumps.
Magnetic Oscillations
Measurement.
Thomson Scattering Certain wavelengths of light will scatter off a plasma. This light can be detected and used to reconstruct the plasmas' behavior. This technique can be used to find its density and temperature. It is common in Inertial confinement fusion, Tokamaks and fusors. In ICF systems, this can be done by firing a second beam into a gold foil adjacent to the target. This makes x-rays that scatter or traverse the plasma. In Tokamaks, this can be done using mirrors and detectors to reflect light across a plane (two dimensions) or in a line (one dimension).
Langmuir probe This is a metal object placed in a plasma. A potential is applied to it, giving it a positive or negative voltage against the surrounding plasma. The metal collects charged particles, drawing a current. As the voltage changes, the current changes. This makes a IV Curve. The IV-curve can be used to determine the local plasma density, potential and temperature.
Geiger counter Deuterium or tritium fusion produces neutrons. Geiger counters record the rate of neutron production, so they are an essential tool for demonstrating success.
Flux loop A loop of wire is inserted into the magnetic field. As the field passes through the loop, a current is made. The current is measured and used to find the total magnetic flux through that loop. This has been used on the National Compact Stellarator Experiment, the polywell and the LDX machines.
X-ray detector All plasma loses energy by emitting light. This covers the whole spectrum: visible, IR, UV, and X-rays. This occurs anytime a particle changes speed, for any reason. If the reason is deflection by a magnetic field, the radiation is Cyclotron radiation at low speeds and Synchrotron radiation at high speeds. If the reason is deflection by another particle, plasma radiates X-rays, known as Bremsstrahlung radiation. X-rays are termed in both hard and soft, based on their energy.
Power production.
Steam turbines It has been proposed that steam turbines be used to convert the heat from the fusion chamber into electricity. The heat is transferred into a working fluid that turns into steam, driving electric generators.
Neutron blankets Deuterium and tritium fusion generates neutrons. This varies by technique (NIF has a record of 3E14 neutrons per second while a typical fusor produces 1E5–1E9 neutrons per second). It has been proposed to use these neutrons as a way to regenerate spent fission fuel or as a way to breed tritium from a liquid lithium blanket.
Direct conversion This is a method where the kinetic energy of a particle is converted into voltage. It was first suggested by Richard F. Post in conjunction with magnetic mirrors, in the late sixties. It has also been suggested for Field-Reversed Configurations. The process takes the plasma, expands it, and converts a large fraction of the random energy of the fusion products into directed motion. The particles are then collected on electrodes at various large electrical potentials. This method has demonstrated an experimental efficiency of 48 percent.
Confinement.
Confinement refers to all the conditions necessary to keep a plasma dense and hot long enough to undergo fusion. Here are some general principles.
To produce self-sustaining fusion, the energy released by the reaction (or at least a fraction of it) must be used to heat new reactant nuclei and keep them hot long enough that they also undergo fusion reactions.
Unconfined.
The first human-made, large-scale fusion reaction was the test of the hydrogen bomb, Ivy Mike, in 1952. As part of the PACER project, it was once proposed to use hydrogen bombs as a source of power by detonating them in underground caverns and then generating electricity from the heat produced, but such a power station is unlikely ever to be constructed.
Magnetic confinement.
At the temperatures required for fusion, the fuel is heated to a plasma state. In this state it has a very good electrical conductivity. This opens the possibility of confining the plasma with magnetic fields. This is the case of magnetized plasma, where the magnetic fields and plasma intermix. This is generally known as magnetic confinement. The field lines put a Lorentz force on the plasma. The force works perpendicular to the magnetic fields, so one problem in magnetic confinement is preventing the plasma from leaking out the ends of the field lines. A general measure of magnetic trapping in fusion is the beta ratio:
formula_7 
This is the ratio of the externally applied field to the internal pressure of the plasma. A value of 1 is ideal trapping. Some examples of beta vales include:
Magnetic Mirror One example of magnetic confinement is with the magnetic mirror effect. If a particle follows the field line and enters a region of higher field strength, the particles can be reflected. There are several devices that try to use this effect. The most famous was the magnetic mirror machines, which was a series of large, expensive devices built at the Lawrence Livermore National Laboratory from the 1960s to mid 1980s. Some other examples include the magnetic bottles and Biconic cusp. Because the mirror machines were straight, they had some advantages over a ring shape. First, mirrors would easier to construct and maintain and second direct conversion energy capture, was easier to implement. As the confinement achieved in experiments was poor, this approach was abandoned.
Magnetic Loops Another example of magnetic confinement is to bend the field lines back on themselves, either in circles or more commonly in nested toroidal surfaces. The most highly developed system of this type is the "tokamak", with the "stellarator" being next most advanced, followed by the Reversed field pinch. Compact toroids, especially the "Field-Reversed Configuration" and the spheromak, attempt to combine the advantages of toroidal magnetic surfaces with those of a simply connected (non-toroidal) machine, resulting in a mechanically simpler and smaller confinement area.
Inertial confinement.
Inertial confinement is the use of rapidly imploding shell to heat and confine plasma. The shell is imploded using a direct laser blast (direct drive) or a secondary x-ray blast (indirect drive) or heavy ion beams. Theoretically, fusion using lasers would be done using tiny pellets of fuel that explode several times a second. To induce the explosion, the pellet must be compressed to about 30 times solid density with energetic beams. If direct drive is used—the beams are focused directly on the pellet—it can in principle be very efficient, but in practice is difficult to obtain the needed uniformity. The alternative approach, indirect drive, uses beams to heat a shell, and then the shell radiates x-rays, which then implode the pellet. The beams are commonly laser beams, but heavy and light ion beams and electron beams have all been investigated.
Electrostatic confinement.
There are also electrostatic confinement fusion devices. These devices confine ions using electrostatic fields. The best known is the Fusor. This device has an cathode inside an anode wire cage. Positive ions fly towards the negative inner cage, and are heated by the electric field in the process. If they miss the inner cage they can collide and fuse. Ions typically hit the cathode, however, creating prohibitory high conduction losses. Also, fusion rates in fusors are very low because of competing physical effects, such as energy loss in the form of light radiation. Designs have been proposed to avoid the problems associated with the cage, by generating the field using a non-neutral cloud. These include a plasma oscillating device, a magnetically-shielded-grid a penning trap and the polywell. The technology is relatively immature, however, and many scientific and engineering questions remain.
History of research.
1920s.
Research into nuclear fusion started in the early part of the 20th century. In 1920 the British physicist Francis William Aston discovered that the total mass equivalent of four hydrogen atoms (two protons and two neutrons) are heavier than the total mass of one helium atom (He-4), which implied that net energy can be released by combining hydrogen atoms together to form helium, and provided the first hints of a mechanism by which stars could produce energy in the quantities being measured. Through the 1920s, Arthur Stanley Eddington became a major proponent of the proton–proton chain reaction (PP reaction) as the primary system running the Sun.
1930s.
A theory was verified by Hans Bethe in 1939 showing that beta decay and quantum tunneling in the Sun's core might convert one of the protons into a neutron and thereby producing deuterium rather than a diproton. The deuterium would then fuse through other reactions to further increase the energy output. For this work, Bethe won the Nobel Prize in Physics.
1940s.
In 1942, nuclear fusion research was subsumed into the Manhattan Project when the secrecy surrounding the field obscured by the science. The first patent related to a fusion reactor was registered in 1946 by the United Kingdom Atomic Energy Authority. The inventors were Sir George Paget Thomson and Moses Blackman. This was the first detailed examination of the Z-pinch concept.
Z-pinch is based on the fact that plasmas are electrically conducting. Running a current through the plasma, will generate a magnetic field around the plasma. This field will, according to Lenz's law, create an inward directed force that causes the plasma to collapse inward, raising its density. Denser plasmas generate denser magnetic fields, increasing the inward force, leading to a chain reaction. If the conditions are correct, this can lead to the densities and temperatures needed for fusion. The difficulty is getting the current into the plasma, which would normally melt any sort of mechanical electrode. A solution emerges again because of the conducting nature of the plasma; by placing the plasma in the middle of an electromagnet, induction can be used to generate the current.
Starting in 1947, two UK teams carried out small experiments and began building a series of ever-larger experiments. When the Huemul results hit the news (see below), James L. Tuck, a UK physicist working at Los Alamos, introduced the pinch concept in the US and produced a series of machines known as the Perhapsatron. The Soviet Union, unbeknownst to the West, was also building a series of similar machines. All of these devices quickly demonstrated a series of instabilities when the pinch was applied. This broke up the plasma column long before it reached the densities and temperatures required for fusion.
1950s.
The first successful man-made fusion device was the boosted fission weapon tested in 1951 in the Greenhouse Item test. This was followed by true fusion weapons in 1952's Ivy Mike, and the first practical examples in 1954's Castle Bravo. This was uncontrolled fusion. In these devices, the energy released by the fission explosion is used to compress and heat fusion fuel, starting a fusion reaction. Fusion releases neutrons. These neutrons hit the surrounding fission fuel, causing the atoms to split apart much faster than normal fission processes—almost instantly by comparison. This increases the effectiveness of bombs: normal fission weapons blow themselves apart before all their fuel is used; fusion/fission weapons do not have this practical upper limit.
In 1949 an expatriate German, Ronald Richter, proposed the Huemul Project in Argentina, announcing positive results in 1951. These turned out to be fake, but it prompted considerable interest in the concept as a whole. In particular, it prompted Lyman Spitzer to begin considering ways to solve some of the more obvious problems involved in confining a hot plasma, and, unaware of the z-pinch efforts, he developed a new solution to the problem known as the stellarator. Spitzer applied to the US Atomic Energy Commission for funding to build a test device. During this period, Jim Tuck who had worked with the UK teams had been introducing the z-pinch concept to his coworkers at his new job at Los Alamos National Laboratory (LANL). When he heard of Spitzer's pitch for funding, he applied to build a machine of his own, the Perhapsatron.
Spitzer's idea won funding and he began work on the stellarator under the code name Project Matterhorn. His work led to the creation of the Princeton Plasma Physics Laboratory. Tuck returned to LANL and arranged local funding to build his machine. By this time, however, it was clear that all of the pinch machines were suffering from the same issues involving stability, and progress stalled. In 1953, Tuck and others suggested a number of solutions to the stability problems. This led to the design of a second series of pinch machines, led by the UK ZETA and Sceptre devices.
Spitzer had planned an aggressive development project of four machines, A, B, C, and D. A and B were small research devices, C would be the prototype of a power-producing machine, and D would be the prototype of a commercial device. A worked without issue, but even by the time B was being used it was clear the stellarator was also suffering from instabilities and plasma leakage. Progress on C slowed as attempts were made to correct for these problems.
By the mid-1950s it was clear that the simple theoretical tools being used to calculate the performance of all fusion machines were simply not predicting their actual behavior. Machines invariably leaked their plasma from their confinement area at rates far higher than predicted. In 1954, Edward Teller held a gathering of fusion researchers at the Princeton Gun Club, near the Project Matterhorn (now known as Project Sherwood) grounds. Teller started by pointing out the problems that everyone was having, and suggested that any system where the plasma was confined within concave fields was doomed to fail. Attendees remember him saying something to the effect that the fields were like rubber bands, and they would attempt to snap back to a straight configuration whenever the power was increased, ejecting the plasma. He went on to say that it appeared the only way to confine the plasma in a stable configuration would be to use convex fields, a "cusp" configuration.
When the meeting concluded, most of the researchers quickly turned out papers saying why Teller's concerns did not apply to their particular device. The pinch machines did not use magnetic fields in this way at all, while the mirror and stellarator seemed to have various ways out. This was soon followed by a paper by Martin David Kruskal and Martin Schwarzschild discussing pinch machines, however, which demonstrated instabilities in those devices were inherent to the design.
The largest "classic" pinch device was the ZETA, including all of these suggested upgrades, starting operations in the UK in 1957. In early 1958, John Cockcroft announced that fusion had been achieved in the ZETA, an announcement that made headlines around the world. When physicists in the US expressed concerns about the claims they were initially dismissed. US experiments soon demonstrated the same neutrons, although temperature measurements suggested these could not be from fusion reactions. The neutrons seen in the UK were later demonstrated to be from different versions of the same instability processes that plagued earlier machines. Cockcroft was forced to retract the fusion claims, and the entire field was tainted for years. ZETA ended its experiments in 1968.
The first controlled fusion experiment was accomplished using Scylla I at the Los Alamos National Laboratory in 1958. This was a pinch machine, with a cylinder full of deuterium. Electric current shot down the sides of the cylinder. The current made magnetic fields that compressed the plasma to 15 million degrees Celsius, squeezed the gas, fused it and produced neutrons.
In 1950–1951 I.E. Tamm and A.D. Sakharov in the Soviet Union, first discussed a tokamak-like approach. Experimental research on those designs began in 1956 at the Kurchatov Institute in Moscow by a group of Soviet scientists led by Lev Artsimovich. The tokamak essentially combined a low-power pinch device with a low-power simple stellarator. The key was to combine the fields in such a way that the particles orbited within the reactor a particular number of times, today known as the "safety factor". The combination of these fields dramatically improved confinement times and densities, resulting in huge improvements over existing devices.
1960s.
A key plasma physics text was published by Lyman Spitzer at Princeton in 1963. Spitzer took the ideal gas laws and adopted them to an ionized plasma, developing many of the fundamental equations used to model a plasma.
Laser fusion was suggested in 1962 by scientists at Lawrence Livermore National Laboratory, shortly after the invention of the laser itself in 1960. At the time, Lasers were low power machines, but low-level research began as early as 1965. Laser fusion, formally known as inertial confinement fusion, involves imploding a target by using laser beams. There are two ways to do this: indirect drive and direct drive. In direct drive, the laser blasts a pellet of fuel. In indirect drive, the lasers blast a structure around the fuel. This makes x-rays that squeeze the fuel. Both methods compress the fuel so that fusion can take place.
At the 1964 World's Fair, the public was given its first demonstration of nuclear fusion. The device was a θ-pinch from General Electric. This was similar to the Scylla machine developed earlier at Los Alamos.
The magnetic mirror was first published in 1967 by Richard F. Post and many others at the Lawrence Livermore National Laboratory. The mirror consisted of two large magnets arranged so they had strong fields within them, and a weaker, but connected, field between them. Plasma introduced in the area between the two magnets would "bounce back" from the stronger fields in the middle.
The A.D. Sakharov group constructed the first tokamaks, the most successful being the T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, producing the world's first quasistationary fusion reaction. When this were first announced, the international community was highly skeptical. A British team was invited to see T-3, however, and after measuring it in depth they released their results that confirmed the Soviet claims. A burst of activity followed as many planned devices were abandoned and new tokamaks were introduced in their place — the C model stellarator, then under construction after many redesigns, was quickly converted to the Symmetrical Tokamak.
In his work with vacuum tubes, Philo Farnsworth observed that electric charge would accumulate in regions of the tube. Today, this effect is known as the Multipactor effect. Farnsworth reasoned that if ions were concentrated high enough they could collide and fuse. In 1962, he filed a patent on a design using a positive inner cage to concentrate plasma, in order to achieve nuclear fusion. During this time, Robert L. Hirsch joined the Farnsworth Television labs and began work on what became the fusor. Hirsch patented the design in 1966 and published the design in 1967.
1970s.
In 1972, John Nuckolls outlined the idea of ignition. This is a fusion chain reaction. Hot helium made during fusion reheats the fuel and starts more reactions. John argued that ignition would require lasers of about 1 kJ. This turned out to be wrong. Nuckolls's paper started a major development effort. Several laser systems were built at LLNL. These included the argus, the Cyclops, the Janus, the long path, the Shiva laser and the Nova in 1984. This prompted the UK to build the Central Laser Facility in 1976.
During this time, great strides in understanding the tokamak system were made. A number of improvements to the design are now part of the "advanced tokamak" concept, which includes non-circular plasma, internal diverters and limiters, often superconducting magnets, and operate in the so-called "H-mode" island of increased stability. Two other designs have also become fairly well studied; the compact tokamak is wired with the magnets on the inside of the vacuum chamber, while the spherical tokamak reduces its cross section as much as possible.
In 1974 a study of the ZETA results demonstrated an interesting side-effect; after an experimental run ended, the plasma would enter a short period of stability. This led to the reversed field pinch concept, which has seen some level of development since. On May 1, 1974, the KMS fusion company (founded by Kip Siegel) achieves the world's first laser induced fusion in a deuterium-tritium pellet.
In the mid-1970s, Project PACER, carried out at Los Alamos National Laboratory (LANL) explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. It would also require a large, continuous supply of nuclear bombs, however, making the economics of such a system rather questionable.
In 1976, the two beam Argus laser becomes operational at livermore. In 1977, The 20 beam Shiva laser at Livermore is completed, capable of delivering 10.2 kilojoules of infrared energy on target. At a price of $25 million and a size approaching that of a football field, Shiva is the first of the megalasers . That same year, the JET project is approved by the European Commission and a site is selected.
1980s.
As a result of advocacy, the cold war, and the 1970s energy crisis a massive magnetic mirror program was funded by the US federal government in the late 1970s and early 1980s. This program resulted in a series of large magnetic mirror devices including: 2X, Baseball I, Baseball II, the Tandem Mirror Experiment, the Tandem mirror experiment upgrade, the Mirror Fusion Test Facility and the MFTF-B. These machines were built and tested at Livermore from the late 1960s to the mid 1980s. A number of institutions collaborated on these machines, conducting experiments. These included the Institute for Advanced Study and the University of Wisconsin–Madison. The last machine, the Mirror Fusion Test Facility cost 372 million dollars and was, at that time, the most expensive project in Livermore history. It opened on February 21, 1986 and was promptly shut down. The reason given was to balance the United States federal budget. This program was supported from within the Carter and early Reagan administrations by Edwin E. Kintner, a US Navy captain, under Alvin Trivelpiece.
In Laser fusion progressed: in 1983, the NOVETTE laser was completed. The following December 1984, the ten beam NOVA laser was finished. Five years later, NOVA would produce a maximum of 120 kilojoules of infrared light, during a nanosecond pulse . Meanwhile, efforts focused on either fast delivery or beam smoothness. Both tried to deliver the energy uniformly to implode the target. One early problem was that the light in the infrared wavelength, lost lots of energy before hitting the fuel. Breakthroughs were made at the Laboratory for Laser Energetics at the University of Rochester. Rochester scientists used frequency-tripling crystals to transform the infrared laser beams into ultraviolet beams. In 1985, Donna Strickland and Gérard Mourou invented a method to amplify lasers pulses by "chirping". This method changes a single wavelength into a full spectrum. The system then amplifies the laser at each wavelength and then reconstitutes the beam into one color. Chirp pulsed amplification became instrumental in building the National Ignition Facility and the Omega EP system. Most research into ICF was towards weapons research, because the implosion is relevant to nuclear weapons.
During this time Los Alamos National Laboratory constructed a series of laser facilities. This included Gemini (a two beam system), Helios (eight beams), Antares (24 beams) and Aurora (96 beams). The program ended in the early nineties with a cost on the order of one billion dollars.
In 1987, Akira Hasegawa noticed that in a dipolar magnetic field, fluctuations tended compress the plasma without energy loss. This effect was noticed in data taken by Voyager 2, when it encountered Uranus. This observation would become the basis for a fusion approach known as the Levitated dipole.
In Tokamaks, the Tore Supra was under construction over the middle of the eighties (1983 to 1988). This was a Tokamak built in Cadarache, France. In 1983, the JET was completed and first plasmas achieved. In 1985, the Japanese tokamak, JT-60 was completed. In 1988, the T-15 a Soviet tokamak was completed. It was the first industrial fusion reactor to use superconducting magnets to control the plasma. These were Helium cooled.
In 1989, Pons and Fleischmann submitted papers to the "Journal of Electroanalytical Chemistry" claiming that they had observed fusion in a room temperature device and disclosing their work in a press release. Some scientists reported excess heat, neutrons, tritium, helium and other nuclear effects in so-called cold fusion systems, which for a time gained interest as showing promise. Hopes fell when replication failures were weighed in view of several reasons cold fusion is not likely to occur, the discovery of possible sources of experimental error, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. However, a small community of researchers continues to investigate cold fusion claiming to replicate Fleishmann and Pons' results including nuclear reaction byproducts. Claims related to cold fusion are largely disbelieved in the mainstream scientific community. In 1989, the majority of a review panel organized by the US Department of Energy (DOE) found that the evidence for the discovery of a new nuclear process was not persuasive. A second DOE review, convened in 2004 to look at new research, reached conclusions similar to the first.
In 1984, Martin Peng of ORNL proposed an alternate arrangement of the magnet coils that would greatly reduce the aspect ratio while avoiding the erosion issues of the compact tokamak: a Spherical tokamak. Instead of wiring each magnet coil separately, he proposed using a single large conductor in the center, and wiring the magnets as half-rings off of this conductor. What was once a series of individual rings passing through the hole in the center of the reactor was reduced to a single post, allowing for aspect ratios as low as 1.2. The ST concept appeared to represent an enormous advance in tokamak design. However, it was being proposed during a period when US fusion research budgets were being dramatically scaled back. ORNL was provided with funds to develop a suitable central column built out of a high-strength copper alloy called "Glidcop". However, they were unable to secure funding to build a demonstration machine, "STX". Failing to build an ST at ORNL, Peng began a worldwide effort to interest other teams in the ST concept and get a test machine built. One way to do this quickly would be to convert a spheromak machine to the Spherical tokamak layout. Peng's advocacy also caught the interest of Derek Robinson, of the United Kingdom Atomic Energy Authority fusion center at Culham. Robinson was able to gather together a team and secure funding on the order of 100,000 pounds to build an experimental machine, the Small Tight Aspect Ratio Tokamak, or START. Several parts of the machine were recycled from earlier projects, while others were loaned from other labs, including a 40 keV neutral beam injector from ORNL. Construction of START began in 1990, it was assembled rapidly and started operation in January 1991.
1990s.
In 1991 the Preliminary Tritium Experiment at the Joint European Torus in England achieved the world’s first controlled release of fusion power.
In 1992, a major article was published in Physics Today by Robert McCory at the Laboratory for laser energetics outlying the current state of ICF and advocating for a national ignition facility. This was followed up by a major review article, from John Lindl in 1995, advocating for NIF. During this time a number of ICF subsystems were developing, including target manufacturing, cryogenic handling systems, new laser designs (notably the NIKE laser at NRL) and improved diagnostics like time of flight analyzers and Thomson scattering. This work was done at the NOVA laser system, General Atomics, Laser Mégajoule and the GEKKO XII system in Japan. Through this work and lobbying by groups like the fusion power associates and John Sethian at NRL, a vote was made in congress, authorizing funding for the NIF project in the late nineties.
In the early nineties, theory and experimental work regarding fusors and polywells was published. In response, Todd Rider at MIT developed general models of these devices. Rider argued that all plasma systems at thermodynamic equilibrium were fundamentally limited. In 1995, William Nevins published a criticism arguing that the particles inside fusors and polywells would build up angular momentum, causing the dense core to degrade.
In 1995, the University of Wisconsin–Madison built a large fusor, known as HOMER, which is still in operation. Meanwhile, Dr George H. Miley at Illinois, built a small fusor that has produced neutrons using deuterium gas and discovered the "star mode" of fusor operation. The following year, the first "US-Japan Workshop on IEC Fusion", was conducted. At this time in Europe, an IEC device was developed as a commercial neutron source by Daimler-Chrysler and NSD Fusion.
In 1996, the Z-machine was upgraded and opened to the public by the US Army in August 1998 in Scientific American. The key attributes of Sandia’s Z machine are its 18 million amperes and a discharge time of less than 100 nanoseconds. This generates a magnetic pulse, inside a large oil tank, this strikes an array of tungsten wires called a "liner". Firing the Z-machine has become a way to test very high energy, high temperature (2 billion degrees) conditions. In 1996, the Tore Supra creates a plasma for two minutes with a current of almost 1 million amperes driven non-inductively by 2.3 MW of lower hybrid frequency waves. This is 280 MJ of injected and extracted energy. This result was possible because of the actively cooled plasma-facing components 
In 1997, JET produced a peak of 16.1MW of fusion power (65% of input power), with fusion power of over 10MW sustained for over 0.5 sec. Its successor, the International Thermonuclear Experimental Reactor (ITER), was officially announced as part of a seven-party consortium (six countries and the EU). ITER is designed to produce ten times more fusion power than the power put into the plasma. ITER is currently under construction in Cadarache, France.
In the late nineties, a team at Columbia University and MIT developed the Levitated dipole a fusion device which consisted of a superconducting electromagnet, floating in a saucer shaped vacuum chamber. Plasma swirled around this donut and fused along the center axis.
2000s.
In the March 8, 2002 issue of the peer-reviewed journal "Science", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. Taleyarkhan was later found guilty of misconduct, the Office of Naval Research debarred him for 28 months from receiving Federal Funding, and his name was listed in the 'Excluded Parties List'.
"Fast ignition" was developed in the late nineties, and was part of a push by the Laboratory for Laser Energetics for building the Omega EP system. This system was finished in 2008. Fast ignition showed such dramatic power savings that ICF appears to be a useful technique for energy production. There are even proposals to build an experimental facility dedicated to the fast ignition approach, known as HiPER.
In April 2005, a team from UCLA announced it had devised a way of producing fusion using a machine that "fits on a lab bench", using lithium tantalate to generate enough voltage to smash deuterium atoms together. The process, however, does not generate net power (see Pyroelectric fusion). Such a device would be useful in the same sort of roles as the fusor. In 2006, China's EAST test reactor is completed. This was the first tokamak to use superconducting magnets to generate both the toroidal and poloidal fields.
In the early 2000s, Researchers at LANL reasoned that a plasma oscillating could be at local thermodynamic equilibrium. This prompted the POPS and Penning trap designs. At this time, researchers at MIT became interested in fusors for space propulsion and powering space vehicles. Specifically, researchers developed fusors with multiple inner cages. Greg Piefer graduated from Madison and founded Phoenix Nuclear Labs, a company that developed the fusor into a neutron source for the mass production of medical isotopes. Robert Bussard began speaking openly about the Polywell in 2006. He attempted to generate interest in the research, before his death. In 2008, Taylor Wilson achieved notoriety for achieving nuclear fusion at 14, with a homemade fusor.
In 2009, a high-energy laser system, the National Ignition Facility (NIF), was finished in the US, which can heat hydrogen atoms to temperatures only existing in nature in the cores of stars. The new laser is expected to have the ability to produce, for the first time, more energy from controlled, inertially confined nuclear fusion than was required to initiate the reaction.
2010s.
In 2010, NIF researchers were conducting a series of "tuning" shots to determine the optimal target design and laser parameters for high-energy ignition experiments with fusion fuel in the following months. Two firing tests were performed on October 31, 2010 and November 2, 2010. In early 2012, NIF director Mike Dunne expected the laser system to generate fusion with net energy gain by the end of 2012. However, it was delayed and not achieved by that date.
Inertial (laser) confinement is being developed at the United States National Ignition Facility (NIF) based at Lawrence Livermore National Laboratory in California, the French Laser Mégajoule, and the planned European Union High Power laser Energy Research (HiPER) facility. NIF reached initial operational status in 2010 and has been in the process of increasing the power and energy of its "shots", with fusion ignition tests to follow. A three-year goal announced in 2009 to produce net energy from fusion by 2012 was missed; in September 2013, however, the facility announced a significant milestone from an August 2013 test that produced more energy from the fusion reaction than had been provided to the fuel pellet. This was reported as the first time this had been accomplished in fusion power research. The facility reported that their next step involved improving the system to prevent the hohlraum from either breaking up asymmetrically or too soon.
A 2012 paper demonstrated that a dense plasma focus had achieved temperatures of 1.8 billion degrees Celsius, sufficient for boron fusion, and that fusion reactions were occurring primarily within the contained plasmoid, a necessary condition for net power. The focus consists of two coaxial cylindrical electrodes made from copper or beryllium and housed in a vacuum chamber containing a low-pressure fusible gas. An electrical pulse is applied across the electrodes, heating the gas into a plasma. The current forms into a minuscule vortex along the axis of the machine, which then kinks into a cage of current with an associated magnetic field. The cage of current and magnetic-field-entrapped plasma is called a plasmoid. The acceleration of the electrons about the magnetic field lines heats the nuclei within the plasmoid to fusion temperatures.
In April 2014, Lawrence Livermore National Laboratory ended the Laser Inertial Fusion Energy (LIFE) program and redirected their efforts towards NIF. In August 2014, Phoenix Nuclear Labs announced the sale of a high-yield neutron generator that could sustain 5×1011 deuterium fusion reactions per second over a 24-hour period. In October 2014, Lockheed Martin's Skunk Works announced the development of a high-beta fusion reactor that they hope to yield a functioning 100-megawatt prototype by 2017 and to be ready for regular operation by 2022.
Deep-space exploration, as well as higher-velocity lower-cost space transport services in general would be enabled by this compact fusion reactor technology.
In January 2015, the polywell was presented at Microsoft Research.
In August, 2015, MIT announced a tokamak it named ARC fusion reactor design using rare-earth barium-copper oxide (REBCO) superconducting tapes to produce high-magnetic field coils that it claimed produce comparable magnetic field strength in a smaller configuration than other designs.
In October 2015, researchers at the Max Planck Institute of Plasma Physics completed building the largest stellarator to date, named Wendelstein 7-X, and on December 10, they successfully produced the first Helium plasma. It was followed on February 3, 2016 by the first Hydrogen plasma thereby starting the experimental journey of this sophisticated new device. With plasma discharges lasting up to 30 minutes, Wendelstein 7-X will try to demonstrate the essential stellarator useful attribute, continuous operation of a high temperature hydrogen plasma.
Fuels.
By firing particle beams at targets, many fusion reactions have been tested, while the fuels considered for power have all been light elements like the isotopes of hydrogen—deuterium and tritium. Other reactions like the deuterium and Helium3 reaction or the Helium3 and Helium3 reactions, would require a supply of Helium3. This can either come from other nuclear reactions or from extraterrestrial sources. Finally, researchers hope to do the p- reaction, because it does not directly produce neutrons, though side reactions can.
Deuterium, tritium.
The easiest nuclear reaction, at the lowest energy, is:
This reaction is common in research, industrial and military applications, usually as a convenient source of neutrons. Deuterium is a naturally occurring isotope of hydrogen and is commonly available. The large mass ratio of the hydrogen isotopes makes their separation easy compared to the difficult uranium enrichment process. Tritium is a natural isotope of hydrogen, but because it has a short half-life of 12.32 years, it is hard to find, store, produce, and is expensive. Consequently, the deuterium-tritium fuel cycle requires the breeding of tritium from lithium using one of the following reactions:
The reactant neutron is supplied by the D-T fusion reaction shown above, and the one that has the greatest yield of energy. The reaction with 6Li is exothermic, providing a small energy gain for the reactor. The reaction with 7Li is endothermic but does not consume the neutron. At least some 7Li reactions are required to replace the neutrons lost to absorption by other elements. Most reactor designs use the naturally occurring mix of lithium isotopes.
Several drawbacks are commonly attributed to D-T fusion power:
The neutron flux expected in a commercial D-T fusion reactor is about 100 times that of current fission power reactors, posing problems for material design. After a series of D-T tests at JET, the vacuum vessel was sufficiently radioactive that remote handling was required for the year following the tests.
In a production setting, the neutrons would be used to react with lithium in order to create more tritium. This also deposits the energy of the neutrons in the lithium, which would then be transferred to drive electrical production. The lithium neutron absorption reaction protects the outer portions of the reactor from the neutron flux. Newer designs, the advanced tokamak in particular, also use lithium inside the reactor core as a key element of the design. The plasma interacts directly with the lithium, preventing a problem known as "recycling". The advantage of this design was demonstrated in the Lithium Tokamak Experiment.
Deuterium.
This is the second easiest fusion reaction, fusing deuterium with itself. The reaction has two branches that occur with nearly equal probability:
This reaction is also common in research. The optimum energy to initiate this reaction is 15 keV, only slightly higher than the optimum for the D-T reaction. The first branch does not produce neutrons, but it does produce tritium, so that a D-D reactor will not be completely tritium-free, even though it does not require an input of tritium or lithium. Unless the tritons can be quickly removed, most of the tritium produced would be burned before leaving the reactor, which would reduce the handling of tritium, but would produce more neutrons, some of which are very energetic. The neutron from the second branch has an energy of only , whereas the neutron from the D-T reaction has an energy of , resulting in a wider range of isotope production and material damage. When the tritons are removed quickly while allowing the 3He to react, the fuel cycle is called "tritium suppressed fusion" The removed tritium decays to 3He with a 12.5 year half life. By recycling the 3He produced from the decay of tritium back into the fusion reactor, the fusion reactor does not require materials resistant to fast neutrons.
Assuming complete tritium burn-up, the reduction in the fraction of fusion energy carried by neutrons would be only about 18%, so that the primary advantage of the D-D fuel cycle is that tritium breeding would not be required. Other advantages are independence from scarce lithium resources and a somewhat softer neutron spectrum. The disadvantage of D-D compared to D-T is that the energy confinement time (at a given pressure) must be 30 times longer and the power produced (at a given pressure and volume) would be 68 times less .
Assuming complete removal of tritium and recycling of 3He, only 6% of the fusion energy is carried by neutrons. The tritium-suppressed D-D fusion requires an energy confinement that is 10 times longer compared to D-T and a plasma temperature that is twice as high.
Deuterium, helium 3.
A second-generation approach to controlled fusion power involves combining helium-3 (3He) and deuterium (2H):
This reaction produces a helium-4 nucleus (4He) and a high-energy proton. As with the p-11B aneutronic fusion fuel cycle, most of the reaction energy is released as charged particles, reducing activation of the reactor housing and potentially allowing more efficient energy harvesting (via any of several speculative technologies). In practice, D-D side reactions produce a significant number of neutrons, resulting in p-11B being the preferred cycle for aneutronic fusion.
Proton, boron 11.
If aneutronic fusion is the goal, then the most promising candidate may be the Hydrogen-1 (proton)/boron reaction, which releases alpha (helium) particles, but does not rely on neutron scattering for energy transfer.
Under reasonable assumptions, side reactions will result in about 0.1% of the fusion power being carried by neutrons. At 123 keV, the optimum temperature for this reaction is nearly ten times higher than that for the pure hydrogen reactions, the energy confinement must be 500 times better than that required for the D-T reaction, and the power density will be 2500 times lower than for D-T.
Because the confinement properties of conventional approaches to fusion such as the tokamak and laser pellet fusion are marginal, most proposals for aneutronic fusion are based on radically different confinement concepts, such as the Polywell and the Dense Plasma Focus. Results have been extremely promising:
Material selection.
Considerations.
Any power station using hot plasma, is going to have plasma facing walls. In even the simplest plasma approaches, the material will get blasted with matter and energy. This leads to a minimum list of considerations, including dealing with:
Depending on the approach, these effects may be higher or lower than typical fission reactors like the pressurized water reactor (PWR). One estimate put the radiation at 100 times the (PWR). Materials need to be selected or developed that can withstand these basic conditions. Depending on the approach, however, there may be other considerations such as electrical conductivity, magnetic permeability and mechanical strength. There is also a need for materials whose primary components and impurities do not result in long-lived radioactive wastes.
Durability.
For long term use, each atom in the wall is expected to be hit by a neutron and displaced about a hundred times before the material is replaced. High-energy neutrons will produce hydrogen and helium by way of various nuclear reactions that tends to form bubbles at grain boundaries and result in swelling, blistering or embrittlement.
Selection.
One can choose either a low-Z material, such as graphite or beryllium, or a high-Z material, usually tungsten with molybdenum as a second choice. Use of liquid metals (lithium, gallium, tin) has also been proposed, e.g., by injection of 1–5 mm thick streams flowing at 10 m/s on solid substrates.
If graphite is used, the gross erosion rates due to physical and chemical sputtering would be many meters per year, so one must rely on redeposition of the sputtered material. The location of the redeposition will not exactly coincide with the location of the sputtering, so one is still left with erosion rates that may be prohibitive. An even larger problem is the tritium co-deposited with the redeposited graphite. The tritium inventory in graphite layers and dust in a reactor could quickly build up to many kilograms, representing a waste of resources and a serious radiological hazard in case of an accident. The consensus of the fusion community seems to be that graphite, although a very attractive material for fusion experiments, cannot be the primary PFC material in a commercial reactor.
The sputtering rate of tungsten by the plasma fuel ions is orders of magnitude smaller than that of carbon, and tritium is much less incorporated into redeposited tungsten, making this a more attractive choice. On the other hand, tungsten impurities in a plasma are much more damaging than carbon impurities, and self-sputtering of tungsten can be high, so it will be necessary to ensure that the plasma in contact with the tungsten is not too hot (a few tens of eV rather than hundreds of eV). Tungsten also has disadvantages in terms of eddy currents and melting in off-normal events, as well as some radiological issues.
Safety and the environment.
Accident potential.
Nuclear fusion is unlike nuclear fission: fusion requires extremely precise and controlled temperature, pressure and magnetic field parameters for any net energy to be produced. If a reactor suffers damage or loses even a small degree of required control, fusion reactions and heat generation would rapidly cease. Additionally, fusion reactors contain relatively small amounts of fuel, enough to "burn" for minutes, or in some cases, microseconds. Unless they are actively refueled, the reactions will quickly end. Therefore, fusion reactors are considered extremely safe.
Runaway reactions cannot occur in a fusion reactor. The plasma is burnt at optimal conditions, and any significant change will quench the reactions. The reaction process is so delicate that this level of safety is inherent. Although the plasma in a fusion power station is expected to have a volume of or more, the plasma density is low and the total amount of fusion fuel in the vessel typically only a few grams. If the fuel supply is closed, the reaction stops within seconds. In comparison, a fission reactor is typically loaded with enough fuel for several months or years, and no additional fuel is necessary to continue the reaction. It is this large amount of fuel that gives rise to the possibility of a meltdown; nothing analogous exists in a fusion reactor.
In the magnetic approach, strong fields are developed in coils that are held in place mechanically by the reactor structure. Failure of this structure could release this tension and allow the magnet to "explode" outward. The severity of this event would be similar to any other industrial accident or an MRI machine quench/explosion, and could be effectively stopped with a containment building similar to those used in existing (fission) nuclear generators. The laser-driven inertial approach is generally lower-stress because of the increased size of the reaction chamber. Although failure of the reaction chamber is possible, simply stopping fuel delivery would prevent any sort of catastrophic failure.
Most reactor designs rely on liquid lithium as both a coolant and a method for converting stray neutrons from the reaction into tritium, which is fed back into the reactor as fuel. Lithium is highly flammable, and in the case of a fire it is possible that the lithium stored on-site could be burned up and escape. In this case, the tritium contents of the lithium would be released into the atmosphere, posing a radiation risk. Calculations suggest that at about 1 kg the total amount of tritium and other radioactive gases in a typical power station would be so small that they would have diluted to legally acceptable limits by the time they blew as far as the station's perimeter fence.
The likelihood of "small industrial" accidents including the local release of radioactivity and injury to staff cannot be estimated yet. These would include accidental releases of lithium or tritium or mis-handling of decommissioned radioactive components of the reactor itself.
Magnet quench.
A quench is an abnormal termination of magnet operation that occurs when part of the superconducting coil enters the normal (resistive) state. This can occur because the field inside the magnet is too large, the rate of change of field is too large (causing eddy currents and resultant heating in the copper support matrix), or a combination of the two.
More rarely a defect in the magnet can cause a quench. When this happens, that particular spot is subject to rapid Joule heating from the enormous current, which raises the temperature of the surrounding regions. This pushes those regions into the normal state as well, which leads to more heating in a chain reaction. The entire magnet rapidly becomes normal (this can take several seconds, depending on the size of the superconducting coil). This is accompanied by a loud bang as the energy in the magnetic field is converted to heat, and rapid boil-off of the cryogenic fluid. The abrupt decrease of current can result in kilovolt inductive voltage spikes and arcing. Permanent damage to the magnet is rare, but components can be damaged by localized heating, high voltages, or large mechanical forces.
In practice, magnets usually have safety devices to stop or limit the current when the beginning of a quench is detected. If a large magnet undergoes a quench, the inert vapor formed by the evaporating cryogenic fluid can present a significant asphyxiation hazard to operators by displacing breathable air.
A large section of the superconducting magnets in CERN's Large Hadron Collider unexpectedly quenched during start-up operations in 2008, necessitating the replacement of a number of magnets. In order to mitigate against potentially destructive quenches, the superconducting magnets that form the LHC are equipped with fast-ramping heaters which are activated once a quench event is detected by the complex quench protection system. As the dipole bending magnets are connected in series, each power circuit includes 154 individual magnets, and should a quench event occur, the entire combined stored energy of these magnets must be dumped at once. This energy is transferred into dumps that are massive blocks of metal which heat up to several hundreds of degrees Celsius—because of resistive heating—in a matter of seconds. Although undesirable, a magnet quench is a "fairly routine event" during the operation of a particle accelerator.
Effluents.
The natural product of the fusion reaction is a small amount of helium, which is completely harmless to life. Of more concern is tritium, which, like other isotopes of hydrogen, is difficult to retain completely. During normal operation, some amount of tritium will be continually released. There would be no acute danger, but the cumulative effect on the world's population from a fusion economy could be a matter of concern.
Although tritium is volatile and biologically active, the health risk posed by a release is much lower than that of most radioactive contaminants, because of tritium's short half-life (12.32 years) and very low decay energy (~14.95 keV), and because it does not bioaccumulate (instead being cycled out of the body as water, with a biological half-life of 7 to 14 days). Current ITER designs are investigating total containment facilities for any tritium.
Waste management.
The large flux of high-energy neutrons in a reactor will make the structural materials radioactive. The radioactive inventory at shut-down may be comparable to that of a fission reactor, but there are important differences.
The half-life of the radioisotopes produced by fusion tends to be less than those from fission, so that the inventory decreases more rapidly. Unlike fission reactors, whose waste remains radioactive for thousands of years, most of the radioactive material in a fusion reactor would be the reactor core itself, which would be dangerous for about 50 years, and low-level waste for another 100. Although this waste will be considerably more radioactive during those 50 years than fission waste, the very short half-life makes the process very attractive, as the waste management is fairly straightforward. By 500 years the material would have the same radiotoxicity as coal ash.
Additionally, the choice of materials used in a fusion reactor is less constrained than in a fission design, where many materials are required for their specific neutron cross-sections. This allows a fusion reactor to be designed using materials that are selected specifically to be "low activation", materials that do not easily become radioactive. Vanadium, for example, would become much less radioactive than stainless steel. Carbon fiber materials are also low-activation, as well as being strong and light, and are a promising area of study for laser-inertial reactors where a magnetic field is not required.
In general terms, fusion reactors would create far less radioactive material than a fission reactor, the material it would create is less damaging biologically, and the radioactivity "burns off" within a time period that is well within existing engineering capabilities for safe long-term waste storage.
Nuclear proliferation.
Although fusion power uses nuclear technology, the overlap with nuclear weapons would be limited. A huge amount of tritium could be produced by a fusion power station; tritium is used in the trigger of hydrogen bombs and in a modern boosted fission weapon, but it can also be produced by nuclear fission. The energetic neutrons from a fusion reactor could be used to breed weapons-grade plutonium or uranium for an atomic bomb (for example by transmutation of U238 to Pu239, or Th232 to U233).
A study conducted 2011 assessed the risk of three scenarios:
Another study concludes that "[..]large fusion reactors – even if not designed for fissile material breeding – could easily produce several hundred kg Pu per year with high weapon quality and very low source material requirements." It was emphasized that the implementation of features for intrinsic proliferation resistance might only be possible at this phase of research and development. The theoretical and computational tools needed for hydrogen bomb design are closely related to those needed for inertial confinement fusion, but have very little in common with the more scientifically developed magnetic confinement fusion.
Energy source.
Large-scale reactors using neutronic fuels (e.g. ITER) and thermal power production (turbine based) are most comparable to fission power from an engineering and economics viewpoint. Both fission and fusion power stations involve a relatively compact heat source powering a conventional steam turbine-based power station, while producing enough neutron radiation to make activation of the station materials problematic. The main distinction is that fusion power produces no high-level radioactive waste (though activated station materials still need to be disposed of). There are some power station ideas that may significantly lower the cost or size of such stations; however, research in these areas is nowhere near as advanced as in tokamaks.
Fusion power commonly proposes the use of deuterium, an isotope of hydrogen, as fuel and in many current designs also use lithium. Assuming a fusion energy output equal to the 1995 global power output of about 100 EJ/yr (= 1 × 1020 J/yr) and that this does not increase in the future, which is unlikely, then the known current lithium reserves would last 3000 years. Lithium from sea water would last 60 million years, however, and a more complicated fusion process using only deuterium from sea water would have fuel for 150 billion years. To put this in context, 150 billion years is close to 30 times the remaining lifespan of the sun, and more than 10 times the estimated age of the universe.
Economics.
While fusion power is still in early stages of development, substantial sums have been and continue to be invested in research. In the EU almost was spent on fusion research up to the end of the 1990s, and the new ITER reactor alone is budgeted at 
It is estimated that up to the point of possible implementation of electricity generation by nuclear fusion, R&D will need further promotion totalling around over a period of or so (of which within the EU) based on a report from 2002. Nuclear fusion research receives (excluding ITER funding) from the European Union, compared with for sustainable energy research, putting research into fusion power well ahead of that of any single rivaling technology. Indeed, the size of the investments and time frame of the expected results mean that fusion research is almost exclusively publicly funded, while research in other forms of energy can be done by the private sector. In spite of that, a number of start-up companies active in the field of fusion power have managed to attract private money.
Advantages.
Fusion power would provide more energy for a given weight of fuel than any fuel-consuming energy source currently in use, and the fuel itself (primarily deuterium) exists abundantly in the Earth's ocean: about 1 in 6500 hydrogen atoms in seawater is deuterium. Although this may seem a low proportion (about 0.015%), because nuclear fusion reactions are so much more energetic than chemical combustion and seawater is easier to access and more plentiful than fossil fuels, fusion could potentially supply the world's energy needs for millions of years.
Despite being technically non-renewable, fusion power has many of the benefits of renewable energy sources (such as being a long-term energy supply and emitting no greenhouse gases) as well as some of the benefits of the resource-limited energy sources as hydrocarbons and nuclear fission (without reprocessing). Like these currently dominant energy sources, fusion could provide very high power-generation density and uninterrupted power delivery (because it is not dependent on the weather, unlike wind and solar power).
Another aspect of fusion energy is that the cost of production does not suffer from diseconomies of scale. The cost of water and wind energy, for example, goes up as the optimal locations are developed first, while further generators must be sited in less ideal conditions. With fusion energy the production cost will not increase much even if large numbers of stations are built, because the raw resource (seawater) is abundant and widespread.
Some problems that are expected to be an issue in this century, such as fresh water shortages, can alternatively be regarded as problems of energy supply. For example, in desalination stations, seawater can be purified through distillation or reverse osmosis. Nonetheless, these processes are energy intensive. Even if the first fusion stations are not competitive with alternative sources, fusion could still become competitive if large-scale desalination requires more power than the alternatives are able to provide.
A scenario has been presented of the effect of the commercialization of fusion power on the future of human civilization. ITER and later Demo are envisioned to bring online the first commercial nuclear fusion energy reactor by 2050. Using this as the starting point and the history of the uptake of nuclear fission reactors as a guide, the scenario depicts a rapid take up of nuclear fusion energy starting after the middle of this century.
Fusion power could be used in interstellar space, where solar energy is not available.
Criticism.
Because commercial fusion projects are very large and complex, and ongoing funding is a political issue, such projects usually involve cost overruns and missed deadlines. For example, the construction of the National Ignition Facility cost $5 billion and took seven years longer than expected. ITER's expected cost has gone from $5 billion to $20 billion, and the date for full power operation has been put back to 2027, from the original estimate of 2016. And ITER will never supply electricity to the power grid. It is only a "proof of concept" science project.

</doc>
<doc id="55020" url="https://en.wikipedia.org/wiki?curid=55020" title="Jin dynasty (265–420)">
Jin dynasty (265–420)

The Jin dynasty (, ) was a Chinese dynasty lasting between the years AD 265 to 420. There are two main divisions in the history of the dynasty, the first being Western Jin (, 265–316) and the second Eastern Jin (, 317–420). Western Jin was founded by Sima Yan, with its capital at Luoyang, while Eastern Jin was begun by Sima Rui, with its capital at Jiankang. The two periods are also known as "Liang Jin" (; literally: two Jin) and "Sima Jin" () by scholars, to distinguish this dynasty from other dynasties that use the same Chinese character, such as the Later Jin dynasty ().
Foundation.
The Sima clan was initially subordinate to the Wei dynasty, but the clan's influence and power grew greatly after the coup d'état in 249 known as the incident at Gaoping Tombs. In 263, Sima Zhao unified the lands of Shu and captured Liu Shan, which was followed a year later by Zhong Hui's Rebellion.
In 265, Sima Yan forced Emperor Cao Huan of Wei to abdicate the throne to him, ending Wei and founding the Jin. Sima Yan was enthroned as Emperor Wu of Jin. He named his dynasty after the state of Jin of the Spring and Autumn period that once ruled the Sima clan's home county of Wen in Henei (modern Wen County, Henan). In 280, the Jin conquered Eastern Wu and unified China, but internal conflicts, corruption and political turmoil quickly weakened the dynasty, and the unification lasted only ten years. Upon the advent of the second Jin emperor, Emperor Hui, various imperial princes tried to grab power in the devastating War of the Eight Princes. The Uprising of the Five Barbarians followed, during which large numbers of refugees fled south while the north was occupied by various nomadic groups. This marked the end of the Western Jin dynasty in 316 when the Jin court evacuated to the region south of the Huai River, and the beginning of the Eastern Jin and the Sixteen Kingdoms period.
Sima Rui founded the Eastern Jin at Jiankang in 317, with its territory stretching across most of today's southern China. The combination of the Eastern Jin and Sixteen Kingdoms period is sometimes called the "Eastern Jin Sixteen Kingdoms" (). During this period, huge numbers of people moved south from the central plain, stimulating the development of Southern China. The Emperors of Eastern Jin had limited power, owing to their dependence on the support of both local and refugee noble families which possessed military power. These families included the Wang family, including the chancellor Wang Dao, and the Xie family of Xie An and Xie Xuan. Many fangzhen (; literally: military county) started to have ambitions which resulted in military revolts, like the rebellions of Wang Dun, Su Jun, and the dictatorship of Huan Wen. Even though there was the stated goal of getting back the "northern lost lands", paranoia within the royal family and a constant string of disruptions to the throne caused the loss of support of many officials.
In 383, Former Qin mobilized its troops and intended to conquer Eastern Jin. Faced by the threat of invasion, many Jin officials cooperated hoping to repel the attack. After the battle of Fei river, Xie An, Xie Xuan, and other generals were able to push back the Qin's assault and seized back a huge amount of territory from their enemy. However, more internal political battles from different groups of officials followed Huan Xuan's usurpation of the throne. As civilian administration suffered, more revolts from Sun En, Lu Xun, and the declaration of a new kingdom called Western Shu by the militarist Qiao Zong in Eastern Jin's Shu region. Ultimately, Liu Yu's rise ended major chaos and later he took the throne for himself, marking the ending of the Jin dynasty and the start of the Liu Song dynasty, and the Northern and Southern dynasties period of Chinese history.
History.
The Western Jin dynasty (, 265–316) was founded by Emperor Wu, better known as Sima Yan. Although it provided a brief period of unity after conquering Eastern Wu in 280, the Jin suffered a devastating civil war, War of the Eight Princes, after which they could not contain the revolt of nomadic tribes known as the Wu Hu.
The capital, Luoyang was captured in 311 by Liu Cong, and Emperor Huai was captured and later executed. His successor, Emperor Min was also captured in Chang'an in 316, and later executed. The remnants of the Jin court fled to the east and reestablished the government at Jiankang, near modern-day Nanjing, under a member of the royal family.
This Prince of Langye was proclaimed the Emperor Yuan of the Eastern Jin dynasty (, 317–420) when news of the fall of Chang'an reached the south. (The rival Wu Hu states in the north, which did not recognize the legitimacy of Jin, would sometimes refer to it as "Langye".)
Military crises, such as the rebellions of generals Wang Dun and Su Jun, plagued the Eastern Jin throughout its 104 years of existence. However, the Battle of Fei River turned out to be a major Jin victory, due to the short-lived cooperation of Huan Chong, brother of a great general Huan Wen, and Prime Minister Xie An. Later, Huan Xuan, son of Huan Wen, usurped the throne and changed the dynasty's name to Chu. He, in turn, was toppled by Liu Yu, who after reinstating Emperor An, ordered him strangled and installed his brother, Emperor Gong, in 419.
Emperor Gong abdicated in 420 in favor of Liu Yu, ushering in the Liu Song dynasty the first of the Southern dynasties. The Jin Dynasty thus came to an end.
Meanwhile, North China was ruled by the Sixteen Kingdoms, many of which were founded by the Wu Hu. The last of these, Northern Liang, was conquered by the Northern Wei dynasty in 439, ushering in the Northern dynasties period.
Jin ceramics.
The Jin dynasty is well known for the quality of its greenish celadon porcelain wares, which immediately followed the development of proto-celadon. Jar designs often incorporated animal, as well as Buddhist, figures.
Examples of Yue ware are also known from the Jin dynasty.
Imperial Family.
Sima Fei (司馬朏) was a descendant of the Jin dynasty royalty who fled north to the Xianbei Northern Wei in exile and married the Xianbei Princess Huayang (華陽公主), the daughter of Emperor Xiaowen of Northern Wei.
The Song dynasty chancellor Sima Guang (1019–1086) was descended from the Jin Imperial family.

</doc>
<doc id="55021" url="https://en.wikipedia.org/wiki?curid=55021" title="Josquin des Prez">
Josquin des Prez

Josquin des Prez (;  – 27 August 1521), often referred to simply as Josquin, was a Franco-Flemish composer of the Renaissance. His original name is sometimes given as Josquin Lebloitte and his later name is given under a wide variety of spellings in French, Italian, and Latin, including and . His motet "Illibata Dei virgo nutrix" includes an acrostic of his name, where he spelled it "Josquin des Prez". He was the most famous European composer between Guillaume Dufay and Palestrina, and is usually considered to be the central figure of the Franco-Flemish School. Josquin is widely considered by music scholars to be the first master of the high Renaissance style of polyphonic vocal music that was emerging during his lifetime.
During the 16th century, Josquin gradually acquired the reputation as the greatest composer of the age, his mastery of technique and expression universally imitated and admired. Writers as diverse as Baldassare Castiglione and Martin Luther wrote about his reputation and fame; theorists such as Heinrich Glarean and Gioseffo Zarlino held his style as that best representing perfection. He was so admired that many anonymous compositions were attributed to him by copyists, probably to increase their sales. More than 370 works are attributed to him; it was only after the advent of modern analytical scholarship that some of these mistaken attributions have been challenged, on the basis of stylistic features and manuscript evidence. Yet in spite of Josquin's colossal reputation, which endured until the beginning of the Baroque era and was revived in the 20th century, his biography is shadowy, and next to nothing is known about his personality. The only surviving work which may be in his own hand is a graffito on the wall of the Sistine Chapel, and only one contemporary mention of his character is known, in a letter to Duke Ercole I of Ferrara. The lives of dozens of minor composers of the Renaissance are better documented than the life of Josquin.
Josquin wrote both sacred and secular music, and in all of the significant vocal forms of the age, including masses, motets, chansons and frottole. During the 16th century, he was praised for both his supreme melodic gift and his use of ingenious technical devices. In modern times, scholars have attempted to ascertain the basic details of his biography, and have tried to define the key characteristics of his style to correct misattributions, a task that has proved difficult, as Josquin liked to solve compositional problems in different ways in successive compositions—sometimes he wrote in an austere style devoid of ornamentation, and at other times he wrote music requiring considerable virtuosity. Heinrich Glarean wrote in 1547 that Josquin was not only a "magnificent virtuoso" (the Latin can be translated also as "show-off") but capable of being a "mocker", using satire effectively. While the focus of scholarship in recent years has been to remove music from the "Josquin canon" (including some of his most famous pieces) and to reattribute it to his contemporaries, the remaining music represents some of the most famous and enduring of the Renaissance.
Life.
Birth and early career.
Little is known for certain of Josquin's early life. Much is inferential and speculative, though numerous clues have emerged from his works and the writings of contemporary composers, theorists, and writers of the next several generations. Josquin was born in the area controlled by the Dukes of Burgundy, and was possibly born either in Hainaut (modern-day Belgium), or immediately across the border in modern-day France, since several times in his life he was classified legally as a Frenchman (for instance, when he made his will). Josquin was long mistaken for a man with a similar name, Josquin de Kessalia, born around the year 1440, who sang in Milan from 1459 to 1474, dying in 1498. More recent scholarship has shown that Josquin des Prez was born around 1450 or a few years later, and did not go to Italy until the early 1480s.
Around 1466, perhaps on the death of his father, Josquin was named by his uncle and aunt, Gille Lebloitte dit Desprez and Jacque Banestonne, as their heir. Their will gives Josquin's actual surname as Lebloitte. According to Matthews and Merkley, "des Prez" was a nickname.
According to an account by Claude Hémeré, a friend and librarian of Cardinal Richelieu whose evidence dates as late as 1633, and who used the records of the collegiate church of Saint-Quentin, Josquin became a choirboy with his friend and colleague the Franco Flemish composer Jean Mouton at Saint-Quentin's royal church, probably around 1460. Doubt has been cast on the accuracy of Hémeré's account, however. He may have studied counterpoint under Ockeghem, whom he greatly admired throughout his life: this is suggested both by the testimony of Gioseffo Zarlino and Lodovico Zacconi, writing later in the 16th century, and by Josquin's eloquent lament on the death of Ockeghem in 1497, "Nymphes des bois/Requiem aeternam", based on the poem by Jean Molinet. All records from Saint-Quentin were destroyed in 1669; however the collegiate chapel there was a center of music-making for the entire area, and in addition was an important center of royal patronage. Both Jean Mouton and Loyset Compère were buried there and it is certainly possible that Josquin acquired his later connections with the French royal chapel through early experiences at Saint-Quentin.
The first definite record of his employment is dated 19 April 1477, and it shows that he was a singer at the chapel of René, Duke of Anjou, in Aix-en-Provence. He remained there at least until 1478. No certain records of his movements exist for the period from March 1478 until 1483, but if he remained in the employ of René he would have transferred to Paris in 1481 along with the rest of the chapel. One of Josquin's early motets, "Misericordias Domini in aeternum cantabo", suggests a direct connection with Louis XI, who was king during this time. In 1483 Josquin returned to Condé to claim his inheritance from his aunt and uncle, who may have been killed by the army of Louis XI in May 1478, when they besieged the town, locked the population into the church, and burned them alive.
Milan.
The period from 1480 to 1482 has puzzled biographers; contradictory evidence exists suggesting either that Josquin was still in France, or was already in the service of the Sforza family, specifically with Ascanio Sforza, who had been banished from Milan and resided temporarily in Ferrara or Naples. Residence in Ferrara in the early 1480s could explain the "Missa Hercules dux Ferrariae", composed for Ercole d'Este, but which stylistically does not fit with the usual date of 1503–4 when Josquin was known to be in Ferrara. Alternatively it has been suggested that Josquin spent some of that time in Hungary, based on a mid-16th-century Roman document describing the Hungarian court in those years, and including Josquin as one of the musicians present.
In either 1483 or 1484, Josquin is known to have been in the service of the Sforza family in Milan. While in their employ, he made one or more trips to Rome, and possibly also to Paris; while in Milan he made the acquaintance of Franchinus Gaffurius, who was "maestro di cappella" of the cathedral there. He was in Milan again in 1489, after a possible period of travel; but he left that year.
Rome.
From 1489 to 1495, Josquin was a member of the papal choir, first under Pope Innocent VIII, and later under the Borgia pope Alexander VI. He may have gone there as part of a singer exchange with Gaspar van Weerbeke, who went back to Milan at the same time. While there, he may have been the one who carved his name into the wall of the Sistine Chapel; a "JOSQUINJ" was recently revealed by workers restoring the chapel. Since it was traditional for singers to carve their names into the walls, and hundreds of names were inscribed there during the period from the 15th to the 18th centuries, it is considered highly likely that the graffiti is by Josquin – and if so, it would be his only surviving autograph.
Josquin's mature style evolved during this period; as in Milan he had absorbed the influence of light Italian secular music, in Rome he refined his techniques of sacred music. Several of his motets have been dated to the years he spent at the papal chapel.
Departure from Rome; Milan and France.
Around 1498, Josquin most likely re-entered the service of the Sforza family, on the evidence of a pair of letters between the Gonzaga and Sforza families. He probably did not stay in Milan long, for in 1499 Louis XII captured Milan in his invasion of northern Italy and imprisoned Josquin's former employers. Around this time Josquin most likely returned to France, although documented details of his career around the turn of the 16th century are lacking. Prior to departing Italy he most likely wrote one of his most famous secular compositions, the frottola "El grillo" (the Cricket), as well as "In te Domine speravi" ("I have placed my hope in you, Lord"), based on Psalm 30. The latter composition may have been a veiled reference to the religious reformer Girolamo Savonarola, who had been burned at the stake in Florence in 1498, and for whom Josquin seems to have had a special reverence; the text was the Dominican friar's favorite psalm, a meditation on which he left incomplete in prison prior to his execution.
Some of Josquin's compositions, such as the instrumental "Vive le roy", have been tentatively dated to the period around 1500 when he was in France. A motet, "Memor esto verbi tui servo tuo" ("Remember thy promise unto thy servant"), was, according to Heinrich Glarean writing in the "Dodecachordon" of 1547, composed as a gentle reminder to the king to keep his promise of a benefice to Josquin, which he had forgotten to keep. According to Glarean's story, it worked: the court applauded, and the king gave Josquin his benefice. Upon receiving it, Josquin reportedly wrote a motet on the text "Benefecisti servo tuo, Domine" ("Lord, thou hast dealt graciously with thy servant") to show his gratitude to the king.
Ferrara.
Josquin probably remained in the service of Louis XII until 1503, when Duke Ercole I of Ferrara hired him for the chapel there. One of the rare mentions of Josquin's personality survives from this time. Prior to hiring Josquin, one of Duke Ercole's assistants recommended that he hire Heinrich Isaac instead, since Isaac was easier to get along with, more companionable, was more willing to compose on demand, and would cost significantly less (120 ducats vs. 200). Ercole, however, chose Josquin.
While in Ferrara, Josquin wrote some of his most famous compositions, including the austere, Savonarola-influenced "Miserere", which became one of the most widely distributed motets of the 16th century; the utterly contrasting, virtuoso motet "Virgo salutiferi"; and possibly the "Missa Hercules Dux Ferrariae", which is written on a "cantus firmus" derived from the musical letters in the Duke's name, a technique known as "soggetto cavato".
Josquin did not stay in Ferrara long. An outbreak of the plague in the summer of 1503 prompted the evacuation of the Duke and his family, as well as two-thirds of the citizens, and Josquin left by April of the next year, possibly also to escape the plague. His replacement, Jacob Obrecht, died of the plague in the summer of 1505, to be replaced by Antoine Brumel in 1506, who stayed until the disbanding of the chapel in 1510.
Retirement to Condé-sur-l'Escaut.
Josquin went directly from Ferrara to his home region of Condé-sur-l'Escaut, southeast of Lille on the present-day border between Belgium and France, becoming provost of the collegiate church of Notre-Dame on 3 May 1504, a large musical establishment that he headed for the rest of his life. While the chapter at Bourges Cathedral asked him to become master of the choirboys there in 1508, it is not known how he responded, and there is no record of his having been employed there; most scholars presume he remained in Condé. In 1509, he held concurrently provost and choir master offices at Saint Quentin collegiate church.
During the last two decades of his life, Josquin's fame spread abroad along with his music. The newly developed technology of printing made wide dissemination of his music possible, and Josquin was the favorite of the first printers: one of Petrucci's first publications, and the earliest surviving print of music by a single composer, was a book of Josquin's masses which he printed in Venice in 1502. This publication was successful enough that Petrucci published two further volumes of Josquin's masses, in 1504 and 1514, and reissued them several times.
On his death-bed, Josquin asked that he be listed on the rolls as a foreigner, so that his property would not pass to the Lords and Ladies of Condé. This bit of evidence has been used to show that he was French by birth. Additionally, he left an endowment for the performance of his late motet, "Pater noster", at all general processions in the town when they passed in front of his house, stopping to place a wafer on the marketplace altar to the Holy Virgin. "Pater noster" may have been his last work.
Music.
Overview.
Josquin lived during a transitional stage in music history. Musical styles were changing rapidly, in part owing to the movement of musicians between different regions of Europe. Many northern musicians moved to Italy, the heart of the Renaissance, attracted by the Italian nobility's patronage of the arts; while in Italy, these composers were influenced by the native Italian styles, and often brought those ideas with them back to their homelands. The sinuous musical lines of the Ockeghem generation, the contrapuntal complexity of the Netherlanders, and the homophonic textures of the Italian lauda and secular music began to merge into a unified style; indeed Josquin was to be the leading figure in this musical process, which eventually resulted in the formation of an international musical language, of which the most famous composers included Palestrina and Lassus.
Josquin likely learned his craft in his home region in the North, in France, and then in Italy when he went to Milan and Rome. His early sacred works emulate the contrapuntal complexity and ornamented, melismatic lines of Ockeghem and his contemporaries, but at the same time he was learning his contrapuntal technique he was acquiring an Italianate idiom for his secular music: after all, he was surrounded by Italian popular music in Milan. By the end of his long creative career, which spanned approximately 50 productive years, he had developed a simplified style in which each voice of a polyphonic composition exhibited free and smooth motion, and close attention was paid to clear setting of text as well as clear alignment of text with musical motifs. While other composers were influential on the development of Josquin's style, especially in the late 15th century, he himself became the most influential composer in Europe, especially after the development of music printing, which was concurrent with the years of his maturity and peak output. This event made his influence even more decisive than it might otherwise have been.
Many "modern" musical compositional practices were being born in the era around 1500. Josquin made extensive use of "motivic cells" in his compositions, short, easily recognizable melodic fragments which passed from voice to voice in a contrapuntal texture, giving it an inner unity. This is a basic organizational principle in music which has been practiced continuously from approximately 1500 until the present day.
Josquin wrote in all of the important forms current at the time, including masses, motets, chansons, and frottole. He even contributed to the development of a new form, the motet-chanson, of which he left at least three examples. In addition, some of his pieces were probably intended for instrumental performance.
Each area of his output can be further subdivided by form or by hypothetical period of composition. Since dating Josquin's compositions is particularly problematic, with scholarly consensus only achieved on a minority of works, discussion here is by type.
Masses.
Josquin wrote towards the end of the period in which the mass was the predominant form of sacred composition in Europe. The mass, as it had developed through the 15th century, was a long, multi-section form, with opportunities for large-scale structure and organization not possible in the other forms such as the motet. Josquin wrote some of the most famous examples of the genre, most using some kind of cyclic organization.
He wrote masses using the following general techniques, although there is considerable overlap between techniques in individual compositions:
Most of these techniques, particularly paraphrase and parody, became standardized during the first half of the 16th century; Josquin was very much a pioneer, and what was perceived by later observers as the mixing of these techniques was actually the process by which they were created.
Cantus-firmus masses.
Prior to Josquin's mature period, the most common technique for writing masses was the cantus firmus, a technique which had been in use already for most of the 15th century. It was the technique that Josquin used earliest in his career, with the "Missa L'ami Baudichon", possibly his first mass. This mass is based on a secular – indeed ribald – tune similar to "Three Blind Mice". That basing a mass on such a source was an accepted procedure is evident from the existence of the mass in Sistine Chapel part-books copied during the papacy of Julius II (1503 to 1513).
Josquin's most famous cantus-firmus masses are the two based on the L'homme armé tune, which was the favorite tune for mass composition of the entire Renaissance. The earlier of the two, "Missa L'homme armé super voces musicales", is a technical tour-de-force on the tune, containing numerous mensuration canons and contrapuntal display. It was by far the most famous of all his masses. The second, "Missa L'homme armé sexti toni", is a "fantasia on the theme of the armed man." While based on a cantus firmus, it is also a paraphrase mass, for fragments of the tune appear in all voices. Technically it is almost restrained, compared to the other "L'homme armé" mass, until the closing Agnus Dei, which contains a complex canonic structure including a rare retrograde canon, around which other voices are woven.
Paraphrase masses.
The paraphrase technique differs from the cantus-firmus technique in that the source material, though it still consists of a monophonic original, is embellished, often with ornaments. As in the cantus-firmus technique, the source tune may appear in many voices of the mass.
Several of Josquin's masses feature the paraphrase technique, and they include some of his most famous work. The relatively early "Missa Ave maris stella", which probably dates from his years in the Sistine Chapel choir, paraphrases the Marian antiphon of the same name; it is also one of his shortest masses. The late "Missa de Beata Virgine" paraphrases plainchants in praise of the Virgin Mary; it is a Lady Mass, a votive mass for Saturday performance, and was his most popular mass in the 16th century.
By far the most famous of Josquin's masses using the technique, and one of the most famous mass settings of the entire era, was the "Missa pange lingua", based on the hymn by Thomas Aquinas for the Vespers of Corpus Christi. It was probably the last mass that Josquin composed. This mass is an extended fantasia on the tune, using the melody in all voices and in all parts of the mass, in elaborate and ever-changing polyphony. One of the high points of the mass is the "et incarnatus est" section of the Credo, where the texture becomes homophonic, and the tune appears in the topmost voice; here the portion which would normally set "Sing, O my tongue, of the mystery of the divine body" is instead given the words "And he became incarnate by the Holy Ghost from the Virgin Mary, and was made man."
Parody masses, masses on popular songs.
In parody masses, the source material was not a single line, but an entire texture, often of a popular song. Several works by Josquin fall loosely into this category, including the "Missa Fortuna desperata", based on the three-voice song "Fortuna desperata" (possibly by Antoine Busnois); the "Missa Malheur me bat" (based on a chanson variously ascribed to Obrecht, Ockeghem, or, most likely, Abertijne Malcourt); and the "Missa Mater Patris", based on a three-voice motet by Antoine Brumel. The "Missa Mater Patris" is probably the first true parody mass to be composed, for it no longer contains any hint of a cantus firmus. Parody technique was to become the most usual means of mass composition for the remainder of the 16th century, although the mass gradually fell out of favor as the motet grew in esteem.
Masses on solmization syllables.
The earliest known mass by any composer using this method of composition – the "soggetto cavato" – is the "Missa Hercules Dux Ferrariae", which Josquin probably wrote in the early 1480s for the powerful Ercole I, Duke of Ferrara. The notes of the cantus firmus are drawn from the musical syllables of the Duke's name in the following way: "Ercole, Duke of Ferrara" in Latin is "Hercules Dux Ferrarie". Taking the solmization syllables with the same vowels gives: (in modern nomenclature: ). Another mass using this technique is the "Missa La sol fa re mi", based on the musical syllables contained in "Lascia fare mi" ("let me do it"). The story, as told by Glareanus in 1547, was that an unknown aristocrat used to order suitors away with this phrase, and Josquin immediately wrote an "exceedingly elegant" mass on it as a jab at him.
Canonic masses.
Canonic masses came into increasing prominence in the latter part of the 15th century. Early examples include Ockeghem's famous "Missa prolationum", consisting entirely of mensuration canons, the "Missa L'homme armé" of Guillaume Faugues, whose cantus firmus is presented in canon at the descending fifth, and the "Missa" ["Ad fugam"] of Marbrianus de Orto, based on freely composed canons at the fifth between superius and tenor. Josquin makes use of canon in the Osanna and Agnus Dei III of the "Missa L'homme armé sexti toni", throughout the "Missa Sine nomine", and in the final three movements of the "Missa De beata virgine". The "Missa L'homme armé super voces musicales" incorporates mensuration canons in the Kyrie, Benedictus, and Agnus Dei II.
Motets.
Josquin's motet style varied from almost strictly homophonic settings with block chords and syllabic text declamation to highly ornate contrapuntal fantasias, to the psalm settings which combined these extremes with the addition of rhetorical figures and text-painting that foreshadowed the later development of the madrigal. He wrote many of his motets for four voices, an ensemble size which had become the compositional norm around 1500, and he also was a considerable innovator in writing motets for five and six voices. No motets of more than six voices have been reliably attributed to Josquin.
Almost all of Josquin's motets use some kind of compositional constraint on the process; they are not freely composed. Some of them use a cantus firmus as a unifying device; some are canonic; some use a motto which repeats throughout; some use several of these methods. The motets that use canon can be roughly divided into two groups: those in which the canon is plainly designed to be heard and appreciated as such, and another group in which a canon is present, but almost impossible to hear, and seemingly written to be appreciated by the eye, and by connoisseurs.
Josquin frequently used imitation, especially paired imitation, in writing his motets, with sections akin to fugal expositions occurring on successive lines of the text he was setting. An example is his setting of "Dominus regnavit" (Psalm 93), for four voices; each of the lines of the psalm begins with a voice singing a new tune alone, quickly followed by entries of other three voices in imitation.
In writing polyphonic settings of psalms, Josquin was a pioneer, and psalm settings form a large proportion of the motets of his later years. Few composers prior to Josquin had written polyphonic psalm settings. Some of Josquin's settings include the famous "Miserere", written in Ferrara in 1503 or 1504 and most likely inspired by the recent execution of the reformist monk Girolamo Savonarola, "Memor esto verbi tui", based on Psalm 119, and two settings of "De profundis" (Psalm 130), both of which are often considered to be among his most significant accomplishments.
Chansons and instrumental compositions.
In the domain of secular music, Josquin left numerous French chansons, for from three to six voices, as well as a handful of Italian secular songs known as frottole, as well as some pieces which were probably intended for instrumental performance. Problems of attribution are even more acute with the chansons than they are with other portions of his output: while about 70 three and four-voice chansons were published under his name during his lifetime, only six of the more than thirty five- and six-voice chansons attributed to him were circulated under his name during the same time. Many of the attributions added after his death are considered to be unreliable, and much work has been done in the last decades of the 20th century to correct attributions on stylistic grounds.
Josquin's earliest chansons were probably composed in northern Europe, under the influence of composers such as Ockeghem and Busnois. Unlike them, however, he never adhered strictly to the conventions of the "formes fixes" – the rigid and complex repetition patterns of the rondeau, virelai, and ballade – instead he often wrote his early chansons in strict imitation, a feature they shared with many of his sacred works. He was one of the first composers of chansons to make all voices equal parts of the texture; and many of his chansons contain points of imitation, in the manner of motets. However he did use melodic repetition, especially where the lines of text rhymed, and many of his chansons had a lighter texture, as well as a faster tempo, than his motets.
Inside of his chansons, he often used a cantus firmus, sometimes a popular song whose origin can no longer be traced, as in "Si j'avoye Marion". Other times he used a tune originally associated with a separate text; and still other times he freely composed an entire song, using no apparent external source material. Another technique he sometimes used was to take a popular song and write it as a canon with itself, in two inner voices, and write new melodic material above and around it, to a new text: he used this technique in one of his most famous chansons, "Faulte d'argent" ("The problem with money"), a song sung by a man who wakes in bed with a prostitute, broke and unable to pay her.
Some of his chansons were doubtless designed to be performed instrumentally. That Petrucci published many of them without text is strong evidence of this; additionally, some of the pieces (for example, the fanfare-like "Vive le roy") contain writing more idiomatic for instruments than voices.
Josquin's most famous chansons circulated widely in Europe. Some of the better known include his lament on the death of Ockeghem, "Nymphes des bois/Requiem aeternam"; "Mille regretz" (the attribution of which has recently been questioned); "Plus nulz regretz"; and "Je me complains".
In addition to his French chansons, he wrote at least three pieces in the manner of the Italian frottola, a popular Italian song form which he would have encountered during his years in Milan. These songs include "Scaramella", "El grillo", and "In te domine speravi". They are even simpler in texture than his French chansons, being almost uniformly syllabic and homophonic, and they remain among the most frequently sung portions of his output.
Motet-chansons.
While in Milan, Josquin wrote several examples of a new type of piece developed by the composers there, the motet-chanson. These compositions were texturally very similar to 15th century chansons in the "formes fixes" mold, except that unlike those completely secular works, they contained a chant-derived Latin cantus-firmus in the lowest of the three voices. The other voices, in French, sang a secular text which had either a symbolic relationship to the sacred Latin text, or commented on it. Josquin's three known motet-chansons, "Que vous madame/In pace", "A la mort/Monstra te esse matrem", and "Fortune destrange plummaige/Pauper sum ego", are similar stylistically to those by the other composers of the Milan chapel, such as Loyset Compère and Alexander Agricola.
Influence.
Josquin's fame lasted throughout the 16th century, and indeed increased for several decades after his death. Zarlino, writing in the 1580s, was still using examples from Josquin in his treatises on composition; and Josquin's fame was only eclipsed after the beginning of the Baroque era, with the decline of the pre-tonal polyphonic style. During the 18th and 19th centuries Josquin's fame was overshadowed by later Roman School composer Palestrina, whose music was seen as the summit of polyphonic refinement, and codified into a system of composition by theorists such as Johann Fux; however, during the 20th century, Josquin's reputation has grown steadily, to the point where scholars again consider him "the greatest and most successful composer of the age." According to Richard Sherr, writing in the introduction to the "Josquin Companion", addressing specifically the shrinking of Josquin's canon due to correction of misattributions, "Josquin will survive because his best music really is as magnificent as everybody has always said it was."
Since the 1950s Josquin's reputation has been boosted by the increasing availability of recordings, of which there are many, and the rise of ensembles specializing in the performance of 16th century vocal music, many of which place Josquin's output at the heart of their repertoire.
Works list.
The difficulties in compiling a works list for Josquin cannot be overstated. Because of his immense prestige in the early sixteenth century, many scribes and publishers did not resist the temptation of attributing anonymous or otherwise spurious works to Josquin. The German editor Georg Forster summed up the situation admirably in 1540 when he wrote, "I remember a certain eminent man saying that, now that Josquin is dead, he is putting out more works than when he was alive." Thus, the authenticity of many of the works listed below is disputed.
Masses.
Doubtful works:
Mass fragments.
Of questionable authenticity, except the "Credo De tous biens playne" and "Credo toni" 

</doc>
<doc id="55022" url="https://en.wikipedia.org/wiki?curid=55022" title="Gait">
Gait

Gait is the pattern of movement of the limbs of animals, including humans, during locomotion over a solid substrate. Most animals use a variety of gaits, selecting gait based on speed, terrain, the need to maneuver, and energetic efficiency. Different animal species may use different gaits due to differences in anatomy that prevent use of certain gaits, or simply due to evolved innate preferences as a result of habitat differences. While various gaits are given specific names, the complexity of biological systems and interacting with the environment make these distinctions 'fuzzy' at best. Gaits are typically classified according to footfall patterns, but recent studies often prefer definitions based on mechanics. The term typically does not refer to limb-based propulsion through fluid mediums such as water or air, but rather to propulsion across a solid substrate by generating reactive forces against it (which can apply to walking while underwater as well as on land).
Due to the rapidity of animal movement, simple direct observation is rarely sufficient to give any insight into the pattern of limb movement. In spite of early attempts to classify gaits based on footprints or the sound of footfalls, it wasn't until Eadweard Muybridge and Étienne-Jules Marey began taking rapid series of photographs that proper scientific examination of gaits could begin.
Overview.
Milton Hildebrand pioneered the contemporary scientific analysis and the classification of gaits. The movement of each limb was partitioned into a stance phase, where the foot was in contact with the ground, and a swing phase, where the foot was lifted and moved forwards. Each limb must complete a cycle in the same length of time, otherwise one limb's relationship to the others can change with time, and a steady pattern cannot occur. Thus, any gait can completely be described in terms of the beginning and end of stance phase of three limbs relative to a cycle of a reference limb, usually the left hindlimb.
Variables.
Gaits are generally classed as "symmetrical" and "asymmetrical" based on limb movement. It is important to note that these terms have nothing to do with left-right symmetry. In a symmetrical gait, the left and right limbs of a pair alternate, while in an asymmetrical gait, the limbs move together. Asymmetrical gaits are sometimes termed "leaping gaits", due to the presence of a suspended phase.
The key variables for gait are the duty factor and the forelimb-hindlimb phase relationship. Duty factor is simply the percent of the total cycle which a given foot is on the ground. This value will usually be the same for forelimbs and hindlimbs unless the animal is moving with a specially trained gait or is accelerating or decelerating. Duty factors over 50% are considered a "walk", while those less than 50% are considered a run. Forelimb-hindlimb phase is the temporal relationship between the limb pairs. If the same-side forelimbs and hindlimbs initiate stance phase at the same time, the phase is 0 (or 100%). If the same-side forelimb contacts the ground half of the cycle later than the hindlimb, the phase is 50%.
Differences between species.
Any given animal uses a relatively restricted set of gaits, and different species use different gaits. Almost all animals are capable of symmetrical gaits, while asymmetrical gaits are largely confined to mammals, who are capable of enough spinal flexion to increase stride length (though small crocodilians are capable of using a bounding gait). Lateral sequence gaits during walking and running are most common in mammals, but arboreal mammals such as monkeys, some possums, and kinkajous use diagonal sequence walks for enhanced stability. Diagonal sequence walks and runs (aka trots) are most frequently used by sprawling tetrapods such as salamanders and lizards, due to the lateral oscillations of their bodies during movement. Bipeds are a unique case, and most bipeds will display only three gaits - walking, running, and hopping - during natural locomotion. Other gaits, such as human skipping, are not used without deliberate effort.
Physiological effects of gait.
Gait choice can have effects beyond immediate changes in limb movement and speed, notably in terms of ventilation. Because they lack a diaphragm, lizards and salamanders must expand and contract their body wall in order to force air in and out of their lungs, but these are the same muscles used to laterally undulate the body during locomotion. Thus, they cannot move and breathe at the same time, a situation called Carrier's constraint, though some, such as monitor lizards, can circumvent this restriction via buccal pumping. In contrast, the spinal flexion of a galloping mammal causes the abdominal viscera to act as a piston, inflating and deflating the lungs as the animal's spine flexes and extends, increasing ventilation and allowing greater oxygen exchange.
Energy-based gait classification.
While gaits can be classified by footfall, new work involving whole-body kinematics and force-plate records has given rise to an alternative classification scheme, based on the mechanics of the movement. In this scheme, movements are divided into walking and running. Walking gaits are all characterized by a 'vaulting' movement of the body over the legs, frequently described as in inverted pendulum (displaying fluctuations in kinetic and potential energy which are perfectly out of phase). In running, the kinetic and potential energy fluctuate in-phase, and the energy change is passed on to muscles, bones, tendons and ligaments acting as springs (thus it is described by the spring-mass model).
Energetics.
Speed generally governs gait selection, with quadrupedal mammals moving from a walk to a run to a gallop as speed increases. Each of these gaits has an optimum speed, at which the minimum calories per meter are consumed, and costs increase at slower or faster speeds. Gait transitions occur near the speed where the cost of a fast walk becomes higher than the cost of a slow run. Unrestrained animals will typically move at the optimum speed for their gait to minimize energy cost. The cost of transport is used to compare the energetics of different gaits, as well as the gaits of different animals.
Non-tetrapod gaits.
In spite of the differences in leg number shown in terrestrial vertebrates, according to the inverted pendulum model of walking and spring-mass model of running, "walks" and "runs" are seen in animals with 2, 4, 6, or more legs. The term 'gait' has even been applied to flying and swimming organisms that produce distinct patterns of wake vortices.

</doc>
<doc id="55023" url="https://en.wikipedia.org/wiki?curid=55023" title="Jin dynasty (1115–1234)">
Jin dynasty (1115–1234)

The Jin dynasty (Jurchen: Anchun Gurun; , ); Manchu: Aisin Gurun; Khitan language: Nik, Niku; ; 1115–1234), officially the Great Jin (), also known as the Jurchen dynasty, was founded by the Wanyan clan of the Jurchen people, the ancestors of the Manchu people who established the Qing dynasty some 500 years later. The name is sometimes written as Kin to differentiate it from an earlier Jìn dynasty of China whose name is identically spelled using the Latin alphabet.
The Jurchen tribes were united by the chieftain and later first Jin emperor, Wanyan Aguda, who overthrew the Khitan-led Liao dynasty. During the reign of Aguda's successor, the Jin declared war against the Song dynasty and conquered much of northern China. The Song were forced to flee south of Yangtze River. The Jin dynasty fell after their defeat against the rising Mongol Empire, a steppe confederation that had formerly been a Jurchen vassal.
Name.
The Jin dynasty was officially known as the "Great Jin" at that time. Furthermore, the Jin emperors referred to their state as "Zhongguo" (中國) like some other non-Han dynasties. Non-Han rulers expanded the definition of "China" to include non-Han peoples in addition to Han people, whenever they ruled China. Yuan, Jin, and Northern Wei documents indicate the usage of "China" by dynasties to refer to themselves began earlier than previously thought.
History.
The Jin dynasty was created in what would become northern Manchuria by the Jurchen tribal chieftain Wanyan Aguda in 1115. Aguda adopted the Chinese term for "gold" as the name of his state, itself a translation of "Anchuhu" River, which meant "golden" in Jurchen. The Jurchens' early rival was the Liao dynasty, which had held sway over northern China, including Manchuria and part of Mongolia, for several centuries. In 1121, the Jurchens entered into the Alliance Conducted at Sea with the Song dynasty and agreed to jointly invade the Liao. While the Song armies faltered, the Jurchens succeeded in driving the Liao to Central Asia. In 1125, after the death of Aguda, the Jin broke the alliance with the Song and invaded North China. When the Song reclaimed the southern part of the Liao where Han Chinese lived, they were "fiercely resisted" by the Han Chinese population there who had previously been under Liao rule, while when the Jurchens invaded that area, the Han Chinese did not oppose them at all and handed over the Southern Capital (modern day Beijing, then known as Yanjing) to them. The Jin were supported by the anti-Song, Beijing based noble Han clans. On January 9, 1127, Jin forces ransacked Kaifeng, capital of the Northern Song dynasty, capturing both Emperor Qinzong and his father, Emperor Huizong, who had abdicated in panic in the face of Jin forces. Following the fall of Kaifeng, the succeeding Southern Song dynasty continued to fight the Jin for over a decade, eventually signing the Treaty of Shaoxing in 1141, which called for the cession of all Song land north of the Huai River to the Jin and the execution of Song general Yue Fei in return for peace. The peace treaty was formally ratified on 11 October 1142 when a Jin envoy visited the Song court.
The migration south.
After taking over Northern China, the Jin dynasty became increasingly sinicized. About three million people, half of them Jurchens, migrated south into northern China over two decades, and this minority governed about thirty million people. The Jurchens were given land grants and organized into hereditary military units: 300 households formed a "mou-ke" (company) and 7-10 "mouke"s formed a "meng-an" (battalion). Many married Hans, although the ban on Jurchen nobles marrying Hans was not lifted until 1191. After Emperor Taizong died in 1135, the next three Jin emperors were grandsons of Wanyan Aguda by three different princes. Emperor Xizong (r. 1135–1149) studied the classics and wrote Chinese poetry. He adopted Han cultural traditions, but the Jurchen nobles had the top positions.
Later in life, Emperor Xizong became an alcoholic and executed many officials for criticizing him. He also had Jurchen leaders who opposed him murdered, even those in his own Wanyan family clan. In 1149 he was murdered by a cabal of relatives and nobles, who made his cousin Wanyan Liang the next Jin emperor. Because of the brutality of both his domestic and foreign policy, Wanyan Liang was posthumously demoted from the position of emperor. Consequently, historians have commonly referred to him by the posthumous name "Prince of Hailing".
Rebellions in the north.
Having usurped the throne, Wanyan Liang (Prince of Hailing) embarked on the program of legitimizing his rule as an emperor of China. In 1153, he moved the empire's main capital from Huining Prefecture in northern Manchuria (south of present-day Harbin) to the former Liao capital, Yanjing (now Beijing).
Four years later, in 1157, to emphasize the permanence of the move, he razed the nobles’ residences in Huining. The Prince of Hailing also reconstructed the former Song capital, Bianjing (now Kaifeng), which had been sacked in 1127, making it the Jin's southern capital.
The Prince of Hailing also tried to suppress dissent by killing Jurchen nobles, executing 155 princes. To fulfill his dream of becoming the ruler of all China, Hailing attacked the Southern Song in 1161. Meanwhile, two simultaneous rebellions erupted in Manchuria: one of Jurchen nobles, led by Hailing's cousin, soon-to-be crowned Wanyan Yong, and the other of Khitan tribesmen. Hailing had to withdraw Jin troops from southern China to quell the uprisings. The Jin were defeated in the Battle of Caishi and Battle of Tangdao. With a depleted military force, Hailing failed to make headway in his attempted invasion of the Southern Song. Finally he was assassinated by his own generals in December of 1161, due to his defeats. His son and heir was also assassinated in the capital.
Although crowned in October, Wanyan Yong was not officially recognized as Emperor Shizong until the murder of Hailing's heir. The Khitan uprising was not suppressed until 1164; their horses were confiscated so that the rebels had to take up farming. Other Khitan and Xi cavalry units had been incorporated into the Jin army. Because these internal uprisings had severely weakened the Jin's capacity to confront the Southern Song militarily, the Jin court under Emperor Shizong began negotiating for peace. The Treaty of Longxing (隆興和議) was signed in 1164 and ushered in more than 40 years of peace between the two empires.
In the early 1180s, Emperor Shizong instituted a restructuring of 200 "meng'an" units to remove tax abuses and help Jurchens. Communal farming was encouraged. The Jin empire prospered and had a large surplus of grain in reserve. Although learned in Chinese classics, Shizong was also known as a promoter of Jurchen language and culture; during his reign, a number of Chinese classics were translated into Jurchen, the Imperial Jurchen Academy was founded, and the Imperial examinations started to be offered in the Jurchen language. Emperor Shizong's reign (1161–1189) was remembered by the posterity as the time of comparative peace and prosperity, and the emperor himself was compared to the legendary Yao and Shun
Emperor Shizong's grandson, Emperor Zhangzong (r. 1189–1208) venerated Jurchen values, but he also immersed himself in Chinese culture and married an ethnic Han woman. The "Taihe Code of law" was promulgated in 1201 and was based mostly on the Tang Code. In 1207 the Song tried to invade, but the Jin forces effectively repulsed them. In the peace agreement the Song had to pay higher annual indemnities and behead Han Tuozhou, the leader of their hawkish party.
Fall of Jin.
Starting from the early 13th century the Jin dynasty began to feel the pressure of Mongols from the north. Genghis Khan first led the Mongols into Western Xia territory in 1205 and ravaged it four years later. In 1211 about 50,000 Mongols on horses invaded the Jin Empire and began absorbing Khitan and Jurchen rebels. The Jin army had a half million men with 150,000 cavalry but abandoned the “western capital” Datong (see also Badger's Mount Campaign). The next year the Mongols went north and looted the Jin "eastern capital", and in 1213 they besieged the "central capital", Zhongdu (Beijing). In 1214 the Jin made a humiliating treaty but retained the capital. That summer, Emperor Xuanzong abandoned the central capital and moved the government to the "southern capital" of Kaifeng, making it the official seat of Jin dynasty power. In 1216 a war faction persuaded Emperor Xuanzong to attack the Song, but in 1219 they were defeated at the same place by the Yangtze River where the Prince of Hailing had been defeated in 1161. The Jin now faced a two front war that they could not afford. Furthermore, Emperor Aizong won a succession struggle against his brother and then quickly ended the war and went back to the capital. He made peace with the Tanguts of Western Xia, who had been allied with the Mongols.
Many Han Chinese and Khitans defected to the Mongols to fight against the Jin. Two Han Chinese leaders, Shi Tianze, Liu Heima (劉黑馬, Liu Ni), and the Khitan Xiao Zhala defected and commanded the 3 tumens in the Mongol army. Liu Heima and Shi Tianze served Ogödei Khan. Liu Heima and Shi Tianxiang led armies against Western Xia for the Mongols. There were 4 Han tumens and 3 Khitan tumens, with each tumen consisting of 10,000 troops.
Shi Tianze was a Han Chinese who lived in the Jin dynasty (1115–1234). Interethnic marriage between Han and Jurchen became common at this time. His father was Shi Bingzhi (史秉直). Shi Bingzhi was married to a Jurchen woman (surname Na-ho) and a Han Chinese woman (surname Chang), it is unknown which of them was Shi Tianze's mother. Shi Tianze was married to two Jurchen women, a Han Chinese woman, and a Korean woman, and his son Shi Gang was born to one of his Jurchen wives. His Jurchen wive's surnames were Mo-nien and Na-ho, his Korean wife's surname was Li, and his Han Chinese wife's surname was Shi. Shi Tianze defected to the Mongol Empire's forces upon their invasion of the Jin dynasty. His son Shi Gang married a Kerait woman, the Kerait were Mongolified Turkic people and considered as part of the "Mongol nation".
The Yuan dynasty created a "Han Army" out of defected Jin troops.
Genghis Khan died in 1227 while his armies were conquering Western Xia. His son Ögedei Khan invaded the Jin Empire in 1232 with assistance from the Southern Song dynasty. The Jurchens tried to resist; but when the Mongols besieged Kaifeng in 1233, Emperor Aizong fled south to the city of Caizhou. An allied army of Song and Mongols looted the capital, and the next year Emperor Aizong committed suicide to avoid being captured when the Mongols besieged Caizhou, ending the Jin dynasty in 1234. The territory of the Jin was to be divided between the Mongols and the Song. However, due to lingering territorial disputes, the Song and the Mongols eventually went to war with one another over these territories.
In "Empire of The Steppes", René Grousset reports that the Mongols were always amazed at the valor of the Jin warriors, who held out until seven years after the death of Genghis Khan.
Military.
Contemporary Chinese writers ascribed Jurchen success in overwhelming the Liao and Northern Song mainly to their cavalry. Already during Aguda's rebellion against the Liao, all Jurchen fighters were mounted. It was said that the Jurchen cavalry tactics were a carryover from their hunting skills.
Jurchen horsemen were provided with heavy armor; on occasions, they would use a team of horses attached to each other with chains (拐子马, "guaizi ma")
As the Liao Empire fell apart and the Song retreated beyond the Yangtze, the army of the new Jin dynasty absorbed many soldiers who formerly fought for the Liao or Song. The new Jin empire adopted many of the Song's weapons, including various machines for siege warfare and artillery. In fact, the Jin's use of cannons, grenades, and even rockets to defend besieged Kaifeng against the Mongols in 1233 is considered the first ever battle in human history in which gunpowder was used effectively, even though it failed to prevent the eventual Jin defeat.
On the other hand, the Jin was not particularly good at naval warfare. Both in 1129–30 and in 1161 Jin forces were defeated by the Southern Song navies when trying to cross the Yangtze River into the core Southern Song territory (see Battle of Tangdao and Battle of Caishi), even though for the latter campaign the Jin had equipped a large navy of their own, using Chinese shipbuildiers and even Chinese captains who had defected from the Southern Song.
In 1130 the Jin army reached Hangzhou and Ningbo in southern China. But heavy Chinese resistance and the geography of the area halted the Jin advance, and they were forced retreat and withdraw, and they had not been able to escape the Song navy when trying to return until they were directed by a Chinese defector who helped them escape in Chenkiang. Southern China was then cleared of the Jurchen forces.
The Jin Great Wall.
In order to prevent incursion from the Mongols, a large construction program was launched. The records show that two important sections of the Great Wall were completed by the Jurchen Jin dynasty.
The Great Wall as constructed by the Jurchens differed from the previous dynasties. Known as the Border Fortress or the Boundary Ditch of the Jin, it was formed by digging ditches within which lengths of wall were built. In some places subsidiary walls and ditches were added for extra strength. The construction was started in about 1123 and completed by about 1198. The two sections attributable to the Jin Dynasty are known as the Old Mingchang Walls and New Great Walls, together stretching more than 2,000 kilometers in length.[http://www.travelchinaguide.com/china_great_wall/history/jin/]
Government.
The government of the Jin dynasty merged Jurchen customs with institutions adopted from the Liao and Song dynasties. The predynastic Jurchen government was based on the quasi-egalitarian tribal council. Jurchen society at the time did not have a strong political hierarchy. The "Shuo Fu" (說郛) records that the Jurchen tribes were not ruled by central authority and locally elected their chieftains. Tribal customs were retained after Aguda united the Jurchen tribes and formed the Jin dynasty, coexisting alongside more centralized institutions. The Jin dynasty had five capitals, a practice they adopted from the Balhae and the Liao. The Jin had to overcome the difficulties of controlling a multi-cultural empire composed of territories once ruled by the Liao and Northern Song. The solution of the early Jin government was to establish separate government structures for different ethnic groups.
Legacy.
In the 17th century, the Jurchen chief Nurhaci combined the three Jurchen tribes after thirty years of struggle and founded the Later Jin dynasty (1616–1636). Nurhaci's eighth son and heir, Huangtaiji, later changed the name of his people from Jurchen to Manchu in 1635. The next year, he changed the name of the Later Jin to Qing in 1636. However, the Qing imperial family, the Aisin Gioro, are unrelated to the Jin imperial family, the Wanyan.

</doc>
<doc id="55028" url="https://en.wikipedia.org/wiki?curid=55028" title="Frederick Douglas">
Frederick Douglas

Frederick Douglas may refer to:

</doc>
<doc id="55029" url="https://en.wikipedia.org/wiki?curid=55029" title="Larry Bird">
Larry Bird

Larry Joe Bird (born December 7, 1956) is an American retired professional basketball player who played for the Boston Celtics of the National Basketball Association (NBA). Since retiring as a player, he has been a mainstay in the Indiana Pacers organization, currently serving as team president. Drafted into the NBA sixth overall by the Boston Celtics in 1978, Bird started at small forward and power forward for thirteen seasons, spearheading one of the NBA's most formidable frontcourts that included center Robert Parish and forward Kevin McHale. Bird was a 12-time NBA All-Star and was named the league's Most Valuable Player (MVP) three consecutive times (1984–1986). He played his entire professional career for Boston, winning three NBA championships and two NBA Finals MVP awards.
He was a member of the 1992 United States men's Olympic basketball team ("The Dream Team") that won the gold medal at the 1992 Summer Olympics. Bird was voted to the NBA's 50th Anniversary All-Time Team in 1996 and inducted into the Naismith Memorial Basketball Hall of Fame in 1998 (and was inducted again 2010 as a member of the "Dream Team").
He served as head coach of the Indiana Pacers from 1997 to 2000. In 2003, he assumed the role of president of basketball operations for the Pacers, holding the position until retiring in 2012. After a year away from the position, he announced he would return to the Pacers as president of basketball operations in 2013. In addition to being part of the 50–40–90 club, he is the only person in NBA history to be named Most Valuable Player, Coach of the Year, and Executive of the Year.
Early life.
Bird was born in West Baden, Indiana to Georgia (née Kerns) and Claude Joseph "Joe" Bird. He was raised in nearby French Lick, where his mother worked two jobs to support Larry and his five siblings. Bird has said that being poor as a child still motivates him "to this day". Georgia and Joe divorced when Larry was in high school, and Joe committed suicide about a year later. Larry used basketball as an escape from his family troubles, starring for Springs Valley High School and averaging 31 points, 21 rebounds, and 4 assists as a senior on his way to becoming the school's all-time scoring leader.
College career.
Bird received a scholarship to play college basketball for the Indiana University Hoosiers in 1974. After less than a month on campus he dropped out of school, finding the adjustment between his small hometown and the large student population of Bloomington to be overwhelming. He returned to French Lick, enrolling at Northwood Institute in nearby West Baden and working municipal jobs for a year before enrolling at Indiana State University in 1975. He had a successful three-year career with the Sycamores, helping them reach the NCAA tournament for the first time in school history and leading them to the championship game against Michigan State in 1979. Indiana State would lose the game 75–64, with Bird scoring 19 points but making only 7 of 21 shots for 33.3 percent shooting rate. The game achieved the highest ever rating for a college basketball game in large part because of the match-up between Bird and Spartans' point guard Earvin "Magic" Johnson, a rivalry that lasted throughout their professional careers. Despite failing to win the championship, Bird earned numerous year-end awards and honors for his outstanding play, including the Naismith College Player of the Year Award. For his college career, he averaged 30.3 points, 13.3 rebounds, and 4.6 assists per game, leading the Sycamores to an 81–13 record during his tenure.
Professional career.
Joining the Celtics (1978–79).
Bird was selected by the Boston Celtics with the sixth overall pick in the 1978 NBA draft. He did not sign with the Celtics immediately; instead, he played out his final season at Indiana State and led the Sycamores to the NCAA title game. Red Auerbach publicly stated that he would not pay Bird more than any Celtic on the current roster, but Bird's agent bluntly told Red that Bird would reject any sub-market offers and simply enter the 1979 NBA Draft instead, where Boston's rights would expire the second the draft began and Bird would have been a likely top-2 or 3 pick. After protracted negotiations, Bird inked a five-year, $3.25 million contract with the team, making him the highest paid rookie in league history at the time. Shortly afterwards, NBA draft eligibility rules were changed to prevent teams from drafting players before they were ready to sign, a rule known as the Bird Collegiate Rule.
Early success (1979–83).
Bird immediately transformed the Celtics into a title contender, helping them improve their win total by 32 games from the year before he was drafted and finish first in the Eastern Conference. With averages of 21.3 points, 10.4 rebounds, 4.5 assists, and 1.7 steals per game for the season, he was selected to the All-Star Team and named Rookie of the Year. In the Conference Finals, Boston was eliminated by the Philadelphia 76ers.
Before the 1980–81 season, the Celtics selected forward Kevin McHale in the draft and acquired center Robert Parish from the Golden State Warriors, forming a Hall of Fame trio for years to come. Behind Bird's leadership and Boston's upgraded roster, the Celtics again advanced to the Conference Finals for a rematch with the 76ers. Boston fell behind 3–1 to start the series but won the next three games to advance to the Finals against the Houston Rockets, winning in six games and earning Bird his first championship. He averaged 21.9 points, 14 rebounds, 6.1 assists, and 2.3 steals per game for the postseason and 15.3 points, 15.3 rebounds, and 7 assists per game for the Finals but lost out on the Finals MVP Award to teammate Cedric Maxwell.
At the 1982 All-Star Game, Bird scored 19 points en route to winning the All-Star Game MVP Award. At the conclusion of the season, he earned his first All-Defensive Team selection. He eventually finished runner-up in Most Valuable Player Award voting to Moses Malone. In the Conference Finals, the Celtics faced the 76ers for the third consecutive year, losing in seven games. Boston's misfortunes continued into the next season, with Bird again finishing second in MVP voting to Malone and the team losing in the Conference Semifinals to the Milwaukee Bucks.
Battles with the Lakers and MVP tenure (1983–87).
Bird was named MVP of the 1983–84 season with averages of 24.2 points, 10.1 rebounds, 6.6 assists, and 1.8 steals per game. In the playoffs, the Celtics avenged their loss from the year before to the Bucks, winning in five games in the Conference Finals to advance to the Finals against the Los Angeles Lakers. The Lakers, led by Bird's college rival Magic Johnson, were on the verge of putting the series away in Game 4 before a flagrant foul was committed on Kurt Rambis that resulted in a brawl and caused Los Angeles to lose their composure. Boston came back to win the game, eventually winning the series in seven. Bird was named Finals MVP behind 27.4 points, 14 rebounds, and 3.6 assists per game.
On March 12 of the 1984–85 season, Bird scored a career-high and franchise record 60 points in a game against the Atlanta Hawks. The performance came just nine days after Kevin McHale set the previous Celtics record for points in a game with 56. At the conclusion of the year, Bird was named MVP for the second consecutive season behind averages of 28.7 points, 10.5 rebounds, and 6.6 assists per game. Boston advanced through the playoffs to earn a rematch with the Lakers, this time losing in six games.
In the summer of 1985, Larry injured his back shoveling crushed rock to create a driveway at his mother's house. At least partially as a result of this, he experienced back problems for the remainder of his career.
Before the start of the 1985–86 season, the Celtics made a daring trade for Bill Walton, an All-Star center with a history of injury. The risk paid off; Walton's acquisition helped Boston win a league best 67 games. One of Bird's career highlights occurred at the 1986 NBA All-Star Weekend when he walked into the locker room at the inaugural Three-Point Shootout and asked who was going to finish second before winning the shootout. With averages of 25.8 points, 9.8 rebounds, and 6.8 assists, and 2 steals per game, Bird became just the third player in NBA history to win three consecutive MVP Awards. In the playoffs, the Celtics lost only one game through the first three rounds en route to a match-up against the Rockets in the Finals. Bird averaged 24 points, 9.7 rebounds, and 9.5 assists per game for the championship round, leading Boston to victory in six games. The '86 Celtics are commonly ranked as one of the greatest basketball teams of all-time, with the "Boston Globe"'s Peter May and Grantland's Bill Simmons listing them at number one.
In 1987, the Celtics made their last Finals appearance of Bird's career, fighting through difficult series against the Milwaukee Bucks and Detroit Pistons but as they reached the NBA Finals, the Celtics, hampered by devastating injuries, lost to a dominant Lakers team which had won 65 games during the season. The Celtics ended up losing to the Lakers in six games, with Bird averaging 24.2 points on .445 shooting, 10 rebounds and 5.5 assists per game in the championship series. The Celtics would fall short in 1988 losing to the Detroit Pistons in 6 games in the Eastern Conference Finals as the Pistons made up from the heartbreak the previous season. Between them, Bird and Johnson captured eight NBA championships during the 1980s, with Magic getting five and Bird three. During the 1980s, either Boston or Los Angeles appeared in every NBA Finals.
Throughout the 1980s, contests between the Celtics and the Lakers—both during the regular season and in the Finals—attracted enormous television audiences. The first regular season game between the Celtics and the Lakers in the 1987–88 season proved to be a classic with Magic Johnson banking in an off balance shot from near the three-point line at the buzzer for a 115–114 Lakers win at Boston Garden. The historical rift between the teams, which faced each other several times in championship series of the 1960s, fueled fan interest in the rivalry. Not since Bill Russell squared off against Wilt Chamberlain had professional basketball enjoyed such a marquee matchup. The apparent contrast between the two players and their respective teams seemed scripted for television: Bird, the introverted small-town hero with the blue-collar work ethic, fit perfectly with the throwback, hard-nosed style of the Celtics, while the stylish, gregarious Johnson ran the Lakers' fast-paced Showtime offense amidst the bright lights and celebrities of Los Angeles. A 1980s Converse commercial for its "Weapon" line of basketball shoes (endorsed by both Bird and Johnson) reflected the perceived dichotomy between the two players. In the commercial, Bird is practicing alone on a rural basketball court when Johnson pulls up in a sleek limousine and he challenged him to a one-on-one match.
Despite the intensity of their rivalry, Bird and Johnson became friends off the court. Their friendship blossomed when the two players worked together to film the Converse commercial, which depicted them as archenemies. Johnson appeared at Bird's retirement ceremony on February 4, 1993 and emotionally described Bird as a "friend forever".
Waning years (1988–92).
In 1988, Bird had the best statistical season of his career, but the Celtics failed to reach the NBA Finals for the first time in five years, losing to the Pistons in six games during the Eastern Conference Finals. Bird started the 1988–89 season, but ended his season after six games to have bone spurs surgically removed from both of his heels. He returned to the Celtics in 1989, but debilitating back problems and an aging Celtic roster prevented him from regaining his mid-1980s form. Nonetheless, through the final years of his career, Bird maintained his status as one of the premier players in the game. He averaged over 20 points, 9 rebounds and 7 assists a game in his last three seasons with the Celtics, and shot better than 45% from the field in each. Bird led the Celtics to playoff appearances in each of those three seasons.
Bird's body, however, continued to break down. He had been bothered by back problems for years, and his back became progressively worse. After leading the Celtics to a 29–5 start to the 1990–91 season, he missed 22 games due to a compressed nerve root in his back, a condition that would eventually lead to his retirement. He had off-season surgery to remove a disc from his back, but his back problems continued and he missed 37 games during the 1991–92 season. His past glory would be briefly rekindled, however, in a game that season in which he scored 49 points in a double-overtime victory over the Portland Trail Blazers. During the 1992 Eastern Conference semi-finals against the Cleveland Cavaliers, Bird missed four of the seven games in the series due to those recurring back problems.
In the summer of 1992, Bird joined Magic Johnson, Michael Jordan and other NBA stars to play for the United States basketball team in that year's Olympics in Barcelona, Spain. It was the first time in America's Olympic history that the country sent professional basketball players to compete. The "Dream Team" won the men's basketball gold medal.
Following his Olympic experience, on August 18, 1992, Bird announced his retirement as an NBA player. He finished his career with averages of more than 24 points, 10 rebounds and 6 assists per game, while shooting 49.6% from the field, 88.6% from the free throw line and 37.6% from three-point range. Following Bird's departure, the Celtics promptly retired his jersey number 33.
In 1989, Bird published his autobiography, "" with Bob Ryan. The book chronicles his life and career up to the 1989 NBA season.
Post-retirement career.
The Celtics employed Bird as a special assistant in the team's front office from 1992 until 1997. In 1997, Bird accepted the position of coach of the Indiana Pacers and said he would be on the job for no more than three years. Despite having no previous coaching experience, Bird led the Pacers to a 58–24 record—the franchise's best as an NBA team at the time—in the 1997–98 season, and pushed the Bulls to seven games in the Eastern Conference finals. He was named the NBA Coach of the Year for his efforts, becoming the only man in NBA history to have won both the MVP and Coach of the Year awards. He then led the Pacers to two consecutive Central Division titles in 1999 and 2000, and a berth in the 2000 NBA Finals.
Bird resigned as Pacers coach shortly after the end of the 2000 season, following through on his initial promise to coach for only three years. In 2003, he returned as the Pacers' President of Basketball Operations, overseeing team personnel and coaching moves, as well as the team's draft selections. Bird promoted David Morway to general manager in 2008, but Bird still had the final say in basketball matters. After the 2011–2012 NBA season, Bird was named NBA Executive of the Year.
On June 27, 2012, a day before the 2012 NBA draft, Bird and the Pacers announced that they would be parting ways later that summer. Bird said health issues were among the reasons for his leaving. Donnie Walsh was named to replace him.
On June 26, 2013, almost exactly a year later, it was announced that Bird would be returning to the Pacers as president of basketball operations. Pacers owner Herb Simon briefly addressed Bird's prior health concerns, stating that "He's got his energy back, his health back and he's raring to go".
Awards and honors.
As player:
As coach:
As executive:
Personal life.
Bird married Dinah Mattingly in 1989. They have two adopted children, Conner and Mariah. Bird also has a biological daughter, Corrie, from his first marriage. He has four brothers, Mike, Mark, Jeff, and Eddie, and a sister, Linda. Eddie also played basketball at Indiana State from 1986 to 1990 and today is the city park superintendent at Terre Haute.
In the 1980s and 1990s, Bird co-owned Larry Bird's Boston Connection, a hotel and restaurant in downtown Terre Haute. The property is now a Quality Inn.
Legacy.
In 1999, Bird ranked No. 30 in "ESPN's ".
For the 2008 NBA Finals, which featured a rematch of the Celtics-Lakers rivalry, Bird appeared in a split-screen advertisement with Magic Johnson (as part of the "There Can Only Be One" campaign which had played throughout the 2008 NBA Playoffs but to that point only featured players from the two teams competing in a given series) discussing the meaning of rivalries.
Bird was widely considered one of Red Auerbach's favorite players. He considered Bird to be the greatest basketball player of all time. Auerbach was so enamored with the player that he drafted him out of Indiana State and waited a year before Bird was eligible to suit up for the Celtics. During his introductory press conference, after Auerbach's contentious negotiations with agent Bob Woolf, Bird announced he "would have played for free". This was after Woolf asked for the most lucrative contract in NBA history, to which Auerbach was quick to point out that Bird had not played a game in the NBA yet.
Bird is the only man to be named an MVP, Coach of the Year, and Executive of the Year in the NBA.
Player profile.
Bird, a versatile wing man who played the small forward and power forward positions, is considered one of the greatest players of all time, to which his twelve All-Star team nominations are a testament. The sharpshooting Bird made his name stepping up his performance in critical situations, and is credited with a long list of dominating games, buzzer beaters and clutch defensive plays. He won two NBA Finals MVP and three regular-season MVP awards. He won them all in a row, a feat only shared by Bill Russell and Wilt Chamberlain.
Bird possessed an uncanny and unparalleled ability to anticipate and react to the strategies of his opponents. His talent for recognizing the moves of opponents and teammates prompted his first coach with the Celtics, Bill Fitch, to nickname him "Kodak", because he seemed to formulate mental pictures of every play that took place on the court.
Bird scored 24.3 points per game in his career on a high .496 field goal average, a stellar .886 free throw average (9th best all-time) and a 37.6 percentage on three-point shots. Bird was also a good rebounder (10.0 rebound career average) and an excellent playmaker (6.3 assist career average). His multidimensional game made him a consistent triple-double threat; Bird currently ranks fifth all-time in triple-doubles with 59, not including the 10 he recorded in the playoffs. Bird's lifetime player efficiency rating (PER) is 23.5, 18th all-time, a further testament to his all around game. Additionally, he is the only 20, 10, 5 player in NBA history (points, rebounds, assists per game) with a lifetime PRA rating (points + rebounds + assists per game) of 40.6, which is 8th all-time. Bird was the first player in NBA history to shoot 50% or better on field goals, 40% on three-pointers, and 90% on free-throws in a single NBA season while achieving the league minimum for makes in each category. Bird accomplished this feat twice and is second only to Steve Nash for seasons in the 50–40–90 club.
Bird is also remembered as an excellent defender. While he was neither fast nor quick-footed, and could not always shut down an individual player one-on-one, he consistently displayed a knack for anticipating the moves of his opponent, allowing him to intercept passes and create turnovers. His 1,556 career steals ranks 27th all-time. Unspectacular but effective defensive moves, such as jumping into a passing lane to make a steal or allowing his man to step past and drive to the hoop, then blocking the opponent's shot from behind, were staples of Bird's defensive game. In recognition of his defensive abilities, Bird was named to three All-Defensive Second Teams.
Bird's humble roots were the source of his most frequently used moniker, "The Hick From French Lick". Other observers called him "The Great White Hope". He has also acquired the nickname "Larry Legend".
Trash-talking.
Bird's competitive nature often emerged in nearly constant trash-talking on the court. Some notable examples follow:
Memorable moments.
Bird is remembered as one of the foremost clutch performers in the history of the NBA. Few players have performed as brilliantly in critical moments of games.

</doc>
<doc id="55030" url="https://en.wikipedia.org/wiki?curid=55030" title="Ogonek">
Ogonek

The ogonek (Polish: , "little tail", the diminutive of "ogon"; , "nasal") is a diacritic hook placed under the lower right corner of a vowel in the Latin alphabet used in several European languages, and directly under a vowel in several Native American languages.
Use.
Example in Polish:
Example in Cayuga:
Example in Dogrib:
Example in Lithuanian:
Example in Elfdalian:
Example in Western Apache:
lęk'e' created
Values.
Nasalization.
The use of the ogonek to indicate nasality is common in the transcription of the indigenous languages of the Americas. This usage originated in the orthographies created by Christian missionaries to transcribe these languages. Later, the practice was continued by Americanist anthropologists and linguists who still follow this convention in phonetic transcription to the present day (see Americanist phonetic notation).
The ogonek is also used in academic transliteration of Old Church Slavonic. In Polish, Old Church Slavonic, Navajo, Western Apache, Chiricahua, Tłįchǫ Yatiì, Slavey, Dëne Sųłiné and Elfdalian it indicates that the vowel is nasalized. Even if "ę" is nasalized "e" in Polish, "ą" is nasalized "o" not "a" (this is so because of the vowel change — "ą" was a long nasal "a", which turned into short nasal "o", when the vowel quantity distinction disappeared).
Length.
In Lithuanian, the nosinė (literally, "nasal") mark originally indicated vowel nasalization but about the end of the 17th century nasal vowels gradually evolved into corresponding long non-nasal vowels in most dialects. Thus in modern Lithuanian the mark has de facto become an indicator of vowel length (though not the only one available; the length of etymologically non-nasal vowels is marked differently in writing). At the same time the nosinė mark performs the useful task of distinguishing different grammatical forms which otherwise would be written identically (though not identically pronounced).
Openness.
In Rheinische Dokumenta, it marks vowels which are more open than those denoted by their base letters Ää, Oo, Öö. Here it can be combined with umlaut marks in two cases.
Similar diacritics.
E caudata and o caudata.
The "E caudata" ("ę"), a symbol similar to an "e" with ogonek, evolved from a ligature of "a" and "e" in medieval scripts, in Latin and Irish palaeography. The "O caudata" of Old Norse (letter "ǫ", with "ǫ́") is used to write the open-mid back rounded vowel, . Medieval Nordic manuscripts show this "hook" in both directions, in combination with several vowels. Despite this distinction, the term "ogonek" is sometimes used in discussions of typesetting and encoding Norse texts, as "o caudata" is typographically identical to o with ogonek.
Cedilla and comma.
The ogonek is functionally equivalent to the cedilla and comma diacritics. If two of these three are used within the same orthography their respective use is restricted to certain classes of letters, i.e. usually the ogonek is used with vowels whereas the cedilla is applied to consonants. In handwritten text the marks may even look the same.
Typographical notes.
The ogonek should be almost the same size as a descender (in larger type sizes may be relatively quite shorter) and should not be confused with the cedilla or comma diacritic marks used in other languages.
When used for Native American languages, the ogonek should be placed directly under the letter rather than to the side as is the norm for European languages. European-style placement is acceptable when no other alternatives are available.
LaTeX2e.
In LaTeX2e, macro codice_1 will typeset a letter with ogonek, if it is supported by the font encoding, e.g. codice_2 will typeset "ą". (The default LaTeX OT1 encoding does not support it, but the newer T1 one does. It may be enabled by saying codice_3 in the preamble.)
However, codice_4 rather places the diacritic "right-aligned" with the carrying "e" (ę), suitably for Polish, while codice_5 horizontally "centers" the diacritic with respect to the carrier, suitably for Native American Languages as well as for e caudata and o caudata. So codice_6 better fits the latter purposes. Actually, codice_7 (for ǫ) is defined to result in codice_8, and codice_9 is defined to result in codice_10.
The package TIPA, activated by using the command "codice_11", offers a different way: "codice_12" will produce "ą".

</doc>
<doc id="55032" url="https://en.wikipedia.org/wiki?curid=55032" title="Outline of agriculture">
Outline of agriculture

The following outline is provided as an overview of and topical guide to agriculture:
Agriculture – cultivation of animals, plants, fungi and other life forms for food, fiber, and other products used to sustain life.
What "type" of thing is agriculture?
Agriculture can be described as all of the following:
Branches of agriculture.
By industry.
Farming.
Farming equipment.
Farm equipment – any kind of machinery used on a farm to help with farming.
Fishing.
Fishing – activity of trying to catch fish. Fish are normally caught in the wild. Techniques for catching fish include hand gathering, spearing, netting, angling and trapping.
Forestry.
Forestry – interdisciplinary profession embracing the science, art, and craft of creating, managing, using, and conserving forests and associated resources in a sustainable manner to meet desired goals, needs, and values for human benefit.
Ranching.
Ranching – practice of raising grazing livestock such as cattle or sheep for meat or wool.
Agricultural Disciplines.
Agricultural chemistry.
Agricultural chemistry – study of both chemistry and biochemistry which are important in agricultural production, the processing of raw products into foods and beverages, and in environmental monitoring and remediation.
Agricultural communication.
Agricultural communication – field of study and work that focuses on communication about agricultural related information among agricultural stakeholders and between agricultural and non-agricultural stakeholders.
Agricultural economics.
Agricultural economics – originally applied the principles of economics to the production of crops and livestock — a discipline known as agronomics. Agronomics was a branch of economics that specifically dealt with land usage. It focused on maximizing the crop yield while maintaining a good soil ecosystem. Throughout the 20th century the discipline expanded and the current scope of the discipline is much broader. Agricultural economics today includes a variety of applied areas, having considerable overlap with conventional economics.
Agricultural education.
Agricultural education – instruction about crop production, livestock management, soil and water conservation, and various other aspects of agriculture.Farmers acquire adequate knowledge required on the correct amount use of agrochemicals and other agriculture related technologies.
Agricultural universities and colleges – tertiary agricultural educational institutions around the world
Agricultural engineering.
Agricultural engineering – engineering discipline that applies engineering science and technology to agricultural production and processing.
Agricultural philosophy.
Agricultural philosophy – discipline devoted to the systematic critique of the philosophical frameworks (or ethical world views) that are the foundation for decisions regarding agriculture.
Agricultural policy.
Agricultural policy – set of laws relating to domestic agriculture and imports of foreign agricultural products. 
Agronomy.
Agronomy – science and technology of producing and using plants for food, fuel, feed, fiber, and reclamation.
Horticulture.
Horticulture – art, science, technology and business of intensive plant cultivation for human use.
Agricultural soil science.
Agricultural soil science – branch of soil science that deals with the study of edaphic conditions as they relate to the production of food and fiber.
Agroecology.
Agroecology – application of ecological principles to the production of food, fuel, fiber, and pharmaceuticals and the management of agroecosystems.
History of agriculture.
History of agriculture – developed at least 10,000 years ago, although some forms of agriculture such as forest gardening and fire-stick farming date back even earlier to prehistoric times.
Agriculturally based manufacturing industries.
Food industry.
Food industry – complex, global collective of diverse businesses that together supply much of the food energy consumed by the world population.
Pulp and paper industry.
Pulp and paper industry – comprises companies that use wood as raw material and produce pulp, paper, board and other cellulose-based products.

</doc>
<doc id="55033" url="https://en.wikipedia.org/wiki?curid=55033" title="Great Dividing Range">
Great Dividing Range

The Great Dividing Range, or the Eastern Highlands, is Australia's most substantial mountain range and the third longest land-based range in the world. The range stretches more than from Dauan Island off the northeastern tip of Queensland, running the entire length of the eastern coastline through New South Wales, then into Victoria and turning west, before finally fading into the central plain at the Grampians in western Victoria. The width of the range varies from about to over .
The sharp rise between the coastal lowlands and the eastern uplands has affected Australia's climate, mainly due to orographic precipitation, and these areas of highest relief have revealed an impressive gorge country.
Terminology.
The Dividing Range does not consist of a single mountain range. It consists of a complex of mountain ranges, plateaus, upland areas and escarpments with an ancient and complex geological history. The physiographic division name for the landmass is called the "East Australian Cordillera". In some places the terrain is relatively flat, consisting of very low hills. Typically the highlands range from 300 m to 1,600 m in height.
The mountains and plateaus, which consist of limestones, sandstone, quartzite, schists and dolomite, have been created by faulting and folding processes.
The crest of the range is defined by the watershed or boundary between the drainage basins of rivers which drain directly eastward into the Pacific Ocean, or southward into Bass Strait, and those rivers which drain into the Murray–Darling river system towards the west and south. In the north, the rivers on the west side of the range drain towards the Gulf of Carpentaria.
The higher and more rugged parts of the "range" do not necessarily form part of the crest of the range, but may be branches and offshoots from it. The term "Great Dividing Range" may refer specifically to the watershed crest of the range, or to the entire upland complex including all of the hills and mountains between the east coast of Australia and the central plains and lowlands. At some places it can be up to 400 km wide. Notable ranges and other features which form part of the range complex have their own distinctive names.
History.
The Great Dividing Range was formed during the Carboniferous period—some 300 million years ago—when Australia collided with what is now parts of South America and New Zealand. The range has experienced significant erosion since. (See Geology of Australia.)
Prior to British colonisation the ranges were home to Aboriginal Australian tribes. Evidence remains in some places of their occupation by decorated caves, campsites and trails used to travel between the coastal and inland regions.
After British colonisation in 1788, the ranges were an obstacle to exploration and settlement by the British settlers. Although not high, parts of the highlands were very rugged. Crossing the Blue Mountains was particularly challenging due to the mistaken idea that the creeks should be followed rather than the ridges, and almost impenetrable, labyrinthine, sandstone mountains.
In 1813, a usable route was finally discovered directly westward from Sydney across the Blue Mountains to Bathurst by an expedition jointly led by Gregory Blaxland, William Lawson and William Charles Wentworth. They found a passage by following the top of a ridge. Towns in the Blue Mountains were later named after each of these men. This was the start of the development of the agricultural districts of inland New South Wales. A road was built to Blaxland by convicts within six months. Easier routes to inland New South Wales were discovered towards Goulburn to the southwest, and westwards from Newcastle.
Subsequent explorations were made across and around the ranges by Allan Cunningham, John Oxley, Hamilton Hume, Paul Edmund Strzelecki, Ludwig Leichhardt and Thomas Mitchell. These explorers were mainly concerned with finding good agricultural land.
By the late 1830s the most fertile rangelands adjacent to the mountains ranges had been explored and some settled. These included the Gippsland and Riverina regions in the south, up to the Liverpool Plains and the Darling Downs in the north.
Various road and railway routes were subsequently established through many parts of the ranges, although many areas remain remote to this day. For example, in eastern Victoria there is only one major road crossing the highlands from north to south.
Notable components.
Parts of the highlands consisting of relatively flat and, by Australian standards, well-watered land were developed for agricultural and pastoral uses. Such areas include the Atherton Tableland and Darling Downs in Queensland, and the Northern Tablelands, Southern Highlands and Southern Tablelands in New South Wales. Other parts of the highlands are too rugged for agriculture and have been used for forestry. Many parts of the highlands which were not developed are now included in National Parks.
All of mainland Australia's alpine areas, including its highest mountain, Mount Kosciuszko ( AHD), are part of this range, called the Main Range. The highest areas in southern New South Wales and eastern Victoria are known as the Australian Alps.
The central core of the Great Dividing Range is dotted with hundreds of peaks and is surrounded by many smaller mountain ranges or spurs, canyons, valleys and plains of regional significance. Some of the major plains include the High Plains of South-Eastern Australia, the Southern Highlands the Central Highlands and Bogong High Plains of Victoria. Other tablelands considered part of the Great dividing range are the Atherton Tableland, Canberra wine region and the Southern Tablelands.
The Dandenong Ranges, Barrington Tops, Bunya Mountains, Blue Mountains, Liverpool Range, McPherson Ranges and the Moonbi Range are some of the smaller spurs and ranges that make up the greater dividing range. Other notable ranges and tablelands which form part of the Great Dividing Range include the Liverpool Range, Mount Royal Range and the Monaro District. Whilst some of the peaks of the highlands reach respectable heights of a little over 2,000 metres, the age of the range and its erosion mean that most of the mountains are not very steep, and virtually all peaks can be reached without mountaineering equipment.
In some areas, such as the Snowy Mountains, Victorian Alps, the Scenic Rim and the eastern escarpments of the New England region, the highlands form a significant barrier. The eastern escarpment is the site of many spectacular waterfalls which were formed by rivers plunging off the tablelands. In other areas the slopes are gentle and in places the range is barely perceptible.
Well known passes on the range include Coxs Gap, Cunninghams Gap, Dead Horse Gap, Nowlands Gap, and Spicers Gap.
Major cities located on the upland areas of the range include Canberra, Toowoomba and the outer suburbs of Sydney, Melbourne, Brisbane and Cairns in north Queensland. Many towns and cities are located on the range, and also in lowland areas and foothills adjacent to the highlands. There is a strong natural history and cultural attachment to the Dividing Range region in towns and on many, sometimes remote landholdings.
Some of the towns/cities located on the range include:
Water catchments.
The lower reaches are used for forestry, an activity that causes friction with conservationists. The range is also the source of virtually all of eastern Australia's water supply, both through runoff caught in dams, and throughout much of Queensland, through the Great Artesian Basin.
Valleys along the chain of mountains have yielded a water source for important reservoirs and water supply projects such as the Upper Nepean Scheme, Snowy Mountains Scheme and Warragamba Dam. The Bradfield Scheme has been mooted as a way to transport water from the tropics in coastal Queensland south to dryer regions.
The Great Dividing Range creates the drainage basins of the Australian south-east coast drainage division and the Australian north-east coast drainage division, whose water flows to the east coast and into the Pacific Ocean, Tasman Sea, and Bass Strait with the westerly Murray–Darling basin which flow inland, away from the coast into the interior plains.
Some of the rivers which flow west of the ranges includes the Condamine River, Flinders River, Herbert River, Lachlan River, Macdonald River, Macintyre River and Namoi River. Rivers that flow north into the Murray–Darling Basin from Victoria include the Goulburn, Mitta Mitta, Kiewa, Ovens, King, Loddon and Campaspe rivers. Rivers that flow east into the Pacific Ocean include the Brisbane River, Burdekin River, Clarence River, Hastings River, Hawkesbury River, Hunter River, Macleay River, Mary River, Richmond River, Shoalhaven River and the Snowy River. Those that flow south to the ocean in Victoria include the Snowy, Cann, Tambo, Mitchell, Latrobe, Thomson, Yarra, Werribee, Hopkins and Glenelg rivers.
Features.
At some high hill passes the range provides cool sites appropriate for vineyards.
Railways.
The engineers of early rail passages across the Great Dividing Range needed to find low sections of the range to cross, as well as suitable, low gradient paths up the mountains on either side. Rail passages include:
Road transport.
Many of Australia's highways such as the Alpine Way, Great Alpine Road, Hume Highway, Great Western Highway, Capricorn Highway, Cunningham Highway, New England Highway, Oxley Highway, Warrego Highway, Waterfall Way, Thunderbolts Way, the Calder Highway, the Western Highway, and the Murray Valley Highway traverse parts of the range.
Protected areas.
Much of the range lies within a succession of national parks and other reserves, most of the national parks are listed below, there are almost double the amount of state forests;

</doc>
<doc id="55034" url="https://en.wikipedia.org/wiki?curid=55034" title="Spine">
Spine

Spine or Spinal may refer to:

</doc>
<doc id="55036" url="https://en.wikipedia.org/wiki?curid=55036" title="Fragaria">
Fragaria

Fragaria is a genus of flowering plants in the rose family, Rosaceae, commonly known as strawberries for their edible fruits. There are more than 20 described species and many hybrids and cultivars. The most common strawberries grown commercially are cultivars of the garden strawberry, a hybrid known as "Fragaria" × "ananassa". Strawberries have a taste that varies by cultivar, and ranges from quite sweet to rather tart. Strawberries are an important commercial fruit crop, widely grown in all temperate regions of the world.
Description.
Strawberries are not true berries. The fleshy and edible part of the fruit is a receptacle, and the parts that are sometimes mistakenly called "seeds" are achenes.
Although it is commonly thought that strawberries get their name from straw being used as a mulch in cultivating the plants, the etymology of the word is possibly derived from "strewn berry" in reference to the fruit being "strewn" about the base of the plants.
Classification.
There are more than 20 different "Fragaria" species worldwide. Numbers of other species have been proposed, some of which are now recognized as subspecies. Key to the classification of strawberry species is recognizing that they vary in the number of chromosomes. There are seven basic "types" of chromosomes that they all have in common. However, they exhibit different polyploidy. Some species are diploid, having two sets of the seven chromosomes (14 chromosomes total). Others are tetraploid (four sets, 28 chromosomes total), hexaploid (six sets, 42 chromosomes total), octoploid (eight sets, 56 chromosomes total), or decaploid (ten sets, 70 chromosomes total).
As a rough rule (with exceptions), strawberry species with more chromosomes tend to be more robust and produce larger plants with larger berries.
Uncategorized hybrids.
"F. var. ‘Lipstick’", red-flowered runnering ornamental, sparse small globular fruits.
Ecology.
A number of species of butterflies and moths feed on strawberry plants: see list of Lepidoptera that feed on strawberry plants.
References.
Hogan, Sean (chief consultant), "Flora" (subtitle) "A Gardener’s Encyclopedia", (Portland, Oregon USA) Timber Press, 2003. ISBN 0-88192-538-1.

</doc>
<doc id="55037" url="https://en.wikipedia.org/wiki?curid=55037" title="Shellfish">
Shellfish

Shellfish is a culinary and fisheries term for exoskeleton-bearing aquatic invertebrates used as food, including various species of molluscs, crustaceans, and echinoderms. Although most kinds of shellfish are harvested from saltwater environments, some kinds are found in freshwater. In addition a few species of land crabs are eaten, for example "Cardisoma guanhumi" in the Caribbean.
Despite the name, shellfish are not a kind of fish, but are simply water-dwelling animals. Many varieties of shellfish (crustaceans in particular) are actually closely related to insects and arachnids, making up one of the main classes of the phylum Arthropoda. Cephalopods (squid, octopus, cuttlefish) and bivalves (clams, oysters) are molluscs, as are snails and slugs.
Familiar marine molluscs enjoyed as a food source by humans include many species of clams, mussels, oysters, winkles, and scallops. Some crustaceans commonly eaten are shrimp, lobster, crayfish, and crabs. Echinoderms are not as frequently harvested for food as molluscs and crustaceans; however, sea urchin roe is quite popular in many parts of the world.
Most shellfish eat a diet composed primarily of phytoplankton and zooplankton.
Shellfish are among the most common food allergens.
Terminology.
The term "shellfish" is used both broadly and specifically. In common parlance, as in having "shellfish" for dinner, it can refer to anything from clams and oysters to lobster and shrimp. For regulatory purposes it is often narrowly defined as "filter-feeding molluscs" such as clams, mussels, and oyster to the exclusion of crustaceans and all else.
Although the term is primarily applied to marine species, edible freshwater invertebrates such as crayfish and river mussels are also sometimes grouped under the umbrella of "shellfish".
Although their shells may differ, all shellfish are invertebrates. As non-mammalian animals that spend their entire lives in water they are "fish" in an informal sense; however the term "finfish" is sometimes used to distinguish fish as animals "defined by having vertebrae" from shellfish in modern terminology.
The word "shellfish" is both singular and plural; the rarely used "shellfishes" is sometimes employed to distinguish among various types of shellfish.
Shellfish in various cuisines.
Archaeological finds has shown that humans have been making use of shellfish as a food item for hundreds of thousands of years. In the present, shellfish dishes are a feature of almost all the cuisines of the world, providing an important source of protein in many cuisines around the world, especially in the countries with coastal areas.
In Japan.
In the Japanese cuisine, chefs often use shellfish and their roe in different dishes. Sushi (vinegared rice, topped with other ingredients, including shellfish, fish, meat and vegetables) features both raw and cooked shellfish. Sashimi primarily consists of very fresh raw seafood, sliced into thin pieces. Both sushi and sashimi are served with soy sauce and wasabi paste (a Japanese horseradish root, a spice with extremely strong hot flavor), thinly sliced pickled ginger root, and a simple garnish such as shiso (a kitchen herb, member of the mint family) or finely shredded daikon radish, or both.
In the United States.
Lobster in particular is a great delicacy in the United States, where families in the Northeast region make them into the centerpiece of a clam bake, usually for special occasions. Lobsters are eaten on much of the East Coast; the American lobster ranges from Newfoundland down to about the Carolinas, but is most often associated with Maine. A typical meal involves boiling the lobster with some slight seasoning and then serving it with drawn butter, baked potato, and corn on the cob.
Clamming is done both commercially and recreationally along the Northeast coastline of the US. Various type of clams are incorporated into the cuisine of New England. The soft-shelled clam is eaten either fried or steamed (and then called "steamers"). Many types of clams can be used for clam chowder, but the quahog, a hard shelled clam also known as a chowder clam, is often used because the long cooking time softens its tougher meat.
The Chesapeake Bay and Maryland region has generally been associated more with crabs, but in recent years the area has been trying to reduce its catch of blue crabs, as wild populations have been depleted. This has not, however, stemmed the demand: Maryland-style crabcakes are still a well known treat in crabhouses all over the bay, though the catch now comes from points farther south.
In the Southeast, and particularly the gulf states, shrimping is an important industry. Copious amounts of shrimp are harvested each year in the Gulf of Mexico and the Atlantic Ocean to satisfy a national demand for shrimp. Locally, prawns and shrimp are often deep fried; in the Cajun and Creole kitchens of Louisiana, shrimp and prawns are a common addition to traditional recipes like jambalaya and certain stews. Crawfish are a well known and much eaten delicacy here, often boiled in huge pots and heavily spiced.
In many major cities with active fishing ports, raw oyster bars are also a feature of shellfish consumption. When served freshly shucked (opened) and iced, one may find a liquid inside the shell, called the liquor. Some believe that oysters have the properties of an aphrodisiac.
Inter-tidal herbivorous shellfish such as mussels and clams can help people reach a healthy balance of omega-3 and omega-6 fats in their diets, instead of the current Western diets. For this reason, the eating of shellfish is often encouraged by dietitians.
Shellfish, however, are a rich source of the amino acid taurine.
Around the world.
Shellfish is a common part of indigenous cuisines throughout the globe.
Some popular dishes using shellfish:
Religious dietary restrictions.
The Old Testament forbids the consumption of shellfish, particularly in the books of Leviticus and Deuteronomy. Observant Jews therefore do not eat shellfish.
Toxic content.
Some shellfish, such as whelk, contain arsenic. A sample of whelk was found to have a total content of arsenic at of which 1% is inorganic arsenic.

</doc>
<doc id="55040" url="https://en.wikipedia.org/wiki?curid=55040" title="Reconstruction Era">
Reconstruction Era

The term Reconstruction Era, in the context of the history of the United States, has two senses: the first covers the complete history of the entire country from 1865 to 1877 following the Civil War; the second sense focuses on the transformation of the Southern United States from 1863 to 1877, as directed by Congress, with the reconstruction of state and society.
From 1863 to 1865, Presidents Abraham Lincoln and Andrew Johnson both took moderate positions designed to bring the South back to normal as quickly as possible, while the Radical Republicans used Congress to block any moderate approaches, impose harsh terms, and upgrade the rights of the freedmen. Johnson followed a lenient policy toward ex-Confederates much like Lincoln's. Lincoln's last speeches show that he was leaning toward supporting the enfranchisement of all freedmen, whereas Johnson was opposed to this.
Johnson's interpretations of Lincoln's policies prevailed until the Congressional elections of 1866 in the North, which enabled the Radicals to take control of policy, remove former Confederates from power, and enfranchise the freedmen. A Republican coalition came to power in nearly all the southern states and set out to transform the society by setting up a free labor economy, using the U.S. Army and the Freedmen's Bureau. The Bureau protected the legal rights of freedmen, negotiated labor contracts, and set up schools and even churches for them. Thousands of Northerners came South as missionaries, teachers, businessmen and politicians; hostile elements called them "Carpetbaggers". Rebuilding the rundown railroad system was a major strategy, but it collapsed when a nationwide depression (called the Panic of 1873) struck the economy. The Radicals, frustrated by Johnson's opposition to Congressional Reconstruction, filed impeachment charges but the action failed by one vote in the Senate. In early 1866, Congress passed the Freedmen’s Bureau and Civil Rights Bills and sent them to Johnson for his signature. The first bill extended the life of the bureau, originally established as a temporary organization charged with assisting refugees and freed slaves, while the second defined all persons born in the United States as national citizens who were to enjoy equality before the law. After Johnson vetoed the bills–causing a permanent rupture in his relationship with Congress that would culminate in his impeachment in 1868–the Civil Rights Act became the first major bill to become law over presidential veto.
President Ulysses S. Grant supported Radical Reconstruction and enforced the protection of African Americans in the South through the use of the Enforcement Acts passed by Congress. Grant suppressed the Ku Klux Klan, but was unable to resolve the escalating tensions inside the Republican party between the Carpetbaggers and the Scalawags (native whites in the South). Meanwhile, self-styled Conservatives (in close cooperation with the Democratic Party) strongly opposed Republican rule. They alleged widespread corruption by the Carpetbaggers, excessive state spending and ruinous taxes. The opposition violently counterattacked and regained power in each "redeemed" Southern state by 1877. Meanwhile, public support for Reconstruction policies faded in the North, as voters decided the Civil War was over and slavery was dead. The Democrats, who strongly opposed Reconstruction, regained control of the House of Representatives in 1874; the presidential electoral vote in 1876 was very close and confused, forcing Congress to make the final decision. The deployment of the U.S. Army was central to the survival of Republican state governments; they collapsed when the Army was removed in 1877 as part of a Congressional bargain to elect Republican Rutherford B. Hayes as president.
Reconstruction was a significant chapter in the history of civil rights in the United States, but most historians consider it a failure because the South became a poverty-stricken backwater attached to agriculture, while white Southerners attempted to re-establish dominance through violence, intimidation and discrimination, forcing freedmen into second class citizenship with limited rights, and excluding them from the political process. Historian Eric Foner argues, "What remains certain is that Reconstruction failed, and that for blacks its failure was a disaster whose magnitude cannot be obscured by the genuine accomplishments that did endure."
Dating the Reconstruction Era.
In the different states Reconstruction began and ended at different times; federal Reconstruction finally ended with the Compromise of 1877. In recent decades most historians follow Foner (1988) in dating the Reconstruction of the South as starting in 1863 (with Emancipation) rather than 1865; the usual ending has always been 1877. Reconstruction policies were debated in the North when the war began, and commenced in earnest after Lincoln's Emancipation Proclamation, issued on January 1, 1863.
Overview.
As Confederate states came back under control of the US Army, President Abraham Lincoln set up reconstructed governments in Tennessee, Arkansas, and Louisiana during the war. He experimented by giving land to former slaves in South Carolina. By fall 1865, the new President Andrew Johnson declared the war goals of national unity and the ending of slavery achieved and reconstruction completed. Republicans in Congress, refusing to accept Johnson's terms, rejected new members of Congress, some of whom had been high Confederate officials a few months before. Johnson broke with the Republicans after vetoing two key bills that supported the Freedman's Bureau and provided federal civil rights to the freedmen. The 1866 Congressional elections turned on the issue of Reconstruction, producing a sweeping Republican victory in the North, and providing the Radical Republicans with sufficient control of Congress to override Johnson's vetoes and commence their own "Radical Reconstruction" in 1867.
That same year, Congress removed civilian governments in the South, and placed the former Confederacy under the rule of the U.S. Army. The army conducted new elections in which the freed slaves could vote, while whites who had held leading positions under the Confederacy were temporarily denied the vote and were not permitted to run for office.
In ten states, coalitions of freedmen, recent black and white arrivals from the North (carpetbaggers), and white Southerners who supported Reconstruction (scalawags) cooperated to form Republican biracial state governments. They introduced various reconstruction programs including: funding public schools, establishing charitable institutions, raising taxes, and offering massive aid to support improved railroad transportation and shipping. Conservative opponents called the Republican regimes corrupt and instigated violence toward freedmen and whites who supported Reconstruction. Much of the violence was carried out by members of the Ku Klux Klan (KKK), a secret terrorist organization; this led to federal intervention by President Ulysses S. Grant in 1871 that suppressed the Klan. White Democrats, calling themselves "Redeemers", regained control state by state, sometimes using fraud and violence to control state elections. A deep national economic depression following the Panic of 1873 led to major Democratic gains in the North, the collapse of many railroad schemes in the South, and a growing sense of frustration in the North.
The end of Reconstruction was a staggered process, and the period of Republican control ended at different times in different states. With the Compromise of 1877, Army intervention in the South ceased and Republican control collapsed in the last three state governments in the South. This was followed by a period that white Southerners labeled Redemption, during which white-dominated state legislatures enacted Jim Crow laws and, beginning in 1890, disenfranchised most blacks and many poor whites through a combination of constitutional amendments and electoral laws. The white Democrat Southerners' memory of Reconstruction played a major role in imposing the system of white supremacy and second-class citizenship for blacks, known as the age of Jim Crow.
Purpose.
Reconstruction addressed how the eleven seceding states would regain what the Constitution calls a "republican form of government" and be reseated in Congress, the civil status of the former leaders of the Confederacy, and the Constitutional and legal status of freedmen, especially their civil rights and whether they should be given the right to vote. Intense controversy erupted throughout the South over these issues.
The laws and constitutional amendments that laid the foundation for the most radical phase of Reconstruction were adopted from 1866 to 1871. By the 1870s, Reconstruction had officially provided freedmen with equal rights under the constitution, and blacks were voting and taking political office. Republican legislatures, coalitions of whites and blacks, established the first public school systems and numerous charitable institutions in the South. White paramilitary organizations, especially the Ku Klux Klan and also the White League and Red Shirts formed with the political aim of driving out the Republicans. They also disrupted political organizing and terrorized blacks to bar them from the polls. President Grant used federal power to effectively shut down the KKK in the early 1870s, though the other, smaller, groups continued to operate. From 1873 to 1877, conservative whites (calling themselves "Redeemers") regained power in the Southern states. They joined the Bourbon wing of the national Democratic Party.
In the 1860s and 1870s the terms "radical" and "conservative" had distinctive meanings. "Conservative" was the name of a faction, often led by the planter class. Leaders who had been Whigs were committed to economic modernization, built around railroads, factories, banks and cities. Most of the "radical" Republicans in the North were men who believed in free enterprise and industrialization; most were also modernizers and former Whigs. The "Liberal Republicans" of 1872 shared the same outlook except they were especially opposed to the corruption they saw around President Grant, and believed that the goals of the Civil War had been achieved so that the federal military intervention could now end.
Passage of the 13th, 14th, and 15th Amendments is the constitutional legacy of Reconstruction. These Reconstruction Amendments established the rights that led to Supreme Court rulings in the mid-20th century that struck down school segregation. A "Second Reconstruction", sparked by the Civil Rights Movement, led to civil rights laws in 1964 and 1965 that ended segregation and opened the polls to blacks.
Material devastation of the South in 1865.
Reconstruction played out against an economy in ruin. The Confederacy in 1861 had 297 towns and cities with a total population of 835,000 people; of these 162 with 681,000 people were at one point occupied by Union forces. Eleven were destroyed or severely damaged by war action, including Atlanta (with an 1860 population of 9,600), Charleston, Columbia, and Richmond (with prewar populations of 40,500, 8,100, and 37,900, respectively); the eleven contained 115,900 people in the 1860 census, or 14% of the urban South. The number of people who lived in the destroyed towns represented just over 1% of the Confederacy's combined urban and rural populations. The rate of damage in smaller towns was much lower—only 45 courthouses were burned out of a total of 830.
Farms were in disrepair, and the prewar stock of horses, mules and cattle was much depleted; two-fifths, or 40%, of the South's livestock had been killed. The South's farms were not highly mechanized, but the value of farm implements and machinery in the 1860 Census was $81 million and was reduced by two-fifths, or 40%, by 1870. The transportation infrastructure lay in ruins, with little railroad or riverboat service available to move crops and animals to market. Railroad mileage was located mostly in rural areas and over two-thirds of the South's rails, bridges, rail yards, repair shops and rolling stock were in areas reached by Union armies, which systematically destroyed what they could. Even in untouched areas, the lack of maintenance and repair, the absence of new equipment, the heavy over-use, and the deliberate relocation of equipment by the Confederates from remote areas to the war zone ensured the system would be ruined at war's end. Restoring the infrastructure — especially the railroad system — became a high priority for Reconstruction state governments.
The enormous cost of the Confederate war effort took a high toll on the South's economic infrastructure. The direct costs to the Confederacy in human capital, government expenditures, and physical destruction from the war totaled $3.3 billion. By 1865, the Confederate dollar was worthless due to high inflation, and people in the South had to resort to bartering services for goods, or else use scarce Union dollars. With the emancipation of the southern slaves, the entire economy of the South had to be rebuilt. Having lost their enormous investment in slaves, white planters had minimal capital to pay freedmen workers to bring in crops. As a result, a system of sharecropping was developed where landowners broke up large plantations and rented small lots to the freedmen and their families. The South was transformed from an elite minority of landed gentry slaveholders into a tenant farming agriculture system.
The end of the Civil War was accompanied by a large migration of new freed people to the cities. In the cities, African Americans were relegated to the lowest paying jobs such as unskilled and service labor. Men worked as rail workers, rolling and lumber mills workers, and hotel workers. The large population of slave artisans during the antebellum period had not been translated into a large number of freemen artisans during Reconstruction. Black women were largely confined to domestic work employed as cooks, maids, and child nurses. Others worked in hotels. A large number became laundresses. The dislocations had a severe negative impact on the black population, with a large amount of sickness and death.
Over a quarter of Southern white men of military age — the backbone of the South's white workforce — died during the war, leaving countless families destitute. Per capita income for white southerners declined from $125 in 1857 to a low of $80 in 1879. By the end of the 19th century and well into the 20th century, the South was locked into a system of poverty. How much of this failure was caused by the war and by previous reliance on agriculture remains the subject of debate among economists and historians.
Restoring the South to the Union.
During the Civil War, the Radical Republican leaders argued that slavery and the Slave Power had to be permanently destroyed, and that all forms of Confederate nationalism had to be suppressed. Moderates said this could be easily accomplished as soon as Confederate armies surrendered and the Southern states repealed secession and accepted the 13th Amendment – most of which happened by December 1865.
President Lincoln was the leader of the moderate Republicans and wanted to speed up Reconstruction and reunite the nation painlessly and quickly. Lincoln formally began Reconstruction in late 1863 with his Ten percent plan, which went into operation in several states but which Radical Republicans opposed. Lincoln pocket vetoed the Radical plan, the Wade–Davis Bill of 1864, which was much more strict than the Ten-Percent Plan.
The opposing faction of Radical Republicans was skeptical of Southern intentions and demanded stringent federal action. Congressman Thaddeus Stevens of Pennsylvania and Senator Charles Sumner of Massachusetts led the Radicals. Sumner argued that secession had destroyed statehood but the Constitution still extended its authority and its protection over individuals, as in existing U.S. territories. Stevens and his followers viewed secession as having left the states in a status like new territories. The Republicans sought to prevent Southern politicians from "restoring the historic subordination of Negroes". Since slavery was abolished, the three-fifths compromise no longer applied to counting the population of blacks. After the 1870 census, the South would gain numerous additional representatives in Congress, based on the population of freedmen. One Illinois Republican expressed a common fear that if the South were allowed to simply restore its previous established powers, that the "reward of treason will be an increased representation".
Upon Lincoln's assassination in April 1865, Andrew Johnson of Tennessee, who had been elected with Lincoln in 1864 as vice president, became president. Johnson rejected the Radical program of Reconstruction and instead appointed his own governors and tried to finish reconstruction by the end of 1865. Thaddeus Stevens vehemently opposed President Johnson's plans for an abrupt end to Reconstruction, insisting that Reconstruction must "revolutionize Southern institutions, habits, and manners ... The foundations of their institutions ... must be broken up and relaid, or all our blood and treasure have been spent in vain." Johnson broke decisively with the Republicans in Congress when he vetoed the Civil Rights Act in early 1865. While Democrats cheered, the Republicans pulled together, passed the bill again, and overturned Johnson's repeat veto. Full-scale political warfare now existed between Johnson (now allied with the Democrats) and the Radical Republicans.
Congress rejected Johnson's argument that he had the war power to decide what to do, since the war was over. Congress decided it had the primary authority to decide how Reconstruction should proceed, because the Constitution stated the United States had to guarantee each state a republican form of government. The Radicals insisted that meant Congress decided how Reconstruction should be achieved. The issues were multiple: who should decide, Congress or the president? How should republicanism operate in the South? What was the status of the Confederate states? What was the citizenship status of the leaders of the Confederacy? What was the citizenship and suffrage status of freedmen?
The election of 1866 decisively changed the balance of power, giving the Republicans two-thirds majorities in both houses of Congress, and enough votes to overcome Johnson's vetoes. They moved to impeach Johnson because of his constant attempts to thwart Radical Reconstruction measures, by using the Tenure of Office Act. Johnson was acquitted by one vote, but he lost the influence to shape Reconstruction policy.
The Republican Congress established military districts in the South and used Army personnel to administer the region until new governments loyal to the Union could be established. Congress temporarily suspended the ability to vote of approximately 10,000 to 15,000 white men who had been Confederate officials or senior officers, while constitutional amendments gave full citizenship and suffrage to former slaves.
With the power to vote, freedmen started participating in politics. While many slaves were illiterate, educated blacks (including escaped slaves) moved down from the North to aid them, and natural leaders also stepped forward. They elected white and black men to represent them in constitutional conventions. A Republican coalition of freedmen, southerners supportive of the Union (derisively called scalawags by white Democrats), and northerners who had migrated to the South (derisively called carpetbaggers) — some of whom were returning natives, but were mostly Union veterans – organized to create constitutional conventions. They created new state constitutions to set new directions for southern states.
Loyalty.
The issue of loyalty emerged in the debates over the Wade–Davis Bill of 1864. The bill required voters to take the "ironclad oath", swearing they had never supported the Confederacy or been one of its soldiers. Pursuing a policy of "malice toward none" announced in his second inaugural address, Lincoln asked voters only to support the Union. The Radicals lost support following Lincoln's veto of the Wade–Davis Bill but regained strength after Lincoln's assassination in April 1865.
Suffrage.
Congress had to consider how to restore to full status and representation within the Union those southern states that had declared their independence from the United States and had withdrawn their representation. Suffrage for former Confederates was one of two main concerns. A decision needed to be made whether to allow just some or all former Confederates to vote (and to hold office). The moderates in Congress wanted virtually all of them to vote, but the Radicals resisted. They repeatedly imposed the ironclad oath, which would effectively have allowed no former Confederates to vote. Historian Harold Hyman says that in 1866 Congressmen "described the oath as the last bulwark against the return of ex-rebels to power, the barrier behind which Southern Unionists and Negroes protected themselves." Radical Republican leader Thaddeus Stevens proposed, unsuccessfully, that all former Confederates lose the right to vote for five years. The compromise that was reached disenfranchised many Confederate civil and military leaders. No one knows how many temporarily lost the vote, but one estimate was that it was as high as 10,000 to 15,000 out of a total white population of roughly eight million.
Second, and closely related, was the issue of whether the roughly four million freedmen should be allowed to vote. The issue was how to receive the four million former slaves as citizens. If they were to be fully counted as citizens, some sort of representation for apportionment of seats in Congress had to be determined. Before the war, the population of slaves had been counted as three-fifths of a corresponding number of free whites. By having four million freedmen counted as full citizens, the South would gain additional seats in Congress. If blacks were denied the vote and the right to hold office, then only whites would represent them. Many conservatives, including most white southerners, northern Democrats, and some northern Republicans, opposed black voting. Some northern states that had referenda on the subject limited the ability of their own small populations of blacks to vote.
Lincoln had supported a middle position to allow some black men to vote, especially army veterans. Johnson also believed that such service should be rewarded with citizenship. Lincoln proposed giving the vote to "the very intelligent, and especially those who have fought gallantly in our ranks." In 1864, Governor Johnson said, "The better class of them will go to work and sustain themselves, and that class ought to be allowed to vote, on the ground that a loyal negro is more worthy than a disloyal white man." As President in 1865, Johnson wrote to the man he appointed as governor of Mississippi, recommending, "If you could extend the elective franchise to all persons of color who can read the Constitution in English and write their names, and to all persons of color who own real estate valued at least two hundred and fifty dollars, and pay taxes thereon, you would completely disarm the adversary in Congress, and set an example the other states will follow."
Charles Sumner and Thaddeus Stevens, leaders of the Radical Republicans, were initially hesitant to enfranchise the largely illiterate former slave population. Sumner preferred at first impartial requirements that would have imposed literacy restrictions on blacks and whites. He believed that he would not succeed in passing legislation to disfranchise illiterate whites who already had the vote.
In the South, many poor whites were illiterate as there was almost no public education before the war. In 1880, for example, the white illiteracy rate was about 25% in Tennessee, Kentucky, Alabama, South Carolina, and Georgia; and as high as 33% in North Carolina. This compares with the 9% national rate, and a black rate of illiteracy that was over 70% in the South. By 1900, however, with emphasis within the black community on education, the majority of blacks had achieved literacy.
Sumner soon concluded that "there was no substantial protection for the freedman except in the franchise." This was necessary, he stated, "(1) For his own protection; (2) For the protection of the white Unionist; and (3) For the peace of the country. We put the musket in his hands because it was necessary; for the same reason we must give him the franchise." The support for voting rights was a compromise between moderate and Radical Republicans.
The Republicans believed that the best way for men to get political experience was to be able to vote and to participate in the political system. They passed laws allowing all male freedmen to vote. In 1867, black men voted for the first time. Over the course of Reconstruction, more than 1,500 African Americans held public office in the South; some of them were men who had escaped to the North and gained educations, and returned to the South. They did not hold office in numbers representative of their proportion in the population, but often elected whites to represent them. The question of women's suffrage was also debated but was rejected.
From 1890 to 1908, southern states passed new constitutions and laws that disfranchised most blacks and tens of thousands of poor whites with new voter registration and electoral rules. When establishing new requirements such as subjectively administered literacy tests, in some states, they used "grandfather clauses" to enable illiterate whites to vote.
Southern Treaty Commission.
The Five Civilized Tribes that had been relocated to Indian Territory (now part of Oklahoma) held black slaves and signed treaties supporting the Confederacy. During the war, a war among pro- and anti-Union Indians had raged. Congress passed a statute that gave the President the authority to suspend the appropriations of any tribe if the tribe is "in a state of actual hostility to the government of the United States ... and, by proclamation, to declare all treaties with such tribe to be abrogated by such tribe"(25 USC Sec. 72).
As a component of Reconstruction, the Interior Department ordered a meeting of representatives from all Indian tribes which had affiliated with the Confederacy. The Council, the Southern Treaty Commission, was first held in Ft. Smith, Arkansas in September 1865, was attended by hundreds of Indians representing dozens of tribes. Over the next several years the commission negotiated treaties with tribes that resulted in additional relocations to Indian Territory and the de facto creation (initially by treaty) of an unorganized Oklahoma Territory.
Lincoln's presidential Reconstruction.
Preliminary events.
President Lincoln signed two Confiscation Acts into law, the first on August 6, 1861, and the second on July 17, 1862, safeguarding fugitive slaves from the Confederacy that came over into Union lines and giving them indirect emancipation if their masters continued insurrection against the United States. The laws allowed the confiscation of lands for colonization from those who aided and supported the rebellion. However, these laws had limited effect as they were poorly funded by Congress and poorly enforced by Attorney General Edward Bates.
In August 1861, Maj. Gen. John C. Frémont, Union commander of the Western Department, declared martial law in Missouri, confiscated Confederate property, and emancipated their slaves. President Lincoln immediately ordered Frémont to rescind his emancipation declaration stating, "I think there is great danger that ... the liberating slaves of traitorous owners, will alarm our Southern Union friends, and turn them against us – perhaps ruin our fair prospect for Kentucky." After Frémont refused to rescind the emancipation order, President Lincoln terminated him from active duty on November 2, 1861. Lincoln was concerned that border states would bolt from the Union if slaves were given their freedom. On May 26, 1862, Union Maj. Gen. David Hunter emancipated slaves in South Carolina, Georgia, and Florida stated all "persons ... heretofore held as slaves ... forever free." Lincoln, embarrassed by the order, rescinded Hunter's declaration and canceled the emancipations.
On April 16, 1862 Lincoln signed a bill into law outlawing slavery in Washington D.C. and freeing the estimated 3,500 slaves in the city and on June 19, 1862 he signed legislation outlawing slavery in all U.S. territories. On July 17, 1862 under the authority of the Confiscation Acts and an amended Force Bill of 1795, he authorized the recruitment of freed slaves into the Union army and seizure of any Confederate property for military purposes.
Gradual emancipation and compensation.
In an effort to keep border states in the Union, President Lincoln as early as 1861 designed gradual compensated emancipation programs paid for by government bonds. Lincoln desired Delaware, Maryland, Kentucky, and Missouri to "adopt a system of gradual emancipation which should work the extinction of slavery in twenty years." On March 26, 1862 Lincoln met with Senator Charles Sumner and recommended that a special joint session of Congress be convened to discuss giving financial aid to any border states who initiated a gradual emancipation plan. In April 1862, the joint session of Congress met, however, the border states were not interested and did not make any response to Lincoln or any Congressional emancipation proposal. Lincoln advocated compensated emancipation during the 1865 River Queen steamer conference.
Colonization.
In August 1862, President Lincoln met with African-American leaders and urged them to colonize some place in Central America. Lincoln planned to free the Southern slaves in the Emancipation Proclamation and he was concerned that freedmen would not be well treated in the United States by Whites in both the North and South. Although Lincoln gave assurances that the United States government would support and protect any colonies, the leaders declined the offer of colonization. Many free blacks had been opposed to colonization plans in the past and wanted to remain in the United States. President Lincoln persisted in his colonization plan believing that emancipation and colonization were part of the same program. Lincoln was successful by April 1863 at sending black colonists to Haiti and 453 to Chiriqui in Central America; however, none of the colonies was able to remain self-sufficient. Frederick Douglass, a prominent 19th-century American civil rights activist, criticized that Lincoln was "showing all his inconsistencies, his pride of race and blood, his contempt for Negroes and his canting hypocrisy." African Americans, according to Douglass, wanted citizen rights rather than to be colonized. Historians debate if Lincoln gave up on African-American colonization at the end of 1863 or if he actually planned to continue this policy up until 1865.
Military governors installed.
Starting in March 1862, in an effort to forestall Reconstruction by the Radicals in Congress, President Lincoln installed military governors in certain rebellious states under Union military control. Although the states would not be recognized by the Radicals until an undetermined time, installation of military governors kept the administration of Reconstruction under Presidential control, rather than that of the increasingly unsympathetic Radical Congress. On March 3, 1862, Lincoln installed a loyalist Democrat Senator Andrew Johnson, as Military Governor with the rank of Brigadier General in his home state of Tennessee. In May 1862, Lincoln appointed Edward Stanly Military Governor of the coastal region of North Carolina with the rank of Brigadier General. Stanly resigned almost a year later when he angered Lincoln by closing two schools for black children in New Bern. After Lincoln installed Brigadier General George F. Sheply as Military Governor of Louisiana in May 1862, Sheply sent two anti-slavery representatives, Benjamin Flanders and Michael Hahn, elected in December 1862, to the House which capitulated and voted to seat them. In July 1862, Lincoln installed Colonel John S. Phelps as Military Governor of Arkansas, though he resigned soon after due to poor health.
Emancipation Proclamation.
In July 1862, President Lincoln became convinced that "a military necessity" was needed to strike at slavery in order to win the Civil War for the Union. The Confiscation Acts were only having a minimal effect to end slavery. On July 22, he wrote a first draft of the Emancipation Proclamation that freed the slaves in states in rebellion. After he showed his cabinet the document, slight alterations were made in the wording. Lincoln decided that the defeat of the Confederate invasion of the North at Sharpsburg was enough of a battlefield victory to enable him to release the preliminary Emancipation Proclamation that gave the rebels 100 days to return to the Union or the actual Proclamation would be issued.
On January 1, 1863, the actual Emancipation Proclamation was issued, specifically naming ten states in which slaves would be "forever free". The proclamation did not name the states of Tennessee, Kentucky, Missouri, Maryland, and Delaware, and specifically excluded numerous counties in some other states. Eventually, as the Union Armies advanced into the Confederacy millions of slaves were set free. Many of these freedmen joined the Union army and fought in battles against the Confederate forces. Yet hundreds of thousands of freed slaves died during emancipation from illness that devastated army regiments. Freed slaves suffered from smallpox, yellow fever, and malnutrition.
Louisiana 10% electorate plan.
President Abraham Lincoln was concerned to effect a speedy restoration of the Confederate states to the Union after the Civil War. In 1863, President Lincoln proposed a moderate plan for the Reconstruction of the captured Confederate State of Louisiana. The plan granted amnesty to Rebels who took an oath of loyalty to the Union. Black Freedmen workers were tied to labor on plantations for one year at $10 a month pay. Only 10% of the state's electorate had to take the loyalty oath in order for the state to be readmitted into U.S. Congress. The state was required to abolish slavery in its new constitution. Identical Reconstruction plans would be adopted in Arkansas and Tennessee. By December 1864, the Lincoln plan of Reconstruction had been enacted in Louisiana and the legislature sent two Senators and five Representatives to take their seats in Washington. However, Congress refused to count any of the votes from Louisiana, Arkansas, and Tennessee, in essence rejecting Lincoln's moderate Reconstruction plan. Congress, at this time controlled by the Radicals, proposed the Wade–Davis Bill that required a majority of the state electorates to take the oath of loyalty to be admitted to Congress. Lincoln pocket-vetoed the bill and the rift widened between the moderates, who wanted to save the Union and win the war, and the Radicals, who wanted to effect a more complete change within Southern society. Frederick Douglass denounced Lincoln's 10% electorate plan as undemocratic since state admission and loyalty only depended on a minority vote.
Legalization of slave unions.
Before 1864, slave marriages had not been recognized legally; emancipation did not affect them. When freed, many former slaves made official marriages. Before emancipation, slaves could not enter into contracts, including the marriage contract. After emancipation, former slaves and whites both began to view the lack of officially recognized marriage for their unions as problematic. Not all free people formalized their unions. Some continued to have common-law marriages or community-recognized relationships. The acknowledgement of marriage by the state increased the state's recognition of freedpeople as legal actors and eventually helped make the case for parental rights for freedpeople against the practice of apprenticeship of black children. These children were legally taken away from their families under the guise of "providing them with guardianship and 'good' homes until they reached the age of consent at twenty-one" under acts such as the Georgia 1866 Apprentice Act. Such children were generally used as sources of unpaid labor.
Freedmen's Bureau.
On March 3, 1865 the Freedmen's Bureau Bill became law, sponsored by the Republicans to aid freedmen and white refugees. A federal Bureau was created to provide food, clothing, fuel, and advice on negotiating labor contracts. It attempted to oversee new relations between freedmen and their former masters in a free labor market. The Act, without deference to a person's color, authorized the Bureau to lease confiscated land for a period of three years and to sell it in portions of up to per buyer. The Bureau was to expire one year after the termination of the War. Lincoln was assassinated before he could appoint a commissioner of the Bureau. A popular myth was that the Act offered 40 acres and a mule, or that slaves had been promised this.
With the help of the Bureau, the recently freed slaves began voting, forming political parties, and assuming the control of labor in many areas. The Bureau helped to start a change of power in the South that drew national attention from the Republicans in the North to the conservative Democrats in the South. This is especially evident in the election between Grant and Seymour (Johnson did not get the Democratic nomination), where almost 700,000 black voters voted and swayed the election 300,000 votes in Grant's favor.
Even with the benefits that it gave to the freedmen, the Freedmen's Bureau failed to protect and take care of former slaves in certain areas. Because the Bureau only provided help with labor, food, and housing, medical attention for the former slaves was severely lacking. Furthermore, neither the Bureau nor other government institutions were able to protect the slaves from groups like the KKK. Terrorizing freedmen for trying to vote, hold a political office, or own land, the KKK was the antithesis to the Freedmen's Bureau. The Bureau seemed to be unable to address the issue of white groups who held racist views which permeated the South.
Bans color discrimination.
Other legislation was signed that broadened equality and rights for African Americans. Lincoln outlawed discrimination on account of color, in carrying U.S. mail, in riding on public street cars in Washington D.C., and in pay for soldiers.
February 1865 peace conference.
Lincoln and Secretary of State William H. Seward met with three southern representatives to discuss the peaceful reconstruction of the Union and the Confederacy on February 3, 1865 in Hampton Roads, Virginia. The southern delegation included Confederate vice-president, Alexander H. Stephens, John A. Campbell, and Robert M. T. Hunter. The southerners proposed the Union recognition of the Confederacy, a joint Union-Confederate attack on Mexico to oust dictator Maximillian, and an alternative subordinate status of servitude for blacks rather than slavery. Lincoln flatly denied recognition of the Confederacy, and said that the slaves covered by his Emancipation Proclamation would not be re-enslaved. He said that the Union States were about to pass the Thirteenth Amendment outlawing slavery. Lincoln urged the governor of Georgia to remove Confederate troops and "ratify this Constitutional Amendment "prospectively", so as to take effect—say in five years ... Slavery is doomed." Lincoln also urged compensated emancipation for the slaves as he thought the North should be willing to share the costs of freedom. Although the meeting was cordial, the parties did not settle on agreements.
Historical legacy debated.
Lincoln continued to advocate his Louisiana Plan as a model for all states up until his assassination on April 14, 1865. The plan successfully started the Reconstruction process of ratifying the Thirteenth Amendment in all states. Lincoln is typically portrayed as taking the moderate position and fighting the Radical positions. There is considerable debate on how well Lincoln, had he lived, would have handled Congress during the Reconstruction process that took place after the Civil War ended. One historical camp argues that Lincoln's flexibility, pragmatism, and superior political skills with Congress would have solved Reconstruction with far less difficulty. The other camp believes the Radicals would have attempted to impeach Lincoln, just as they did to his successor, Andrew Johnson, in 1868.
Johnson's presidential Reconstruction.
Northern anger over the assassination of Lincoln and the immense human cost of the war led to demands for punitive policies. Vice President Andrew Johnson had taken a hard line and spoke of hanging rebel Confederates, but when he succeeded Lincoln as President, Johnson took a much softer position, pardoning many Confederate leaders and former Confederates. Jefferson Davis was held in prison for two years, but other Confederate leaders were not. There were no treason trials. Only one person—Captain Henry Wirz, the commandant of the prison camp in Andersonville, Georgia—was executed for war crimes. Andrew Johnson's conservative view of Reconstruction did not include blacks or former slaves involvement in government and he refused to heed Northern concerns when southern state legislatures implemented Black Codes that set the status of the freedmen much lower than that of citizens.
Smith argues that, "Johnson attempted to carry forward what he considered to be Lincoln's plans for Reconstruction." McKitrick says that in 1865 Johnson had strong support in the Republican Party, "It was naturally from the great moderate sector of Unionist opinion in the North that Johnson could draw his greatest comfort." Billington says, " One faction, the Moderate Republicans under the leadership of Presidents Abraham Lincoln and Andrew Johnson, favored a mild policy toward the South." Lincoln biographers Randall and Current argued that:
Historians agree that President Johnson was an inept politician who lost all his advantages by his clumsy moves. He broke with Congress in early 1866 and then became defiant and tried to block enforcement of Reconstruction laws passed by the U.S. Congress. He was in constant conflict constitutionally with the Radicals in Congress over the status of freedmen and whites in the defeated South. Although resigned to the abolition of slavery, many former Confederates were unwilling to accept both social changes and political domination by former slaves. In the words of Benjamin F. Perry, President Johnson's choice as the provisional governor of South Carolina: "First, the Negro is to be invested with all political power, and then the antagonism of interest between capital and labor is to work out the result."
However, the fears of the mostly conservative planter elite and other leading white citizens were partly assuaged by the actions of President Johnson, who ensured that a wholesale land redistribution from the planters to the freedman did not occur. President Johnson ordered that confiscated or abandoned lands administered by the Freedmen's Bureau would not be redistributed to the freedmen but be returned to pardoned owners. Land was returned that would have been forfeited under the Confiscation Acts passed by Congress in 1861 and 1862.
Freedmen and the enactment of Black Codes.
Southern state governments quickly enacted the restrictive "black codes". However, they were abolished in 1866 and seldom had effect, because the Freedmen's Bureau (not the local courts) handled the legal affairs of freedmen.
The Black Codes indicated the plans of the southern whites for the former slaves. The freedmen would have more rights than did free blacks before the war, but they still had only a limited set of second-class civil rights, no voting rights and no citizenship. They could not own firearms, serve on a jury in a lawsuit involving whites or move about without employment. The Black Codes outraged northern opinion. They were overthrown by the Civil Rights Act of 1866 that gave the freedmen full legal equality (except for the right to vote).
The freedmen, with the strong backing of the Freedmen's Bureau, rejected gang-labor work patterns that had been used in slavery. Instead of gang labor, freedpeople preferred family-based labor groups. They forced planters to bargain for their labor. Such bargaining soon led to the establishment of the system of sharecropping, which gave the freedmen greater economic independence and social autonomy than gang labor. However, because they lacked capital and the planters continued to own the means of production (tools, draft animals and land), the freedmen were forced into producing cash crops (mainly cotton) for the land-owners and merchants, and they entered into a crop-lien system. Widespread poverty, disruption to an agricultural economy too dependent on cotton, and the falling price of cotton, led within decades to the routine indebtedness of the majority of the freedmen, and poverty by many planters.
Northern officials gave varying reports on conditions for the freedmen in the South. One harsh assessment came from Carl Schurz, who reported on the situation in the states along the Gulf Coast. His report documented dozens of extra-judicial killings and claimed that hundreds or thousands more African Americans were killed.
The number of murders and assaults perpetrated upon Negroes is very great; we can form only an approximative estimate of what is going on in those parts of the South which are not closely garrisoned, and from which no regular reports are received, by what occurs under the very eyes of our military authorities. As to my personal experience, I will only mention that during my two days sojourn at Atlanta, one Negro was stabbed with fatal effect on the street, and three were poisoned, one of whom died. While I was at Montgomery, one negro was cut across the throat evidently with intent to kill, and another was shot, but both escaped with their lives. Several papers attached to this report give an account of the number of capital cases that occurred at certain places during a certain period of time. It is a sad fact that the perpetration of those acts is not confined to that class of people which might be called the rabble.
Carl Schurz, "Report on the Condition of the South", December 1865 (U.S. Senate Exec. Doc. No. 2, 39th Congress, 1st session).
The report included sworn testimony from soldiers and officials of the Freedmen's Bureau. In Selma, Alabama, Major J.P. Houston noted that whites who killed twelve African Americans in his district never came to trial. Many more killings never became official cases. Captain Poillon described white patrols in southwestern Alabama
who board some of the boats; after the boats leave they hang, shoot, or drown the victims they may find on them, and all those found on the roads or coming down the rivers are almost invariably murdered. The bewildered and terrified freedmen know not what to do—to leave is death; to remain is to suffer the increased burden imposed upon them by the cruel taskmaster, whose only interest is their labor, wrung from them by every device an inhuman ingenuity can devise; hence the lash and murder is resorted to intimidate those whom fear of an awful death alone cause to remain, while patrols, Negro dogs and spies, disguised as Yankees, keep constant guard over these unfortunate people.
Much of the violence that was perpetrated against African Americans was shaped by gendered prejudices regarding African Americans. Black women were in a particularly vulnerable situation. To convict a white man of sexually assaulting black women in this period was exceedingly difficult. Black women were socially constructed as sexually avaricious and since they were portrayed as having little virtue, society held that they could not be raped. One report indicates two freedwomen, Frances Thompson and Lucy Smith, describe their violent sexual assault during the Memphis Riots of 1866. However, black women were vulnerable even in times of relative normalcy. Sexual assaults on African-American women were so pervasive, particularly on the part of their white employers, that black men sought to reduce the contact between white males and black females by having the women in their family avoid doing work that was closely overseen by whites. Black men were construed as being extremely sexually aggressive and their supposed or rumored threats to white women were often used as a pretext for lynching and castrations.
Moderate responses.
During fall 1865, out of response to the Black codes and worrisome signs of Southern recalcitrance, the Radical Republicans blocked the readmission of the former rebellious states to the Congress. Johnson, however, was content with allowing former Confederate states into the Union as long as their state governments adopted the 13th Amendment abolishing slavery. By December 6, 1865, the amendment was ratified and Johnson considered Reconstruction over. Johnson was following the moderate Lincoln Presidential Reconstruction policy to get the states readmitted as soon as possible.
Congress, however, controlled by the Radicals, had other plans. The Radicals were led by Charles Sumner in the Senate and Thaddeus Stevens in the House of Representatives. Congress, on December 4, 1865, rejected Johnson's moderate Presidential Reconstruction, and organized the Joint Committee on Reconstruction, a 15-member panel to devise reconstruction requirements for the Southern states to be restored to the Union.
In January 1866, Congress renewed the Freedmen's Bureau; however, Johnson vetoed the Freedmen's Bureau Bill in February 1866. Although Johnson had sympathies for the plights of the freedmen, he was against federal assistance. An attempt to override the veto failed on February 20, 1866. This veto shocked the Congressional Radicals. In response, both the Senate and House passed a joint resolution not to allow any Senator or Representative seat admittance until Congress decided when Reconstruction was finished.
Senator Lyman Trumbull of Illinois, leader of the moderate Republicans, took affront at the black codes. He proposed the first Civil Rights Law, because the abolition of slavery was empty if
laws are to be enacted and enforced depriving persons of African descent of privileges which are essential to freemen ... A law that does not allow a colored person to go from one county to another, and one that does not allow him to hold property, to teach, to preach, are certainly laws in violation of the rights of a freeman ... The purpose of this bill is to destroy all these discriminations.
The key to the bill was the opening section:
All persons born in the United States ... are hereby declared to be citizens of the United States; and such citizens of every race and color, without regard to any previous condition of slavery ... shall have the same right in every State ... to make and enforce contracts, to sue, be parties, and give evidence, to inherit, purchase, lease, sell, hold, and convey real and personal property, and to full and equal benefit of all laws and proceedings for the security of person and property, as is enjoyed by white citizens, and shall be subject to like punishment, pains, and penalties and to none other, any law, statute, ordinance, regulation, or custom to the Contrary notwithstanding.
The bill did not give Freedmen the right to vote. Congress quickly passed the Civil Rights bill; the Senate on February 2 voted 33–12; the House on March 13 voted 111–38.
Johnson's vetoes.
Although strongly urged by moderates in Congress to sign the Civil Rights bill, Johnson broke decisively with them by vetoing it on March 27, 1866. His veto message objected to the measure because it conferred citizenship on the freedmen at a time when eleven out of thirty-six states were unrepresented and attempted to fix by Federal law "a perfect equality of the white and black races in every State of the Union." Johnson said it was an invasion by Federal authority of the rights of the States; it had no warrant in the Constitution and was contrary to all precedents. It was a "stride toward centralization and the concentration of all legislative power in the national government."
The Democratic Party, proclaiming itself the party of white men, north and south, supported Johnson. However the Republicans in Congress overrode his veto (the Senate by the close vote of 33:15, the House by 122:41) and the Civil Rights bill became law. Congress also passed a toned-down Freedmen's Bureau Bill; Johnson quickly vetoed as he had done to the previous bill. Once again, however, Congress had enough support and overrode Johnson's veto.
The last moderate proposal was the Fourteenth Amendment, whose principal drafter was Representative John Bingham. It was designed to put the key provisions of the Civil Rights Act into the Constitution, but it went much further. It extended citizenship to everyone born in the United States (except visitors and Indians on reservations), penalized states that did not give the vote to freedmen, and most importantly, created new federal civil rights that could be protected by federal courts. It guaranteed the Federal war debt would be paid (and promised the Confederate debt would never be paid). Johnson used his influence to block the amendment in the states since three-fourths of the states were required for ratification (the amendment was later ratified.). The moderate effort to compromise with Johnson had failed, and a political fight broke out between the Republicans (both Radical and moderate) on one side, and on the other side, Johnson and his allies in the Democratic Party in the North, and the conservative groupings (which used different names) in each southern state.
Radical Reconstruction.
Concerned that President Johnson viewed Congress as an "illegal body" and wanted to overthrow the government, Republicans in Congress took control of Reconstruction policies after the election of 1866. Johnson ignored the policy mandate, and he openly encouraged southern states to deny ratification of the 14th Amendment (except for Tennessee, all former Confederate states did refuse to ratify, as did the border states of Delaware, Maryland and Kentucky). Radical Republicans in Congress, led by Stevens and Sumner, opened the way to suffrage for male freedmen. They were generally in control, although they had to compromise with the moderate Republicans (the Democrats in Congress had almost no power). Historians refer to this period as "Radical Reconstruction" or "Congressional Reconstruction."
The South's white leaders, who held power in the immediate postwar era before the vote was granted to the freedmen, renounced secession and slavery, but not white supremacy. People who had previously held power were angered in 1867 when new elections were held. New Republican lawmakers were elected by a coalition of white Unionists, freedmen and northerners who had settled in the South. Some leaders in the South tried to accommodate to new conditions.
Constitutional amendments.
Three Constitutional amendments, known as the Reconstruction Amendments, were adopted. The 13th Amendment abolishing slavery was ratified in 1865. The 14th Amendment was proposed in 1866 and ratified in 1868, guaranteeing United States citizenship to all persons born or naturalized in the United States and granting them federal civil rights. The 15th Amendment, proposed in late February 1869 and passed in early February 1870, decreed that the right to vote could not be denied because of "race, color, or previous condition of servitude". The amendment did not declare the vote an unconditional right; it prohibited these types of discrimination. States would still determine voter registration and electoral laws. The amendments were directed at ending slavery and providing full citizenship to freedmen. Northern Congressmen believed that providing black men with the right to vote would be the most rapid means of political education and training.
Many blacks took an active part in voting and political life, and rapidly continued to build churches and community organizations. Following Reconstruction, white Democrats and insurgent groups used force to regain power in the state legislatures, and pass laws that effectively disfranchised most blacks and many poor whites in the South. Around the start of the 20th century, from 1890 to 1910, southern states passed new constitutions that completed disfranchisement of blacks. U.S. Supreme Court rulings on these provisions upheld many of these new southern constitutions and laws, and most blacks were prevented from voting in the South until the 1960s. Full federal enforcement of the Fourteenth and Fifteenth Amendments did not occur until after passage of legislation in the mid-1960s as a result of the Civil Rights Movement.
For details, see:
Statutes.
The Reconstruction Acts as originally passed, were initially called "An act to provide for the more efficient Government of the Rebel States" the legislation was enacted by the 39th Congress, on March 2, 1867. It was vetoed by President Johnson, and the veto overridden by two-thirds majority, in both the House and the Senate, the same day. Congress also clarified the scope of the federal writ of habeas corpus to allow federal courts to vacate unlawful state court convictions or sentences in 1867 (28 U.S.C. §2254).
Military reconstruction.
With the Radicals in control, Congress passed the Reconstruction Acts on July 19, 1867. The first Reconstruction Act, authored by Oregon Sen. George H. Williams, a Radical Republican, placed ten Confederate states under military control, grouping them into five military districts:
20,000 U.S. troops were deployed to enforce the Act.
Tennessee was not made part of a military district (having already been readmitted to representation in Congress), and therefore federal controls did not apply.
The ten Southern state governments were re-constituted under the direct control of the United States Army. One major purpose was to recognize and protect the right of African Americans to vote. There was little or no combat, but rather a state of martial law in which the military closely supervised local government, supervised elections, and tried to protect office holders and freedmen from violence. Blacks were enrolled as voters; former Confederate leaders were excluded for a limited period. No one state was entirely representative. Randolph Campbell describes what happened in Texas:
The first critical step ... was the registration of voters according to guidelines established by Congress and interpreted by Generals Sheridan and Charles Griffin. The Reconstruction Acts called for registering all adult males, white and black, except those who had ever sworn an oath to uphold the Constitution of the United States and then engaged in rebellion ... Sheridan interpreted these restrictions stringently, barring from registration not only all pre-1861 officials of state and local governments who had supported the Confederacy but also all city officeholders and even minor functionaries such as sextons of cemeteries. In May Griffin ... appointed a three-man board of registrars for each county, making his choices on the advice of known scalawags and local Freedmen's Bureau agents. In every county where practicable a freedman served as one of the three registrars ... Final registration amounted to approximately 59,633 whites and 49,479 blacks. It is impossible to say how many whites were rejected or refused to register (estimates vary from 7,500 to 12,000), but blacks, who constituted only about 30 percent of the state's population, were significantly overrepresented at 45 percent of all voters.
State constitutional conventions: 1867–69.
The eleven Southern states held constitutional conventions giving black men the right to vote., where the factions divided into the Radical Conservative, and in-between delegates. The Radicals were a coalition: 40% were Southern white Republicans (“scalawags”); 25% were white Carpetbaggers, and 34% were black. Scalawags wanted to disfranchise all of the traditional white leadership class, but moderate Republican leaders in the North warned against that, and black delegates typically called for universal voting rights. The carpetbaggers inserted provisions designed to promote economic growth, especially financial aid to rebuild the ruined railroad system. The conventions set up systems of free public schools funded by tax money, but did not require them to be racially integrated.
Until 1872, most former Confederate or prewar Southern office holders were disqualified from voting or holding office; all but 500 top Confederate leaders were pardoned by the Amnesty Act of 1872. "Proscription" was the policy of disqualifying as many ex-Confederates as possible. It appealed to the Scalawag element. For example in 1865 Tennessee had disfranchised 80,000 ex-Confederates. However, prescription was soundly rejected by the black element, which insisted on universal suffrage. The issue would come up repeatedly in several states , especially in Texas and Virginia. In Virginia, an effort was made to disqualify for public office every man who had served in the Confederate Army even as a private, and any civilian farmer who sold food to the Confederate army. Disfranchising Southern whites was also opposed by moderate Republicans in the North, who felt that ending proscription would bring the South closer to a republican form of government based on the consent of the governed, as called for by the Constitution and the Declaration of Independence. Strong measures that were called for in order to forestall a return to the defunct Confederacy increasingly seemed out of place, and the role of the United States Army and controlling politics in the state was troublesome. Increasingly, historian Mark Summers states, "the disfranchisers had to fall back on the contention that denial of the vote was meant as punishment, and a lifelong punishment at that... Month by month, the unrepublican character of the regime looked more glaring."
Readmission to representation in Congress.
The conventions had all written new constitutions that were adopted by popular vote, at which point Congress was ready to readmit them. All Southern states were readmitted to representation in Congress by the end of 1870. 
Politics.
Grant: the Radical President.
During the Civil War, many in the North believed that fighting for the Union was a noble cause – for the preservation of the Union and the end of slavery. After the war ended, with the North victorious, the fear among Radicals was that President Johnson too quickly assumed that slavery and Confederate nationalism were dead and that the southern states could return. The Radicals sought out a candidate for President who represented their viewpoint.
In 1868, the Republicans unanimously chose Ulysses S. Grant to be the Republican Presidential candidate. Grant won favor with the Radicals after he allowed Edwin M. Stanton, a Radical, to be reinstated as Secretary of War. As early as 1862, during the Civil War, Grant had appointed the Ohio military chaplain John Eaton to protect and gradually incorporate refugee slaves in west Tennessee and northern Mississippi into the Union War effort, and pay them for their labor. It was the beginning of his vision for the Freedmen's Bureau. Grant opposed President Johnson by supporting the Reconstruction Acts passed by the Radicals.
Immediately upon Inauguration in 1869, Grant bolstered Reconstruction by prodding Congress to readmit Virginia, Mississippi, and Texas into the Union, while ensuring their constitutions protected every citizen's voting rights. Grant met with prominent black leaders for consultation, and signed a bill into law that guaranteed equal rights to both blacks and whites in Washington D.C.
In Grant's two terms he strengthened Washington's legal capabilities to directly intervene to protect citizenship rights even if the states ignored the problem. He worked with Congress to create the Department of Justice and Office of Solicitor General, led by Attorney General Amos Akerman and the first Solicitor General Benjamin Bristow. Congress passed three powerful Enforcement Acts in 1870-71. These were criminal codes which protected the Freedmen's right to vote, to hold office, to serve on juries, and receive equal protection of laws. Most important, they authorized the federal government to intervene when states did not act. Grant's new Justice Department prosecuted thousands of Klansmen under the tough new laws. Grant sent federal troops to nine South Carolina counties to suppress Klan violence in 1871. Grant supported passage of the Fifteenth Amendment stating that no state could deny a man the the right to vote on the basis of race. Congress passed the Civil Rights Act of 1875 giving people access to public facilities regardless of race.
To counter vote fraud in the Democratic stronghold of New York City, Grant sent in tens of thousands of armed, uniformed federal marshals and other election officials to regulate the 1870 and subsequent elections. Democrats across the North then mobilized to defend their base and attacked Grant's entire set of policies. On October 21, 1876 President Grant deployed troops to protect black and white Republican voters in Petersburg, Virginia.
Grant's support from Congress and the nation declined due to scandals within his administration and the political resurgence of the Democrats in the North and South. By 1870, most Republicans felt the war goals had been achieved, and they turned their attention to other issues such as economic policies.
Congressional investigation (1871–1872).
On April 20, 1871, the U.S. Congress launched a 21-member investigation committee on the status of the Southern Reconstruction states: North Carolina, South Carolina, Georgia, Mississippi, Alabama, and Florida. Congressional members on the committee included Rep. Benjamin Butler, Sen. Zachariah Chandler, and Sen. Francis P. Blair. Subcommittee members traveled into the South to interview the people living in their respective states. Those interviewed included top-ranking officials, such as Wade Hampton, former South Carolina Gov. James L. Orr, and Nathan B. Forrest, a former Confederate general and (alleged) prominent Ku Klux Klan leader (Forrest denied in his Congressional testimony being a member). Other southerners interviewed included farmers, doctors, merchants, teachers, and clergymen. The committee heard numerous reports of white violence against blacks, while many whites denied Klan membership or knowledge of violent activities. The majority report by Republicans concluded that the government would not tolerate any Southern "conspiracy" to resist violently the Congressional Reconstruction. The committee completed its 13-volume report in February 1872. While Grant had been able to suppress the KKK through the Enforcement Acts, other paramilitary insurgents organized, including the White League in 1874, active in Louisiana; and the Red Shirts, with chapters active in Mississippi and the Carolinas. They used intimidation and outright attacks to run Republicans out of office and repress voting by blacks, leading to white Democrats regaining power by the elections of the mid-to-late 1870s.
African-American officeholders.
Republicans took control of all Southern state governorships and state legislatures, except for Virginia. The Republican coalition elected numerous African Americans to local, state, and national offices; though they did not dominate any electoral offices, black men as representatives voting in state and federal legislatures marked a drastic social change. At the beginning of 1867, no African American in the South held political office, but within three or four years "about 15 percent of the officeholders in the South were black—a larger proportion than in 1990." In 1860 blacks were the majority of the population in Mississippi and South Carolina, 47% in Louisiana, 45% in Alabama, and 44% in Georgia and Florida, so their political influence was still far less than their percentage of the population.
About 137 black officeholders had lived outside the South before the Civil War. Some who had escaped from slavery to the North and had become educated returned to help the South advance in the postwar era. Others were free blacks before the war, who had achieved education and positions of leadership elsewhere. Other African-American men who served were already leaders in their communities, including a number of preachers. As happened in white communities, not all leadership depended upon wealth and literacy.
There were few African Americans elected or appointed to national office. African Americans voted for both white and black candidates. The Fifteenth Amendment to the United States Constitution guaranteed only that voting could not be restricted on the basis of race, color or previous condition of servitude. From 1868 on, campaigns and elections were surrounded by violence as white insurgents and paramilitary tried to suppress the black vote, and fraud was rampant. Many Congressional elections in the South were contested. Even states with majority African-American population often elected only one or two African-American representatives to Congress. Exceptions included South Carolina; at the end of Reconstruction, four of its five Congressmen were African American.
Social and economic factors.
Organized religion.
Freedmen were very active in forming their own churches, mostly Baptist or Methodist, and giving their ministers both moral and political leadership roles. In a process of self-segregation, practically all blacks left white churches so that few racially integrated congregations remained (apart from some Catholic churches in Louisiana). Four main groups competed with each other across the South to form new Methodist churches composed of freedmen. They were the African Methodist Episcopal Church; the African Methodist Episcopal Zion Church; the Colored Methodist Episcopal Church (which was sponsored by the white Methodist Episcopal Church, South) and the well-funded Methodist Episcopal Church (Northern white Methodists). By 1871 the Northern Methodists had 88,000 black members in the South, and had opened numerous schools for them.
Blacks in the South were a core element of the Republican Party and their ministers had powerful political roles that were distinctive since they did not depend on white support, in contrast to teachers, politicians, businessmen, and tenant farmers. Acting on the principle as stated by Charles H. Pearce, an AME minister in Florida: "A man in this State cannot do his whole duty as a minister except he looks out for the political interests of his people," over 100 black ministers were elected to state legislatures during Reconstruction, as well as several to Congress and one, Hiram Revels, to the U.S. Senate.
In a highly controversial move, the Northern Methodists used the Army to seize control of Methodist churches in large cities, over the vehement protests of the Southern Methodists. Historian Ralph Morrow reports:
Across the North most evangelical denominations, especially the Methodists, Congregationalists and Presbyterians, as well as the Quakers, were strong supporters of Radical policies. The focus on social problems paved the way for the Social Gospel movement. Matthew Simpson, a Methodist Bishop, played a leading role in mobilizing the Northern Methodists for the cause. His biographer calls him the "High Priest of the Radical Republicans." The Methodist Ministers Association of Boston, meeting two weeks after Lincoln's assassination, called for a hard line against the Confederate leadership:
The denominations all sent missionaries, teachers and activists to the South to help the Freedmen. Only the Methodists made many converts, however. Activists sponsored by Northern Methodist Church played a major role in the Freedmen's Bureau, notably in such key educational roles as the Bureau's state superintendent or assistant superintendent of education for Virginia, Florida, Alabama and South Carolina.
Many Americans interpreted great events in religious terms. Historian Wilson Fallin contrasts the interpretation of Civil War and Reconstruction in white versus black Baptist sermons in Alabama. White Baptists expressed the view that:
In sharp contrast, Black Baptists interpreted the Civil War, emancipation and Reconstruction as:
Public schools.
Historian James D. Anderson argues that the freed slaves were the first Southerners "to campaign for universal, state-supported public education." Blacks in the Republican coalition played a critical role in establishing the principle in state constitutions for the first time during congressional Reconstruction. Some slaves had learned to read from white playmates or colleagues before formal education was allowed by law; African Americans started "native schools" before the end of the war; Sabbath schools were another widespread means that freedmen developed to teach literacy. When they gained suffrage, black politicians took this commitment to public education to state constitutional conventions.
The Republicans created a system of public schools, which were segregated by race everywhere except New Orleans. Generally, elementary and a few secondary schools were built in most cities, and occasionally in the countryside, but the South had few cities.
The rural areas faced many difficulties opening and maintaining public schools. In the country, the public school was often a one-room affair that attracted about half the younger children. The teachers were poorly paid, and their pay was often in arrears. Conservatives contended the rural schools were too expensive and unnecessary for a region where the vast majority of people were cotton or tobacco farmers. They had no vision of a better future for their residents. One historian found that the schools were less effective than they might have been because "poverty, the inability of the states to collect taxes, and inefficiency and corruption in many places prevented successful operation of the schools." After Reconstruction ended and the whites disfranchised the blacks and imposed Jim Crow, they consistently underfunded black institutions, including the schools.
After the war, northern missionaries founded numerous private academies and colleges across the South for freedmen. In addition, every state founded state colleges for freedmen, such as Alcorn State University in Mississippi. The normal schools and state colleges produced generations of teachers who were integral to the education of African-American children under the segregated system. By the end of the century, the majority of African Americans were literate.
In the late 19th century, the federal government established land grant legislation to provide funding for higher education across the United States. Learning that blacks were excluded from land grant colleges in the South, in 1890, the federal government insisted that southern states establish black state institutions as land grant colleges to provide for black higher education, in order to continue to receive funds for their already established white schools. Some states classified their black state colleges as land grant institutions. Former Congressman John Roy Lynch wrote, "there are very many liberal, fair-minded and influential Democrats in the State who are strongly in favor of having the State provide for the liberal education of both races."
Railroad subsidies and payoffs.
Every Southern state subsidized railroads, which modernizers felt could haul the South out of isolation and poverty. Millions of dollars in bonds and subsidies were fraudulently pocketed. One ring in North Carolina spent $200,000 in bribing the legislature and obtained millions in state money for its railroads. Instead of building new track, however, it used the funds to speculate in bonds, reward friends with extravagant fees, and enjoy lavish trips to Europe. Taxes were quadrupled across the South to pay off the railroad bonds and the school costs. There were complaints among taxpayers, because taxes had historically been low, since there was so little commitment to public works or public education. Taxes historically had been much lower than in the North, reflecting a lack of government investment by the communities. Nevertheless, thousands of miles of lines were built as the Southern system expanded from 11,000 miles (17,700 km) in 1870 to 29,000 miles (46,700 km) in 1890. The lines were owned and directed overwhelmingly by Northerners. Railroads helped create a mechanically skilled group of craftsmen and broke the isolation of much of the region. Passengers were few, however, and apart from hauling the cotton crop when it was harvested, there was little freight traffic. As Franklin explains, "numerous railroads fed at the public trough by bribing legislators ... and through the use and misuse of state funds." The effect, according to one businessman, "was to drive capital from the State, paralyze industry, and demoralize labor."
Taxation during Reconstruction.
Reconstruction changed the means of taxation in the South. In the U.S. from the earliest days until today, a major source of state revenue was the property tax. In the South, wealthy landowners were allowed to self-assess the value of their own land. These fraudulent assessments were almost valueless, and pre-war property tax collections were lacking due to property value misrepresentation. State revenues came from fees and from sales taxes on slave auctions. Some states assessed property owners by a combination of land value and a capitation tax, a tax on each worker employed. This tax was often assessed in a way to discourage a free labor market, where a slave was assessed at 75 cents, while a free white was assessed at a dollar or more, and a free African American at $3 or more. Some revenue also came from poll taxes. These taxes were more than poor people could pay, with the designed and inevitable consequence that they did not vote.
During Reconstruction, the state legislature mobilized to provide for public need more than had previous governments: establishing public schools and investing in infrastructure, as well as charitable institutions such as hospitals and asylums. The needed to increase taxes which were abnormally low. The planters had provided privately for their own needs. There was some fraudulent spending in the postwar years; a collapse in state credit because of huge deficits, forced the states to increase property tax rates. In places, the rate went up to ten times higher—despite the poverty of the region. The planters had not invested in infrastructure and much had been destroyed during the war. In part, the new tax system was designed to force owners of large plantations with huge tracts of uncultivated land either to sell or to have it confiscated for failure to pay taxes. The taxes would serve as a market-based system for redistributing the land to the landless freedmen and white poor. Mississippi, for instance, was mostly frontier, with 90% of the bottomlands in the interior undeveloped.
The following table shows property tax rates for South Carolina and Mississippi. Note that many local town and county assessments effectively doubled the tax rates reported in the table. These taxes were still levied upon the landowners' own sworn testimony as to the value of their land, which remained the dubious and exploitable system used by wealthy landholders in the South well into the 20th century.
Called upon to pay taxes on their property, essentially for the first time, angry plantation owners revolted. The conservatives shifted their focus away from race to taxes. Former Congressman John R. Lynch, a black Republican leader from Mississippi, later wrote,
Ending Reconstruction.
Southern Democrats.
While the "Scalawag" element of Republican whites supported measures for black civil rights, the conservative whites typically opposed these measures. Some supported armed attacks to suppress black power. They self-consciously defended their own actions within the framework of an Anglo-American discourse of resistance against tyrannical government, and they broadly succeeded in convincing many fellow white citizens says Steedman.
The opponents of Reconstruction formed state political parties, affiliated with the national Democratic party and often named the "Conservative party." They supported or tolerated violent paramilitary groups, such as the White League in Louisiana and the Red Shirts in Mississippi and the Carolinas, that assassinated and intimidated both black and white Republican leaders at election time. Historian George C. Rable called such groups the "military arm of the Democratic Party." By the mid-1870s, the Conservatives and Democrats had aligned with the national Democratic Party, which enthusiastically supported their cause even as the national Republican Party was losing interest in Southern affairs. Historian Walter Lynwood Fleming, associated with the Dunning School, describes mounting anger of Southern whites:
The Negro troops, even at their best, were everywhere considered offensive by the native whites ... The Negro soldier, impudent by reason of his new freedom, his new uniform, and his new gun, was more than Southern temper could tranquilly bear, and race conflicts were frequent.
Often, these white Southerners identified as the "Conservative Party" or the "Democratic and Conservative Party" in order to distinguish themselves from the national Democratic Party and to obtain support from former Whigs. These parties sent delegates to the 1868 Democratic National Convention and abandoned their separate names by 1873 or 1874.
Most members of both the planter/business class and common farmer class of the South opposed black power, carpetbaggers and military rule, and sought white supremacy. Democrats nominated blacks for political office and tried to steal other blacks from the Republican side. When these attempts to combine with the blacks failed, the planters joined the common farmers in simply trying to displace the Republican governments. The planters and their business allies dominated the self-styled "conservative" coalition that finally took control in the South. They were paternalistic toward the blacks but feared they would use power to raise taxes and slow business development.
Fleming described the first results of the insurgent movement as "good," and the later ones as "both good and bad." According to Fleming (1907), the KKK "quieted the Negroes, made life and property safer, gave protection to women, stopped burnings, forced the Radical leaders to be more moderate, made the Negroes work better, drove the worst of the Radical leaders from the country and started the whites on the way to gain political supremacy." The evil result, Fleming said, was that lawless elements "made use of the organization as a cloak to cover their misdeeds ... the lynching habits of today are largely due to conditions, social and legal, growing out of Reconstruction." Historians have noted that the peak of lynchings took place years after Reconstruction ended as whites were imposing Jim Crow laws and that they were more often used to keep black men down, with a rate associated with settlement of sharecropper accounts at the end of the season, than for any other reason.
Ellis Paxson Oberholtzer (a northern scholar) in 1917 explained:
Outrages upon the former slaves in the South there were in plenty. Their sufferings were many. But white men, too, were victims of lawless violence, and in all portions of the North and the late "rebel" states. Not a political campaign passed without the exchange of bullets, the breaking of skulls with sticks and stones, the firing of rival club-houses. Republican clubs marched the streets of Philadelphia, amid revolver shots and brickbats, to save the negroes from the "rebel" savages in Alabama ... The project to make voters out of black men was not so much for their social elevation as for the further punishment of the Southern white people—for the capture of offices for Radical scamps and the entrenchment of the Radical party in power for a long time to come in the South and in the country at large.
As Reconstruction continued, whites accompanied elections with increased violence in an attempt to run Republicans out of office and suppress black voting. The victims of this violence were overwhelmingly African American, as in the Colfax Massacre of 1873. After federal suppression of the Klan in the early 1870s, white insurgent groups tried to avoid open conflict with federal forces. In 1874 in the Battle of Liberty Place, the White League entered New Orleans with 5,000 members and defeated the police and militia, to occupy federal offices for three days in an attempt to overturn the disputed government of William Kellogg, but retreated before federal troops reached the city. None were prosecuted. Their election-time tactics included violent intimidation of African-American and Republican voters prior to elections while avoiding conflict with the U.S. Army or the state militias and then withdrawing completely on election day. Conservative reaction continued in both the north and south; the "white liners" movement to elect candidates dedicated to white supremacy reached as far as Ohio in 1875.
Redemption 1873–77.
Republicans split nationally: election of 1872.
As early as 1868 Supreme Court Chief Justice Salmon P. Chase, a leading Radical during the war, concluded that:
Congress was right in not limiting, by its reconstruction acts, the right of suffrage to whites; but wrong in the exclusion from suffrage of certain classes of citizens and all unable to take its prescribed retrospective oath, and wrong also in the establishment of despotic military governments for the States and in authorizing military commissions for the trial of civilians in time of peace. There should have been as little military government as possible; no military commissions; no classes excluded from suffrage; and no oath except one of faithful obedience and support to the Constitution and laws, and of sincere attachment to the constitutional Government of the United States.
By 1872, President Ulysses S. Grant had alienated large numbers of leading Republicans, including many Radicals, by the corruption of his administration and his use of federal soldiers to prop up Radical state regimes in the South. The opponents, called "Liberal Republicans", included founders of the party who expressed dismay that the party had succumbed to corruption. They were further wearied by the continued insurgent violence of whites against blacks in the South, especially around every election cycle, which demonstrated the war was not over and changes were fragile. Leaders included editors of some of the nation's most powerful newspapers. Charles Sumner, embittered by the corruption of the Grant administration, joined the new party, which nominated editor Horace Greeley. The badly organized Democratic party also supported Greeley.
Grant made up for the defections by new gains among Union veterans and by strong support from the "Stalwart" faction of his party (which depended on his patronage), and the Southern Republican parties. Grant won with 55.6% of the vote to Greeley's 43.8%. The Liberal Republican party vanished and many former supporters—even former abolitionists—abandoned the cause of Reconstruction.
Republican coalition splinters in South.
In the South, political–racial tensions built up inside the Republican party as they were attacked by the Democrats. In 1868, Georgia Democrats, with support from some Republicans, expelled all 28 black Republican members from the state house, arguing blacks were eligible to vote but not to hold office. In most states, the more conservative scalawags fought for control with the more radical carpetbaggers and their black allies. Most of the 430 Republican newspapers in the South were edited by scalawags – only 20 percent were edited by carpetbaggers. White businessmen generally boycotted Republican papers, which survived through government patronage. Nevertheless, in the increasingly bitter battles inside the Republican Party, the scalawags usually lost; many of the disgruntled losers switched over to the conservative or Democratic side. In Mississippi, the conservative faction led by scalawag James Lusk Alcorn was decisively defeated by the radical faction led by carpetbagger Adelbert Ames. The party lost support steadily as many scalawags left it; few recruits were acquired. The most bitter contest took place inside the Republican Party in Arkansas, where the two sides armed their forces and confronted each other in the streets; no actual combat took place in the Brooks–Baxter War. The carpetbagger faction led by Elisha Baxter finally prevailed when the White House intervened, but both sides were badly weakened, and the Democrats soon came to power.
Meanwhile, in state after state the freedmen were demanding a bigger share of the offices and patronage, squeezing out carpetbagger allies but never commanding the numbers equivalent to their population proportion. By the mid-1870s, "The hard realities of Southern political life had taught the lesson that black constituents needed to be represented by black officials." The financial depression increased the pressure on Reconstruction governments, dissolving progress.
Finally, some of the more prosperous freedmen were joining the Democrats, as they were angered at the failure of the Republicans to help them acquire land. The South was "sparsely settled"; only 10% of Louisiana was cultivated, and 90% of Mississippi bottomland were undeveloped in areas away from the riverfronts, but freedmen often did not have the stake to get started. They hoped government would help them acquire land which they would work. Only South Carolina created any land redistribution, establishing a land commission and resettling about 14,000 freedmen families and some poor whites on land purchased by the state.
Although historians such as W. E. B. Du Bois celebrated a cross-racial coalition of poor whites and blacks, such coalitions rarely formed in these years. Writing in 1915, former Congressman Lynch, recalling his experience as a black leader in Mississippi, explained that,
While the colored men did not look with favor upon a political alliance with the poor whites, it must be admitted that, with very few exceptions, that class of whites did not seek, and did not seem to desire such an alliance.
Lynch reported that poor whites resented the job competition from freedmen. Furthermore, the poor whites
with a few exceptions, were less efficient, less capable, and knew less about matters of state and governmental administration than many of the former slaves ... As a rule, therefore, the whites that came into the leadership of the Republican party between 1872 and 1875 were representatives of the most substantial families of the land.
Democrats try a "New Departure".
By 1870, the Democratic–Conservative leadership across the South decided it had to end its opposition to Reconstruction and black suffrage to survive and move on to new issues. The Grant administration had proven by its crackdown on the Ku Klux Klan that it would use as much federal power as necessary to suppress open anti-black violence. Democrats in the North concurred with these Southern Democrats. They wanted to fight the Republican Party on economic grounds rather than race. The New Departure offered the chance for a clean slate without having to re-fight the Civil War every election. Furthermore, many wealthy Southern landowners thought they could control part of the newly enfranchised black electorate to their own advantage.
Not all Democrats agreed; an insurgent element continued to resist Reconstruction no matter what. Eventually, a group called "Redeemers" took control of the party in the Southern states. They formed coalitions with conservative Republicans, including scalawags and carpetbaggers, emphasizing the need for economic modernization. Railroad building was seen as a panacea since northern capital was needed. The new tactics were a success in Virginia where William Mahone built a winning coalition. In Tennessee, the Redeemers formed a coalition with Republican governor DeWitt Senter. Across the South, some Democrats switched from the race issue to taxes and corruption, charging that Republican governments were corrupt and inefficient. With continuing decrease in cotton prices, taxes squeezed cash-poor farmers who rarely saw $20 in currency a year but had to pay taxes in currency or lose their farm. But major planters, who had never paid taxes before, often recovered their property even after confiscation.
In North Carolina, Republican Governor William Woods Holden used state troops against the Klan, but the prisoners were released by federal judges. Holden became the first governor in American history to be impeached and removed from office. Republican political disputes in Georgia split the party and enabled the Redeemers to take over.
In the lower South, violence increased as new insurgent groups arose, including the Red Shirts in Mississippi and the Carolinas, and the White League in Louisiana. The disputed election in Louisiana in 1872 found both Republican and Democratic candidates holding inaugural balls while returns were reviewed. Both certified their own slates for local parish offices in many places, causing local tensions to rise. Finally, Federal support helped certify the Republican as governor.
Slates for local offices were certified by each candidate. In rural Grant Parish in Red River Valley, freedmen fearing a Democratic attempt to take over the parish government reinforced defenses at the small Colfax courthouse in late March. White militias gathered from the area a few miles outside the settlement. Rumors and fears abounded on both sides. William Ward, an African-American Union veteran and militia captain, mustered his company in Colfax and went to the courthouse. On Easter Sunday, April 13, 1873, the whites attacked the defenders at the courthouse. There was confusion about who shot one of the white leaders after an offer by the defenders to surrender. It was a catalyst to mayhem. In the end, three whites died and 120–150 blacks were killed, some 50 that evening while being held as prisoners. The disproportionate numbers of black to white fatalities and documentation of brutalized bodies are why contemporary historians call it the Colfax Massacre rather than the Colfax Riot, as it was known locally.
This marked the beginning of heightened insurgency and attacks on Republican officeholders and freedmen in Louisiana and other Deep South states. In Louisiana, Judge T. S. Crawford and District Attorney P. H. Harris of the 12th Judicial District were shot off their horses and killed from ambush October 8, 1873, while going to court. One widow wrote to the Department of Justice that her husband was killed because he was a Union man and "... of the efforts made to screen those who committed a crime ..."
In the North, a live-and-let-live attitude made elections more like a sporting contest. But in the Deep South, many white citizens had not reconciled with the defeat of the war or the granting of citizenship to freedmen. As an Alabama scalawag explained,
Panic of 1873.
The Panic of 1873 (a depression) hit the Southern economy hard and disillusioned many Republicans who had gambled that railroads would pull the South out of its poverty. The price of cotton fell by half; many small landowners, local merchants and cotton factors (wholesalers) went bankrupt. Sharecropping for black and white farmers became more common as a way to spread the risk of owning land. The old abolitionist element in the North was aging away, or had lost interest, and was not replenished. Many carpetbaggers returned to the North or joined the Redeemers. Blacks had an increased voice in the Republican Party, but across the South it was divided by internal bickering and was rapidly losing its cohesion. Many local black leaders started emphasizing individual economic progress in cooperation with white elites, rather than racial political progress in opposition to them, a conservative attitude that foreshadowed Booker T. Washington.
Nationally, President Grant was blamed for the depression; the Republican Party lost 96 seats in all parts of the country in the 1874 elections. The Bourbon Democrats took control of the House and were confident of electing Samuel J. Tilden president in 1876. President Grant was not running for re-election and seemed to be losing interest in the South. States fell to the Redeemers, with only four in Republican hands in 1873, Arkansas, Louisiana, Mississippi and South Carolina; Arkansas then fell after the violent Brooks–Baxter War in 1874 ripped apart the Republican party there.
Violence.
Political violence was endemic in Louisiana. In 1874 the white militias coalesced into paramilitary organizations such as the White League, first in parishes of the Red River Valley. The new organization operated openly and had political goals: the violent overthrow of Republican rule and suppression of black voting. White League chapters soon rose in many rural parishes, receiving financing for advanced weaponry from wealthy men. In the Coushatta Massacre in 1874, the White League assassinated six white Republican officeholders and five to twenty black witnesses outside Coushatta, Red River Parish. Four of the white men were related to the Republican representative of the parish, who was married to a local woman; three were native to the region.
Later in 1874 the White League mounted a serious attempt to unseat the Republican governor of Louisiana, in a dispute that had simmered since the 1872 election. It brought 5000 troops to New Orleans to engage and overwhelm forces of the Metropolitan Police and state militia to turn Republican Governor William P. Kellogg out of office and seat John McEnery. The White League took over and held the state house and city hall, but they retreated before the arrival of reinforcing Federal troops. Kellogg had asked for reinforcements before, and Grant finally responded, sending additional troops to try to quell violence throughout plantation areas of the Red River Valley, although 2,000 troops were already in the state.
Similarly, the Red Shirts, another paramilitary group, arose in 1875 in Mississippi and the Carolinas. Like the White League and White Liner rifle clubs, to which 20,000 men belonged in North Carolina alone, these groups operated as a "military arm of the Democratic Party", to restore white supremacy.
Democrats and many northern Republicans agreed that Confederate nationalism and slavery were dead—the war goals were achieved—and further federal military interference was an undemocratic violation of historic Republican values. The victory of Rutherford Hayes in the hotly contested Ohio gubernatorial election of 1875 indicated his "let alone" policy toward the South would become Republican policy, as happened when he won the 1876 Republican nomination for president.
An explosion of violence accompanied the campaign for the Mississippi's 1875 election, in which Red Shirts and Democratic rifle clubs, operating in the open, threatened or shot enough Republicans to decide the election for the Democrats. Hundreds of black men were killed. Republican Governor Adelbert Ames asked Grant for federal troops to fight back; Grant initially refused, saying public opinion was "tired out" of the perpetual troubles in the South. Ames fled the state as the Democrats took over Mississippi.
The campaigns and elections of 1876 were marked by additional murders and attacks on Republicans in Louisiana, North and South Carolina, and Florida. In South Carolina the campaign season of 1876 was marked by murderous outbreaks and fraud against freedmen. Red Shirts paraded with arms behind Democratic candidates; they killed blacks in the Hamburg and Ellenton SC massacres; and one historian estimated 150 blacks were killed in the weeks before the 1876 election across South Carolina. Red Shirts prevented almost all black voting in two majority-black counties. The Red Shirts were also active in North Carolina.
Election of 1876.
Reconstruction continued in South Carolina, Louisiana and Florida until 1877. The elections of 1876 were accompanied by heightened violence across the Deep South. A combination of ballot stuffing and intimidating blacks suppressed their vote even in majority black counties. The White League was active in Louisiana. After Republican Rutherford Hayes won the disputed 1876 presidential election, the national Compromise of 1877 was reached.
The white Democrats in the South agreed to accept Hayes's victory if he withdrew the last Federal troops. By this point, the North was weary of insurgency. White Democrats controlled most of the Southern legislatures and armed militias controlled small towns and rural areas. Blacks considered Reconstruction a failure because the Federal government withdrew from enforcing their ability to exercise their rights as citizens.
Hayes ends Reconstruction.
After assuming office on March 4, 1877, President Hayes removed troops from the capitals of the remaining Reconstruction states, Louisiana and South Carolina, allowing the Redeemers to have full control of these states. President Grant had already removed troops from Florida, before Hayes was inaugurated, and troops from the other Reconstruction states had long since been withdrawn. Hayes appointed David M. Key from Tennessee, a Southern Democrat, to the position of Postmaster General. By 1879, thousands of African-American "Exodusters" packed up and headed to new opportunities in Kansas.
The Democrats gained control of the Senate, and had complete control of Congress, having taken over the House in 1875. Hayes vetoed bills from the Democrats that outlawed the Republican Enforcement Acts; however, with the military underfunded, Hayes could not adequately enforce these laws. Blacks remained involved in Southern politics, particularly in Virginia, which was run by the biracial Readjuster Party.
Numerous blacks were elected to local office through the 1880s, and in the 1890s in some states, biracial coalitions of Populists and Republicans briefly held control of state legislatures. In the last decade of the 19th century, southern states elected five black US Congressmen before disfranchising constitutions were passed throughout the former Confederacy.
Legacy and historiography.
The interpretation of Reconstruction has been a topic of controversy. Nearly all historians hold that Reconstruction ended in failure but for different reasons.
The first generation of Northern historians believed that the former Confederates were traitors and Johnson was their ally who threatened to undo the Union's constitutional achievements. By the 1880s, however, Northern historians argued that Johnson and his allies were not traitors but had blundered badly in rejecting the 14th Amendment and setting the stage for Radical Reconstruction.
The black leader Booker T. Washington, who grew up in West Virginia during Reconstruction, concluded later that, "the Reconstruction experiment in racial democracy failed because it began at the wrong end, emphasizing political means and civil rights acts rather than economic means and self-determination." His solution was to concentrate on building the economic infrastructure of the black community, in part by his leadership and the southern Tuskegee Institute.
Dunning School: 1900 to 1920s.
The Dunning School of scholars were trained at the history department of Columbia University under Professor William A. Dunning analyzed Reconstruction as a failure after 1866 for different reasons. They claimed that Congress took freedoms and rights from qualified whites and gave them to unqualified blacks who were being duped by corrupt "carpetbaggers and scalawags." As T. Harry Williams (who was a sharp critic of the Dunning school) notes, the Dunningites portrayed the era in stark terms:
Reconstruction was a battle between two extremes: the Democrats, as the group which included the vast majority of the whites, standing for decent government and racial supremacy, versus the Republicans, the Negroes, alien carpetbaggers, and renegade scalawags, standing for dishonest government and alien ideals. These historians wrote literally in terms of white and black.
Revisionists and Beardians, 1930s–1940s.
In the 1930s, "revisionism" became popular among scholars. As disciples of Charles A. Beard, revisionists focused on economics, downplaying politics and constitutional issues. The central figure was a young scholar at the University Wisconsin, Howard K. Beale, who in his PhD dissertation, finished in 1924, developed a complex new interpretation of Reconstruction. The Dunning School portrayed Freedmen as mere pawns in the hands of the Carpetbaggers. Beale argued that the Carpetbaggers themselves were pawns in the hands of northern industrialists, who were the real villains of Reconstruction. These industrialists had taken control of the nation during the Civil War, and set up high tariffs to protect their profits, as well as a lucrative national banking system and a railroad network fueled by government subsidies and secret payoffs. The return to power of the southern whites would seriously threaten all their gains, and so the ex-Confederates had to be kept out of power. The tool used by the industrialists was the combination of the Northern Republican Party and sufficient Southern support using Carpetbaggers and black voters. The rhetoric of civil rights for blacks, and the dream of equality, was rhetoric designed to fool idealistic voters. Beale called it "claptrap," arguing, "Constitutional discussions of the rights of the negro, the status of Southern states, the legal position of ex-rebels, and the powers of Congress and the president determined nothing. They were pure sham."
President Andrew Johnson had tried, and failed, to stop the juggernaut of the industrialists. The Dunning school had praised Johnson for upholding the rights of the white men in the South and endorsing white supremacy. Beale was not a racist, and indeed was one of the most vigorous historians working for black civil rights in the 1930s and 1940s. In his view, Johnson was not a hero for his racism, but rather for his forlorn battle against the industrialists. Charles A. Beard and Mary Beard had already published "The Rise of American Civilization" (1927) three years before Beale, and had given very wide publicity to a similar theme. The Beard-Beale interpretation of Reconstruction became known as "revisionism," and replaced the Dunning school for most historians, until the 1950s.
The Beardian interpretation of the causes of the Civil War downplayed slavery, abolitionism, and issues of morality. It ignored constitutional issues of states rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict, as the Beards explained in "The Rise of American Civilization" (1927), the Civil War was really a:
The Beards were especially interested in the Reconstruction era, as the industrialists of the Northeast and the farmers of the West cashed in on their great victory over the southern aristocracy. Historian Richard Hofstadter paraphrases the Beards as arguing that in victory:
Wisconsin historian William Hesseltine added the point that the Northeastern businessmen wanted to control the Southern economy directly, which they did through ownership of the railroads. The Beard-Beale interpretation of the monolithic Northern industrialists fell apart in the 1950s when it was closely examined by numerous historians, including Robert P. Sharkey, Irwin Unger, and Stanley Coben. The younger scholars conclusively demonstrated that there was no unified economic policy on the part of the dominant Republican Party. Some wanted high tariffs and some low. Some wanted Greenbacks and others wanted gold. There was no conspiracy to use Reconstruction to impose any such unified economic policy on the nation. Northern businessmen were widely divergent on monetary or tariff policy, and seldom paid attention to Reconstruction issues. Furthermore, the rhetoric on behalf of the rights of the Freedman was not claptrap but deeply held and very serious political philosophy.
Black historians.
The black scholar W. E. B. Du Bois, in his "Black Reconstruction in America, 1860–1880", published in 1935, compared results across the states to show achievements by the Reconstruction legislatures and to refute claims about wholesale African-American control of governments. He showed black contributions, as in the establishment of universal public education, charitable and social institutions and universal suffrage as important results, and he noted their collaboration with whites. He also pointed out that whites benefited most by the financial deals made, and he put excesses in the perspective of the war's aftermath. He noted that despite complaints, several states kept their Reconstruction constitutions for nearly a quarter of a century. Despite receiving favorable reviews, his work was largely ignored by white historians of his time.
Neo-Abolitionists.
In the 1960s neoabolitionist historians emerged, led by John Hope Franklin, Kenneth Stampp, Leon Litwack, and Eric Foner. Influenced by the Civil Rights Movement, they rejected the Dunning school and found a great deal to praise in Radical Reconstruction. Foner, the primary advocate of this view, argued that it was never truly completed, and that a Second Reconstruction was needed in the late 20th century to complete the goal of full equality for African Americans. The neo-abolitionists followed the revisionists in minimizing the corruption and waste created by Republican state governments, saying it was no worse than Boss Tweed's ring in New York City.
Instead, they emphasized that suppression of the rights of African Americans was a worse scandal and a grave corruption of America's republican ideals. They argued that the tragedy of Reconstruction was not that it failed because blacks were incapable of governing, especially as they did not dominate any state government, but that it failed because whites raised an insurgent movement to restore white supremacy. White elite-dominated state legislatures passed disfranchising constitutions from 1890 to 1908 that effectively barred most blacks and many poor whites from voting. This disfranchisement affected millions of people for decades into the 20th century, and closed African Americans and poor whites out of the political process in the South.
Re-establishment of white supremacy meant that within a decade African Americans were excluded from virtually all local, state and federal governance in all states of the South." Lack of representation meant that they were treated as second-class citizens, with schools and services consistently underfunded in segregated societies, no representation on juries or in law enforcement, and bias in other legislation. It was not until the Civil Rights Movement and the passage in 1964 and 1965 of Federal legislation that outlawed segregation and restored the suffrage under what is sometimes referred to as the "Second Reconstruction."
In 1990 Eric Foner concluded that from the black point of view, "Reconstruction must be judged a failure." Foner stated Reconstruction was "a noble if flawed experiment, the first attempt to introduce a genuine inter-racial democracy in the United States". The many factors contributing to the failure included: lack of a permanent federal agency "specifically" designed for the enforcement of civil rights; the Morrison R. Waite Supreme Court decisions that dismantled previous congressional civil rights legislation; and the economic reestablishment of conservative white planters in the South by 1877. Historian William McFeely explained that although the constitutional amendments and civil rights legislation on their own merit were remarkable achievements, no permanent government agency whose specific purpose was civil rights enforcement had been created.
More recent work by Nina Silber, David W. Blight, Cecelia O'Leary, Laura Edwards, LeeAnn Whites and Edward J. Blum, has encouraged greater attention to race, religion and issues of gender while at the same time pushing the "end" of Reconstruction to the end of the 19th century, while monographs by Charles Reagan Wilson, Gaines Foster, W. Scott Poole and Bruce Baker have offered new views of the Southern "Lost Cause".
Dating the end of the Reconstruction era.
While 1877 is the usual date given for the end of Reconstruction, some historians extend the era to the 1890s.
The "failure" issue.
Reconstruction is widely considered a failure, though the reason for this is a matter of controversy.
Historian Donald R. Shaffer maintained that the gains during Reconstruction for African Americans were not entirely extinguished. The legalization of African-American marriage and family and the independence of black churches from white denominations were a source of strength during the Jim Crow era. Reconstruction was never forgotten among the black community and remained as a source of inspiration. The system of share-cropping allowed blacks a considerable amount of freedom as compared to slavery.
However, in 2014 historian Mark Summers argues for looking at the "failure" question from the viewpoint of the war goals; in that case, he argues:
In popular culture.
The journalist Joel Chandler Harris, writing as "Joe Harris" for the "Atlanta Constitution" (mostly after Reconstruction), tried to advance racial and sectional reconciliation in the late 19th century. He supported Henry Grady's vision of a New South during Grady's time as editor from 1880 to 1889. Harris wrote many editorials encouraging southern acceptance of the changed conditions and some Northern influence, although he also asserted his belief that it should proceed under white supremacy.
In popular literature, two early 20th-century novels by Thomas Dixon—"The Clansman" (1905) and "The Leopard's Spots: A Romance of the White Man's Burden – 1865–1900" (1902)— romanticized white resistance to Northern/black coercion, hailing vigilante action by the KKK. D. W. Griffith adapted Dixon's "The Clansman" for the screen in his anti-Republican movie "The Birth of a Nation" (1915), considered to have contributed to the 20th-century revival of the KKK. Many other authors romanticized the benevolence of slavery and the élite world of the antebellum plantations in memoirs and histories published in the late nineteenth and early twentieth centuries, and the United Daughters of the Confederacy promoted influential works by women in these genres.
Reconstruction state-by-state – significant dates.
Only Georgia has a separate article about its experiences under Reconstruction. The other state names below link to a specific section in the state history article about the Reconstruction era.
Bibliography.
Secondary sources.
For much more detail see 

</doc>
<doc id="55042" url="https://en.wikipedia.org/wiki?curid=55042" title="Bo Hansson">
Bo Hansson

Bo Hansson (April 10, 1943 – April 23, 2010) was a Swedish musician best known for his four instrumental albums released in the 1970s.
Early life and musical career.
Hansson spent his early life in a remote village in the pine forests of northern Sweden, but a change in his parents' fortunes forced a move to Stockholm and they were forced to leave the young Hansson behind, in the care of family friends. As a teenager he joined his parents in Stockholm, where he soon became interested in the burgeoning rock and roll scene and taught himself to play the guitar, before joining the band Rock-Olga.
After the rock and roll craze gave way to jazz and blues in the late fifties, he joined 'Slim' Notini's Blues Gang as a guitarist. Hansson was able to move on and form his own blues group The Merrymen, who supported The Rolling Stones on an early Scandinavian tour.
In 1966, Hansson saw American jazz organist Jack McDuff perform at Stockholm's Gyllene Cirkeln Club, and was so captivated by the performance that he decided to leave The Merrymen to expand his musical horizons. Encouraged by fellow Merryman Bill Öhrström, he eventually acquired his own Hammond organ. Öhrström became an A&R man and producer at Polydor Sweden, and introduced Hansson to other musicians, one of whom was drummer Janne Carlsson. Hansson and Carlsson immediately hit it off and were signed by Polydor under the band name Hansson & Karlsson, playing up-tempo Hammond organ based music and releasing three albums between 1967 and 1969. They became immensely popular in their home country and some parts of Europe, and even reached the ear of Jimi Hendrix, who took time out from his tour to jam with the duo, along with George Clemons on drums and Georg Wadenius on guitar, at the Klub Filips in Stockholm in late 1967. Hendrix went on to record a Hansson song, "Tax Free".
Solo debut.
By 1969, Janne Carlsson had become a successful comedian and TV host, and Hansson decide to break up the partnership. Entranced by a copy of J.R.R. Tolkien's "The Lord of the Rings", which he had purloined from his girlfriend, he moved into a friend's vacant apartment and started writing. When the unfortunate friend returned, he found that he had been evicted from his apartment after numerous complaints about the noise Hansson was creating. Hansson retreated to a remote cottage on an island off Stockholm where he, drummer Rune Carlsson and engineer Anders Lind, who had worked previously with Hansson & Karlsson, spent the winter of 1969 recording what was to become Hansson's debut solo album on a borrowed four track recorder. The resourceful Lind was even able to gain use of the only eight track recorder in Sweden at that time at the Swedish National Radio station, on the pretext that he was interested in buying one himself and wanted to test it. Once there, he persuaded session musicians Gunnar Bergsten and Sten Bergman to flesh out the recordings.
"Sagan Om Ringen" was released on Silence Records (Sweden's first independent record label which was set up by Anders Lind) in autumn 1970 and became a huge hit. Copies of the album began to filter across to Britain where it came to the attention of Tony Stratton-Smith, who was so impressed that he released the album as "Music Inspired by Lord of the Rings" on his own Famous Charisma label in September 1972. The album peaked at #34 on the UK Album Chart and became Hansson's only UK Top 40 album.
Further musical career.
Encouraged by the success of his first album, Hansson was booked into Stockholm's Studio Decibel where he began work on the follow-up. Using the same team, along with guitarist Kenny Håkansson, the new recordings benefitted from the superior equipment and "Ur Trollkarlens Hatt" was released on Silence Records in late 1972, and on Charisma in the UK as "Magician's Hat" in October 1973. Although critically acclaimed, it failed to reach the popularity of the earlier work, and did not chart in the UK.
Popularity in Sweden, however, put pressure on Hansson to tour, and a touring band was hastily assembled. The tour was cancelled by the reclusive keyboard player, however, citing a lack of motivation. Instead, he returned to Studio Decibel and started work on his third album. Using the tried and trusted backing musicians the recordings continued the progression of the previous album, and "Mellanväsen" was issued on Silence Records in October 1975, and as "Attic Thoughts" on Charisma in the UK in February 1976. Despite being the most accomplished record of Hansson's career, it did not achieve popularity, although it did feature the song "Rabbit Music" which would point the way to Hansson's next album.
In 1976 Hansson and Silence Records parted company, and he was able to negotiate a worldwide deal with Charisma. He returned once again to Studio Decibel and began work on recordings that were inspired by another book; Richard Adams' "Watership Down". Using the same session musicians but with a new producer, Pontus Olssen, the recordings were issued in September 1977 as "El 'Ahrairah" (after the novel's rabbit god) and "Music Inspired by Watership Down" on Charisma in the UK, and on Sire Records in the US. Another disappointing chart performance led to Hansson's withdrawal from the popular music scene, and though he worked on a number of projects with friends, little was heard from him until 1985 when he released, in Sweden only, the album "Mitt I Livet" (In the Middle of Life) on Silence Records (SRS 4700). He then fell into obscurity.
Later years.
Hansson found a new following amongst Swedish DJs in recent years, who sampled his music – something which apparently pleased him enormously. Although the better known "Music Inspired by Lord of the Rings" made it onto CD in 1988, in a remixed version accompanied by selected tracks from "Magician's Hat" and "Attic Thoughts", his other 1970s albums remained unavailable in full until 2005 when Silence Records (through EMI) re-issued them on CD, digitally re-mastered and with previously unreleased extra material.
In 1998 Hansson & Karlsson reunited for some live concerts and a compilation album.
Due to his pioneering work and the mysteries surrounding Hansson & Karlsson's rise and fall, Hansson received the status of a living legend among the independent musical community in Sweden. He occasionally performed live sets with fellow organist Eric Malmberg who has been greatly inspired by Hansson's work.
Bo Hansson died in Stockholm on April 23, 2010.
Discography.
As Bo Hansson.
Compilation.
The CD reissues of the first three solo records are distributed internationally by the original company, Silence Records.

</doc>
<doc id="55044" url="https://en.wikipedia.org/wiki?curid=55044" title="Transition">
Transition

Transition or transitional may refer to:

</doc>
<doc id="55051" url="https://en.wikipedia.org/wiki?curid=55051" title="Capparales">
Capparales

Capparales is a botanical name of an order of flowering plants. It was used in the Cronquist system for an order in subclass Dilleniidae and in the Kubitzki system, nowadays. In the 1981 version of this system it included :
The APG II system includes all the plants involved in the (expanded) order Brassicales. For names above the rank of family the principle of priority is not obligatory, hence the difference between the two names.

</doc>
<doc id="55052" url="https://en.wikipedia.org/wiki?curid=55052" title="Batales">
Batales

Batales is a botanical name of an order of flowering plants. This name was used in several systems, sometimes in the spelling Batidales. Often this order consisted of the genus "Batis" only. In the 1981 version of the Cronquist system it was an order placed in subclass Dilleniidae with the following circumscription:
The APG II system, used here, includes all the plants involved in the (expanded) order "Brassicales".

</doc>
<doc id="55054" url="https://en.wikipedia.org/wiki?curid=55054" title="Nepenthales">
Nepenthales

Nepenthales (Nepenthales Bercht. & J.Presl) is an order of flowering plants in the Cronquist system of plant classification. Plant systematists currently favor the APG III system of 2009 over the older Cronquist for classifying flowering plants. The order was placed in the subclass Dilleniidae, which in the 1981 version of this system included: 
The APG II system assigns the first two families to the order Caryophyllales and the last family to the order Ericales. 
All three families are carnivorous plant families. The Droseraceae contains three extant genera: "Drosera" (sundews), which catch insects with adhesive droplets; and "Dionaea" (Venus flytrap) and "Aldrovanda" (waterwheel plant), which capture them in leaves with interlocking teeth. The other two families include pitcher plants, which drown their prey.

</doc>
<doc id="55058" url="https://en.wikipedia.org/wiki?curid=55058" title="Primulales">
Primulales

Primulales is the botanical name of an order of flowering plants. This name was used in several systems with little variation in circumscription (see Bentham & Hooker, Engler and Wettstein system). In the 1981 version of the Cronquist system it was an order placed in subclass Dilleniidae with the following circumscription:
The APG III system includes all the plants involved in the (expanded) order Ericales.

</doc>
<doc id="55063" url="https://en.wikipedia.org/wiki?curid=55063" title="China proper">
China proper

China proper, Inner China or the Eighteen Provinces was a term used by Western writers on the Qing dynasty to express a distinction between the core and frontier regions of China. There is no fixed extent for China proper, as many administrative, cultural, and linguistic shifts have occurred in Chinese history. One definition refers to the original area of Chinese civilization, the North China Plain; another to the "Eighteen Provinces" system of the Qing dynasty. There is no direct translation for "China proper" in the Chinese language due to differences in terminology used by the Qing to refer to the regions and the expression is controversial among scholars, particularly in China, partly because it implies the frontier regions outside China proper are in some way separate or even illegitimate territories of China.
Origin of the concept.
It is not clear when the concept of "China proper" in the Western world appeared. However, it is plausible that historians during the age of empires and the fast changing borders in the eighteenth century, applied it to distinguish China's 18-provinces from its newly acquired properties. This would also apply to Great Britain proper versus the British Empire, which would encompass vast lands overseas. The same would apply to France proper in contrast to the French Empire of the time, which Napoleon managed to expand all the way to Moscow.
According to Harry Harding, the concept can date back to 1827. But as early as in 1795, William Winterbotham adopted this concept in his book. When describing the Chinese Empire under the Qing dynasty, Winterbotham divided it into three parts: China proper, Chinese Tartary, and the States Tributary to China. He adopted the opinions of Du Halde and Grosier and suspected that the name of "China" came from Qin dynasty. He then said: "China, properly so called... comprehends from north to south eighteen degrees; its extent from east to west being somewhat less..."
However, to introduce China proper, Winterbotham still used the outdated 15-province system of the Ming dynasty, which the Qing dynasty used until 1662. Although Ming dynasty also had 15 basic local divisions, Winterbotham uses the name of Kiang-nan (江南, Jiāngnán) province, which had been called Nan-Zhili (南直隶, Nán-Zhílì) during the Ming dynasty and was renamed to Kiang-nan (i.e., Jiangnan) in 1645, the second year after the Manchu Qing dynasty overthrew the Ming. This 15-province system was gradually replaced by the 18-province system between 1662 and 1667. Using the 15-province system and the name of Kiang-nan Province indicates that the concept of China proper probably had appeared between 1645 and 1662 and this concept may reflect the idea that identifies China as the territory of the former Ming dynasty after the Qing conquest of the Ming.
The concept of "China proper" also appeared before this 1795 book. It can be found in "The Gentleman's Magazine", published in 1790, and "The Monthly Review", published in 1749. In the nineteenth century, the term "China proper" was sometimes used by Chinese officials when they were communicating in foreign languages. For instance, the Qing ambassador to Britain Zeng Jize used it in an English language article, which he published in 1887.
Dulimbai Gurun is the Manchu name for China (中國, Zhongguo; "Middle Kingdom"). After conquering the Ming, the Manchu Qing identified their state as "China" (Zhongguo), and referred to it as "Dulimbai Gurun" in Manchu. The Manchu Qing Emperors equated the lands of the Qing state (including both "China proper" and present day Manchuria, Xinjiang, Mongolia, Tibet and other areas as "China" in both the Chinese and Manchu languages, defining China as a multi ethnic state, rejecting the idea that China only meant Han areas in "China proper", proclaiming that both Han and non-Han peoples were part of "China", using "China" to refer to the Qing in official documents, international treaties, and foreign affairs, and the "Chinese language" (Dulimbai gurun i bithe) referred to Chinese, Manchu, and Mongol languages, and the term "Chinese people" (中國人, Zhongguo ren; Manchu: Dulimbai gurun i niyalma) referred to all Han, Manchus, and Mongol subjects of the Qing.
When the Qing conquered Dzungaria in 1759, they proclaimed that the new land was absorbed into "China" (Dulimbai Gurun) in a Manchu language memorial. The Qing expounded on their ideology that they were bringing together the "outer" non-Han Chinese like the Inner Mongols, Eastern Mongols, Oirat Mongols, and Tibetans together with the "inner" Han Chinese, into "one family" united in the Qing state, showing that the diverse subjects of the Qing were all part of one family, the Qing used the phrase "Zhong Wai Yi Jia" (中外一家) or "Nei Wai Yi Jia" (內外一家, "interior and exterior as one family"), to convey this idea of "unification" of the different peoples. A Manchu language version of a treaty with the Russian Empire concerning criminal jurisdiction over bandits called people from the Qing as "people of the Central Kingdom (Dulimbai Gurun)". In the Manchu official Tulisen's Manchu language account of his meeting with the Torghut Mongol leader Ayuki Khan, it was mentioned that while the Torghuts were unlike the Russians, the "people of the Central Kingdom" (dulimba-i gurun; 中國, Zhongguo) were like the Torghut Mongols, and the "people of the Central Kingdom" referred to the Manchus.
While the Manchu Qing sought used China (Zhongguo) to describe non-Han areas, however some Han scholar-officials opposed the Qing Manchu Emperor's use of Zhongguo to refer to non-Han areas, using Zhongguo to mark a distinction between the culturally Han Chinese areas and the territory newly brought into the Manchu Qing empire. In the early 19th century, Wei Yuan’s "Shengwuji" (Military History of the Qing Dynasty) calls the inner Asian polities "guo", while the seventeen provinces of the traditional heartland, that is, "China proper," and three eastern provinces of Manchuria are called ""Zhongguo"." Some Han Chinese Ming loyalists refused to use Zhongguo to refer to areas outside the borders of the Ming Empire such as outer Mongolia, in effect refusing to acknowledge the Qing state.
The Manchu Qing referred to the Han Chinese inhabited 18 provinces as "nèidì shíbā shěng" (內地十八省), which meant the "interior region eighteen provinces", or abbreviated it as "nèidì" (內地), "interior region" and also as "jùnxiàn" (郡县), while they referred to the non-Han areas of China such as the Northeast, Outer Mongolia, Inner Mongolia, Xinjiang, and Tibet as "wàifān" (外藩) which means "outer feudatories" or "outer vassals", or as "fānbù" (藩部, "feudatory region"). These waifan were fully subjected to and governed by the Qing government and were considered part of the China (Zhongguo), unlike wàiguó (外國, "outer/foreign countries") like Korea, Vietnam, and the Ryukyus, who paid tribute to the Qing but were not part of China.
Modern.
Today, China proper is a controversial concept in China itself, since the current official paradigm does not contrast the core and the periphery of China. There is no single widely used term corresponding to it in the Mandarin language.
The separation of China into a "China proper" dominated by Han Chinese and one or more "Other Chinas" of ethnic minorities impugns on the legitimacy of China's current borders, which is based on the succession of states principle. According to Sinologist Colin Mackerras, foreign governments have generally accepted Chinese claims over its minority areas, because to redefine a country's territory every time it underwent a change of regime would cause endless instability and warfare. Also, he asks, "if the boundaries of the Qing were considered illegitimate, why should it go back to the much smaller Ming in preference to the quite extensive Tang dynasty boundaries?"
Extent.
There is no fixed extent for China proper, as it is used to express the contrast between the core and frontier regions of China from multiple perspectives: historical, administrative, cultural, and linguistic.
Historical perspective.
One way of thinking about China proper is to refer to ancient Han Chinese dynasties. Chinese civilization developed from a core region in the North China Plain, and expanded outwards over several millennia, conquering and assimilating surrounding peoples, or being conquered and influenced in turn. Some dynasties, such as the Han and Tang dynasties, were particularly expansionist, extending far into Central Asia, while others, such as the Jin and Song dynasties, were forced to relinquish the North China Plain itself to rivals from Northeastern and Central Asia.
The Ming Dynasty was the last Han Chinese dynasty and second-last imperial dynasty to rule China. It governed fifteen administrative entities, which included thirteen provinces (Chinese: 布政使司; Pinyin: Bùzhèngshǐ Sī) and two "directly-governed" areas. After the Manchu-founded Qing Dynasty conquered the Ming Dynasty, the Qing court decided to continue to use the Ming administrative system to rule over former Ming lands, without applying it to other domains within the Qing Dynasty, namely Manchuria, Mongolia, Xinjiang, and Tibet. The 15 administrative units of the Ming Dynasty underwent minor reforms to become the Eighteen Provinces (一十八行省 Pinyin: "Yīshíbā Xíngshěng", or 十八省 "Shíbā Shěng") of China proper under the Qing Dynasty. It was these eighteen provinces that early Western sources referred to as China proper.
There are some minor differences between the extent of Ming China and the extent of the eighteen provinces of Qing China: for example, some parts of Manchuria were a Ming possession belonging to the Ming province of Liaodong (now Liaoning); however, the Qing conquered it before the rest of China and did not put the region back into the provinces of China proper. On the other hand, Taiwan was a new acquisition of the Qing Dynasty, and it was put into Fujian, one of the provinces of China proper. Eastern Kham in Greater Tibet was added to Sichuan, while much of what now constitutes northern Burma was added to Yunnan.
Near the end of the Qing Dynasty, there was an effort to extend the province system of China proper to the rest of the empire. Taiwan was made into a separate province in 1885; however it was ceded to Japan in 1895. Xinjiang was reorganized into a province in 1884. Manchuria was split into the three provinces of Fengtian, Jilin and Heilongjiang in 1907. There was discussion to do the same in Tibet, Kokonor, Inner Mongolia, and Outer Mongolia, but these proposals were not put to practice, and these areas were outside the province system of China proper when the Qing Dynasty fell in 1912.
The Provinces of the Qing Dynasty were:
Some of the revolutionaries who sought to overthrow Qing rule desired to establish a state independent of the Qing Dynasty within the bounds of the Eighteen Provinces, as evinced by the Eighteen-Star Flag they used. Others favoured the replacement of the entire Qing Dynasty by a new republic, as evinced by the Five-Striped Flag they used. Some revolutionaries, such as Zou Rong, used the term "Zhongguo Benbu" (中国本部) which roughly identifies the Eighteen Provinces. When the Qing Dynasty fell, the abdication decree of the Qing Emperor bequeathed the entire Empire to the newborn Republic of China, and the latter idea was therefore adopted by the new republic as the principle of Five Races Under One Union, with Five Races referring to the Han Chinese, Manchus, Mongols, Muslims (Uyghurs, Hui etc.) and Tibetans. The Five-Striped Flag was adopted as the national flag, and the Republic of China viewed itself as a single state encompassing all five regions handed down by the Qing Dynasty. The People's Republic of China, which was founded in 1949 and replaced the Republic of China on the mainland, has continued to claim essentially the same borders, with the only major exception being the recognition of independent Mongolia. As a result, the concept of China proper fell out of favour in China.
The Eighteen Provinces of the Qing Dynasty still exist, but their boundaries have changed. Beijing and Tianjin were eventually split from Hebei (renamed from Zhili), Shanghai from Jiangsu, Chongqing from Sichuan, Ningxia autonomous region from Gansu, and Hainan from Guangdong. Guangxi is now an autonomous region. The provinces that the late Qing dynasty set up have also been kept: Xinjiang became an autonomous region under the People's Republic of China, while the three provinces of Manchuria now have somewhat different borders, with Fengtian renamed as Liaoning.
When the Qing Dynasty fell, Republican Chinese control of Qing territory, including of those generally considered to be in "China proper", was tenuous, and practically nonexistent in Tibet and Outer Mongolia (since 1922), which were controlled by governments that declared independence. The Republic of China subdivided Inner Mongolia in its time on the mainland, although the People's Republic of China later joined Mongol-inhabited territory into a single autonomous region. The PRC joined the Qamdo area into the Tibet area (later the Tibet Autonomous Region). Nationalist China was forced to acknowledge the independence of Mongolia (former Outer Mongolia) and Tannu Uriankhai (now part of Russia as The Tyva Republic), in 1945.
Ethnic perspective.
China proper is often associated with the Han Chinese, the majority ethnic group of China and with the extent of the Chinese language(s), an important unifying element of the Han Chinese ethnicity.
However, Han Chinese areas in the present day do not correspond well to the Eighteen Provinces of the Qing Dynasty. Much of southwestern China, such as areas in the provinces of Yunnan, Guangxi, and Guizhou, was part of successive Han Chinese dynasties, including the Ming Dynasty and the Eighteen Provinces of the Qing Dynasty. However, these areas were and continue to be populated by various non-Han Chinese minority groups, such as the Zhuang, the Miao people, and the Bouyei. Conversely, today Han Chinese form the majority in most of Manchuria, much of Inner Mongolia, many areas in Xinjiang and scattered parts of Tibet, not least due to the expansion of Han Chinese settlement encouraged by the late Qing dynasty, the Republic of China, and the People's Republic of China.
Ethnic Han Chinese is not synonymous with speakers of the Chinese language. Many non-Han Chinese ethnicities, such as the Hui and Manchu, are essentially monolingual in Chinese, but do not identify as Han Chinese. The Chinese language itself is also a complex entity, and should be described as a family of related languages rather than a single language if the criterion of mutual intelligibility is used to classify its subdivisions.
In polls a slim majority of the people of Taiwan call themselves "Taiwanese" only with the rest identifying as "Taiwanese and Chinese" or "Chinese" only. 98% of the people of Taiwan are descendants of immigrants from China since the 1600s, but the inclusion of Taiwan in China, or in the China proper, is still a controversial subject. See History of Taiwan and Political status of Taiwan for more information.

</doc>
<doc id="55064" url="https://en.wikipedia.org/wiki?curid=55064" title="Inner Mongolia">
Inner Mongolia

Inner Mongolia (Mongolian: ["Öbür Monggol"] in Mongolian script, and ["Övör Mongol"] in Mongolian Cyrillic; ), officially Inner Mongolia Autonomous Region or Nei Mongol Autonomous Region, is an autonomous region of the People's Republic of China, located in the north of the country, containing most of China's border with Mongolia (the rest of the China-Mongolia border is taken up by the Xinjiang autonomous region and Gansu province) and a small section of the border with Russia. Its capital is Hohhot, and other major cities include Baotou, Chifeng, and Ordos.
The Autonomous Region was established in 1947, incorporating the areas of the former Republic of China provinces of Suiyuan, Chahar, Rehe, Liaobei and Xing'an, along with the northern parts of Gansu and Ningxia.
It is the third largest subdivision of China, spanning approximately or 12% of China's total land area. It recorded a population of 24,706,321 in the 2010 census, accounting for 1.84% of Mainland China's total population. Inner Mongolia is the country's 23rd most populous province-level division. The majority of the population in the region is Han Chinese, with a sizeable titular Mongol minority. The official languages are Chinese and Mongolian, the latter of which is written in the traditional Mongolian script, as opposed to the Mongolian Cyrillic alphabet, which is used in the state of Mongolia.
Name.
In Chinese, the region is known as "Inner Mongolia", where the terms of "Inner/Outer" are derived from Manchu "dorgi"/"tulergi" (cf. Mongolian "dotugadu"/"gadagadu"). Inner Mongolia is distinct from Outer Mongolia, which was a term used by the Republic of China and previous governments to refer to what is now the independent state of Mongolia plus the Republic of Tuva in Russia. In Mongolian, the region was called "Dotugadu monggol" during Qing rule and was renamed into "Öbür Monggol" in 1947, "öbür" meaning the southern side of a mountain, while the Chinese term "nei menggu" was retained. In recent years, some Mongols began to call Inner Mongolia "Nan Menggu" (Mandarin, literally "South Mongolia") and with it came the change of English translation from Inner Mongolia to Southern Mongolia.
History.
Much of what is known about the history of Greater Mongolia, including Inner Mongolia, is known through Chinese chronicles and historians. Before the rise of the Mongols in the 13th century, what is now central and western Inner Mongolia, especially the Hetao region, alternated in control between Chinese agriculturalists in the south and Xiongnu, Xianbei, Khitan, Jurchen, Tujue, and nomadic Mongol of the north. The historical narrative of what is now Eastern Inner Mongolia mostly consists of alternations between different Tungusic and Mongol tribes, rather than the struggle between nomads and Chinese agriculturalists.
Early history.
Slab Grave cultural monuments are found in northern, central and eastern Mongolia, Inner Mongolia, north-western China, southern, central-eastern and southern Baikal territory. Mongolian scholars prove that this culture related to the Proto-Mongols.
During the Zhou Dynasty, central and western Inner Mongolia (the Hetao region and surrounding areas) were inhabited by nomadic peoples such as the Loufan, Linhu, and Dí, while eastern Inner Mongolia was inhabited by the Donghu. During the Warring States period, King Wuling (340–295 BC) of the state of Zhao based in what is now Hebei and Shanxi provinces pursued an expansionist policy towards the region. After destroying the Dí state of Zhongshan in what is now Hebei province, he defeated the Linhu and Loufan and created the commandery of Yunzhong near modern Hohhot. King Wuling of Zhao also built a long wall stretching through the Hetao region. After Qin Shihuang created the first unified Chinese empire in 221 BC, he sent the general Meng Tian to drive the Xiongnu from the region, and incorporated the old Zhao wall into the Qin Dynasty Great Wall of China. He also maintained two commanderies in the region: Jiuyuan and Yunzhong, and moved 30,000 households there to solidify the region. After the Qin Dynasty collapsed in 206 BC, these efforts were abandoned.
During the Western Han Dynasty, Emperor Wu sent the general Wei Qing to reconquer the Hetao region from the Xiongnu in 127 BC. After the conquest, Emperor Wu continued the policy of building settlements in Hetao to defend against the Xiong-Nu. In that same year he established the commanderies of Shuofang and Wuyuan in Hetao. At the same time, what is now eastern Inner Mongolia was controlled by the Xianbei, who would later on eclipse the Xiongnu in power and influence.
During the Eastern Han Dynasty (25–220 AD), Xiongnu who surrendered to the Han Dynasty began to be settled in Hetao, and intermingled with the Han immigrants in the area. Later on during the Western Jin dynasty, it was a Xiongnu noble from Hetao, Liu Yuan, who established the Han Zhao kingdom in the region, thereby beginning the Sixteen Kingdoms period that saw the disintegration of northern China under a variety of Han and non-Han (including Xiongnu and Xianbei) regimes.
The Sui Dynasty (581–618) and Tang Dynasty (618–907) re-established a unified Chinese empire, and like their predecessors they conquered and settled people into Hetao, though once again these efforts were aborted when the Tang empire began to collapse. Hetao (along with the rest of what now consists Inner Mongolia) was then taken over by the Khitan Empire (Liao Dynasty), founded by the Khitans, a nomadic people originally from what is now the southern part of Manchuria and eastern Inner Mongolia. They were followed by the Western Xia of the Tanguts, who took control of what is now the western part of Inner Mongolia (including western Hetao) . The Khitans were later replaced by the Jurchens, precursors to the modern Manchus, who established the Jin dynasty over Manchuria and northern China.
Mongol and Ming periods.
After Genghis Khan unified the Mongol tribes in 1206 and founded the Mongol Empire, the Tangut Western Xia empire was ultimately conquered in 1227, and the Jurchen Jin dynasty fell in 1234. In 1271, Kublai Khan, the grandson of Genghis Khan established the Yuan dynasty. Kublai Khan's summer capital Shangdu (aka Xanadu) was located near present-day Dolonnor. During that time Ongud and Khunggirad peoples dominated the area of what is now Inner Mongolia. After the Yuan dynasty was overthrown by the Han-led Ming dynasty in 1368, the Ming captured parts of Inner Mongolia including Shangdu and Yingchang. The Ming rebuilt the Great Wall of China at its present location, which roughly follows the southern border of the modern Inner Mongolia Autonomous Region (though it deviates significantly at the Hebei-Inner Mongolia border). The Ming established the Three Guards composed of the Mongols there. Soon after the Tumu incident in 1449, when the Oirat ruler Esen taishi captured the Chinese emperor, Mongols flooded south from Outer Mongolia to Inner Mongolia. Thus from then on until 1635, Inner Mongolia was the political and cultural center of the Mongols during the Northern Yuan dynasty.
Qing period.
The eastern Mongol tribes near and in Manchuria, particularly the Khorchin and Southern Khalkha in today's Inner Mongolia intermarried, formed alliances with, and fought against the Jurchen tribes until Nurhaci, the founder of the new Jin dynasty, consolidated his control over all groups in the area in 1593. The Manchus gained far-reaching control of the Inner Mongolian tribes in 1635, when Ligden Khan's son surrendered the Chakhar Mongol tribes to the Manchus. The Manchus subsequently invaded Ming China in 1644, bringing it under the control of their newly established Qing dynasty. Under the Qing dynasty (1636–1912), Greater Mongolia was administered in a different way for each region:
The Inner Mongolian Chahar leader Ligdan Khan, a descendant of Genghis Khan, opposed and fought against the Qing until he died of smallpox in 1634. Thereafter, the Inner Mongols under his son Ejei Khan surrendered to the Qing and was given the title of Prince (Qin Wang, 親王), and Inner Mongolian nobility became closely tied to the Qing royal family and intermarried with them extensively. Ejei Khan died in 1661 and was succeeded by his brother Abunai. After Abunai showed disaffection with Manchu Qing rule, he was placed under house arrest in 1669 in Shenyang and the Kangxi Emperor gave his title to his son Borni. Abunai then bid his time and then he and his brother Lubuzung revolted against the Qing in 1675 during the Revolt of the Three Feudatories, with 3,000 Chahar Mongol followers joining in on the revolt. The revolt was put down within two months, the Qing then crushed the rebels in a battle on April 20, 1675, killing Abunai and all his followers. Their title was abolished, all Chahar Mongol royal males were executed even if they were born to Manchu Qing princesses, and all Chahar Mongol royal females were sold into slavery except the Manchu Qing princesses. The Chahar Mongols were then put under the direct control of the Qing Emperor unlike the other Inner Mongol leagues which maintained their autonomy.
Despite officially prohibiting Han Chinese settlement on the Manchu and Mongol lands, by the 18th century the Qing decided to settle Han refugees from northern China who were suffering from famine, floods, and drought into Manchuria and Inner Mongolia so that Han Chinese farmed 500,000 hectares in Manchuria and tens of thousands of hectares in Inner Mongolia by the 1780s.
Ordinary Mongols were not allowed to travel outside their own leagues. Mongols were forbidden by the Qing from crossing the borders of their banners, even into other Mongol Banners and from crossing into neidi (the Han Chinese 18 provinces) and were given serious punishments if they did in order to keep the Mongols divided against each other to benefit the Qing.
During the eighteenth century, growing numbers of Han Chinese settlers had illegally begun to move into the Inner Mongolian steppe. By 1791 there had been so many Han Chinese settlers in the Front Gorlos Banner that the jasak had petitioned the Qing government to legalize the status of the peasants who had already settled there.
During the nineteenth century, the Manchus were becoming increasingly sinicized, and faced with the Russian threat, they began to encourage Han Chinese farmers to settle in both Mongolia and Manchuria. This policy was followed by subsequent governments. The railroads that were being built in these regions were especially useful to the Han Chinese settlers. Land was either sold by Mongol Princes, or leased to Han Chinese farmers, or simply taken away from the nomads and given to Han Chinese farmers. The Jindandao Incident, a rebellion by an ethnic Chinese secret society called Jindandao occurred in Inner Mongolia in November 1891 and massacred 150,000 Mongols before being suppressed by government troops in late December.
Republic of China period.
Outer Mongolia gained independence from the Qing Dynasty in 1911, when the Jebtsundamba Khutugtu of the Khalkha was declared the Bogd Khan of Mongolia. Although almost all banners of Inner Mongolia recognized the Bogd Khan as the supreme ruler of Mongols, the internal strife within the region prevented a full reunification. The Mongol rebellions in Inner Mongolia were counterbalanced by princes who hoped to see a restored Qing dynasty in Manchuria and Mongolia, as they considered the theocratic rule of the Bogd Khan would be against their modernizing objectives for Mongolia. Eventually, the newly formed Republic of China promised a new nation of five races (Han, Manchu, Mongol, Tibetan and Uyghur), and suppressed the Mongol rebellions in the area, forcing the Inner Mongolian princes to recognize the Republic of China.
The Republic of China reorganized Inner Mongolia into provinces:
Some Republic of China maps still show this structure.
Mengjiang period.
Mengjiang was an autonomous area of Reorganized National Government of China, which was a puppet regime of Japan.
In 1931 Manchuria came under the control of the Japanese puppet state Manchukuo, taking the Mongol areas in the Manchurian provinces (i.e. Hulunbuir and Jirim leagues) along. Rehe was also incorporated into Manchukuo in 1933, taking Juu Uda and Josutu leagues along with it. These areas were administered by Manchukuo until the end of World War II in 1945.
In 1937, open war broke out between the Republic of China and the Empire of Japan. On December 8, 1937, Mongolian Prince Demchugdongrub (also known as "De Wang") declared the independence of the remaining parts of Inner Mongolia (i.e. the Suiyuan and Chahar provinces) as Mengkiang or Mengkukuo, and signed close agreements with Manchukuo and Japan. The capital was established at Zhangbei (now in Hebei province), with the puppet government's control extending as far west as the Hohhot region. In August 1945, Mengkiang was taken by Soviet and Outer Mongolian troops during Manchurian Strategic Offensive Operation. Despite a considerable movement among Inner Mongolia's Mongols (who comprised then around 15% of Inner Mongolia's population, while Han Chinese around 83%) for unification with Outer Mongolia, Inner Mongolia remained part of China.
Communist era.
The Communist movement gradually gained momentum as part of the Third Communist International in Inner Mongolia during the Japanese period. By the end of WWII, the Inner Mongolian faction of the ComIntern had a functional militia, and actively opposed the attempts at independence by De Wang's Chinggisid princes on the grounds of fighting feudalism. Following the end of World War II, the Chinese Communists gained control of Manchuria as well as the Inner Mongolian Communists with decisive Soviet support, and established the Inner Mongolia Autonomous Region in 1947. The Comintern army was absorbed into the People's Liberation Army. Initially the autonomous region included just the Hulunbuir region. Over the next decade, as the communists established the People's Republic of China and consolidated control over mainland China, Inner Mongolia was expanded westwards to include five of the six original leagues (except Josutu League, which remains in Liaoning province), the northern part of the Chahar region, by then a league as well (southern Chahar remains in Hebei province), the Hetao region, and the Alashan and Ejine banners. Eventually, near all areas with sizeable Mongol populations were incorporated into the region, giving present-day Inner Mongolia its elongated shape. The leader of Inner Mongolia during that time, as both regional CPC secretary and head of regional government, was Ulanhu.
During the Cultural Revolution, the administration of Ulanhu was purged, and a wave of repressions was initiated against the Mongol population of the autonomous region. In 1969 much of Inner Mongolia was distributed among surrounding provinces, with Hulunbuir divided between Heilongjiang and Jilin, Jirim going to Jilin, Juu Uda to Liaoning, and the Alashan and Ejine region divided among Gansu and Ningxia. This was reversed in 1979.
Inner Mongolia has seen considerable development since Deng Xiaoping instituted Chinese economic reform in 1978. For about ten years since 2000, Inner Mongolia's GDP growth has been the highest in the country, (along with Guangdong) largely owing to the success of natural resource industries in the region. GDP growth has continually been over 10%, even 15% and connections with the Wolf Economy to the north has helped development. However, growth has come at a cost with huge amounts of pollution and degradation to the grasslands. Attempts to attract ethnic Chinese to migrate from other regions, as well as urbanise those rural nomads and peasants has led to huge amounts of corruption and waste in public spending, such as Ordos City. Acute uneven wealth distribution has further exacerbated ethnic tensions, many indigenous Mongolians feeling they are increasingly marginalised in their own homeland, leading to riots in 2011 and 2013.
Geography.
Officially Inner Mongolia is classified as one of the provincial-level divisions of North China, but its great stretch means that parts of it belong to Northeast China and Northwest China as well. It borders eight provincial-level divisions in all three of the aforementioned regions (Heilongjiang, Jilin, Liaoning, Hebei, Shanxi, Shaanxi, Ningxia, and Gansu), tying with Shaanxi for the greatest number of bordering provincial-level divisions. Most of its international border is with Mongolia, which, in Chinese, is sometimes called “Outer Mongolia” (外蒙古), while a small portion is with Russia.
Inner Mongolia largely consists of the northern side of the North China Craton, a tilted and sedimented Precambrian block. In the extreme southwest is the edge of the Tibetan Plateau where the autonomous region’s highest peak, Main Peak in the Helan Mountains reaches , and is still being pushed up today in short bursts. Most of Inner Mongolia is a plateau averaging around in altitude and covered by extensive loess and sand deposits. The northern part consists of the Mesozoic era Khingan Mountains, and is owing to the cooler climate more forested, chiefly with Manchurian elm, ash, birch, Mongolian oak and a number of pine and spruce species. Where discontinuous permafrost is present north of Hailar District, forests are almost exclusively coniferous. In the south the natural vegetation is grassland in the east and very sparse in the arid west, and grazing is the dominant economic activity.
Owing to the ancient, weathered rocks lying under its deep sedimentary cover, Inner Mongolia is a major mining district, possessing large reserves of coal, iron ore and rare earth minerals, which have made it a major industrial region today.
Climate.
Due to its elongated shape, Inner Mongolia has a wide variety of regional climates. Throughout the region, the climate is based off a four-season, monsoon climate. The winters in Inner Mongolia are very long, cold, and dry with frequent blizzards, though snowfall is so light that Inner Mongolia has no modern glaciers even on the highest Helan peaks. The spring is short, mild and arid, with large, dangerous sandstorms, whilst the summer is very warm to hot and relatively humid except in the west where it remains dry. Autumn is brief and sees a steady cooling, with temperatures below reached in October in the north and November in the south.
Officially, most of Inner Mongolia is classified as either a cold arid or steppe regime (Köppen "BWk, BSk", respectively). The small portion besides these are classified as humid continental (Köppen "Dwb") in the northeast, or subarctic (Köppen "Dwc") in the far north near Hulunbuir.
Administrative divisions.
Inner Mongolia is divided into twelve prefecture-level divisions. Until the late 1990s, most of Inner Mongolia's prefectural regions were known as "Leagues" (), a usage retained from Mongol divisions of the Qing Dynasty. Similarly, county-level divisions are often known as "Banners" (). Since the 1990s, numerous Leagues have converted into prefecture-level cities, although Banners remain. The restructuring led to the conversion of primate cities in most leagues to convert to districts administratively (i.e.: Hailar, Jining and Dongsheng). Some newly founded prefecture-level cities have chosen to retain the original name of League (i.e.: Hulunbuir, Bayannur and Ulanqab), some have adopted the Chinese name of their primate city (Chifeng, Tongliao), and one League (Yekejuu) simply renamed itself Ordos. Despite these recent administrative changes, there is no indication that the Alxa, Hinggan, and Xilingol Leagues will convert to prefecture-level cities in the near future.
Many of the prefecture-level cities were converted very recently from leagues.
The twelve prefecture-level divisions of Inner Mongolia are subdivided into 102 county-level divisions, including 22 districts, 11 county-level cities, 17 counties, 49 banners, and 3 autonomous banners. Those are in turn divided into 1425 township-level divisions, including 532 towns, 407 townships, 277 sumu, eighteen ethnic townships, one ethnic sumu, and 190 subdistricts.
Economy.
Farming of crops such as wheat takes precedence along the river valleys. In the more arid grasslands, herding of goats, sheep and so on is a traditional method of subsistence. Forestry and hunting are somewhat important in the Greater Khingan ranges in the east. Reindeer herding is carried out by Evenks in the Evenk Autonomous Banner. More recently, growing grapes and winemaking have become an economic factor in the Wuhai area.
Inner Mongolia has abundance of resources especially coal, cashmere, natural gas, rare earth elements, and has more deposits of naturally occurring niobium, zirconium and beryllium than any other province-level region in China. However, in the past, the exploitation and utilisation of resources were rather inefficient, which resulted in poor returns from rich resources. Inner Mongolia is also an important coal production base, with more than a quarter of the world's coal reserves located in the province. It plans to double annual coal output by 2010 (from the 2005 volume of 260 million tons) to 500 million tons of coal a year.
Industry in Inner Mongolia has grown up mainly around coal, power generation, forestry-related industries, and related industries.
Inner Mongolia now encourages six competitive industries: energy, chemicals, metallurgy, equipment manufacturing, processing of farm (including dairy) produce, and high technology. Well-known Inner Mongolian enterprises include companies such as ERDOS, Yili, and Mengniu.
The nominal GDP of Inner Mongolia in 2010 was 1.16 trillion yuan (US$172.1 billion), a growth of 16.9% from 2008, with an average annual increase of 20% from the period 2003-2007. Its per capita GDP reached 37,287 yuan (US$5,460) in 2009. In 2008, Inner Mongolia's primary, secondary, and tertiary industries were worth 90.7 billion yuan, 427.1 billion yuan, and 258.4 billion yuan respectively. The urban per capita disposable income and rural per capita net income were 14,431 yuan and 4,656 yuan, up 16.6% and 17.8% respectively.
As with much of China, economic growth has led to a boom in construction, including new commercial development and large apartment complexes.
In addition to its large reserves of natural resources, Inner Mongolia also has the largest usable wind power capacity in China thanks to strong winds which develop in the province's grasslands. Some private companies have set up wind parks in parts of Inner Mongolia such as Bailingmiao, Hutengliang and Zhouzi.
Economic and Technological Development Zones.
Hohhot Export Processing Zone was established on June 21, 2002, by the State Council, which is located in the west of the Hohhot, with a planning area of 2.2 km2. Industries encouraged in the export processing zone include Electronics Assembly & Manufacturing, Telecommunications Equipment, Garment and Textiles Production, Trading and Distribution, Biotechnology/Pharmaceuticals, Food/Beverage Processing, Instruments & Industrial Equipment Production, Medical Equipment and Supplies, Shipping/Warehousing/Logistics, Heavy Industry.
Government and politics.
Under the Constitution of the People's Republic of China, articles 112-122, autonomous regions have limited autonomy in both the political and economic arena. Autonomous regions have more discretion in administering economic policy in the region in accordance with national guidelines. Structurally, the Chairman—who legally must be an ethnic minority and is usually ethnic Mongolian—is always kept in check by the Communist Party Regional Committee Secretary, who is usually from a different part of China (to reduce corruption) and Han Chinese. The current party secretary is Wang Jun. The Inner Mongolian government and its subsidiaries follow roughly the same structure as that of a Chinese province. With regards to economic policy, as a part of increased federalism characteristics in China, Inner Mongolia has become more independent in implementing its own economic roadmap.
Demographics.
When the autonomous region was established in 1947, Han Chinese comprised 83.6% of the population, while the Mongols comprised 14.8% of the population. By 2000, the percentage of Han Chinese had fallen to 79.2%. While the Hetao region along the Yellow River has always alternated between farmers from the south and nomads from the north, the most recent episode of Han Chinese migration began in the early 18th century with encouragement from the Qing Dynasty, and continued into the 20th century. Han Chinese live mostly in the Hetao region as well as various population centres in central and eastern Inner Mongolia. Over 70% of Mongols are concentrated in less than 18% of Inner Mongolia's territory (Hinggan League, and prefectures Tongliao and Chifeng).
Mongols are the second largest ethnic group, comprising 17.11% of the population. They include many diverse Mongolian-speaking groups; groups such as the Buryats and the Oirats are also officially considered to be Mongols in China. Many of the traditionally nomadic Mongols have settled in permanent homes as their pastoral economy was collectivized during the Maoist Era.
Other ethnic groups include the Daur, the Evenks, the Oroqen, the Hui, the Manchus, and the Koreans.
Excludes members of the People's Liberation Army in active service.
Language and culture.
By law, all street signs, commercial outlets, and government documents must be bilingual, written in both Mongolian and Chinese. There are three Mongolian TV channels in the Inner Mongolia Satellite TV network. In public transportation, all announcements are to be bilingual.
Alongside Chinese, Mongolian is the official provincial language of the Inner Mongolia Autonomous Region, where there are at least 4.1 million ethnic Mongols. Across the whole of China, the language is spoken by roughly half of the country's 5.8 million ethnic Mongols (2005 estimate) However, the exact number of Mongolian speakers in China is unknown, as there is no data available on the language proficiency of that country's citizens. The use of Mongolian in China, specifically in Inner Mongolia, has witnessed periods of decline and revival over the last few hundred years. The language experienced a decline during the late Qing period, a revival between 1947 and 1965, a second decline between 1966 and 1976, a second revival between 1977 and 1992, and a third decline between 1995 and 2012. However, in spite of the decline of the Mongolian language in some of Inner Mongolia's urban areas and educational spheres, the ethnic identity of the urbanized Chinese-speaking Mongols is most likely going to survive due to the presence of urban ethnic communities. The multilingual situation in Inner Mongolia does not appear to obstruct efforts by ethnic Mongols to preserve their language. Although an unknown number of Mongols in China, such as the Tumets, may have completely or partially lost the ability to speak their language, they are still registered as ethnic Mongols and continue to identify themselves as ethnic Mongols. The children of inter-ethnic Mongol-Chinese marriages also claim to be and are registered as ethnic Mongols.
Mongols in Inner Mongolia speak Mongolian dialects such as Chakhar, Xilingol, Baarin, Khorchin and Kharchin Mongolian and, depending on definition and analysis, further dialects or closely related independent Central Mongolic languages such as Ordos, Khamnigan, Barghu Buryat and the arguably Oirat dialect Alasha. The standard pronunciation of Mongolian in China is based on the Chakhar dialect of the Plain Blue Banner, located in central Inner Mongolia, while the grammar is based on all Southern Mongolian dialects. This is different from the Mongolian state, where the standard pronunciation is based on the closely related Khalkha dialect. There are a number of independent languages spoken in Hulunbuir such as the somewhat more distant Mongolic language Dagur and the Tungusic language Evenki. Officially, even the Evenki dialect Oroqin is considered a language.
The Han Chinese of Inner Mongolia speak a variety of dialects, depending on the region. Those in the eastern parts tend to speak Northeastern Mandarin, which belongs to the Mandarin group of dialects; those in the central parts, such as the Huang He valley, speak varieties of Jin, another subdivision of Chinese, due to its proximity to other Jin-speaking areas in China such as the Shanxi province. Cities such as Hohhot and Baotou both have their unique brand of Jin Chinese such as the Zhangjiakou–Hohhot dialect which are sometimes incomprehensible with dialects spoken in northeastern regions such as Hailar.
The vast grasslands have long symbolised Inner Mongolia. Mongolian art often depicts the grassland in an uplifting fashion and emphasizes Mongolian nomadic traditions. The Mongols of Inner Mongolia still practice their traditional arts. Inner Mongolian cuisine has Mongol roots and consists of dairy-related products and "hand-held lamb" (手扒肉). In recent years, franchises based on Hot pot have appeared Inner Mongolia, the best known of which is "Xiaofeiyang" (小肥羊). Notable Inner Mongolian commercial brand names include Mengniu and Yili, both of which began as dairy product and ice cream producers.
Among the Han Chinese of Inner Mongolia, Jinju (晉劇) or Shanxi Opera is a popular traditional form of entertainment. See also: Shanxi.
A popular career in Inner Mongolia is circus acrobatics. The internationally known Inner Mongolia Acrobatic Troupe travels and performs with the renowned Ringling Bros. and Barnum and Bailey Circus.
Religion.
According to researches conducted by the Religious Studies Department of Minzu University of China, adherents of the five officially recognised religions of the state (Buddhism, Taoism, Protestantism, Catholicism and Islam) constitute only 3.7% of the population of Inner Mongolia. According to the Chinese General Social Survey of 2009, Christianity is the religious identity of 2% of the population of the region, while a demographic analysis of the year 2010, reported that Muslims form the 0.91%.
At the same time, 80% of the inhabitants of the region declare to worship "Tian" and "aobao", features of both Chinese folk religion and Mongolian shamanism.
The cult of Genghis Khan, present in the form of various Genghis Khan temples, is a tradition of Mongolian shamanism, in which he is considered a cultural hero and divine ancestor, an embodiment of the "Tenger" (Heaven, God of Heaven). His worship in special temples, greatly developed in Inner Mongolia since the 1980s, is also shared by the Han Chinese, claiming his spirit as the founding principle of the Yuan dynasty.
Tibetan Buddhism (Mongolian Buddhism) is the dominant form of Buddhism in Inner Mongolia, also practiced by many Han Chinese, and its influence may be far larger than what the official adherents statistics would testify. Another form of Buddhism, practiced by the Chinese, are the schools of Chinese Buddhism.
Tourism.
In the capital city Hohhot:
Elsewhere in Inner Mongolia:
Chinese space program.
One of China's space vehicle launch facilities, Jiuquan Satellite Launch Center (JSLC) (酒泉卫星发射中心), is located in the extreme west of Inner Mongolia, in the Alxa League's Ejin Banner. It was founded in 1958, making it the PRC's first launch facility. More Chinese launches have occurred at Jiuquan than anywhere else. As with all Chinese launch facilities, it is remote and generally closed to the public. It is named as such since Jiuquan is the nearest urban center, although Jiuquan is in the nearby province of Gansu. Many space vehicles have also made their touchdowns in Inner Mongolia. For example, the crew of Shenzhou 6 landed in Siziwang Banner, near Hohhot.
Education.
Colleges and universities.
All of the above are under the authority of the autonomous region government. Institutions without full-time bachelor programs are not listed.

</doc>
<doc id="55066" url="https://en.wikipedia.org/wiki?curid=55066" title="Nanking Massacre">
Nanking Massacre

The Nanking Massacre or Nanjing Massacre, also known as the Rape of Nanking or Rape of Nanjing, was an episode during the Second Sino-Japanese War of mass murder and mass rape by Japanese troops against the residents of Nanjing (then spelled "Nanking"), then capital of the Republic of China. The massacre occurred over six weeks starting December 13, 1937, the day that the Japanese captured Nanjing. During this period, soldiers of the Imperial Japanese Army murdered Chinese civilians and disarmed combatants numbering an estimated 40,000 to over 300,000, and perpetrated widespread rape and looting. Several key perpetrators were tried and found guilty at the International Military Tribunal for the Far East and the Nanjing War Crimes Tribunal, and were executed. A key perpetrator, Prince Asaka of the Imperial Family, escaped prosecution by having earlier been granted immunity by the Allies.
Since most Japanese military records on the killings were kept secret or destroyed shortly after the surrender of Japan in 1945, historians have not been able to accurately estimate the death toll of the massacre. The International Military Tribunal for the Far East estimated in 1948 that over 200,000 Chinese were killed in the incident. China's official estimate is more than 300,000 dead based on the evaluation of the Nanjing War Crimes Tribunal in 1947. The death toll has been actively contested among scholars since the 1980s.
The event remains a contentious political issue, as aspects of it have been disputed by historical negationists and Japanese nationalists, who assert that the massacre has been either exaggerated or fabricated for propaganda purposes. The controversy surrounding the massacre remains a stumbling block in Sino-Japanese relations and in Japanese relations with other Asia-Pacific nations, such as South Korea and the Philippines.
Although the Japanese government has admitted to the killing of a large number of non-combatants, looting, and other violence committed by the Imperial Japanese Army after the fall of Nanking, and Japanese veterans who served there have confirmed that a massacre took place, a small but vocal minority within both the Japanese government and society have argued that the death toll was military in nature and that no such crimes ever occurred. Denial of the massacre and revisionist accounts of the killings have become a staple of Japanese nationalism. In Japan, public opinion of the massacres varies, but few deny outright that the conflict occurred.
Military situation.
In August 1937, the Japanese army invaded Shanghai where they met strong resistance and suffered heavy casualties. The battle was bloody as both sides faced attrition in urban hand-to-hand combat. By mid-November the Japanese had captured Shanghai with the help of naval bombardment. The General Staff Headquarters in Tokyo initially decided not to expand the war due to heavy casualties and low troop morale. Nevertheless, on December 1, headquarters ordered the Central China Area Army and the 10th Army to capture Nanjing, then-capital of the Republic of China.
Relocation of the capital.
After losing the Battle of Shanghai, Chiang Kai-shek knew that the fall of Nanjing was a matter of time. He and his staff realized that they could not risk the annihilation of their elite troops in a symbolic but hopeless defense of the capital. To preserve the army for future battles, most of it was withdrawn. Chiang's strategy was to follow the suggestion of his German advisers to draw the Japanese army deep into China and use China's vast territory as a defensive strength. Chiang planned to fight a protracted war of attrition to wear down the Japanese in the hinterland of China.
Leaving General Tang Shengzhi in charge of the city for the Battle of Nanking, Chiang and many of his advisors flew to Wuhan, where they stayed until it was attacked in 1938.
Strategy for the defense of Nanking.
In a press release to foreign reporters, Tang Shengzhi announced the city would not surrender and would fight to the death. Tang gathered about 100,000 soldiers, largely untrained, including Chinese troops who had participated in the Battle of Shanghai. To prevent civilians from fleeing the city, he ordered troops to guard the port, as instructed by Chiang Kai-shek. The defense force blocked roads, destroyed boats, and burnt nearby villages, preventing widespread evacuation.
The Chinese government left for relocation on December 1, and the president left on December 7, leaving the fate of Nanking to an International Committee led by John Rabe.
The defense plan fell apart quickly. Those defending the city encountered Chinese troops fleeing from previous defeats such as the Battle of Shanghai, running from the advancing Japanese army. This did nothing to help the morale of the defenders, many of whom were killed during the defense of the city and subsequent Japanese occupation.
Approach of the Imperial Japanese Army.
Japanese war crimes on the march to Nanking.
Although the massacre is generally described as having occurred over a six-week period after the fall of Nanjing, the crimes committed by the Japanese army were not limited to that period. Many atrocities were reported to have been committed as the Japanese army advanced from Shanghai to Nanjing.
According to one Japanese journalist embedded with Imperial forces at the time, "The reason that the Army is advancing to Nanking quite rapidly is due to the tacit consent among the officers and men that they could loot and rape as they wish."
Novelist Tatsuzō Ishikawa vividly described how the 16th Division of the Shanghai Expeditionary Force committed atrocities on the march between Shanghai and Nanjing in his novel "Ikiteiru Heitai" (Living Soldiers), which was based on interviews that Ishikawa conducted with troops in Nanjing in January 1938.
Perhaps the most notorious atrocity was a killing contest between two Japanese officers as reported in the "Tokyo Nichi Nichi Shimbun" and the English language "Japan Advertiser". The contest — a race between the two officers to see which could kill 100 people first using only a sword — was covered much like a sporting event with regular updates on the score over a series of days. In Japan, the veracity of the newspaper article about the contest was the subject of ferocious debate for several decades starting in 1967.
In 2000, historian Bob Tadashi Wakabayashi concurred with certain Japanese scholars who had argued that the contest was a concocted story, with the collusion of the soldiers themselves for the purpose of raising the national fighting spirit. In 2005, a Tokyo district judge dismissed a suit by the families of the lieutenants, stating that "the lieutenants admitted the fact that they raced to kill 100 people" and that the story cannot be proven to be clearly false. The judge also ruled against the civil claim of the plaintiffs because the original article was more than 60 years old. The historicity of the event remains disputed in Japan.
Flight of Chinese civilians.
As the Japanese army drew closer to Nanjing, panicked Chinese civilians fled in droves, not only because of the dangers of the anticipated battle but also because they feared the deprivation inherent in the scorched earth strategy that the Chinese troops were implementing in the area surrounding the city.
The Nanjing garrison force set fire to buildings and houses in the areas close to Xiakuan to the north as well as in the environs of the eastern and southern city gates. Targets within and outside of the city walls—such as military barracks, private homes, the Chinese Ministry of Communication, forests and even entire villages—were burnt to cinders, at an estimated value of 20 to 30 million (1937) US dollars.
Establishment of the Nanking Safety Zone.
Many Westerners were living in the city at that time, conducting trade or on missionary trips. As the Japanese army approached Nanking, most of them fled the city, leaving 27 foreigners. Five of these were journalists who remained in the city a few days after it was captured, leaving the city on December 16. Fifteen of the remaining 22 foreigners formed a committee, called the International Committee for the Nanking Safety Zone in the western quarter of the city. German businessman John Rabe was elected as its leader, in part because of his status as a member of the Nazi Party and the existence of the German-Japanese bilateral Anti-Comintern Pact.
The Japanese government had previously agreed not to attack parts of the city that did not contain Chinese military forces, and the members of the Committee managed to persuade the Chinese government to move their troops out of the area.
On December 1, 1937, Nanking Mayor Ma Chao-chun ordered all Chinese citizens remaining in Nanking to move into the "Safety Zone". Many fled the city on December 7, and the International Committee took over as the "de facto" government of Nanking.
Prince Asaka appointed as commander.
In a memorandum for the palace rolls, Hirohito singled Prince Yasuhiko Asaka out for censure as the one imperial kinsman whose attitude was "not good". He assigned Asaka to Nanjing as an opportunity to make amends. It appears that Hirohito had never learned about, or had refused to admit, Asaka's role in the ensuing massacre.
On December 5, Asaka left Tokyo by plane and arrived at the front three days later. He met with division commanders, lieutenant-generals Kesago Nakajima and Heisuke Yanagawa, who informed him that the Japanese troops had almost completely surrounded 300,000 Chinese troops in the vicinity of Nanjing and that preliminary negotiations suggested that the Chinese were ready to surrender.
Prince Asaka is alleged to have issued an order to "kill all captives", thus providing official sanction for the crimes which took place during and after the battle. Some authors record that Prince Asaka signed the order for Japanese soldiers in Nanking to "kill all captives". Others assert that lieutenant colonel Isamu Chō, Asaka's aide-de-camp, sent this order under the Prince's sign manual without the Prince's knowledge or assent. Nevertheless, even if Chō took the initiative, Asaka was nominally the officer in charge and gave no orders to stop the carnage. When General Matsui arrived four days after it had begun, he issued strict orders that resulted in its eventual end.
While the extent of Prince Asaka's responsibility for the massacre remains a matter of debate, the ultimate sanction for the massacre and the crimes committed during the invasion of China were issued in Emperor Hirohito's ratification of the Japanese army's proposition to remove the constraints of international law on the treatment of Chinese prisoners on August 5, 1937.
Battle of Nanking.
Siege of the city.
The Japanese military continued to move forward, breaching the last lines of Chinese resistance, and arriving outside the walled city of Nanking on December 9.
Demand for surrender.
At noon on December 9, the military dropped leaflets into the city, urging the surrender of Nanking within 24 hours, promising annihilation if refused.
Meanwhile, members of the Committee contacted Tang and suggested a plan for three-day cease-fire, during which the Chinese troops could withdraw without fighting while the Japanese troops would stay in their present position.
General Tang agreed with this proposal if the International Committee could acquire permission of Generalissimo Chiang Kai-shek, who had already fled to Hankow to which he had temporarily shifted the military headquarters two days earlier.
John Rabe boarded the U.S. gunboat on December 9 and sent two telegrams, one to Chiang Kai-shek by way of the American ambassador in Hankow, and one to the Japanese military authority in Shanghai. The next day he was informed that Chiang Kai-shek, who had ordered that Nanking be defended "to the last man," had refused to accept the proposal.
Assault and capture of Nanking.
The Japanese awaited an answer to their demand for surrender but no response was received from the Chinese by the deadline on December 10. General Iwane Matsui waited another hour before issuing the command to take Nanking by force. The Japanese army mounted its assault on the Nanking walls from multiple directions; the SEF's 16th Division attacked three gates on the eastern side, the 6th Division of the 10A launched its offensive on the western walls, and the SEF's 9th Division advanced into the area in-between.
On December 12, under heavy artillery fire and aerial bombardment, General Tang Sheng-chi ordered his men to retreat. What followed was nothing short of chaos. Some Chinese soldiers stripped civilians of their clothing in a desperate attempt to blend in, and many others were shot by the Chinese supervisory unit as they tried to flee.
On 13 December, the 6th and the 116th Divisions of the Japanese Army were the first to enter the city, facing little military resistance. Simultaneously, the 9th Division entered nearby Guanghua Gate, and the 16th Division entered the Zhongshan and Taiping gates. That same afternoon, two small Japanese Navy fleets arrived on both sides of the Yangtze River.
Pursuit and mopping-up operations.
Japanese troops pursued the retreating Chinese army units, primarily in the Xiakuan area to the north of the city walls and around the Zijin Mountain in the east. Although most sources suggest that the final phase of the battle consisted of a one-sided slaughter of Chinese troops by the Japanese, some Japanese historians maintain that the remaining Chinese military still posed a serious threat to the Japanese. Prince Yasuhiko Asaka told a war correspondent later that he was in a very perilous position when his headquarters was ambushed by Chinese forces that were in the midst of fleeing from Nanking east of the city. On the other side of the city, the 11th Company of the 45th Regiment encountered some 20,000 Chinese soldiers who were making their way from Xiakuan.
The Japanese army conducted its mopping-up operation both inside and outside the Nanking Safety Zone. Since the area outside the safety zone had been almost completely evacuated, the mopping-up effort was concentrated in the safety zone. The safety zone, an area of 3.85 square kilometres, was packed with the remaining population of Nanking. The Japanese army leadership assigned sections of the safety zone to some units to separate alleged plain-clothed soldiers from the civilians.
Massacre.
Eyewitness accounts of Westerners and Chinese present at Nanking in the weeks after the fall of the city say that, over the course of six weeks following the fall of Nanking, Japanese troops engaged in rape, murder, theft, arson, and other war crimes. Some of these accounts, including the diaries of John Rabe and American Minnie Vautrin, came from foreigners who opted to stay behind to protect Chinese civilians from harm. Other accounts include first-person testimonies of Nanking Massacre survivors, eyewitness reports of journalists (both Western and Japanese), as well as the field diaries of military personnel. American missionary John Magee stayed behind to provide a 16 mm film documentary and first-hand photographs of the Nanking Massacre.
A group of foreign expatriates headed by Rabe had formed the 15-man International Committee on November 22 and mapped out the Nanking Safety Zone in order to safeguard civilians in the city, where the population numbered from 200,000 to 250,000. Rabe and American missionary Lewis S. C. Smythe, secretary of the International Committee and a professor of sociology at the University of Nanking, recorded the actions of the Japanese troops and filed complaints to the Japanese embassy.
Massacre contest.
In 1937, the "Osaka Mainichi Shimbun" and its sister newspaper, the "Tokyo Nichi Nichi Shimbun", covered a "contest" between two Japanese officers, Toshiaki Mukai and Tsuyoshi Noda of the Japanese 16th Division. The two men were described as vying to be the first to kill 100 people with a sword before the capture of Nanking. From Jurong to Tangshan (two cities in Jiangshu Province, China), Mukai had killed 89 people while Noda had killed 78 people. The contest continued because neither had killed 100 people. By the time they had arrived at Zijin Mountain, Noda had killed 105 people while Mukai had killed 106 people. Both officers supposedly surpassed their goal during the heat of battle, making it impossible to determine which officer had actually won the contest. Therefore, (according to journalists Asami Kazuo and Suzuki Jiro, writing in the Tokyo Nichi-Nichi Shimbun of December 13), they decided to begin another contest to kill 150 people. The "Nichi Nichi" headline of the story of December 13 read "'Incredible Record' the Contest to Behead 100 People—Mukai 106 – 105 Noda—Both 2nd Lieutenants Go Into Extra Innings".
After Japan surrendered, Mukai and Noda were arrested, each charged as a "Civilized Public Enemy", and executed at gunpoint in Nanking.
Rape.
The International Military Tribunal for the Far East estimated that, in an addition to children and the elderly, 20,000 women were raped. A large portion of these rapes were systematized in a process in which soldiers would go from door to door, searching for girls, with many women being captured and gang raped. The women were often killed immediately after being raped, often through explicit mutilation or by penetrating vaginas with bayonets, long sticks of bamboo, or other objects. Young children were not exempt from these atrocities and were cut open to allow Japanese soldiers to rape them.
On 19 December 1937, the Reverend James M. McCallum wrote in his diary:
I know not where to end. Never I have heard or read such brutality. Rape! Rape! Rape! We estimate at least 1,000 cases a night and many by day. In case of resistance or anything that seems like disapproval, there is a bayonet stab or a bullet ... People are hysterical ... Women are being carried off every morning, afternoon and evening. The whole Japanese army seems to be free to go and come as it pleases, and to do whatever it pleases.
On March 7, 1938, Robert O. Wilson, a surgeon at the American-administered University Hospital in the Safety Zone, wrote in a letter to his family, "a conservative estimate of people slaughtered in cold blood is somewhere about 100,000, including of course thousands of soldiers that had thrown down their arms".
Here are two excerpts from his letters of 15 and 18 December 1937 to his family:
The slaughter of civilians is appalling. I could go on for pages telling of cases of rape and brutality almost beyond belief. Two bayoneted corpses are the only survivors of seven street cleaners who were sitting in their headquarters when Japanese soldiers came in without warning or reason and killed five of their number and wounded the two that found their way to the hospital. 
Let me recount some instances occurring in the last two days. Last night the house of one of the Chinese staff members of the university was broken into and two of the women, his relatives, were raped. Two girls, about 16, were raped to death in one of the refugee camps. In the University Middle School where there are 8,000 people the Japs came in ten times last night, over the wall, stole food, clothing, and raped until they were satisfied. They bayoneted one little boy of eight who five bayonet wounds including one that penetrated his stomach, a portion of omentum was outside the abdomen. I think he will live. 
In his diary kept during the aggression against the city and its occupation by the Imperial Japanese Army, the leader of the Safety Zone, John Rabe, wrote many comments about Japanese atrocities. For 17 December:
Two Japanese soldiers have climbed over the garden wall and are about to break into our house. When I appear they give the excuse that they saw two Chinese soldiers climb over the wall. When I show them my party badge, they return the same way. In one of the houses in the narrow street behind my garden wall, a woman was raped, and then wounded in the neck with a bayonet. I managed to get an ambulance so we can take her to Kulou Hospital ... Last night up to 1,000 women and girls are said to have been raped, about 100 girls at Ginling College. . . alone. You hear nothing but rape. If husbands or brothers intervene, they're shot. What you hear and see on all sides is the brutality and bestiality of the Japanese soldiers. 
There are also accounts of Japanese troops forcing families to commit incestuous acts. Sons were forced to rape their mothers, fathers were forced to rape their daughters. One pregnant woman who was gang-raped by Japanese soldiers gave birth only a few hours later; although the baby appeared to be physically unharmed (Robert B. Edgerton, "Warriors of the Rising Sun"). Monks who had declared a life of celibacy were also forced to rape women.
Massacre of civilians.
Following the capture of Nanking, a massacre, which was perpetrated by the Imperial Japanese Army (IJA), led to the deaths of up to 60,000 residents in the city, a figure difficult to precisely calculate due to the many bodies deliberately burnt, buried in mass graves, or deposited in the Yangtze River by the IJA. Japanese ultra-nationalists have strongly disputed such death tolls, with some stating that only several hundred civilians were killed during the massacre. B. Campbell, in an article published in the journal "Sociological Theory", has described the Nanking Massacre as a genocide considering the fact that the residents were still unilaterally killed "en masse" during the aftermath, despite the successful and certain outcome in battle. On 13 December 1937, John Rabe wrote in his diary:
It is not until we tour the city that we learn the extent of destruction. We come across corpses every 100 to 200 yards. The bodies of civilians that I examined had bullet holes in their backs. These people had presumably been fleeing and were shot from behind. The Japanese march through the city in groups of ten to twenty soldiers and loot the shops ... I watched with my own eyes as they looted the café of our German baker Herr Kiessling. Hempel's hotel was broken into as well, as almost every shop on Chung Shang and Taiping Road.
On 10 February 1938, Legation Secretary of the German Embassy, Rosen, wrote to his Foreign Ministry about a film made in December by Reverend John Magee to recommend its purchase. Here is an excerpt from his letter and a description of some of its shots, kept in the Political Archives of the Foreign Ministry in Berlin.
During the Japanese reign of terror in Nanking – which, by the way, continues to this day to a considerable degree – the Reverend John Magee, a member of the American Episcopal Church Mission who has been here for almost a quarter of a century, took motion pictures that eloquently bear witness to the atrocities committed by the Japanese ... One will have to wait and see whether the highest officers in the Japanese army succeed, as they have indicated, in stopping the activities of their troops, which continue even today.
On December 13, about 30 soldiers came to a Chinese house at #5 Hsing Lu Koo in the southeastern part of Nanking, and demanded entrance. The door was open by the landlord, a Mohammedan named Ha. They killed him immediately with a revolver and also Mrs. Ha, who knelt before them after Ha's death, begging them not to kill anyone else. Mrs. Ha asked them why they killed her husband and they shot her. Mrs. Hsia was dragged out from under a table in the guest hall where she had tried to hide with her 1 year old baby. After being stripped and raped by one or more men, she was bayoneted in the chest, and then had a bottle thrust into her vagina. The baby was killed with a bayonet. Some soldiers then went to the next room, where Mrs. Hsia's parents, aged 76 and 74, and her two daughters aged 16 and 14 . They were about to rape the girls when the grandmother tried to protect them. The soldiers killed her with a revolver. The grandfather grasped the body of his wife and was killed. The two girls were then stripped, the elder being raped by 2–3 men, and the younger by 3. The older girl was stabbed afterwards and a cane was rammed in her vagina. The younger girl was bayoneted also but was spared the horrible treatment that had been meted out to her sister and mother. The soldiers then bayoneted another sister of between 7–8, who was also in the room. The last murders in the house were of Ha's two children, aged 4 and 2 respectively. The older was bayoneted and the younger split down through the head with a sword. 
Pregnant women were targeted for murder, as their stomachs were often bayoneted, sometimes after rape. Tang Junshan, survivor and witness to one of the Japanese army's systematic mass killings, testified:
The seventh and last person in the first row was a pregnant woman. The soldier thought he might as well rape her before killing her, so he pulled her out of the group to a spot about ten meters away. As he was trying to rape her, the woman resisted fiercely ... The soldier abruptly stabbed her in the belly with a bayonet. She gave a final scream as her intestines spilled out. Then the soldier stabbed the fetus, with its umbilical cord clearly visible, and tossed it aside.
According to Navy veteran Sho Mitani, "The Army used a trumpet sound that meant 'Kill all Chinese who run away'". Thousands were led away and mass-executed in an excavation known as the "Ten-Thousand-Corpse Ditch", a trench measuring about 300 m long and 5 m wide. Since records were not kept, estimates regarding the number of victims buried in the ditch range from 4,000 to 20,000. However, most scholars and historians consider the number to be more than 12,000 victims.
The Hui people, a minority Chinese group who are mainly Muslim, also suffered during the massacre, after which one mosque was found destroyed and others found to be "filled with dead bodies". Hui volunteers and imams buried over 100 Hui following Muslim ritual.
Extrajudicial killing of Chinese prisoners of war.
On 6 August 1937, Hirohito had personally ratified his army's proposition to remove the constraints of international law on the treatment of Chinese prisoners. This directive also advised staff officers to stop using the term "prisoner of war" (POW).
Immediately after the fall of the city, Japanese troops embarked on a determined search for former soldiers, in which thousands of young men were captured. Many were taken to the Yangtze River, where they were machine-gunned. What was probably the single largest massacre of Chinese troops occurred along the banks of the Yangtze River on December 18 in the Straw String Gorge Massacre. Japanese soldiers took most of the morning tying all of the POWs' hands together; in the dusk, the soldiers divided POWs into four columns and opened fire. Unable to escape, the POWs could only scream and thrash in desperation. It took an hour for the sounds of death to stop and even longer for the Japanese to bayonet each individual. Most were dumped into the Yangtze. It is estimated that at least 57,500 Chinese POWs were killed.
The Japanese troops gathered 1,300 Chinese soldiers and civilians at Taiping Gate and killed them. The victims were blown up with landmines, then doused with petrol before being set on fire. Those who were alive afterward were killed with bayonets. F. Tillman Durdin and Archibald Steele, American news correspondents, reported that they had seen bodies of killed Chinese soldiers forming mounds six feet high at the Nanking Yijiang gate in the north. Durdin, who was working for "The New York Times", toured Nanking before his departure from the city. He heard waves of machine-gun fire and witnessed the Japanese soldiers gun down some two hundred Chinese within ten minutes. Two days later, in his report to "The New York Times", he stated that the alleys and street were filled with civilian bodies, including women and children.
According to a testimony delivered by missionary Ralph L. Phillips to the U.S. State Assembly Investigating Committee, he was "forced to watch while the Japs disembowled a Chinese soldier" and "roasted his heart and liver and ate them".
Theft and arson.
One-third of the city was destroyed as a result of arson. According to reports, Japanese troops torched newly built government buildings as well as the homes of many civilians. There was considerable destruction to areas outside the city walls. Soldiers pillaged from the poor and the wealthy alike. The lack of resistance from Chinese troops and civilians in Nanking meant that the Japanese soldiers were free to divide up the city's valuables as they saw fit. This resulted in the widespread looting and burglary.
On 17 December, chairman John Rabe wrote a complaint to Kiyoshi Fukui, second secretary of the Japanese Embassy. The following is an excerpt:
In other words, on the 13th when your troops entered the city, we had nearly all the civilian population gathered in a Zone in which there had been very little destruction by stray shells and no looting by Chinese soldiers even in full retreat ... All 27 Occidentals in the city at that time and our Chinese population were totally surprised by the reign of robbery, raping and killing initiated by your soldiers on the 14th. All we are asking in our protest is that you restore order among your troops and get the normal city life going as soon as possible. In the latter process we are glad to cooperate in any way we can. But even last night between 8 and 9 p.m. when five Occidental members of our staff and Committee toured the Zone to observe conditions, we did not find any single Japanese patrol either in the Zone or at the entrances! 
Nanking Safety Zone and the role of foreigners.
The Japanese troops did respect the Zone to an extent; until the Japanese occupation, no shells entered that part of the city except a few stray shots. During the chaos following the attack of the city, some were killed in the Safety Zone, but the crimes that occurred in the rest of the city were far greater by all accounts.
The Japanese soldiers committed actions in the Safety Zone that were part of the larger Nanking Massacre. The International Committee appealed a number of times to the Japanese army, with Rabe using his credentials as a Nazi Party member, but to no avail. Rabe wrote that, from time to time, the Japanese would enter the Safety Zone at will, carry off a few hundred men and women, and either summarily execute them or rape and then kill them.
By February 5, 1938, the International Committee had forwarded to the Japanese embassy a total of 450 cases of murder, rape, and general disorder by Japanese soldiers that had been reported after the American, British and German diplomats had returned to their embassies.
It is said that Rabe rescued between 200,000 and 250,000 Chinese people.
Causes.
Jonathan Spence writes "there is no obvious explanation for this grim event, nor can one be found. The Japanese soldiers, who had expected easy victory, instead had been fighting hard for months and had taken infinitely higher casualties than anticipated. They were bored, angry, frustrated, tired. The Chinese women were undefended, their menfolk powerless or absent. The war, still undeclared, had no clear-cut goal or purpose. Perhaps all Chinese, regardless of sex or age, seemed marked out as victims."
Matsui's reaction to the massacre.
On December 18, 1937, as General Iwane Matsui began to comprehend the full extent of the rape, murder, and looting in the city, he grew increasingly dismayed. He reportedly told one of his civilian aides: "I now realize that we have unknowingly wrought a most grievous effect on this city. When I think of the feelings and sentiments of many of my Chinese friends who have fled from Nanking and of the future of the two countries, I cannot but feel depressed. I am very lonely and can never get in a mood to rejoice about this victory." He even let a tinge of regret flavor the statement he released to the press that morning: "I personally feel sorry for the tragedies to the people, but the Army must continue unless China repents. Now, in the winter, the season gives time to reflect. I offer my sympathy, with deep emotion, to a million innocent people." On New Year's Day, Matsui was still upset about the behavior of the Japanese soldiers at Nanking. Over a toast he confided to a Japanese diplomat: "My men have done something very wrong and extremely regrettable."
End of the massacre.
In late January 1938, the Japanese army forced all refugees in the Safety Zone to return home, immediately claiming to have "restored order". After the establishment of the "weixin zhengfu" (the collaborating government) in 1938, order was gradually restored in Nanking and atrocities by Japanese troops lessened considerably.
On 18 February 1938, the Nanking Safety Zone International Committee was forcibly renamed ""Nanking International Rescue Committee"", and the Safety Zone effectively ceased to function. The last refugee camps were closed in May 1938.
Recall of Matsui and Asaka.
In February 1938 both Prince Asaka and General Matsui were recalled to Japan. Matsui returned to retirement, but Prince Asaka remained on the Supreme War Council until the end of the war in August 1945. He was promoted to the rank of general in August 1939, though he held no further military commands.
Death toll estimates.
Estimates of the number of victims vary based on the definitions of the geographical range and the duration of the event.
The extent of the atrocities is debated, with numbers ranging from some Japanese claims of several hundred, to the Chinese claim of a non-combatant death toll of 300,000. Historian Tokushi Kasahara states "more than 100,000 and close to 200,000, or maybe more", referring to his own book. This estimation includes the surrounding area outside of the city of Nanking, which is objected to by a Chinese researcher (the same book, p. 146). Hiroshi Yoshida concludes "more than 200,000" in his book. Tomio Hora writes of 50,000–100,000 deaths.
Mainstream scholars consider figures from 40,000 to over 300,000 to be an accurate estimate. According to the International Military Tribunal for the Far East, estimates made at a later date indicate that the total number of civilians and prisoners of war murdered in Nanking and its vicinity during the first six weeks of the Japanese occupation was up to 200,000. These estimates are borne out by the figures of burial societies and other organizations, which testify to over 155,000 buried bodies. These figures do not take into account those persons whose bodies were destroyed by burning, drowning or by other means, or whose bodies were interred in mass graves.
According to the verdict of the Nanjing War Crimes Tribunal on 10 March 1947, there are "more than 190,000 mass slaughtered civilians and Chinese soldiers killed by machine gun by the Japanese army, whose corpses have been burned to destroy proof. Besides, we count more than 150,000 victims of barbarian acts buried by the charity organizations. We thus have a total of more than 300,000 victims." However, this estimate includes an accusation that the Japanese Army murdered 57,418 Chinese POWs at Mufushan, though the latest research indicates that between 4,000 and 20,000 were massacred, and it also includes the 112,266 corpses allegedly buried by the Chongshantang, a charitable association, though today mainstream historians agree that the Chongshantang's records were at least greatly exaggerated if not entirely fabricated. Bob Wakabayashi concludes from this that estimates of over 200,000 are not credible. Ikuhiko Hata considers the number 300,000 to be a "symbolic figure" representative of China's wartime suffering and not a figure to be taken literally.
Some researchers estimate that between 40,000 and 60,000 people were killed, which corresponds to the figures from three sources; one is the Red Army's official journal of the time, Hangdibao and another is that of Miner Searle Bates of the International Safety Zone Committee, and the third is the aforementioned figure written by John Rabe in a letter. John Rabe, Chairman of the International Committee and Nanking Safety Zone, estimated that between 50,000 and 60,000 (civilians) were killed. However, Erwin Wickert, the editor of "The diaries of John Rabe", points out that "It is likely that Rabe's estimate is too low, since he could not have had an overview of the entire municipal area during the period of the worst atrocities. Moreover, many troops of captured Chinese soldiers were led out of the city and down to the Yangtze, where they were summarily executed. But, as noted, no one actually counted the dead."
The casualty count of 300,000 was first promulgated in January 1938 by Harold Timperley, a journalist in China during the Japanese invasion, based on reports from contemporary eyewitnesses. Other sources, including Iris Chang's "The Rape of Nanking", also conclude that the death toll reached 300,000. In December 2007, newly declassified U.S. government archive documents revealed that a telegraph by the U.S. ambassador to Germany in Berlin sent one day after the Japanese army occupied Nanking, stated that he heard the Japanese Ambassador in Germany boasting that Japanese army killed 500,000 Chinese people as the Japanese army advanced from Shanghai to Nanking. According to the archives research "The telegrams sent by the U.S. diplomats Berlin pointed to the massacre of an estimated half a million people in Shanghai, Suzhou, Jiaxing, Hangzhou, Shaoxing, Wuxi and Changzhou".
In the 2010 Japan-China Joint History Research Committee meeting, scholars from the Japanese side set the maximum possible number of civilian victims at 200,000, with estimates of around 40,000 or 20,000. The Chinese scholars of the committee maintained that at least 300,000 were killed.
Range and duration.
The most conservative viewpoint is that the geographical area of the incident should be limited to the few km2 of the city known as the Safety Zone, where the civilians gathered after the invasion. Many Japanese historians seized upon the fact that during the Japanese invasion there were only 200,000–250,000 citizens in Nanking as reported by John Rabe, to argue that the PRC's estimate of 300,000 deaths is a vast exaggeration.
However, many historians include a much larger area around the city. Including the Xiaguan district (the suburbs north of Nanking, about 31 km2 in size) and other areas on the outskirts of the city, the population of greater Nanking was running between 535,000 and 635,000 civilians and soldiers just prior to the Japanese occupation. Some historians also include six counties around Nanking, known as the Nanking Special Municipality.
The duration of the incident is naturally defined by its geography: the earlier the Japanese entered the area, the longer the duration. The Battle of Nanking ended on December 13, when the divisions of the Japanese Army entered the walled city of Nanking. The Tokyo War Crime Tribunal defined the period of the massacre to the ensuing six weeks. More conservative estimates say that the massacre started on December 14, when the troops entered the Safety Zone, and that it lasted for six weeks. Historians who define the Nanking Massacre as having started from the time that the Japanese Army entered Jiangsu province push the beginning of the massacre to around mid-November to early December (Suzhou fell on November 19), and stretch the end of the massacre to late March 1938.
Various estimates.
Japanese historians, depending on their definition of the geographical and time duration of the killings, give wide-ranging estimates for the number of massacred civilians, from several thousand to upwards of 200,000. The lowest estimate by a Japanese historian is 40,000.
Chinese language sources tend to place the figure of massacred civilians upwards of 200,000. For example, a postwar investigation by the Nanking District Court put the number of dead during the incident as 295,525, 76% of them men, 22% women and 2% children.
A 42-part Taiwanese documentary produced from 1995 to 1997, entitled "An Inch of Blood For An Inch of Land" (一寸河山一寸血), asserts that 340,000 Chinese civilians died in Nanking City as a result of the Japanese invasion: 150,000 through bombing and crossfire in the five-day battle, and 190,000 in the massacre, based on the evidence presented at the Tokyo Trials.
War crimes tribunals.
Shortly after the surrender of Japan, the primary officers in charge of the Japanese troops at Nanking were put on trial. General Matsui was indicted before the International Military Tribunal for the Far East for "deliberately and recklessly" ignoring his legal duty "to take adequate steps to secure the observance and prevent breaches" of the Hague Convention. Hisao Tani, the lieutenant general of the 6th Division of the Japanese army in Nanking, was tried by the Nanjing War Crimes Tribunal.
Other Japanese military leaders in charge at the time of the Nanking Massacre were not tried. Prince Kan'in, chief of staff of the Imperial Japanese Army during the massacre, had died before the end of the war in May 1945. Prince Asaka was granted immunity because of his status as a member of the imperial family. Isamu Chō, the aide of Prince Asaka, and whom some historians believe issued the "kill all captives" memo, had committed suicide during the defense of Okinawa.
Grant of immunity to Prince Asaka.
On May 1, 1946, SCAP officials interrogated Prince Asaka, who was the ranking officer in the city at the height of the atrocities, about his involvement in the Nanking Massacre and the deposition was submitted to the International Prosecution Section of the Tokyo tribunal. Asaka denied the existence of any massacre and claimed never to have received complaints about the conduct of his troops. Whatever his culpability may have been, Asaka was not prosecuted before the International Military Tribunal for the Far East at least in part because under the pact concluded between General MacArthur and Hirohito, the Emperor himself and all the members of the imperial family were granted immunity from prosecution.
Evidence and testimony.
The prosecution began the Nanking phase of its case in July 1946. Dr. Robert O. Wilson, a surgeon and a member of the International Committee for the Nanking Safety Zone, took the witness stand first.
Other members of the International Committee for the Nanking Safety Zone who took the witness stand included Miner Searle Bates and John Magee. George A. Fitch, Lewis Smythe and James McCallum filed affidavits with their diaries and letters.
Another piece of evidence that was submitted to the tribunal was Harold Timperley's telegram regarding the Nanking Massacre which had been intercepted and decoded by the Americans on January 17, 1938.
One of the books by Hsü, Documents of the Nanking Safety Zone, was also adduced in court.
According to Matsui's own diary, one day after he made the ceremonial triumphal entry into the city on December 17, 1937, he instructed the chiefs of staff from each division to tighten military discipline and try to eradicate the sense of disdain for Chinese people among their soldiers.
On February 7, 1938, Matsui delivered a speech at a memorial service for the Japanese officers and men of the Shanghai Expeditionary Force who were killed in action. In front of the high-ranking officers, Domei News Agency reported, he emphasized the necessity to "put an end to various reports affecting the prestige of the Japanese troops."
The entry for the same day in Matsui's diary read, "I could only feel sadness and responsibility today, which has been overwhelmingly piercing my heart. This is caused by the Army's misbehaviors after the fall of Nanking and failure to proceed with the autonomous government and other political plans."
Matsui's defense.
Matsui's defence varied between denying the mass-scale atrocities and evading his responsibility for what had happened. Eventually he ended up making numerous conflicting statements.
In the interrogation in Sugamo prison preceding the trial Matsui admitted that he heard about the many outrages committed by his troops from Japanese diplomats when he entered Nanking on December 17, 1937.
In court, he contradicted the earlier testimony and told the judges that he was not "officially" briefed at the consulate about the evildoings, presumably to avoid admitting any contact with the consulate officials such as Second Secretary (later Acting Consul-General) Fukui Kiyoshi and Attaché Fukuda Tokuyasu who received and dealt with the protests filed by the International Committee.
In the same interrogation session before the trial Matsui said one officer and three low-ranking soldiers were court-martialled because of their misbehavior in Nanking and the officer was sentenced to death.
In his affidavit Matsui said he ordered his officers to investigate the massacre and to take necessary action. In court, however, Matsui said that he did not have jurisdiction over the soldiers' misconduct since he was not in the position of supervising military discipline and morals.
Matsui asserted that he had never ordered the execution of Chinese POWs. He further argued that he had directed his army division commanders to discipline their troops for criminal acts, and was not responsible for their failure to carry out his directives. At trial, Matsui went out of his way to protect Prince Asaka by shifting blame to lower ranking division commanders.
Verdict.
In the end the Tribunal convicted only two defendants to the Rape of Nanking.
Matsui was convicted of count 55, which charged him with being one of the senior officers who "deliberately and recklessly disregarded their legal duty virtue of their respective offices to take adequate steps to secure the observance the Laws and Customs of War and prevent breaches thereof, and thereby violated the laws of war."
Kōki Hirota, who had been the Foreign Minister when Japan conquered Nanking, was convicted of participating in "the formulation or execution of a common plan or conspiracy" (count 1), waging "a war of aggression and a war in violation of international laws, treaties, agreements and assurances against the Republic of China" (count 27) and count 55.
Matsui was convicted by a majority of the judges at the Tokyo tribunal who ruled that he bore ultimate responsibility for the "orgy of crime" at Nanking because, "He did nothing, or nothing effective, to abate these horrors."
Organized and wholesale murder of male civilians was conducted with the apparent sanction of the commanders on the pretext that Chinese soldiers had removed their uniforms and were mingling with the population. Groups of Chinese civilians were formed, bound with their hands behind their backs, and marched outside the walls of the city where they were killed in groups by machine gun fire and with bayonets. --- From Judgment of the International Military Tribunal
Radhabinod Pal, the member of the tribunal from India, dissented from the conviction arguing that the commander-in-chief must rely on his subordinate officers to enforce soldier discipline. "The name of Justice," Pal wrote in his dissent, "should not be allowed to be invoked only for ... vindictive retaliation."
Sentence.
On November 12, 1948, Matsui and Hirota, along with five other convicted Class-A war criminals, were sentenced to death by hanging. Eighteen others received lesser sentences. The death sentence imposed on Hirota, a six-to-five decision by the eleven judges, shocked the general public and prompted a petition on his behalf, which soon gathered over 300,000 signatures but did not succeed in commuting the Minister's sentence.
General Hisao Tani was sentenced to death by the Nanking War Crimes Tribunal.
Memorials.
In 1985, the Nanjing Massacre Memorial Hall was built by the Nanking Municipal Government in remembrance of the victims and to raise awareness of the Nanking Massacre. It is located near a site where thousands of bodies were buried, called the "pit of ten thousand corpses" ("wàn rén kēng").
In 1995, Daniel Kwan held a photograph exhibit in Los Angeles titled, "The Forgotten Holocaust".
In 2005, John Rabe's former residence in Nanking was renovated and now accommodates the "John Rabe and International Safety Zone Memorial Hall", which opened in 2006.
On December 13, 2014, China held its first Nanjing Massacre memorial day.
On October 9, 2015, Documents of Nanjing Massacre have been listed on the UNESCO Memory of the World Register.
Controversy.
China and Japan have both acknowledged the occurrence of wartime atrocities. Disputes over the historical portrayal of these events continue to cause tensions between Japan on one side and China and other East Asian countries on the other side.
Cold War.
Before the 1970s, China did relatively little to draw attention to the Nanking massacre. In her book "Rape of Nanking" Iris Chang asserted that the politics of the Cold War encouraged Mao to stay relatively silent about Nanking in order to keep a trade relationship with Japan. In turn, China and Japan occasionally used Nanking as an opportunity to demonize one another.
Debate in Japan.
The major waves of Japanese treatment of these events have ranged from total cover-up during the war, confessions and documentation by the Japanese soldiers during the 1950s and 1960s, minimization of the extent of the Nanking Massacre during the 1970s and 1980s, official Japanese government distortion and rewriting of history during the 1980s, and total denial of the occurrence of the Nanking Massacre by some government officials in 1990.
The debate concerning the massacre took place mainly in the 1970s. During this time, the Chinese government's statements about the event were attacked by the Japanese because they were said to rely too heavily on personal testimonies and anecdotal evidence. Aspersions were cast regarding the authenticity and accuracy of burial records and photographs presented in the Tokyo War Crime Court, which were said to be fabrications by the Chinese government, artificially manipulated or incorrectly attributed to the Nanking Massacre.
During the 1970s, Katsuichi Honda wrote a series of articles for the "Asahi Shimbun" on war crimes committed by Japanese soldiers during World War II (such as the Nanking Massacre). The publication of these articles triggered a vehement response from Japanese right-wingers regarding the Japanese treatment of the war crimes. In response, Shichihei Yamamoto and Akira Suzuki wrote two controversial yet influential articles which sparked the negationist movement.
In 1984, in an attempt to refute the allegations of war crimes in Nanking, the Japanese Army Veterans Association (Kaikosha) interviewed former Japanese soldiers who had served in the Nanking area from 1937 to 1938. Instead of refuting the allegations, the interviewed veterans confirmed that a massacre had taken place and openly described and admitted to taking part in the atrocities. The results of the survey were published in the association's magazine, "Kaiko", in 1985 along with an admission and apology that read, "Whatever the severity of war or special circumstances of war psychology, we just lose words faced with this mass illegal killing. As those who are related to the prewar military, we simply apologize deeply to the people of China. It was truly a regrettable act of barbarity."
Apology and condolences by the Prime Minister and Emperor of Japan.
On August 15, 1995, the fiftieth anniversary of the Surrender of Japan, the Japanese prime minister Tomiichi Murayama gave the first clear and formal apology for Japanese actions during the war. He apologized for Japan's wrongful aggression and the great suffering that it inflicted in Asia. He offered his heartfelt apology to all survivors and to the relatives and friends of the victims. That day, the prime minister and the Japanese Emperor Akihito pronounced statements of mourning at Tokyo's Nippon Budokan. The emperor offered his condolences and expressed the hope that such atrocities would never be repeated. Iris Chang, author of "The Rape of Nanking", criticized Murayama for not providing the written apology that had been expected. She said that the people of China "don't believe that an... unequivocal and sincere apology has ever been made by Japan to China" and that a written apology from Japan would send a better message to the international community.
Denials of the massacre by public officials in Japan.
In May 1994, Justice Minister Shigeto Nagano called the Nanjing Massacre a "fabrication".
On June 19, 2007, a group of around 100 Liberal Democratic Party (LDP) lawmakers again denounced the Nanjing Massacre as a fabrication, arguing that there was no evidence to prove the allegations of mass killings by Japanese soldiers. They accused Beijing of using the alleged incident as a "political advertisement".
On February 20, 2012, Takashi Kawamura, mayor of Nagoya, told a visiting delegation from Nanjing that the massacre "probably never happened". Two days later he defended his remarks, saying, "Even since I was a national Diet representative, I have said there was no [Nanjing massacre that resulted in murders of several hundred thousands of people." On April 1, 2013, Kawamura said his position remained unchanged when the issue came up during an election debate.
On February 24, 2012, Tokyo governor Shintaro Ishihara said that he also believes that the Nanjing massacre never happened. He reportedly claims it would have been impossible to kill so many people in such a short period of time. He believes the actual death toll was 10,000.
On February 3, 2014, Naoki Hyakuta, a member of the board of governors of Japan's public broadcasting company, NHK, was quoted as saying the massacre never occurred. He said that there were isolated incidents of brutality but no widespread atrocity, and criticized the Tokyo Trials figure of 200,000.
Legacy.
Effect on international relations.
The memory of the Nanking Massacre has been a stumbling block in Sino-Japanese relations since the early 1970s. Bilateral exchanges on trade, culture and education have increased greatly since the two countries normalized their bilateral relations and Japan became China's most important trading partner. Trade between the two nations is worth over $200 billion annually. Despite this, many Chinese people still have a strong sense of mistrust and animosity toward Japan that originates from the memory of Japanese war crimes such as the Nanking Massacre. This sense of mistrust is strengthened by the belief that Japan is unwilling to admit to and apologize for the atrocities.
Takashi Yoshida described how changing political concerns and perceptions of the "national interest" in Japan, China, and Western countries have shaped collective memory of the Nanking massacre. Yoshida asserted that over time the event has acquired different meanings to different people.
Many Japanese prime ministers have visited the Yasukuni Shrine, a shrine for dead Japanese soldiers of World War II, including some war criminals of the Nanking Massacre. In the museum adjacent to the shrine, a panel informs visitors that there was no massacre in Nanjing, but that Chinese soldiers in plain clothes were "dealt with severely". In 2006 former Japanese prime minister Junichiro Koizumi made a pilgrimage to the shrine despite warnings from China and South Korea. His decision to visit the shrine regardless sparked international outrage. Although Koizumi denied that he was trying to glorify war or historical Japanese militarism, The Chinese Foreign Ministry accused Koizumi of "wrecking the political foundations of China-Japan relations". An official from South Korea said they would summon the Tokyo ambassador to protest.
As a component of national identity.
Takashi Yoshida asserts that, "Nanking has figured in the attempts of all three nations Japan and the United States to preserve and redefine national and ethnic pride and identity, assuming different kinds of significance based on each country's changing internal and external enemies."
Japan.
In Japan, the Nanking Massacre touches upon national identity and notions of "pride, honor and shame". Yoshida argues that "Nanking crystallizes a much larger conflict over what should constitute the ideal perception of the nation: Japan, as a nation, acknowledges its past and apologizes for its wartime wrongdoings; or ... stands firm against foreign pressures and teaches Japanese youth about the benevolent and courageous martyrs who fought a just war to save Asia from Western aggression." Recognizing the Nanking Massacre as such can be viewed in some circles in Japan as "Japan bashing" (in the case of foreigners) or "self-flagellation" (in the case of Japanese).
The majority of Japanese acknowledge that Japanese troops committed atrocities during the Nanking Massacre. Some Japanese officials and writers have openly denied the incident, claiming it to be propaganda designed to spark an anti-Japan movement. In many ways, how "atrocious" the massacre was is the touchstone of left–right divide in Japan; i.e., leftists feel this is a defining moment of the Imperial Japanese Army; rightists believe Perry's opening of Japan and the atomic bombings are far more significant events.
The government of Japan believe it can not be denied that the killing of a large number of noncombatants, looting and other acts by Japanese army occurred. However, the actual number of victims is hard to be determined according to government of Japan. In the 2010 Japan-China Joint History Research Committee meeting, scholars from the Japanese side set the maximum possible number of civilian victims at 200,000, with estimates of around 40,000 or 20,000. The Chinese scholars of the committee maintained that at least 300,000 were killed.
According to a brief reference to Nanking at the Yasukuni museum in Tokyo, the Japanese general in charge gave his men maps showing foreign settlements and a civilian "safety zone", and ordered them to maintain strict military discipline. The visitor is left to assume they did. The museum notes only that "Chinese soldiers disguised in civilian clothes were severely prosecuted".
This nationalist view does not, however, represent a widely shared understanding of what happened at Nanking, as illustrated by Japanese textbooks' rather different treatment of the atrocity. While the books' take on Nanking is stilted and feels like the product of a committee, in various versions they acknowledge the deaths of thousands of Chinese including women and children, as well as looting, arson and assaults by Japanese soldiers. They do not spell out the sexual nature of these assaults.
"During this period, when the Japanese Army occupied Nanjing it killed a large number of Chinese and carried out looting, arson and assaults. In regard to the number of victims of this Nanjing Massacre... the Tokyo (War Crime) Trials later found it in excess of 200,000, and prosecuted Japan's responsibility severely," reads one Japanese textbook.
China.
The Nanking massacre has emerged as a fundamental keystone in the construction of the modern Chinese national identity. Modern Chinese (including citizens of the PRC, Taiwan, and overseas) will refer to the Nanking Massacre to explain certain stances they hold or ideas they have; this 'national unifying event' holds true to middle-school educated peasants and to senior government officials alike.
Records.
In December 2007, the PRC government published the names of 13,000 people who were killed by Japanese troops in the Nanking Massacre. According to Xinhua News Agency, it is the most complete record to date. The report consists of eight volumes and was released to mark the 70th anniversary of the start of the massacre. It also lists the Japanese army units that were responsible for each of the deaths and states the way in which the victims were killed. Zhang Xianwen, editor-in-chief of the report, states that the information collected was based on "a combination of Chinese, Japanese and Western raw materials, which is objective and just and is able to stand the trial of history." This report formed part of a 55-volume series (Collection of Historical Materials of Nanjing Massacre (南京大屠杀史料集) about the massacre.

</doc>
<doc id="55072" url="https://en.wikipedia.org/wiki?curid=55072" title="Puyi">
Puyi

Puyi (; 7 February 190617 October 1967), of the Manchu Aisin Gioro clan, commonly known as Henry Pu Yi, was the last Emperor of China and the twelfth and final ruler of the Qing dynasty. When a child, he ruled as the Xuantong Emperor () from 1908 until his forced abdication on 12 February 1912, after the successful Xinhai Revolution. From 1 to 12 July 1917, he was briefly restored to the throne as emperor by the warlord Zhang Xun. In 1932, after the occupation of Manchuria the state of Manchukuo was established by Japan, and he was chosen to become 'Chief Executive' of the new state using the era-name of Datong (Ta-tung). In 1934, he was declared the Kangde Emperor (or Kang-te Emperor) of Manchukuo and ruled until the end of the Second Sino-Japanese War in 1945. After the People's Republic of China was established in 1949, Puyi was imprisoned as a war criminal for 10 years, wrote his memoirs and became a titular member of the Chinese People's Political Consultative Conference and the National People's Congress.
Names and titles.
Name.
Puyi's name is romanised in English as either "Puyi" or "Pu-yi". This naming is in accordance with the Manchu tradition of avoiding the use of a person's clan name and given name together, but is in complete contravention of Chinese tradition, whereby the given name of a ruler was considered taboo and ineffable. Using a former emperor's personal name (or even using a Chinese character from the name) was a punishable offense under traditional Chinese law. However, after Puyi lost his imperial title in 1924, he was officially styled "Mr. Puyi" (Mr. Pu-yi; ) in Chinese. His clan name "Aisin Gioro" () was seldom used.
Puyi also adopted other names — his "zi" (字; courtesy name) was "Yaozhi" (), and his "hao" (號; pseudonym) was "Haoran" ().
Puyi is also known to have used a Western given name, "Henry," which was chosen by his English-language teacher, Scotsman Reginald Johnston.
Titles.
When he ruled as Emperor of the Qing Dynasty from 1908 to 1912 and during his brief restoration in 1917, Puyi's era name was "Xuantong", so he was known as the "Xuantong Emperor" () during those two periods of time.
As Puyi was also the last ruling Emperor of China, he is widely known as "The Last Emperor" () in China and throughout the rest of the world. Some refer to him as "The Last Emperor of the Qing Dynasty" ().
Due to his abdication, Puyi is also known as "Xun Di" () or "Fei Di" (). Sometimes a "Qing" () is added in front of the two titles to indicate his affiliation with the Qing Dynasty.
When Puyi ruled the puppet state of Manchukuo and assuming the title of Chief Executive of the new state, his era name was "Datong" (Ta-tung). And he became the emperor from 1934 to 1945, his era name was "Kangde" (Kang-te), so he was known as the "Kangde Emperor" (, ) during that period of time.
Ancestry.
Paternal side.
Puyi's great-grandfather was the Daoguang Emperor (r. 1820–1850), who was succeeded by his fourth son, the Xianfeng Emperor (r. 1850–1861).
Puyi's paternal grandfather was Yixuan, Prince Chun (1840–1891), the seventh son of the Daoguang Emperor and a younger half-brother of the Xianfeng Emperor. The Xianfeng Emperor was succeeded by his only son, who became the Tongzhi Emperor (r. 1861–1875).
The Tongzhi Emperor died at the age of 18 without a son, and was succeeded by the Guangxu Emperor (r. 1875–1908), son of 1st Prince Chun and Lady Yehenara Wanzhen (younger sister of Empress Dowager Cixi). The Guangxu Emperor died without an heir.
Puyi, who succeeded the Guangxu Emperor, was the eldest son of Zaifeng, Prince Chun, who was born to Yixuan, Prince Chun and his second concubine Lady Lingiya (1866–1925). Lady Lingiya used to be a maid in the residence of Yixuan. Born to a Han Bannerman family, her original family name was Liu (劉), and this was changed to the Manchu clan name Lingiya when she became the concubine of Yixuan and was transferred to a Manchu banner. Zaifeng was therefore a younger half-brother of the Guangxu Emperor and the first in line to succession after Guangxu.
Puyi was in a branch of the Aisin Gioro clan with close ties to Empress Dowager Cixi, who was from the Yehenara clan. Cixi's niece, who later became Empress Dowager Longyu (1868–1913), was married to the Guangxu Emperor.
Puyi had a younger full brother, Pujie (1907–1994), who married a cousin of Emperor Hirohito, Lady Hiro Saga. The rules of succession were changed to allow Pujie to succeed Puyi, who had no children.
Puyi's last surviving younger half-brother Puren (b. 1918) has adopted the Chinese name Jin Youzhi and lived in China until his death in 2015. In 2006 Jin Youzhi filed a lawsuit in regards to the rights to Puyi's image and privacy. The lawsuit claimed that those rights were violated by the exhibit "China's Last Monarch and His Family".
Puyi's second cousin, Pu Xuezhai (溥雪齋), was a musician who played the guqin, and an artist of Chinese painting.
Maternal side.
Puyi's mother was Youlan (1884–1921), the daughter of Ronglu (1836–1903), a statesman and general from the Guwalgiya clan. Ronglu was one of the leaders of the conservative faction in the Qing court, and a staunch supporter of Empress Dowager Cixi; Cixi rewarded his support by marrying his daughter, Puyi's mother, into the imperial family.
The Guwalgiya clan was regarded as one of the most powerful Manchu clans in the Qing Dynasty. Oboi, an influential military commander and statesman who was a regent during the Kangxi Emperor's reign, was from the Guwalgiya clan.
Biography.
Emperor of China (1908–1912).
Chosen by Empress Dowager Cixi on her deathbed, Puyi became emperor at the age of 2 years and 10 months in December 1908 after the Guangxu Emperor died on 14 November. Titled the Xuantong Emperor (Wade-Giles: Hsuan-tung Emperor), Puyi's introduction to the life of an emperor began when palace officials arrived at his family residence to take him. The toddler Puyi screamed and resisted as the officials ordered the eunuch attendants to pick him up. His father, Prince Chun, became Prince Regent (摄政王). During Puyi's coronation in the Hall of Supreme Harmony, the young emperor was carried onto the throne by his father. Puyi was so frightened by the scene before him and the deafening sounds of ceremonial drums and music that he started crying. His father could do nothing except to quietly comfort him, "Don't cry, it'll be over soon."
Puyi's wet nurse, Wen-Chao Wang, was the only one who could console him, and therefore she accompanied him to the Forbidden City. Puyi did not see his biological mother, Princess Consort Chun, for the next seven years. He developed a special bond with Wen-Chao Wang and credited her with being the only person who could control him. She was sent away when he was eight years old. After Puyi married, he would occasionally bring her to the Forbidden City, and later Manchukuo, to visit him. After his special government pardon in 1959, he visited her adopted son and only then learned of her personal sacrifices to be his nurse.
Puyi's upbringing was hardly conducive to the raising of a healthy, well-balanced child. Overnight, he was treated as a king and unable to behave as a child. The adults in his life, except for his wet-nurse Wen-Chao Wang, were all strangers, remote, distant, and unable to discipline him. Wherever he went, grown men would kneel down in a ritual kowtow, averting their eyes until he passed. Soon the young Puyi discovered the absolute power he wielded over the eunuchs, and he frequently had them beaten for small transgressions.
Eunuchs and the Household Department.
Quotation of Puyi:
No account of my childhood would be complete without mentioning the eunuchs. They waited on me when I ate, dressed and slept; they accompanied me on my walks and to my lessons; they told me stories; and had rewards and beatings from me, but they never left my presence. They were my slaves; and they were my earliest teachers.
After his marriage, Puyi began to take control of the palace. He described "an orgy of looting" taking place that involved "everyone from the highest to the lowest". According to Puyi, by the end of his wedding ceremony, the pearls and jade in the empress's crown had been stolen. Locks were broken, areas ransacked, and on June 27, 1923, a fire destroyed the area around the Palace of Established Happiness. Puyi suspected it was arson to cover theft. The emperor overheard conversations among the eunuchs that made him fear for his life. In response, he evicted the eunuchs from the palace. His own brother, Pujie, was rumored to steal treasures and art collections and sell to wealthy collectors in the black market. His next plan of action was to reform the Household Department. In this period, he brought in more outsiders to replace the traditionally aristocratic officers in order to improve the accountability. He appointed Zheng Xiaoxu as the minister of Household Department and Zheng Xiaoxu hired Tong Jixu, a former Air Force officer from the Beiyang Army, as his chief of staff to clean up the act. However, the reform did not last long before Puyi was forced out of the Forbidden City by Feng Yuxiang.
Abdication.
Puyi's father, Prince Chun, served as a regent until 6 December 1911 when Empress Dowager Longyu took over following the Xinhai Revolution.
Empress Dowager Longyu endorsed the "Imperial Edict of the Abdication of the Qing Emperor" (清帝退位詔書) on 12 February 1912 under a deal brokered by Yuan Shikai (a general of the Beiyang Army) with the imperial court in Beijing and the Republicans in southern China. Signed with the new Republic of China, Puyi was to retain his imperial title and be treated by the government of the Republic with the protocol attached to a foreign monarch. This was similar to Italy's Law of Guarantees (1870) which accorded the Pope certain honors and privileges similar to those enjoyed by the King of Italy. Puyi and the imperial court were allowed to remain in the northern half of the Forbidden City (the Private Apartments) as well as in the Summer Palace. A hefty annual subsidy of four million silver taels was granted by the Republic to the imperial household, although it was never fully paid and was abolished after just a few years.
The Articles of Favourable Treatment of the Great Qing Emperor after his Abdication.
The document is dated 26 December 1914.
Brief restoration (1917).
In 1917 the warlord Zhang Xun restored Puyi to the throne from July 1 to July 12. Zhang Xun ordered his army to keep their queues to display loyalty to the emperor. During that period of time, a small bomb was dropped over the Forbidden City by a Republican plane, causing minor damage. This is considered the first aerial bombardment ever in East Asia. The restoration failed due to extensive opposition across China, and the decisive intervention of another warlord, Duan Qirui.
The "Articles of Favourable Treatment of the Great Qing Emperor after his Abdication" (清帝退位 優待條件) were revised on November 5, 1924, after the coup by General Feng Yuxiang: the revised articles stated that Puyi was losing his imperial title and henceforth becoming a regular citizen of the Republic of China. Puyi was expelled from the Forbidden City that same day.
Life in the Forbidden City.
Reginald Johnston was appointed as Puyi's English tutor in 1919. Puyi could not speak Manchu; he only knew a single word in the language, "Yili," which meant arise. Despite studying Manchu for years, he admitted that it was his "worst" subject among everything he studied. According to the journalist S. M. Ali, Puyi spoke Mandarin when interviewed but Ali believed that he could understand English.
Reginald Johnston arranged for the Marquis of Extended Grace Zhu Yuxun, a descendant of the Ming dynasty Imperial family, to visit Puyi in the Forbidden City in September 1924, which was the first time the heirs of both the deposed Ming and Qing dynasties came face to face.
Residence in Tianjin (1925–1931).
Puyi was expelled from the Forbidden City by warlord Feng Yuxiang who denounced the previous agreement with the Qing imperial house following a coup in 1924. Puyi spent a few days at the house of his father Prince Chun, and then temporarily resided in the Japanese embassy in Beijing. In February 1925, he moved to the Japanese Concession of Tianjin, first into the Zhang Garden (張園), and in 1927 into the former residence of Lu Zongyu known as the Garden of Serenity (). During this period, Puyi and his advisers Chen Baochen, Zheng Xiaoxu and Luo Zhenyu discussed plans to restore Puyi as Emperor. Zheng and Luo favoured enlisting assistance from external parties, while Chen opposed the idea. In September 1931 Puyi sent a letter to Jirō Minami, the Japanese Minister of War, expressing his desire to be restored to the throne. He was visited by Kenji Doihara, head of the espionage office of the Japanese Kwantung Army, who proposed establishing Puyi as head of a Manchurian state. In the Tientsin Incident during November 1931, Puyi and Zheng Xiaoxu traveled to Manchuria to complete plans for the puppet state of Manchukuo. The Chinese government ordered Puyi's arrest for treason, but was unable to breach the Japanese protection. Chen Baochen returned to Beijing where he died in 1935.
Ruler of Manchukuo (1932–1945).
On 1 March 1932, Puyi was installed by the Japanese as the Chief Executive of Manchukuo, a puppet state of the Empire of Japan, under the reign title Datong (Wade-Giles: Ta-tung; 大同). In 1934, he was officially crowned the emperor of Manchukuo under the reign title Kangde (Wade-Giles: Kang-te; 康德). He was constantly at odds with the Japanese in private, though submissive in public. He resented being "Head of State" and then "Emperor of Manchukuo" rather than being fully restored as a Qing Emperor. Puyi lived in a palace (now the Museum of the Imperial Palace of the Manchu State) in this period. At his enthronement he clashed with Japan over dress; they wanted him to wear a Manchukuo-style uniform whereas he considered it an insult to wear anything but traditional Manchu robes. In a typical compromise, he wore a Western military uniform to his enthronement (the only Chinese emperor ever to do so) and a dragon robe to the announcement of his accession at the Temple of Heaven.
Puyi's younger full brother Pujie, who married Lady Hiro Saga, a distant cousin to the Japanese Emperor Hirohito, was proclaimed heir apparent. The marriage had been politically arranged by Shigeru Honjō, a general of the Kwantung Army. Puyi thereafter would not speak candidly in front of his brother and refused to eat any food provided by Hiro Saga. Puyi was forced to sign an agreement that if he himself had a male heir, the child would be sent to Japan to be raised by the Japanese.
From 1935 to 1945 Kwantung Army senior staff officer Yoshioka Yasunori (吉岡安則) was assigned to Puyi as Attaché to the Imperial Household in Manchukuo. He acted as a spy for the Japanese government, controlling Puyi through fear, intimidation, and direct orders. There were many attempts on Puyi's life during this period, including a 1937 stabbing by a palace servant. During Puyi's reign as Emperor of Manchukuo, his household was closely watched by the Japanese, who increasingly took steps toward the full Japanisation of Manchuria, to prevent him from becoming too independent. He was feted by the Japanese populace during his visits there, but had to remain subservient to Emperor Hirohito. It is unclear whether the adoption of ancient Chinese styles and rites, such as using "His Majesty" instead of his real name, was the product of Puyi's interest or a Japanese imposition of their own imperial house rules.
During these years, Puyi began taking a greater interest in traditional Chinese law and religion (such as Confucianism and Buddhism), but this was disallowed by the Japanese. Gradually his old supporters were eliminated and pro-Japanese ministers put in their place. During this period Puyi's life consisted mostly of signing laws prepared by Japan, reciting prayers, consulting oracles, and making formal visits throughout his state.
By 1940, the Japanisation of Manchuria had become extreme, and an altar to the Shinto goddess Amaterasu was built on the grounds of Puyi's palace. The origins of the altar are unclear, with the postwar Japanese claiming that Puyi aimed for a closer connection to the Japanese Emperor as a means of resisting the political machinations of the Manchukuo elites, while Puyi in his Chinese Communist-published autobiography claims that he was forced to submit to this by the Japanese. In any case, Puyi's wartime duties came to include sitting through Chinese-language Shinto prayers. Hirohito was surprised when he heard of this, asking why a Temple of Heaven had not been built instead.
Later life (1945–1967).
At the end of World War II, Puyi was captured by the Soviet Red Army on 16 August 1945 while he was in an aeroplane fleeing to Japan. The Soviets took him to the Siberian town of Chita. He lived in a sanatorium, then later in Khabarovsk near the Chinese border.
In 1946, he testified at the International Military Tribunal for the Far East in Tokyo, detailing his resentment at how he had been treated by the Japanese.
When the Chinese Communist Party under Mao Zedong came to power in 1949, Puyi was repatriated to China after negotiations between the Soviet Union and China. Except for a period during the Korean War, when he was moved to Harbin, Puyi spent ten years in the Fushun War Criminals Management Centre in Liaoning province until he was declared reformed. Puyi came to Beijing in 1959 with special permission from Chairman Mao Zedong and lived the next six months in an ordinary Beijing residence with his sister before being transferred to a government-sponsored hotel. He voiced his support for the Communists and worked at the Beijing Botanical Gardens. At the age of 56, he married Li Shuxian, a hospital nurse, on 30 April 1962, in a ceremony held at the Banquet Hall of the Consultative Conference. From 1964 until his death he worked as an editor for the literary department of the Chinese People's Political Consultative Conference, where his monthly salary was around 100 yuan.
In the 1960s, with encouragement from Chairman Mao Zedong and Premier Zhou Enlai, and the public endorsement of the Chinese government, Puyi wrote his autobiography "Wo De Qian Ban Sheng" (; translated into English as "From Emperor to Citizen") together with Li Wenda, an editor at the People's Publishing Bureau. In this book (as translated into English and published by Oxford University Press), he made the following statement regarding his testimony at the Tokyo War Crimes Tribunal:
Death and burial.
Mao Zedong started the Cultural Revolution in 1966, and the youth militia known as the Red Guards saw Puyi, who symbolised Imperial China, as an easy target of attack. Puyi was placed under protection by the local public security bureau and, although his food rations, salary, and various luxuries, including his sofa and desk, were removed, he was not publicly humiliated as was common at the time. But by now, Puyi had aged and his health began to decline. He died in Beijing of complications arising from kidney cancer and heart disease on 17 October 1967 at the age of 61.
In accordance with the laws of the People's Republic of China at the time, Puyi's body was cremated. His ashes were first placed at the Babaoshan Revolutionary Cemetery, alongside those of other party and state dignitaries. (This was the burial ground of imperial concubines and eunuchs prior to the establishment of the People's Republic of China.) In 1995, as a part of a commercial arrangement, Puyi's widow transferred his ashes to a new commercial cemetery named Hualong Imperial Cemetery (华龙皇家陵园) in return for monetary support. The cemetery is located near the Western Qing Tombs, southwest of Beijing, where four of the nine Qing emperors preceding him are interred, along with three empresses and 69 princes, princesses and imperial concubines.
Family.
Quotation from Puyi:
"The Pedigree of the Qing House" flow chart can be found in Puyi's autobiography.
Siblings.
Puyi had three younger brothers:
Puyi had seven younger sisters, the first three were his full sisters:
Spouses.
Quotation from Puyi (referring only to his first four wives):
In 1921, it was decided by the Dowager Consorts (the four widows of the emperors before Puyi) that it was time for the 15-year-old Puyi to be married, although court politics dragged the complete process (from selecting the bride, up through the wedding ceremony) out for almost two years. Puyi saw marriage as his coming of age benchmark, when others would no longer control him. He was given four photographs to choose from. Puyi stated they all looked alike to him, with the exception of different clothing. He chose Wenxiu. Political factions within the palace made the actual choice as to whom Puyi would marry. The selection process alone took an entire year.
Wanrong.
Puyi's second choice for his wife was Wanrong, a Daur. She married Puyi in 1922 and became his Empress. Her father, Rong Yuan (榮源), was a Minister of Domestic Affairs. She was considered beautiful and came from a wealthy family. By Puyi's own account, he abandoned Wanrong in the bridal chamber and went back to his own room. He maintained that she was willing to be a wife in name only, in order to carry the title of Empress. The couple's relationship was good initially, and Puyi showed preference over Wenxiu for Wanrong and displayed trust in her. However, after Wenxiu left in 1931, Puyi blamed Wanrong and stopped speaking to her and ignored her presence. She became addicted to opium, and eventually died in a prison in Yanji, Jilin after being arrested by Chinese Communist soldiers.
Wenxiu.
Puyi's first choice for his wife was Wenxiu, from the Erdet (鄂爾德特) clan. She married Puyi in 1922. Although she was Puyi's first choice, the Four Dowager Consorts felt that Wenxiu came from an unacceptable impoverished family and was not beautiful enough to be Empress, so they told the court officials to ask Puyi to choose again. The second time Puyi chose Wanrong, who became Empress, while Wenxiu was designated as Consort Shu (淑妃). Puyi and Wenxiu divorced in 1931. Puyi awarded her a house in Beijing and $300,000 in alimony, to be provided by the Japanese. In his autobiography, Puyi stated her reason for the divorce was the emptiness of life with him in exile, her desire for an ordinary family life, and his own inability to see women as anything but slaves and tools of men. According to Puyi, she worked as a school teacher for some years after the divorce. She married Major Liu Zhendong in 1947.
Tan Yuling.
Puyi's third wife, Tan Yuling, was a Manchu of the Tatara (他他拉) clan. She married Puyi in 1937 at the age of 16 on the recommendation of the daughter of Yulang (毓朗), a "beile". She was designated as Puyi's Concubine Xiang (祥貴人). Puyi married her as "punishment" for Wanrong, and, ""...because a second wife was as essential as palace furniture."" She was also a wife in name only. She became ill in 1942 with typhoid, which the Japanese doctor said would not be fatal. After the doctor's consultation with Attaché to the Imperial Household Yasunori Yoshioka, Tan Yuling suddenly died. Puyi became suspicious of the circumstances when the Japanese immediately offered him photographs of Japanese girls for marriage. Puyi posthumously granted her the title Noble Consort Mingxian (明賢貴妃).
Li Yuqin.
In 1943 Puyi married his fourth wife, a 15-year-old student named Li Yuqin, who was a Han Chinese from Changchun, Jilin. She was designated as Puyi's Concubine Fu (福貴人). In February 1943, school principal Kobayashi and teacher Fujii of the Nan-Ling Girls Academy took ten girl students to a photography studio for portraits. Three weeks later, the school teacher and the principal visited Li Yuqin's home and told her Puyi ordered her to go to the Manchukuo palace to study. She was first taken directly to Yasunori Yoshioka who thoroughly questioned her. Yoshioka then drove her back to her parents and told them Puyi ordered her to study at the palace. Money was promised to the parents. She was subjected to a medical examination and then taken to Puyi's sister Yunhe and instructed in palace protocol.
Li Shuxian.
In 1962 under an arrangement with premier Zhou Enlai, Puyi married his fifth and last wife, Li Shuxian, a nurse of Han Chinese ethnicity. They had no children. She died of lung cancer in 1997. Li Shuxian recounted that they dated for six months before the marriage, and she found him to be, ""...a man who desperately needed my love and was ready to give me as much love as he could.""
Notes.
¹ Aisin-Gioro is the clan's name in Manchu, pronounced Àixīn Juéluó in Mandarin; Pǔyí is the Chinese given name as pronounced in Mandarin.

</doc>
